{"id": "2508.01255", "pdf": "https://arxiv.org/pdf/2508.01255", "abs": "https://arxiv.org/abs/2508.01255", "authors": ["Cuong Chi Le", "Cuong Duc Van", "Tung Duy Vu", "Thai Minh Pham Vu", "Hoang Nhat Phan", "Huy Nhat Phan", "Tien N. Nguyen"], "title": "TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Regression testing ensures that code changes do not unintentionally break\nexisting functionality. While recent advances in large language models (LLMs)\nhave shown promise in automating test generation for regression testing, they\noften suffer from limited reasoning about program execution, resulting in\nstagnated coverage growth - a phenomenon known as the coverage plateau. In this\npaper, we present TestWeaver, a novel LLM-based approach that integrates\nlightweight program analysis to guide test generation more effectively.\nTestWeaver introduces three key innovations: (1) it reduces hallucinations and\nimproves focus by supplying the LLM with the backward slice from the target\nline instead of full program context; (2) it identifies and incorporates close\ntest cases - those that share control-flow similarities with the path to the\ntarget line - to provide execution context within the LLM's context window; and\n(3) it enhances LLM's reasoning with execution in-line annotations that encode\nvariable states as comments along executed paths. By equipping LLMs with these\ntargeted and contextualized inputs, TestWeaver improves coverage-guided test\ngeneration and mitigates redundant explorations. Empirical results demonstrate\nthat TestWeaver accelerates code coverage growth and generates more effective\nregression test cases than existing LLM-based approaches.", "AI": {"tldr": "TestWeaver\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7a0b\u5e8f\u5206\u6790\u6307\u5bfc\u6d4b\u8bd5\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u8986\u76d6\u5e73\u53f0\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u56de\u5f52\u6d4b\u8bd5\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u56de\u5f52\u6d4b\u8bd5\u751f\u6210\u4e2d\u7531\u4e8e\u7a0b\u5e8f\u6267\u884c\u63a8\u7406\u6709\u9650\uff0c\u5bfc\u81f4\u8986\u76d6\u589e\u957f\u505c\u6ede\uff0cTestWeaver\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TestWeaver\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u7a0b\u5e8f\u5206\u6790\uff0c\u63d0\u4f9b\u76ee\u6807\u884c\u7684\u53cd\u5411\u5207\u7247\u3001\u5171\u4eab\u63a7\u5236\u6d41\u76f8\u4f3c\u6027\u7684\u6d4b\u8bd5\u7528\u4f8b\u4ee5\u53ca\u6267\u884c\u5185\u8054\u6ce8\u91ca\uff0c\u4ee5\u4f18\u5316LLM\u7684\u6d4b\u8bd5\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTestWeaver\u52a0\u901f\u4e86\u4ee3\u7801\u8986\u76d6\u589e\u957f\uff0c\u751f\u6210\u7684\u56de\u5f52\u6d4b\u8bd5\u7528\u4f8b\u6bd4\u73b0\u6709LLM\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "TestWeaver\u901a\u8fc7\u6539\u8fdbLLM\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u548c\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u5f52\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2508.01337", "pdf": "https://arxiv.org/pdf/2508.01337", "abs": "https://arxiv.org/abs/2508.01337", "authors": ["Wei Liu", "Linqiang Guo", "Yi Wen Heng", "Chenglin Li", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "Screencast-Based Analysis of User-Perceived GUI Responsiveness", "categories": ["cs.SE"], "comment": null, "summary": "GUI responsiveness is critical for a positive user experience in mobile\napplications. Even brief delays in visual feedback can frustrate users and lead\nto negative reviews. However, detecting and quantifying such user-perceived\ndelays remains challenging, especially in industrial testing pipelines that\nevaluate thousands of apps daily across diverse devices and OS versions.\nExisting techniques based on static analysis or system metrics, while useful,\nmay not accurately capture user-perceived issues or scale effectively.\n  In this experience paper, we present \\tool, a lightweight and black-box\ntechnique that measures GUI responsiveness directly from mobile screencasts --\nvideo recordings captured during automated GUI testing. \\tool detects user\ninteractions and visual delays, helping developers identify GUI performance\nissues that affect the user experience. It uses computer vision to detect user\ninteractions and analyzes frame-level visual changes to compute two key\nmetrics: response time (from user action to first visual feedback) and finish\ntime (until visual feedback stabilizes). We evaluate \\tool on a manually\nannotated benchmark of 2,458 interactions from 64 popular Android apps. \\tool\nachieves 0.96 precision and 0.93 recall in detecting interactions, and measures\nresponse and finish times within 50\\,ms and 100\\,ms error, respectively, for\nover 89\\% of interactions. The tool has been deployed in an industrial testing\npipeline and analyzes thousands of screencasts daily, uncovering responsiveness\nissues missed by traditional tools and improving performance debugging\nefficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\tool\u7684\u8f7b\u91cf\u7ea7\u9ed1\u76d2\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u79fb\u52a8\u8bbe\u5907\u5c4f\u5e55\u5f55\u50cf\u76f4\u63a5\u6d4b\u91cfGUI\u54cd\u5e94\u6027\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522b\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u4e2dGUI\u54cd\u5e94\u6027\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349\u7528\u6237\u611f\u77e5\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u6d4b\u8bd5\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u68c0\u6d4b\u7528\u6237\u4ea4\u4e92\u5e76\u5206\u6790\u5e27\u7ea7\u89c6\u89c9\u53d8\u5316\uff0c\u8ba1\u7b97\u4e24\u4e2a\u5173\u952e\u6307\u6807\uff1a\u54cd\u5e94\u65f6\u95f4\u548c\u5b8c\u6210\u65f6\u95f4\u3002", "result": "\u572864\u4e2a\u6d41\u884cAndroid\u5e94\u7528\u76842,458\u6b21\u4ea4\u4e92\u6d4b\u8bd5\u4e2d\uff0c\\tool\u7684\u4ea4\u4e92\u68c0\u6d4b\u7cbe\u5ea6\u4e3a0.96\uff0c\u53ec\u56de\u7387\u4e3a0.93\uff0c\u54cd\u5e94\u65f6\u95f4\u548c\u5b8c\u6210\u65f6\u95f4\u7684\u6d4b\u91cf\u8bef\u5dee\u5206\u522b\u63a7\u5236\u572850ms\u548c100ms\u4ee5\u5185\uff0c89%\u4ee5\u4e0a\u7684\u4ea4\u4e92\u8fbe\u5230\u6b64\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u5177\u5df2\u5728\u5de5\u4e1a\u6d4b\u8bd5\u7ba1\u9053\u4e2d\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u8c03\u8bd5\u6548\u7387\uff0c\u5e76\u53d1\u73b0\u4e86\u4f20\u7edf\u5de5\u5177\u9057\u6f0f\u7684\u54cd\u5e94\u6027\u95ee\u9898\u3002"}}
{"id": "2508.01357", "pdf": "https://arxiv.org/pdf/2508.01357", "abs": "https://arxiv.org/abs/2508.01357", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Guwen Lyu", "Zhe Cui"], "title": "HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection", "categories": ["cs.SE"], "comment": null, "summary": "Code clone detection is a critical task in software engineering, aimed at\nidentifying duplicated or similar code fragments within or across software\nsystems. Traditional methods often fail to capture functional equivalence,\nparticularly for semantic clones (Type 4), where code fragments implement\nidentical functionality despite differing syntactic structures. Recent advances\nin large language models (LLMs) have shown promise in understanding code\nsemantics. However, directly applying LLMs to code clone detection yields\nsuboptimal results due to their sensitivity to syntactic differences. To\naddress these challenges, we propose a novel two-stage framework that combines\nLLM-based screening with execution-based validation for detecting semantic\nclones in Python programs. In the first stage, an LLM evaluates code pairs to\nfilter out obvious non-clones based on semantic analysis. For pairs not\nidentified as clones, the second stage employs an execution-based validation\napproach, utilizing LLM-generated test inputs to assess functional equivalence\nthrough cross-execution validation. Our experimental evaluation demonstrates\nsignificant improvements in precision, recall, and F1-score compared to direct\nLLM-based detection, highlighting the framework's effectiveness in identifying\nsemantic clones. Future work includes exploring cross-language clone detection\nand optimizing the framework for large-scale applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408LLM\u7b5b\u9009\u4e0e\u6267\u884c\u9a8c\u8bc1\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bPython\u7a0b\u5e8f\u4e2d\u7684\u8bed\u4e49\u514b\u9686\uff08Type 4\u514b\u9686\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8bed\u4e49\u514b\u9686\uff08\u529f\u80fd\u76f8\u540c\u4f46\u8bed\u6cd5\u4e0d\u540c\uff09\uff0c\u800c\u76f4\u63a5\u5e94\u7528LLM\u53c8\u56e0\u5bf9\u8bed\u6cd5\u5dee\u5f02\u654f\u611f\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528LLM\u7b5b\u9009\u8bed\u4e49\u76f8\u4f3c\u7684\u4ee3\u7801\u5bf9\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7LLM\u751f\u6210\u7684\u6d4b\u8bd5\u8f93\u5165\u6267\u884c\u9a8c\u8bc1\u529f\u80fd\u7b49\u4ef7\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u76f4\u63a5\u4f7f\u7528LLM\uff0c\u8be5\u6846\u67b6\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u514b\u9686\u68c0\u6d4b\u95ee\u9898\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u8de8\u8bed\u8a00\u68c0\u6d4b\u548c\u5927\u89c4\u6a21\u5e94\u7528\u4f18\u5316\u3002"}}
{"id": "2508.01358", "pdf": "https://arxiv.org/pdf/2508.01358", "abs": "https://arxiv.org/abs/2508.01358", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "An Empirical Validation of Open Source Repository Stability Metrics", "categories": ["cs.SE"], "comment": null, "summary": "Over the past few decades, open source software has been continuously\nintegrated into software supply chains worldwide, drastically increasing\nreliance and dependence. Because of the role this software plays, it is\nimportant to understand ways to measure and promote its stability and potential\nfor sustainability. Recent work proposed the use of control theory to\nunderstand repository stability and evaluate repositories' ability to return to\nequilibrium after a disturbance such as the introduction of a new feature\nrequest, a spike in bug reports, or even the influx or departure of\ncontributors. This approach leverages commit frequency patterns, issue\nresolution rate, pull request merge rate, and community activity engagement to\nprovide a Composite Stability Index (CSI). While this framework has theoretical\nfoundations, there is no empirical validation of the CSI in practice. In this\npaper, we present the first empirical validation of the proposed CSI by\nexperimenting with 100 highly ranked GitHub repositories. Our results suggest\nthat (1) sampling weekly commit frequency pattern instead of daily is a more\nfeasible measure of commit frequency stability across repositories and (2)\nimproved statistical inferences (swapping mean with median), particularly with\nascertaining resolution and review times in issues and pull request, improves\nthe overall issue and pull request stability index. Drawing on our empirical\ndataset, we also derive data-driven half-width parameters that better align\nstability scores with real project behavior. These findings both confirm the\nviability of a control-theoretic lens on open-source health and provide\nconcrete, evidence-backed applications for real-world project monitoring tools.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u5f00\u6e90\u8f6f\u4ef6\u7a33\u5b9a\u6027\u7684\u63a7\u5236\u7406\u8bba\u6846\u67b6\uff08CSI\uff09\uff0c\u53d1\u73b0\u5468\u91c7\u6837\u4f18\u4e8e\u65e5\u91c7\u6837\uff0c\u5e76\u6539\u8fdb\u4e86\u7edf\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u9879\u76ee\u76d1\u6d4b\u5de5\u5177\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u8f6f\u4ef6\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5176\u7a33\u5b9a\u6027\u548c\u53ef\u6301\u7eed\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u63a7\u5236\u7406\u8bba\u6846\u67b6\uff08CSI\uff09\u7f3a\u4e4f\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "method": "\u5b9e\u9a8c\u5206\u6790\u4e86100\u4e2aGitHub\u4ed3\u5e93\uff0c\u6bd4\u8f83\u5468\u91c7\u6837\u4e0e\u65e5\u91c7\u6837\u7684\u6548\u679c\uff0c\u5e76\u6539\u8fdb\u7edf\u8ba1\u65b9\u6cd5\uff08\u7528\u4e2d\u4f4d\u6570\u4ee3\u66ff\u5747\u503c\uff09\u3002", "result": "\u5468\u91c7\u6837\u66f4\u53ef\u884c\uff0c\u6539\u8fdb\u7684\u7edf\u8ba1\u65b9\u6cd5\u63d0\u5347\u4e86\u95ee\u9898\u4e0e\u62c9\u53d6\u8bf7\u6c42\u7684\u7a33\u5b9a\u6027\u6307\u6570\uff0c\u5e76\u4f18\u5316\u4e86\u534a\u5bbd\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u63a7\u5236\u7406\u8bba\u5728\u5f00\u6e90\u5065\u5eb7\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u5b9e\u9645\u9879\u76ee\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2508.00904", "pdf": "https://arxiv.org/pdf/2508.00904", "abs": "https://arxiv.org/abs/2508.00904", "authors": ["Rajeev Patwari", "Ashish Sirasao", "Devleena Das"], "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling", "categories": ["cs.PF", "cs.AI", "cs.AR", "cs.LG"], "comment": "10 pages, 9 figures", "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.", "AI": {"tldr": "LIFE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5f02\u6784\u8bbe\u5907\u4e0a\u7684LLM\u63a8\u7406\u6027\u80fd\uff0c\u652f\u6301\u591a\u79cd\u4f18\u5316\u6280\u672f\u4e14\u65e0\u9700\u5927\u91cf\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7531\u4e8e\u5f02\u6784\u8bbe\u5907\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u52a8\u6001\u53d8\u5316\uff0c\u73b0\u6709\u7684GPU\u57fa\u51c6\u6d4b\u8bd5\u6216\u673a\u5668\u5b66\u4e60\u5ef6\u8fdf\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86LIFE\u6846\u67b6\u3002", "method": "LIFE\u901a\u8fc7\u6a21\u5757\u5316\u5206\u6790\u6a21\u578b\u548c\u786c\u4ef6\u65e0\u5173\u7684\u914d\u7f6e\uff0c\u91cf\u5316\u8f6f\u4ef6\u4e0e\u6a21\u578b\u4f18\u5316\u5bf9\u6027\u80fd\u6307\u6807\u7684\u5f71\u54cd\uff0c\u4ec5\u9700\u786c\u4ef6\u89c4\u683c\u5373\u53ef\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728AMD Ryzen CPU\u3001NPU\u3001iGPU\u548cNVIDIA V100 GPU\u4e0a\u9a8c\u8bc1\u4e86LIFE\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u3002", "conclusion": "LIFE\u4e3a\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6027\u80fd\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2508.01199", "pdf": "https://arxiv.org/pdf/2508.01199", "abs": "https://arxiv.org/abs/2508.01199", "authors": ["Avinash Malik"], "title": "Efficient compilation and execution of synchronous programs via type-state programming", "categories": ["cs.PL"], "comment": null, "summary": "Synchronous programs are used extensively in implementation of safety\ncritical embedded software. Imperative synchronous programming languages model\nmultiple Finite State Machines (FSMs) executing in lockstep at logical clock\nticks. The synchronous view of time along with the FSM based design enables\neasier formal verification. The synchronous composition of multiple FSMs,\nduring compilation, results in the well known state space explosion problem.\nHence, efficiently compiling imperative synchronous programs into small and\nfast executables is challenging. This paper introduces a novel linear time\ncompilation technique for automata based compilation of synchronous programs.\nGraph based rewrite rules for kernel programming constructs are introduced. A\nlinear time algorithm applies these rules to produce a FSM. The FSM is then\nencoded into a type-state program using template meta-programming in C++.\nExperimental results show that the compilation time and generated binary size\nis comparable, while the execution times are on average 31-60% faster than\ncurrent state-of-the-art compilers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u65f6\u95f4\u7f16\u8bd1\u6280\u672f\uff0c\u7528\u4e8e\u540c\u6b65\u7a0b\u5e8f\u7684\u81ea\u52a8\u673a\u7f16\u8bd1\uff0c\u901a\u8fc7\u56fe\u91cd\u5199\u89c4\u5219\u548c\u6a21\u677f\u5143\u7f16\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387\u3002", "motivation": "\u540c\u6b65\u7a0b\u5e8f\u5728\u5b89\u5168\u5173\u952e\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4f20\u7edf\u7f16\u8bd1\u65b9\u6cd5\u9762\u4e34\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\uff0c\u5bfc\u81f4\u7f16\u8bd1\u548c\u6267\u884c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6539\u5199\u89c4\u5219\uff0c\u63d0\u51fa\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u751f\u6210\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\uff0c\u5e76\u901a\u8fc7C++\u6a21\u677f\u5143\u7f16\u7a0b\u5c06\u5176\u7f16\u7801\u4e3a\u7c7b\u578b\u72b6\u6001\u7a0b\u5e8f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7f16\u8bd1\u65f6\u95f4\u548c\u4e8c\u8fdb\u5236\u5927\u5c0f\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u6267\u884c\u65f6\u95f4\u5e73\u5747\u5feb31-60%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u540c\u6b65\u7f16\u7a0b\u7684\u9ad8\u6548\u7f16\u8bd1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6027\u80fd\u3002"}}
{"id": "2508.01168", "pdf": "https://arxiv.org/pdf/2508.01168", "abs": "https://arxiv.org/abs/2508.01168", "authors": ["Hu Zhangfeng", "Shi mengxin"], "title": "Graph-based Interaction Augmentation Network for Robust Multimodal Sentiment Analysis", "categories": ["cs.MM"], "comment": null, "summary": "The inevitable modality imperfection in real-world scenarios poses\nsignificant challenges for Multimodal Sentiment Analysis (MSA). While existing\nmethods tailor reconstruction or joint representation learning strategies to\nrestore missing semantics, they often overlook complex dependencies within and\nacross modalities. Consequently, they fail to fully leverage available\nmodalities to capture complementary semantics. To this end, this paper proposes\na novel graph-based framework to exploit both intra- and inter-modality\ninteractions, enabling imperfect samples to derive missing semantics from\ncomplementary parts for robust MSA. Specifically, we first devise a learnable\nhypergraph to model intra-modality temporal dependencies to exploit contextual\ninformation within each modality. Then, a directed graph is employed to explore\ninter-modality correlations based on attention mechanism, capturing\ncomplementary information across different modalities. Finally, the knowledge\nfrom perfect samples is integrated to supervise our interaction processes,\nguiding the model toward learning reliable and robust joint representations.\nExtensive experiments on MOSI and MOSEI datasets demonstrate the effectiveness\nof our method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6a21\u6001\u7684\u4e0d\u53ef\u907f\u514d\u7684\u4e0d\u5b8c\u6574\u6027\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff08MSA\uff09\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u6001\u95f4\u7684\u4e92\u8865\u8bed\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u8d85\u56fe\u6765\u5efa\u6a21\u6a21\u6001\u5185\u7684\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u6709\u5411\u56fe\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u63a2\u7d22\u6a21\u6001\u95f4\u7684\u76f8\u5173\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u5b8c\u7f8e\u6837\u672c\u7684\u77e5\u8bc6\u76d1\u7763\u4ea4\u4e92\u8fc7\u7a0b\u3002", "result": "\u5728MOSI\u548cMOSEI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5145\u5206\u5229\u7528\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u63d0\u5347\u4e0d\u5b8c\u6574\u6837\u672c\u7684\u60c5\u611f\u5206\u6790\u6027\u80fd\u3002"}}
{"id": "2508.00843", "pdf": "https://arxiv.org/pdf/2508.00843", "abs": "https://arxiv.org/abs/2508.00843", "authors": ["Sumit Kumar", "Sarthak Kapoor", "Harsh Vardhan", "Yao Zhao"], "title": "Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing industries by enhancing\nefficiency, scalability, and innovation. This paper investigates the potential\nof LLMs in automating Computer-Aided Design (CAD) workflows, by integrating\nFreeCAD with LLM as CAD design tool. Traditional CAD processes are often\ncomplex and require specialized sketching skills, posing challenges for rapid\nprototyping and generative design. We propose a framework where LLMs generate\ninitial CAD scripts from natural language descriptions, which are then executed\nand refined iteratively based on error feedback. Through a series of\nexperiments with increasing complexity, we assess the effectiveness of this\napproach. Our findings reveal that LLMs perform well for simple to moderately\ncomplex designs but struggle with highly constrained models, necessitating\nmultiple refinements. The study highlights the need for improved memory\nretrieval, adaptive prompt engineering, and hybrid AI techniques to enhance\nscript robustness. Future directions include integrating cloud-based execution\nand exploring advanced LLM capabilities to further streamline CAD automation.\nThis work underscores the transformative potential of LLMs in design workflows\nwhile identifying critical areas for future development.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u5316\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\uff08CAD\uff09\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5c06FreeCAD\u4e0eLLM\u96c6\u6210\u4f5c\u4e3a\u8bbe\u8ba1\u5de5\u5177\uff0c\u53d1\u73b0LLMs\u5728\u7b80\u5355\u5230\u4e2d\u7b49\u590d\u6742\u5ea6\u8bbe\u8ba1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9ad8\u7ea6\u675f\u6a21\u578b\u9700\u591a\u6b21\u4f18\u5316\u3002", "motivation": "\u4f20\u7edfCAD\u6d41\u7a0b\u590d\u6742\u4e14\u9700\u4e13\u4e1a\u6280\u80fd\uff0c\u963b\u788d\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u751f\u6210\u8bbe\u8ba1\uff0cLLMs\u7684\u5f15\u5165\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7LLMs\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u521d\u59cbCAD\u811a\u672c\uff0c\u5e76\u57fa\u4e8e\u9519\u8bef\u53cd\u9988\u8fed\u4ee3\u6267\u884c\u548c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u9002\u7528\u4e8e\u7b80\u5355\u5230\u4e2d\u7b49\u590d\u6742\u5ea6\u4efb\u52a1\uff0c\u4f46\u5bf9\u9ad8\u7ea6\u675f\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u8bb0\u5fc6\u68c0\u7d22\u548c\u63d0\u793a\u5de5\u7a0b\u3002", "conclusion": "LLMs\u5728CAD\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6280\u672f\u4ee5\u63d0\u5347\u811a\u672c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00950", "pdf": "https://arxiv.org/pdf/2508.00950", "abs": "https://arxiv.org/abs/2508.00950", "authors": ["Ying Zhang", "Niklas Groene", "Karsten Klein", "Giuseppe Liotta", "Falk Schreiber"], "title": "Investigating Crossing Perception in 3D Graph Visualisation", "categories": ["cs.GR"], "comment": null, "summary": "Human perception of graph drawings is influenced by a variety of impact\nfactors for which quality measures are used as a proxy indicator. The\ninvestigation of those impact factors and their effects is important to\nevaluate and improve quality measures and drawing algorithms. The number of\nedge crossings in a 2D graph drawing has long been a main quality measure for\ndrawing evaluation. The use of stereoscopic 3D graph visualisations has gained\nattraction over the last years, and results from several studies indicate that\nthey can improve analysis efficiency for a range of analysis scenarios. While\nedge crossings can also occur in 3D, there are edge configurations in space\nthat are not crossings but might be perceived as such from a specific\nviewpoint. Such configurations create crossings when projected on the\ncorresponding 2D image plane and could impact readability similar to 2D\ncrossings. In 3D drawings, the additional depth aspect and the subsequent\nimpact factors of edge distance and relative edge direction in space might\nfurther influence the importance of those configurations for readability. We\ninvestigate the impact of such factors in an empirical study and report on\nfindings of difference between major factor categories.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e863D\u56fe\u7ed8\u5236\u4e2d\u8fb9\u4ea4\u53c9\u53ca\u5176\u611f\u77e5\u5bf9\u53ef\u8bfb\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u56e0\u7d20\u7684\u91cd\u8981\u6027\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d223D\u56fe\u53ef\u89c6\u5316\u4e2d\u8fb9\u4ea4\u53c9\u53ca\u5176\u611f\u77e5\u5bf9\u53ef\u8bfb\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u6539\u8fdb\u8d28\u91cf\u8bc4\u4f30\u548c\u7ed8\u5236\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u67903D\u56fe\u4e2d\u8fb9\u914d\u7f6e\u7684\u611f\u77e5\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u6df1\u5ea6\u3001\u8fb9\u8ddd\u79bb\u548c\u76f8\u5bf9\u65b9\u5411\u7b49\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u56e0\u7d20\u7c7b\u522b\u5728\u53ef\u8bfb\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "3D\u56fe\u7684\u53ef\u8bfb\u6027\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u9700\u7efc\u5408\u8003\u8651\u8fb9\u4ea4\u53c9\u53ca\u5176\u611f\u77e5\u6548\u679c\u4ee5\u4f18\u5316\u7ed8\u5236\u8d28\u91cf\u3002"}}
{"id": "2508.01067", "pdf": "https://arxiv.org/pdf/2508.01067", "abs": "https://arxiv.org/abs/2508.01067", "authors": ["Veeti Ahvonen", "Maurice Funk", "Damian Heiman", "Antti Kuusisto", "Carsten Lutz"], "title": "Expressive Power of Graph Transformers via Logic", "categories": ["cs.LO", "cs.AI", "F.4.1; F.1.1; I.2.0"], "comment": null, "summary": "Transformers are the basis of modern large language models, but relatively\nlittle is known about their precise expressive power on graphs. We study the\nexpressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and\nGPS-networks by Ramp\\'asek et al. (2022), both under soft-attention and average\nhard-attention. Our study covers two scenarios: the theoretical setting with\nreal numbers and the more practical case with floats. With reals, we show that\nin restriction to vertex properties definable in first-order logic (FO),\nGPS-networks have the same expressive power as graded modal logic (GML) with\nthe global modality. With floats, GPS-networks turn out to be equally\nexpressive as GML with the counting global modality. The latter result is\nabsolute, not restricting to properties definable in a background logic. We\nalso obtain similar characterizations for GTs in terms of propositional logic\nwith the global modality (for reals) and the counting global modality (for\nfloats).", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u53d8\u6362\u5668\uff08GTs\uff09\u548cGPS\u7f51\u7edc\u5728\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u5b9e\u6570\u4e0e\u6d6e\u70b9\u6570\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u7406\u8bba\u6027\u8d28\u53ca\u5176\u4e0e\u903b\u8f91\u8868\u8fbe\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u56fe\u53d8\u6362\u5668\u548cGPS\u7f51\u7edc\u5728\u56fe\u6570\u636e\u4e0a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u662f\u5b83\u4eec\u4e0e\u4e0d\u540c\u903b\u8f91\u6a21\u6001\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\u3002", "method": "\u5206\u6790\u4e86GTs\u548cGPS-networks\u5728\u8f6f\u6ce8\u610f\u529b\u548c\u5e73\u5747\u786c\u6ce8\u610f\u529b\u4e0b\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e86\u5b9e\u6570\u4e0e\u6d6e\u70b9\u6570\u4e24\u79cd\u8ba1\u7b97\u73af\u5883\u3002", "result": "\u5728\u5b9e\u6570\u73af\u5883\u4e0b\uff0cGPS-networks\u4e0e\u5e26\u5168\u5c40\u6a21\u6001\u7684GML\u8868\u8fbe\u80fd\u529b\u76f8\u540c\uff1b\u5728\u6d6e\u70b9\u6570\u73af\u5883\u4e0b\uff0c\u4e0e\u5e26\u8ba1\u6570\u5168\u5c40\u6a21\u6001\u7684GML\u7b49\u4ef7\u3002GTs\u7684\u8868\u73b0\u4e5f\u7c7b\u4f3c\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86GTs\u548cGPS-networks\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e0e\u903b\u8f91\u6a21\u6001\u7684\u7b49\u4ef7\u6027\uff0c\u4e3a\u7406\u89e3\u5176\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2508.01136", "pdf": "https://arxiv.org/pdf/2508.01136", "abs": "https://arxiv.org/abs/2508.01136", "authors": ["Wei Zhou", "Peng Sun", "Xuanhe Zhou", "Qianglei Zang", "Ji Xu", "Tieying Zhang", "Guoliang Li", "Fan Wu"], "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/", "summary": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.", "AI": {"tldr": "DBAIOps\u662f\u4e00\u79cd\u7ed3\u5408LLM\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u6df7\u5408\u6570\u636e\u5e93\u8fd0\u7ef4\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u5229\u7528\u4e13\u5bb6\u7ecf\u9a8c\u8fdb\u884c\u8bca\u65ad\uff0c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6570\u636e\u5e93\u8fd0\u7ef4\u65b9\u6cd5\uff08\u5305\u62ec\u5546\u4e1a\u4ea7\u54c1\uff09\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u89c4\u5219\u548cLLM\u65b9\u6cd5\u5404\u6709\u4e0d\u8db3\u3002", "method": "DBAIOps\u91c7\u7528\u5f02\u6784\u56fe\u6a21\u578b\u8868\u793a\u8bca\u65ad\u7ecf\u9a8c\uff0c\u6784\u5efa\u4e86800+\u53ef\u91cd\u7528\u5f02\u5e38\u6a21\u578b\u548c\u4e24\u9636\u6bb5\u56fe\u6f14\u5316\u673a\u5236\uff0c\u7ed3\u5408\u63a8\u7406LLM\u63a8\u65ad\u6839\u56e0\u3002", "result": "\u5728\u56db\u79cd\u4e3b\u6d41\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\uff0cDBAIOps\u5728\u6839\u56e0\u548c\u4eba\u5de5\u8bc4\u4f30\u51c6\u786e\u7387\u4e0a\u5206\u522b\u6bd4\u57fa\u51c6\u9ad834.85%\u548c47.22%\u3002", "conclusion": "DBAIOps\u901a\u8fc7\u7ed3\u5408LLM\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5e93\u8fd0\u7ef4\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2508.00825", "pdf": "https://arxiv.org/pdf/2508.00825", "abs": "https://arxiv.org/abs/2508.00825", "authors": ["L-F Pau"], "title": "Towards a quantum synapse for quantum sensing", "categories": ["cs.ET", "quant-ph", "81P68, 81R50, 92XX, 17B75, 46N50, 68Q07, 92B25, 81Q35, 68Q05", "B.1; B.7; C.3; F.m; I.m"], "comment": null, "summary": "As a step in the architectural design of a quantum processing or sensing\nsystem with control and signaling, an attempt is made at putting in parallel\nfunctional properties of the random flows between neurons through electrical\nsynapses, and quantum particle flows inside a quantum processing system\nmimicking biological processes. Based on a simplified dynamic electrical\nsynapse model, a quantum synapse circuit design is proposed. This is extended\nto the case of bidirectional flows through a synapse, highlighting the possible\nrole of quantum synapse circuits as highly parallel controlled interfaces\ncrucial in sensing and sensor fusion systems. A short status of the quantum\nsimulation is provided.", "AI": {"tldr": "\u91cf\u5b50\u7a81\u89e6\u7535\u8def\u8bbe\u8ba1\u6a21\u62df\u751f\u7269\u795e\u7ecf\u5143\u7535\u7a81\u89e6\u529f\u80fd\uff0c\u63d0\u51fa\u5176\u5728\u91cf\u5b50\u5904\u7406\u548c\u4f20\u611f\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u7cfb\u7edf\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u91cf\u5b50\u5904\u7406\u548c\u4f20\u611f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "method": "\u57fa\u4e8e\u7b80\u5316\u52a8\u6001\u7535\u7a81\u89e6\u6a21\u578b\u8bbe\u8ba1\u91cf\u5b50\u7a81\u89e6\u7535\u8def\uff0c\u5e76\u6269\u5c55\u81f3\u53cc\u5411\u6d41\u52a8\u573a\u666f\u3002", "result": "\u91cf\u5b50\u7a81\u89e6\u7535\u8def\u53ef\u4f5c\u4e3a\u9ad8\u5e76\u884c\u63a7\u5236\u63a5\u53e3\uff0c\u63d0\u5347\u4f20\u611f\u548c\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "conclusion": "\u91cf\u5b50\u7a81\u89e6\u7535\u8def\u5728\u91cf\u5b50\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u91cf\u5b50\u6a21\u62df\u9a8c\u8bc1\u3002"}}
{"id": "2508.01180", "pdf": "https://arxiv.org/pdf/2508.01180", "abs": "https://arxiv.org/abs/2508.01180", "authors": ["Bowen Wang", "Marco Bertuletti", "Yichao Zhang", "Victor J. B. Jung", "Luca Benini"], "title": "A Dynamic Allocation Scheme for Adaptive Shared-Memory Mapping on Kilo-core RV Clusters for Attention-Based Model Deployment", "categories": ["cs.AR"], "comment": "8 pages, 9 figures, 36th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors", "summary": "Attention-based models demand flexible hardware to manage diverse kernels\nwith varying arithmetic intensities and memory access patterns. Large clusters\nwith shared L1 memory, a common architectural pattern, struggle to fully\nutilize their processing elements (PEs) when scaled up due to reduced\nthroughput in the hierarchical PE-to-L1 intra-cluster interconnect. This paper\npresents Dynamic Allocation Scheme (DAS), a runtime programmable address\nremapping hardware unit coupled with a unified memory allocator, designed to\nminimize data access contention of PEs onto the multi-banked L1. We evaluated\nDAS on an aggressively scaled-up 1024-PE RISC-V cluster with Non-Uniform Memory\nAccess (NUMA) PE-to-L1 interconnect to demonstrate its potential for improving\ndata locality in large parallel machine learning workloads. For a Vision\nTransformer (ViT)-L/16 model, each encoder layer executes in 5.67 ms, achieving\na 1.94x speedup over the fixed word-level interleaved baseline with 0.81 PE\nutilization. Implemented in 12nm FinFET technology, DAS incurs <0.1 % area\noverhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5206\u914d\u65b9\u6848\uff08DAS\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u6838\u5904\u7406\u5668\u4e2d\u5904\u7406\u5355\u5143\uff08PEs\uff09\u5bf9\u5171\u4eabL1\u5185\u5b58\u7684\u8bbf\u95ee\uff0c\u4ee5\u51cf\u5c11\u6570\u636e\u8bbf\u95ee\u51b2\u7a81\u3002\u901a\u8fc7\u57281024-PE\u96c6\u7fa4\u4e0a\u7684\u6d4b\u8bd5\uff0c\u8bc1\u660eDAS\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6570\u636e\u5c40\u90e8\u6027\u3002", "motivation": "\u9488\u5bf9\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u7684\u786c\u4ef6\u5728\u5904\u7406\u591a\u6837\u6027\u5185\u6838\u65f6\u7684\u6548\u7387\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u96c6\u7fa4\u4e2d\u5171\u4eabL1\u5185\u5b58\u7684\u67b6\u6784\u4e0b\uff0cPEs\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8fd0\u884c\u65f6\u53ef\u7f16\u7a0b\u7684\u5730\u5740\u91cd\u6620\u5c04\u786c\u4ef6\u5355\u5143\uff08DAS\uff09\u548c\u7edf\u4e00\u5185\u5b58\u5206\u914d\u5668\uff0c\u4ee5\u51cf\u5c11PEs\u5bf9\u591abank L1\u5185\u5b58\u7684\u8bbf\u95ee\u51b2\u7a81\u3002", "result": "\u57281024-PE RISC-V\u96c6\u7fa4\u4e0a\uff0cDAS\u4e3aVision Transformer\u6a21\u578b\u5e26\u6765\u4e861.94\u500d\u7684\u52a0\u901f\uff0cPE\u5229\u7528\u7387\u8fbe0.81\uff0c\u786c\u4ef6\u5f00\u9500\u4f4e\u4e8e0.1%\u3002", "conclusion": "DAS\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u5e76\u884c\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u786c\u4ef6\u5f00\u9500\u3002"}}
{"id": "2508.01373", "pdf": "https://arxiv.org/pdf/2508.01373", "abs": "https://arxiv.org/abs/2508.01373", "authors": ["Dariusz R. Kowalski", "Jan Olkowski"], "title": "Deterministic Fault-Tolerant Local Load Balancing and its Applications against Adaptive Adversaries", "categories": ["cs.DC", "cs.DS", "F.2.2"], "comment": null, "summary": "Load balancing is among the basic primitives in distributed computing. In\nthis paper, we consider this problem when executed locally on a network with\nnodes prone to failures. We show that there exist lightweight network\ntopologies that are immune to message delivery failures incurred by (at most) a\nconstant fraction of all nodes. More precisely, we design a novel deterministic\nfault-tolerant local load balancing (LLB) algorithm, which, similarly to their\nclassical counterparts working in fault-free networks, has a relatively simple\nstructure and guarantees exponentially fast convergence to the average value\ndespite crash and omission failures.\n  As the second part of our contribution, we show three applications of the\nnewly developed fault-tolerant local load balancing protocol. We give a\nrandomized consensus algorithm, working against $t < n / 3$ crash failures,\nthat improves over the best-known consensus solution by Hajiaghayi et al. with\nrespect to communication complexity, yet with an arguable simpler technique of\ncombining a randomly and locally selected virtual communication graph with a\ndeterministic fault-tolerant local load balancing on this graph.\n  We also give a new solution for consensus for networks with omission\nfailures. Our solution works against $t < \\frac{n}{C\\log{n} (\\log\\log n)^2}$\nomissions, for some constant $C$, is nearly optimal in terms of time\ncomplexity, but most notably -- it has communication complexity $O((t^2 +\nn)\\text{ polylog } {n})$, matching, within a polylogarithmic factor, the lower\nbound by Abraham et. al. with respect to both terms depending on $t$ and $n$.\nOurs is the first algorithm in the literature that is simultaneously nearly\noptimal, in terms of $n,t$, with respect to both complexity measures, against\nthe adaptive omission-causing adversary.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8282\u70b9\u6613\u5931\u6548\u7684\u7f51\u7edc\u4e2d\u5b9e\u73b0\u672c\u5730\u8d1f\u8f7d\u5747\u8861\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u5bb9\u9519\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u80fd\u591f\u5728\u5b58\u5728\u6545\u969c\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u6536\u655b\u5230\u5e73\u5747\u503c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5171\u8bc6\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u662f\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u4f46\u5728\u8282\u70b9\u6613\u5931\u6548\u7684\u7f51\u7edc\u4e2d\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u7684\u672c\u5730\u8d1f\u8f7d\u5747\u8861\u5c1a\u672a\u5145\u5206\u89e3\u51b3\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u5bb9\u9519\u6027\u5f3a\u7684\u672c\u5730\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5171\u8bc6\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u786e\u5b9a\u6027\u5bb9\u9519\u672c\u5730\u8d1f\u8f7d\u5747\u8861\uff08FTLLB\uff09\u7b97\u6cd5\uff0c\u80fd\u591f\u5bb9\u5fcd\u6700\u591a\u6052\u5b9a\u6bd4\u4f8b\u7684\u8282\u70b9\u5931\u6548\u3002\u7b97\u6cd5\u7ed3\u6784\u7b80\u5355\uff0c\u5177\u6709\u5feb\u901f\u6536\u655b\u6027\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u968f\u673a\u9009\u62e9\u7684\u865a\u62df\u901a\u4fe1\u56fe\u548cFTLLB\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u968f\u673a\u5171\u8bc6\u7b97\u6cd5\u548c\u65b0\u7684\u7701\u7565\u6545\u969c\u4e0b\u7684\u5171\u8bc6\u89e3\u51b3\u65b9\u6848\u3002", "result": "FTLLB\u7b97\u6cd5\u5728\u5b58\u5728\u6545\u969c\uff08\u5d29\u6e83\u548c\u7701\u7565\uff09\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5feb\u901f\u6536\u655b\u5230\u5e73\u5747\u503c\u3002\u5171\u8bc6\u7b97\u6cd5\u7684\u901a\u4fe1\u590d\u6742\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u4e14\u5728\u7701\u7565\u6545\u969c\u4e0b\u7684\u5171\u8bc6\u65b9\u6848\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u590d\u6742\u5ea6\u4e0a\u5747\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684FTLLB\u7b97\u6cd5\u53ca\u5176\u5728\u5171\u8bc6\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u5728\u5bb9\u9519\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.01430", "pdf": "https://arxiv.org/pdf/2508.01430", "abs": "https://arxiv.org/abs/2508.01430", "authors": ["Kaveh Shahedi", "Matthew Khouzam", "Heng Li", "Maxime Lamothe", "Foutse Khomh"], "title": "From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool", "categories": ["cs.SE"], "comment": null, "summary": "System tracing has become essential for understanding complex software\nbehavior in modern systems, yet sophisticated trace analysis tools face\nsignificant adoption gaps in industrial settings. Through a year-long\ncollaboration with Ericsson Montr\\'eal, developing TMLL (Trace-Server Machine\nLearning Library, now in the Eclipse Foundation), we investigated barriers to\ntrace analysis adoption. Contrary to assumptions about complexity or automation\nneeds, practitioners struggled with translating expert knowledge into\nactionable insights, integrating analysis into their workflows, and trusting\nautomated results they could not validate. We identified what we called the\nExcellence Paradox: technical excellence can actively impede adoption when\nconflicting with usability, transparency, and practitioner trust. TMLL\naddresses this through adoption-focused design that embeds expert knowledge in\ninterfaces, provides transparent explanations, and enables incremental\nadoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's\nintegration, and a survey of 40 industry and academic professionals revealed\nconsistent patterns: survey results showed that 77.5% prioritize quality and\ntrust in results over technical sophistication, while 67.5% prefer\nsemi-automated analysis with user control, findings supported by qualitative\nfeedback from industrial collaboration and external peer review. Results\nvalidate three core principles: cognitive compatibility, embedded expertise,\nand transparency-based trust. This challenges conventional capability-focused\ntool development, demonstrating that sustainable adoption requires\nreorientation toward adoption-focused design with actionable implications for\nautomated software engineering tools.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u590d\u6742\u8f6f\u4ef6\u884c\u4e3a\u5206\u6790\u5de5\u5177\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u91c7\u7528\u969c\u788d\uff0c\u63d0\u51fa\u4e86\u5353\u8d8a\u6096\u8bba\uff08Excellence Paradox\uff09\uff0c\u5e76\u901a\u8fc7TMLL\u5de5\u5177\u9a8c\u8bc1\u4e86\u8ba4\u77e5\u517c\u5bb9\u6027\u3001\u5d4c\u5165\u4e13\u4e1a\u77e5\u8bc6\u548c\u900f\u660e\u4fe1\u4efb\u4e09\u5927\u539f\u5219\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5de5\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u8f6f\u4ef6\u8ffd\u8e2a\u5de5\u5177\u7684\u91c7\u7528\u969c\u788d\uff0c\u5c24\u5176\u662f\u5728\u4e13\u5bb6\u77e5\u8bc6\u8f6c\u5316\u4e3a\u5b9e\u7528\u6d1e\u5bdf\u3001\u5de5\u4f5c\u6d41\u96c6\u6210\u548c\u81ea\u52a8\u5316\u7ed3\u679c\u4fe1\u4efb\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4e0eEricsson\u5408\u4f5c\u5f00\u53d1TMLL\u5de5\u5177\uff0c\u5e76\u6536\u96c6\u4e13\u5bb6\u53cd\u9988\u3001\u884c\u4e1a\u8c03\u67e5\u548c\u5916\u90e8\u8bc4\u5ba1\u6570\u636e\uff0c\u5206\u6790\u4e86\u5de5\u5177\u91c7\u7528\u7684\u6838\u5fc3\u969c\u788d\u548c\u6539\u8fdb\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b077.5%\u7684\u53d7\u8bbf\u8005\u66f4\u5173\u6ce8\u7ed3\u679c\u8d28\u91cf\u548c\u4fe1\u4efb\u800c\u975e\u6280\u672f\u590d\u6742\u5ea6\uff0c67.5%\u503e\u5411\u4e8e\u534a\u81ea\u52a8\u5316\u5206\u6790\u3002TMLL\u901a\u8fc7\u8bbe\u8ba1\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u91c7\u7528\u7387\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u8868\u660e\u53ef\u6301\u7eed\u91c7\u7528\u9700\u8981\u4ece\u529f\u80fd\u5bfc\u5411\u8f6c\u5411\u4ee5\u7528\u6237\u4f53\u9a8c\u4e3a\u4e2d\u5fc3\u7684\u9002\u5e94\u6027\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u4e86\u8ba4\u77e5\u517c\u5bb9\u6027\u548c\u900f\u660e\u4fe1\u4efb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "1905.13011", "pdf": "https://arxiv.org/pdf/1905.13011", "abs": "https://arxiv.org/abs/1905.13011", "authors": ["Pratyush Mahapatra", "Mark D. Hill", "Michael M. Swift"], "title": "Don't Persist All : Efficient Persistent Data Structures", "categories": ["cs.DB", "cs.AR", "cs.DS", "cs.PF"], "comment": "10 pages, 12 figures", "summary": "Data structures used in software development have inbuilt redundancy to\nimprove software reliability and to speed up performance. Examples include a\nDoubly Linked List which allows a faster deletion due to the presence of the\nprevious pointer. With the introduction of Persistent Memory, storing the\nredundant data fields into persistent memory adds a significant write overhead,\nand reduces performance. In this work, we focus on three data structures -\nDoubly Linked List, B+Tree and Hashmap, and showcase alternate partly\npersistent implementations where we only store a limited set of data fields to\npersistent memory. After a crash/restart, we use the persistent data fields to\nrecreate the data structures along with the redundant data fields. We compare\nour implementation with the base implementation and show that we achieve\nspeedups around 5-20% for some data structures, and up to 165% for a\nflush-dominated data structure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u6301\u4e45\u5185\u5b58\u7684\u90e8\u5206\u6301\u4e45\u5316\u6570\u636e\u7ed3\u6784\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u6570\u636e\u7684\u5199\u5165\u5f00\u9500\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u7ed3\u6784\u5728\u6301\u4e45\u5185\u5b58\u4e2d\u5b58\u50a8\u5197\u4f59\u6570\u636e\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5199\u5165\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u9488\u5bf9\u53cc\u5411\u94fe\u8868\u3001B+\u6811\u548c\u54c8\u5e0c\u8868\uff0c\u8bbe\u8ba1\u90e8\u5206\u6301\u4e45\u5316\u5b9e\u73b0\uff0c\u4ec5\u5b58\u50a8\u5fc5\u8981\u6570\u636e\u5b57\u6bb5\uff0c\u5d29\u6e83\u540e\u91cd\u5efa\u5197\u4f59\u5b57\u6bb5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u53475-20%\uff0c\u90e8\u5206\u573a\u666f\u53ef\u8fbe165%\u3002", "conclusion": "\u90e8\u5206\u6301\u4e45\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6301\u4e45\u5185\u5b58\u5199\u5165\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.02305", "pdf": "https://arxiv.org/pdf/2508.02305", "abs": "https://arxiv.org/abs/2508.02305", "authors": ["Rose Bohrer"], "title": "Proceedings 14th International Workshop on Trends in Functional Programming in Education", "categories": ["cs.PL"], "comment": null, "summary": "The goal of TFPIE is to gather researchers, teachers and professionals that\nuse, or are interested in the use of, functional programming in education.\nTFPIE aims to be a venue where novel ideas, classroom-tested ideas and\nwork-in-progress on the use of functional programming in education are\ndiscussed. The one-day workshop will foster a spirit of open discussion by\nhaving a review process for publication after the workshop.", "AI": {"tldr": "TFPIE\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6559\u80b2\u4e2d\u51fd\u6570\u5f0f\u7f16\u7a0b\u5e94\u7528\u7684\u7814\u8ba8\u4f1a\uff0c\u65e8\u5728\u805a\u96c6\u76f8\u5173\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u5206\u4eab\u65b0\u60f3\u6cd5\u548c\u6559\u5b66\u7ecf\u9a8c\u3002", "motivation": "\u4fc3\u8fdb\u51fd\u6570\u5f0f\u7f16\u7a0b\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5f00\u653e\u7684\u8ba8\u8bba\u5e73\u53f0\u3002", "method": "\u4e3e\u529e\u4e3a\u671f\u4e00\u5929\u7684\u7814\u8ba8\u4f1a\uff0c\u91c7\u7528\u4f1a\u540e\u518d\u5ba1\u7a3f\u7684\u51fa\u7248\u6d41\u7a0b\u3002", "result": "\u5e0c\u671b\u4e3a\u6559\u80b2\u4e2d\u7684\u51fd\u6570\u5f0f\u7f16\u7a0b\u63d0\u4f9b\u4e00\u4e2a\u52a8\u6001\u7684\u4ea4\u6d41\u73af\u5883\u3002", "conclusion": "TFPIE\u81f4\u529b\u4e8e\u63a8\u52a8\u51fd\u6570\u5f0f\u7f16\u7a0b\u5728\u6559\u80b2\u4e2d\u7684\u521b\u65b0\u4e0e\u8ba8\u8bba\u3002"}}
{"id": "2508.01644", "pdf": "https://arxiv.org/pdf/2508.01644", "abs": "https://arxiv.org/abs/2508.01644", "authors": ["Peiyuan Jiang", "Yao Liu", "Qiao Liu", "Zongshun Zhang", "Jiaye Yang", "Lu Liu", "Daibing Yao"], "title": "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": "Published in ACM Multimedia 2025. 10 pages, 4 figures", "summary": "Multimodal emotion recognition (MER) aims to identify emotional states by\nintegrating and analyzing information from multiple modalities. However,\ninherent modality heterogeneity and inconsistencies in emotional cues remain\nkey challenges that hinder performance. To address these issues, we propose a\nDecoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF\nconsists of two main modules: an Optimized Representation Learning (ORL) Module\nand a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual\ninformation estimation method with progressive modality augmentation to\ndecouple task-relevant shared representations and modality-specific features\nwhile mitigating modality heterogeneity. KF includes a lightweight\nself-attention-based Fusion Encoder (FE) that identifies the dominant modality\nand integrates emotional information from other modalities to enhance the fused\nrepresentation. To handle potential errors from incorrect dominant modality\nselection under emotionally inconsistent conditions, we introduce an Emotion\nDiscrimination Submodule (ED), which enforces the fused representation to\nretain discriminative cues of emotional inconsistency. This ensures that even\nif the FE selects an inappropriate dominant modality, the Emotion\nClassification Submodule (EC) can still make accurate predictions by leveraging\npreserved inconsistency information. Experiments show that DRKF achieves\nstate-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code\nis publicly available at https://github.com/PANPANKK/DRKF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRKF\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u5f81\u548c\u77e5\u8bc6\u878d\u5408\u89e3\u51b3\u6a21\u6001\u5f02\u8d28\u6027\u548c\u60c5\u611f\u7ebf\u7d22\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u6a21\u6001\u5f02\u8d28\u6027\u548c\u60c5\u611f\u7ebf\u7d22\u4e0d\u4e00\u81f4\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u8026\u5171\u4eab\u8868\u5f81\u548c\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u3002", "method": "DRKF\u5305\u542b\u4f18\u5316\u8868\u5f81\u5b66\u4e60\u6a21\u5757\uff08ORL\uff09\u548c\u77e5\u8bc6\u878d\u5408\u6a21\u5757\uff08KF\uff09\uff0cORL\u89e3\u8026\u8868\u5f81\uff0cKF\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u878d\u5408\u60c5\u611f\u4fe1\u606f\u5e76\u5f15\u5165\u60c5\u611f\u5224\u522b\u5b50\u6a21\u5757\u5904\u7406\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDRKF\u5728IEMOCAP\u3001MELD\u548cM3ED\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "DRKF\u901a\u8fc7\u89e3\u8026\u548c\u878d\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00846", "pdf": "https://arxiv.org/pdf/2508.00846", "abs": "https://arxiv.org/abs/2508.00846", "authors": ["Songlin Xu", "Xinyu Zhang"], "title": "Cognitive Exoskeleton: Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we introduce an AI-mediated framework that can provide\nintelligent feedback to augment human cognition. Specifically, we leverage deep\nreinforcement learning (DRL) to provide adaptive time pressure feedback to\nimprove user performance in a math arithmetic task. Time pressure feedback\ncould either improve or deteriorate user performance by regulating user\nattention and anxiety. Adaptive time pressure feedback controlled by a DRL\npolicy according to users' real-time performance could potentially solve this\ntrade-off problem. However, the DRL training and hyperparameter tuning may\nrequire large amounts of data and iterative user studies. Therefore, we propose\na dual-DRL framework that trains a regulation DRL agent to regulate user\nperformance by interacting with another simulation DRL agent that mimics user\ncognition behaviors from an existing dataset. Our user study demonstrates the\nfeasibility and effectiveness of the dual-DRL framework in augmenting user\nperformance, in comparison to the baseline group.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u8c03\u8282\u7684\u53ccDRL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u538b\u529b\u53cd\u9988\u63d0\u5347\u7528\u6237\u6570\u5b66\u7b97\u672f\u4efb\u52a1\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u6570\u636e\u9700\u6c42\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfDRL\u65b9\u6cd5\u5728\u63d0\u4f9b\u81ea\u9002\u5e94\u65f6\u95f4\u538b\u529b\u53cd\u9988\u65f6\u5bf9\u5927\u6570\u636e\u548c\u7528\u6237\u7814\u7a76\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u53ccDRL\u6846\u67b6\u3002", "method": "\u91c7\u7528\u53ccDRL\u6846\u67b6\uff1a\u4e00\u4e2a\u8c03\u8282DRL\u4ee3\u7406\u8c03\u63a7\u7528\u6237\u8868\u73b0\uff0c\u53e6\u4e00\u4e2a\u6a21\u62dfDRL\u4ee3\u7406\u4ece\u73b0\u6709\u6570\u636e\u4e2d\u6a21\u62df\u7528\u6237\u884c\u4e3a\uff0c\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u53ccDRL\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u8868\u73b0\uff0c\u4f18\u4e8e\u5bf9\u7167\u7ec4\u3002", "conclusion": "\u53ccDRL\u6846\u67b6\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7AI\u667a\u80fd\u53cd\u9988\u589e\u5f3a\u4eba\u7c7b\u8ba4\u77e5\u8868\u73b0\u3002"}}
{"id": "2508.01242", "pdf": "https://arxiv.org/pdf/2508.01242", "abs": "https://arxiv.org/abs/2508.01242", "authors": ["Shuangkang Fang", "I-Chao Shen", "Yufeng Wang", "Yi-Hsuan Tsai", "Yi Yang", "Shuchang Zhou", "Wenrui Ding", "Takeo Igarashi", "Ming-Hsuan Yang"], "title": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by ICCV", "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.", "AI": {"tldr": "MeshLLM\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7406\u89e3\u548c\u751f\u62103D\u7f51\u683c\u6587\u672c\u5e8f\u5217\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Primitive-Mesh\u5206\u89e3\u7b56\u7565\u89e3\u51b3\u6570\u636e\u89c4\u6a21\u548c\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u7f51\u683c\u751f\u6210\u8d28\u91cf\u548c\u5f62\u72b6\u7406\u89e3\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6587\u672c\u5e8f\u5217\u53163D\u7f51\u683c\u65f6\u5b58\u5728\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u548c\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0cMeshLLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528Primitive-Mesh\u5206\u89e3\u7b56\u7565\u5c063D\u7f51\u683c\u5212\u5206\u4e3a\u6709\u7ed3\u6784\u610f\u4e49\u7684\u5b50\u5355\u5143\uff0c\u5e76\u63d0\u51fa\u63a8\u65ad\u9762\u8fde\u63a5\u6027\u548c\u5c40\u90e8\u7f51\u683c\u7ec4\u88c5\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a1500k+\u6837\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7f51\u683c\u751f\u6210\u8d28\u91cf\u548c\u5f62\u72b6\u7406\u89e3\u6027\u80fd\u4f18\u4e8eLLaMA-Mesh\u3002", "conclusion": "MeshLLM\u5c55\u793a\u4e86\u5728\u5904\u7406\u6587\u672c\u5e8f\u5217\u53163D\u7f51\u683c\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.01535", "pdf": "https://arxiv.org/pdf/2508.01535", "abs": "https://arxiv.org/abs/2508.01535", "authors": ["Yeonseok Lee", "Koji Nakazawa"], "title": "Relative Completeness of Incorrectness Separation Logic", "categories": ["cs.LO"], "comment": "This is an extended version of a paper that appeared in the Asian\n  Symposium on Programming Languages and Systems (APLAS) 2024: Lee, Y.,\n  Nakazawa, K. \"Relative Completeness of Incorrectness Separation Logic.\" In:\n  Kiselyov, O. (eds) Programming Languages and Systems. Lecture Notes in\n  Computer Science, vol 15194. Springer, Singapore. DOI:\n  10.1007/978-981-97-8943-6_13", "summary": "Incorrectness Separation Logic (ISL) is a proof system that is tailored\nspecifically to resolve problems of under-approximation in programs that\nmanipulate heaps, and it primarily focuses on bug detection. This approach is\ndifferent from the over-approximation methods that are used in traditional\nlogics such as Hoare Logic or Separation Logic. Although the soundness of ISL\nhas been established, its completeness remains unproven. In this study, we\nestablish relative completeness by leveraging the expressiveness of the weakest\npostconditions; expressiveness is a factor that is critical to demonstrating\nrelative completeness in Reverse Hoare Logic. In our ISL framework, we allow\nfor infinite disjunctions in disjunctive normal forms, where each clause\ncomprises finite symbolic heaps with existential quantifiers. To compute the\nweakest postconditions in ISL, we introduce a canonicalization that includes\nvariable aliasing.", "AI": {"tldr": "ISL\u662f\u4e00\u79cd\u4e13\u6ce8\u4e8e\u5806\u64cd\u4f5c\u7a0b\u5e8f\u4e2d\u9519\u8bef\u68c0\u6d4b\u7684\u8bc1\u660e\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u6700\u5f31\u540e\u6761\u4ef6\u7684\u8868\u8fbe\u529b\u8bc1\u660e\u4e86\u76f8\u5bf9\u5b8c\u5907\u6027\u3002", "motivation": "\u4f20\u7edf\u903b\u8f91\uff08\u5982Hoare Logic\u6216Separation Logic\uff09\u91c7\u7528\u8fc7\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u800cISL\u4e13\u95e8\u89e3\u51b3\u5806\u64cd\u4f5c\u7a0b\u5e8f\u7684\u6b20\u8fd1\u4f3c\u95ee\u9898\uff0c\u4f46\u5176\u5b8c\u5907\u6027\u5c1a\u672a\u8bc1\u660e\u3002", "method": "\u5229\u7528\u6700\u5f31\u540e\u6761\u4ef6\u7684\u8868\u8fbe\u529b\u8bc1\u660eISL\u7684\u76f8\u5bf9\u5b8c\u5907\u6027\uff0c\u5f15\u5165\u5141\u8bb8\u65e0\u9650\u6790\u53d6\u7684\u6790\u53d6\u8303\u5f0f\uff0c\u5e76\u5305\u542b\u53d8\u91cf\u522b\u540d\u7684\u89c4\u8303\u5316\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86ISL\u7684\u76f8\u5bf9\u5b8c\u5907\u6027\uff0c\u5e76\u901a\u8fc7\u89c4\u8303\u5316\u8ba1\u7b97\u6700\u5f31\u540e\u6761\u4ef6\u3002", "conclusion": "ISL\u5728\u5806\u64cd\u4f5c\u7a0b\u5e8f\u9519\u8bef\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2508.01405", "pdf": "https://arxiv.org/pdf/2508.01405", "abs": "https://arxiv.org/abs/2508.01405", "authors": ["Mengzhao Wang", "Boyu Tan", "Yunjun Gao", "Hai Jin", "Yingfeng Zhang", "Xiangyu Ke", "Xiaoliang Xu", "Yifan Zhu"], "title": "Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search", "categories": ["cs.DB"], "comment": null, "summary": "Hybrid search, the integration of lexical and semantic retrieval, has become\na cornerstone of modern information retrieval systems, driven by demanding\napplications like Retrieval-Augmented Generation (RAG). The architectural\ndesign space for these systems is vast and complex, yet a systematic, empirical\nunderstanding of the trade-offs among their core components--retrieval\nparadigms, combination schemes, and re-ranking methods--is critically lacking.\nTo address this, and informed by our experience building the Infinity\nopen-source database, we present the first systematic benchmark of advanced\nhybrid search architectures. Our framework evaluates four retrieval\nparadigms--Full-Text Search (FTS), Sparse Vector Search (SVS), Dense Vector\nSearch (DVS), and Tensor Search (TenS)--benchmarking their combinations and\nre-ranking strategies across 11 real-world datasets. Our results reveal three\nkey findings for practitioners and researchers: (1) A \"weakest link\"\nphenomenon, where a single underperforming retrieval path can\ndisproportionately degrade overall accuracy, highlighting the need for\npath-wise quality assessment before fusion. (2) A data-driven map of the\nperformance trade-offs, demonstrating that optimal configurations depend\nheavily on resource constraints and data characteristics, moving beyond a\none-size-fits-all approach. (3) The identification of Tensor-based Re-ranking\nFusion (TRF) as a high-efficacy alternative to mainstream fusion methods,\noffering the semantic power of tensor search at a fraction of the computational\nand memory cost. Our findings offer concrete guidelines for designing the next\ngeneration of adaptive, scalable hybrid search systems while also identifying\nkey directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u6df7\u5408\u641c\u7d22\u67b6\u6784\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u5176\u6027\u80fd\u7684\u4e09\u5927\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u6df7\u5408\u641c\u7d22\u5728\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6838\u5fc3\u7ec4\u4ef6\uff08\u5982\u68c0\u7d22\u8303\u5f0f\u3001\u7ec4\u5408\u65b9\u6848\u548c\u91cd\u6392\u5e8f\u65b9\u6cd5\uff09\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5f00\u6e90\u6570\u636e\u5e93Infinity\uff0c\u8bba\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u68c0\u7d22\u8303\u5f0f\uff08FTS\u3001SVS\u3001DVS\u3001TenS\uff09\u53ca\u5176\u7ec4\u5408\u548c\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u6d4b\u8bd5\u4e8611\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\"\u6700\u5f31\u73af\u8282\"\u73b0\u8c61\uff1b\uff082\uff09\u914d\u7f6e\u9700\u6839\u636e\u8d44\u6e90\u548c\u6570\u636e\u7279\u6027\u7075\u6d3b\u8c03\u6574\uff1b\uff083\uff09TRF\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u4e0b\u4e00\u4ee3\u6df7\u5408\u641c\u7d22\u7cfb\u7edf\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.00832", "pdf": "https://arxiv.org/pdf/2508.00832", "abs": "https://arxiv.org/abs/2508.00832", "authors": ["Arimondo Scrivano"], "title": "A Comparative Study of Classical and Post-Quantum Cryptographic Algorithms in the Era of Quantum Computing", "categories": ["cs.ET", "cs.CR", "68-01"], "comment": "16 pages, 2 figures", "summary": "The advent of quantum computing poses a significant threat to the\nfoundational cryptographic algorithms that secure modern digital\ncommunications. Protocols such as HTTPS, digital certificates, and public key\ninfrastructures (PKIs) heavily rely on cryptographic primitives like RSA, ECC,\nand Diffie-Hellman, which are vulnerable to quantum attacks -- most notably\nShor's algorithm. This paper presents a comprehensive comparative analysis\nbetween classical cryptographic algorithms currently in widespread use and\nemerging post-quantum cryptographic schemes designed to withstand quantum\nadversaries. We review the cryptographic mechanisms underpinning modern\ninternet security, outline the mathematical foundations of quantum attacks, and\nevaluate the security, performance, and implementation feasibility of\nquantum-resistant alternatives such as Kyber, Dilithium, and Falcon.\nAdditionally, we assess the hybrid approaches currently being explored by\ninstitutions and tech companies to enable a smooth transition to post-quantum\ncryptography. By providing an in-depth comparison, this study aims to guide\nresearchers, developers, and policymakers in understanding the critical\nimplications of quantum computing on cryptographic infrastructures and the\nnecessary steps for securing communications in the quantum era.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u7ecf\u5178\u5bc6\u7801\u7b97\u6cd5\u4e0e\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\uff0c\u5206\u6790\u4e86\u91cf\u5b50\u8ba1\u7b97\u5bf9\u73b0\u4ee3\u52a0\u5bc6\u6280\u672f\u7684\u5a01\u80c1\u53ca\u5e94\u5bf9\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5bf9\u73b0\u6709\u52a0\u5bc6\u6280\u672f\u6784\u6210\u5a01\u80c1\uff0c\u9700\u8981\u7814\u7a76\u80fd\u591f\u62b5\u5fa1\u91cf\u5b50\u653b\u51fb\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u7ecf\u5178\u5bc6\u7801\u7b97\u6cd5\uff08\u5982RSA\u3001ECC\uff09\u548c\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\uff08\u5982Kyber\u3001Dilithium\uff09\uff0c\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u3001\u6027\u80fd\u548c\u53ef\u884c\u6027\u3002", "result": "\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\u80fd\u6709\u6548\u62b5\u5fa1\u91cf\u5b50\u653b\u51fb\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u548c\u63a8\u5e7f\u5b9e\u65bd\u3002", "conclusion": "\u540e\u91cf\u5b50\u5bc6\u7801\u662f\u672a\u6765\u52a0\u5bc6\u6280\u672f\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u9700\u8981\u8de8\u9886\u57df\u5408\u4f5c\u4ee5\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\u3002"}}
{"id": "2508.01786", "pdf": "https://arxiv.org/pdf/2508.01786", "abs": "https://arxiv.org/abs/2508.01786", "authors": ["Subhasish Mitra", "Subho Banerjee", "Martin Dixon", "Rama Govindaraju", "Peter Hochschild", "Eric X. Liu", "Bharath Parthasarathy", "Parthasarathy Ranganathan"], "title": "Silent Data Corruption by 10x Test Escapes Threatens Reliable Computing", "categories": ["cs.AR", "B.7; C.0; C.5"], "comment": null, "summary": "Too many defective compute chips are escaping existing manufacturing tests --\nat least an order of magnitude more than industrial targets across all compute\nchip types in data centers. Silent data corruptions (SDCs) caused by test\nescapes, when left unaddressed, pose a major threat to reliable computing. We\npresent a three-pronged approach to future directions in overcoming test\nescapes: (a) Quick diagnosis of defective chips directly from system-level\nincorrect behaviors. Such diagnosis is critical for gaining insights into why\nso many defective chips escape existing manufacturing testing. (b) In-field\ndetection of defective chips. (c) New test experiments to understand the\neffectiveness of new techniques for detecting defective chips. These\nexperiments must overcome the drawbacks and pitfalls of previous industrial\ntest experiments and case studies.", "AI": {"tldr": "\u9488\u5bf9\u8ba1\u7b97\u82af\u7247\u5236\u9020\u6d4b\u8bd5\u4e2d\u9057\u6f0f\u7684\u7f3a\u9677\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a\u5feb\u901f\u8bca\u65ad\u3001\u73b0\u573a\u68c0\u6d4b\u548c\u65b0\u6d4b\u8bd5\u5b9e\u9a8c\u3002", "motivation": "\u89e3\u51b3\u5236\u9020\u6d4b\u8bd5\u4e2d\u9057\u6f0f\u7684\u7f3a\u9677\u82af\u7247\u95ee\u9898\uff0c\u907f\u514d\u56e0\u6b64\u5bfc\u81f4\u7684\u65e0\u58f0\u6570\u636e\u635f\u574f\uff0c\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\u3002", "method": "\uff08a\uff09\u4ece\u7cfb\u7edf\u7ea7\u884c\u4e3a\u5feb\u901f\u8bca\u65ad\u7f3a\u9677\u82af\u7247\uff1b\uff08b\uff09\u73b0\u573a\u68c0\u6d4b\u7f3a\u9677\u82af\u7247\uff1b\uff08c\uff09\u8bbe\u8ba1\u65b0\u6d4b\u8bd5\u5b9e\u9a8c\u8bc4\u4f30\u65b0\u6280\u672f\u6548\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u51cf\u5c11\u6d4b\u8bd5\u9057\u6f0f\u7f3a\u9677\u82af\u7247\u7684\u98ce\u9669\uff0c\u5e76\u4e3a\u5de5\u4e1a\u6d4b\u8bd5\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u7ba1\u9f50\u4e0b\u65b9\u6cd5\u6709\u671b\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u82af\u7247\u7684\u6d4b\u8bd5\u9057\u6f0f\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2508.01494", "pdf": "https://arxiv.org/pdf/2508.01494", "abs": "https://arxiv.org/abs/2508.01494", "authors": ["Steven Santillan", "Cristina L. Abad"], "title": "An Analysis of HPC and Edge Architectures in the Cloud", "categories": ["cs.DC", "cs.SE"], "comment": "8 pages, 10 figures, accepted at 2nd Workshop on Accelerated HPC in\n  the Cloud-Edge Continuum 2025, held in conjunction with 13th IEEE\n  International Conference on Cloud Engineering (IC2E 2025)", "summary": "We analyze a recently published dataset of 396 real-world cloud architectures\ndeployed on AWS, from companies belonging to a wide range of industries. From\nthis dataset, we identify those architectures that contain HPC or edge\ncomponents and characterize their designs. Specifically, we investigate the\nprevalence and interplay of AWS services within these architectures, examine\nthe types of storage systems employed, assess architectural complexity and the\nuse of machine learning services, discuss the implications of our findings and\nhow representative these results are of HPC and edge architectures in the\ncloud. This characterization provides valuable insights into current industry\npractices and trends in building robust and scalable HPC and edge solutions in\nthe cloud continuum, and can be valuable for those seeking to better understand\nhow these architectures are being built and to guide new research.", "AI": {"tldr": "\u5206\u6790\u4e86396\u4e2a\u771f\u5b9eAWS\u4e91\u67b6\u6784\u4e2dHPC\u548c\u8fb9\u7f18\u7ec4\u4ef6\uff0c\u63a2\u8ba8\u5176\u8bbe\u8ba1\u7279\u70b9\u3001AWS\u670d\u52a1\u4ea4\u4e92\u3001\u5b58\u50a8\u7cfb\u7edf\u3001\u590d\u6742\u6027\u53caML\u670d\u52a1\u4f7f\u7528\u3002", "motivation": "\u7814\u7a76\u5f53\u524d\u884c\u4e1a\u5728\u4e91\u4e2d\u6784\u5efaHPC\u548c\u8fb9\u7f18\u67b6\u6784\u7684\u5b9e\u9645\u505a\u6cd5\u53ca\u8d8b\u52bf\uff0c\u4e3a\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4ece\u6570\u636e\u96c6\u7b5b\u9009\u542bHPC\u6216\u8fb9\u7f18\u7ec4\u4ef6\u7684\u67b6\u6784\uff0c\u5206\u6790\u5176AWS\u670d\u52a1\u3001\u5b58\u50a8\u7cfb\u7edf\u3001\u590d\u6742\u6027\u53caML\u670d\u52a1\u3002", "result": "\u63ed\u793a\u4e86HPC\u548c\u8fb9\u7f18\u67b6\u6784\u7684\u8bbe\u8ba1\u7279\u70b9\u3001\u670d\u52a1\u4ea4\u4e92\u53ca\u884c\u4e1a\u5b9e\u8df5\uff0c\u63d0\u4f9b\u4e86\u4ee3\u8868\u6027\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u7406\u89e3\u884c\u4e1a\u5b9e\u8df5\u3001\u6307\u5bfc\u672a\u6765\u7814\u7a76\u548c\u6784\u5efa\u7a33\u5065\u4e91\u67b6\u6784\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.01443", "pdf": "https://arxiv.org/pdf/2508.01443", "abs": "https://arxiv.org/abs/2508.01443", "authors": ["Jingzhi Gong", "Rafail Giavrimis", "Paul Brookes", "Vardan Voskanyan", "Fan Wu", "Mari Ashiga", "Matthew Truscott", "Mike Basios", "Leslie Kanthan", "Jie Xu", "Zheng Wang"], "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to ASE'25 Industry Showcase", "summary": "There is a growing interest in leveraging large language models (LLMs) for\nautomated code optimization. However, industrial platforms deploying multiple\nLLMs face a critical challenge: prompts optimized for one LLM often fail with\nothers, requiring expensive model-specific prompt engineering. This cross-model\nprompt engineering bottleneck severely limits the practical deployment of\nmulti-LLM optimization systems in production environments. To address this, we\nintroduce Meta-Prompted Code Optimization (MPCO), a framework that\nautomatically generates high-quality, task-specific prompts across diverse LLMs\nwhile maintaining industrial efficiency requirements. MPCO leverages\nmeta-prompting to dynamically synthesize context-aware optimization prompts by\nintegrating project metadata, task requirements, and LLM-specific contexts, and\nit seamlessly deploys on the ARTEMIS industrial platform for automated\nvalidation and scaling.\n  Our comprehensive evaluation on five real-world codebases with 366 hours of\nruntime benchmarking demonstrates MPCO's effectiveness: it achieves overall\nperformance improvements up to 19.06% with the best statistical rank across all\nsystems compared to baseline methods. Analysis shows that 96% of the\ntop-performing optimizations stem from meaningful edits. Through systematic\nablation studies and meta-prompter sensitivity analysis, we identify that\ncomprehensive context integration is essential for effective meta-prompting,\nand that all three major LLMs can serve effectively as meta-prompters,\nproviding actionable insights for industrial practitioners.", "AI": {"tldr": "MPCO\u6846\u67b6\u901a\u8fc7\u5143\u63d0\u793a\u6280\u672f\u81ea\u52a8\u751f\u6210\u9002\u7528\u4e8e\u591a\u79cdLLM\u7684\u9ad8\u8d28\u91cf\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u5e73\u53f0\u4e0a\u591aLLM\u90e8\u7f72\u65f6\u7684\u63d0\u793a\u5de5\u7a0b\u74f6\u9888\u3002", "motivation": "\u5de5\u4e1a\u5e73\u53f0\u4e0a\u90e8\u7f72\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u9488\u5bf9\u4e00\u4e2a\u6a21\u578b\u4f18\u5316\u7684\u63d0\u793a\u5f80\u5f80\u4e0d\u9002\u7528\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u6a21\u578b\u7279\u5b9a\u63d0\u793a\u5de5\u7a0b\u6210\u672c\uff0c\u9650\u5236\u4e86\u591aLLM\u4f18\u5316\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "MPCO\u4f7f\u7528\u5143\u63d0\u793a\u6280\u672f\u52a8\u6001\u5408\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4f18\u5316\u63d0\u793a\uff0c\u6574\u5408\u9879\u76ee\u5143\u6570\u636e\u3001\u4efb\u52a1\u9700\u6c42\u548cLLM\u7279\u5b9a\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728ARTEMIS\u5de5\u4e1a\u5e73\u53f0\u4e0a\u81ea\u52a8\u9a8c\u8bc1\u548c\u6269\u5c55\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4ee3\u7801\u5e93\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMPCO\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe19.06%\uff0c96%\u7684\u9876\u7ea7\u4f18\u5316\u6765\u81ea\u6709\u6548\u7f16\u8f91\uff0c\u4e14\u4e09\u5927LLM\u5747\u53ef\u4f5c\u4e3a\u5143\u63d0\u793a\u751f\u6210\u5668\u3002", "conclusion": "MPCO\u901a\u8fc7\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u96c6\u6210\u548c\u591aLLM\u517c\u5bb9\u6027\uff0c\u4e3a\u5de5\u4e1a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01506", "pdf": "https://arxiv.org/pdf/2508.01506", "abs": "https://arxiv.org/abs/2508.01506", "authors": ["Zishan Shao", "Yixiao Wang", "Qinsi Wang", "Ting Jiang", "Zhixu Du", "Hancheng Ye", "Danyang Zhuo", "Yiran Chen", "Hai Li"], "title": "FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": "Technical Report", "summary": "Singular Value Decomposition (SVD) has recently seen a surge of interest as a\nsimple yet powerful tool for large language models (LLMs) compression, with a\ngrowing number of works demonstrating 20-80% parameter reductions at minimal\naccuracy loss. Previous SVD-based approaches have focused primarily on reducing\nthe memory footprint of model weights, largely overlooking the additional\nactivation memory overhead incurred during inference when applying truncated\nfactors via standard dense CUDA kernels. Our experiments demonstrate that this\nactivation overhead, scaling with sequence length and hidden dimension,\nprevents current SVD compression techniques from achieving any reduction in\npeak inference memory, thereby limiting their viability for real-world,\non-device deployments.\n  We introduce FlashSVD, a novel, end-to-end rank-aware streaming inference\nframework specifically designed for SVD-compressed large language models.\nFlashSVD can be seamlessly integrated with any model that employs SVD-based\nmethods for parameter reduction. By fusing low-rank projection kernels directly\ninto both the self-attention and feed-forward network (FFN) pipelines, FlashSVD\navoid materializing full-size activation buffers. Instead, small tiles of the\ntruncated factors are loaded into on-chip SRAM, multiplied and reduced on the\nfly, and immediately evicted, preserving high GPU occupancy and adding no extra\nlatency. On standard encoder benchmarks (e.g., BERT-Base), FlashSVD cuts peak\nactivation memory by up to 70.2% and intermediate transient memory by 75%, all\nwhile incur no accuracy loss with upstreaming compression methods, offering a\npractical path toward memory-constrained deployment of low-rank LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlashSVD\uff0c\u4e00\u79cd\u4e13\u4e3aSVD\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u5185\u5b58\u5f00\u9500\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SVD\u538b\u7f29\u65b9\u6cd5\u867d\u80fd\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u91cf\uff0c\u4f46\u5ffd\u7565\u4e86\u63a8\u7406\u65f6\u7684\u6fc0\u6d3b\u5185\u5b58\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "FlashSVD\u901a\u8fc7\u878d\u5408\u4f4e\u79e9\u6295\u5f71\u6838\u5230\u81ea\u6ce8\u610f\u529b\u548cFFN\u7ba1\u9053\u4e2d\uff0c\u907f\u514d\u5168\u5c3a\u5bf8\u6fc0\u6d3b\u7f13\u51b2\u533a\u7684\u751f\u6210\uff0c\u4ece\u800c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlashSVD\u5728BERT-Base\u4e0a\u51cf\u5c11\u5cf0\u503c\u6fc0\u6d3b\u5185\u5b5870.2%\uff0c\u4e2d\u95f4\u4e34\u65f6\u5185\u5b5875%\uff0c\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "FlashSVD\u4e3a\u4f4e\u79e9LLMs\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.00952", "pdf": "https://arxiv.org/pdf/2508.00952", "abs": "https://arxiv.org/abs/2508.00952", "authors": ["Matthew G Crowson", "Leo Celi A. Celi"], "title": "Academic Vibe Coding: Opportunities for Accelerating Research in an Era of Resource Constraint", "categories": ["cs.CY", "cs.AI", "cs.PL", "cs.SE"], "comment": null, "summary": "Academic laboratories face mounting resource constraints: budgets are\ntightening, grant overheads are potentially being capped, and the market rate\nfor data-science talent significantly outstrips university compensation. Vibe\ncoding, which is structured, prompt-driven code generation with large language\nmodels (LLMs) embedded in reproducible workflows, offers one pragmatic\nresponse. It aims to compress the idea-to-analysis timeline, reduce staffing\npressure on specialized data roles, and maintain rigorous, version-controlled\noutputs. This article defines the vibe coding concept, situates it against the\ncurrent academic resourcing crisis, details a beginner-friendly toolchain for\nits implementation, and analyzes inherent limitations that necessitate\ngovernance and mindful application.", "AI": {"tldr": "Vibe coding\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\uff0c\u4ee5\u7f13\u89e3\u5b66\u672f\u5b9e\u9a8c\u5ba4\u8d44\u6e90\u7d27\u5f20\u7684\u95ee\u9898\u3002", "motivation": "\u5b66\u672f\u5b9e\u9a8c\u5ba4\u9762\u4e34\u9884\u7b97\u7d27\u7f29\u548c\u4eba\u624d\u77ed\u7f3a\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u6784\u5316\u3001\u63d0\u793a\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\uff0c\u5d4c\u5165\u53ef\u91cd\u590d\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u7f29\u77ed\u5206\u6790\u65f6\u95f4\uff0c\u51cf\u8f7b\u4eba\u529b\u8d44\u6e90\u538b\u529b\uff0c\u4fdd\u6301\u4e25\u683c\u7248\u672c\u63a7\u5236\u3002", "conclusion": "Vibe coding\u662f\u4e00\u79cd\u5b9e\u7528\u5de5\u5177\uff0c\u4f46\u9700\u7ed3\u5408\u76d1\u7ba1\u548c\u8c28\u614e\u4f7f\u7528\u3002"}}
{"id": "2508.01181", "pdf": "https://arxiv.org/pdf/2508.01181", "abs": "https://arxiv.org/abs/2508.01181", "authors": ["Zhiyuan Han", "Beier Zhu", "Yanlong Xu", "Peipei Song", "Xun Yang"], "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning", "categories": ["cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "68", "I.2.10"], "comment": "ACM Multimedia 2025", "summary": "Despite their strong performance in multimodal emotion reasoning, existing\nMultimodal Large Language Models (MLLMs) often overlook the scenarios involving\nemotion conflicts, where emotional cues from different modalities are\ninconsistent. To fill this gap, we first introduce CA-MER, a new benchmark\ndesigned to examine MLLMs under realistic emotion conflicts. It consists of\nthree subsets: video-aligned, audio-aligned, and consistent, where only one or\nall modalities reflect the true emotion. However, evaluations on our CA-MER\nreveal that current state-of-the-art emotion MLLMs systematically over-rely on\naudio signal during emotion conflicts, neglecting critical cues from visual\nmodality. To mitigate this bias, we propose MoSEAR, a parameter-efficient\nframework that promotes balanced modality integration. MoSEAR consists of two\nmodules: (1)MoSE, modality-specific experts with a regularized gating mechanism\nthat reduces modality bias in the fine-tuning heads; and (2)AR, an attention\nreallocation mechanism that rebalances modality contributions in frozen\nbackbones during inference. Our framework offers two key advantages: it\nmitigates emotion conflicts and improves performance on consistent\nsamples-without incurring a trade-off between audio and visual modalities.\nExperiments on multiple benchmarks-including MER2023, EMER, DFEW, and our\nCA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,\nparticularly under modality conflict conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u57fa\u51c6CA-MER\u548c\u6846\u67b6MoSEAR\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u51b2\u7a81\u4e2d\u7684\u97f3\u9891\u504f\u597d\u95ee\u9898\uff0c\u5b9e\u73b0\u5e73\u8861\u7684\u591a\u6a21\u6001\u6574\u5408\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u60c5\u611f\u51b2\u7a81\u573a\u666f\uff08\u4e0d\u540c\u6a21\u6001\u60c5\u611f\u7ebf\u7d22\u4e0d\u4e00\u81f4\uff09\u4e0b\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faMoSEAR\u6846\u67b6\uff0c\u5305\u542bMoSE\uff08\u51cf\u5c11\u6a21\u6001\u504f\u89c1\u7684\u5fae\u8c03\u5934\uff09\u548cAR\uff08\u91cd\u5206\u914d\u6ce8\u610f\u529b\u673a\u5236\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoSEAR\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u6a21\u6001\u51b2\u7a81\u6761\u4ef6\u4e0b\u3002", "conclusion": "MoSEAR\u6709\u6548\u89e3\u51b3\u6a21\u6001\u504f\u89c1\uff0c\u63d0\u5347\u60c5\u611f\u63a8\u7406\u6027\u80fd\uff0c\u65e0\u6a21\u6001\u95f4\u6743\u8861\u3002"}}
{"id": "2508.00847", "pdf": "https://arxiv.org/pdf/2508.00847", "abs": "https://arxiv.org/abs/2508.00847", "authors": ["Sofia Sahab", "Jawad Haqbeen", "Diksha Sapkota", "Takayuki Ito"], "title": "GPT Chatbots for Alleviating Anxiety and Depression: A Pilot Randomized Controlled Trial with Afghan Women", "categories": ["cs.HC", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this study, we investigated the effects of GPT-4, with and without\nspecific conversational instructions, on the mental health of Afghan women.\nThese women face multifaceted challenges, including Taliban-imposed\nrestrictions, societal inequalities, and domestic violence, adversely affecting\ntheir well-being. We conducted a randomized controlled trial with 60\nparticipants, dividing them into three groups: GPT-4, a supportive listener\n(GPT-4 with empathetic engagement instructions), and a waiting list. The\nHospital Anxiety and Depression Scale (HADS) was used to measure anxiety and\ndepression before and after the intervention. Linguistic analysis of chat data\nexamined personal pronouns, tones, emotions, and Language Style Matching (LSM).\nThe supportive listener group showed a significant reduction in HADS scores\ncompared to the other groups. Linguistic analysis revealed a more positive tone\nand higher LSM in the supportive listener group, with a significant negative\ncorrelation between LSM and changes in HADS scores, indicating greater\nlinguistic alignment was linked to reductions in anxiety and depression.\nPerceived empathy ratings were also significantly higher in the supportive\nlistener group. These findings highlight the potential of AI-driven\ninterventions, like GPT-4, in providing accessible mental health support.\nHowever, such interventions should complement traditional psychotherapy,\nensuring a collaborative approach to optimize therapeutic outcomes.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86GPT-4\u5bf9\u963f\u5bcc\u6c57\u5973\u6027\u5fc3\u7406\u5065\u5eb7\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5e26\u6709\u5171\u60c5\u6307\u4ee4\u7684GPT-4\u7ec4\u663e\u8457\u964d\u4f4e\u4e86\u7126\u8651\u548c\u6291\u90c1\u5206\u6570\u3002", "motivation": "\u963f\u5bcc\u6c57\u5973\u6027\u9762\u4e34\u5854\u5229\u73ed\u9650\u5236\u3001\u793e\u4f1a\u4e0d\u5e73\u7b49\u548c\u5bb6\u5ead\u66b4\u529b\u7b49\u591a\u91cd\u6311\u6218\uff0c\u4e9f\u9700\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u3002", "method": "\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c60\u540d\u53c2\u4e0e\u8005\u5206\u4e3a\u4e09\u7ec4\uff1a\u6807\u51c6GPT-4\u3001\u5e26\u5171\u60c5\u6307\u4ee4\u7684GPT-4\u548c\u7b49\u5f85\u7ec4\uff0c\u4f7f\u7528HADS\u6d4b\u91cf\u5fc3\u7406\u72b6\u6001\u3002", "result": "\u5171\u60c5\u6307\u4ee4\u7ec4HADS\u5206\u6570\u663e\u8457\u4e0b\u964d\uff0c\u8bed\u8a00\u5206\u6790\u663e\u793a\u66f4\u5f3a\u7684\u8bed\u8a00\u4e00\u81f4\u6027\u4e0e\u5fc3\u7406\u5065\u5eb7\u6539\u5584\u76f8\u5173\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u5e72\u9884\u53ef\u4f5c\u4e3a\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u8865\u5145\uff0c\u4f46\u9700\u4e0e\u4f20\u7edf\u7597\u6cd5\u7ed3\u5408\u4ee5\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2508.01381", "pdf": "https://arxiv.org/pdf/2508.01381", "abs": "https://arxiv.org/abs/2508.01381", "authors": ["Onat Vuran", "Hsuan-I Ho"], "title": "ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers", "categories": ["cs.GR", "cs.CV"], "comment": "BMVC 2025 paper, 17 pages, 10 figures", "summary": "The reconstruction of multi-layer 3D garments typically requires expensive\nmulti-view capture setups and specialized 3D editing efforts. To support the\ncreation of life-like clothed human avatars, we introduce ReMu for\nreconstructing multi-layer clothed humans in a new setup, Image Layers, which\ncaptures a subject wearing different layers of clothing with a single RGB\ncamera. To reconstruct physically plausible multi-layer 3D garments, a unified\n3D representation is necessary to model these garments in a layered manner.\nThus, we first reconstruct and align each garment layer in a shared coordinate\nsystem defined by the canonical body pose. Afterwards, we introduce a\ncollision-aware optimization process to address interpenetration and further\nrefine the garment boundaries leveraging implicit neural fields. It is worth\nnoting that our method is template-free and category-agnostic, which enables\nthe reconstruction of 3D garments in diverse clothing styles. Through our\nexperiments, we show that our method reconstructs nearly penetration-free 3D\nclothed humans and achieves competitive performance compared to\ncategory-specific methods. Project page: https://eth-ait.github.io/ReMu/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReMu\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76eeRGB\u76f8\u673a\u6293\u53d6\u56fe\u50cf\u5c42\uff0c\u91cd\u5efa\u591a\u5c423D\u670d\u88c5\uff0c\u65e0\u9700\u6a21\u677f\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u670d\u88c5\u98ce\u683c\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u903c\u771f\u7684\u7a7f\u8863\u4eba\u4f53\u5316\u8eab\u7684\u521b\u5efa\uff0c\u907f\u514d\u6602\u8d35\u7684\u591a\u89c6\u89d2\u6355\u6349\u8bbe\u5907\u548c\u4e13\u4e1a3D\u7f16\u8f91\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u5750\u6807\u7cfb\u91cd\u5efa\u548c\u5bf9\u9f50\u6bcf\u4e00\u5c42\u670d\u88c5\uff0c\u5e76\u91c7\u7528\u78b0\u649e\u611f\u77e5\u4f18\u5316\u548c\u9690\u5f0f\u795e\u7ecf\u573a\u7ec6\u5316\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u91cd\u5efa\u4e86\u8fd1\u4e4e\u65e0\u7a7f\u900f\u76843D\u7a7f\u8863\u4eba\u4f53\uff0c\u6027\u80fd\u4e0e\u7c7b\u522b\u7279\u5b9a\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ReMu\u65b9\u6cd5\u4e3a\u91cd\u5efa\u591a\u5c423D\u670d\u88c5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01758", "pdf": "https://arxiv.org/pdf/2508.01758", "abs": "https://arxiv.org/abs/2508.01758", "authors": ["Pinaki Chakraborty", "Tristan Caulfield", "David Pym"], "title": "Causality and Decision-making: A Logical Framework for Systems and Security Modelling", "categories": ["cs.LO", "cs.MA"], "comment": "28 pages", "summary": "Causal reasoning is essential for understanding decision-making about the\nbehaviour of complex `ecosystems' of systems that underpin modern society, with\nsecurity -- including issues around correctness, safety, resilience, etc. --\ntypically providing critical examples. We present a theory of strategic\nreasoning about system modelling based on minimal structural assumptions and\nemploying the methods of transition systems, supported by a modal logic of\nsystem states in the tradition of van Benthem, Hennessy, and Milner, and\nvalidated through equivalence theorems. Our framework introduces an\nintervention operator and a separating conjunction to capture actual causal\nrelationships between component systems of the ecosystem, aligning naturally\nwith Halpern and Pearl's counterfactual approach based on Structural Causal\nModels. We illustrate the applicability through examples of of decision-making\nabout microservices in distributed systems. We discuss localized\ndecision-making through a separating conjunction. This work unifies a formal,\nminimalistic notion of system behaviour with a Halpern--Pearl-compatible theory\nof counterfactual reasoning, providing a logical foundation for studying\ndecision making about causality in complex interacting systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u7ed3\u6784\u5047\u8bbe\u7684\u6218\u7565\u6027\u7cfb\u7edf\u5efa\u6a21\u7406\u8bba\uff0c\u5f15\u5165\u5e72\u9884\u7b97\u5b50\u548c\u5206\u79bb\u5408\u53d6\uff0c\u7edf\u4e00\u5f62\u5f0f\u5316\u7684\u7cfb\u7edf\u884c\u4e3a\u4e0eHalpern-Pearl\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002", "motivation": "\u4e3a\u7406\u89e3\u590d\u6742\u7cfb\u7edf\u4e2d\u51b3\u7b56\u5236\u5b9a\u7684\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u76f8\u5173\u9886\u57df\u3002", "method": "\u91c7\u7528\u8f6c\u6362\u7cfb\u7edf\u65b9\u6cd5\u548c\u6a21\u6001\u903b\u8f91\uff0c\u5f15\u5165\u5e72\u9884\u7b97\u5b50\u548c\u5206\u79bb\u5408\u53d6\uff0c\u7ed3\u5408Halpern-Pearl\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5fae\u670d\u52a1\u51b3\u7b56\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u590d\u6742\u7cfb\u7edf\u4e2d\u56e0\u679c\u51b3\u7b56\u63d0\u4f9b\u4e86\u903b\u8f91\u57fa\u7840\uff0c\u7edf\u4e00\u4e86\u5f62\u5f0f\u5316\u884c\u4e3a\u4e0e\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002"}}
{"id": "2508.01931", "pdf": "https://arxiv.org/pdf/2508.01931", "abs": "https://arxiv.org/abs/2508.01931", "authors": ["Wenjie Hu", "Guanzhou Hu", "Mahesh Balakrishnan", "Xiangyao Yu"], "title": "Marlin: Efficient Coordination for Autoscaling Cloud DBMS (Extended Version)", "categories": ["cs.DB"], "comment": null, "summary": "Modern cloud databases are shifting from converged architectures to storage\ndisaggregation, enabling independent scaling and billing of compute and\nstorage. However, cloud databases still rely on external, converged\ncoordination services (e.g., ZooKeeper) for their control planes. These\nservices are effectively lightweight databases optimized for low-volume\nmetadata. As the control plane scales in the cloud, this approach faces similar\nlimitations as converged databases did before storage disaggregation:\nscalability bottlenecks, low cost efficiency, and increased operational burden.\n  We propose to disaggregate the cluster coordination to achieve the same\nbenefits that storage disaggregation brought to modern cloud DBMSs. We present\nMarlin, a cloud-native coordination mechanism that fully embraces storage\ndisaggregation. Marlin eliminates the need for external coordination services\nby consolidating coordination functionality into the existing cloud-native\ndatabase it manages. To achieve failover without an external coordination\nservice, Marlin allows cross-node modifications on coordination states. To\nensure data consistency, Marlin employs transactions to manage both\ncoordination and application states and introduces MarlinCommit, an optimized\ncommit protocol that ensures strong transactional guarantees even under\ncross-node modifications. Our evaluations demonstrate that Marlin improves cost\nefficiency by up to 4.4x and reduces reconfiguration duration by up to 4.9x\ncompared to converged coordination solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMarlin\uff0c\u4e00\u79cd\u4e91\u539f\u751f\u534f\u8c03\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u8026\u96c6\u7fa4\u534f\u8c03\u89e3\u51b3\u4e86\u4f20\u7edf\u5916\u90e8\u534f\u8c03\u670d\u52a1\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u672c\u6548\u7387\u548c\u91cd\u6784\u901f\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u4e91\u6570\u636e\u5e93\u867d\u5df2\u5b9e\u73b0\u5b58\u50a8\u89e3\u8026\uff0c\u4f46\u63a7\u5236\u5e73\u9762\u4ecd\u4f9d\u8d56\u5916\u90e8\u534f\u8c03\u670d\u52a1\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u74f6\u9888\u548c\u6210\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMarlin\uff0c\u5c06\u534f\u8c03\u529f\u80fd\u96c6\u6210\u5230\u4e91\u539f\u751f\u6570\u636e\u5e93\u4e2d\uff0c\u652f\u6301\u8de8\u8282\u70b9\u4fee\u6539\u534f\u8c03\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u4e8b\u52a1\u548c\u4f18\u5316\u7684\u63d0\u4ea4\u534f\u8bae\u786e\u4fdd\u6570\u636e\u4e00\u81f4\u6027\u3002", "result": "Marlin\u6bd4\u4f20\u7edf\u89e3\u51b3\u65b9\u6848\u6210\u672c\u6548\u7387\u63d0\u53474.4\u500d\uff0c\u91cd\u6784\u65f6\u95f4\u7f29\u77ed4.9\u500d\u3002", "conclusion": "Marlin\u4e3a\u4e91\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u534f\u8c03\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5916\u90e8\u534f\u8c03\u670d\u52a1\u7684\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.00837", "pdf": "https://arxiv.org/pdf/2508.00837", "abs": "https://arxiv.org/abs/2508.00837", "authors": ["Yuqi Zhang", "Yuxin Yang", "Cheng-Chang Lu", "Weiwen Jiang", "Feixiong Cheng", "Bo Fang", "Qiang Guan"], "title": "QDockBank: A Dataset for Ligand Docking on Protein Fragments Predicted on Utility-Level Quantum Computers", "categories": ["cs.ET"], "comment": "15 pages, 7 figures. Accepted at SC 2025. Preprint for conference\n  submission", "summary": "Protein structure prediction is a core challenge in computational biology,\nparticularly for fragments within ligand-binding regions, where accurate\nmodeling is still difficult. Quantum computing offers a novel first-principles\nmodeling paradigm, but its application is currently limited by hardware\nconstraints, high computational cost, and the lack of a standardized\nbenchmarking dataset. In this work, we present QDockBank-the first large-scale\nprotein fragment structure dataset generated entirely using utility-level\nquantum computers, specifically designed for protein-ligand docking tasks.\nQDockBank comprises 55 protein fragments extracted from ligand-binding pockets.\nThe dataset was generated through tens of hours of execution on superconducting\nquantum processors, making it the first quantum-based protein structure dataset\nwith a total computational cost exceeding one million USD. Experimental\nevaluations demonstrate that structures predicted by QDockBank outperform those\npredicted by AlphaFold2 and AlphaFold3 in terms of both RMSD and docking\naffinity scores. QDockBank serves as a new benchmark for evaluating\nquantum-based protein structure prediction.", "AI": {"tldr": "QDockBank\u662f\u9996\u4e2a\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u673a\u751f\u6210\u7684\u5927\u89c4\u6a21\u86cb\u767d\u8d28\u7247\u6bb5\u7ed3\u6784\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u86cb\u767d\u8d28-\u914d\u4f53\u5bf9\u63a5\u4efb\u52a1\u4e2d\u7684\u7ed3\u6784\u9884\u6d4b\u95ee\u9898\u3002", "motivation": "\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u662f\u8ba1\u7b97\u751f\u7269\u5b66\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5c24\u5176\u662f\u914d\u4f53\u7ed3\u5408\u533a\u57df\u7684\u7247\u6bb5\u5efa\u6a21\u4ecd\u5177\u96be\u5ea6\u3002\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f46\u53d7\u9650\u4e8e\u786c\u4ef6\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u7814\u7a76\u56e2\u961f\u4f7f\u7528\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u751f\u6210\u4e8655\u4e2a\u86cb\u767d\u8d28\u7247\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u603b\u8ba1\u7b97\u6210\u672c\u8d85\u8fc7100\u4e07\u7f8e\u5143\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cQDockBank\u9884\u6d4b\u7684\u7ed3\u6784\u5728RMSD\u548c\u5bf9\u63a5\u4eb2\u548c\u529b\u5f97\u5206\u4e0a\u4f18\u4e8eAlphaFold2\u548cAlphaFold3\u3002", "conclusion": "QDockBank\u4e3a\u91cf\u5b50\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.01800", "pdf": "https://arxiv.org/pdf/2508.01800", "abs": "https://arxiv.org/abs/2508.01800", "authors": ["Ajay Kumar M", "Cian O'Mahoney", "Pedro Kreutz Werle", "Shreejith Shanker", "Dimitrios S. Nikolopoulos", "Bo Ji", "Hans Vandierendonck", "Deepu John"], "title": "MARVEL: An End-to-End Framework for Generating Model-Class Aware Custom RISC-V Extensions for Lightweight AI", "categories": ["cs.AR"], "comment": "To be published in IEEE Open Journal of Circuits and Systems", "summary": "Deploying deep neural networks (DNNs) on resource-constrained IoT devices\nremains a challenging problem, often requiring hardware modifications tailored\nto individual AI models. Existing accelerator-generation tools, such as AMD's\nFINN, do not adequately address extreme resource limitations faced by IoT\nendpoints operating in bare-metal environments without an operating system\n(OS). To overcome these constraints, we propose MARVEL-an automated, end-to-end\nframework that generates custom RISC-V ISA extensions tailored to specific DNN\nmodel classes, with a primary focus on convolutional neural networks (CNNs).\nThe proposed method profiles high-level DNN representations in Python and\ngenerates an ISA-extended RISC-V core with associated compiler tools for\nefficient deployment. The flow leverages (1) Apache TVM for translating\nhigh-level Python-based DNN models into optimized C code, (2) Synopsys ASIP\nDesigner for identifying compute-intensive kernels, modeling, and generating a\ncustom RISC-V and (3) Xilinx Vivado for FPGA implementation. Beyond a model\nclass specific RISC-V, our approach produces an optimized bare-metal C\nimplementation, eliminating the need for an OS or extensive software\ndependencies. Unlike conventional deployment pipelines relying on\nTensorFlow/PyTorch runtimes, our solution enables seamless execution in highly\nresource-constrained environments. We evaluated the flow on popular DNN models\nsuch as LeNet-5*, MobileNetV1, ResNet50, VGG16, MobileNetV2 and DenseNet121\nusing the Synopsys trv32p3 RISC-V core as a baseline. Results show a 2x speedup\nin inference and upto 2x reduction in energy per inference at a 28.23% area\noverhead when implemented on an AMD Zynq UltraScale+ ZCU104 FPGA platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMARVEL\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5b9a\u5236RISC-V ISA\u6269\u5c55\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u5728\u6781\u7aef\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\uff08\u65e0\u64cd\u4f5c\u7cfb\u7edf\uff09\u4e0a\u90e8\u7f72DNN\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528Apache TVM\u3001Synopsys ASIP Designer\u548cXilinx Vivado\uff0c\u5c06Python\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9a\u5236RISC-V ISA\u6269\u5c55\u53ca\u4f18\u5316\u7684C\u4ee3\u7801\u3002", "result": "\u5728FPGA\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e862\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c\u9ad8\u8fbe2\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u9762\u79ef\u5f00\u9500\u4e3a28.23%\u3002", "conclusion": "MARVEL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2dDNN\u90e8\u7f72\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.01762", "pdf": "https://arxiv.org/pdf/2508.01762", "abs": "https://arxiv.org/abs/2508.01762", "authors": ["Yann Bourreau", "Sebastian Brandt", "Alexandre Nolin"], "title": "Faster Distributed $\u0394$-Coloring via a Reduction to MIS", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "Recent improvements on the deterministic complexities of fundamental graph\nproblems in the LOCAL model of distributed computing have yielded\nstate-of-the-art upper bounds of $\\tilde{O}(\\log^{5/3} n)$ rounds for maximal\nindependent set (MIS) and $(\\Delta + 1)$-coloring [Ghaffari, Grunau, FOCS'24]\nand $\\tilde{O}(\\log^{19/9} n)$ rounds for the more restrictive\n$\\Delta$-coloring problem [Ghaffari, Kuhn, FOCS'21; Ghaffari, Grunau, FOCS'24;\nBourreau, Brandt, Nolin, STOC'25]. In our work, we show that $\\Delta$-coloring\ncan be solved deterministically in $\\tilde{O}(\\log^{5/3} n)$ rounds as well,\nmatching the currently best bound for $(\\Delta + 1)$-coloring.\n  We achieve our result by developing a reduction from $\\Delta$-coloring to MIS\nthat guarantees that the (asymptotic) complexity of $\\Delta$-coloring is at\nmost the complexity of MIS, unless MIS can be solved in sublogarithmic time, in\nwhich case, due to the $\\Omega(\\log n)$-round $\\Delta$-coloring lower bound\nfrom [BFHKLRSU, STOC'16], our reduction implies a tight complexity of\n$\\Theta(\\log n)$ for $\\Delta$-coloring. In particular, any improvement on the\ncomplexity of the MIS problem will yield the same improvement for the\ncomplexity of $\\Delta$-coloring (up to the true complexity of\n$\\Delta$-coloring).\n  Our reduction yields improvements for $\\Delta$-coloring in the randomized\nLOCAL model and when complexities are parameterized by both $n$ and $\\Delta$.\nWe obtain a randomized complexity bound of $\\tilde{O}(\\log^{5/3} \\log n)$\nrounds (improving over the state of the art of $\\tilde{O}(\\log^{8/3} \\log n)$\nrounds) on general graphs and tight complexities of $\\Theta(\\log n)$ and\n$\\Theta(\\log \\log n)$ for the deterministic, resp.\\ randomized, complexity on\nbounded-degree graphs. In the special case of graphs of constant clique number\n(which for instance include bipartite graphs), we also give a reduction to the\n$(\\Delta+1)$-coloring problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u786e\u5b9a\u6027\u0394-\u7740\u8272\u95ee\u9898\u5728LOCAL\u6a21\u578b\u4e0b\u7684\u590d\u6742\u5ea6\u53ef\u4ee5\u4ece\u73b0\u6709\u7684O(log^{19/9} n)\u964d\u81f3O(log^{5/3} n)\u8f6e\uff0c\u4e0e\u76ee\u524d(\u0394+1)-\u7740\u8272\u7684\u6700\u4f73\u4e0a\u754c\u5339\u914d\u3002\u901a\u8fc7\u5c06\u0394-\u7740\u8272\u5f52\u7ea6\u4e3aMIS\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8fd9\u4e00\u7ed3\u679c\u3002", "motivation": "\u63d0\u5347\u0394-\u7740\u8272\u95ee\u9898\u7684\u786e\u5b9a\u6027\u590d\u6742\u5ea6\uff0c\u4f7f\u5176\u4e0e(\u0394+1)-\u7740\u8272\u95ee\u9898\u7684\u6700\u65b0\u4e0a\u754c\u4e00\u81f4\uff0c\u5e76\u63a2\u7d22\u5176\u4e0eMIS\u95ee\u9898\u7684\u5173\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ece\u0394-\u7740\u8272\u5230MIS\u95ee\u9898\u7684\u5f52\u7ea6\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u0394-\u7740\u8272\u7684\u590d\u6742\u5ea6\u4e0d\u4f1a\u8d85\u8fc7MIS\u7684\u590d\u6742\u5ea6\uff0c\u9664\u975eMIS\u80fd\u5728\u4e9a\u5bf9\u6570\u65f6\u95f4\u5185\u89e3\u51b3\u3002", "result": "\u0394-\u7740\u8272\u95ee\u9898\u7684\u590d\u6742\u5ea6\u964d\u81f3O(log^{5/3} n)\uff0c\u540c\u65f6\u5728\u968f\u673aLOCAL\u6a21\u578b\u548c\u53c2\u6570\u5316\u590d\u6742\u5ea6\u4e0b\u4e5f\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "\u4efb\u4f55\u5bf9MIS\u590d\u6742\u5ea6\u7684\u6539\u8fdb\u90fd\u5c06\u76f4\u63a5\u63d0\u5347\u0394-\u7740\u8272\u95ee\u9898\u7684\u590d\u6742\u5ea6\uff0c\u4f7f\u5176\u8fbe\u5230\u5176\u771f\u6b63\u590d\u6742\u5ea6\u7684\u4e0a\u9650\u3002"}}
{"id": "2508.01047", "pdf": "https://arxiv.org/pdf/2508.01047", "abs": "https://arxiv.org/abs/2508.01047", "authors": ["Efe A\u011flamazlar", "Emirhan Eken", "Harun Batur Ge\u00e7ici"], "title": "A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation", "categories": ["cs.NI", "cs.AI", "C.2.2; I.2.6; I.2.8"], "comment": "This paper presents a novel TCP congestion control algorithm based on\n  Deep Reinforcement Learning. The study includes 5 figures and 8 pages of\n  content", "summary": "This paper presents a novel TCP congestion control algorithm based on Deep\nReinforcement Learning. The proposed approach utilizes Deep Q-Networks to\noptimize the congestion window (cWnd) by observing key network parameters and\ntaking real-time actions. The algorithm is trained and evaluated within the\nNS-3 network simulator using the OpenGym interface. The results demonstrate\nsignificant improvements over traditional TCP New Reno in terms of latency and\nthroughput, with better adaptability to changing network conditions. This study\nemphasizes the potential of reinforcement learning techniques for solving\ncomplex congestion control problems in modern networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684TCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u73b0\u4ee3\u7f51\u7edc\u4e2d\u590d\u6742\u62e5\u585e\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528Deep Q-Networks\u4f18\u5316\u62e5\u585e\u7a97\u53e3\uff0c\u901a\u8fc7NS-3\u6a21\u62df\u5668\u548cOpenGym\u63a5\u53e3\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u76f8\u6bd4\u4f20\u7edfTCP New Reno\uff0c\u7b97\u6cd5\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u5e94\u6027\u66f4\u5f3a\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5728\u62e5\u585e\u63a7\u5236\u4e2d\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.01472", "pdf": "https://arxiv.org/pdf/2508.01472", "abs": "https://arxiv.org/abs/2508.01472", "authors": ["Lukas Kirschner", "Ezekiel Soremekun"], "title": "Directed Grammar-Based Test Generation", "categories": ["cs.SE", "68N99", "D.2.5"], "comment": "21 pages, 10 figures, 13 tables, submitted to IEEE Transactions on\n  Software Engineering, for replication package, see\n  https://tinyurl.com/FDLoop-V3", "summary": "To effectively test complex software, it is important to generate\ngoal-specific inputs, i.e., inputs that achieve a specific testing goal.\nHowever, most state-of-the-art test generators are not designed to target\nspecific goals. Notably, grammar-based test generators, which (randomly)\nproduce syntactically valid inputs via an input specification (i.e., grammar)\nhave a low probability of achieving an arbitrary testing goal. This work\naddresses this challenge by proposing an automated test generation approach\n(called FdLoop) which iteratively learns relevant input properties from\nexisting inputs to drive the generation of goal-specific inputs. Given a\ntesting goal, FdLoop iteratively selects, evolves and learn the input\ndistribution of goal-specific test inputs via test feedback and a probabilistic\ngrammar. We concretize FdLoop for four testing goals, namely unique code\ncoverage, input-to-code complexity, program failures (exceptions) and long\nexecution time. We evaluate FdLoop using three (3) well-known input formats\n(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,\nFdLoop outperforms all five tested baselines namely the baseline grammar-based\ntest generators (random, probabilistic and inverse-probabilistic methods),\nEvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best\nbaseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that\nthe main components of FdLoop (i.e., input mutator, grammar mutator and test\nfeedbacks) contribute positively to its effectiveness. Finally, our evaluation\ndemonstrates that FdLoop effectively achieves single testing goals (revealing\nerroneous behaviors, generating complex inputs, or inducing long execution\ntime) and scales to multiple testing goals across varying parameter settings.", "AI": {"tldr": "FdLoop\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u8f93\u5165\u5c5e\u6027\u6765\u751f\u6210\u76ee\u6807\u7279\u5b9a\u7684\u6d4b\u8bd5\u8f93\u5165\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u6cd5\u7684\u6d4b\u8bd5\u751f\u6210\u5668\u96be\u4ee5\u9488\u5bf9\u7279\u5b9a\u6d4b\u8bd5\u76ee\u6807\u751f\u6210\u6709\u6548\u8f93\u5165\uff0cFdLoop\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "FdLoop\u901a\u8fc7\u6d4b\u8bd5\u53cd\u9988\u548c\u6982\u7387\u8bed\u6cd5\uff0c\u8fed\u4ee3\u9009\u62e9\u3001\u6f14\u5316\u5e76\u5b66\u4e60\u76ee\u6807\u7279\u5b9a\u6d4b\u8bd5\u8f93\u5165\u7684\u5206\u5e03\u3002", "result": "\u572886%\u7684\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0cFdLoop\u4f18\u4e8e\u4e94\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u5f15\u53d1\u9519\u8bef\u884c\u4e3a\u65b9\u9762\u6548\u679c\u63d0\u5347\u4e24\u500d\u3002", "conclusion": "FdLoop\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5355\u4e00\u548c\u591a\u91cd\u6d4b\u8bd5\u76ee\u6807\uff0c\u5e76\u5728\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.01635", "pdf": "https://arxiv.org/pdf/2508.01635", "abs": "https://arxiv.org/abs/2508.01635", "authors": ["Wenzhuo Qian", "Hailiang Zhao", "Tianlv Chen", "Jiayi Chen", "Ziqi Wang", "Kingsum Chow", "Shuiguang Deng"], "title": "Learning Unified System Representations for Microservice Tail Latency Prediction", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "comment": null, "summary": "Microservice architectures have become the de facto standard for building\nscalable cloud-native applications, yet their distributed nature introduces\nsignificant challenges in performance monitoring and resource management.\nTraditional approaches often rely on per-request latency metrics, which are\nhighly sensitive to transient noise and fail to reflect the holistic behavior\nof complex, concurrent workloads. In contrast, window-level P95 tail latency\nprovides a stable and meaningful signal that captures both system-wide trends\nand user-perceived performance degradation. We identify two key shortcomings in\nexisting methods: (i) inadequate handling of heterogeneous data, where\ntraffic-side features propagate across service dependencies and resource-side\nsignals reflect localized bottlenecks, and (ii) the lack of principled\narchitectural designs that effectively distinguish and integrate these\ncomplementary modalities. To address these challenges, we propose USRFNet, a\ndeep learning network that explicitly separates and models traffic-side and\nresource-side features. USRFNet employs GNNs to capture service interactions\nand request propagation patterns, while gMLP modules independently model\ncluster resource dynamics. These representations are then fused into a unified\nsystem embedding to predict window-level P95 latency with high accuracy. We\nevaluate USRFNet on real-world microservice benchmarks under large-scale stress\ntesting conditions, demonstrating substantial improvements in prediction\naccuracy over state-of-the-art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUSRFNet\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u6027\u80fd\u76d1\u63a7\u548c\u8d44\u6e90\u7ba1\u7406\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u5efa\u6a21\u6d41\u91cf\u4fa7\u4e0e\u8d44\u6e90\u4fa7\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c3e\u5ef6\u8fdf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u56e0\u5176\u5206\u5e03\u5f0f\u7279\u6027\u5728\u6027\u80fd\u76d1\u63a7\u548c\u8d44\u6e90\u7ba1\u7406\u4e0a\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u8bf7\u6c42\u7ea7\u5ef6\u8fdf\u6307\u6807\u5b58\u5728\u566a\u58f0\u654f\u611f\u548c\u65e0\u6cd5\u53cd\u6620\u5168\u5c40\u884c\u4e3a\u7684\u5c40\u9650\u3002", "method": "USRFNet\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6355\u6349\u670d\u52a1\u4ea4\u4e92\u548c\u8bf7\u6c42\u4f20\u64ad\u6a21\u5f0f\uff0c\u540c\u65f6\u901a\u8fc7gMLP\u6a21\u5757\u72ec\u7acb\u5efa\u6a21\u96c6\u7fa4\u8d44\u6e90\u52a8\u6001\uff0c\u6700\u540e\u5c06\u4e24\u8005\u878d\u5408\u4e3a\u7edf\u4e00\u7cfb\u7edf\u5d4c\u5165\u4ee5\u9884\u6d4b\u7a97\u53e3\u7ea7P95\u5c3e\u5ef6\u8fdf\u3002", "result": "\u5728\u5927\u89c4\u6a21\u538b\u529b\u6d4b\u8bd5\u7684\u5b9e\u9645\u5fae\u670d\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUSRFNet\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "USRFNet\u901a\u8fc7\u6709\u6548\u5206\u79bb\u548c\u6574\u5408\u6d41\u91cf\u4fa7\u4e0e\u8d44\u6e90\u4fa7\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u9ad8\u7cbe\u5ea6\u7684\u5fae\u670d\u52a1\u6027\u80fd\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01974", "pdf": "https://arxiv.org/pdf/2508.01974", "abs": "https://arxiv.org/abs/2508.01974", "authors": ["Jiahao Zhang", "Xiao Cheng", "Yuxiang Lei"], "title": "Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Flow-sensitive pointer analysis constitutes an essential component of precise\nprogram analysis for accurately modeling pointer behaviors by incorporating\ncontrol flows. Flow-sensitive pointer analysis is extensively used in alias\nanalysis, taint analysis, program understanding, compiler optimization, etc.\nExisting flow-sensitive pointer analysis approaches, which are conducted based\non control flow graphs, have significantly advanced the precision of pointer\nanalysis via sophisticated techniques to leverage control flow information.\nHowever, they inevitably suffer from computational inefficiencies when\nresolving points-to information due to the inherent complex structures of\ncontrol flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph\n(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of\ncontrol-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to\nleverage the structural advantages of set-constraint graphs (which are commonly\nused in flow-insensitive pointer analysis) while keeping the flow sensitivity\nof variable definitions and uses, allowing the incorporation of sophisticated\ngraph optimization and dynamic solving techniques. In this way, CG-FSPTA\nachieves significant efficiency improvements while keeping the precision of\nflow-sensitive analysis. Experimental evaluations on benchmark programs\ndemonstrate that CG-FSPTA, significantly reduces both memory usage and\nexecution time while maintaining precision. In particular, by solving in the\nFSConsG, CG-FSPTA achieves an average memory reduction of 33.05\\% and\naccelerates flow-sensitive pointer analysis by 7.27x compared to the\nstate-of-art method. These experimental results underscore the efficacy of\nCG-FSPTA as a scalable solution to analyze large-scale software systems,\nestablishing a robust foundation for future advancements in efficient program\nanalysis frameworks.", "AI": {"tldr": "CG-FSPTA\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u654f\u611f\u7ea6\u675f\u56fe\u7684\u6307\u9488\u5206\u6790\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63a7\u5236\u6d41\u56fe\u7684\u6d41\u654f\u611f\u6307\u9488\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u6307\u9488\u4fe1\u606f\u65f6\u6548\u7387\u4f4e\u4e0b\uff0cCG-FSPTA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CG-FSPTA\u5229\u7528\u6d41\u654f\u611f\u53d8\u4f53\u7684\u96c6\u5408\u7ea6\u675f\u56fe\u7ed3\u6784\u4f18\u52bf\uff0c\u7ed3\u5408\u56fe\u4f18\u5316\u548c\u52a8\u6001\u6c42\u89e3\u6280\u672f\uff0c\u4fdd\u6301\u6d41\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCG-FSPTA\u5e73\u5747\u51cf\u5c1133.05%\u5185\u5b58\u4f7f\u7528\uff0c\u52a0\u901f7.27\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u6027\u3002", "conclusion": "CG-FSPTA\u4e3a\u9ad8\u6548\u5206\u6790\u5927\u89c4\u6a21\u8f6f\u4ef6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6848\uff0c\u5960\u5b9a\u4e86\u672a\u6765\u7a0b\u5e8f\u5206\u6790\u6846\u67b6\u7684\u57fa\u7840\u3002"}}
{"id": "2508.01205", "pdf": "https://arxiv.org/pdf/2508.01205", "abs": "https://arxiv.org/abs/2508.01205", "authors": ["Lei Teng", "Senran Fan", "Chen Dong", "Haotai Liang", "Zhicheng Bao", "Xiaodong Xu", "Rui Meng", "Ping Zhang"], "title": "Conquering High Packet-Loss Erasure: MoE Swin Transformer-Based Video Semantic Communication", "categories": ["cs.ET", "cs.AI", "cs.MM"], "comment": null, "summary": "Semantic communication with joint semantic-channel coding robustly transmits\ndiverse data modalities but faces challenges in mitigating semantic information\nloss due to packet drops in packet-based systems. Under current protocols,\npackets with errors are discarded, preventing the receiver from utilizing\nerroneous semantic data for robust decoding. To address this issue, a\npacket-loss-resistant MoE Swin Transformer-based Video Semantic Communication\n(MSTVSC) system is proposed in this paper. Semantic vectors are encoded by\nMSTVSC and transmitted through upper-layer protocol packetization. To\ninvestigate the impact of the packetization, a theoretical analysis of the\npacketization strategy is provided. To mitigate the semantic loss caused by\npacket loss, a 3D CNN at the receiver recovers missing information using\nun-lost semantic data and an packet-loss mask matrix. Semantic-level\ninterleaving is employed to reduce concentrated semantic loss from packet\ndrops. To improve compression, a common-individual decomposition approach is\nadopted, with downsampling applied to individual information to minimize\nredundancy. The model is lightweighted for practical deployment. Extensive\nsimulations and comparisons demonstrate strong performance, achieving an\nMS-SSIM greater than 0.6 and a PSNR exceeding 20 dB at a 90% packet loss rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMoE Swin Transformer\u7684\u89c6\u9891\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edfMSTVSC\uff0c\u901a\u8fc73D CNN\u6062\u590d\u4e22\u5931\u6570\u636e\uff0c\u51cf\u5c11\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\uff0c\u63d0\u5347\u4e86\u5728\u9ad8\u4e22\u5305\u7387\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5728\u57fa\u4e8e\u5206\u7ec4\u7684\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u5206\u7ec4\u4e22\u5f03\u5bfc\u81f4\u7684\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u6570\u636e\u4f20\u8f93\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528MSTVSC\u7f16\u7801\u8bed\u4e49\u5411\u91cf\uff0c\u5229\u75283D CNN\u6062\u590d\u4e22\u5931\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u7ea7\u4ea4\u9519\u548c\u5171\u540c-\u4e2a\u4f53\u5206\u89e3\u65b9\u6cd5\u4f18\u5316\u538b\u7f29\u548c\u8f7b\u91cf\u5316\u90e8\u7f72\u3002", "result": "\u572890%\u7684\u5206\u7ec4\u4e22\u5931\u7387\u4e0b\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86MS-SSIM\u5927\u4e8e0.6\u548cPSNR\u8d85\u8fc720 dB\u7684\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "MSTVSC\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5206\u7ec4\u4e22\u5931\u5bfc\u81f4\u7684\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5728\u9ad8\u4e22\u5305\u7387\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00848", "pdf": "https://arxiv.org/pdf/2508.00848", "abs": "https://arxiv.org/abs/2508.00848", "authors": ["Agniva Banerjee", "Bhanu Partap Paregi", "Haroon R. Lone"], "title": "RestAware: Non-Invasive Sleep Monitoring Using FMCW Radar and AI-Generated Summaries", "categories": ["cs.HC", "cs.CY", "eess.SP"], "comment": null, "summary": "Monitoring sleep posture and behavior is critical for diagnosing sleep\ndisorders and improving overall sleep quality. However, traditional approaches,\nsuch as wearable devices, cameras, and pressure sensors, often compromise user\ncomfort, fail under obstructions like blankets, and raise privacy concerns. To\novercome these limitations, we present RestAware, a non-invasive, contactless\nsleep monitoring system based on a 24GHz frequency-modulated continuous wave\n(FMCW) radar. Our system is evaluated on 25 participants across eight common\nsleep postures, achieving 92% classification accuracy and an F1-score of 0.91\nusing a K-Nearest Neighbors (KNN) classifier. In addition, we integrate\ninstruction-tuned large language models (Mistral, Llama, and Falcon) to\ngenerate personalized, human-readable sleep summaries from radar-derived\nposture data. This low-cost ($ 35), privacy-preserving solution offers a\npractical alternative for real-time deployment in smart homes and clinical\nenvironments.", "AI": {"tldr": "RestAware\u662f\u4e00\u4e2a\u57fa\u4e8e24GHz\u96f7\u8fbe\u7684\u975e\u63a5\u89e6\u5f0f\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8212\u9002\u6027\u548c\u9690\u79c1\u95ee\u9898\uff0c\u51c6\u786e\u7387\u8fbe92%\u3002", "motivation": "\u4f20\u7edf\u7761\u7720\u76d1\u6d4b\u65b9\u6cd5\uff08\u5982\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u6444\u50cf\u5934\uff09\u5b58\u5728\u8212\u9002\u6027\u5dee\u3001\u906e\u6321\u5931\u6548\u548c\u9690\u79c1\u95ee\u9898\uff0c\u9700\u8981\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u752824GHz FMCW\u96f7\u8fbe\u548cKNN\u5206\u7c7b\u5668\u76d1\u6d4b\u7761\u7720\u59ff\u52bf\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u7761\u7720\u62a5\u544a\u3002", "result": "\u7cfb\u7edf\u572825\u540d\u53c2\u4e0e\u8005\u4e2d\u5b9e\u73b092%\u5206\u7c7b\u51c6\u786e\u7387\u548c0.91\u7684F1\u5206\u6570\uff0c\u6210\u672c\u4f4e\u81f335\u7f8e\u5143\u3002", "conclusion": "RestAware\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u9690\u79c1\u53cb\u597d\u7684\u975e\u63a5\u89e6\u5f0f\u7761\u7720\u76d1\u6d4b\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5bb6\u5ead\u548c\u4e34\u5e8a\u73af\u5883\u3002"}}
{"id": "2508.01590", "pdf": "https://arxiv.org/pdf/2508.01590", "abs": "https://arxiv.org/abs/2508.01590", "authors": ["Hua Yu", "Jiao Liu", "Xu Gui", "Melvin Wong", "Yaqing Hou", "Yew-Soon Ong"], "title": "A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In-betweening human motion generation aims to synthesize intermediate motions\nthat transition between user-specified keyframes. In addition to maintaining\nsmooth transitions, a crucial requirement of this task is to generate diverse\nmotion sequences. It is still challenging to maintain diversity, particularly\nwhen it is necessary for the motions within a generated batch sampling to\ndiffer meaningfully from one another due to complex motion dynamics. In this\npaper, we propose a novel method, termed the Multi-Criteria Guidance with\nIn-Betweening Motion Model (MCG-IMM), for in-betweening human motion\ngeneration. A key strength of MCG-IMM lies in its plug-and-play nature: it\nenhances the diversity of motions generated by pretrained models without\nintroducing additional parameters This is achieved by providing a sampling\nprocess of pretrained generative models with multi-criteria guidance.\nSpecifically, MCG-IMM reformulates the sampling process of pretrained\ngenerative model as a multi-criteria optimization problem, and introduces an\noptimization process to explore motion sequences that satisfy multiple\ncriteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play\nmulti-criteria guidance is compatible with different families of generative\nmodels, including denoised diffusion probabilistic models, variational\nautoencoders, and generative adversarial networks. Experiments on four popular\nhuman motion datasets demonstrate that MCG-IMM consistently state-of-the-art\nmethods in in-betweening motion generation task.", "AI": {"tldr": "MCG-IMM\u662f\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u4eba\u7c7b\u52a8\u4f5c\u8fc7\u6e21\u5e8f\u5217\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u51c6\u5219\u4f18\u5316\u63d0\u9ad8\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u6837\u6027\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u52a8\u4f5c\u8fc7\u6e21\u751f\u6210\u4e2d\u591a\u6837\u6027\u548c\u5e73\u6ed1\u6027\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6279\u91cf\u751f\u6210\u65f6\u52a8\u4f5c\u95f4\u7684\u5dee\u5f02\u6027\u3002", "method": "\u5229\u7528\u591a\u51c6\u5219\u4f18\u5316\u5c06\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u91cd\u65b0\u8868\u8ff0\uff0c\u63a2\u7d22\u6ee1\u8db3\u591a\u6837\u6027\u548c\u5e73\u6ed1\u6027\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cMCG-IMM\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MCG-IMM\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u4e0e\u591a\u79cd\u751f\u6210\u6a21\u578b\u517c\u5bb9\uff0c\u6709\u6548\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.01866", "pdf": "https://arxiv.org/pdf/2508.01866", "abs": "https://arxiv.org/abs/2508.01866", "authors": ["Berend van Starkenburg", "Henning Basold", "Chase Ford"], "title": "Separation Logic of Generic Resources via Sheafeology", "categories": ["cs.LO", "D.3.1; F.3.1; F.3.2; F.3.3; F.4.1"], "comment": "55 pages including appendix", "summary": "Separation logic was conceived in order to make the verification of pointer\nprograms scalable to large systems and it has proven extremely effective. The\nkey idea is that programs typically access only small parts of memory, allowing\nfor local reasoning. This idea is implemented in separation logic by extending\nfirst-order logic with separating connectives, which inspect local regions of\nmemory. It turns that this approach not only applies to pointer programs, but\nalso to programs involving other resource structures. Various theories have\nbeen put forward to extract and apply the ideas of separation logic more\nbroadly. This resulted in algebraic abstractions of memory and many variants of\nseparation logic for, e.g., concurrent programs and stochastic processes.\nHowever, none of the existing approaches formulate the combination of\nfirst-order logic with separating connectives in a theory that could\nimmediately yield program logics for different resources. In this paper, we\npropose a framework based on the idea that separation logic can obtained by\nmaking first-order logic resource-aware. First-order logic can be understood in\nterms of categorical logic, specifically fibrations. Our contribution is to\nmake these resource-aware by developing categorical logic internally in\ncategories of sheaves, which is what we call sheafeology. The role of sheaves\nis to model views on resources, through which resources can be localised and\ncombined, which enables the scalability promised by separation logic. We\ncontribute constructions of an internal fibration in sheaf categories that\nmodels predicates on resources, and that admits first-order and separating\nconnectives. Thereby, we attain a general framework of separation logic for\ngeneric resources, a claim we substantiate by instantiating our framework to\nvarious memory models and random variables.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d44\u6e90\u611f\u77e5\u7684\u5206\u79bb\u903b\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e00\u9636\u903b\u8f91\u4e0e\u5206\u7c7b\u903b\u8f91\u7ed3\u5408\uff0c\u5229\u7528\u6a21\u578b\u8bba\u4e2d\u7684\u7ea4\u7ef4\u5316\u548csheaf\u7406\u8bba\uff0c\u5b9e\u73b0\u4e86\u9002\u7528\u4e8e\u591a\u79cd\u8d44\u6e90\u7684\u901a\u7528\u5206\u79bb\u903b\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u79bb\u903b\u8f91\u65b9\u6cd5\u867d\u7136\u5728\u6307\u9488\u7a0b\u5e8f\u9a8c\u8bc1\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4f46\u7f3a\u4e4f\u4e00\u79cd\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u8d44\u6e90\uff08\u5982\u5e76\u53d1\u7a0b\u5e8f\u3001\u968f\u673a\u53d8\u91cf\u7b49\uff09\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u903b\u8f91\u548csheaf\u7406\u8bba\uff0c\u5c06\u8d44\u6e90\u611f\u77e5\u5f15\u5165\u4e00\u9636\u903b\u8f91\uff0c\u6784\u5efa\u5185\u90e8\u7ea4\u7ef4\u5316\u6a21\u578b\uff0c\u652f\u6301\u8c13\u8bcd\u548c\u5206\u79bb\u8fde\u63a5\u8bcd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u5206\u79bb\u903b\u8f91\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5316\u4e3a\u4e0d\u540c\u5185\u5b58\u6a21\u578b\u548c\u968f\u673a\u53d8\u91cf\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7sheafeology\u65b9\u6cd5\uff0c\u672c\u6587\u4e3a\u5206\u79bb\u903b\u8f91\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8d44\u6e90\u7c7b\u578b\u3002"}}
{"id": "2508.02280", "pdf": "https://arxiv.org/pdf/2508.02280", "abs": "https://arxiv.org/abs/2508.02280", "authors": ["Francesco Gargiulo", "Rossano Venturini"], "title": "OnPair: Short Strings Compression for Fast Random Access", "categories": ["cs.DB", "H.2.4; E.4; H.3.2"], "comment": null, "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aOnPair\u7684\u5b57\u5178\u538b\u7f29\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u538b\u7f29\u6bd4\u548c\u5feb\u901f\u968f\u673a\u8bbf\u95ee\u7684\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\uff0c\u5e73\u8861\u4e86\u538b\u7f29\u6bd4\u4e0e\u901f\u5ea6\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u5728\u538b\u7f29\u6bd4\u548c\u901f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0cOnPair\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u5b57\u5178\u6784\u5efa\u6280\u672f\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u7f13\u5b58\u53cb\u597d\u7684\u5b57\u5178\u6784\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u5355\u6b21\u987a\u5e8f\u626b\u63cf\u6570\u636e\u6837\u672c\u9010\u6b65\u5408\u5e76\u9ad8\u9891\u76f8\u90bb\u5b50\u4e32\uff0c\u5e76\u63a8\u51fa\u4e86OnPair16\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOnPair\u53ca\u5176\u53d8\u4f53\u5728\u538b\u7f29\u6bd4\u4e0eBPE\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u3002", "conclusion": "OnPair\u6210\u529f\u5e73\u8861\u4e86\u538b\u7f29\u6bd4\u4e0e\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u3002"}}
{"id": "2508.01056", "pdf": "https://arxiv.org/pdf/2508.01056", "abs": "https://arxiv.org/abs/2508.01056", "authors": ["Sebastian Elbaum", "Jonathan Panther"], "title": "Managing Escalation in Off-the-Shelf Large Language Models", "categories": ["cs.ET", "cs.AI"], "comment": null, "summary": "U.S. national security customers have begun to utilize large language models,\nincluding enterprise versions of ``off-the-shelf'' models (e.g., ChatGPT)\nfamiliar to the public. This uptake will likely accelerate. However, recent\nstudies suggest that off-the-shelf large language models frequently suggest\nescalatory actions when prompted with geopolitical or strategic scenarios. We\ndemonstrate two simple, non-technical interventions to control these\ntendencies. Introducing these interventions into the experimental wargame\ndesign of a recent study, we substantially reduce escalation throughout the\ngame. Calls to restrict the use of large language models in national security\napplications are thus premature. The U.S. government is already, and will\ncontinue, employing large language models for scenario planning and suggesting\ncourses of action. Rather than warning against such applications, this study\nacknowledges the imminent adoption of large language models, and provides\nactionable measures to align them with national security goals, including\nescalation management.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fd\u5bb6\u5b89\u5168\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u975e\u6280\u672f\u6027\u5e72\u9884\u63aa\u65bd\u4ee5\u51cf\u5c11\u5176\u5728\u5730\u7f18\u653f\u6cbb\u573a\u666f\u4e2d\u7684\u5347\u7ea7\u503e\u5411\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fd\u5bb6\u5b89\u5168\u5e94\u7528\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u5347\u7ea7\u884c\u4e3a\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u4e0d\u9650\u5236\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\u5bf9\u9f50\u5176\u4e0e\u56fd\u5bb6\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b80\u5355\u3001\u975e\u6280\u672f\u6027\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u5e76\u5728\u4e00\u9879\u5b9e\u9a8c\u6027\u6218\u4e89\u6e38\u620f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u663e\u8457\u51cf\u5c11\u4e86\u6e38\u620f\u4e2d\u7684\u5347\u7ea7\u884c\u4e3a\u3002", "conclusion": "\u5e94\u901a\u8fc7\u5b9e\u9645\u63aa\u65bd\u800c\u975e\u9650\u5236\u4f7f\u7528\u6765\u7ba1\u7406\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fd\u5bb6\u5b89\u5168\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u4fc3\u8fdb\u5176\u4e0e\u56fd\u5bb6\u76ee\u6807\u7684\u5bf9\u9f50\u3002"}}
{"id": "2508.02007", "pdf": "https://arxiv.org/pdf/2508.02007", "abs": "https://arxiv.org/abs/2508.02007", "authors": ["Konstantinos Kanellopoulos", "Konstantinos Sgouras", "Andreas Kosmas Kakolyris", "Vlad-Petru Nitu", "Berkin Kerim Konar", "Rahul Bera", "Onur Mutlu"], "title": "Revelator: Rapid Data Fetching via OS-Driven Hash-based Speculative Address Translation", "categories": ["cs.AR", "cs.OS", "B.3; D.4"], "comment": null, "summary": "Address translation is a major performance bottleneck in modern computing\nsystems. Speculative address translation can hide this latency by predicting\nthe physical address (PA) of requested data early in the pipeline. However,\npredicting the PA from the virtual address (VA) is difficult due to the\nunpredictability of VA-to-PA mappings in conventional OSes. Prior works try to\novercome this but face two key issues: (i) reliance on large pages or VA-to-PA\ncontiguity, which is not guaranteed, and (ii) costly hardware changes to store\nspeculation metadata with limited effectiveness.\n  We introduce Revelator, a hardware-OS cooperative scheme enabling highly\naccurate speculative address translation with minimal modifications. Revelator\nemploys a tiered hash-based allocation strategy in the OS to create predictable\nVA-to-PA mappings, falling back to conventional allocation when needed. On a\nTLB miss, a lightweight speculation engine, guided by this policy, generates\ncandidate PAs for both program data and last-level page table entries (PTEs).\nThus, Revelator (i) speculatively fetches requested data before translation\nresolves, reducing access latency, and (ii) fetches the fourth-level PTE before\nthe third-level PTE is accessed, accelerating page table walks.\n  We prototype Revelator's OS support in Linux and evaluate it in simulation\nacross 11 diverse, data-intensive benchmarks in native and virtualized\nenvironments. Revelator achieves average speedups of 27% (20%) in native\n(virtualized) settings, surpasses a state-of-the-art speculative mechanism by\n5%, and reduces energy use by 9% compared to baseline. Our RTL prototype shows\nminimal area and power overheads on a modern CPU.", "AI": {"tldr": "Revelator\u662f\u4e00\u79cd\u786c\u4ef6-\u64cd\u4f5c\u7cfb\u7edf\u534f\u4f5c\u65b9\u6848\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u5730\u5740\u7ffb\u8bd1\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747\u52a0\u901f27%\uff0820%\uff09\uff0c\u51cf\u5c11\u80fd\u80179%\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5730\u5740\u7ffb\u8bd1\u662f\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6848\u4f9d\u8d56\u5927\u9875\u6216\u786c\u4ef6\u4fee\u6539\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u5206\u5c42\u54c8\u5e0c\u5206\u914d\u7b56\u7565\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u63a8\u6d4b\u5f15\u64ce\uff0c\u63d0\u524d\u83b7\u53d6\u6570\u636e\u548c\u9875\u8868\u9879\u3002", "result": "\u572811\u4e2a\u6570\u636e\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u52a0\u901f27%\uff0820%\uff09\uff0c\u80fd\u8017\u964d\u4f4e9%\u3002", "conclusion": "Revelator\u4ee5\u6700\u5c0f\u786c\u4ef6\u5f00\u9500\u5b9e\u73b0\u9ad8\u6548\u5730\u5740\u7ffb\u8bd1\uff0c\u9002\u7528\u4e8e\u539f\u751f\u548c\u865a\u62df\u5316\u73af\u5883\u3002"}}
{"id": "2508.01856", "pdf": "https://arxiv.org/pdf/2508.01856", "abs": "https://arxiv.org/abs/2508.01856", "authors": ["Xu Yuan", "Fang Luo", "Muhammad Zeeshan Haider", "Zhikui Chen", "Yucheng Li"], "title": "Efficient Byzantine Consensus MechanismBased on Reputation in IoT Blockchain", "categories": ["cs.DC", "cs.CR", "cs.DB", "cs.SE"], "comment": null, "summary": "Blockchain technology has advanced rapidly in recent years and is now widely\nused in a variety of fields. Blockchain appears to be one of the best solutions\nfor managing massive heterogeneous devices while achieving advanced data\nsecurity and data reputation, particularly in the field of large-scale IoT\n(Internet of Things) networks. Despite the numerous advantages, there are still\nchallenges while deploying IoT applications on blockchain systems due to the\nlimited storage, power, and computing capability of IoT devices, and some of\nthese problems are caused by the consensus algorithm, which plays a significant\nrole in blockchain systems by ensuring overall system reliability and\nrobustness. Nonetheless, most existing consensus algorithms are prone to poor\nnode reliability, low transaction per second (TPS) rates, and scalability\nissues. Aiming at some critical problems in the existing consensus algorithms,\nthis paper proposes the Efficient Byzantine Reputation-based Consensus (EBRC)\nmechanism to resolve the issues raised above. In comparison to traditional\nalgorithms, we reinvented ways to evaluate node reliability and robustness and\nmanage active nodes. Our experiments show that the EBRC algorithm has lower\nconsensus delay, higher throughput, improved security, and lower verification\ncosts. It offers new reference ideas for solving the Internet of\nThings+blockchain+Internet court construction problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u62dc\u5360\u5ead\u4fe1\u8a89\u5171\u8bc6\u673a\u5236\uff08EBRC\uff09\uff0c\u4ee5\u89e3\u51b3\u533a\u5757\u94fe\u5728\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u5b58\u50a8\u3001\u529f\u7387\u548c\u8ba1\u7b97\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "motivation": "\u533a\u5757\u94fe\u6280\u672f\u5728\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5171\u8bc6\u7b97\u6cd5\u5b58\u5728\u8282\u70b9\u53ef\u9760\u6027\u4f4e\u3001\u541e\u5410\u91cf\u4e0d\u8db3\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faEBRC\u673a\u5236\uff0c\u91cd\u65b0\u8bbe\u8ba1\u8282\u70b9\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4f18\u5316\u6d3b\u52a8\u8282\u70b9\u7ba1\u7406\u3002", "result": "EBRC\u5728\u5171\u8bc6\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u5b89\u5168\u6027\u548c\u9a8c\u8bc1\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "conclusion": "EBRC\u4e3a\u7269\u8054\u7f51+\u533a\u5757\u94fe+\u4e92\u8054\u7f51\u6cd5\u9662\u6784\u5efa\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u53c2\u8003\u3002"}}
{"id": "2508.01060", "pdf": "https://arxiv.org/pdf/2508.01060", "abs": "https://arxiv.org/abs/2508.01060", "authors": ["Ibrahim Althamary", "Chen-Fu Chou", "Chih-Wei Huang"], "title": "Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Managing connectivity in integrated satellite-terrestrial vehicular networks\nis critical for 6G, yet is challenged by dynamic conditions and partial\nobservability. This letter introduces the Multi-Agent Actor-Critic with\nSatellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent\nreinforcement learning framework that enables vehicles to autonomously manage\nconnectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure\n(V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the\nintegration of a multi-head attention mechanism, which allows for robust state\nestimation even with fluctuating and limited information sharing among\nvehicles. The framework further leverages self-imitation learning (SIL) and\nfingerprinting to improve learning efficiency and real-time decisions.\nSimulation results, based on realistic SUMO traffic models and 3GPP-compliant\nconfigurations, demonstrate that MAAC-SAM outperforms state-of-the-art\nterrestrial and satellite-assisted baselines by up to 14% in transmission\nutility and maintains high estimation accuracy across varying vehicle densities\nand sharing levels.", "AI": {"tldr": "\u4e3a\u89e3\u51b36G\u7f51\u7edc\u4e2d\u536b\u661f-\u5730\u9762\u8f66\u8054\u7f51\u7684\u52a8\u6001\u6027\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6MAAC-SAM\uff0c\u7ed3\u5408\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u6a21\u4eff\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u8f93\u6548\u7528\u548c\u72b6\u6001\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u536b\u661f-\u5730\u9762\u8f66\u8054\u7f51\u7684\u52a8\u6001\u6027\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4f7f\u5f97\u8fde\u63a5\u7ba1\u7406\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u4e3b\u51b3\u7b56\u7684\u65b0\u578b\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86MAAC-SAM\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08SAM\uff09\u3001\u81ea\u6a21\u4eff\u5b66\u4e60\uff08SIL\uff09\u548c\u6307\u7eb9\u6280\u672f\uff0c\u4ee5\u4f18\u5316\u8f66\u8f86\u5728\u591a\u8fde\u63a5\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u3002", "result": "\u57fa\u4e8eSUMO\u548c3GPP\u7684\u4eff\u771f\u663e\u793a\uff0cMAAC-SAM\u5728\u4f20\u8f93\u6548\u7528\u4e0a\u6bd4\u73b0\u6709\u57fa\u7ebf\u9ad814%\uff0c\u5e76\u5728\u4e0d\u540c\u8f66\u8f86\u5bc6\u5ea6\u548c\u4fe1\u606f\u5171\u4eab\u6c34\u5e73\u4e0b\u4fdd\u6301\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "MAAC-SAM\u4e3a\u536b\u661f-\u5730\u9762\u8f66\u8054\u7f51\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fde\u63a5\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.01489", "pdf": "https://arxiv.org/pdf/2508.01489", "abs": "https://arxiv.org/abs/2508.01489", "authors": ["SK. Golam Saroar", "Waseefa Ahmed", "Elmira Onagh", "Maleknaz Nayebi"], "title": "GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development", "categories": ["cs.SE"], "comment": "SANER 2025 journal first paper", "summary": "GitHub, a central hub for collaborative software development, has\nrevolutionized the open-source software (OSS) ecosystem through its GitHub\nMarketplace, a platform launched in 2017 to host automation tools aimed at\nenhancing the efficiency and scalability of software projects. As the adoption\nof automation in OSS production grows, understanding the trends,\ncharacteristics, and underlying dynamics of this marketplace has become vital.\nFurthermore, despite the rich repository of academic research on software\nautomation, a disconnect persists between academia and industry practices. This\nstudy seeks to bridge this gap by providing a systematic analysis of the GitHub\nMarketplace, comparing trends observed in industry tools with advancements\nreported in academic literature, and identifying areas where academia can\ncontribute to practical innovation.", "AI": {"tldr": "GitHub Marketplace\uff082017\u5e74\u63a8\u51fa\uff09\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u4e86OSS\u9879\u76ee\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u7814\u7a76\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u8be5\u5e73\u53f0\uff0c\u6bd4\u8f83\u5de5\u4e1a\u5de5\u5177\u4e0e\u5b66\u672f\u6587\u732e\u7684\u8fdb\u5c55\uff0c\u65e8\u5728\u5f25\u5408\u5b66\u672f\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "GitHub Marketplace\u7684\u81ea\u52a8\u5316\u5de5\u5177\u5728OSS\u751f\u4ea7\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5176\u8d8b\u52bf\u3001\u7279\u5f81\u53ca\u52a8\u6001\uff0c\u5e76\u5f25\u5408\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u7684\u8131\u8282\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u5206\u6790\u65b9\u6cd5\uff0c\u6bd4\u8f83GitHub Marketplace\u7684\u5de5\u4e1a\u5de5\u5177\u8d8b\u52bf\u4e0e\u5b66\u672f\u6587\u732e\u7684\u8fdb\u5c55\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5b66\u672f\u754c\u53ef\u4ee5\u4e3a\u5de5\u4e1a\u5b9e\u8df5\u63d0\u4f9b\u521b\u65b0\u8d21\u732e\u7684\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u7684\u534f\u540c\u521b\u65b0\u63d0\u4f9b\u4e86\u6865\u6881\uff0c\u52a9\u529bGitHub Marketplace\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.01226", "pdf": "https://arxiv.org/pdf/2508.01226", "abs": "https://arxiv.org/abs/2508.01226", "authors": ["Xin Zhou", "Yongjie Wang", "Zhiqi Shen"], "title": "CM$^3$: Calibrating Multimodal Recommendation", "categories": ["cs.IR", "cs.MM"], "comment": "Working Paper: https://github.com/enoche/CM3", "summary": "Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5bf9\u9f50\u548c\u5747\u5300\u6027\u539f\u5219\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u76f8\u4f3c\u6027\u7684\u6821\u51c6\u5747\u5300\u6027\u635f\u5931\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7403\u5f62B\u00e9zier\u65b9\u6cd5\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u8fc7\u5ea6\u5f3a\u8c03\u5747\u5300\u6027\u800c\u5ffd\u7565\u4e86\u5bf9\u9f50\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6821\u51c6\u5747\u5300\u6027\u635f\u5931\u548c\u6539\u8fdb\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u7684\u5185\u5728\u76f8\u4f3c\u6027\u6821\u51c6\u5747\u5300\u6027\u5206\u5e03\uff0c\u5f15\u5165\u7403\u5f62B\u00e9zier\u65b9\u6cd5\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff0c\u786e\u4fdd\u7279\u5f81\u4f4d\u4e8e\u540c\u4e00\u8d85\u7403\u9762\u6d41\u5f62\u4e0a\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0cNDCG@20\u6027\u80fd\u6700\u9ad8\u63d0\u53475.4%\u3002", "conclusion": "\u6821\u51c6\u5747\u5300\u6027\u635f\u5931\u548c\u6539\u8fdb\u7684\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2508.00850", "pdf": "https://arxiv.org/pdf/2508.00850", "abs": "https://arxiv.org/abs/2508.00850", "authors": ["Nadja R. Ging-Jehli", "Russell K. Childers", "Joshua Lu", "Robert Gemma", "Rachel Zhu"], "title": "Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "How do we learn when to persist, when to let go, and when to shift gears?\nGearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed\nto model how humans and artificial agents adapt to shifting environment\ndemands. Grounded in cognitive neuroscience, computational psychiatry,\neconomics, and artificial intelligence, Supertasks combine computational\nneurocognitive modeling with serious gaming. This creates a dynamic,\nmulti-mission environment engineered to assess mechanisms of adaptive behavior\nacross cognitive and social contexts. Computational parameters explain behavior\nand probe mechanisms by controlling the game environment. Unlike traditional\ntasks, GF enables neurocognitive modeling of individual differences across\nperceptual decisions, learning, and meta-cognitive levels. This positions GF as\na flexible testbed for understanding how cognitive-affective control processes,\nlearning styles, strategy use, and motivational shifts adapt across contexts\nand over time. It serves as an experimental platform for scientists, a\nphenotype-to-mechanism intervention for clinicians, and a training tool for\nplayers aiming to strengthen self-regulated learning, mood, and stress\nresilience. Online study (n = 60, ongoing) results show that GF recovers\neffects from traditional neuropsychological tasks (construct validity),\nuncovers novel patterns in how learning differs across contexts and how\nclinical features map onto distinct adaptations. These findings pave the way\nfor developing in-game interventions that foster self-efficacy and agency to\ncope with real-world stress and uncertainty. GF builds a new adaptive ecosystem\ndesigned to accelerate science, transform clinical care, and foster individual\ngrowth. It offers a mirror and training ground where humans and machines\nco-develop together deeper flexibility and awareness.", "AI": {"tldr": "Gearshift Fellowship (GF) \u662f\u4e00\u4e2a\u65b0\u7684 Supertask \u8303\u5f0f\u539f\u578b\uff0c\u7528\u4e8e\u7814\u7a76\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5982\u4f55\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002\u5b83\u7ed3\u5408\u4e86\u8ba1\u7b97\u795e\u7ecf\u8ba4\u77e5\u5efa\u6a21\u548c\u4e25\u8083\u6e38\u620f\uff0c\u65e8\u5728\u8bc4\u4f30\u8de8\u8ba4\u77e5\u548c\u793e\u4f1a\u73af\u5883\u7684\u9002\u5e94\u6027\u884c\u4e3a\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5982\u4f55\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7075\u6d3b\u8c03\u6574\u884c\u4e3a\uff0c\u4ee5\u5e94\u5bf9\u53d8\u5316\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u548c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8ba1\u7b97\u795e\u7ecf\u5efa\u6a21\u548c\u4e25\u8083\u6e38\u620f\uff0cGF \u521b\u5efa\u4e86\u4e00\u4e2a\u52a8\u6001\u591a\u4efb\u52a1\u73af\u5883\uff0c\u7528\u4e8e\u91cf\u5316\u4e2a\u4f53\u5728\u611f\u77e5\u51b3\u7b56\u3001\u5b66\u4e60\u548c\u5143\u8ba4\u77e5\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u5728\u7ebf\u7814\u7a76\uff08n = 60\uff09\u8868\u660e GF \u80fd\u591f\u9a8c\u8bc1\u4f20\u7edf\u795e\u7ecf\u5fc3\u7406\u5b66\u4efb\u52a1\u7684\u6548\u5e94\uff0c\u5e76\u63ed\u793a\u4e0d\u540c\u5b66\u4e60\u73af\u5883\u548c\u4e34\u5e8a\u7279\u5f81\u7684\u65b0\u6a21\u5f0f\u3002", "conclusion": "GF \u662f\u4e00\u4e2a\u591a\u529f\u80fd\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u79d1\u5b66\u7814\u7a76\u3001\u4e34\u5e8a\u5e72\u9884\u548c\u4e2a\u4f53\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u7c7b\u548c\u673a\u5668\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.02443", "pdf": "https://arxiv.org/pdf/2508.02443", "abs": "https://arxiv.org/abs/2508.02443", "authors": ["Thomas Gottwald", "Edgar Heinert", "Matthias Rottmann"], "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u65af\u6cfc\u6e85\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u7684\u6295\u5f71\u83b7\u5f97\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u5e76\u5728\u65b0\u89c6\u56fe\u4e2d\u901a\u8fc7\u6e32\u67d3\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u5728\u673a\u5668\u4eba\u548c\u533b\u5b66\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f30\u8ba1\u9ad8\u65af\u57fa\u5143\u7684\u65b9\u5dee\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u6295\u5f71\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u5230\u57fa\u5143\u4e0a\uff0c\u5efa\u7acb\u57fa\u5143\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u793a\uff0c\u6e32\u67d3\u65b0\u89c6\u56fe\u7684\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u7ea7\u56de\u5f52\u805a\u5408\u8fd9\u4e9b\u7279\u5f81\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0e\u771f\u5b9e\u8bef\u5dee\u9ad8\u5ea6\u76f8\u5173\uff0c\u5c24\u5176\u5728\u524d\u666f\u5bf9\u8c61\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u56de\u5f52\u6a21\u578b\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u5728\u65b0\u573a\u666f\u4e2d\u65e0\u9700\u4fdd\u7559\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u6539\u8fdb\u4e86\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u5173\u952e\u5e94\u7528\u573a\u666f\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65b0\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.02301", "pdf": "https://arxiv.org/pdf/2508.02301", "abs": "https://arxiv.org/abs/2508.02301", "authors": ["Marek Chalupa", "Thomas A. Henzinger", "Ana Oliveira da Costa"], "title": "Monitoring Hyperproperties over Observed and Constructed Traces", "categories": ["cs.LO", "68Q60, 68Q45", "F.3.1; D.3.1"], "comment": null, "summary": "We study the problem of monitoring at runtime whether a system fulfills a\nspecification defined by a hyperproperty, such as linearizability or variants\nof non-interference. For this purpose, we introduce specifications with both\npassive and active quantification over traces. While passive trace quantifiers\nrange over the traces that are observed, active trace quantifiers are\ninstantiated with \\emph{generator functions}, which are part of the\nspecification. Generator functions enable the monitor to construct traces that\nmay never be observed at runtime, such as the linearizations of a concurrent\ntrace. As specification language, we extend hypernode logic with trace\nquantifiers over generator functions and interpret these hypernode formulas\nover possibly infinite domains. We present a corresponding monitoring\nalgorithm, which we implemented and evaluated on a range of hyperproperties for\nconcurrency and security applications. Our method enables, for the first time,\nthe monitoring of asynchronous hyperproperties that contain alternating trace\nquantifiers.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u8fd0\u884c\u65f6\u76d1\u63a7\u7cfb\u7edf\u662f\u5426\u6ee1\u8db3\u7531\u8d85\u5c5e\u6027\u5b9a\u4e49\u7684\u89c4\u8303\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e3b\u52a8\u548c\u88ab\u52a8\u8ffd\u8e2a\u91cf\u8bcd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u751f\u6210\u5668\u51fd\u6570\u7684\u76d1\u63a7\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u76d1\u63a7\u5f02\u6b65\u8d85\u5c5e\u6027\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5305\u542b\u4ea4\u66ff\u8ffd\u8e2a\u91cf\u8bcd\u7684\u89c4\u8303\u3002", "method": "\u6269\u5c55\u4e86hypernode\u903b\u8f91\uff0c\u5f15\u5165\u4e86\u751f\u6210\u5668\u51fd\u6570\u4f5c\u4e3a\u4e3b\u52a8\u8ffd\u8e2a\u91cf\u8bcd\uff0c\u5e76\u5728\u53ef\u80fd\u65e0\u9650\u57df\u4e0a\u89e3\u91ca\u8fd9\u4e9b\u516c\u5f0f\u3002", "result": "\u5b9e\u73b0\u4e86\u76d1\u63a7\u7b97\u6cd5\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u9996\u6b21\u652f\u6301\u4e86\u5305\u542b\u4ea4\u66ff\u8ffd\u8e2a\u91cf\u8bcd\u7684\u5f02\u6b65\u8d85\u5c5e\u6027\u76d1\u63a7\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u76d1\u63a7\u590d\u6742\u8d85\u5c5e\u6027\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5e76\u53d1\u548c\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2508.02458", "pdf": "https://arxiv.org/pdf/2508.02458", "abs": "https://arxiv.org/abs/2508.02458", "authors": ["Yichao Feng"], "title": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning", "categories": ["cs.DB"], "comment": null, "summary": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u548c\u793e\u4ea4\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u9700\u8981\u63a8\u65ad\u9690\u542b\u5fc3\u7406\u72b6\u6001\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e13\u5bb6\u5fc3\u7406\u6a21\u5f0f\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5fc3\u7406\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u7684\u5fc3\u7406\u573a\u666f\u6539\u8fdb\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6a21\u4eff\u4e13\u5bb6\u5fc3\u7406\u6a21\u5f0f\uff0c\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u523a\u6fc0\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u6307\u5bfc\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e13\u5bb6\u7ea7\u8868\u73b0\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5fc3\u7406\u5b66\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u5fc3\u7406\u72b6\u6001\u63a8\u65ad\uff0c\u63a8\u52a8\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.02236", "pdf": "https://arxiv.org/pdf/2508.02236", "abs": "https://arxiv.org/abs/2508.02236", "authors": ["Lu Chen", "Dingyi Zhao", "Zihao Yu", "Ninghui Sun", "Yungang Bao"], "title": "GSIM: Accelerating RTL Simulation for Large-Scale Designs", "categories": ["cs.AR"], "comment": null, "summary": "Register Transfer Level (RTL) simulation is widely used in design space\nexploration, verification, debugging, and preliminary performance evaluation\nfor hardware design. Among various RTL simulation approaches, software\nsimulation is the most commonly used due to its flexibility, low cost, and ease\nof debugging. However, the slow simulation of complex designs has become the\nbottleneck in design flow. In this work, we explore the sources of computation\noverhead of RTL simulation and conclude them into four factors. To optimize\nthese factors, we propose several techniques at the supernode level, node\nlevel, and bit level. Finally, we implement these techniques in a novel RTL\nsimulator GSIM. GSIM succeeds in simulating XiangShan, the state-of-the-art\nopen-source RISC-V processor. Besides, compared to Verilator, GSIM can achieve\nspeedup of 7.34x for booting Linux on XiangShan, and 19.94x for running\nCoreMark on Rocket.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86RTL\u6a21\u62df\u7684\u8ba1\u7b97\u5f00\u9500\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u591a\u5c42\u6b21\u7684\u4f18\u5316\u6280\u672f\uff0c\u6700\u7ec8\u5728GSIM\u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "RTL\u6a21\u62df\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u590d\u6742\u8bbe\u8ba1\u7684\u6a21\u62df\u901f\u5ea6\u6162\u6210\u4e3a\u74f6\u9888\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316RTL\u6a21\u62df\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8ba1\u7b97\u5f00\u9500\u7684\u56db\u4e2a\u6765\u6e90\uff0c\u5206\u522b\u5728\u8d85\u8282\u70b9\u7ea7\u3001\u8282\u70b9\u7ea7\u548c\u4f4d\u7ea7\u63d0\u51fa\u4f18\u5316\u6280\u672f\uff0c\u5e76\u5728GSIM\u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u3002", "result": "GSIM\u6210\u529f\u6a21\u62df\u4e86\u5148\u8fdb\u7684RISC-V\u5904\u7406\u5668XiangShan\uff0c\u5bf9\u6bd4Verilator\uff0c\u901f\u5ea6\u63d0\u5347\u663e\u8457\uff08\u8fd0\u884cLinux\u63d0\u53477.34\u500d\uff0c\u8fd0\u884cCoreMark\u63d0\u534719.94\u500d\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86RTL\u6a21\u62df\u901f\u5ea6\uff0cGSIM\u6a21\u62df\u5668\u5728\u590d\u6742\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.01911", "pdf": "https://arxiv.org/pdf/2508.01911", "abs": "https://arxiv.org/abs/2508.01911", "authors": ["Muhammad Farhan Khan", "Muhammad Ahmed Mohsin", "Zeeshan Alam", "Muhammad Saad", "Muhammad Waqar"], "title": "Machine Learning-Driven Performance Analysis of Compressed Communication in Aerial-RIS Networks for Future 6G Networks", "categories": ["cs.DC", "cs.IT", "cs.NI", "math.IT"], "comment": "Submitted to Mobile Networks and Applications", "summary": "In the future 6G and wireless networks, particularly in dense urban\nenvironments, bandwidth exhaustion and limited capacity pose significant\nchallenges to enhancing data rates. We introduce a novel system model designed\nto improve the data rate of users in next-generation multi-cell networks by\nintegrating Unmanned Aerial Vehicle (UAV)-Assisted Reconfigurable Intelligent\nSurfaces (RIS), Non-Orthogonal Multiple Access (NOMA), and Coordinated\nMultipoint Transmission (CoMP). Optimally deploying Aerial RIS for higher data\nrates, employing NOMA to improve spectral efficiency, and utilizing CoMP to\nmitigate inter-cell interference (ICI), we significantly enhance the overall\nsystem capacity and sum rate. Furthermore, we address the challenge of feedback\noverhead associated with Quantized Phase Shifts (QPS) from the receiver to RIS.\nThe feedback channel is band-limited and cannot support a large overhead of QPS\nfor uplink communication. To ensure seamless transmission, we propose a Machine\nLearning Autoencoder technique for a compressed communication of QPS from the\nreceiver to RIS, while maintaining high accuracy. Additionally, we investigate\nthe impact of the number of Aerial RIS elements and power allocation ratio for\nNOMA on the individual data rate of users. Our simulation results demonstrate\nsubstantial improvements in spectral efficiency, outage probability, and\nbandwidth utilization, highlighting the potential of the proposed architecture\nto enhance network performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u4eba\u673a\u8f85\u52a9\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u3001\u975e\u6b63\u4ea4\u591a\u5740\u548c\u534f\u4f5c\u591a\u70b9\u4f20\u8f93\u7684\u65b0\u7cfb\u7edf\u6a21\u578b\uff0c\u4ee5\u63d0\u53476G\u7f51\u7edc\u7684\u6570\u636e\u901f\u7387\u548c\u7cfb\u7edf\u5bb9\u91cf\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u4f18\u5316\u53cd\u9988\u673a\u5236\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u672a\u67656G\u7f51\u7edc\u5e26\u5bbd\u4e0d\u8db3\u548c\u5bb9\u91cf\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u65e0\u4eba\u673a\u8f85\u52a9RIS\u3001NOMA\u548cCoMP\u6280\u672f\uff0c\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u7f16\u7801\u5668\u6765\u538b\u7f29\u53cd\u9988\u4fe1\u606f\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u9891\u8c31\u6548\u7387\u3001\u964d\u4f4e\u4e86\u4e2d\u65ad\u6982\u7387\uff0c\u5e76\u4f18\u5316\u4e86\u5e26\u5bbd\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u672a\u67656G\u7f51\u7edc\u4e2d\u5177\u6709\u663e\u8457\u63d0\u5347\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01298", "pdf": "https://arxiv.org/pdf/2508.01298", "abs": "https://arxiv.org/abs/2508.01298", "authors": ["Azadeh Sadat Miraftab", "Ahmadreza Montazerolghaem", "Behrad Mahboobi"], "title": "Improving performance of content-centric networks via decentralized coded caching for multi-level popularity and access", "categories": ["cs.NI"], "comment": null, "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u96c6\u6210\u5206\u6563\u5f0f\u7f16\u7801\u7f13\u5b58\uff08DCC\uff09\u6539\u8fdb\u5185\u5bb9\u4e2d\u5fc3\u7f51\u7edc\uff08CCN\uff09\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eIP\u7684\u7f51\u7edc\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0cCCN\u901a\u8fc7\u5185\u5bb9\u6807\u8bc6\u7b26\u76f4\u63a5\u5bfb\u5740\u548c\u8def\u7531\u63d0\u5347\u6027\u80fd\uff0c\u800cDCC\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u7b56\u7565\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u5bf9CCN\u67b6\u6784\u7684\u4fee\u6539\uff0c\u6574\u5408DCC\u6280\u672f\uff0c\u5305\u62ec\u57fa\u4e8e\u7528\u6237\u8bbf\u95ee\u6743\u9650\u7684\u5dee\u5f02\u5316\u7f16\u7801\u7b56\u7565\u548c\u8def\u5f84\u4e0a\u7684\u6570\u636e\u91cd\u7f16\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7edc\u541e\u5410\u91cf\u3001\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u5185\u5bb9\u4ea4\u4ed8\u5ef6\u8fdf\u3002", "conclusion": "\u901a\u8fc7DCC\u7684\u96c6\u6210\uff0cCCN\u7684\u6027\u80fd\u5f97\u5230\u4e86\u660e\u663e\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.01492", "pdf": "https://arxiv.org/pdf/2508.01492", "abs": "https://arxiv.org/abs/2508.01492", "authors": ["Angel C. Chavez-Moreno", "Cristina L. Abad"], "title": "OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications", "categories": ["cs.SE"], "comment": "8 pages, 7 figures, 13th IEEE International Conference on Cloud\n  Engineering (IC2E 2025, accepted, to appear)", "summary": "Function-as-a-Service (FaaS) is at the core of serverless computing, enabling\ndevelopers to easily deploy applications without managing computing resources.\nWith an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless\nFramework use YAML configurations to define and deploy APIs, tasks, workflows,\nand event-driven applications on cloud providers, promoting zero-friction\ndevelopment. As with any rapidly evolving ecosystem, there is a need for\nupdated insights into how these tools are used in real-world projects. Building\non the methodology established by the Wonderless dataset for serverless\ncomputing (and applying multiple new filtering steps), OpenLambdaVerse\naddresses this gap by creating a dataset of current GitHub repositories that\nuse the Serverless Framework in applications that contain one or more AWS\nLambda functions. We then analyze and characterize this dataset to get an\nunderstanding of the state-of-the-art in serverless architectures based on this\nstack. Through this analysis we gain important insights on the size and\ncomplexity of current applications, which languages and runtimes they employ,\nhow are the functions triggered, the maturity of the projects, and their\nsecurity practices (or lack of). OpenLambdaVerse thus offers a valuable,\nup-to-date resource for both practitioners and researchers that seek to better\nunderstand evolving serverless workloads.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86OpenLambdaVerse\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5206\u6790GitHub\u4e0a\u4f7f\u7528Serverless Framework\u548cAWS Lambda\u7684\u5b9e\u9645\u9879\u76ee\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5f53\u524d\u65e0\u670d\u52a1\u5668\u67b6\u6784\u7684\u6700\u65b0\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u5b9e\u9645\u7684\u5de5\u5177\u4f7f\u7528\u60c5\u51b5\u6765\u4e86\u89e3\u5176\u5e94\u7528\u73b0\u72b6\u3002", "method": "\u57fa\u4e8eWonderless\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u7b5b\u9009\u6b65\u9aa4\u521b\u5efaOpenLambdaVerse\u6570\u636e\u96c6\uff0c\u5206\u6790\u4f7f\u7528Serverless Framework\u7684\u9879\u76ee\u3002", "result": "\u6570\u636e\u96c6\u63ed\u793a\u4e86\u5e94\u7528\u7684\u5927\u5c0f\u3001\u590d\u6742\u6027\u3001\u4f7f\u7528\u7684\u8bed\u8a00\u548c\u8fd0\u884c\u65f6\u3001\u51fd\u6570\u89e6\u53d1\u65b9\u5f0f\u3001\u9879\u76ee\u6210\u719f\u5ea6\u53ca\u5b89\u5168\u5b9e\u8df5\u3002", "conclusion": "OpenLambdaVerse\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e86\u89e3\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u6f14\u53d8\u7684\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2508.01852", "pdf": "https://arxiv.org/pdf/2508.01852", "abs": "https://arxiv.org/abs/2508.01852", "authors": ["Junlong Tong", "Wei Zhang", "Yaohui Jin", "Xiaoyu Shen"], "title": "Context Guided Transformer Entropy Modeling for Video Compression", "categories": ["cs.CV", "cs.MM"], "comment": "ICCV 2025", "summary": "Conditional entropy models effectively leverage spatio-temporal contexts to\nreduce video redundancy. However, incorporating temporal context often\nintroduces additional model complexity and increases computational cost. In\nparallel, many existing spatial context models lack explicit modeling the\nordering of spatial dependencies, which may limit the availability of relevant\ncontext during decoding. To address these issues, we propose the Context Guided\nTransformer (CGT) entropy model, which estimates probability mass functions of\nthe current frame conditioned on resampled temporal context and\ndependency-weighted spatial context. A temporal context resampler learns\npredefined latent queries to extract critical temporal information using\ntransformer encoders, reducing downstream computational overhead. Meanwhile, a\nteacher-student network is designed as dependency-weighted spatial context\nassigner to explicitly model the dependency of spatial context order. The\nteacher generates an attention map to represent token importance and an entropy\nmap to reflect prediction certainty from randomly masked inputs, guiding the\nstudent to select the weighted top-k tokens with the highest spatial\ndependency. During inference, only the student is used to predict undecoded\ntokens based on high-dependency context. Experimental results demonstrate that\nour CGT model reduces entropy modeling time by approximately 65% and achieves\nan 11% BD-Rate reduction compared to the previous state-of-the-art conditional\nentropy model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCGT\u7684\u71b5\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u65b0\u91c7\u6837\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u52a0\u6743\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6765\u51cf\u5c11\u89c6\u9891\u5197\u4f59\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u6761\u4ef6\u71b5\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u4e0a\u4e0b\u6587\u65f6\u589e\u52a0\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4e14\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u7a7a\u95f4\u4f9d\u8d56\u987a\u5e8f\u5efa\u6a21\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u4e0a\u4e0b\u6587\u91cd\u91c7\u6837\u5668\u548c\u4f9d\u8d56\u52a0\u6743\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5206\u914d\u5668\uff08\u57fa\u4e8e\u5e08\u751f\u7f51\u7edc\uff09\u6765\u4f18\u5316\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "result": "CGT\u6a21\u578b\u5c06\u71b5\u5efa\u6a21\u65f6\u95f4\u51cf\u5c11\u7ea665%\uff0cBD-Rate\u964d\u4f4e11%\u3002", "conclusion": "CGT\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u89c6\u9891\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00852", "pdf": "https://arxiv.org/pdf/2508.00852", "abs": "https://arxiv.org/abs/2508.00852", "authors": ["Yuemin Ma", "Uksang Yoo", "Yunchao Yao", "Shahram Najam Syed", "Luca Bondi", "Jonathan Francis", "Jean Oh", "Jeffrey Ichnowski"], "title": "Visuo-Acoustic Hand Pose and Contact Estimation", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Accurately estimating hand pose and hand-object contact events is essential\nfor robot data-collection, immersive virtual environments, and biomechanical\nanalysis, yet remains challenging due to visual occlusion, subtle contact cues,\nlimitations in vision-only sensing, and the lack of accessible and flexible\ntactile sensing. We therefore introduce VibeMesh, a novel wearable system that\nfuses vision with active acoustic sensing for dense, per-vertex hand contact\nand pose estimation. VibeMesh integrates a bone-conduction speaker and sparse\npiezoelectric microphones, distributed on a human hand, emitting structured\nacoustic signals and capturing their propagation to infer changes induced by\ncontact. To interpret these cross-modal signals, we propose a graph-based\nattention network that processes synchronized audio spectra and RGB-D-derived\nhand meshes to predict contact with high spatial resolution. We contribute: (i)\na lightweight, non-intrusive visuo-acoustic sensing platform; (ii) a\ncross-modal graph network for joint pose and contact inference; (iii) a dataset\nof synchronized RGB-D, acoustic, and ground-truth contact annotations across\ndiverse manipulation scenarios; and (iv) empirical results showing that\nVibeMesh outperforms vision-only baselines in accuracy and robustness,\nparticularly in occluded or static-contact settings.", "AI": {"tldr": "VibeMesh\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u548c\u4e3b\u52a8\u58f0\u5b66\u611f\u77e5\u7684\u53ef\u7a7f\u6234\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7684\u624b\u90e8\u59ff\u52bf\u548c\u63a5\u89e6\u4e8b\u4ef6\u4f30\u8ba1\u3002", "motivation": "\u624b\u90e8\u59ff\u52bf\u548c\u63a5\u89e6\u4e8b\u4ef6\u7684\u51c6\u786e\u4f30\u8ba1\u5728\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u3001\u865a\u62df\u73b0\u5b9e\u548c\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u89c6\u89c9\u906e\u6321\u3001\u63a5\u89e6\u4fe1\u53f7\u5fae\u5f31\u3001\u89c6\u89c9\u611f\u77e5\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u7f3a\u4e4f\u7075\u6d3b\u89e6\u89c9\u611f\u77e5\u6280\u672f\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86VibeMesh\u7cfb\u7edf\uff0c\u7ed3\u5408\u9aa8\u4f20\u5bfc\u626c\u58f0\u5668\u548c\u7a00\u758f\u538b\u7535\u9ea6\u514b\u98ce\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u58f0\u5b66\u4fe1\u53f7\u4f20\u64ad\u6765\u63a8\u65ad\u63a5\u89e6\u53d8\u5316\u3002\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7684\u7f51\u7edc\uff0c\u5904\u7406\u540c\u6b65\u7684\u97f3\u9891\u9891\u8c31\u548cRGB-D\u751f\u6210\u7684\u624b\u90e8\u7f51\u683c\u3002", "result": "VibeMesh\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u7eaf\u89c6\u89c9\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u906e\u6321\u6216\u9759\u6001\u63a5\u89e6\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VibeMesh\u4e3a\u624b\u90e8\u59ff\u52bf\u548c\u63a5\u89e6\u4e8b\u4ef6\u7684\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u975e\u4fb5\u5165\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u4e86\u8de8\u6a21\u6001\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00937", "pdf": "https://arxiv.org/pdf/2508.00937", "abs": "https://arxiv.org/abs/2508.00937", "authors": ["Bernarda Petek", "David Nabergoj", "Erik \u0160trumbelj"], "title": "A General Approach to Visualizing Uncertainty in Statistical Graphics", "categories": ["stat.ME", "cs.GR", "cs.LG"], "comment": null, "summary": "Visualizing uncertainty is integral to data analysis, yet its application is\noften hindered by the need for specialized methods for quantifying and\nrepresenting uncertainty for different types of graphics. We introduce a\ngeneral approach that simplifies this process. The core idea is to treat the\nstatistical graphic as a function of the underlying distribution. Instead of\nfirst calculating uncertainty metrics and then plotting them, the method\npropagates uncertainty through to the visualization. By repeatedly sampling\nfrom the data distribution and generating a complete statistical graphic for\neach sample, a distribution over graphics is produced. These graphics are\naggregated pixel-by-pixel to create a single, static image. This approach is\nversatile, requires no specific knowledge from the user beyond how to create\nthe basic statistical graphic, and comes with theoretical coverage guarantees\nfor standard cases such as confidence intervals and bands. We provide a\nreference implementation as a Python library to demonstrate the method's\nutility. Our approach not only reproduces conventional uncertainty\nvisualizations for point estimates and regression lines but also seamlessly\nextends to non-standard cases, including pie charts, stacked bar charts, and\ntables. This approach makes uncertainty visualization more accessible to\npractitioners and can be a valuable tool for teaching uncertainty.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7edf\u8ba1\u56fe\u5f62\u89c6\u4e3a\u5e95\u5c42\u5206\u5e03\u7684\u51fd\u6570\uff0c\u76f4\u63a5\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\u5230\u53ef\u89c6\u5316\u4e2d\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u8868\u793a\u7684\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u56fe\u5f62\u4f7f\u7528\u4e13\u95e8\u7684\u6280\u672f\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u4f7f\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u66f4\u6613\u4e8e\u5b9e\u73b0\u548c\u4f7f\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u7edf\u8ba1\u56fe\u5f62\u89c6\u4e3a\u5e95\u5c42\u5206\u5e03\u7684\u51fd\u6570\uff0c\u901a\u8fc7\u4ece\u6570\u636e\u5206\u5e03\u4e2d\u91cd\u590d\u91c7\u6837\u5e76\u4e3a\u6bcf\u4e2a\u6837\u672c\u751f\u6210\u5b8c\u6574\u7684\u7edf\u8ba1\u56fe\u5f62\uff0c\u4ece\u800c\u4ea7\u751f\u56fe\u5f62\u4e0a\u7684\u5206\u5e03\uff0c\u6700\u7ec8\u901a\u8fc7\u50cf\u7d20\u7ea7\u805a\u5408\u751f\u6210\u5355\u4e00\u9759\u6001\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u91cd\u73b0\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\uff08\u5982\u7f6e\u4fe1\u533a\u95f4\u548c\u5e26\uff09\uff0c\u8fd8\u80fd\u6269\u5c55\u5230\u975e\u6807\u51c6\u6848\u4f8b\uff08\u5982\u997c\u56fe\u3001\u5806\u53e0\u6761\u5f62\u56fe\u548c\u8868\u683c\uff09\uff0c\u4e14\u65e0\u9700\u7528\u6237\u5177\u5907\u7279\u5b9a\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u901a\u7528\u65b9\u6cd5\u4f7f\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u66f4\u6613\u4e8e\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u7edf\u8ba1\u56fe\u5f62\uff0c\u5e76\u53ef\u4f5c\u4e3a\u6559\u6388\u4e0d\u786e\u5b9a\u6027\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2302.06506", "pdf": "https://arxiv.org/pdf/2302.06506", "abs": "https://arxiv.org/abs/2302.06506", "authors": ["Nicola Cotumaccio"], "title": "A Myhill-Nerode Theorem for Generalized Automata, with Applications to Pattern Matching and Compression", "categories": ["cs.FL", "cs.DS", "cs.LO"], "comment": null, "summary": "The model of generalized automata, introduced by Eilenberg in 1974, allows\nrepresenting a regular language more concisely than conventional automata by\nallowing edges to be labeled not only with characters, but also strings.\nGiammaresi and Montalbano introduced a notion of determinism for generalized\nautomata [STACS 1995]. While generalized deterministic automata retain many\nproperties of conventional deterministic automata, the uniqueness of a minimal\ngeneralized deterministic automaton is lost.\n  In the first part of the paper, we show that the lack of uniqueness can be\nexplained by introducing a set $ \\mathcal{W(A)} $ associated with a generalized\nautomaton $ \\mathcal{A} $. By fixing $ \\mathcal{W(A)} $, we are able to derive\nfor the first time a full Myhill-Nerode theorem for generalized automata, which\ncontains the textbook Myhill-Nerode theorem for conventional automata as a\ndegenerate case.\n  In the second part of the paper, we show that the set $ \\mathcal{W(A)} $\nleads to applications for pattern matching and data compression. Wheeler\nautomata [TCS 2017, SODA 2020] are a popular class of automata that can be\ncompactly stored using $ e \\log \\sigma (1 + o(1)) + O(e) $ bits ($ e $ being\nthe number of edges, $ \\sigma $ being the size of the alphabet) in such a way\nthat pattern matching queries can be solved in $ \\tilde{O}(m) $ time ($ m $\nbeing the length of the pattern). In the paper, we show how to extend these\nresults to generalized automata. More precisely, a Wheeler generalized automata\ncan be stored using $ \\mathfrak{e} \\log \\sigma (1 + o(1)) + O(e + rn) $ bits so\nthat pattern matching queries can be solved in $ \\tilde{O}(r m) $ time, where $\n\\mathfrak{e} $ is the total length of all edge labels, $ r $ is the maximum\nlength of an edge label and $ n $ is the number of states.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5e7f\u4e49\u81ea\u52a8\u673a\u7684Myhill-Nerode\u5b9a\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6a21\u5f0f\u5339\u914d\u548c\u6570\u636e\u538b\u7f29\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5e7f\u4e49\u81ea\u52a8\u673a\u6bd4\u5e38\u89c4\u81ea\u52a8\u673a\u66f4\u7b80\u6d01\uff0c\u4f46\u6700\u5c0f\u5316\u7684\u552f\u4e00\u6027\u7f3a\u5931\u3002\u7814\u7a76\u65e8\u5728\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u5e76\u6269\u5c55\u5176\u5e94\u7528\u3002", "method": "\u5f15\u5165\u96c6\u5408W(A)\u6765\u56fa\u5b9a\u5e7f\u4e49\u81ea\u52a8\u673a\u7684\u7ed3\u6784\uff0c\u63a8\u5bfcMyhill-Nerode\u5b9a\u7406\uff0c\u5e76\u5e94\u7528\u4e8eWheeler\u81ea\u52a8\u673a\u7684\u6269\u5c55\u3002", "result": "\u6210\u529f\u4e3a\u5e7f\u4e49\u81ea\u52a8\u673a\u5efa\u7acbMyhill-Nerode\u5b9a\u7406\uff0c\u5e76\u4f18\u5316\u5176\u5b58\u50a8\u548c\u67e5\u8be2\u6548\u7387\u3002", "conclusion": "\u5e7f\u4e49\u81ea\u52a8\u673a\u7684\u7406\u8bba\u6269\u5c55\u4e3a\u65b0\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u7279\u522b\u662f\u5728\u538b\u7f29\u548c\u6a21\u5f0f\u5339\u914d\u9886\u57df\u3002"}}
{"id": "2508.02508", "pdf": "https://arxiv.org/pdf/2508.02508", "abs": "https://arxiv.org/abs/2508.02508", "authors": ["Kyoseung Koo", "Bogyeong Kim", "Bongki Moon"], "title": "M2: An Analytic System with Specialized Storage Engines for Multi-Model Workloads", "categories": ["cs.DB"], "comment": null, "summary": "Modern data analytic workloads increasingly require handling multiple data\nmodels simultaneously. Two primary approaches meet this need: polyglot\npersistence and multi-model database systems. Polyglot persistence employs a\ncoordinator program to manage several independent database systems but suffers\nfrom high communication costs due to its physically disaggregated architecture.\nMeanwhile, existing multi-model database systems rely on a single storage\nengine optimized for a specific data model, resulting in inefficient processing\nacross diverse data models. To address these limitations, we present M2, a\nmulti-model analytic system with integrated storage engines. M2 treats all data\nmodels as first-class entities, composing query plans that incorporate\noperations across models. To effectively combine data from different models,\nthe system introduces a specialized inter-model join algorithm called\nmulti-stage hash join. Our evaluation demonstrates that M2 outperforms existing\napproaches by up to 188x speedup on multi-model analytics, confirming the\neffectiveness of our proposed techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM2\u7684\u591a\u6a21\u578b\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u5b58\u50a8\u5f15\u64ce\u548c\u521b\u65b0\u7684\u591a\u9636\u6bb5\u54c8\u5e0c\u8fde\u63a5\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u578b\u6570\u636e\u5206\u6790\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u578b\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff08\u5982\u591a\u8bed\u8a00\u6301\u4e45\u5316\u548c\u5355\u5b58\u50a8\u5f15\u64ce\u591a\u6a21\u578b\u6570\u636e\u5e93\uff09\u5b58\u5728\u901a\u4fe1\u6210\u672c\u9ad8\u6216\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u6570\u636e\u5206\u6790\u9700\u6c42\u3002", "method": "M2\u7cfb\u7edf\u5c06\u6240\u6709\u6570\u636e\u6a21\u578b\u89c6\u4e3a\u4e00\u7b49\u5b9e\u4f53\uff0c\u5f00\u53d1\u4e86\u591a\u9636\u6bb5\u54c8\u5e0c\u8fde\u63a5\u7b97\u6cd5\uff0c\u5b9e\u73b0\u8de8\u6a21\u578b\u7684\u9ad8\u6548\u67e5\u8be2\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cM2\u5728\u591a\u6a21\u578b\u5206\u6790\u4e2d\u7684\u6027\u80fd\u6700\u9ad8\u53ef\u63d0\u5347188\u500d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "conclusion": "M2\u901a\u8fc7\u96c6\u6210\u5b58\u50a8\u5f15\u64ce\u548c\u521b\u65b0\u7684\u8fde\u63a5\u7b97\u6cd5\uff0c\u4e3a\u591a\u6a21\u578b\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01958", "pdf": "https://arxiv.org/pdf/2508.01958", "abs": "https://arxiv.org/abs/2508.01958", "authors": ["Alejandro Mata Ali"], "title": "Introduction to QUDO, Tensor QUDO and HOBO formulations: Qudits, Equivalences, Knapsack Problem, Traveling Salesman Problem and Combinatorial Games", "categories": ["cs.ET", "quant-ph", "90C27, 90C20, 81Q99", "G.1.6; G.2.1"], "comment": "18 pages, 5 figures", "summary": "In this paper, we present a brief review and introduction to Quadratic\nUnconstrained D-ary Optimization (QUDO), Tensor Quadratic Unconstrained D-ary\nOptimization (T-QUDO) and Higher-Order Unconstrained Binary Optimization (HOBO)\nformulations for combinatorial optimization problems. We also show their\nequivalences. To help their understanding, we make some examples for the\nknapsack problem, traveling salesman problem and different combinatorial games.\nThe games chosen to exemplify are: Hashiwokakero, N-Queens, Kakuro, Inshi no\nheya, and Peg Solitaire. Although some of these games have already been\nformulated in a QUBO formulation, we are going to approach them with more\ngeneral formulations, allowing their execution in new quantum or\nquantum-inspired optimization algorithms. This can be an easier way to\nintroduce these more complicated formulations for harder problems.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u5e76\u4ecb\u7ecd\u4e86D\u5143\u4e8c\u6b21\u65e0\u7ea6\u675f\u4f18\u5316\uff08QUDO\uff09\u3001\u5f20\u91cfD\u5143\u4e8c\u6b21\u65e0\u7ea6\u675f\u4f18\u5316\uff08T-QUDO\uff09\u548c\u9ad8\u9636\u4e8c\u8fdb\u5236\u65e0\u7ea6\u675f\u4f18\u5316\uff08HOBO\uff09\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u5e2e\u52a9\u7406\u89e3\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u7528\u66f4\u590d\u6742\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\uff0c\u5c24\u5176\u662f\u4e3a\u65b0\u7684\u91cf\u5b50\u6216\u91cf\u5b50\u542f\u53d1\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u793a\u4f8b\uff08\u5982\u80cc\u5305\u95ee\u9898\u3001\u65c5\u884c\u5546\u95ee\u9898\u548c\u591a\u79cd\u7ec4\u5408\u6e38\u620f\uff09\u5c55\u793aQUDO\u3001T-QUDO\u548cHOBO\u7684\u7b49\u4ef7\u6027\u548c\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86\u8fd9\u4e9b\u5f62\u5f0f\u5728\u4e0d\u540c\u7ec4\u5408\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u548c\u7b49\u4ef7\u6027\uff0c\u4e3a\u66f4\u590d\u6742\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u901a\u8fc7\u5177\u4f53\u793a\u4f8b\u5c55\u793a\u4e86QUDO\u3001T-QUDO\u548cHOBO\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u672a\u6765\u5728\u91cf\u5b50\u6216\u91cf\u5b50\u542f\u53d1\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.02304", "pdf": "https://arxiv.org/pdf/2508.02304", "abs": "https://arxiv.org/abs/2508.02304", "authors": ["Fangxin Liu", "Haomin Li", "Bowen Zhu", "Zongwu Wang", "Zhuoran Song", "Habing Guan", "Li Jiang"], "title": "ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering", "categories": ["cs.AR", "cs.ET", "cs.GR"], "comment": "Accepted by the 2025 International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025). The\n  paper will be presented at ASPLOS 2026", "summary": "Neural Radiance Fields (NeRF) offer significant promise for generating\nphotorealistic images and videos. However, existing mainstream neural rendering\nmodels often fall short in meeting the demands for immediacy and power\nefficiency in practical applications. Specifically, these models frequently\nexhibit irregular access patterns and substantial computational overhead,\nleading to undesirable inference latency and high power consumption.\nComputing-in-memory (CIM), an emerging computational paradigm, has the\npotential to address these access bottlenecks and reduce the power consumption\nassociated with model execution.\n  To bridge the gap between model performance and real-world scene\nrequirements, we propose an algorithm-architecture co-design approach,\nabbreviated as ASDR, a CIM-based accelerator supporting efficient neural\nrendering. At the algorithmic level, we propose two rendering optimization\nschemes: (1) Dynamic sampling by online sensing of the rendering difficulty of\ndifferent pixels, thus reducing access memory and computational overhead. (2)\nReducing MLP overhead by decoupling and approximating the volume rendering of\ncolor and density. At the architecture level, we design an efficient\nReRAM-based CIM architecture with efficient data mapping and reuse\nmicroarchitecture. Experiments demonstrate that our design can achieve up to\n$9.55\\times$ and $69.75\\times$ speedup over state-of-the-art NeRF accelerators\nand Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5b58\u8ba1\u7b97\uff08CIM\uff09\u7684\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5ASDR\uff0c\u7528\u4e8e\u9ad8\u6548\u795e\u7ecf\u6e32\u67d3\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u529f\u8017\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u6ee1\u8db3\u5373\u65f6\u6027\u548c\u80fd\u6548\u9700\u6c42\uff0c\u5b58\u5728\u4e0d\u89c4\u5219\u8bbf\u95ee\u6a21\u5f0f\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u5728\u7b97\u6cd5\u5c42\u9762\u63d0\u51fa\u52a8\u6001\u91c7\u6837\u548c\u989c\u8272\u5bc6\u5ea6\u89e3\u8026\u4f18\u5316\uff1b\u5728\u67b6\u6784\u5c42\u9762\u8bbe\u8ba1\u57fa\u4e8eReRAM\u7684\u9ad8\u6548CIM\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793aASDR\u5728\u56fe\u5f62\u6e32\u67d3\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u52a0\u901f\u5668\u5feb9.55\u500d\uff0c\u6bd4Xavier NX GPU\u5feb69.75\u500d\uff0c\u4ec5\u635f\u59310.1 PSNR\u3002", "conclusion": "ASDR\u901a\u8fc7\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u6e32\u67d3\u7684\u6548\u7387\u548c\u80fd\u6548\u3002"}}
{"id": "2508.01989", "pdf": "https://arxiv.org/pdf/2508.01989", "abs": "https://arxiv.org/abs/2508.01989", "authors": ["Chao Wang", "Pengfei Zuo", "Zhangyu Chen", "Yunkai Liang", "Zhou Yu", "Ming-Chang Yang"], "title": "Prefill-Decode Aggregation or Disaggregation? Unifying Both for Goodput-Optimized LLM Serving", "categories": ["cs.DC"], "comment": "17 pages, 19 figures", "summary": "An ongoing debate considers whether prefill-decode (PD) aggregation or\ndisaggregation is superior for serving large language models (LLMs). This has\ndriven optimizations for both approaches, each showing distinct advantages.\nThis paper compares PD aggregation and disaggregation, showing that each excels\nunder different service-level objectives (SLOs): aggregation is optimal for\ntight time-to-first-token (TTFT) and relaxed time-per-output-token (TPOT),\nwhile disaggregation excels for strict TPOT and relaxed TTFT. However, under\nbalanced TTFT and TPOT SLOs, neither approach delivers optimal goodput.\n  This paper proposes TaiChi, an LLM serving system that unifies PD\ndisaggregation and aggregation for optimal goodput under any combination of\nTTFT and TPOT SLOs. TaiChi uses a unified disaggregation-aggregation\narchitecture with differentiated-capability GPU instances: prefill-heavy (fast\nprefill, high-interference decode) and decode-heavy (low-interference decode,\nslow prefill). Three configurable sliders control the ratio between these\ninstances and their chunk sizes. TaiChi adapts to various SLO regimes by\nadjusting sliders. When TTFT constraints are tight, TaiChi resembles a PD\naggregation configuration; when TPOT dominates, it adapts toward PD\ndisaggregation. Crucially, under balanced SLOs, TaiChi enables a hybrid mode\nfor superior goodput. The key innovation behind this hybrid mode is latency\nshifting: selectively reallocating GPU resources from requests that meet SLOs\nto those at risk of violation, maximizing the number of SLO-satisfied requests.\nThis fine-grained latency shifting is orchestrated by two scheduling\nmechanisms: flowing decode scheduling to control TPOTs and length-aware prefill\nscheduling to manage TTFTs, which jointly optimize request assignment. Our\nexperiments show TaiChi improves goodput by up to 77% over state-of-the-art\nsystems under balanced TTFT and TPOT SLOs.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cdLLM\u670d\u52a1\u65b9\u6cd5\uff08\u9884\u586b\u5145-\u89e3\u7801\u805a\u5408\u4e0e\u5206\u89e3\uff09\uff0c\u63d0\u51faTaiChi\u7cfb\u7edf\u7edf\u4e00\u4e24\u8005\uff0c\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9884\u586b\u5145-\u89e3\u7801\u805a\u5408\u4e0e\u5206\u89e3\u5728\u7279\u5b9aSLO\u4e0b\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTaiChi\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\u9002\u5e94\u4e0d\u540cSLO\u3002", "result": "\u5728\u5e73\u8861SLO\u4e0b\uff0cTaiChi\u6027\u80fd\u63d0\u534777%\u3002", "conclusion": "TaiChi\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u52a8\u6001\u8c03\u5ea6\uff0c\u663e\u8457\u4f18\u5316LLM\u670d\u52a1\u6027\u80fd\u3002"}}
{"id": "2508.01805", "pdf": "https://arxiv.org/pdf/2508.01805", "abs": "https://arxiv.org/abs/2508.01805", "authors": ["Yongjie Zeng", "Hongyang Du"], "title": "M3LLM: Model Context Protocol-aided Mixture of Vision Experts For Multimodal LLMs in Networks", "categories": ["cs.NI"], "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) rely on centralized\narchitectures and often suffer from poor alignment between the input task and\ntheir fixed visual encoding modules, which limits performance on diverse and\ndynamic visual tasks. With the increasing deployment of resource-efficient\nmodels on edge devices in wireless networks, a new opportunity emerges to\ndynamically use distributed vision experts for improved MLLM inference quality.\nTo enable this, we propose M3LLM, where the Model Context Protocol (MCP)\ncoordinates a mixture of vision experts to achieve distributed MLLMs.\nSpecifically, MCP is an open protocol that structures the input task context\ninto interpretable representations, enabling wireless network-aware\ncoordination between the central model backbone and edge-hosted vision experts.\nBased on the MCP representation, M3LLM formulates vision expert routing as a\njoint optimization problem that balances task-expert semantic compatibility and\nchannel performance. To solve the resulting gradient conflicts, we develop a\ndual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and\nintroduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical\nBayesian modeling to ensure effective routing. Experiments show that M3LLM\nimproves task accuracy, reduces communication cost, and enhances expert routing\nadaptability under dynamic wireless network conditions.", "AI": {"tldr": "M3LLM\u901a\u8fc7\u5206\u5e03\u5f0f\u89c6\u89c9\u4e13\u5bb6\u548c\u534f\u8bae\u534f\u8c03\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLM\u67b6\u6784\u96c6\u4e2d\u4e8e\u4e2d\u592e\u5316\uff0c\u89c6\u89c9\u7f16\u7801\u6a21\u5757\u4e0e\u4efb\u52a1\u5bf9\u9f50\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u52a8\u6001\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faM3LLM\uff0c\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u534f\u8c03\u89c6\u89c9\u4e13\u5bb6\uff1b\u901a\u8fc7\u53cc\u6d41SAC\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u7a33\u5b9a\u6027\u589e\u5f3a\u6a21\u5757\uff08ASEM\uff09\u4f18\u5316\u8def\u7531\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cM3LLM\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\uff0c\u589e\u5f3a\u4e86\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u8def\u7531\u9002\u5e94\u6027\u3002", "conclusion": "M3LLM\u4e3a\u5206\u5e03\u5f0fMLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.01523", "pdf": "https://arxiv.org/pdf/2508.01523", "abs": "https://arxiv.org/abs/2508.01523", "authors": ["Ningzhi Tang", "Emory Smith", "Yu Huang", "Collin McMillan", "Toby Jia-Jun Li"], "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification", "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "This paper presents a study of using large language models (LLMs) in\nmodifying existing code. While LLMs for generating code have been widely\nstudied, their role in code modification remains less understood. Although\n\"prompting\" serves as the primary interface for developers to communicate\nintents to LLMs, constructing effective prompts for code modification\nintroduces challenges different from generation. Prior work suggests that\nnatural language summaries may help scaffold this process, yet such approaches\nhave been validated primarily in narrow domains like SQL rewriting. This study\ninvestigates two prompting strategies for LLM-assisted code modification:\nDirect Instruction Prompting, where developers describe changes explicitly in\nfree-form language, and Summary-Mediated Prompting, where changes are made by\nediting the generated summaries of the code. We conducted an exploratory study\nwith 15 developers who completed modification tasks using both techniques\nacross multiple scenarios. Our findings suggest that developers followed an\niterative workflow: understanding the code, localizing the edit, and validating\noutputs through execution or semantic reasoning. Each prompting strategy\npresented trade-offs: direct instruction prompting was more flexible and easier\nto specify, while summary-mediated prompting supported comprehension, prompt\nscaffolding, and control. Developers' choice of strategy was shaped by task\ngoals and context, including urgency, maintainability, learning intent, and\ncode familiarity. These findings highlight the need for more usable prompt\ninteractions, including adjustable summary granularity, reliable summary-code\ntraceability, and consistency in generated summaries.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4fee\u6539\u4ee3\u7801\u7684\u4e24\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u76f4\u63a5\u6307\u4ee4\u63d0\u793a\u548c\u6458\u8981\u8f85\u52a9\u63d0\u793a\uff0c\u5e76\u5206\u6790\u4e86\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u4ee3\u7801\u4fee\u6539\u4e2d\u7684\u4f5c\u7528\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u3002", "method": "\u901a\u8fc715\u540d\u5f00\u53d1\u8005\u7684\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u76f4\u63a5\u6307\u4ee4\u63d0\u793a\u548c\u6458\u8981\u8f85\u52a9\u63d0\u793a\u5728\u4ee3\u7801\u4fee\u6539\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u76f4\u63a5\u6307\u4ee4\u63d0\u793a\u66f4\u7075\u6d3b\u6613\u7528\uff0c\u6458\u8981\u8f85\u52a9\u63d0\u793a\u6709\u52a9\u4e8e\u4ee3\u7801\u7406\u89e3\u548c\u63a7\u5236\uff0c\u7b56\u7565\u9009\u62e9\u53d7\u4efb\u52a1\u76ee\u6807\u548c\u4e0a\u4e0b\u6587\u5f71\u54cd\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u63d0\u793a\u4ea4\u4e92\u7684\u53ef\u7528\u6027\uff0c\u5305\u62ec\u6458\u8981\u7c92\u5ea6\u8c03\u6574\u3001\u53ef\u9760\u7684\u4ee3\u7801\u53ef\u8ffd\u6eaf\u6027\u53ca\u6458\u8981\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.01980", "pdf": "https://arxiv.org/pdf/2508.01980", "abs": "https://arxiv.org/abs/2508.01980", "authors": ["Xiaoyu Zhang", "Ziwei Wang", "Hai Dong", "Zhifeng Bao", "Jiajun Liu"], "title": "On-the-Fly Object-aware Representative Point Selection in Point Cloud", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Point clouds are essential for object modeling and play a critical role in\nassisting driving tasks for autonomous vehicles (AVs). However, the significant\nvolume of data generated by AVs creates challenges for storage, bandwidth, and\nprocessing cost. To tackle these challenges, we propose a representative point\nselection framework for point cloud downsampling, which preserves critical\nobject-related information while effectively filtering out irrelevant\nbackground points. Our method involves two steps: (1) Object Presence\nDetection, where we introduce an unsupervised density peak-based classifier and\na supervised Na\\\"ive Bayes classifier to handle diverse scenarios, and (2)\nSampling Budget Allocation, where we propose a strategy that selects\nobject-relevant points while maintaining a high retention rate of object\ninformation. Extensive experiments on the KITTI and nuScenes datasets\ndemonstrate that our method consistently outperforms state-of-the-art baselines\nin both efficiency and effectiveness across varying sampling rates. As a\nmodel-agnostic solution, our approach integrates seamlessly with diverse\ndownstream models, making it a valuable and scalable addition to the 3D point\ncloud downsampling toolkit for AV applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u70b9\u4e91\u4e0b\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u5173\u952e\u7269\u4f53\u4fe1\u606f\u5e76\u8fc7\u6ee4\u65e0\u5173\u80cc\u666f\u70b9\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5927\u91cf\u70b9\u4e91\u6570\u636e\u7684\u5b58\u50a8\u548c\u5904\u7406\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u751f\u6210\u7684\u70b9\u4e91\u6570\u636e\u91cf\u5de8\u5927\uff0c\u5bf9\u5b58\u50a8\u3001\u5e26\u5bbd\u548c\u5904\u7406\u6210\u672c\u63d0\u51fa\u6311\u6218\uff0c\u9700\u9ad8\u6548\u4e0b\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u5305\u62ec\u4e24\u6b65\uff1a\u65e0\u76d1\u7763\u5bc6\u5ea6\u5cf0\u503c\u5206\u7c7b\u5668\u4e0e\u76d1\u7763\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u68c0\u6d4b\u7269\u4f53\u5b58\u5728\uff1b\u5206\u914d\u91c7\u6837\u9884\u7b97\u7b56\u7565\u4ee5\u4fdd\u7559\u7269\u4f53\u4fe1\u606f\u3002", "result": "\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u6548\u7387\u548c\u6548\u679c\u5747\u4f18\uff0c\u4e14\u6a21\u578b\u65e0\u5173\u6027\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u70b9\u4e91\u4e0b\u91c7\u6837\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u6a21\u578b\u3002"}}
{"id": "2508.00856", "pdf": "https://arxiv.org/pdf/2508.00856", "abs": "https://arxiv.org/abs/2508.00856", "authors": ["Steph Grohmann"], "title": "EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "In biomedical science, review by a Research Ethics Committee (REC) is an\nindispensable way of protecting human subjects from harm. However, in social\nscience and the humanities, mandatory ethics compliance has long been met with\nscepticism as biomedical models of ethics can map poorly onto methodologies\ninvolving complex socio-political and cultural considerations. As a result,\ntailored ethics training and support as well as access to RECs with the\nnecessary expertise is lacking in some areas, including parts of Europe and\nlow- and middle-income countries. This paper suggests that Generative AI can\nmeaningfully contribute to closing these gaps, illustrating this claim by\npresenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics\nsupport system for social science and humanities researchers. Drawing on\nconstitutional AI technology and a collaborative prompt development\nmethodology, EthicAlly provides structured ethics assessment that incorporates\nboth universal ethics principles and contextual and interpretive considerations\nrelevant to most social science research. In supporting researchers in ethical\nresearch design and preparation for REC submission, this kind of system can\nalso contribute to easing the burden on institutional RECs, without attempting\nto automate or replace human ethical oversight.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5de5\u5177EthicAlly\u4e3a\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u7814\u7a76\u8005\u63d0\u4f9b\u4f26\u7406\u652f\u6301\uff0c\u5f25\u8865\u73b0\u6709\u4f26\u7406\u5ba1\u67e5\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u7684\u4f26\u7406\u5ba1\u67e5\u7f3a\u4e4f\u9002\u5e94\u5176\u65b9\u6cd5\u8bba\u7684\u652f\u6301\u7cfb\u7edf\uff0c\u5bfc\u81f4\u7814\u7a76\u8005\u96be\u4ee5\u83b7\u5f97\u4e13\u4e1a\u6307\u5bfc\u3002", "method": "\u57fa\u4e8e\u5baa\u6cd5AI\u6280\u672f\u548c\u534f\u4f5c\u5f0f\u63d0\u793a\u5f00\u53d1\u65b9\u6cd5\uff0cEthicAlly\u7ed3\u5408\u666e\u9002\u4f26\u7406\u539f\u5219\u4e0e\u60c5\u5883\u5316\u8003\u91cf\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u4f26\u7406\u8bc4\u4f30\u3002", "result": "EthicAlly\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\u539f\u578b\uff0c\u53ef\u534f\u52a9\u7814\u7a76\u8005\u8bbe\u8ba1\u4f26\u7406\u7814\u7a76\u5e76\u51cf\u8f7b\u4f26\u7406\u5ba1\u67e5\u59d4\u5458\u4f1a\u8d1f\u62c5\uff0c\u4f46\u4e0d\u53d6\u4ee3\u4eba\u5de5\u76d1\u7763\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u80fd\u6709\u6548\u586b\u8865\u4f26\u7406\u652f\u6301\u7cfb\u7edf\u7684\u7a7a\u767d\uff0c\u540c\u65f6\u9700\u4fdd\u6301\u4eba\u7c7b\u76d1\u7763\u7684\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2508.01537", "pdf": "https://arxiv.org/pdf/2508.01537", "abs": "https://arxiv.org/abs/2508.01537", "authors": ["Nianyi Wang", "Yu Chen", "Shuai Zheng"], "title": "FluidFormer: Transformer with Continuous Convolution for Particle-based Fluid Simulation", "categories": ["cs.CE", "cs.GR", "cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Learning-based fluid simulation networks have been proven as viable\nalternatives to traditional numerical solvers for the Navier-Stokes equations.\nExisting neural methods follow Smoothed Particle Hydrodynamics (SPH)\nframeworks, which inherently rely only on local inter-particle interactions.\nHowever, we emphasize that global context integration is also essential for\nlearning-based methods to stabilize complex fluid simulations. We propose the\nfirst Fluid Attention Block (FAB) with a local-global hierarchy, where\ncontinuous convolutions extract local features while self-attention captures\nglobal dependencies. This fusion suppresses the error accumulation and models\nlong-range physical phenomena. Furthermore, we pioneer the first Transformer\narchitecture specifically designed for continuous fluid simulation, seamlessly\nintegrated within a dual-pipeline architecture. Our method establishes a new\nparadigm for neural fluid simulation by unifying convolution-based local\nfeatures with attention-based global context modeling. FluidFormer demonstrates\nstate-of-the-art performance, with stronger stability in complex fluid\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c40\u90e8-\u5168\u5c40\u5c42\u6b21\u7684Fluid Attention Block\uff08FAB\uff09\u548c\u4e13\u4e3a\u6d41\u4f53\u6a21\u62df\u8bbe\u8ba1\u7684Transformer\u67b6\u6784FluidFormer\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u6d41\u4f53\u6a21\u62df\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6d41\u4f53\u6a21\u62df\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5c40\u90e8\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\uff0c\u4f46\u5ffd\u89c6\u5168\u5c40\u4e0a\u4e0b\u6587\u96c6\u6210\u53ef\u80fd\u5bfc\u81f4\u590d\u6742\u6d41\u4f53\u6a21\u62df\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86Fluid Attention Block\uff08FAB\uff09\uff0c\u901a\u8fc7\u8fde\u7eed\u5377\u79ef\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e13\u4e3a\u8fde\u7eed\u6d41\u4f53\u6a21\u62df\u8bbe\u8ba1\u7684Transformer\u67b6\u6784FluidFormer\uff0c\u91c7\u7528\u53cc\u7ba1\u9053\u7ed3\u6784\u96c6\u6210\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u3002", "result": "FluidFormer\u5728\u590d\u6742\u6d41\u4f53\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u6210\u4e3a\u795e\u7ecf\u6d41\u4f53\u6a21\u62df\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u6d41\u4f53\u6a21\u62df\u7684\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00853", "pdf": "https://arxiv.org/pdf/2508.00853", "abs": "https://arxiv.org/abs/2508.00853", "authors": ["Kei Itoh"], "title": "A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation", "categories": ["cs.AI", "cs.LO"], "comment": "43 pages, 8 figures, 8 Tables, in English, in Japanese", "summary": "This study aims to reinforce the theoretical foundation for diverse\nsystems--including the axiomatic definition of intelligence--by introducing a\nmathematically rigorous and unified formal structure for the concept of\n'state,' which has long been used without consensus or formal clarity. First, a\n'hierarchical state grid' composed of two axes--state depth and mapping\nhierarchy--is proposed to provide a unified notational system applicable across\nmathematical, physical, and linguistic domains. Next, the 'Intermediate\nMeta-Universe (IMU)' is introduced to enable explicit descriptions of definers\n(ourselves) and the languages we use, thereby allowing conscious meta-level\noperations while avoiding self-reference and logical inconsistency. Building on\nthis meta-theoretical foundation, this study expands inter-universal theory\nbeyond mathematics to include linguistic translation and agent integration,\nintroducing the conceptual division between macrocosm-inter-universal and\nmicrocosm-inter-universal operations for broader expressivity. Through these\ncontributions, this paper presents a meta-formal logical framework--grounded in\nthe principle of definition = state--that spans time, language, agents, and\noperations, providing a mathematically robust foundation applicable to the\ndefinition of intelligence, formal logic, and scientific theory at large.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u4e0a\u4e25\u8c28\u7684\u72b6\u6001\u5b9a\u4e49\u6846\u67b6\uff0c\u65e8\u5728\u7edf\u4e00\u8de8\u9886\u57df\u7684\u2018\u72b6\u6001\u2019\u6982\u5ff5\uff0c\u5e76\u5f15\u5165\u2018\u5206\u5c42\u72b6\u6001\u7f51\u683c\u2019\u4e0e\u2018\u4e2d\u95f4\u5143\u5b87\u5b99\u2019\u4ee5\u89e3\u51b3\u81ea\u5f15\u7528\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u2018\u72b6\u6001\u2019\u6982\u5ff5\u7684\u5171\u8bc6\u548c\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u963b\u788d\u4e86\u667a\u80fd\u5b9a\u4e49\u7b49\u7cfb\u7edf\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u2018\u5206\u5c42\u72b6\u6001\u7f51\u683c\u2019\u548c\u2018\u4e2d\u95f4\u5143\u5b87\u5b99\u2019\uff0c\u7528\u4e8e\u7edf\u4e00\u8868\u793a\u548c\u907f\u514d\u81ea\u5f15\u7528\u95ee\u9898\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8de8\u65f6\u95f4\u3001\u8bed\u8a00\u548c\u64cd\u4f5c\u7684\u5f62\u5f0f\u903b\u8f91\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u5b9a\u4e49\u7b49\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u4e3a\u667a\u80fd\u5b9a\u4e49\u548c\u79d1\u5b66\u7406\u8bba\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u7a33\u5065\u7684\u57fa\u7840\u3002"}}
{"id": "2508.02548", "pdf": "https://arxiv.org/pdf/2508.02548", "abs": "https://arxiv.org/abs/2508.02548", "authors": ["Enrico Franconi", "Beno\u00eet Groz", "Jan Hidders", "Nina Pardal", "S\u0142awek Staworko", "Jan Van den Bussche", "Piotr Wieczorek"], "title": "The KG-ER Conceptual Schema Language", "categories": ["cs.DB", "cs.AI", "68P15"], "comment": null, "summary": "We propose KG-ER, a conceptual schema language for knowledge graphs that\ndescribes the structure of knowledge graphs independently of their\nrepresentation (relational databases, property graphs, RDF) while helping to\ncapture the semantics of the information stored in a knowledge graph.", "AI": {"tldr": "KG-ER\u662f\u4e00\u79cd\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u6982\u5ff5\u6a21\u5f0f\u8bed\u8a00\uff0c\u72ec\u7acb\u4e8e\u5176\u8868\u793a\u5f62\u5f0f\uff08\u5982\u5173\u7cfb\u6570\u636e\u5e93\u3001\u5c5e\u6027\u56fe\u3001RDF\uff09\u63cf\u8ff0\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\uff0c\u5e76\u5e2e\u52a9\u6355\u6349\u5176\u4e2d\u4fe1\u606f\u7684\u8bed\u4e49\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u7684\u8868\u793a\u5f62\u5f0f\u591a\u6837\uff08\u5982\u5173\u7cfb\u6570\u636e\u5e93\u3001\u5c5e\u6027\u56fe\u3001RDF\uff09\uff0c\u4f46\u7f3a\u4e4f\u4e00\u79cd\u7edf\u4e00\u7684\u8bed\u8a00\u6765\u63cf\u8ff0\u5176\u7ed3\u6784\u5e76\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86KG-ER\uff0c\u4f5c\u4e3a\u4e00\u79cd\u6982\u5ff5\u6a21\u5f0f\u8bed\u8a00\uff0c\u72ec\u7acb\u4e8e\u5177\u4f53\u8868\u793a\u5f62\u5f0f\uff0c\u4e13\u6ce8\u4e8e\u63cf\u8ff0\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u3002", "result": "KG-ER\u80fd\u591f\u72ec\u7acb\u4e8e\u8868\u793a\u5f62\u5f0f\u63cf\u8ff0\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\uff0c\u5e76\u6709\u6548\u6355\u6349\u5176\u8bed\u4e49\u4fe1\u606f\u3002", "conclusion": "KG-ER\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u63cf\u8ff0\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.02284", "pdf": "https://arxiv.org/pdf/2508.02284", "abs": "https://arxiv.org/abs/2508.02284", "authors": ["Yukai Chen", "Massimiliano Di Todaro", "Bjorn Vermeersch", "Herman Oprins", "Daniele Jahier Pagliari", "Julien Ryckaert", "Dwaipayan Biswas", "James Myers"], "title": "Thermal Implications of Non-Uniform Power in BSPDN-Enabled 2.5D/3D Chiplet-based Systems-in-Package using Nanosheet Technology", "categories": ["cs.ET"], "comment": "Accepted for publication at the 51st IEEE European Solid-State\n  Electronics Research Conference (ESSERC 2025)", "summary": "Advances in nanosheet technologies have significantly increased power\ndensities, exacerbating thermal management challenges in 2.5D/3D chiplet-based\nSystems-in-Package (SiP). While traditional thermal analyses often employ\nuniform power maps to simplify computational complexity, this practice neglects\nlocalized heating effects, leading to inaccuracies in thermal estimations,\nespecially when comparing power delivery networks (PDN) in 3D integration. This\nwork examines the thermal impact of non-uniform power distributions on SiPs\nutilizing frontside (FSPDN) and backside (BSPDN) power delivery approaches.\nUsing high-resolution thermal simulations with non-uniform power maps at\nresolutions down to 5 micrometers, we demonstrate that uniform power\nassumptions substantially underestimate peak temperatures and fail to reveal\ncritical thermal differences between BSPDN and FSPDN configurations in 3D\nscenarios. Our results highlight that BSPDN configurations in 3D, although\nbeneficial in simplified uniform scenarios, exhibit pronounced thermal\npenalties under realistic, localized workloads due to limited lateral heat\nspreading. These findings emphasize the necessity of adopting fine-grained,\nworkload-aware power maps in early-stage thermal modeling to enable accurate\nPDN assessment and informed thermal-aware design decisions in advanced\nnanosheet-based 3D SiP.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u5747\u5300\u529f\u7387\u5206\u5e03\u5bf92.5D/3D\u82af\u7247\u7cfb\u7edf\u5c01\u88c5\uff08SiP\uff09\u4e2d\u524d\u4fa7\uff08FSPDN\uff09\u548c\u540e\u4fa7\uff08BSPDN\uff09\u7535\u6e90\u4f20\u8f93\u65b9\u6cd5\u7684\u6e29\u5ea6\u5f71\u54cd\u3002\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u70ed\u6a21\u62df\uff0c\u53d1\u73b0\u4f20\u7edf\u5747\u5300\u529f\u7387\u5047\u8bbe\u4f1a\u4f4e\u4f30\u5cf0\u503c\u6e29\u5ea6\uff0c\u5e76\u63a9\u76d6BSPDN\u548cFSPDN\u57283D\u573a\u666f\u4e2d\u7684\u5173\u952e\u70ed\u5dee\u5f02\u3002\u7ed3\u679c\u663e\u793a\uff0cBSPDN\u5728\u73b0\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7531\u4e8e\u6a2a\u5411\u70ed\u6269\u6563\u53d7\u9650\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740\u7eb3\u7c73\u7247\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u529f\u7387\u5bc6\u5ea6\u589e\u52a0\uff0c\u5bfc\u81f42.5D/3D\u82af\u7247\u7cfb\u7edf\u5c01\u88c5\u4e2d\u7684\u70ed\u7ba1\u7406\u6311\u6218\u52a0\u5267\u3002\u4f20\u7edf\u70ed\u5206\u6790\u901a\u5e38\u4f7f\u7528\u5747\u5300\u529f\u7387\u56fe\u7b80\u5316\u8ba1\u7b97\uff0c\u4f46\u5ffd\u89c6\u4e86\u5c40\u90e8\u52a0\u70ed\u6548\u5e94\uff0c\u5bfc\u81f4\u70ed\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u975e\u5747\u5300\u529f\u7387\u5206\u5e03\u5bf9\u70ed\u7ba1\u7406\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5bf9FSPDN\u548cBSPDN\u7684\u5bf9\u6bd4\u6548\u679c\u3002", "method": "\u91c7\u7528\u9ad8\u5206\u8fa8\u7387\u70ed\u6a21\u62df\u6280\u672f\uff0c\u5206\u8fa8\u7387\u4f4e\u81f35\u5fae\u7c73\uff0c\u57fa\u4e8e\u975e\u5747\u5300\u529f\u7387\u6620\u5c04\uff0c\u5206\u6790\u4e86FSPDN\u548cBSPDN\u57283D\u96c6\u6210\u573a\u666f\u4e2d\u7684\u70ed\u6548\u5e94\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5747\u5300\u529f\u7387\u5047\u8bbe\u4f1a\u663e\u8457\u4f4e\u4f30\u5cf0\u503c\u6e29\u5ea6\uff0c\u5e76\u65e0\u6cd5\u63ed\u793aBSPDN\u548cFSPDN\u57283D\u914d\u7f6e\u4e2d\u7684\u70ed\u5dee\u5f02\u3002BSPDN\u5728\u5b9e\u9645\u5c40\u90e8\u8d1f\u8f7d\u4e0b\u7531\u4e8e\u6a2a\u5411\u70ed\u6269\u6563\u6709\u9650\u800c\u8868\u73b0\u51fa\u663e\u8457\u7684\u70ed\u52a3\u52bf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u65e9\u671f\u70ed\u5efa\u6a21\u4e2d\u91c7\u7528\u7ec6\u7c92\u5ea6\u3001\u8d1f\u8f7d\u611f\u77e5\u7684\u529f\u7387\u6620\u5c04\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u7535\u6e90\u4f20\u8f93\u7f51\u7edc\u8bc4\u4f30\uff0c\u5e76\u4e3a\u5148\u8fdb\u7684\u7eb3\u7c73\u7247\u57fa3D SiP\u8bbe\u8ba1\u63d0\u4f9b\u70ed\u611f\u77e5\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.02536", "pdf": "https://arxiv.org/pdf/2508.02536", "abs": "https://arxiv.org/abs/2508.02536", "authors": ["Yuqi Xue", "Jian Huang"], "title": "ReGate: Enabling Power Gating in Neural Processing Units", "categories": ["cs.AR"], "comment": "Accepted to MICRO'25", "summary": "The energy efficiency of neural processing units (NPU) is playing a critical\nrole in developing sustainable data centers. Our study with different\ngenerations of NPU chips reveals that 30%-72% of their energy consumption is\ncontributed by static power dissipation, due to the lack of power management\nsupport in modern NPU chips. In this paper, we present ReGate, which enables\nfine-grained power-gating of each hardware component in NPU chips with\nhardware/software co-design. Unlike conventional power-gating techniques for\ngeneric processors, enabling power-gating in NPUs faces unique challenges due\nto the fundamental difference in hardware architecture and program execution\nmodel. To address these challenges, we carefully investigate the power-gating\nopportunities in each component of NPU chips and decide the best-fit power\nmanagement scheme (i.e., hardware- vs. software-managed power gating).\nSpecifically, for systolic arrays (SAs) that have deterministic execution\npatterns, ReGate enables cycle-level power gating at the granularity of\nprocessing elements (PEs) following the inherent dataflow execution in SAs. For\ninter-chip interconnect (ICI) and HBM controllers that have long idle\nintervals, ReGate employs a lightweight hardware-based idle-detection\nmechanism. For vector units and SRAM whose idle periods vary significantly\ndepending on workload patterns, ReGate extends the NPU ISA and allows software\nlike compilers to manage the power gating. With implementation on a\nproduction-level NPU simulator, we show that ReGate can reduce the energy\nconsumption of NPU chips by up to 32.8% (15.5% on average), with negligible\nimpact on AI workload performance. The hardware implementation of power-gating\nlogic introduces less than 3.3% overhead in NPU chips.", "AI": {"tldr": "ReGate\u662f\u4e00\u79cd\u901a\u8fc7\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0NPU\u82af\u7247\u7ec6\u7c92\u5ea6\u7535\u6e90\u95e8\u63a7\u7684\u6280\u672f\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u80fd\u8017\u4e14\u5bf9\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u73b0\u4ee3NPU\u82af\u7247\u56e0\u7f3a\u4e4f\u7535\u6e90\u7ba1\u7406\u652f\u6301\uff0c\u9759\u6001\u529f\u8017\u5360\u6bd4\u9ad8\uff0830%-72%\uff09\uff0c\u4e9f\u9700\u8282\u80fd\u65b9\u6848\u3002", "method": "\u9488\u5bf9NPU\u5404\u7ec4\u4ef6\u7279\u6027\u8bbe\u8ba1\u4e0d\u540c\u7535\u6e90\u7ba1\u7406\u65b9\u6848\uff1a\u5bf9\u786e\u5b9a\u6027\u6267\u884c\u7684\u8109\u52a8\u9635\u5217\u542f\u7528\u5468\u671f\u7ea7\u95e8\u63a7\uff1b\u5bf9\u957f\u7a7a\u95f2\u7684\u4e92\u8054\u548cHBM\u63a7\u5236\u5668\u91c7\u7528\u786c\u4ef6\u7a7a\u95f2\u68c0\u6d4b\uff1b\u5bf9\u8d1f\u8f7d\u591a\u53d8\u7684\u5411\u91cf\u5355\u5143\u548cSRAM\u901a\u8fc7\u7f16\u8bd1\u5668\u7ba1\u7406\u3002", "result": "\u5728NPU\u6a21\u62df\u5668\u4e0a\u5b9e\u73b0\uff0c\u80fd\u8017\u5e73\u5747\u964d\u4f4e15.5%\uff08\u6700\u9ad832.8%\uff09\uff0c\u786c\u4ef6\u5f00\u9500\u4ec53.3%\uff0c\u4e14\u4e0d\u5f71\u54cdAI\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "ReGate\u8bc1\u660e\u4e86\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u7ec6\u7c92\u5ea6\u7535\u6e90\u95e8\u63a7\u5728NPU\u82af\u7247\u4e2d\u7684\u9ad8\u6548\u6027\u4e0e\u53ef\u884c\u6027\u3002"}}
{"id": "2508.01996", "pdf": "https://arxiv.org/pdf/2508.01996", "abs": "https://arxiv.org/abs/2508.01996", "authors": ["Yizhou Shi", "Qianpiao Ma", "Yan Xu", "Junlong Zhou", "Ming Hu", "Yunming Liao", "Hongli Xu"], "title": "DySTop", "categories": ["cs.DC"], "comment": null, "summary": "Federated Learning (FL) has emerged as a potential distributed learning\nparadigm that enables model training on edge devices (i.e., workers) while\npreserving data privacy. However, its reliance on a centralized server leads to\nlimited scalability. Decentralized federated learning (DFL) eliminates the\ndependency on a centralized server by enabling peer-to-peer model exchange.\nExisting DFL mechanisms mainly employ synchronous communication, which may\nresult in training inefficiencies under heterogeneous and dynamic edge\nenvironments. Although a few recent asynchronous DFL (ADFL) mechanisms have\nbeen proposed to address these issues, they typically yield stale model\naggregation and frequent model transmission, leading to degraded training\nperformance on non-IID data and high communication overhead. To overcome these\nissues, we present DySTop, an innovative mechanism that jointly optimizes\ndynamic staleness control and topology construction in ADFL. In each round,\nmultiple workers are activated, and a subset of their neighbors is selected to\ntransmit models for aggregation, followed by local training. We provide a\nrigorous convergence analysis for DySTop, theoretically revealing the\nquantitative relationships between the convergence bound and key factors such\nas maximum staleness, activating frequency, and data distribution among\nworkers. From the insights of the analysis, we propose a worker activation\nalgorithm (WAA) for staleness control and a phase-aware topology construction\nalgorithm (PTCA) to reduce communication overhead and handle data non-IID.\nExtensive evaluations through both large-scale simulations and real-world\ntestbed experiments demonstrate that our DySTop reduces completion time by\n51.8% and the communication resource consumption by 57.1% compared to\nstate-of-the-art solutions, while maintaining the same model accuracy.", "AI": {"tldr": "DySTop\u662f\u4e00\u4e2a\u521b\u65b0\u7684ADFL\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u9648\u65e7\u6027\u548c\u4f18\u5316\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684DFL\u673a\u5236\u5728\u5f02\u6784\u548c\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e0b\u6548\u7387\u4f4e\u4e0b\uff0c\u5f02\u6b65DFL\uff08ADFL\uff09\u53c8\u5b58\u5728\u6a21\u578b\u9648\u65e7\u548c\u901a\u4fe1\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "DySTop\u7ed3\u5408\u52a8\u6001\u9648\u65e7\u6027\u63a7\u5236\u548c\u62d3\u6251\u7ed3\u6784\u4f18\u5316\uff0c\u901a\u8fc7\u6fc0\u6d3b\u90e8\u5206\u8282\u70b9\u548c\u9009\u62e9\u90bb\u5c45\u4f20\u8f93\u6a21\u578b\u6765\u805a\u5408\u548c\u672c\u5730\u8bad\u7ec3\u3002", "result": "DySTop\u5728\u5b8c\u6210\u65f6\u95f4\u548c\u901a\u4fe1\u8d44\u6e90\u6d88\u8017\u4e0a\u5206\u522b\u51cf\u5c11\u4e8651.8%\u548c57.1%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u4e0d\u53d8\u3002", "conclusion": "DySTop\u901a\u8fc7\u4f18\u5316AFL\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u9002\u7528\u4e8e\u975eIID\u6570\u636e\u548c\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u3002"}}
{"id": "2508.01898", "pdf": "https://arxiv.org/pdf/2508.01898", "abs": "https://arxiv.org/abs/2508.01898", "authors": ["Yijing Zhang", "Md-Ferdous Pervej", "Andreas F. Molisch"], "title": "Revenue Optimization in Wireless Video Caching Networks: A Privacy-Preserving Two-Stage Solution", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "Under review for possible publication in the IEEE Transactions on\n  Communications", "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0c\u7528\u4e8e\u4f18\u5316\u65e0\u7ebf\u89c6\u9891\u7f13\u5b58\u7f51\u7edc\u7684\u6536\u76ca\u3002\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u9884\u6d4b\u591a\u65f6\u6bb5\u9700\u6c42\uff0c\u5e76\u7ed3\u5408\u5185\u5bb9\u6d41\u884c\u5ea6\u4f18\u5316\u7f13\u5b58\u7b56\u7565\u3002", "motivation": "\u89c6\u9891\u7f13\u5b58\u80fd\u663e\u8457\u63d0\u5347\u4f20\u8f93\u6548\u7387\uff0c\u4f46\u7f13\u5b58\u5927\u5c0f\u6709\u9650\uff0c\u9700\u9002\u5e94\u52a8\u6001\u7528\u6237\u9700\u6c42\u4ee5\u6700\u5927\u5316\u7cfb\u7edf\u6536\u76ca\u3002\u9700\u6c42\u9884\u6d4b\u662f\u7f13\u5b58\u89c4\u5212\u7684\u91cd\u8981\u524d\u63d0\u3002", "method": "\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u7684\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3Transformer\u9884\u6d4b\u672a\u6765\u9700\u6c42\uff0c\u7ed3\u5408\u5185\u5bb9\u6d41\u884c\u5ea6\u4f30\u8ba1\u9700\u6c42\u5206\u5e03\uff1b\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4f18\u5316\u957f\u671f\u7f13\u5b58\u7b56\u7565\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u65b9\u6848\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u96c6\u4e2d\u5f0f\u65b9\u6848\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u65b0\u7684\u6536\u76ca\u4f18\u5316\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u65b9\u6cd5\u63d0\u4f9b\u66f4\u6df1\u5c42\u6b21\u7684\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u4e24\u9636\u6bb5\u65b9\u6848\u6709\u6548\u4f18\u5316\u4e86\u65e0\u7ebf\u89c6\u9891\u7f13\u5b58\u7f51\u7edc\u7684\u957f\u671f\u6536\u76ca\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002"}}
{"id": "2508.01550", "pdf": "https://arxiv.org/pdf/2508.01550", "abs": "https://arxiv.org/abs/2508.01550", "authors": ["Zhilong Chen", "Chengzong Zhao", "Boyuan Chen", "Dayi Lin", "Yihao Chen", "Arthur Leung", "Gopi Krishnan Rajbahadur", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale", "categories": ["cs.SE"], "comment": null, "summary": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks.", "AI": {"tldr": "RepoForge\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u3001\u8bc4\u4f30\u548c\u8bad\u7ec3\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u57fa\u7840\u8bbe\u65bd\u6602\u8d35\u3001\u6570\u636e\u7a00\u7f3a\u548c\u8d28\u91cf\u63a7\u5236\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002", "motivation": "\u8bad\u7ec3\u8f6f\u4ef6\u5de5\u7a0bLLMs\u9762\u4e34\u57fa\u7840\u8bbe\u65bd\u6602\u8d35\u3001\u8bc4\u4f30\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u8d28\u91cf\u63a7\u5236\u6210\u672c\u9ad8\u7b49\u74f6\u9888\u95ee\u9898\uff0cRepoForge\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "RepoForge\u901a\u8fc7\u667a\u80fd\u4f9d\u8d56\u7ba1\u7406\u3001\u5206\u5e03\u5f0f\u8bc4\u4f30\u3001\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u548cSPICE\u96be\u5ea6\u8bc4\u4f30\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b58\u50a8\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\u5ec9\u7684\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002", "result": "RepoForge-8B-Agent\u5728SWE-Bench-Verified\u4e0a\u8fbe\u523017.4%\u7684SOTA\u6027\u80fd\uff0c\u5b58\u50a8\u51cf\u5c1114\u500d\uff0c\u8bc4\u4f30\u901f\u5ea6\u63d0\u9ad870%\uff0c\u6807\u6ce8\u6210\u672c\u964d\u4f4e19,000\u500d\u3002", "conclusion": "RepoForge\u901a\u8fc7\u7edf\u4e00\u591a\u9879\u521b\u65b0\u6280\u672f\uff0c\u8bc1\u660e\u4e86\u5373\u4f7f\u662f\u22648B\u7684\u5c0f\u6a21\u578b\u4e5f\u80fd\u5728\u82db\u523b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u89e3\u51b3\u4e86SWE\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002"}}
{"id": "2508.02172", "pdf": "https://arxiv.org/pdf/2508.02172", "abs": "https://arxiv.org/abs/2508.02172", "authors": ["Lei Yao", "Yi Wang", "Yi Zhang", "Moyun Liu", "Lap-Pui Chau"], "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "14 pages, 8 figures, accepted by MM'25", "summary": "The significance of informative and robust point representations has been\nwidely acknowledged for 3D scene understanding. Despite existing\nself-supervised pre-training counterparts demonstrating promising performance,\nthe model collapse and structural information deficiency remain prevalent due\nto insufficient point discrimination difficulty, yielding unreliable\nexpressions and suboptimal performance. In this paper, we present\nGaussianCross, a novel cross-modal self-supervised 3D representation learning\narchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques\nto address current challenges. GaussianCross seamlessly converts\nscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian\nrepresentation without missing details, enabling stable and generalizable\npre-training. Subsequently, a tri-attribute adaptive distillation splatting\nmodule is incorporated to construct a 3D feature field, facilitating synergetic\nfeature capturing of appearance, geometry, and semantic cues to maintain\ncross-modal consistency. To validate GaussianCross, we perform extensive\nevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In\nparticular, GaussianCross shows a prominent parameter and data efficiency,\nachieving superior performance through linear probing (<0.1% parameters) and\nlimited data training (1% of scenes) compared to state-of-the-art methods.\nFurthermore, GaussianCross demonstrates strong generalization capabilities,\nimproving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on\nScanNet200 semantic and instance segmentation tasks, respectively, supporting\nthe effectiveness of our approach. The code, weights, and visualizations are\npublicly available at\n\\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.", "AI": {"tldr": "GaussianCross\u662f\u4e00\u79cd\u65b0\u578b\u8de8\u6a21\u6001\u81ea\u76d1\u77633D\u8868\u793a\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u6a21\u578b\u5d29\u584c\u548c\u7ed3\u6784\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u5b58\u5728\u70b9\u533a\u5206\u96be\u5ea6\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u5d29\u584c\u548c\u7ed3\u6784\u4fe1\u606f\u7f3a\u5931\uff0c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGaussianCross\u67b6\u6784\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5c06\u70b9\u4e91\u7edf\u4e00\u5f52\u4e00\u5316\u4e3a\u9ad8\u65af\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e09\u5c5e\u6027\u81ea\u9002\u5e94\u84b8\u998f\u6a21\u5757\uff0c\u6355\u6349\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728ScanNet\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGaussianCross\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u53c2\u6570\u548c\u6570\u636e\u6548\u7387\uff0c\u5e76\u5728\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "GaussianCross\u901a\u8fc7\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u9ad8\u6548\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00929", "pdf": "https://arxiv.org/pdf/2508.00929", "abs": "https://arxiv.org/abs/2508.00929", "authors": ["Shumeng Zhang", "Raul Masu", "Mela Bettega", "Mingming Fan"], "title": "Accessibility and Social Inclusivity: A Literature Review of Music Technology for Blind and Low Vision People", "categories": ["cs.HC", "cs.CY", "cs.SD", "eess.AS"], "comment": "Accepted by ASSETS'25 - The 27th International ACM SIGACCESS\n  Conference on Computers and Accessibility", "summary": "This paper presents a systematic literature review of music technology\ntailored for blind and low vision (BLV) individuals. Music activities can be\nparticularly beneficial for BLV people. However, a systematic approach to\norganizing knowledge on designing accessible technology for BLV people has yet\nto be attempted. We categorize the existing studies based on the type of\ntechnology and the extent of BLV people's involvement in the research. We\nidentify six main categories of BLV people-oriented music technology and\nhighlight four key trends in design goals. Based on these categories, we\npropose four general insights focusing on (1) spatial awareness, (2) access to\ninformation, (3) (non-verbal) communication, and (4) memory. The identified\ntrends suggest that more empirical studies involving BLV people in real-world\nscenarios are needed to ensure that technological advancements can enhance\nmusical experiences and social inclusion. This research proposes collaborative\nmusic technology and inclusive real-world testing with the target group as two\nkey areas missing in current research. They serve as a foundational step in\nshifting the focus from ``accessible technology'' to ``inclusive technology''\nfor BLV individuals within the broader field of accessibility research.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u9488\u5bf9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\uff08BLV\uff09\u4eba\u7fa4\u7684\u97f3\u4e50\u6280\u672f\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u8bbe\u8ba1\u8d8b\u52bf\u548c\u89c1\u89e3\uff0c\u5e76\u547c\u5401\u66f4\u591a\u5b9e\u8bc1\u7814\u7a76\u548c\u5305\u5bb9\u6027\u6280\u672f\u5f00\u53d1\u3002", "motivation": "\u97f3\u4e50\u6d3b\u52a8\u5bf9BLV\u4eba\u7fa4\u7279\u522b\u6709\u76ca\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u53ef\u8bbf\u95ee\u6027\u6280\u672f\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u7c7b\u73b0\u6709\u7814\u7a76\u5e76\u603b\u7ed3\u8bbe\u8ba1\u8d8b\u52bf\uff0c\u63d0\u51fa\u56db\u4e2a\u5173\u952e\u89c1\u89e3\u3002", "result": "\u8bc6\u522b\u4e86\u516d\u7c7bBLV\u97f3\u4e50\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u8bbe\u8ba1\u8d8b\u52bf\uff08\u7a7a\u95f4\u611f\u77e5\u3001\u4fe1\u606f\u83b7\u53d6\u3001\uff08\u975e\u8bed\u8a00\uff09\u6c9f\u901a\u548c\u8bb0\u5fc6\uff09\u3002", "conclusion": "\u9700\u8981\u66f4\u591a\u5b9e\u8bc1\u7814\u7a76\u548c\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\uff0c\u4ee5\u63a8\u52a8\u4ece\u201c\u53ef\u8bbf\u95ee\u6280\u672f\u201d\u5230\u201c\u5305\u5bb9\u6027\u6280\u672f\u201d\u7684\u8f6c\u53d8\u3002"}}
{"id": "2508.00944", "pdf": "https://arxiv.org/pdf/2508.00944", "abs": "https://arxiv.org/abs/2508.00944", "authors": ["Amaury Pouly", "Mahsa Shirmohammadi", "James Worrell"], "title": "Positivity of Nearly Linearly Recurrent Sequences", "categories": ["math.DS", "cs.LO", "F.4.3"], "comment": null, "summary": "In this paper we formulate the Positivity Problem for nearly linear\n  recurrent sequences. This is a generalisation of the Positivity\n  Problem for linear recurrence sequences and is a special case of the\n  non-reachability problem for linear time-invariant systems. Our main\n  contribution is a decision procedure for the Positivity Problem for\n  nearly linear recurrences of order at most 3 whose characteristic\n  roots have absolute value at most one. The decision procedure\n  relies on a new transcendence result for infinite series that is of\n  independent interest.", "AI": {"tldr": "\u7814\u7a76\u9488\u5bf9\u51e0\u4e4e\u7ebf\u6027\u9012\u5f52\u5e8f\u5217\u7684Positivity\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u51b3\u7b56\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u7279\u5f81\u6839\u7edd\u5bf9\u503c\u4e0d\u8d85\u8fc71\u7684\u4e09\u9636\u5e8f\u5217\uff0c\u5e76\u4f9d\u8d56\u4e00\u4e2a\u65b0\u7684\u8d85\u8d8a\u6027\u7ed3\u679c\u3002", "motivation": "\u5c06Positivity\u95ee\u9898\u4ece\u7ebf\u6027\u9012\u5f52\u5e8f\u5217\u63a8\u5e7f\u5230\u51e0\u4e4e\u7ebf\u6027\u9012\u5f52\u5e8f\u5217\uff0c\u540c\u65f6\u8003\u8651\u5176\u5728\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u4e2d\u7684\u975e\u53ef\u8fbe\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u7279\u5f81\u6839\u7edd\u5bf9\u503c\u4e0d\u8d85\u8fc71\u7684\u4e09\u9636\u51e0\u4e4e\u7ebf\u6027\u9012\u5f52\u5e8f\u5217\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u65b0\u7684\u5173\u4e8e\u65e0\u9650\u7ea7\u6570\u7684\u8d85\u8d8a\u6027\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3a\u51e0\u4e4e\u7ebf\u6027\u9012\u5f52\u5e8f\u5217\u7684Positivity\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u7684\u6570\u5b66\u5de5\u5177\u7684\u5e94\u7528\u3002"}}
{"id": "2508.01108", "pdf": "https://arxiv.org/pdf/2508.01108", "abs": "https://arxiv.org/abs/2508.01108", "authors": ["Mohsen Dehghankar", "Raghav Mittal", "Suraj Shetiya", "Abolfazl Asudeh", "Gautam Das"], "title": "Efficient Direct-Access Ranked Retrieval", "categories": ["cs.DS", "cs.CG", "cs.DB"], "comment": null, "summary": "We study the problem of Direct-Access Ranked Retrieval (DAR) for interactive\ndata tooling, where evolving data exploration practices, combined with\nlarge-scale and high-dimensional datasets, create new challenges. DAR concerns\nthe problem of enabling efficient access to arbitrary rank positions according\nto a ranking function, without enumerating all preceding tuples. To address\nthis need, we formalize the DAR problem and propose a theoretically efficient\nalgorithm based on geometric arrangements, achieving logarithmic query time.\nHowever, this method suffers from exponential space complexity in high\ndimensions. Therefore, we develop a second class of algorithms based on\n$\\varepsilon$-sampling, which consume a linear space. Since exactly locating\nthe tuple at a specific rank is challenging due to its connection to the range\ncounting problem, we introduce a relaxed variant called Conformal Set Ranked\nRetrieval (CSR), which returns a small subset guaranteed to contain the target\ntuple. To solve the CSR problem efficiently, we define an intermediate problem,\nStripe Range Retrieval (SRR), and design a hierarchical sampling data structure\ntailored for narrow-range queries. Our method achieves practical scalability in\nboth data size and dimensionality. We prove near-optimal bounds on the\nefficiency of our algorithms and validate their performance through extensive\nexperiments on real and synthetic datasets, demonstrating scalability to\nmillions of tuples and hundreds of dimensions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u76f4\u63a5\u8bbf\u95ee\u6392\u540d\u68c0\u7d22\uff08DAR\uff09\u95ee\u9898\uff0c\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u6392\u5217\u4f46\u7a7a\u95f4\u590d\u6742\u5ea6\u9ad8\uff0c\u53e6\u4e00\u79cd\u57fa\u4e8e\u03b5\u91c7\u6837\uff0c\u7a7a\u95f4\u7ebf\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u7cbe\u786e\u6392\u540d\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u677e\u5f1b\u53d8\u4f53CSR\uff0c\u5e76\u901a\u8fc7\u5b9a\u4e49\u4e2d\u95f4\u95ee\u9898SRR\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u7ed3\u6784\u3002", "motivation": "\u968f\u7740\u6570\u636e\u63a2\u7d22\u5b9e\u8df5\u7684\u53d1\u5c55\u548c\u5927\u578b\u9ad8\u7ef4\u6570\u636e\u96c6\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u76f4\u63a5\u8bbf\u95ee\u4efb\u610f\u6392\u540d\u4f4d\u7f6e\u800c\u65e0\u9700\u679a\u4e3e\u6240\u6709\u524d\u7f6e\u5143\u7ec4\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u57fa\u4e8e\u51e0\u4f55\u6392\u5217\u7684\u7406\u8bba\u9ad8\u6548\u7b97\u6cd5\u548c\u57fa\u4e8e\u03b5\u91c7\u6837\u7684\u7ebf\u6027\u7a7a\u95f4\u7b97\u6cd5\uff1b\u5f15\u5165CSR\u677e\u5f1b\u53d8\u4f53\u5e76\u901a\u8fc7SRR\u95ee\u9898\u8bbe\u8ba1\u4e86\u5206\u5c42\u91c7\u6837\u6570\u636e\u7ed3\u6784\u3002", "result": "\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u6548\u6027\uff0c\u80fd\u591f\u6269\u5c55\u5230\u6570\u767e\u4e07\u5143\u7ec4\u548c\u6570\u767e\u7ef4\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u76f4\u63a5\u8bbf\u95ee\u6392\u540d\u68c0\u7d22\u95ee\u9898\u4e0a\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\uff0c\u9002\u5408\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u5ea6\u573a\u666f\u3002"}}
{"id": "2508.00844", "pdf": "https://arxiv.org/pdf/2508.00844", "abs": "https://arxiv.org/abs/2508.00844", "authors": ["Christopher Wissuchek", "Patrick Zschech"], "title": "Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework", "categories": ["cs.AI", "cs.ET", "cs.MA", "econ.GN", "q-fin.EC"], "comment": "Preprint accepted for archival and presentation at the Pacific-Asia\n  Conference on Information Systems (PACIS) 2025, Kuala Lumpur, Malaysia", "summary": "Artificial intelligence (AI) systems are evolving beyond passive tools into\nautonomous agents capable of reasoning, adapting, and acting with minimal human\nintervention. Despite their growing presence, a structured framework is lacking\nto classify and compare these systems. This paper develops a typology of\nagentic AI systems, introducing eight dimensions that define their cognitive\nand environmental agency in an ordinal structure. Using a multi-phase\nmethodological approach, we construct and refine this typology, which is then\nevaluated through a human-AI hybrid approach and further distilled into\nconstructed types. The framework enables researchers and practitioners to\nanalyze varying levels of agency in AI systems. By offering a structured\nperspective on the progression of AI capabilities, the typology provides a\nfoundation for assessing current systems and anticipating future developments\nin agentic AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u548c\u6bd4\u8f83\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5305\u62ec\u516b\u4e2a\u7ef4\u5ea6\uff0c\u5b9a\u4e49\u4e86\u5b83\u4eec\u7684\u8ba4\u77e5\u548c\u73af\u5883\u4e3b\u52a8\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u65b9\u6cd5\u6784\u5efa\u548c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u5206\u7c7b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\u6765\u5206\u7c7b\u548c\u6bd4\u8f83\u4e0d\u65ad\u53d1\u5c55\u7684\u81ea\u4e3bAI\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\u6784\u5efa\u548c\u7ec6\u5316\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u548c\u63d0\u70bc\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u516b\u4e2a\u7ef4\u5ea6\u7684AI\u7cfb\u7edf\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u5e2e\u52a9\u7814\u7a76\u548c\u5b9e\u8df5\u8005\u5206\u6790AI\u7684\u4e0d\u540c\u81ea\u4e3b\u6027\u6c34\u5e73\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6846\u67b6\u4e3a\u8bc4\u4f30\u5f53\u524dAI\u7cfb\u7edf\u548c\u9884\u6d4b\u672a\u6765\u81ea\u4e3bAI\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.02202", "pdf": "https://arxiv.org/pdf/2508.02202", "abs": "https://arxiv.org/abs/2508.02202", "authors": ["Rui Eduardo Lopes", "Duarte Raposo", "Pedro V. Teixeira", "Susana Sargento"], "title": "Self-assessment approach for resource management protocols in heterogeneous computational systems", "categories": ["cs.DC", "C.2.2; C.2.3; K.6.4"], "comment": "13 pages, 12 figures, 6 tables", "summary": "With an ever growing number of heterogeneous applicational services running\non equally heterogeneous computational systems, the problem of resource\nmanagement becomes more essential. Although current solutions consider some\nnetwork and time requirements, they mostly handle a pre-defined list of\nresource types by design and, consequently, fail to provide an extensible\nsolution to assess any other set of requirements or to switch strategies on its\nresource estimation. This work proposes an heuristics-based estimation solution\nto support any computational system as a self-assessment, including\nconsiderations on dynamically weighting the requirements, how to compute each\nnode's capacity towards an admission request, and also offers the possibility\nto extend the list of resource types considered for assessment, which is an\nuncommon view in related works. This algorithm can be used by distributed and\ncentralized resource allocation protocols to decide the best node(s) for a\nservice intended for deployment. This approach was validated across its\ncomponents and the results show that its performance is straightforward in\nresource estimation while allowing scalability and extensibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u8d44\u6e90\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4efb\u4f55\u8ba1\u7b97\u7cfb\u7edf\u7684\u81ea\u8bc4\u4f30\uff0c\u5305\u62ec\u52a8\u6001\u52a0\u6743\u9700\u6c42\u548c\u6269\u5c55\u8d44\u6e90\u7c7b\u578b\u5217\u8868\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5f02\u6784\u5e94\u7528\u670d\u52a1\u548c\u8ba1\u7b97\u7cfb\u7edf\u7684\u589e\u591a\uff0c\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u65e5\u76ca\u91cd\u8981\u3002\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u8d44\u6e90\u7c7b\u578b\uff0c\u7f3a\u4e4f\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u52a8\u6001\u52a0\u6743\u9700\u6c42\uff0c\u8ba1\u7b97\u8282\u70b9\u5bb9\u91cf\uff0c\u5e76\u652f\u6301\u8d44\u6e90\u7c7b\u578b\u7684\u6269\u5c55\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u8d44\u6e90\u4f30\u8ba1\u4e2d\u8868\u73b0\u76f4\u63a5\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u5206\u5e03\u5f0f\u548c\u96c6\u4e2d\u5f0f\u8d44\u6e90\u5206\u914d\u534f\u8bae\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\uff0c\u9002\u7528\u4e8e\u670d\u52a1\u90e8\u7f72\u3002"}}
{"id": "2508.02001", "pdf": "https://arxiv.org/pdf/2508.02001", "abs": "https://arxiv.org/abs/2508.02001", "authors": ["Chungang Lin", "Weiyao Zhang", "Tianyu Zuo", "Chao Zha", "Yilong Jiang", "Ruiqi Meng", "Haitong Luo", "Xuying Meng", "Yujun Zhang"], "title": "Convolutions are Competitive with Transformers for Encrypted Traffic Classification with Pre-training", "categories": ["cs.NI", "cs.LG"], "comment": "Under review", "summary": "Encrypted traffic classification is vital for modern network management and\nsecurity. To reduce reliance on handcrafted features and labeled data, recent\nmethods focus on learning generic representations through pre-training on\nlarge-scale unlabeled data. However, current pre-trained models face two\nlimitations originating from the adopted Transformer architecture: (1) Limited\nmodel efficiency due to the self-attention mechanism with quadratic complexity;\n(2) Unstable traffic scalability to longer byte sequences, as the explicit\npositional encodings fail to generalize to input lengths not seen during\npre-training. In this paper, we investigate whether convolutions, with linear\ncomplexity and implicit positional encoding, are competitive with Transformers\nin encrypted traffic classification with pre-training. We first conduct a\nsystematic comparison, and observe that convolutions achieve higher efficiency\nand scalability, with lower classification performance. To address this\ntrade-off, we propose NetConv, a novel pre-trained convolution model for\nencrypted traffic classification. NetConv employs stacked traffic convolution\nlayers, which enhance the ability to capture localized byte-sequence patterns\nthrough window-wise byte scoring and sequence-wise byte gating. We design a\ncontinuous byte masking pre-training task to help NetConv learn\nprotocol-specific patterns. Experimental results on four tasks demonstrate that\nNetConv improves average classification performance by 6.88% and model\nthroughput by 7.41X over existing pre-trained models.", "AI": {"tldr": "NetConv\uff0c\u4e00\u79cd\u65b0\u578b\u9884\u8bad\u7ec3\u5377\u79ef\u6a21\u578b\uff0c\u901a\u8fc7\u5c40\u90e8\u5b57\u8282\u6a21\u5f0f\u548c\u8fde\u7eed\u5b57\u8282\u63a9\u7801\u4efb\u52a1\u63d0\u5347\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6548\u7387\u548c\u957f\u5e8f\u5217\u6269\u5c55\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faNetConv\uff0c\u91c7\u7528\u5806\u53e0\u6d41\u91cf\u5377\u79ef\u5c42\u548c\u8fde\u7eed\u5b57\u8282\u63a9\u7801\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4f18\u5316\u5c40\u90e8\u6a21\u5f0f\u6355\u6349\u548c\u534f\u8bae\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNetConv\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u5e73\u5747\u63d0\u53476.88%\uff0c\u6a21\u578b\u541e\u5410\u91cf\u63d0\u53477.41\u500d\u3002", "conclusion": "\u5377\u79ef\u6a21\u578b\u5728\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0cNetConv\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2508.02340", "pdf": "https://arxiv.org/pdf/2508.02340", "abs": "https://arxiv.org/abs/2508.02340", "authors": ["Fan Hu", "Zijie Xin", "Xirong Li"], "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "Accepted by ACMMM2025", "summary": "Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLPD\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7279\u5f81\u7279\u5b9a\u7684\u5171\u540c\u7a7a\u95f4\u5e76\u5f15\u5165\u53bb\u76f8\u5173\u635f\u5931\uff0c\u4ee5\u89e3\u51b3Ad-hoc\u89c6\u9891\u641c\u7d22\u4e2d\u7684\u89c6\u89c9\u591a\u6837\u6027\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660eLPD\u5728TRECVID AVS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u589e\u5f3a\u7ed3\u679c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u878d\u5408\u591a\u7279\u5f81\u5230\u5171\u540c\u7a7a\u95f4\u65f6\u5ffd\u7565\u4e86\u591a\u6837\u6027\u9700\u6c42\uff0cLPD\u65e8\u5728\u5145\u5206\u5229\u7528\u4e2a\u4f53\u7279\u5f81\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63d0\u5347\u89c6\u9891\u641c\u7d22\u7684\u5168\u9762\u6027\u3002", "method": "LPD\u901a\u8fc7\u7279\u5f81\u7279\u5b9a\u7684\u5171\u540c\u7a7a\u95f4\u6784\u5efa\u548c\u53bb\u76f8\u5173\u635f\u5931\uff0c\u591a\u6837\u5316\u8d1f\u6837\u672c\u6392\u5e8f\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u71b5\u7684\u516c\u5e73\u591a\u7a7a\u95f4\u4e09\u5143\u7ec4\u6392\u5e8f\u635f\u5931\u4ee5\u786e\u4fdd\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728TRECVID AVS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLPD\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u901a\u8fc7\u7a7a\u95f4\u591a\u6837\u6027\u53ef\u89c6\u5316\u8bc1\u660e\u4e86\u5176\u589e\u5f3a\u7ed3\u679c\u591a\u6837\u6027\u7684\u80fd\u529b\u3002", "conclusion": "LPD\u901a\u8fc7\u591a\u7a7a\u95f4\u8bbe\u8ba1\u548c\u53bb\u76f8\u5173\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86Ad-hoc\u89c6\u9891\u641c\u7d22\u7684\u5168\u9762\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.01070", "pdf": "https://arxiv.org/pdf/2508.01070", "abs": "https://arxiv.org/abs/2508.01070", "authors": ["Zhengxin Zhang", "Shufang Qian", "Yi Wang", "Xiao Liu", "Thuong Hoang", "Chetan Arora", "Jingjing Zhang", "Henry Been Lirn Duh"], "title": "How Long Does It Take to Alleviate Discomfort? A Preliminary Study on Reducing Cybersickness in Novice Users", "categories": ["cs.HC"], "comment": null, "summary": "Cybersickness significantly impacts the user experience in VR applications.\nLocomotion tunneling is a widely adopted technique for mitigating cybersickness\nin susceptible users. However, there is a lack of research investigating the\neffects of prolonged use of locomotion tunneling among novice users. To fill\nthis gap, we used VRChat as our experimental platform. We recruited 24 novice\nVR users, defined as participants with no prior experience using immersive\nvirtual environments. We collected five days of data within a one-week period.\nThe results indicated that participants exhibited significant mitigation to\ncybersickness by Day 4. However, a change in the VR scene on Day 5 led to a\nnotable increase in cybersickness symptoms. Qualitative feedback revealed\nparticipant-perceived causes of cybersickness and suggested that the\neffectiveness of locomotion tunneling was limited in some scenarios. Finally,\nwe discussed the limitations of the study and proposed directions for future\nresearch.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86VR\u65b0\u624b\u5728\u957f\u65f6\u95f4\u4f7f\u7528\u8fd0\u52a8\u96a7\u9053\u6280\u672f\u540e\u7684\u6655\u52a8\u75c7\u7f13\u89e3\u60c5\u51b5\uff0c\u53d1\u73b0\u7b2c\u56db\u5929\u75c7\u72b6\u663e\u8457\u51cf\u8f7b\uff0c\u4f46\u7b2c\u4e94\u5929\u573a\u666f\u53d8\u5316\u5bfc\u81f4\u75c7\u72b6\u53cd\u5f39\u3002", "motivation": "\u63a2\u7d22\u8fd0\u52a8\u96a7\u9053\u6280\u672f\u5bf9VR\u65b0\u624b\u6655\u52a8\u75c7\u7684\u957f\u671f\u7f13\u89e3\u6548\u679c\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "24\u540dVR\u65b0\u624b\u5728VRChat\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e3a\u671f\u4e00\u5468\u7684\u5b9e\u9a8c\uff0c\u6536\u96c6\u4e94\u5929\u7684\u6570\u636e\u3002", "result": "\u7b2c\u56db\u5929\u6655\u52a8\u75c7\u663e\u8457\u51cf\u8f7b\uff0c\u7b2c\u4e94\u5929\u573a\u666f\u53d8\u5316\u5bfc\u81f4\u75c7\u72b6\u52a0\u91cd\uff1b\u8fd0\u52a8\u96a7\u9053\u6280\u672f\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u8fd0\u52a8\u96a7\u9053\u6280\u672f\u7684\u6548\u679c\u53d7\u573a\u666f\u53d8\u5316\u5f71\u54cd\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6280\u672f\u5e76\u63a2\u7d22\u5176\u4ed6\u7f13\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2508.02368", "pdf": "https://arxiv.org/pdf/2508.02368", "abs": "https://arxiv.org/abs/2508.02368", "authors": ["Ronaldo A. Garcia", "Mark Helman", "Dan Reznik"], "title": "Poncelet triangles: two harmonious loci and two attractive envelopes", "categories": ["math.MG", "cs.GR", "51M04, 51N20, 51N35, 68T20"], "comment": "18 pages, 14 figures, 2 tables", "summary": "We prove that over a Poncelet triangle family interscribed between two nested\nellipses $\\E,\\E_c$, (i) the locus of the orthocenter is not only a conic, but\nit is axis-aligned and homothetic to a $90^o$-rotated copy of $\\E$, and (ii)\nthe locus of the isogonal conjugate of a fixed point $P$ is also a conic (the\nexpected degree was four); a parabola (resp. line) if $P$ is on the\n(degree-four) envelope of the circumcircle (resp. on $\\E$). We also show that\nthe envelope of both the circumcircle and radical axis of incircle and\ncircumcircle contain a conic component if and only if $\\E_c$ is a circle. The\nformer case is the union of two circles!", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\uff0c\u5728\u4e00\u5bf9\u5d4c\u5957\u692d\u5706\u4e4b\u95f4\u7684Poncelet\u4e09\u89d2\u5f62\u65cf\u4e2d\uff0c\u5782\u5fc3\u7684\u8f68\u8ff9\u4e0d\u4ec5\u662f\u4e00\u6761\u4e8c\u6b21\u66f2\u7ebf\uff0c\u800c\u4e14\u662f\u8f74\u5bf9\u9f50\u7684\uff0c\u5e76\u4e0e\u65cb\u8f6c90\u5ea6\u7684\u692d\u5706\u76f8\u4f3c\uff1b\u6b64\u5916\uff0c\u56fa\u5b9a\u70b9P\u7684\u7b49\u89d2\u5171\u8f6d\u8f68\u8ff9\u4e5f\u662f\u4e8c\u6b21\u66f2\u7ebf\u3002\u6b64\u5916\uff0c\u5f53\u5185\u692d\u5706\u662f\u5706\u65f6\uff0c\u5916\u63a5\u5706\u548c\u5185\u5916\u5fc3\u5171\u8f6d\u8f74\u7684\u5305\u7edc\u624d\u5305\u542b\u4e8c\u6b21\u66f2\u7ebf\u5206\u91cf\u3002", "motivation": "\u63a2\u8ba8Poncelet\u4e09\u89d2\u5f62\u65cf\u5728\u5d4c\u5957\u692d\u5706\u4e4b\u95f4\u7684\u51e0\u4f55\u6027\u8d28\u53ca\u5176\u8f68\u8ff9\u95ee\u9898\uff0c\u7279\u522b\u662f\u5782\u5fc3\u548c\u7b49\u89d2\u5171\u8f6d\u70b9\u7684\u8f68\u8ff9\u5f62\u72b6\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u548c\u4ee3\u6570\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8f68\u8ff9\u7684\u6027\u8d28\uff0c\u5305\u62ec\u8f74\u5bf9\u9f50\u3001\u76f8\u4f3c\u6027\u548c\u4e8c\u6b21\u66f2\u7ebf\u7279\u6027\u3002", "result": "\u5782\u5fc3\u7684\u8f68\u8ff9\u662f\u8f74\u5bf9\u9f50\u4e14\u4e0e\u65cb\u8f6c\u540e\u7684\u692d\u5706\u76f8\u4f3c\u7684\u4e8c\u6b21\u66f2\u7ebf\uff1b\u56fa\u5b9a\u70b9P\u7684\u7b49\u89d2\u5171\u8f6d\u8f68\u8ff9\u4e5f\u662f\u4e8c\u6b21\u66f2\u7ebf\uff1b\u7279\u5b9a\u6761\u4ef6\u4e0b\u5305\u7edc\u5305\u542b\u4e8c\u6b21\u66f2\u7ebf\u5206\u91cf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Poncelet\u4e09\u89d2\u5f62\u65cf\u5728\u7279\u5b9a\u51e0\u4f55\u914d\u7f6e\u4e0b\u7684\u8f68\u8ff9\u6027\u8d28\uff0c\u4e3a\u76f8\u5173\u51e0\u4f55\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u3002"}}
{"id": "2508.01763", "pdf": "https://arxiv.org/pdf/2508.01763", "abs": "https://arxiv.org/abs/2508.01763", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Reasoning Systems as Structured Processes: Foundations, Failures, and Formal Criteria", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "This paper outlines a general formal framework for reasoning systems,\nintended to support future analysis of inference architectures across domains.\nWe model reasoning systems as structured tuples comprising phenomena,\nexplanation space, inference and generation maps, and a principle base. The\nformulation accommodates logical, algorithmic, and learning-based reasoning\nprocesses within a unified structural schema, while remaining agnostic to any\nspecific reasoning algorithm or logic system. We survey basic internal\ncriteria--including coherence, soundness, and completeness-and catalog typical\nfailure modes such as contradiction, incompleteness, and non-convergence. The\nframework also admits dynamic behaviors like iterative refinement and principle\nevolution. The goal of this work is to establish a foundational structure for\nrepresenting and comparing reasoning systems, particularly in contexts where\ninternal failure, adaptation, or fragmentation may arise. No specific solution\narchitecture is proposed; instead, we aim to support future theoretical and\npractical investigations into reasoning under structural constraint.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u6bd4\u8f83\u8de8\u9886\u57df\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u672a\u6765\u5bf9\u63a8\u7406\u67b6\u6784\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u7814\u7a76\u3002", "motivation": "\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5bf9\u63a8\u7406\u7cfb\u7edf\u7684\u5206\u6790\u548c\u6bd4\u8f83\uff0c\u5c24\u5176\u662f\u5728\u5185\u90e8\u5931\u8d25\u3001\u9002\u5e94\u6216\u5206\u88c2\u53ef\u80fd\u53d1\u751f\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u5c06\u63a8\u7406\u7cfb\u7edf\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u5143\u7ec4\uff0c\u5305\u542b\u73b0\u8c61\u3001\u89e3\u91ca\u7a7a\u95f4\u3001\u63a8\u7406\u548c\u751f\u6210\u6620\u5c04\u4ee5\u53ca\u539f\u5219\u57fa\u7840\uff0c\u5e76\u7814\u7a76\u5176\u5185\u90e8\u6807\u51c6\u548c\u5178\u578b\u6545\u969c\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5bb9\u7eb3\u903b\u8f91\u3001\u7b97\u6cd5\u548c\u5b66\u4e60\u4e3a\u57fa\u7840\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u52a8\u6001\u884c\u4e3a\u5982\u8fed\u4ee3\u4f18\u5316\u548c\u539f\u5219\u6f14\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u7684\u7406\u8bba\u7814\u7a76\u548c\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5904\u7406\u63a8\u7406\u7cfb\u7edf\u7684\u7ed3\u6784\u7ea6\u675f\u65f6\u3002"}}
{"id": "2508.01244", "pdf": "https://arxiv.org/pdf/2508.01244", "abs": "https://arxiv.org/abs/2508.01244", "authors": ["Longlong Lin", "Yue He", "Wei Chen", "Pingpeng Yuan", "Rong-Hua Li", "Tao Jia"], "title": "Effective and Efficient Conductance-based Community Search at Billion Scale", "categories": ["cs.SI", "cs.DB"], "comment": null, "summary": "Community search is a widely studied semi-supervised graph clustering\nproblem, retrieving a high-quality connected subgraph containing the\nuser-specified query vertex. However, existing methods primarily focus on\ncohesiveness within the community but ignore the sparsity outside the\ncommunity, obtaining sub-par results. Inspired by this, we adopt the well-known\nconductance metric to measure the quality of a community and introduce a novel\nproblem of conductance-based community search (CCS). CCS aims at finding a\nsubgraph with the smallest conductance among all connected subgraphs that\ncontain the query vertex. We prove that the CCS problem is NP-hard. To\nefficiently query CCS, a four-stage subgraph-conductance-based community search\nalgorithm, SCCS, is proposed. Specifically, we first greatly reduce the entire\ngraph using local sampling techniques. Then, a three-stage local optimization\nstrategy is employed to continuously refine the community quality. Namely, we\nfirst utilize a seeding strategy to obtain an initial community to enhance its\ninternal cohesiveness. Then, we iteratively add qualified vertices in the\nexpansion stage to guarantee the internal cohesiveness and external sparsity of\nthe community. Finally, we gradually remove unqualified vertices during the\nverification stage. Extensive experiments on real-world datasets containing one\nbillion-scale graph and synthetic datasets show the effectiveness, efficiency,\nand scalability of our solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u7535\u5bfc\u7387\u7684\u793e\u533a\u641c\u7d22\u95ee\u9898\uff08CCS\uff09\uff0c\u65e8\u5728\u627e\u5230\u5305\u542b\u67e5\u8be2\u9876\u70b9\u7684\u8fde\u901a\u5b50\u56fe\u4e2d\u7535\u5bfc\u7387\u6700\u5c0f\u7684\u5b50\u56fe\uff0c\u5e76\u8bc1\u660e\u5176NP\u96be\u89e3\u6027\u3002", "motivation": "\u73b0\u6709\u793e\u533a\u641c\u7d22\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u793e\u533a\u5185\u90e8\u7684\u7d27\u5bc6\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u793e\u533a\u5916\u90e8\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSCCS\u7b97\u6cd5\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\uff08\u91c7\u6837\u3001\u64ad\u79cd\u3001\u6269\u5c55\u3001\u9a8c\u8bc1\uff09\u7b56\u7565\u4f18\u5316\u793e\u533a\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u56fe\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u3001\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CCS\u95ee\u9898\u548cSCCS\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u793e\u533a\u641c\u7d22\u4e2d\u5185\u5916\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00869", "pdf": "https://arxiv.org/pdf/2508.00869", "abs": "https://arxiv.org/abs/2508.00869", "authors": ["Dmitriy Kashitsyn", "Dmitriy Shabanov"], "title": "Discrete approach to machine learning", "categories": ["cs.LG", "cs.ET", "cs.IT", "math.IT"], "comment": "preprint, 52 pages, 37 figures", "summary": "The article explores an encoding and structural information processing\napproach using sparse bit vectors and fixed-length linear vectors. The\nfollowing are presented: a discrete method of speculative stochastic\ndimensionality reduction of multidimensional code and linear spaces with linear\nasymptotic complexity; a geometric method for obtaining discrete embeddings of\nan organised code space that reflect the internal structure of a given\nmodality. The structure and properties of a code space are investigated using\nthree modalities as examples: morphology of Russian and English languages, and\nimmunohistochemical markers. Parallels are drawn between the resulting map of\nthe code space layout and so-called pinwheels appearing on the mammalian\nneocortex. A cautious assumption is made about similarities between neocortex\norganisation and processes happening in our models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4f4d\u5411\u91cf\u548c\u56fa\u5b9a\u957f\u5ea6\u7ebf\u6027\u5411\u91cf\u7684\u7f16\u7801\u53ca\u7ed3\u6784\u4fe1\u606f\u5904\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u79bb\u6563\u7684\u968f\u673a\u7ef4\u5ea6\u7f29\u51cf\u65b9\u6cd5\u548c\u51e0\u4f55\u5d4c\u5165\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u6a21\u6001\u5c55\u793a\u4e86\u4ee3\u7801\u7a7a\u95f4\u7684\u7ed3\u6784\u548c\u7279\u6027\u3002", "motivation": "\u63a2\u7d22\u4ee3\u7801\u7a7a\u95f4\u7684\u7f16\u7801\u4e0e\u7ed3\u6784\u4fe1\u606f\u5904\u7406\uff0c\u901a\u8fc7\u7a00\u758f\u4f4d\u5411\u91cf\u548c\u7ebf\u6027\u5411\u91cf\u65b9\u6cd5\u63ed\u793a\u5176\u5185\u5728\u7279\u6027\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u7684\u968f\u673a\u7ef4\u5ea6\u7f29\u51cf\u65b9\u6cd5\u548c\u51e0\u4f55\u5d4c\u5165\u65b9\u6cd5\uff0c\u5206\u6790\u4e09\u79cd\u6a21\u6001\uff08\u4fc4\u8bed\u3001\u82f1\u8bed\u5f62\u6001\u5b66\u548c\u514d\u75ab\u7ec4\u5316\u6807\u8bb0\uff09\u7684\u4ee3\u7801\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u751f\u6210\u4e86\u4ee3\u7801\u7a7a\u95f4\u7684\u6620\u5c04\u56fe\uff0c\u5e76\u89c2\u5bdf\u5230\u5176\u4e0e\u54fa\u4e73\u52a8\u7269\u65b0\u76ae\u8d28\u4e2d\u7684\u201c\u98ce\u8f66\u7ed3\u6784\u201d\u76f8\u4f3c\uff0c\u63a8\u6d4b\u6a21\u578b\u4e2d\u7684\u8fc7\u7a0b\u4e0e\u65b0\u76ae\u8d28\u7ec4\u7ec7\u95f4\u5b58\u5728\u76f8\u4f3c\u6027\u3002", "conclusion": "\u521d\u6b65\u5047\u8bbe\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u5904\u7406\u4e0e\u54fa\u4e73\u52a8\u7269\u65b0\u76ae\u8d28\u7ec4\u7ec7\u6709\u6f5c\u5728\u76f8\u4f3c\u6027\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2508.02230", "pdf": "https://arxiv.org/pdf/2508.02230", "abs": "https://arxiv.org/abs/2508.02230", "authors": ["Yachao Yuan", "Zhen Yu", "Jin Wang", "Zhipeng Cheng", "Jianhua Hu"], "title": "FedAPTA: Federated Multi-task Learning in Computing Power Networks with Adaptive Layer-wise Pruning and Task-aware Aggregation", "categories": ["cs.DC"], "comment": null, "summary": "Federated Learning (FL) has shown considerable promise in Computing Power\nNetworks (CPNs) for privacy protection, efficient data utilization, and dynamic\ncollaboration. Although it offers practical benefits, applying FL in CPNs\ncontinues to encounter a major obstacle, i.e., multi-task deployment. However,\nexisting work mainly focuses on mitigating FL's computation and communication\noverhead of a single task while overlooking the computing resource wastage\nissue of heterogeneous devices across multiple tasks in FL under CPNs. To\ntackle this, we design FedAPTA, a federated multi-task learning framework in\nCPNs. FedAPTA alleviates computing resource wastage through the developed\nlayer-wise model pruning technique, which reduces local model size while\nconsidering both data and device heterogeneity. To aggregate structurally\nheterogeneous local models of different tasks, we introduce a heterogeneous\nmodel recovery strategy and a task-aware model aggregation method that enables\nthe aggregation through infilling local model architecture with the shared\nglobal model and clustering local models according to their specific tasks. We\ndeploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL\nmethods. The experimental outcomes demonstrate that the proposed FedAPTA\nconsiderably outperforms the state-of-the-art FL methods by up to 4.23%. Our\ncode is available at https://github.com/Zhenzovo/FedCPN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FedAPTA\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u8ba1\u7b97\u7f51\u7edc\u4e2d\u7684\u591a\u4efb\u52a1\u90e8\u7f72\u95ee\u9898\uff0c\u901a\u8fc7\u5c42\u95f4\u6a21\u578b\u4fee\u526a\u548c\u4efb\u52a1\u611f\u77e5\u6a21\u578b\u805a\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u591a\u4efb\u52a1\u90e8\u7f72\u4e2d\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u548c\u8bbe\u5907\u5f02\u6784\u6027\u6311\u6218\uff0c\u73b0\u6709\u5de5\u4f5c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "FedAPTA\u91c7\u7528\u5c42\u95f4\u6a21\u578b\u4fee\u526a\u6280\u672f\u51cf\u5c11\u672c\u5730\u6a21\u578b\u5927\u5c0f\uff0c\u5e76\u7ed3\u5408\u5f02\u6784\u6a21\u578b\u6062\u590d\u7b56\u7565\u548c\u4efb\u52a1\u611f\u77e5\u6a21\u578b\u805a\u5408\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedAPTA\u4f18\u4e8e\u73b0\u6709\u4e5d\u79cd\u5148\u8fdb\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe4.23%\u3002", "conclusion": "FedAPTA\u6709\u6548\u7f13\u89e3\u4e86\u591a\u4efb\u52a1\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02031", "pdf": "https://arxiv.org/pdf/2508.02031", "abs": "https://arxiv.org/abs/2508.02031", "authors": ["Tian Qin", "Guang Cheng", "Zihan Chen", "Yuyang Zhou"], "title": "PRIME: Plasticity-Robust Incremental Model for Encrypted Traffic Classification in Dynamic Network Environments", "categories": ["cs.NI"], "comment": null, "summary": "With the continuous development of network environments and technologies,\nensuring cyber security and governance is increasingly challenging. Network\ntraffic classification(ETC) can analyzes attributes such as application\ncategories and malicious intent, supporting network management services like\nQoS optimization, intrusion detection, and targeted billing. As the prevalence\nof traffic encryption increases, deep learning models are relied upon for\ncontent-agnostic analysis of packet sequences. However, the emergence of new\nservices and attack variants often leads to incremental tasks for ETC models.\nTo ensure model effectiveness, incremental learning techniques are essential;\nhowever, recent studies indicate that neural networks experience declining\nplasticity as tasks increase. We identified plasticity issues in existing\nincremental learning methods across diverse traffic samples and proposed the\nPRIME framework. By observing the effective rank of model parameters and the\nproportion of inactive neurons, the PRIME architecture can appropriately\nincrease the parameter scale when the model's plasticity deteriorates.\nExperiments show that in multiple encrypted traffic datasets and different\ncategory increment scenarios, the PRIME architecture performs significantly\nbetter than other incremental learning algorithms with minimal increase in\nparameter scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRIME\u6846\u67b6\uff0c\u89e3\u51b3\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u4e2d\u589e\u91cf\u5b66\u4e60\u6a21\u578b\u56e0\u4efb\u52a1\u589e\u52a0\u800c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u6574\u53c2\u6570\u89c4\u6a21\u548c\u4f18\u5316\u795e\u7ecf\u5143\u6d3b\u6027\u63d0\u5347\u6a21\u578b\u53ef\u5851\u6027\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u6d41\u91cf\u52a0\u5bc6\u589e\u52a0\u548c\u65b0\u670d\u52a1\u51fa\u73b0\uff0c\u73b0\u6709\u589e\u91cf\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u5851\u6027\u4e0b\u964d\uff0c\u5f71\u54cd\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faPRIME\u6846\u67b6\uff0c\u901a\u8fc7\u89c2\u5bdf\u6a21\u578b\u53c2\u6570\u6709\u6548\u79e9\u548c\u795e\u7ecf\u5143\u4e0d\u6d3b\u8dc3\u6bd4\u4f8b\uff0c\u52a8\u6001\u8c03\u6574\u53c2\u6570\u89c4\u6a21\u4ee5\u63d0\u5347\u6a21\u578b\u53ef\u5851\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cPRIME\u5728\u591a\u4e2a\u52a0\u5bc6\u6d41\u91cf\u6570\u636e\u96c6\u548c\u7c7b\u522b\u589e\u91cf\u573a\u666f\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u589e\u91cf\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e14\u53c2\u6570\u89c4\u6a21\u589e\u52a0\u6700\u5c0f\u3002", "conclusion": "PRIME\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u53ef\u5851\u6027\u95ee\u9898\uff0c\u4e3a\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02023", "pdf": "https://arxiv.org/pdf/2508.02023", "abs": "https://arxiv.org/abs/2508.02023", "authors": ["Huashan Lei", "Guanping Xiao", "Yepang Liu", "Zheng Zheng"], "title": "PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades", "categories": ["cs.SE"], "comment": "52 pages, 33 figures", "summary": "Python third-party libraries (TPLs) are essential in modern software\ndevelopment, but upgrades often cause compatibility issues, leading to system\nfailures. These issues fall into two categories: version compatibility issues\n(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect\ndependency conflicts but overlook code-level incompatibilities, with no\nsolution fully automating the inference of compatible versions for both VCIs\nand CCIs. To fill this gap, we propose PCREQ, the first approach to\nautomatically infer compatible requirements by combining version and code\ncompatibility analysis. PCREQ integrates six modules: knowledge acquisition,\nversion compatibility assessment, invoked APIs and modules extraction, code\ncompatibility assessment, version change, and missing TPL completion. PCREQ\ncollects candidate versions, checks for conflicts, identifies API usage,\nevaluates code compatibility, and iteratively adjusts versions to generate a\ncompatible requirements.txt with a detailed repair report. To evaluate PCREQ,\nwe construct REQBench, a large-scale benchmark with 2,095 upgrade test cases\n(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%\ninference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and\nLLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each\ncase from REQBench in 60.79s on average, demonstrating practical efficiency.\nPCREQ significantly reduces manual effort in troubleshooting upgrades,\nadvancing Python dependency maintenance automation.", "AI": {"tldr": "PCREQ\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u89e3\u51b3Python\u7b2c\u4e09\u65b9\u5e93\u5347\u7ea7\u4e2d\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5305\u62ec\u7248\u672c\u548c\u4ee3\u7801\u517c\u5bb9\u6027\uff0c\u6210\u529f\u7387\u8fbe94.03%\u3002", "motivation": "Python\u7b2c\u4e09\u65b9\u5e93\u5347\u7ea7\u5e38\u5bfc\u81f4\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u5b8c\u5168\u81ea\u52a8\u5316\u89e3\u51b3\u7248\u672c\u548c\u4ee3\u7801\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981PCREQ\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PCREQ\u6574\u5408\u516d\u4e2a\u6a21\u5757\uff0c\u901a\u8fc7\u7248\u672c\u548c\u4ee3\u7801\u517c\u5bb9\u6027\u5206\u6790\u3001\u8fed\u4ee3\u8c03\u6574\u7248\u672c\uff0c\u751f\u6210\u517c\u5bb9\u7684requirements.txt\u3002", "result": "PCREQ\u5728REQBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u8fbe94.03%\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "PCREQ\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u89e3\u51b3\u5347\u7ea7\u95ee\u9898\u7684\u52aa\u529b\uff0c\u63a8\u52a8\u4e86Python\u4f9d\u8d56\u7ef4\u62a4\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2508.01092", "pdf": "https://arxiv.org/pdf/2508.01092", "abs": "https://arxiv.org/abs/2508.01092", "authors": ["Maryam Cheema", "Sina Elahimanesh", "Samuel Martin", "Pooyan Fazli", "Hasti Seifi"], "title": "DescribePro: Collaborative Audio Description with Human-AI Interaction", "categories": ["cs.HC"], "comment": "ASSETS 25 19 pages, 8 figures", "summary": "Audio description (AD) makes video content accessible to millions of blind\nand low vision (BLV) users. However, creating high-quality AD involves a\ntrade-off between the precision of human-crafted descriptions and the\nefficiency of AI-generated ones. To address this, we present DescribePro a\ncollaborative AD authoring system that enables describers to iteratively refine\nAI-generated descriptions through multimodal large language model prompting and\nmanual editing. DescribePro also supports community collaboration by allowing\nusers to fork and edit existing ADs, enabling the exploration of different\nnarrative styles. We evaluate DescribePro with 18 describers (9 professionals\nand 9 novices) using quantitative and qualitative methods. Results show that AI\nsupport reduces repetitive work while helping professionals preserve their\nstylistic choices and easing the cognitive load for novices. Collaborative tags\nand variations show potential for providing customizations, version control,\nand training new describers. These findings highlight the potential of\ncollaborative, AI-assisted tools to enhance and scale AD authorship.", "AI": {"tldr": "DescribePro\u662f\u4e00\u4e2a\u534f\u4f5c\u5f0f\u97f3\u9891\u63cf\u8ff0\uff08AD\uff09\u521b\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408AI\u751f\u6210\u4e0e\u4eba\u5de5\u7f16\u8f91\uff0c\u63d0\u9ad8AD\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u7cbe\u786e\u63cf\u8ff0\u4e0eAI\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u5347BLV\u7528\u6237\u7684\u89c6\u9891\u5185\u5bb9\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u548c\u624b\u52a8\u7f16\u8f91\uff0c\u652f\u6301\u7528\u6237\u8fed\u4ee3\u4f18\u5316AI\u751f\u6210\u7684\u63cf\u8ff0\uff0c\u5e76\u5141\u8bb8\u793e\u533a\u534f\u4f5c\u7f16\u8f91\u3002", "result": "AI\u652f\u6301\u51cf\u5c11\u91cd\u590d\u5de5\u4f5c\uff0c\u4fdd\u7559\u4e13\u4e1a\u7528\u6237\u7684\u98ce\u683c\u9009\u62e9\uff0c\u964d\u4f4e\u65b0\u624b\u8ba4\u77e5\u8d1f\u62c5\uff1b\u534f\u4f5c\u529f\u80fd\u6709\u52a9\u4e8e\u5b9a\u5236\u5316\u548c\u57f9\u8bad\u3002", "conclusion": "\u534f\u4f5c\u5f0fAI\u5de5\u5177\u80fd\u589e\u5f3a\u5e76\u6269\u5927AD\u521b\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.00906", "pdf": "https://arxiv.org/pdf/2508.00906", "abs": "https://arxiv.org/abs/2508.00906", "authors": ["Subaveerapandiyan A", "Pragya Lohia", "Dattatraya Kalbande", "Naved Ahmad", "Kailash Chand Sharma"], "title": "Exploring the Role of Gamification in Enhancing Academic Library Services: A Survey of Library Leaders in India", "categories": ["cs.CY", "cs.DL", "cs.ET"], "comment": "The final published version will appear in College & Research\n  Libraries, March 2026", "summary": "This study explores the role of gamification in enhancing academic library\nservices in India by surveying library leaders across various institutions.\nUsing game-like elements in non-game contexts, gamification can boost user\nengagement and improve services such as information literacy and research\nconsultations. Findings reveal moderate awareness and generally positive\nperceptions of gamification's effectiveness. However, challenges like\ninsufficient staff expertise, infrastructure, and limited funding hinder\nimplementation. The study emphasises the need for additional resources,\nincluding staff training and technological upgrades, to unlock the full\npotential of gamification in academic libraries.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6e38\u620f\u5316\u5728\u5370\u5ea6\u5b66\u672f\u56fe\u4e66\u9986\u670d\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u867d\u80fd\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u56e0\u8d44\u6e90\u4e0d\u8db3\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u6e38\u620f\u5316\u5982\u4f55\u63d0\u5347\u5b66\u672f\u56fe\u4e66\u9986\u670d\u52a1\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6548\u679c\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u5370\u5ea6\u5404\u673a\u6784\u7684\u56fe\u4e66\u9986\u9886\u5bfc\u8005\uff0c\u5206\u6790\u6e38\u620f\u5316\u5143\u7d20\u7684\u5e94\u7528\u548c\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6e38\u620f\u5316\u7684\u63a5\u53d7\u5ea6\u8f83\u9ad8\uff0c\u4f46\u53d7\u9650\u4e8e\u8d44\u6e90\u4e0d\u8db3\uff08\u5982\u6280\u672f\u3001\u8d44\u91d1\u3001\u57f9\u8bad\uff09\u3002", "conclusion": "\u5efa\u8bae\u589e\u52a0\u8d44\u6e90\u6295\u5165\uff08\u5982\u57f9\u8bad\u548c\u6280\u672f\u5347\u7ea7\uff09\uff0c\u4ee5\u5145\u5206\u53d1\u6325\u6e38\u620f\u5316\u6f5c\u529b\u3002"}}
{"id": "2508.01995", "pdf": "https://arxiv.org/pdf/2508.01995", "abs": "https://arxiv.org/abs/2508.01995", "authors": ["Sefatun-Noor Puspa", "Mashrur Chowdhury"], "title": "GPU in the Blind Spot: Overlooked Security Risks in Transportation", "categories": ["cs.CR", "cs.AR"], "comment": null, "summary": "Graphics processing units (GPUs) are becoming an essential part of the\nintelligent transportation system (ITS) for enabling video-based and artificial\nintelligence (AI) based applications. GPUs provide high-throughput and\nenergy-efficient computing for tasks like sensor fusion and roadside video\nanalytics. However, these GPUs are one of the most unmonitored components in\nterms of security. This makes them vulnerable to cyber and hardware attacks,\nincluding unauthorized crypto mining. This paper highlights GPU security as a\ncritical blind spot in transportation cybersecurity. To support this concern,\nit also presents a case study showing the impact of stealthy unauthorized\ncrypto miners on critical AI workloads, along with a detection strategy. We\nused a YOLOv8-based video processing pipeline running on an RTX 2060 GPU for\nthe case study. A multi-streaming application was executed while a T-Rex crypto\nminer ran in the background. We monitored how the miner degraded GPU\nperformance by reducing the frame rate and increasing power consumption, which\ncould be a serious concern for GPUs operating in autonomous vehicles or\nbattery-powered edge devices. We observed measurable impacts using GPU\ntelemetry (nvidia-smi) and Nsight Compute profiling, where frame rate dropped\nby 50 percent, and power usage increased by up to 90%. To detect, we trained\nlightweight classifiers using extracted telemetry features. All models achieved\nhigh accuracy, precision, recall, and F1-score. This paper raises urgent\nawareness about GPU observability gaps in ITS and offers a replicable framework\nfor detecting GPU misuse through on-device telemetry.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86GPU\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u76f2\u70b9\uff0c\u63ed\u793a\u4e86\u672a\u7ecf\u6388\u6743\u52a0\u5bc6\u6316\u77ff\u5bf9AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bbe\u5907\u9065\u6d4b\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "GPU\u56e0\u5176\u9ad8\u6027\u80fd\u548c\u80fd\u6548\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5b89\u5168\u6027\u5e38\u88ab\u5ffd\u89c6\uff0c\u6613\u53d7\u653b\u51fb\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u52a0\u5bc6\u6316\u77ff\u5bf9YOLOv8\u89c6\u9891\u5904\u7406\u7ba1\u9053\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u5e76\u5229\u7528GPU\u9065\u6d4b\u6570\u636e\u8bad\u7ec3\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u52a0\u5bc6\u6316\u77ff\u5bfc\u81f4\u5e27\u7387\u4e0b\u964d50%\uff0c\u529f\u8017\u589e\u52a090%\uff0c\u800c\u5206\u7c7b\u5668\u5728\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u53ec\u56de\u7387\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u52a0\u5f3a\u5bf9GPU\u5b89\u5168\u7684\u76d1\u63a7\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2508.02309", "pdf": "https://arxiv.org/pdf/2508.02309", "abs": "https://arxiv.org/abs/2508.02309", "authors": ["Yilong Zhao", "Mingyu Gao", "Huanchen Zhang", "Fangxin Liu", "Gongye Chen", "He Xian", "Haibing Guan", "Li Jiang"], "title": "PUSHtap: PIM-based In-Memory HTAP with Unified Data Storage Format", "categories": ["cs.DC"], "comment": "Accepted by the 2025 International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025) The\n  paper will be presented at ASPLOS 2026", "summary": "Hybrid transaction/analytical processing (HTAP) is an emerging database\nparadigm that supports both online transaction processing (OLTP) and online\nanalytical processing (OLAP) workloads. Computing-intensive OLTP operations,\ninvolving row-wise data manipulation, are suitable for row-store format. In\ncontrast, memory-intensive OLAP operations, which are column-centric, benefit\nfrom column-store format. This \\emph{data-format dilemma} prevents HTAP systems\nfrom concurrently achieving three design goals: performance isolation, data\nfreshness, and workload-specific optimization. Another background technology is\nProcessing-in-Memory (PIM), which integrates computing units (PIM units) inside\nDRAM memory devices to accelerate memory-intensive workloads, including OLAP.\n  Our key insight is to combine the interleaved CPU access and localized PIM\nunit access to provide two-dimensional access to address the data format\ncontradictions inherent in HTAP. First, we propose a unified data storage\nformat with novel data alignment and placement techniques to optimize the\neffective bandwidth of CPUs and PIM units and exploit the PIM's parallelism.\nSecond, we implement the multi-version concurrency control (MVCC) essential for\nsingle-instance HTAP. Third, we extend the commercial PIM architecture to\nsupport the OLAP operations and concurrent access from PIM and CPU. Experiments\nshow that PUSHtap can achieve 3.4\\texttimes{}/4.4\\texttimes{} OLAP/OLTP\nthroughput improvement compared to multi-instance PIM-based design.", "AI": {"tldr": "PUSHtap\u901a\u8fc7\u7ed3\u5408CPU\u548cPIM\u5355\u5143\u7684\u4e8c\u7ef4\u8bbf\u95ee\u65b9\u5f0f\uff0c\u89e3\u51b3\u4e86HTAP\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u683c\u5f0f\u77db\u76fe\uff0c\u63d0\u5347\u4e86OLAP/OLTP\u6027\u80fd\u3002", "motivation": "HTAP\u7cfb\u7edf\u5728\u5904\u7406OLTP\u548cOLAP\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u9762\u4e34\u6570\u636e\u683c\u5f0f\u7684\u77db\u76fe\uff0c\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u6027\u80fd\u9694\u79bb\u3001\u6570\u636e\u65b0\u9c9c\u5ea6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u6570\u636e\u5b58\u50a8\u683c\u5f0f\uff0c\u91c7\u7528\u6570\u636e\u5bf9\u9f50\u548c\u653e\u7f6e\u6280\u672f\uff0c\u4f18\u5316CPU\u548cPIM\u5355\u5143\u7684\u5e26\u5bbd\u5229\u7528\uff0c\u5e76\u6269\u5c55PIM\u67b6\u6784\u4ee5\u652f\u6301OLAP\u64cd\u4f5c\u548c\u5e76\u53d1\u8bbf\u95ee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPUSHtap\u76f8\u6bd4\u57fa\u4e8e\u591a\u5b9e\u4f8bPIM\u7684\u8bbe\u8ba1\uff0cOLAP/OLTP\u541e\u5410\u91cf\u63d0\u9ad8\u4e863.4\u500d/4.4\u500d\u3002", "conclusion": "PUSHtap\u901a\u8fc7\u521b\u65b0\u6570\u636e\u8bbf\u95ee\u65b9\u5f0f\u548c\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86HTAP\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.02282", "pdf": "https://arxiv.org/pdf/2508.02282", "abs": "https://arxiv.org/abs/2508.02282", "authors": ["Ziyue Huang", "Chungang Lin", "Weiyao Zhang", "Xuying Meng", "Yujun Zhang"], "title": "Distillation-Enhanced Clustering Acceleration for Encrypted Traffic Classification", "categories": ["cs.NI"], "comment": "Under review", "summary": "Traffic classification plays a significant role in network service\nmanagement. The advancement of deep learning has established pretrained models\nas a robust approach for this task. However, contemporary encrypted traffic\nclassification systems face dual limitations. Firstly, pretrained models\ntypically exhibit large-scale architectures, where their extensive\nparameterization results in slow inference speeds and high computational\nlatency. Secondly, reliance on labeled data for fine-tuning restricts these\nmodels to predefined supervised classes, creating a bottleneck when novel\ntraffic types emerge in the evolving Internet landscape. To address these\nchallenges, we propose NetClus, a novel framework integrating pretrained models\nwith distillation-enhanced clustering acceleration. During fine-tuning, NetClus\nfirst introduces a cluster-friendly loss to jointly reshape the latent space\nfor both classification and clustering. With the fine-tuned model, it distills\nthe model into a lightweight Feed-Forward Neural Network model to retain\nsemantics. During inference, NetClus performs heuristic merge with near-linear\nruntime, and valid the cluster purity with newly proposed metrics ASI to\nidentify emergent traffic types while expediting classification. Benchmarked\nagainst existing pretrained methods, NetClus achieves up to 6.2x acceleration\nwhile maintaining classification degradation below 1%.", "AI": {"tldr": "NetClus\u6846\u67b6\u901a\u8fc7\u84b8\u998f\u589e\u5f3a\u805a\u7c7b\u52a0\u901f\uff0c\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u6162\u548c\u5904\u7406\u65b0\u6d41\u91cf\u7c7b\u578b\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u7cfb\u7edf\u5b58\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u8fc7\u591a\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u4ee5\u53ca\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u9650\u5236\u6a21\u578b\u9002\u5e94\u65b0\u6d41\u91cf\u7c7b\u578b\u7684\u95ee\u9898\u3002", "method": "NetClus\u63d0\u51fa\u4e00\u79cd\u6846\u67b6\uff0c\u7ed3\u5408\u805a\u7c7b\u53cb\u597d\u635f\u5931\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u538b\u7f29\u6a21\u578b\u4e3a\u8f7b\u91cf\u7ea7\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u540c\u65f6\u4f7f\u7528\u542f\u53d1\u5f0f\u5408\u5e76\u548cASI\u6307\u6807\u9a8c\u8bc1\u805a\u7c7b\u7eaf\u5ea6\u3002", "result": "NetClus\u5b9e\u73b0\u6700\u9ad86.2\u500d\u52a0\u901f\uff0c\u4e14\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\u4f4e\u4e8e1%\u3002", "conclusion": "NetClus\u6709\u6548\u89e3\u51b3\u4e86\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.02144", "pdf": "https://arxiv.org/pdf/2508.02144", "abs": "https://arxiv.org/abs/2508.02144", "authors": ["Yusaku Kato", "Norihiro Yoshida", "Erina Makihara", "Katsuro Inoue"], "title": "BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games", "categories": ["cs.SE"], "comment": "4 pages, 5 figures", "summary": "Open-world video games present a broader search space than other games,\nposing challenges for test automation. Fuzzing, which generates new inputs by\nmutating an initial input, is commonly used to uncover failures. In this study,\nwe proposed BiFuzz, a two-stage fuzzer designed for automated testing of\nopen-world video games, and investigated its effectiveness. The results\nrevealed that BiFuzz mutated the overall strategy of gameplay and test cases,\nincluding actual movement paths, step by step. Consequently, BiFuzz can detect\n`stucking' failures. The tool and its video are at\nhttps://github.com/Yusaku-Kato/BiFuzz.", "AI": {"tldr": "BiFuzz\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u89c6\u9891\u6e38\u620f\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6e38\u620f\u4e2d\u7684\u5361\u987f\u6545\u969c\u3002", "motivation": "\u5f00\u653e\u4e16\u754c\u89c6\u9891\u6e38\u620f\u7684\u641c\u7d22\u7a7a\u95f4\u8f83\u5927\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u81ea\u52a8\u5316\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "BiFuzz\u901a\u8fc7\u9010\u6b65\u53d8\u5f02\u6e38\u620f\u7b56\u7565\u548c\u6d4b\u8bd5\u7528\u4f8b\uff08\u5305\u62ec\u5b9e\u9645\u79fb\u52a8\u8def\u5f84\uff09\u6765\u751f\u6210\u65b0\u8f93\u5165\u3002", "result": "BiFuzz\u80fd\u591f\u6210\u529f\u68c0\u6d4b\u6e38\u620f\u4e2d\u7684\u5361\u987f\u6545\u969c\u3002", "conclusion": "BiFuzz\u4e3a\u5f00\u653e\u4e16\u754c\u6e38\u620f\u7684\u6d4b\u8bd5\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01110", "pdf": "https://arxiv.org/pdf/2508.01110", "abs": "https://arxiv.org/abs/2508.01110", "authors": ["Ezequiel Santos"], "title": "Cross-Device Motion Interaction via Apple's Native System Frameworks", "categories": ["cs.HC"], "comment": null, "summary": "We introduce an open-source, fully offline pipeline that transforms a\nconsumer-grade iPhone into a motion controller with real-time tactile feedback,\nusing only native Apple frameworks. Designed for rapid prototyping and applied\nmobile HCI scenarios, the system integrates CoreMotion for inertial sensing,\nMultipeerConnectivity for peer-to-peer data transmission at 10 Hz, and\nCoreHaptics for immediate tactile confirmation. A built-in logger captures\nend-to-end latency without requiring clock synchronization, yielding a mean\ndelay of 70.4 ms and 95th percentile below 74 ms on typical 5 GHz Wi-Fi (-55\ndBm RSSI). We validated the pipeline through a real-time demonstrator game,\nKeepCalm, deployed during a public event with 21 participants. Results showed\nstable connections, zero packet loss, and negligible power impact (24 mW on\niPhone 13 mini). With fewer than 500 lines of Swift code and no reliance on\ncloud infrastructure, this system provides a compact, reproducible foundation\nfor embodied interaction research, casual games, and offline educational tools.\nAll source code, latency logs, and provisioning scripts are openly released\nunder an MIT license.", "AI": {"tldr": "\u5c06iPhone\u8f6c\u5316\u4e3a\u5e26\u89e6\u89c9\u53cd\u9988\u7684\u5b9e\u65f6\u8fd0\u52a8\u63a7\u5236\u5668\uff0c\u4ec5\u4f7f\u7528\u82f9\u679c\u539f\u751f\u6846\u67b6\uff0c\u5e73\u5747\u5ef6\u8fdf70.4ms\uff0c\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u53ca\u79fb\u52a8HCI\u5e94\u7528\u3002", "motivation": "\u4e3a\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u79fb\u52a8HCI\u5e94\u7528\u573a\u666f\uff0c\u63d0\u4f9b\u4e00\u4e2a\u65e0\u9700\u4e91\u57fa\u7840\u8bbe\u65bd\u3001\u5b8c\u5168\u79bb\u7ebf\u7684\u5b9e\u65f6\u8fd0\u52a8\u63a7\u5236\u5668\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6574\u5408CoreMotion\u8fdb\u884c\u60ef\u6027\u4f20\u611f\uff0cMultipeerConnectivity\u5b9e\u73b010Hz\u7684P2P\u6570\u636e\u4f20\u8f93\uff0cCoreHaptics\u63d0\u4f9b\u5373\u65f6\u89e6\u89c9\u53cd\u9988\uff0c\u5e76\u5185\u7f6e\u8bb0\u5f55\u5668\u6d4b\u91cf\u5ef6\u8fdf\u3002", "result": "\u5e73\u5747\u5ef6\u8fdf70.4ms\uff0c95%\u5ef6\u8fdf\u4f4e\u4e8e74ms\uff0c21\u4eba\u53c2\u4e0e\u7684\u516c\u5f00\u6d3b\u52a8\u4e2d\u9a8c\u8bc1\u96f6\u4e22\u5305\u3001\u8fde\u63a5\u7a33\u5b9a\uff0c\u529f\u8017\u4ec524mW\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5177\u8eab\u4ea4\u4e92\u7814\u7a76\u3001\u4f11\u95f2\u6e38\u620f\u53ca\u79bb\u7ebf\u6559\u80b2\u5de5\u5177\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u5f00\u6e90MIT\u8bb8\u53ef\u3002"}}
{"id": "2508.01871", "pdf": "https://arxiv.org/pdf/2508.01871", "abs": "https://arxiv.org/abs/2508.01871", "authors": ["Yuanyuan Liang", "Lei Pan", "Tingyu Xie", "Yunshi Lan", "Weining Qian"], "title": "Multi-turn Natural Language to Graph Query Language Translation", "categories": ["cs.AI", "cs.DB"], "comment": "21 pages", "summary": "In recent years, research on transforming natural language into graph query\nlanguage (NL2GQL) has been increasing. Most existing methods focus on\nsingle-turn transformation from NL to GQL. In practical applications, user\ninteractions with graph databases are typically multi-turn, dynamic, and\ncontext-dependent. While single-turn methods can handle straightforward\nqueries, more complex scenarios often require users to iteratively adjust their\nqueries, investigate the connections between entities, or request additional\ndetails across multiple dialogue turns. Research focused on single-turn\nconversion fails to effectively address multi-turn dialogues and complex\ncontext dependencies. Additionally, the scarcity of high-quality multi-turn\nNL2GQL datasets further hinders the progress of this field. To address this\nchallenge, we propose an automated method for constructing multi-turn NL2GQL\ndatasets based on Large Language Models (LLMs) , and apply this method to\ndevelop the MTGQL dataset, which is constructed from a financial market graph\ndatabase and will be publicly released for future research. Moreover, we\npropose three types of baseline methods to assess the effectiveness of\nmulti-turn NL2GQL translation, thereby laying a solid foundation for future\nresearch.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u6784\u5efa\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86MTGQL\u6570\u636e\u96c6\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e09\u79cd\u57fa\u7ebf\u65b9\u6cd5\u8bc4\u4f30\u591a\u8f6e\u8f6c\u6362\u6548\u679c\u3002", "motivation": "\u73b0\u6709NL2GQL\u65b9\u6cd5\u591a\u4e3a\u5355\u8f6e\u8f6c\u6362\uff0c\u96be\u4ee5\u5904\u7406\u591a\u8f6e\u52a8\u6001\u5bf9\u8bdd\u548c\u590d\u6742\u4e0a\u4e0b\u6587\u4f9d\u8d56\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u591a\u8f6e\u6570\u636e\u96c6\u3002", "method": "\u5229\u7528LLM\u81ea\u52a8\u6784\u5efa\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\uff08MTGQL\uff09\uff0c\u5e76\u63d0\u4f9b\u4e09\u79cd\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u57fa\u4e8e\u91d1\u878d\u5e02\u573a\u7684MTGQL\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u591a\u8f6eNL2GQL\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u67e5\u8be2\u8f6c\u6362\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.01285", "pdf": "https://arxiv.org/pdf/2508.01285", "abs": "https://arxiv.org/abs/2508.01285", "authors": ["Yujing Ke", "Kevin George", "Kathan Pandya", "David Blumenthal", "Maximilian Sprang", "Gerrit Gro\u00dfmann", "Sebastian Vollmer", "David Antony Selby"], "title": "BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation", "categories": ["cs.AI", "cs.ET", "cs.IR", "stat.AP"], "comment": "7 pages main content + 11 pages appendices", "summary": "Identifying novel hypotheses is essential to scientific research, yet this\nprocess risks being overwhelmed by the sheer volume and complexity of available\ninformation. Existing automated methods often struggle to generate novel and\nevidence-grounded hypotheses, lack robust iterative refinement and rarely\nundergo rigorous temporal evaluation for future discovery potential. To address\nthis, we propose BioDisco, a multi-agent framework that draws upon language\nmodel-based reasoning and a dual-mode evidence system (biomedical knowledge\ngraphs and automated literature retrieval) for grounded novelty, integrates an\ninternal scoring and feedback loop for iterative refinement, and validates\nperformance through pioneering temporal and human evaluations and a\nBradley-Terry paired comparison model to provide statistically-grounded\nassessment. Our evaluations demonstrate superior novelty and significance over\nablated configurations representative of existing agentic architectures.\nDesigned for flexibility and modularity, BioDisco allows seamless integration\nof custom language models or knowledge graphs, and can be run with just a few\nlines of code. We anticipate researchers using this practical tool as a\ncatalyst for the discovery of new hypotheses.", "AI": {"tldr": "BioDisco\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u53cc\u6a21\u5f0f\u8bc1\u636e\u7cfb\u7edf\u751f\u6210\u65b0\u9896\u4e14\u57fa\u4e8e\u8bc1\u636e\u7684\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u65f6\u95f4\u9a8c\u8bc1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u4e2d\u65b0\u5047\u8bbe\u7684\u53d1\u73b0\u5e38\u56e0\u4fe1\u606f\u91cf\u5927\u4e14\u590d\u6742\u800c\u53d7\u9650\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u751f\u6210\u65b0\u9896\u4e14\u57fa\u4e8e\u8bc1\u636e\u7684\u5047\u8bbe\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u65f6\u95f4\u9a8c\u8bc1\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u53cc\u6a21\u5f0f\u8bc1\u636e\u7cfb\u7edf\uff08\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u732e\u81ea\u52a8\u68c0\u7d22\uff09\uff0c\u7ed3\u5408\u5185\u90e8\u8bc4\u5206\u548c\u53cd\u9988\u5faa\u73af\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u548c\u4eba\u7c7b\u8bc4\u4f30\u53caBradley-Terry\u914d\u5bf9\u6bd4\u8f83\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "BioDisco\u5728\u65b0\u9896\u6027\u548c\u663e\u8457\u6027\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u7075\u6d3b\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u81ea\u5b9a\u4e49\u8bed\u8a00\u6a21\u578b\u6216\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "BioDisco\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u9ad8\u6548\u53d1\u73b0\u65b0\u5047\u8bbe\u3002"}}
{"id": "2508.02446", "pdf": "https://arxiv.org/pdf/2508.02446", "abs": "https://arxiv.org/abs/2508.02446", "authors": ["Yichao Zhang", "Zexin Fu", "Tim Fischer", "Yinrong Li", "Marco Bertuletti", "Luca Benini"], "title": "TeraNoC: A Multi-Channel 32-bit Fine-Grained, Hybrid Mesh-Crossbar NoC for Efficient Scale-up of 1000+ Core Shared-L1-Memory Clusters", "categories": ["cs.DC"], "comment": "8 pages, 9 figures. Proceeded by The 43rd IEEE International\n  Conference on Computer Design (ICCD 2025)", "summary": "A key challenge in on-chip interconnect design is to scale up bandwidth while\nmaintaining low latency and high area efficiency. 2D-meshes scale with low\nwiring area and congestion overhead; however, their end-to-end latency\nincreases with the number of hops, making them unsuitable for latency-sensitive\ncore-to-L1-memory access. On the other hand, crossbars offer low latency, but\ntheir routing complexity grows quadratically with the number of I/Os, requiring\nlarge physical routing resources and limiting area-efficient scalability. This\ntwo-sided interconnect bottleneck hinders the scale-up of many-core,\nlow-latency, tightly coupled shared-memory clusters, pushing designers toward\ninstantiating many smaller and loosely coupled clusters, at the cost of\nhardware and software overheads. We present TeraNoC, an open-source, hybrid\nmesh-crossbar on-chip interconnect that offers both scalability and low\nlatency, while maintaining very low routing overhead. The topology, built on\n32bit word-width multi-channel 2D-meshes and crossbars, enables the\narea-efficient scale-up of shared-memory clusters. A router remapper is\ndesigned to balance traffic load across interconnect channels. Using TeraNoC,\nwe build a cluster with 1024 single-stage, single-issue cores that share a\n4096-banked L1 memory, implemented in 12nm technology. The low interconnect\nstalls enable high compute utilization of up to 0.85 IPC in compute-intensive,\ndata-parallel key GenAI kernels. TeraNoC only consumes 7.6\\% of the total\ncluster power in kernels dominated by crossbar accesses, and 22.7\\% in kernels\nwith high 2D-mesh traffic. Compared to a hierarchical crossbar-only cluster,\nTeraNoC reduces die area by 37.8\\% and improves area efficiency (GFLOP/s/mm2)\nby up to 98.7\\%, while occupying only 10.9\\% of the logic area.", "AI": {"tldr": "TeraNoC\u662f\u4e00\u79cd\u6df7\u5408\u7f51\u683c-\u4ea4\u53c9\u5f00\u5173\u7247\u4e0a\u4e92\u8fde\u6280\u672f\uff0c\u517c\u5177\u53ef\u6269\u5c55\u6027\u548c\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8def\u7531\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u7247\u4e0a\u4e92\u8fde\u8bbe\u8ba1\u4e2d\u5e26\u5bbd\u6269\u5c55\u4e0e\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u9762\u79ef\u6548\u7387\u7684\u77db\u76fe\u95ee\u9898\u3002", "method": "\u7ed3\u540832\u4f4d\u5b57\u5bbd\u591a\u901a\u90532D\u7f51\u683c\u548c\u4ea4\u53c9\u5f00\u5173\u62d3\u6251\uff0c\u8bbe\u8ba1\u8def\u7531\u5668\u91cd\u6620\u5c04\u5668\u4ee5\u5e73\u8861\u6d41\u91cf\u3002", "result": "\u572812nm\u6280\u672f\u4e2d\u5b9e\u73b01024\u6838\u96c6\u7fa4\uff0cIPC\u8fbe0.85\uff0c\u529f\u8017\u548c\u9762\u79ef\u6548\u7387\u663e\u8457\u4f18\u4e8e\u7eaf\u4ea4\u53c9\u5f00\u5173\u8bbe\u8ba1\u3002", "conclusion": "TeraNoC\u4e3a\u591a\u6838\u4f4e\u5ef6\u8fdf\u5171\u4eab\u5185\u5b58\u96c6\u7fa4\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02373", "pdf": "https://arxiv.org/pdf/2508.02373", "abs": "https://arxiv.org/abs/2508.02373", "authors": ["Iulisloi Zacarias", "Oussama Ben Taarit", "Admela Jukan"], "title": "On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)", "categories": ["cs.NI", "cs.DC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Future networks, such as 6G, will need to support a vast and diverse range of\ninterconnected devices and applications, each with its own set of requirements.\nWhile traditional network management approaches will suffice, an automated\nsolutions are becoming a must. However, network automation frameworks are prone\nto errors, and often they employ ML-based techniques that require training to\nlearn how the network can be optimized. In this sense, network digital twins\nare a useful tool that allows for the simulation, testing, and training of AI\nmodels without affecting the real-world networks and users. This paper presents\nan AI-based Network Digital Twin (AI-NDT) that leverages a multi-layered\nknowledge graph architecture and graph neural networks to predict network\nmetrics that directly affect the quality of experience of users. An evaluation\nof the four most prominent Graph Neural Networks (GNN) architectures was\nconducted to assess their effectiveness in developing network digital twins. We\ntrained the digital twin on publicly available measurement data from RIPE\nAtlas, therefore obtaining results close to what is expected in real-world\napplications. The results show that among the four architectures evaluated,\nGraphTransformer presents the best performance. However, other architectures\nmight fit better in scenarios where shorter training time is important, while\nalso delivering acceptable results. The results of this work are indicative of\nwhat might become common practice for proactive network management, offering a\nscalable and accurate solution aligned with the requirements of the\nnext-generation networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\uff08AI-NDT\uff09\uff0c\u5229\u7528\u591a\u5c42\u77e5\u8bc6\u56fe\u8c31\u548cGNN\u9884\u6d4b\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u7684\u7f51\u7edc\u6307\u6807\uff0c\u8bc4\u4f30\u4e86\u56db\u79cdGNN\u67b6\u6784\uff0c\u5176\u4e2dGraphTransformer\u8868\u73b0\u6700\u4f73\u3002", "motivation": "6G\u7b49\u672a\u6765\u7f51\u7edc\u9700\u652f\u6301\u591a\u6837\u5316\u8bbe\u5907\u548c\u5e94\u7528\uff0c\u4f20\u7edf\u7f51\u7edc\u7ba1\u7406\u65b9\u6cd5\u4e0d\u8db3\uff0c\u6570\u5b57\u5b6a\u751f\u80fd\u65e0\u98ce\u9669\u5730\u8bad\u7ec3AI\u6a21\u578b\u3002", "method": "\u91c7\u7528\u591a\u5c42\u77e5\u8bc6\u56fe\u8c31\u548cGNN\u67b6\u6784\uff0c\u57fa\u4e8eRIPE Atlas\u516c\u5f00\u6570\u636e\u8bad\u7ec3\u6570\u5b57\u5b6a\u751f\u3002", "result": "GraphTransformer\u6027\u80fd\u6700\u4f73\uff0c\u5176\u4ed6\u67b6\u6784\u5728\u8bad\u7ec3\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\u53ef\u80fd\u66f4\u9002\u7528\u3002", "conclusion": "\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7cbe\u51c6\u7684\u4e3b\u52a8\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2508.02167", "pdf": "https://arxiv.org/pdf/2508.02167", "abs": "https://arxiv.org/abs/2508.02167", "authors": ["Yuxuan Wang", "Cristian Tirelli", "Giovanni Ansaloni", "Laura Pozzi", "David Atienza"], "title": "An MLIR-based Compilation Framework for Control Flow Management on CGRAs", "categories": ["cs.SE"], "comment": null, "summary": "Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility\nand efficiency, making them well-suited for the acceleration of intensive\nworkloads. Nevertheless, a key barrier towards their widespread adoption is\nposed by CGRA compilation, which must cope with a multi-dimensional space\nspanning both the spatial and the temporal domains. Indeed, state-of-the-art\ncompilers are limited in scope as they mostly deal with the data flow of\napplications, while having little or no support for control flow. Hence, they\nmostly target the mapping of single loops and/or delegate the management of\ncontrol flow divergences to ad-hoc hardware units.\n  Conversely, in this paper we show that control flow can be effectively\nmanaged and optimized at the compilation level, allowing for a broad set of\napplications to be targeted while being hardware-agnostic and achieving high\nperformance. We embody our methodology in a modular compilation framework\nconsisting of transformation and optimization passes, enabling support for\napplications with arbitrary control flows running on abstract CGRA meshes. We\nalso introduce a novel mapping methodology that acts as a compilation back-end,\naddressing the limitations in available CGRA hardware resources and\nguaranteeing a feasible solution in the compilation process. Our framework\nachieves up to 2.1X speedups over state-of-the-art approaches, purely through\ncompilation optimizations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411CGRA\u7684\u7f16\u8bd1\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u7ba1\u7406\u548c\u4f18\u5316\u63a7\u5236\u6d41\uff0c\u5b9e\u73b0\u4e86\u786c\u4ef6\u65e0\u5173\u7684\u9ad8\u6027\u80fd\u6620\u5c04\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe2.1\u500d\u3002", "motivation": "CGRA\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u9650\u4e8e\u7f16\u8bd1\u5668\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u63a7\u5236\u6d41\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u6d41\u4e14\u5c40\u9650\u4e8e\u5355\u5faa\u73af\u6620\u5c04\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u7f16\u8bd1\u6846\u67b6\uff0c\u5305\u542b\u8f6c\u6362\u548c\u4f18\u5316\u6b65\u9aa4\uff0c\u652f\u6301\u4efb\u610f\u63a7\u5236\u6d41\u5e94\u7528\u7684\u62bd\u8c61CGRA\u7f51\u683c\u6620\u5c04\uff1b\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u540e\u7aef\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u7eaf\u7f16\u8bd1\u4f18\u5316\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u76842.1\u500d\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u7f16\u8bd1\u7ea7\u63a7\u5236\u6d41\u4f18\u5316\u7684\u6709\u6548\u6027\uff0c\u4e3aCGRA\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.01155", "pdf": "https://arxiv.org/pdf/2508.01155", "abs": "https://arxiv.org/abs/2508.01155", "authors": ["Yuma Akiba", "Shota Nakayama", "Keigo Ushiyama", "Izumi Mizoguchi", "Hiroyuki Kajimoto"], "title": "Presentation of Low-Frequency Vibration to the Face Using Amplitude Modulation", "categories": ["cs.HC"], "comment": "Submitted version, in IEEE Transactions on Haptics, 2025", "summary": "This study proposes a method to present pure low-frequency vibration\nsensations to the face that cannot be presented by small commercially available\nvibrators. The core innovation lies in utilizing an amplitude modulation\ntechnique with a carrier frequency of approximately 200 Hz. Due to the absence\nof Pacinian corpuscles in the facial region - receptors responsible for\ndetecting high-frequency vibrations around 200 Hz - only the original\nlow-frequency signal is perceived. Three experiments were conducted.\nExperiments 1 and 2 were performed on the forehead to confirm that the proposed\namplitude modulation method could produce the desired low-frequency perception\nand to evaluate the subjective quality of the vibration. The results suggested\nthat the proposed method could produce the perception of desired pure\nlow-frequency vibration when applied to the forehead. In Experiment 3, the\nproposed method was applied to the whole face, and its range of applicability\nwas explored. The results indicated that the original low-frequency vibration\nwas clearly perceptible around the eyes, cheeks, and lower lip area.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u632f\u5e45\u8c03\u5236\u6280\u672f\u5728\u9762\u90e8\u5448\u73b0\u7eaf\u4f4e\u9891\u632f\u52a8\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u9762\u90e8\u7f3a\u4e4f\u68c0\u6d4b\u9ad8\u9891\u632f\u52a8\u7684\u53d7\u4f53\uff0c\u73b0\u6709\u5c0f\u578b\u632f\u52a8\u5668\u65e0\u6cd5\u5448\u73b0\u4f4e\u9891\u632f\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528200\u8d6b\u5179\u7684\u8f7d\u6ce2\u9891\u7387\u8fdb\u884c\u632f\u5e45\u8c03\u5236\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e86\u989d\u5934\u548c\u6574\u4e2a\u9762\u90e8\u7684\u632f\u52a8\u611f\u77e5\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u989d\u5934\u548c\u9762\u90e8\u7279\u5b9a\u533a\u57df\uff08\u5982\u773c\u775b\u3001\u8138\u988a\u548c\u4e0b\u5507\uff09\u6709\u6548\u5448\u73b0\u7eaf\u4f4e\u9891\u632f\u52a8\u3002", "conclusion": "\u8be5\u632f\u5e45\u8c03\u5236\u6280\u672f\u4e3a\u9762\u90e8\u4f4e\u9891\u632f\u52a8\u7684\u5448\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.02084", "pdf": "https://arxiv.org/pdf/2508.02084", "abs": "https://arxiv.org/abs/2508.02084", "authors": ["Yuki Yamagata", "Koji Kyoda", "Hiroya Itoga", "Emi Fujisawa", "Shuichi Onami"], "title": "SSBD Ontology: A Two-Tier Approach for Interoperable Bioimaging Metadata", "categories": ["cs.DL", "cs.AI", "cs.DB"], "comment": "Accepted to the 24th International Semantic Web Conference Resource\n  Track (ISWC 2025)", "summary": "Advanced bioimaging technologies have enabled the large-scale acquisition of\nmultidimensional data, yet effective metadata management and interoperability\nremain significant challenges. To address these issues, we propose a new\nontology-driven framework for the Systems Science of Biological Dynamics\nDatabase (SSBD) that adopts a two-tier architecture. The core layer provides a\nclass-centric structure referencing existing biomedical ontologies, supporting\nboth SSBD:repository -- which focuses on rapid dataset publication with minimal\nmetadata -- and SSBD:database, which is enhanced with biological and\nimaging-related annotations. Meanwhile, the instance layer represents actual\nimaging dataset information as Resource Description Framework individuals that\nare explicitly linked to the core classes. This layered approach aligns\nflexible instance data with robust ontological classes, enabling seamless\nintegration and advanced semantic queries. By coupling flexibility with rigor,\nthe SSBD Ontology promotes interoperability, data reuse, and the discovery of\nnovel biological mechanisms. Moreover, our solution aligns with the Recommended\nMetadata for Biological Images guidelines and fosters compatibility.\nUltimately, our approach contributes to establishing a Findable, Accessible,\nInteroperable, and Reusable data ecosystem within the bioimaging community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u9a71\u52a8\u7684\u4e24\u5c42\u7ea7\u67b6\u6784\uff08\u6838\u5fc3\u5c42\u548c\u5b9e\u4f8b\u5c42\uff09\uff0c\u7528\u4e8e\u751f\u7269\u52a8\u529b\u5b66\u6570\u636e\u5e93\uff08SSBD\uff09\uff0c\u4ee5\u89e3\u51b3\u751f\u7269\u6210\u50cf\u6570\u636e\u7684\u5143\u6570\u636e\u7ba1\u7406\u548c\u4e92\u64cd\u4f5c\u6027\u6311\u6218\u3002", "motivation": "\u751f\u7269\u6210\u50cf\u6280\u672f\u751f\u6210\u7684\u591a\u7ef4\u6570\u636e\u5728\u5143\u6570\u636e\u7ba1\u7406\u548c\u4e92\u64cd\u4f5c\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u4e14\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u7ea7\u67b6\u6784\uff1a\u6838\u5fc3\u5c42\u5f15\u7528\u73b0\u6709\u751f\u7269\u533b\u5b66\u672c\u4f53\uff0c\u652f\u6301\u5feb\u901f\u6570\u636e\u96c6\u53d1\u5e03\u548c\u589e\u5f3a\u6ce8\u91ca\uff1b\u5b9e\u4f8b\u5c42\u5c06\u5b9e\u9645\u6570\u636e\u8868\u793a\u4e3aRDF\u4e2a\u4f53\uff0c\u4e0e\u6838\u5fc3\u7c7b\u663e\u5f0f\u94fe\u63a5\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u6570\u636e\u7684\u65e0\u7f1d\u6574\u5408\u548c\u9ad8\u7ea7\u8bed\u4e49\u67e5\u8be2\uff0c\u4fc3\u8fdb\u4e86\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u91cd\u7528\u548c\u65b0\u751f\u7269\u5b66\u673a\u5236\u7684\u53d1\u73b0\u3002", "conclusion": "SSBD\u672c\u4f53\u7ed3\u5408\u7075\u6d3b\u6027\u4e0e\u4e25\u8c28\u6027\uff0c\u9075\u5faa\u751f\u7269\u56fe\u50cf\u5143\u6570\u636e\u6307\u5357\uff0c\u4e3a\u751f\u7269\u6210\u50cf\u793e\u533a\u5efa\u7acb\u4e86FAIR\uff08\u53ef\u67e5\u627e\u3001\u53ef\u8bbf\u95ee\u3001\u53ef\u4e92\u64cd\u4f5c\u3001\u53ef\u91cd\u7528\uff09\u6570\u636e\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2508.01371", "pdf": "https://arxiv.org/pdf/2508.01371", "abs": "https://arxiv.org/abs/2508.01371", "authors": ["Zeke Xiao", "Yuekang Li", "Qin Wang", "Shiping Chen"], "title": "Prompt to Pwn: Automated Exploit Generation for Smart Contracts", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": null, "summary": "We explore the feasibility of using LLMs for Automated Exploit Generation\n(AEG) against vulnerable smart contracts. We present \\textsc{ReX}, a framework\nintegrating LLM-based exploit synthesis with the Foundry testing suite,\nenabling the automated generation and validation of proof-of-concept (PoC)\nexploits. We evaluate five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro,\nClaude Opus 4, DeepSeek, and Qwen3 Plus) on both synthetic benchmarks and\nreal-world smart contracts affected by known high-impact exploits. Our results\nshow that modern LLMs can reliably generate functional PoC exploits for diverse\nvulnerability types, with success rates reaching up to 92\\%. Notably, Gemini\n2.5 Pro and GPT-4.1 consistently outperform others in both synthetic and\nreal-world scenarios. We further analyze factors influencing AEG effectiveness,\nincluding model capabilities, contract structure, and vulnerability types. We\nalso collect the first curated dataset of real-world PoC exploits to support\nfuture research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u81ea\u52a8\u5316\u6f0f\u6d1e\u5229\u7528\u751f\u6210\uff08AEG\uff09\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u96c6\u6210LLM\u4e0eFoundry\u6d4b\u8bd5\u5957\u4ef6\u7684\u6846\u67b6ReX\uff0c\u6210\u529f\u7387\u9ad8\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u5229\u7528\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u81ea\u52a8\u5316\u751f\u6210\u548c\u9a8c\u8bc1\u6f0f\u6d1e\u5229\u7528\u3002", "method": "\u63d0\u51faReX\u6846\u67b6\uff0c\u7ed3\u5408LLM\u548cFoundry\u6d4b\u8bd5\u5957\u4ef6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u5148\u8fdbLLM\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u73b0\u4ee3LLM\u80fd\u9ad8\u6548\u751f\u6210\u529f\u80fd\u6027\u7684\u6f0f\u6d1e\u5229\u7528\uff0c\u6210\u529f\u7387\u9ad8\u8fbe92%\uff0cGemini 2.5 Pro\u548cGPT-4.1\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLM\u5728AEG\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u771f\u5b9e\u6f0f\u6d1e\u5229\u7528\u6570\u636e\u96c6\u3002"}}
{"id": "2508.02520", "pdf": "https://arxiv.org/pdf/2508.02520", "abs": "https://arxiv.org/abs/2508.02520", "authors": ["Ao Xiao", "Bangzheng He", "Baoquan Zhang", "Baoxing Huai", "Bingji Wang", "Bo Wang", "Bo Xu", "Boyi Hou", "Chan Yang", "Changhong Liu", "Cheng Cui", "Chenyu Zhu", "Cong Feng", "Daohui Wang", "Dayun Lin", "Duo Zhao", "Fengshao Zou", "Fu Wang", "Gangqiang Zhang", "Gengyuan Dan", "Guanjie Chen", "Guodong Guan", "Guodong Yang", "Haifeng Li", "Haipei Zhu", "Hao Feng", "Hao Huang", "Hao Xu", "Hengrui Ma", "Hengtao Fan", "Hui Liu", "Jia Li", "Jiang Liu", "Jiang Xu", "Jie Meng", "Jinhan Xin", "Junhao Hu", "Juwei Chen", "Lan Yu", "Lanxin Miao", "Liang Liu", "Linan Jing", "Lu Zhou", "Meina Han", "Mingkun Deng", "Mingyu Deng", "Naitian Deng", "Nizhong Lin", "Peihan Zhao", "Peng Pan", "Pengfei Shen", "Ping Li", "Qi Zhang", "Qin Zhang", "Qingrong Xia", "Qingyi Zhang", "Qunchao Fu", "Ren Guo", "Ruimin Gao", "Shaochun Li", "Sheng Long", "Shentian Li", "Shining Wan", "Shuai Shen", "Shuangfu Zeng", "Shuming Jing", "Siqi Yang", "Song Zhang", "Tao Xu", "Tianlin Du", "Ting Chen", "Wanxu Wu", "Wei Jiang", "Weinan Tong", "Weiwei Chen", "Wen Peng", "Wenli Zhou", "Wenquan Yang", "Wenxin Liang", "Xiang Liu", "Xiaoli Zhou", "Xin Jin", "Xinyu Duan", "Xu Li", "Xu Zhang", "Xusheng Chen", "Yalong Shan", "Yang Gan", "Yao Lu", "Yi Deng", "Yi Zheng", "Yingfei Zheng", "Yiyun Zheng", "Yizhou Shan", "Yong Gao", "Yongqiang Yang", "Yuanjin Gong", "Yue Yu", "Yuetao Chen", "Yukun Zhu", "Yulong He", "Yusu Zhao", "Yuyan Wu", "Zenan Zhang", "Zhaojin Zhuo", "Zhaoyang Ji", "Zhefeng Wang", "Zheng Wang", "Zhenhua Yang", "Zhenli Sheng", "Zhibin Yu", "Zhigang Ji", "Zhihao Ren", "Zhipeng Bian", "Zhixia Liu", "Zhiyu Dong", "Zhonghua Li", "Zhou Yu", "Zhuoming Shen", "Zhuwei Peng", "Zi Ye", "Zihao Xiang", "Zimin Fu", "Zixuan Zhang"], "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384", "categories": ["cs.DC"], "comment": null, "summary": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u534e\u4e3a\u4e91\u7684xDeepServe\u7cfb\u7edf\uff0c\u4e13\u4e3aSuperPod\u89c4\u6a21\u7684\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\uff0c\u91c7\u7528Transformerless\u67b6\u6784\u5206\u89e3Transformer\u6a21\u578b\uff0c\u652f\u6301\u5927\u89c4\u6a21\u63a8\u7406\u3002", "motivation": "\u968f\u7740LLM\u6a21\u578b\u548cSuperPod\u786c\u4ef6\u7684\u53d1\u5c55\uff0c\u8fd0\u884c\u5927\u89c4\u6a21MoE\u6a21\u578b\u9762\u4e34\u65b0\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u6267\u884c\u6a21\u578b\u548c\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u3002", "method": "\u63d0\u51faxDeepServe\u7cfb\u7edf\uff0c\u6838\u5fc3\u662fTransformerless\u67b6\u6784\uff0c\u5c06Transformer\u6a21\u578b\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u5355\u5143\uff0c\u7ed3\u5408XCCL\u901a\u4fe1\u5e93\u548cFlowServe\u5f15\u64ce\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5b9e\u73b0\u4e86\u8de8\u6570\u767e\u4e2aNPU\u7684\u53ef\u6269\u5c55\u63a8\u7406\uff0c\u652f\u6301\u8ba1\u7b97\u548c\u5185\u5b58\u7684\u72ec\u7acb\u6269\u5c55\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002", "conclusion": "xDeepServe\u4e3aSuperPod\u89c4\u6a21\u7684\u5927\u89c4\u6a21AI\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02571", "pdf": "https://arxiv.org/pdf/2508.02571", "abs": "https://arxiv.org/abs/2508.02571", "authors": ["Yongzhe Xu", "Weitong Li", "Eeshan Umrani", "Taejoong Chung"], "title": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata", "categories": ["cs.NI"], "comment": null, "summary": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure.", "AI": {"tldr": "ASINT\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u7ed3\u5408\u4e86\u6279\u91cf\u6ce8\u518c\u6570\u636e\u548cWeb\u8d44\u6e90\uff0c\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6307\u5bfcLLM\u63a8\u7406\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6620\u5c04ASNs\u5230\u5176\u7ec4\u7ec7\u5bb6\u65cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982WHOIS\u6216PeeringDB\uff09\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u7ec4\u7ec7\u5173\u7cfb\uff08\u5982\u8de8\u533a\u57df\u522b\u540d\u3001\u7236\u5b50\u6240\u6709\u6743\uff09\uff0c\u4e14\u65e0\u6cd5\u7edf\u4e00\u5206\u6563\u5728\u4e0d\u540cRIR\u6807\u8bc6\u7b26\u4e2d\u7684\u7ec4\u7ec7\u3002", "method": "ASINT\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u878d\u5408\u6ce8\u518c\u6570\u636e\u4e0eWeb\u8d44\u6e90\uff0c\u5229\u7528RAG\u4e0eLLM\u63a8\u7406\uff0c\u5c06ASNs\u5408\u5e76\u4e3a\u201c\u7ec4\u7ec7\u5bb6\u65cf\u201d\u3002", "result": "ASINT\u6620\u5c04\u4e8611\u4e07\u591a\u4e2aASNs\u52308\u4e07\u591a\u4e2a\u7ec4\u7ec7\u5bb6\u65cf\uff0c\u8bc6\u522b\u4e86\u66f4\u591a\u8de8\u533a\u57df\u7fa4\u4f53\uff0c\u5e76\u5728\u5b89\u5168\u4e0e\u6d4b\u91cf\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "ASINT\u5c55\u793a\u4e86\u901a\u8fc7Web\u8bc1\u636e\u53ef\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u4e14\u52a8\u6001\u7684\u4e92\u8054\u7f51\u7ec4\u7ec7\u7ed3\u6784\u89c6\u56fe\uff0c\u652f\u6301\u5b9a\u671f\u66f4\u65b0\u4e0e\u6210\u672c\u654f\u611f\u578bLLM\u9009\u62e9\u3002"}}
{"id": "2508.02176", "pdf": "https://arxiv.org/pdf/2508.02176", "abs": "https://arxiv.org/abs/2508.02176", "authors": ["Andrew Tropin"], "title": "Highly Interactive Testing for Uninterrupted Development Flow", "categories": ["cs.SE", "cs.HC", "D.2.3; D.2.6; D.2.5; H.5.2"], "comment": "12 pages, ICFP-2025", "summary": "Highly interactive development environments (HIDEs) enable uninterrupted\ndevelopment flow through continuous program evolution and rapid hypothesis\nchecking. However, traditional testing approaches -- typically executed\nseparately via CLI -- isolate tests from HIDE tooling (interactive debuggers,\nvalue and stack inspectors, etc.) and introduce disruptive delays due to coarse\nexecution granularity and lack of runtime context. This disconnect breaks\ndevelopment flow by exceeding critical attention thresholds. In this paper we\npresent a library that provides runtime representation for tests, allowing\ntight integration with HIDEs, and enabling immediate access to HIDE tooling in\nthe context of test failure. We then describe development workflows enhanced\nwith testing and demonstrate how they achieve subsecond test reexecution times\ncrucial for maintaining developer focus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e93\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u8868\u793a\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u4e0e\u9ad8\u5ea6\u4ea4\u4e92\u5f00\u53d1\u73af\u5883\uff08HIDE\uff09\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u4ece\u800c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u4e0eHIDE\u5de5\u5177\u9694\u79bb\uff0c\u6267\u884c\u5ef6\u8fdf\u9ad8\uff0c\u7834\u574f\u4e86\u5f00\u53d1\u6d41\u7a0b\u7684\u8fde\u7eed\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u63d0\u4f9b\u6d4b\u8bd5\u8fd0\u884c\u65f6\u8868\u793a\u7684\u5e93\uff0c\u4e0eHIDE\u7d27\u5bc6\u96c6\u6210\uff0c\u652f\u6301\u6d4b\u8bd5\u5931\u8d25\u65f6\u5373\u65f6\u8bbf\u95eeHIDE\u5de5\u5177\u3002", "result": "\u5b9e\u73b0\u4e86\u4e9a\u79d2\u7ea7\u7684\u6d4b\u8bd5\u91cd\u65b0\u6267\u884c\u65f6\u95f4\uff0c\u6709\u52a9\u4e8e\u4fdd\u6301\u5f00\u53d1\u8005\u7684\u4e13\u6ce8\u529b\u3002", "conclusion": "\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u6d4b\u8bd5\u4e0eHIDE\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6d41\u7a0b\u7684\u6548\u7387\u548c\u8fde\u7eed\u6027\u3002"}}
{"id": "2508.01165", "pdf": "https://arxiv.org/pdf/2508.01165", "abs": "https://arxiv.org/abs/2508.01165", "authors": ["Jing Tang", "Qing Xiao", "Kunxu Du", "Zaiqiao Ye"], "title": "RoboLinker: A Diffusion-model-based Matching Clothing Generator Between Humans and Companion Robots", "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": "3 pages, 3 figures, accepted by UIST Adjunct'25", "summary": "We present RoboLinker, a generative design system that creates matching\noutfits for humans and their robots. Using a diffusion-based model, the system\ntakes a robot image and a style prompt from users as input, and outputs a human\noutfit that visually complements the robot's attire. Through an interactive\ninterface, users can refine the generated designs. We evaluate RoboLinker with\nboth humanoid and pet-like robots, demonstrating its capacity to produce\nstylistically coherent and emotionally resonant results.", "AI": {"tldr": "RoboLinker\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u8bbe\u8ba1\u7cfb\u7edf\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4e3a\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u642d\u914d\u670d\u9970\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u754c\u9762\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u8bbe\u8ba1\u548c\u8c10\u7684\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u670d\u9970\uff0c\u589e\u5f3a\u89c6\u89c9\u548c\u60c5\u611f\u5171\u9e23\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u8f93\u5165\u673a\u5668\u4eba\u56fe\u50cf\u548c\u98ce\u683c\u63d0\u793a\uff0c\u751f\u6210\u4eba\u7c7b\u670d\u9970\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u4e3a\u7c7b\u4eba\u548c\u5ba0\u7269\u673a\u5668\u4eba\u751f\u6210\u98ce\u683c\u4e00\u81f4\u3001\u60c5\u611f\u5171\u9e23\u7684\u670d\u9970\u3002", "conclusion": "RoboLinker\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u8bbe\u8ba1\u5728\u534f\u8c03\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5f62\u8c61\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.02091", "pdf": "https://arxiv.org/pdf/2508.02091", "abs": "https://arxiv.org/abs/2508.02091", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Chris Shum", "Jiwei Li"], "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Preprint Version", "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", "AI": {"tldr": "CRINN\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684ANNS\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u66f4\u5feb\u5b9e\u73b0\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u5e94\u7528\u7684\u6027\u80fd\u3002", "motivation": "ANNS\u7b97\u6cd5\u5728AI\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u5c24\u5176\u662f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u5e94\u7528\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06ANNS\u4f18\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4ee5\u6267\u884c\u901f\u5ea6\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u81ea\u52a8\u751f\u6210\u66f4\u5feb\u7684\u5b9e\u73b0\u3002", "result": "\u5728\u516d\u4e2a\u5e38\u7528NNS\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCRINN\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5e76\u5217\u7b2c\u4e00\u3002", "conclusion": "CRINN\u7684\u6210\u529f\u4e0d\u4ec5\u8bc1\u660e\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u81ea\u52a8\u5316\u590d\u6742\u7b97\u6cd5\u4f18\u5316\uff0c\u8fd8\u4e3aLLM\u589e\u5f3a\u5e94\u7528\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.01586", "pdf": "https://arxiv.org/pdf/2508.01586", "abs": "https://arxiv.org/abs/2508.01586", "authors": ["Nguyen Cong Luong", "Nguyen Duc Hai", "Duc Van Le", "Huy T. Nguyen", "Thai-Hoc Vu", "Thien Huynh-The", "Ruichen Zhang", "Nguyen Duc Duy Anh", "Dusit Niyato", "Marco Di Renzo", "Dong In Kim", "Quoc-Viet Pham"], "title": "Diffusion Models for Future Networks and Communications: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.IT", "cs.NI", "math.IT"], "comment": "This work was submitted to Proceedings of the IEEE", "summary": "The rise of Generative AI (GenAI) in recent years has catalyzed\ntransformative advances in wireless communications and networks. Among the\nmembers of the GenAI family, Diffusion Models (DMs) have risen to prominence as\na powerful option, capable of handling complex, high-dimensional data\ndistribution, as well as consistent, noise-robust performance. In this survey,\nwe aim to provide a comprehensive overview of the theoretical foundations and\npractical applications of DMs across future communication systems. We first\nprovide an extensive tutorial of DMs and demonstrate how they can be applied to\nenhance optimizers, reinforcement learning and incentive mechanisms, which are\npopular approaches for problems in wireless networks. Then, we review and\ndiscuss the DM-based methods proposed for emerging issues in future networks\nand communications, including channel modeling and estimation, signal detection\nand data reconstruction, integrated sensing and communication, resource\nmanagement in edge computing networks, semantic communications and other\nnotable issues. We conclude the survey with highlighting technical limitations\nof DMs and their applications, as well as discussing future research\ndirections.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u7406\u8bba\u4e0e\u5e94\u7528\uff0c\u6db5\u76d6\u4f18\u5316\u5668\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u4fe1\u9053\u5efa\u6a21\u7b49\u9886\u57df\uff0c\u5e76\u6307\u51fa\u4e86\u6280\u672f\u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u6269\u6563\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u548c\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u5bf9\u590d\u6742\u6570\u636e\u5206\u5e03\u548c\u566a\u58f0\u9c81\u68d2\u6027\u80fd\u7684\u5904\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6559\u7a0b\u548c\u5e94\u7528\u6848\u4f8b\uff0c\u5c55\u793aDMs\u5728\u4f18\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u4fe1\u9053\u5efa\u6a21\u7b49\u591a\u79cd\u65e0\u7ebf\u7f51\u7edc\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86DMs\u5728\u591a\u4e2a\u901a\u4fe1\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u7a81\u51fa\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u6280\u672f\u6311\u6218\u3002", "conclusion": "DMs\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6280\u672f\u9650\u5236\u5e76\u63a2\u7d22\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.02552", "pdf": "https://arxiv.org/pdf/2508.02552", "abs": "https://arxiv.org/abs/2508.02552", "authors": ["Siamak Abdi", "Giuseppe Di Fatta", "Atta Badii", "Giancarlo Fortino"], "title": "Blockchain Epidemic Consensus for Large-Scale Networks", "categories": ["cs.DC"], "comment": "2025 IEEE 7th International Conference on Blockchain Computing and\n  Applications (BCCA)", "summary": "Blockchain is a distributed ledger technology that has applications in many\ndomains such as cryptocurrency, smart contracts, supply chain management, and\nmany others. Distributed consensus is a fundamental component of blockchain\nsystems that enables secure, precise, and tamper-proof verification of data\nwithout relying on central authorities. Existing consensus protocols,\nnevertheless, suffer from drawbacks, some of which are related to scalability,\nresource consumption, and fault tolerance. We introduce Blockchain Epidemic\nConsensus Protocol (BECP), a novel fully decentralised consensus protocol for\nblockchain networks at a large scale. BECP follows epidemic communication\nprinciples, without fixed roles like validators or leaders, and achieves\nprobabilistic convergence, efficient message dissemination, and tolerance to\nmessage delays. We provide an extensive experimental comparison of BECP against\nclassic protocols like PAXOS, RAFT, and PBFT, and newer epidemic-based\nprotocols like Avalanche and Snowman. The findings indicate that BECP provides\ndesirable gains in throughput, consensus latency, and substantial\nmessage-passing efficiency compared to existing epidemic-based approaches,\nvalidating its usability as an effective and scalable approach for\nnext-generation blockchain systems.", "AI": {"tldr": "\u533a\u5757\u94fe\u9700\u8981\u4e00\u4e2a\u9ad8\u6548\u7684\u5171\u8bc6\u534f\u8bae\u6765\u89e3\u51b3\u73b0\u6709\u534f\u8bae\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u534f\u8baeBECP\uff0c\u57fa\u4e8e\u6d41\u884c\u75c5\u901a\u4fe1\u539f\u5219\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u6d88\u606f\u4f20\u9012\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u7684\u533a\u5757\u94fe\u5171\u8bc6\u534f\u8bae\u5728\u53ef\u6269\u5c55\u6027\u3001\u8d44\u6e90\u6d88\u8017\u548c\u5bb9\u9519\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u533a\u5757\u94fe\u6d41\u884c\u75c5\u5171\u8bc6\u534f\u8bae\uff08BECP\uff09\uff0c\u91c7\u7528\u6d41\u884c\u75c5\u901a\u4fe1\u539f\u5219\uff0c\u65e0\u56fa\u5b9a\u89d2\u8272\uff08\u5982\u9a8c\u8bc1\u8005\u6216\u9886\u5bfc\u8005\uff09\uff0c\u5b9e\u73b0\u6982\u7387\u6536\u655b\u3001\u9ad8\u6548\u6d88\u606f\u4f20\u64ad\u548c\u5bf9\u6d88\u606f\u5ef6\u8fdf\u7684\u5bb9\u5fcd\u3002", "result": "\u901a\u8fc7\u4e0e\u7ecf\u5178\u534f\u8bae\uff08PAXOS\u3001RAFT\u3001PBFT\uff09\u53ca\u65b0\u578b\u6d41\u884c\u75c5\u534f\u8bae\uff08Avalanche\u3001Snowman\uff09\u7684\u6bd4\u8f83\uff0cBECP\u5728\u541e\u5410\u91cf\u3001\u5171\u8bc6\u5ef6\u8fdf\u548c\u6d88\u606f\u4f20\u9012\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "BECP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5171\u8bc6\u534f\u8bae\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u533a\u5757\u94fe\u7cfb\u7edf\u3002"}}
{"id": "2508.00851", "pdf": "https://arxiv.org/pdf/2508.00851", "abs": "https://arxiv.org/abs/2508.00851", "authors": ["Abdurrahman Tolay"], "title": "eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices", "categories": ["cs.CR", "cs.NI", "C.2.0; C.2.1; D.4.6"], "comment": "10 pages, 5 figures, includes evaluation on Docker and Raspberry Pi\n  testbeds. Keywords: IoT Security, DDoS Mitigation, eBPF, XDP, Raspberry Pi.\n  Submitted to IEEE Internet of Things Journal", "summary": "The rapid expansion of the Internet of Things (IoT) has intensified security\nchallenges, notably from Distributed Denial of Service (DDoS) attacks launched\nby compromised, resource-constrained devices. Traditional defenses are often\nill-suited for the IoT paradigm, creating a need for lightweight,\nhigh-performance, edge-based solutions. This paper presents the design,\nimplementation, and evaluation of an IoT security framework that leverages the\nextended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for\nin-kernel mitigation of DDoS attacks. The system uses a rate-based detection\nalgorithm to identify and block malicious traffic at the earliest stage of the\nnetwork stack. The framework is evaluated using both Docker-based simulations\nand real-world deployment on a Raspberry Pi 4, showing over 97% mitigation\neffectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected,\nand system stability is preserved even under attack. These results confirm that\neBPF/XDP provides a viable and highly efficient solution for hardening IoT edge\ndevices against volumetric network attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eeBPF/XDP\u7684\u8f7b\u91cf\u7ea7IoT\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u7f13\u89e3DDoS\u653b\u51fb\uff0c\u5c55\u793a\u4e86\u8d85\u8fc797%\u7684\u7f13\u89e3\u6548\u679c\u3002", "motivation": "IoT\u8bbe\u5907\u7684\u5feb\u901f\u6269\u5c55\u5e26\u6765\u4e86\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u53d1\u8d77\u7684DDoS\u653b\u51fb\uff0c\u4f20\u7edf\u7684\u9632\u5fa1\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eIoT\u73af\u5883\u3002", "method": "\u4f7f\u7528eBPF\u548cXDP\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5185\u6838\u7ea7\u522b\u7684DDoS\u653b\u51fb\u7f13\u89e3\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8e\u901f\u7387\u7684\u68c0\u6d4b\u7b97\u6cd5\u5728\u65e9\u671f\u8bc6\u522b\u5e76\u62e6\u622a\u6076\u610f\u6d41\u91cf\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u90e8\u7f72\uff08Raspberry Pi 4\uff09\u4e2d\uff0c\u7cfb\u7edf\u5bf9100 Mbps\u7684\u653b\u51fb\u6d41\u91cf\u5b9e\u73b0\u4e86\u8d85\u8fc797%\u7684\u7f13\u89e3\u6548\u679c\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5408\u6cd5\u6d41\u91cf\u548c\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "conclusion": "eBPF/XDP\u4e3aIoT\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u9632\u5fa1\u5927\u89c4\u6a21\u7f51\u7edc\u653b\u51fb\u3002"}}
{"id": "2508.02233", "pdf": "https://arxiv.org/pdf/2508.02233", "abs": "https://arxiv.org/abs/2508.02233", "authors": ["Vincenzo De Martino", "Joel Casta\u00f1o", "Fabio Palomba", "Xavier Franch", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "title": "A Methodological Framework for LLM-Based Mining of Software Repositories", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5206\u6790\u4e86LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u65b9\u6cd5\u548c\u6f5c\u5728\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86PRIMES 2.0\u6846\u67b6\u4ee5\u63d0\u5347\u7814\u7a76\u7684\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u8f6f\u4ef6\u4ed3\u5e93\u6316\u6398\uff08MSR\uff09\u7814\u7a76\u4e2d\u7684\u65b9\u6cd5\u5b66\u6574\u5408\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6307\u5bfc\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff08\u5feb\u901f\u56de\u987e\u548c\u95ee\u5377\u8c03\u67e5\uff09\uff0c\u5206\u6790LLM4MSR\u9886\u57df\u7684\u65b9\u6cd5\u548c\u5a01\u80c1\u3002", "result": "\u8bc6\u522b\u4e8615\u79cd\u65b9\u6cd5\u30019\u7c7b\u5a01\u80c1\u53ca25\u79cd\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86PRIMES 2.0\u6846\u67b6\u3002", "conclusion": "PRIMES 2.0\u4e3aLLM-based MSR\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u6846\u67b6\uff0c\u652f\u6301\u65b9\u6cd5\u5b66\u548c\u5a01\u80c1\u7ba1\u7406\u7684\u5168\u751f\u547d\u5468\u671f\u3002"}}
{"id": "2508.01235", "pdf": "https://arxiv.org/pdf/2508.01235", "abs": "https://arxiv.org/abs/2508.01235", "authors": ["Yaxin Hu", "Arissa J. Sato", "Jingxin Du", "Chenming Ye", "Anjun Zhu", "Pragathi Praveena", "Bilge Mutlu"], "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration", "categories": ["cs.HC", "cs.RO", "68"], "comment": null, "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u96c6\u6210\u4f4d\u7f6e\u611f\u77e5\u7684LLM\u53d9\u4e8b\u80fd\u529b\u63d0\u5347\u673a\u5668\u4eba\u8fdc\u7a0b\u63a2\u7d22\u7684\u4f53\u9a8c\uff0c\u5e76\u5728\u535a\u7269\u9986\u4e2d\u6d4b\u8bd5\u4e86\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u5b58\u5728\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7528\u6237\u5bf9\u73af\u5883\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u5728\u964c\u751f\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aNarraGuide\u7684\u539f\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bf9\u8bdd\u754c\u9762\u63d0\u4f9b\u53d9\u4e8b\u5f15\u5bfc\uff0c\u5e76\u5728\u5730\u8d28\u535a\u7269\u9986\u4e2d\u90e8\u7f72\u6d4b\u8bd5\u3002", "result": "\u7528\u6237\u5bf9\u673a\u5668\u4eba\u7684\u89d2\u8272\u611f\u77e5\u3001\u5bf9\u8bdd\u53c2\u4e0e\u4ee5\u53ca\u5bf9\u65c1\u89c2\u8005\u4e92\u52a8\u7684\u504f\u597d\u8868\u73b0\u51fa\u79ef\u6781\u53cd\u9988\u3002", "conclusion": "LLM\u8d4b\u80fd\u7684\u673a\u5668\u4eba\u80fd\u591f\u63d0\u4f9b\u4f4d\u7f6e\u611f\u77e5\u7684\u53d9\u4e8b\u5f15\u5bfc\uff0c\u4e30\u5bcc\u8fdc\u7a0b\u63a2\u7d22\u7684\u4f53\u9a8c\u3002"}}
{"id": "2508.02270", "pdf": "https://arxiv.org/pdf/2508.02270", "abs": "https://arxiv.org/abs/2508.02270", "authors": ["Tiantian Liu", "Xiao Li", "Huan Li", "Hua Lu", "Christian S. Jensen", "Jianliang Xu"], "title": "Skeleton-Guided Learning for Shortest Path Search", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Shortest path search is a core operation in graph-based applications, yet\nexisting methods face important limitations. Classical algorithms such as\nDijkstra's and A* become inefficient as graphs grow more complex, while\nindex-based techniques often require substantial preprocessing and storage.\nRecent learning-based approaches typically focus on spatial graphs and rely on\ncontext-specific features like geographic coordinates, limiting their general\napplicability. We propose a versatile learning-based framework for shortest\npath search on generic graphs, without requiring domain-specific features. At\nthe core of our approach is the construction of a skeleton graph that captures\nmulti-level distance and hop information in a compact form. A Skeleton Graph\nNeural Network (SGNN) operates on this structure to learn node embeddings and\npredict distances and hop lengths between node pairs. These predictions support\nLSearch, a guided search algorithm that uses model-driven pruning to reduce the\nsearch space while preserving accuracy. To handle larger graphs, we introduce a\nhierarchical training strategy that partitions the graph into subgraphs with\nindividually trained SGNNs. This structure enables HLSearch, an extension of\nour method for efficient path search across graph partitions. Experiments on\nfive diverse real-world graphs demonstrate that our framework achieves strong\nperformance across graph types, offering a flexible and effective solution for\nlearning-based shortest path search.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u901a\u7528\u56fe\u4e0a\u8fdb\u884c\u6700\u77ed\u8def\u5f84\u641c\u7d22\uff0c\u65e0\u9700\u7279\u5b9a\u9886\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u6784\u5efa\u9aa8\u67b6\u56fe\u548cSGNN\u6765\u5b66\u4e60\u8282\u70b9\u5d4c\u5165\u5e76\u9884\u6d4b\u8ddd\u79bb\uff0c\u652f\u6301LSearch\u548cHLSearch\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u79cd\u56fe\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6700\u77ed\u8def\u5f84\u641c\u7d22\u65b9\u6cd5\u5728\u590d\u6742\u56fe\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u6216\u9700\u8981\u5927\u91cf\u9884\u5904\u7406\u7684\u7d22\u5f15\u6280\u672f\uff0c\u5b66\u4e60\u578b\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u7279\u5f81\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u6784\u5efa\u9aa8\u67b6\u56fe\u6355\u6349\u591a\u7ea7\u8ddd\u79bb\u548c\u8df3\u6570\u4fe1\u606f\uff0c\u4f7f\u7528SGNN\u5b66\u4e60\u8282\u70b9\u5d4c\u5165\u548c\u9884\u6d4b\u8ddd\u79bb\uff0c\u63d0\u51faLSearch\u548cHLSearch\u7b97\u6cd5\u8fdb\u884c\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u3002", "result": "\u5728\u4e94\u79cd\u771f\u5b9e\u56fe\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u901a\u7528\u56fe\u4e0a\u7684\u6700\u77ed\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01671", "pdf": "https://arxiv.org/pdf/2508.01671", "abs": "https://arxiv.org/abs/2508.01671", "authors": ["Guanting Ren", "Babar Shahzaad", "Balsam Alkouz", "Abdallah Lakhdari", "Athman Bouguettaya"], "title": "Energy-Predictive Planning for Optimizing Drone Service Delivery", "categories": ["cs.RO", "cs.DC", "cs.ET"], "comment": "37 pages, 16 figures. This is an accepted paper, and it is going to\n  appear in the Expert Systems with Applications journal", "summary": "We propose a novel Energy-Predictive Drone Service (EPDS) framework for\nefficient package delivery within a skyway network. The EPDS framework\nincorporates a formal modeling of an EPDS and an adaptive bidirectional Long\nShort-Term Memory (Bi-LSTM) machine learning model. This model predicts the\nenergy status and stochastic arrival times of other drones operating in the\nsame skyway network. Leveraging these predictions, we develop a heuristic\noptimization approach for composite drone services. This approach identifies\nthe most time-efficient and energy-efficient skyway path and recharging\nschedule for each drone in the network. We conduct extensive experiments using\na real-world drone flight dataset to evaluate the performance of the proposed\nframework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u9884\u6d4b\u7684\u65e0\u4eba\u673a\u670d\u52a1\u6846\u67b6(EPDS)\uff0c\u7528\u4e8e\u4f18\u5316\u7a7a\u8def\u7f51\u7edc\u4e2d\u7684\u5305\u88f9\u914d\u9001\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u65e0\u4eba\u673a\u5728\u7a7a\u8def\u7f51\u7edc\u4e2d\u914d\u9001\u5305\u88f9\u7684\u6548\u7387\u548c\u80fd\u91cf\u5229\u7528\u7387\u3002", "method": "\u7ed3\u5408\u53cc\u5411LSTM\u6a21\u578b\u9884\u6d4b\u80fd\u91cf\u72b6\u6001\u548c\u968f\u673a\u5230\u8fbe\u65f6\u95f4\uff0c\u5e76\u4f7f\u7528\u542f\u53d1\u5f0f\u4f18\u5316\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u8def\u5f84\u548c\u5145\u7535\u8ba1\u5212\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EPDS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u914d\u9001\u7684\u65f6\u95f4\u548c\u80fd\u91cf\u6548\u7387\u3002"}}
{"id": "2508.02595", "pdf": "https://arxiv.org/pdf/2508.02595", "abs": "https://arxiv.org/abs/2508.02595", "authors": ["Siamak Abdi", "Giuseppe Di Fatta", "Atta Badii", "Giancarlo Fortino"], "title": "Fully Decentralised Consensus for Extreme-scale Blockchain", "categories": ["cs.DC", "cs.NI"], "comment": "IEEE Global Blockchain Conference (GBC) 2025", "summary": "Blockchain is a decentralised, immutable ledger technology that has been\nwidely adopted in many sectors for various applications such as\ncryptocurrencies, smart contracts and supply chain management. Distributed\nconsensus is a fundamental component of blockchain, which is required to ensure\ntrust, security, and integrity of the data stored and the transactions\nprocessed in the blockchain. Various consensus algorithms have been developed,\neach affected from certain issues such as node failures, high resource\nconsumption, collusion, etc. This work introduces a fully decentralised\nconsensus protocol, Blockchain Epidemic Consensus Protocol (BECP), suitable for\nvery large and extreme-scale blockchain systems. The proposed approach\nleverages the benefits of epidemic protocols, such as no reliance on a fixed\nset of validators or leaders, probabilistic guarantees of convergence,\nefficient use of network resources, and tolerance to node and network failures.\nA comparative experimental analysis has been carried out with traditional\nprotocols including PAXOS, RAFT, and Practical Byzantine Fault Tolerance\n(PBFT), as well as a relatively more recent protocol such as Avalanche, which\nis specifically designed for very large-scale systems. The results illustrate\nhow BECP outperforms them in terms of throughput, scalability and consensus\nlatency. BECP achieves an average of 1.196 times higher throughput in terms of\nconsensus on items and 4.775 times better average consensus latency.\nFurthermore, BECP significantly reduces the number of messages compared to\nAvalanche. These results demonstrate the effectiveness and efficiency of fully\ndecentralised consensus for blockchain technology based on epidemic protocols.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u5171\u8bc6\u534f\u8baeBECP\uff0c\u5229\u7528\u6d41\u884c\u75c5\u534f\u8bae\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u8d85\u5927\u89c4\u6a21\u533a\u5757\u94fe\u7cfb\u7edf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBECP\u5728\u541e\u5410\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u5171\u8bc6\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u53ca\u65b0\u578b\u534f\u8bae\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u5171\u8bc6\u7b97\u6cd5\u5b58\u5728\u8282\u70b9\u6545\u969c\u3001\u9ad8\u8d44\u6e90\u6d88\u8017\u7b49\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u5171\u8bc6\u534f\u8bae\uff0c\u4ee5\u9002\u5e94\u8d85\u5927\u89c4\u6a21\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u6d41\u884c\u75c5\u534f\u8bae\u8bbe\u8ba1BECP\uff0c\u7279\u70b9\u662f\u4e0d\u4f9d\u8d56\u56fa\u5b9a\u9a8c\u8bc1\u8005\u96c6\u3001\u63d0\u4f9b\u6982\u7387\u6536\u655b\u4fdd\u8bc1\u3001\u9ad8\u6548\u5229\u7528\u7f51\u7edc\u8d44\u6e90\uff0c\u5e76\u5bf9\u8282\u70b9\u548c\u7f51\u7edc\u6545\u969c\u5177\u6709\u5bb9\u5fcd\u6027\u3002", "result": "BECP\u5728\u541e\u5410\u91cf\u548c\u5171\u8bc6\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8ePAXOS\u3001RAFT\u3001PBFT\u548cAvalanche\u7b49\u534f\u8bae\uff0c\u6d88\u606f\u6570\u91cf\u4e5f\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "BECP\u8bc1\u660e\u4e86\u57fa\u4e8e\u6d41\u884c\u75c5\u534f\u8bae\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5171\u8bc6\u5728\u533a\u5757\u94fe\u6280\u672f\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u9002\u5408\u8d85\u5927\u89c4\u6a21\u7cfb\u7edf\u3002"}}
{"id": "2508.00947", "pdf": "https://arxiv.org/pdf/2508.00947", "abs": "https://arxiv.org/abs/2508.00947", "authors": ["Shiyao Sang", "Yinggang Ling"], "title": "Service Discovery-Based Hybrid Network Middleware for Efficient Communication in Distributed Robotic Systems", "categories": ["cs.RO", "cs.DC", "cs.NI", "I.2.9; C.2.4; D.4.7"], "comment": "8 pages, 8 figures, accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025, oral presentation", "summary": "Robotic middleware is fundamental to ensuring reliable communication among\nsystem components and is crucial for intelligent robotics, autonomous vehicles,\nand smart manufacturing. However, existing robotic middleware often struggles\nto meet the diverse communication demands, optimize data transmission\nefficiency, and maintain scheduling determinism between Orin computing units in\nlarge-scale L4 autonomous vehicle deployments. This paper presents RIMAOS2C, a\nservice discovery-based hybrid network communication middleware designed to\ntackle these challenges. By leveraging multi-level service discovery multicast,\nRIMAOS2C supports a wide variety of communication modes, including multiple\ncross-chip Ethernet protocols and PCIe communication capabilities. Its core\nmechanism, the Message Bridge, optimizes data flow forwarding and employs\nshared memory for centralized message distribution, reducing message redundancy\nand minimizing transmission delay uncertainty. Tested on L4 vehicles and Jetson\nOrin domain controllers, RIMAOS2C leverages TCP-based ZeroMQ to overcome the\nlarge-message transmission bottleneck in native CyberRT. In scenarios with two\ncross-chip subscribers, it eliminates message redundancy and improves\nlarge-data transmission efficiency by 36 to 40 percent while reducing callback\nlatency variation by 42 to 906 percent. This research advances the\ncommunication capabilities of robotic operating systems and proposes a novel\napproach to optimizing communication in distributed computing architectures for\nautonomous driving.", "AI": {"tldr": "RIMAOS2C\u662f\u4e00\u79cd\u57fa\u4e8e\u670d\u52a1\u53d1\u73b0\u7684\u6df7\u5408\u7f51\u7edc\u901a\u4fe1\u4e2d\u95f4\u4ef6\uff0c\u89e3\u51b3\u4e86L4\u81ea\u52a8\u9a7e\u9a76\u4e2d\u901a\u4fe1\u591a\u6837\u6027\u3001\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u8c03\u5ea6\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7Message Bridge\u673a\u5236\u4f18\u5316\u6570\u636e\u6d41\uff0c\u63d0\u5347\u5927\u6d88\u606f\u4f20\u8f93\u6548\u7387\u5e76\u51cf\u5c11\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u5728\u5927\u89c4\u6a21L4\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u901a\u4fe1\u9700\u6c42\u3001\u4f18\u5316\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u786e\u4fdd\u8c03\u5ea6\u786e\u5b9a\u6027\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faRIMAOS2C\uff0c\u5229\u7528\u591a\u7ea7\u670d\u52a1\u53d1\u73b0\u591a\u64ad\u652f\u6301\u591a\u79cd\u901a\u4fe1\u6a21\u5f0f\uff0c\u901a\u8fc7Message Bridge\u4f18\u5316\u6570\u636e\u6d41\u548c\u5171\u4eab\u5185\u5b58\u5206\u53d1\u6d88\u606f\u3002", "result": "\u5728L4\u8f66\u8f86\u548cJetson Orin\u63a7\u5236\u5668\u6d4b\u8bd5\u4e2d\uff0cRIMAOS2C\u6d88\u9664\u6d88\u606f\u5197\u4f59\uff0c\u5927\u6d88\u606f\u4f20\u8f93\u6548\u7387\u63d0\u534736-40\uff05\uff0c\u56de\u8c03\u5ef6\u8fdf\u53d8\u5316\u51cf\u5c1142-906\uff05\u3002", "conclusion": "RIMAOS2C\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u901a\u4fe1\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u901a\u4fe1\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2508.02279", "pdf": "https://arxiv.org/pdf/2508.02279", "abs": "https://arxiv.org/abs/2508.02279", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "title": "Dialogue Systems Engineering: A Survey and Future Directions", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u5bf9\u8bdd\u7cfb\u7edf\u751f\u547d\u5468\u671f\u7684\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u79f0\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\uff0c\u5e76\u8c03\u67e5\u8be5\u9886\u57df\u53ca\u5176\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u5bf9\u8bdd\u7cfb\u7edf\u6838\u5fc3\u6280\u672f\u663e\u8457\u53d1\u5c55\uff0c\u9700\u8981\u6b63\u786e\u9ad8\u6548\u5730\u6784\u5efa\u3001\u8fd0\u884c\u548c\u6539\u8fdb\u5bf9\u8bdd\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8eSWEBOK 4.0\u5b9a\u4e49\u7684\u8f6f\u4ef6\u5de5\u7a0b\u77e5\u8bc6\u9886\u57df\uff0c\u5217\u4e3e\u5e76\u8c03\u67e5\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u7684\u77e5\u8bc6\u9886\u57df\u3002", "result": "\u8bc6\u522b\u4e86\u6bcf\u4e2a\u9886\u57df\u4e2d\u672a\u63a2\u7d22\u7684\u4e3b\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u7684\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u9700\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5546\u4e1a\u548c\u793e\u4f1a\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2508.01279", "pdf": "https://arxiv.org/pdf/2508.01279", "abs": "https://arxiv.org/abs/2508.01279", "authors": ["Jiajun Zhu", "Xinyu Cheng", "Zhongsu Luo", "Yunfan Zhou", "Xinhuan Shu", "Di Weng", "Yingcai Wu"], "title": "ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts", "categories": ["cs.HC"], "comment": "Accepted at Annual ACM Symposium on User Interface Software and\n  Technology (UIST'25), September 28-October 1, 2025, Busan, Republic of Korea", "summary": "Large language models (LLMs) enable the rapid generation of data wrangling\nscripts based on natural language instructions, but these scripts may not fully\nadhere to user-specified requirements, necessitating careful inspection and\niterative refinement. Existing approaches primarily assist users in\nunderstanding script logic and spotting potential issues themselves, rather\nthan providing direct validation of correctness. To enhance debugging\nefficiency and optimize the user experience, we develop ViseGPT, a tool that\nautomatically extracts constraints from user prompts to generate comprehensive\ntest cases for verifying script reliability. The test results are then\ntransformed into a tailored Gantt chart, allowing users to intuitively assess\nalignment with semantic requirements and iteratively refine their scripts. Our\ndesign decisions are informed by a formative study (N=8) that explores user\npractices and challenges. We further evaluate the effectiveness and usability\nof ViseGPT through a user study (N=18). Results indicate that ViseGPT\nsignificantly improves debugging efficiency for LLM-generated data-wrangling\nscripts, enhances users' ability to detect and correct issues, and streamlines\nthe workflow experience.", "AI": {"tldr": "ViseGPT\u901a\u8fc7\u81ea\u52a8\u63d0\u53d6\u7528\u6237\u63d0\u793a\u4e2d\u7684\u7ea6\u675f\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u9a8c\u8bc1\u811a\u672c\u53ef\u9760\u6027\uff0c\u5e76\u4ee5\u7518\u7279\u56fe\u5c55\u793a\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u8c03\u8bd5\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u81ea\u884c\u68c0\u67e5\u811a\u672c\u903b\u8f91\u548c\u95ee\u9898\uff0c\u7f3a\u4e4f\u76f4\u63a5\u9a8c\u8bc1\u529f\u80fd\uff0cViseGPT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1ViseGPT\u5de5\u5177\uff0c\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u901a\u8fc7\u7518\u7279\u56fe\u76f4\u89c2\u5c55\u793a\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cViseGPT\u663e\u8457\u63d0\u9ad8\u4e86\u8c03\u8bd5\u6548\u7387\uff0c\u589e\u5f3a\u4e86\u95ee\u9898\u68c0\u6d4b\u4e0e\u4fee\u6b63\u80fd\u529b\uff0c\u4f18\u5316\u4e86\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "ViseGPT\u4e3aLLM\u751f\u6210\u7684\u6570\u636e\u6574\u7406\u811a\u672c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u63d0\u5347\u4e86\u7528\u6237\u64cd\u4f5c\u6548\u7387\u4e0e\u4f53\u9a8c\u3002"}}
{"id": "2508.01765", "pdf": "https://arxiv.org/pdf/2508.01765", "abs": "https://arxiv.org/abs/2508.01765", "authors": ["Kaining Zhang", "Catarina Moreira", "Pedro Belchior", "Gun Lee", "Mark Billinghurst", "Joaquim Jorge"], "title": "HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "We introduce \\textit{HeadZoom}, a hands-free interaction technique for\nnavigating two-dimensional visual content using head movements. The system\nenables fluid zooming and panning by only using real-time head tracking. It\nsupports natural control in applications such as map exploration, radiograph\ninspection, and image browsing, particularly where physical interaction is\nlimited. We evaluated HeadZoom in a within-subjects user study comparing three\ninteraction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial,\nerror, and subjective metrics. Results show that Parallel Zoom significantly\nreduced total head movement compared to Static and Tilt modes. Users reported\nsignificantly lower perceived exertion for Parallel Zoom, confirming its\nsuitability for prolonged or precision-based tasks. By minimising movement\ndemands while maintaining task effectiveness, HeadZoom advances the design of\nhead-based 2D interaction in VR, creating new opportunities for immersive,\naccessible, and hands-free systems for image exploration.", "AI": {"tldr": "HeadZoom\u662f\u4e00\u79cd\u57fa\u4e8e\u5934\u90e8\u8fd0\u52a8\u7684\u514d\u624b\u52a8\u4e8c\u7ef4\u89c6\u89c9\u5185\u5bb9\u5bfc\u822a\u6280\u672f\uff0c\u652f\u6301\u6d41\u7545\u7f29\u653e\u548c\u5e73\u79fb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7269\u7406\u4ea4\u4e92\u53d7\u9650\u7684\u573a\u666f\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u514d\u624b\u52a8\u4ea4\u4e92\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u7269\u7406\u4ea4\u4e92\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4e8c\u7ef4\u89c6\u89c9\u5185\u5bb9\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u65f6\u5934\u90e8\u8ffd\u8e2a\u5b9e\u73b0\u7f29\u653e\u548c\u5e73\u79fb\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u4ea4\u4e92\u6280\u672f\uff08Static\u3001Tilt Zoom\u3001Parallel Zoom\uff09\u7684\u6027\u80fd\u3002", "result": "Parallel Zoom\u663e\u8457\u51cf\u5c11\u4e86\u5934\u90e8\u8fd0\u52a8\u603b\u91cf\uff0c\u7528\u6237\u611f\u77e5\u52aa\u529b\u66f4\u4f4e\uff0c\u9002\u5408\u957f\u65f6\u95f4\u6216\u7cbe\u786e\u4efb\u52a1\u3002", "conclusion": "HeadZoom\u4e3aVR\u4e2d\u57fa\u4e8e\u5934\u90e8\u7684\u4e8c\u7ef4\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u65b9\u5411\uff0c\u652f\u6301\u66f4\u6c89\u6d78\u548c\u6613\u7528\u7684\u514d\u624b\u52a8\u56fe\u50cf\u63a2\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2508.01469", "pdf": "https://arxiv.org/pdf/2508.01469", "abs": "https://arxiv.org/abs/2508.01469", "authors": ["Imtiaz Karim", "Hyunwoo Lee", "Hassan Asghar", "Kazi Samin Mubasshir", "Seulgi Han", "Mashroor Hasan Bhuiyan", "Elisa Bertino"], "title": "VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments", "categories": ["cs.CR", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "We present VWAttacker, the first systematic testing framework for analyzing\nthe security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations.\nVWAttacker includes a complete VoWiFi network testbed that communicates with\nCommercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the\nbehavior of diverse VoWiFi UE implementations; uses property-guided adversarial\ntesting to uncover security issues in different UEs systematically. To reduce\nmanual effort in extracting and testing properties, we introduce an LLM-based,\nsemi-automatic, and scalable approach for property extraction and testcase (TC)\ngeneration. These TCs are systematically mutated by two domain-specific\ntransformations. Furthermore, we introduce two deterministic oracles to detect\nproperty violations automatically. Coupled with these techniques, VWAttacker\nextracts 63 properties from 11 specifications, evaluates 1,116 testcases, and\ndetects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret\nto 0 to supporting weak algorithms. These issues result in attacks that expose\nthe victim UE's identity or establish weak channels, thus severely hampering\nthe security of cellular networks. We responsibly disclose the findings to all\nthe related vendors. At the time of writing, one of the vulnerabilities has\nbeen acknowledged by MediaTek with high severity.", "AI": {"tldr": "VWAttacker\u662f\u4e00\u4e2a\u6d4b\u8bd5VoWiFi UE\u5b9e\u73b0\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u548c\u5c5e\u6027\u5f15\u5bfc\u6d4b\u8bd5\u53d1\u73b0\u591a\u4e2a\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u7814\u7a76VoWiFi UE\u5b9e\u73b0\u7684\u5b89\u5168\u6027\u6f0f\u6d1e\uff0c\u4ee5\u63d0\u5347\u8702\u7a9d\u7f51\u7edc\u7684\u5b89\u5168\u3002", "method": "\u7ed3\u5408LLM\u7684\u534a\u81ea\u52a8\u5c5e\u6027\u63d0\u53d6\u548c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\uff0c\u7cfb\u7edf\u6027\u53d8\u5f02\u548c\u786e\u5b9a\u6027\u68c0\u6d4b\u3002", "result": "\u68c0\u6d4b\u523021\u4e2aUE\u4e2d\u768413\u4e2a\u95ee\u9898\uff0c\u5305\u62ec\u8eab\u4efd\u66b4\u9732\u548c\u5f31\u4fe1\u9053\u5efa\u7acb\u7b49\u3002", "conclusion": "VWAttacker\u6709\u6548\u53d1\u73b0\u5b89\u5168\u95ee\u9898\uff0c\u90e8\u5206\u95ee\u9898\u5df2\u88ab\u5382\u5546\u786e\u8ba4\u4e3a\u9ad8\u98ce\u9669\u3002"}}
{"id": "2508.02335", "pdf": "https://arxiv.org/pdf/2508.02335", "abs": "https://arxiv.org/abs/2508.02335", "authors": ["Matteo Cancellieri", "Martin Docekal", "David Pride", "Morane Gruenpeter", "David Douard", "Petr Knoth"], "title": "Interoperable verification and dissemination of software assets in repositories using COAR Notify", "categories": ["cs.SE", "cs.DL"], "comment": "8 pages. Presented at the 20th International Conference on Open\n  Repositories, June 15-18 2025, Chicago, Illinois, USA", "summary": "The discoverability, attribution, and reusability of open research software\nare often hindered by its obscurity within academic manuscripts. To address\nthis, the SoFAIR project (2024-2025) introduces a comprehensive workflow\nleveraging machine learning tools for extracting software mentions from\nresearch papers. The project integrates repository systems, authors, and\nservices like HAL and Software Heritage to ensure proper archiving, citation,\nand accessibility of research software in alignment with FAIR principles. To\nenable interoperable communication across the various systems we present an\nintegration of the COAR Notify Protocol, which facilitates automated,\ninteroperable communication among repositories and authors to validate and\ndisseminate software mentions. This paper outlines the SoFAIR workflow and the\nimplementation of the COAR Notify Protocol, emphasising its potential to\nenhance the visibility and credibility of research software as first-class\nbibliographic records.", "AI": {"tldr": "SoFAIR\u9879\u76ee\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5de5\u5177\u4ece\u8bba\u6587\u4e2d\u63d0\u53d6\u8f6f\u4ef6\u5f15\u7528\uff0c\u7ed3\u5408COAR Notify\u534f\u8bae\uff0c\u63d0\u9ad8\u5f00\u6e90\u7814\u7a76\u8f6f\u4ef6\u7684\u53ef\u89c1\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f00\u6e90\u7814\u7a76\u8f6f\u4ef6\u5728\u5b66\u672f\u8bba\u6587\u4e2d\u5e38\u5e38\u88ab\u5ffd\u89c6\uff0c\u5f71\u54cd\u5176\u53ef\u53d1\u73b0\u6027\u3001\u5f52\u56e0\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002", "method": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u5de5\u5177\u63d0\u53d6\u8f6f\u4ef6\u5f15\u7528\uff0c\u96c6\u6210HAL\u548cSoftware Heritage\u7b49\u7cfb\u7edf\uff0c\u91c7\u7528COAR Notify\u534f\u8bae\u5b9e\u73b0\u8de8\u7cfb\u7edf\u901a\u4fe1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u786e\u4fdd\u7814\u7a76\u8f6f\u4ef6\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u5f15\u7528\u548c\u5b58\u6863\u7b26\u5408FAIR\u539f\u5219\u3002", "conclusion": "SoFAIR\u9879\u76ee\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u63d0\u5347\u4e86\u7814\u7a76\u8f6f\u4ef6\u7684\u53ef\u89c6\u5316\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u5176\u4f5c\u4e3a\u4e00\u6d41\u6587\u732e\u8bb0\u5f55\u7684\u6807\u51c6\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.01282", "pdf": "https://arxiv.org/pdf/2508.01282", "abs": "https://arxiv.org/abs/2508.01282", "authors": ["Jiawei Li", "Linjie Qiu", "Zhiqing Wu", "Qiongyan Chen", "Ziyan Wang", "Mingming Fan"], "title": "ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance", "categories": ["cs.HC"], "comment": "10 pages, 5 figures, Proceedings of the 33rd ACM International\n  Conference on Multimedia", "summary": "Older adults tend to encounter challenges when learning to use new smartphone\napps due to age-related cognitive and physical changes. Compared to traditional\nsupport methods such as video tutorials, trial-and-error allows older adults to\nlearn to use smartphone apps by making and correcting mistakes. However, it\nremains unknown how trial-and-error should be designed to empower older adults\nto use smartphone apps and how well it would work for older adults. Informed by\nthe guidelines derived from prior work, we designed and implemented ExplorAR,\nan AR-based trial-and-error system that offers real-time and situated visual\nguidance in the augmented space around the smartphone to empower older adults\nto explore and correct mistakes independently. We conducted a user study with\n18 older adults to compare ExplorAR with traditional video tutorials and a\nsimplified version of ExplorAR. Results show that the AR-supported\ntrial-and-error method enhanced older adults' learning experience by fostering\ndeeper cognitive engagement and improving confidence in exploring unknown\noperations.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86ExplorAR\u7cfb\u7edf\uff0c\u901a\u8fc7AR\u6280\u672f\u652f\u6301\u8001\u5e74\u4eba\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u667a\u80fd\u624b\u673a\u5e94\u7528\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6bd4\u89c6\u9891\u6559\u7a0b\u66f4\u6709\u6548\u3002", "motivation": "\u8001\u5e74\u4eba\u56e0\u8ba4\u77e5\u548c\u751f\u7406\u53d8\u5316\u5728\u5b66\u4e60\u4f7f\u7528\u65b0\u667a\u80fd\u624b\u673a\u5e94\u7528\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u8bd5\u9519\u6cd5\u53ef\u80fd\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0ExplorAR\u7cfb\u7edf\uff0c\u63d0\u4f9b\u5b9e\u65f6AR\u89c6\u89c9\u6307\u5bfc\uff0c\u4e0e\u89c6\u9891\u6559\u7a0b\u548c\u7b80\u5316\u7248\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "AR\u8bd5\u9519\u6cd5\u589e\u5f3a\u4e86\u8001\u5e74\u4eba\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u6df1\u5ea6\u8ba4\u77e5\u53c2\u4e0e\u5e76\u63d0\u5347\u63a2\u7d22\u4fe1\u5fc3\u3002", "conclusion": "AR\u652f\u6301\u7684\u8bd5\u9519\u6cd5\u662f\u8001\u5e74\u4eba\u5b66\u4e60\u667a\u80fd\u624b\u673a\u5e94\u7528\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.01784", "pdf": "https://arxiv.org/pdf/2508.01784", "abs": "https://arxiv.org/abs/2508.01784", "authors": ["Xin He", "Junxi Shen", "Zhenheng Tang", "Xiaowen Chu", "Bo Li", "Ivor W. Tsang", "Yew-Soon Ong"], "title": "RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "comment": "MoE, Model Merging, Fingerprint", "summary": "Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution\nfor consolidating multiple task-specific models into a unified sparse\narchitecture, where each expert is derived from a model fine-tuned on a\ndistinct task. While effective for multi-task integration, this paradigm\nintroduces a critical yet underexplored challenge: how to attribute and protect\nthe intellectual property (IP) of individual experts after merging. We propose\nRouteMark, a framework for IP protection in merged MoE models through the\ndesign of expert routing fingerprints. Our key insight is that task-specific\nexperts exhibit stable and distinctive routing behaviors under probing inputs.\nTo capture these patterns, we construct expert-level fingerprints using two\ncomplementary statistics: the Routing Score Fingerprint (RSF), quantifying the\nintensity of expert activation, and the Routing Preference Fingerprint (RPF),\ncharacterizing the input distribution that preferentially activates each\nexpert. These fingerprints are reproducible, task-discriminative, and\nlightweight to construct. For attribution and tampering detection, we introduce\na similarity-based matching algorithm that compares expert fingerprints between\na suspect and a reference (victim) model. Extensive experiments across diverse\ntasks and CLIP-based MoE architectures show that RouteMark consistently yields\nhigh similarity for reused experts and clear separation from unrelated ones.\nMoreover, it remains robust against both structural tampering (expert\nreplacement, addition, deletion) and parametric tampering (fine-tuning,\npruning, permutation), outperforming weight- and activation-based baseliness.\nOur work lays the foundation for RouteMark as a practical and broadly\napplicable framework for IP verification in MoE-based model merging.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRouteMark\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e13\u5bb6\u8def\u7531\u6307\u7eb9\u4fdd\u62a4\u5408\u5e76MoE\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u4ea7\u6743\uff08IP\uff09\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5408\u5e76MoE\u6a21\u578b\u4e2d\u4e13\u5bb6\u77e5\u8bc6\u4ea7\u6743\u5f52\u5c5e\u548c\u4fdd\u62a4\u7684\u672a\u63a2\u7d22\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u4e13\u5bb6\u8def\u7531\u884c\u4e3a\u6784\u5efa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6307\u7eb9\uff08RSF\u548cRPF\uff09\uff0c\u5e76\u8bbe\u8ba1\u76f8\u4f3c\u6027\u5339\u914d\u7b97\u6cd5\u8fdb\u884cIP\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRouteMark\u80fd\u6709\u6548\u8bc6\u522b\u88ab\u91cd\u7528\u4e13\u5bb6\uff0c\u5e76\u5bf9\u6297\u591a\u79cd\u7be1\u6539\u64cd\u4f5c\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "RouteMark\u4e3aMoE\u6a21\u578b\u5408\u5e76\u4e2d\u7684IP\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00960", "pdf": "https://arxiv.org/pdf/2508.00960", "abs": "https://arxiv.org/abs/2508.00960", "authors": ["Sudip K. Seal", "Maksudul Alam", "Jorge Ramirez", "Sajal Dash", "Hao Lu"], "title": "Compression-Induced Communication-Efficient Large Model Training and Inferencing", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Energy efficiency of training and inferencing with large neural network\nmodels is a critical challenge facing the future of sustainable large-scale\nmachine learning workloads. This paper introduces an alternative strategy,\ncalled phantom parallelism, to minimize the net energy consumption of\ntraditional tensor (model) parallelism, the most energy-inefficient component\nof large neural network training. The approach is presented in the context of\nfeed-forward network architectures as a preliminary, but comprehensive,\nproof-of-principle study of the proposed methodology. We derive new forward and\nbackward propagation operators for phantom parallelism, implement them as\ncustom autograd operations within an end-to-end phantom parallel training\npipeline and compare its parallel performance and energy-efficiency against\nthose of conventional tensor parallel training pipelines. Formal analyses that\npredict lower bandwidth and FLOP counts are presented with supporting empirical\nresults on up to 256 GPUs that corroborate these gains. Experiments are shown\nto deliver ~50% reduction in the energy consumed to train FFNs using the\nproposed phantom parallel approach when compared with conventional tensor\nparallel methods. Additionally, the proposed approach is shown to train smaller\nphantom models to the same model loss on smaller GPU counts as larger tensor\nparallel models on larger GPU counts offering the possibility for even greater\nenergy savings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5e7b\u5f71\u5e76\u884c\u201d\u7684\u65b0\u7b56\u7565\uff0c\u65e8\u5728\u51cf\u5c11\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u4f20\u7edf\u5f20\u91cf\u5e76\u884c\u7684\u80fd\u6e90\u6d88\u8017\uff0c\u5b9e\u9a8c\u663e\u793a\u80fd\u8282\u7701\u7ea650%\u7684\u8bad\u7ec3\u80fd\u6e90\u3002", "motivation": "\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u9ad8\u80fd\u8017\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u53ef\u6301\u7eed\u7684\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5e7b\u5f71\u5e76\u884c\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edf\u5f20\u91cf\u5e76\u884c\u7684\u9ad8\u80fd\u8017\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u65b0\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u7b97\u5b50\uff0c\u5e76\u5c06\u5176\u5b9e\u73b0\u4e3a\u81ea\u5b9a\u4e49\u7684\u81ea\u52a8\u5fae\u5206\u64cd\u4f5c\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5e7b\u5f71\u5e76\u884c\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728256\u4e2aGPU\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5e7b\u5f71\u5e76\u884c\u80fd\u663e\u8457\u51cf\u5c11\u5e26\u5bbd\u548c\u6d6e\u70b9\u8fd0\u7b97\u9700\u6c42\uff0c\u8bad\u7ec3\u80fd\u6e90\u6d88\u8017\u964d\u4f4e\u7ea650%\uff0c\u4e14\u5728\u5c0f\u89c4\u6a21GPU\u4e0a\u4e5f\u80fd\u8fbe\u5230\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u540c\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5e7b\u5f71\u5e76\u884c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u80fd\u6e90\u6d88\u8017\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u53ef\u6301\u7eed\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2508.02338", "pdf": "https://arxiv.org/pdf/2508.02338", "abs": "https://arxiv.org/abs/2508.02338", "authors": ["Jiahui Wu", "Chengjie Lu", "Aitor Arrieta", "Shaukat Ali", "Thomas Peyrucain"], "title": "Vision Language Model-based Testing of Industrial Autonomous Mobile Robots", "categories": ["cs.SE", "cs.RO"], "comment": null, "summary": "Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,\nwarehouses, retail spaces, and offices), where they work alongside humans.\nGiven that human behavior can be unpredictable and that AMRs may not have been\ntrained to handle all possible unknown and uncertain behaviors, it is important\nto test AMRs under a wide range of human interactions to ensure their safe\nbehavior. Moreover, testing in real environments with actual AMRs and humans is\noften costly, impractical, and potentially hazardous (e.g., it could result in\nhuman injury). To this end, we propose a Vision Language Model (VLM)-based\ntesting approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.\nBased on the functional and safety requirements, RVSG uses the VLM to generate\ndiverse human behaviors that violate these requirements. We evaluated RVSG with\nseveral requirements and navigation routes in a simulator using the latest AMR\nfrom PAL Robotics. Our results show that, compared with the baseline, RVSG can\neffectively generate requirement-violating scenarios. Moreover, RVSG-generated\nscenarios increase variability in robot behavior, thereby helping reveal their\nuncertain behaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff08RVSG\uff09\uff0c\u7528\u4e8e\u5de5\u4e1a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff08AMR\uff09\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u4eba\u7c7b\u884c\u4e3a\u8fdd\u53cd\u529f\u80fd\u548c\u5b89\u5168\u8981\u6c42\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u5b9e\u9645\u6d4b\u8bd5\u7684\u9ad8\u6210\u672c\u4e0e\u5371\u9669\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u6d4b\u8bd5\u65b9\u6cd5\u6765\u9a8c\u8bc1AMR\u5728\u4e0d\u540c\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528VLM\u751f\u6210\u8fdd\u53cd\u529f\u80fd\u548c\u5b89\u5168\u8981\u6c42\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u5e76\u5728\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30RVSG\u7684\u6027\u80fd\u3002", "result": "RVSG\u80fd\u6709\u6548\u751f\u6210\u8fdd\u53cd\u8981\u6c42\u7684\u60c5\u666f\uff0c\u5e76\u589e\u52a0\u673a\u5668\u4eba\u884c\u4e3a\u7684\u591a\u6837\u6027\uff0c\u5e2e\u52a9\u63ed\u793a\u5176\u4e0d\u786e\u5b9a\u884c\u4e3a\u3002", "conclusion": "RVSG\u4e3aAMR\u7684\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7387\u548c\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01318", "pdf": "https://arxiv.org/pdf/2508.01318", "abs": "https://arxiv.org/abs/2508.01318", "authors": ["Zheng Lian"], "title": "AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Emotion Recognition", "categories": ["cs.HC"], "comment": null, "summary": "Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict\nemotions without being constrained by predefined label spaces, enabling\nfine-grained and human-like emotion understanding. Unlike traditional\ndiscriminative methods, OV-MER leverages generative models, such as large\nlanguage models (LLMs) with extensive vocabularies, to capture the full\nspectrum of emotions. Previous approaches (like AffectGPT) primarily rely on\ntoken-level loss for training. However, this objective does not align with the\nemotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately,\nEW-based metrics cannot be directly optimized via gradient backpropagation. In\nthis paper, we propose AffectGPT-R1, a reinforcement learning framework that\ndirectly optimizes performance on EW-based metrics. Specifically, we treat\nthese metrics as the reward function and employ Group Relative Policy\nOptimization (GRPO) to maximize rewards. Experimental results demonstrate that\nAffectGPT-R1 achieves significant improvements on OV-MER. We hope this work\nadvances the field of multimodal emotion recognition. Our code will be publicly\navailable at:https://github.com/zeroQiaoba/AffectGPT.", "AI": {"tldr": "AffectGPT-R1 \u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316\u57fa\u4e8e\u60c5\u611f\u8f6e\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684 token-level \u635f\u5931\u8bad\u7ec3\u76ee\u6807\u4e0e\u57fa\u4e8e\u60c5\u611f\u8f6e\u7684\u8bc4\u4ef7\u6307\u6807\u4e0d\u4e00\u81f4\uff0c\u4e14\u65e0\u6cd5\u76f4\u63a5\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u878d\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5c06\u8bc4\u4ef7\u6307\u6807\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\uff0c\u91c7\u7528 GRPO \u4f18\u5316\u6a21\u578b\u3002", "result": "AffectGPT-R1 \u5728 OV-MER \u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u63a8\u8fdb\u4e86\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2508.01893", "pdf": "https://arxiv.org/pdf/2508.01893", "abs": "https://arxiv.org/abs/2508.01893", "authors": ["Cheng Chu", "Lei Jiang", "Fan Chen"], "title": "BVQC: A Backdoor-style Watermarking Scheme for Variational Quantum Circuits", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Variational Quantum Circuits (VQCs) have emerged as a powerful quantum\ncomputing paradigm, demonstrating a scaling advantage for problems intractable\nfor classical computation. As VQCs require substantial resources and\nspecialized expertise for their design, they represent significant intellectual\nproperties (IPs). However, existing quantum circuit watermarking techniques\nsuffer from two primary drawbacks: (1) watermarks can be removed during\nre-compilation of the circuits, and (2) these methods significantly increase\ntask loss due to the extensive length of the inserted watermarks across\nmultiple compilation stages. To address these challenges, we propose BVQC, a\nbackdoor-based watermarking technique for VQCs that preserves the original loss\nin typical execution settings, while deliberately increasing the loss to a\npredefined level during watermark extraction. Additionally, BVQC employs a\ngrouping algorithm to minimize the watermark task's interference with the base\ntask, ensuring optimal accuracy for the base task. BVQC retains the original\ncompilation workflow, ensuring robustness against re-compilation. Our\nevaluations show that BVQC greatly reduces Probabilistic Proof of Authorship\n(PPA) changes by 9.89e-3 and ground truth distance (GTD) by 0.089 compared to\nprior watermarking technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u95e8\u7684VQC\u6c34\u5370\u6280\u672fBVQC\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u56e0\u91cd\u65b0\u7f16\u8bd1\u5bfc\u81f4\u6c34\u5370\u4e22\u5931\u53ca\u4efb\u52a1\u635f\u5931\u589e\u52a0\u7684\u7f3a\u70b9\u3002", "motivation": "\u73b0\u6709VQC\u6c34\u5370\u6280\u672f\u6613\u56e0\u91cd\u65b0\u7f16\u8bd1\u4e22\u5931\u6c34\u5370\uff0c\u4e14\u63d2\u5165\u6c34\u5370\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6c34\u5370\u6280\u672f\u3002", "method": "BVQC\u91c7\u7528\u540e\u95e8\u673a\u5236\uff0c\u5728\u5e38\u89c4\u6267\u884c\u4e2d\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u4ec5\u5728\u6c34\u5370\u63d0\u53d6\u65f6\u589e\u52a0\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u5206\u7ec4\u7b97\u6cd5\u6700\u5c0f\u5316\u6c34\u5370\u5bf9\u57fa\u7840\u4efb\u52a1\u7684\u5e72\u6270\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0cBVQC\u5c06PPA\u53d8\u5316\u964d\u4f4e\u4e869.89e-3\uff0cGTD\u964d\u4f4e\u4e860.089\uff0c\u4e14\u4fdd\u6301\u4e86\u539f\u59cb\u7f16\u8bd1\u6d41\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "BVQC\u6709\u6548\u89e3\u51b3\u4e86VQC\u6c34\u5370\u6280\u672f\u7684\u4e24\u5927\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2508.01002", "pdf": "https://arxiv.org/pdf/2508.01002", "abs": "https://arxiv.org/abs/2508.01002", "authors": ["Agrim Bari", "Parikshit Hegde", "Gustavo de Veciana"], "title": "Optimal Scheduling Algorithms for LLM Inference: Theory and Practice", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "With the growing use of Large Language Model (LLM)-based tools like ChatGPT,\nPerplexity, and Gemini across industries, there is a rising need for efficient\nLLM inference systems. These systems handle requests with a unique two-phase\ncomputation structure: a prefill-phase that processes the full input prompt and\na decode-phase that autoregressively generates tokens one at a time. This\nstructure calls for new strategies for routing and scheduling requests.\n  In this paper, we take a comprehensive approach to this challenge by\ndeveloping a theoretical framework that models routing and scheduling in LLM\ninference systems. We identify two key design principles-optimal tiling and\ndynamic resource allocation-that are essential for achieving high throughput.\nGuided by these principles, we propose the Resource-Aware Dynamic (RAD)\nscheduler and prove that it achieves throughput optimality under mild\nconditions. To address practical Service Level Objectives (SLOs) such as\nserving requests with different Time Between Token (TBT) constraints, we design\nthe SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements\nto prioritize decode requests that are close to missing their TBT deadlines and\nreorders prefill requests based on known prompt lengths to further reduce the\nTime To First Token (TTFT) delays.\n  We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model\non an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the\nmedian TTFT by 53% and increases the maximum serving capacity by 26% such that\nmedian TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8c03\u5ea6\u5668SLAI\uff0c\u7528\u4e8e\u4f18\u5316LLM\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684\u8def\u7531\u548c\u8c03\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9996\u4ee4\u724c\u65f6\u95f4\u548c\u63d0\u9ad8\u4e86\u670d\u52a1\u5bb9\u91cf\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5de5\u5177\uff08\u5982ChatGPT\uff09\u5728\u5404\u884c\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9ad8\u6548\u7684LLM\u63a8\u7406\u7cfb\u7edf\u9700\u6c42\u589e\u957f\u3002\u8fd9\u7c7b\u7cfb\u7edf\u5177\u6709\u7279\u6b8a\u7684\u53cc\u9636\u6bb5\u8ba1\u7b97\u7ed3\u6784\uff0c\u9700\u8981\u65b0\u7684\u8def\u7531\u548c\u8c03\u5ea6\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u6700\u4f18\u5206\u5757\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u539f\u5219\uff0c\u8bbe\u8ba1\u4e86RAD\u8c03\u5ea6\u5668\u3002\u968f\u540e\u9488\u5bf9\u5b9e\u9645SLO\u9700\u6c42\uff0c\u5f00\u53d1\u4e86SLAI\u8c03\u5ea6\u5668\uff0c\u5229\u7528\u5b9e\u65f6\u6d4b\u91cf\u548c\u8bf7\u6c42\u91cd\u6392\u5e8f\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728Openchat ShareGPT4\u6570\u636e\u96c6\u548cMistral-7B\u6a21\u578b\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cSLAI\u5c06\u4e2d\u4f4d\u9996\u4ee4\u724c\u65f6\u95f4\u964d\u4f4e\u4e8653%\uff0c\u6700\u5927\u670d\u52a1\u5bb9\u91cf\u63d0\u9ad8\u4e8626%\uff0c\u540c\u65f6\u6ee1\u8db3\u5c3e\u90e8\u4ee4\u724c\u95f4\u65f6\u95f4\u5ef6\u8fdf\u7ea6\u675f\u3002", "conclusion": "SLAI\u8c03\u5ea6\u5668\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7cfb\u7edf\u7684\u6548\u7387\u548c\u670d\u52a1\u80fd\u529b\u3002"}}
{"id": "2508.01863", "pdf": "https://arxiv.org/pdf/2508.01863", "abs": "https://arxiv.org/abs/2508.01863", "authors": ["Sanjay Singh", "Mitendra Mahto"], "title": "Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS", "categories": ["cs.CR", "cs.NI", "cs.SE"], "comment": "6 pages, 3 figures", "summary": "In today's enterprise environment, traditional access methods such as Virtual\nPrivate Networks (VPNs) and application-specific Single Sign-On (SSO) often\nfall short when it comes to securely scaling access for a distributed and\ndynamic workforce. This paper presents our experience implementing a modern,\nZero Trust-aligned architecture that leverages a reverse proxy integrated with\nMutual TLS (mTLS) and centralized SSO, along with the key challenges we\nencountered and lessons learned during its deployment and scaling. This\nmultidimensional solution involves both per-device and per-user authentication,\ncentralized enforcement of security policies, and comprehensive observability,\nhence enabling organizations to deliver secure and seamless access to their\ninternal applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u4fe1\u4efb\u67b6\u6784\u7684\u591a\u7ef4\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u53cd\u5411\u4ee3\u7406\u3001mTLS\u548c\u96c6\u4e2d\u5f0fSSO\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u52a8\u6001\u52b3\u52a8\u529b\u7684\u5b89\u5168\u8bbf\u95ee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfVPN\u548cSSO\u65b9\u6cd5\u5728\u5b89\u5168\u6269\u5c55\u5206\u5e03\u5f0f\u52b3\u52a8\u529b\u8bbf\u95ee\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u73b0\u4ee3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u79cd\u96f6\u4fe1\u4efb\u5bf9\u9f50\u67b6\u6784\uff0c\u6574\u5408\u53cd\u5411\u4ee3\u7406\u3001mTLS\u548c\u96c6\u4e2d\u5f0fSSO\uff0c\u6d89\u53ca\u8bbe\u5907\u548c\u7528\u6237\u8ba4\u8bc1\u3001\u96c6\u4e2d\u5b89\u5168\u7b56\u7565\u548c\u5168\u9762\u76d1\u63a7\u3002", "result": "\u8be5\u65b9\u6848\u80fd\u591f\u4e3a\u5185\u90e8\u5e94\u7528\u63d0\u4f9b\u5b89\u5168\u65e0\u7f1d\u7684\u8bbf\u95ee\u3002", "conclusion": "\u901a\u8fc7\u73b0\u4ee3\u96f6\u4fe1\u4efb\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u90e8\u7f72\u548c\u6269\u5c55\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u7ecf\u9a8c\u3002"}}
{"id": "2508.02397", "pdf": "https://arxiv.org/pdf/2508.02397", "abs": "https://arxiv.org/abs/2508.02397", "authors": ["Lida Zhao", "Chaofan Li", "Yueming Wu", "Lyuye Zhang", "Jiahui Wu", "Chengwei Liu", "Sen Chen", "Yutao Hu", "Zhengzi Xu", "Yi Liu", "Jingquan Ge", "Jun Sun", "Yang Liu"], "title": "JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis", "categories": ["cs.SE"], "comment": null, "summary": "While reusing third-party libraries (TPL) facilitates software development,\nits chaotic management has brought great threats to software maintenance and\nthe unauthorized use of source code also raises ethical problems such as\nmisconduct on copyrighted code. To identify TPL reuse in projects, Software\nComposition Analysis (SCA) is employed, and two categories of SCA techniques\nare used based on how TPLs are introduced: clone-based SCA and\npackage-manager-based SCA (PM-based SCA). Although introducing TPLs by clones\nis prevalent in Java, no clone-based SCA tools are specially designed for Java.\nAlso, directly applying clone-based SCA techniques from other tools is\nproblematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA\ntool that aims to accurately and comprehensively identify instances of TPL\nreuse introduced by source code clones in Java projects. JC-Finder achieves\nboth accuracy and efficiency in identifying TPL reuse from code cloning by\ncapturing features at the class level, maintaining inter-function\nrelationships, and excluding trivial or duplicated elements. To evaluate the\nefficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as\nreference data and tested the TPL reuse of 1,000 GitHub projects. The result\nshows that JC-Finder achieved an F1-score of 0.818, outperforming the other\nfunction-level tool by 0.427. The average time taken for resolving TPL reuse is\n14.2 seconds, which is approximately 9 times faster than the other tool. We\nfurther applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code\nclones in 789 projects (about 9.89% of all projects) and identifying a total of\n2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not\nexplicitly declared in package managers.", "AI": {"tldr": "JC-Finder \u662f\u4e00\u79cd\u4e13\u4e3a Java \u8bbe\u8ba1\u7684\u514b\u9686\u68c0\u6d4b\u5de5\u5177\uff0c\u7528\u4e8e\u8bc6\u522b\u7b2c\u4e09\u65b9\u5e93\uff08TPL\uff09\u7684\u91cd\u7528\uff0c\u6bd4\u73b0\u6709\u5de5\u5177\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u3002", "motivation": "TPL \u7684\u91cd\u7528\u7ba1\u7406\u6df7\u4e71\u53ef\u80fd\u5a01\u80c1\u8f6f\u4ef6\u7ef4\u62a4\uff0c\u5e76\u5f15\u53d1\u7248\u6743\u95ee\u9898\u3002\u73b0\u6709\u5de5\u5177\u5bf9 Java \u7f3a\u4e4f\u9488\u5bf9\u6027\u3002", "method": "JC-Finder \u901a\u8fc7\u7c7b\u7ea7\u7279\u5f81\u6355\u83b7\u3001\u7ef4\u62a4\u51fd\u6570\u95f4\u5173\u7cfb\u5e76\u6392\u9664\u5197\u4f59\u5143\u7d20\uff0c\u5b9e\u73b0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728\u6d4b\u8bd5\u4e2d\uff0cJC-Finder \u7684 F1 \u5206\u6570\u4e3a 0.818\uff0c\u6bd4\u73b0\u6709\u5de5\u5177\u5feb 9 \u500d\uff0c\u5e76\u53d1\u73b0\u66f4\u591a\u672a\u58f0\u660e\u7684 TPL \u91cd\u7528\u3002", "conclusion": "JC-Finder \u6709\u6548\u5730\u586b\u8865\u4e86 Java \u514b\u9686\u68c0\u6d4b\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86 TPL \u91cd\u7528\u7684\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2508.01388", "pdf": "https://arxiv.org/pdf/2508.01388", "abs": "https://arxiv.org/abs/2508.01388", "authors": ["Rukshani Somarathna", "Madhawa Perera", "Tom Gedeon", "Matt Adcock"], "title": "An Appraisal-Based Approach to Human-Centred Explanations", "categories": ["cs.HC"], "comment": null, "summary": "Explainability remains a critical challenge in artificial intelligence (AI)\nsystems, particularly in high stakes domains such as healthcare, finance, and\ndecision support, where users must understand and trust automated reasoning.\nTraditional explainability methods such as feature importance and post-hoc\njustifications often fail to capture the cognitive processes that underlie\nhuman decision making, leading to either too technical or insufficiently\nmeaningful explanations. We propose a novel appraisal based framework inspired\nby the Component Process Model (CPM) for explainability to address this gap.\nWhile CPM has traditionally been applied to emotion research, we use its\nappraisal component as a cognitive model for generating human aligned\nexplanations. By structuring explanations around key appraisal dimensions such\nas relevance, implications, coping potential, and normative significance our\nframework provides context sensitive, cognitively meaningful justifications for\nAI decisions. This work introduces a new paradigm for generating intuitive,\nhuman-centred explanations in AI driven systems by bridging cognitive science\nand explainable AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCPM\u7684\u65b0\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684AI\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u7684\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u8981\u6c42\u9886\u57df\u4e2d\u5bf9AI\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u7684\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u89e3\u91ca\u3002", "method": "\u5229\u7528CPM\u7684\u8bc4\u4f30\u7ec4\u4ef6\uff0c\u56f4\u7ed5\u5173\u952e\u7ef4\u5ea6\uff08\u5982\u76f8\u5173\u6027\u3001\u5f71\u54cd\u7b49\uff09\u6784\u5efa\u89e3\u91ca\u6846\u67b6\u3002", "result": "\u63d0\u4f9b\u4e86\u4e0a\u4e0b\u6587\u654f\u611f\u4e14\u8ba4\u77e5\u4e0a\u6709\u610f\u4e49\u7684AI\u51b3\u7b56\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u4e0e\u53ef\u89e3\u91caAI\uff0c\u4e3a\u751f\u6210\u66f4\u76f4\u89c2\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.01915", "pdf": "https://arxiv.org/pdf/2508.01915", "abs": "https://arxiv.org/abs/2508.01915", "authors": ["Akshay Paruchuri", "Sinan Hersek", "Lavisha Aggarwal", "Qiao Yang", "Xin Liu", "Achin Kulshrestha", "Andrea Colaco", "Henry Fuchs", "Ishan Chatterjee"], "title": "EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses", "categories": ["cs.CV", "cs.ET", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "comment": "15 pages, 6 figres, 6 tables. Accepted to ISMAR 2025 as a TVCG\n  journal paper", "summary": "All-day smart glasses are likely to emerge as platforms capable of continuous\ncontextual sensing, uniquely positioning them for unprecedented assistance in\nour daily lives. Integrating the multi-modal AI agents required for human\nmemory enhancement while performing continuous sensing, however, presents a\nmajor energy efficiency challenge for all-day usage. Achieving this balance\nrequires intelligent, context-aware sensor management. Our approach,\nEgoTrigger, leverages audio cues from the microphone to selectively activate\npower-intensive cameras, enabling efficient sensing while preserving\nsubstantial utility for human memory enhancement. EgoTrigger uses a lightweight\naudio model (YAMNet) and a custom classification head to trigger image capture\nfrom hand-object interaction (HOI) audio cues, such as the sound of a drawer\nopening or a medication bottle being opened. In addition to evaluating on the\nQA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement\nQuestion-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated\nfirst-person QA pairs from full-length Ego4D videos that were curated to ensure\nthat they contained audio, focusing on HOI moments critical for contextual\nunderstanding and memory. Our results show EgoTrigger can use 54% fewer frames\non average, significantly saving energy in both power-hungry sensing components\n(e.g., cameras) and downstream operations (e.g., wireless transmission), while\nachieving comparable performance on datasets for an episodic memory task. We\nbelieve this context-aware triggering strategy represents a promising direction\nfor enabling energy-efficient, functional smart glasses capable of all-day use\n-- supporting applications like helping users recall where they placed their\nkeys or information about their routine activities (e.g., taking medications).", "AI": {"tldr": "EgoTrigger \u662f\u4e00\u79cd\u901a\u8fc7\u97f3\u9891\u89e6\u53d1\u667a\u80fd\u773c\u955c\u6444\u50cf\u5934\u7684\u65b9\u6cd5\uff0c\u4ee5\u8282\u7701\u80fd\u6e90\u5e76\u63d0\u9ad8\u4eba\u7c7b\u8bb0\u5fc6\u589e\u5f3a\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u773c\u955c\u5168\u5929\u4f7f\u7528\u65f6\u7684\u80fd\u6e90\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u591a\u6a21\u6001AI\u4ee3\u7406\u7684\u96c6\u6210\u4ee5\u589e\u5f3a\u4eba\u7c7b\u8bb0\u5fc6\u3002", "method": "\u5229\u7528\u8f7b\u91cf\u7ea7\u97f3\u9891\u6a21\u578b\uff08YAMNet\uff09\u548c\u81ea\u5b9a\u4e49\u5206\u7c7b\u5934\uff0c\u901a\u8fc7\u624b\u90e8\u4e0e\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u7684\u97f3\u9891\u7ebf\u7d22\u9009\u62e9\u6027\u6fc0\u6d3b\u6444\u50cf\u5934\u3002", "result": "EgoTrigger \u5e73\u5747\u53ef\u51cf\u5c1154%\u7684\u5e27\u6570\u4f7f\u7528\uff0c\u663e\u8457\u8282\u7701\u80fd\u6e90\uff0c\u540c\u65f6\u5728\u8bb0\u5fc6\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u53ef\u6bd4\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u89e6\u53d1\u7b56\u7565\u4e3a\u5168\u5929\u5019\u80fd\u6e90\u9ad8\u6548\u667a\u80fd\u773c\u955c\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2508.01107", "pdf": "https://arxiv.org/pdf/2508.01107", "abs": "https://arxiv.org/abs/2508.01107", "authors": ["Shima Yousefi", "Motahare Mounesan", "Saptarshi Debroy"], "title": "AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "In recent years, Deep Neural Networks (DNNs) have become increasingly\nintegral to IoT-based environments, enabling realtime visual computing.\nHowever, the limited computational capacity of these devices has motivated the\nadoption of collaborative DNN inference, where the IoT device offloads part of\nthe inference-related computation to a remote server. Such offloading often\nrequires dynamic DNN partitioning information to be exchanged among the\nparticipants over an unsecured network or via relays/hops, leading to novel\nprivacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial\nvariational autoencoder (VAE)-based misclassification attack, leveraging\nclassifiers to detect model information and a VAE to generate untraceable\nmanipulated samples, specifically designed to compromise the collaborative\ninference process. AdVAR-DNN attack uses the sensitive information exchange\nvulnerability of collaborative DNN inference and is black-box in nature in\nterms of having no prior knowledge about the DNN model and how it is\npartitioned. Our evaluation using the most popular object classification DNNs\non the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms\nof high attack success rate with little to no probability of detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdVAR-DNN\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u6297\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u654f\u611f\u4fe1\u606f\u4ea4\u6362\u6f0f\u6d1e\u7834\u574f\u534f\u4f5cDNN\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u5e38\u91c7\u7528\u534f\u4f5cDNN\u63a8\u7406\u5c06\u90e8\u5206\u8ba1\u7b97\u5378\u8f7d\u5230\u8fdc\u7a0b\u670d\u52a1\u5668\uff0c\u800c\u52a8\u6001\u5206\u533a\u4fe1\u606f\u7684\u4ea4\u6362\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u3002", "method": "\u63d0\u51faAdVAR-DNN\u653b\u51fb\uff0c\u7ed3\u5408\u5bf9\u6297\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5206\u7c7b\u5668\uff0c\u65e0\u9700DNN\u6a21\u578b\u548c\u5206\u533a\u77e5\u8bc6\u5373\u53ef\u751f\u6210\u4e0d\u53ef\u8ffd\u8e2a\u7684\u64cd\u7eb5\u6837\u672c\u3002", "result": "\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cAdVAR-DNN\u653b\u51fb\u6210\u529f\u7387\u9ad8\u4e14\u51e0\u4e4e\u65e0\u6cd5\u88ab\u68c0\u6d4b\u5230\u3002", "conclusion": "AdVAR-DNN\u66b4\u9732\u4e86\u534f\u4f5c\u63a8\u7406\u4e2d\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u9700\u8981\u52a0\u5f3a\u5b89\u5168\u6027\u7814\u7a76\u3002"}}
{"id": "2508.02209", "pdf": "https://arxiv.org/pdf/2508.02209", "abs": "https://arxiv.org/abs/2508.02209", "authors": ["Yigit Turkmen", "Baturalp Buyukates", "Melih Bastopcu"], "title": "Balancing Information Accuracy and Response Timeliness in Networked LLMs", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u7531\u591a\u4e2a\u5c0f\u578b\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u7ec4\u6210\u7684\u7f51\u7edc\u7cfb\u7edf\uff0c\u901a\u8fc7\u805a\u5408\u5b83\u4eec\u7684\u8f93\u51fa\u6765\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u805a\u5408\u54cd\u5e94\u6bd4\u5355\u4e2a\u6a21\u578b\u66f4\u51c6\u786e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5bf9\u8bad\u7ec3\u6570\u636e\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u80fd\u6e90\u7684\u9ad8\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u7814\u7a76\u63d0\u51fa\u5229\u7528\u5c0f\u578b\u4e13\u7528\u6a21\u578b\u805a\u5408\u8f93\u51fa\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7531\u7528\u6237\u3001\u4e2d\u592e\u4efb\u52a1\u5904\u7406\u5668\u548c\u591a\u4e2a\u4e13\u7528LLM\u96c6\u7fa4\u7ec4\u6210\u7684\u7f51\u7edc\u7cfb\u7edf\u3002\u5904\u7406\u5668\u5c06\u7528\u6237\u7684\u4e8c\u5143\u67e5\u8be2\u8def\u7531\u5230\u9009\u5b9a\u7684\u6a21\u578b\u96c6\u7fa4\uff0c\u805a\u5408\u4e2a\u4f53\u54cd\u5e94\u540e\u8fd4\u56de\u7ed3\u679c\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u805a\u5408\u54cd\u5e94\u7684\u51c6\u786e\u6027\u663e\u8457\u9ad8\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u53c2\u4e0e\u6a21\u578b\u7684\u6027\u80fd\u76f8\u8fd1\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002", "conclusion": "\u901a\u8fc7\u805a\u5408\u5c0f\u578b\u4e13\u7528\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.02407", "pdf": "https://arxiv.org/pdf/2508.02407", "abs": "https://arxiv.org/abs/2508.02407", "authors": ["Xinyi Wang", "Qinghua Xu", "Paolo Arcaini", "Shaukat Ali", "Thomas Peyrucain"], "title": "Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots", "categories": ["cs.SE"], "comment": null, "summary": "Robots are increasingly becoming part of our daily lives, interacting with\nboth the environment and humans to perform their tasks. The software of such\nrobots often undergoes upgrades, for example, to add new functionalities, fix\nbugs, or delete obsolete functionalities. As a result, regression testing of\nrobot software becomes necessary. However, determining the expected correct\nbehavior of robots (i.e., a test oracle) is challenging due to the potentially\nunknown environments in which the robots must operate. To address this\nchallenge, machine learning (ML)-based test oracles present a viable solution.\nThis paper reports on the development of a test oracle to support regression\ntesting of autonomous mobile robots built by PAL Robotics (Spain), using\nquantum machine learning (QML), which enables faster training and the\nconstruction of more precise test oracles. Specifically, we propose a hybrid\nframework, QuReBot, that combines both quantum reservoir computing (QRC) and a\nsimple neural network, inspired by residual connection, to predict the expected\nbehavior of a robot. Results show that QRC alone fails to converge in our case,\nyielding high prediction error. In contrast, QuReBot converges and achieves 15%\nreduction of prediction error compared to the classical neural network\nbaseline. Finally, we further examine QuReBot under different configurations\nand offer practical guidance on optimal settings to support future robot\nsoftware testing.", "AI": {"tldr": "\u4e3a\u652f\u6301\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u56de\u5f52\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u6d4b\u8bd5\u51c6\u5219\u6846\u67b6QuReBot\uff0c\u7ed3\u5408\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee15%\uff0c\u5e76\u63d0\u4f9b\u4e86\u914d\u7f6e\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u673a\u5668\u4eba\u8f6f\u4ef6\u5347\u7ea7\u540e\u9700\u8981\u56de\u5f52\u6d4b\u8bd5\uff0c\u4f46\u4f20\u7edf\u6d4b\u8bd5\u51c6\u5219\u96be\u4ee5\u9002\u5e94\u672a\u77e5\u73af\u5883\uff0c\u56e0\u6b64\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4ee5\u6784\u5efa\u66f4\u7cbe\u786e\u7684\u6d4b\u8bd5\u51c6\u5219\u3002", "method": "\u63d0\u51faQuReBot\u6846\u67b6\uff0c\u7ed3\u5408\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\uff08QRC\uff09\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u4f18\u5316\u6d4b\u8bd5\u51c6\u5219\u3002", "result": "QRC\u5355\u72ec\u5e94\u7528\u5931\u8d25\uff0c\u4f46QuReBot\u6536\u655b\u4e14\u9884\u6d4b\u8bef\u5dee\u6bd4\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u964d\u4f4e15%\u3002", "conclusion": "QuReBot\u4e3a\u673a\u5668\u4eba\u8f6f\u4ef6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u914d\u7f6e\u3002"}}
{"id": "2508.01520", "pdf": "https://arxiv.org/pdf/2508.01520", "abs": "https://arxiv.org/abs/2508.01520", "authors": ["Jianhua Li", "Shang Gao", "Michelle Harvey", "Trina Myers"], "title": "Unlocking Excellence: The Impact of Voucher Incentives on Cybersecurity Education", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "While voucher incentives have been popular for primary and secondary schools,\nthey are less used in higher education. In this study, we leverage industry\nvoucher incentives to inspire students in cybersecurity education (CSE). We\nadopt a 100% portfolio-based assessment strategy, where students can freely\nselect their target grades in the investigated unit. We purposely design one of\nthe high distinction (HD) tasks to be obtaining an industry certificate and\nprovide vouchers to those who can accomplish a predefined set of tasks before a\nmidpoint. The voucher recipients will use the voucher to access the industry\ncertificate training materials and sit the certificate exam for free. Passing\nthe certificate exam is one of the conditions for gaining an HD grade. Our\nsurvey and interviews reveal a substantial influence of voucher incentives on\nstudents' career aspirations. In light of the findings, recommendations on\nadopting voucher incentives in CSE or broader ICT education are offered for\ninstitutions and researchers.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9ad8\u7b49\u6559\u80b2\u4e2d\u884c\u4e1a\u4ee3\u91d1\u5238\u6fc0\u52b1\u5bf9\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u5b66\u751f\u804c\u4e1a\u5fd7\u5411\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u9ad8\u7b49\u6559\u80b2\u4e2d\u8f83\u5c11\u4f7f\u7528\u7684\u4ee3\u91d1\u5238\u6fc0\u52b1\u5728\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u4e2d\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528100%\u57fa\u4e8e\u4f5c\u54c1\u96c6\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u8bbe\u8ba1\u9ad8\u533a\u5206\u5ea6\u4efb\u52a1\u5e76\u63d0\u4f9b\u4ee3\u91d1\u5238\u652f\u6301\u5b66\u751f\u83b7\u53d6\u884c\u4e1a\u8bc1\u4e66\u3002", "result": "\u4ee3\u91d1\u5238\u6fc0\u52b1\u663e\u8457\u5f71\u54cd\u5b66\u751f\u7684\u804c\u4e1a\u5fd7\u5411\u3002", "conclusion": "\u5efa\u8bae\u5728\u7f51\u7edc\u5b89\u5168\u6216\u66f4\u5e7f\u6cdb\u7684ICT\u6559\u80b2\u4e2d\u91c7\u7528\u4ee3\u91d1\u5238\u6fc0\u52b1\u7b56\u7565\u3002"}}
{"id": "2508.01997", "pdf": "https://arxiv.org/pdf/2508.01997", "abs": "https://arxiv.org/abs/2508.01997", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed", "Anthony Green"], "title": "DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": null, "summary": "The rapid advancement and widespread adoption of generative artificial\nintelligence (AI) pose significant threats to the integrity of personal\nidentity, including digital cloning, sophisticated impersonation, and the\nunauthorized monetization of identity-related data. Mitigating these risks\nnecessitates the development of robust AI-generated content detection systems,\nenhanced legal frameworks, and ethical guidelines. This paper introduces the\nDigital Identity Rights Framework (DIRF), a structured security and governance\nmodel designed to protect behavioral, biometric, and personality-based digital\nlikeness attributes to address this critical need. Structured across nine\ndomains and 63 controls, DIRF integrates legal, technical, and hybrid\nenforcement mechanisms to secure digital identity consent, traceability, and\nmonetization. We present the architectural foundations, enforcement strategies,\nand key use cases supporting the need for a unified framework. This work aims\nto inform platform builders, legal entities, and regulators about the essential\ncontrols needed to enforce identity rights in AI-driven systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6570\u5b57\u8eab\u4efd\u6743\u5229\u6846\u67b6\uff08DIRF\uff09\uff0c\u4ee5\u5e94\u5bf9\u751f\u6210\u5f0fAI\u5bf9\u4e2a\u4eba\u8eab\u4efd\u5b8c\u6574\u6027\u7684\u5a01\u80c1\uff0c\u5305\u62ec\u6cd5\u5f8b\u3001\u6280\u672f\u548c\u7ba1\u7406\u673a\u5236\u7684\u4fdd\u62a4\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u6570\u5b57\u514b\u9686\u3001\u8eab\u4efd\u5192\u5145\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4fdd\u62a4\u6570\u5b57\u8eab\u4efd\u7684\u5b89\u5168\u548c\u6743\u76ca\u3002", "method": "\u5f00\u53d1\u4e86DIRF\u6846\u67b6\uff0c\u5305\u542b\u4e5d\u5927\u9886\u57df\u548c63\u9879\u63a7\u5236\u63aa\u65bd\uff0c\u6574\u5408\u6cd5\u5f8b\u548c\u6280\u672f\u624b\u6bb5\u3002", "result": "DIRF\u4e3a\u4fdd\u62a4\u6570\u5b57\u8eab\u4efd\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u6db5\u76d6\u4e86\u8eab\u4efd\u540c\u610f\u3001\u8ffd\u8e2a\u548c\u5546\u4e1a\u5316\u7684\u9700\u6c42\u3002", "conclusion": "DIRF\u6846\u67b6\u4e3a\u5e73\u53f0\u5f00\u53d1\u8005\u3001\u6cd5\u5f8b\u5b9e\u4f53\u548c\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u63a7\u5236\u63aa\u65bd\uff0c\u4ee5\u4fdd\u969cAI\u7cfb\u7edf\u4e2d\u7684\u8eab\u4efd\u6743\u5229\u3002"}}
{"id": "2508.01448", "pdf": "https://arxiv.org/pdf/2508.01448", "abs": "https://arxiv.org/abs/2508.01448", "authors": ["Mirza Ahad Baig", "Christoph U. G\u00fcnther", "Krzysztof Pietrzak"], "title": "Nakamoto Consensus from Multiple Resources", "categories": ["cs.CR", "cs.DC"], "comment": "Full version of the paper published at AFT25", "summary": "The blocks in the Bitcoin blockchain record the amount of work W that went\ninto creating them through proofs of work. When honest parties control a\nmajority of the work, consensus is achieved by picking the chain with the\nhighest recorded weight. Resources other than work have been considered to\nsecure such longest-chain blockchains. In Chia, blocks record the amount of\nspace S (via a proof of space) and sequential computational steps V (via a\nVDF).\n  In this paper, we ask what weight functions {\\Gamma}(S,V,W) (that assign a\nweight to a block as a function of the recorded space, speed, and work) are\nsecure in the sense that whenever the weight of the resources controlled by\nhonest parties is larger than the weight of adversarial parties, the blockchain\nis secure against private double-spending attacks.\n  We completely classify such functions in an idealized \"continuous\" model:\n{\\Gamma}(S,V,W) is secure against private double-spending attacks if and only\nif it is homogeneous of degree one in the timed resources V and W, i.e.,\n{\\alpha}{\\Gamma}(S,V,W)={\\Gamma}(S,{\\alpha}V, {\\alpha}W). This includes Bitcoin\nrule {\\Gamma}(S,V,W)=W and Chia rule {\\Gamma}(S,V,W) = SV. In a more realistic\nmodel where blocks are created at discrete time-points, one additionally needs\nsome mild assumptions on the dependency on S (basically, the weight should not\ngrow too much if S is slightly increased, say linear as in Chia).\n  Our classification is more general and allows various instantiations of the\nsame resource. It provides a powerful tool for designing new longest-chain\nblockchains. E.g., consider combining different PoWs to counter centralization,\nsay the Bitcoin PoW W_1 and a memory-hard PoW W_2. Previous work suggested to\nuse W_1+W_2 as weight. Our results show that using\n{\\sqrt}(W_1){\\cdot}{\\sqrt}(W_2), {\\min}{W_1,W_2} are also secure, and we argue\nthat in practice these are much better choices.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6bd4\u7279\u5e01\u548cChia\u533a\u5757\u94fe\u4e2d\u6743\u91cd\u51fd\u6570\u7684\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u4e86\u786e\u4fdd\u533a\u5757\u94fe\u514d\u53d7\u53cc\u82b1\u653b\u51fb\u7684\u6743\u91cd\u51fd\u6570\u6761\u4ef6\uff0c\u5e76\u7ed9\u51fa\u4e86\u5177\u4f53\u5b9e\u4f8b\u548c\u5e94\u7528\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u786e\u5b9a\u54ea\u4e9b\u6743\u91cd\u51fd\u6570\u53ef\u4ee5\u786e\u4fdd\u533a\u5757\u94fe\u5728\u8bda\u5b9e\u65b9\u8d44\u6e90\u6743\u91cd\u5360\u4f18\u65f6\u514d\u53d7\u53cc\u82b1\u653b\u51fb\uff0c\u4e3a\u8bbe\u8ba1\u65b0\u7684\u6700\u957f\u94fe\u533a\u5757\u94fe\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u901a\u8fc7\u7406\u60f3\u5316\u7684\u8fde\u7eed\u6a21\u578b\u548c\u79bb\u6563\u65f6\u95f4\u70b9\u7684\u5b9e\u9645\u6a21\u578b\uff0c\u5206\u6790\u6743\u91cd\u51fd\u6570\u7684\u6027\u8d28\uff0c\u5305\u62ec\u9f50\u6b21\u6027\u548c\u5bf9\u7a7a\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u786e\u5b9a\u4e86\u6743\u91cd\u51fd\u6570\u9700\u6ee1\u8db3\u7684\u6761\u4ef6\uff1a\u5728\u8fde\u7eed\u6a21\u578b\u4e2d\u9700\u662fV\u548cW\u7684\u4e00\u6b21\u9f50\u6b21\u51fd\u6570\uff0c\u79bb\u6563\u6a21\u578b\u4e2d\u8fd8\u9700\u6ee1\u8db3\u5bf9\u7a7a\u95f4\u7684\u9650\u5236\u3002\u63d0\u51fa\u4e86\u591a\u79cd\u5b89\u5168\u6743\u91cd\u51fd\u6570\u7684\u5b9e\u4f8b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u533a\u5757\u94fe\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5e76\u5efa\u8bae\u5728\u5b9e\u8df5\u4e2d\u91c7\u7528\u66f4\u4f18\u7684\u6743\u91cd\u7ec4\u5408\u65b9\u5f0f\uff0c\u5982\u5e73\u65b9\u6839\u6216\u6700\u5c0f\u503c\u6cd5\u3002"}}
{"id": "2508.02455", "pdf": "https://arxiv.org/pdf/2508.02455", "abs": "https://arxiv.org/abs/2508.02455", "authors": ["Daniele Cipollone", "Egor Bogomolov", "Arie van Deursen", "Maliheh Izadi"], "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs", "categories": ["cs.SE", "cs.AI", "cs.IR"], "comment": null, "summary": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u5206\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u4ee3\u7801\u8865\u5168\u7684\u9759\u6001\u5efa\u8bae\u6392\u540d\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u4ee3\u7801\u8865\u5168\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u9759\u6001\u5206\u6790\u7684\u5efa\u8bae\u6392\u540d\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u9879\u76ee\u548c\u7f16\u7801\u98ce\u683c\u3002", "method": "\u5c06\u6240\u6709\u6709\u6548\u8865\u5168\u7ec4\u7ec7\u6210\u524d\u7f00\u6811\uff0c\u8fdb\u884c\u5355\u6b21\u8d2a\u5a6a\u89e3\u7801\u4ee5\u6536\u96c6\u5168\u6811\u7684\u5206\u503c\uff0c\u65e0\u9700\u675f\u641c\u7d22\u6216\u6a21\u578b\u9002\u914d\u3002", "result": "\u65b9\u6cd5\u5feb\u901f\u3001\u67b6\u6784\u65e0\u5173\u4e14\u517c\u5bb9\u73b0\u6709\u6a21\u578b\uff0c\u4e3aIDE\u5de5\u5177\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u54cd\u5e94\u66f4\u5feb\u7684\u5f00\u53d1\u8f85\u52a9\u3002", "conclusion": "\u4e3a\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u73b0\u6709IDE\u5de5\u5177\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u9ad8\u6548\u8def\u5f84\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u4f53\u9a8c\u3002"}}
{"id": "2508.01547", "pdf": "https://arxiv.org/pdf/2508.01547", "abs": "https://arxiv.org/abs/2508.01547", "authors": ["Yongsu Ahn", "Nam Wook Kim"], "title": "Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper investigates why recent generative AI models outperform humans in\ndata visualization knowledge tasks. Through systematic comparative analysis of\nresponses to visualization questions, we find that differences exist between\ntwo ChatGPT models and human outputs over rhetorical structure, knowledge\nbreadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more\nadvanced model, displays a hybrid of characteristics from both humans and\nChatGPT-3.5. The two models were generally favored over human responses, while\ntheir strengths in coverage and breadth, and emphasis on technical and\ntask-oriented visualization feedback collectively shaped higher overall\nquality. Based on our findings, we draw implications for advancing user\nexperiences based on the potential of LLMs and human perception over their\ncapabilities, with relevance to broader applications of AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4eba\u7c7b\u7684\u539f\u56e0\uff0c\u901a\u8fc7\u5bf9\u6bd4ChatGPT-3.5\u3001ChatGPT-4\u4e0e\u4eba\u7c7b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u53d1\u73b0ChatGPT-4\u517c\u5177\u4eba\u7c7b\u548cChatGPT-3.5\u7684\u7279\u70b9\uff0c\u5728\u8986\u76d6\u5e7f\u5ea6\u548c\u6280\u672f\u53cd\u9988\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u4e3a\u4ec0\u4e48\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5c24\u5176\u662f\u5728\u4fee\u8f9e\u7ed3\u6784\u3001\u77e5\u8bc6\u5e7f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u5206\u6790ChatGPT-3.5\u3001ChatGPT-4\u548c\u4eba\u7c7b\u5728\u53ef\u89c6\u5316\u95ee\u9898\u4e0a\u7684\u56de\u7b54\uff0c\u8bc4\u4f30\u5176\u5728\u4fee\u8f9e\u7ed3\u6784\u3001\u77e5\u8bc6\u5e7f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0ChatGPT-4\u7ed3\u5408\u4e86\u4eba\u7c7b\u548cChatGPT-3.5\u7684\u7279\u70b9\uff0c\u4e14\u5728\u8986\u76d6\u5e7f\u5ea6\u548c\u6280\u672f\u53cd\u9988\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u603b\u4f53\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u4f53\u9a8c\u6539\u8fdb\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e76\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684AI\u5e94\u7528\u9886\u57df\u3002"}}
{"id": "2508.02250", "pdf": "https://arxiv.org/pdf/2508.02250", "abs": "https://arxiv.org/abs/2508.02250", "authors": ["Stefan Porfir", "Bram F. Haverkort", "Federico Sbravati", "Aida Todri-Sanial"], "title": "Solving Sudoku Using Oscillatory Neural Networks", "categories": ["cond-mat.dis-nn", "cs.ET"], "comment": "5 pages, 7 figures", "summary": "This paper explores the application of Oscillatory Neural Networks (ONNs) to\nsolving Sudoku puzzles, presenting a biologically inspired approach based on\nphase synchronization. Each cell is represented by an oscillator whose phase\nencodes a digit, and the synchronization is governed by the Kuramoto model. The\nsystem dynamically evolves towards a valid solution by having the puzzle\nconstraints encoded into the weight matrix of the network, and through a\nproposed novel phase mapping of the Sudoku digits. Experimental results show\nthat ONNs achieve high performance for puzzles with moderate difficulty and\noutperform Hopfield Neural Networks, particularly in cases with up to 20\ninitially unknown values. Although the performance decreases with increased\nambiguity, ONNs still produce correct solutions in some of the iterations,\ncases in which the baseline Hopfield Neural Network algorithm fails. The\nfindings support ONNs as a viable alternative for solving constraint\noptimization problems and reinforce their relevance within emerging non-von\nNeumann computing paradigms.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u632f\u8361\u795e\u7ecf\u7f51\u7edc\uff08ONNs\uff09\u5728\u89e3\u51b3\u6570\u72ec\u8c1c\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u76f8\u4f4d\u540c\u6b65\u7684\u751f\u7269\u542f\u53d1\u6027\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u9ad8\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1ONNs\u5728\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6570\u72ec\u8fd9\u7c7b\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22\u975e\u51af\u00b7\u8bfa\u4f9d\u66fc\u8ba1\u7b97\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Kuramoto\u6a21\u578b\u63a7\u5236\u632f\u8361\u5668\u7684\u76f8\u4f4d\u540c\u6b65\uff0c\u5c06\u6570\u72ec\u7ea6\u675f\u7f16\u7801\u5230\u7f51\u7edc\u7684\u6743\u91cd\u77e9\u9635\u4e2d\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6570\u5b57\u76f8\u4f4d\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cONNs\u5728\u4e2d\u7b49\u96be\u5ea6\u6570\u72ec\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u521d\u59cb\u672a\u77e5\u503c\u4e0d\u8d85\u8fc720\u65f6\u4f18\u4e8eHopfield\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "ONNs\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u7a81\u663e\u4e86\u5176\u5728\u975e\u51af\u00b7\u8bfa\u4f9d\u66fc\u8ba1\u7b97\u8303\u5f0f\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.01485", "pdf": "https://arxiv.org/pdf/2508.01485", "abs": "https://arxiv.org/abs/2508.01485", "authors": ["Arindam Khanda", "Satyaki Roy", "Prithwiraj Roy", "Sajal K. Das"], "title": "A Parallel Algorithm for Finding Robust Spanners in Large Social Networks", "categories": ["cs.SI", "cs.DC"], "comment": null, "summary": "Social networks, characterized by community structures, often rely on nodes\ncalled structural hole spanners to facilitate inter-community information\ndissemination. However, the dynamic nature of these networks, where spanner\nnodes may be removed, necessitates resilient methods to maintain\ninter-community communication. To this end, we introduce robust spanners (RS)\nas nodes uniquely equipped to bridge communities despite disruptions, such as\nnode or edge removals. We propose a novel scoring technique to identify RS\nnodes and present a parallel algorithm with a CUDA implementation for efficient\nRS detection in large networks. Empirical analysis of real-world social\nnetworks reveals that high-scoring nodes exhibit a spanning capacity comparable\nto those identified by benchmark spanner detection algorithms while offering\nsuperior robustness. Our implementation on Nvidia GPUs achieves an average\nspeedup of 244X over traditional spanner detection techniques, demonstrating\nits efficacy to identify RS in large social networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7a33\u5065\u8de8\u8d8a\u8005\uff08RS\uff09\u7684\u8282\u70b9\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u793e\u4ea4\u7f51\u7edc\u4e2d\u7ef4\u6301\u793e\u533a\u95f4\u901a\u4fe1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u5e76\u884c\u7b97\u6cd5\u9ad8\u6548\u8bc6\u522bRS\u8282\u70b9\u3002", "motivation": "\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u7ed3\u6784\u6d1e\u8de8\u8d8a\u8005\u5728\u793e\u533a\u95f4\u4fe1\u606f\u4f20\u64ad\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u7f51\u7edc\u52a8\u6001\u6027\u53ef\u80fd\u5bfc\u81f4\u5176\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u6280\u672f\u8bc6\u522bRS\u8282\u70b9\uff0c\u5e76\u8bbe\u8ba1\u4e86CUDA\u5e76\u884c\u7b97\u6cd5\u4ee5\u9ad8\u6548\u68c0\u6d4b\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7684RS\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u8bc4\u5206\u8282\u70b9\u7684\u8de8\u8d8a\u80fd\u529b\u4e0e\u57fa\u51c6\u7b97\u6cd5\u76f8\u5f53\uff0c\u4f46\u66f4\u7a33\u5065\uff1bGPU\u5b9e\u73b0\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb244\u500d\u3002", "conclusion": "RS\u8282\u70b9\u53ca\u5176\u9ad8\u6548\u8bc6\u522b\u7b97\u6cd5\u4e3a\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u793e\u533a\u95f4\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02657", "pdf": "https://arxiv.org/pdf/2508.02657", "abs": "https://arxiv.org/abs/2508.02657", "authors": ["Irtiza Hasan", "Ahmed Arafa"], "title": "RC-Gossip: Information Freshness in Clustered Networks with Rate-Changing Gossip", "categories": ["cs.IT", "cs.NI", "eess.SP", "math.IT"], "comment": "To appear in the 2025 Asilomar Conference on Signals, Systems, and\n  Computers", "summary": "A clustered gossip network is considered in which a source updates its\ninformation over time, and end-nodes, organized in clusters through\nclusterheads, are keeping track of it. The goal for the nodes is to remain as\nfresh as possible, i.e., have the same information as the source, which we\nassess by the long-term average binary freshness metric. We introduce a smart\nmechanism of information dissemination which we coin rate-changing gossip\n(RC-Gossip). Its main idea is that gossiping is directed towards nodes that\nneed it the most, and hence the rate of gossiping changes based on the number\nof fresh nodes in the network at a given time. While Stochastic Hybrid System\n(SHS) analysis has been the norm in studying freshness of gossip networks, we\npresent an equivalent way to analyze freshness using a renewal-reward-based\napproach. Using that, we show that RC-gossip significantly increases freshness\nof nodes in different clustered networks, with optimal cluster sizes, compared\nto traditional gossiping techniques.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aRC-Gossip\u7684\u667a\u80fd\u4fe1\u606f\u4f20\u64ad\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f20\u64ad\u901f\u7387\u63d0\u5347\u8282\u70b9\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u96c6\u7fa4\u5316\u7f51\u7edc\u4e2d\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u4f20\u64ad\u673a\u5236\uff0c\u4f7f\u8282\u70b9\u4fdd\u6301\u4fe1\u606f\u65b0\u9c9c\u5ea6\u3002", "method": "\u63d0\u51faRC-Gossip\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u4f20\u64ad\u901f\u7387\uff0c\u5e76\u7ed3\u5408\u66f4\u65b0\u5956\u52b1\u7406\u8bba\u5206\u6790\u65b0\u9c9c\u5ea6\u3002", "result": "RC-Gossip\u663e\u8457\u63d0\u9ad8\u4e86\u4e0d\u540c\u96c6\u7fa4\u7f51\u7edc\u4e2d\u8282\u70b9\u7684\u65b0\u9c9c\u5ea6\u3002", "conclusion": "RC-Gossip\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u606f\u4f20\u64ad\u673a\u5236\uff0c\u9002\u7528\u4e8e\u96c6\u7fa4\u5316\u7f51\u7edc\u3002"}}
{"id": "2508.02473", "pdf": "https://arxiv.org/pdf/2508.02473", "abs": "https://arxiv.org/abs/2508.02473", "authors": ["Xinfang Chen", "Siyang Xiao", "Xianying Zhu", "Junhong Xie", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li", "Peng Di"], "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs", "categories": ["cs.SE", "cs.LG", "68N30", "D.2.3; D.1.2; I.2.2"], "comment": "13 pages", "summary": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNES\u7684\u4f4e\u5ef6\u8fdf\u3001\u514d\u6307\u4ee4\u7684\u4ee3\u7801\u7f16\u8f91\u6846\u67b6\uff0c\u5229\u7528\u5f00\u53d1\u8005\u5386\u53f2\u7f16\u8f91\u6a21\u5f0f\u9884\u6d4b\u4e0b\u4e00\u6b65\u7f16\u8f91\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u4ee3\u7801\u7f16\u8f91\u5de5\u5177\u4f9d\u8d56\u4eba\u5de5\u8f93\u5165\u4e14\u5ef6\u8fdf\u9ad8\uff0c\u96be\u4ee5\u878d\u5165\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u578b\u67b6\u6784\uff0c\u7ed3\u5408SFT\u548cDAPO\u6570\u636e\u96c6\u8bad\u7ec3LLM\uff0c\u4f18\u5316\u63a8\u7406\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u5728\u9884\u6d4b\u7f16\u8f91\u4f4d\u7f6e\u548c\u610f\u56fe\u5bf9\u9f50\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0cNES\u5206\u522b\u8fbe\u523075.6%\u300181.6%\u51c6\u786e\u7387\u548c91.36% ES\u300127.7% EMR\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NES\u4e3a\u53ef\u6269\u5c55\u7684\u884c\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u5728FinTech\u516c\u53f8\u6210\u529f\u5e94\u7528\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u63d0\u5347\u5f00\u6e90CodeLLMs\u6027\u80fd\u3002"}}
{"id": "2508.01553", "pdf": "https://arxiv.org/pdf/2508.01553", "abs": "https://arxiv.org/abs/2508.01553", "authors": ["Sameer Neupane", "Mithun Saha", "David M. Almeida", "Santosh Kumar"], "title": "How Many Times Do People Usually Experience Different Kinds of Stressors Each Day?", "categories": ["cs.HC"], "comment": "In Companion of the 2025 ACM International Joint Conference on\n  Pervasive and Ubiquitous Computing (UbiComp Companion '25)", "summary": "Understanding how frequently people experience different kinds of daily\nstressors is crucial for interpreting stress exposure and informing mental\nhealth care. But it can't be directly estimated from current assessment\nmethods, such as diaries, end-of-day interviews, and ecological momentary\nassessments (EMA), that use sparse sampling to limit participant burden, and a\nstructured response format for uniformity. In this paper, we utilize stressor\ndata collected in a 100-day field study with 68 participants that adopted\nwearable-triggered prompts and a freeform format to solicit stressors soon\nafter they occurred, but limited its prompts to a small subset to keep the\nburden low. We develop asymptotic models to estimate the latent frequency of\ndifferent kinds of real-life stressors that address sample sparsity and\nsampling bias. We find that people experience 5.39 stressors per day, on\naverage. The top three are related to work (1.76/day), health (0.59/day), and\ntransportation (0.55/day). These estimates offer a principled benchmark for\ninterpreting individual stressor loads. They can also inform mental health care\ntreatments and interventions by establishing population-level baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f30\u8ba1\u65e5\u5e38\u538b\u529b\u6e90\u9891\u7387\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7f\u6234\u8bbe\u5907\u89e6\u53d1\u63d0\u793a\u548c\u81ea\u7531\u683c\u5f0f\u6536\u96c6\u6570\u636e\uff0c\u7ed3\u5408\u6e10\u8fd1\u6a21\u578b\u89e3\u51b3\u6837\u672c\u7a00\u758f\u6027\u548c\u504f\u5dee\u3002\u7814\u7a76\u53d1\u73b0\u5e73\u5747\u6bcf\u4eba\u6bcf\u5929\u7ecf\u53865.39\u4e2a\u538b\u529b\u6e90\uff0c\u4e3b\u8981\u4e3a\u5de5\u4f5c\u3001\u5065\u5eb7\u548c\u4ea4\u901a\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u65e5\u8bb0\u3001\u8bbf\u8c08\u548cEMA\uff09\u56e0\u7a00\u758f\u91c7\u6837\u548c\u7ed3\u6784\u5316\u683c\u5f0f\u65e0\u6cd5\u51c6\u786e\u4f30\u7b97\u65e5\u5e38\u538b\u529b\u6e90\u7684\u9891\u7387\uff0c\u5f71\u54cd\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7a7f\u6234\u8bbe\u5907\u89e6\u53d1\u63d0\u793a\u548c\u81ea\u7531\u683c\u5f0f\u6536\u96c6\u6570\u636e\uff0c\u7ed3\u5408\u6e10\u8fd1\u6a21\u578b\u89e3\u51b3\u6837\u672c\u7a00\u758f\u6027\u548c\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5e73\u5747\u6bcf\u4eba\u6bcf\u5929\u7ecf\u53865.39\u4e2a\u538b\u529b\u6e90\uff0c\u5176\u4e2d\u5de5\u4f5c\uff081.76/\u5929\uff09\u3001\u5065\u5eb7\uff080.59/\u5929\uff09\u548c\u4ea4\u901a\uff080.55/\u5929\uff09\u4e3a\u4e3b\u8981\u538b\u529b\u6e90\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3a\u4e2a\u4f53\u538b\u529b\u8d1f\u8377\u63d0\u4f9b\u57fa\u51c6\uff0c\u5e76\u4e3a\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u63d0\u4f9b\u4eba\u7fa4\u7ea7\u57fa\u7ebf\u53c2\u8003\u3002"}}
{"id": "2508.02487", "pdf": "https://arxiv.org/pdf/2508.02487", "abs": "https://arxiv.org/abs/2508.02487", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Commit Stability as a Signal for Risk in Open-Source Projects", "categories": ["cs.SE"], "comment": null, "summary": "Open source software (OSS) generates trillions of dollars in economic value\nand has become essential to technical infrastructures worldwide. As\norganizations increasingly depend on OSS, understanding project evolution is\ncritical. While existing metrics provide insights into project health, one\ndimension remains understudied: project resilience -- the ability to return to\nnormal operations after disturbances such as contributor departures, security\nvulnerabilities, and bug report spikes. We hypothesize that stable commit\npatterns reflect underlying project characteristics such as mature governance,\nsustained contributors, and robust development processes that enable\nresilience. Building on the Composite Stability Index (CSI) framework, we\nempirically validate commit frequency patterns across 100 highly ranked\nrepositories. Our findings reveal that only 2\\% of repositories exhibit daily\nstability, 29\\% achieve weekly stability, and 50\\% demonstrate monthly\nstability, while half remain unstable across all temporal levels. Programming\nlanguages and blockchain applications were the most stable. We identified two\nexemplary repositories that achieved stability at all three granularities,\nwhose governance models, CI cadence, and release policies could serve as\nreference frameworks. We observed that large yearly commit throughput does not\nnecessarily correlate with stability. Beyond commits, stability can be enriched\nwith issue-resolution times, PR merge rates, and community-engagement metrics\nto broaden resilience assessment and sharpen stability-based risk evaluation.", "AI": {"tldr": "\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u7a33\u5b9a\u6027\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u6709\u5c11\u6570\u9879\u76ee\u5728\u6bcf\u65e5\u3001\u6bcf\u5468\u6216\u6bcf\u6708\u7ef4\u5ea6\u4e0a\u8868\u73b0\u51fa\u7a33\u5b9a\u63d0\u4ea4\u6a21\u5f0f\uff0c\u533a\u5757\u94fe\u9879\u76ee\u4e0e\u7f16\u7a0b\u8bed\u8a00\u9879\u76ee\u6700\u4e3a\u7a33\u5b9a\u3002", "motivation": "\u7406\u89e3\u5f00\u6e90\u9879\u76ee\u97e7\u6027\u5bf9\u5168\u7403\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6307\u6807\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u9879\u76ee\u5728\u5e72\u6270\u540e\u6062\u590d\u6b63\u5e38\u8fd0\u4f5c\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u590d\u5408\u7a33\u5b9a\u6027\u6307\u6570\u6846\u67b6\uff0c\u5bf9100\u4e2a\u9ad8\u8bc4\u5206\u4ed3\u5e93\u7684\u63d0\u4ea4\u9891\u7387\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u4ec52%\u7684\u9879\u76ee\u6bcf\u65e5\u7a33\u5b9a\uff0c29%\u6bcf\u5468\u7a33\u5b9a\uff0c50%\u6bcf\u6708\u7a33\u5b9a\uff1b\u7f16\u7a0b\u8bed\u8a00\u4e0e\u533a\u5757\u94fe\u9879\u76ee\u6700\u7a33\u5b9a\u3002", "conclusion": "\u7a33\u5b9a\u6027\u7814\u7a76\u53ef\u6269\u5c55\u81f3\u95ee\u9898\u89e3\u51b3\u65f6\u95f4\u7b49\u6307\u6807\uff0c\u4e3a\u9879\u76ee\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u66f4\u5168\u9762\u6846\u67b6\u3002"}}
{"id": "2508.01743", "pdf": "https://arxiv.org/pdf/2508.01743", "abs": "https://arxiv.org/abs/2508.01743", "authors": ["Shiyao Zhang", "Omar Faruk", "Robert Porzel", "Dennis K\u00fcster", "Tanja Schultz", "Hui Liu"], "title": "Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation", "categories": ["cs.HC"], "comment": "\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "An increasing number of online interaction settings now provide the\npossibility to visually represent oneself via an animated avatar instead of a\nvideo stream. Benefits include protecting the communicator's privacy while\nstill providing a means to express their individuality. In consequence, there\nhas been a surge in means for avatar-based personalization, ranging from\nclassic human representations to animals, food items, and more. However, using\navatars also has drawbacks. Depending on the human-likeness of the avatar and\nthe corresponding disparities between the avatar and the original expresser,\navatars may elicit discomfort or even hinder effective nonverbal communication\nby distorting emotion perception. This study examines the relationship between\nthe human-likeness of virtual avatars and emotion perception for Ekman's six\n\"basic emotions\". Research reveals that avatars with varying degrees of\nhuman-likeness have distinct effects on emotion perception. High human-likeness\navatars, such as human avatars, tend to elicit more negative emotional\nresponses from users, a phenomenon that is consistent with the concept of\nUncanny Valley in aesthetics, which suggests that closely resembling humans can\nprovoke negative emotional responses. Conversely, a raccoon avatar and a shark\navatar, known as cuteness, which exhibit moderate human similarity in this\nstudy, demonstrate a positive influence on emotion perception. Our initial\nresults suggest that the human-likeness of avatars is an important factor for\nemotion perception. The results from the follow-up study further suggest that\nthe cuteness of avatars and their natural facial status may also play a\nsignificant role in emotion perception and elicitation. We discuss practical\nimplications for strategically conveying specific human behavioral messages\nthrough avatars in multiple applications, such as business and counseling.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u5316\u8eab\u7684\u62df\u4eba\u5316\u7a0b\u5ea6\u4e0e\u60c5\u611f\u611f\u77e5\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u9ad8\u5ea6\u62df\u4eba\u5316\u7684\u5316\u8eab\u53ef\u80fd\u5f15\u53d1\u8d1f\u9762\u60c5\u7eea\uff0c\u800c\u4e2d\u7b49\u62df\u4eba\u5316\u7684\u53ef\u7231\u5f62\u8c61\u5219\u6709\u52a9\u4e8e\u79ef\u6781\u60c5\u611f\u611f\u77e5\u3002", "motivation": "\u968f\u7740\u865a\u62df\u5316\u8eab\u5728\u5728\u7ebf\u4e92\u52a8\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u62df\u4eba\u5316\u7a0b\u5ea6\u5bf9\u60c5\u611f\u611f\u77e5\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u8fd9\u4e00\u5173\u7cfb\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57c3\u514b\u66fc\u7684\u516d\u79cd\u201c\u57fa\u672c\u60c5\u7eea\u201d\u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\uff0c\u6bd4\u8f83\u4e0d\u540c\u62df\u4eba\u5316\u7a0b\u5ea6\u5316\u8eab\uff08\u5982\u4eba\u7c7b\u3001\u6d63\u718a\u3001\u9ca8\u9c7c\u7b49\uff09\u7684\u60c5\u611f\u611f\u77e5\u6548\u679c\u3002", "result": "\u9ad8\u5ea6\u62df\u4eba\u5316\u7684\u5316\u8eab\uff08\u5982\u4eba\u7c7b\u5f62\u8c61\uff09\u5f15\u53d1\u66f4\u591a\u8d1f\u9762\u60c5\u7eea\u53cd\u5e94\uff0c\u800c\u4e2d\u7b49\u62df\u4eba\u5316\u7684\u53ef\u7231\u5f62\u8c61\uff08\u5982\u6d63\u718a\u548c\u9ca8\u9c7c\uff09\u5219\u663e\u8457\u6539\u5584\u60c5\u611f\u611f\u77e5\u3002", "conclusion": "\u5316\u8eab\u7684\u62df\u4eba\u5316\u7a0b\u5ea6\u662f\u60c5\u611f\u611f\u77e5\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u53ef\u7231\u5ea6\u548c\u81ea\u7136\u9762\u90e8\u72b6\u6001\u4e5f\u53ef\u80fd\u5bf9\u60c5\u611f\u4f20\u9012\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u4e3a\u5546\u4e1a\u548c\u5fc3\u7406\u54a8\u8be2\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u7b56\u7565\u6027\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2508.01669", "pdf": "https://arxiv.org/pdf/2508.01669", "abs": "https://arxiv.org/abs/2508.01669", "authors": ["Ziru Niu", "Hai Dong", "A. K. Qin"], "title": "Boosting Generalization Performance in Model-Heterogeneous Federated Learning Using Variational Transposed Convolution", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning (FL) is a pioneering machine learning paradigm that\nenables distributed clients to process local data effectively while ensuring\ndata privacy. However, the efficacy of FL is usually impeded by the data\nheterogeneity among clients, resulting in local models with low generalization\nperformance. To address this problem, traditional model-homogeneous approaches\nmainly involve debiasing the local training procedures with regularization or\ndynamically adjusting client weights in aggregation. Nonetheless, these\napproaches become incompatible for scenarios where clients exhibit\nheterogeneous model architectures. In this paper, we propose a\nmodel-heterogeneous FL framework that can improve clients' generalization\nperformance over unseen data without model aggregation. Instead of model\nparameters, clients exchange the feature distributions with the server,\nincluding the mean and the covariance. Accordingly, clients train a variational\ntransposed convolutional (VTC) neural network with Gaussian latent variables\nsampled from the feature distributions, and use the VTC model to generate\nsynthetic data. By fine-tuning local models with the synthetic data, clients\nsignificantly increase their generalization performance. Experimental results\nshow that our approach obtains higher generalization accuracy than existing\nmodel-heterogeneous FL frameworks, as well as lower communication costs and\nmemory consumption", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5206\u5e03\u4ea4\u6362\u7684\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5ba2\u6237\u7aef\u6a21\u578b\u67b6\u6784\u4e0d\u540c\u7684\u573a\u666f\u4e0b\u3002", "method": "\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u4ea4\u6362\u7279\u5f81\u5206\u5e03\uff08\u5747\u503c\u548c\u534f\u65b9\u5dee\uff09\uff0c\u8bad\u7ec3\u53d8\u5206\u8f6c\u7f6e\u5377\u79ef\u7f51\u7edc\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u5fae\u8c03\u672c\u5730\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u5728\u6cdb\u5316\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff0c\u4e14\u901a\u4fe1\u6210\u672c\u548c\u5185\u5b58\u6d88\u8017\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02497", "pdf": "https://arxiv.org/pdf/2508.02497", "abs": "https://arxiv.org/abs/2508.02497", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation", "categories": ["cs.SE"], "comment": null, "summary": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5f00\u6e90\u6280\u672f\u6587\u6863\u7ffb\u8bd1\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u5176\u51c6\u786e\u6027\u548c\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u7ffb\u8bd1\u4fdd\u771f\u5ea6\u8bc4\u5206\u6846\u67b6TRIFID\u3002", "motivation": "\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u591a\u8bed\u8a00\u6587\u6863\uff0cLLM\u5728\u6280\u672f\u6587\u6863\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u8bc4\u4f30\u4e8650\u4efdREADME\u6587\u4ef6\u7684\u82f1\u5fb7\u7ffb\u8bd1\uff0c\u5bf9\u6bd4\u4e86ChatGPT 4\u548cClaude\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51faTRIFID\u6846\u67b6\u3002", "result": "LLM\u80fd\u63d0\u4f9b\u51c6\u786e\u7ffb\u8bd1\uff0c\u4f46\u5728\u4fdd\u7559\u7ed3\u6784\u548c\u683c\u5f0f\u4e0a\u5b58\u5728\u6311\u6218\u3002", "conclusion": "LLM\u5728\u6587\u6863\u56fd\u9645\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u4fdd\u771f\u5ea6\u95ee\u9898\uff0cTRIFID\u4e3a\u5b9e\u73b0\u81ea\u52a8\u5316\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.02541", "pdf": "https://arxiv.org/pdf/2508.02541", "abs": "https://arxiv.org/abs/2508.02541", "authors": ["Peter Hamfelt", "Ricardo Britto", "Lincoln Rocha", "Camilo Almendra"], "title": "Automatic Identification of Machine Learning-Specific Code Smells", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86ML\u5e94\u7528\u4e2d\u7684\u4ee3\u7801\u5f02\u5473\u95ee\u9898\uff0c\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u9759\u6001\u4ee3\u7801\u5206\u6790\u5de5\u5177MLpylint\uff0c\u5e76\u901a\u8fc7\u6587\u732e\u56de\u987e\u3001\u4e13\u5bb6\u54a8\u8be2\u548c\u5b9e\u9645\u6570\u636e\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u76ee\u524d\u9488\u5bf9ML\u5e94\u7528\u4e2d\u4ee3\u7801\u5f02\u5473\u7684\u7814\u7a76\u7f3a\u4e4f\u5de5\u5177\u548c\u652f\u6301\uff0c\u4e9f\u9700\u8bbe\u8ba1\u4e13\u7528\u5de5\u5177\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u732e\u56de\u987e\u548c\u4e13\u5bb6\u54a8\u8be2\u8bbe\u8ba1\u5de5\u5177MLpylint\uff0c\u5e76\u5728160\u4e2a\u5f00\u6e90ML\u5e94\u7528\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "MLpylint\u88ab\u8bc1\u660e\u662f\u6709\u6548\u4e14\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u9759\u6001\u9a8c\u8bc1\u548c\u4e13\u5bb6\u8c03\u67e5\u7ed3\u679c\u5747\u652f\u6301\u5176\u4ef7\u503c\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u672a\u6765\u5c06MLpylint\u96c6\u6210\u5230\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u521b\u65b0\u6027\u3002"}}
{"id": "2508.01789", "pdf": "https://arxiv.org/pdf/2508.01789", "abs": "https://arxiv.org/abs/2508.01789", "authors": ["Laura Sch\u00fctz", "Sasan Matinfar", "Ulrich Eck", "Daniel Roth", "Nassir Navab"], "title": "Sonify Anything: Towards Context-Aware Sonic Interactions in AR", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5; H.5.2; H.5.1; I.3.5"], "comment": null, "summary": "In Augmented Reality (AR), virtual objects interact with real objects.\nHowever, the lack of physicality of virtual objects leads to the absence of\nnatural sonic interactions. When virtual and real objects collide, either no\nsound or a generic sound is played. Both lead to an incongruent multisensory\nexperience, reducing interaction and object realism. Unlike in Virtual Reality\n(VR) and games, where predefined scenes and interactions allow for the playback\nof pre-recorded sound samples, AR requires real-time sound synthesis that\ndynamically adapts to novel contexts and objects to provide audiovisual\ncongruence during interaction. To enhance real-virtual object interactions in\nAR, we propose a framework for context-aware sounds using methods from computer\nvision to recognize and segment the materials of real objects. The material's\nphysical properties and the impact dynamics of the interaction are used to\ngenerate material-based sounds in real-time using physical modelling synthesis.\nIn a user study with 24 participants, we compared our congruent material-based\nsounds to a generic sound effect, mirroring the current standard of\nnon-context-aware sounds in AR applications. The results showed that\nmaterial-based sounds led to significantly more realistic sonic interactions.\nMaterial-based sounds also enabled participants to distinguish visually similar\nmaterials with significantly greater accuracy and confidence. These findings\nshow that context-aware, material-based sonic interactions in AR foster a\nstronger sense of realism and enhance our perception of real-world\nsurroundings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdAR\u4e2d\u57fa\u4e8e\u6750\u6599\u611f\u77e5\u7684\u5b9e\u65f6\u58f0\u97f3\u5408\u6210\u6846\u67b6\uff0c\u63d0\u5347\u865a\u5b9e\u4e92\u52a8\u7684\u771f\u5b9e\u611f\u3002", "motivation": "\u89e3\u51b3AR\u4e2d\u865a\u62df\u7269\u4f53\u56e0\u7f3a\u4e4f\u7269\u7406\u6027\u5bfc\u81f4\u7684\u97f3\u6548\u4e0d\u81ea\u7136\u95ee\u9898\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u8bc6\u522b\u771f\u5b9e\u7269\u4f53\u6750\u6599\uff0c\u7ed3\u5408\u7269\u7406\u5efa\u6a21\u5408\u6210\u5b9e\u65f6\u58f0\u97f3\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u6750\u6599\u7684\u58f0\u97f3\u663e\u8457\u63d0\u5347\u4e86\u58f0\u97f3\u4e92\u52a8\u7684\u771f\u5b9e\u611f\u548c\u6750\u6599\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u58f0\u97f3\u5408\u6210\u589e\u5f3a\u4e86AR\u4e2d\u7684\u771f\u5b9e\u611f\u548c\u73b0\u5b9e\u611f\u77e5\u3002"}}
{"id": "2508.01675", "pdf": "https://arxiv.org/pdf/2508.01675", "abs": "https://arxiv.org/abs/2508.01675", "authors": ["Ali Forootani", "Raffaele Iervolino"], "title": "Asynchronous Federated Learning with non-convex client objective functions and heterogeneous dataset", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices while preserving data privacy. However, traditional FL\nsuffers from communication overhead, system heterogeneity, and straggler\neffects. Asynchronous Federated Learning (AFL) addresses these by allowing\nclients to update independently, improving scalability and reducing\nsynchronization delays. This paper extends AFL to handle non-convex objective\nfunctions and heterogeneous datasets, common in modern deep learning. We\npresent a rigorous convergence analysis, deriving bounds on the expected\ngradient norm and studying the effects of staleness, variance, and\nheterogeneity. To mitigate stale updates, we introduce a staleness aware\naggregation that prioritizes fresher updates and a dynamic learning rate\nschedule that adapts to client staleness and heterogeneity, improving stability\nand convergence. Our framework accommodates variations in computational power,\ndata distribution, and communication delays, making it practical for real world\napplications. We also analyze the impact of client selection\nstrategies-sampling with or without replacement-on variance and convergence.\nImplemented in PyTorch with Python's asyncio, our approach is validated through\nexperiments demonstrating improved performance and scalability for\nasynchronous, heterogeneous, and non-convex FL scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\uff08AFL\uff09\u5728\u975e\u51f8\u76ee\u6807\u51fd\u6570\u548c\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9648\u65e7\u6027\u611f\u77e5\u7684\u805a\u5408\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u901a\u4fe1\u5f00\u9500\u3001\u7cfb\u7edf\u5f02\u6784\u6027\u548c\u6ede\u540e\u6548\u5e94\u7684\u95ee\u9898\uff0c\u6269\u5c55AFL\u4ee5\u9002\u5e94\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u975e\u51f8\u6027\u548c\u6570\u636e\u5f02\u6784\u6027\u3002", "method": "\u5f15\u5165\u9648\u65e7\u6027\u611f\u77e5\u805a\u5408\u548c\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u5206\u6790\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u5bf9\u6536\u655b\u7684\u5f71\u54cd\uff0c\u5e76\u5728PyTorch\u548cPython\u7684asyncio\u4e2d\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5f02\u6b65\u3001\u5f02\u6784\u548c\u975e\u51f8FL\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AFL\u4e2d\u7684\u9648\u65e7\u6027\u3001\u5f02\u6784\u6027\u548c\u6536\u655b\u6027\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.02611", "pdf": "https://arxiv.org/pdf/2508.02611", "abs": "https://arxiv.org/abs/2508.02611", "authors": ["Vali Tawosia", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "Meta-RAG on Large Codebases Using Code Summarization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u4ee3\u7406\u7cfb\u7edfMeta-RAG\uff0c\u5229\u7528\u4fe1\u606f\u68c0\u7d22\u548cLLM\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5b9a\u4f4d\u9519\u8bef\uff0c\u901a\u8fc7\u4ee3\u7801\u5e93\u538b\u7f29\u548c\u81ea\u7136\u8bed\u8a00\u8868\u793a\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u7684\u590d\u6742\u6027\u4e0d\u4ec5\u9650\u4e8e\u4ee3\u7801\u5b9e\u73b0\uff0c\u8fd8\u5305\u62ec\u7ef4\u62a4\u9636\u6bb5\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5b9a\u4f4d\u9519\u8bef\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u9ad8\u6548\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5Meta-RAG\uff0c\u901a\u8fc7\u6458\u8981\u538b\u7f29\u4ee3\u7801\u5e93\uff08\u5e73\u5747\u51cf\u5c1179.8%\uff09\uff0c\u5e76\u5229\u7528LLM\u4ee3\u7406\u8bc6\u522b\u5173\u952e\u4ee3\u7801\u533a\u57df\u3002", "result": "\u5728SWE-bench Lite\u6570\u636e\u96c6\u4e0a\uff0cMeta-RAG\u7684\u6587\u4ef6\u7ea7\u548c\u51fd\u6570\u7ea7\u6b63\u786e\u5b9a\u4f4d\u7387\u5206\u522b\u8fbe\u523084.67%\u548c53.0%\uff0c\u6027\u80fd\u9886\u5148\u3002", "conclusion": "Meta-RAG\u4e3a\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u7684\u9519\u8bef\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86LLM\u5728\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01837", "pdf": "https://arxiv.org/pdf/2508.01837", "abs": "https://arxiv.org/abs/2508.01837", "authors": ["Mark Steyvers", "Lukas Mayer"], "title": "When not to help: planning for lasting human-AI collaboration", "categories": ["cs.HC"], "comment": null, "summary": "AI systems and technologies that can interact with humans in real time face a\ncommunication dilemma: when to offer assistance and how frequently. Overly\nfrequent or contextually redundant assistance can cause users to disengage,\nundermining the long-term benefits of AI assistance. We introduce a cognitive\nmodeling framework based on Partially Observable Markov Decision Processes\n(POMDPs) that addresses this timing challenge by inferring a user's latent\ncognitive state related to AI engagement over time. Additionally, our framework\nincorporates reasoning about the long-term effects of AI assistance, explicitly\naiming to avoid actions that could lead the human user to disengage or\ndeactivate the AI. A key component of our approach is counterfactual reasoning:\nat each time step, the AI considers how well the user would perform\nindependently and weighs the potential boost in performance against the risk of\ndiminishing engagement with the AI. Through simulations, we show that this\nadaptive strategy significantly outperforms baseline policies in which\nassistance is always provided or never provided. Our results highlight the\nimportance of balancing short-term decision accuracy with sustained user\nengagement, showing how communication strategies can be optimized to avoid\nalert fatigue while preserving the user's receptiveness to AI guidance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePOMDP\u7684\u8ba4\u77e5\u5efa\u6a21\u6846\u67b6\uff0c\u89e3\u51b3AI\u5b9e\u65f6\u4ea4\u4e92\u4e2d\u7684\u65f6\u673a\u95ee\u9898\uff0c\u5e73\u8861\u5373\u65f6\u63f4\u52a9\u4e0e\u957f\u671f\u7528\u6237\u53c2\u4e0e\u3002", "motivation": "\u89e3\u51b3AI\u4ea4\u4e92\u4e2d\u9891\u7e41\u6216\u5197\u4f59\u63f4\u52a9\u5bfc\u81f4\u7528\u6237\u9000\u51fa\u7684\u95ee\u9898\uff0c\u4f18\u5316\u6c9f\u901a\u7b56\u7565\u4ee5\u907f\u514d\u75b2\u52b3\u3002", "method": "\u4f7f\u7528POMDP\u5efa\u6a21\u7528\u6237\u9690\u6027\u8ba4\u77e5\u72b6\u6001\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u63a8\u7406\u6743\u8861\u72ec\u7acb\u8868\u73b0\u4e0eAI\u63f4\u52a9\u3002", "result": "\u81ea\u9002\u5e94\u7b56\u7565\u5728\u4eff\u771f\u4e2d\u663e\u8457\u4f18\u4e8e\u59cb\u7ec8\u63d0\u4f9b\u6216\u4ece\u4e0d\u63d0\u4f9b\u63f4\u52a9\u7684\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u5e73\u8861\u77ed\u671f\u51b3\u7b56\u4e0e\u957f\u671f\u7528\u6237\u53c2\u4e0e\u662f\u4f18\u5316AI\u6c9f\u901a\u7b56\u7565\u7684\u5173\u952e\u3002"}}
{"id": "2508.01745", "pdf": "https://arxiv.org/pdf/2508.01745", "abs": "https://arxiv.org/abs/2508.01745", "authors": ["Xiangwang Hou", "Jingjing Wang", "Fangming Guan", "Jun Du", "Chunxiao Jiang", "Yong Ren"], "title": "Energy-Efficient Federated Learning for Edge Real-Time Vision via Joint Data, Computation, and Communication Design", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Emerging real-time computer vision (CV) applications on wireless edge devices\ndemand energy-efficient and privacy-preserving learning. Federated learning\n(FL) enables on-device training without raw data sharing, yet remains\nchallenging in resource-constrained environments due to energy-intensive\ncomputation and communication, as well as limited and non-i.i.d. local data. We\npropose FedDPQ, an ultra energy-efficient FL framework for real-time CV over\nunreliable wireless networks. FedDPQ integrates diffusion-based data\naugmentation, model pruning, communication quantization, and transmission power\ncontrol to enhance training efficiency. It expands local datasets using\nsynthetic data, reduces computation through pruning, compresses updates via\nquantization, and mitigates transmission outages with adaptive power control.\nWe further derive a closed-form energy-convergence model capturing the coupled\nimpact of these components, and develop a Bayesian optimization(BO)-based\nalgorithm to jointly tune data augmentation strategy, pruning ratio,\nquantization level, and power control. To the best of our knowledge, this is\nthe first work to jointly optimize FL performance from the perspectives of\ndata, computation, and communication under unreliable wireless conditions.\nExperiments on representative CV tasks show that FedDPQ achieves superior\nconvergence speed and energy efficiency.", "AI": {"tldr": "FedDPQ\u662f\u4e00\u4e2a\u9488\u5bf9\u65e0\u7ebf\u7f51\u7edc\u73af\u5883\u4e0b\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u8d85\u9ad8\u6548\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u3001\u6a21\u578b\u526a\u679d\u3001\u901a\u4fe1\u91cf\u5316\u548c\u529f\u7387\u63a7\u5236\u7b49\u6280\u672f\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u80fd\u6e90\u5229\u7528\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u8fb9\u7f18\u8bbe\u5907\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\u65f6\u7684\u9ad8\u80fd\u8017\u3001\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u53ca\u901a\u4fe1\u4e0d\u53ef\u9760\u7b49\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6570\u636e\u589e\u5f3a\u3001\u6a21\u578b\u526a\u679d\u3001\u901a\u4fe1\u91cf\u5316\u548c\u81ea\u9002\u5e94\u529f\u7387\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u8054\u5408\u8c03\u53c2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedDPQ\u5728\u6536\u655b\u901f\u5ea6\u548c\u80fd\u6e90\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "FedDPQ\u9996\u6b21\u8054\u5408\u4f18\u5316\u6570\u636e\u3001\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u4e3a\u4e0d\u53ef\u9760\u65e0\u7ebf\u7f51\u7edc\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01850", "pdf": "https://arxiv.org/pdf/2508.01850", "abs": "https://arxiv.org/abs/2508.01850", "authors": ["Lala Shakti Swarup Ray", "Vitor Fortes Rey", "Bo Zhou", "Paul Lukowicz", "Sungho Suh"], "title": "ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Prolonged seated activity is increasingly common in modern environments,\nraising concerns around musculoskeletal health, ergonomics, and the design of\nresponsive interactive systems. Existing posture sensing methods such as\nvision-based or wearable approaches face limitations including occlusion,\nprivacy concerns, user discomfort, and restricted deployment flexibility. We\nintroduce ChairPose, the first full body, wearable free seated pose estimation\nsystem that relies solely on pressure sensing and operates independently of\nchair geometry. ChairPose employs a two stage generative model trained on\npressure maps captured from a thin, chair agnostic sensing mattress. Unlike\nprior approaches, our method explicitly incorporates chair morphology into the\ninference process, enabling accurate, occlusion free, and privacy preserving\npose estimation. To support generalization across diverse users and chairs, we\nintroduce a physics driven data augmentation pipeline that simulates realistic\nvariations in posture and seating conditions. Evaluated across eight users and\nfour distinct chairs, ChairPose achieves a mean per joint position error of\n89.4 mm when both the user and the chair are unseen, demonstrating robust\ngeneralization to novel real world generalizability. ChairPose expands the\ndesign space for posture aware interactive systems, with potential applications\nin ergonomics, healthcare, and adaptive user interfaces.", "AI": {"tldr": "ChairPose\u662f\u4e00\u4e2a\u57fa\u4e8e\u538b\u529b\u4f20\u611f\u7684\u65e0\u7a7f\u6234\u5168\u8eab\u5750\u59ff\u4f30\u8ba1\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u906e\u6321\u3001\u9690\u79c1\u95ee\u9898\u7b49\u9650\u5236\u3002", "motivation": "\u73b0\u4ee3\u73af\u5883\u4e2d\u957f\u65f6\u95f4\u5750\u59ff\u6d3b\u52a8\u666e\u904d\uff0c\u73b0\u6709\u59ff\u6001\u611f\u77e5\u65b9\u6cd5\u5b58\u5728\u8bf8\u591a\u9650\u5236\uff0c\u9700\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u6280\u672f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u538b\u529b\u6620\u5c04\uff0c\u7ed3\u5408\u6905\u5b50\u5f62\u6001\u63a8\u65ad\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6cdb\u5316\u6027\u3002", "result": "\u5728\u672a\u89c1\u7528\u6237\u548c\u6905\u5b50\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u4e3a89.4\u6beb\u7c73\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ChairPose\u4e3a\u59ff\u52bf\u611f\u77e5\u4ea4\u4e92\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u9002\u7528\u4e8e\u4eba\u4f53\u5de5\u5b66\u3001\u533b\u7597\u7b49\u9886\u57df\u3002"}}
{"id": "2508.01807", "pdf": "https://arxiv.org/pdf/2508.01807", "abs": "https://arxiv.org/abs/2508.01807", "authors": ["Ignacy St\u0119pka", "Nicholas Gisolfi", "Kacper Tr\u0119bacz", "Artur Dubrawski"], "title": "Mitigating Persistent Client Dropout in Asynchronous Decentralized Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Presented on FedKDD Workshop at KDD 2025", "summary": "We consider the problem of persistent client dropout in asynchronous\nDecentralized Federated Learning (DFL). Asynchronicity and decentralization\nobfuscate information about model updates among federation peers, making\nrecovery from a client dropout difficult. Access to the number of learning\nepochs, data distributions, and all the information necessary to precisely\nreconstruct the missing neighbor's loss functions is limited. We show that\nobvious mitigations do not adequately address the problem and introduce\nadaptive strategies based on client reconstruction. We show that these\nstrategies can effectively recover some performance loss caused by dropout. Our\nwork focuses on asynchronous DFL with local regularization and differs\nsubstantially from that in the existing literature. We evaluate the proposed\nmethods on tabular and image datasets, involve three DFL algorithms, and three\ndata heterogeneity scenarios (iid, non-iid, class-focused non-iid). Our\nexperiments show that the proposed adaptive strategies can be effective in\nmaintaining robustness of federated learning, even if they do not reconstruct\nthe missing client's data precisely. We also discuss the limitations and\nidentify future avenues for tackling the problem of client dropout.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f02\u6b65\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u4e2d\u7684\u5ba2\u6237\u7aef\u6301\u7eed\u9000\u51fa\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5ba2\u6237\u7aef\u91cd\u5efa\u7684\u81ea\u9002\u5e94\u7b56\u7565\u6765\u7f13\u89e3\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u5f02\u6b65\u548c\u53bb\u4e2d\u5fc3\u5316\u5bfc\u81f4\u8054\u90a6\u5b66\u4e60\u4e2d\u5bf9\u5ba2\u6237\u7aef\u9000\u51fa\u540e\u7684\u4fe1\u606f\u6062\u590d\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5ba2\u6237\u7aef\u91cd\u5efa\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u573a\u666f\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8fd9\u4e9b\u7b56\u7565\u80fd\u6709\u6548\u6062\u590d\u90e8\u5206\u6027\u80fd\u635f\u5931\uff0c\u5c3d\u7ba1\u65e0\u6cd5\u7cbe\u786e\u91cd\u5efa\u7f3a\u5931\u5ba2\u6237\u7aef\u7684\u6570\u636e\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u89e3\u51b3\u5ba2\u6237\u7aef\u9000\u51fa\u95ee\u9898\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.00858", "pdf": "https://arxiv.org/pdf/2508.00858", "abs": "https://arxiv.org/abs/2508.00858", "authors": ["Christina Butsko", "Kristof Van Tricht", "Gabriel Tseng", "Giorgia Milli", "David Rolnick", "Ruben Cartuyvels", "Inbal Becker Reshef", "Zoltan Szantoi", "Hannah Kerner"], "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "The increasing availability of geospatial foundation models has the potential\nto transform remote sensing applications such as land cover classification,\nenvironmental monitoring, and change detection. Despite promising benchmark\nresults, the deployment of these models in operational settings is challenging\nand rare. Standardized evaluation tasks often fail to capture real-world\ncomplexities relevant for end-user adoption such as data heterogeneity,\nresource constraints, and application-specific requirements. This paper\npresents a structured approach to integrate geospatial foundation models into\noperational mapping systems. Our protocol has three key steps: defining\napplication requirements, adapting the model to domain-specific data and\nconducting rigorous empirical testing. Using the Presto model in a case study\nfor crop mapping, we demonstrate that fine-tuning a pre-trained model\nsignificantly improves performance over conventional supervised methods. Our\nresults highlight the model's strong spatial and temporal generalization\ncapabilities. Our protocol provides a replicable blueprint for practitioners\nand lays the groundwork for future research to operationalize foundation models\nin diverse remote sensing applications. Application of the protocol to the\nWorldCereal global crop-mapping system showcases the framework's scalability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5c06\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u4f5c\u7269\u6d4b\u7ed8\u4e2d\u7684\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u4ecd\u9762\u4e34\u6570\u636e\u5f02\u8d28\u6027\u3001\u8d44\u6e90\u9650\u5236\u7b49\u590d\u6742\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u64cd\u4f5c\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u6b65\u534f\u8bae\uff1a\u5b9a\u4e49\u5e94\u7528\u9700\u6c42\u3001\u6839\u636e\u9886\u57df\u6570\u636e\u8c03\u6574\u6a21\u578b\u3001\u8fdb\u884c\u4e25\u683c\u7684\u5b9e\u8bc1\u6d4b\u8bd5\uff0c\u5e76\u4ee5Presto\u6a21\u578b\u4e3a\u4f8b\u8fdb\u884c\u4f5c\u7269\u6d4b\u7ed8\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f5c\u7269\u6d4b\u7ed8\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\uff0c\u4e14\u6a21\u578b\u5177\u5907\u5f3a\u65f6\u7a7a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21\u4f5c\u7269\u6d4b\u7ed8\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.01860", "pdf": "https://arxiv.org/pdf/2508.01860", "abs": "https://arxiv.org/abs/2508.01860", "authors": ["Mansi Sharma", "Shuang Chen", "Philipp M\u00fcller", "Maurice Rekrut", "Antonio Kr\u00fcger"], "title": "Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "For machines to effectively assist humans in challenging visual search tasks,\nthey must differentiate whether a human is simply glancing into a scene\n(navigational intent) or searching for a target object (informational intent).\nPrevious research proposed combining electroencephalography (EEG) and\neye-tracking measurements to recognize such search intents implicitly, i.e.,\nwithout explicit user input. However, the applicability of these approaches to\nreal-world scenarios suffers from two key limitations. First, previous work\nused fixed search times in the informational intent condition -- a stark\ncontrast to visual search, which naturally terminates when the target is found.\nSecond, methods incorporating EEG measurements addressed prediction scenarios\nthat require ground truth training data from the target user, which is\nimpractical in many use cases. We address these limitations by making the first\npublicly available EEG and eye-tracking dataset for navigational vs.\ninformational intent recognition, where the user determines search times. We\npresent the first method for cross-user prediction of search intents from EEG\nand eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out\nevaluations -- comparable to within-user prediction accuracy (85.5%) but\noffering much greater flexibility", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408EEG\u548c\u773c\u52a8\u8ffd\u8e2a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u533a\u5206\u4eba\u7c7b\u7684\u5bfc\u822a\u610f\u56fe\u548c\u4fe1\u606f\u641c\u7d22\u610f\u56fe\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u9996\u4e2a\u76f8\u5173\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u56fa\u5b9a\u641c\u7d22\u65f6\u95f4\u548c\u9700\u8981\u76ee\u6807\u7528\u6237\u7684\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u8de8\u7528\u6237\u9884\u6d4b\u641c\u7d22\u610f\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u516c\u5f00\u4e86\u9996\u4e2aEEG\u548c\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u96c6\u3002", "result": "\u5728\u7559\u4e00\u7528\u6237\u8bc4\u4f30\u4e2d\u8fbe\u523084.5%\u7684\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u7528\u6237\u5185\u9884\u6d4b\u768485.5%\uff0c\u4f46\u7075\u6d3b\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533a\u5206\u641c\u7d22\u610f\u56fe\u4e0a\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548c\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.02317", "pdf": "https://arxiv.org/pdf/2508.02317", "abs": "https://arxiv.org/abs/2508.02317", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\\veomni\uff0c\u7528\u4e8e\u52a0\u901f\u5168\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5f00\u53d1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u548c\u5de5\u7a0b\u5f00\u9500\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u5168\u6a21\u6001LLMs\u8bad\u7ec3\u9762\u4e34\u67b6\u6784\u5f02\u6784\u6027\u53ca\u5e76\u884c\u903b\u8f91\u4e0e\u6a21\u578b\u5b9a\u4e49\u8026\u5408\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u53d7\u9650\u4e14\u5de5\u7a0b\u5f00\u9500\u5927\u3002", "method": "\\veomni\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u4e2d\u5fc3\u7684\u5206\u5e03\u5f0f\u65b9\u6848\u5c06\u901a\u4fe1\u4e0e\u8ba1\u7b97\u89e3\u8026\uff0c\u652f\u6301\u9ad8\u65483D\u5e76\u884c\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u914d\u7f6e\u63a5\u53e3\u4ee5\u65e0\u7f1d\u96c6\u6210\u65b0\u6a21\u6001\u3002", "result": "\u4f7f\u7528\\veomni\u8bad\u7ec330B\u53c2\u6570\u7684\u5168\u6a21\u6001\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MoE\uff09\uff0c\u5728128 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d22,800 token/GPU\u7684\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u53ef\u6269\u5c55\u81f3160K\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "\\veomni\u5c55\u73b0\u4e86\u5168\u6a21\u6001LLMs\u8bad\u7ec3\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2508.01878", "pdf": "https://arxiv.org/pdf/2508.01878", "abs": "https://arxiv.org/abs/2508.01878", "authors": ["Xinwu Ye", "Jun-Hsiang Yao", "Jielin Feng", "Shuhong Mei", "Xingyu Lan", "Siming Chen"], "title": "VidAnimator: User-Guided Stylized 3D Character Animation from Human Videos", "categories": ["cs.HC"], "comment": "14 pages, 7 figures, Xinwu Ye and Jun-Hsiang Yao contributed equally\n  to this work", "summary": "With captivating visual effects, stylized 3D character animation has gained\nwidespread use in cinematic production, advertising, social media, and the\npotential development of virtual reality (VR) non-player characters (NPCs).\nHowever, animating stylized 3D characters often requires significant time and\neffort from animators. We propose a mixed-initiative framework and interactive\nsystem to enable stylized 3D characters to mimic motion in human videos. The\nframework takes a single-view human video and a stylized 3D character (the\ntarget character) as input, captures the motion of the video, and then\ntransfers the motion to the target character. In addition, it involves two\ninteraction modules for customizing the result. Accordingly, the system\nincorporates two authoring tools that empower users with intuitive\nmodification. A questionnaire study offers tangible evidence of the framework's\ncapability of generating natural stylized 3D character animations similar to\nthe motion in the video. Additionally, three case studies demonstrate the\nutility of our approach in creating diverse results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e3b\u52a8\u6846\u67b6\u548c\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u89c6\u89d2\u89c6\u9891\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8fc1\u79fb\u5230\u98ce\u683c\u53163D\u89d2\u8272\u4e0a\uff0c\u7b80\u5316\u52a8\u753b\u5236\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u98ce\u683c\u53163D\u89d2\u8272\u52a8\u753b\u5236\u4f5c\u8017\u65f6\u8d39\u529b\uff0c\u9700\u7b80\u5316\u6d41\u7a0b\u4ee5\u6ee1\u8db3\u5f71\u89c6\u3001\u5e7f\u544a\u53caVR NPC\u5f00\u53d1\u7684\u9700\u6c42\u3002", "method": "\u6846\u67b6\u4ee5\u5355\u89c6\u89d2\u4eba\u7c7b\u89c6\u9891\u548c3D\u89d2\u8272\u4e3a\u8f93\u5165\uff0c\u6355\u6349\u52a8\u4f5c\u5e76\u8fc1\u79fb\uff1b\u5305\u542b\u4e24\u79cd\u4ea4\u4e92\u6a21\u5757\u548c\u7528\u6237\u81ea\u5b9a\u4e49\u5de5\u5177\u3002", "result": "\u95ee\u5377\u7814\u7a76\u663e\u793a\u6846\u67b6\u80fd\u751f\u6210\u81ea\u7136\u52a8\u753b\uff1b\u4e09\u4e2a\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u9ad8\u6548\u751f\u6210\u98ce\u683c\u53163D\u52a8\u753b\uff0c\u62d3\u5c55\u4e86\u52a8\u753b\u5236\u4f5c\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.01249", "pdf": "https://arxiv.org/pdf/2508.01249", "abs": "https://arxiv.org/abs/2508.01249", "authors": ["Peiran Wang", "Yang Liu", "Yunfei Lu", "Yifeng Cai", "Hongbo Chen", "Qingyou Yang", "Jie Zhang", "Jue Hong", "Ye Wu"], "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's working traces as graph-based intermediate\nrepresentations with control flow and data flow described within; (2) a\nproperty registry that attaches security-relevant metadata of interacted tools\n& data, and (3) a type system that performs static inference and checking over\nthe intermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis over sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can achieve 95.75% of TPR, with\nonly 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect\nprompt injection vulnerabilities and enforce fine-grained security constraints.", "AI": {"tldr": "AgentArmor\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u4ee3\u7406\u7684\u8fd0\u884c\u8ddf\u8e2a\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7a0b\u5e8f\u4f9d\u8d56\u8868\u793a\uff0c\u6765\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5e76\u6267\u884c\u5b89\u5168\u7b56\u7565\u3002", "motivation": "\u7531\u4e8eLLM\u4ee3\u7406\u7684\u52a8\u6001\u548c\u4e0d\u900f\u660e\u884c\u4e3a\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5206\u6790\u5176\u884c\u4e3a\u5e76\u589e\u5f3a\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faAgentArmor\u6846\u67b6\uff0c\u5305\u62ec\u56fe\u6784\u9020\u5668\u3001\u5c5e\u6027\u6ce8\u518c\u8868\u548c\u7c7b\u578b\u7cfb\u7edf\uff0c\u5c06\u4ee3\u7406\u884c\u4e3a\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7a0b\u5e8f\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728AgentDojo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentArmor\u5b9e\u73b0\u4e8695.75%\u7684\u771f\u5b9e\u9633\u6027\u7387\u548c3.66%\u7684\u5047\u9633\u6027\u7387\u3002", "conclusion": "AgentArmor\u80fd\u6709\u6548\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u5e76\u6267\u884c\u7ec6\u7c92\u5ea6\u5b89\u5168\u7ea6\u675f\u3002"}}
{"id": "2508.01881", "pdf": "https://arxiv.org/pdf/2508.01881", "abs": "https://arxiv.org/abs/2508.01881", "authors": ["Connor Bailey", "Michael Gleicher"], "title": "Anchoring and Alignment: Data Factors in Part-to-Whole Visualization", "categories": ["cs.HC"], "comment": "5 pages, 3 figures, IEEE Visualization conference, repository URL:\n  https://github.com/uwgraphics/PartToWhole, preregistration URL:\n  https://osf.io/e36au", "summary": "We explore the effects of data and design considerations through the example\ncase of part-to-whole data relationships. Standard part-to-whole\nrepresentations like pie charts and stacked bar charts make the relationships\nof parts to the whole explicit. Value estimation in these charts benefits from\ntwo perceptual mechanisms: anchoring, where the value is close to a reference\nvalue with an easily recognized shape, and alignment where the beginning or end\nof the shape is aligned with a marker. In an online study, we explore how data\nand design factors such as value, position, and encoding together impact these\neffects in making estimations in part-to-whole charts. The results show how\nsalient values and alignment to positions on a scale affect task performance.\nThis demonstrates the need for informed visualization design based around how\ndata properties and design factors affect perceptual mechanisms.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u6570\u636e\u548c\u8bbe\u8ba1\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u90e8\u5206\u4e0e\u6574\u4f53\u5173\u7cfb\u7684\u53ef\u89c6\u5316\uff0c\u7279\u522b\u662f\u951a\u5b9a\u548c\u5bf9\u9f50\u673a\u5236\u7684\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8\u90e8\u5206\u4e0e\u6574\u4f53\u5173\u7cfb\u53ef\u89c6\u5316\uff08\u5982\u997c\u56fe\u548c\u5806\u53e0\u6761\u5f62\u56fe\uff09\u4e2d\u6570\u636e\u548c\u8bbe\u8ba1\u56e0\u7d20\u5982\u4f55\u901a\u8fc7\u951a\u5b9a\u548c\u5bf9\u9f50\u673a\u5236\u5f71\u54cd\u6570\u503c\u4f30\u8ba1\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\uff0c\u5206\u6790\u6570\u503c\u3001\u4f4d\u7f6e\u548c\u7f16\u7801\u7b49\u6570\u636e\u548c\u8bbe\u8ba1\u56e0\u7d20\u5728\u90e8\u5206\u4e0e\u6574\u4f53\u56fe\u8868\u4e2d\u5bf9\u4f30\u8ba1\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7a81\u51fa\u6570\u503c\u548c\u4e0e\u523b\u5ea6\u5bf9\u9f50\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u6570\u636e\u548c\u8bbe\u8ba1\u56e0\u7d20\u5bf9\u611f\u77e5\u673a\u5236\u7684\u7406\u89e3\u8fdb\u884c\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u53ef\u89c6\u5316\u8bbe\u8ba1\u5e94\u7ed3\u5408\u6570\u636e\u5c5e\u6027\u548c\u8bbe\u8ba1\u56e0\u7d20\u5bf9\u611f\u77e5\u673a\u5236\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u90e8\u5206\u4e0e\u6574\u4f53\u5173\u7cfb\u7684\u8868\u73b0\u3002"}}
{"id": "2508.02534", "pdf": "https://arxiv.org/pdf/2508.02534", "abs": "https://arxiv.org/abs/2508.02534", "authors": ["Shunxian Gu", "Chaoqun You", "Bangbang Ren", "Deke Guo"], "title": "Communication and Computation Efficient Split Federated Learning in O-RAN", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The hierarchical architecture of Open Radio Access Network (O-RAN) has\nenabled a new Federated Learning (FL) paradigm that trains models using data\nfrom non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs).\nHowever, the ever-increasing model size leads to longer training time,\njeopardizing the deadline requirements for both non-RT and near-RT RICs. To\naddress this issue, split federated learning (SFL) offers an approach by\noffloading partial model layers from near-RT-RIC to high-performance\nnon-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent\ndata/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur\nsignificant communication cost in O-RAN. (ii) Proper allocation of\ncomputational and communication resources in O-RAN is vital to satisfying the\ndeadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL\nframework that exploits mutual learning to alternately and independently train\nthe near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating\nfrequent transfers. The ''inverse'' of the inverse model is derived via a\nzeroth-order technique to integrate the final model. Then, we solve a joint\noptimization problem for SplitMe to minimize overall resource costs with\ndeadline-aware selection of near-RT-RICs and adaptive local updates. Our\nnumerical results demonstrate that SplitMe remarkably outperforms FL frameworks\nlike SFL, FedAvg and O-RANFed regarding costs and convergence.", "AI": {"tldr": "\u63d0\u51fa\u4e86SplitMe\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8054\u5408\u5b66\u4e60\u89e3\u51b3O-RAN\u4e2d\u8bad\u7ec3\u65f6\u95f4\u957f\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8054\u5408\u5b66\u4e60\u5728O-RAN\u4e2d\u56e0\u6a21\u578b\u589e\u5927\u5bfc\u81f4\u7684\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528SplitMe\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bad\u7ec3\u548c\u53cd\u5411\u6a21\u578b\u6280\u672f\u6d88\u9664\u9891\u7e41\u4f20\u8f93\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u672c\u5730\u66f4\u65b0\u3002", "result": "SplitMe\u5728\u6210\u672c\u548c\u6536\u655b\u6027\u4e0a\u663e\u8457\u4f18\u4e8eSFL\u3001FedAvg\u548cO-RANFed\u3002", "conclusion": "SplitMe\u6709\u6548\u89e3\u51b3\u4e86O-RAN\u4e2d\u7684\u8054\u5408\u5b66\u4e60\u95ee\u9898\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u5229\u7528\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2508.01451", "pdf": "https://arxiv.org/pdf/2508.01451", "abs": "https://arxiv.org/abs/2508.01451", "authors": ["Mohammed Sayagh", "Mohammad Ghafari"], "title": "Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Machine learning and Large language models (LLMs) for vulnerability detection\nhas received significant attention in recent years. Unfortunately,\nstate-of-the-art techniques show that LLMs are unsuccessful in even\ndistinguishing the vulnerable function from its benign counterpart, due to\nthree main problems: Vulnerability detection requires deep analysis, which LLMs\noften struggle with when making a one-shot prediction. Existing techniques\ntypically perform function-level analysis, whereas effective vulnerability\ndetection requires contextual information beyond the function scope. The focus\non binary classification can result in identifying a vulnerability but\nassociating it with the wrong security weaknesses (CWE), which may mislead\ndevelopers. We propose a novel multi-agent LLM approach to address the\nchallenges of identifying CWEs. This approach consists of three steps: (1) a\nteam of LLM agents performs an exhaustive search for potential CWEs in the\nfunction under review, (2) another team of agents identifies relevant external\ncontext to support or refute each candidate CWE, and (3) a final agent makes\ninformed acceptance or rejection decisions for each CWE based on the gathered\ncontext. A preliminary evaluation of our approach shows promising results. In\nthe PrimeVul dataset, Step 1 correctly identifies the appropriate CWE in 40.9\\%\nof the studied vulnerable functions. We further evaluated the full pipeline on\nten synthetic programs and found that incorporating context information\nsignificantly reduced false positives from 6 to 9 CWEs to just 1 to 2, while\nstill correctly identifying the true CWE in 9 out of 10 cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53LLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u6b65\u6d41\u7a0b\u6539\u8fdb\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u663e\u8457\u51cf\u5c11\u8bef\u62a5\u5e76\u63d0\u9ad8CWE\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a\u4e00\u6b21\u6027\u9884\u6d4b\u7684\u6df1\u5ea6\u5206\u6790\u4e0d\u8db3\u3001\u7f3a\u4e4f\u51fd\u6570\u8303\u56f4\u5916\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3001\u4ee5\u53ca\u9519\u8bef\u7684CWE\u5173\u8054\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u591a\u667a\u80fd\u4f53LLM\u65b9\u6cd5\uff1a(1)\u641c\u7d22\u6f5c\u5728CWE\uff0c(2)\u901a\u8fc7\u5916\u90e8\u4e0a\u4e0b\u6587\u9a8c\u8bc1CWE\uff0c(3)\u57fa\u4e8e\u4e0a\u4e0b\u6587\u505a\u51fa\u6700\u7ec8\u51b3\u7b56\u3002", "result": "\u5728PrimeVul\u6570\u636e\u96c6\u4e2d\uff0c\u6b65\u9aa41\u6b63\u786e\u8bc6\u522bCWE\u7684\u51c6\u786e\u7387\u4e3a40.9%\uff1b\u5168\u6d41\u7a0b\u6d4b\u8bd5\u4e2d\uff0c\u8bef\u62a5\u4ece6-9\u964d\u81f31-2\uff0c\u4e149/10\u6848\u4f8b\u6b63\u786e\u8bc6\u522bCWE\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7ed3\u5408\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u8bef\u62a5\u548c\u63d0\u9ad8CWE\u51c6\u786e\u6027\u65b9\u9762\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2508.01894", "pdf": "https://arxiv.org/pdf/2508.01894", "abs": "https://arxiv.org/abs/2508.01894", "authors": ["Haozhe Zhou", "Riku Arakawa", "Yuvraj Agarwal", "Mayank Goel"], "title": "IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "IMUs are regularly used to sense human motion, recognize activities, and\nestimate full-body pose. Users are typically required to place sensors in\npredefined locations that are often dictated by common wearable form factors\nand the machine learning model's training process. Consequently, despite the\nincreasing number of everyday devices equipped with IMUs, the limited\nadaptability has seriously constrained the user experience to only using a few\nwell-explored device placements (e.g., wrist and ears). In this paper, we\nrethink IMU-based motion sensing by acknowledging that signals can be captured\nfrom any point on the human body. We introduce IMU over Continuous Coordinates\n(IMUCoCo), a novel framework that maps signals from a variable number of IMUs\nplaced on the body surface into a unified feature space based on their spatial\ncoordinates. These features can be plugged into downstream models for pose\nestimation and activity recognition. Our evaluations demonstrate that IMUCoCo\nsupports accurate pose estimation in a wide range of typical and atypical\nsensor placements. Overall, IMUCoCo supports significantly more flexible use of\nIMUs for motion sensing than the state-of-the-art, allowing users to place\ntheir sensors-laden devices according to their needs and preferences. The\nframework also supports the ability to change device locations depending on the\ncontext and suggests placement depending on the use case.", "AI": {"tldr": "\u63d0\u51faIMUCoCo\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u4f4d\u7f6e\u7684IMU\u4f20\u611f\u5668\u8fdb\u884c\u8fd0\u52a8\u611f\u77e5\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edfIMU\u4f7f\u7528\u53d7\u9650\u4e8e\u56fa\u5b9a\u4f4d\u7f6e\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u591a\u8bbe\u5907\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u7a7a\u95f4\u5750\u6807\u7684\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\uff0c\u5c06\u4efb\u610f\u4f4d\u7f6e\u7684IMU\u4fe1\u53f7\u6620\u5c04\u5176\u4e2d\u3002", "result": "IMUCoCo\u5728\u5178\u578b\u548c\u975e\u5178\u578b\u653e\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u7075\u6d3b\u4f7f\u7528\u3002", "conclusion": "IMUCoCo\u63d0\u5347\u4e86IMU\u7684\u9002\u5e94\u6027\uff0c\u652f\u6301\u7528\u6237\u6309\u9700\u653e\u7f6e\u4f20\u611f\u5668\u3002"}}
{"id": "2508.01906", "pdf": "https://arxiv.org/pdf/2508.01906", "abs": "https://arxiv.org/abs/2508.01906", "authors": ["Yingfan Zhou", "Ester Chen", "Manasa Pisipati", "Aiping Xiong", "Sarah Rajtmajer"], "title": "Effect of AI Performance, Risk Perception, and Trust on Human Dependence in Deepfake Detection AI system", "categories": ["cs.HC"], "comment": null, "summary": "Synthetic images, audio, and video can now be generated and edited by\nArtificial Intelligence (AI). In particular, the malicious use of synthetic\ndata has raised concerns about potential harms to cybersecurity, personal\nprivacy, and public trust. Although AI-based detection tools exist to help\nidentify synthetic content, their limitations often lead to user mistrust and\nconfusion between real and fake content. This study examines the role of AI\nperformance in influencing human trust and decision making in synthetic data\nidentification. Through an online human subject experiment involving 400\nparticipants, we examined how varying AI performance impacts human trust and\ndependence on AI in deepfake detection. Our findings indicate how participants\ncalibrate their dependence on AI based on their perceived risk and the\nprediction results provided by AI. These insights contribute to the development\nof transparent and explainable AI systems that better support everyday users in\nmitigating the harms of synthetic media.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u6027\u80fd\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u548c\u5408\u6210\u6570\u636e\u8bc6\u522b\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u4f1a\u6839\u636e\u611f\u77e5\u98ce\u9669\u548cAI\u9884\u6d4b\u7ed3\u679c\u8c03\u6574\u5bf9AI\u7684\u4f9d\u8d56\u3002", "motivation": "\u5408\u6210\u6570\u636e\u7684\u6076\u610f\u4f7f\u7528\u5f15\u53d1\u4e86\u5bf9\u7f51\u7edc\u5b89\u5168\u3001\u4e2a\u4eba\u9690\u79c1\u548c\u516c\u5171\u4fe1\u4efb\u7684\u62c5\u5fe7\uff0c\u4f46AI\u68c0\u6d4b\u5de5\u5177\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u7528\u6237\u5bf9\u5176\u4e0d\u4fe1\u4efb\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u6d89\u53ca400\u540d\u53c2\u4e0e\u8005\u7684\u5728\u7ebf\u5b9e\u9a8c\uff0c\u7814\u7a76AI\u6027\u80fd\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5728Deepfake\u68c0\u6d4b\u4e2d\u5bf9AI\u7684\u4fe1\u4efb\u548c\u4f9d\u8d56\u3002", "result": "\u53c2\u4e0e\u8005\u6839\u636e\u611f\u77e5\u98ce\u9669\u548cAI\u9884\u6d4b\u7ed3\u679c\u8c03\u6574\u5bf9AI\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\uff0c\u5e2e\u52a9\u666e\u901a\u7528\u6237\u51cf\u5c11\u5408\u6210\u5a92\u4f53\u7684\u5371\u5bb3\u3002"}}
{"id": "2508.01655", "pdf": "https://arxiv.org/pdf/2508.01655", "abs": "https://arxiv.org/abs/2508.01655", "authors": ["Zhihao Li", "Chaozheng Wang", "Zongjie Li", "Xinyong Peng", "Qun Xia", "Haochuan Lu", "Ting Xiong", "Shuzheng Gao", "Cuiyun Gao", "Shuai Wang", "Yuetang Deng", "Huafeng Ma"], "title": "JSidentify-V2: Leveraging Dynamic Memory Fingerprinting for Mini-Game Plagiarism Detection", "categories": ["cs.CR", "cs.SE"], "comment": "12 pages", "summary": "The explosive growth of mini-game platforms has led to widespread code\nplagiarism, where malicious users access popular games' source code and\nrepublish them with modifications. While existing static analysis tools can\ndetect simple obfuscation techniques like variable renaming and dead code\ninjection, they fail against sophisticated deep obfuscation methods such as\nencrypted code with local or cloud-based decryption keys that completely\ndestroy code structure and render traditional Abstract Syntax Tree analysis\nineffective. To address these challenges, we present JSidentify-V2, a novel\ndynamic analysis framework that detects mini-game plagiarism by capturing\nmemory invariants during program execution. Our key insight is that while\nobfuscation can severely distort static code characteristics, runtime memory\nbehavior patterns remain relatively stable. JSidentify-V2 employs a four-stage\npipeline: (1) static pre-analysis and instrumentation to identify potential\nmemory invariants, (2) adaptive hot object slicing to maximize execution\ncoverage of critical code segments, (3) Memory Dependency Graph construction to\nrepresent behavioral fingerprints resilient to obfuscation, and (4) graph-based\nsimilarity analysis for plagiarism detection.\n  We evaluate JSidentify-V2 against eight obfuscation methods on a\ncomprehensive dataset of 1,200 mini-games ...", "AI": {"tldr": "JSidentify-V2\u662f\u4e00\u4e2a\u52a8\u6001\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5c0f\u6e38\u620f\u5e73\u53f0\u7684\u4ee3\u7801\u6284\u88ad\uff0c\u901a\u8fc7\u6355\u6349\u7a0b\u5e8f\u6267\u884c\u65f6\u7684\u5185\u5b58\u4e0d\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u5206\u6790\u5de5\u5177\u65e0\u6cd5\u5e94\u5bf9\u7684\u6df1\u5ea6\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5c0f\u6e38\u620f\u5e73\u53f0\u7684\u7206\u70b8\u6027\u589e\u957f\uff0c\u4ee3\u7801\u6284\u88ad\u73b0\u8c61\u666e\u904d\uff0c\u6076\u610f\u7528\u6237\u901a\u8fc7\u4fee\u6539\u6e90\u4ee3\u7801\u91cd\u65b0\u53d1\u5e03\u3002\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u5bf9\u7b80\u5355\u6df7\u6dc6\u6709\u6548\uff0c\u4f46\u5bf9\u6df1\u5ea6\u6df7\u6dc6\uff08\u5982\u52a0\u5bc6\u4ee3\u7801\uff09\u65e0\u6548\u3002", "method": "JSidentify-V2\u91c7\u7528\u56db\u9636\u6bb5\u6d41\u7a0b\uff1a\u9759\u6001\u9884\u5206\u6790\u4e0e\u63d2\u6869\u3001\u81ea\u9002\u5e94\u70ed\u70b9\u5bf9\u8c61\u5207\u7247\u3001\u5185\u5b58\u4f9d\u8d56\u56fe\u6784\u5efa\u548c\u57fa\u4e8e\u56fe\u7684\u76f8\u4f3c\u6027\u5206\u6790\u3002", "result": "JSidentify-V2\u57281,200\u4e2a\u5c0f\u6e38\u620f\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u80fd\u591f\u5e94\u5bf9\u516b\u79cd\u6df7\u6dc6\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u5206\u6790\u5185\u5b58\u884c\u4e3a\u6a21\u5f0f\u53ef\u6709\u6548\u68c0\u6d4b\u6284\u88ad\uff0cJSidentify-V2\u4e3a\u590d\u6742\u6df7\u6dc6\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02075", "pdf": "https://arxiv.org/pdf/2508.02075", "abs": "https://arxiv.org/abs/2508.02075", "authors": ["Ekai Hashimoto", "Takeshi Mizumoto", "Kohei Nagira", "Shun Shiramatsu"], "title": "Human Capital Visualization using Speech Amount during Meetings", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work", "summary": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u4f1a\u8bae\u4e2d\u7684\u53d1\u8a00\u91cf\uff0c\u63d0\u51fa\u53ef\u89c6\u5316\u4eba\u529b\u8d44\u672c\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e0d\u540c\u5c5e\u6027\u548c\u53c2\u4e0e\u8005\u7684\u53d1\u8a00\u5dee\u5f02\uff0c\u5e76\u4e0e\u8fde\u7eed\u5c5e\u6027\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u672a\u6db5\u76d6\u5bf9\u8bdd\u5728\u4eba\u529b\u8d44\u672c\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u5bf9\u8bdd\u53ef\u89c6\u5316\u6280\u672f\u91cf\u5316\u53d1\u8a00\u91cf\uff0c\u5206\u6790\u6027\u522b\u3001\u804c\u4f4d\u7b49\u5c5e\u6027\u7684\u53d1\u8a00\u5dee\u5f02\u53ca\u4e0e\u8fde\u7eed\u5c5e\u6027\u7684\u76f8\u5173\u6027\u3002", "result": "\u901a\u8fc7\u4e2d\u5c0f\u4f01\u4e1a\u7684\u5468\u4f1a\u6570\u636e\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53d1\u8a00\u91cf\u5206\u6790\u53ef\u4f5c\u4e3a\u4eba\u529b\u8d44\u672c\u53ef\u89c6\u5316\u7684\u6709\u6548\u5de5\u5177\uff0c\u52a9\u529b\u7ec4\u7ec7\u521b\u65b0\u4e0e\u6c9f\u901a\u3002"}}
{"id": "2508.01750", "pdf": "https://arxiv.org/pdf/2508.01750", "abs": "https://arxiv.org/abs/2508.01750", "authors": ["Changze Huang", "Di Wang", "Zhi Quan Zhou"], "title": "LLM-Assisted Model-Based Fuzzing of Protocol Implementations", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Testing network protocol implementations is critical for ensuring the\nreliability, security, and interoperability of distributed systems. Faults in\nprotocol behavior can lead to vulnerabilities and system failures, especially\nin real-time and mission-critical applications. A common approach to protocol\ntesting involves constructing Markovian models that capture the state\ntransitions and expected behaviors of the protocol. However, building such\nmodels typically requires significant domain expertise and manual effort,\nmaking the process time-consuming and difficult to scale across diverse\nprotocols and implementations.\n  We propose a novel method that leverages large language models (LLMs) to\nautomatically generate sequences for testing network protocol implementations.\nOur approach begins by defining the full set of possible protocol states, from\nwhich the LLM selects a subset to model the target implementation. Using this\nstate-based model, we prompt the LLM to generate code that produces sequences\nof states. This program serves as a protocol-specific sequences generator. The\nsequences generator then generates test inputs to call the protocol\nimplementation under various conditions. We evaluated our approach on three\nwidely used network protocol implementations and successfully identified 12\npreviously unknown vulnerabilities. We have reported them to the respective\ndevelopers for confirmation. This demonstrates the practical effectiveness of\nour LLM-assisted fuzzing framework in uncovering real-world security issues.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u7f51\u7edc\u534f\u8bae\u6d4b\u8bd5\u5e8f\u5217\u7684\u65b0\u65b9\u6cd5\uff0c\u6210\u529f\u8bc6\u522b\u4e8612\u4e2a\u672a\u77e5\u6f0f\u6d1e\u3002", "motivation": "\u6d4b\u8bd5\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u5bf9\u4e8e\u786e\u4fdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u624b\u52a8\u5efa\u6a21\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5229\u7528LLM\u4ece\u534f\u8bae\u72b6\u6001\u96c6\u4e2d\u9009\u62e9\u5b50\u96c6\u5efa\u6a21\uff0c\u751f\u6210\u72b6\u6001\u5e8f\u5217\u4ee3\u7801\u4f5c\u4e3a\u6d4b\u8bd5\u5e8f\u5217\u751f\u6210\u5668\uff0c\u8fdb\u800c\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\u3002", "result": "\u5728\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u4e2d\u6210\u529f\u53d1\u73b0\u4e8612\u4e2a\u672a\u77e5\u6f0f\u6d1e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7LLM\u8f85\u52a9\u7684\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u53d1\u73b0\u5b9e\u9645\u5b89\u5168\u6f0f\u6d1e\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.02133", "pdf": "https://arxiv.org/pdf/2508.02133", "abs": "https://arxiv.org/abs/2508.02133", "authors": ["Yitong Zhu", "Lei Han", "GuanXuan Jiang", "PengYuan Zhou", "Yuyang Wang"], "title": "Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs", "categories": ["cs.HC"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for human-computer\ninteraction, yet real-world challenges like dynamic modality incompleteness and\nasynchrony severely limit its robustness. Existing methods often assume\nconsistently complete data or lack dynamic adaptability. To address these\nlimitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)\nframework for robust continuous emotion prediction. This framework employs a\ndual-layer expert structure. A Modality Expert Bank utilizes soft routing to\ndynamically handle missing modalities and achieve robust information fusion. A\nsubsequent Emotion Expert Bank leverages differential-attention routing to\nflexibly attend to emotional prototypes, enabling fine-grained emotion\nrepresentation. Additionally, a cross-modal alignment module explicitly\naddresses temporal shifts and semantic inconsistencies between modalities.\nExtensive experiments on benchmark datasets DEAP and DREAMER demonstrate our\nmodel's state-of-the-art performance in continuous emotion regression,\nshowcasing exceptional robustness under challenging conditions such as dynamic\nmodality absence and asynchronous sampling. This research significantly\nadvances the development of intelligent emotion systems adaptable to complex\nreal-world environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHi-MoE\u7684\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u52a8\u6001\u6a21\u6001\u7f3a\u5931\u548c\u5f02\u6b65\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5bf9\u6570\u636e\u5b8c\u6574\u6027\u5047\u8bbe\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u4e14\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u52a8\u6001\u6a21\u6001\u7f3a\u5931\u548c\u5f02\u6b65\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7684Hi-MoE\u6846\u67b6\u91c7\u7528\u53cc\u5c42\u4e13\u5bb6\u7ed3\u6784\uff1a\u6a21\u6001\u4e13\u5bb6\u5e93\u901a\u8fc7\u8f6f\u8def\u7531\u52a8\u6001\u5904\u7406\u7f3a\u5931\u6a21\u6001\u4fe1\u606f\uff0c\u60c5\u611f\u4e13\u5bb6\u5e93\u4f7f\u7528\u5dee\u5f02\u6ce8\u610f\u529b\u8def\u7531\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u60c5\u611f\u8868\u793a\uff0c\u53e6\u5916\u8fd8\u5f15\u5165\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u89e3\u51b3\u65f6\u95f4\u504f\u79fb\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5728DEAP\u548cDREAMER\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u8fde\u7eed\u60c5\u611f\u56de\u5f52\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u52a8\u6001\u6a21\u6001\u7f3a\u5931\u548c\u5f02\u6b65\u91c7\u6837\u7684\u5f3a\u5927\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u663e\u8457\u63a8\u52a8\u4e86\u9002\u5e94\u590d\u6742\u73b0\u5b9e\u73af\u5883\u7684\u667a\u80fd\u60c5\u611f\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.02173", "pdf": "https://arxiv.org/pdf/2508.02173", "abs": "https://arxiv.org/abs/2508.02173", "authors": ["Zhuangze Hou", "Jingze Tian", "Nianlong Li", "Farong Ren", "Can Liu"], "title": "EchoLadder: Progressive AI-Assisted Design of Immersive VR Scenes", "categories": ["cs.HC"], "comment": "To appear at UIST 2025", "summary": "Mixed reality platforms allow users to create virtual environments, yet\nnovice users struggle with both ideation and execution in spatial design. While\nexisting AI models can automatically generate scenes based on user prompts, the\nlack of interactive control limits users' ability to iteratively steer the\noutput. In this paper, we present EchoLadder, a novel human-AI collaboration\npipeline that leverages large vision-language model (LVLM) to support\ninteractive scene modification in virtual reality. EchoLadder accepts users'\nverbal instructions at varied levels of abstraction and spatial specificity,\ngenerates concrete design suggestions throughout a progressive design process.\nThe suggestions can be automatically applied, regenerated and retracted by\nusers' toggle control.Our ablation study showed effectiveness of our pipeline\ncomponents. Our user study found that, compared to baseline without showing\nsuggestions, EchoLadder better supports user creativity in spatial design. It\nalso contributes insights on users' progressive design strategies under AI\nassistance, providing design implications for future systems.", "AI": {"tldr": "EchoLadder\u662f\u4e00\u79cd\u65b0\u578b\u4eba\u673a\u534f\u4f5c\u7ba1\u9053\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u652f\u6301\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u4ea4\u4e92\u5f0f\u573a\u666f\u4fee\u6539\uff0c\u63d0\u5347\u7528\u6237\u7684\u521b\u9020\u529b\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u7a7a\u95f4\u8bbe\u8ba1\u4e2d\u7684\u6784\u601d\u4e0e\u6267\u884c\u56f0\u96be\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u63a7\u5236\u4ee5\u907f\u514d\u73b0\u6709AI\u6a21\u578b\u7f3a\u4e4f\u8fed\u4ee3\u5f15\u5bfc\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u53e3\u5934\u6307\u4ee4\u751f\u6210\u5177\u4f53\u8bbe\u8ba1\u5efa\u8bae\uff0c\u652f\u6301\u81ea\u52a8\u5e94\u7528\u3001\u91cd\u65b0\u751f\u6210\u548c\u64a4\u9500\u3002", "result": "\u7814\u7a76\u663e\u793aEchoLadder\u6709\u6548\u652f\u6301\u7528\u6237\u521b\u9020\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7b56\u7565\u89c1\u89e3\u3002", "conclusion": "EchoLadder\u6210\u529f\u63d0\u5347\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e3aAI\u8f85\u52a9\u8bbe\u8ba1\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2508.02216", "pdf": "https://arxiv.org/pdf/2508.02216", "abs": "https://arxiv.org/abs/2508.02216", "authors": ["Hyeok Kim", "Jeffrey Heer"], "title": "Data Augmentation for Visualization Design Knowledge Bases", "categories": ["cs.HC"], "comment": "10 pages, 4 tables, 7 figures, accepted to IEEE VIS 2025", "summary": "Visualization knowledge bases enable computational reasoning and\nrecommendation over a visualization design space. These systems evaluate design\ntrade-offs using numeric weights assigned to different features (e.g., binning\na variable). Feature weights can be learned automatically by fitting a model to\na collection of chart pairs, in which one chart is deemed preferable to the\nother. To date, labeled chart pairs have been drawn from published empirical\nresearch results; however, such pairs are not comprehensive, resulting in a\ntraining corpus that lacks many design variants and fails to systematically\nassess potential trade-offs. To improve knowledge base coverage and accuracy,\nwe contribute data augmentation techniques for generating and labeling chart\npairs. We present methods to generate novel chart pairs based on design\npermutations and by identifying under-assessed features -- leading to an\nexpanded corpus with thousands of new chart pairs, now in need of labels.\nAccordingly, we next compare varied methods to scale labeling efforts to\nannotate chart pairs, in order to learn updated feature weights. We evaluate\nour methods in the context of the Draco knowledge base, demonstrating\nimprovements to both feature coverage and chart recommendation performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u548c\u6807\u6ce8\u56fe\u8868\u5bf9\uff0c\u6269\u5c55\u53ef\u89c6\u5316\u77e5\u8bc6\u5e93\u7684\u8986\u76d6\u8303\u56f4\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u6807\u6ce8\u65b9\u6cd5\u4ee5\u4f18\u5316\u7279\u5f81\u6743\u91cd\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u5bf9\u6570\u636e\u96c6\u4e0d\u5168\u9762\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u7cfb\u7edf\u6027\u7684\u8bbe\u8ba1\u53d8\u4f53\u8bc4\u4f30\uff0c\u5f71\u54cd\u4e86\u53ef\u89c6\u5316\u77e5\u8bc6\u5e93\u7684\u6784\u5efa\u548c\u63a8\u8350\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u6392\u5217\u548c\u8bc6\u522b\u672a\u5145\u5206\u8bc4\u4f30\u7684\u7279\u5f81\u751f\u6210\u65b0\u56fe\u8868\u5bf9\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6807\u6ce8\u65b9\u6cd5\u4ee5\u9ad8\u6548\u6807\u6ce8\u8fd9\u4e9b\u65b0\u6570\u636e\u3002", "result": "\u65b9\u6cd5\u5728Draco\u77e5\u8bc6\u5e93\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u8986\u76d6\u548c\u56fe\u8868\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "\u6570\u636e\u589e\u5f3a\u548c\u9ad8\u6548\u6807\u6ce8\u6280\u672f\u80fd\u591f\u6709\u6548\u6269\u5c55\u77e5\u8bc6\u5e93\uff0c\u63d0\u9ad8\u53ef\u89c6\u5316\u8bbe\u8ba1\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\u3002"}}
{"id": "2508.02359", "pdf": "https://arxiv.org/pdf/2508.02359", "abs": "https://arxiv.org/abs/2508.02359", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "title": "Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue", "categories": ["eess.SP", "cs.CV", "cs.SE"], "comment": null, "summary": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u9ad8\u5360\u7a7a\u6bd4\u89c6\u89c9\u523a\u6fc0\u5bf9\u7f13\u89e3SSVEP\u6280\u672f\u4e2d\u773c\u75b2\u52b3\u7684\u5f71\u54cd\uff0c\u53d1\u73b085%\u5360\u7a7a\u6bd4\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3SSVEP\u6280\u672f\u4e2d\u56e0\u89c6\u89c9\u75b2\u52b3\u548cPWM\u7cbe\u5ea6\u95ee\u9898\u5bfc\u81f4\u7684\u957f\u671f\u4f7f\u7528\u9650\u5236\u3002", "method": "\u4f7f\u752850-95%\u5360\u7a7a\u6bd4\u7684PWM\u523a\u6fc0\uff0c\u8bb0\u5f55EEG\u6570\u636e\u5e76\u6d4b\u8bd5\u5341\u540d\u53d7\u8bd5\u8005\u7684\u53cd\u5e94\u3002", "result": "\u9ad8\u5360\u7a7a\u6bd4\u51cf\u5c11\u89c6\u89c9\u75b2\u52b3\uff0c85%\u5360\u7a7a\u6bd4\u65f6SSVEP\u54cd\u5e94\u6700\u4f73\uff0c\u4e14\u6548\u679c\u53d7\u8bd5\u8005\u72ec\u7acb\u3002", "conclusion": "\u9ad8\u5360\u7a7a\u6bd4\u523a\u6fc0\u53ef\u63d0\u5347SSVEP\u6280\u672f\u7684\u5b9e\u7528\u6027\uff0c85%\u5360\u7a7a\u6bd4\u4e3a\u63a8\u8350\u503c\u3002"}}
{"id": "2508.02232", "pdf": "https://arxiv.org/pdf/2508.02232", "abs": "https://arxiv.org/abs/2508.02232", "authors": ["Lei Han", "Mingnan Wei", "Qiongyan Chen", "Anqi Wang", "Rong Pang", "Kefei Liu", "Rongrong Chen", "David Yip"], "title": "Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults", "categories": ["cs.HC"], "comment": null, "summary": "Photo-based reminiscence has the potential to have a positive impact on older\nadults' reconnection with their personal history and improve their well-being.\nSupporting reminiscence in older adults through technological implementations\nis becoming an increasingly important area of research in the fields of HCI and\nCSCW. However, the impact of integrating gaze and speech as mixed-initiative\ninteractions in LLM-powered reminiscence conversations remains under-explored.\nTo address this, we conducted expert interviews to understand the challenges\nthat older adults face with LLM-powered, photo-based reminiscence experiences.\nBased on these design considerations, we developed Eye2Recall, a system that\nintegrates eye tracking for detecting visual interest with natural language\ninteraction to create a mixed-initiative reminiscence experience. We evaluated\nits effectiveness through a user study involving ten older adults. The results\nhave important implications for the future design of more accessible and\nempowering reminiscence technologies that better align with older adults'\nnatural interaction patterns and enhance their positive aging.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u51dd\u89c6\u548c\u8bed\u97f3\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u5728\u57fa\u4e8e\u7167\u7247\u7684\u56de\u5fc6\u6280\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u5f00\u53d1\u4e86Eye2Recall\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u5e2e\u52a9\u8001\u5e74\u4eba\u66f4\u81ea\u7136\u5730\u56de\u5fc6\u4e2a\u4eba\u5386\u53f2\uff0c\u63d0\u5347\u5176\u5e78\u798f\u611f\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u5728\u56de\u5fc6\u6280\u672f\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u4e86\u89e3\u8001\u5e74\u4eba\u9700\u6c42\uff0c\u5f00\u53d1\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684Eye2Recall\u7cfb\u7edf\uff0c\u5e76\u8fdb\u884c10\u540d\u8001\u5e74\u4eba\u7684\u7528\u6237\u7814\u7a76\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cEye2Recall\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301\u8001\u5e74\u4eba\u7684\u81ea\u7136\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u56de\u5fc6\u6280\u672f\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002", "conclusion": "\u591a\u6a21\u6001\u4ea4\u4e92\u6280\u672f\uff08\u5982Eye2Recall\uff09\u80fd\u63d0\u5347\u8001\u5e74\u4eba\u56de\u5fc6\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u79ef\u6781\u8001\u9f84\u5316\uff0c\u672a\u6765\u8bbe\u8ba1\u9700\u66f4\u6ce8\u91cd\u81ea\u7136\u4ea4\u4e92\u6a21\u5f0f\u3002"}}
{"id": "2508.02427", "pdf": "https://arxiv.org/pdf/2508.02427", "abs": "https://arxiv.org/abs/2508.02427", "authors": ["Tung-Thuy Pham", "Duy-Quan Luong", "Minh-Quan Duong", "Trung-Hieu Nguyen", "Thu-Trang Nguyen", "Son Nguyen", "Hieu Dinh Vo"], "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86CABENCH\uff0c\u4e00\u4e2a\u5305\u542b70\u4e2a\u73b0\u5b9e\u53ef\u7ec4\u5408AI\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6bd4\u8f83\u4e86\u4eba\u5de5\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u4e0eLLM\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u53ef\u7ec4\u5408AI\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86CABENCH\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4eba\u5de5\u8bbe\u8ba1\u4e0eLLM\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5c55\u793a\u4e86\u53ef\u7ec4\u5408AI\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u6548\u6267\u884c\u6d41\u6c34\u7ebf\u7684\u65b9\u6cd5\u3002", "conclusion": "\u53ef\u7ec4\u5408AI\u6709\u524d\u666f\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1\u81ea\u52a8\u5316\u65b9\u6cd5\u4ee5\u5145\u5206\u91ca\u653e\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.02274", "pdf": "https://arxiv.org/pdf/2508.02274", "abs": "https://arxiv.org/abs/2508.02274", "authors": ["Arjun Kumar", "Noppanat Wadlom", "Jaeheon Kwak", "Si-Hyuck Kang", "Insik Shin"], "title": "mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of Arrhythmia", "categories": ["cs.HC", "cs.LG", "92C55, 68T07"], "comment": "15 pages, 27 images", "summary": "Arrhythmia is a common cardiac condition that can precipitate severe\ncomplications without timely intervention. While continuous monitoring is\nessential for timely diagnosis, conventional approaches such as\nelectrocardiogram and wearable devices are constrained by their reliance on\nspecialized medical expertise and patient discomfort from their contact nature.\nExisting contactless monitoring, primarily designed for healthy subjects, face\nsignificant challenges when analyzing reflected signals from arrhythmia\npatients due to disrupted spatial stability and temporal consistency.\n  In this paper, we introduce mCardiacDx, a radar-driven contactless system\nthat accurately analyzes reflected signals and reconstructs heart pulse\nwaveforms for arrhythmia monitoring and diagnosis. The key contributions of our\nwork include a novel precise target localization (PTL) technique that locates\nreflected signals despite spatial disruptions, and an encoder-decoder model\nthat transforms these signals into HPWs, addressing temporal inconsistencies.\nOur evaluation on a large dataset of healthy subjects and arrhythmia patients\nshows that both mCardiacDx and PTL outperform state-of-the-art approach in\narrhythmia monitoring and diagnosis, also demonstrating improved performance in\nhealthy subjects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86mCardiacDx\u7cfb\u7edf\uff0c\u5229\u7528\u96f7\u8fbe\u6280\u672f\u65e0\u63a5\u89e6\u76d1\u6d4b\u5fc3\u5f8b\u5931\u5e38\uff0c\u901a\u8fc7PTL\u6280\u672f\u548c\u7f16\u7801-\u89e3\u7801\u6a21\u578b\u89e3\u51b3\u4fe1\u53f7\u7a7a\u95f4\u548c\u65f6\u5e8f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5fc3\u5f8b\u5931\u5e38\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e13\u4e1a\u533b\u7597\u8bbe\u5907\u548c\u63a5\u89e6\u5f0f\u8bbe\u5907\uff0c\u7528\u6237\u4f53\u9a8c\u5dee\uff1b\u73b0\u6709\u65e0\u63a5\u89e6\u76d1\u6d4b\u6280\u672f\u5bf9\u5fc3\u5f8b\u5931\u5e38\u60a3\u8005\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u96f7\u8fbe\u6280\u672f\uff0c\u7ed3\u5408\u7cbe\u786e\u76ee\u6807\u5b9a\u4f4d(PTL)\u548c\u7f16\u7801-\u89e3\u7801\u6a21\u578b\uff0c\u91cd\u6784\u5fc3\u810f\u8109\u51b2\u6ce2\u5f62\u3002", "result": "\u5728\u5927\u91cf\u5065\u5eb7\u4eba\u548c\u60a3\u8005\u6570\u636e\u4e2d\uff0cmCardiacDx\u548cPTL\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "mCardiacDx\u4e3a\u65e0\u63a5\u89e6\u5fc3\u5f8b\u5931\u5e38\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02470", "pdf": "https://arxiv.org/pdf/2508.02470", "abs": "https://arxiv.org/abs/2508.02470", "authors": ["Hyunjn An", "Yongwon Kim", "Wonduk Seo", "Joonil Park", "Daye Kang", "Changhoon Oh", "Dokyun Kim", "Seunghyun Lee"], "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.", "AI": {"tldr": "AIAP\u5e73\u53f0\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u7b80\u5316AI\u670d\u52a1\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u975e\u4e13\u5bb6\u7528\u6237\u5728\u8868\u8fbe\u610f\u56fe\u548c\u7ba1\u7406\u7cfb\u7edf\u590d\u6742\u6027\u65f6\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u5de5\u5177\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "AIAP\u91c7\u7528\u65e0\u4ee3\u7801\u5e73\u53f0\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u548c\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5c06\u6a21\u7cca\u6307\u4ee4\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u6b65\u9aa4\u3002", "result": "32\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u8868\u660e\uff0cAIAP\u7684\u529f\u80fd\uff08\u5982AI\u5efa\u8bae\u3001\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff09\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u76f4\u89c2\u5f00\u53d1\u670d\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u53ef\u89c6\u5316\u7f16\u7a0b\u80fd\u663e\u8457\u964d\u4f4eAI\u670d\u52a1\u8bbe\u8ba1\u7684\u95e8\u69db\uff0c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.02328", "pdf": "https://arxiv.org/pdf/2508.02328", "abs": "https://arxiv.org/abs/2508.02328", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits", "categories": ["cs.HC", "cs.CL", "cs.IR", "H.5.2; I.2.7; H.1.2"], "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables", "summary": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7528\u6237\u5bf9\u63a2\u7d22\u5f0f\u5bf9\u8bdd\u7684\u504f\u597d\u53d7\u6109\u60a6\u611f\u3001\u6709\u7528\u6027\u3001\u65b0\u9896\u6027\u548c\u5bf9\u8bdd\u8d28\u91cf\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u4e94\u79cd\u6f5c\u5728\u7528\u6237\u7c7b\u578b\u3002", "motivation": "\u63a2\u8ba8\u5f71\u54cd\u7528\u6237\u5728\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u4ea4\u4e92\u504f\u597d\u7684\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u5185\u90e8\u5206\u7ec4\u7814\u7a76\u8bbe\u8ba1\uff0c\u53c2\u4e0e\u8005\u63a5\u89e6\u4e24\u79cd\u811a\u672c\u5bf9\u8bdd\u5e76\u8bc4\u4ef7\u4f53\u9a8c\uff0c\u901a\u8fc7\u903b\u8f91\u56de\u5f52\u548c\u805a\u7c7b\u5206\u6790\u6570\u636e\u3002", "result": "\u63a2\u7d22\u504f\u597d\u7531\u6109\u60a6\u611f\u7b49\u56db\u4e2a\u56e0\u7d20\u9884\u6d4b\uff1b\u53d1\u73b0\u4e94\u79cd\u7528\u6237\u7c7b\u578b\uff1b\u5e74\u9f84\u3001\u6027\u522b\u548c\u504f\u597d\u63a7\u5236\u663e\u8457\u5f71\u54cd\u9009\u62e9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aCRS\u7528\u6237\u5efa\u6a21\u548c\u5bf9\u8bdd\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u60c5\u611f\u3001\u8ba4\u77e5\u53ca\u7279\u8d28\u5c42\u9762\u7684\u89c1\u89e3\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u9002\u5e94\u7528\u6237\u9700\u6c42\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002"}}
{"id": "2508.02609", "pdf": "https://arxiv.org/pdf/2508.02609", "abs": "https://arxiv.org/abs/2508.02609", "authors": ["Jiayin Jin", "Zhimeng Pan", "Yang Tang", "Jiarui Feng", "Kungang Li", "Chongyuan Xiang", "Jiacheng Li", "Runze Su", "Siping Ji", "Han Sun", "Ling Leng", "Prathibha Deshikachar"], "title": "Entity Representation Learning Through Onsite-Offsite Graph for Pinterset Ads", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Graph Neural Networks (GNN) have been extensively applied to industry\nrecommendation systems, as seen in models like GraphSage\\cite{GraphSage},\nTwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were\nconstructed based on users' activities on the platforms, and various graph\nmodels were developed to effectively learn node embeddings. In addition to\nusers' onsite activities, their offsite conversions are crucial for Ads models\nto capture their shopping interest. To better leverage offsite conversion data\nand explore the connection between onsite and offsite activities, we\nconstructed a large-scale heterogeneous graph based on users' onsite ad\ninteractions and opt-in offsite conversion activities. Furthermore, we\nintroduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph\nEmbedding (KGE) model, to more efficiently integrate graph embeddings into Ads\nranking models. However, our Ads ranking models initially struggled to directly\nincorporate Knowledge Graph Embeddings (KGE), and only modest gains were\nobserved during offline experiments. To address this challenge, we employed the\nLarge ID Embedding Table technique and innovated an attention based KGE\nfinetuning approach within the Ads ranking models. As a result, we observed a\nsignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)\nprediction models. Moreover, this framework has been deployed in Pinterest's\nAds Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC\nreduction. We believe the techniques presented in this paper can be leveraged\nby other large-scale industrial models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u73b0\u573a\u548c\u573a\u5916\u7528\u6237\u6d3b\u52a8\u7684\u5f02\u6784\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86TransRA\u6a21\u578b\u6765\u4f18\u5316\u5e7f\u544a\u6392\u540d\uff0c\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86CTR\u548cCVR\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528\u7528\u6237\u7684\u573a\u5916\u8f6c\u5316\u6570\u636e\u5e76\u63a2\u7d22\u73b0\u573a\u548c\u573a\u5916\u6d3b\u52a8\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u63d0\u5347\u5e7f\u544a\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u679c\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5f02\u6784\u56fe\uff0c\u4ecb\u7ecd\u4e86TransRA\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u5927ID\u5d4c\u5165\u8868\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684KGE\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5728CTR\u548cCVR\u9884\u6d4b\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684AUC\u63d0\u5347\uff0c\u5e76\u5728Pinterest\u7684\u5e7f\u544a\u6a21\u578b\u4e2d\u8d21\u732e\u4e862.69%\u7684CTR\u63d0\u5347\u548c1.34%\u7684CPC\u964d\u4f4e\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6280\u672f\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u5927\u89c4\u6a21\u5de5\u4e1a\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2508.02371", "pdf": "https://arxiv.org/pdf/2508.02371", "abs": "https://arxiv.org/abs/2508.02371", "authors": ["Matou\u0161 Jel\u00ednek", "Nadine Schlicker", "Ewart de Visser"], "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u516d\u6761\u8bbe\u8ba1\u51c6\u5219\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u8005\u4f18\u5316\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u4fe1\u4efb\u5ea6\u8bc4\u4f30\uff0c\u4ece\u800c\u4fc3\u8fdb\u9053\u5fb7\u548c\u8d1f\u8d23\u4efb\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u6821\u51c6\u7528\u6237\u5728\u81ea\u52a8\u5316\u7cfb\u7edf\u4e2d\u7684\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u5176\u5b89\u5168\u548c\u65e0\u7f1d\u878d\u5165\u793e\u4f1a\u3002\u7528\u6237\u5e94\u5728\u7cfb\u7edf\u5efa\u8bae\u6b63\u786e\u65f6\u4f9d\u8d56\uff0c\u9519\u8bef\u65f6\u62d2\u7edd\u3002", "method": "\u7ed3\u5408\u4eba\u673a\u4ea4\u4e92\u3001\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u81ea\u52a8\u5316\u7814\u7a76\u3001\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\u548c\u4f26\u7406\u5b66\u7b49\u591a\u9886\u57df\u6587\u732e\uff0c\u5e76\u5f15\u5165\u8bed\u7528\u5b66\u4e2d\u7684\u5171\u540c\u57fa\u7840\u548c\u683c\u8d56\u65af\u6c9f\u901a\u51c6\u5219\u3002", "result": "\u63d0\u51fa\u7684\u51c6\u5219\u4e3a\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4ee5\u521b\u5efa\u663e\u793a\u76f8\u5173\u4fe1\u4efb\u5ea6\u7ebf\u7d22\u7684\u7cfb\u7edf\uff0c\u4ece\u800c\u57f9\u517b\u6821\u51c6\u4fe1\u4efb\u3002", "conclusion": "\u8fd9\u4e9b\u51c6\u5219\u4e0d\u4ec5\u6709\u52a9\u4e8e\u8bbe\u8ba1\u65b0\u7cfb\u7edf\uff0c\u8fd8\u80fd\u8bc4\u4f30\u73b0\u6709\u7cfb\u7edf\u662f\u5426\u652f\u6301\u7528\u6237\u51c6\u786e\u5224\u65ad\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2508.02376", "pdf": "https://arxiv.org/pdf/2508.02376", "abs": "https://arxiv.org/abs/2508.02376", "authors": ["Matus Krajcovic", "Peter Demcak", "Eduard Kuric"], "title": "Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction", "categories": ["cs.HC", "H.5; I.2"], "comment": null, "summary": "Embodied conversational agents (ECAs) are increasingly more realistic and\ncapable of dynamic conversations. In online surveys, anthropomorphic agents\ncould help address issues like careless responding and satisficing, which\noriginate from the lack of personal engagement and perceived accountability.\nHowever, there is a lack of understanding of how ECAs in user experience\nresearch may affect participant engagement, satisfaction, and the quality of\nresponses. As a proof of concept, we propose an instrument that enables the\nincorporation of conversations with a virtual avatar into surveys, using on\nAI-driven video generation, speech recognition, and Large Language Models. In\nour between-subjects study, 80 participants (UK, stratified random sample of\ngeneral population) either talked to a voice-based agent with an animated video\navatar, or interacted with a chatbot. Across surveys based on two self-reported\npsychometric tests, 2,265 conversation responses were obtained. Statistical\ncomparison of results indicates that embodied agents can contribute\nsignificantly to more informative, detailed responses, as well as higher yet\nmore time-efficient engagement. Furthermore, qualitative analysis provides\nvaluable insights for causes of no significant change to satisfaction, linked\nto personal preferences, turn-taking delays and Uncanny Valley reactions. These\nfindings support the pursuit and development of new methods toward human-like\nagents for the transformation of online surveys into more natural interactions\nresembling in-person interviews.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5177\u8eab\u5bf9\u8bdd\u4ee3\u7406\uff08ECA\uff09\u5728\u5728\u7ebf\u8c03\u67e5\u4e2d\u5bf9\u53c2\u4e0e\u8005\u53c2\u4e0e\u5ea6\u3001\u6ee1\u610f\u5ea6\u548c\u56de\u7b54\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0ECA\u80fd\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u548c\u9ad8\u6548\u7684\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u8c03\u67e5\u4e2d\u56e0\u7f3a\u4e4f\u4eba\u9645\u4e92\u52a8\u548c\u8d23\u4efb\u611f\u5bfc\u81f4\u7684\u7c97\u5fc3\u56de\u7b54\u548c\u6ee1\u610f\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u968f\u673a\u62bd\u6837\u5bf980\u540d\u82f1\u56fd\u53c2\u4e0e\u8005\u8fdb\u884c\u4e24\u7ec4\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u4e00\u7ec4\u4e0e\u5177\u8eab\u4ee3\u7406\u4e92\u52a8\uff0c\u53e6\u4e00\u7ec4\u4e0e\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\uff0c\u6536\u96c6\u4e862265\u6761\u5bf9\u8bdd\u56de\u590d\u3002", "result": "\u5177\u8eab\u4ee3\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u56de\u7b54\u7684\u8be6\u7ec6\u5ea6\u548c\u53c2\u4e0e\u6548\u7387\uff0c\u4f46\u5bf9\u6ee1\u610f\u5ea6\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u539f\u56e0\u5305\u62ec\u4e2a\u4eba\u504f\u597d\u548c\u2018\u6050\u6016\u8c37\u2019\u6548\u5e94\u3002", "conclusion": "\u652f\u6301\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u9645\u4e92\u52a8\u7684ECA\uff0c\u4ee5\u63d0\u5347\u5728\u7ebf\u8c03\u67e5\u7684\u81ea\u7136\u6027\u548c\u6548\u679c\u3002"}}
{"id": "2508.02413", "pdf": "https://arxiv.org/pdf/2508.02413", "abs": "https://arxiv.org/abs/2508.02413", "authors": ["Antrea Christou", "Cogan Shimizu"], "title": "Improving Knowledge Graph Understanding with Contextual Views", "categories": ["cs.HC"], "comment": "12 pages", "summary": "Navigating, visualizing, and discovery in graph data is frequently a\ndifficult prospect. This is especially true for knowledge graphs (KGs), due to\nhigh number of possible labeled connections to other data.\n  However, KGs are frequently equipped with an ontology as a schema. That is,\nit informs how the relationships between data may be constrained. This\nadditional information can be leveraged to improve how (knowledge) graph data\ncan be navigated, visualized, or otherwise utilized in a discovery process.\n  In this manuscript, we introduce the Interactive Knowledge (InK) Browser.\nThis tool specifically takes advantage ontological information (i.e.,\nknowledge) when found in KGs. Specifically, we use modular views that provide\nvarious perspectives over the graph, including an interactive schema view, data\nlistings based on type, neighborhood connections, and geospatial depiction\n(where appropriate). For this manuscript, we have evaluated the basic premise\nof this tool over a user group ($n= With this grown user survey, we continue to\nevaluate how scalable tools, including flexible views, can make KG exploration\neasier for a range of applications.)", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86Interactive Knowledge (InK) Browser\u5de5\u5177\uff0c\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u672c\u4f53\u4fe1\u606f\u6539\u8fdb\u56fe\u7684\u5bfc\u822a\u3001\u53ef\u89c6\u5316\u548c\u53d1\u73b0\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u8c03\u67e5\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u7531\u4e8e\u5176\u9ad8\u5ea6\u590d\u6742\u7684\u8fde\u63a5\u5173\u7cfb\uff0c\u4f7f\u5f97\u5bfc\u822a\u3001\u53ef\u89c6\u5316\u548c\u53d1\u73b0\u8fc7\u7a0b\u53d8\u5f97\u56f0\u96be\u3002\u672c\u4f53\u4fe1\u606f\u53ef\u4ee5\u63d0\u4f9b\u7ea6\u675f\u5173\u7cfb\uff0c\u4ece\u800c\u6539\u8fdb\u8fd9\u4e9b\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165Interactive Knowledge (InK) Browser\u5de5\u5177\uff0c\u5229\u7528\u672c\u4f53\u4fe1\u606f\u63d0\u4f9b\u6a21\u5757\u5316\u89c6\u56fe\uff0c\u5305\u62ec\u4ea4\u4e92\u5f0f\u6a21\u5f0f\u89c6\u56fe\u3001\u57fa\u4e8e\u7c7b\u578b\u7684\u6570\u636e\u5217\u8868\u3001\u90bb\u57df\u8fde\u63a5\u548c\u5730\u7406\u7a7a\u95f4\u5c55\u793a\u3002", "result": "\u901a\u8fc7\u7528\u6237\u8c03\u67e5\u8bc4\u4f30\u5de5\u5177\u7684\u57fa\u672c\u524d\u63d0\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u7684\u63a2\u7d22\u8fc7\u7a0b\u3002", "conclusion": "\u7075\u6d3b\u89c6\u56fe\u7684\u5de5\u5177\uff08\u5982InK Browser\uff09\u53ef\u4ee5\u7b80\u5316\u77e5\u8bc6\u56fe\u8c31\u7684\u63a2\u7d22\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.02550", "pdf": "https://arxiv.org/pdf/2508.02550", "abs": "https://arxiv.org/abs/2508.02550", "authors": ["Kristin M. Kostick-Quenet", "Meghan E. Hurley", "Syed Ayaz", "John Herrington", "Casey Zampella", "Julia Parish-Morris", "Birkan Tun\u00e7", "Gabriel L\u00e1zaro-Mu\u00f1oz", "J. S. Blumenthal-Barby", "Eric A. Storch"], "title": "Stakeholder Perspectives on Humanistic Implementation of Computer Perception in Healthcare: A Qualitative Study", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "65 pages", "summary": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u8ba1\u7b97\u673a\u611f\u77e5\u6280\u672f\u5728\u533b\u7597\u4e2d\u7684\u5e94\u7528,\u63a2\u8ba8\u4e86\u5229\u76ca\u76f8\u5173\u8005\u7684\u4e03\u5927\u5173\u6ce8\u9886\u57df,\u5e76\u63d0\u51fa\u201c\u4e2a\u6027\u5316\u8def\u7ebf\u56fe\u201d\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u8ba1\u7b97\u673a\u611f\u77e5\u6280\u672f\u4e3a\u4e2a\u6027\u5316\u533b\u7597\u63d0\u4f9b\u4e86\u673a\u4f1a,\u4f46\u5176\u5e26\u6765\u7684\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\u5c1a\u672a\u88ab\u5168\u9762\u7406\u89e3,\u4e9f\u9700\u5229\u76ca\u76f8\u5173\u8005\u7684\u89c6\u89d2\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7102\u4f4d\u5229\u76ca\u76f8\u5173\u8005\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08,\u91c7\u7528\u591a\u5b66\u79d1\u56e2\u961f\u7684\u4e3b\u9898\u5206\u6790,\u5e76\u9a8c\u8bc1\u4e86\u53ef\u9760\u6027\u3002", "result": "\u786e\u5b9a\u4e86\u4e03\u5927\u5173\u952e\u5173\u6ce8\u9886\u57df,\u5e76\u63d0\u51fa\u201c\u4e2a\u6027\u5316\u8def\u7ebf\u56fe\u201d\u6846\u67b6,\u4ee5\u5e73\u8861\u6280\u672f\u5e94\u7528\u4e0e\u4eba\u6587\u5173\u6000\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u3001\u4e34\u5e8a\u533b\u751f\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6,\u4ee5\u5728\u5229\u7528\u6280\u672f\u7684\u540c\u65f6\u4fdd\u62a4\u533b\u7597\u7684\u4eba\u6587\u6838\u5fc3\u3002"}}
{"id": "2508.02592", "pdf": "https://arxiv.org/pdf/2508.02592", "abs": "https://arxiv.org/abs/2508.02592", "authors": ["Andrew McNutt", "Shiyi He", "Sujit Kumar Kamaraj", "Purbid Bambroo", "Nastaran Jadidi", "John Bovard", "Chang Han"], "title": "Teaching Critical Visualization: A Field Report", "categories": ["cs.HC"], "comment": "Accepted to EduVIS25", "summary": "Critical Visualization is gaining popularity and academic focus, yet\nrelatively few academic courses have been offered to support students in this\ncomplex area. This experience report describes a recent experimental course on\nthe topic, exploring both what the topic could be as well as an experimental\ncontent structure (namely as scavenger hunt). Generally the course was\nsuccessful, achieving the learning objectives of developing critical thinking\nskills, improving communication about complex ideas, and developing a knowledge\nabout theories in the area. While improvements can be made, we hope that\nhumanistic notions of criticality are embraced more deeply in visualization\npedagogy.", "AI": {"tldr": "\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u95e8\u5173\u4e8e\u6279\u5224\u6027\u53ef\u89c6\u5316\u7684\u5b9e\u9a8c\u8bfe\u7a0b\uff0c\u63a2\u7d22\u4e86\u8bfe\u7a0b\u5185\u5bb9\u7ed3\u6784\u548c\u6559\u5b66\u65b9\u6cd5\u3002\u8bfe\u7a0b\u6210\u529f\u57f9\u517b\u4e86\u5b66\u751f\u7684\u6279\u5224\u6027\u601d\u7ef4\u3001\u590d\u6742\u4ea4\u6d41\u80fd\u529b\u53ca\u76f8\u5173\u7406\u8bba\u77e5\u8bc6\u3002", "motivation": "\u6279\u5224\u6027\u53ef\u89c6\u5316\u5728\u5b66\u672f\u754c\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u76f8\u5173\u8bfe\u7a0b\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u9a8c\u8bfe\u7a0b\u63a2\u7d22\u8fd9\u4e00\u9886\u57df\u7684\u6559\u5b66\u65b9\u6cd5\u548c\u5185\u5bb9\u7ed3\u6784\u3002", "method": "\u8bfe\u7a0b\u91c7\u7528\u63a2\u7d22\u6027\u7ed3\u6784\uff08\u5982\u5bfb\u5b9d\u6e38\u620f\u5f62\u5f0f\uff09\uff0c\u5e2e\u52a9\u5b66\u751f\u53d1\u5c55\u6279\u5224\u6027\u601d\u7ef4\u3001\u590d\u6742\u6c9f\u901a\u80fd\u529b\u53ca\u76f8\u5173\u7406\u8bba\u77e5\u8bc6\u3002", "result": "\u8bfe\u7a0b\u6210\u529f\u5b9e\u73b0\u4e86\u5b66\u4e60\u76ee\u6807\uff0c\u57f9\u517b\u4e86\u5b66\u751f\u7684\u6279\u5224\u6027\u601d\u7ef4\u548c\u6c9f\u901a\u80fd\u529b\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u4f5c\u8005\u5e0c\u671b\u6279\u5224\u6027\u53ef\u89c6\u5316\u6559\u5b66\u80fd\u591f\u66f4\u6df1\u5165\u5730\u878d\u5165\u4eba\u6587\u4e3b\u4e49\u7684\u6279\u5224\u6027\u7406\u5ff5\u3002"}}
{"id": "2508.02593", "pdf": "https://arxiv.org/pdf/2508.02593", "abs": "https://arxiv.org/abs/2508.02593", "authors": ["Catalina Gomez", "Lalithkumar Seenivasan", "Xinrui Zou", "Jeewoo Yoon", "Sirui Chu", "Ariel Leong", "Patrick Kramer", "Yu-Chun Ku", "Jose L. Porras", "Alejandro Martin-Gomez", "Masaru Ishii", "Mathias Unberath"], "title": "Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition", "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Traditional surgical skill acquisition relies heavily on expert feedback, yet\ndirect access is limited by faculty availability and variability in subjective\nassessments. While trainees can practice independently, the lack of\npersonalized, objective, and quantitative feedback reduces the effectiveness of\nself-directed learning. Recent advances in computer vision and machine learning\nhave enabled automated surgical skill assessment, demonstrating the feasibility\nof automatic competency evaluation. However, it is unclear whether such\nArtificial Intelligence (AI)-driven feedback can contribute to skill\nacquisition. Here, we examine the effectiveness of explainable AI\n(XAI)-generated feedback in surgical training through a human-AI study. We\ncreate a simulation-based training framework that utilizes XAI to analyze\nvideos and extract surgical skill proxies related to primitive actions. Our\nintervention provides automated, user-specific feedback by comparing trainee\nperformance to expert benchmarks and highlighting deviations from optimal\nexecution through understandable proxies for actionable guidance. In a\nprospective user study with medical students, we compare the impact of\nXAI-guided feedback against traditional video-based coaching on task outcomes,\ncognitive load, and trainees' perceptions of AI-assisted learning. Results\nshowed improved cognitive load and confidence post-intervention. While no\ndifferences emerged between the two feedback types in reducing performance gaps\nor practice adjustments, trends in the XAI group revealed desirable effects\nwhere participants more closely mimicked expert practice. This work encourages\nthe study of explainable AI in surgical education and the development of\ndata-driven, adaptive feedback mechanisms that could transform learning\nexperiences and competency assessment.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u53ef\u89e3\u91caAI\uff08XAI\uff09\u751f\u6210\u53cd\u9988\u5728\u5916\u79d1\u57f9\u8bad\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793aXAI\u53cd\u9988\u80fd\u6539\u5584\u8ba4\u77e5\u8d1f\u8377\u548c\u4fe1\u5fc3\uff0c\u4f46\u4e0e\u4f20\u7edf\u89c6\u9891\u53cd\u9988\u5728\u7ee9\u6548\u5dee\u8ddd\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u4f20\u7edf\u5916\u79d1\u6280\u80fd\u57f9\u8bad\u4f9d\u8d56\u4e13\u5bb6\u53cd\u9988\uff0c\u4f46\u53d7\u9650\u4e8e\u4e3b\u89c2\u6027\u548c\u8d44\u6e90\u4e0d\u8db3\u3002AI\u9a71\u52a8\u7684\u53cd\u9988\u53ef\u80fd\u5f25\u8865\u8fd9\u4e00\u7f3a\u53e3\u3002", "method": "\u901a\u8fc7XAI\u5206\u6790\u624b\u672f\u89c6\u9891\u63d0\u53d6\u6280\u80fd\u6307\u6807\uff0c\u5e76\u4e0e\u4e13\u5bb6\u57fa\u51c6\u6bd4\u8f83\u751f\u6210\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u5728\u533b\u5b66\u5b66\u751f\u4e2d\u5f00\u5c55\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "XAI\u53cd\u9988\u6539\u5584\u4e86\u8ba4\u77e5\u8d1f\u8377\u548c\u4fe1\u5fc3\uff0c\u4f46\u5728\u7ee9\u6548\u5dee\u8ddd\u548c\u7ec3\u4e60\u8c03\u6574\u4e0a\u4e0e\u4f20\u7edf\u53cd\u9988\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u652f\u6301XAI\u5728\u5916\u79d1\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u8bae\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u53cd\u9988\u673a\u5236\u3002"}}
{"id": "2508.02610", "pdf": "https://arxiv.org/pdf/2508.02610", "abs": "https://arxiv.org/abs/2508.02610", "authors": ["Sanchita S. Kamath", "Omar Khan", "Anurag Choudhary", "Jan Meyerhoff-Liang", "Soyoung Choi", "JooYoung Seo"], "title": "PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players", "categories": ["cs.HC"], "comment": null, "summary": "Blind and low-vision (BLV) individuals experience lower levels of physical\nactivity (PA) compared to sighted peers due to a lack of accessible, engaging\nexercise options. Existing solutions often rely on auditory cues but do not\nfully integrate rich sensory feedback or support spatial navigation, limiting\ntheir effectiveness. This study introduces PunchPulse, a virtual reality (VR)\nboxing exergame designed to motivate BLV users to reach and sustain moderate to\nvigorous physical activity (MVPA) levels. Over a seven-month, multi-phased\nstudy, PunchPulse was iteratively refined with three BLV co-designers, informed\nby two early pilot testers, and evaluated by six additional BLV user-study\nparticipants. Data collection included both qualitative (researcher\nobservations, SOPI) and quantitative (MVPA zones, aid usage, completion times)\nmeasures of physical exertion and gameplay performance. The user study revealed\nthat all participants reached moderate MVPA thresholds, with high levels of\nimmersion and engagement observed. This work demonstrates the potential of VR\nas an inclusive medium for promoting meaningful PA in the BLV community and\naddresses a critical gap in accessible, intensity-driven exercise\ninterventions.", "AI": {"tldr": "PunchPulse\u662f\u4e00\u6b3e\u9488\u5bf9\u89c6\u969c\u4eba\u58eb\u8bbe\u8ba1\u7684VR\u62f3\u51fb\u6e38\u620f\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u611f\u5b98\u53cd\u9988\u548c\u7a7a\u95f4\u5bfc\u822a\u652f\u6301\uff0c\u5e2e\u52a9\u4ed6\u4eec\u8fbe\u5230\u4e2d\u9ad8\u5f3a\u5ea6\u8fd0\u52a8\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u65b9\u6848\u5bf9\u89c6\u969c\u4eba\u58eb\u4e0d\u53cb\u597d\uff0c\u7f3a\u4e4f\u591a\u611f\u5b98\u53cd\u9988\u548c\u7a7a\u95f4\u5bfc\u822a\u652f\u6301\uff0c\u5bfc\u81f4\u53c2\u4e0e\u5ea6\u4f4e\u3002", "method": "\u901a\u8fc7\u4e03\u4e2a\u6708\u7684\u8fed\u4ee3\u8bbe\u8ba1\uff0c\u4e0e3\u4f4d\u89c6\u969c\u5408\u4f5c\u8005\u5171\u540c\u5f00\u53d1\uff0c\u5e76\u7ecf\u8fc76\u4f4d\u7528\u6237\u6d4b\u8bd5\uff0c\u91c7\u7528\u5b9a\u6027\u548c\u5b9a\u91cf\u65b9\u6cd5\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u6240\u6709\u53c2\u4e0e\u8005\u5747\u8fbe\u5230\u4e2d\u9ad8\u5f3a\u5ea6\u8fd0\u52a8\u6c34\u5e73\uff0c\u6e38\u620f\u8868\u73b0\u51fa\u9ad8\u6c89\u6d78\u611f\u548c\u53c2\u4e0e\u5ea6\u3002", "conclusion": "VR\u4f5c\u4e3a\u5305\u5bb9\u6027\u5a92\u4ecb\uff0c\u6709\u671b\u586b\u8865\u89c6\u969c\u4eba\u58eb\u4e2d\u9ad8\u5f3a\u5ea6\u8fd0\u52a8\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.02639", "pdf": "https://arxiv.org/pdf/2508.02639", "abs": "https://arxiv.org/abs/2508.02639", "authors": ["Tingying He", "Jason Dykes", "Petra Isenberg", "Tobias Isenberg"], "title": "Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable", "categories": ["cs.HC"], "comment": null, "summary": "We present a new comprehensive theory for explaining, exploring, and using\npattern as a visual variable in visualization. Although patterns have long been\nused for data encoding and continue to be valuable today, their conceptual\nfoundations are precarious: the concepts and terminology used across the\nresearch literature and in practice are inconsistent, making it challenging to\nuse patterns effectively and to conduct research to inform their use. To\naddress this problem, we conduct a comprehensive cross-disciplinary literature\nreview that clarifies ambiguities around the use of \"pattern\" and \"texture\". As\na result, we offer a new consistent treatment of pattern as a composite visual\nvariable composed of structured groups of graphic primitives that can serve as\nmarks for encoding data individually and collectively. This new and widely\napplicable formulation opens a sizable design space for the visual variable\npattern, which we formalize as a new system comprising three sets of variables:\nthe spatial arrangement of primitives, the appearance relationships among\nprimitives, and the retinal visual variables that characterize individual\nprimitives. We show how our pattern system relates to existing visualization\ntheory and highlight opportunities for visualization design. We further explore\npatterns based on complex spatial arrangements, demonstrating explanatory power\nand connecting our conceptualization to broader theory on maps and cartography.\nAn author version and additional materials are available on OSF: osf.io/z7ae2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7efc\u5408\u7406\u8bba\uff0c\u7528\u4e8e\u89e3\u91ca\u3001\u63a2\u7d22\u548c\u4f7f\u7528\u6a21\u5f0f\u4f5c\u4e3a\u53ef\u89c6\u5316\u4e2d\u7684\u89c6\u89c9\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u5ff5\u548c\u672f\u8bed\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u6a21\u5f0f\u957f\u671f\u4ee5\u6765\u88ab\u7528\u4e8e\u6570\u636e\u7f16\u7801\uff0c\u4f46\u73b0\u6709\u6982\u5ff5\u548c\u672f\u8bed\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u4e86\u5176\u6709\u6548\u4f7f\u7528\u548c\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u6587\u732e\u7efc\u8ff0\uff0c\u6f84\u6e05\u4e86\u201c\u6a21\u5f0f\u201d\u548c\u201c\u7eb9\u7406\u201d\u7684\u6a21\u7cca\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u5c06\u6a21\u5f0f\u4f5c\u4e3a\u590d\u5408\u89c6\u89c9\u53d8\u91cf\u7684\u65b0\u5904\u7406\u65b9\u5f0f\u3002", "result": "\u5f62\u5f0f\u5316\u4e86\u4e00\u4e2a\u65b0\u7684\u6a21\u5f0f\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u7ec4\u53d8\u91cf\uff1a\u57fa\u5143\u7684\u7a7a\u95f4\u6392\u5217\u3001\u5916\u89c2\u5173\u7cfb\u53ca\u4e2a\u4f53\u57fa\u5143\u7684\u89c6\u89c9\u53d8\u91cf\u3002", "conclusion": "\u65b0\u7406\u8bba\u6269\u5c55\u4e86\u6a21\u5f0f\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u4e0e\u73b0\u6709\u53ef\u89c6\u5316\u7406\u8bba\u53ca\u5730\u56fe\u5b66\u5efa\u7acb\u4e86\u8054\u7cfb\u3002"}}
{"id": "1907.00326", "pdf": "https://arxiv.org/pdf/1907.00326", "abs": "https://arxiv.org/abs/1907.00326", "authors": ["Jie Cao", "Michael Tanana", "Zac E. Imel", "Eric Poitras", "David C. Atkins", "Vivek Srikumar"], "title": "Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2019", "summary": "Automatically analyzing dialogue can help understand and guide behavior in\ndomains such as counseling, where interactions are largely mediated by\nconversation. In this paper, we study modeling behavioral codes used to asses a\npsychotherapy treatment style called Motivational Interviewing (MI), which is\neffective for addressing substance abuse and related problems. Specifically, we\naddress the problem of providing real-time guidance to therapists with a\ndialogue observer that (1) categorizes therapist and client MI behavioral codes\nand, (2) forecasts codes for upcoming utterances to help guide the conversation\nand potentially alert the therapist. For both tasks, we define neural network\nmodels that build upon recent successes in dialogue modeling. Our experiments\ndemonstrate that our models can outperform several baselines for both tasks. We\nalso report the results of a careful analysis that reveals the impact of the\nvarious network design tradeoffs for modeling therapy dialogue.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5bf9\u8bdd\u5206\u6790\u5b9e\u65f6\u6307\u5bfc\u5fc3\u7406\u6cbb\u7597\u5e08\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5206\u7c7b\u548c\u9884\u6d4b\u6cbb\u7597\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u4ee3\u7801\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u81ea\u52a8\u5206\u6790\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\uff0c\u5e2e\u52a9\u6307\u5bfc\u548c\u6539\u8fdb\u6cbb\u7597\u5e08\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u52a8\u673a\u8bbf\u8c08\uff08MI\uff09\u8fd9\u79cd\u6709\u6548\u6cbb\u7597\u7269\u8d28\u6ee5\u7528\u7b49\u95ee\u9898\u7684\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6784\u5efa\u5bf9\u8bdd\u89c2\u5bdf\u5668\uff0c\u5206\u7c7b\u548c\u9884\u6d4b\u6cbb\u7597\u5e08\u4e0e\u60a3\u8005\u7684MI\u884c\u4e3a\u4ee3\u7801\uff0c\u5e76\u5bf9\u6a21\u578b\u8bbe\u8ba1\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u7f51\u7edc\u8bbe\u8ba1\u5bf9\u6cbb\u7597\u5bf9\u8bdd\u5efa\u6a21\u7684\u5177\u4f53\u5f71\u54cd\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u652f\u6301\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u7684\u5b9e\u65f6\u5206\u6790\u548c\u6307\u5bfc\uff0c\u4e3a\u6cbb\u7597\u5e08\u63d0\u4f9b\u6709\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2412.13395", "pdf": "https://arxiv.org/pdf/2412.13395", "abs": "https://arxiv.org/abs/2412.13395", "authors": ["Jie Cao", "Abhijit Suresh", "Jennifer Jacobs", "Charis Clevenger", "Amanda Howard", "Chelsea Brown", "Brent Milne", "Tom Fischaber", "Tamara Sumner", "James H. Martin"], "title": "Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to COLING'2025", "summary": "Human tutoring interventions play a crucial role in supporting student\nlearning, improving academic performance, and promoting personal growth. This\npaper focuses on analyzing mathematics tutoring discourse using talk moves - a\nframework of dialogue acts grounded in Accountable Talk theory. However,\nscaling the collection, annotation, and analysis of extensive tutoring\ndialogues to develop machine learning models is a challenging and\nresource-intensive task. To address this, we present SAGA22, a compact dataset,\nand explore various modeling strategies, including dialogue context, speaker\ninformation, pretraining datasets, and further fine-tuning. By leveraging\nexisting datasets and models designed for classroom teaching, our results\ndemonstrate that supplementary pretraining on classroom data enhances model\nperformance in tutoring settings, particularly when incorporating longer\ncontext and speaker information. Additionally, we conduct extensive ablation\nstudies to underscore the challenges in talk move modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SAGA22\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4e86\u5728\u6570\u5b66\u8f85\u5bfc\u5bf9\u8bdd\u4e2d\u5e94\u7528\u2018\u8c08\u8bdd\u52a8\u4f5c\u2019\u6846\u67b6\u7684\u6a21\u578b\u5efa\u6a21\u7b56\u7565\uff0c\u7ed3\u679c\u8868\u660e\u8bfe\u5802\u6570\u636e\u7684\u9884\u8bad\u7ec3\u80fd\u63d0\u5347\u8f85\u5bfc\u573a\u666f\u7684\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u8f85\u5bfc\u5e72\u9884\u5bf9\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u8f85\u5bfc\u5bf9\u8bdd\u7684\u6536\u96c6\u548c\u5206\u6790\u662f\u8d44\u6e90\u5bc6\u96c6\u578b\u4efb\u52a1\u3002", "method": "\u4f7f\u7528SAGA22\u6570\u636e\u96c6\uff0c\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u3001\u8bf4\u8bdd\u8005\u4fe1\u606f\u3001\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5fae\u8c03\u7b49\u5efa\u6a21\u7b56\u7565\u3002", "result": "\u8bfe\u5802\u6570\u636e\u7684\u9884\u8bad\u7ec3\u5c24\u5176\u7ed3\u5408\u957f\u4e0a\u4e0b\u6587\u548c\u8bf4\u8bdd\u8005\u4fe1\u606f\u65f6\uff0c\u53ef\u63d0\u5347\u8f85\u5bfc\u573a\u666f\u7684\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u51f8\u663e\u4e86\u8c08\u8bdd\u52a8\u4f5c\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00859", "pdf": "https://arxiv.org/pdf/2508.00859", "abs": "https://arxiv.org/abs/2508.00859", "authors": ["Martin J. O'Connor", "Marcos Martinez-Romero", "Attila L. Egyedi", "Mete U. Akdogan", "Michael V. Dorf", "Mark A. Musen"], "title": "Author Once, Publish Everywhere: Portable Metadata Authoring with the CEDAR Embeddable Editor", "categories": ["cs.DL", "cs.HC", "68N19 (Primary) 68M11 (Secondary)", "D.2.2; D.2.11; D.2.13; H.3.5; H.5.2"], "comment": null, "summary": "High-quality, \"rich\" metadata are essential for making research data\nfindable, interoperable, and reusable. The Center for Expanded Data Annotation\nand Retrieval (CEDAR) has long addressed this need by providing tools to design\nmachine-actionable metadata templates that encode community standards in a\ncomputable form. To make these capabilities more accessible within real-world\nresearch workflows, we have developed the CEDAR Embeddable Editor (CEE)-a\nlightweight, interoperable Web Component that brings structured,\nstandards-based metadata authoring directly into third-party platforms. The CEE\ndynamically renders metadata forms from machine-actionable templates and\nproduces semantically rich metadata in JSON-LD format. It supports\nontology-based value selection via the BioPortal ontology repository, and it\nincludes external authority resolution for persistent identifiers such as\nORCIDs for individuals and RORs for research organizations. Crucially, the CEE\nrequires no custom user-interface development, allowing deployment across\ndiverse platforms. The CEE has been successfully integrated into generalist\nscientific data repositories such as Dryad and the Open Science Framework,\ndemonstrating its ability to support discipline-specific metadata creation. By\nsupporting the embedding of metadata authoring within existing research\nenvironments, the CEE can facilitate the adoption of community standards and\nhelp improve metadata quality across scientific disciplines.", "AI": {"tldr": "CEDAR Embeddable Editor (CEE) \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7 Web \u7ec4\u4ef6\uff0c\u53ef\u5d4c\u5165\u7b2c\u4e09\u65b9\u5e73\u53f0\uff0c\u76f4\u63a5\u751f\u6210\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u6807\u51c6\u7684\u5143\u6570\u636e\uff0c\u63d0\u5347\u5143\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u9ad8\u8d28\u91cf\u3001\u4e30\u5bcc\u7684\u5143\u6570\u636e\u5bf9\u7814\u7a76\u6570\u636e\u7684\u53ef\u67e5\u627e\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u91cd\u7528\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "CEDAR \u5f00\u53d1\u4e86 CEE\uff0c\u652f\u6301\u52a8\u6001\u6e32\u67d3\u5143\u6570\u636e\u8868\u5355\uff0c\u5e76\u901a\u8fc7 BioPortal \u672c\u4f53\u5e93\u548c\u5916\u90e8\u6743\u5a01\u89e3\u6790\uff08\u5982 ORCID \u548c ROR\uff09\u5b9e\u73b0\u57fa\u4e8e\u672c\u4f53\u7684\u503c\u9009\u62e9\u3002", "result": "CEE \u5df2\u6210\u529f\u96c6\u6210\u5230 Dryad \u548c Open Science Framework \u7b49\u5e73\u53f0\uff0c\u652f\u6301\u5b66\u79d1\u7279\u5b9a\u5143\u6570\u636e\u521b\u5efa\u3002", "conclusion": "CEE \u901a\u8fc7\u5728\u73b0\u6709\u7814\u7a76\u73af\u5883\u4e2d\u5d4c\u5165\u5143\u6570\u636e\u521b\u4f5c\uff0c\u4fc3\u8fdb\u793e\u533a\u6807\u51c6\u91c7\u7528\uff0c\u63d0\u9ad8\u8de8\u5b66\u79d1\u5143\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2508.00899", "pdf": "https://arxiv.org/pdf/2508.00899", "abs": "https://arxiv.org/abs/2508.00899", "authors": ["Abeer Dyoub", "Ivan Letteri", "Francesca A. Lisi"], "title": "ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "The emergence of Symbiotic AI (SAI) introduces new challenges to ethical\ndecision-making as it deepens human-AI collaboration. As symbiosis grows, AI\nsystems pose greater ethical risks, including harm to human rights and trust.\nEthical Risk Assessment (ERA) thus becomes crucial for guiding decisions that\nminimize such risks. However, ERA is hindered by uncertainty, vagueness, and\nincomplete information, and morality itself is context-dependent and imprecise.\nThis motivates the need for a flexible, transparent, yet robust framework for\nERA. Our work supports ethical decision-making by quantitatively assessing and\nprioritizing multiple ethical risks so that artificial agents can select\nactions aligned with human values and acceptable risk levels. We introduce\nff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic\nHierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks\nvia an Ethical Risk Score (ERS) for each risk type. The final ERS combines the\nFAHP-derived weight, propagated CF, and risk level. The framework offers a\nrobust mathematical approach for collaborative ERA modeling and systematic,\nstep-by-step analysis. A case study confirms that ff4ERA yields\ncontext-sensitive, ethically meaningful risk scores reflecting both expert\ninput and sensor-based evidence. Risk scores vary consistently with relevant\nfactors while remaining robust to unrelated inputs. Local sensitivity analysis\nshows predictable, mostly monotonic behavior across perturbations, and global\nSobol analysis highlights the dominant influence of expert-defined weights and\ncertainty factors, validating the model design. Overall, the results\ndemonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware\nethical assessments, enabling what-if analyses and guiding designers in\ncalibrating membership functions and expert judgments for reliable ethical\ndecision support.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aff4ERA\u7684\u6a21\u7cca\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u4f26\u7406\u98ce\u9669\uff0c\u7ed3\u5408\u4e86\u6a21\u7cca\u903b\u8f91\u3001\u6a21\u7cca\u5c42\u6b21\u5206\u6790\u6cd5\u548c\u786e\u5b9a\u6027\u56e0\u7d20\uff0c\u4ee5\u652f\u6301\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bfc\u5411\u7684\u51b3\u7b56\u3002", "motivation": "\u968f\u7740\u5171\u751fAI\u7684\u53d1\u5c55\uff0c\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\uff08ERA\uff09\u9762\u4e34\u4e0d\u786e\u5b9a\u6027\u3001\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u4e0d\u5b8c\u6574\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u7075\u6d3b\u4e14\u900f\u660e\u7684\u6846\u67b6\u6765\u91cf\u5316\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86ff4ERA\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u3001FAHP\u548c\u786e\u5b9a\u6027\u56e0\u7d20\uff0c\u901a\u8fc7\u4f26\u7406\u98ce\u9669\u8bc4\u5206\uff08ERS\uff09\u91cf\u5316\u98ce\u9669\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cff4ERA\u80fd\u751f\u6210\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4f26\u7406\u98ce\u9669\u8bc4\u5206\uff0c\u5bf9\u4e13\u5bb6\u8f93\u5165\u548c\u4f20\u611f\u5668\u6570\u636e\u654f\u611f\u4e14\u7a33\u5065\u3002", "conclusion": "ff4ERA\u4e3a\u4f26\u7406\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u8ffd\u8e2a\u7684\u98ce\u9669\u8bc4\u4f30\uff0c\u652f\u6301\u5047\u8bbe\u5206\u6790\u548c\u8bbe\u8ba1\u6821\u51c6\u3002"}}
{"id": "2508.00928", "pdf": "https://arxiv.org/pdf/2508.00928", "abs": "https://arxiv.org/abs/2508.00928", "authors": ["Chrysovalanto Messiou", "Riender Happee", "Georgios Papaioannou"], "title": "Modeling Head-Neck Dynamics under Lateral Perturbations Using MPC to Mimic CNS postural stabilization strategy", "categories": ["eess.SY", "cs.HC", "cs.SY"], "comment": null, "summary": "Automated vehicles will allow occupants to engage in non-driving tasks, but\nlimited visual cues will make them vulnerable to unexpected movements. These\nunpredictable perturbations create a \"surprise factor,\" forcing the central\nnervous system to rely on compensatory postural adjustments, which are less\neffective, and are more likely to trigger sensory conflicts. Since the head is\na key reference for sensory input (vestibular and vision), models accurately\ncapturing head-neck postural stabilization are essential for assessing AV\ncomfort. This study extends an existing model predictive control-based\nframework to simulate head-neck postural control under lateral perturbations.\nExperimental validation against human data demonstrates that the model can\naccurately reproduce dynamic responses during lateral trunk perturbations. The\nresults show that muscle effort combined with partial somatosensory feedback\nprovides the best overall dynamic fit without requiring corrective relative and\nglobal head orientation integrators for posture.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u52a8\u8f66\u8f86\u4e2d\u7a81\u53d1\u4fa7\u5411\u6270\u52a8\u5bf9\u4e58\u5458\u5934\u90e8-\u9888\u90e8\u59ff\u52bf\u63a7\u5236\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4eba\u4f53\u6570\u636e\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u4e58\u5458\u53ef\u80fd\u906d\u9047\u610f\u5916\u6270\u52a8\uff0c\u7f3a\u4e4f\u89c6\u89c9\u63d0\u793a\u4f1a\u5f71\u54cd\u59ff\u52bf\u8c03\u6574\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u51c6\u786e\u7684\u5934\u90e8-\u9888\u90e8\u59ff\u52bf\u63a7\u5236\u6a21\u578b\u4ee5\u8bc4\u4f30\u8212\u9002\u6027\u3002", "method": "\u6269\u5c55\u4e86\u73b0\u6709\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u6a21\u62df\u4fa7\u5411\u6270\u52a8\u4e0b\u7684\u5934\u90e8-\u9888\u90e8\u59ff\u52bf\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u6a21\u578b\u6709\u6548\u6027\u3002", "result": "\u6a21\u578b\u80fd\u51c6\u786e\u6a21\u62df\u4fa7\u5411\u6270\u52a8\u4e0b\u7684\u52a8\u6001\u54cd\u5e94\uff0c\u663e\u793a\u808c\u8089\u52aa\u529b\u548c\u90e8\u5206\u4f53\u611f\u53cd\u9988\u63d0\u4f9b\u4e86\u6700\u4f73\u52a8\u6001\u62df\u5408\uff0c\u65e0\u9700\u989d\u5916\u7684\u5934\u90e8\u65b9\u5411\u79ef\u5206\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e58\u5458\u8212\u9002\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u4f53\u611f\u53cd\u9988\u548c\u808c\u8089\u63a7\u5236\u5728\u59ff\u52bf\u7a33\u5b9a\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00975", "pdf": "https://arxiv.org/pdf/2508.00975", "abs": "https://arxiv.org/abs/2508.00975", "authors": ["Lynnette Hui Xian Ng", "Divyaansh Sinha", "Kathleen M. Carley"], "title": "Star Network Motifs on X during COVID-19", "categories": ["cs.SI", "cs.HC"], "comment": "Accepted into SBP-BRiMS 2025", "summary": "Social network motifs are recurring patterns of small subgraphs that indicate\nfundamental patterns of social communication. In this work, we study the simple\nstar network motifs that recur on X during the COVID-19 discourse. We study the\nprofile of the manifestation of the star network among bot and human users.\nThere are six primary patterns of the star motif, differentiating by the bots\nand humans being either egos and alters. We describe the presentation of each\nof these six patterns in our data, demonstrating how the motif patterns can\ninform social media behavioral analysis.", "AI": {"tldr": "\u7814\u7a76COVID-19\u671f\u95f4X\u5e73\u53f0\u4e0a\u661f\u5f62\u7f51\u7edc\u6a21\u56e0\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u7528\u6237\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u5206\u6790\u793e\u4ea4\u7f51\u7edc\u4e2d\u661f\u5f62\u6a21\u56e0\u7684\u91cd\u590d\u51fa\u73b0\u6a21\u5f0f\uff0c\u4ee5\u63ed\u793a\u793e\u4ea4\u5a92\u4f53\u7684\u57fa\u672c\u884c\u4e3a\u7279\u5f81\u3002", "method": "\u7814\u7a76X\u5e73\u53f0\u4e0aCOVID-19\u8ba8\u8bba\u4e2d\u7684\u661f\u5f62\u7f51\u7edc\u6a21\u56e0\uff0c\u5206\u4e3a\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u7528\u6237\u4f5c\u4e3a\u6838\u5fc3\u6216\u5916\u56f4\u7684\u516d\u79cd\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa\u516d\u79cd\u661f\u5f62\u6a21\u56e0\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5982\u4f55\u4e3a\u793e\u4ea4\u5a92\u4f53\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4fe1\u606f\u3002", "conclusion": "\u661f\u5f62\u7f51\u7edc\u6a21\u56e0\u6a21\u5f0f\u6709\u52a9\u4e8e\u7406\u89e3\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u7528\u6237\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u884c\u4e3a\u5dee\u5f02\u3002"}}
{"id": "2508.01186", "pdf": "https://arxiv.org/pdf/2508.01186", "abs": "https://arxiv.org/abs/2508.01186", "authors": ["Chaojia Yu", "Zihan Cheng", "Hanwen Cui", "Yishuo Gao", "Zexu Luo", "Yijin Wang", "Hangbin Zheng", "Yong Zhao"], "title": "A Survey on Agent Workflow -- Status and Future", "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 3 figures, accepted to IEEE Conference,\n  ICAIBD(International Conference of Artificial Intelligence and Big Data)\n  2025. This is the author's version, not the publisher's. See\n  https://ieeexplore.ieee.org/document/11082076", "summary": "In the age of large language models (LLMs), autonomous agents have emerged as\na powerful paradigm for achieving general intelligence. These agents\ndynamically leverage tools, memory, and reasoning capabilities to accomplish\nuser-defined goals. As agent systems grow in complexity, agent\nworkflows-structured orchestration frameworks-have become central to enabling\nscalable, controllable, and secure AI behaviors. This survey provides a\ncomprehensive review of agent workflow systems, spanning academic frameworks\nand industrial implementations. We classify existing systems along two key\ndimensions: functional capabilities (e.g., planning, multi-agent collaboration,\nexternal API integration) and architectural features (e.g., agent roles,\norchestration flows, specification languages). By comparing over 20\nrepresentative systems, we highlight common patterns, potential technical\nchallenges, and emerging trends. We further address concerns related to\nworkflow optimization strategies and security. Finally, we outline open\nproblems such as standardization and multimodal integration, offering insights\nfor future research at the intersection of agent design, workflow\ninfrastructure, and safe automation.", "AI": {"tldr": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\u4e0b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u5206\u7c7b\u6bd4\u8f83\u4e8620\u591a\u4e2a\u4ee3\u8868\u6027\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u5de5\u4f5c\u6d41\u7cfb\u7edf\u6210\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u548c\u5b89\u5168AI\u884c\u4e3a\u7684\u5173\u952e\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u529f\u80fd\u80fd\u529b\u548c\u67b6\u6784\u7279\u5f81\u4e24\u5927\u7ef4\u5ea6\uff0c\u6bd4\u8f83\u5206\u6790\u4e8620\u591a\u4e2a\u4ee3\u8868\u6027\u7cfb\u7edf\u3002", "result": "\u603b\u7ed3\u4e86\u5e38\u89c1\u6a21\u5f0f\u3001\u6280\u672f\u6311\u6218\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u5de5\u4f5c\u6d41\u4f18\u5316\u548c\u5b89\u5168\u76f8\u5173\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u6807\u51c6\u5316\u548c\u591a\u6a21\u6001\u96c6\u6210\u7b49\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8\u667a\u80fd\u4f53\u8bbe\u8ba1\u548c\u5de5\u4f5c\u6d41\u57fa\u7840\u8bbe\u65bd\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213", "abs": "https://arxiv.org/abs/2508.01213", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\uff0c\u5c06\u804a\u5929\u67e5\u8be2\u5206\u5272\u4e3a\u8bf7\u6c42\u5185\u5bb9\u3001\u89d2\u8272\u3001\u67e5\u8be2\u7279\u5b9a\u4e0a\u4e0b\u6587\u548c\u9644\u52a0\u8868\u8fbe\uff0c\u53d1\u73b0LLM\u67e5\u8be2\u4e2d\u7684\u8bf7\u6c42\u884c\u4e3a\u4e0e\u4eba\u9645\u4ea4\u4e92\u6709\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u8868\u8fbe\u7684\u65f6\u95f4\u7ef4\u5ea6\u5206\u6790\u5c55\u793a\u4e86\u7528\u6237\u884c\u4e3a\u968f\u7ecf\u9a8c\u548c\u6a21\u578b\u80fd\u529b\u7684\u53d8\u5316\u3002", "motivation": "\u7814\u7a76LLM\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\uff0c\u56e0\u67e5\u8be2\u53d8\u5f02\u6027\u5e38\u88ab\u63a9\u76d6\uff0c\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u6765\u5206\u5272\u548c\u5206\u6790\u804a\u5929\u65e5\u5fd7\u3002", "method": "\u63d0\u51fa\u65b0\u4efb\u52a1\uff0c\u5206\u6bb5\u804a\u5929\u67e5\u8be2\u4e3a\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u57fa\u4e8e\u6570\u636e\u8d44\u6e90\u8fdb\u884c\u5386\u65f6\u5206\u6790\u3002", "result": "\u53d1\u73b0\u65e9\u671f\u67e5\u8be2\u4fa7\u91cd\u8bf7\u6c42\uff0c\u7528\u6237\u968f\u7ecf\u9a8c\u8d8b\u4e8e\u6536\u655b\uff1b\u6a21\u578b\u80fd\u529b\u53d8\u5316\uff08\u5982\u65b0\u6a21\u578b\u5f15\u5165\uff09\u5bf9\u7528\u6237\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLM\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u968f\u7ecf\u9a8c\u548c\u6a21\u578b\u66f4\u65b0\u800c\u53d8\u5316\uff0c\u65b0\u4efb\u52a1\u548c\u5386\u65f6\u5206\u6790\u4e3a\u7406\u89e3\u7528\u6237\u884c\u4e3a\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\u3002"}}
{"id": "2508.01240", "pdf": "https://arxiv.org/pdf/2508.01240", "abs": "https://arxiv.org/abs/2508.01240", "authors": ["Juntong Chen", "Huayuan Ye", "He Zhu", "Siwei Fu", "Changbo Wang", "Chenhui Li"], "title": "RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation", "categories": ["cs.LG", "cs.HC"], "comment": "9 pages, 14 figures, paper accepted to IEEE VIS 2025", "summary": "Accurate and reliable visualization of spatiotemporal sensor data such as\nenvironmental parameters and meteorological conditions is crucial for informed\ndecision-making. Traditional spatial interpolation methods, however, often fall\nshort of producing reliable interpolation results due to the limited and\nirregular sensor coverage. This paper introduces a novel spatial interpolation\npipeline that achieves reliable interpolation results and produces a novel\nheatmap representation with uncertainty information encoded. We leverage\nimputation reference data from Graph Neural Networks (GNNs) to enhance\nvisualization reliability and temporal resolution. By integrating Principal\nNeighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), our\nmodel effectively learns the spatiotemporal dependencies. Furthermore, we\npropose an extrinsic, static visualization technique for interpolation-based\nheatmaps that effectively communicates the uncertainties arising from various\nsources in the interpolated map. Through a set of use cases, extensive\nevaluations on real-world datasets, and user studies, we demonstrate our\nmodel's superior performance for data imputation, the improvements to the\ninterpolant with reference data, and the effectiveness of our visualization\ndesign in communicating uncertainties.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u63d2\u503c\u7ba1\u9053\uff0c\u7ed3\u5408GNN\u548c\u5730\u7406\u7f16\u7801\uff0c\u63d0\u9ad8\u4e86\u65f6\u7a7a\u6570\u636e\u7684\u53ef\u89c6\u5316\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u70ed\u56fe\u7f16\u7801\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u7a7a\u95f4\u63d2\u503c\u65b9\u6cd5\u56e0\u4f20\u611f\u5668\u8986\u76d6\u6709\u9650\u4e14\u4e0d\u89c4\u5219\uff0c\u96be\u4ee5\u63d0\u4f9b\u53ef\u9760\u7ed3\u679c\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u6574\u5408GNN\u7684\u63d2\u8865\u53c2\u8003\u6570\u636e\u548cPNA\u3001GPE\u6280\u672f\uff0c\u5b66\u4e60\u65f6\u7a7a\u4f9d\u8d56\uff0c\u63d0\u51fa\u9759\u6001\u53ef\u89c6\u5316\u6280\u672f\u5c55\u793a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u6a21\u578b\u5728\u6570\u636e\u63d2\u8865\u548c\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u6570\u636e\u63d2\u503c\u7684\u53ef\u9760\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f20\u8fbe\u6548\u679c\u3002"}}
{"id": "2508.01316", "pdf": "https://arxiv.org/pdf/2508.01316", "abs": "https://arxiv.org/abs/2508.01316", "authors": ["Mohsen Abbaspour Onari", "Lucie Charlotte Magister", "Yaoxin Wu", "Amalia Lupi", "Dario Creazzo", "Mattia Tordin", "Luigi Di Donatantonio", "Emilio Quaia", "Chao Zhang", "Isel Grau", "Marco S. Nobile", "Yingqian Zhang", "Pietro Li\u00f2"], "title": "Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Distal myopathy represents a genetically heterogeneous group of skeletal\nmuscle disorders with broad clinical manifestations, posing diagnostic\nchallenges in radiology. To address this, we propose a novel multimodal\nattention-aware fusion architecture that combines features extracted from two\ndistinct deep learning models, one capturing global contextual information and\nthe other focusing on local details, representing complementary aspects of the\ninput data. Uniquely, our approach integrates these features through an\nattention gate mechanism, enhancing both predictive performance and\ninterpretability. Our method achieves a high classification accuracy on the\nBUSI benchmark and a proprietary distal myopathy dataset, while also generating\nclinically relevant saliency maps that support transparent decision-making in\nmedical diagnosis. We rigorously evaluated interpretability through (1)\nfunctionally grounded metrics, coherence scoring against reference masks and\nincremental deletion analysis, and (2) application-grounded validation with\nseven expert radiologists. While our fusion strategy boosts predictive\nperformance relative to single-stream and alternative fusion strategies, both\nquantitative and qualitative evaluations reveal persistent gaps in anatomical\nspecificity and clinical usefulness of the interpretability. These findings\nhighlight the need for richer, context-aware interpretability methods and\nhuman-in-the-loop feedback to meet clinicians' expectations in real-world\ndiagnostic settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u8fdc\u7aef\u808c\u75c5\u7684\u5206\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8fdc\u7aef\u808c\u75c5\u7684\u8bca\u65ad\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u4e92\u8865\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u95e8\u673a\u5236\u878d\u5408\uff0c\u751f\u6210\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u663e\u8457\u56fe\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f46\u89e3\u5256\u7279\u5f02\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u91ca\u65b9\u6cd5\u548c\u4eba\u673a\u53cd\u9988\uff0c\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u9700\u6c42\u3002"}}
{"id": "2508.01510", "pdf": "https://arxiv.org/pdf/2508.01510", "abs": "https://arxiv.org/abs/2508.01510", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "title": "DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset", "categories": ["eess.SP", "cs.HC"], "comment": null, "summary": "A fully customisable chip-on board (COB) LED design to evoke two brain\nresponses simultaneously (steady state visual evoked potential (SSVEP) and\ntransient evoked potential, P300) is discussed in this paper. Considering\ndifferent possible modalities in braincomputer interfacing (BCI), SSVEP is\nwidely accepted as it requires a lesser number of electroencephalogram (EEG)\nelectrodes and minimal training time. The aim of this work was to produce a\nhybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced\nfatigue and improved classification performance. The system comprises of four\nindependent radial green visual stimuli controlled individually by a 32-bit\nmicrocontroller platform to evoke SSVEP and four red LEDs flashing at random\nintervals to generate P300 events. The system can also record the P300 event\ntimestamps that can be used in classification, to improve the accuracy and\nreliability. The hybrid stimulus was tested for realtime classification\naccuracy by controlling a LEGO robot to move in four directions.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u4e00\u79cd\u53ef\u81ea\u5b9a\u4e49\u7684\u82af\u7247\u677f\uff08COB\uff09LED\u8bbe\u8ba1\uff0c\u7528\u4e8e\u540c\u65f6\u5f15\u53d1\u4e24\u79cd\u8111\u53cd\u5e94\uff08\u7a33\u6001\u89c6\u89c9\u8bf1\u53d1\u7535\u4f4d\uff08SSVEP\uff09\u548c\u77ac\u6001\u8bf1\u53d1\u7535\u4f4dP300\uff09\uff0c\u65e8\u5728\u51cf\u5c11\u75b2\u52b3\u5e76\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408SSVEP\u548cP300\u7684\u4f18\u52bf\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u6df7\u5408BCI\u786c\u4ef6\u5e73\u53f0\uff0c\u4ee5\u51cf\u5c11EEG\u7535\u6781\u6570\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u752832\u4f4d\u5fae\u63a7\u5236\u5668\u72ec\u7acb\u63a7\u5236\u56db\u4e2a\u7eff\u8272\u89c6\u89c9\u523a\u6fc0\uff08SSVEP\uff09\u548c\u56db\u4e2a\u7ea2\u8272LED\u968f\u673a\u95ea\u70c1\uff08P300\uff09\uff0c\u5e76\u8bb0\u5f55P300\u4e8b\u4ef6\u65f6\u95f4\u6233\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u7528\u4e8e\u5b9e\u65f6\u5206\u7c7b\uff0c\u901a\u8fc7\u63a7\u5236\u4e50\u9ad8\u673a\u5668\u4eba\u5728\u56db\u4e2a\u65b9\u5411\u79fb\u52a8\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "conclusion": "\u8be5\u6df7\u5408\u523a\u6fc0\u8bbe\u8ba1\u6709\u6548\u7ed3\u5408\u4e86SSVEP\u548cP300\uff0c\u51cf\u5c11\u4e86\u75b2\u52b3\u5e76\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.01545", "pdf": "https://arxiv.org/pdf/2508.01545", "abs": "https://arxiv.org/abs/2508.01545", "authors": ["Emilio Barkett", "Olivia Long", "Paul Kr\u00f6ger"], "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51b3\u7b56\u4e2d\u7684\u8ba4\u77e5\u504f\u89c1\u8868\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u73af\u5883\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u7ec4\u7ec7\u538b\u529b\u4f1a\u663e\u8457\u589e\u52a0\u5176\u201c\u627f\u8bfa\u5347\u7ea7\u201d\u884c\u4e3a\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u7ee7\u627f\u4eba\u7c7b\u8ba4\u77e5\u504f\u89c1\uff08\u5982\u627f\u8bfa\u5347\u7ea7\uff09\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u89c1\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u56db\u7ec4\u5b9e\u9a8c\u6761\u4ef6\uff08\u6a21\u578b\u4f5c\u4e3a\u6295\u8d44\u8005\u3001\u987e\u95ee\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u590d\u5408\u538b\u529b\u573a\u666f\uff09\u8fdb\u884c6,500\u6b21\u8bd5\u9a8c\u3002", "result": "\u4e2a\u4f53\u51b3\u7b56\u4e2dLLMs\u8868\u73b0\u7406\u6027\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff08\u5bf9\u79f0\u7ed3\u6784\uff09\u548c\u590d\u5408\u538b\u529b\u4e0b\u8868\u73b0\u51fa\u9ad8\u6bd4\u4f8b\u7684\u201c\u627f\u8bfa\u5347\u7ea7\u201d\u3002", "conclusion": "LLMs\u7684\u504f\u89c1\u8868\u73b0\u53d6\u51b3\u4e8e\u793e\u4f1a\u548c\u7ec4\u7ec7\u73af\u5883\uff0c\u9700\u8b66\u60d5\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u65e0\u76d1\u7763\u64cd\u4f5c\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656", "abs": "https://arxiv.org/abs/2508.01656", "authors": ["Lucio La Cava", "Dominik Macko", "R\u00f3bert M\u00f3ro", "Ivan Srba", "Andrea Tagarelli"], "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u533a\u5206\u673a\u5668\u751f\u6210\u6587\u672c\uff08MGT\uff09\u548c\u4eba\u7c7b\u5199\u4f5c\u5185\u5bb9\u7684\u6311\u6218\uff0c\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u4f5c\u8005\u5f52\u5c5e\uff08AA\uff09\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u5355\u8bed\u8a00AA\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u53ca\u5176\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u8fbe\u5230\u7c7b\u4eba\u6c34\u5e73\uff0c\u533a\u5206\u673a\u5668\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u5199\u4f5c\u5185\u5bb9\u53d8\u5f97\u6108\u53d1\u56f0\u96be\u3002\u5f53\u524dAA\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u8bed\u8a00\uff08\u5c24\u5176\u662f\u82f1\u8bed\uff09\u73af\u5883\u4e2d\uff0c\u5ffd\u89c6\u4e86\u591a\u8bed\u8a00\u573a\u666f\u7684\u590d\u6742\u6027\u548c\u73b0\u4ee3LLMs\u7684\u591a\u8bed\u8a00\u4f7f\u7528\u3002", "method": "\u7814\u7a76\u8986\u76d6\u4e8618\u79cd\u8bed\u8a00\uff08\u6db5\u76d6\u591a\u79cd\u8bed\u7cfb\u548c\u4e66\u5199\u7cfb\u7edf\uff09\u548c8\u79cd\u751f\u6210\u5668\uff087\u4e2aLLMs\u548c\u4eba\u7c7b\u4e66\u5199\u7c7b\uff09\uff0c\u8bc4\u4f30\u4e86\u5355\u8bed\u8a00AA\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\uff0c\u4ee5\u53ca\u4e0d\u540c\u751f\u6210\u5668\u5bf9AA\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u5355\u8bed\u8a00AA\u65b9\u6cd5\u53ef\u4ee5\u9002\u5e94\u591a\u8bed\u8a00\u73af\u5883\uff0c\u4f46\u5728\u8de8\u8bed\u7cfb\u8fc1\u79fb\u65f6\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u548c\u6311\u6218\uff0c\u51f8\u663e\u4e86\u591a\u8bed\u8a00AA\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u4ee5\u5339\u914d\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "\u591a\u8bed\u8a00AA\u95ee\u9898\u5177\u6709\u6311\u6218\u6027\uff0c\u5f53\u524d\u5355\u8bed\u8a00\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u8868\u660e\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u5e94\u5bf9\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u9700\u6c42\u3002"}}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2a\u6027\u5316\u65f6\u7528\u6237\u52a8\u6001\u504f\u597d\u7684\u95ee\u9898\uff0c\u63d0\u51faCUPID\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLMs\u662f\u5426\u80fd\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u7528\u6237\u504f\u597d\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709LLMs\u4e2a\u6027\u5316\u7814\u7a76\u5047\u8bbe\u7528\u6237\u504f\u597d\u662f\u9759\u6001\u7684\uff0c\u800c\u73b0\u5b9e\u4e2d\u7528\u6237\u504f\u597d\u662f\u52a8\u6001\u53d8\u5316\u7684\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLMs\u5982\u4f55\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u548c\u9002\u5e94\u7528\u6237\u7684\u4e0a\u4e0b\u6587\u504f\u597d\u3002", "method": "\u63d0\u51faCUPID\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b756\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u7528\u6237\u4e0eLLM\u4ea4\u4e92\u5386\u53f2\u8bb0\u5f55\uff0c\u8bc4\u4f30LLMs\u5728\u65b0\u8bf7\u6c42\u4e2d\u63a8\u65ad\u548c\u5e94\u7528\u7528\u6237\u504f\u597d\u7684\u80fd\u529b\u3002", "result": "\u8bc4\u4f3010\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLMs\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u504f\u597d\u7684\u8868\u73b0\u5dee\uff08\u7cbe\u786e\u5ea6\u4f4e\u4e8e50%\uff0c\u53ec\u56de\u7387\u4f4e\u4e8e65%\uff09\u3002", "conclusion": "\u7814\u7a76\u8bf4\u660eLLMs\u5728\u4e0a\u4e0b\u6587\u4e2a\u6027\u5316\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u5e76\u5efa\u8baeCUPID\u4f5c\u4e3a\u63a8\u52a8\u6539\u8fdb\u7684\u8d44\u6e90\u3002"}}
{"id": "2508.01736", "pdf": "https://arxiv.org/pdf/2508.01736", "abs": "https://arxiv.org/abs/2508.01736", "authors": ["Tyrone Justin Sta Maria", "Faith Griffin", "Jordan Aiko Deja"], "title": "Set the Stage: Enabling Storytelling with Multiple Robots through Roleplaying Metaphors", "categories": ["cs.RO", "cs.HC"], "comment": "3 pages, 2 figures, UIST Poster, adjunct proceedings", "summary": "Gestures are an expressive input modality for controlling multiple robots,\nbut their use is often limited by rigid mappings and recognition constraints.\nTo move beyond these limitations, we propose roleplaying metaphors as a\nscaffold for designing richer interactions. By introducing three roles:\nDirector, Puppeteer, and Wizard, we demonstrate how narrative framing can guide\nthe creation of diverse gesture sets and interaction styles. These roles enable\na variety of scenarios, showing how roleplay can unlock new possibilities for\nmulti-robot systems. Our approach emphasizes creativity, expressiveness, and\nintuitiveness as key elements for future human-robot interaction design.", "AI": {"tldr": "\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u9690\u55bb\uff08\u5982\u5bfc\u6f14\u3001\u6728\u5076\u5e08\u548c\u5deb\u5e08\uff09\u8bbe\u8ba1\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4e30\u5bcc\u624b\u52bf\u4ea4\u4e92\uff0c\u5f3a\u8c03\u521b\u9020\u6027\u3001\u8868\u73b0\u529b\u548c\u76f4\u89c2\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u52bf\u63a7\u5236\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u53d7\u9650\u4e8e\u50f5\u786c\u7684\u6620\u5c04\u548c\u8bc6\u522b\u7ea6\u675f\uff0c\u9700\u66f4\u5bcc\u8868\u73b0\u529b\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u89d2\u8272\uff08\u5bfc\u6f14\u3001\u6728\u5076\u5e08\u3001\u5deb\u5e08\uff09\u4f5c\u4e3a\u8bbe\u8ba1\u6846\u67b6\uff0c\u6307\u5bfc\u591a\u6837\u5316\u7684\u624b\u52bf\u96c6\u548c\u4ea4\u4e92\u98ce\u683c\u3002", "result": "\u89d2\u8272\u626e\u6f14\u9690\u55bb\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u4ea4\u4e92\u53ef\u80fd\u6027\u3002", "conclusion": "\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\u4e3a\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u521b\u9020\u6027\u3001\u8868\u73b0\u529b\u548c\u76f4\u89c2\u6027\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.01823", "pdf": "https://arxiv.org/pdf/2508.01823", "abs": "https://arxiv.org/abs/2508.01823", "authors": ["Mansi Sharma", "Antonio Kruger"], "title": "Unraveling the Connection: How Cognitive Workload Shapes Intent Recognition in Robot-Assisted Surgery", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Robot-assisted surgery has revolutionized the healthcare industry by\nproviding surgeons with greater precision, reducing invasiveness, and improving\npatient outcomes. However, the success of these surgeries depends heavily on\nthe robotic system ability to accurately interpret the intentions of the\nsurgical trainee or even surgeons. One critical factor impacting intent\nrecognition is the cognitive workload experienced during the procedure. In our\nrecent research project, we are building an intelligent adaptive system to\nmonitor cognitive workload and improve learning outcomes in robot-assisted\nsurgery. The project will focus on achieving a semantic understanding of\nsurgeon intents and monitoring their mental state through an intelligent\nmulti-modal assistive framework. This system will utilize brain activity, heart\nrate, muscle activity, and eye tracking to enhance intent recognition, even in\nmentally demanding situations. By improving the robotic system ability to\ninterpret the surgeons intentions, we can further enhance the benefits of\nrobot-assisted surgery and improve surgery outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u76d1\u6d4b\u8ba4\u77e5\u8d1f\u8377\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u7684\u610f\u56fe\u8bc6\u522b\u548c\u624b\u672f\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u51c6\u786e\u7406\u89e3\u5916\u79d1\u533b\u751f\u610f\u56fe\uff0c\u4f46\u8ba4\u77e5\u8d1f\u8377\u4f1a\u5f71\u54cd\u610f\u56fe\u8bc6\u522b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u76d1\u6d4b\u548c\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8f85\u52a9\u6846\u67b6\uff0c\u5229\u7528\u8111\u6d3b\u52a8\u3001\u5fc3\u7387\u3001\u808c\u8089\u6d3b\u52a8\u548c\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u533b\u751f\u610f\u56fe\u7684\u8bed\u4e49\u7406\u89e3\u548c\u8ba4\u77e5\u8d1f\u8377\u76d1\u6d4b\u3002", "result": "\u8be5\u7cfb\u7edf\u6709\u671b\u63d0\u9ad8\u610f\u56fe\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u9ad8\u538b\u73af\u5883\u4e0b\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u610f\u56fe\u8bc6\u522b\u548c\u76d1\u6d4b\u8ba4\u77e5\u8d1f\u8377\uff0c\u667a\u80fd\u81ea\u9002\u5e94\u7cfb\u7edf\u5c06\u663e\u8457\u6539\u5584\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u5b66\u4e60\u6548\u679c\u548c\u624b\u672f\u7ed3\u679c\u3002"}}
{"id": "2508.01853", "pdf": "https://arxiv.org/pdf/2508.01853", "abs": "https://arxiv.org/abs/2508.01853", "authors": ["Mansi Sharma", "Camilo Andr\u00e9s Mart\u00ednez Mart\u00ednez", "Benedikt Emanuel Wirth", "Antonio Kr\u00fcger", "Philipp M\u00fcller"], "title": "Distinguishing Target and Non-Target Fixations with EEG and Eye Tracking in Realistic Visual Scenes", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Distinguishing target from non-target fixations during visual search is a\nfundamental building block to understand users' intended actions and to build\neffective assistance systems. While prior research indicated the feasibility of\nclassifying target vs. non-target fixations based on eye tracking and\nelectroencephalography (EEG) data, these studies were conducted with explicitly\ninstructed search trajectories, abstract visual stimuli, and disregarded any\nscene context. This is in stark contrast with the fact that human visual search\nis largely driven by scene characteristics and raises questions regarding\ngeneralizability to more realistic scenarios. To close this gap, we, for the\nfirst time, investigate the classification of target vs. non-target fixations\nduring free visual search in realistic scenes. In particular, we conducted a\n36-participants user study using a large variety of 140 realistic visual search\nscenes in two highly relevant application scenarios: searching for icons on\ndesktop backgrounds and finding tools in a cluttered workshop. Our approach\nbased on gaze and EEG features outperforms the previous state-of-the-art\napproach based on a combination of fixation duration and saccade-related\npotentials. We perform extensive evaluations to assess the generalizability of\nour approach across scene types. Our approach significantly advances the\nability to distinguish between target and non-target fixations in realistic\nscenarios, achieving 83.6% accuracy in cross-user evaluations. This\nsubstantially outperforms previous methods based on saccade-related potentials,\nwhich reached only 56.9% accuracy.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c\u52a8\u548c\u8111\u7535\u56fe\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u533a\u5206\u73b0\u5b9e\u573a\u666f\u4e2d\u76ee\u6807\u4e0e\u975e\u76ee\u6807\u6ce8\u89c6\u70b9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5206\u7c7b\u76ee\u6807\u4e0e\u975e\u76ee\u6807\u6ce8\u89c6\u70b9\u65f6\uff0c\u5ffd\u89c6\u4e86\u573a\u666f\u7279\u6027\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u752836\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\uff0c\u4f7f\u7528140\u4e2a\u73b0\u5b9e\u573a\u666f\uff0c\u7ed3\u5408\u773c\u52a8\u548c\u8111\u7535\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8de8\u7528\u6237\u8bc4\u4f30\u4e2d\u8fbe\u523083.6%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u57fa\u4e8e\u626b\u89c6\u76f8\u5173\u7535\u4f4d\u7684\u65b9\u6cd5\uff0856.9%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u76ee\u6807\u4e0e\u975e\u76ee\u6807\u6ce8\u89c6\u70b9\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.02094", "pdf": "https://arxiv.org/pdf/2508.02094", "abs": "https://arxiv.org/abs/2508.02094", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u9752\u5c11\u5e74\u5bf9\u793e\u4ea4\u5a92\u4f53\u4e2d\u6709\u6bd2\u5185\u5bb9\u7684\u4e3b\u89c2\u611f\u77e5\uff0c\u53d1\u73b0\u4e86\u4e0e\u6210\u4eba\u4e0d\u540c\u7684\u2018\u9752\u5c11\u5e74\u6bd2\u6027\u2019\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u6784\u5efa\u9996\u4e2a\u4e2d\u6587\u2018\u9752\u5c11\u5e74\u6bd2\u6027\u2019\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u4e0a\u4e0b\u6587\u56e0\u7d20\u91cd\u8981\uff0c\u6539\u8fdb\u73b0\u6709\u68c0\u6d4b\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u9752\u5c11\u5e74\u72ec\u7279\u7684\u6bd2\u6027\u611f\u77e5\uff08\u6210\u4eba\u8ba4\u4e3a\u65e0\u6bd2\u4f46\u9752\u5c11\u5e74\u8ba4\u4e3a\u6709\u6bd2\u7684\u5185\u5bb9\uff09\uff0c\u586b\u8865\u73b0\u6709\u6bd2\u6027\u68c0\u6d4b\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4ee5\u4e2d\u56fd\u9752\u5c11\u5e74\u4e3a\u76ee\u6807\uff0c\u6784\u5efa\u9996\u4e2a\u4e2d\u6587\u2018\u9752\u5c11\u5e74\u6bd2\u6027\u2019\u6570\u636e\u96c6\uff0c\u5206\u6790\u7279\u5f81\u5e76\u6d4b\u8bd5\u73b0\u6709\u68c0\u6d4b\u6280\u672f\u7684\u51c6\u786e\u6027\u3002", "result": "\u53d1\u73b0\u9752\u5c11\u5e74\u7684\u6bd2\u6027\u611f\u77e5\u4e0e\u4e0a\u4e0b\u6587\u56e0\u7d20\u76f8\u5173\uff0c\u6539\u8fdb\u73b0\u6709\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u9752\u5c11\u5e74\u4e2d\u5fc3\u7684\u6bd2\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6d1e\u5bdf\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.02096", "pdf": "https://arxiv.org/pdf/2508.02096", "abs": "https://arxiv.org/abs/2508.02096", "authors": ["Raj Mahmud", "Yufeng Wu", "Abdullah Bin Sawad", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches", "categories": ["cs.IR", "cs.AI", "cs.HC", "H.3.3; H.5.2; I.2.7"], "comment": "Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables", "summary": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM.\n  Our findings reveal persistent limitations: post hoc surveys dominate,\nturn-level affective UX constructs are rarely assessed, and adaptive behaviours\nare seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,\nincluding epistemic opacity and verbosity, yet evaluations infrequently address\nthese issues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e862017-2025\u5e74\u95f423\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf(CRSs)\u4e2d\u7528\u6237\u4f53\u9a8c(UX)\u7684\u8bc4\u4f30\u73b0\u72b6\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765LLM\u611f\u77e5\u7684UX\u8bc4\u4f30\u65b9\u5411\u3002", "motivation": "\u586b\u8865\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u81ea\u9002\u5e94\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u7cfb\u7edf\u4e2d\u3002", "method": "\u9075\u5faaPRISMA\u6307\u5357\uff0c\u7cfb\u7edf\u7efc\u8ff0\u4e8623\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86UX\u7684\u5b9a\u4e49\u3001\u6d4b\u91cf\u53ca\u5176\u4e0e\u9886\u57df\u3001\u81ea\u9002\u5e94\u6027\u548cLLM\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u7814\u7a76\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u8fc7\u5ea6\u4f9d\u8d56\u4e8b\u540e\u8c03\u67e5\u3001\u7f3a\u4e4f\u5bf9\u8f6c\u5411\u7ea7\u60c5\u611fUX\u7684\u8bc4\u4f30\uff0c\u4ee5\u53caLLM\u5e26\u6765\u7684\u65b0\u6311\u6218\uff08\u5982\u4e0d\u900f\u660e\u6027\u548c\u5197\u957f\u6027\uff09\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7ed3\u6784\u5316UX\u6307\u6807\u4f53\u7cfb\u3001\u81ea\u9002\u5e94\u4e0e\u975e\u81ea\u9002\u5e94\u7cfb\u7edf\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u53ca\u9762\u5411LLM\u7684UX\u8bc4\u4f30\u8bae\u7a0b\uff0c\u4ee5\u652f\u6301\u66f4\u900f\u660e\u3001\u66f4\u7528\u6237\u4e2d\u5fc3\u7684CRS\u8bc4\u4f30\u5b9e\u8df5\u3002"}}
{"id": "2508.02354", "pdf": "https://arxiv.org/pdf/2508.02354", "abs": "https://arxiv.org/abs/2508.02354", "authors": ["Cuno Sankey-Olsen", "Rasmus Hvass Olesen", "Tobias Oliver Eberhard", "Andreas Triantafyllopoulos", "Bj\u00f6rn Schuller", "Ilhan Aslan"], "title": "Detecting COPD Through Speech Analysis: A Dataset of Danish Speech and Machine Learning Approach", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "comment": null, "summary": "Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating\ndisease affecting millions around the world. Its early detection using\nnon-invasive means could enable preventive interventions that improve quality\nof life and patient outcomes, with speech recently shown to be a valuable\nbiomarker. Yet, its validity across different linguistic groups remains to be\nseen. To that end, audio data were collected from 96 Danish participants\nconducting three speech tasks (reading, coughing, sustained vowels). Half of\nthe participants were diagnosed with different levels of COPD and the other\nhalf formed a healthy control group. Subsequently, we investigated different\nbaseline models using openSMILE features and learnt x-vector embeddings. We\nobtained a best accuracy of 67% using openSMILE features and logistic\nregression. Our findings support the potential of speech-based analysis as a\nnon-invasive, remote, and scalable screening tool as part of future COPD\nhealthcare solutions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8bed\u97f3\u5206\u6790\u7684\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\u5728\u68c0\u6d4b\u6162\u6027\u963b\u585e\u6027\u80ba\u75be\u75c5\uff08COPD\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728\u4e0d\u540c\u8bed\u8a00\u7fa4\u4f53\u4e2d\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u6536\u96c6\u4e39\u9ea6\u53c2\u4e0e\u8005\u7684\u8bed\u97f3\u6570\u636e\uff0c\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u5206\u6790\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe\u523067%\u3002", "motivation": "COPD\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u6539\u5584\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u8bed\u97f3\u4f5c\u4e3a\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5176\u5728\u4e0d\u540c\u8bed\u8a00\u7fa4\u4f53\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u660e\u786e\u9a8c\u8bc1\u3002", "method": "\u6536\u96c696\u540d\u4e39\u9ea6\u53c2\u4e0e\u8005\u7684\u8bed\u97f3\u6570\u636e\uff08\u5305\u62ec\u9605\u8bfb\u3001\u54b3\u55fd\u3001\u6301\u7eed\u5143\u97f3\u4efb\u52a1\uff09\uff0c\u5176\u4e2d\u4e00\u534a\u4e3aCOPD\u60a3\u8005\uff0c\u53e6\u4e00\u534a\u4e3a\u5065\u5eb7\u5bf9\u7167\u7ec4\u3002\u4f7f\u7528openSMILE\u7279\u5f81\u548cx-\u5411\u91cf\u5d4c\u5165\u5efa\u6a21\u5206\u6790\u3002", "result": "\u903b\u8f91\u56de\u5f52\u7ed3\u5408openSMILE\u7279\u5f81\u65f6\u6700\u9ad8\u51c6\u786e\u7387\u4e3a67%\u3002", "conclusion": "\u8bed\u97f3\u5206\u6790\u5177\u6709\u4f5c\u4e3a\u975e\u4fb5\u5165\u6027\u3001\u8fdc\u7a0b\u3001\u53ef\u6269\u5c55\u7684COPD\u7b5b\u67e5\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u53ef\u5e94\u7528\u4e8e\u672a\u6765\u533b\u7597\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02630", "pdf": "https://arxiv.org/pdf/2508.02630", "abs": "https://arxiv.org/abs/2508.02630", "authors": ["Amine Allouah", "Omar Besbes", "Josu\u00e9 D Figueroa", "Yash Kanoria", "Akshit Kumar"], "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u5728\u5728\u7ebf\u5e02\u573a\u4e2d\u7684\u8d2d\u7269\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5bf9\u4ea7\u54c1\u4f4d\u7f6e\u3001\u4ef7\u683c\u3001\u8bc4\u7ea7\u7b49\u56e0\u7d20\u7684\u504f\u597d\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u5356\u5bb6\u7b56\u7565\u548c\u5e73\u53f0\u8bbe\u8ba1\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4e86\u89e3\u5b83\u4eec\u5982\u4f55\u8d2d\u7269\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u9009\u62e9\u7279\u5b9a\u4ea7\u54c1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86ACES\u6c99\u76d2\u73af\u5883\uff0c\u901a\u8fc7\u968f\u673a\u5316\u4ea7\u54c1\u4f4d\u7f6e\u3001\u4ef7\u683c\u7b49\u56e0\u7d20\uff0c\u6d4b\u8bd5\u524d\u6cbfVLM\u4ee3\u7406\u7684\u8d2d\u7269\u884c\u4e3a\u3002", "result": "AI\u4ee3\u7406\u7684\u8d2d\u7269\u884c\u4e3a\u663e\u793a\u4e86\u5bf9\u7279\u5b9a\u4f4d\u7f6e\u7684\u504f\u597d\u3001\u5bf9\u8d5e\u52a9\u6807\u7b7e\u7684\u60e9\u7f5a\u4ee5\u53ca\u5bf9\u5e73\u53f0\u8ba4\u53ef\u7684\u5956\u52b1\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e3a\u5356\u5bb6\u7b56\u7565\u3001\u5e73\u53f0\u8bbe\u8ba1\u548c\u76d1\u7ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
