{"id": "2507.17233", "pdf": "https://arxiv.org/pdf/2507.17233", "abs": "https://arxiv.org/abs/2507.17233", "authors": ["Marco Ciccal\u00e8", "Daniel Jurjo-Rivas", "Jose F. Morales", "Pedro L\u00f3pez-Garc\u00eda", "Manuel V. Hermenegildo"], "title": "Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs", "categories": ["cs.PL"], "comment": "Accepted for publication in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Higher-order constructs enable more expressive and concise code by allowing\nprocedures to be parameterized by other procedures. Assertions allow expressing\npartial program specifications, which can be verified either at compile time\n(statically) or run time (dynamically). In higher-order programs, assertions\ncan also describe higher-order arguments. While in the context of (C)LP,\nrun-time verification of higher-order assertions has received some attention,\ncompile-time verification remains relatively unexplored. We propose a novel\napproach for statically verifying higher-order (C)LP programs with higher-order\nassertions. Although we use the Ciao assertion language for illustration, our\napproach is quite general and we believe is applicable to similar contexts.\nHigher-order arguments are described using predicate properties -- a special\nkind of property which exploits the (Ciao) assertion language. We refine the\nsyntax and semantics of these properties and introduce an abstract criterion to\ndetermine conformance to a predicate property at compile time, based on a\nsemantic order relation comparing the predicate property with the predicate\nassertions. We then show how to handle these properties using an abstract\ninterpretation-based static analyzer for programs with first-order assertions\nby reducing predicate properties to first-order properties. Finally, we report\non a prototype implementation and evaluate it through various examples within\nthe Ciao system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u9a8c\u8bc1\u9ad8\u9636(C)LP\u7a0b\u5e8f\u548c\u9ad8\u9636\u65ad\u8a00\u7684\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u62bd\u8c61\u89e3\u91ca\u5c06\u8c13\u8bcd\u5c5e\u6027\u964d\u4e3a\u4e00\u9636\u5c5e\u6027\u3002", "motivation": "\u76ee\u524d\u5bf9\u9ad8\u9636\u7a0b\u5e8f\u7684\u8fd0\u884c\u65f6\u9a8c\u8bc1\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u7f16\u8bd1\u65f6\u9a8c\u8bc1\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5229\u7528\u8c13\u8bcd\u5c5e\u6027\u63cf\u8ff0\u9ad8\u9636\u53c2\u6570\uff0c\u901a\u8fc7\u62bd\u8c61\u987a\u5e8f\u5173\u7cfb\u548c\u62bd\u8c61\u89e3\u91ca\u5b9e\u73b0\u9759\u6001\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e86\u539f\u578b\u5e76\u5728Ciao\u7cfb\u7edf\u4e2d\u901a\u8fc7\u591a\u4e2a\u793a\u4f8b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u7c7b\u4f3c\u7684\u9ad8\u9636\u7a0b\u5e8f\u9a8c\u8bc1\u573a\u666f\u3002"}}
{"id": "2507.17087", "pdf": "https://arxiv.org/pdf/2507.17087", "abs": "https://arxiv.org/abs/2507.17087", "authors": ["Anjiang Wei", "Rohan Yadav", "Hang Song", "Wonchan Lee", "Ke Wang", "Alex Aiken"], "title": "Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs", "categories": ["cs.DC", "cs.PL"], "comment": null, "summary": "Optimizing parallel programs for distributed heterogeneous systems remains a\ncomplex task, often requiring significant code modifications. Task-based\nprogramming systems improve modularity by separating performance decisions from\ncore application logic, but their mapping interfaces are often too low-level.\nIn this work, we introduce Mapple, a high-level, declarative programming\ninterface for mapping distributed applications. Mapple provides transformation\nprimitives to resolve dimensionality mismatches between iteration and processor\nspaces, including a key primitive, decompose, that helps minimize communication\nvolume. We implement Mapple on top of the Legion runtime by translating Mapple\nmappers into its low-level C++ interface. Across nine applications, including\nsix matrix multiplication algorithms and three scientific computing workloads,\nMapple reduces mapper code size by 14X and enables performance improvements of\nup to 1.34X over expert-written C++ mappers. In addition, the decompose\nprimitive achieves up to 1.83X improvement over existing\ndimensionality-resolution heuristics. These results demonstrate that Mapple\nsimplifies the development of high-performance mappers for distributed\napplications.", "AI": {"tldr": "Mapple \u662f\u4e00\u79cd\u9ad8\u7ea7\u58f0\u660e\u5f0f\u7f16\u7a0b\u63a5\u53e3\uff0c\u7528\u4e8e\u4f18\u5316\u5206\u5e03\u5f0f\u5e94\u7528\u7a0b\u5e8f\u7684\u6620\u5c04\uff0c\u51cf\u5c11\u4e86\u4ee3\u7801\u91cf\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f18\u5316\u5206\u5e03\u5f0f\u5f02\u6784\u7cfb\u7edf\u7684\u5e76\u884c\u7a0b\u5e8f\u901a\u5e38\u9700\u8981\u5927\u91cf\u4ee3\u7801\u4fee\u6539\uff0c\u800c\u73b0\u6709\u4efb\u52a1\u7f16\u7a0b\u7cfb\u7edf\u7684\u6620\u5c04\u63a5\u53e3\u8fc7\u4e8e\u5e95\u5c42\u3002", "method": "Mapple \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7ea7\u58f0\u660e\u5f0f\u63a5\u53e3\u548c transformation \u539f\u8bed\uff0c\u7279\u522b\u662f decompose \u539f\u8bed\uff0c\u4ee5\u51cf\u5c11\u901a\u4fe1\u91cf\uff0c\u5e76\u5728 Legion \u8fd0\u884c\u65f6\u4e0a\u5b9e\u73b0\u3002", "result": "\u5728\u4e5d\u4e2a\u5e94\u7528\u4e2d\uff0cMapple \u5c06\u6620\u5c04\u4ee3\u7801\u91cf\u51cf\u5c11\u4e86 14 \u500d\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe 1.34 \u500d\uff0cdecompose \u539f\u8bed\u6bd4\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u5347\u4e86 1.83 \u500d\u3002", "conclusion": "Mapple \u7b80\u5316\u4e86\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u5e94\u7528\u7a0b\u5e8f\u6620\u5c04\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.17453", "pdf": "https://arxiv.org/pdf/2507.17453", "abs": "https://arxiv.org/abs/2507.17453", "authors": ["Guanqin Zhang", "Kota Fukuda", "Zhenya Zhang", "H. M. N. Dilum Bandara", "Shiping Chen", "Jianjun Zhao", "Yulei Sui"], "title": "Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees", "categories": ["cs.LG", "cs.PL", "cs.SE"], "comment": "This is an extended version of the ECOOP 2025 paper, with a\n  comparison with DATE 2025 (Figure 7 of RQ1 in Section 5.2), as well as an\n  in-depth discussion of OOPSLA 2025 in the related work (Section 6)", "summary": "The vulnerability of neural networks to adversarial perturbations has\nnecessitated formal verification techniques that can rigorously certify the\nquality of neural networks. As the state-of-the-art, branch and bound (BaB) is\na \"divide-and-conquer\" strategy that applies off-the-shelf verifiers to\nsub-problems for which they perform better. While BaB can identify the\nsub-problems that are necessary to be split, it explores the space of these\nsub-problems in a naive \"first-come-first-serve\" manner, thereby suffering from\nan issue of inefficiency to reach a verification conclusion. To bridge this\ngap, we introduce an order over different sub-problems produced by BaB,\nconcerning with their different likelihoods of containing counterexamples.\nBased on this order, we propose a novel verification framework Oliva that\nexplores the sub-problem space by prioritizing those sub-problems that are more\nlikely to find counterexamples, in order to efficiently reach the conclusion of\nthe verification. Even if no counterexample can be found in any sub-problem, it\nonly changes the order of visiting different sub-problem and so will not lead\nto a performance degradation. Specifically, Oliva has two variants, including\n$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that\nare more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy\ninspired by simulated annealing that gradually shifts from exploration to\nexploitation to locate the globally optimal sub-problems. We experimentally\nevaluate the performance of Oliva on 690 verification problems spanning over 5\nmodels with datasets MNIST and CIFAR10. Compared to the state-of-the-art\napproaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up\nto 80X in CIFAR10.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faOliva\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u66f4\u53ef\u80fd\u5305\u542b\u53cd\u4f8b\u7684\u5b50\u95ee\u9898\uff0c\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u901f\u5ea6\u63d0\u5347\u6548\u679c\u663e\u8457\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u8106\u5f31\u6027\u9700\u8981\u4e25\u683c\u7684\u9a8c\u8bc1\u6280\u672f\uff0c\u800c\u73b0\u6709\u5206\u652f\u5b9a\u754c\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5b50\u95ee\u9898\u4f18\u5148\u7ea7\u7684\u9a8c\u8bc1\u6846\u67b6Oliva\uff0c\u5305\u62ec\u8d2a\u5a6a\u7b56\u7565\u548c\u6a21\u62df\u9000\u706b\u7b56\u7565\u3002", "result": "\u5728MNIST\u548cCIFAR10\u6570\u636e\u96c6\u4e0a\uff0cOliva\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe25\u500d\u548c80\u500d\u3002", "conclusion": "Oliva\u80fd\u9ad8\u6548\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.17691", "pdf": "https://arxiv.org/pdf/2507.17691", "abs": "https://arxiv.org/abs/2507.17691", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG", "cs.PL"], "comment": null, "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.", "AI": {"tldr": "CASCADE \u662f\u4e00\u79cd\u7ed3\u5408 Gemini \u548c JSIR \u7684\u65b0\u578b\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u53cd\u6df7\u6dc6 JavaScript \u4ee3\u7801\uff0c\u6062\u590d\u539f\u59cb\u8bed\u4e49\u5e76\u51cf\u5c11\u9006\u5411\u5de5\u7a0b\u5de5\u4f5c\u91cf\u3002", "motivation": "\u89e3\u51b3 JavaScript \u4ee3\u7801\u6df7\u6dc6\u5e26\u6765\u7684\u5206\u6790\u6311\u6218\uff0c\u5982\u6d4b\u8bd5\u3001\u9759\u6001\u5206\u6790\u548c\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u3002", "method": "\u5229\u7528 Gemini \u8bc6\u522b\u5173\u952e\u524d\u5bfc\u51fd\u6570\uff0c\u5e76\u901a\u8fc7 JSIR \u8fdb\u884c\u4ee3\u7801\u8f6c\u6362\uff0c\u6062\u590d\u539f\u59cb\u5b57\u7b26\u4e32\u548c API \u540d\u79f0\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u53cd\u6df7\u6dc6\u6548\u7387\uff0c\u51cf\u5c11\u786c\u7f16\u7801\u89c4\u5219\uff0c\u5df2\u5728 Google \u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "conclusion": "CASCADE \u5728\u53ef\u9760\u6027\u548c\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u4ee3\u7801\u5206\u6790\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17291", "pdf": "https://arxiv.org/pdf/2507.17291", "abs": "https://arxiv.org/abs/2507.17291", "authors": ["Damiano Azzolini", "Fabrizio Riguzzi", "Theresa Swift"], "title": "Integrating Belief Domains into Probabilistic Logic Programs", "categories": ["cs.LO", "cs.AI"], "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Probabilistic Logic Programming (PLP) under the Distribution Semantics is a\nleading approach to practical reasoning under uncertainty. An advantage of the\nDistribution Semantics is its suitability for implementation as a Prolog or\nPython library, available through two well-maintained implementations, namely\nProbLog and cplint/PITA. However, current formulations of the Distribution\nSemantics use point-probabilities, making it difficult to express epistemic\nuncertainty, such as arises from, for example, hierarchical classifications\nfrom computer vision models. Belief functions generalize probability measures\nas non-additive capacities, and address epistemic uncertainty via interval\nprobabilities. This paper introduces interval-based Capacity Logic Programs\nbased on an extension of the Distribution Semantics to include belief\nfunctions, and describes properties of the new framework that make it amenable\nto practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u95f4\u6982\u7387\u7684\u5bb9\u91cf\u903b\u8f91\u7a0b\u5e8f\u6269\u5c55\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5206\u5e03\u8bed\u4e49\u4e2d\u96be\u4ee5\u8868\u8fbe\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u6982\u7387\u903b\u8f91\u7f16\u7a0b\u4f7f\u7528\u70b9\u6982\u7387\uff0c\u96be\u4ee5\u8868\u8fbe\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u5206\u7c7b\u5c42\u6b21\uff09\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4fe1\u5ff5\u51fd\u6570\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6269\u5c55\u4e86\u5206\u5e03\u8bed\u4e49\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u533a\u95f4\u6982\u7387\u7684\u5bb9\u91cf\u903b\u8f91\u7a0b\u5e8f\uff0c\u5e76\u5206\u6790\u4e86\u5176\u9002\u7528\u6027\u3002", "result": "\u65b0\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u533a\u95f4\u6982\u7387\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u5bb9\u91cf\u903b\u8f91\u7a0b\u5e8f\u6269\u5c55\u4e86\u4f20\u7edf\u5206\u5e03\u8bed\u4e49\uff0c\u4e3a\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.17232", "pdf": "https://arxiv.org/pdf/2507.17232", "abs": "https://arxiv.org/abs/2507.17232", "authors": ["Mashiro Toyooka", "Kiyoharu Aizawa", "Yoko Yamakata"], "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task", "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u70f9\u996a\u98df\u8c31\u4e2d\u5bf9\u98df\u6750\u72b6\u6001\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8e\u70f9\u996a\u98df\u8c31\u4e2d\u5e38\u7701\u7565\u98df\u6750\u7684\u4e2d\u95f4\u72b6\u6001\uff0cLLMs\u96be\u4ee5\u51c6\u786e\u8ddf\u8e2a\u548c\u7406\u89e3\u98df\u8c31\u5185\u5bb9\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u6e05\u6670\u7684\u65e5\u672c\u98df\u8c31\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u65b0\u4efb\u52a1\u6765\u8bc4\u4f30LLMs\u5bf9\u98df\u6750\u72b6\u6001\u53d8\u5316\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u98df\u6750\u72b6\u6001\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86LLMs\u5bf9\u70f9\u996a\u8fc7\u7a0b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u4e0e\u5546\u4e1a\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u72b6\u6001\u63a2\u6d4b\u65b9\u6cd5\u8bc4\u4f30LLMs\u7684\u4e16\u754c\u7406\u89e3\u80fd\u529b\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u5176\u5728\u5177\u4f53\u9886\u57df\uff08\u5982\u70f9\u996a\uff09\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.16869", "pdf": "https://arxiv.org/pdf/2507.16869", "abs": "https://arxiv.org/abs/2507.16869", "authors": ["Yue Ma", "Kunyu Feng", "Zhongyuan Hu", "Xinyu Wang", "Yucheng Wang", "Mingzhe Zheng", "Xuanhua He", "Chenyang Zhu", "Hongyu Liu", "Yingqing He", "Zeyu Wang", "Zhifeng Li", "Xiu Li", "Wei Liu", "Dan Xu", "Linfeng Zhang", "Qifeng Chen"], "title": "Controllable Video Generation: A Survey", "categories": ["cs.GR", "cs.CV"], "comment": "project page:\n  https://github.com/mayuelala/Awesome-Controllable-Video-Generation", "summary": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u5173\u952e\u6982\u5ff5\u3001\u7406\u8bba\u8fdb\u5c55\u53ca\u63a7\u5236\u673a\u5236\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u3001\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u591a\u6a21\u6001\u7528\u6237\u9700\u6c42\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408\u76f8\u673a\u8fd0\u52a8\u3001\u6df1\u5ea6\u56fe\u7b49\u975e\u6587\u672c\u6761\u4ef6\u6269\u5c55\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u63a7\u5236\u4fe1\u53f7\u5982\u4f55\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u7cfb\u7edf\u5206\u7c7b\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5355\u6761\u4ef6\u3001\u591a\u6761\u4ef6\u548c\u901a\u7528\u53ef\u63a7\u751f\u6210\uff09\uff0c\u5e76\u6574\u7406\u4e86\u76f8\u5173\u6587\u732e\u5e93\u3002", "conclusion": "\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u7814\u7a76\u63d0\u5347\u4e86AIGC\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u7528\u6027\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u591a\u6a21\u6001\u63a7\u5236\u3002"}}
{"id": "2507.17074", "pdf": "https://arxiv.org/pdf/2507.17074", "abs": "https://arxiv.org/abs/2507.17074", "authors": ["Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan", "Madhusanka Liyanage"], "title": "Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond", "categories": ["cs.CR", "cs.NI", "cs.PF"], "comment": "Table 5, Figures 7, This paper has been accepted as a regular paper\n  at LCN 2025 and will appear in the conference proceedings. The final version\n  will be published by IEEE and the copyright will belong to IEEE", "summary": "The advent of quantum computing threatens the security of classical\npublic-key cryptographic systems, prompting the transition to post-quantum\ncryptography (PQC). While PQC has been analyzed in theory, its performance in\npractical wireless communication environments remains underexplored. This paper\npresents a detailed implementation and performance evaluation of NIST-selected\nPQC algorithms in user equipment (UE) to UE communications over 5G networks.\nUsing a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3\nvia BoringSSL and liboqs, we examine key encapsulation mechanisms and digital\nsignature schemes across realistic network conditions. We evaluate performance\nbased on handshake latency, CPU and memory usage, bandwidth, and retransmission\nrates, under varying cryptographic configurations and client loads. Our\nfindings show that ML-KEM with ML-DSA offers the best efficiency for\nlatency-sensitive applications, while SPHINCS+ and HQC combinations incur\nhigher computational and transmission overheads, making them unsuitable for\nsecurity-critical but time-sensitive 5G scenarios.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u8bc4\u4f30\u4e86NIST\u9009\u5b9a\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u57285G\u7f51\u7edc\u4e2d\u7528\u6237\u8bbe\u5907\u95f4\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u53d1\u73b0ML-KEM\u548cML-DSA\u7ec4\u5408\u6548\u7387\u6700\u4f73\uff0c\u9002\u5408\u65f6\u5ef6\u654f\u611f\u5e94\u7528\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u5174\u8d77\u5a01\u80c1\u4f20\u7edf\u516c\u94a5\u5bc6\u7801\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u4fc3\u4f7f\u8f6c\u5411\u540e\u91cf\u5b50\u5bc6\u7801\uff08PQC\uff09\uff0c\u4f46\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6027\u80fd\u5c1a\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5b8c\u65745G\u4eff\u771f\u5806\u6808\uff08Open5GS\u548cUERANSIM\uff09\u548cPQC-enabled TLS 1.3\uff0c\u6d4b\u8bd5\u4e86NIST\u9009\u5b9a\u7684PQC\u7b97\u6cd5\u57285G\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5bc6\u94a5\u5c01\u88c5\u673a\u5236\u548c\u6570\u5b57\u7b7e\u540d\u65b9\u6848\u3002", "result": "ML-KEM\u4e0eML-DSA\u7ec4\u5408\u5728\u65f6\u5ef6\u654f\u611f\u5e94\u7528\u4e2d\u6548\u7387\u6700\u9ad8\uff0c\u800cSPHINCS+\u548cHQC\u7ec4\u5408\u56e0\u8ba1\u7b97\u548c\u4f20\u8f93\u5f00\u9500\u8f83\u5927\uff0c\u4e0d\u9002\u5408\u65f6\u95f4\u654f\u611f\u76845G\u573a\u666f\u3002", "conclusion": "\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u57285G\u7f51\u7edc\u4e2d\u9700\u6743\u8861\u5b89\u5168\u6027\u4e0e\u6027\u80fd\uff0cML-KEM\u548cML-DSA\u7ec4\u5408\u662f\u5f53\u524d\u6700\u4f18\u9009\u62e9\u3002"}}
{"id": "2507.17188", "pdf": "https://arxiv.org/pdf/2507.17188", "abs": "https://arxiv.org/abs/2507.17188", "authors": ["Lijie Zheng", "Ji He", "Shih Yu Chang", "Yulong Shen", "Dusit Niyato"], "title": "LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks", "categories": ["cs.NI", "cs.AI", "cs.CR"], "comment": "Submitted to IEEE Transactions on Mobile Computing", "summary": "This work tackles the physical layer security (PLS) problem of maximizing the\nsecrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy\nconstraints. Unlike prior studies that assume uniform UAV capabilities or\noverlook energy-security trade-offs, we consider a realistic scenario where\nUAVs with diverse payloads and computation resources collaborate to serve\nground terminals in the presence of eavesdroppers. To manage the complex\ncoupling between UAV motion and communication, we propose a hierarchical\noptimization framework. The inner layer uses a semidefinite relaxation\n(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex\n(d.c.) programming to solve the secrecy precoding problem with fixed UAV\npositions. The outer layer introduces a Large Language Model (LLM)-guided\nheuristic multi-agent reinforcement learning approach (LLM-HeMARL) for\ntrajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics\npolicy generated by the LLM, enabling UAVs to learn energy-aware,\nsecurity-driven trajectories without the inference overhead of real-time LLM\ncalls. The simulation results show that our method outperforms existing\nbaselines in secrecy rate and energy efficiency, with consistent robustness\nacross varying UAV swarm sizes and random seeds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f02\u6784\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u6700\u5927\u5316\u4fdd\u5bc6\u7387\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b9\u6cd5\uff0c\u7ed3\u5408SDR\u548cLLM\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u548c\u9884\u7f16\u7801\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u80fd\u91cf\u7ea6\u675f\u4e0b\u7684\u4fdd\u5bc6\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u65e0\u4eba\u673a\u80fd\u529b\u5047\u8bbe\u5355\u4e00\u548c\u80fd\u91cf-\u5b89\u5168\u6743\u8861\u88ab\u5ffd\u7565\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u5185\u5c42\u7528SDR\u548cDC\u7f16\u7a0b\u89e3\u51b3\u9884\u7f16\u7801\u95ee\u9898\uff1b\u5916\u5c42\u7528LLM\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4fdd\u5bc6\u7387\u548c\u80fd\u6548\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u5728\u65e0\u4eba\u673a\u7fa4\u89c4\u6a21\u548c\u968f\u673a\u6027\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u901a\u4fe1\u4f18\u5316\u548c\u65e0\u4eba\u673a\u8fd0\u52a8\u89c4\u5212\uff0c\u4e3a\u5f02\u6784\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u7269\u7406\u5c42\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16819", "pdf": "https://arxiv.org/pdf/2507.16819", "abs": "https://arxiv.org/abs/2507.16819", "authors": ["Kayhan Latifzadeh", "Luis A. Leiva", "Klen \u010copi\u010d Pucihar", "Matja\u017e Kljun", "Iztok Devetak", "Lili Steblovnik"], "title": "Assessing Medical Training Skills via Eye and Head Movements", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "We examined eye and head movements to gain insights into skill development in\nclinical settings. A total of 24 practitioners participated in simulated baby\ndelivery training sessions. We calculated key metrics, including pupillary\nresponse rate, fixation duration, or angular velocity. Our findings indicate\nthat eye and head tracking can effectively differentiate between trained and\nuntrained practitioners, particularly during labor tasks. For example,\nhead-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas\npupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results\nlay the groundwork for computational models that support implicit skill\nassessment and training in clinical settings by using commodity eye-tracking\nglasses as a complementary device to more traditional evaluation methods such\nas subjective scores.", "AI": {"tldr": "\u901a\u8fc7\u773c\u90e8\u548c\u5934\u90e8\u8fd0\u52a8\u5206\u6790\u4e34\u5e8a\u6280\u80fd\u53d1\u5c55\uff0c\u7814\u7a76\u663e\u793a\u773c\u52a8\u548c\u5934\u52a8\u6570\u636e\u80fd\u6709\u6548\u533a\u5206\u8bad\u7ec3\u548c\u672a\u8bad\u7ec3\u7684\u4ece\u4e1a\u8005\uff0c\u4e3a\u8ba1\u7b97\u6a21\u578b\u7684\u6280\u80fd\u8bc4\u4f30\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u63a2\u7d22\u773c\u90e8\u548c\u5934\u90e8\u8fd0\u52a8\u5728\u4e34\u5e8a\u6280\u80fd\u53d1\u5c55\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u6280\u80fd\u8bc4\u4f30\u63d0\u4f9b\u5ba2\u89c2\u6570\u636e\u652f\u6301\u3002", "method": "24\u540d\u4ece\u4e1a\u8005\u53c2\u4e0e\u6a21\u62df\u5a74\u513f\u5206\u5a29\u8bad\u7ec3\uff0c\u6d4b\u91cf\u77b3\u5b54\u53cd\u5e94\u7387\u3001\u6ce8\u89c6\u65f6\u95f4\u548c\u89d2\u901f\u5ea6\u7b49\u6307\u6807\u3002", "result": "\u5934\u90e8\u7279\u5f81F1\u5206\u65700.85\uff0cAUC 0.86\uff1b\u77b3\u5b54\u7279\u5f81F1\u5206\u65700.77\uff0cAUC 0.85\u3002", "conclusion": "\u773c\u52a8\u8ffd\u8e2a\u773c\u955c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u8865\u5145\uff0c\u652f\u6301\u4e34\u5e8a\u6280\u80fd\u7684\u9690\u6027\u8bc4\u4f30\u548c\u8bad\u7ec3\u3002"}}
{"id": "2507.17139", "pdf": "https://arxiv.org/pdf/2507.17139", "abs": "https://arxiv.org/abs/2507.17139", "authors": ["Benjamin Watson", "Victoria Spaulding", "Neff Walker", "William Ribarsky"], "title": "Evaluation of the effects of frame time variation on VR task performance", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "We present a first study of the effects of frame time variations, in both\ndeviation around mean frame times and period of fluctuation, on task\nperformance in a virtual environment (VE). Chosen are open and closed loop\ntasks that are typical for current applications or likely to be prominent in\nfuture ones. The results show that at frame times in the range deemed\nacceptable for many applications, fairly large deviations in amplitude over a\nfairly wide range of periods do not significantly affect task performance.\nHowever, at a frame time often considered a minimum for immersive VR, frame\ntime variations do produce significant effects on closed loop task performance.\nThe results will be of use to designers of VEs and immersive applications, who\noften must control frame time variations due to large fluctuations of\ncomplexity (graphical and otherwise) in the VE.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5e27\u65f6\u95f4\u53d8\u5316\uff08\u5305\u62ec\u5e73\u5747\u5e27\u65f6\u95f4\u7684\u504f\u5dee\u548c\u6ce2\u52a8\u5468\u671f\uff09\u5bf9\u865a\u62df\u73af\u5883\u4e2d\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5927\u591a\u6570\u5e94\u7528\u53ef\u63a5\u53d7\u7684\u5e27\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u8f83\u5927\u7684\u632f\u5e45\u504f\u5dee\u548c\u8f83\u5bbd\u7684\u5468\u671f\u8303\u56f4\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5728\u6c89\u6d78\u5f0fVR\u7684\u6700\u4f4e\u5e27\u65f6\u95f4\u4e0b\uff0c\u5e27\u65f6\u95f4\u53d8\u5316\u4f1a\u5bf9\u95ed\u73af\u4efb\u52a1\u6027\u80fd\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5e27\u65f6\u95f4\u53d8\u5316\u5bf9\u865a\u62df\u73af\u5883\u4e2d\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u865a\u62df\u73af\u5883\u548c\u6c89\u6d78\u5f0f\u5e94\u7528\u7684\u8bbe\u8ba1\u8005\u63d0\u4f9b\u53c2\u8003\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5e94\u5bf9\u56e0\u73af\u5883\u590d\u6742\u6027\u5bfc\u81f4\u7684\u5e27\u65f6\u95f4\u6ce2\u52a8\u95ee\u9898\u3002", "method": "\u9009\u62e9\u4e86\u5178\u578b\u6216\u672a\u6765\u53ef\u80fd\u5e38\u89c1\u7684\u5f00\u73af\u548c\u95ed\u73af\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u5e27\u65f6\u95f4\u53d8\u5316\uff08\u504f\u5dee\u548c\u6ce2\u52a8\u5468\u671f\uff09\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u53ef\u63a5\u53d7\u7684\u5e27\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u5e27\u65f6\u95f4\u53d8\u5316\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\uff1b\u4f46\u5728\u6c89\u6d78\u5f0fVR\u7684\u6700\u4f4e\u5e27\u65f6\u95f4\u4e0b\uff0c\u5e27\u65f6\u95f4\u53d8\u5316\u4f1a\u663e\u8457\u5f71\u54cd\u95ed\u73af\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u865a\u62df\u73af\u5883\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5728\u8bbe\u8ba1\u65f6\u66f4\u597d\u5730\u63a7\u5236\u5e27\u65f6\u95f4\u6ce2\u52a8\uff0c\u5c24\u5176\u662f\u5728\u6c89\u6d78\u5f0fVR\u5e94\u7528\u4e2d\u3002"}}
{"id": "2507.17215", "pdf": "https://arxiv.org/pdf/2507.17215", "abs": "https://arxiv.org/abs/2507.17215", "authors": ["Omkar Bhalerao", "Yunjie Pan", "C. Seshadhri", "Nishil Talati"], "title": "Triadic First-Order Logic Queries in Temporal Networks", "categories": ["cs.DB", "cs.DS", "cs.IR", "cs.SI"], "comment": null, "summary": "Motif counting is a fundamental problem in network analysis, and there is a\nrich literature of theoretical and applied algorithms for this problem. Given a\nlarge input network $G$, a motif $H$ is a small \"pattern\" graph indicative of\nspecial local structure. Motif/pattern mining involves finding all matches of\nthis pattern in the input $G$. The simplest, yet challenging, case of motif\ncounting is when $H$ has three vertices, often called a \"triadic\" query. Recent\nwork has focused on \"temporal graph mining\", where the network $G$ has edges\nwith timestamps (and directions) and $H$ has time constraints.\n  Inspired by concepts in logic and database theory, we introduce the study of\n\"thresholded First Order Logic (FOL) Motif Analysis\" for massive temporal\nnetworks. A typical triadic motif query asks for the existence of three\nvertices that form a desired temporal pattern. An \"FOL\" motif query is obtained\nby having both existential and thresholded universal quantifiers. This allows\nfor query semantics that can mine richer information from networks. A typical\ntriadic query would be \"find all triples of vertices $u,v,w$ such that they\nform a triangle within one hour\". A thresholded FOL query can express \"find all\npairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$\nalso formed an edge within an hour\".\n  We design the first algorithm, FOLTY, for mining thresholded FOL triadic\nqueries. The theoretical running time of FOLTY matches the best known running\ntime for temporal triangle counting in sparse graphs. We give an efficient\nimplementation of FOLTY using specialized temporal data structures. FOLTY has\nexcellent empirical behavior, and can answer triadic FOL queries on graphs with\nnearly 70M edges is less than hour on commodity hardware. Our work has the\npotential to start a new research direction in the classic well-studied problem\nof motif analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9608\u503c\u4e00\u9636\u903b\u8f91\uff08FOL\uff09\u7684\u65f6\u5e8f\u7f51\u7edc\u6a21\u4f53\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7b97\u6cd5FOLTY\uff0c\u7528\u4e8e\u5904\u7406\u590d\u6742\u7684\u65f6\u5e8f\u6a21\u4f53\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u5e8f\u7f51\u7edc\u6a21\u4f53\u8ba1\u6570\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u7684\u4e09\u5143\u67e5\u8be2\uff0c\u65e0\u6cd5\u8868\u8fbe\u66f4\u590d\u6742\u7684\u6a21\u5f0f\u3002\u4f5c\u8005\u53d7\u903b\u8f91\u548c\u6570\u636e\u5e93\u7406\u8bba\u542f\u53d1\uff0c\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u9608\u503c\u4e00\u9636\u903b\u8f91\u6765\u4e30\u5bcc\u6a21\u4f53\u5206\u6790\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u9608\u503c\u4e00\u9636\u903b\u8f91\uff08FOL\uff09\u6a21\u4f53\u5206\u6790\u6846\u67b6\uff0c\u652f\u6301\u5305\u542b\u5b58\u5728\u91cf\u8bcd\u548c\u9608\u503c\u5168\u79f0\u91cf\u8bcd\u7684\u67e5\u8be2\u3002\u8bbe\u8ba1\u7b97\u6cd5FOLTY\uff0c\u5229\u7528\u4e13\u95e8\u7684\u65f6\u5e8f\u6570\u636e\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u3002", "result": "FOLTY\u5728\u7a00\u758f\u56fe\u4e2d\u7684\u7406\u8bba\u65f6\u95f4\u590d\u6742\u5ea6\u4e0e\u73b0\u6709\u6700\u4f73\u65f6\u5e8f\u4e09\u89d2\u8ba1\u6570\u7b97\u6cd5\u4e00\u81f4\u3002\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7b97\u6cd5\u80fd\u5728\u4e0d\u5230\u4e00\u5c0f\u65f6\u5185\u5904\u7406\u5305\u542b\u8fd17000\u4e07\u6761\u8fb9\u7684\u56fe\u3002", "conclusion": "FOLTY\u7b97\u6cd5\u4e3a\u6a21\u4f53\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u9608\u503c\u4e00\u9636\u903b\u8f91\u5728\u65f6\u5e8f\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17493", "pdf": "https://arxiv.org/pdf/2507.17493", "abs": "https://arxiv.org/abs/2507.17493", "authors": ["Alexander Beiser", "Markus Hecher", "Stefan Woltran"], "title": "Automated Hybrid Grounding Using Structural and Data-Driven Heuristics", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "The grounding bottleneck poses one of the key challenges that hinders the\nwidespread adoption of Answer Set Programming in industry. Hybrid Grounding is\na step in alleviating the bottleneck by combining the strength of standard\nbottom-up grounding with recently proposed techniques where rule bodies are\ndecoupled during grounding. However, it has remained unclear when hybrid\ngrounding shall use body-decoupled grounding and when to use standard bottom-up\ngrounding. In this paper, we address this issue by developing automated hybrid\ngrounding: we introduce a splitting algorithm based on data-structural\nheuristics that detects when to use body-decoupled grounding and when standard\ngrounding is beneficial. We base our heuristics on the structure of rules and\nan estimation procedure that incorporates the data of the instance. The\nexperiments conducted on our prototypical implementation demonstrate promising\nresults, which show an improvement on hard-to-ground scenarios, whereas on\nhard-to-solve instances we approach state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6df7\u5408\u63a5\u5730\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u57fa\u4e8e\u6570\u636e\u7ed3\u6784\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528\u4f53\u89e3\u8026\u63a5\u5730\u6216\u6807\u51c6\u81ea\u5e95\u5411\u4e0a\u63a5\u5730\u3002", "motivation": "\u89e3\u51b3\u63a5\u5730\u74f6\u9888\u95ee\u9898\uff0c\u4fc3\u8fdb\u7b54\u6848\u96c6\u7f16\u7a0b\u5728\u5de5\u4e1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7ed3\u6784\u548c\u5b9e\u4f8b\u6570\u636e\u7684\u542f\u53d1\u5f0f\u5206\u88c2\u7b97\u6cd5\uff0c\u7528\u4e8e\u51b3\u5b9a\u63a5\u5730\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u96be\u4ee5\u63a5\u5730\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u5728\u96be\u4ee5\u6c42\u89e3\u7684\u60c5\u51b5\u4e0b\u63a5\u8fd1\u73b0\u6709\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u81ea\u52a8\u6df7\u5408\u63a5\u5730\u65b9\u6848\u5728\u63d0\u5347\u63a5\u5730\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.17653", "pdf": "https://arxiv.org/pdf/2507.17653", "abs": "https://arxiv.org/abs/2507.17653", "authors": ["Liyun Zhang", "Zheng Lian", "Hong Liu", "Takanori Takebe", "Yuta Nakashima"], "title": "QuMAB: Query-based Multi-annotator Behavior Pattern Learning", "categories": ["cs.MM", "cs.IR", "Artificial intelligence"], "comment": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:2503.15237", "summary": "Multi-annotator learning traditionally aggregates diverse annotations to\napproximate a single ground truth, treating disagreements as noise. However,\nthis paradigm faces fundamental challenges: subjective tasks often lack\nabsolute ground truth, and sparse annotation coverage makes aggregation\nstatistically unreliable. We introduce a paradigm shift from sample-wise\naggregation to annotator-wise behavior modeling. By treating annotator\ndisagreements as valuable information rather than noise, modeling\nannotator-specific behavior patterns can reconstruct unlabeled data to reduce\nannotation cost, enhance aggregation reliability, and explain annotator\ndecision behavior. To this end, we propose QuMATL (Query-based Multi-Annotator\nBehavior Pattern Learning), which uses light-weight queries to model individual\nannotators while capturing inter-annotator correlations as implicit\nregularization, preventing overfitting to sparse individual data while\nmaintaining individualization and improving generalization, with a\nvisualization of annotator focus regions offering an explainable analysis of\nbehavior understanding. We contribute two large-scale datasets with dense\nper-annotator labels: STREET (4,300 labels/annotator) and AMER (average 3,118\nlabels/annotator), the first multimodal multi-annotator dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4ece\u4f20\u7edf\u6837\u672c\u7ea7\u6807\u6ce8\u805a\u5408\u8f6c\u5411\u6807\u6ce8\u8005\u884c\u4e3a\u5efa\u6a21\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6355\u6349\u6807\u6ce8\u8005\u884c\u4e3a\u6a21\u5f0f\u6765\u63d0\u5347\u6807\u6ce8\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u6807\u6ce8\u5dee\u5f02\u89c6\u4e3a\u566a\u58f0\uff0c\u5ffd\u89c6\u4e86\u4e3b\u89c2\u4efb\u52a1\u4e2d\u6807\u6ce8\u8005\u884c\u4e3a\u7684\u591a\u6837\u6027\uff0c\u4e14\u7a00\u758f\u6807\u6ce8\u8986\u76d6\u4f7f\u5f97\u7edf\u8ba1\u805a\u5408\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faQuMATL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u67e5\u8be2\u5efa\u6a21\u5355\u4e2a\u6807\u6ce8\u8005\u884c\u4e3a\uff0c\u5e76\u5229\u7528\u6807\u6ce8\u8005\u95f4\u76f8\u5173\u6027\u4f5c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89c6\u5316\u5206\u6790\u3002", "result": "\u8d21\u732e\u4e86\u4e24\u4e2a\u5927\u89c4\u6a21\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\u96c6STREET\u548cAMER\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6807\u6ce8\u8005\u884c\u4e3a\u5efa\u6a21\u80fd\u66f4\u597d\u5730\u5229\u7528\u6807\u6ce8\u5dee\u5f02\u4fe1\u606f\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.17029", "pdf": "https://arxiv.org/pdf/2507.17029", "abs": "https://arxiv.org/abs/2507.17029", "authors": ["Luchuan Song", "Yang Zhou", "Zhan Xu", "Yi Zhou", "Deepali Aneja", "Chenliang Xu"], "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "12 pages, 15 Figures", "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.", "AI": {"tldr": "StreamME\u662f\u4e00\u79cd\u5feb\u901f3D\u5934\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u89c6\u9891\u6d41\u7684\u540c\u6b65\u8bb0\u5f55\u4e0e\u91cd\u5efa\uff0c\u65e0\u9700\u9884\u7f13\u5b58\u6570\u636e\uff0c\u5e76\u4f18\u5316\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u67093D\u5934\u50cf\u91cd\u5efa\u65b9\u6cd5\u5bf9\u9884\u7f13\u5b58\u6570\u636e\u7684\u4f9d\u8d56\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u5e76\u964d\u4f4e\u901a\u4fe1\u5e26\u5bbd\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\uff0c\u7b80\u5316\u51e0\u4f55\u4f9d\u8d56\u5e76\u5f15\u5165\u57fa\u4e8e\u4e3b\u70b9\u7684\u7b80\u5316\u7b56\u7565\uff0c\u4f18\u5316\u70b9\u4e91\u5206\u5e03\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u73b0\u4e86\u5feb\u901f\u9002\u5e94\u9762\u90e8\u8868\u60c5\u7684\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "StreamME\u4e3aVR\u7cfb\u7edf\u548c\u5728\u7ebf\u4f1a\u8bae\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u52a8\u753b\u548c\u91cd\u65b0\u7167\u660e\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2507.17094", "pdf": "https://arxiv.org/pdf/2507.17094", "abs": "https://arxiv.org/abs/2507.17094", "authors": ["Sukjin Kim", "Seongyeon Park", "Si Ung Noh", "Junguk Hong", "Taehee Kwon", "Hunseong Lim", "Jinho Lee"], "title": "PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search", "categories": ["cs.DC"], "comment": "ATC 2025", "summary": "Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in\nnumerous applications, such as recommendation systems, natural language\nprocessing, and computer vision. While recent works on GPU-based acceleration\nhave significantly advanced ANNS performance, the ever-growing scale of\ndatasets now demands efficient multi-GPU solutions. However, the design of\nexisting works overlooks multi-GPU scalability, resulting in naive approaches\nthat treat additional GPUs as a means to extend memory capacity for large\ndatasets. This inefficiency arises from partitioning the dataset and\nindependently searching for data points similar to the queries in each GPU. We\ntherefore propose PathWeaver, a novel multi-GPU framework designed to scale and\naccelerate ANNS for large datasets. First, we propose pipelining-based path\nextension, a GPU-aware pipelining mechanism that reduces prior work's redundant\nsearch iterations by leveraging GPU-to-GPU communication. Second, we design\nghost staging that leverages a representative dataset to identify optimal query\nstarting points, reducing the search space for challenging queries. Finally, we\nintroduce direction-guided selection, a data selection technique that filters\nirrelevant points early in the search process, minimizing unnecessary memory\naccesses and distance computations. Comprehensive evaluations across diverse\ndatasets demonstrate that PathWeaver achieves 3.24$\\times$ geomean speedup and\nup to 5.30$\\times$ speedup on 95% recall rate over state-of-the-art\nmulti-GPU-based ANNS frameworks.", "AI": {"tldr": "PathWeaver \u662f\u4e00\u4e2a\u65b0\u578b\u591aGPU\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u5c55\u548c\u52a0\u901f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\uff0c\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u641c\u7d22\u3001\u4f18\u5316\u67e5\u8be2\u8d77\u59cb\u70b9\u548c\u8fc7\u6ee4\u65e0\u5173\u6570\u636e\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GPU\u52a0\u901fANNS\u65b9\u6cd5\u5728\u591aGPU\u73af\u5883\u4e0b\u6269\u5c55\u6027\u4e0d\u8db3\uff0c\u4ec5\u4ec5\u5c06\u989d\u5916GPU\u7528\u4e8e\u6269\u5c55\u5185\u5b58\u5bb9\u91cf\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "PathWeaver \u63d0\u51fa\u4e86\u57fa\u4e8e\u6d41\u6c34\u7ebf\u7684\u8def\u5f84\u6269\u5c55\u3001\u5229\u7528\u4ee3\u8868\u6570\u636e\u96c6\u4f18\u5316\u8d77\u59cb\u70b9\u7684\u5e7d\u7075\u6682\u5b58\u6280\u672f\uff0c\u4ee5\u53ca\u65b9\u5411\u5f15\u5bfc\u9009\u62e9\u4ee5\u51cf\u5c11\u4e0d\u5fc5\u8981\u8ba1\u7b97\u3002", "result": "PathWeaver \u572895%\u53ec\u56de\u7387\u4e0b\u5b9e\u73b0\u4e863.24\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u548c\u6700\u9ad85.30\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "PathWeaver \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591aGPU ANNS\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5904\u7406\u901f\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2507.17618", "pdf": "https://arxiv.org/pdf/2507.17618", "abs": "https://arxiv.org/abs/2507.17618", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "categories": ["cs.CL", "cs.PF"], "comment": null, "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "AI": {"tldr": "\u63d0\u51faSPADE\u89e3\u7801\u65b9\u6cd5\uff0c\u5bf9\u9f50\u4e2d\u95f4\u5c42\u4e0e\u8f93\u51fa\u5c42\u8868\u793a\uff0c\u7ed3\u5408\u6df7\u5408\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\u964d\u4f4e\u63a8\u7406\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u907f\u514d\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\u56e0\u4e2d\u95f4\u5c42\u4e0e\u8f93\u51fa\u5c42\u8868\u793a\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f00\u53d1SPADE\u89e3\u7801\u65b9\u6cd5\uff0c\u4f20\u64ad\u6700\u5c0f\u5316\u5e8f\u5217\uff08\u8d77\u59cb\u548c\u7b54\u6848\u6807\u8bb0\uff09\uff0c\u8bad\u7ec3\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\u8ba1\u7b97\u57fa\u4e8e\u71b5\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u6df7\u5408\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\u3002", "result": "\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SPADE\u4e0e\u6df7\u5408\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.17195", "pdf": "https://arxiv.org/pdf/2507.17195", "abs": "https://arxiv.org/abs/2507.17195", "authors": ["Jianpeng Qi", "Chao Liu", "Rui Wang", "Junyu Dong", "Yanwei Yu"], "title": "Closed-Form and Boundary Expressions for Task-Success Probability in Status-Driven Systems", "categories": ["cs.NI"], "comment": "10 pages, 10 figures", "summary": "Timely and efficient dissemination of server status is critical in\ncompute-first networking systems, where user tasks arrive dynamically and\ncomputing resources are limited and stochastic. In such systems, the access\npoint plays a key role in forwarding tasks to a server based on its latest\nreceived server status. However, modeling the task-success probability\nsuffering the factors of stochastic arrivals, limited server capacity, and\nbidirectional link delays. Therefore, we introduce a unified analytical\nframework that abstracts the AP forwarding rule as a single probability and\nmodels all network and waiting delays via their Laplace transforms. This\napproach yields a closed form expression for the end to end task success\nprobability, together with upper and lower bounds that capture Erlang loss\nblocking, information staleness, and random uplink/downlink delays. We validate\nour results through simulations across a wide range of parameters, showing that\ntheoretical predictions and bounds consistently enclose observed success rates.\nOur framework requires only two interchangeable inputs (the forwarding\nprobability and the delay transforms), making it readily adaptable to\nalternative forwarding policies and delay distributions. Experiments\ndemonstrate that our bounds are able to achieve accuracy within 0.01 (upper\nbound) and 0.016 (lower bound) of the empirical task success probability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u5728\u8ba1\u7b97\u4f18\u5148\u7f51\u7edc\u7cfb\u7edf\u4e2d\u4efb\u52a1\u6210\u529f\u7684\u6982\u7387\uff0c\u8003\u8651\u4e86\u968f\u673a\u5230\u8fbe\u3001\u6709\u9650\u670d\u52a1\u5668\u5bb9\u91cf\u548c\u53cc\u5411\u94fe\u8def\u5ef6\u8fdf\u7b49\u56e0\u7d20\u3002", "motivation": "\u5728\u8ba1\u7b97\u4f18\u5148\u7f51\u7edc\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u4efb\u52a1\u52a8\u6001\u5230\u8fbe\u3001\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4e14\u968f\u673a\uff0c\u53ca\u65f6\u6709\u6548\u5730\u4f20\u64ad\u670d\u52a1\u5668\u72b6\u6001\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u6355\u83b7\u4efb\u52a1\u6210\u529f\u7684\u6982\u7387\u3002", "method": "\u901a\u8fc7\u5c06AP\u8f6c\u53d1\u89c4\u5219\u62bd\u8c61\u4e3a\u5355\u4e00\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u53d8\u6362\u5efa\u6a21\u6240\u6709\u7f51\u7edc\u548c\u7b49\u5f85\u5ef6\u8fdf\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u6790\u6846\u67b6\uff0c\u5f97\u5230\u4e86\u4efb\u52a1\u6210\u529f\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u53ca\u5176\u4e0a\u4e0b\u754c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u7406\u8bba\u9884\u6d4b\u548c\u4e0a\u4e0b\u754c\u80fd\u591f\u4e00\u81f4\u5730\u5305\u56f4\u89c2\u6d4b\u5230\u7684\u6210\u529f\u7387\u3002\u4e0a\u4e0b\u754c\u4e0e\u7ecf\u9a8c\u4efb\u52a1\u6210\u529f\u6982\u7387\u7684\u8bef\u5dee\u5206\u522b\u57280.01\u548c0.016\u4ee5\u5185\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ec5\u9700\u4e24\u4e2a\u53ef\u4e92\u6362\u7684\u8f93\u5165\uff08\u8f6c\u53d1\u6982\u7387\u548c\u5ef6\u8fdf\u53d8\u6362\uff09\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u8f6c\u53d1\u7b56\u7565\u548c\u5ef6\u8fdf\u5206\u5e03\u3002"}}
{"id": "2507.17024", "pdf": "https://arxiv.org/pdf/2507.17024", "abs": "https://arxiv.org/abs/2507.17024", "authors": ["Chase Stokes", "Kylie Lin", "Cindy Xiong Bearfield"], "title": "Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances", "categories": ["cs.HC", "H.5.0"], "comment": "11 pages, 8 figures, accepted to IEEE VIS", "summary": "A growing body of work on visualization affordances highlights how specific\ndesign choices shape reader takeaways from information visualizations. However,\nmapping the relationship between design choices and reader conclusions often\nrequires labor-intensive crowdsourced studies, generating large corpora of\nfree-response text for analysis. To address this challenge, we explored\nalternative scalable research methodologies to assess chart affordances. We\ntest four elicitation methods from human-subject studies: free response,\nvisualization ranking, conclusion ranking, and salience rating, and compare\ntheir effectiveness in eliciting reader interpretations of line charts, dot\nplots, and heatmaps. Overall, we find that while no method fully replicates\naffordances observed in free-response conclusions, combinations of ranking and\nrating methods can serve as an effective proxy at a broad scale. The two\nranking methodologies were influenced by participant bias towards certain chart\ntypes and the comparison of suggested conclusions. Rating conclusion salience\ncould not capture the specific variations between chart types observed in the\nother methods. To supplement this work, we present a case study with GPT-4o,\nexploring the use of large language models (LLMs) to elicit human-like chart\ninterpretations. This aligns with recent academic interest in leveraging LLMs\nas proxies for human participants to improve data collection and analysis\nefficiency. GPT-4o performed best as a human proxy for the salience rating\nmethodology but suffered from severe constraints in other areas. Overall, the\ndiscrepancies in affordances we found between various elicitation\nmethodologies, including GPT-4o, highlight the importance of intentionally\nselecting and combining methods and evaluating trade-offs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u56db\u79cd\u53ef\u89c6\u5316\u7814\u7a76\u65b9\u6cd5\uff08\u81ea\u7531\u56de\u7b54\u3001\u56fe\u8868\u6392\u5e8f\u3001\u7ed3\u8bba\u6392\u5e8f\u548c\u663e\u8457\u6027\u8bc4\u5206\uff09\u5728\u8bc4\u4f30\u56fe\u8868\u8bbe\u8ba1\u5bf9\u8bfb\u8005\u89e3\u8bfb\u7684\u5f71\u54cd\u65f6\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7ec4\u5408\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u6d4b\u8bd5\u4e86GPT-4o\u4f5c\u4e3a\u4eba\u7c7b\u4ee3\u7406\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u901a\u8fc7\u81ea\u7531\u56de\u7b54\u5206\u6790\u56fe\u8868\u89e3\u8bfb\u7684\u89c4\u6a21\u5316\u96be\u9898\uff0c\u7814\u7a76\u5bfb\u627e\u66ff\u4ee3\u65b9\u6cd5\u6765\u9ad8\u6548\u8bc4\u4f30\u56fe\u8868\u8bbe\u8ba1\u5bf9\u8bfb\u8005\u7ed3\u8bba\u7684\u5f71\u54cd\u3002", "method": "\u6d4b\u8bd5\u4e86\u56db\u79cd\u7814\u7a76\u65b9\u6cd5\uff08\u81ea\u7531\u56de\u7b54\u3001\u56fe\u8868\u6392\u5e8f\u3001\u7ed3\u8bba\u6392\u5e8f\u548c\u663e\u8457\u6027\u8bc4\u5206\uff09\uff0c\u5e76\u7528GPT-4o\u4f5c\u4e3a\u4eba\u7c7b\u4ee3\u7406\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6392\u5e8f\u548c\u8bc4\u5206\u65b9\u6cd5\u7684\u7ec4\u5408\u53ef\u4f5c\u4e3a\u81ea\u7531\u56de\u7b54\u7684\u6709\u6548\u66ff\u4ee3\uff1bGPT-4o\u5728\u663e\u8457\u6027\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5176\u4ed6\u65b9\u6cd5\u53d7\u9650\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u6839\u636e\u76ee\u7684\u9009\u62e9\u6216\u7ec4\u5408\u65b9\u6cd5\uff0c\u5e76\u660e\u786eGPT-4o\u7b49\u5de5\u5177\u7684\u9002\u7528\u6027\u4e0e\u5c40\u9650\u6027\u3002"}}
{"id": "2507.17580", "pdf": "https://arxiv.org/pdf/2507.17580", "abs": "https://arxiv.org/abs/2507.17580", "authors": ["Amandeep Singh Bhatia", "Sabre Kais"], "title": "Enhancing Quantum Federated Learning with Fisher Information-Based Optimization", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.ET", "quant-ph"], "comment": null, "summary": "Federated Learning (FL) has become increasingly popular across different\nsectors, offering a way for clients to work together to train a global model\nwithout sharing sensitive data. It involves multiple rounds of communication\nbetween the global model and participating clients, which introduces several\nchallenges like high communication costs, heterogeneous client data, prolonged\nprocessing times, and increased vulnerability to privacy threats. In recent\nyears, the convergence of federated learning and parameterized quantum circuits\nhas sparked significant research interest, with promising implications for\nfields such as healthcare and finance. By enabling decentralized training of\nquantum models, it allows clients or institutions to collaboratively enhance\nmodel performance and outcomes while preserving data privacy. Recognizing that\nFisher information can quantify the amount of information that a quantum state\ncarries under parameter changes, thereby providing insight into its geometric\nand statistical properties. We intend to leverage this property to address the\naforementioned challenges. In this work, we propose a Quantum Federated\nLearning (QFL) algorithm that makes use of the Fisher information computed on\nlocal client models, with data distributed across heterogeneous partitions.\nThis approach identifies the critical parameters that significantly influence\nthe quantum model's performance, ensuring they are preserved during the\naggregation process. Our research assessed the effectiveness and feasibility of\nQFL by comparing its performance against other variants, and exploring the\nbenefits of incorporating Fisher information in QFL settings. Experimental\nresults on ADNI and MNIST datasets demonstrate the effectiveness of our\napproach in achieving better performance and robustness against the quantum\nfederated averaging method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFisher\u4fe1\u606f\u7684\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\uff08QFL\uff09\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6210\u672c\u3001\u6570\u636e\u5f02\u6784\u6027\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u867d\u53d7\u6b22\u8fce\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u6570\u636e\u5f02\u6784\u6027\u3001\u9690\u79c1\u5a01\u80c1\u7b49\u95ee\u9898\u3002\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u4e0eFisher\u4fe1\u606f\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "method": "\u5229\u7528Fisher\u4fe1\u606f\u91cf\u5316\u91cf\u5b50\u72b6\u6001\u7684\u51e0\u4f55\u548c\u7edf\u8ba1\u7279\u6027\uff0c\u8bbe\u8ba1QFL\u7b97\u6cd5\uff0c\u8bc6\u522b\u5e76\u4fdd\u7559\u5bf9\u6a21\u578b\u6027\u80fd\u5173\u952e\u7684\u53c2\u6570\uff0c\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728ADNI\u548cMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQFL\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u91cf\u5b50\u8054\u90a6\u5e73\u5747\u65b9\u6cd5\u3002", "conclusion": "\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7ed3\u5408Fisher\u4fe1\u606f\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u884c\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.17507", "pdf": "https://arxiv.org/pdf/2507.17507", "abs": "https://arxiv.org/abs/2507.17507", "authors": ["Vasileios Papastergios", "Lisa Ehrlinger", "Anastasios Gounaris"], "title": "Unfolding Data Quality Dimensions in Practice: A Survey", "categories": ["cs.DB"], "comment": null, "summary": "Data quality describes the degree to which data meet specific requirements\nand are fit for use by humans and/or downstream tasks (e.g., artificial\nintelligence). Data quality can be assessed across multiple high-level concepts\ncalled dimensions, such as accuracy, completeness, consistency, or timeliness.\nWhile extensive research and several attempts for standardization (e.g.,\nISO/IEC 25012) exist for data quality dimensions, their practical application\noften remains unclear. In parallel to research endeavors, a large number of\ntools have been developed that implement functionalities for the detection and\nmitigation of specific data quality issues, such as missing values or outliers.\nWith this paper, we aim to bridge this gap between data quality theory and\npractice by systematically connecting low-level functionalities offered by data\nquality tools with high-level dimensions, revealing their many-to-many\nrelationships. Through an examination of seven open-source data quality tools,\nwe provide a comprehensive mapping between their functionalities and the data\nquality dimensions, demonstrating how individual functionalities and their\nvariants partially contribute to the assessment of single dimensions. This\nsystematic survey provides both practitioners and researchers with a unified\nview on the fragmented landscape of data quality checks, offering actionable\ninsights for quality assessment across multiple dimensions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06\u6570\u636e\u8d28\u91cf\u5de5\u5177\u7684\u4f4e\u5c42\u529f\u80fd\u4e0e\u9ad8\u5c42\u7ef4\u5ea6\u8fde\u63a5\u8d77\u6765\uff0c\u5f25\u5408\u6570\u636e\u8d28\u91cf\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "motivation": "\u5c3d\u7ba1\u7814\u7a76\u548c\u6807\u51c6\u5316\u5c1d\u8bd5\uff08\u5982ISO/IEC 25012\uff09\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u4e0d\u6e05\u6670\u3002\u9700\u8981\u5c06\u7406\u8bba\u4e0e\u5b9e\u8df5\u7ed3\u5408\uff0c\u4e3a\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u7edf\u4e00\u7684\u89c6\u89d2\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e03\u4e2a\u5f00\u6e90\u6570\u636e\u8d28\u91cf\u5de5\u5177\u7684\u529f\u80fd\uff0c\u7cfb\u7edf\u5730\u6620\u5c04\u5176\u529f\u80fd\u4e0e\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u591a\u5bf9\u591a\u5173\u7cfb\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4efd\u5168\u9762\u7684\u6620\u5c04\uff0c\u5c55\u793a\u4e86\u5355\u4e2a\u529f\u80fd\u53ca\u5176\u53d8\u4f53\u5982\u4f55\u90e8\u5206\u8d21\u732e\u4e8e\u5355\u4e00\u7ef4\u5ea6\u7684\u8bc4\u4f30\u3002", "conclusion": "\u8fd9\u9879\u7cfb\u7edf\u6027\u8c03\u67e5\u4e3a\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u6570\u636e\u8d28\u91cf\u68c0\u67e5\u5206\u6563\u60c5\u51b5\u7684\u7edf\u4e00\u89c6\u56fe\uff0c\u5e76\u4e3a\u8de8\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.17343", "pdf": "https://arxiv.org/pdf/2507.17343", "abs": "https://arxiv.org/abs/2507.17343", "authors": ["Xiaohao Liu", "Xiaobo Xia", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Principled Multimodal Representation Learning", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "32 pages, 9 figures, 10 tables", "summary": "Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPMRL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8868\u793a\u77e9\u9635\u7684\u4e3b\u5bfc\u5947\u5f02\u503c\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u7684\u65e0\u951a\u70b9\u4f9d\u8d56\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u951a\u70b9\u6a21\u6001\uff0c\u9650\u5236\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u3002PMRL\u65e8\u5728\u6d88\u9664\u8fd9\u79cd\u4f9d\u8d56\uff0c\u4ee5\u66f4\u7a33\u5b9a\u7684\u65b9\u5f0f\u5b9e\u73b0\u591a\u6a21\u6001\u5bf9\u9f50\u3002", "method": "PMRL\u901a\u8fc7\u4f18\u5316\u8868\u793a\u77e9\u9635\u7684\u79e91 Gram\u77e9\u9635\u6027\u8d28\uff0c\u91c7\u7528\u57fa\u4e8esoftmax\u7684\u635f\u5931\u51fd\u6570\u548c\u5b9e\u4f8b\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PMRL\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PMRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u951a\u70b9\u4f9d\u8d56\u3001\u7a33\u5b9a\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.17174", "pdf": "https://arxiv.org/pdf/2507.17174", "abs": "https://arxiv.org/abs/2507.17174", "authors": ["Myeongwon Jung", "Takanori Fujiwara", "Jaemin Jo"], "title": "GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP", "categories": ["cs.GR", "cs.HC", "cs.LG"], "comment": null, "summary": "Despite the widespread use of Uniform Manifold Approximation and Projection\n(UMAP), the impact of its stochastic optimization process on the results\nremains underexplored. We observed that it often produces unstable results\nwhere the projections of data points are determined mostly by chance rather\nthan reflecting neighboring structures. To address this limitation, we\nintroduce (r,d)-stability to UMAP: a framework that analyzes the stochastic\npositioning of data points in the projection space. To assess how stochastic\nelements, specifically initial projection positions and negative sampling,\nimpact UMAP results, we introduce \"ghosts\", or duplicates of data points\nrepresenting potential positional variations due to stochasticity. We define a\ndata point's projection as (r,d)-stable if its ghosts perturbed within a circle\nof radius r in the initial projection remain confined within a circle of radius\nd for their final positions. To efficiently compute the ghost projections, we\ndevelop an adaptive dropping scheme that reduces a runtime up to 60% compared\nto an unoptimized baseline while maintaining approximately 90% of unstable\npoints. We also present a visualization tool that supports the interactive\nexploration of the (r,d)-stability of data points. Finally, we demonstrate the\neffectiveness of our framework by examining the stability of projections of\nreal-world datasets and present usage guidelines for the effective use of our\nframework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u8bc4\u4f30UMAP\u968f\u673a\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6570\u636e\u70b9\u6295\u5f71\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "motivation": "UMAP\u7684\u968f\u673a\u4f18\u5316\u8fc7\u7a0b\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u4f5c\u8005\u53d1\u73b0\u5176\u6295\u5f71\u7ed3\u679c\u5e38\u56e0\u968f\u673a\u6027\u800c\u4e0d\u7a33\u5b9a\u3002", "method": "\u5f15\u5165(r,d)-\u7a33\u5b9a\u6027\u6982\u5ff5\u548c\u201c\u5e7d\u7075\u201d\u70b9\u6765\u6a21\u62df\u968f\u673a\u6027\u5f71\u54cd\uff0c\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u4e22\u5f03\u65b9\u6848\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\uff0c\u80fd\u591f\u8bc6\u522b\u4e0d\u7a33\u5b9a\u70b9\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u89e3\u51b3UMAP\u968f\u673a\u6027\u95ee\u9898\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f7f\u7528\u6307\u5357\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002"}}
{"id": "2507.17120", "pdf": "https://arxiv.org/pdf/2507.17120", "abs": "https://arxiv.org/abs/2507.17120", "authors": ["Wanyi Zheng", "Minxian Xu", "Shengye Song", "Kejiang Ye"], "title": "BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving", "categories": ["cs.DC", "cs.AI"], "comment": "9 pages", "summary": "Large language models (LLMs) have become increasingly popular in various\nareas, traditional business gradually shifting from rule-based systems to\nLLM-based solutions. However, the inference of LLMs is resource-intensive or\nlatency-sensitive, posing significant challenges for serving systems. Existing\nLLM serving systems often use static or continuous batching strategies, which\ncan lead to inefficient GPU memory utilization and increased latency,\nespecially under heterogeneous workloads. These methods may also struggle to\nadapt to dynamic workload fluctuations, resulting in suboptimal throughput and\npotential service level objective (SLO) violations. In this paper, we introduce\nBucketServe, a bucket-based dynamic batching framework designed to optimize LLM\ninference performance. By grouping requests into size-homogeneous buckets based\non sequence length, BucketServe minimizes padding overhead and optimizes GPU\nmemory usage through real-time batch size adjustments preventing out-of-memory\n(OOM) errors. It introduces adaptive bucket splitting/merging and\npriority-aware scheduling to mitigate resource fragmentation and ensure SLO\ncompliance. Experiment shows that BucketServe significantly outperforms UELLM\nin throughput, achieving up to 3.58x improvement. It can also handle 1.93x more\nrequest load under the SLO attainment of 80% compared with DistServe and\ndemonstrates 1.975x higher system load capacity compared to the UELLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBucketServe\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u6876\u548c\u4f18\u5148\u7ea7\u8c03\u5ea6\u4f18\u5316LLM\u63a8\u7406\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u7cfb\u7edf\u8d1f\u8f7d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u5728\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b58\u5728GPU\u5185\u5b58\u5229\u7528\u4e0d\u8db3\u548c\u5ef6\u8fdf\u589e\u52a0\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\u3002", "method": "BucketServe\u901a\u8fc7\u6309\u5e8f\u5217\u957f\u5ea6\u5206\u7ec4\u8bf7\u6c42\u3001\u52a8\u6001\u8c03\u6574\u6279\u5927\u5c0f\u53ca\u4f18\u5148\u7ea7\u8c03\u5ea6\u6765\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBucketServe\u541e\u5410\u91cf\u63d0\u53473.58\u500d\uff0c\u8d1f\u8f7d\u80fd\u529b\u63d0\u9ad81.975\u500d\uff0c\u4f18\u4e8eUELLM\u548cDistServe\u3002", "conclusion": "BucketServe\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.17403", "pdf": "https://arxiv.org/pdf/2507.17403", "abs": "https://arxiv.org/abs/2507.17403", "authors": ["Alice Le Bihan", "Felix Flentge", "Juan A. Fraire"], "title": "Custody Transfer and Compressed Status Reporting for Bundle Protocol Version 7", "categories": ["cs.NI"], "comment": null, "summary": "As space missions increase, there is a growing need to replace point-to-point\ncommunication with an efficient and reliable network-centric communication\napproach. Disruption/Delay Tolerant Networking (DTN) with the Bundle Protocol\n(BP) has been selected as an interoperable network protocol in the LunaNet\nInteroperability Specification. It is also considered for future Earth\nObservation and Mars communication scenarios. In a DTN, the \"bundle\" -- the\nfundamental data unit of BP -- requires dedicated mechanisms to ensure\nreliability due to the challenges posed by intermittent connectivity and long\ndelays. The previous version of BP, BPv6, contained a mechanism for reliable\ntransfer between \"custodial nodes\" called \"custody transfer\". However, this\napproach has been removed from the core protocol specification for BPv7, which\nrequires a corresponding BP reliability extension to be defined separately.\nThis paper introduces a new custody transfer process for BPv7 (expected to be\npublished by CCSDS as an experimental specification in 2025). The core features\nof this new custody transfer method for BPv7 are: (1) A strategy to efficiently\nidentify sets of bundles by sequence numbering (2) A new Custody Transfer\nExtension Block and a corresponding administrative record, Compressed Custody\nSignal, to efficiently report on the acceptance or rejection of custody using\nsequence numbering (3) A new Compressed Reporting Extension Block requesting\nreporting on bundle processing steps using a corresponding administrative\nrecord with sequence numbering for efficiency. The paper will describe those\nconcepts and their design, specification, and implementation in detail. These\nmechanisms have been prototyped in the ESA BP implementation and tested in\nEarth Observation and Lunar communication simulation scenarios. The results\nwill be presented, as will an outlook on future work in the DTN reliable\ntransfer domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684BPv7\u4fdd\u7ba1\u8f6c\u79fb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u95f4\u6b47\u8fde\u63a5\u548c\u957f\u5ef6\u8fdf\u5e26\u6765\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u592a\u7a7a\u4efb\u52a1\u7684\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u7f51\u7edc\u4e2d\u5fc3\u901a\u4fe1\u65b9\u6cd5\u66ff\u4ee3\u70b9\u5bf9\u70b9\u901a\u4fe1\u3002", "method": "\u4ecb\u7ecd\u4e86BPv7\u7684\u65b0\u4fdd\u7ba1\u8f6c\u79fb\u6d41\u7a0b\uff0c\u5305\u62ec\u5e8f\u5217\u53f7\u7b56\u7565\u3001\u65b0\u7684\u4fdd\u7ba1\u8f6c\u79fb\u6269\u5c55\u5757\u548c\u538b\u7f29\u62a5\u544a\u6269\u5c55\u5757\u3002", "result": "\u8fd9\u4e9b\u673a\u5236\u5df2\u5728ESA BP\u5b9e\u73b0\u4e2d\u8fdb\u884c\u4e86\u539f\u578b\u8bbe\u8ba1\uff0c\u5e76\u5728\u5730\u7403\u89c2\u6d4b\u548c\u6708\u7403\u901a\u4fe1\u6a21\u62df\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u4e3aDTN\u53ef\u9760\u4f20\u8f93\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2507.17647", "pdf": "https://arxiv.org/pdf/2507.17647", "abs": "https://arxiv.org/abs/2507.17647", "authors": ["Manuel Widmoser", "Daniel Kocher", "Nikolaus Augsten"], "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory", "categories": ["cs.DB"], "comment": null, "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0f\u5185\u5b58\u7684\u6269\u5c55HNSW\u7d22\u5f15\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u5206\u533a\u65b9\u6cd5\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u6548\u7f13\u5b58\u673a\u5236\u514b\u670d\u7f51\u7edc\u5e26\u5bbd\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9ad8\u7ef4\u5411\u91cf\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5355\u673aHNSW\u7684\u7cbe\u5ea6\u3002", "method": "\u6784\u5efa\u56fe\u4fdd\u7559\u7d22\u5f15\uff0c\u7ed3\u5408\u9ad8\u6548\u7f13\u5b58\u673a\u5236\u548c\u903b\u8f91\u7f13\u5b58\u7ec4\u5408\uff0c\u63d0\u5347\u6574\u4f53\u7f13\u5b58\u6548\u679c\u3002", "result": "\u5728\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8fbe\u5230\u4e0e\u5355\u673aHNSW\u76f8\u540c\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u5185\u5b58\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684ANN\u641c\u7d22\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u3002"}}
{"id": "2507.17049", "pdf": "https://arxiv.org/pdf/2507.17049", "abs": "https://arxiv.org/abs/2507.17049", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "categories": ["cs.SE", "cs.RO"], "comment": null, "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9VLA\u6a21\u578b\u7684\u516b\u79cd\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u548c\u4e94\u79cd\u8d28\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u8d28\u91cf\u548c\u6a21\u578b\u81ea\u4fe1\u5ea6\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6307\u6807\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u6709\u4e2d\u5ea6\u5230\u5f3a\u76f8\u5173\u6027\uff0c\u6311\u6218\u4e86\u4ec5\u4f9d\u8d56\u4e8c\u5143\u6210\u529f\u7387\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u7684\u8bc4\u4f30\u4ec5\u4f9d\u8d56\u4efb\u52a1\u6210\u529f\u7387\uff0c\u65e0\u6cd5\u6355\u6349\u4efb\u52a1\u6267\u884c\u8d28\u91cf\u548c\u6a21\u578b\u81ea\u4fe1\u5ea6\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u6307\u6807\u4ee5\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u516b\u79cd\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u548c\u4e94\u79cd\u8d28\u91cf\u6307\u6807\uff0c\u901a\u8fc7\u5728908\u6b21\u6210\u529f\u4efb\u52a1\u4e2d\u5e94\u7528\u4e09\u79cdVLA\u6a21\u578b\uff0c\u7ed3\u5408\u4e13\u5bb6\u4eba\u5de5\u6807\u6ce8\u5206\u6790\u6307\u6807\u4e0e\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u3002", "result": "\u591a\u4e2a\u6307\u6807\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u6709\u4e2d\u5ea6\u5230\u5f3a\u76f8\u5173\u6027\uff0c\u90e8\u5206\u6307\u6807\u8fd8\u80fd\u533a\u5206\u4e0d\u540c\u8d28\u91cf\u7684\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u65b0\u6307\u6807\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30VLA\u6a21\u578b\uff0c\u4e3a\u5b9e\u65f6\u76d1\u63a7\u548c\u9002\u5e94\u6027\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.17402", "pdf": "https://arxiv.org/pdf/2507.17402", "abs": "https://arxiv.org/abs/2507.17402", "authors": ["Li Jun", "Wang Jinpeng", "Tan Chaolei", "Lian Niu", "Chen Long", "Zhang Min", "Wang Yaowei", "Xia Shu-Tao", "Chen Bin"], "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "Accepted by ICCV'25. 13 pages, 6 figures, 4 tables", "summary": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.", "AI": {"tldr": "HLFormer\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u66f2\u7a7a\u95f4\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u4e2d\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u51e0\u4f55\u626d\u66f2\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u7a7a\u95f4\u7f16\u7801\u548c\u52a8\u6001\u7279\u5f81\u878d\u5408\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5b58\u5728\u51e0\u4f55\u626d\u66f2\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u89c6\u9891\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5bfc\u81f4\u65f6\u95f4\u5efa\u6a21\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faHLFormer\u6846\u67b6\uff0c\u7ed3\u5408\u6d1b\u4f26\u5179\u6ce8\u610f\u529b\u5757\u548c\u6b27\u51e0\u91cc\u5f97\u6ce8\u610f\u529b\u5757\u8fdb\u884c\u6df7\u5408\u7a7a\u95f4\u7f16\u7801\uff0c\u5e76\u5f15\u5165\u5747\u503c\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u4ea4\u4e92\u6a21\u5757\u548c\u504f\u5e8f\u4fdd\u6301\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHLFormer\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u53cc\u66f2\u7a7a\u95f4\u5efa\u6a21\u80fd\u6709\u6548\u63d0\u5347\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u6df7\u5408\u7a7a\u95f4\u7f16\u7801\u548c\u52a8\u6001\u878d\u5408\u662f\u5173\u952e\u3002"}}
{"id": "2507.17184", "pdf": "https://arxiv.org/pdf/2507.17184", "abs": "https://arxiv.org/abs/2507.17184", "authors": ["Hui Zhao"], "title": "A Scientist Question: Research on the Impact of Super Structured Quadrilateral Meshes on Convergence and Accuracy of Finite Element Analysis", "categories": ["cs.GR", "cs.NA", "math.NA"], "comment": "in Chinese and English", "summary": "In the current practices of both industry and academia, the convergence and\naccuracy of finite element calculations are closely related to the methods and\nquality of mesh generation. For years, the research on high-quality mesh\ngeneration in the domestic academic field has mainly referred to the local\nquality of quadrilaterals and hexahedrons approximating that of squares and\ncubes. The main contribution of this paper is to propose a brand-new research\ndirection and content: it is necessary to explore and study the influence of\nthe overall global arrangement structure and pattern of super structured\nquadrilateral meshes on the convergence and calculation accuracy of finite\nelement calculations. Through the research in this new field, it can help solve\nthe non-rigorous state of serious reliance on \"experience\" in the mesh\ngeneration stage during simulation in the current industry and academia, and\nmake clear judgments on which global arrangements of mesh generation can ensure\nthe convergence of finite element calculations. In order to generate and design\nsuper-structured quadrilateral meshes with controllable overall arrangement\nstructures, a large number of modern two-dimensional and three-dimensional\ngeometric topology theories are required, such as moduli space, Teichm\\\"uller\nspace, harmonic foliations, dynamical systems, surface mappings, meromorphic\nquadratic differentials, surface mappings, etc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a\u7814\u7a76\u8d85\u7ed3\u6784\u5316\u56db\u8fb9\u5f62\u7f51\u683c\u7684\u6574\u4f53\u5168\u5c40\u6392\u5217\u7ed3\u6784\u5bf9\u6709\u9650\u5143\u8ba1\u7b97\u6536\u655b\u6027\u548c\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u4f9d\u8d56\u7ecf\u9a8c\u7684\u975e\u4e25\u8c28\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5728\u6a21\u62df\u8fc7\u7a0b\u4e2d\u4e25\u91cd\u4f9d\u8d56\u7ecf\u9a8c\u7684\u975e\u4e25\u8c28\u72b6\u6001\uff0c\u9700\u660e\u786e\u54ea\u79cd\u7f51\u683c\u5168\u5c40\u6392\u5217\u80fd\u786e\u4fdd\u6709\u9650\u5143\u8ba1\u7b97\u6536\u655b\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u73b0\u4ee3\u4e8c\u7ef4\u548c\u4e09\u7ef4\u51e0\u4f55\u62d3\u6251\u7406\u8bba\uff08\u5982\u6a21\u7a7a\u95f4\u3001Teichm\u00fcller\u7a7a\u95f4\u7b49\uff09\u751f\u6210\u53ef\u63a7\u6574\u4f53\u6392\u5217\u7684\u8d85\u7ed3\u6784\u5316\u56db\u8fb9\u5f62\u7f51\u683c\u3002", "result": "\u65b0\u7814\u7a76\u65b9\u5411\u53ef\u7cfb\u7edf\u6027\u63a2\u7d22\u7f51\u683c\u6574\u4f53\u7ed3\u6784\u5bf9\u8ba1\u7b97\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6709\u9650\u5143\u8ba1\u7b97\u4e2d\u7684\u7f51\u683c\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6709\u671b\u63d0\u5347\u6536\u655b\u6027\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2507.17128", "pdf": "https://arxiv.org/pdf/2507.17128", "abs": "https://arxiv.org/abs/2507.17128", "authors": ["Minxian Xu", "Linfeng Wen", "Junhan Liao", "Huaming Wu", "Kejiang Ye", "Chengzhong Xu"], "title": "Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy", "categories": ["cs.DC"], "comment": "14 pages", "summary": "The interactions within cloud-native applications are complex, with a\nconstantly changing number of services and loads, posing higher demands on\nauto-scaling approach. This mainly involves several challenges such as\nmicroservices dependency analysis, performance profiling, anomaly detection,\nworkload characterization and task co-location. Therefore, some advanced\nalgorithms have been investigated into auto-scaling cloud-native applications\nto optimize system and application performance. These algorithms can learn from\nhistorical data and appropriately adjust resource allocation based on the\ncurrent environment and load conditions to optimize resource utilization and\nsystem performance. In this paper, we systematically review the literature on\nstate-of-the-art auto-scaling approaches for cloud-native applications from\n2020, and further explore the technological evolution. Additionally, we propose\na detailed taxonomy to categorize current research from five perspectives,\nincluding infrastructure, architecture, scaling methods, optimization\nobjectives, and behavior modeling. Then, we provide a comprehensive comparison\nand in-depth discussion of the key features, advantages, limitations, and\napplication scenarios of each approach, considering their performance in\ndiverse environments and under various conditions. Finally, we summarize the\ncurrent state of research in this field, identify the gaps and unresolved\nchallenges, and emphasize promising directions for future exploration,\nparticularly in areas such as the application of large models, microservice\ndependency management, and the use of meta-learning techniques to enhance model\napplicability and adaptability across different environments.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e862020\u5e74\u4ee5\u6765\u4e91\u539f\u751f\u5e94\u7528\u81ea\u52a8\u6269\u5c55\u7684\u524d\u6cbf\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4ece\u57fa\u7840\u8bbe\u65bd\u3001\u67b6\u6784\u3001\u6269\u5c55\u65b9\u6cd5\u3001\u4f18\u5316\u76ee\u6807\u548c\u884c\u4e3a\u5efa\u6a21\u4e94\u4e2a\u89d2\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u8ba8\u8bba\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u548c\u5e94\u7528\u573a\u666f\uff0c\u6700\u540e\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u4e91\u539f\u751f\u5e94\u7528\u4ea4\u4e92\u590d\u6742\u6027\u548c\u8d1f\u8f7d\u53d8\u5316\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u81ea\u52a8\u6269\u5c55\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7b97\u6cd5\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4ece\u4e94\u4e2a\u89d2\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5bf9\u5404\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u548c\u6df1\u5165\u8ba8\u8bba\u3002", "result": "\u603b\u7ed3\u5404\u7c7b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u548c\u5e94\u7528\u573a\u666f\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u548c\u672a\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5927\u6a21\u578b\u5e94\u7528\u3001\u5fae\u670d\u52a1\u4f9d\u8d56\u7ba1\u7406\u548c\u5143\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2507.17064", "pdf": "https://arxiv.org/pdf/2507.17064", "abs": "https://arxiv.org/abs/2507.17064", "authors": ["Nafisa Anjum", "Tasnuva Farheen"], "title": "SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "With the advent of modern technology, critical infrastructure,\ncommunications, and national security depend increasingly on space-based\nassets. These assets, along with associated assets like data relay systems and\nground stations, are, therefore, in serious danger of cyberattacks. Strong\nsecurity defenses are essential to ensure data integrity, maintain secure\noperations, and protect assets in space and on the ground against various\nthreats. Previous research has found discrete vulnerabilities in space systems\nand suggested specific solutions to address them. Such research has yielded\nvaluable insights, but lacks a thorough examination of space cyberattack\nvectors and a rigorous assessment of the efficacy of mitigation techniques.\nThis study tackles this issue by taking a comprehensive approach to analyze the\nrange of possible space cyber-attack vectors, which include ground, space,\nsatellite, and satellite constellations. In order to address the particular\nthreats, the study also assesses the efficacy of mitigation measures that are\nlinked with space infrastructures and proposes a Risk Scoring Framework. Based\non the analysis, this paper identifies potential research challenges for\ndeveloping and testing cutting-edge technology solutions, encouraging robust\ncybersecurity measures needed in space.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5168\u9762\u5206\u6790\u4e86\u7a7a\u95f4\u7f51\u7edc\u653b\u51fb\u8f7d\u4f53\u53ca\u7f13\u89e3\u63aa\u65bd\uff0c\u63d0\u51fa\u4e86\u98ce\u9669\u8bc4\u5206\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u73b0\u4ee3\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7a7a\u95f4\u8d44\u4ea7\u5bf9\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u548c\u56fd\u5bb6\u5b89\u5168\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u5176\u9762\u4e34\u4e25\u91cd\u7684\u7f51\u7edc\u653b\u51fb\u5a01\u80c1\uff0c\u9700\u8981\u5f3a\u5316\u5b89\u5168\u9632\u5fa1\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7efc\u5408\u65b9\u6cd5\u5206\u6790\u4e86\u5730\u9762\u3001\u7a7a\u95f4\u3001\u536b\u661f\u53ca\u536b\u661f\u661f\u5ea7\u7b49\u53ef\u80fd\u7684\u7f51\u7edc\u653b\u51fb\u8f7d\u4f53\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u7f13\u89e3\u63aa\u65bd\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u98ce\u9669\u8bc4\u5206\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u672a\u6765\u5f00\u53d1\u4e0e\u6d4b\u8bd5\u5148\u8fdb\u6280\u672f\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u5728\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7a7a\u95f4\u7f51\u7edc\u5b89\u5168\u63aa\u65bd\u7684\u7d27\u8feb\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.17209", "pdf": "https://arxiv.org/pdf/2507.17209", "abs": "https://arxiv.org/abs/2507.17209", "authors": ["Haoran Jiang", "Shaohan Shi", "Yunjie Yao", "Chang Jiang", "Quan Li"], "title": "HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Modern scientific discovery faces growing challenges in integrating vast and\nheterogeneous knowledge critical to breakthroughs in biomedicine and drug\ndevelopment. Traditional hypothesis-driven research, though effective, is\nconstrained by human cognitive limits, the complexity of biological systems,\nand the high cost of trial-and-error experimentation. Deep learning models,\nespecially graph neural networks (GNNs), have accelerated prediction\ngeneration, but the sheer volume of outputs makes manual selection for\nvalidation unscalable. Large language models (LLMs) offer promise in filtering\nand hypothesis generation, yet suffer from hallucinations and lack grounding in\nstructured knowledge, limiting their reliability. To address these issues, we\npropose HypoChainer, a collaborative visualization framework that integrates\nhuman expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance\nhypothesis generation and validation. HypoChainer operates in three stages:\nFirst, exploration and contextualization -- experts use retrieval-augmented\nLLMs (RAGs) and dimensionality reduction to navigate large-scale GNN\npredictions, assisted by interactive explanations. Second, hypothesis chain\nformation -- experts iteratively examine KG relationships around predictions\nand semantically linked entities, refining hypotheses with LLM and KG\nsuggestions. Third, validation prioritization -- refined hypotheses are\nfiltered based on KG-supported evidence to identify high-priority candidates\nfor experimentation, with visual analytics further strengthening weak links in\nreasoning. We demonstrate HypoChainer's effectiveness through case studies in\ntwo domains and expert interviews, highlighting its potential to support\ninterpretable, scalable, and knowledge-grounded scientific discovery.", "AI": {"tldr": "\u63d0\u51faHypoChainer\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u3001LLM\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f18\u5316\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u5047\u8bbe\u751f\u6210\u4e0e\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u53d7\u9650\u4e8e\u8ba4\u77e5\u548c\u6210\u672c\uff0c\u800c\u73b0\u6709AI\u5de5\u5177\u6709\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a\u63a2\u7d22\u4e0e\u60c5\u5883\u5316\u3001\u5047\u8bbe\u94fe\u5f62\u6210\u3001\u9a8c\u8bc1\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u878d\u5408LLM\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u548c\u4e13\u5bb6\u8bbf\u8c08\u8bc1\u660e\u5176\u6709\u6548\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u79d1\u5b66\u53d1\u73b0\u3002", "conclusion": "HypoChainer\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2507.17241", "pdf": "https://arxiv.org/pdf/2507.17241", "abs": "https://arxiv.org/abs/2507.17241", "authors": ["Mattia Sabella", "Monica Vitali"], "title": "Eco-Friendly AI: Unleashing Data Power for Green Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DC"], "comment": null, "summary": "The widespread adoption of Artificial Intelligence (AI) and Machine Learning\n(ML) comes with a significant environmental impact, particularly in terms of\nenergy consumption and carbon emissions. This pressing issue highlights the\nneed for innovative solutions to mitigate AI's ecological footprint. One of the\nkey factors influencing the energy consumption of ML model training is the size\nof the training dataset. ML models are often trained on vast amounts of data\ncontinuously generated by sensors and devices distributed across multiple\nlocations. To reduce data transmission costs and enhance privacy, Federated\nLearning (FL) enables model training without the need to move or share raw\ndata. While FL offers these advantages, it also introduces challenges due to\nthe heterogeneity of data sources (related to volume and quality),\ncomputational node capabilities, and environmental impact.\n  This paper contributes to the advancement of Green AI by proposing a\ndata-centric approach to Green Federated Learning. Specifically, we focus on\nreducing FL's environmental impact by minimizing the volume of training data.\nOur methodology involves the analysis of the characteristics of federated\ndatasets, the selecting of an optimal subset of data based on quality metrics,\nand the choice of the federated nodes with the lowest environmental impact. We\ndevelop a comprehensive methodology that examines the influence of data-centric\nfactors, such as data quality and volume, on FL training performance and carbon\nemissions. Building on these insights, we introduce an interactive\nrecommendation system that optimizes FL configurations through data reduction,\nminimizing environmental impact during training. Applying this methodology to\ntime series classification has demonstrated promising results in reducing the\nenvironmental impact of FL tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u4e2d\u5fc3\u7684\u7eff\u8272\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u91cf\u6765\u964d\u4f4e\u8054\u90a6\u5b66\u4e60\u7684\u73af\u5883\u5f71\u54cd\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u7684\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u4e86\u663e\u8457\u7684\u73af\u5883\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u65b9\u9762\u3002\u8054\u90a6\u5b66\u4e60\u867d\u964d\u4f4e\u4e86\u6570\u636e\u4f20\u8f93\u6210\u672c\u5e76\u589e\u5f3a\u9690\u79c1\uff0c\u4f46\u5176\u6570\u636e\u6e90\u7684\u5f02\u8d28\u6027\u548c\u73af\u5883\u95ee\u9898\u4ecd\u9700\u89e3\u51b3\u3002", "method": "\u5206\u6790\u8054\u90a6\u6570\u636e\u96c6\u7684\u7279\u5f81\uff0c\u57fa\u4e8e\u8d28\u91cf\u6307\u6807\u9009\u62e9\u6700\u4f18\u6570\u636e\u5b50\u96c6\uff0c\u5e76\u9009\u62e9\u73af\u5883\u5f71\u54cd\u6700\u5c0f\u7684\u8054\u90a6\u8282\u70b9\uff0c\u5f00\u53d1\u63a8\u8350\u7cfb\u7edf\u4f18\u5316\u914d\u7f6e\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u4efb\u52a1\u7684\u73af\u5883\u5f71\u54cd\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u7684\u7eff\u8272\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u73af\u5883\u5f71\u54cd\uff0c\u4e3a\u7eff\u8272AI\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.17093", "pdf": "https://arxiv.org/pdf/2507.17093", "abs": "https://arxiv.org/abs/2507.17093", "authors": ["Danushka Liyanage", "Nelum Attanayake", "Zijian Luo", "Rahul Gopinath"], "title": "Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing", "categories": ["cs.SE", "68N30", "D.2.4; D.2.5; D.2.8"], "comment": "ICSME'25 Registered Report", "summary": "Background: Fuzzers are often guided by coverage, making the estimation of\nmaximum achievable coverage a key concern in fuzzing. However, achieving 100%\ncoverage is infeasible for most real-world software systems, regardless of\neffort. While static reachability analysis can provide an upper bound, it is\noften highly inaccurate. Recently, statistical estimation methods based on\nspecies richness estimators from biostatistics have been proposed as a\npotential solution. Yet, the lack of reliable benchmarks with labeled ground\ntruth has limited rigorous evaluation of their accuracy.\n  Objective: This work examines the reliability of reachability estimators from\ntwo axes: addressing the lack of labeled ground truth and evaluating their\nreliability on real-world programs.\n  Methods: (1) To address the challenge of labeled ground truth, we propose an\nevaluation framework that synthetically generates large programs with complex\ncontrol flows, ensuring well-defined reachability and providing ground truth\nfor evaluation. (2) To address the criticism from use of synthetic benchmarks,\nwe adapt a reliability check for reachability estimators on real-world\nbenchmarks without labeled ground truth -- by varying the size of sampling\nunits, which, in theory, should not affect the estimate.\n  Results: These two studies together will help answer the question of whether\ncurrent reachability estimators are reliable, and defines a protocol to\nevaluate future improvements in reachability estimation.", "AI": {"tldr": "\u8bc4\u4f30\u57fa\u4e8e\u751f\u7269\u7edf\u8ba1\u5b66\u7269\u79cd\u4e30\u5bcc\u5ea6\u4f30\u8ba1\u5668\u7684\u53ef\u8fbe\u6027\u4f30\u8ba1\u65b9\u6cd5\u5728\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7531\u4e8e\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u53ef\u8fbe\u6027\u4f30\u8ba1\u7684\u91cd\u8981\u6027\u53ca\u7f3a\u4e4f\u53ef\u9760\u57fa\u51c6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u73b0\u6709\u4f30\u8ba1\u5668\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5408\u6210\u7a0b\u5e8f\u6846\u67b6\u751f\u6210\u771f\u5b9e\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u53ef\u9760\u6027\u68c0\u67e5\u65b9\u6cd5\u6d4b\u8bd5\u5b9e\u9645\u7a0b\u5e8f\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u5f53\u524d\u4f30\u8ba1\u5668\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5236\u5b9a\u4e86\u8bc4\u4f30\u672a\u6765\u6539\u8fdb\u7684\u534f\u8bae\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u4e0e\u5b9e\u9645\u7a0b\u5e8f\u7684\u53cc\u91cd\u9a8c\u8bc1\uff0c\u4e3a\u53ef\u8fbe\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.17265", "pdf": "https://arxiv.org/pdf/2507.17265", "abs": "https://arxiv.org/abs/2507.17265", "authors": ["Xin Chen", "Yunhai Wang", "Huaiwei Bao", "Kecheng Lu", "Jaemin Jo", "Chi-Wing Fu", "Jean-Daniel Fekete"], "title": "Visualization-Driven Illumination for Density Plots", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "We present a novel visualization-driven illumination model for density plots,\na new technique to enhance density plots by effectively revealing the detailed\nstructures in high- and medium-density regions and outliers in low-density\nregions, while avoiding artifacts in the density field's colors. When\nvisualizing large and dense discrete point samples, scatterplots and dot\ndensity maps often suffer from overplotting, and density plots are commonly\nemployed to provide aggregated views while revealing underlying structures.\nYet, in such density plots, existing illumination models may produce color\ndistortion and hide details in low-density regions, making it challenging to\nlook up density values, compare them, and find outliers. The key novelty in\nthis work includes (i) a visualization-driven illumination model that\ninherently supports density-plot-specific analysis tasks and (ii) a new image\ncomposition technique to reduce the interference between the image shading and\nthe color-encoded density values. To demonstrate the effectiveness of our\ntechnique, we conducted a quantitative study, an empirical evaluation of our\ntechnique in a controlled study, and two case studies, exploring twelve\ndatasets with up to two million data point samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89c6\u5316\u9a71\u52a8\u5149\u7167\u6a21\u578b\uff0c\u7528\u4e8e\u589e\u5f3a\u5bc6\u5ea6\u56fe\u7684\u7ec6\u8282\u663e\u793a\uff0c\u540c\u65f6\u907f\u514d\u989c\u8272\u5931\u771f\u3002", "motivation": "\u73b0\u6709\u5bc6\u5ea6\u56fe\u7684\u5149\u7167\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u989c\u8272\u5931\u771f\u5e76\u9690\u85cf\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u7ec6\u8282\uff0c\u5f71\u54cd\u5bc6\u5ea6\u503c\u7684\u67e5\u627e\u3001\u6bd4\u8f83\u548c\u5f02\u5e38\u503c\u68c0\u6d4b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u9a71\u52a8\u7684\u5149\u7167\u6a21\u578b\u548c\u65b0\u7684\u56fe\u50cf\u5408\u6210\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u56fe\u50cf\u9634\u5f71\u4e0e\u989c\u8272\u7f16\u7801\u5bc6\u5ea6\u503c\u4e4b\u95f4\u7684\u5e72\u6270\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u7814\u7a76\u3001\u5b9e\u8bc1\u8bc4\u4f30\u548c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6d4b\u8bd5\u4e86\u591a\u8fbe200\u4e07\u4e2a\u6570\u636e\u70b9\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u5ea6\u56fe\u7684\u7ec6\u8282\u663e\u793a\u548c\u5f02\u5e38\u503c\u68c0\u6d4b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u53ef\u89c6\u5316\u3002"}}
{"id": "2507.17133", "pdf": "https://arxiv.org/pdf/2507.17133", "abs": "https://arxiv.org/abs/2507.17133", "authors": ["Jianmin Hu", "Minxian Xu", "Kejiang Ye", "Chengzhong Xu"], "title": "BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs", "categories": ["cs.DC"], "comment": "12 pages", "summary": "In recent years, the Mixture-of-Experts (MoE) architecture has been widely\napplied to large language models (LLMs), providing a promising solution that\nactivates only a subset of the model's parameters during computation, thereby\nreducing overall memory requirements and allowing for faster inference compared\nto dense models. Despite these advantages, existing systems still face issues\nof low efficiency due to static model placement and lack of dynamic workloads\nadaptation. This leads to suboptimal resource utilization and increased\nlatency, especially during bursty requests periods.\n  To address these challenges, this paper introduces BrownoutServe, a novel\nserving framework designed to optimize inference efficiency and maintain\nservice reliability for MoE-based LLMs under dynamic computational demands and\ntraffic conditions. BrownoutServe introduces \"united experts\" that integrate\nknowledge from multiple experts, reducing the times of expert access and\ninference latency. Additionally, it proposes a dynamic brownout mechanism to\nadaptively adjust the processing of certain tokens, optimizing inference\nperformance while guaranteeing service level objectives (SLOs) are met. Our\nevaluations show the effectiveness of BrownoutServe under various workloads: it\nachieves up to 2.07x throughput improvement compared to vLLM and reduces SLO\nviolations by 90.28%, showcasing its robustness under bursty traffic while\nmaintaining acceptable inference accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86BrownoutServe\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u548c\u8054\u5408\u4e13\u5bb6\u4f18\u5316MoE\u67b6\u6784LLM\u7684\u63a8\u7406\u6548\u7387\u548c\u670d\u52a1\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MoE\u67b6\u6784LLM\u7cfb\u7edf\u4e2d\u7531\u4e8e\u9759\u6001\u6a21\u578b\u653e\u7f6e\u548c\u7f3a\u4e4f\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u9002\u5e94\u800c\u5bfc\u81f4\u7684\u4f4e\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u5f15\u5165\u8054\u5408\u4e13\u5bb6\uff08united experts\uff09\u4ee5\u51cf\u5c11\u4e13\u5bb6\u8bbf\u95ee\u6b21\u6570\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u964d\u8f7d\u673a\u5236\uff08dynamic brownout mechanism\uff09\u81ea\u9002\u5e94\u8c03\u6574\u5904\u7406\u3002", "result": "\u5728\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0cBrownoutServe\u5b9e\u73b0\u4e86\u6bd4vLLM\u9ad82.07\u500d\u7684\u541e\u5410\u91cf\uff0c\u5e76\u5c06SLO\u8fdd\u89c4\u51cf\u5c11\u4e8690.28%\u3002", "conclusion": "BrownoutServe\u663e\u8457\u63d0\u5347\u4e86MoE\u67b6\u6784LLM\u7684\u63a8\u7406\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u7a81\u53d1\u6d41\u91cf\u573a\u666f\u3002"}}
{"id": "2507.17218", "pdf": "https://arxiv.org/pdf/2507.17218", "abs": "https://arxiv.org/abs/2507.17218", "authors": ["Yang Ouyang", "Yuchen Wu", "Xiyuan Wang", "Laixin Xie", "Weicong Cheng", "Jianping Gan", "Quan Li", "Xiaojuan Ma"], "title": "OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena", "categories": ["cs.HC"], "comment": "To appear at the IEEE VIS Conference 2025", "summary": "Communicating the complexity of oceanic phenomena-such as hypoxia and\nacidification-poses a persistent challenge for marine science. Despite advances\nin sensing technologies and computational models, conventional formats like\nstatic visualizations and text-based reports often fall short in conveying the\ndynamics of ocean changes. To address this gap, we present OceanVive, an\nimmersive and interactive visualization system that transforms complex ocean\ndatasets into navigable spatial narratives. OceanVive incorporates an\nexploratory panel on a table-sized tablet for managing immersive content on a\nlarge screen and integrates adaptive visual encodings, contextual storytelling,\nand intuitive navigation pathways to support effective communication. We\nvalidate the system through expert interviews, demonstrating its potential to\nenhance science communication and promote deeper public understanding.", "AI": {"tldr": "OceanVive\u662f\u4e00\u4e2a\u6c89\u6d78\u5f0f\u4ea4\u4e92\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5c06\u590d\u6742\u7684\u6d77\u6d0b\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u5bfc\u822a\u7684\u7a7a\u95f4\u53d9\u4e8b\uff0c\u63d0\u5347\u6d77\u6d0b\u79d1\u5b66\u4f20\u64ad\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u53ef\u89c6\u5316\u548c\u6587\u672c\u62a5\u544a\u96be\u4ee5\u6709\u6548\u4f20\u8fbe\u6d77\u6d0b\u53d8\u5316\u7684\u590d\u6742\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u76f4\u89c2\u7684\u4f20\u64ad\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1OceanVive\u7cfb\u7edf\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u89c6\u89c9\u7f16\u7801\u3001\u4e0a\u4e0b\u6587\u53d9\u4e8b\u548c\u76f4\u89c2\u5bfc\u822a\uff0c\u901a\u8fc7\u684c\u9762\u5e73\u677f\u7ba1\u7406\u5927\u5c4f\u5e55\u5185\u5bb9\u3002", "result": "\u4e13\u5bb6\u8bbf\u8c08\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u5728\u63d0\u5347\u79d1\u5b66\u4f20\u64ad\u548c\u516c\u4f17\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "OceanVive\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u6709\u6548\u89e3\u51b3\u4e86\u6d77\u6d0b\u79d1\u5b66\u4f20\u64ad\u590d\u6742\u6027\u7684\u95ee\u9898\u3002"}}
{"id": "2507.17487", "pdf": "https://arxiv.org/pdf/2507.17487", "abs": "https://arxiv.org/abs/2507.17487", "authors": ["Lorenzo Marconi", "Flavia Ricci", "Riccardo Rosati"], "title": "CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)", "categories": ["cs.AI", "cs.DB"], "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025)", "summary": "We investigate Controlled Query Evaluation (CQE) over ontologies, where\ninformation disclosure is regulated by epistemic dependencies (EDs), a family\nof logical rules recently proposed for the CQE framework. In particular, we\ncombine EDs with the notion of optimal GA censors, i.e. maximal sets of ground\natoms that are entailed by the ontology and can be safely revealed. We focus on\nanswering Boolean unions of conjunctive queries (BUCQs) with respect to the\nintersection of all optimal GA censors - an approach that has been shown in\nother contexts to ensure strong security guarantees with favorable\ncomputational behavior. First, we characterize the security of this\nintersection-based approach and identify a class of EDs (namely, full EDs) for\nwhich it remains safe. Then, for a subclass of EDs and for DL-Lite_R\nontologies, we show that answering BUCQs in the above CQE semantics is in AC^0\nin data complexity by presenting a suitable, detailed first-order rewriting\nalgorithm. Finally, we report on experiments conducted in two different\nevaluation scenarios, showing the practical feasibility of our rewriting\nfunction.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u77e5\u8bc6\u4f9d\u8d56\u6027\u7684\u63a7\u5236\u67e5\u8be2\u8bc4\u4f30\uff0c\u7ed3\u5408\u6700\u4f18GA\u4f20\u611f\u5668\u548cBUCQs\u67e5\u8be2\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u5e76\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u7279\u5b9a\u573a\u666f\u7684\u4e00\u9636\u91cd\u5199\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7a76\u5728\u77e5\u8bc6\u4f9d\u8d56\u6027\u6846\u67b6\u4e0b\u5982\u4f55\u5b89\u5168\u6709\u6548\u5730\u63a7\u5236\u67e5\u8be2\u7ed3\u679c\uff0c\u4ee5\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u5e76\u4f18\u5316\u8ba1\u7b97\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u6700\u4f18GA\u4f20\u611f\u5668\u548c\u5e03\u5c14\u8054\u5408\u67e5\u8be2\uff08BUCQs\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u96c6\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u9636\u91cd\u5199\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728DL-Lite_R\u77e5\u8bc6\u5e93\u4e2d\u7684\u9ad8\u6548\u6027\uff08AC^0\u6570\u636e\u590d\u6742\u5ea6\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u540c\u65f6\u6ee1\u8db3\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u63a7\u5236\u67e5\u8be2\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17165", "pdf": "https://arxiv.org/pdf/2507.17165", "abs": "https://arxiv.org/abs/2507.17165", "authors": ["Taher A. Ghaleb", "Dulina Rathnayake"], "title": "Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations", "categories": ["cs.SE"], "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) services, such as GitHub Actions, require\ndevelopers to write YAML-based configurations, which can be tedious and\nerror-prone. Despite the increasing use of Large Language Models (LLMs) to\nautomate software engineering tasks, their ability to generate CI\nconfigurations remains underexplored. This paper presents a preliminary study\nevaluating six LLMs for generating GitHub Actions configurations from natural\nlanguage descriptions. We assess three general-purpose foundation models\n(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code\nLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,\nconstructed from GitHub Actions documentation, pairing descriptions with\ncorresponding best-practice YAML configurations. Zero-shot prompting achieves\nup to 69% similarity with the ground truth, with only 3% perfect matches.\nCode-pretrained models slightly underperform compared to general-purpose ones\nin YAML-based CI tasks, revealing LLM limitations for CI configuration\ngeneration. Analyzing GPT-4o outputs reveals issues like missing or renamed\nsteps, misinterpreted descriptions, and unnecessary additions that may affect\nstructural and contextual correctness, indicating a gap between generation\nquality and the precision required for executable CI configurations. Our\nresearch offers insights for improving LLM alignment with configuration\nlanguages and guiding future efforts on CI automation and tooling support.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u516d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210GitHub Actions\u914d\u7f6e\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u96f6-shot\u63d0\u793a\u4e0b\u6700\u9ad8\u76f8\u4f3c\u5ea6\u4e3a69%\uff0c\u4f46\u5b8c\u7f8e\u5339\u914d\u7387\u4ec53%\u3002\u4ee3\u7801\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u7565\u900a\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u63ed\u793a\u4e86LLM\u5728CI\u914d\u7f6e\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f00\u53d1\u8005\u7f16\u5199YAML\u914d\u7f6e\u7e41\u7410\u4e14\u6613\u9519\uff0c\u800cLLM\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e86\u516d\u79cdLLM\uff08\u4e09\u79cd\u901a\u7528\u57fa\u7840\u548c\u4e09\u79cd\u4ee3\u7801\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5c06\u63cf\u8ff0\u4e0e\u6700\u4f73\u5b9e\u8df5YAML\u914d\u7f6e\u914d\u5bf9\u3002", "result": "\u96f6-shot\u63d0\u793a\u4e0b\u6700\u9ad8\u76f8\u4f3c\u5ea6\u4e3a69%\uff0c\u5b8c\u7f8e\u5339\u914d\u4ec53%\u3002\u4ee3\u7801\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u7565\u5dee\uff0c\u5b58\u5728\u6b65\u9aa4\u7f3a\u5931\u3001\u8bef\u89e3\u63cf\u8ff0\u7b49\u95ee\u9898\u3002", "conclusion": "LLM\u5728CI\u914d\u7f6e\u751f\u6210\u4e2d\u4ecd\u9700\u6539\u8fdb\u3002\u7814\u7a76\u4e3a\u63d0\u5347LLM\u4e0e\u914d\u7f6e\u8bed\u8a00\u7684\u9002\u914d\u6027\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.17336", "pdf": "https://arxiv.org/pdf/2507.17336", "abs": "https://arxiv.org/abs/2507.17336", "authors": ["Hyeongmin Lee", "Kyungjune Baek"], "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting", "categories": ["cs.GR"], "comment": "21 pages, 10 figures", "summary": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed\nrendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric\nvideos. However, the large number of Gaussians, substantial temporal\nredundancies, and especially the absence of an entropy-aware compression\nframework result in large storage requirements. Consequently, this poses\nsignificant challenges for practical deployment, efficient edge-device\nprocessing, and data transmission. In this paper, we introduce a novel\nend-to-end RD-optimized compression framework tailored for 4DGS, aiming to\nenable flexible, high-fidelity rendering across varied computational platforms.\nLeveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the\nstate-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS\ncompression methods for compatibility while effectively addressing additional\nchallenges introduced by the temporal axis. In particular, instead of storing\nmotion trajectories independently per point, we employ a wavelet transform to\nreflect the real-world smoothness prior, significantly enhancing storage\nefficiency. This approach yields significantly improved compression ratios and\nprovides a user-controlled balance between compression efficiency and rendering\nquality. Extensive experiments demonstrate the effectiveness of our method,\nachieving up to 91x compression compared to the original Ex4DGS model while\nmaintaining high visual fidelity. These results highlight the applicability of\nour framework for real-time dynamic scene rendering in diverse scenarios, from\nresource-constrained edge devices to high-performance environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf94D\u9ad8\u65af\u6cfc\u6e85\u7684\u7aef\u5230\u7aefRD\u4f18\u5316\u538b\u7f29\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b58\u50a8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "4DGS\u5728\u52a8\u6001\u573a\u666f\u6e32\u67d3\u4e2d\u5b58\u50a8\u9700\u6c42\u5927\uff0c\u7f3a\u4e4f\u9ad8\u6548\u7684\u538b\u7f29\u6846\u67b6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u8fb9\u7f18\u8bbe\u5907\u5904\u7406\u3002", "method": "\u57fa\u4e8eEx4DGS\uff0c\u91c7\u7528\u5c0f\u6ce2\u53d8\u6362\u538b\u7f29\u8fd0\u52a8\u8f68\u8ff9\uff0c\u4f18\u5316\u5b58\u50a8\u6548\u7387\uff0c\u5b9e\u73b0\u538b\u7f29\u6548\u7387\u548c\u6e32\u67d3\u8d28\u91cf\u7684\u7528\u6237\u53ef\u63a7\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5b9e\u73b0\u4e8691\u500d\u7684\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4ece\u8fb9\u7f18\u8bbe\u5907\u5230\u9ad8\u6027\u80fd\u73af\u5883\u7684\u5b9e\u65f6\u52a8\u6001\u573a\u666f\u6e32\u67d3\u3002"}}
{"id": "2507.17301", "pdf": "https://arxiv.org/pdf/2507.17301", "abs": "https://arxiv.org/abs/2507.17301", "authors": ["Chi-Wei Chu", "Ding-Yong Hong", "Jan-Jan Wu"], "title": "Efficient Column-Wise N:M Pruning on RISC-V CPU", "categories": ["cs.DC"], "comment": null, "summary": "In deep learning frameworks, weight pruning is a widely used technique for\nimproving computational efficiency by reducing the size of large models. This\nis especially critical for convolutional operators, which often act as\nperformance bottlenecks in convolutional neural networks (CNNs). However, the\neffectiveness of pruning heavily depends on how it is implemented, as different\nmethods can significantly impact both computational performance and memory\nfootprint. In this work, we propose a column-wise N:M pruning strategy applied\nat the tile level and modify XNNPACK to enable efficient execution of pruned\nmodels on the RISC-V vector architecture. Additionally, we propose fusing the\noperations of im2col and data packing to minimize redundant memory accesses and\nmemory overhead. To further optimize performance, we incorporate AITemplate's\nprofiling technique to identify the optimal implementation for each\nconvolutional operator. Our proposed approach effectively increases ResNet\ninference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy\nwithin 2.1\\% of the dense baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5217\u7ea7N:M\u526a\u679d\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u7ed3\u5408XNNPACK\u548cAITemplate\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u5377\u79ef\u64cd\u4f5c\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6027\u80fd\u74f6\u9888\u4e4b\u4e00\uff0c\u526a\u679d\u6280\u672f\u7684\u5b9e\u65bd\u65b9\u5f0f\u5bf9\u5176\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u635f\u8017\u7684\u526a\u679d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u5217\u7ea7N:M\u526a\u679d\u7b56\u7565\uff0c\u7ed3\u5408XNNPACK\u652f\u6301RISC-V\u67b6\u6784\uff0c\u540c\u65f6\u878d\u5408im2col\u548c\u6570\u636e\u6253\u5305\u64cd\u4f5c\u4ee5\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5229\u7528AITemplate\u5206\u6790\u6700\u4f73\u5b9e\u73b0\u3002", "result": "\u65b9\u6cd5\u4f7fResNet\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe4.0\u500d\uff0cImageNet top-1\u51c6\u786e\u7387\u4ec5\u4e0b\u964d2.1%\u4ee5\u5185\u3002", "conclusion": "\u5217\u7ea7N:M\u526a\u679d\u7b56\u7565\u53ca\u5176\u914d\u5957\u4f18\u5316\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.17214", "pdf": "https://arxiv.org/pdf/2507.17214", "abs": "https://arxiv.org/abs/2507.17214", "authors": ["Amod Kant Agrawal"], "title": "Our Cars Can Talk: How IoT Brings AI to Vehicles", "categories": ["cs.AI", "cs.CY", "cs.NI", "cs.SY", "eess.SY", "I.2; B.8; C.2; I.5; J.7"], "comment": "3 pages, 1 figure; To appear in IEEE Computer (Nov 2025)", "summary": "Bringing AI to vehicles and enabling them as sensing platforms is key to\ntransforming maintenance from reactive to proactive. Now is the time to\nintegrate AI copilots that speak both languages: machine and driver. This\narticle offers a conceptual and technical perspective intended to spark\ninterdisciplinary dialogue and guide future research and development in\nintelligent vehicle systems, predictive maintenance, and AI-powered user\ninteraction.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06AI\u6574\u5408\u5230\u8f66\u8f86\u4e2d\u4f5c\u4e3a\u611f\u77e5\u5e73\u53f0\u7684\u91cd\u8981\u6027\uff0c\u65e8\u5728\u63a8\u52a8\u4ece\u88ab\u52a8\u7ef4\u62a4\u5230\u4e3b\u52a8\u7ef4\u62a4\u7684\u8f6c\u53d8\uff0c\u5e76\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5bf9\u8bdd\u3002", "motivation": "\u5c06AI\u5f15\u5165\u8f66\u8f86\uff0c\u4f7f\u5176\u6210\u4e3a\u611f\u77e5\u5e73\u53f0\uff0c\u4ece\u800c\u5c06\u8f66\u8f86\u7ef4\u62a4\u4ece\u88ab\u52a8\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u3002", "method": "\u63d0\u4f9b\u6982\u5ff5\u548c\u6280\u672f\u89c6\u89d2\uff0c\u65e8\u5728\u6fc0\u53d1\u8de8\u5b66\u79d1\u5bf9\u8bdd\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86AI\u52a9\u624b\u540c\u65f6\u7406\u89e3\u673a\u5668\u548c\u9a7e\u9a76\u5458\u9700\u6c42\u7684\u89c2\u70b9\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u667a\u80fd\u8f66\u8f86\u7cfb\u7edf\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548cAI\u9a71\u52a8\u7684\u7528\u6237\u4ea4\u4e92\u3002"}}
{"id": "2507.17226", "pdf": "https://arxiv.org/pdf/2507.17226", "abs": "https://arxiv.org/abs/2507.17226", "authors": ["Sarah \"Magz\" Fernandez", "Greg L Nelson"], "title": "A \"watch your replay videos\" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection", "categories": ["cs.HC", "K.3"], "comment": null, "summary": "Generative AI is disrupting computing education. Most interventions focus on\nteaching GenAI use rather than helping students understand how AI changes their\nprogramming process. We designed and deployed a novel comparative video\nreflection assignment adapting the Describe, Examine, then Articulate Learning\n(DEAL) framework. In an introductory software engineering course, students\nrecorded themselves programming during their team project two times: first\nwithout, then with using generative AI. Students then analyzed their own videos\nusing a scaffolded set of reflection questions, including on their programming\nprocess and human, internet, and AI help-seeking. We conducted a qualitative\nthematic analysis of the reflections, finding students developed insights about\nplanning, debugging, and help-seeking behaviors that transcended AI use.\nStudents reported learning to slow down and understand before writing or\ngenerating code, recognized patterns in their problem-solving approaches, and\narticulated specific process improvements. Students also learned and reflected\non AI limits and downsides, and strategies to use AI more critically, including\nbetter prompting but also to benefit their learning instead of just completing\ntasks. Unexpectedly, the comparative reflection also scaffolded reflection on\nprogramming not involving AI use, and even led to students spontaneously\nsetting future goals to adopt video and other regular reflection. This work\ndemonstrates structured reflection on programming session videos can develop\nmetacognitive skills essential for programming with and without generative AI\nand also lifelong learning in our evolving field.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u89c6\u9891\u53cd\u601d\u4f5c\u4e1a\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u751f\u6210\u5f0fAI\u5982\u4f55\u6539\u53d8\u7f16\u7a0b\u8fc7\u7a0b\uff0c\u53d1\u73b0\u5b66\u751f\u4e0d\u4ec5\u53cd\u601d\u4e86AI\u7684\u4f7f\u7528\uff0c\u8fd8\u63d0\u5347\u4e86\u7f16\u7a0b\u7684\u5143\u8ba4\u77e5\u6280\u80fd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5728\u6539\u53d8\u8ba1\u7b97\u673a\u6559\u80b2\uff0c\u4f46\u73b0\u6709\u5e72\u9884\u63aa\u65bd\u591a\u805a\u7126\u4e8eAI\u5de5\u5177\u7684\u4f7f\u7528\uff0c\u800c\u975e\u5176\u5bf9\u7f16\u7a0b\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u4e2d\uff0c\u5b66\u751f\u5f55\u5236\u4e24\u6b21\u7f16\u7a0b\u8fc7\u7a0b\uff08\u672a\u4f7f\u7528AI\u548c\u4f7f\u7528AI\uff09\uff0c\u5e76\u901a\u8fc7DEAL\u6846\u67b6\u5206\u6790\u89c6\u9891\uff0c\u56de\u7b54\u53cd\u601d\u95ee\u9898\u3002", "result": "\u5b66\u751f\u901a\u8fc7\u53cd\u601d\u8ba4\u8bc6\u5230\u89c4\u5212\u3001\u8c03\u8bd5\u548c\u6c42\u52a9\u884c\u4e3a\u7684\u6539\u8fdb\uff0c\u5e76\u5b66\u4f1a\u6279\u5224\u6027\u4f7f\u7528AI\uff0c\u540c\u65f6\u610f\u5916\u5f15\u53d1\u4e86\u5bf9\u975eAI\u7f16\u7a0b\u7684\u53cd\u601d\u3002", "conclusion": "\u7ed3\u6784\u5316\u89c6\u9891\u53cd\u601d\u80fd\u6709\u6548\u57f9\u517b\u7f16\u7a0b\u7684\u5143\u8ba4\u77e5\u6280\u80fd\uff0c\u9002\u7528\u4e8eAI\u65f6\u4ee3\u7684\u5b66\u4e60\u548c\u7ec8\u8eab\u5b66\u4e60\u3002"}}
{"id": "2507.17736", "pdf": "https://arxiv.org/pdf/2507.17736", "abs": "https://arxiv.org/abs/2507.17736", "authors": ["Shreya Meel", "Sennur Ulukus"], "title": "Symmetric Private Information Retrieval (SPIR) on Graph-Based Replicated Systems", "categories": ["cs.IT", "cs.CR", "cs.DB", "cs.NI", "eess.SP", "math.IT"], "comment": null, "summary": "We introduce the problem of symmetric private information retrieval (SPIR) on\nreplicated databases modeled by a simple graph. In this model, each vertex\ncorresponds to a server, and a message is replicated on two servers if and only\nif there is an edge between them. We consider the setting where the server-side\ncommon randomness necessary to accomplish SPIR is also replicated at the\nservers according to the graph, and we call this as message-specific common\nrandomness. In this setting, we establish a lower bound on the SPIR capacity,\ni.e., the maximum download rate, for general graphs, by proposing an achievable\nSPIR scheme. Next, we prove that, for any SPIR scheme to be feasible, the\nminimum size of message-specific randomness should be equal to the size of a\nmessage. Finally, by providing matching upper bounds, we derive the exact SPIR\ncapacity for the class of path and regular graphs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u56fe\u6a21\u578b\u590d\u5236\u7684\u5bf9\u79f0\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\uff08SPIR\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\u5e76\u9a8c\u8bc1\u4e86\u5176\u4e0b\u8f7d\u901f\u7387\u7684\u4e0b\u754c\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6d88\u606f\u7279\u5b9a\u968f\u673a\u6027\u7684\u6700\u5c0f\u5927\u5c0f\uff0c\u4ee5\u53ca\u5bf9\u8def\u5f84\u56fe\u548c\u6b63\u5219\u56fe\u7c7b\u7684\u786e\u5207SPIR\u5bb9\u91cf\u3002", "motivation": "\u7814\u7a76\u590d\u5236\u7684\u6570\u636e\u5e93\u6a21\u578b\u4e2dSPIR\u95ee\u9898\uff0c\u63a2\u7d22\u670d\u52a1\u5668\u95f4\u7684\u968f\u673a\u6027\u5206\u914d\u5bf9SPIR\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6a21\u578b\u7684SPIR\u65b9\u6848\uff0c\u5206\u6790\u4e86\u6d88\u606f\u7279\u5b9a\u968f\u673a\u6027\u7684\u6700\u5c0f\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u754c\u5339\u914d\u63a8\u5bfc\u4e86\u7279\u5b9a\u56fe\u7c7b\u7684SPIR\u5bb9\u91cf\u3002", "result": "\u786e\u7acb\u4e86SPIR\u5bb9\u91cf\u7684\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u6d88\u606f\u7279\u5b9a\u968f\u673a\u6027\u7684\u6700\u5c0f\u5927\u5c0f\u7b49\u4e8e\u6d88\u606f\u5927\u5c0f\uff0c\u5e76\u63a8\u5bfc\u4e86\u8def\u5f84\u56fe\u548c\u6b63\u5219\u56fe\u7684\u7cbe\u786eSPIR\u5bb9\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u5236\u7684\u6570\u636e\u5e93\u6a21\u578b\u4e2d\u7684SPIR\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7279\u5b9a\u56fe\u7c7b\u7684SPIR\u5bb9\u91cf\u7cbe\u786e\u89e3\u3002"}}
{"id": "2507.17235", "pdf": "https://arxiv.org/pdf/2507.17235", "abs": "https://arxiv.org/abs/2507.17235", "authors": ["Andriy Miranskyy", "Jos\u00e9 Campos", "Anila Mjeda", "Lei Zhang", "Ignacio Garc\u00eda Rodr\u00edguez de Guzm\u00e1n"], "title": "On the Feasibility of Quantum Unit Testing", "categories": ["cs.SE", "quant-ph"], "comment": null, "summary": "The increasing complexity of quantum software presents significant challenges\nfor software verification and validation, particularly in the context of unit\ntesting. This work presents a comprehensive study on quantum-centric unit\ntests, comparing traditional statistical approaches with tests specifically\ndesigned for quantum circuits. These include tests that run only on a classical\ncomputer, such as the Statevector test, as well as those executable on quantum\nhardware, such as the Swap test and the novel Inverse test. Through an\nempirical study and detailed analysis on 1,796,880 mutated quantum circuits, we\ninvestigate (a) each test's ability to detect subtle discrepancies between the\nexpected and actual states of a quantum circuit, and (b) the number of\nmeasurements required to achieve high reliability. The results demonstrate that\nquantum-centric tests, particularly the Statevector test and the Inverse test,\nprovide clear advantages in terms of precision and efficiency, reducing both\nfalse positives and false negatives compared to statistical tests. This work\ncontributes to the development of more robust and scalable strategies for\ntesting quantum software, supporting the future adoption of fault-tolerant\nquantum computers and promoting more reliable practices in quantum software\nengineering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u91cf\u5b50\u4e2d\u5fc3\u5316\u5355\u5143\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u4e0e\u4e13\u4e3a\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u91cf\u5b50\u4e2d\u5fc3\u5316\u6d4b\u8bd5\uff08\u5982\u72b6\u6001\u5411\u91cf\u6d4b\u8bd5\u548c\u9006\u5411\u6d4b\u8bd5\uff09\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8f6f\u4ef6\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u6709\u6548\u7684\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u6269\u5c55\u6027\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u4f20\u7edf\u7edf\u8ba1\u6d4b\u8bd5\u4e0e\u91cf\u5b50\u4e2d\u5fc3\u5316\u6d4b\u8bd5\uff08\u5982\u72b6\u6001\u5411\u91cf\u6d4b\u8bd5\u3001\u4ea4\u6362\u6d4b\u8bd5\u548c\u9006\u5411\u6d4b\u8bd5\uff09\uff0c\u5e76\u901a\u8fc71796880\u4e2a\u53d8\u5f02\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u91cf\u5b50\u4e2d\u5fc3\u5316\u6d4b\u8bd5\uff08\u5c24\u5176\u662f\u72b6\u6001\u5411\u91cf\u6d4b\u8bd5\u548c\u9006\u5411\u6d4b\u8bd5\uff09\u5728\u68c0\u6d4b\u7535\u8def\u8bef\u5dee\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bef\u62a5\u548c\u6f0f\u62a5\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6240\u9700\u6d4b\u91cf\u6b21\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u9ad8\u6548\u7684\u7b56\u7565\uff0c\u652f\u6301\u672a\u6765\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\uff0c\u5e76\u4fc3\u8fdb\u4e86\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u7684\u53ef\u9760\u6027\u5b9e\u8df5\u3002"}}
{"id": "2507.17440", "pdf": "https://arxiv.org/pdf/2507.17440", "abs": "https://arxiv.org/abs/2507.17440", "authors": ["Christoph Schied", "Alexander Keller"], "title": "Parametric Integration with Neural Integral Operators", "categories": ["cs.GR"], "comment": null, "summary": "Real-time rendering imposes strict limitations on the sampling budget for\nlight transport simulation, often resulting in noisy images. However, denoisers\nhave demonstrated that it is possible to produce noise-free images through\nfiltering. We enhance image quality by removing noise before material shading,\nrather than filtering already shaded noisy images. This approach allows for\nmaterial-agnostic denoising (MAD) and leverages machine learning by\napproximating the light transport integral operator with a neural network,\neffectively performing parametric integration with neural operators. Our method\noperates in real-time, requires data from only a single frame, seamlessly\nintegrates with existing denoisers and temporal anti-aliasing techniques, and\nis efficient to train. Additionally, it is straightforward to incorporate with\nphysically based rendering algorithms.", "AI": {"tldr": "\u901a\u8fc7\u5728\u5b66\u4e60\u9636\u6bb5\u5bf9\u5149\u7ebf\u4f20\u8f93\u8fdb\u884c\u53c2\u6570\u5316\u79ef\u5206\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u9700\u591a\u5e27\u6570\u636e\u7684\u6750\u6599\u65e0\u5173\u53bb\u566a\uff08MAD\uff09\uff0c\u63d0\u5347\u6e32\u67d3\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5b9e\u65f6\u6e32\u67d3\u53d7\u9650\u4e8e\u91c7\u6837\u9884\u7b97\uff0c\u5e38\u5bfc\u81f4\u566a\u58f0\u56fe\u50cf\u3002\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u5728\u7740\u8272\u540e\u8fc7\u6ee4\uff0c\u800c\u6750\u6599\u65e0\u5173\u53bb\u566a\u5728\u7740\u8272\u524d\u8fdb\u884c\uff0c\u66f4\u9ad8\u6548\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u5149\u7ebf\u4f20\u8f93\u79ef\u5206\u7b97\u5b50\uff0c\u5b9e\u73b0\u53c2\u6570\u5316\u79ef\u5206\uff1b\u7ed3\u5408\u73b0\u6709\u53bb\u566a\u5668\u548c\u65f6\u57df\u6297\u952f\u9f7f\u6280\u672f\uff0c\u5355\u5e27\u6570\u636e\u5373\u53ef\u64cd\u4f5c\u3002", "result": "\u65b9\u6cd5\u5b9e\u65f6\u9ad8\u6548\uff0c\u6613\u4e8e\u8bad\u7ec3\uff0c\u517c\u5bb9\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u6750\u6599\u65e0\u5173\u53bb\u566a\uff08MAD\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u5b9e\u65f6\u6e32\u67d3\u53bb\u566a\u65b9\u6848\u3002"}}
{"id": "2507.17411", "pdf": "https://arxiv.org/pdf/2507.17411", "abs": "https://arxiv.org/abs/2507.17411", "authors": ["P\u00e1l Andr\u00e1s Papp", "Toni B\u00f6hnlein", "A. N. Yzelman"], "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions", "categories": ["cs.DC", "90B35, 90C10, 68Q10, 68W10", "C.1.4"], "comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)", "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u591a\u5904\u7406\u5668\u548c\u4e24\u7ea7\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u4e2d\u8c03\u5ea6\u901a\u7528\u8ba1\u7b97DAG\u7684\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5206\u79bb\u4f18\u5316\u5e76\u884c\u5316\u548c\u5185\u5b58\u7ba1\u7406\u4f1a\u5bfc\u81f4\u89e3\u504f\u79bb\u6700\u4f18\u89e3\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u7684\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u591a\u5904\u7406\u5668\u548c\u4e24\u7ea7\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u4e2dDAG\u8c03\u5ea6\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u8d1f\u8f7d\u5e73\u8861\u3001\u901a\u4fe1\u548c\u7f13\u5b58\u9650\u5236\u5e26\u6765\u7684\u6570\u636e\u79fb\u52a8\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u8868\u793a\u548c\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eILP\u7684\u6574\u4f53\u8c03\u5ea6\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eILP\u7684\u65b9\u6cd5\u6bd4\u4f20\u7edf\u8c03\u5ea6\u7b97\u6cd5\u548c\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u7684\u7ec4\u5408\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u57fa\u4e8eILP\u7684\u8c03\u5ea6\u7b97\u6cd5\u5728\u5904\u7406DAG\u8c03\u5ea6\u95ee\u9898\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u5e76\u884c\u5316\u548c\u5185\u5b58\u7ba1\u7406\u7684\u7efc\u5408\u6027\u80fd\u3002"}}
{"id": "2507.17491", "pdf": "https://arxiv.org/pdf/2507.17491", "abs": "https://arxiv.org/abs/2507.17491", "authors": ["Nazatul H. Sultan", "Xinlong Guan", "Josef Pieprzyk", "Wei Ni", "Sharif Abuadbba", "Hajime Suzuki"], "title": "Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement", "categories": ["cs.CR", "cs.NI", "68M25", "C.2.2"], "comment": "Accepted at RAID 2025", "summary": "As 5G networks expand into critical infrastructure, secure and efficient user\nauthentication is more important than ever. The 5G-AKA protocol, standardized\nby 3GPP in TS 33.501, is central to authentication in current 5G deployments.\nIt provides mutual authentication, user privacy, and key secrecy. However,\ndespite its adoption, 5G-AKA has known limitations in both security and\nperformance. While it focuses on protecting privacy against passive attackers,\nrecent studies show its vulnerabilities to active attacks. It also relies on a\nsequence number mechanism to prevent replay attacks, requiring perfect\nsynchronization between the device and the core network. This stateful design\nadds complexity, causes desynchronization, and incurs extra communication\noverhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing\npast communications if long-term keys are compromised-an increasing concern\namid sophisticated threats. This paper proposes an enhanced authentication\nprotocol that builds on 5G-AKA's design while addressing its shortcomings.\nFirst, we introduce a stateless version that removes sequence number reliance,\nreducing complexity while staying compatible with existing SIM cards and\ninfrastructure. We then extend this design to add PFS with minimal\ncryptographic overhead. Both protocols are rigorously analyzed using ProVerif,\nconfirming their compliance with all major security requirements, including\nresistance to passive and active attacks, as well as those defined by 3GPP and\nacademic studies. We also prototype both protocols and evaluate their\nperformance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the\nproposed protocols offer stronger security with only minor computational\noverhead, making them practical, future-ready solutions for 5G and beyond.", "AI": {"tldr": "5G-AKA\u534f\u8bae\u5b58\u5728\u5b89\u5168\u548c\u6027\u80fd\u7f3a\u9677\uff0c\u672c\u6587\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\uff0c\u5305\u62ec\u65e0\u72b6\u6001\u8bbe\u8ba1\u548c\u5b8c\u7f8e\u524d\u5411\u4fdd\u5bc6\uff0c\u901a\u8fc7ProVerif\u9a8c\u8bc1\u5176\u5b89\u5168\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e5G-AKA\u3002", "motivation": "5G\u7f51\u7edc\u6269\u5c55\u81f3\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u9700\u63d0\u5347\u8ba4\u8bc1\u534f\u8bae\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\uff0c\u5f25\u88655G-AKA\u5728\u4e3b\u52a8\u653b\u51fb\u9632\u5fa1\u548c\u5b8c\u7f8e\u524d\u5411\u4fdd\u5bc6\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "1. \u63d0\u51fa\u65e0\u72b6\u6001\u7248\u672c\uff0c\u907f\u514d\u5e8f\u5217\u53f7\u4f9d\u8d56\uff1b2. \u5f15\u5165\u5b8c\u7f8e\u524d\u5411\u4fdd\u5bc6\u673a\u5236\u3002\u901a\u8fc7ProVerif\u5206\u6790\u5b89\u5168\u6027\uff0c\u5e76\u539f\u578b\u5b9e\u73b0\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u6539\u8fdb\u534f\u8bae\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e5G-AKA\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u9002\u7528\u4e8e5G\u53ca\u672a\u6765\u7f51\u7edc\u3002", "conclusion": "\u6539\u8fdb\u534f\u8bae\u89e3\u51b3\u4e865G-AKA\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u517c\u5177\u9ad8\u6548\u4e0e\u5f3a\u5b89\u5168\u6027\uff0c\u4e3a5G\u53ca\u672a\u6765\u7f51\u7edc\u63d0\u4f9b\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.17230", "pdf": "https://arxiv.org/pdf/2507.17230", "abs": "https://arxiv.org/abs/2507.17230", "authors": ["Clara Scalzer", "Saurav Pokhrel", "Sara Hunt", "Greg L Nelson"], "title": "Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series", "categories": ["cs.HC", "K.3"], "comment": null, "summary": "Students continue their education when they feel their learning is meaningful\nand relevant for their future careers. Computing educators now face the\nchallenge of preparing students for careers increasingly shaped by generative\nAI (GenAI) with the goals of supporting their learning, motivation, ethics, and\ncareer development. Our longitudinal qualitative study of students in a\nGenAI-integrated creative media course shows how this is a \"wicked\" problem:\nprogress on one goal can then impede progress on other goals. Students\ndeveloped concerning patterns despite extensive instruction in critical and\nethical GenAI use including prompt engineering, ethics and bias, and industry\npanels on GenAI's career impact. We present an analysis of two students'\nexperiences to showcase this complexity. Increasing GenAI use skills can lower\nethics; for example, Pat started from purposefully avoiding GenAI use, to\ndependency. He described himself as a \"notorious cheater\" who now uses GenAi to\n\"get all the right answers\" while acknowledging he's learning less. Increasing\nethical awareness can lower the learning of GenAI use skills; for example,\nJay's newfound environmental concerns led to self-imposed usage limits that\nimpeded skill development, and new serious fears that GenAI would eliminate\ncreative careers they had been passionate about. Increased GenAI proficiency, a\npotential career skill, did not improve their career confidence. These findings\nsuggest that supporting student development in the GenAI era is a \"wicked\"\nproblem requiring multi-dimensional evaluation and design, rather than\noptimizing learning, GenAI skills, ethics, or career motivation individually.", "AI": {"tldr": "\u5b66\u751f\u8ba4\u4e3a\u5b66\u4e60\u6709\u610f\u4e49\u4e14\u4e0e\u672a\u6765\u804c\u4e1a\u76f8\u5173\u65f6\u4f1a\u66f4\u613f\u610f\u7ee7\u7eed\u5b66\u4e60\u3002\u8ba1\u7b97\u6559\u80b2\u8005\u9762\u4e34\u5982\u4f55\u5c06\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u878d\u5165\u8bfe\u7a0b\u4ee5\u652f\u6301\u5b66\u4e60\u3001\u52a8\u673a\u3001\u4f26\u7406\u548c\u804c\u4e1a\u53d1\u5c55\u7684\u590d\u6742\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u662f\u4e00\u4e2a\u201c\u68d8\u624b\u201d\u95ee\u9898\uff0c\u89e3\u51b3\u4e00\u4e2a\u76ee\u6807\u53ef\u80fd\u635f\u5bb3\u5176\u4ed6\u76ee\u6807\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5c06GenAI\u878d\u5165\u6559\u80b2\u4ee5\u5e73\u8861\u5b66\u4e60\u3001\u4f26\u7406\u3001\u52a8\u673a\u548c\u804c\u4e1a\u53d1\u5c55\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u4e00\u95e8GenAI\u878d\u5165\u521b\u610f\u5a92\u4f53\u8bfe\u7a0b\u7684\u5b66\u751f\u8fdb\u884c\u7eb5\u5411\u5b9a\u6027\u7814\u7a76\uff0c\u5206\u6790\u4e24\u540d\u5b66\u751f\u7684\u7ecf\u5386\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u5347GenAI\u6280\u80fd\u53ef\u80fd\u964d\u4f4e\u4f26\u7406\u610f\u8bc6\uff0c\u800c\u63d0\u9ad8\u4f26\u7406\u610f\u8bc6\u53ef\u80fd\u963b\u788d\u6280\u80fd\u5b66\u4e60\uff0c\u4e14GenAI\u719f\u7ec3\u5ea6\u5e76\u672a\u589e\u5f3a\u804c\u4e1a\u4fe1\u5fc3\u3002", "conclusion": "\u652f\u6301\u5b66\u751f\u5728GenAI\u65f6\u4ee3\u7684\u53d1\u5c55\u662f\u4e00\u4e2a\u591a\u7ef4\u5ea6\u7684\u201c\u68d8\u624b\u201d\u95ee\u9898\uff0c\u9700\u8981\u7efc\u5408\u8bbe\u8ba1\u800c\u975e\u5355\u4e00\u4f18\u5316\u3002"}}
{"id": "2507.17264", "pdf": "https://arxiv.org/pdf/2507.17264", "abs": "https://arxiv.org/abs/2507.17264", "authors": ["Jenny T. Liang", "Chenyang Yang", "Agnia Sergeyuk", "Travis D. Breaux", "Brad A. Myers"], "title": "Understanding Prompt Programming Tasks and Questions", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u63d0\u793a\u7f16\u7a0b\u4e2d\u7684\u5f00\u53d1\u8005\u9700\u6c42\uff0c\u901a\u8fc7\u5206\u7c7b\u4efb\u52a1\u548c\u95ee\u9898\uff0c\u53d1\u73b0\u5f53\u524d\u5de5\u5177\u652f\u6301\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u673a\u4f1a\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f00\u53d1\u8005\u901a\u8fc7\u63d0\u793a\u7f16\u7a0b\u5b9e\u73b0\u65b0\u529f\u80fd\uff0c\u4f46\u5f53\u524d\u5de5\u5177\u672a\u80fd\u5145\u5206\u652f\u6301\u5f00\u53d1\u8005\u7684\u9700\u6c42\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u9700\u6c42\u5e76\u4e3a\u5de5\u5177\u6539\u8fdb\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0816\u540d\u63d0\u793a\u7a0b\u5e8f\u5458\u3001\u89c2\u5bdf8\u540d\u5f00\u53d1\u8005\u4fee\u6539\u63d0\u793a\u5185\u5bb9\uff0c\u4ee5\u53ca\u8c03\u67e550\u540d\u5f00\u53d1\u8005\uff0c\u6784\u5efa\u4e8625\u4e2a\u4efb\u52a1\u548c51\u4e2a\u95ee\u9898\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u6bd4\u8f83\u4e8648\u4e2a\u7814\u7a76\u53ca\u5546\u4e1a\u5de5\u5177\u3002", "result": "\u53d1\u73b0\u63d0\u793a\u7f16\u7a0b\u7684\u652f\u6301\u4e0d\u8db3\uff0c\u6240\u6709\u4efb\u52a1\u4ecd\u9700\u624b\u52a8\u5b8c\u6210\uff0c51\u4e2a\u95ee\u9898\u4e2d\u670916\u4e2a\uff08\u5305\u62ec\u591a\u6570\u91cd\u8981\u95ee\u9898\uff09\u672a\u5f97\u5230\u89e3\u7b54\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u63d0\u793a\u7f16\u7a0b\u5de5\u5177\u7684\u6539\u8fdb\u673a\u4f1a\uff0c\u547c\u5401\u672a\u6765\u5de5\u5177\u66f4\u597d\u5730\u652f\u6301\u5f00\u53d1\u8005\u9700\u6c42\u3002"}}
{"id": "2507.17248", "pdf": "https://arxiv.org/pdf/2507.17248", "abs": "https://arxiv.org/abs/2507.17248", "authors": ["Xiaoan Liu", "Difan Jia", "Xianhao Carton Liu", "Mar Gonzalez-Franco", "Chen Zhu-Tian"], "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations", "categories": ["cs.HC", "cs.AI", "cs.GR", "H.5.2; I.3.6"], "comment": "16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025", "summary": "Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u4ee3\u7406\uff08\u62bd\u8c61\u7684\u865a\u62df\u5bf9\u8c61\u4ee3\u8868\u73b0\u5b9e\u7269\u4f53\uff09\u6765\u89e3\u8026\u6df7\u5408\u73b0\u5b9e\u4e2d\u7684\u4ea4\u4e92\u95ee\u9898\uff0cReality Proxy\u7cfb\u7edf\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u9009\u62e9\u548c\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u73b0\u5b9e\u4e2d\u7531\u4e8e\u7269\u4f53\u62e5\u6324\u3001\u8ddd\u79bb\u8fdc\u6216\u88ab\u906e\u6321\u5bfc\u81f4\u7684\u9009\u62e9\u548c\u64cd\u4f5c\u56f0\u96be\u3002", "method": "\u63d0\u51faReality Proxy\u7cfb\u7edf\uff0c\u5c06\u4ea4\u4e92\u4ece\u7269\u7406\u5bf9\u8c61\u8f6c\u79fb\u5230\u5176\u4ee3\u7406\u4e0a\uff0c\u5e76\u901a\u8fc7AI\u589e\u5f3a\u4ee3\u7406\u7684\u8bed\u4e49\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u591a\u79cd\u65b0\u9896\u4ea4\u4e92\uff08\u5982\u5feb\u901f\u6d4f\u89c8\u3001\u5c5e\u6027\u7b5b\u9009\u3001\u5d4c\u5957\u5bfc\u822a\u548c\u591a\u5bf9\u8c61\u9009\u62e9\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u4ee3\u7406\u62bd\u8c61\u4e3a\u672a\u6765\u6df7\u5408\u73b0\u5b9e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u901a\u7528\u7684\u4ea4\u4e92\u8303\u5f0f\u3002"}}
{"id": "2507.17458", "pdf": "https://arxiv.org/pdf/2507.17458", "abs": "https://arxiv.org/abs/2507.17458", "authors": ["Marco Pulimeno", "Italo Epicoco", "Massimo Cafaro"], "title": "Distributed P2P quantile tracking with relative value error", "categories": ["cs.DC"], "comment": null, "summary": "In this paper we present \\textsc{DUDDSketch}, a distributed version of the\n\\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm\nis a fully decentralized, gossip-based distributed protocol working in the\ncontext of unstructured P2P networks. We discuss the algorithm's design and\nformally prove its correctness. We also show, through extensive experimental\nresults, that the algorithm converges to the results provided by the sequential\nalgorithm, which is a fundamental and highly desirable property.", "AI": {"tldr": "DUDDSketch\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u7248\u672c\u7684UDDSketch\u7b97\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u8ddf\u8e2a\u5206\u4f4d\u6570\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u7684P2P\u7f51\u7edc\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u3001\u57fa\u4e8egossip\u7684\u5206\u5e03\u5f0f\u534f\u8bae\uff0c\u4ee5\u5728\u975e\u7ed3\u6784\u5316P2P\u7f51\u7edc\u4e2d\u51c6\u786e\u8ddf\u8e2a\u5206\u4f4d\u6570\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u534f\u8bae\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u5176\u6b63\u786e\u6027\uff0c\u540c\u65f6\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u5230\u987a\u5e8f\u7b97\u6cd5\u63d0\u4f9b\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DUDDSketch\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5206\u5e03\u5f0f\u5206\u4f4d\u6570\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316P2P\u7f51\u7edc\u3002"}}
{"id": "2507.17495", "pdf": "https://arxiv.org/pdf/2507.17495", "abs": "https://arxiv.org/abs/2507.17495", "authors": ["Raj Kamleshkumar Madhu", "Visuttha Manthamkarn", "Zheshen Zhang", "Jianqing Liu"], "title": "A Virtual Quantum Network Prototype for Open Access", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "The rise of quantum networks has revolutionized domains such as\ncommunication, sensing, and cybersecurity. Despite this progress, current\nquantum network systems remain limited in scale, are highly\napplication-specific (e.g., for quantum key distribution), and lack a clear\nroad map for global expansion. These limitations are largely driven by a\nshortage of skilled professionals, limited accessibility to quantum\ninfrastructure, and the high complexity and cost associated with building and\noperating quantum hardware. To address these challenges, this paper proposes an\nopen-access software-based quantum network virtualization platform designed to\nfacilitate scalable and remote interaction with quantum hardware. The system is\nbuilt around a cloud application that virtualizes the core hardware components\nof a lab-scale quantum network testbed, including the time tagger and optical\nswitch, enabling users to perform coincidence counts of the photon\nentanglements while ensuring fair resource allocation. The fairness is ensured\nby employing the Hungarian Algorithm to allocate nearly equal effective\nentanglement rates among users. We provide implementation details and\nperformance analysis from the perspectives of hardware, software, and cloud\nplatform, which demonstrates the functionality and efficiency of the developed\nprototype.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u4ef6\u7684\u91cf\u5b50\u7f51\u7edc\u865a\u62df\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u91cf\u5b50\u7f51\u7edc\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u8fdc\u7a0b\u8bbf\u95ee\u95ee\u9898\u3002\u5e73\u53f0\u901a\u8fc7\u4e91\u8ba1\u7b97\u865a\u62df\u5316\u6838\u5fc3\u786c\u4ef6\u7ec4\u4ef6\uff0c\u5e76\u5229\u7528\u5308\u7259\u5229\u7b97\u6cd5\u786e\u4fdd\u8d44\u6e90\u516c\u5e73\u5206\u914d\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u7f51\u7edc\u7cfb\u7edf\u89c4\u6a21\u6709\u9650\u3001\u9ad8\u5ea6\u5b9a\u5236\u5316\u4e14\u7f3a\u4e4f\u5168\u7403\u6269\u5c55\u8def\u5f84\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4e13\u4e1a\u4eba\u5458\u77ed\u7f3a\u3001\u57fa\u7840\u8bbe\u65bd\u83b7\u53d6\u56f0\u96be\u53ca\u9ad8\u6602\u6210\u672c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u91cf\u5b50\u7f51\u7edc\u865a\u62df\u5316\u5e73\u53f0\uff0c\u901a\u8fc7\u4e91\u5e94\u7528\u865a\u62df\u5316\u5b9e\u9a8c\u5ba4\u89c4\u6a21\u7684\u91cf\u5b50\u7f51\u7edc\u6d4b\u8bd5\u53f0\u6838\u5fc3\u786c\u4ef6\u7ec4\u4ef6\uff0c\u5305\u62ec\u65f6\u95f4\u6807\u8bb0\u5668\u548c\u5149\u5b66\u5f00\u5173\uff0c\u5e76\u91c7\u7528\u5308\u7259\u5229\u7b97\u6cd5\u786e\u4fdd\u7528\u6237\u95f4\u7684\u516c\u5e73\u8d44\u6e90\u5206\u914d\u3002", "result": "\u6027\u80fd\u5206\u6790\u8868\u660e\uff0c\u8be5\u539f\u578b\u5728\u786c\u4ef6\u3001\u8f6f\u4ef6\u548c\u4e91\u5e73\u53f0\u65b9\u9762\u5747\u8868\u73b0\u51fa\u529f\u80fd\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u91cf\u5b50\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6027\u548c\u8fdc\u7a0b\u8bbf\u95ee\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u91cf\u5b50\u7f51\u7edc\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.17242", "pdf": "https://arxiv.org/pdf/2507.17242", "abs": "https://arxiv.org/abs/2507.17242", "authors": ["Gege Ming", "Weihua Pei", "Sen Tian", "Xiaogang Chen", "Xiaorong Gao", "Yijun Wang"], "title": "High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces", "categories": ["cs.HC", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Brain-computer interface (BCI) technology establishes a direct communication\npathway between the brain and external devices. Current visual BCI systems\nsuffer from insufficient information transfer rates (ITRs) for practical use.\nSpatial information, a critical component of visual perception, remains\nunderexploited in existing systems because the limited spatial resolution of\nrecording methods hinders the capture of the rich spatiotemporal dynamics of\nbrain signals. This study proposed a frequency-phase-space fusion encoding\nmethod, integrated with 256-channel high-density electroencephalogram (EEG)\nrecordings, to develop high-speed BCI systems. In the classical frequency-phase\nencoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode\nconfigurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50%\nover the traditional 64-9 setup. In the proposed frequency-phase-space encoding\n200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and\n103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm.\nThis study demonstrates the essential role and immense potential of\nhigh-density EEG in decoding the spatiotemporal information of visual stimuli.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387-\u76f8\u4f4d-\u7a7a\u95f4\u878d\u5408\u7f16\u7801\u65b9\u6cd5\uff0c\u7ed3\u5408256\u901a\u9053\u9ad8\u5bc6\u5ea6\u8111\u7535\u56fe\u8bb0\u5f55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u7cfb\u7edf\u7684\u4fe1\u606f\u4f20\u8f93\u901f\u7387\uff08ITR\uff09\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9BCI\u7cfb\u7edf\u7684\u4fe1\u606f\u4f20\u8f93\u901f\u7387\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6316\u6398\u89c6\u89c9\u611f\u77e5\u4e2d\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9891\u7387-\u76f8\u4f4d-\u7a7a\u95f4\u878d\u5408\u7f16\u7801\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u5bc6\u5ea6\uff08256\u901a\u9053\uff09\u8111\u7535\u56fe\u8bb0\u5f55\uff0c\u5f00\u53d1\u9ad8\u901fBCI\u7cfb\u7edf\u3002", "result": "\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\uff0c\u7406\u8bbaITR\u63d0\u5347\u6700\u9ad8\u8fbe195.56%\uff0c\u5728\u7ebf\u7cfb\u7edf\u5e73\u5747\u5b9e\u9645ITR\u8fbe\u5230472.7 bpm\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u9ad8\u5bc6\u5ea6\u8111\u7535\u56fe\u5728\u89e3\u7801\u89c6\u89c9\u523a\u6fc0\u65f6\u7a7a\u4fe1\u606f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u548c\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.17270", "pdf": "https://arxiv.org/pdf/2507.17270", "abs": "https://arxiv.org/abs/2507.17270", "authors": ["Alessandro Aneggi", "Andrea Janes"], "title": "Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning", "categories": ["cs.SE"], "comment": "Accepted @ XP2025 Poster session", "summary": "This experience report analyses a one year project focused on building a\ndistributed real-time analytics system using edge computing and machine\nlearning. The project faced critical setbacks due to a big-bang integration\napproach, where all components developed by multiple geographically dispersed\npartners were merged at the final stage. The integration effort resulted in\nonly six minutes of system functionality, far below the expected 40 minutes.\nThrough root cause analysis, the study identifies technical and organisational\nbarriers, including poor communication, lack of early integration testing, and\nresistance to topdown planning. It also considers psychological factors such as\na bias toward fully developed components over mockups. The paper advocates for\nearly mock based deployment, robust communication infrastructures, and the\nadoption of topdown thinking to manage complexity and reduce risk in reactive,\ndistributed projects. These findings underscore the limitations of traditional\nAgile methods in such contexts and propose simulation-driven engineering and\nstructured integration cycles as key enablers for future success.", "AI": {"tldr": "\u8be5\u62a5\u544a\u5206\u6790\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u5b9e\u65f6\u5206\u6790\u7cfb\u7edf\u9879\u76ee\uff0c\u56e0\u91c7\u7528\u4e00\u6b21\u6027\u96c6\u6210\u65b9\u6cd5\u5bfc\u81f4\u5931\u8d25\uff0c\u4ec5\u8fd0\u884c6\u5206\u949f\u3002\u5efa\u8bae\u65e9\u671f\u6a21\u62df\u90e8\u7f72\u548c\u6539\u8fdb\u6c9f\u901a\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u5b9e\u65f6\u5206\u6790\u7cfb\u7edf\u9879\u76ee\u4e2d\u96c6\u6210\u5931\u8d25\u7684\u6839\u6e90\uff0c\u63a2\u7d22\u5982\u4f55\u6539\u8fdb\u590d\u6742\u5206\u5e03\u5f0f\u9879\u76ee\u7684\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6839\u56e0\u5206\u6790\uff0c\u8bc6\u522b\u6280\u672f\u548c\u7ec4\u7ec7\u969c\u788d\uff0c\u5982\u6c9f\u901a\u4e0d\u8db3\u548c\u7f3a\u4e4f\u65e9\u671f\u6d4b\u8bd5\u3002", "result": "\u9879\u76ee\u56e0\u4e00\u6b21\u6027\u96c6\u6210\u5931\u8d25\uff0c\u7cfb\u7edf\u4ec5\u8fd0\u884c6\u5206\u949f\u3002\u5efa\u8bae\u91c7\u7528\u65e9\u671f\u6a21\u62df\u548c\u7ed3\u6784\u5316\u96c6\u6210\u5468\u671f\u3002", "conclusion": "\u4f20\u7edf\u654f\u6377\u65b9\u6cd5\u5728\u590d\u6742\u5206\u5e03\u5f0f\u9879\u76ee\u4e2d\u53d7\u9650\uff0c\u9700\u7ed3\u5408\u6a21\u62df\u9a71\u52a8\u548c\u7ed3\u6784\u5316\u96c6\u6210\u4ee5\u964d\u4f4e\u98ce\u9669\u3002"}}
{"id": "2507.17228", "pdf": "https://arxiv.org/pdf/2507.17228", "abs": "https://arxiv.org/abs/2507.17228", "authors": ["Wei Fan", "JinYi Yoon", "Xiaochang Li", "Huajie Shao", "Bo Ji"], "title": "P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted as invited paper in The 34th International Conference on\n  Computer Communications and Networks (ICCCN 2025)", "summary": "Split Learning (SL) is an emerging privacy-preserving machine learning\ntechnique that enables resource constrained edge devices to participate in\nmodel training by partitioning a model into client-side and server-side\nsub-models. While SL reduces computational overhead on edge devices, it\nencounters significant challenges in heterogeneous environments where devices\nvary in computing resources, communication capabilities, environmental\nconditions, and privacy requirements. Although recent studies have explored\nheterogeneous SL frameworks that optimize split points for devices with varying\nresource constraints, they often neglect personalized privacy requirements and\nlocal model customization under varying environmental conditions. To address\nthese limitations, we propose P3SL, a Personalized Privacy-Preserving Split\nLearning framework designed for heterogeneous, resource-constrained edge device\nsystems. The key contributions of this work are twofold. First, we design a\npersonalized sequential split learning pipeline that allows each client to\nachieve customized privacy protection and maintain personalized local models\ntailored to their computational resources, environmental conditions, and\nprivacy needs. Second, we adopt a bi-level optimization technique that empowers\nclients to determine their own optimal personalized split points without\nsharing private sensitive information (i.e., computational resources,\nenvironmental conditions, privacy requirements) with the server. This approach\nbalances energy consumption and privacy leakage risks while maintaining high\nmodel accuracy. We implement and evaluate P3SL on a testbed consisting of 7\ndevices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,\nusing diverse model architectures and datasets under varying environmental\nconditions.", "AI": {"tldr": "P3SL\u662f\u4e00\u4e2a\u9762\u5411\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u7684\u4e2a\u6027\u5316\u9690\u79c1\u4fdd\u62a4\u5206\u5272\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u5ba2\u6237\u81ea\u5b9a\u4e49\u9690\u79c1\u9700\u6c42\u4e0e\u672c\u5730\u6a21\u578b\uff0c\u540c\u65f6\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6280\u672f\u786e\u5b9a\u6700\u4f73\u5206\u5272\u70b9\u3002", "motivation": "\u73b0\u6709\u5206\u5272\u5b66\u4e60\u6846\u67b6\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5ffd\u89c6\u4e2a\u6027\u5316\u9690\u79c1\u9700\u6c42\u548c\u672c\u5730\u6a21\u578b\u5b9a\u5236\uff0cP3SL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u652f\u6301\u4e2a\u6027\u5316\u9690\u79c1\u4fdd\u62a4\u548c\u672c\u5730\u6a21\u578b\u5b9a\u5236\u7684\u5206\u5272\u5b66\u4e60\u6d41\u7a0b\uff0c\u5e76\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6280\u672f\u8ba9\u5ba2\u6237\u81ea\u4e3b\u9009\u62e9\u5206\u5272\u70b9\u3002", "result": "\u57287\u79cd\u8bbe\u5907\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0cP3SL\u5728\u5e73\u8861\u80fd\u8017\u3001\u9690\u79c1\u6cc4\u6f0f\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "P3SL\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2507.17655", "pdf": "https://arxiv.org/pdf/2507.17655", "abs": "https://arxiv.org/abs/2507.17655", "authors": ["Shams Shaikh", "Trima P. Fernandes e Fizardo"], "title": "Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses", "categories": ["cs.CR", "cs.NI", "cs.SE", "C.2.4; D.4.6; E.3; E.5; K.6.5"], "comment": "9 pages, 2 Flowcharts, 2 Tables", "summary": "As organizations rapidly migrate to the cloud, the security of cryptographic\nkey management has become a growing concern. Hardware Security Modules (HSMs)\nand Trusted Platform Modules (TPMs), traditionally seen as the gold standard\nfor securing encryption keys and digital trust, are increasingly challenged by\ncloud-native threats. Real-world breaches have exposed weaknesses in cloud\ndeployments, including misconfigurations, API abuse, and privilege escalations,\nallowing attackers to access sensitive key material and bypass protections.\nThese incidents reveal that while the hardware remains secure, the surrounding\ncloud ecosystem introduces systemic vulnerabilities. This paper analyzes\nnotable security failures involving HSMs and TPMs, identifies common attack\nvectors, and questions longstanding assumptions about their effectiveness in\ndistributed environments. We explore alternative approaches such as\nconfidential computing, post-quantum cryptography, and decentralized key\nmanagement. Our findings highlight that while HSMs and TPMs still play a role,\nmodern cloud security requires more adaptive, layered architectures. By\nevaluating both current weaknesses and emerging models, this research equips\ncloud architects and security engineers with strategies to reinforce\ncryptographic trust in the evolving threat landscape.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4e91\u73af\u5883\u4e2dHSMs\u548cTPMs\u7684\u5b89\u5168\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u66ff\u4ee3\u65b9\u6848\u4ee5\u9002\u5e94\u73b0\u4ee3\u4e91\u5b89\u5168\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u5feb\u901f\u8fc1\u79fb\u81f3\u4e91\u7aef\uff0c\u5bc6\u94a5\u7ba1\u7406\u7684\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f20\u7edf\u7684HSMs\u548cTPMs\u5728\u4e91\u73af\u5883\u4e2d\u9762\u4e34\u65b0\u5a01\u80c1\u3002", "method": "\u5206\u6790HSMs\u548cTPMs\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63a2\u8ba8\u673a\u5bc6\u8ba1\u7b97\u3001\u540e\u91cf\u5b50\u5bc6\u7801\u548c\u53bb\u4e2d\u5fc3\u5316\u5bc6\u94a5\u7ba1\u7406\u7b49\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0HSMs\u548cTPMs\u4ecd\u6709\u4ef7\u503c\uff0c\u4f46\u73b0\u4ee3\u4e91\u5b89\u5168\u9700\u8981\u66f4\u7075\u6d3b\u7684\u5206\u5c42\u67b6\u6784\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e91\u67b6\u6784\u5e08\u548c\u5b89\u5168\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u5f3a\u5316\u52a0\u5bc6\u4fe1\u4efb\u7684\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u5a01\u80c1\u3002"}}
{"id": "2507.17271", "pdf": "https://arxiv.org/pdf/2507.17271", "abs": "https://arxiv.org/abs/2507.17271", "authors": ["Shuaiyu Zhou", "Zhengran Zeng", "Xiaoling Zhou", "Rui Xie", "Shikun Zhang", "Wei Ye"], "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "categories": ["cs.SE"], "comment": null, "summary": "Unit tests play a vital role in the software development lifecycle. Recent\nadvances in Large Language Model (LLM)-based approaches have significantly\nimproved automated test generation, garnering attention from both academia and\nindustry. We revisit LLM-based unit test generation from a novel perspective by\ndecoupling prefix generation and assertion generation. To characterize their\nrespective challenges, we define Initialization Complexity and adopt Cyclomatic\nComplexity to measure the difficulty of prefix and assertion generation,\nrevealing that the former primarily affects compilation success, while the\nlatter influences test coverage. To address these challenges, we propose\nSeed&Steer, a two-step approach that combines traditional unit testing\ntechniques with the capabilities of large language models. Seed&Steer leverages\nconventional unit testing tools (e.g., EvoSuite) to generate method invocations\nwith high compilation success rates, which serve as seeds to guide LLMs in\nconstructing effective test contexts. It then introduces branching cues to help\nLLMs explore diverse execution paths (e.g., normal, boundary, and exception\ncases) and generate assertions with high coverage. We evaluate Seed&Steer on\nfive real-world Java projects against state-of-the-art baselines. Results show\nthat Seed&Steer improves the compilation pass rate by approximately 7%,\nsuccessfully compiling 792 and 887 previously failing cases on two LLMs. It\nalso achieves up to ~73% branch and line coverage across focal methods of\nvarying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our\ncode, dataset, and experimental scripts will be publicly released to support\nfuture research and reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeed&Steer\u7684\u4e24\u6b65\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u5355\u5143\u6d4b\u8bd5\u6280\u672f\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u5206\u79bb\u524d\u7f00\u751f\u6210\u548c\u65ad\u8a00\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7f16\u8bd1\u6210\u529f\u7387\u4e0e\u8986\u76d6\u7387\u3002", "motivation": "\u7814\u7a76LLM\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u524d\u7f00\u751f\u6210\u548c\u65ad\u8a00\u751f\u6210\u5206\u522b\u5f71\u54cd\u7f16\u8bd1\u6210\u529f\u7387\u548c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0c\u4e9f\u9700\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528Seed&Steer\u65b9\u6cd5\uff0c\u5148\u4f7f\u7528\u4f20\u7edf\u5de5\u5177\u751f\u6210\u9ad8\u7f16\u8bd1\u6210\u529f\u7387\u7684\u65b9\u6cd5\u8c03\u7528\u4f5c\u4e3a\u79cd\u5b50\uff0c\u518d\u901a\u8fc7LLM\u751f\u6210\u591a\u6837\u5316\u6267\u884c\u8def\u5f84\u548c\u65ad\u8a00\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9eJava\u9879\u76ee\u4e2d\uff0cSeed&Steer\u663e\u8457\u63d0\u5347\u7f16\u8bd1\u901a\u8fc7\u7387\u548c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u7f16\u8bd1\u6210\u529f\u7387\u63d0\u53477%\uff0c\u8986\u76d6\u7387\u63d0\u9ad81.09*\u81f31.26*\u3002", "conclusion": "Seed&Steer\u7ed3\u5408\u4f20\u7edf\u6280\u672f\u4e0eLLM\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f16\u8bd1\u4e0e\u8986\u76d6\u7387\u95ee\u9898\uff0c\u4e3a\u5355\u5143\u6d4b\u8bd5\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.17695", "pdf": "https://arxiv.org/pdf/2507.17695", "abs": "https://arxiv.org/abs/2507.17695", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein"], "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks", "categories": ["cs.AI", "cs.NI"], "comment": "Submitted to Computer Networks AI for 6G", "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\u7ed3\u5408\u5b9e\u65f6\u4f18\u5316\u7b97\u6cd5\uff0c\u63d0\u51fa\u201c\u5171\u751f\u4ee3\u7406\u201d\u8303\u5f0f\uff0c\u63d0\u53476G\u7f51\u7edc\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u9519\u8bef\u5e76\u964d\u4f4e\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408LLM\u4e0e\u5b9e\u65f6\u4f18\u5316\u7b97\u6cd5\uff0c\u4ece\u4e13\u7528\u667a\u80fd\u8f6c\u5411\u66f4\u5e7f\u6cdb\u7684AGI\u9a71\u52a8\u7f51\u7edc\uff0c\u4ee5\u63d0\u5347\u7f51\u7edc\u7ba1\u7406\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u4e24\u79cd\u4ee3\u7406\uff08\u65e0\u7ebf\u63a5\u5165\u7f51\u4f18\u5316\u5668\u548cSLA\u591a\u4ee3\u7406\u534f\u5546\u5668\uff09\uff0c\u63d0\u51fa\u7aef\u5230\u7aefAGI\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u57285G\u6d4b\u8bd5\u5e8a\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5171\u751f\u4ee3\u7406\u6bd4\u72ec\u7acbLLM\u4ee3\u7406\u51cf\u5c11\u51b3\u7b56\u9519\u8bef5\u500d\uff0c\u5c0f\u6a21\u578b\u8d44\u6e90\u5f00\u9500\u964d\u4f4e99.9%\uff0c\u591a\u4ee3\u7406\u534f\u4f5c\u51cf\u5c11RAN\u8fc7\u8f7d\u7ea644%\u3002", "conclusion": "\u5171\u751f\u4ee3\u7406\u4e3a\u4e0b\u4e00\u4ee3AGI\u9a71\u52a8\u7f51\u7edc\u63d0\u4f9b\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u5728LLM\u53d1\u5c55\u4e2d\u4fdd\u6301\u9002\u5e94\u6027\u548c\u53ef\u4fe1\u6027\u3002"}}
{"id": "2507.17320", "pdf": "https://arxiv.org/pdf/2507.17320", "abs": "https://arxiv.org/abs/2507.17320", "authors": ["Yuet Ling Wong", "Niklas Elmqvist"], "title": "EventLines: Time Compression for Discrete Event Timelines", "categories": ["cs.HC"], "comment": "10 pages, 6 figures", "summary": "Discrete event sequences serve as models for numerous real-world datasets,\nincluding publications over time, project milestones, and medication dosing\nduring patient treatments. These event sequences typically exhibit bursty\nbehavior, where events cluster together in rapid succession, interspersed with\nperiods of inactivity. Standard timeline charts with linear time axes fail to\nadequately represent such data, resulting in cluttered regions during event\nbursts while leaving other areas unutilized. We introduce EventLines, a novel\ntechnique that dynamically adjusts the time scale to match the underlying event\ndistribution, enabling more efficient use of screen space. To address the\nchallenges of non-linear time scaling, EventLines employs the time axis's\nvisual representation itself to communicate the varying scale. We present\nfindings from a crowdsourced graphical perception study that examines how\ndifferent time scale representations influence temporal perception.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEventLines\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u5c3a\u5ea6\u4ee5\u66f4\u9ad8\u6548\u5730\u5c55\u793a\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7a81\u53d1\u6027\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u65f6\u95f4\u8f74\u5728\u663e\u793a\u8fd9\u7c7b\u6570\u636e\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\uff08\u5982\u51fa\u7248\u7269\u65f6\u95f4\u3001\u9879\u76ee\u91cc\u7a0b\u7891\u3001\u60a3\u8005\u6cbb\u7597\u4e2d\u7684\u7528\u836f\u8bb0\u5f55\uff09\u5e38\u8868\u73b0\u51fa\u7a81\u53d1\u6027\u884c\u4e3a\uff0c\u800c\u4f20\u7edf\u7ebf\u6027\u65f6\u95f4\u8f74\u65e0\u6cd5\u6709\u6548\u5c55\u793a\u8fd9\u7c7b\u6570\u636e\uff0c\u9020\u6210\u89c6\u89c9\u6df7\u4e71\u548c\u7a7a\u95f4\u6d6a\u8d39\u3002", "method": "\u63d0\u51faEventLines\u6280\u672f\uff0c\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u5c3a\u5ea6\u4ee5\u5339\u914d\u4e8b\u4ef6\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u8f74\u7684\u89c6\u89c9\u8868\u73b0\u4f20\u8fbe\u53d8\u5316\u7684\u5c3a\u5ea6\u3002\u91c7\u7528\u4f17\u5305\u56fe\u5f62\u611f\u77e5\u7814\u7a76\u8bc4\u4f30\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u8868\u793a\u5bf9\u65f6\u95f4\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u901a\u8fc7\u56fe\u5f62\u611f\u77e5\u5b9e\u9a8c\u9a8c\u8bc1\u4e86EventLines\u5728\u5c55\u793a\u7a81\u53d1\u6027\u4e8b\u4ef6\u5e8f\u5217\u65f6\u7684\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "EventLines\u901a\u8fc7\u975e\u7ebf\u6027\u7684\u65f6\u95f4\u5c3a\u5ea6\u8c03\u6574\uff0c\u4f18\u5316\u4e86\u7a81\u53d1\u6027\u4e8b\u4ef6\u5e8f\u5217\u7684\u5c55\u793a\u6548\u679c\uff0c\u4e3a\u7c7b\u4f3c\u6570\u636e\u7684\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.17293", "pdf": "https://arxiv.org/pdf/2507.17293", "abs": "https://arxiv.org/abs/2507.17293", "authors": ["Saiful Khan", "Joyraj Chakraborty", "Philip Beaucamp", "Niraj Bhujel", "Min Chen"], "title": "Data Virtualization for Machine Learning", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Nowadays, machine learning (ML) teams have multiple concurrent ML workflows\nfor different applications. Each workflow typically involves many experiments,\niterations, and collaborative activities and commonly takes months and\nsometimes years from initial data wrangling to model deployment.\nOrganizationally, there is a large amount of intermediate data to be stored,\nprocessed, and maintained. \\emph{Data virtualization} becomes a critical\ntechnology in an infrastructure to serve ML workflows. In this paper, we\npresent the design and implementation of a data virtualization service,\nfocusing on its service architecture and service operations. The infrastructure\ncurrently supports six ML applications, each with more than one ML workflow.\nThe data virtualization service allows the number of applications and workflows\nto grow in the coming years.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u865a\u62df\u5316\u670d\u52a1\u7684\u67b6\u6784\u4e0e\u5b9e\u73b0\uff0c\u4ee5\u652f\u6301\u591a\u5e76\u53d1\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u7684\u6570\u636e\u5b58\u50a8\u3001\u5904\u7406\u548c\u7ef4\u62a4\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u589e\u591a\u548c\u590d\u6742\u5316\uff0c\u4e2d\u95f4\u6570\u636e\u7684\u5b58\u50a8\u3001\u5904\u7406\u548c\u7ef4\u62a4\u6210\u4e3a\u7ec4\u7ec7\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6570\u636e\u865a\u62df\u5316\u6280\u672f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u670d\u52a1\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5176\u670d\u52a1\u67b6\u6784\u548c\u670d\u52a1\u64cd\u4f5c\u3002", "result": "\u8be5\u57fa\u7840\u8bbe\u65bd\u76ee\u524d\u652f\u63016\u4e2a\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u6bcf\u4e2a\u5e94\u7528\u5305\u542b\u591a\u4e2a\u5de5\u4f5c\u6d41\uff0c\u5e76\u80fd\u652f\u6301\u672a\u6765\u66f4\u591a\u7684\u5e94\u7528\u548c\u5de5\u4f5c\u6d41\u6269\u5c55\u3002", "conclusion": "\u6570\u636e\u865a\u62df\u5316\u670d\u52a1\u662f\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u6570\u636e\u7ba1\u7406\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0c\u5177\u5907\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.17430", "pdf": "https://arxiv.org/pdf/2507.17430", "abs": "https://arxiv.org/abs/2507.17430", "authors": ["Yan Dong", "Hanjie Yu", "Yanran Chen", "Zipeng Zhang", "Qiong Wu"], "title": "Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces", "categories": ["cs.HC"], "comment": "21 pages, 16 figures, published in ACM CHI 2025", "summary": "Integrating technology with the distinctive characteristics of craftsmanship\nhas become a key issue in the field of digital craftsmanship. This paper\nintroduces Layered Interactions, a design approach that seamlessly merges\nHuman-Computer Interaction (HCI) technologies with traditional lacquerware\ncraftsmanship. By leveraging the multi-layer structure and material properties\nof lacquerware, we embed interactive circuits and integrate programmable\nhardware within the layers, creating tangible interfaces that support diverse\ninteractions. This method enhances the adaptability and practicality of\ntraditional crafts in modern digital contexts. Through the development of a\nlacquerware toolkit, along with user experiments and semi-structured\ninterviews, we demonstrate that this approach not only makes technology more\naccessible to traditional artisans but also enhances the materiality and\nemotional qualities of interactive interfaces. Additionally, it fosters mutual\nlearning and collaboration between artisans and technologists. Our research\nintroduces a cross-disciplinary perspective to the HCI community, broadening\nthe material and design possibilities for interactive interfaces.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u5206\u5c42\u4ea4\u4e92'\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u4e0e\u4f20\u7edf\u6f06\u5668\u5de5\u827a\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u4f20\u7edf\u5de5\u827a\u5728\u73b0\u4ee3\u6570\u5b57\u80cc\u666f\u4e0b\u7684\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u6574\u5408\u6280\u672f\u4e0e\u5de5\u827a\u7279\u8272\u662f\u6570\u5b57\u5de5\u827a\u9886\u57df\u7684\u5173\u952e\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u4f20\u7edf\u5de5\u827a\u7684\u73b0\u4ee3\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5229\u7528\u6f06\u5668\u7684\u591a\u5c42\u7ed3\u6784\u548c\u6750\u6599\u7279\u6027\uff0c\u5d4c\u5165\u4ea4\u4e92\u7535\u8def\u4e0e\u53ef\u7f16\u7a0b\u786c\u4ef6\uff0c\u5f00\u53d1\u4e86\u6f06\u5668\u5de5\u5177\u5305\uff0c\u5e76\u8fdb\u884c\u7528\u6237\u5b9e\u9a8c\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4f7f\u4f20\u7edf\u5de5\u5320\u66f4\u5bb9\u6613\u63a5\u89e6\u6280\u672f\uff0c\u8fd8\u63d0\u5347\u4e86\u4ea4\u4e92\u754c\u9762\u7684\u7269\u8d28\u6027\u548c\u60c5\u611f\u54c1\u8d28\uff0c\u4fc3\u8fdb\u4e86\u5de5\u5320\u4e0e\u6280\u672f\u4eba\u5458\u7684\u5408\u4f5c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u5f15\u5165\u4e86\u8de8\u5b66\u79d1\u89c6\u89d2\uff0c\u62d3\u5bbd\u4e86\u4ea4\u4e92\u754c\u9762\u7684\u6750\u6599\u548c\u8bbe\u8ba1\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.17314", "pdf": "https://arxiv.org/pdf/2507.17314", "abs": "https://arxiv.org/abs/2507.17314", "authors": ["Ricardo Hidalgo Arag\u00f3n", "Jes\u00fas M. Gonz\u00e1lez-Barahona", "Gregorio Robles"], "title": "How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?", "categories": ["cs.SE", "K.3.2, D.2.m, D.1.7"], "comment": "Registered Report accepted at ICSME 2025", "summary": "Context. Code smells, which are recurring anomalies in design or style, have\nbeen extensively researched in professional code. However, their significance\nin block-based projects created by novices is still largely unknown.\nBlock-based environments such as Scratch offer a unique, data-rich setting to\nexamine how emergent design problems intersect with the cultivation of\ncomputational-thinking (CT) skills. Objective. This research explores the\nconnection between CT proficiency and design-level code smells--issues that may\nhinder software maintenance and evolution--in programs created by Scratch\ndevelopers. We seek to identify which CT dimensions align most strongly with\nwhich code smells and whether task context moderates those associations.\nMethod. A random sample of aprox. 2 million public Scratch projects is mined.\nUsing open-source linters, we extract nine CT scores and 40 code smell\nindicators from these projects. After rigorous pre-processing, we apply\ndescriptive analytics, robust correlation tests, stratified cross-validation,\nand exploratory machine-learning models; qualitative spot-checks contextualize\nquantitative patterns. Impact. The study will deliver the first large-scale,\nfine-grained map linking specific CT competencies to concrete design flaws and\nantipatterns. Results are poised to (i) inform evidence-based curricula and\nautomated feedback systems, (ii) provide effect-size benchmarks for future\neducational interventions, and (iii) supply an open, pseudonymized dataset and\nreproducible analysis pipeline for the research community. By clarifying how\nprogramming habits influence early skill acquisition, the work advances both\ncomputing-education theory and practical tooling for sustainable software\nmaintenance and evolution.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65b0\u624b\u5728Scratch\u4e2d\u521b\u5efa\u7684\u4ee3\u7801\u5757\u9879\u76ee\u4e2d\u7684\u8bbe\u8ba1\u95ee\u9898\uff08\u4ee3\u7801\u5f02\u5473\uff09\u4e0e\u8ba1\u7b97\u601d\u7ef4\uff08CT\uff09\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u63ed\u793a\u4e86CT\u7ef4\u5ea6\u4e0e\u4ee3\u7801\u5f02\u5473\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u65e8\u5728\u4e3a\u6559\u80b2\u5e72\u9884\u548c\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u7814\u7a76\u80cc\u666f\u662f\u4ee3\u7801\u5f02\u5473\u5728\u4e13\u4e1a\u4ee3\u7801\u4e2d\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u65b0\u624b\u521b\u5efa\u7684\u5757\u72b6\u9879\u76ee\u4e2d\u4ecd\u4e0d\u660e\u786e\uff0cScratch\u73af\u5883\u4e3a\u7814\u7a76\u8bbe\u8ba1\u95ee\u9898\u4e0e\u8ba1\u7b97\u601d\u7ef4\u6280\u80fd\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\u3002", "method": "\u4ece\u7ea6200\u4e07\u4e2a\u516c\u5171Scratch\u9879\u76ee\u4e2d\u968f\u673a\u62bd\u6837\uff0c\u4f7f\u7528\u5f00\u6e90\u5de5\u5177\u63d0\u53d69\u4e2aCT\u5206\u6570\u548c40\u4e2a\u4ee3\u7801\u5f02\u5473\u6307\u6807\uff0c\u901a\u8fc7\u63cf\u8ff0\u6027\u5206\u6790\u3001\u76f8\u5173\u6d4b\u8bd5\u3001\u4ea4\u53c9\u9a8c\u8bc1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5c06\u9996\u6b21\u5927\u89c4\u6a21\u63ed\u793a\u7279\u5b9aCT\u80fd\u529b\u4e0e\u8bbe\u8ba1\u7f3a\u9677\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u4e3a\u6559\u80b2\u8bfe\u7a0b\u548c\u53cd\u9988\u7cfb\u7edf\u63d0\u4f9b\u4f9d\u636e\uff0c\u5e76\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u5f00\u653e\u6570\u636e\u96c6\u548c\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u7f16\u7a0b\u4e60\u60ef\u5bf9\u65e9\u671f\u6280\u80fd\u4e60\u5f97\u7684\u5f71\u54cd\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u6559\u80b2\u7406\u8bba\u548c\u8f6f\u4ef6\u7ef4\u62a4\u5de5\u5177\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.17614", "pdf": "https://arxiv.org/pdf/2507.17614", "abs": "https://arxiv.org/abs/2507.17614", "authors": ["Marco De Pascale", "Tobias Valentin Bauer", "Yaknan John Gambo", "Mario Hern\u00e1ndez Vera", "Stefan Huber", "Burak Mete", "Amit Jamadagni", "Amine Bentellis", "Marita Oliv", "Luigi Iapichino", "Jeanette Miriam Lorenz"], "title": "Comparing performance of variational quantum algorithm simulations on HPC systems", "categories": ["quant-ph", "cs.DC"], "comment": null, "summary": "Variational quantum algorithms are of special importance in the research on\nquantum computing applications because of their applicability to current Noisy\nIntermediate-Scale Quantum (NISQ) devices. The main building blocks of these\nalgorithms (among them, the definition of the Hamiltonian and of the ansatz,\nthe optimizer) define a relatively large parameter space, making the comparison\nof results and performance between different approaches and software simulators\ncumbersome and prone to errors. In this paper, we employ a generic description\nof the problem, in terms of both Hamiltonian and ansatz, to port a problem\ndefinition consistently among different simulators. Three use cases of\nrelevance for current quantum hardware (ground state calculation for the\nHydrogen molecule, MaxCut, Travelling Salesman Problem) have been run on a set\nof HPC systems and software simulators to study the dependence of performance\non the runtime environment, the scalability of the simulation codes and the\nmutual agreement of the physical results, respectively. The results show that\nour toolchain can successfully translate a problem definition between different\nsimulators. On the other hand, variational algorithms are limited in their\nscaling by the long runtimes with respect to their memory footprint, so they\nexpose limited parallelism to computation. This shortcoming is partially\nmitigated by using techniques like job arrays. The potential of the parser tool\nfor exploring HPC performance and comparisons of results of variational\nalgorithm simulations is highlighted.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u63cf\u8ff0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u5668\u95f4\u4e00\u81f4\u5730\u8f6c\u6362\u95ee\u9898\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u7528\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u6269\u5c55\u6027\u9650\u5236\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u5728\u5f53\u524dNISQ\u8bbe\u5907\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u53c2\u6570\u7a7a\u95f4\u5927\u5bfc\u81f4\u4e0d\u540c\u6a21\u62df\u5668\u7684\u7ed3\u679c\u6bd4\u8f83\u56f0\u96be\u4e14\u6613\u51fa\u9519\u3002", "method": "\u4f7f\u7528\u901a\u7528\u63cf\u8ff0\u65b9\u6cd5\uff08\u5305\u62ec\u54c8\u5bc6\u987f\u91cf\u548cansatz\u5b9a\u4e49\uff09\u5728\u591a\u4e2aHPC\u7cfb\u7edf\u548c\u8f6f\u4ef6\u6a21\u62df\u5668\u4e0a\u8fd0\u884c\u4e09\u4e2a\u7528\u4f8b\uff08\u6c22\u5206\u5b50\u57fa\u6001\u8ba1\u7b97\u3001MaxCut\u3001\u65c5\u884c\u5546\u95ee\u9898\uff09\u3002", "result": "\u5de5\u5177\u94fe\u80fd\u6210\u529f\u5728\u4e0d\u540c\u6a21\u62df\u5668\u95f4\u8f6c\u6362\u95ee\u9898\u5b9a\u4e49\uff0c\u4f46\u53d8\u5206\u7b97\u6cd5\u56e0\u8fd0\u884c\u65f6\u95f4\u957f\u9650\u5236\u4e86\u6269\u5c55\u6027\uff0c\u90e8\u5206\u901a\u8fc7\u4f5c\u4e1a\u6570\u7ec4\u7b49\u6280\u672f\u7f13\u89e3\u3002", "conclusion": "\u89e3\u6790\u5de5\u5177\u5728\u63a2\u7d22HPC\u6027\u80fd\u548c\u53d8\u5206\u7b97\u6cd5\u7ed3\u679c\u6bd4\u8f83\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.17524", "pdf": "https://arxiv.org/pdf/2507.17524", "abs": "https://arxiv.org/abs/2507.17524", "authors": ["Jiahao Tang", "Youjun Li", "Xiangting Fan", "Yangxuan Zheng", "Siyuan Lu", "Xueping Li", "Peng Fang", "Chenxi Li", "Zi-Gang Huang"], "title": "SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition", "categories": ["cs.HC"], "comment": null, "summary": "Electroencephalography(EEG) based emotion recognition holds great promise for\naffective brain-computer interfaces (aBCIs), yet practical deployment remains\nchallenging due to substantial inter-subject variability and the lack of\nlabeled data in target domains. To overcome these limitations, we present a\nnovel unsupervised Semantic-Dynamic Consistency domain adaptation network for\nfully label-free cross-subject EEG emotion recognition. First, we introduce a\nSame-Subject Same-Trial Mixup strategy that generates augmented samples via\nintra-trial interpolation, enhancing data diversity while explicitly preserving\nindividual identity to mitigate label ambiguity. Second, we construct a dynamic\ndistribution alignment module in reproducing kernel Hilbert space (RKHS),\njointly aligning marginal and conditional distributions through multi-objective\nkernel mean embedding, and leveraging a confidence-aware pseudo-labeling\nstrategy to ensure stable adaptation. Third, we propose a dual-domain\nsimilarity consistency learning mechanism that enforces cross-domain structural\nconstraints based on latent pairwise similarities, enabling semantic boundary\nlearning without relying on temporal synchronization or label priors. To\nvalidate the effectiveness and robustness of the proposed SDC-Net, extensive\nexperiments are conducted on three widely used EEG benchmark datasets: SEED,\nSEED-IV, and Faced. Comparative results against existing unsupervised domain\nadaptation methods demonstrate that SDC-Net achieves state-of-the-art\nperformance in emotion recognition under both cross-subject and cross-session\nconditions. This advancement significantly improves the accuracy and\ngeneralization capability of emotion decoding, and lays a solid foundation for\nreal-world applications of personalized affective brain-computer interfaces\n(aBCIs). The source code will be released at\nhttps://github.com/XuanSuTrum/SDC-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u57df\u9002\u5e94\u7f51\u7edcSDC-Net\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u3001\u52a8\u6001\u5206\u5e03\u5bf9\u9f50\u548c\u53cc\u57df\u76f8\u4f3c\u6027\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u89e3\u51b3\u8de8\u88ab\u8bd5EEG\u60c5\u611f\u8bc6\u522b\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u8de8\u88ab\u8bd5\u5dee\u5f02\u548c\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528Same-Subject Same-Trial Mixup\u6570\u636e\u589e\u5f3a\u3001RKHS\u4e2d\u7684\u52a8\u6001\u5206\u5e03\u5bf9\u9f50\u53ca\u4f2a\u6807\u7b7e\u7b56\u7565\u3001\u53cc\u57df\u76f8\u4f3c\u6027\u4e00\u81f4\u6027\u5b66\u4e60\u3002", "result": "\u5728SEED\u3001SEED-IV\u548cFaced\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "SDC-Net\u663e\u8457\u63d0\u5347\u60c5\u611f\u89e3\u7801\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e2a\u6027\u5316aBCIs\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.17369", "pdf": "https://arxiv.org/pdf/2507.17369", "abs": "https://arxiv.org/abs/2507.17369", "authors": ["Corentin Latappy", "Thomas Degueule", "Jean-R\u00e9my Falleri", "Romain Robbes", "Lina Ochoa"], "title": "Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java", "categories": ["cs.SE"], "comment": null, "summary": "Understanding API evolution and the introduction of breaking changes (BCs) in\nsoftware libraries is essential for library maintainers to manage backward\ncompatibility and for researchers to conduct empirical studies on software\nlibrary evolution. In Java, tools such as JApiCmp and Revapi are commonly used\nto detect BCs between library releases, but their reliance on binary JARs\nlimits their applicability. This restriction hinders large-scale longitudinal\nstudies of API evolution and fine-grained analyses such as commit-level BC\ndetection. In this paper, we introduce Roseau, a novel static analysis tool\nthat constructs technology-agnostic API models from library code equipped with\nrich semantic analyses. API models can be analyzed to study API evolution and\ncompared to identify BCs between any two versions of a library (releases,\ncommits, branches, etc.). Unlike traditional approaches, Roseau can build API\nmodels from source code or bytecode, and is optimized for large-scale\nlongitudinal analyses of library histories. We assess the accuracy,\nperformance, and suitability of Roseau for longitudinal studies of API\nevolution, using JApiCmp and Revapi as baselines. We extend and refine an\nestablished benchmark of BCs and show that Roseau achieves higher accuracy (F1\n= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular\nlibraries from Maven Central and find that Roseau delivers excellent\nperformance, detecting BCs between versions in under two seconds, including in\nlibraries with hundreds of thousands of lines of code. We further illustrate\nthe limitations of JApiCmp and Revapi for longitudinal studies and the novel\nanalysis capabilities offered by Roseau by tracking the evolution of Google's\nGuava API and the introduction of BCs over 14 years and 6,839 commits, reducing\nanalysis times from a few days to a few minutes.", "AI": {"tldr": "Roseau\u662f\u4e00\u79cd\u65b0\u578b\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u4ee3\u7801\u6784\u5efa\u6280\u672f\u65e0\u5173\u7684API\u6a21\u578b\uff0c\u4ee5\u7814\u7a76API\u6f14\u5316\u548c\u68c0\u6d4b\u7834\u574f\u6027\u53d8\u66f4\uff08BCs\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u5177JApiCmp\u548cRevapi\u3002", "motivation": "\u7814\u7a76API\u6f14\u5316\u548cBCs\u5bf9\u5e93\u7ef4\u62a4\u8005\u548c\u7814\u7a76\u4eba\u5458\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u4f9d\u8d56\u4e8c\u8fdb\u5236JAR\u6587\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faRoseau\uff0c\u652f\u6301\u4ece\u6e90\u4ee3\u7801\u6216\u5b57\u8282\u7801\u6784\u5efaAPI\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5927\u89c4\u6a21\u7eb5\u5411\u5206\u6790\u3002", "result": "Roseau\u5728\u51c6\u786e\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u5de5\u5177\uff0cF1\u5f97\u5206\u4e3a0.99\uff0c\u5206\u6790\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Roseau\u4e3aAPI\u6f14\u5316\u548cBCs\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7814\u7a76\u3002"}}
{"id": "2507.17543", "pdf": "https://arxiv.org/pdf/2507.17543", "abs": "https://arxiv.org/abs/2507.17543", "authors": ["Xue Wen Tan", "Kenneth See", "Stanley Kok"], "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams", "categories": ["cs.HC"], "comment": null, "summary": "The rapid growth of messaging scams creates an escalating challenge for user\nsecurity and financial safety. In this paper, we present the Anticipate,\nSimulate, Reason (ASR) framework, a generative AI method that enables users to\nproactively identify and comprehend scams within instant messaging platforms.\nUsing large language models, ASR predicts scammer responses, creates realistic\nscam conversations, and delivers real-time, interpretable support to end-users.\nWe develop ScamGPT-J, a domain-specific language model fine-tuned on a new,\nhigh-quality dataset of scam conversations covering multiple scam types.\nThorough experimental evaluation shows that the ASR framework substantially\nenhances scam detection, particularly in challenging contexts such as job\nscams, and uncovers important demographic patterns in user vulnerability and\nperceptions of AI-generated assistance. Our findings reveal a contradiction\nwhere those most at risk are often least receptive to AI support, emphasizing\nthe importance of user-centered design in AI-driven fraud prevention. This work\nadvances both the practical and theoretical foundations for interpretable,\nhuman-centered AI systems in combating evolving digital threats.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ASR\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u65b9\u6cd5\u5e2e\u52a9\u7528\u6237\u4e3b\u52a8\u8bc6\u522b\u548c\u7406\u89e3\u5373\u65f6\u901a\u8baf\u5e73\u53f0\u4e2d\u7684\u8bc8\u9a97\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u8bc8\u9a97\u4fe1\u606f\u7684\u5feb\u901f\u589e\u957f\uff0c\u7528\u6237\u5728\u5b89\u5168\u548c\u8d22\u52a1\u65b9\u9762\u9762\u4e34\u65e5\u76ca\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u8bc6\u522b\u548c\u7406\u89e3\u8bc8\u9a97\u7684\u5de5\u5177\u3002", "method": "ASR\u6846\u67b6\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u9884\u6d4b\u8bc8\u9a97\u8005\u56de\u590d\u3001\u751f\u6210\u903c\u771f\u7684\u8bc8\u9a97\u5bf9\u8bdd\uff0c\u5e76\u63d0\u4f9b\u5b9e\u65f6\u89e3\u91ca\u6027\u652f\u6301\u3002\u540c\u65f6\uff0c\u8bba\u6587\u5f00\u53d1\u4e86\u9488\u5bf9\u8bc8\u9a97\u5bf9\u8bdd\u7684\u9886\u57df\u4e13\u7528\u6a21\u578bScamGPT-J\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASR\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u8bc8\u9a97\u68c0\u6d4b\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5de5\u4f5c\u8bc8\u9a97\u7b49\u590d\u6742\u573a\u666f\u4e2d\uff0c\u5e76\u63ed\u793a\u4e86\u7528\u6237\u5bf9AI\u751f\u6210\u652f\u6301\u7684\u53cd\u611f\u4e0e\u9ad8\u98ce\u9669\u7528\u6237\u7fa4\u4f53\u7684\u77db\u76fe\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u5728AI\u9a71\u52a8\u7684\u8bc8\u9a97\u9884\u9632\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u53ef\u89e3\u91ca\u3001\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2507.17389", "pdf": "https://arxiv.org/pdf/2507.17389", "abs": "https://arxiv.org/abs/2507.17389", "authors": ["Tianlin Li", "Yunxiang Wei", "Zhiming Li", "Aishan Liu", "Qing Guo", "Xianglong Liu", "Dongning Sun", "Yang Liu"], "title": "Investigating Training Data Detection in AI Coders", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Recent advances in code large language models (CodeLLMs) have made them\nindispensable tools in modern software engineering. However, these models\noccasionally produce outputs that contain proprietary or sensitive code\nsnippets, raising concerns about potential non-compliant use of training data,\nand posing risks to privacy and intellectual property. To ensure responsible\nand compliant deployment of CodeLLMs, training data detection (TDD) has become\na critical task. While recent TDD methods have shown promise in natural\nlanguage settings, their effectiveness on code data remains largely\nunderexplored. This gap is particularly important given code's structured\nsyntax and distinct similarity criteria compared to natural language. To\naddress this, we conduct a comprehensive empirical study of seven\nstate-of-the-art TDD methods on source code data, evaluating their performance\nacross eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a\nfunction-level benchmark dataset comprising 9,000 code samples in three\nprogramming languages, each explicitly labeled as either included or excluded\nfrom CodeLLM training. Beyond evaluation on the original CodeSnitch, we design\ntargeted mutation strategies to test the robustness of TDD methods under three\ndistinct settings. These mutation strategies are grounded in the\nwell-established Type-1 to Type-4 code clone detection taxonomy. Our study\nprovides a systematic assessment of current TDD techniques for code and offers\ninsights to guide the development of more effective and robust detection\nmethods in the future.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08CodeLLMs\uff09\u4e2d\u8bad\u7ec3\u6570\u636e\u68c0\u6d4b\uff08TDD\uff09\u7684\u6311\u6218\uff0c\u63d0\u51faCodeSnitch\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e03\u79cdTDD\u65b9\u6cd5\u5728\u4ee3\u7801\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "CodeLLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8f93\u51fa\u53ef\u80fd\u5305\u542b\u654f\u611f\u4ee3\u7801\u7247\u6bb5\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u77e5\u8bc6\u4ea7\u6743\u95ee\u9898\uff0c\u9700\u8981\u7814\u7a76TDD\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u4e86\u4e03\u79cdTDD\u65b9\u6cd5\uff0c\u5f15\u5165CodeSnitch\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4ee3\u7801\u514b\u9686\u5206\u7c7b\u7684\u7a81\u53d8\u7b56\u7565\u4ee5\u6d4b\u8bd5\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u7f16\u7a0b\u8bed\u8a00\u76849000\u4e2a\u4ee3\u7801\u6837\u672c\u4e0a\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5f53\u524dTDD\u6280\u672f\u5728\u4ee3\u7801\u6570\u636e\u4e0a\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u548c\u9c81\u68d2\u7684TDD\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.17597", "pdf": "https://arxiv.org/pdf/2507.17597", "abs": "https://arxiv.org/abs/2507.17597", "authors": ["Sue Min Cho", "Alexander Do", "Russell H. Taylor", "Mathias Unberath"], "title": "Explainable AI for Collaborative Assessment of 2D/3D Registration Quality", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "As surgery embraces digital transformation--integrating sophisticated\nimaging, advanced algorithms, and robotics to support and automate complex\nsub-tasks--human judgment of system correctness remains a vital safeguard for\npatient safety. This shift introduces new \"operator-type\" roles tasked with\nverifying complex algorithmic outputs, particularly at critical junctures of\nthe procedure, such as the intermediary check before drilling or implant\nplacement. A prime example is 2D/3D registration, a key enabler of image-based\nsurgical navigation that aligns intraoperative 2D images with preoperative 3D\ndata. Although registration algorithms have advanced significantly, they\noccasionally yield inaccurate results. Because even small misalignments can\nlead to revision surgery or irreversible surgical errors, there is a critical\nneed for robust quality assurance. Current visualization-based strategies alone\nhave been found insufficient to enable humans to reliably detect 2D/3D\nregistration misalignments. In response, we propose the first artificial\nintelligence (AI) framework trained specifically for 2D/3D registration quality\nverification, augmented by explainability features that clarify the model's\ndecision-making. Our explainable AI (XAI) approach aims to enhance informed\ndecision-making for human operators by providing a second opinion together with\na rationale behind it. Through algorithm-centric and human-centered\nevaluations, we systematically compare four conditions: AI-only, human-only,\nhuman-AI, and human-XAI. Our findings reveal that while explainability features\nmodestly improve user trust and willingness to override AI errors, they do not\nexceed the standalone AI in aggregate performance. Nevertheless, future work\nextending both the algorithmic design and the human-XAI collaboration elements\nholds promise for more robust quality assurance of 2D/3D registration.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u76842D/3D\u914d\u51c6\u8d28\u91cf\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u7279\u5f81\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86AI\u3001\u4eba\u7c7b\u4ee5\u53ca\u4e24\u8005\u534f\u4f5c\u7684\u6548\u679c\u3002", "motivation": "\u624b\u672f\u6570\u5b57\u5316\u8f6c\u578b\u4e2d\uff0c2D/3D\u914d\u51c6\u7b97\u6cd5\u7684\u5076\u5c14\u9519\u8bef\u53ef\u80fd\u5f15\u53d1\u4e25\u91cd\u540e\u679c\uff0c\u73b0\u6709\u53ef\u89c6\u5316\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u8d28\u91cf\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u9a8c\u8bc1\u624b\u6bb5\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf92D/3D\u914d\u51c6\u8d28\u91cf\u9a8c\u8bc1\u7684AI\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u7279\u5f81\uff08XAI\uff09\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u548c\u4eba\u4e3a\u4e2d\u5fc3\u8bc4\u4f30\u6bd4\u8f83\u4e86\u56db\u79cd\u6761\u4ef6\uff08AI-only\u3001human-only\u3001human-AI\u3001human-XAI\uff09\u3002", "result": "\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u867d\u5c0f\u5e45\u63d0\u5347\u7528\u6237\u4fe1\u4efb\u548c\u7ea0\u9519\u610f\u613f\uff0c\u4f46\u672a\u8d85\u8d8a\u5355\u72ecAI\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u53ef\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u8fdb\u4e00\u6b65\u63d0\u53472D/3D\u914d\u51c6\u8d28\u91cf\u9a8c\u8bc1\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.17542", "pdf": "https://arxiv.org/pdf/2507.17542", "abs": "https://arxiv.org/abs/2507.17542", "authors": ["Lara Khatib", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests", "categories": ["cs.SE"], "comment": null, "summary": "Bug reproduction is critical in the software debugging and repair process,\nyet the majority of bugs in open-source and industrial settings lack executable\ntests to reproduce them at the time they are reported, making diagnosis and\nresolution more difficult and time-consuming. To address this challenge, we\nintroduce AssertFlip, a novel technique for automatically generating Bug\nReproducible Tests (BRTs) using large language models (LLMs). Unlike existing\nmethods that attempt direct generation of failing tests, AssertFlip first\ngenerates passing tests on the buggy behaviour and then inverts these tests to\nfail when the bug is present. We hypothesize that LLMs are better at writing\npassing tests than ones that crash or fail on purpose. Our results show that\nAssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a\nbenchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass\nsuccess rate of 43.6% on the SWT-Bench-Verified subset.", "AI": {"tldr": "AssertFlip\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u751f\u6210\u53ef\u590d\u73b0\u9519\u8bef\u6d4b\u8bd5\uff08BRTs\uff09\u7684\u65b0\u6280\u672f\uff0c\u5176\u901a\u8fc7\u9996\u5148\u751f\u6210\u901a\u8fc7\u6d4b\u8bd5\u518d\u53cd\u8f6c\u4ee5\u68c0\u6d4b\u9519\u8bef\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f00\u6e90\u548c\u5de5\u4e1a\u73af\u5883\u4e2d\u5927\u591a\u6570\u9519\u8bef\u56e0\u7f3a\u4e4f\u53ef\u6267\u884c\u6d4b\u8bd5\u800c\u96be\u4ee5\u590d\u73b0\u548c\u8bca\u65ad\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u751f\u6210\u901a\u8fc7\u6d4b\u8bd5\uff0c\u518d\u53cd\u8f6c\u8fd9\u4e9b\u6d4b\u8bd5\u4ee5\u5728\u9519\u8bef\u5b58\u5728\u65f6\u5931\u8d25\u3002", "result": "\u5728SWT-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5931\u8d25\u5230\u6210\u529f\u7387\u4e3a43.6%\u3002", "conclusion": "AssertFlip\u901a\u8fc7LLMs\u751f\u6210BRTs\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u590d\u73b0\u7684\u6548\u7387\u3002"}}
{"id": "2507.17688", "pdf": "https://arxiv.org/pdf/2507.17688", "abs": "https://arxiv.org/abs/2507.17688", "authors": ["Mohammad Nur Hossain Khan", "David creswell", "Jordan Albert", "Patrick O'Connell", "Shawn Fallon", "Mathew Polowitz", "Xuhai \"orson\" Xu", "Bashima islam"], "title": "Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills", "categories": ["cs.HC", "cs.LG"], "comment": "Accepted in Proc. ACM Interact. Mob. Wearable Ubiquitous Technology\n  (IMWUT)", "summary": "Mindfulness training is widely recognized for its benefits in reducing\ndepression, anxiety, and loneliness. With the rise of smartphone-based\nmindfulness apps, digital meditation has become more accessible, but sustaining\nlong-term user engagement remains a challenge. This paper explores whether\nrespiration biosignal feedback and mindfulness skill estimation enhance system\nusability and skill development. We develop a smartphone's accelerometer-based\nrespiration tracking algorithm, eliminating the need for additional wearables.\nUnlike existing methods, our approach accurately captures slow breathing\npatterns typical of mindfulness meditation. Additionally, we introduce the\nfirst quantitative framework to estimate mindfulness skills-concentration,\nsensory clarity, and equanimity-based on accelerometer-derived respiration\ndata. We develop and test our algorithms on 261 mindfulness sessions in both\ncontrolled and real-world settings. A user study comparing an experimental\ngroup receiving biosignal feedback with a control group using a standard app\nshows that respiration feedback enhances system usability. Our respiration\ntracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,\nclosely aligning with ground truth data, while our mindfulness skill estimation\nattains F1 scores of 80-84% in tracking skill progression. By integrating\nrespiration tracking and mindfulness estimation into a commercial app, we\ndemonstrate the potential of smartphone sensors to enhance digital mindfulness\ntraining.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u547c\u5438\u751f\u7269\u4fe1\u53f7\u53cd\u9988\u548c\u6b63\u5ff5\u6280\u80fd\u63d0\u5347\u7cfb\u7edf\u53ef\u7528\u6027\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u52a0\u901f\u5ea6\u8ba1\u7684\u65e0\u7a7f\u6234\u8bbe\u5907\u547c\u5438\u8ffd\u8e2a\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u5b9a\u91cf\u6846\u67b6\u6765\u8bc4\u4f30\u6b63\u5ff5\u6280\u80fd\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u6b63\u5ff5\u5e94\u7528\u666e\u53ca\uff0c\u4f46\u957f\u671f\u7528\u6237\u53c2\u4e0e\u5ea6\u4f4e\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u547c\u5438\u53cd\u9988\u548c\u6280\u80fd\u4f30\u8ba1\u63d0\u5347\u7cfb\u7edf\u6548\u679c\u3002", "method": "\u5f00\u53d1\u52a0\u901f\u5ea6\u8ba1\u547c\u5438\u8ffd\u8e2a\u7b97\u6cd5\u548c\u6b63\u5ff5\u6280\u80fd\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7261\u6b21\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u6807\u51c6\u5e94\u7528\u5bf9\u6bd4\u7528\u6237\u7814\u7a76\u3002", "result": "\u547c\u5438\u8ffd\u8e2aMAE\u4e3a1.6\u6b21/\u5206\u949f\uff0c\u6280\u80fd\u8bc4\u4f30F1\u5206\u657080-84%\uff0c\u547c\u5438\u53cd\u9988\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "conclusion": "\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u53ef\u7528\u4e8e\u589e\u5f3a\u6570\u5b57\u6b63\u5ff5\u8bad\u7ec3\uff0c\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.17548", "pdf": "https://arxiv.org/pdf/2507.17548", "abs": "https://arxiv.org/abs/2507.17548", "authors": ["Lingxiao Tang", "He Ye", "Zhongxin Liu", "Xiaoxue Ren", "Lingfeng Bao"], "title": "CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning", "categories": ["cs.SE"], "comment": null, "summary": "Code reasoning is a fundamental capability for large language models (LLMs)\nin the code domain. It involves understanding and predicting a program's\nexecution behavior, such as determining the output for a given input or whether\na specific statement will be executed. This capability is essential for\ndownstream tasks like debugging, code generation, and program repair. Prior\napproaches mainly rely on supervised fine-tuning to improve performance in code\nreasoning tasks. However, they often show limited gains and fail to generalize\nacross diverse scenarios. We argue this is due to two core issues: the low\nquality of training data and the limitations of supervised fine-tuning, which\nstruggles to teach general reasoning skills. To address these challenges, we\npropose CodeReasoner, a framework that spans both dataset construction and a\ntwo-stage training process. First, we introduce a method to construct datasets\nthat focus on the core execution logic of Python programs. Next, we apply\ninstruction tuning to inject execution-specific knowledge distilled from a\npowerful teacher model. We then enhance reasoning and generalization through\nGRPO reinforcement learning on top of the fine-tuned model. Experiments on\nthree widely-used code reasoning benchmarks show that CodeReasoner improves\nperformance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the\n7B model matches GPT-4o on key tasks like input/output and coverage prediction.\nWhen scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.\nAblation studies confirm the effectiveness of each training stage and highlight\nthe importance of reasoning chains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCodeReasoner\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u96c6\u6784\u5efa\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4f4e\u548c\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u6559\u6388\u901a\u7528\u63a8\u7406\u6280\u80fd\u3002", "method": "CodeReasoner\u6846\u67b6\u5305\u62ec\u6784\u5efa\u4e13\u6ce8\u4e8ePython\u7a0b\u5e8f\u6838\u5fc3\u6267\u884c\u903b\u8f91\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u6ce8\u5165\u6267\u884c\u76f8\u5173\u77e5\u8bc6\uff0c\u5e76\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodeReasoner\uff087B\u6a21\u578b\uff09\u6027\u80fd\u63d0\u5347\u4e8627.1%\u81f340.2%\uff0c\u4e14\u4e0eGPT-4o\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u8868\u73b0\u76f8\u5f53\uff1b14B\u6a21\u578b\u5728\u6240\u6709\u57fa\u51c6\u4e0a\u8d85\u8d8aGPT-4o\u3002", "conclusion": "CodeReasoner\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u63a8\u7406\u94fe\u6761\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.17734", "pdf": "https://arxiv.org/pdf/2507.17734", "abs": "https://arxiv.org/abs/2507.17734", "authors": ["Liwenhan Xie", "Yanna Lin", "Can Liu", "Huamin Qu", "Xinhuan Shu"], "title": "DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models", "categories": ["cs.HC"], "comment": "Accepted to the IEEE Visualization Conference (VIS'25). 11 pages, 6\n  figures", "summary": "Creating aesthetically pleasing data visualizations remains challenging for\nusers without design expertise or familiarity with visualization tools. To\naddress this gap, we present DataWink, a system that enables users to create\ncustom visualizations by adapting high-quality examples. Our approach combines\nlarge multimodal models (LMMs) to extract data encoding from existing SVG-based\nvisualization examples, featuring an intermediate representation of\nvisualizations that bridges primitive SVG and visualization programs. Users may\nexpress adaptation goals to a conversational agent and control the visual\nappearance through widgets generated on demand. With an interactive interface,\nusers can modify both data mappings and visual design elements while\nmaintaining the original visualization's aesthetic quality. To evaluate\nDataWink, we conduct a user study (N=12) with replication and free-form\nexploration tasks. As a result, DataWink is recognized for its learnability and\neffectiveness in personalized authoring tasks. Our results demonstrate the\npotential of example-driven approaches for democratizing visualization\ncreation.", "AI": {"tldr": "DataWink\u662f\u4e00\u4e2a\u901a\u8fc7\u793a\u4f8b\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u5e2e\u52a9\u975e\u4e13\u4e1a\u7528\u6237\u521b\u5efa\u7f8e\u89c2\u7684\u6570\u636e\u53ef\u89c6\u5316\u56fe\u8868\u3002", "motivation": "\u89e3\u51b3\u975e\u8bbe\u8ba1\u6216\u6280\u672f\u7528\u6237\u521b\u5efa\u9ad8\u8d28\u91cf\u53ef\u89c6\u5316\u7684\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u63d0\u53d6SVG\u793a\u4f8b\u7684\u6570\u636e\u7f16\u7801\uff0c\u63d0\u4f9b\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u548c\u52a8\u6001\u63a7\u4ef6\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u6613\u5b66\u4e14\u6709\u6548\u3002", "conclusion": "\u793a\u4f8b\u9a71\u52a8\u65b9\u6cd5\u6709\u52a9\u4e8e\u666e\u53ca\u53ef\u89c6\u5316\u521b\u4f5c\u3002"}}
{"id": "2507.17690", "pdf": "https://arxiv.org/pdf/2507.17690", "abs": "https://arxiv.org/abs/2507.17690", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "Contextual Code Retrieval for Commit Message Generation: A Preliminary Study", "categories": ["cs.SE"], "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM)", "summary": "A commit message describes the main code changes in a commit and plays a\ncrucial role in software maintenance. Existing commit message generation (CMG)\napproaches typically frame it as a direct mapping which inputs a code diff and\nproduces a brief descriptive sentence as output. However, we argue that relying\nsolely on the code diff is insufficient, as raw code diff fails to capture the\nfull context needed for generating high-quality and informative commit\nmessages. In this paper, we propose a contextual code retrieval-based method\ncalled C3Gen to enhance CMG by retrieving commit-relevant code snippets from\nthe repository and incorporating them into the model input to provide richer\ncontextual information at the repository scope. In the experiments, we\nevaluated the effectiveness of C3Gen across various models using four objective\nand three subjective metrics. Meanwhile, we design and conduct a human\nevaluation to investigate how C3Gen-generated commit messages are perceived by\nhuman developers. The results show that by incorporating contextual code into\nthe input, C3Gen enables models to effectively leverage additional information\nto generate more comprehensive and informative commit messages with greater\npractical value in real-world development scenarios. Further analysis\nunderscores concerns about the reliability of similaritybased metrics and\nprovides empirical insights for CMG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4ee3\u7801\u68c0\u7d22\u7684\u65b9\u6cd5C3Gen\uff0c\u7528\u4e8e\u6539\u8fdb\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\uff08CMG\uff09\uff0c\u901a\u8fc7\u68c0\u7d22\u5e76\u6574\u5408\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u751f\u6210\u66f4\u5168\u9762\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u63d0\u4ea4\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4ee3\u7801\u5dee\u5f02\uff0c\u7f3a\u4e4f\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u63d0\u4ea4\u4fe1\u606f\u3002", "method": "\u63d0\u51faC3Gen\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u63d0\u4ea4\u76f8\u5173\u7684\u4ee3\u7801\u7247\u6bb5\u5e76\u5c06\u5176\u6574\u5408\u5230\u6a21\u578b\u8f93\u5165\u4e2d\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cC3Gen\u80fd\u591f\u5229\u7528\u989d\u5916\u4fe1\u606f\u751f\u6210\u66f4\u5168\u9762\u4e14\u5b9e\u7528\u7684\u63d0\u4ea4\u4fe1\u606f\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "C3Gen\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u4ee3\u7801\u63d0\u5347\u4e86\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\u7684\u5b9e\u7528\u6027\u548c\u4fe1\u606f\u91cf\uff0c\u4e3aCMG\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2507.17743", "pdf": "https://arxiv.org/pdf/2507.17743", "abs": "https://arxiv.org/abs/2507.17743", "authors": ["Andre Menolli", "Bruno Strik"], "title": "Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence", "categories": ["cs.SE"], "comment": null, "summary": "Object-Oriented programming is frequently challenging for undergraduate\nComputer Science students, particularly in understanding abstract concepts such\nas encapsulation, inheritance, and polymorphism. Although the literature\noutlines various methods to identify potential design and coding issues in\nobject-oriented programming through source code analysis, such as code smells\nand SOLID principles, few studies explore how these code-level issues relate to\nlearning difficulties in Object-Oriented Programming. In this study, we explore\nthe relationship of the code issue indicators with common challenges\nencountered during the learning of object-oriented programming. Using\nqualitative analysis, we identified the main categories of learning\ndifficulties and, through a literature review, established connections between\nthese difficulties, code smells, and violations of the SOLID principles. As a\nresult, we developed a conceptual map that links code-related issues to\nspecific learning challenges in Object-Oriented Programming. The model was then\nevaluated by an expert who applied it in the analysis of the student code to\nassess its relevance and applicability in educational contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u4e2d\u4ee3\u7801\u95ee\u9898\u4e0e\u5b66\u4e60\u96be\u70b9\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5efa\u7acb\u4e86\u4ee3\u7801\u5f02\u5473\u548cSOLID\u539f\u5219\u4e0e\u5b66\u4e60\u56f0\u96be\u7684\u8054\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u4e86\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u4e2d\u7684\u4ee3\u7801\u95ee\u9898\uff0c\u4f46\u5c11\u6709\u7814\u7a76\u63a2\u8ba8\u8fd9\u4e9b\u95ee\u9898\u4e0e\u5b66\u4e60\u56f0\u96be\u7684\u5173\u7cfb\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u548c\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u5b66\u4e60\u96be\u70b9\u7c7b\u522b\uff0c\u5e76\u4e0e\u4ee3\u7801\u5f02\u5473\u548cSOLID\u539f\u5219\u5efa\u7acb\u8054\u7cfb\uff0c\u5f00\u53d1\u6982\u5ff5\u6a21\u578b\u5e76\u8bf7\u4e13\u5bb6\u8bc4\u4f30\u5176\u9002\u7528\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5c06\u4ee3\u7801\u95ee\u9898\u4e0e\u5b66\u4e60\u96be\u70b9\u5173\u8054\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5176\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7684\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u5c06\u4ee3\u7801\u95ee\u9898\u4e0e\u5b66\u4e60\u56f0\u96be\u5173\u8054\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u7684\u6559\u5b66\u65b9\u6cd5\u3002"}}
{"id": "2507.16540", "pdf": "https://arxiv.org/pdf/2507.16540", "abs": "https://arxiv.org/abs/2507.16540", "authors": ["Radowanul Haque", "Aftab Ali", "Sally McClean", "Naveed Khan"], "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.", "AI": {"tldr": "ExplainVulD\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bC/C++\u4ee3\u7801\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u5d4c\u5165\u548c\u8fb9\u7f18\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "motivation": "\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u6f0f\u6d1e\u51fd\u6570\u5360\u6bd4\u4f4e\u5bfc\u81f4\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u53ec\u56de\u7387\u9ad8\u4f46\u8bef\u62a5\u7387\u9ad8\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u963b\u788d\u5176\u5728\u5b89\u5168\u5de5\u4f5c\u4e2d\u7684\u96c6\u6210\u3002", "method": "\u6784\u5efa\u4ee3\u7801\u5c5e\u6027\u56fe\uff0c\u4f7f\u7528\u53cc\u901a\u9053\u5d4c\u5165\u8868\u793a\u8282\u70b9\uff08\u6355\u6349\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\uff09\uff0c\u5e76\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u8fb9\u7c7b\u578b\u5d4c\u5165\u3002\u91c7\u7528\u7c7b\u522b\u52a0\u6743\u4ea4\u53c9\u71b5\u635f\u5931\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728ReVeal\u6570\u636e\u96c6\u4e0a\uff0cExplainVulD\u5e73\u5747\u51c6\u786e\u738788.25%\uff0cF1\u5f97\u520648.23%\uff0c\u76f8\u5bf9ReVeal\u6a21\u578b\u5206\u522b\u63d0\u53474.6%\u548c16.9%\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u9759\u6001\u5206\u6790\u5de5\u5177\u3002", "conclusion": "ExplainVulD\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u589e\u5f3a\u5b89\u5168\u6392\u969c\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2507.17401", "pdf": "https://arxiv.org/pdf/2507.17401", "abs": "https://arxiv.org/abs/2507.17401", "authors": ["Rachel Ringe", "Mihai Pomarlan", "Nikolaos Tsiogkas", "Stefano De Giorgis", "Maria Hedblom", "Rainer Malaka"], "title": "The Wilhelm Tell Dataset of Affordance Demonstrations", "categories": ["cs.RO", "cs.HC"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Affordances - i.e. possibilities for action that an environment or objects in\nit provide - are important for robots operating in human environments to\nperceive. Existing approaches train such capabilities on annotated static\nimages or shapes. This work presents a novel dataset for affordance learning of\ncommon household tasks. Unlike previous approaches, our dataset consists of\nvideo sequences demonstrating the tasks from first- and third-person\nperspectives, along with metadata about the affordances that are manifested in\nthe task, and is aimed towards training perception systems to recognize\naffordance manifestations. The demonstrations were collected from several\nparticipants and in total record about seven hours of human activity. The\nvariety of task performances also allows studying preparatory maneuvers that\npeople may perform for a task, such as how they arrange their task space, which\nis also relevant for collaborative service robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b66\u4e60\u5e38\u89c1\u5bb6\u5ead\u4efb\u52a1\u4e2d\u7684affordances\uff0c\u533a\u522b\u4e8e\u4ee5\u5f80\u57fa\u4e8e\u9759\u6001\u56fe\u50cf\u6216\u5f62\u72b6\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u611f\u77e5\u884c\u52a8\u53ef\u80fd\u6027\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u89c6\u9891\u5e8f\u5217\uff08\u5305\u542b\u7b2c\u4e00\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff09\u53ca\u4efb\u52a1\u4e2d\u7684affordances\u5143\u6570\u636e\uff0c\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u7ea67\u5c0f\u65f6\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bb0\u5f55\uff0c\u5c55\u793a\u4e86\u4efb\u52a1\u6267\u884c\u548c\u9884\u5907\u52a8\u4f5c\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u8bad\u7ec3\u611f\u77e5\u7cfb\u7edf\u8bc6\u522baffordances\uff0c\u5bf9\u534f\u4f5c\u670d\u52a1\u673a\u5668\u4eba\u5c24\u5176\u91cd\u8981\u3002"}}
{"id": "2507.16887", "pdf": "https://arxiv.org/pdf/2507.16887", "abs": "https://arxiv.org/abs/2507.16887", "authors": ["Youpeng Li", "Weiliang Qi", "Xuyu Wang", "Fuxun Yu", "Xinda Wang"], "title": "Revisiting Pre-trained Language Models for Vulnerability Detection", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "The rapid advancement of pre-trained language models (PLMs) has demonstrated\npromising results for various code-related tasks. However, their effectiveness\nin detecting real-world vulnerabilities remains a critical challenge. % for the\nsecurity community. While existing empirical studies evaluate PLMs for\nvulnerability detection (VD), their inadequate consideration in data\npreparation, evaluation setups, and experimental settings undermines the\naccuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,\nan extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and\nlarge-scale PLMs using newly constructed datasets. Specifically, we compare the\nperformance of PLMs under both fine-tuning and prompt engineering, assess their\neffectiveness and generalizability across various training and testing\nsettings, and analyze their robustness against code normalization, abstraction,\nand semantic-preserving transformations.\n  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks\ndesigned to capture the syntactic and semantic patterns of code outperform both\ngeneral-purpose PLMs and those solely pre-trained or fine-tuned on large code\ncorpora. However, these models face notable challenges in real-world scenarios,\nsuch as difficulties in detecting vulnerabilities with complex dependencies,\nhandling perturbations introduced by code normalization and abstraction, and\nidentifying semantic-preserving vulnerable code transformations. Also, the\ntruncation caused by the limited context windows of PLMs can lead to a\nnon-negligible amount of labeling errors. This study underscores the importance\nof thorough evaluations of model performance in practical scenarios and\noutlines future directions to help enhance the effectiveness of PLMs for\nrealistic VD applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e8617\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u6ce8\u4e8e\u4ee3\u7801\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30PLMs\u5728\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u7814\u7a76\u5728\u6570\u636e\u51c6\u5907\u3001\u8bc4\u4f30\u8bbe\u7f6e\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165RevisitVD\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u5efa\u6570\u636e\u96c6\uff0c\u6bd4\u8f83PLM\u5728\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u4ee3\u7801\u89c4\u8303\u5316\u3001\u62bd\u8c61\u5316\u548c\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e13\u6ce8\u4e8e\u4ee3\u7801\u8bed\u6cd5\u548c\u8bed\u4e49\u7684PLMs\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u4f9d\u8d56\u3001\u4ee3\u7801\u6270\u52a8\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u63d0\u5347PLMs\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u6709\u6548\u6027\u7684\u65b9\u5411\u3002"}}
{"id": "2507.17481", "pdf": "https://arxiv.org/pdf/2507.17481", "abs": "https://arxiv.org/abs/2507.17481", "authors": ["Lizhu Zhang", "Cecilia X. Wang"], "title": "AI in Design Education at College Level-Educators' Perspectives and Challenges", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Artificial intelligence has deeply permeated numerous fields, especially the\ndesign area which relies on technology as a tool for innovation. This change\nnaturally extends to the field of design education, which is closest to design\npractice. This has led to further exploration of the impact of AI on\ncollege-level education in the design discipline. This study aims to examine\nhow current design educators perceive the role of AI in college-level design\neducation, their perspectives on integrating AI into teaching and research, and\ntheir concerns regarding its potential challenges in design education and\nresearch. Through qualitative, semi-structured, in-depth interviews with seven\nfaculties in U.S. design colleges, the findings reveal that AI, as a tool and\nsource of information, has become an integral part of design education. AI-\nderived functionalities are increasingly utilized in design software, and\neducators are actively incorporating AI as a theoretical framework in their\nteaching. Educators can guide students in using AI tools, but only if they\nfirst acquire a strong foundation in basic design principles and skills. This\nstudy also indicates the importance of promoting a cooperative relationship\nbetween design educators and AI. At the same time, educators express\nanticipation for advancements in ethical standards, authenticity, and the\nresolution of copyright issues related to AI.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5728\u8bbe\u8ba1\u6559\u80b2\u4e2d\u7684\u89d2\u8272\u3001\u6559\u80b2\u8005\u7684\u89c2\u70b9\u53ca\u6311\u6218\uff0c\u53d1\u73b0AI\u5df2\u6210\u4e3a\u8bbe\u8ba1\u6559\u80b2\u7684\u91cd\u8981\u5de5\u5177\u548c\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u63a2\u7d22AI\u5bf9\u8bbe\u8ba1\u6559\u80b2\u7684\u5f71\u54cd\u53ca\u6559\u80b2\u8005\u7684\u770b\u6cd5\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u6df1\u5ea6\u8bbf\u8c08\u7f8e\u56fd\u8bbe\u8ba1\u5b66\u9662\u7684\u4e03\u540d\u6559\u5e08\u3002", "result": "AI\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8bbe\u8ba1\u6559\u80b2\u4e2d\uff0c\u6559\u80b2\u8005\u9700\u5148\u638c\u63e1\u57fa\u672c\u8bbe\u8ba1\u6280\u80fd\u3002", "conclusion": "\u9700\u4fc3\u8fdb\u6559\u80b2\u8005\u4e0eAI\u7684\u5408\u4f5c\uff0c\u5e76\u5173\u6ce8\u4f26\u7406\u548c\u7248\u6743\u95ee\u9898\u3002"}}
{"id": "2507.17518", "pdf": "https://arxiv.org/pdf/2507.17518", "abs": "https://arxiv.org/abs/2507.17518", "authors": ["Vita Santa Barletta", "Vito Bavaro", "Miriana Calvano", "Antonio Curci", "Antonio Piccinno", "Davide Pio Posa"], "title": "Enabling Cyber Security Education through Digital Twins and Generative AI", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.HC", "cs.SE"], "comment": null, "summary": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability\nto replicate complex IT (Information Technology), OT (Operational Technology),\nand IoT (Internet of Things) infrastructures, allowing for real time\nmonitoring, threat analysis, and system simulation. This study investigates how\nintegrating DTs with penetration testing tools and Large Language Models (LLMs)\ncan enhance cybersecurity education and operational readiness. By simulating\nrealistic cyber environments, this approach offers a practical, interactive\nframework for exploring vulnerabilities and defensive strategies. At the core\nof this research is the Red Team Knife (RTK), a custom penetration testing\ntoolkit aligned with the Cyber Kill Chain model. RTK is designed to guide\nlearners through key phases of cyberattacks, including reconnaissance,\nexploitation, and response within a DT powered ecosystem. The incorporation of\nLarge Language Models (LLMs) further enriches the experience by providing\nintelligent, real-time feedback, natural language threat explanations, and\nadaptive learning support during training exercises. This combined DT LLM\nframework is currently being piloted in academic settings to develop hands on\nskills in vulnerability assessment, threat detection, and security operations.\nInitial findings suggest that the integration significantly improves the\neffectiveness and relevance of cybersecurity training, bridging the gap between\ntheoretical knowledge and real-world application. Ultimately, the research\ndemonstrates how DTs and LLMs together can transform cybersecurity education to\nmeet evolving industry demands.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u6570\u5b57\u5b6a\u751f\uff08DTs\uff09\u4e0e\u6e17\u900f\u6d4b\u8bd5\u5de5\u5177\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u548c\u64cd\u4f5c\u51c6\u5907\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7684\u7f51\u7edc\u73af\u5883\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u3001\u4e92\u52a8\u5f0f\u7684\u6846\u67b6\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u63a2\u7d22\u6f0f\u6d1e\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u5f25\u8865\u7406\u8bba\u77e5\u8bc6\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86Red Team Knife\uff08RTK\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u6740\u4f24\u94fe\u6a21\u578b\u7684\u6e17\u900f\u6d4b\u8bd5\u5de5\u5177\u5305\uff0c\u5e76\u4e0e\u6570\u5b57\u5b6a\u751f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u652f\u6301\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u96c6\u6210\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7edc\u5b89\u5168\u57f9\u8bad\u7684\u6548\u679c\u548c\u76f8\u5173\u6027\uff0c\u589e\u5f3a\u4e86\u5b66\u4e60\u8005\u5728\u6f0f\u6d1e\u8bc4\u4f30\u3001\u5a01\u80c1\u68c0\u6d4b\u548c\u5b89\u5168\u64cd\u4f5c\u65b9\u9762\u7684\u5b9e\u8df5\u80fd\u529b\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u80fd\u591f\u9769\u65b0\u7f51\u7edc\u5b89\u5168\u6559\u80b2\uff0c\u6ee1\u8db3\u884c\u4e1a\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u3002"}}
{"id": "2507.17718", "pdf": "https://arxiv.org/pdf/2507.17718", "abs": "https://arxiv.org/abs/2507.17718", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u8bed\u97f3AI\u6280\u672f\u8fdb\u884c\u7535\u8bdd\u8c03\u67e5\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u5408LLM\u3001ASR\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u8bed\u97f3AI\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7AI\u7535\u8bdd\u8c03\u67e5\u5b9e\u73b0\u91cf\u5316\u7814\u7a76\u7684\u89c4\u6a21\u5316\uff0c\u540c\u65f6\u517c\u987e\u4eba\u673a\u4ea4\u4e92\u7684\u53cb\u597d\u6027\u548c\u65b9\u6cd5\u7684\u4e25\u8c28\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u3001ASR\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9a\u91cf\u8c03\u67e5\uff0c\u4e25\u683c\u9075\u5b88\u7814\u7a76\u89c4\u8303\u5982\u968f\u673a\u5316\u95ee\u9898\u987a\u5e8f\u7b49\u3002", "result": "\u901a\u8fc7\u4e24\u9879\u8bd5\u70b9\u8c03\u67e5\u9a8c\u8bc1\uff0c\u53d1\u73b0\u8f83\u77ed\u7684\u5de5\u5177\u548c\u66f4\u7075\u6d3b\u7684AI\u8bbf\u8c08\u8005\u80fd\u63d0\u9ad8\u5b8c\u6210\u7387\u3001\u964d\u4f4e\u4e2d\u65ad\u7387\u5e76\u63d0\u5347\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u8bed\u97f3AI\u6280\u672f\u53ef\u7528\u4e8e\u5b9a\u91cf\u8c03\u67e5\uff0c\u4f18\u5316\u8bbe\u8ba1\u548c\u4ea4\u4e92\u4f53\u9a8c\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u679c\u3002"}}
{"id": "2507.17744", "pdf": "https://arxiv.org/pdf/2507.17744", "abs": "https://arxiv.org/abs/2507.17744", "authors": ["Xiaofeng Mao", "Shaoheng Lin", "Zhen Li", "Chuanhao Li", "Wenshuo Peng", "Tong He", "Jiangmiao Pang", "Mingmin Chi", "Yu Qiao", "Kaipeng Zhang"], "title": "Yume: An Interactive World Generation Model", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.", "AI": {"tldr": "Yume \u662f\u4e00\u4e2a\u901a\u8fc7\u56fe\u7247\u3001\u6587\u672c\u6216\u89c6\u9891\u521b\u5efa\u4ea4\u4e92\u5f0f\u3001\u771f\u5b9e\u4e14\u52a8\u6001\u4e16\u754c\u7684\u9879\u76ee\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5176\u521d\u6b65\u7248\u672c\uff0c\u80fd\u591f\u4ece\u56fe\u7247\u751f\u6210\u52a8\u6001\u4e16\u754c\uff0c\u5e76\u901a\u8fc7\u952e\u76d8\u63a7\u5236\u63a2\u7d22\u3002\u6280\u672f\u6838\u5fc3\u5305\u62ec\u76f8\u673a\u8fd0\u52a8\u91cf\u5316\u3001\u89c6\u9891\u751f\u6210\u67b6\u6784\u3001\u5148\u8fdb\u91c7\u6837\u5668\u548c\u6a21\u578b\u52a0\u901f\u3002", "motivation": "\u65e8\u5728\u6253\u9020\u4e00\u4e2a\u9ad8\u4fdd\u771f\u3001\u4ea4\u4e92\u5f0f\u7684\u89c6\u9891\u4e16\u754c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u5b9e\u73b0\u52a8\u6001\u4e16\u754c\u63a2\u7d22\u4e0e\u63a7\u5236\u3002", "method": "\u91c7\u7528\u56db\u90e8\u5206\u6846\u67b6\uff1a\u76f8\u673a\u8fd0\u52a8\u91cf\u5316\u3001\u57fa\u4e8e MVDT \u7684\u89c6\u9891\u751f\u6210\u67b6\u6784\u3001AAM \u548c TTS-SDE \u91c7\u6837\u5668\u4f18\u5316\u3001\u6a21\u578b\u52a0\u901f\u6280\u672f\u3002", "result": "\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u6837\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "conclusion": "Yume \u5c55\u793a\u4e86\u9ad8\u6548\u52a8\u6001\u4e16\u754c\u751f\u6210\u7684\u6f5c\u529b\uff0c\u5e76\u8ba1\u5212\u6301\u7eed\u66f4\u65b0\u4ee5\u5b9e\u73b0\u5176\u539f\u59cb\u76ee\u6807\u3002"}}
