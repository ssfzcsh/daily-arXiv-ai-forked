{"id": "2507.15887", "pdf": "https://arxiv.org/pdf/2507.15887", "abs": "https://arxiv.org/abs/2507.15887", "authors": ["Ori Press", "Brandon Amos", "Haoyu Zhao", "Yikai Wu", "Samuel K. Ainsworth", "Dominik Krupke", "Patrick Kidger", "Touqir Sajed", "Bartolomeo Stellato", "Jisun Park", "Nathanael Bosch", "Eli Meril", "Albert Steppi", "Arman Zharmagambetov", "Fangzhao Zhang", "David Perez-Pineiro", "Alberto Mercurio", "Ni Zhan", "Talor Abramovich", "Kilian Lieret", "Hanlin Zhang", "Shirley Huang", "Matthias Bethge", "Ofir Press"], "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite progress in language model (LM) capabilities, evaluations have thus\nfar focused on models' performance on tasks that humans have previously solved,\nincluding in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,\n2024). We therefore propose testing models' ability to design and implement\nalgorithms in an open-ended benchmark: We task LMs with writing code that\nefficiently solves computationally challenging problems in computer science,\nphysics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks\ncollected from domain experts and a framework for validating and timing\nLM-synthesized solution code, which is compared to reference implementations\nfrom popular open-source packages. In addition, we develop a baseline LM agent,\nAlgoTuner, and evaluate its performance across a suite of frontier models.\nAlgoTuner achieves an average 1.72x speedup against our reference solvers,\nwhich use libraries such as SciPy, sk-learn and CVXPY. However, we find that\ncurrent models fail to discover algorithmic innovations, instead preferring\nsurface-level optimizations. We hope that AlgoTune catalyzes the development of\nLM agents exhibiting creative problem solving beyond state-of-the-art human\nperformance."}
{"id": "2507.15889", "pdf": "https://arxiv.org/pdf/2507.15889", "abs": "https://arxiv.org/abs/2507.15889", "authors": ["Noah van der Vleuten"], "title": "Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing", "categories": ["cs.SE", "cs.AI"], "comment": "Master's thesis, University of Amsterdam, 2023\n  (https://scripties.uba.uva.nl/search?id=record_54126). Code and experiments\n  available at: https://github.com/NoahVl/Dr-Boot", "summary": "Language models for program synthesis are usually trained and evaluated on\nprogramming competition datasets (MBPP, APPS). However, these datasets are\nlimited in size and quality, while these language models are extremely data\nhungry. Additionally, the language models have a misaligned program synthesis\nprocess compared to humans. While humans iteratively develop code with the help\nof a compiler, most program synthesis models currently produce code in one go.\nTo solve these issues, we introduce a bootstrapping algorithm for program\nsynthesis, that supports teaching models how to repair. We show that\nbootstrapping consistently outperforms regular fine-tuning. Compared to other\nwork, our bootstrapped model performs on par with fine-tuned models that are\n68\\% larger. Notably, bootstrapping with repairing also improves non-repairing\nperformance compared to regular bootstrapping during inference. However, on our\nmodels, repairing during inference is likely inferior to simply sampling the\nsame number of solutions. Furthermore, we find that there are issues with the\nexample test cases in the training portion of the APPS dataset that are\nvaluable to the community, as many repairing and reinforcement learning methods\nrely on them."}
{"id": "2507.15892", "pdf": "https://arxiv.org/pdf/2507.15892", "abs": "https://arxiv.org/abs/2507.15892", "authors": ["Elijah Nnorom", "Md Basim Uddin Ahmed", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "title": "StaAgent: An Agentic Framework for Testing Static Analyzers", "categories": ["cs.SE"], "comment": null, "summary": "Static analyzers play a critical role in identifying bugs early in the\nsoftware development lifecycle, but their rule implementations are often\nunder-tested and prone to inconsistencies. To address this, we propose\nStaAgent, an agentic framework that harnesses the generative capabilities of\nLarge Language Models (LLMs) to systematically evaluate static analyzer rules.\nStaAgent comprises four specialized agents: a Seed Generation Agent that\ntranslates bug detection rules into concrete, bug-inducing seed programs; a\nCode Validation Agent that ensures the correctness of these seeds; a Mutation\nGeneration Agent that produces semantically equivalent mutants; and an Analyzer\nEvaluation Agent that performs metamorphic testing by comparing the static\nanalyzer's behavior on seeds and their corresponding mutants. By revealing\ninconsistent behaviors, StaAgent helps uncover flaws in rule implementations.\nThis LLM-driven, multi-agent framework offers a scalable and adaptable solution\nto improve the reliability of static analyzers. We evaluated StaAgent with five\nstate-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)\nacross five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,\nInfer, and PMD). The experimental results show that our approach can help\nreveal 64 problematic rules in the latest versions of these five static\nanalyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,\nand 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the\nSOTA baseline. We have reported all the bugs to developers, with two of them\nalready fixed. Three more have been confirmed by developers, while the rest are\nawaiting response. These results demonstrate the effectiveness of our approach\nand underscore the promise of agentic, LLM-driven data synthesis to advance\nsoftware engineering."}
{"id": "2507.16037", "pdf": "https://arxiv.org/pdf/2507.16037", "abs": "https://arxiv.org/abs/2507.16037", "authors": ["Zhili Zeng", "Kimya Khakzad Shahandashti", "Alvine Boaye Belle", "Song Wang", "Zhen Ming", "Jiang"], "title": "A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights", "categories": ["cs.SE"], "comment": null, "summary": "The rapid advancement of mobile applications has led to a significant demand\nfor cross-platform compatibility, particularly between the Android and iOS\nplatforms. Traditional approaches to mobile application translation often rely\non manual intervention or rule-based systems, which are labor-intensive and\ntime-consuming. While recent advancements in machine learning have introduced\nautomated methods, they often lack contextual understanding and adaptability,\nresulting in suboptimal translations. Large Language Models (LLMs) were\nrecently leveraged to enhance code translation at different granularities,\nincluding the method, class, and repository levels. Researchers have\ninvestigated common errors, limitations, and potential strategies to improve\nthese tasks. However, LLM-based application translation across different\nplatforms, such as migrating mobile applications between Android and iOS or\nadapting software across diverse frameworks, remains underexplored.\nUnderstanding the performance, strengths, and limitations of LLMs in\ncross-platform application translation is critical for advancing software\nengineering automation. This study aims to fill this gap by evaluating\nLLM-based agentic approaches for mobile application translation, identifying\nkey failure points, and proposing guidelines to improve translation\nperformance. We developed a chain of agents that account for dependencies,\nspecifications, program structure, and program control flow when translating\napplications from Android to iOS. To evaluate the performance, we manually\nexamined the translated code for syntactic correctness, semantic accuracy, and\nfunctional completeness. For translation failures, we further conducted a\ndetailed root cause analysis to understand the underlying limitations of the\nagentic translation process and identify opportunities for improvement."}
{"id": "2507.15979", "pdf": "https://arxiv.org/pdf/2507.15979", "abs": "https://arxiv.org/abs/2507.15979", "authors": ["Marcel C. BÃ¼hler", "Ye Yuan", "Xueting Li", "Yangyi Huang", "Koki Nagano", "Umar Iqbal"], "title": "Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs\nanimatable 3D human avatars from a single image. This is achieved by leveraging\nmulti-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of\n3D Gaussians. Given an image, we first dream plausible multi-views using a\nvideo diffusion model, capturing rich geometric and appearance details. These\nviews are then lifted into unstructured 3D Gaussians. To enable animation, we\npropose a transformer-based encoder that models global spatial relationships\nand projects these Gaussians into a structured latent representation aligned\nwith the UV space of a parametric body model. This latent code is decoded into\nUV-space Gaussians that can be animated via body-driven deformation and\nrendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV\nmanifold, our method ensures consistency during animation while preserving fine\nvisual details. DLA enables real-time rendering and intuitive editing without\nrequiring post-processing. Our method outperforms state-of-the-art approaches\non ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric\naccuracy. By combining the generative strengths of video diffusion models with\na pose-aware UV-space Gaussian mapping, DLA bridges the gap between\nunstructured 3D representations and high-fidelity, animation-ready avatars."}
{"id": "2507.16051", "pdf": "https://arxiv.org/pdf/2507.16051", "abs": "https://arxiv.org/abs/2507.16051", "authors": ["Juan Altmayer Pizzorno", "Emery D. Berger"], "title": "RightTyper: Effective and Efficient Type Annotation for Python", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Python type annotations bring the benefits of static type checking to the\nlanguage. However, manually writing annotations can be time-consuming and\ntedious. The result is that most real-world Python code remains largely\nuntyped. Past approaches to annotating types in Python code fall short in a\nnumber of ways. Static approaches struggle with dynamic features and infer\noverly broad types. AI-based methods are inherently unsound and can miss rare\nor user-defined types. Dynamic methods can impose extreme runtime overheads,\ndegrading performance by up to 270x, abort execution as they exhaust resources,\nand even infer incorrect types that lead to runtime errors. Crucially, all\nprior work assumes implicitly that the code to be annotated is already correct.\nThis assumption is generally unwarranted, especially for large codebases that\nhave been untyped.\n  This paper presents RightTyper, a novel approach for Python that overcomes\nthese disadvantages. RightTyper not only generates precise type annotations\nbased on actual program behavior, improving recall in type checking relative to\nprior approaches. It also turns type checking into anomaly detection, allowing\nthe type checker to identify corner cases that the programmer can audit for\nunintended behavior. RightTyper is also fast and space-efficient, imposing just\n30% performance overhead on average. RightTyper achieves these characteristics\nby a principled yet pervasive use of sampling--guided by self-profiling--along\nwith statistical filtering and careful resolution and aggregation of type\ninformation."}
{"id": "2507.16396", "pdf": "https://arxiv.org/pdf/2507.16396", "abs": "https://arxiv.org/abs/2507.16396", "authors": ["Xian Mo", "Fei Liu", "Rui Tang", "Jintao", "Gao", "Hao Liu"], "title": "Knowledge-aware Diffusion-Enhanced Multimedia Recommendation", "categories": ["cs.MM", "cs.IR"], "comment": null, "summary": "Multimedia recommendations aim to use rich multimedia content to enhance\nhistorical user-item interaction information, which can not only indicate the\ncontent relatedness among items but also reveal finer-grained preferences of\nusers. In this paper, we propose a Knowledge-aware Diffusion-Enhanced\narchitecture using contrastive learning paradigms (KDiffE) for multimedia\nrecommendations. Specifically, we first utilize original user-item graphs to\nbuild an attention-aware matrix into graph neural networks, which can learn the\nimportance between users and items for main view construction. The\nattention-aware matrix is constructed by adopting a random walk with a restart\nstrategy, which can preserve the importance between users and items to generate\naggregation of attention-aware node features. Then, we propose a guided\ndiffusion model to generate strongly task-relevant knowledge graphs with less\nnoise for constructing a knowledge-aware contrastive view, which utilizes user\nembeddings with an edge connected to an item to guide the generation of\nstrongly task-relevant knowledge graphs for enhancing the item's semantic\ninformation. We perform comprehensive experiments on three multimedia datasets\nthat reveal the effectiveness of our KDiffE and its components on various\nstate-of-the-art methods. Our source codes are available\nhttps://github.com/1453216158/KDiffE."}
{"id": "2507.15913", "pdf": "https://arxiv.org/pdf/2507.15913", "abs": "https://arxiv.org/abs/2507.15913", "authors": ["Renato Neves", "JosÃ© ProenÃ§a", "Juliana Souza"], "title": "An Adequate While-Language for Stochastic Hybrid Computation", "categories": ["cs.LO", "F.3"], "comment": "Extended version of a paper accepted at PPDP'25", "summary": "We introduce a language for formally reasoning about programs that combine\ndifferential constructs with probabilistic ones. The language harbours, for\nexample, such systems as adaptive cruise controllers, continuous-time random\nwalks, and physical processes involving multiple collisions, like in Einstein's\nBrownian motion.\n  We furnish the language with an operational semantics and use it to implement\na corresponding interpreter. We also present a complementary, denotational\nsemantics and establish an adequacy theorem between both cases."}
{"id": "2507.16438", "pdf": "https://arxiv.org/pdf/2507.16438", "abs": "https://arxiv.org/abs/2507.16438", "authors": ["Yuqi Zhao", "Giovanni Dettori", "Matteo Boffa", "Luca Vassio", "Marco Mellia"], "title": "The Sweet Danger of Sugar: Debunking Representation Learning for Encrypted Traffic Classification", "categories": ["cs.NI", "cs.LG"], "comment": "This paper has been accepted at ACM SIGCOMM 2025. It will appear in\n  the proceedings with DOI 10.1145/3718958.3750498", "summary": "Recently we have witnessed the explosion of proposals that, inspired by\nLanguage Models like BERT, exploit Representation Learning models to create\ntraffic representations. All of them promise astonishing performance in\nencrypted traffic classification (up to 98% accuracy). In this paper, with a\nnetworking expert mindset, we critically reassess their performance. Through\nextensive analysis, we demonstrate that the reported successes are heavily\ninfluenced by data preparation problems, which allow these models to find easy\nshortcuts - spurious correlation between features and labels - during\nfine-tuning that unrealistically boost their performance. When such shortcuts\nare not present - as in real scenarios - these models perform poorly. We also\nintroduce Pcap-Encoder, an LM-based representation learning model that we\nspecifically design to extract features from protocol headers. Pcap-Encoder\nappears to be the only model that provides an instrumental representation for\ntraffic classification. Yet, its complexity questions its applicability in\npractical settings. Our findings reveal flaws in dataset preparation and model\ntraining, calling for a better and more conscious test design. We propose a\ncorrect evaluation methodology and stress the need for rigorous benchmarking."}
{"id": "2507.16649", "pdf": "https://arxiv.org/pdf/2507.16649", "abs": "https://arxiv.org/abs/2507.16649", "authors": ["Bingxin Liu", "Yinghui Huang", "Jianhua Gao", "Jianjun Shi", "Yongpeng Liu", "Yipin Sun", "Weixing Ji"], "title": "From Profiling to Optimization: Unveiling the Profile Guided Optimization", "categories": ["cs.PF"], "comment": null, "summary": "Profile Guided Optimization (PGO) uses runtime profiling to direct compiler\noptimization decisions, effectively combining static analysis with actual\nexecution behavior to enhance performance. Runtime profiles, collected through\ninstrumentation or hardware- and software-assisted sampling, provide detailed\ninsights into control flow, branch predictions, and memory access patterns.\nThis survey systematically categorizes PGO research by profiling method\n(instrumentation vs. sampling), optimizations (compile time and link/post-link\ntime), compiler integration (GCC, LLVM), and target architectures. Key\nalgorithms and frameworks are shown in terms of design principles. Performance\nevaluation on representative examples demonstrates PGO's speedups, overheads,\nand integration maturity. Finally, we identify open challenges, such as\nreducing sampling overhead, dynamic input workloads, and supporting\ncross-architecture portability, and propose future research directions to\nlow-overhead profiling and advanced compilers."}
{"id": "2507.15981", "pdf": "https://arxiv.org/pdf/2507.15981", "abs": "https://arxiv.org/abs/2507.15981", "authors": ["Gennie Mansi", "Mark Riedl"], "title": "Implications of Current Litigation on the Design of AI Systems for Healthcare Delivery", "categories": ["cs.HC"], "comment": "15 pages, 8 Figures", "summary": "Many calls for explainable AI (XAI) systems in medicine are tied to a desire\nfor AI accountability--accounting for, mitigating, and ultimately preventing\nharms from AI systems. Because XAI systems provide human-understandable\nexplanations for their output, they are often viewed as a primary path to\nprevent harms to patients. However, when harm occurs, laws, policies, and\nregulations also shape AI accountability by impacting how harmed individuals\ncan obtain recourse. Current approaches to XAI explore physicians' medical and\nrelational needs to counter harms to patients, but there is a need to\nunderstand how XAI systems should account for the legal considerations of those\nimpacted. We conduct an analysis of 31 legal cases and reported harms to\nidentify patterns around how AI systems impact patient care. Our findings\nreflect how patients' medical care relies on a complex web of\nstakeholders--physicians, state health departments, health insurers, care\nfacilities, among others--and many AI systems deployed across their healthcare\ndelivery negatively impact their care. In response, patients have had no option\nbut to seek legal recourse for harms. We shift the frame from\nphysician-centered to patient-centered accountability approaches by describing\nhow lawyers and technologists need to recognize and address where AI harms\nhappen. We present paths for preventing or countering harm (1) by changing\nliability structures to reflect the role of many stakeholders in shaping how AI\nsystems impact patient care; and (2) by designing XAI systems that can help\nadvocates, such as legal representatives, who provide critical legal expertise\nand practically support recourse for patients."}
{"id": "2507.15860", "pdf": "https://arxiv.org/pdf/2507.15860", "abs": "https://arxiv.org/abs/2507.15860", "authors": ["Albert Lu", "Reza Arghavani", "Hiu Yung Wong"], "title": "Prediction of Alpha-Particle-Immune Gate-All-Around Field-Effect Transistors (GAA-FET) Based SRAM Design", "categories": ["cs.ET", "physics.comp-ph"], "comment": null, "summary": "In this paper, using 3D Technology Computer-Aided-Design (TCAD) simulations,\nwe show that it is possible to design a static random-access memory (SRAM)\nusing gate-all-around field-effect-transistor (GAA-FET) technology so that it\nis immune to single alpha particle radiation error. In other words, with the\ndesign, there will be no single-event upset (SEU) due to alpha particles. We\nfirst use ab initio calculations in PHITS to show that there is a maximum\nlinear energy transfer (LET), LETmax, for the alpha particle in Si and\nSi$_x$Ge$_{1-x}$. Based on that, by designing a sub-7nm GAA-FET-based SRAM with\nbottom dielectric isolation (BDI), we show that the SRAM does not flip even if\nthe particle strike is in the worst-case scenario (LET > LETmax)."}
{"id": "2507.16109", "pdf": "https://arxiv.org/pdf/2507.16109", "abs": "https://arxiv.org/abs/2507.16109", "authors": ["Zihao Chen", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "title": "Resilience Evaluation of Kubernetes in Cloud-Edge Environments via Failure Injection", "categories": ["cs.DC"], "comment": null, "summary": "Kubernetes has emerged as an essential platform for deploying containerised\napplications across cloud and edge infrastructures. As Kubernetes gains\nincreasing adoption for mission-critical microservices, evaluating system\nresilience under realistic fault conditions becomes crucial. However,\nsystematic resilience assessments of Kubernetes in hybrid cloud-edge\nenvironments are currently limited in research. To address this gap, a novel\nresilience evaluation framework integrates mainstream fault injection tools\nwith automated workload generation for comprehensive cloud-edge Kubernetes\ntesting. Multiple fault injection platforms, including Chaos Mesh, Gremlin, and\nChaosBlade are combined with realistic traffic simulation tools, enabling\nautomated orchestration of complex failure scenarios. Through this framework,\ncomprehensive experiments are conducted that systematically target node-level,\npod-level, and network failures across cloud and cloud-edge environments. The\nfirst comprehensive resilience dataset for hybrid cloud-edge Kubernetes\ndeployments is created, comprising over 30 GB of performance data from 11,965\nfault injection scenarios including response times, failure rates, and error\npatterns. Analysis reveals that cloud-edge deployments demonstrate 80% superior\nresponse stability under network delay and partition conditions, while cloud\ndeployments exhibit 47% better resilience under bandwidth limitations,\nproviding quantitative guidance for architectural decision-making in cloud-edge\ndeployments."}
{"id": "2507.16177", "pdf": "https://arxiv.org/pdf/2507.16177", "abs": "https://arxiv.org/abs/2507.16177", "authors": ["Yifan Zhang", "Xiaoyu Niu", "Hongzheng Tian", "Yanjun Zhang", "Bo Yu", "Shaoshan Liu", "Sitao Huang"], "title": "A Sparsity-Aware Autonomous Path Planning Accelerator with HW/SW Co-Design and Multi-Level Dataflow Optimization", "categories": ["cs.AR"], "comment": "Accepted by ACM Transactions on Architecture and Code Optimization\n  (ACM TACO)", "summary": "Path planning is critical for autonomous driving, generating smooth,\ncollision-free, feasible paths based on perception and localization inputs.\nHowever, its computationally intensive nature poses significant challenges for\nresource-constrained autonomous driving hardware. This paper presents an\nend-to-end FPGA-based acceleration framework targeting the quadratic\nprogramming (QP), core of optimization-based path planning. We employ a\nhardware-friendly alternating direction method of multipliers (ADMM) for QP\nsolving and a parallelizable preconditioned conjugate gradient (PCG) method for\nlinear systems. By analyzing sparse matrix patterns, we propose customized\nstorage schemes and efficient sparse matrix multiplication units, significantly\nreducing resource usage and accelerating matrix operations. Our multi-level\ndataflow optimization strategy incorporates intra-operator parallelization and\npipelining, inter-operator fine-grained pipelining, and CPU-FPGA system-level\ntask mapping. Implemented on the AMD ZCU102 platform, our framework achieves\nstate-of-the-art latency and energy efficiency, including 1.48x faster\nperformance than the best FPGA-based design, 2.89x over an Intel i7-11800H CPU,\n5.62x over an ARM Cortex-A57 embedded CPU, and 1.56x over a state-of-the-art\nGPU solution, along with a 2.05x throughput improvement over existing\nFPGA-based designs."}
{"id": "2507.15991", "pdf": "https://arxiv.org/pdf/2507.15991", "abs": "https://arxiv.org/abs/2507.15991", "authors": ["David Fiala", "Laurent Pugin", "Marnix van Berchum", "Martha Thomae", "KÃ©vin Roger"], "title": "A new XML conversion process for mensural music encoding : CMME\\_to\\_MEI (via Verovio)", "categories": ["cs.SD", "cs.DB", "eess.AS"], "comment": null, "summary": "The Ricercar Lab - the musicological research team at the Center for advanced\nStudies in the Renaissance at the University of Tours - has decided to make\navailable in open access, thanks to the support of the French digital\ninfrastructure Biblissima, a large corpus of about 3500 XML files of 15th-c.\nmusic. This corpus was produced by the German musicologist Clemens Goldberg who\nencoded since 2010 onwards the musical content of 34 major 15th-c. music\nmanuscripts and other complementary files, in order to offer on his\nfoundation's website PDF files of complete collections of works by Du Fay,\nBinchois, Okeghem, Busnoys and most of their major contemporaries, focusing on\ntheir secular output. This corpus was encoded in an XML format named CMME\n(Computerized Mensural Music Editing), specifically conceived for mensural\nmusic by Theodor Dumitrescu in the 2000s, together with editorial and\npublication tools which have not been updated since then. This article focuses\non the development of a set of conversion tools for these CMME files to meet\nmore up-to-date standards of music encoding, namely MEI. A workshop was\norganised in September 2024 at the Campus Condorcet in Paris, gathering experts\nwith a wide range of knowledge on mensural music notation, XML formats and\nprogramming. A converter was developped directly in the open-source rendering\nlibrary Verovio, allowing the conversion from CMME to MEI mensural. A\nconversion to MEI CMN was implemented afterwards, enabling to load these files\nin common engraving softwares such as MuseScore with minimal loss of\ninformation. With the availability of a direct import of CMME-XML into Verovio,\nthe corpus of existing CMME files gets a new life. Furthermore, since the\nstand-alone CMME editor still works fine and no alternative is available yet\nfor native MEI, the converter offers a new pipeline for encoding and editing\nmensural music."}
{"id": "2507.16044", "pdf": "https://arxiv.org/pdf/2507.16044", "abs": "https://arxiv.org/abs/2507.16044", "authors": ["Meriem Mastouri", "Emna Ksontini", "Wael Kessentini"], "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws."}
{"id": "2507.16463", "pdf": "https://arxiv.org/pdf/2507.16463", "abs": "https://arxiv.org/abs/2507.16463", "authors": ["Fabrizio Nunnari", "Shailesh Mishra", "Patrick Gebhard"], "title": "MMS Player: an open source software for parametric data-driven animation of Sign Language avatars", "categories": ["cs.GR", "cs.CL"], "comment": null, "summary": "This paper describes the MMS-Player, an open source software able to\nsynthesise sign language animations from a novel sign language representation\nformat called MMS (MultiModal Signstream). The MMS enhances gloss-based\nrepresentations by adding information on parallel execution of signs, timing,\nand inflections. The implementation consists of Python scripts for the popular\nBlender 3D authoring tool and can be invoked via command line or HTTP API.\nAnimations can be rendered as videos or exported in other popular 3D animation\nexchange formats. The software is freely available under GPL-3.0 license at\nhttps://github.com/DFKI-SignLanguage/MMS-Player."}
{"id": "2507.16086", "pdf": "https://arxiv.org/pdf/2507.16086", "abs": "https://arxiv.org/abs/2507.16086", "authors": ["Andrew Marmaduke", "Apoorv Ingle", "J. Garrett Morris"], "title": "Understanding Haskell-style Overloading via Open Data and Open Functions", "categories": ["cs.PL"], "comment": null, "summary": "We present a new, uniform semantics for Haskell-style overloading. We realize\nour approach in a new core language, System F$_\\mathrm{D}$, whose metatheory we\nmechanize in the Lean4 interactive theorem prover. System F$_\\mathrm{D}$ is\ndistinguished by its open data types and open functions, each given by a\ncollection of instances rather than by a single definition. We show that System\nF$_\\mathrm{D}$ can encode advanced features of Haskell's of type class systems,\nmore expressively than current semantics of these features, and without\nassuming additional type equality axioms."}
{"id": "2507.15875", "pdf": "https://arxiv.org/pdf/2507.15875", "abs": "https://arxiv.org/abs/2507.15875", "authors": ["Jerry Li", "Timothy Oh", "Joseph Hoang", "Vardhit Veeramachaneni"], "title": "Differential Multimodal Transformers", "categories": ["cs.AI", "cs.MM"], "comment": null, "summary": "Small language models have gained significant popularity due to their\nefficiency and growing capabilities. However, incorporating additional\nmodalities, such as vision, can exacerbate the challenge of limited context\nwindows by introducing noise. Recent studies have highlighted that Transformer\nattention mechanisms often disproportionately focus on irrelevant contexts. In\nthis work, we extend the Differential Attention mechanism, originally designed\nfor text-only models, to the text-vision model PaliGemma. Our aim is to\nevaluate its ability to mitigate noisy information retrieval and reduce\nhallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,\nincorporating Differential Attention, and experimented with various parameter\nsettings and configurations. We demonstrate that Differential Attention can be\nadapted and integrated into the fine-tuning of existing models to enhance noisy\ninformation retrieval and question-answering capabilities."}
{"id": "2507.16581", "pdf": "https://arxiv.org/pdf/2507.16581", "abs": "https://arxiv.org/abs/2507.16581", "authors": ["Joris Nieuwveld", "JoÃ«l Ouaknine"], "title": "On Expansions of Monadic Second-Order Logic with Dynamical Predicates", "categories": ["cs.LO", "math.NT", "11B37 11J86 11B40 11K16", "F.4.0; G.2.0"], "comment": "Full version of a paper accepted to MFCS 2025", "summary": "Expansions of the monadic second-order (MSO) theory of the structure $\\langle\n\\mathbb{N} ; < \\rangle$ have been a fertile and active area of research ever\nsince the publication of the seminal papers of B\\\"uchi and Elgot & Rabin on the\nsubject in the 1960s. In the present paper, we establish decidability of the\nMSO theory of $\\langle \\mathbb{N} ; <,P \\rangle$, where $P$ ranges over a large\nclass of unary ''dynamical'' predicates, i.e., sets of non-negative values\nassumed by certain integer linear recurrence sequences. One of our key\ntechnical tools is the novel concept of (effective) prodisjunctivity, which we\nexpect may also find independent applications further afield."}
{"id": "2507.16594", "pdf": "https://arxiv.org/pdf/2507.16594", "abs": "https://arxiv.org/abs/2507.16594", "authors": ["Zied Jenhani", "Mounir Bensalem", "Jasenka DizdareviÄ", "Admela Jukan"], "title": "An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes", "categories": ["cs.NI", "cs.AI", "cs.DC"], "comment": "This paper is uploaded here for research community, thus it is for\n  non-commercial purposes", "summary": "Running deep learning inference directly on ultra-low-power edge/IoT nodes\nhas been limited by the tight memory and compute budgets of microcontrollers.\nSplit learning (SL) addresses this limitation in which it executes part of the\ninference process on the sensor and off-loads the remainder to a companion\ndevice. In the context of constrained devices and the related impact of\nlow-power, over-the-air transport protocols, the performance of split learning\nremains largely unexplored. TO the best of our knowledge, this paper presents\nthe first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,\ndesigned to benchmark the over-the-air performance of split learning TinyML in\nedge/IoT environments. We benchmark the performance of a MobileNetV2 image\nrecognition model, which is quantized to 8-bit integers, partitioned, and\ndelivered to the nodes via over-the-air updates. The intermediate activations\nare exchanged through different wireless communication methods: ESP-NOW, BLE,\nand traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on\nidentical hardware. Measurements show that splitting the model after\nblock_16_project_BN layer generates a 5.66 kB tensor that traverses the link in\n3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.\nESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery\nlife further but increases latency beyond 10s."}
{"id": "2507.16274", "pdf": "https://arxiv.org/pdf/2507.16274", "abs": "https://arxiv.org/abs/2507.16274", "authors": ["Zixiao Huang", "Junhao Hu", "Hao Lin", "Chunyang Zhu", "Yueran Tang", "Quanlu Zhang", "Zhen Guo", "Zhenhua Li", "Shengen Yan", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "title": "Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "comment": null, "summary": "The rapid scaling of large language models (LLMs) has significantly increased\nGPU memory pressure, which is further aggravated by training optimization\ntechniques such as virtual pipeline and recomputation that disrupt tensor\nlifespans and introduce considerable memory fragmentation. Default GPU memory\nallocators of popular deep learning frameworks like PyTorch use online\nstrategies without knowledge of tensor lifespans, which can waste up to 43\\% of\nmemory and cause out-of-memory errors, rendering optimization techniques\nineffective or even unusable.\n  To address this, we introduce STWeaver, a GPU memory allocator for deep\nlearning frameworks that reduces fragmentation by exploiting the spatial and\ntemporal regularity in memory allocation behaviors of training workloads.\nSTWeaver introduces a novel paradigm that combines offline planning with online\nallocation. The offline planning leverages spatio-temporal regularities to\ngenerate a near-optimal allocation plan, while the online allocation handles\ncomplex and dynamic models such as Mixture-of-Experts (MoE). Built as a\npluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by\n79.2\\% (up to 100\\%) across both dense and sparse models, with negligible\noverhead. This enables more efficient, high-throughput training configurations\nand improves performance by up to 32.5\\%."}
{"id": "2507.15996", "pdf": "https://arxiv.org/pdf/2507.15996", "abs": "https://arxiv.org/abs/2507.15996", "authors": ["Gennie Mansi", "Mark Riedl"], "title": "Understanding the Impact of Physicians' Legal Considerations on XAI Systems", "categories": ["cs.HC"], "comment": "10 pages, 4 figures", "summary": "Physicians are--and feel--ethically, professionally, and legally responsible\nfor patient outcomes, buffering patients from harmful AI determinations from\nmedical AI systems. Many have called for explainable AI (XAI) systems to help\nphysicians incorporate medical AI recommendations into their workflows in a way\nthat reduces the potential of harms to patients. While prior work has\ndemonstrated how physicians' legal concerns impact their medical decision\nmaking, little work has explored how XAI systems should be designed in light of\nthese concerns. In this study, we conducted interviews with 10 physicians to\nunderstand where and how they anticipate errors that may occur with a medical\nAI system and how these anticipated errors connect to their legal concerns. In\nour study, physicians anticipated risks associated with using an AI system for\npatient care, but voiced unknowns around how their legal risk mitigation\nstrategies may change given a new technical system. Based on these findings, we\ndescribe the implications for designing XAI systems that can address\nphysicians' legal concerns. Specifically, we identify the need to provide AI\nrecommendations alongside contextual information that guides their risk\nmitigation strategies, including how non-legally related aspects of their\nsystems, such as medical documentation and auditing requests, might be\nincorporated into a legal case."}
{"id": "2507.16077", "pdf": "https://arxiv.org/pdf/2507.16077", "abs": "https://arxiv.org/abs/2507.16077", "authors": ["Rodrigo Moreira", "Rafael Pasquini", "Joberto S. B. Martins", "Tereza C. Carvalho", "FlÃ¡vio de Oliveira Silva"], "title": "AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds", "categories": ["cs.ET", "cs.AI", "cs.LG", "cs.MA", "cs.NI"], "comment": "17 pages, 18 figures, 14 tables,", "summary": "Network Slicing (NS) realization requires AI-native orchestration\narchitectures to efficiently and intelligently handle heterogeneous user\nrequirements. To achieve this, network slicing is evolving towards a more\nuser-centric digital transformation, focusing on architectures that incorporate\nnative intelligence to enable self-managed connectivity in an integrated and\nisolated manner. However, these initiatives face the challenge of validating\ntheir results in production environments, particularly those utilizing\nML-enabled orchestration, as they are often tested in local networks or\nlaboratory simulations. This paper proposes a large-scale validation method\nusing a network slicing prediction model to forecast latency using Deep Neural\nNetworks (DNNs) and basic ML algorithms embedded within an NS architecture,\nevaluated in real large-scale production testbeds. It measures and compares the\nperformance of different DNNs and ML algorithms, considering a distributed\ndatabase application deployed as a network slice over two large-scale\nproduction testbeds. The investigation highlights how AI-based prediction\nmodels can enhance network slicing orchestration architectures and presents a\nseamless, production-ready validation method as an alternative to fully\ncontrolled simulations or laboratory setups."}
{"id": "2507.16165", "pdf": "https://arxiv.org/pdf/2507.16165", "abs": "https://arxiv.org/abs/2507.16165", "authors": ["Liam Naddell", "Marcelo Ponce"], "title": "Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric", "categories": ["cs.DC", "cs.GR", "gr-qc"], "comment": "Published and presented at PEARC '25: Practice and Experience in\n  Advanced Research Computing 2025: \"The Power of Collaboration\"", "summary": "Rendering images of black holes by utilizing ray tracing techniques is a\ncommon methodology employed in many aspects of scientific and astrophysical\nvisualizations. Similarly, general ray tracing techniques are widely used in\nareas related to computer graphics. In this work we describe the implementation\nof a parallel open-source program that can ray trace images in the presence of\na black hole geometry. We do this by combining a couple of different techniques\nusually present in parallel scientific computing, such as, mathematical\napproximations, utilization of scientific libraries, shared-memory and\ndistributed-memory parallelism."}
{"id": "2507.16326", "pdf": "https://arxiv.org/pdf/2507.16326", "abs": "https://arxiv.org/abs/2507.16326", "authors": ["Daniel Bascones", "Borja Morcillo"], "title": "Hourglass Sorting: A novel parallel sorting algorithm and its implementation", "categories": ["cs.AR", "B.5.0"], "comment": "6 pages, 5 figures", "summary": "Sorting is one of the fundamental problems in computer science. Playing a\nrole in many processes, it has a lower complexity bound imposed by\n$\\mathcal{O}(n\\log{n})$ when executing on a sequential machine. This limit can\nbe brought down to sub-linear times thanks to parallelization techniques that\nincrease the number of comparisons done in parallel. This, however, increases\nthe cost of implementation, which limits the application of such techniques.\nMoreover, as the size of the arrays increases, a bottleneck arises in moving\nthe vast quantities of data required at the input, and generated at the output\nof such sorter. This might impose time requirements much stricter than those of\nthe sorting itself. In this paper, a novel parallel sorter is proposed for the\nspecific case where the input is parallel, but the output is serial. The design\nis then implemented and verified on an FPGA within the context of a quantum\nLDPC decoder. A latency of $\\log{n}$ is achieved for the output of the first\nelement, after which the rest stream out for a total sorting time of\n$n+\\log{n}$. Contrary to other parallel sorting methods, clock speed does not\ndegrade with $n$, and resources scale linearly with input size."}
{"id": "2507.16073", "pdf": "https://arxiv.org/pdf/2507.16073", "abs": "https://arxiv.org/abs/2507.16073", "authors": ["Annabelle Warner", "Andrew McNutt", "Paul Rosen", "El Kindi Rezig"], "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler", "categories": ["cs.HC", "cs.DB"], "comment": "Accepted to VLDB25 Demo track", "summary": "Preparing datasets -- a critical phase known as data wrangling -- constitutes\nthe dominant phase of data science development, consuming upwards of 80% of the\ntotal project time. This phase encompasses a myriad of tasks: parsing data,\nrestructuring it for analysis, repairing inaccuracies, merging sources,\neliminating duplicates, and ensuring overall data integrity. Traditional\napproaches, typically through manual coding in languages such as Python or\nusing spreadsheets, are not only laborious but also error-prone. These issues\nrange from missing entries and formatting inconsistencies to data type\ninaccuracies, all of which can affect the quality of downstream tasks if not\nproperly corrected. To address these challenges, we present Buckaroo, a\nvisualization system to highlight discrepancies in data and enable on-the-spot\ncorrections through direct manipulations of visual objects. Buckaroo (1)\nautomatically finds \"interesting\" data groups that exhibit anomalies compared\nto the rest of the groups and recommends them for inspection; (2) suggests\nwrangling actions that the user can choose to repair the anomalies; and (3)\nallows users to visually manipulate their data by displaying the effects of\ntheir wrangling actions and offering the ability to undo or redo these actions,\nwhich supports the iterative nature of data wrangling. A video companion is\navailable at https://youtu.be/iXdCYbvpQVE"}
{"id": "2507.16063", "pdf": "https://arxiv.org/pdf/2507.16063", "abs": "https://arxiv.org/abs/2507.16063", "authors": ["Yousab Grees", "Polina Iaremchuk", "Ramtin Ehsani", "Esteban Parra", "Preetha Chatterjee", "Sonia Haiduc"], "title": "AI-Powered Commit Explorer (APCE)", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Commit messages in a version control system provide valuable information for\ndevelopers regarding code changes in software systems. Commit messages can be\nthe only source of information left for future developers describing what was\nchanged and why. However, writing high-quality commit messages is often\nneglected in practice. Large Language Model (LLM) generated commit messages\nhave emerged as a way to mitigate this issue. We introduce the AI-Powered\nCommit Explorer (APCE), a tool to support developers and researchers in the use\nand study of LLM-generated commit messages. APCE gives researchers the option\nto store different prompts for LLMs and provides an additional evaluation\nprompt that can further enhance the commit message provided by LLMs. APCE also\nprovides researchers with a straightforward mechanism for automated and human\nevaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo"}
{"id": "2507.16165", "pdf": "https://arxiv.org/pdf/2507.16165", "abs": "https://arxiv.org/abs/2507.16165", "authors": ["Liam Naddell", "Marcelo Ponce"], "title": "Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric", "categories": ["cs.DC", "cs.GR", "gr-qc"], "comment": "Published and presented at PEARC '25: Practice and Experience in\n  Advanced Research Computing 2025: \"The Power of Collaboration\"", "summary": "Rendering images of black holes by utilizing ray tracing techniques is a\ncommon methodology employed in many aspects of scientific and astrophysical\nvisualizations. Similarly, general ray tracing techniques are widely used in\nareas related to computer graphics. In this work we describe the implementation\nof a parallel open-source program that can ray trace images in the presence of\na black hole geometry. We do this by combining a couple of different techniques\nusually present in parallel scientific computing, such as, mathematical\napproximations, utilization of scientific libraries, shared-memory and\ndistributed-memory parallelism."}
{"id": "2507.16089", "pdf": "https://arxiv.org/pdf/2507.16089", "abs": "https://arxiv.org/abs/2507.16089", "authors": ["Michael J. Sullivan", "Zhibo Chen", "Elvis Pranskevichus", "Robert J. Simmons", "Victor Petrovykh", "AljaÅ¾ Mur ErÅ¾en", "Yury Selivanov"], "title": "Querying Graph-Relational Data", "categories": ["cs.PL", "cs.DB"], "comment": null, "summary": "For applications that store structured data in relational databases, there is\nan impedance mismatch between the flat representations encouraged by relational\ndata models and the deeply nested information that applications expect to\nreceive. In this work, we present the graph-relational database model, which\nprovides a flexible, compositional, and strongly-typed solution to this\n\"object-relational mismatch.\" We formally define the graph-relational database\nmodel and present a static and dynamic semantics for queries. In addition, we\ndiscuss the realization of the graph-relational database model in EdgeQL, a\ngeneral-purpose SQL-style query language, and the Gel system, which compiles\nEdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of\nobject-shaped data manipulation that is frequently provided inefficiently by\nobject-relational mapping (ORM) technologies, while achieving most of the\nefficiency that comes from require writing complex PostgreSQL queries directly."}
{"id": "2507.16193", "pdf": "https://arxiv.org/pdf/2507.16193", "abs": "https://arxiv.org/abs/2507.16193", "authors": ["Zitong Xu", "Huiyu Duan", "Bingnan Liu", "Guangji Ma", "Jiarui Wang", "Liu Yang", "Shiqi Gao", "Xiaoyu Wang", "Jia Wang", "Xiongkuo Min", "Guangtao Zhai", "Weisi Lin"], "title": "LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The rapid advancement of Text-guided Image Editing (TIE) enables image\nmodifications through text prompts. However, current TIE models still struggle\nto balance image quality, editing alignment, and consistency with the original\nimage, limiting their practical applications. Existing TIE evaluation\nbenchmarks and metrics have limitations on scale or alignment with human\nperception. To this end, we introduce EBench-18K, the first large-scale image\nEditing Benchmark including 18K edited images with fine-grained human\npreference annotations for evaluating TIE. Specifically, EBench-18K includes\n1,080 source images with corresponding editing prompts across 21 tasks, 18K+\nedited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion\nscores (MOSs) assessed from three evaluation dimensions, and 18K+\nquestion-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs\nto assess edited images, while the evaluation results, in turn, provide\ninsights into assessing the alignment between the LMMs' understanding ability\nand human preferences. Then, we propose LMM4Edit, a LMM-based metric for\nevaluating image Editing models from perceptual quality, editing alignment,\nattribute preservation, and task-specific QA accuracy in an all-in-one manner.\nExtensive experiments show that LMM4Edit achieves outstanding performance and\naligns well with human preference. Zero-shot validation on the other datasets\nalso shows the generalization ability of our model. The dataset and code are\navailable at https://github.com/IntMeGroup/LMM4Edit."}
{"id": "2507.16620", "pdf": "https://arxiv.org/pdf/2507.16620", "abs": "https://arxiv.org/abs/2507.16620", "authors": ["Faruk Alpay", "Hamdi Al Alakkad"], "title": "Transordinal Fixed-Point Operators and Self-Referential Games: A Categorical Framework for Reflective Semantic Convergence", "categories": ["cs.LO", "68Q55, 03B70, 47H10, 91A05, 18C15, 68T50", "F.3.2; I.2.7; F.1.1; F.4.1"], "comment": "16 pages, 0 figure", "summary": "We present a new theoretical framework that unifies category-theoretic\nfixed-point constructions, transfinite recursion, and game-based semantics to\nmodel how interpretations of language can stabilize through unlimited\nself-reference. By iterating a meaning-refinement operator across all ordinal\nstages, we isolate a unique \"transordinal\" fixed point and show, via a\nhierarchy of reflective games, that this same object is the sole equilibrium of\nan infinite dialogue between a text and its interpreter. The result delivers a\nmathematically rigorous account of semantic convergence without resorting to\nstatistical training or empirical benchmarks, yet remains simple to explain:\nstart with a rough meaning, let speaker and listener correct each other\nforever, and the process provably settles on a single, self-consistent\ninterpretation. Because the construction is entirely symbolic, it offers both\nprecise guarantees for formal linguistics and a blueprint for designing\nlanguage-aware systems that can reason about their own outputs. The paper\ndetails the requisite transordinal machinery, proves existence and uniqueness\ntheorems, and connects them to long-standing questions about reflection, truth,\nand equilibrium in formal systems and semantics."}
{"id": "2507.16077", "pdf": "https://arxiv.org/pdf/2507.16077", "abs": "https://arxiv.org/abs/2507.16077", "authors": ["Rodrigo Moreira", "Rafael Pasquini", "Joberto S. B. Martins", "Tereza C. Carvalho", "FlÃ¡vio de Oliveira Silva"], "title": "AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds", "categories": ["cs.ET", "cs.AI", "cs.LG", "cs.MA", "cs.NI"], "comment": "17 pages, 18 figures, 14 tables,", "summary": "Network Slicing (NS) realization requires AI-native orchestration\narchitectures to efficiently and intelligently handle heterogeneous user\nrequirements. To achieve this, network slicing is evolving towards a more\nuser-centric digital transformation, focusing on architectures that incorporate\nnative intelligence to enable self-managed connectivity in an integrated and\nisolated manner. However, these initiatives face the challenge of validating\ntheir results in production environments, particularly those utilizing\nML-enabled orchestration, as they are often tested in local networks or\nlaboratory simulations. This paper proposes a large-scale validation method\nusing a network slicing prediction model to forecast latency using Deep Neural\nNetworks (DNNs) and basic ML algorithms embedded within an NS architecture,\nevaluated in real large-scale production testbeds. It measures and compares the\nperformance of different DNNs and ML algorithms, considering a distributed\ndatabase application deployed as a network slice over two large-scale\nproduction testbeds. The investigation highlights how AI-based prediction\nmodels can enhance network slicing orchestration architectures and presents a\nseamless, production-ready validation method as an alternative to fully\ncontrolled simulations or laboratory setups."}
{"id": "2507.16710", "pdf": "https://arxiv.org/pdf/2507.16710", "abs": "https://arxiv.org/abs/2507.16710", "authors": ["Andrei-Leonard Nicusan", "Dominik Werner", "Simon Branford", "Simon Hartley", "Andrew J. Morris", "Kit Windows-Yule"], "title": "AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a Unified, Transpiled Codebase", "categories": ["cs.DC", "cs.PF"], "comment": null, "summary": "AcceleratedKernels.jl is introduced as a backend-agnostic library for\nparallel computing in Julia, natively targeting NVIDIA, AMD, Intel, and Apple\naccelerators via a unique transpilation architecture. Written in a unified,\ncompact codebase, it enables productive parallel programming with minimised\nimplementation and usage complexities. Benchmarks of arithmetic-heavy kernels\nshow performance on par with C and OpenMP-multithreaded CPU implementations,\nwith Julia sometimes offering more consistent and predictable numerical\nperformance than conventional C compilers. Exceptional composability is\nhighlighted as simultaneous CPU-GPU co-processing is achievable - such as\nCPU-GPU co-sorting - with transparent use of hardware-specialised MPI\nimplementations. Tests on the Baskerville Tier 2 UK HPC cluster achieved\nworld-class sorting throughputs of 538-855 GB/s using 200 NVIDIA A100 GPUs,\ncomparable to the highest literature-reported figure of 900 GB/s achieved on\n262,144 CPU cores. The use of direct NVLink GPU-to-GPU interconnects resulted\nin a 4.93x speedup on average; normalised by a combined capital, running and\nenvironmental cost, communication-heavy HPC tasks only become economically\nviable on GPUs if GPUDirect interconnects are employed."}
{"id": "2507.16013", "pdf": "https://arxiv.org/pdf/2507.16013", "abs": "https://arxiv.org/abs/2507.16013", "authors": ["Lucas Jasper Jacobsen", "Ute Mertens", "Thorben Jansen", "Kira Elena Weber"], "title": "AI, Expert or Peer? -- Examining the Impact of Perceived Feedback Source on Pre-Service Teachers Feedback Perception and Uptake", "categories": ["cs.HC"], "comment": "37 pages, 2 figures, 6 tables", "summary": "Feedback plays a central role in learning, yet pre-service teachers'\nengagement with feedback depends not only on its quality but also on their\nperception of the feedback content and source. Large Language Models (LLMs) are\nincreasingly used to provide educational feedback; however, negative\nperceptions may limit their practical use, and little is known about how\npre-service teachers' perceptions and behavioral responses differ by feedback\nsource. This study investigates how the perceived source of feedback - LLM,\nexpert, or peer - influences feedback perception and uptake, and whether\nrecognition accuracy and feedback quality moderate these effects. In a\nrandomized experiment with 273 pre-service teachers, participants received\nwritten feedback on a mathematics learning goal, identified its source, rated\nfeedback perceptions across five dimensions (fairness, usefulness, acceptance,\nwillingness to improve, positive and negative affect), and revised the learning\ngoal according to the feedback (i.e. feedback uptake). Results revealed that\nLLM-generated feedback received the highest ratings in fairness and usefulness,\nleading to the highest uptake (52%). Recognition accuracy significantly\nmoderated the effect of feedback source on perception, with particularly\npositive evaluations when LLM feedback was falsely ascribed to experts.\nHigher-quality feedback was consistently assigned to experts, indicating an\nexpertise heuristic in source judgments. Regression analysis showed that only\nfeedback quality significantly predicted feedback uptake. Findings highlight\nthe need to address source-related biases and promote feedback and AI literacy\nin teacher education."}
{"id": "2507.16584", "pdf": "https://arxiv.org/pdf/2507.16584", "abs": "https://arxiv.org/abs/2507.16584", "authors": ["Nico Kraus", "Marvin Erdmann", "Alexander Kuzmany", "Daniel Porawski", "Jonas Stein"], "title": "Quantum Annealing Hyperparameter Analysis for Optimal Sensor Placement in Production Environments", "categories": ["cs.ET", "quant-ph"], "comment": null, "summary": "To increase efficiency in automotive manufacturing, newly produced vehicles\ncan move autonomously from the production line to the distribution area. This\nrequires an optimal placement of sensors to ensure full coverage while\nminimizing the number of sensors used. The underlying optimization problem\nposes a computational challenge due to its large-scale nature. Currently,\nclassical solvers rely on heuristics, often yielding non-optimal solutions for\nlarge instances, resulting in suboptimal sensor distributions and increased\noperational costs.\n  We explore quantum computing methods that may outperform classical heuristics\nin the future. We implemented quantum annealing with D-Wave, transforming the\nproblem into a quadratic unconstrained binary optimization formulation with\none-hot and binary encoding. Hyperparameters like the penalty terms and the\nannealing time are optimized and the results are compared with default\nparameter settings.\n  Our results demonstrate that quantum annealing is capable of solving\ninstances derived from real-world scenarios. Through the use of decomposition\ntechniques, we are able to scale the problem size further, bringing it closer\nto practical, industrial applicability. Through this work, we provide key\ninsights into the importance of quantum annealing parametrization,\ndemonstrating how quantum computing could contribute to cost-efficient,\nlarge-scale optimization problems once the hardware matures."}
{"id": "2507.16350", "pdf": "https://arxiv.org/pdf/2507.16350", "abs": "https://arxiv.org/abs/2507.16350", "authors": ["Serdar Metin"], "title": "Autonomous Dominant Resource Fairness for Blockchain Ecosystems", "categories": ["cs.DC"], "comment": "10 pages, 3 figures", "summary": "Blockchain systems have been a part of mainstream academic research, and a\nhot topic at that. It has spread to almost every subfield in the computer\nscience literature, as well as economics and finance. Especially in a world\nwhere digital trust is much sought for, blockchains offer a rich variety of\ndesired properties, such as immutability, public auditing, decentralised record\nkeeping, among others. Not only has it been a research topic of its own, the\nintegration of blockchains into other systems has been proposed as solutions in\nmany areas, ranging from grid computing, cloud and fog computing, to internet\nof things, self driving vehicles , and smart cities. In many cases the primary\nfunction attributed to blockchains in these contexts is resource management.\nAlthough much attention is paid to this topic, the focus is on single resource\nallocation scenarios. Even the cases where multiple resource types are to be\nallocated, are treated as single resource type scenarios, and problems are\nformulated as allocating standardised bundles consisting of a fixed amount of\neach of them, such as virtual machines. The present study addresses the problem\nof allocating multiple resource types among tasks with heterogeneous resource\ndemands with a smart contract adaptation of Precomputed Dominant Resource\nFairness; an algorithm that approximates Dominant Resource Fairness, without\nloop iterations, which makes it preferable in the blockchain context because of\nthe block gas limit. We present the resulting algorithm, Autonomous Dominant\nResource Fairness, along with the empirical data collected from the tests run\non the algorithm. The results show that Autonomous Dominant Resource Fairness\nis a gas-cost efficient algorithm, which can be used to manage hundreds of\nresource types for unlimited number of users."}
{"id": "2507.16379", "pdf": "https://arxiv.org/pdf/2507.16379", "abs": "https://arxiv.org/abs/2507.16379", "authors": ["Ondrej Vlcek", "Vojtech Mrazek"], "title": "ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing", "categories": ["cs.AR"], "comment": "To appear at ICCAD 2025", "summary": "Approximate computing offers promising energy efficiency benefits for\nerror-tolerant applications, but discovering optimal approximations requires\nextensive design space exploration (DSE). Predicting the accuracy of circuits\ncomposed of approximate components without performing complete synthesis\nremains a challenging problem. Current machine learning approaches used to\nautomate this task require retraining for each new circuit configuration,\nmaking them computationally expensive and time-consuming. This paper presents\nApproxGNN, a construction methodology for a pre-trained graph neural network\nmodel predicting QoR and HW cost of approximate accelerators employing\napproximate adders from a library. This approach is applicable in DSE for\nassignment of approximate components to operations in accelerator. Our approach\nintroduces novel component feature extraction based on learned embeddings\nrather than traditional error metrics, enabling improved transferability to\nunseen circuits. ApproxGNN models can be trained with a small number of\napproximate components, supports transfer to multiple prediction tasks,\nutilizes precomputed embeddings for efficiency, and significantly improves\naccuracy of the prediction of approximation error. On a set of image\nconvolutional filters, our experimental results demonstrate that the proposed\nembeddings improve prediction accuracy (mean square error) by 50% compared to\nconventional methods. Furthermore, the overall prediction accuracy is 30%\nbetter than statistical machine learning approaches without fine-tuning and 54%\nbetter with fast finetuning."}
{"id": "2507.16089", "pdf": "https://arxiv.org/pdf/2507.16089", "abs": "https://arxiv.org/abs/2507.16089", "authors": ["Michael J. Sullivan", "Zhibo Chen", "Elvis Pranskevichus", "Robert J. Simmons", "Victor Petrovykh", "AljaÅ¾ Mur ErÅ¾en", "Yury Selivanov"], "title": "Querying Graph-Relational Data", "categories": ["cs.PL", "cs.DB"], "comment": null, "summary": "For applications that store structured data in relational databases, there is\nan impedance mismatch between the flat representations encouraged by relational\ndata models and the deeply nested information that applications expect to\nreceive. In this work, we present the graph-relational database model, which\nprovides a flexible, compositional, and strongly-typed solution to this\n\"object-relational mismatch.\" We formally define the graph-relational database\nmodel and present a static and dynamic semantics for queries. In addition, we\ndiscuss the realization of the graph-relational database model in EdgeQL, a\ngeneral-purpose SQL-style query language, and the Gel system, which compiles\nEdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of\nobject-shaped data manipulation that is frequently provided inefficiently by\nobject-relational mapping (ORM) technologies, while achieving most of the\nefficiency that comes from require writing complex PostgreSQL queries directly."}
{"id": "2507.16166", "pdf": "https://arxiv.org/pdf/2507.16166", "abs": "https://arxiv.org/abs/2507.16166", "authors": ["Nasir U. Eisty", "David E. Bernholdt", "Alex Koufos", "David J. Luet", "Miranda Mundt"], "title": "Ten Essential Guidelines for Building High-Quality Research Software", "categories": ["cs.SE"], "comment": null, "summary": "High-quality research software is a cornerstone of modern scientific\nprogress, enabling researchers to analyze complex data, simulate phenomena, and\nshare reproducible results. However, creating such software requires adherence\nto best practices that ensure robustness, usability, and sustainability. This\npaper presents ten guidelines for producing high-quality research software,\ncovering every stage of the development lifecycle. These guidelines emphasize\nthe importance of planning, writing clean and readable code, using version\ncontrol, and implementing thorough testing strategies. Additionally, they\naddress key principles such as modular design, reproducibility, performance\noptimization, and long-term maintenance. The paper also highlights the role of\ndocumentation and community engagement in enhancing software usability and\nimpact. By following these guidelines, researchers can create software that\nadvances their scientific objectives and contributes to a broader ecosystem of\nreliable and reusable research tools. This work serves as a practical resource\nfor researchers and developers aiming to elevate the quality and impact of\ntheir research software."}
{"id": "2507.16660", "pdf": "https://arxiv.org/pdf/2507.16660", "abs": "https://arxiv.org/abs/2507.16660", "authors": ["Xuran Cai"], "title": "Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs", "categories": ["cs.PL"], "comment": null, "summary": "This thesis addresses the complexities of compiler optimizations, such as\nregister allocation and Lifetime-optimal Speculative Partial Redundancy\nElimination (LOSPRE), which are often handled using tree decomposition\nalgorithms. However, these methods frequently overlook important sparsity\naspects of Control Flow Graphs (CFGs) and result in high computational costs.\nWe introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework\nthat offers optimal solutions to these challenges. A key contribution is the\nformulation of a general solution for Partial Constraint Satisfaction Problems\n(PCSPs) within graph structures, applied to three optimization problems. First,\nSPL decomposition enhances register allocation by accurately modeling variable\ninterference graphs, leading to efficient register assignments and improved\nperformance across benchmarks. Second, it optimizes LOSPRE by effectively\nidentifying and eliminating redundancies in program execution. Finally, the\nthesis focuses on optimizing the placement of bank selection instructions to\nenhance data retrieval efficiency and reduce latency. Extensive experimentation\ndemonstrates significant performance improvements over existing methods,\nestablishing SPL decomposition as a powerful tool for complex compiler\noptimizations, including register allocation, LOSPRE, and bank selection."}
{"id": "2507.16363", "pdf": "https://arxiv.org/pdf/2507.16363", "abs": "https://arxiv.org/abs/2507.16363", "authors": ["Hailin Yue", "Hulin Kuang", "Jin Liu", "Junjian Li", "Lanlan Wang", "Mengshen He", "Jianxin Wang"], "title": "Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction", "categories": ["cs.LG", "cs.MM"], "comment": null, "summary": "Accurately predicting the survival of cancer patients is crucial for\npersonalized treatment. However, existing studies focus solely on the\nrelationships between samples with known survival risks, without fully\nleveraging the value of censored samples. Furthermore, these studies may suffer\nperformance degradation in modality-missing scenarios and even struggle during\nthe inference process. In this study, we propose a bipartite patient-modality\ngraph learning with event-conditional modelling of censoring for cancer\nsurvival prediction (CenSurv). Specifically, we first use graph structure to\nmodel multimodal data and obtain representation. Then, to alleviate performance\ndegradation in modality-missing scenarios, we design a bipartite graph to\nsimulate the patient-modality relationship in various modality-missing\nscenarios and leverage a complete-incomplete alignment strategy to explore\nmodality-agnostic features. Finally, we design a plug-and-play\nevent-conditional modeling of censoring (ECMC) that selects reliable censored\ndata using dynamic momentum accumulation confidences, assigns more accurate\nsurvival times to these censored data, and incorporates them as uncensored data\ninto training. Comprehensive evaluations on 5 publicly cancer datasets showcase\nthe superiority of CenSurv over the best state-of-the-art by 3.1% in terms of\nthe mean C-index, while also exhibiting excellent robustness under various\nmodality-missing scenarios. In addition, using the plug-and-play ECMC module,\nthe mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of\nCenSurv is available at https://github.com/yuehailin/CenSurv."}
{"id": "2507.16067", "pdf": "https://arxiv.org/pdf/2507.16067", "abs": "https://arxiv.org/abs/2507.16067", "authors": ["Jeroen Spaans", "Jesse Heyninck"], "title": "A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)", "categories": ["cs.AI", "cs.LO"], "comment": "Full version, including proofs and appendices, of paper accepted at\n  IJCAI 2025", "summary": "Constraint Logic Programming (CLP) is a logic programming formalism used to\nsolve problems requiring the consideration of constraints, like resource\nallocation and automated planning and scheduling. It has previously been\nextended in various directions, for example to support fuzzy constraint\nsatisfaction, uncertainty, or negation, with different notions of semiring\nbeing used as a unifying abstraction for these generalizations. None of these\nextensions have studied clauses with negation allowed in the body. We\ninvestigate an extension of CLP which unifies many of these extensions and\nallows negation in the body. We provide semantics for such programs, using the\nframework of approximation fixpoint theory, and give a detailed overview of the\nimpacts of properties of the semirings on the resulting semantics. As such, we\nprovide a unifying framework that captures existing approaches and allows\nextending them with a more expressive language."}
{"id": "2507.16680", "pdf": "https://arxiv.org/pdf/2507.16680", "abs": "https://arxiv.org/abs/2507.16680", "authors": ["Mario Edoardo Pandolfo", "Simone Fiorellino", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "title": "Latent Space Alignment for AI-Native MIMO Semantic Communications", "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "comment": "Proc. of IEEE IJCNN 2025", "summary": "Semantic communications focus on prioritizing the understanding of the\nmeaning behind transmitted data and ensuring the successful completion of tasks\nthat motivate the exchange of information. However, when devices rely on\ndifferent languages, logic, or internal representations, semantic mismatches\nmay occur, potentially hindering mutual understanding. This paper introduces a\nnovel approach to addressing latent space misalignment in semantic\ncommunications, exploiting multiple-input multiple-output (MIMO)\ncommunications. Specifically, our method learns a MIMO precoder/decoder pair\nthat jointly performs latent space compression and semantic channel\nequalization, mitigating both semantic mismatches and physical channel\nimpairments. We explore two solutions: (i) a linear model, optimized by solving\na biconvex optimization problem via the alternating direction method of\nmultipliers (ADMM); (ii) a neural network-based model, which learns semantic\nMIMO precoder/decoder under transmission power budget and complexity\nconstraints. Numerical results demonstrate the effectiveness of the proposed\napproach in a goal-oriented semantic communication scenario, illustrating the\nmain trade-offs between accuracy, communication burden, and complexity of the\nsolutions."}
{"id": "2507.16033", "pdf": "https://arxiv.org/pdf/2507.16033", "abs": "https://arxiv.org/abs/2507.16033", "authors": ["Ding Wang", "Mark DÃ­az", "Charvi Rastogi", "Aida Davani", "Vinodkumar Prabhakaran", "Pushkar Mishra", "Roma Patel", "Alicia Parrish", "Zoe Ashwood", "Michela Paganini", "Tian Huey Teh", "Verena Rieser", "Lora Aroyo"], "title": "\"Just a strange pic\": Evaluating 'safety' in GenAI Image safety annotation tasks from diverse annotators' perspectives", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to AAAI/ACM Conference on Artificial Intelligence, Ethics,\n  and Society 2025 (AIES 2025)", "summary": "Understanding what constitutes safety in AI-generated content is complex.\nWhile developers often rely on predefined taxonomies, real-world safety\njudgments also involve personal, social, and cultural perceptions of harm. This\npaper examines how annotators evaluate the safety of AI-generated images,\nfocusing on the qualitative reasoning behind their judgments. Analyzing 5,372\nopen-ended comments, we find that annotators consistently invoke moral,\nemotional, and contextual reasoning that extends beyond structured safety\ncategories. Many reflect on potential harm to others more than to themselves,\ngrounding their judgments in lived experience, collective risk, and\nsociocultural awareness. Beyond individual perceptions, we also find that the\nstructure of the task itself -- including annotation guidelines -- shapes how\nannotators interpret and express harm. Guidelines influence not only which\nimages are flagged, but also the moral judgment behind the justifications.\nAnnotators frequently cite factors such as image quality, visual distortion,\nand mismatches between prompt and output as contributing to perceived harm\ndimensions, which are often overlooked in standard evaluation frameworks. Our\nfindings reveal that existing safety pipelines miss critical forms of reasoning\nthat annotators bring to the task. We argue for evaluation designs that\nscaffold moral reflection, differentiate types of harm, and make space for\nsubjective, context-sensitive interpretations of AI-generated content."}
{"id": "2507.16413", "pdf": "https://arxiv.org/pdf/2507.16413", "abs": "https://arxiv.org/abs/2507.16413", "authors": ["Xavier Diaz", "Gianluca D'Amico", "Raul Dominguez-Sanchez", "Federico Nesti", "Max Ronecker", "Giorgio Buttazzo"], "title": "Towards Railway Domain Adaptation for LiDAR-based 3D Detection: Road-to-Rail and Sim-to-Real via SynDRA-BBox", "categories": ["cs.CV", "cs.ET"], "comment": "IEEE International Conference on Intelligent Rail Transportation\n  (ICIRT) 2025", "summary": "In recent years, interest in automatic train operations has significantly\nincreased. To enable advanced functionalities, robust vision-based algorithms\nare essential for perceiving and understanding the surrounding environment.\nHowever, the railway sector suffers from a lack of publicly available\nreal-world annotated datasets, making it challenging to test and validate new\nperception solutions in this domain. To address this gap, we introduce\nSynDRA-BBox, a synthetic dataset designed to support object detection and other\nvision-based tasks in realistic railway scenarios. To the best of our\nknowledge, is the first synthetic dataset specifically tailored for 2D and 3D\nobject detection in the railway domain, the dataset is publicly available at\nhttps://syndra.retis.santannapisa.it. In the presented evaluation, a\nstate-of-the-art semi-supervised domain adaptation method, originally developed\nfor automotive perception, is adapted to the railway context, enabling the\ntransferability of synthetic data to 3D object detection. Experimental results\ndemonstrate promising performance, highlighting the effectiveness of synthetic\ndatasets and domain adaptation techniques in advancing perception capabilities\nfor railway environments."}
{"id": "2507.16668", "pdf": "https://arxiv.org/pdf/2507.16668", "abs": "https://arxiv.org/abs/2507.16668", "authors": ["Somayeh Sobati-M"], "title": "FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture", "categories": ["cs.DC"], "comment": null, "summary": "Modern smart grids demand fast, intelligent, and energy-aware computing at\nthe edge to manage real time fluctuations and ensure reliable operation. This\npaper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration\nand Twin based Execution a next-generation fog cloud framework designed to\nenhance autonomy, resilience, and efficiency in distributed energy systems.\nFOGNITE combines three core components: federated learning, reinforcement\nlearning, and digital twin validation. Each fog node trains a local CNN LSTM\nmodel on private energy consumption data, enabling predictive intelligence\nwhile preserving data privacy through federated aggregation. A reinforcement\nlearning agent dynamically schedules tasks based on current system load and\nenergy conditions, optimizing for performance under uncertainty.\n  To prevent unsafe or inefficient decisions, a hierarchical digital twin layer\nsimulates potential actions before deployment, significantly reducing execution\nerrors and energy waste. We evaluate FOGNITE on a real world testbed of\nRaspberry Pi devices, showing up to a 93.7% improvement in load balancing\naccuracy and a 63.2% reduction in energy waste compared to conventional\narchitectures. By shifting smart grid control from reactive correction to\nproactive optimization, FOGNITE represents a step toward more intelligent,\nadaptive, and sustainable energy infrastructures"}
{"id": "2507.16391", "pdf": "https://arxiv.org/pdf/2507.16391", "abs": "https://arxiv.org/abs/2507.16391", "authors": ["Chenqi Lin", "Kang Yang", "Tianshi Xu", "Ling Liang", "Yufei Wang", "Zhaohui Chen", "Runsheng Wang", "Mingyu Gao", "Meng Li"], "title": "Ironman: Accelerating Oblivious Transfer Extension for Privacy-Preserving AI with Near-Memory Processing", "categories": ["cs.AR"], "comment": null, "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."}
{"id": "2507.16208", "pdf": "https://arxiv.org/pdf/2507.16208", "abs": "https://arxiv.org/abs/2507.16208", "authors": ["Sohaib Muhammad", "Ashwati Vipin", "Karan Shetti", "Honey Mittal"], "title": "LOCOFY Large Design Models -- Design to code conversion solution", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code."}
{"id": "2507.16696", "pdf": "https://arxiv.org/pdf/2507.16696", "abs": "https://arxiv.org/abs/2507.16696", "authors": ["Pingyi Fan", "Anbai Jiang", "Shuwei Zhang", "Zhiqiang Lv", "Bing Han", "Xinhu Zheng", "Wenrui Liang", "Junjie Li", "Wei-Qiang Zhang", "Yanmin Qian", "Xie Chen", "Cheng Lu", "Jia Liu"], "title": "FISHER: A Foundation Model for Multi-Modal Industrial Signal Comprehensive Representation", "categories": ["cs.LG", "cs.AI", "cs.MM", "cs.SD"], "comment": "11 pages, 6 figures", "summary": "With the rapid deployment of SCADA systems, how to effectively analyze\nindustrial signals and detect abnormal states is an urgent need for the\nindustry. Due to the significant heterogeneity of these signals, which we\nsummarize as the M5 problem, previous works only focus on small sub-problems\nand employ specialized models, failing to utilize the synergies between\nmodalities and the powerful scaling law. However, we argue that the M5 signals\ncan be modeled in a unified manner due to the intrinsic similarity. As a\nresult, we propose FISHER, a Foundation model for multi-modal Industrial Signal\ncompreHEnsive Representation. To support arbitrary sampling rates, FISHER\nconsiders the increment of sampling rate as the concatenation of sub-band\ninformation. Specifically, FISHER takes the STFT sub-band as the modeling unit\nand adopts a teacher student SSL framework for pre-training. We also develop\nthe RMIS benchmark, which evaluates the representations of M5 industrial\nsignals on multiple health management tasks. Compared with top SSL models,\nFISHER showcases versatile and outstanding capabilities with a general\nperformance gain up to 5.03%, along with much more efficient scaling curves. We\nalso investigate the scaling law on downstream tasks and derive potential\navenues for future works. FISHER is now open-sourced on\nhttps://github.com/jianganbai/FISHER"}
{"id": "2507.16454", "pdf": "https://arxiv.org/pdf/2507.16454", "abs": "https://arxiv.org/abs/2507.16454", "authors": ["Pierangela Bruno", "Carmine Dodaro", "Giuseppe GalatÃ ", "Marco Maratea", "Marco Mochi"], "title": "Improving ASP-based ORS Schedules through Machine Learning Predictions", "categories": ["cs.AI", "cs.LO"], "comment": "17 pages, International Conference on Logic Programming, Under\n  consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "The Operating Room Scheduling (ORS) problem deals with the optimization of\ndaily operating room surgery schedules. It is a challenging problem subject to\nmany constraints, like to determine the starting time of different surgeries\nand allocating the required resources, including the availability of beds in\ndifferent department units. Recently, solutions to this problem based on Answer\nSet Programming (ASP) have been delivered. Such solutions are overall\nsatisfying but, when applied to real data, they can currently only verify\nwhether the encoding aligns with the actual data and, at most, suggest\nalternative schedules that could have been computed. As a consequence, it is\nnot currently possible to generate provisional schedules. Furthermore, the\nresulting schedules are not always robust.\n  In this paper, we integrate inductive and deductive techniques for solving\nthese issues. We first employ machine learning algorithms to predict the\nsurgery duration, from historical data, to compute provisional schedules. Then,\nwe consider the confidence of such predictions as an additional input to our\nproblem and update the encoding correspondingly in order to compute more robust\nschedules. Results on historical data from the ASL1 Liguria in Italy confirm\nthe viability of our integration.\n  Under consideration in Theory and Practice of Logic Programming (TPLP)."}
{"id": "2507.16073", "pdf": "https://arxiv.org/pdf/2507.16073", "abs": "https://arxiv.org/abs/2507.16073", "authors": ["Annabelle Warner", "Andrew McNutt", "Paul Rosen", "El Kindi Rezig"], "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler", "categories": ["cs.HC", "cs.DB"], "comment": "Accepted to VLDB25 Demo track", "summary": "Preparing datasets -- a critical phase known as data wrangling -- constitutes\nthe dominant phase of data science development, consuming upwards of 80% of the\ntotal project time. This phase encompasses a myriad of tasks: parsing data,\nrestructuring it for analysis, repairing inaccuracies, merging sources,\neliminating duplicates, and ensuring overall data integrity. Traditional\napproaches, typically through manual coding in languages such as Python or\nusing spreadsheets, are not only laborious but also error-prone. These issues\nrange from missing entries and formatting inconsistencies to data type\ninaccuracies, all of which can affect the quality of downstream tasks if not\nproperly corrected. To address these challenges, we present Buckaroo, a\nvisualization system to highlight discrepancies in data and enable on-the-spot\ncorrections through direct manipulations of visual objects. Buckaroo (1)\nautomatically finds \"interesting\" data groups that exhibit anomalies compared\nto the rest of the groups and recommends them for inspection; (2) suggests\nwrangling actions that the user can choose to repair the anomalies; and (3)\nallows users to visually manipulate their data by displaying the effects of\ntheir wrangling actions and offering the ability to undo or redo these actions,\nwhich supports the iterative nature of data wrangling. A video companion is\navailable at https://youtu.be/iXdCYbvpQVE"}
{"id": "2507.16480", "pdf": "https://arxiv.org/pdf/2507.16480", "abs": "https://arxiv.org/abs/2507.16480", "authors": ["Sabrina Livanec", "Laura LondoÃ±o", "Michael Gorki", "Adrian RÃ¶fer", "Abhinav Valada", "Andrea Kiesel"], "title": "Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "comment": null, "summary": "The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations."}
{"id": "2507.16710", "pdf": "https://arxiv.org/pdf/2507.16710", "abs": "https://arxiv.org/abs/2507.16710", "authors": ["Andrei-Leonard Nicusan", "Dominik Werner", "Simon Branford", "Simon Hartley", "Andrew J. Morris", "Kit Windows-Yule"], "title": "AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a Unified, Transpiled Codebase", "categories": ["cs.DC", "cs.PF"], "comment": null, "summary": "AcceleratedKernels.jl is introduced as a backend-agnostic library for\nparallel computing in Julia, natively targeting NVIDIA, AMD, Intel, and Apple\naccelerators via a unique transpilation architecture. Written in a unified,\ncompact codebase, it enables productive parallel programming with minimised\nimplementation and usage complexities. Benchmarks of arithmetic-heavy kernels\nshow performance on par with C and OpenMP-multithreaded CPU implementations,\nwith Julia sometimes offering more consistent and predictable numerical\nperformance than conventional C compilers. Exceptional composability is\nhighlighted as simultaneous CPU-GPU co-processing is achievable - such as\nCPU-GPU co-sorting - with transparent use of hardware-specialised MPI\nimplementations. Tests on the Baskerville Tier 2 UK HPC cluster achieved\nworld-class sorting throughputs of 538-855 GB/s using 200 NVIDIA A100 GPUs,\ncomparable to the highest literature-reported figure of 900 GB/s achieved on\n262,144 CPU cores. The use of direct NVLink GPU-to-GPU interconnects resulted\nin a 4.93x speedup on average; normalised by a combined capital, running and\nenvironmental cost, communication-heavy HPC tasks only become economically\nviable on GPUs if GPUDirect interconnects are employed."}
{"id": "2507.16628", "pdf": "https://arxiv.org/pdf/2507.16628", "abs": "https://arxiv.org/abs/2507.16628", "authors": ["Rajpreet Singh", "Vidhi Kothari"], "title": "Augmenting Von Neumann's Architecture for an Intelligent Future", "categories": ["cs.AR"], "comment": "6 pages, 2 figures", "summary": "This work presents a novel computer architecture that extends the Von Neumann\nmodel with a dedicated Reasoning Unit (RU) to enable native artificial general\nintelligence capabilities. The RU functions as a specialized co-processor that\nexecutes symbolic inference, multi-agent coordination, and hybrid\nsymbolic-neural computation as fundamental architectural primitives. This\nhardware-embedded approach allows autonomous agents to perform goal-directed\nplanning, dynamic knowledge manipulation, and introspective reasoning directly\nwithin the computational substrate at system scale. The architecture\nincorporates a reasoning-specific instruction set architecture, parallel\nsymbolic processing pipelines, agent-aware kernel abstractions, and a unified\nmemory hierarchy that seamlessly integrates cognitive and numerical workloads.\nThrough systematic co-design across hardware, operating system, and agent\nruntime layers, this architecture establishes a computational foundation where\nreasoning, learning, and adaptation emerge as intrinsic execution properties\nrather than software abstractions, potentially enabling the development of\ngeneral-purpose intelligent machines."}
{"id": "2507.16327", "pdf": "https://arxiv.org/pdf/2507.16327", "abs": "https://arxiv.org/abs/2507.16327", "authors": ["Karoline NylÃ¦nder", "Aitor Arrieta", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels", "categories": ["cs.SE"], "comment": "9 pages, 3 figures. Accepted at GECCO 2025 (Genetic and Evolutionary\n  Computation Conference), July 14-18, 2025, Malaga, Spain", "summary": "Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt\ntheir behaviors to address unexpected situations while maintaining\ndependability requirements. During the design of such AVs, it is crucial to\nunderstand and identify the settings that should trigger adaptations, enabling\nvalidation of their implementation. To this end, we focus on the navigation\nsoftware of AVs, which must adapt their behavior during operation through\nadaptations. AVs often rely on predefined waypoints to guide them along\ndesignated routes, ensuring safe navigation. We propose a multiobjective\nsearch-based approach, called WPgen, to generate minor modifications to the\npredefined set of waypoints, keeping them as close as possible to the original\nwaypoints, while causing the AV to navigate inappropriately when navigating\nwith the generated waypoints. WPgen uses NSGA-II as the multi-objective search\nalgorithm with three seeding strategies for its initial population, resulting\nin three variations of WPgen. We evaluated these variations on three AVs (one\noverwater tanker and two underwater). We compared the three variations of WPgen\nwith Random Search as the baseline and with each other. Experimental results\nshowed that the effectiveness of these variations varied depending on the AV.\nBased on the results, we present the research and practical implications of\nWPgen."}
{"id": "2507.16074", "pdf": "https://arxiv.org/pdf/2507.16074", "abs": "https://arxiv.org/abs/2507.16074", "authors": ["Natasha Yamane", "Varun Mishra", "Matthew S. Goodwin"], "title": "Toward music-based stress management: Contemporary biosensing systems for affective regulation", "categories": ["cs.HC"], "comment": "37 pages, 3 figures, 1 table", "summary": "In the last decade, researchers have increasingly explored using biosensing\ntechnologies for music-based affective regulation and stress management\ninterventions in laboratory and real-world settings. These systems -- including\ninteractive music applications, brain-computer interfaces, and biofeedback\ndevices -- aim to provide engaging, personalized experiences that improve\ntherapeutic outcomes. In this scoping and mapping review, we summarize and\nsynthesize systematic reviews and empirical research on biosensing systems with\npotential applications in music-based affective regulation and stress\nmanagement, identify gaps in the literature, and highlight promising areas for\nfuture research. We identified 28 studies involving 646 participants, with most\nsystems utilizing prerecorded music, wearable cardiorespiratory sensors, or\ndesktop interfaces. We categorize these systems based on their biosensing\nmodalities, music types, computational models for affect or stress detection\nand music prediction, and biofeedback mechanisms. Our findings highlight the\npromising potential of these systems and suggest future directions, such as\nintegrating multimodal biosensing, exploring therapeutic mechanisms of music,\nleveraging generative artificial intelligence for personalized music\ninterventions, and addressing methodological, data privacy, and user control\nconcerns."}
{"id": "2507.16499", "pdf": "https://arxiv.org/pdf/2507.16499", "abs": "https://arxiv.org/abs/2507.16499", "authors": ["Recep Akif Tasci", "Panagiotis Gavriilidis", "Ertugrul Basar", "George C. Alexandropoulos"], "title": "Active RISs: Modeling and Optimization", "categories": ["cs.IT", "cs.ET", "eess.SP", "math.IT"], "comment": "43 pages, 15 figures, Book chapter", "summary": "Reconfigurable Intelligent Surfaces (RIS)-empowered communication has emerged\nas a transformative technology for next generation wireless networks, enabling\nthe programmable shaping of the propagation environment. However, conventional\nRISs are fundamentally limited by the double path loss effect, which severely\nattenuates the reflected signals. To overcome this, active RIS architectures,\ncapable of amplifying impinging signals, have been proposed. This chapter\ninvestigates the modeling, performance analysis, and optimization of active\nRISs, focusing on two hardware designs: a dual-RIS structure with a single\nPower Amplifier (PA), and a reflection amplification structure at the unit cell\nlevel using tunnel diodes. For the PA-based design, a comprehensive\nmathematical model is developed, and closed-form expressions for the received\nsignal-to-noise ratio, bit error probability, and Energy Efficiency (EE) are\nderived. An optimization framework for configuring the phase shifts and\namplifier gain is proposed to maximize system capacity under power constraints.\nRegarding the second design, the integration of a tunnel diode into the unit\ncell is carefully studied by analyzing its I-V characteristic, enabling the\nderivation of the negative resistance range and the power consumption model.\nFurthermore, the intrinsic phase-amplitude coupling of the reflection\ncoefficient is characterized through compact linear algebra formulations,\nenabling practical optimization of active RISs. Extensive numerical simulations\nvalidate the theoretical analyses, demonstrating that active RISs can\neffectively overcome the double path loss limitation and achieve favorable EE\ntrade-offs compared to passive RISs. Finally, the trade-off between the\navailable power budget and the number of active elements is examined, revealing\nthat a higher number of active elements does not always lead to optimal\nperformance."}
{"id": "2507.16731", "pdf": "https://arxiv.org/pdf/2507.16731", "abs": "https://arxiv.org/abs/2507.16731", "authors": ["Senyao Li", "Haozhao Wang", "Wenchao Xu", "Rui Zhang", "Song Guo", "Jingling Yuan", "Xian Zhong", "Tianwei Zhang", "Ruixuan Li"], "title": "Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges", "categories": ["cs.DC"], "comment": "35 pages, 9 figures", "summary": "As large language models (LLMs) evolve, deploying them solely in the cloud or\ncompressing them for edge devices has become inadequate due to concerns about\nlatency, privacy, cost, and personalization. This survey explores a\ncollaborative paradigm in which cloud-based LLMs and edge-deployed small\nlanguage models (SLMs) cooperate across both inference and training. We present\na unified taxonomy of edge-cloud collaboration strategies. For inference, we\ncategorize approaches into task assignment, task division, and mixture-based\ncollaboration at both task and token granularity, encompassing adaptive\nscheduling, resource-aware offloading, speculative decoding, and modular\nrouting. For training, we review distributed adaptation techniques, including\nparameter alignment, pruning, bidirectional distillation, and\nsmall-model-guided optimization. We further summarize datasets, benchmarks, and\ndeployment cases, and highlight privacy-preserving methods and vertical\napplications. This survey provides the first systematic foundation for LLM-SLM\ncollaboration, bridging system and algorithm co-design to enable efficient,\nscalable, and trustworthy edge-cloud intelligence."}
{"id": "2507.16793", "pdf": "https://arxiv.org/pdf/2507.16793", "abs": "https://arxiv.org/abs/2507.16793", "authors": ["Jianqiao Mo", "Alhad Daftardar", "Joey Ah-kiow", "Kaiyue Guo", "Benedikt BÃ¼nz", "Siddharth Garg", "Brandon Reagen"], "title": "MTU: The Multifunction Tree Unit in zkSpeed for Accelerating HyperPlonk", "categories": ["cs.AR"], "comment": null, "summary": "Zero-Knowledge Proofs (ZKPs) are critical for privacy preservation and\nverifiable computation. Many ZKPs rely on kernels such as the SumCheck protocol\nand Merkle Tree commitments, which enable their security properties. These\nkernels exhibit balanced binary tree computational patterns, which enable\nefficient hardware acceleration. Prior work has investigated accelerating these\nkernels as part of an overarching ZKP protocol; however, a focused study of how\nto best exploit the underlying tree pattern for hardware efficiency remains\nlimited. We conduct a systematic evaluation of these tree-based workloads under\ndifferent traversal strategies, analyzing performance on multi-threaded CPUs\nand a hardware accelerator, the Multifunction Tree Unit (MTU). We introduce a\nhardware-friendly Hybrid Traversal for binary tree that improves parallelism\nand scalability while significantly reducing memory traffic on hardware. Our\nresults show that MTU achieves up to 1478$\\times$ speedup over CPU at DDR-level\nbandwidth and that our hybrid traversal outperforms as standalone approach by\nup to 3$\\times$. These findings offer practical guidance for designing\nefficient hardware accelerators for ZKP workloads with binary tree structures."}
{"id": "2507.16407", "pdf": "https://arxiv.org/pdf/2507.16407", "abs": "https://arxiv.org/abs/2507.16407", "authors": ["Shuhan Liu", "Xing Hu", "Kerui Huang", "Xiaohu Yang", "David Lo", "Xin Xia"], "title": "Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, where the natural language prompt plays a crucial role in\nconveying user intent to the model. However, prior studies have shown that LLMs\nare highly sensitive to prompt perturbations. Minor modifications in wording,\nsyntax, or formatting can significantly reduce the functional correctness of\ngenerated code. As perturbations frequently occur in real-world scenarios,\nimproving the robustness of LLMs to prompt perturbations is essential for\nensuring reliable performance in practical code generation. In this paper, we\nintroduce CREME (Code Robustness Enhancement via Model Editing), a novel\napproach that enhances LLM robustness through targeted parameter updates. CREME\nfirst identifies robustness-sensitive layers by comparing hidden states between\nan original prompt and its perturbed variant. Then, it performs lightweight\nparameter editing at the identified layer to reduce performance degradation. We\nevaluate CREME on two widely used code generation benchmarks (HumanEval and\nMBPP) along with their perturbed counterparts. Experimental results show that\nCREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining\nstable performance on clean inputs, with accuracy deviations within 1%. Further\nanalysis reveals that robustness-sensitive layers are primarily concentrated in\nthe middle and deeper layers of the network, and their locations vary across\ndifferent model architectures. These insights provide a valuable foundation for\ndeveloping future robustness-oriented editing strategies."}
{"id": "2507.16117", "pdf": "https://arxiv.org/pdf/2507.16117", "abs": "https://arxiv.org/abs/2507.16117", "authors": ["Eden Wu", "Dishita G Turakhia", "Guande Wu", "Christos Koutras", "Sarah Keegan", "Wenke Liu", "Beata Szeitz", "David Fenyo", "ClÃ¡udio T. Silva", "Juliana Freire"], "title": "BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation", "categories": ["cs.HC"], "comment": "11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track,\n  submission ID 1204)", "summary": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches."}
{"id": "2507.16735", "pdf": "https://arxiv.org/pdf/2507.16735", "abs": "https://arxiv.org/abs/2507.16735", "authors": ["Laura Moradbakhti", "Dorian Peters", "Jennifer K. Quint", "BjÃ¶rn Schuller", "Darren Cook", "Rafael A. Calvo"], "title": "AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET", "K.4.2; J.3"], "comment": "7 Tables, 4 Figures", "summary": "Asthma-related deaths in the UK are the highest in Europe, and only 30% of\npatients access basic care. There is a need for alternative approaches to\nreaching people with asthma in order to provide health education,\nself-management support and bridges to care. Automated conversational agents\n(specifically, mobile chatbots) present opportunities for providing alternative\nand individually tailored access to health education, self-management support\nand risk self-assessment. But would patients engage with a chatbot, and what\nfactors influence engagement? We present results from a patient survey (N=1257)\ndevised by a team of asthma clinicians, patients, and technology developers,\nconducted to identify optimal factors for efficacy, value and engagement for a\nchatbot. Results indicate that most adults with asthma (53%) are interested in\nusing a chatbot and the patients most likely to do so are those who believe\ntheir asthma is more serious and who are less confident about self-management.\nResults also indicate enthusiasm for 24/7 access, personalisation, and for\nWhatsApp as the preferred access method (compared to app, voice assistant, SMS\nor website). Obstacles to uptake include security/privacy concerns and\nskepticism of technological capabilities. We present detailed findings and\nconsolidate these into 7 recommendations for developers for optimising efficacy\nof chatbot-based health support."}
{"id": "2507.16781", "pdf": "https://arxiv.org/pdf/2507.16781", "abs": "https://arxiv.org/abs/2507.16781", "authors": ["Imran Latif", "Muhammad Ali Shafique", "Hayat Ullah", "Alex C. Newkirk", "Xi Yu", "Arslan Munir"], "title": "Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems", "categories": ["cs.DC"], "comment": "11 pages", "summary": "The unprecedented growth in artificial intelligence (AI) workloads, recently\ndominated by large language models (LLMs) and vision-language models (VLMs),\nhas intensified power and cooling demands in data centers. This study\nbenchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics\nprocessing units (GPUs), using liquid and air cooling. Leveraging GPU Burn,\nWeights and Biases, and IPMItool, we collect detailed thermal, power, and\ncomputation data. Results show that the liquid-cooled systems maintain GPU\ntemperatures between 41-50 degrees Celsius, while the air-cooled counterparts\nfluctuate between 54-72 degrees Celsius under load. This thermal stability of\nliquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU\nvs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead,\nand greater system efficiency than the air-cooled counterparts. These findings\nunderscore the energy and sustainability benefits of liquid cooling, offering a\ncompelling path forward for hyperscale data centers s"}
{"id": "2507.16200", "pdf": "https://arxiv.org/pdf/2507.16200", "abs": "https://arxiv.org/abs/2507.16200", "authors": ["Pengwei Jin", "Di Huang", "Chongxiao Li", "Shuyao Cheng", "Yang Zhao", "Xinyao Zheng", "Jiaguo Zhu", "Shuyi Xing", "Bohan Dou", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu"], "title": "RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs", "categories": ["cs.LG", "cs.AR"], "comment": "The benchmark is open-sourced at\n  https://github.com/IPRC-DIP/RealBench", "summary": "The automatic generation of Verilog code using Large Language Models (LLMs)\nhas garnered significant interest in hardware design automation. However,\nexisting benchmarks for evaluating LLMs in Verilog generation fall short in\nreplicating real-world design workflows due to their designs' simplicity,\ninadequate design specifications, and less rigorous verification environments.\nTo address these limitations, we present RealBench, the first benchmark aiming\nat real-world IP-level Verilog generation tasks. RealBench features complex,\nstructured, real-world open-source IP designs, multi-modal and formatted design\nspecifications, and rigorous verification environments, including 100% line\ncoverage testbenches and a formal checker. It supports both module-level and\nsystem-level tasks, enabling comprehensive assessments of LLM capabilities.\nEvaluations on various LLMs and agents reveal that even one of the\nbest-performing LLMs, o1-preview, achieves only a 13.3% pass@1 on module-level\ntasks and 0% on system-level tasks, highlighting the need for stronger Verilog\ngeneration models in the future. The benchmark is open-sourced at\nhttps://github.com/IPRC-DIP/RealBench."}
{"id": "2507.16439", "pdf": "https://arxiv.org/pdf/2507.16439", "abs": "https://arxiv.org/abs/2507.16439", "authors": ["Gunnar Larsen", "Carol Wong", "Anthony Peruma"], "title": "Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code", "categories": ["cs.SE"], "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Research scientists increasingly rely on implementing software to support\ntheir research. While previous research has examined the impact of identifier\nnames on program comprehension in traditional programming environments, limited\nwork has explored this area in scientific software, especially regarding the\nquality of method names in the code. The recent advances in Large Language\nModels (LLMs) present new opportunities for automating code analysis tasks,\nsuch as identifier name appraisals and recommendations. Our study evaluates\nfour popular LLMs on their ability to analyze grammatical patterns and suggest\nimprovements for 496 method names extracted from Python-based Jupyter\nNotebooks. Our findings show that the LLMs are somewhat effective in analyzing\nthese method names and generally follow good naming practices, like starting\nmethod names with verbs. However, their inconsistent handling of\ndomain-specific terminology and only moderate agreement with human annotations\nindicate that automated suggestions require human evaluation. This work\nprovides foundational insights for improving the quality of scientific code\nthrough AI automation."}
{"id": "2507.16207", "pdf": "https://arxiv.org/pdf/2507.16207", "abs": "https://arxiv.org/abs/2507.16207", "authors": ["Katelyn Morrison", "Arpit Mathur", "Aidan Bradshaw", "Tom Wartmann", "Steven Lundi", "Afrooz Zandifar", "Weichang Dai", "Kayhan Batmanghelich", "Motahhare Eslami", "Adam Perer"], "title": "A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "13 pages, 2 figures, accepted to AAAI/ACM AIES 2025", "summary": "As text-to-image generative models rapidly improve, AI researchers are making\nsignificant advances in developing domain-specific models capable of generating\ncomplex medical imagery from text prompts. Despite this, these technical\nadvancements have overlooked whether and how medical professionals would\nbenefit from and use text-to-image generative AI (GenAI) in practice. By\ndeveloping domain-specific GenAI without involving stakeholders, we risk the\npotential of building models that are either not useful or even more harmful\nthan helpful. In this paper, we adopt a human-centered approach to responsible\nmodel development by involving stakeholders in evaluating and reflecting on the\npromises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through\nexploratory model prompting activities, we uncover the perspectives of medical\nstudents, radiology trainees, and radiologists on the role that text-to-CT Scan\nGenAI can play across medical education, training, and practice. This\nhuman-centered approach additionally enabled us to surface technical challenges\nand domain-specific risks of generating synthetic medical images. We conclude\nby reflecting on the implications of medical text-to-image GenAI."}
{"id": "2507.16767", "pdf": "https://arxiv.org/pdf/2507.16767", "abs": "https://arxiv.org/abs/2507.16767", "authors": ["Aris L. Moustakas", "George C. Alexandropoulos"], "title": "Multi-RIS-Empowered Communication Systems: Capacity Analysis and Optimization", "categories": ["cs.IT", "cs.ET", "eess.SP", "math.IT"], "comment": "24 pages, 5 figures, book chapter", "summary": "In this chapter, using statistical physics methods, asymptotic closed-form\nexpressions for the mean and variance of the mutual information for a\nmulti-antenna transmitter-receiver pair in the presence of multiple\nReconfigurable Intelligent Surfaces (RISs) are presented. While nominally valid\nin the large-system limit, it is shown that the derived Gaussian approximation\nfor the mutual information can be quite accurate, even for modest-sized antenna\narrays and metasurfaces. The above results are particularly useful when\nfast-fading conditions are present, which renders channel estimation\nchallenging. The derived analysis indicates that, when the channel close to an\nRIS is correlated, for instance due to small angle spread which is reasonable\nfor wireless systems with increasing carrier frequencies, the communication\nlink benefits significantly from statistical RIS optimization, resulting in\ngains that are surprisingly higher than the nearly uncorrelated case. More\nimportantly, the presented novel asymptotic properties of the correlation\nmatrices of the impinging and outgoing signals at the RISs can be deployed to\noptimize the metasurfaces without brute-force numerical optimization. The\nnumerical investigation demonstrates that, when the desired reflection from any\nof the RISs departs significantly from geometrical optics, the metasurfaces can\nbe optimized to provide robust communication links, without significant need\nfor their optimal placement."}
{"id": "2507.16014", "pdf": "https://arxiv.org/pdf/2507.16014", "abs": "https://arxiv.org/abs/2507.16014", "authors": ["Aayush Rajesh", "Nikhil Karamchandani", "Vinod M. Prabhakaran"], "title": "Byzantine-Resilient Distributed Computation via Task Replication and Local Computations", "categories": ["cs.IT", "cs.DC", "math.IT"], "comment": "Accepted in 2025 IEEE Information Theory Workshop", "summary": "We study a distributed computation problem in the presence of Byzantine\nworkers where a central node wishes to solve a task that is divided into\nindependent sub-tasks, each of which needs to be solved correctly. The\ndistributed computation is achieved by allocating the sub-task computation\nacross workers with replication, as well as solving a small number of sub-tasks\nlocally, which we wish to minimize due to it being expensive. For a general\nbalanced job allocation, we propose a protocol that successfully solves for all\nsub-tasks using an optimal number of local computations under no communication\nconstraints. Closed-form performance results are presented for cyclic\nallocations. Furthermore, we propose a modification to this protocol to improve\ncommunication efficiency without compromising on the amount of local\ncomputation."}
{"id": "2507.16203", "pdf": "https://arxiv.org/pdf/2507.16203", "abs": "https://arxiv.org/abs/2507.16203", "authors": ["Rui Guo", "Avinash Ayalasomayajula", "Henian Li", "Jingbo Zhou", "Sujan Kumar Saha", "Farimah Farahmandi"], "title": "SVAgent: AI Agent for Hardware Security Verification Assertion", "categories": ["cs.CR", "cs.AI", "cs.AR", "cs.LG"], "comment": null, "summary": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment."}
{"id": "2507.16587", "pdf": "https://arxiv.org/pdf/2507.16587", "abs": "https://arxiv.org/abs/2507.16587", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Alejandro Velasco", "Antonio Mastropaolo", "Denys Poshyvanyk", "Gabriele Bavota"], "title": "On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization", "categories": ["cs.SE"], "comment": "Accepted at TSE. IEEE Transactions on Software Engineering", "summary": "Large Language Models have been recently exploited as judges for complex\nnatural language processing tasks, such as Q&A. The basic idea is to delegate\nto an LLM the assessment of the \"quality\" of the output provided by an\nautomated technique for tasks for which: (i) quantitative metrics would only\ntell part of the story, and; (ii) a large-scale human-based evaluation would be\ntoo expensive. LLMs-as-a-judge, if proven effective for a specific task, can\nalso unlock new possibilities for automation, with several LLMs proposing a\nsolution for a given instance of the task and others judging and deciding what\nis the best output to show the user. We study the effectiveness of\nLLMs-as-a-judge for two code-related tasks, namely code generation and code\nsummarization. The rationale for choosing these tasks is two-fold. First,\nquantitative metrics are usually not enough for the assessment of code\nsummarizers/generators. For example, it is well documented that metrics such as\nBLEU are quite weak proxies for the quality of the generated summaries. Second,\neven state-of-the-art techniques still struggle with handling complex instances\nof these tasks, making them good candidates for benefiting from more advanced\nsolutions envisioning collaboration among LLMs. For code generation, we check\nwhether eight LLMs are able to judge the correctness of 1,405 Java methods and\n1,281 Python functions generated by the same LLMs or implemented by humans. For\ncode summarization, we compare the judgment of five LLMs to those provided by\nnine humans for ~1.2k summaries, related to both Java and Python functions. Our\nfindings show that GPT-4-turbo is the best LLM in terms of judging capabilities\nfor both tasks, with \"smaller\" LLMs featuring tens of billions parameters not\nbeing able to cope with judging tasks. However, even the best-performing LLM\nfrequently misjudges the correctness of the code and summary quality."}
{"id": "2507.16258", "pdf": "https://arxiv.org/pdf/2507.16258", "abs": "https://arxiv.org/abs/2507.16258", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Marius Hoggenmueller", "Callum Paker", "Paul Schmitt", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "title": "Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence", "categories": ["cs.HC"], "comment": null, "summary": "Autonomous mobility systems increasingly operate in environments shared with\nanimals, from urban pets to wildlife. However, their design has largely focused\non human interaction, with limited understanding of how non-human species\nperceive, respond to, or are affected by these systems. Motivated by research\nin Animal-Computer Interaction (ACI) and more-than-human design, this study\ninvestigates animal interactions with autonomous mobility through a\nmulti-method approach combining a scoping review (45 articles), online\nethnography (39 YouTube videos and 11 Reddit discussions), and expert\ninterviews (8 participants). Our analysis surfaces five key areas of concern:\nPhysical Impact (e.g., collisions, failures to detect), Behavioural Effects\n(e.g., avoidance, stress), Accessibility Concerns (particularly for service\nanimals), Ethics and Regulations, and Urban Disturbance. We conclude with\ndesign and policy directions aimed at supporting multispecies coexistence in\nthe age of autonomous systems. This work underscores the importance of\nincorporating non-human perspectives to ensure safer, more inclusive futures\nfor all species."}
{"id": "2507.16036", "pdf": "https://arxiv.org/pdf/2507.16036", "abs": "https://arxiv.org/abs/2507.16036", "authors": ["Felix Burt", "Kuan-Cheng Chen", "Kin K. Leung"], "title": "Entanglement-Efficient Compilation of Quantum Circuits over Large-Scale Quantum Networks", "categories": ["quant-ph", "cs.DC"], "comment": "12 pages, 10 figures, to be published in proceedings of IEEE QCE2025", "summary": "Quantum computers face inherent scaling challenges, a fact that necessitates\ninvestigation of distributed quantum computing systems, whereby scaling is\nachieved through interconnection of smaller quantum processing units. However,\nconnecting large numbers of QPUs will eventually result in connectivity\nconstraints at the network level, where the difficulty of entanglement sharing\nincreases with network path lengths. This increases the complexity of the\nquantum circuit partitioning problem, since the cost of generating entanglement\nbetween end nodes varies with network topologies and existing links. We address\nthis challenge using a simple modification to existing partitioning schemes\ndesigned for all-to-all connected networks, that efficiently accounts for both\nof these factors. We investigate the performance in terms of entanglement\nrequirements and optimisation time of various quantum circuits over different\nnetwork topologies, achieving lower entanglement costs in the majority of cases\nthan state-of-the-art methods. We provide techniques for scaling to large-scale\nquantum networks employing both network and problem coarsening. We show that\ncoarsened methods can achieve improved solution quality in most cases with\nsignificantly lower run-times than direct partitioning methods."}
{"id": "2507.16556", "pdf": "https://arxiv.org/pdf/2507.16556", "abs": "https://arxiv.org/abs/2507.16556", "authors": ["Jon GutiÃ©rrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "title": "Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach", "categories": ["cs.CV", "cs.AI", "cs.AR", "cs.LG", "eess.IV"], "comment": null, "summary": "The use of HSI for autonomous navigation is a promising research field aimed\nat improving the accuracy and robustness of detection, tracking, and scene\nunderstanding systems based on vision sensors. Combining advanced computer\nalgorithms, such as DNNs, with small-size snapshot HSI cameras enhances the\nreliability of these systems. HSI overcomes intrinsic limitations of greyscale\nand RGB imaging in depicting physical properties of targets, particularly\nregarding spectral reflectance and metamerism. Despite promising results in\nHSI-based vision developments, safety-critical systems like ADS demand strict\nconstraints on latency, resource consumption, and security, motivating the\nshift of ML workloads to edge platforms. This involves a thorough\nsoftware/hardware co-design scheme to distribute and optimize the tasks\nefficiently among the limited resources of computing platforms. With respect to\ninference, the over-parameterized nature of DNNs poses significant\ncomputational challenges for real-time on-the-edge deployment. In addition, the\nintensive data preprocessing required by HSI, which is frequently overlooked,\nmust be carefully managed in terms of memory arrangement and inter-task\ncommunication to enable an efficient integrated pipeline design on a SoC. This\nwork presents a set of optimization techniques for the practical co-design of a\nDNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at\nADS, including key optimizations such as functional software/hardware task\ndistribution, hardware-aware preprocessing, ML model compression, and a\ncomplete pipelined deployment. Applied compression techniques significantly\nreduce the complexity of the designed DNN to 24.34% of the original operations\nand to 1.02% of the original number of parameters, achieving a 2.86x speed-up\nin the inference task without noticeable degradation of the segmentation\naccuracy."}
{"id": "2507.16661", "pdf": "https://arxiv.org/pdf/2507.16661", "abs": "https://arxiv.org/abs/2507.16661", "authors": ["Tan Bui", "Yan Naing Tun", "Thanh Phuc Nguyen", "Yindu Su", "Ferdian Thung", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Lwin Khin Shar", "Eng Lieh Ouh", "Ting Zhang", "David Lo"], "title": "VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones", "categories": ["cs.SE"], "comment": null, "summary": "Code reuse is common in modern software development, but it can also spread\nvulnerabilities when developers unknowingly copy risky code. The code fragments\nthat preserve the logic of known vulnerabilities are known as vulnerable code\nclones (VCCs). Detecting those VCCs is a critical but challenging task.\nExisting VCC detection tools often rely on syntactic similarity or produce\ncoarse vulnerability predictions without clear explanations, limiting their\npractical utility. In this paper, we propose VulCoCo, a lightweight and\nscalable approach that combines embedding-based retrieval with large language\nmodel (LLM) validation. Starting from a set of known vulnerable functions, we\nretrieve syntactically or semantically similar candidate functions from a large\ncorpus and use an LLM to assess whether the candidates retain the\nvulnerability. Given that there is a lack of reproducible vulnerable code clone\nbenchmarks, we first construct a synthetic benchmark that spans various clone\ntypes.\n  Our experiments on the benchmark show that VulCoCo outperforms prior\nstate-of-the-art methods in terms of Precision@k and mean average precision\n(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world\nprojects by submitting 400 pull requests (PRs) to 284 open-source projects.\nAmong them, 75 PRs were merged, and 15 resulted in newly published CVEs. We\nalso provide insights to inspire future work to further improve the precision\nof vulnerable code clone detection."}
{"id": "2507.16466", "pdf": "https://arxiv.org/pdf/2507.16466", "abs": "https://arxiv.org/abs/2507.16466", "authors": ["Lin Gao", "Leixian Shen", "Yuheng Zhao", "Jiexiang Lan", "Huamin Qu", "Siming Chen"], "title": "SceneLoom: Communicating Data with Scene Context", "categories": ["cs.HC"], "comment": null, "summary": "In data-driven storytelling contexts such as data journalism and data videos,\ndata visualizations are often presented alongside real-world imagery to support\nnarrative context. However, these visualizations and contextual images\ntypically remain separated, limiting their combined narrative expressiveness\nand engagement. Achieving this is challenging due to the need for fine-grained\nalignment and creative ideation. To address this, we present SceneLoom, a\nVision-Language Model (VLM)-powered system that facilitates the coordination of\ndata visualization with real-world imagery based on narrative intents. Through\na formative study, we investigated the design space of coordination\nrelationships between data visualization and real-world scenes from the\nperspectives of visual alignment and semantic coherence. Guided by the derived\ndesign considerations, SceneLoom leverages VLMs to extract visual and semantic\nfeatures from scene images and data visualization, and perform design mapping\nthrough a reasoning process that incorporates spatial organization, shape\nsimilarity, layout consistency, and semantic binding. The system generates a\nset of contextually expressive, image-driven design alternatives that achieve\ncoherent alignments across visual, semantic, and data dimensions. Users can\nexplore these alternatives, select preferred mappings, and further refine the\ndesign through interactive adjustments and animated transitions to support\nexpressive data communication. A user study and an example gallery validate\nSceneLoom's effectiveness in inspiring creative design and facilitating design\nexternalization."}
{"id": "2507.16134", "pdf": "https://arxiv.org/pdf/2507.16134", "abs": "https://arxiv.org/abs/2507.16134", "authors": ["Baofu Han", "Bing Li", "Yining Qi", "Raja Jurdak", "Kaibin Huang", "Chau Yuen"], "title": "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs."}
{"id": "2507.16676", "pdf": "https://arxiv.org/pdf/2507.16676", "abs": "https://arxiv.org/abs/2507.16676", "authors": ["Vasileios Titopoulos", "Kosmas Alexandridis", "Giorgos Dimitrakopoulos"], "title": "Custom Algorithm-based Fault Tolerance for Attention Layers in Transformers", "categories": ["cs.LG", "cs.AR"], "comment": "IEEE International System-on-Chip Conference (IEEE SOCC 2025)", "summary": "Transformers and large language models (LLMs), powered by the attention\nmechanism, have transformed numerous AI applications, driving the need for\nspecialized hardware accelerators. A major challenge in these accelerators is\nefficiently detecting errors caused by random hardware faults. Traditional\nalgorithm-based fault tolerance (ABFT) techniques verify individual matrix\nmultiplications but fall short in handling the full attention mechanism,\nparticularly due to intermediate softmax normalization. This work proposes\nFlash-ABFT, a novel method that computes an online checksum across the entire\nthree-matrix product of query, key and value matrices, of an attention layer,\nincluding the softmax operation, with a single check. This approach\nsignificantly reduces overhead by eliminating redundant checks while\nmaintaining high fault-detection accuracy. Experimental results demonstrate\nthat Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%\nenergy overhead, making it a cost-effective and robust solution for error\ndetection in attention accelerators."}
{"id": "2507.16685", "pdf": "https://arxiv.org/pdf/2507.16685", "abs": "https://arxiv.org/abs/2507.16685", "authors": ["Duong Nguyen", "Manh Tran-Duc", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "title": "VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models", "categories": ["cs.SE"], "comment": null, "summary": "We present VulGuard, an automated tool designed to streamline the extraction,\nprocessing, and analysis of commits from GitHub repositories for Just-In-Time\nvulnerability prediction (JIT-VP) research. VulGuard automatically mines commit\nhistories, extracts fine-grained code changes, commit messages, and software\nengineering metrics, and formats them for downstream analysis. In addition, it\nintegrates several state-of-the-art vulnerability prediction models, allowing\nresearchers to train, evaluate, and compare models with minimal setup. By\nsupporting both repository-scale mining and model-level experimentation within\na unified framework, VulGuard addresses key challenges in reproducibility and\nscalability in software security research. VulGuard can also be easily\nintegrated into the CI/CD pipeline. We demonstrate the effectiveness of the\ntool in two influential open-source projects, FFmpeg and the Linux kernel,\nhighlighting its potential to accelerate real-world JIT-VP research and promote\nstandardized benchmarking. A demo video is available at:\nhttps://youtu.be/j96096-pxbs"}
{"id": "2507.16542", "pdf": "https://arxiv.org/pdf/2507.16542", "abs": "https://arxiv.org/abs/2507.16542", "authors": ["Qiong Wu", "Yan Dong", "Zipeng Zhang", "Ruochen Hu"], "title": "The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces", "categories": ["cs.HC"], "comment": "23 pages, 6 figures, submitted to Virtual Reality (Springer)", "summary": "In exhibition hybrid spaces, scale consistency between real and virtual\nspaces is crucial for user immersion. However, there is currently a lack of\nsystematic research to determine appropriate virtual-to-real mapping ratios.\nThis study developed an immersive interaction system based on Intel 3D Athlete\nTracking body mapping technology. Two experiments investigated the impact of\nvirtual space and virtual avatar scale on immersion. Experiment 1 investigated\n30 participants' preferences for virtual space scale, while Experiment 2 tested\nthe effect of 6 different virtual avatar sizes (25%-150%) on immersion. A\n5-point Likert scale was used to assess immersion, followed by analysis of\nvariance and Tukey HSD post-hoc tests. Experiment 1 showed that participants\npreferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2\nfound that virtual avatar sizes within the 75%-100% range produced optimal\nimmersion (p < 0.05). Immersion decreased significantly when virtual avatar\nsizes deviated from users' actual height (below 50% or above 125%).\nParticipants were more sensitive to size changes in the 25%-75% range, while\nperception was weaker for changes in the 75%-100% range. Virtual environments\nslightly larger than real space (130%) and virtual avatars slightly smaller\nthan users (75%-100%) optimize user immersion. These findings have been applied\nin the Intel Global Trade Center exhibition hall, demonstrating actionable\ninsights for designing hybrid spaces that enhance immersion and coherence."}
{"id": "2507.16269", "pdf": "https://arxiv.org/pdf/2507.16269", "abs": "https://arxiv.org/abs/2507.16269", "authors": ["Sharareh Alipour", "Arash Ahadi", "Kajal Baghestani"], "title": "Improved Wake-Up Time For Euclidean Freeze-Tag Problem", "categories": ["cs.CG", "cs.DC", "cs.RO"], "comment": null, "summary": "The Freeze-Tag Problem (FTP) involves activating a set of initially asleep\nrobots as quickly as possible, starting from a single awake robot. Once\nactivated, a robot can assist in waking up other robots. Each active robot\nmoves at unit speed. The objective is to minimize the makespan, i.e., the time\nrequired to activate the last robot. A key performance measure is the wake-up\nratio, defined as the maximum time needed to activate any number of robots in\nany primary positions. This work focuses on the geometric (Euclidean) version\nof FTP in $\\mathbb{R}^d$ under the $\\ell_p$ norm, where the initial distance\nbetween each asleep robot and the single active robot is at most 1. For\n$(\\mathbb{R}^2, \\ell_2)$, we improve the previous upper bound of 4.62 ([7],\nCCCG 2024) to 4.31. Note that it is known that 3.82 is a lower bound for the\nwake-up ratio. In $\\mathbb{R}^3$, we propose a new strategy that achieves a\nwake-up ratio of 12 for $(\\mathbb{R}^3, \\ell_1)$ and 12.76 for $(\\mathbb{R}^3,\n\\ell_2)$, improving upon the previous bounds of 13 and $13\\sqrt{3}$,\nrespectively, reported in [2]."}
{"id": "2507.16754", "pdf": "https://arxiv.org/pdf/2507.16754", "abs": "https://arxiv.org/abs/2507.16754", "authors": ["Fangjian Lei", "Mariam El Mezouar", "Shayan Noei", "Ying Zou"], "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs"}
{"id": "2507.16562", "pdf": "https://arxiv.org/pdf/2507.16562", "abs": "https://arxiv.org/abs/2507.16562", "authors": ["Megha Quamara", "Viktor Schmuck", "Cristina Iani", "Axel Primavesi", "Alexander Plaum", "Luca Vigano"], "title": "Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)", "categories": ["cs.HC", "cs.AI"], "comment": "26 pages (18 pages main body, 8 pages user consent form), 3 figures,\n  7 tables", "summary": "In this paper, we present the findings of a user study that evaluated the\nsocial acceptance of eXtended Reality (XR) agent technology, focusing on a\nremotely accessible, web-based XR training system developed for journalists.\nThis system involves user interaction with a virtual avatar, enabled by a\nmodular toolkit. The interactions are designed to provide tailored training for\njournalists in digital-remote settings, especially for sensitive or dangerous\nscenarios, without requiring specialized end-user equipment like headsets. Our\nresearch adapts and extends the Almere model, representing social acceptance\nthrough existing attributes such as perceived ease of use and perceived\nusefulness, along with added ones like dependability and security in the\nuser-agent interaction. The XR agent was tested through a controlled experiment\nin a real-world setting, with data collected on users' perceptions. Our\nfindings, based on quantitative and qualitative measurements involving\nquestionnaires, contribute to the understanding of user perceptions and\nacceptance of XR agent solutions within a specific social context, while also\nidentifying areas for the improvement of XR systems."}
{"id": "2507.16274", "pdf": "https://arxiv.org/pdf/2507.16274", "abs": "https://arxiv.org/abs/2507.16274", "authors": ["Zixiao Huang", "Junhao Hu", "Hao Lin", "Chunyang Zhu", "Yueran Tang", "Quanlu Zhang", "Zhen Guo", "Zhenhua Li", "Shengen Yan", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "title": "Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "comment": null, "summary": "The rapid scaling of large language models (LLMs) has significantly increased\nGPU memory pressure, which is further aggravated by training optimization\ntechniques such as virtual pipeline and recomputation that disrupt tensor\nlifespans and introduce considerable memory fragmentation. Default GPU memory\nallocators of popular deep learning frameworks like PyTorch use online\nstrategies without knowledge of tensor lifespans, which can waste up to 43\\% of\nmemory and cause out-of-memory errors, rendering optimization techniques\nineffective or even unusable.\n  To address this, we introduce STWeaver, a GPU memory allocator for deep\nlearning frameworks that reduces fragmentation by exploiting the spatial and\ntemporal regularity in memory allocation behaviors of training workloads.\nSTWeaver introduces a novel paradigm that combines offline planning with online\nallocation. The offline planning leverages spatio-temporal regularities to\ngenerate a near-optimal allocation plan, while the online allocation handles\ncomplex and dynamic models such as Mixture-of-Experts (MoE). Built as a\npluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by\n79.2\\% (up to 100\\%) across both dense and sparse models, with negligible\noverhead. This enables more efficient, high-throughput training configurations\nand improves performance by up to 32.5\\%."}
{"id": "2507.16808", "pdf": "https://arxiv.org/pdf/2507.16808", "abs": "https://arxiv.org/abs/2507.16808", "authors": ["Zhihao Xu", "Bixin Li", "Lulu Wang"], "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis", "categories": ["cs.SE", "cs.AI", "68N19, 68T05", "B.6.3; D.3.4; I.2.2; I.2.6"], "comment": "13pages with 9 pictures and 2 tables", "summary": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization."}
{"id": "2507.16563", "pdf": "https://arxiv.org/pdf/2507.16563", "abs": "https://arxiv.org/abs/2507.16563", "authors": ["Abdulhaq Adetunji Salako", "Hannes Hagen", "Christian Tominski"], "title": "Animated Transition between Node-Link and Parallel Coordinates Visualizations", "categories": ["cs.HC"], "comment": null, "summary": "Multi-faceted data visualization typically involves several dedicated views.\nTo create a comprehensive understanding of the data, users have to mentally\nintegrate the information from the different views. This integration is\nhindered by context switches between views and usually requires interactive\nmethods such as brushing and linking. Animated transitions have also been shown\nto be able to mediate context switches and improve understanding. Yet, most\nexisting animated transitions consider only basic views showing the same data\nfacet. In this work, we study how the gap between node-link diagrams, showing\ngraph structure, and parallel coordinates plots, showing multivariate\nattributes, can be narrowed via smooth animated transitions. Based on two\ndesign goals (traceability and swiftness), we outline a partial design space\nincluding several design options. These inform the implementation of two\nalternative transition variants: a basic variant with plain interpolation and\nan advanced variant that uses our design space and accepted animation\ntechniques, including staging and staggering. In a preliminary study, we asked\nseven participants for qualitative feedback. We found that the swiftness of the\nbasic variant is preferred, while the traceability of data items is better with\nthe slower advanced variant."}
{"id": "2507.16594", "pdf": "https://arxiv.org/pdf/2507.16594", "abs": "https://arxiv.org/abs/2507.16594", "authors": ["Zied Jenhani", "Mounir Bensalem", "Jasenka DizdareviÄ", "Admela Jukan"], "title": "An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes", "categories": ["cs.NI", "cs.AI", "cs.DC"], "comment": "This paper is uploaded here for research community, thus it is for\n  non-commercial purposes", "summary": "Running deep learning inference directly on ultra-low-power edge/IoT nodes\nhas been limited by the tight memory and compute budgets of microcontrollers.\nSplit learning (SL) addresses this limitation in which it executes part of the\ninference process on the sensor and off-loads the remainder to a companion\ndevice. In the context of constrained devices and the related impact of\nlow-power, over-the-air transport protocols, the performance of split learning\nremains largely unexplored. TO the best of our knowledge, this paper presents\nthe first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,\ndesigned to benchmark the over-the-air performance of split learning TinyML in\nedge/IoT environments. We benchmark the performance of a MobileNetV2 image\nrecognition model, which is quantized to 8-bit integers, partitioned, and\ndelivered to the nodes via over-the-air updates. The intermediate activations\nare exchanged through different wireless communication methods: ESP-NOW, BLE,\nand traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on\nidentical hardware. Measurements show that splitting the model after\nblock_16_project_BN layer generates a 5.66 kB tensor that traverses the link in\n3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.\nESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery\nlife further but increases latency beyond 10s."}
{"id": "2507.15984", "pdf": "https://arxiv.org/pdf/2507.15984", "abs": "https://arxiv.org/abs/2507.15984", "authors": ["I Putu Arya Dharmaadi", "Mohannad Alhanahnah", "Van-Thuan Pham", "Fadi Mohsen", "Fatih Turkmen"], "title": "BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications", "categories": ["cs.CR", "cs.SE"], "comment": "Under peer-review", "summary": "Broken Access Control (BAC) remains one of the most critical and widespread\nvulnerabilities in web applications, allowing attackers to access unauthorized\nresources or perform privileged actions. Despite its severity, BAC is\nunderexplored in automated testing due to key challenges: the lack of reliable\noracles and the difficulty of generating semantically valid attack requests. We\nintroduce BACFuzz, the first gray-box fuzzing framework specifically designed\nto uncover BAC vulnerabilities, including Broken Object-Level Authorization\n(BOLA) and Broken Function-Level Authorization (BFLA) in PHP-based web\napplications. BACFuzz combines LLM-guided parameter selection with runtime\nfeedback and SQL-based oracle checking to detect silent authorization flaws. It\nemploys lightweight instrumentation to capture runtime information that guides\ntest generation, and analyzes backend SQL queries to verify whether\nunauthorized inputs flow into protected operations. Evaluated on 20 real-world\nweb applications, including 15 CVE cases and 2 known benchmarks, BACFuzz\ndetects 16 of 17 known issues and uncovers 26 previously unknown BAC\nvulnerabilities with low false positive rates. All identified issues have been\nresponsibly disclosed, and artifacts will be publicly released."}
{"id": "2507.16586", "pdf": "https://arxiv.org/pdf/2507.16586", "abs": "https://arxiv.org/abs/2507.16586", "authors": ["Choro Ulan Uulu", "Mikhail Kulyabin", "Layan Etaiwi", "Nuno Miguel Martins Pacheco", "Jan Joosten", "Kerstin RÃ¶se", "Filippos Petridis", "Jan Bosch", "Helena HolmstrÃ¶m Olsson"], "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Computer-Aided Engineering (CAE) enables simulation experts to optimize\ncomplex models, but faces challenges in user experience (UX) that limit\nefficiency and accessibility. While artificial intelligence (AI) has\ndemonstrated potential to enhance CAE processes, research integrating these\nfields with a focus on UX remains fragmented. This paper presents a multivocal\nliterature review (MLR) examining how AI enhances UX in CAE software across\nboth academic research and industry implementations. Our analysis reveals\nsignificant gaps between academic explorations and industry applications, with\ncompanies actively implementing LLMs, adaptive UIs, and recommender systems\nwhile academic research focuses primarily on technical capabilities without UX\nvalidation. Key findings demonstrate opportunities in AI-powered guidance,\nadaptive interfaces, and workflow automation that remain underexplored in\ncurrent research. By mapping the intersection of these domains, this study\nprovides a foundation for future work to address the identified research gaps\nand advance the integration of AI to improve CAE user experience."}
{"id": "2507.16051", "pdf": "https://arxiv.org/pdf/2507.16051", "abs": "https://arxiv.org/abs/2507.16051", "authors": ["Juan Altmayer Pizzorno", "Emery D. Berger"], "title": "RightTyper: Effective and Efficient Type Annotation for Python", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Python type annotations bring the benefits of static type checking to the\nlanguage. However, manually writing annotations can be time-consuming and\ntedious. The result is that most real-world Python code remains largely\nuntyped. Past approaches to annotating types in Python code fall short in a\nnumber of ways. Static approaches struggle with dynamic features and infer\noverly broad types. AI-based methods are inherently unsound and can miss rare\nor user-defined types. Dynamic methods can impose extreme runtime overheads,\ndegrading performance by up to 270x, abort execution as they exhaust resources,\nand even infer incorrect types that lead to runtime errors. Crucially, all\nprior work assumes implicitly that the code to be annotated is already correct.\nThis assumption is generally unwarranted, especially for large codebases that\nhave been untyped.\n  This paper presents RightTyper, a novel approach for Python that overcomes\nthese disadvantages. RightTyper not only generates precise type annotations\nbased on actual program behavior, improving recall in type checking relative to\nprior approaches. It also turns type checking into anomaly detection, allowing\nthe type checker to identify corner cases that the programmer can audit for\nunintended behavior. RightTyper is also fast and space-efficient, imposing just\n30% performance overhead on average. RightTyper achieves these characteristics\nby a principled yet pervasive use of sampling--guided by self-profiling--along\nwith statistical filtering and careful resolution and aggregation of type\ninformation."}
{"id": "2507.16735", "pdf": "https://arxiv.org/pdf/2507.16735", "abs": "https://arxiv.org/abs/2507.16735", "authors": ["Laura Moradbakhti", "Dorian Peters", "Jennifer K. Quint", "BjÃ¶rn Schuller", "Darren Cook", "Rafael A. Calvo"], "title": "AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET", "K.4.2; J.3"], "comment": "7 Tables, 4 Figures", "summary": "Asthma-related deaths in the UK are the highest in Europe, and only 30% of\npatients access basic care. There is a need for alternative approaches to\nreaching people with asthma in order to provide health education,\nself-management support and bridges to care. Automated conversational agents\n(specifically, mobile chatbots) present opportunities for providing alternative\nand individually tailored access to health education, self-management support\nand risk self-assessment. But would patients engage with a chatbot, and what\nfactors influence engagement? We present results from a patient survey (N=1257)\ndevised by a team of asthma clinicians, patients, and technology developers,\nconducted to identify optimal factors for efficacy, value and engagement for a\nchatbot. Results indicate that most adults with asthma (53%) are interested in\nusing a chatbot and the patients most likely to do so are those who believe\ntheir asthma is more serious and who are less confident about self-management.\nResults also indicate enthusiasm for 24/7 access, personalisation, and for\nWhatsApp as the preferred access method (compared to app, voice assistant, SMS\nor website). Obstacles to uptake include security/privacy concerns and\nskepticism of technological capabilities. We present detailed findings and\nconsolidate these into 7 recommendations for developers for optimising efficacy\nof chatbot-based health support."}
{"id": "2507.16229", "pdf": "https://arxiv.org/pdf/2507.16229", "abs": "https://arxiv.org/abs/2507.16229", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.SE"], "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions."}
{"id": "2507.15885", "pdf": "https://arxiv.org/pdf/2507.15885", "abs": "https://arxiv.org/abs/2507.15885", "authors": ["Pierluca D'Oro", "Caley Drooff", "Joy Chen", "Joseph Tighe"], "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models have paved the way to powerful and flexible AI agents,\nassisting humans by increasingly integrating into their daily life. This\nflexibility, potential, and growing adoption demands a holistic and\ncross-disciplinary approach to developing, monitoring and discussing the\ncapabilities required for agent-driven user experiences. However, current\nguidance on human-centered AI agent development is scattered: UX heuristics\nfocus on interface behaviors, engineering taxonomies describe internal\npipelines, and ethics checklists address high-level governance. There is no\nconcise, user-facing vocabulary that tells teams what an agent should\nfundamentally be able to do. We introduce ADEPTS, a capability framework\ndefining a set of core user-facing capabilities to provide unified guidance\naround the development of AI agents. ADEPTS is based on six principles for\nhuman-centered agent design, that express the minimal, user-facing capabilities\nan AI agent should demonstrate to be understandable, controllable and\ntrustworthy in everyday use. ADEPTS complements existing frameworks and\ntaxonomies; differently from them, it sits at the interface between technical\nand experience development. By presenting ADEPTS, we aim to condense complex\nAI-UX requirements into a compact framework that is actionable guidance for AI\nresearchers, designers, engineers, and policy reviewers alike. We believe\nADEPTS has the potential of accelerating the improvement of user-relevant agent\ncapabilities, of easing the design of experiences that take advantage of those\ncapabilities, and of providing a shared language to track and discuss progress\naround the development of AI agents."}
{"id": "2507.16395", "pdf": "https://arxiv.org/pdf/2507.16395", "abs": "https://arxiv.org/abs/2507.16395", "authors": ["Bo Hou", "Xin Tan", "Kai Zheng", "Fang Liu", "Yinghao Zhu", "Li Zhang"], "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks."}
{"id": "2507.15997", "pdf": "https://arxiv.org/pdf/2507.15997", "abs": "https://arxiv.org/abs/2507.15997", "authors": ["Onyinye Dibia", "Mengyi Lu", "Prianka Bhattacharjee", "Joseph P. Near", "Yuanyuan Feng"], "title": "\"We Need a Standard\": Toward an Expert-Informed Privacy Label for Differential Privacy", "categories": ["cs.CR", "cs.HC", "68-XX 68-XX 68-XX"], "comment": "13 pages, 5 figures", "summary": "The increasing adoption of differential privacy (DP) leads to public-facing\nDP deployments by both government agencies and companies. However, real-world\nDP deployments often do not fully disclose their privacy guarantees, which vary\ngreatly between deployments. Failure to disclose certain DP parameters can lead\nto misunderstandings about the strength of the privacy guarantee, undermining\nthe trust in DP. In this work, we seek to inform future standards for\ncommunicating the privacy guarantees of DP deployments. Based on\nsemi-structured interviews with 12 DP experts, we identify important DP\nparameters necessary to comprehensively communicate DP guarantees, and describe\nwhy and how they should be disclosed. Based on expert recommendations, we\ndesign an initial privacy label for DP to comprehensively communicate privacy\nguarantees in a standardized format."}
{"id": "2507.16478", "pdf": "https://arxiv.org/pdf/2507.16478", "abs": "https://arxiv.org/abs/2507.16478", "authors": ["Shreya Saxena", "Siva Prasad", "Zishan Ahmad", "Vishal Vaddina"], "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration."}
{"id": "2507.16130", "pdf": "https://arxiv.org/pdf/2507.16130", "abs": "https://arxiv.org/abs/2507.16130", "authors": ["Mahika Phutane", "Aditya Vashistha"], "title": "Disability Across Cultures: A Human-Centered Audit of Ableism in Western and Indic LLMs", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "People with disabilities (PwD) experience disproportionately high levels of\ndiscrimination and hate online, particularly in India, where entrenched stigma\nand limited resources intensify these challenges. Large language models (LLMs)\nare increasingly used to identify and mitigate online hate, yet most research\non online ableism focuses on Western audiences with Western AI models. Are\nthese models adequately equipped to recognize ableist harm in non-Western\nplaces like India? Do localized, Indic language models perform better? To\ninvestigate, we adopted and translated a publicly available ableist speech\ndataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,\nGemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,\nAiravata)--to score and explain ableism. In parallel, we recruited 175 PwD from\nboth the U.S. and India to perform the same task, revealing stark differences\nbetween groups. Western LLMs consistently overestimated ableist harm, while\nIndic LLMs underestimated it. Even more concerning, all LLMs were more tolerant\nof ableism when it was expressed in Hindi and asserted Western framings of\nableist harm. In contrast, Indian PwD interpreted harm through intention,\nrelationality, and resilience--emphasizing a desire to inform and educate\nperpetrators. This work provides groundwork for global, inclusive standards of\nableism, demonstrating the need to center local disability experiences in the\ndesign and evaluation of AI systems."}
{"id": "2507.16586", "pdf": "https://arxiv.org/pdf/2507.16586", "abs": "https://arxiv.org/abs/2507.16586", "authors": ["Choro Ulan Uulu", "Mikhail Kulyabin", "Layan Etaiwi", "Nuno Miguel Martins Pacheco", "Jan Joosten", "Kerstin RÃ¶se", "Filippos Petridis", "Jan Bosch", "Helena HolmstrÃ¶m Olsson"], "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Computer-Aided Engineering (CAE) enables simulation experts to optimize\ncomplex models, but faces challenges in user experience (UX) that limit\nefficiency and accessibility. While artificial intelligence (AI) has\ndemonstrated potential to enhance CAE processes, research integrating these\nfields with a focus on UX remains fragmented. This paper presents a multivocal\nliterature review (MLR) examining how AI enhances UX in CAE software across\nboth academic research and industry implementations. Our analysis reveals\nsignificant gaps between academic explorations and industry applications, with\ncompanies actively implementing LLMs, adaptive UIs, and recommender systems\nwhile academic research focuses primarily on technical capabilities without UX\nvalidation. Key findings demonstrate opportunities in AI-powered guidance,\nadaptive interfaces, and workflow automation that remain underexplored in\ncurrent research. By mapping the intersection of these domains, this study\nprovides a foundation for future work to address the identified research gaps\nand advance the integration of AI to improve CAE user experience."}
{"id": "2507.16184", "pdf": "https://arxiv.org/pdf/2507.16184", "abs": "https://arxiv.org/abs/2507.16184", "authors": ["Myung Ho Kim"], "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)", "categories": ["cs.AI", "cs.HC"], "comment": "21 pages", "summary": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification."}
{"id": "2507.16229", "pdf": "https://arxiv.org/pdf/2507.16229", "abs": "https://arxiv.org/abs/2507.16229", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.SE"], "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions."}
{"id": "2507.16247", "pdf": "https://arxiv.org/pdf/2507.16247", "abs": "https://arxiv.org/abs/2507.16247", "authors": ["Tanusree Sharma", "Yihao Zhou", "Visar Berisha"], "title": "PRAC3 (Privacy, Reputation, Accountability, Consent, Credit, Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Early large-scale audio datasets, such as LibriSpeech, were built with\nhundreds of individual contributors whose voices were instrumental in the\ndevelopment of speech technologies, including audiobooks and voice assistants.\nYet, a decade later, these same contributions have exposed voice actors to a\nrange of risks. While existing ethical frameworks emphasize Consent, Credit,\nand Compensation (C3), they do not adequately address the emergent risks\ninvolving vocal identities that are increasingly decoupled from context,\nauthorship, and control. Drawing on qualitative interviews with 20 professional\nvoice actors, this paper reveals how the synthetic replication of voice without\nenforceable constraints exposes individuals to a range of threats. Beyond\nreputational harm, such as re-purposing voice data in erotic content, offensive\npolitical messaging, and meme culture, we document concerns about\naccountability breakdowns when their voice is leveraged to clone voices that\nare deployed in high-stakes scenarios such as financial fraud, misinformation\ncampaigns, or impersonation scams. In such cases, actors face social and legal\nfallout without recourse, while very few of them have a legal representative or\nunion protection. To make sense of these shifting dynamics, we introduce the\nPRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,\nAccountability, Consent, Credit, and Compensation as interdependent pillars of\ndata used in the synthetic voice economy. This framework captures how privacy\nrisks are amplified through non-consensual training, how reputational harm\narises from decontextualized deployment, and how accountability can be\nreimagined AI Data ecosystems. We argue that voice, as both a biometric\nidentifier and creative labor, demands governance models that restore creator\nagency, ensure traceability, and establish enforceable boundaries for ethical\nreuse."}
{"id": "2507.16298", "pdf": "https://arxiv.org/pdf/2507.16298", "abs": "https://arxiv.org/abs/2507.16298", "authors": ["Gautam Kishore Shahi", "Scot A. Hale"], "title": "WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "WhatsApp tiplines, first launched in 2019 to combat misinformation, enable\nusers to interact with fact-checkers to verify misleading content. This study\nanalyzes 580 unique claims (tips) from 451 users, covering both high-resource\nlanguages (English, Hindi) and a low-resource language (Telugu) during the 2021\nIndian assembly elections using a mixed-method approach. We categorize the\nclaims into three categories, election, COVID-19, and others, and observe\nvariations across languages. We compare content similarity through frequent\nword analysis and clustering of neural sentence embeddings. We also investigate\nuser overlap across languages and fact-checking organizations. We measure the\naverage time required to debunk claims and inform tipline users. Results reveal\nsimilarities in claims across languages, with some users submitting tips in\nmultiple languages to the same fact-checkers. Fact-checkers generally require a\ncouple of days to debunk a new claim and share the results with users. Notably,\nno user submits claims to multiple fact-checking organizations, indicating that\neach organization maintains a unique audience. We provide practical\nrecommendations for using tiplines during elections with ethical consideration\nof users' information."}
{"id": "2507.16398", "pdf": "https://arxiv.org/pdf/2507.16398", "abs": "https://arxiv.org/abs/2507.16398", "authors": ["Lavinia Hriscu", "Alberto Sanfeliu", "Anais Garrell"], "title": "AI or Human? Understanding Perceptions of Embodied Robots with LLMs", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The pursuit of artificial intelligence has long been associated to the the\nchallenge of effectively measuring intelligence. Even if the Turing Test was\nintroduced as a means of assessing a system intelligence, its relevance and\napplication within the field of human-robot interaction remain largely\nunderexplored. This study investigates the perception of intelligence in\nembodied robots by performing a Turing Test within a robotic platform. A total\nof 34 participants were tasked with distinguishing between AI- and\nhuman-operated robots while engaging in two interactive tasks: an information\nretrieval and a package handover. These tasks assessed the robot perception and\nnavigation abilities under both static and dynamic conditions. Results indicate\nthat participants were unable to reliably differentiate between AI- and\nhuman-controlled robots beyond chance levels. Furthermore, analysis of\nparticipant responses reveals key factors influencing the perception of\nartificial versus human intelligence in embodied robotic systems. These\nfindings provide insights into the design of future interactive robots and\ncontribute to the ongoing discourse on intelligence assessment in AI-driven\nsystems."}
{"id": "2507.16515", "pdf": "https://arxiv.org/pdf/2507.16515", "abs": "https://arxiv.org/abs/2507.16515", "authors": ["Siqi Liu", "Guangrong Dai", "Dechao Li"], "title": "Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness", "categories": ["cs.CL", "cs.HC"], "comment": "11 pages, 5 figures, 2 tables. To be published in the Proceedings of\n  the 20th Machine Translation Summit (MT Summit 2025; Geneva, Switzerland)", "summary": "This preliminary study investigates the usefulness of sentence-level Quality\nEstimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),\nfocusing on its impact on post-editing speed and student translators'\nperceptions. It also explores the interaction effects between QE and MT\nquality, as well as between QE and translation expertise. The findings reveal\nthat QE significantly reduces post-editing time. The examined interaction\neffects were not significant, suggesting that QE consistently improves MTPE\nefficiency across medium- and high-quality MT outputs and among student\ntranslators with varying levels of expertise. In addition to indicating\npotentially problematic segments, QE serves multiple functions in MTPE, such as\nvalidating translators' evaluations of MT quality and enabling them to\ndouble-check translation outputs. However, interview data suggest that\ninaccurate QE may hinder post-editing processes. This research provides new\ninsights into the strengths and limitations of QE, facilitating its more\neffective integration into MTPE workflows to enhance translators' productivity."}
{"id": "2507.16704", "pdf": "https://arxiv.org/pdf/2507.16704", "abs": "https://arxiv.org/abs/2507.16704", "authors": ["Viktor Muryn", "Marta Sumyk", "Mariya Hirna", "Sofiya Garkot", "Maksym Shamrai"], "title": "Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Desktop accessibility metadata enables AI agents to interpret screens and\nsupports users who depend on tools like screen readers. Yet, many applications\nremain largely inaccessible due to incomplete or missing metadata provided by\ndevelopers - our investigation shows that only 33% of applications on macOS\noffer full accessibility support. While recent work on structured screen\nrepresentation has primarily addressed specific challenges, such as UI element\ndetection or captioning, none has attempted to capture the full complexity of\ndesktop interfaces by replicating their entire hierarchical structure. To\nbridge this gap, we introduce Screen2AX, the first framework to automatically\ncreate real-time, tree-structured accessibility metadata from a single\nscreenshot. Our method uses vision-language and object detection models to\ndetect, describe, and organize UI elements hierarchically, mirroring macOS's\nsystem-level accessibility structure. To tackle the limited availability of\ndata for macOS desktop applications, we compiled and publicly released three\ndatasets encompassing 112 macOS applications, each annotated for UI element\ndetection, grouping, and hierarchical accessibility metadata alongside\ncorresponding screenshots. Screen2AX accurately infers hierarchy trees,\nachieving a 77% F1 score in reconstructing a complete accessibility tree.\nCrucially, these hierarchy trees improve the ability of autonomous agents to\ninterpret and interact with complex desktop interfaces. We introduce\nScreen2AX-Task, a benchmark specifically designed for evaluating autonomous\nagent task execution in macOS desktop environments. Using this benchmark, we\ndemonstrate that Screen2AX delivers a 2.2x performance improvement over native\naccessibility representations and surpasses the state-of-the-art OmniParser V2\nsystem on the ScreenSpot benchmark."}
