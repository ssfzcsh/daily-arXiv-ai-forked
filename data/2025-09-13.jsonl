{"id": "2509.08843", "pdf": "https://arxiv.org/pdf/2509.08843", "abs": "https://arxiv.org/abs/2509.08843", "authors": ["Sidney Shapiro"], "title": "Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research", "categories": ["cs.SE"], "comment": null, "summary": "Pattern-based file access is a fundamental but often under-documented aspect\nof computational research. The Python glob module provides a simple yet\npowerful way to search, filter, and ingest files using wildcard patterns,\nenabling scalable workflows across disciplines. This paper introduces glob as a\nversatile tool for data science, business analytics, and artificial\nintelligence applications. We demonstrate use cases including large-scale data\ningestion, organizational data analysis, AI dataset construction, and\nreproducible research practices. Through concrete Python examples with widely\nused libraries such as pandas,scikit-learn, and matplotlib, we show how glob\nfacilitates efficient file traversal and integration with analytical pipelines.\nBy situating glob within the broader context of reproducible research and data\nengineering, we highlight its role as a methodological building block. Our goal\nis to provide researchers and practitioners with a concise reference that\nbridges foundational concepts and applied practice, making glob a default\ncitation for file pattern matching in Python-based research workflows."}
{"id": "2509.08857", "pdf": "https://arxiv.org/pdf/2509.08857", "abs": "https://arxiv.org/abs/2509.08857", "authors": ["Marcelino Garcia", "Renato Garcia", "Arthur Parizotto", "Andre Mendes", "Pedro Valle", "Ricardo Vilela", "Renato Balancieri", "Williamson Silva"], "title": "A Systematic Mapping Study on Chatbots in Programming Education", "categories": ["cs.SE", "cs.HC"], "comment": "18 pages, 1 figure, 3 tables", "summary": "Educational chatbots have gained prominence as support tools for teaching\nprogramming, particularly in introductory learning contexts. This paper\npresents a Systematic Mapping Study (SMS) that investigated how such agents\nhave been developed and applied in programming education. From an initial set\nof 3,216 publications, 54 studies were selected and analyzed based on five\nresearch subquestions, addressing chatbot types, programming languages used,\neducational content covered, interaction models, and application contexts. The\nresults reveal a predominance of chatbots designed for Python instruction,\nfocusing on fundamental programming concepts, and employing a wide variety of\npedagogical approaches and technological architectures. In addition to\nidentifying trends and gaps in the literature, this study provides insights to\ninform the development of new educational tools for programming instruction."}
{"id": "2509.08863", "pdf": "https://arxiv.org/pdf/2509.08863", "abs": "https://arxiv.org/abs/2509.08863", "authors": ["Qianqian Luo", "Liuchang Xu", "Qingming Lin", "Sensen Wu", "Ruichen Mao", "Chao Wang", "Hailin Feng", "Bo Huang", "Zhenhong Du"], "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding.However,without expertise in GIS,they continue to encounter\nlimitations.To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture.This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques:Function Calling and Code\nGeneration.The architecture consists of three components-task parsing,agent\ncollaboration,and result integration-aimed at enhancing both the performance\nand scalability of GIS automation.The Planner agent interprets natural language\ntasks into structured GeoJSON commands.Then,specialized Worker agents\ncollaborate according to assigned roles to perform spatial data processing and\nanalysis,either by invoking predefined function APIs or by dynamically\ngenerating and executing Python-based spatial analysis code.Finally,the system\nintegrates the outputs from multiple execution rounds into\nreusable,standards-compliant GeoJSON files.To systematically evaluate the\nperformance of the two approaches,we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model.Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%,while the Code Generation-based agent\nreached 97.14%,both significantly outperforming the best-performing\ngeneral-purpose model (48.57%).Further analysis reveals that the Code\nGeneration provides greater flexibility,whereas the Function Calling approach\noffers more stable execution.This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods,offering new perspectives\nfor improving GeoAI system performance."}
{"id": "2509.08865", "pdf": "https://arxiv.org/pdf/2509.08865", "abs": "https://arxiv.org/abs/2509.08865", "authors": ["Guangyu Zhang", "Xixuan Wang", "Shiyu Sun", "Peiyan Xiao", "Kun Sun", "Yanhai Xiong"], "title": "TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis", "categories": ["cs.SE"], "comment": null, "summary": "Sophisticated evasion tactics in malicious Android applications, combined\nwith their intricate behavioral semantics, enable attackers to conceal\nmalicious logic within legitimate functions, underscoring the critical need for\nrobust and in-depth analysis frameworks. However, traditional analysis\ntechniques often fail to recover deeply hidden behaviors or provide\nhuman-readable justifications for their decisions. Inspired by advances in\nlarge language models (LLMs), we introduce TraceRAG, a retrieval-augmented\ngeneration (RAG) framework that bridges natural language queries and Java code\nto deliver explainable malware detection and analysis. First, TraceRAG\ngenerates summaries of method-level code snippets, which are indexed in a\nvector database. At query time, behavior-focused questions retrieve the most\nsemantically relevant snippets for deeper inspection. Finally, based on the\nmulti-turn analysis results, TraceRAG produces human-readable reports that\npresent the identified malicious behaviors and their corresponding code\nimplementations. Experimental results demonstrate that our method achieves 96\\%\nmalware detection accuracy and 83.81\\% behavior identification accuracy based\non updated VirusTotal (VT) scans and manual verification. Furthermore, expert\nevaluation confirms the practical utility of the reports generated by TraceRAG."}
{"id": "2509.09019", "pdf": "https://arxiv.org/pdf/2509.09019", "abs": "https://arxiv.org/abs/2509.09019", "authors": ["Mohit Tekriwal", "John Sarracino"], "title": "Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs", "categories": ["cs.PL"], "comment": null, "summary": "Scientific computing programs often undergo aggressive compiler optimization\nto achieve high performance and efficient resource utilization. While\nperformance is critical, we also need to ensure that these optimizations are\ncorrect. In this paper, we focus on a specific class of optimizations,\nfloating-point optimizations, notably due to fast math, at the LLVM IR level.\nWe present a preliminary work, which leverages the Verified LLVM framework in\nthe Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)\noptimization for a basic block implementing the arithmetic expression $a * b +\nc$ . We then propose ways to extend this preliminary results by adding more\nprogram features and fast math floating-point optimizations."}
{"id": "2509.08855", "pdf": "https://arxiv.org/pdf/2509.08855", "abs": "https://arxiv.org/abs/2509.08855", "authors": ["Mahmoud Shaqfa"], "title": "Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition", "categories": ["cs.GR", "cs.CG"], "comment": null, "summary": "Harmonic decomposition of surfaces, such as spherical and spheroidal\nharmonics, is used to analyze morphology, reconstruct, and generate surface\ninclusions of particulate microstructures. However, obtaining high-quality\nmeshes of engineering microstructures using these approaches remains an open\nquestion. In harmonic approaches, we usually reconstruct surfaces by evaluating\nthe harmonic bases on equidistantly sampled simplicial complexes of the base\ndomains (e.g., triangular spheroids and disks). However, this traditional\nsampling does not account for local changes in the Jacobian of the basis\nfunctions, resulting in nonuniform discretization after reconstruction or\ngeneration. As it impacts the accuracy and time step, high-quality\ndiscretization of microstructures is crucial for efficient numerical\nsimulations (e.g., finite element and discrete element methods). To circumvent\nthis issue, we propose an efficient hierarchical diffusion-based approach for\nresampling the surface-i.e., performing a reparameterization-to yield an\nequalized mesh triangulation. Analogous to heat problems, we use nonlinear\ndiffusion to resample the curvilinear coordinates of the analysis domain,\nthereby enlarging small triangles at the expense of large triangles on\nsurfaces. We tested isotropic and anisotropic diffusion schemes on the recent\nspheroidal and hemispheroidal harmonics methods. The results show a substantial\nimprovement in the quality metrics for surface triangulation. Unlike\ntraditional surface reconstruction and meshing techniques, this approach\npreserves surface morphology, along with the areas and volumes of surfaces. We\ndiscuss the results and the associated computational costs for large 2D and 3D\nmicrostructures, such as digital twins of concrete and stone masonry, and their\nfuture applications."}
{"id": "2509.09439", "pdf": "https://arxiv.org/pdf/2509.09439", "abs": "https://arxiv.org/abs/2509.09439", "authors": ["John Alistair Kressel", "Hugo Lefeuvre", "Pierre Olivier"], "title": "μFork: Supporting POSIX fork Within a Single-Address-Space OS", "categories": ["cs.OS"], "comment": "Accepted to appear at SOSP 2025", "summary": "Single-address-space operating systems have well-known lightweightness\nbenefits that result from their central design idea: the kernel and\napplications share a unique address space. This model makes these operating\nsystems (OSes) incompatible by design with a large class of software:\nmultiprocess POSIX applications. Indeed, the semantics of the primitive used to\ncreate POSIX processes, fork, are inextricably tied to the existence of\nmultiple address spaces.\n  Prior approaches addressing this issue trade off lightweightness,\ncompatibility and/or isolation. We propose {\\mu}Fork, a single-address-space\noperating system design supporting POSIX fork on modern hardware without\ncompromising on any of these key objectives. {\\mu}Fork emulates POSIX processes\n({\\mu}processes) and achieves fork by creating for the child a copy of the\nparent {\\mu}process' memory at a different location within a single address\nspace. This approach presents two challenges: relocating the child's absolute\nmemory references (pointers), as well as providing user/kernel and\n{\\mu}processes isolation without impacting lightweightness. We address them\nusing CHERI. We implement {\\mu}Fork and evaluate it upon three real-world\nuse-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS\nworker warm-up. {\\mu}Fork outperforms previous work and traditional monolithic\nOSes on key lightweightness metrics by an order of magnitude, e.g. it can offer\na fork-bound FaaS function throughput 24% higher than that of a monolithic OS,\nand can fork a {\\mu}process in 54{\\mu}s, 3.7x faster than a traditional fork."}
{"id": "2509.09218", "pdf": "https://arxiv.org/pdf/2509.09218", "abs": "https://arxiv.org/abs/2509.09218", "authors": ["Bartosz Bednarczyk", "Emanuel Kieroński"], "title": "Guarded Fragments Meet Dynamic Logic: The Story of Regular Guards (Extended Version)", "categories": ["cs.LO"], "comment": "This is an extended version of our paper that will appear at KR 2025.\n  The current appendix has not yet been revised; an updated version will be\n  released in the near future", "summary": "We study the Guarded Fragment with Regular Guards (RGF), which combines the\nexpressive power of the Guarded Fragment (GF) with Propositional Dynamic Logic\nwith Intersection and Converse (ICPDL). Our logic generalizes, in a uniform\nway, many previously-studied extensions of GF, including (conjunctions of)\ntransitive or equivalence guards, transitive or equivalence closure and more.\nWe prove 2EXPTIME-completeness of the satisfiability problem for RGF, showing\nthat RGF is not harder than ICPDL or GF. Shifting to the query entailment\nproblem, we provide undecidability results that significantly strengthen and\nsolidify earlier results along those lines. We conclude by identifying, in a\nnatural sense, the maximal EXPSPACE-complete fragment of RGF."}
{"id": "2509.08915", "pdf": "https://arxiv.org/pdf/2509.08915", "abs": "https://arxiv.org/abs/2509.08915", "authors": ["Duke Lin", "Michael Paskett", "Ying Yang"], "title": "A Contextual Bandits Approach for Personalization of Hand Gesture Recognition", "categories": ["cs.HC"], "comment": null, "summary": "In human-computer interaction applications like hand gesture recognition,\nsupervised learning models are often trained on a large population of users to\nachieve high task accuracy. However, due to individual variability in sensor\nsignals and user behavior, static models may not provide optimal performance\nfor all users. Personalizing pretrained models via calibration--collecting\nlabeled data from each user--can improve performance but introduces user\nfriction and struggles with limited data. To overcome these issues, we propose\na calibrationless longitudinal personalization method: a contextual multi-arm\nbandit (MAB) algorithm combined with a pretrained neural network for gesture\nrecognition. This reinforcement-learning-style approach enables personalization\nusing binary reward signals, either user-provided or inferred by the system.\n  We validated this method in a user study. Participants wore a surface\nelectromyography (sEMG) device and played multiple rounds of a 2-D navigation\ngame using six hand gestures. In the session, they completed a baseline round\nand then a round with our algorithm; in the second session, they played another\nround with our algorithm. Our approach led to a significant reduction in users'\naverage false negative rate by 0.113 from the initial to the final round, with\nfurther decreases between sessions. Average precision also trended upward (by\n0.139) from the start to end of a round, continuing in the next session.\nNotably, some users who could not complete the game with the baseline model\nsucceeded with our contextual MAB model. In summary, our"}
{"id": "2509.09096", "pdf": "https://arxiv.org/pdf/2509.09096", "abs": "https://arxiv.org/abs/2509.09096", "authors": ["Daniel R Korn", "Patrick Golden", "Aaron Odell", "Katherina Cortes", "Shilpa Sundar", "Kevin Schaper", "Sarah Gehrke", "Corey Cox", "Harry Caufield", "Justin Reese", "Evan Morris", "Christopher J Mungall", "Melissa Haendel"], "title": "Koza and Koza-Hub for born-interoperable knowledge graph generation using KGX", "categories": ["cs.DB"], "comment": "9 pages, 1 figure, 1 table", "summary": "Knowledge graph construction has become an essential domain for the future of\nbiomedical research. But current approaches demand a high amount of redundant\nlabor. These redundancies are the result of the lack of data standards and\n\"knowledge-graph ready\" data from sources. Using the KGX standard, we aim to\nsolve these issues. Herein we introduce Koza and the Koza-Hub, a Python\nsoftware package which streamlines ingesting raw biomedical information into\nthe KGX format, and an associated set of conversion processes for thirty gold\nstandard biomedical data sources. Our approach is to turn knowledge graph\ningests into a set of primitive operations, provide configuration through YAML\nfiles, and enforce compliance with the chosen data schema."}
{"id": "2509.09178", "pdf": "https://arxiv.org/pdf/2509.09178", "abs": "https://arxiv.org/abs/2509.09178", "authors": ["Ayan Biswas", "Jimmy Jin"], "title": "Implementation of a 8-bit Wallace Tree Multiplier", "categories": ["cs.AR", "cs.SY", "eess.SY"], "comment": null, "summary": "Wallace tree multipliers are a parallel digital multiplier architecture\ndesigned to minimize the worst-case time complexity of the circuit depth\nrelative to the input size [1]. In particular, it seeks to perform long\nmultiplication in the binary sense, reducing as many partial products per stage\nas possible through full and half adders circuits, achieving O(log(n)) where n\n= bit length of input. This paper provides an overview of the design, progress\nand methodology in the final project of ECE 55900, consisting of the schematic\nand layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in\nCadence Virtuoso, as well as any design attempts prior to the final product.\nThis also includes our endeavors in designing the final MAC (Multiply\nAccumulate) unit with undefined targets, which we chose to implement as a 16\nbit combinational multiply-add."}
{"id": "2509.09420", "pdf": "https://arxiv.org/pdf/2509.09420", "abs": "https://arxiv.org/abs/2509.09420", "authors": ["Haochen Huang", "Shuzhang Zhong", "Zhe Zhang", "Shuangchen Li", "Dimin Niu", "Hongzhong Zheng", "Runsheng Wang", "Meng Li"], "title": "HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing", "categories": ["cs.PF"], "comment": "9 pages, 15 figures, International Conference on Computer-Aided\n  Design (ICCAD) 2025", "summary": "Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures\nachieve superior model performance with reduced computation costs, but at the\ncost of high memory capacity and bandwidth requirements. Near-Memory Processing\n(NMP) accelerators that stack memory directly on the compute through hybrid\nbonding have demonstrated high bandwidth with high energy efficiency, becoming\na promising architecture for MoE models. However, as NMP accelerators comprise\ndistributed memory and computation, how to map the MoE computation directly\ndetermines the LLM inference efficiency. Existing parallel mapping strategies,\nincluding Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from\neither high communication costs or unbalanced computation utilization, leading\nto inferior efficiency. The dynamic routing mechanism of MoE LLMs further\naggravates the efficiency challenges. Therefore, in this paper, we propose\nHD-MoE to automatically optimize the MoE parallel computation across an NMP\naccelerator. HD-MoE features an offline automatic hybrid parallel mapping\nalgorithm and an online dynamic scheduling strategy to reduce the communication\ncosts while maximizing the computation utilization. With extensive experimental\nresults, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to\n1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid\nTP-EP with Compute-Balanced parallelism strategies."}
{"id": "2509.09081", "pdf": "https://arxiv.org/pdf/2509.09081", "abs": "https://arxiv.org/abs/2509.09081", "authors": ["Diwen Xue", "Armin Huremagic", "Wayne Wang", "Ram Sundara Raman", "Roya Ensafi"], "title": "Fingerprinting Deep Packet Inspection Devices by Their Ambiguities", "categories": ["cs.NI", "cs.CR"], "comment": "In: Proceedings of the 2025 ACM SIGSAC Conference on Computer and\n  Communications Security, 2025", "summary": "Users around the world face escalating network interference such as\ncensorship, throttling, and interception, largely driven by the commoditization\nand growing availability of Deep Packet Inspection (DPI) devices. Once reserved\nfor a few well-resourced nation-state actors, the ability to interfere with\ntraffic at scale is now within reach of nearly any network operator. Despite\nthis proliferation, our understanding of DPIs and their deployments on the\nInternet remains limited -- being network intermediary leaves DPI unresponsive\nto conventional host-based scanning tools, and DPI vendors actively obscuring\ntheir products further complicates measurement efforts.\n  In this work, we present a remote measurement framework, dMAP (DPI Mapper),\nthat derives behavioral fingerprints for DPIs to differentiate and cluster\nthese otherwise indistinguishable middleboxes at scale, as a first step toward\nactive reconnaissance of DPIs on the Internet. Our key insight is that parsing\nand interpreting traffic as network intermediaries inherently involves\nambiguities -- from under-specified protocol behaviors to differing RFC\ninterpretations -- forcing DPI vendors into independent implementation choices\nthat create measurable variance among DPIs. Based on differential fuzzing, dMAP\nsystematically discovers, selects, and deploys specialized probes that\ntranslate DPI internal parsing behaviors into externally observable\nfingerprints. Applying dMAP to DPI deployments globally, we demonstrate its\npractical feasibility, showing that even a modest set of 20-40 discriminative\nprobes reliably differentiates a wide range of DPI implementations, including\nmajor nation-state censorship infrastructures and commercial DPI products. We\ndiscuss how our fingerprinting methodology generalizes beyond censorship to\nother forms of targeted interference."}
{"id": "2509.08969", "pdf": "https://arxiv.org/pdf/2509.08969", "abs": "https://arxiv.org/abs/2509.08969", "authors": ["Nima Karimian Kakolaki"], "title": "A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems", "categories": ["cs.DC", "cs.DB"], "comment": null, "summary": "Distributed systems require robust, scalable identifier schemes to ensure\ndata uniqueness and efficient indexing across multiple nodes. This paper\npresents a comprehensive analysis of the evolution of distributed identifiers,\ncomparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We\ncombine mathematical calculation of collision probabilities with empirical\nexperiments measuring generation speed and network transmission overhead in a\nsimulated distributed environment. Results demonstrate that ULIDs significantly\noutperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing\ngeneration speed by 97.32%. statistical analysis further shows ULIDs offer a\n98.42% lower collision risk compared to UUIDv7, while maintaining negligible\ncollision probabilities even at high generation rates. These findings highlight\nULIDs as an optimal choice for high-performance distributed systems, providing\nefficient, time-ordered, and lexicographically sortable identifiers suitable\nfor scalable applications. All source code, datasets, and analysis scripts\nutilized in this research are publicly available in our dedicated repository at\nhttps://github.com/nimakarimiank/uids-comparison. This repository contains\ncomprehensive documentation of the experimental setup, including configuration\nfiles for the distributed environment, producer and consumer implementations,\nand message broker integration. Additionally, it provides the data scripts and\ndatasets. Researchers and practitioners are encouraged to explore the\nrepository for full reproducibility of the experiments and to facilitate\nfurther investigation or extension of the presented work."}
{"id": "2509.08892", "pdf": "https://arxiv.org/pdf/2509.08892", "abs": "https://arxiv.org/abs/2509.08892", "authors": ["Enar de Dios Rodríguez", "Philipp Haslinger", "Johannes Kofler", "Richard Kueng", "Benjamin Orthner", "Alexander Ploier", "Martin Ringbauer", "Clemens Wenger"], "title": "The Sound of Entanglement", "categories": ["quant-ph", "cs.ET", "cs.MM", "cs.SD"], "comment": "13 pages, 12 figures", "summary": "The advent of quantum physics has revolutionized our understanding of the\nuniverse, replacing the deterministic framework of classical physics with a\nparadigm dominated by intrinsic randomness and quantum correlations. This shift\nhas not only enabled groundbreaking technologies, such as quantum sensors,\nnetworks and computers, but has also unlocked entirely new possibilities for\nartistic expressions. In this paper, we explore the intersection of quantum\nmechanics and art, focusing on the use of quantum entanglement and inherent\nrandomness as creative tools. Specifically, we present The Sound of\nEntanglement, a live musical performance driven by real-time measurements of\nentangled photons in a Bell test. By integrating the measured quantum\ncorrelations as a central compositional element and synchronizing live visuals\nwith experimental data, the performance offers a unique and unrepeatable\naudiovisual experience that relies on quantum correlations which cannot be\nproduced by any classical device. Through this fusion of science and art, we\naim to provide a deeper appreciation of quantum phenomena while expanding the\nboundaries of creative expression."}
{"id": "2509.08867", "pdf": "https://arxiv.org/pdf/2509.08867", "abs": "https://arxiv.org/abs/2509.08867", "authors": ["K. Pronk", "Q. Zhao"], "title": "Benchmarking Energy Efficiency of Large Language Models Using vLLM", "categories": ["cs.SE", "cs.AI", "68T01", "I.2.7"], "comment": "6 pages, 6 figures", "summary": "The prevalence of Large Language Models (LLMs) is having an growing impact on\nthe climate due to the substantial energy required for their deployment and\nuse. To create awareness for developers who are implementing LLMs in their\nproducts, there is a strong need to collect more information about the energy\nefficiency of LLMs. While existing research has evaluated the energy efficiency\nof various models, these benchmarks often fall short of representing realistic\nproduction scenarios. In this paper, we introduce the LLM Efficiency Benchmark,\ndesigned to simulate real-world usage conditions. Our benchmark utilizes vLLM,\na high-throughput, production-ready LLM serving backend that optimizes model\nperformance and efficiency. We examine how factors such as model size,\narchitecture, and concurrent request volume affect inference energy efficiency.\nOur findings demonstrate that it is possible to create energy efficiency\nbenchmarks that better reflect practical deployment conditions, providing\nvaluable insights for developers aiming to build more sustainable AI systems."}
{"id": "2509.09059", "pdf": "https://arxiv.org/pdf/2509.09059", "abs": "https://arxiv.org/abs/2509.09059", "authors": ["Paulette Koronkevich", "William J. Bowman"], "title": "Dependent-Type-Preserving Memory Allocation", "categories": ["cs.PL"], "comment": "Submitted and received second place at the Student Research\n  Competition at Principles of Programming Languages 2022", "summary": "Dependently typed programming languages such as Coq, Agda, Idris, and F*,\nallow programmers to write detailed specifications of their programs and prove\ntheir programs meet these specifications. However, these specifications can be\nviolated during compilation since they are erased after type checking. External\nprograms linked with the compiled program can violate the specifications of the\noriginal program and change the behavior of the compiled program -- even when\ncompiled with a verified compiler. For example, since Coq does not allow\nexplicitly allocating memory, a programmer might link their Coq program with a\nC program that can allocate memory. Even if the Coq program is compiled with a\nverified compiler, the external C program can still violate the memory-safe\nspecification of the Coq program by providing an uninitialized pointer to\nmemory. This error could be ruled out by type checking in a language expressive\nenough to indicate whether memory is initialized versus uninitialized. Linking\nwith a program with an uninitialized pointer could be considered ill-typed, and\nour linking process could prevent linking with ill-typed programs. To\nfacilitate type checking during linking, we can use type-preserving\ncompilation, which preserves the types through the compilation process. In this\nongoing work, we develop a typed intermediate language that supports dependent\nmemory allocation, as well as a dependent-type-preserving compiler pass for\nmemory allocation."}
{"id": "2509.08947", "pdf": "https://arxiv.org/pdf/2509.08947", "abs": "https://arxiv.org/abs/2509.08947", "authors": ["Yancheng Cai", "Robert Wanat", "Rafal Mantiuk"], "title": "CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH Asia 2025", "summary": "Accurate measurement of images produced by electronic displays is critical\nfor the evaluation of both traditional and computational displays. Traditional\ndisplay measurement methods based on sparse radiometric sampling and fitting a\nmodel are inadequate for capturing spatially varying display artifacts, as they\nfail to capture high-frequency and pixel-level distortions. While cameras offer\nsufficient spatial resolution, they introduce optical, sampling, and\nphotometric distortions. Furthermore, the physical measurement must be combined\nwith a model of a visual system to assess whether the distortions are going to\nbe visible. To enable perceptual assessment of displays, we propose a\ncombination of a camera-based reconstruction pipeline with a visual difference\npredictor, which account for both the inaccuracy of camera measurements and\nvisual difference prediction. The reconstruction pipeline combines HDR image\nstacking, MTF inversion, vignetting correction, geometric undistortion,\nhomography transformation, and color correction, enabling cameras to function\nas precise display measurement instruments. By incorporating a Visual\nDifference Predictor (VDP), our system models the visibility of various stimuli\nunder different viewing conditions for the human visual system. We validate the\nproposed CameraVDP framework through three applications: defective pixel\ndetection, color fringing awareness, and display non-uniformity evaluation. Our\nuncertainty analysis framework enables the estimation of the theoretical upper\nbound for defect pixel detection performance and provides confidence intervals\nfor VDP quality scores."}
{"id": "2509.09525", "pdf": "https://arxiv.org/pdf/2509.09525", "abs": "https://arxiv.org/abs/2509.09525", "authors": ["Jialiang Huang", "Teng Ma", "Zheng Liu", "Sixing Lin", "Kang Chen", "Jinlei Jiang", "Xia Liao", "Yingdi Shan", "Yongwei Wu", "Ning Zhang", "Mengting Lu", "Tao Ma", "Haifeng Gong", "Mingxing Zhang"], "title": "TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes", "categories": ["cs.DC", "cs.OS"], "comment": "38 pages", "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."}
{"id": "2509.09655", "pdf": "https://arxiv.org/pdf/2509.09655", "abs": "https://arxiv.org/abs/2509.09655", "authors": ["Sanjay Basu", "Sadiq Y. Patel", "Parth Sheth", "Bhairavi Muralidharan", "Namrata Elamaran", "Aakriti Kinra", "Rajaie Batniji"], "title": "Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management", "categories": ["cs.LG", "cs.AI", "cs.LO", "stat.AP"], "comment": "12 pages, 5 figures, 3 tables", "summary": "We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning\n(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds\nto reduce harm while equalizing a chosen fairness target (coverage or harm)\nacross protected subgroups. Using de-identified longitudinal trajectories from\na Medicaid population health management program, we evaluate FG-FARL against\nbehavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global\nconformal safety baseline). We report off-policy value estimates with bootstrap\n95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL\nachieves comparable value to baselines while improving fairness metrics,\ndemonstrating a practical path to safer and more equitable decision support."}
{"id": "2509.08953", "pdf": "https://arxiv.org/pdf/2509.08953", "abs": "https://arxiv.org/abs/2509.08953", "authors": ["Astrid van den Brandt", "Sehi L'Yi", "Huyen N. Nguyen", "Anna Vilanova", "Nils Gehlenborg"], "title": "Characterizing Multimodal Interaction in Visualization Authoring Tools", "categories": ["cs.HC"], "comment": "5 pages, 2 figures", "summary": "Multimodal interaction has been increasingly considered in designing\nvisualization authoring tools. However, multimodal interaction has a broad\nmeaning in visualization authoring, according to our literature review.\nAlthough some previous studies compare different authoring tools, a\ncomprehensive overview of the diverse characteristics of multimodal interaction\nin visualization authoring tools is still missing. This paper seeks to offer a\nsystematic perspective on how multimodal interaction is integrated within\nvisualization authoring tools. Such an overview can enhance understanding of\ncurrent practices, highlight distinguishing features among tools, and help\nidentify future research directions, guiding designers in developing more\naccessible and effective authoring systems. We review 20 visualization\nauthoring tools that incorporate multimodal interaction and characterize how\nmultimodal interaction is applied in these tools. Based on the review results,\nwe discuss design implications and future directions."}
{"id": "2509.09440", "pdf": "https://arxiv.org/pdf/2509.09440", "abs": "https://arxiv.org/abs/2509.09440", "authors": ["Henrik Kirchmann", "Stephan A. Fahrenkrog-Petersen", "Xixi Lu", "Matthias Weidlich"], "title": "Let's Simply Count: Quantifying Distributional Similarity Between Activities in Event Data", "categories": ["cs.DB"], "comment": null, "summary": "To obtain insights from event data, advanced process mining methods assess\nthe similarity of activities to incorporate their semantic relations into the\nanalysis. Here, distributional similarity that captures similarity from\nactivity co-occurrences is commonly employed. However, existing work for\ndistributional similarity in process mining adopt neural network-based\napproaches as developed for natural language processing, e.g., word2vec and\nautoencoders. While these approaches have been shown to be effective, their\ndownsides are high computational costs and limited interpretability of the\nlearned representations.\n  In this work, we argue for simplicity in the modeling of distributional\nsimilarity of activities. We introduce count-based embeddings that avoid a\ncomplex training process and offer a direct interpretable representation. To\nunderpin our call for simple embeddings, we contribute a comprehensive\nbenchmarking framework, which includes means to assess the intrinsic quality of\nembeddings, their performance in downstream applications, and their\ncomputational efficiency. In experiments that compare against the state of the\nart, we demonstrate that count-based embeddings provide a highly effective and\nefficient basis for distributional similarity between activities in event data."}
{"id": "2509.09505", "pdf": "https://arxiv.org/pdf/2509.09505", "abs": "https://arxiv.org/abs/2509.09505", "authors": ["Haoran Wu", "Can Xiao", "Jiayi Nie", "Xuan Guo", "Binglei Lou", "Jeffrey T. H. Wong", "Zhiwen Mo", "Cheng Zhang", "Przemyslaw Forys", "Wayne Luk", "Hongxiang Fan", "Jianyi Cheng", "Timothy M. Jones", "Rika Antonova", "Robert Mullins", "Aaron Zhao"], "title": "Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference", "categories": ["cs.AR"], "comment": null, "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced."}
{"id": "2509.08986", "pdf": "https://arxiv.org/pdf/2509.08986", "abs": "https://arxiv.org/abs/2509.08986", "authors": ["Junbo Jacob Lian"], "title": "Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for Fixed-Time Comparisons", "categories": ["cs.NE", "cs.PF", "stat.CO"], "comment": null, "summary": "Numerous purportedly improved metaheuristics claim superior performance based\non equivalent function evaluations (FEs), yet often conceal additional\ncomputational burdens in more intensive iterations, preprocessing stages, or\nhyperparameter tuning. This paper posits that wall-clock time, rather than\nsolely FEs, should serve as the principal budgetary constraint for equitable\ncomparisons. We formalize a fixed-time, restart-fair benchmarking protocol\nwherein each algorithm is allotted an identical wall-clock time budget per\nproblem instance, permitting unrestricted utilization of restarts, early\ntermination criteria, and internal adaptive mechanisms. We advocate for the\nadoption of anytime performance curves, expected running time (ERT) metrics,\nand performance profiles that employ time as the cost measure, all aimed at\npredefined targets. Furthermore, we introduce a concise, reproducible checklist\nto standardize reporting practices and mitigate undisclosed computational\noverheads. This approach fosters more credible and practically relevant\nevaluations of metaheuristic algorithms."}
{"id": "2509.09193", "pdf": "https://arxiv.org/pdf/2509.09193", "abs": "https://arxiv.org/abs/2509.09193", "authors": ["Haoxiang Luo", "Yu Yan", "Yanhui Bian", "Wenjiao Feng", "Ruichen Zhang", "Yinqiu Liu", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Abbas Jamalipour", "Shiwen Mao"], "title": "AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives", "categories": ["cs.NI"], "comment": null, "summary": "Artificial Intelligence (AI) techniques play a pivotal role in optimizing\nwireless communication networks. However, traditional deep learning approaches\noften act as closed boxes, lacking the structured reasoning abilities needed to\ntackle complex, multi-step decision problems. This survey provides a\ncomprehensive review and outlook of reasoning-enabled AI in wireless\ncommunication networks, with a focus on Large Language Models (LLMs) and other\nadvanced reasoning paradigms. In particular, LLM-based agents can combine\nreasoning with long-term planning, memory, tool utilization, and autonomous\ncross-layer control to dynamically optimize network operations with minimal\nhuman intervention. We begin by outlining the evolution of intelligent wireless\nnetworking and the limitations of conventional AI methods. We then introduce\nemerging AI reasoning techniques. Furthermore, we establish a classification\nsystem applicable to wireless network tasks. We also present a layer-by-layer\nexamination for AI reasoning, covering the physical, data link, network,\ntransport, and application layers. For each part, we identify key challenges\nand illustrate how AI reasoning methods can improve AI-based wireless\ncommunication performance. Finally, we discuss key research directions for AI\nreasoning toward future wireless communication networks. By combining insights\nfrom both communications and AI, this survey aims to chart a path for\nintegrating reasoning techniques into the next-generation wireless networks."}
{"id": "2509.09058", "pdf": "https://arxiv.org/pdf/2509.09058", "abs": "https://arxiv.org/abs/2509.09058", "authors": ["Ajay Kumar", "Praveen Rao", "Peter Sanders"], "title": "Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines", "categories": ["cs.DC"], "comment": "To appear in 14th International Workshop on Parallel and AI-based\n  Bioinformatics and Biomedicine (ParBio), Philadelphia, 2025", "summary": "Variant calling is the first step in analyzing a human genome and aims to\ndetect variants in an individual's genome compared to a reference genome. Due\nto the computationally-intensive nature of variant calling, genomic data are\nincreasingly processed in cloud environments as large amounts of compute and\nstorage resources can be acquired with the pay-as-you-go pricing model. In this\npaper, we address the problem of efficiently executing a variant calling\npipeline for a workload of human genomes on graphics processing unit\n(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach\nfor optimizing the workload execution to minimize the total execution time. Our\napproach encompasses two key techniques: The first technique employs ML to\npredict the execution times of different stages in a variant calling pipeline\nbased on the characteristics of a genome sequence. Using the predicted times,\nthe second technique generates optimal execution plans for the machines by\ndrawing inspiration from the flexible job shop scheduling problem. The plans\nare executed via careful synchronization across different machines. We\nevaluated our approach on a workload of publicly available genome sequences\nusing a testbed with different types of GPU hardware. We observed that our\napproach was effective in predicting the execution times of variant calling\npipeline stages using ML on features such as sequence size, read quality,\npercentage of duplicate reads, and average read length. In addition, our\napproach achieved 2X speedup (on an average) over a greedy approach that also\nused ML for predicting the execution times on the tested workload of sequences.\nFinally, our approach achieved 1.6X speedup (on an average) over a dynamic\napproach that executed the workload based on availability of resources without\nusing any ML-based time predictions."}
{"id": "2509.08976", "pdf": "https://arxiv.org/pdf/2509.08976", "abs": "https://arxiv.org/abs/2509.08976", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "title": "Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance", "categories": ["cs.GT", "cs.ET", "cs.SY", "eess.SY"], "comment": null, "summary": "Cyber warfare has become a central element of modern conflict, especially\nwithin multi-domain operations. As both a distinct and critical domain, cyber\nwarfare requires integrating defensive and offensive technologies into coherent\nstrategies. While prior research has emphasized isolated tactics or fragmented\ntechnologies, a holistic understanding is essential for effective resource\ndeployment and risk mitigation. Game theory offers a unifying framework for\nthis purpose. It not only models attacker-defender interactions but also\nprovides quantitative tools for equilibrium analysis, risk assessment, and\nstrategic reasoning. Integrated with modern AI techniques, game-theoretic\nmodels enable the design and optimization of strategies across multiple levels\nof cyber warfare, from policy and strategy to operations, tactics, and\ntechnical implementations. These models capture the paradoxical logic of\nconflict, where more resources do not always translate into greater advantage,\nand where nonlinear dynamics govern outcomes. To illustrate the approach, this\nchapter examines RedCyber, a synthetic cyber conflict, demonstrating how\ngame-theoretic methods capture the interdependencies of cyber operations. The\nchapter concludes with directions for future research on resilience,\ncros-echelon planning, and the evolving role of AI in cyber warfare."}
{"id": "2509.09072", "pdf": "https://arxiv.org/pdf/2509.09072", "abs": "https://arxiv.org/abs/2509.09072", "authors": ["Ahmed Adnan", "Mushfiqur Rahman", "Saad Sakib Noor", "Kazi Sakib"], "title": "CLARA: A Developer's Companion for Code Comprehension and Analysis", "categories": ["cs.SE"], "comment": "In proceedings at the 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "Code comprehension and analysis of open-source project codebases is a task\nfrequently performed by developers and researchers. However, existing tools\nthat practitioners use for assistance with such tasks often require prior\nproject setup, lack context-awareness, and involve significant manual effort.\nTo address this, we present CLARA, a browser extension that utilizes a\nstate-of-the-art inference model to assist developers and researchers in: (i)\ncomprehending code files and code fragments, (ii) code refactoring, and (iii)\ncode quality attribute detection. We qualitatively evaluated CLARA's inference\nmodel using existing datasets and methodology, and performed a comprehensive\nuser study with 10 developers and academic researchers to assess its usability\nand usefulness. The results show that CLARA is useful, accurate, and practical\nin code comprehension and analysis tasks. CLARA is an open-source tool\navailable at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing\nthe full capabilities of CLARA can be found at\nhttps://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH."}
{"id": "2509.09143", "pdf": "https://arxiv.org/pdf/2509.09143", "abs": "https://arxiv.org/abs/2509.09143", "authors": ["Yuiko Uchida", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Accepted by the ICCV 2025 UniLight Workshop", "summary": "This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric\nfor 3D scenes that explicitly focuses on \"objects,\" which are fundamental units\nof human visual perception. Existing metrics assess overall image quality,\nleading to discrepancies with human perception. Inspired by neuropsychological\ninsights, we hypothesize that human recognition of 3D scenes fundamentally\ninvolves attention to individual objects. OSIM enables object-centric\nevaluations by leveraging an object detection model and its feature\nrepresentations to quantify the \"objectness\" of each object in the scene. Our\nuser study demonstrates that OSIM aligns more closely with human perception\ncompared to existing metrics. We also analyze the characteristics of OSIM using\nvarious approaches. Moreover, we re-evaluate recent 3D reconstruction and\ngeneration models under a standardized experimental setup to clarify\nadvancements in this field. The code is available at\nhttps://github.com/Objectness-Similarity/OSIM."}
{"id": "2509.09657", "pdf": "https://arxiv.org/pdf/2509.09657", "abs": "https://arxiv.org/abs/2509.09657", "authors": ["Steef Hegeman", "Jan Martens", "Alfons Laarman"], "title": "Uniformity within Parameterized Circuit Classes", "categories": ["cs.CC", "cs.LO"], "comment": null, "summary": "We study uniformity conditions for parameterized Boolean circuit families.\nUniformity conditions require that the infinitely many circuits in a circuit\nfamily are in some sense easy to construct from one shared description. For\nshallow circuit families, logtime-uniformity is often desired but quite\ntechnical to prove. Despite that, proving it is often left as an exercise for\nthe reader -- even for recently introduced classes in parameterized circuit\ncomplexity, where uniformity conditions have not yet been explicitly studied.\nWe formally define parameterized versions of linear-uniformity,\nlogtime-uniformity, and FO-uniformity, and prove that these result in\nequivalent complexity classes when imposed on $\\text{para-}\\textsf{AC}^0$ and\n$\\text{para-}\\textsf{AC}^{0\\uparrow}$. Overall, we provide a convenient way to\nverify uniformity for shallow parameterized circuit classes, and thereby\nsubstantiate claims of uniformity in the literature."}
{"id": "2509.08997", "pdf": "https://arxiv.org/pdf/2509.08997", "abs": "https://arxiv.org/abs/2509.08997", "authors": ["Yaman Yu", "Yiren Liu", "Jacky Zhang", "Yun Huang", "Yang Wang"], "title": "YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models", "categories": ["cs.HC"], "comment": "15 pages, 4 figures", "summary": "Large Language Models (LLMs) are increasingly used by teenagers and young\nadults in everyday life, ranging from emotional support and creative expression\nto educational assistance. However, their unique vulnerabilities and risk\nprofiles remain under-examined in current safety benchmarks and moderation\nsystems, leaving this population disproportionately exposed to harm. In this\nwork, we present Youth AI Risk (YAIR), the first benchmark dataset designed to\nevaluate and improve the safety of youth LLM interactions. YAIR consists of\n12,449 annotated conversation snippets spanning 78 fine grained risk types,\ngrounded in a taxonomy of youth specific harms such as grooming, boundary\nviolation, identity confusion, and emotional overreliance. We systematically\nevaluate widely adopted moderation models on YAIR and find that existing\napproaches substantially underperform in detecting youth centered risks, often\nmissing contextually subtle yet developmentally harmful interactions. To\naddress these gaps, we introduce YouthSafe, a real-time risk detection model\noptimized for youth GenAI contexts. YouthSafe significantly outperforms prior\nsystems across multiple metrics on risk detection and classification, offering\na concrete step toward safer and more developmentally appropriate AI\ninteractions for young users."}
{"id": "2509.09482", "pdf": "https://arxiv.org/pdf/2509.09482", "abs": "https://arxiv.org/abs/2509.09482", "authors": ["Agapi Rissaki", "Ilias Fountalis", "Wolfgang Gatterbauer", "Benny Kimelfeld"], "title": "Database Views as Explanations for Relational Deep Learning", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "In recent years, there has been significant progress in the development of\ndeep learning models over relational databases, including architectures based\non heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph\ntransformers. In effect, such architectures state how the database records and\nlinks (e.g., foreign-key references) translate into a large, complex numerical\nexpression, involving numerous learnable parameters. This complexity makes it\nhard to explain, in human-understandable terms, how a model uses the available\ndata to arrive at a given prediction. We present a novel framework for\nexplaining machine-learning models over relational databases, where\nexplanations are view definitions that highlight focused parts of the database\nthat mostly contribute to the model's prediction. We establish such global\nabductive explanations by adapting the classic notion of determinacy by Nash,\nSegoufin, and Vianu (2010). In addition to tuning the tradeoff between\ndeterminacy and conciseness, the framework allows controlling the level of\ngranularity by adopting different fragments of view definitions, such as ones\nhighlighting whole columns, foreign keys between tables, relevant groups of\ntuples, and so on. We investigate the realization of the framework in the case\nof hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive\nsearch over the space of all databases. We propose techniques that are\nmodel-agnostic, and others that are tailored to hetero-GNNs via the notion of\nlearnable masking. Our approach is evaluated through an extensive empirical\nstudy on the RelBench collection, covering a variety of domains and different\nrecord-level tasks. The results demonstrate the usefulness of the proposed\nexplanations, as well as the efficiency of their generation."}
{"id": "2509.09400", "pdf": "https://arxiv.org/pdf/2509.09400", "abs": "https://arxiv.org/abs/2509.09400", "authors": ["Valerio Besozzi", "Enrico Fiasco", "Marco Danelutto", "Patrizio Dazzi"], "title": "WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge", "categories": ["cs.DC", "cs.PF"], "comment": "Accepted at VHPC25", "summary": "Serverless computing at the edge requires lightweight execution environments\nto minimize cold start latency, especially in Urgent Edge Computing (UEC). This\npaper compares WebAssembly and unikernel-based MicroVMs for serverless\nworkloads. We present Limes, a WebAssembly runtime built on Wasmtime, and\nevaluate it against the Firecracker-based environment used in SPARE. Results\nshow that WebAssembly offers lower cold start times for lightweight functions\nbut suffers with complex workloads, while Firecracker provides higher, but\nstable, cold starts and better execution performance, particularly for\nI/O-heavy tasks."}
{"id": "2509.09343", "pdf": "https://arxiv.org/pdf/2509.09343", "abs": "https://arxiv.org/abs/2509.09343", "authors": ["Mohammed M. H. Qazzaz", "Abdelaziz Salama", "Maryam Hafeez", "Syed A. R. Zaidi"], "title": "Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Open Radio Access Network (O-RAN) architecture provides an intrinsic\ncapability to exploit key performance monitoring (KPM) within Radio\nIntelligence Controller (RIC) to derive network optimisation through xApps.\nThese xApps can leverage KPM knowledge to dynamically switch on/off the\nassociated RUs where such a function is supported over the E2 interface.\nSeveral existing studies employ artificial intelligence (AI)/Machine Learning\n(ML) based approaches to realise such dynamic sleeping for increased energy\nefficiency (EE). Nevertheless, most of these approaches rely upon offloading\nuser equipment (UE) to carve out a sleeping opportunity. Such an approach\ninherently creates load imbalance across the network. Such load imbalance may\nimpact the throughput performance of offloaded UEs as they might be allocated a\nlower number of physical resource blocks (PRBs). Maintaining the same PRB\nallocation while addressing the EE at the network level is a challenging task.\nTo that end, in this article, we present a comprehensive ML-based framework for\njoint optimisation of load balancing and EE for ORAN deployments. We formulate\nthe problem as a multi-class classification system that predictively evaluates\npotential RU configurations before optimising the EE, mapping network\nconditions to three load balance categories (Well Balanced, Moderately\nBalanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,\nAggressive) accommodates different operational priorities between energy\nsavings and performance assurance. Experimental evaluation using 4.26 million\nreal network measurements from simulations demonstrates that our Random Forest\nmodel achieves 98.3% F1-macro performance, representing 195% improvement over\ntraditional baseline strategies."}
{"id": "2509.09094", "pdf": "https://arxiv.org/pdf/2509.09094", "abs": "https://arxiv.org/abs/2509.09094", "authors": ["Guochu Xiong", "Xiangzhong Luo", "Weichen Liu"], "title": "Coherence-Aware Task Graph Modeling for Realistic Application", "categories": ["cs.DC"], "comment": "Accepted by MEMOCODE'25, 10 pages", "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."}
{"id": "2509.09005", "pdf": "https://arxiv.org/pdf/2509.09005", "abs": "https://arxiv.org/abs/2509.09005", "authors": ["Hirley Alves", "Nurul H. Mahmood", "Onel L. A. López", "Sumudu Samarakoon", "Seppo Yrjölä", "Matti Latva-Aho", "Markku Juntti", "Ari Pouttu", "Armin Dekorsy", "Arthur Sousa de Sena", "Aydin Sezgin", "Bho Matthiesen", "Chafika Benzaid", "Chathuranga Weeraddana", "David Hutchison", "Dileepa Marasinghe", "Doganalp Ergenc", "Eduard Jorswieck", "Erkki Harjula", "Falko Dressler", "Harri Saarnisaari", "Italo Atzeni", "Jaap Van De Beek", "Jacek Rak", "Konstantin Mikhaylov", "Lauri Loven", "Madhusanka Liyanage", "Marcos Katz", "Marja Matinmikko-Blue", "Mehdi Rasti", "Mika Ylianttila Nhan Nguyen", "Pawani Porambage", "Petar Popovski", "Petri Ahokangas", "Premanandana Rajatheva", "Robert-Jeron Reifert", "Tharaka Hewa", "Tommy Svensson"], "title": "6G Resilience -- White Paper", "categories": ["eess.SP", "cs.ET", "cs.SI"], "comment": null, "summary": "6G must be designed to withstand, adapt to, and evolve amid prolonged,\ncomplex disruptions. Mobile networks' shift from efficiency-first to\nsustainability-aware has motivated this white paper to assert that resilience\nis a primary design goal, alongside sustainability and efficiency, encompassing\ntechnology, architecture, and economics. We promote resilience by analysing\ndependencies between mobile networks and other critical systems, such as\nenergy, transport, and emergency services, and illustrate how cascading\nfailures spread through infrastructures. We formalise resilience using the 3R\nframework: reliability, robustness, resilience. Subsequently, we translate this\ninto measurable capabilities: graceful degradation, situational awareness,\nrapid reconfiguration, and learning-driven improvement and recovery.\n  Architecturally, we promote edge-native and locality-aware designs, open\ninterfaces, and programmability to enable islanded operations, fallback modes,\nand multi-layer diversity (radio, compute, energy, timing). Key enablers\ninclude AI-native control loops with verifiable behaviour, zero-trust security\nrooted in hardware and supply-chain integrity, and networking techniques that\nprioritise critical traffic, time-sensitive flows, and inter-domain\ncoordination.\n  Resilience also has a techno-economic aspect: open platforms and high-quality\ncomplementors generate ecosystem externalities that enhance resilience while\nopening new markets. We identify nine business-model groups and several\npatterns aligned with the 3R objectives, and we outline governance and\nstandardisation. This white paper serves as an initial step and catalyst for 6G\nresilience. It aims to inspire researchers, professionals, government\nofficials, and the public, providing them with the essential components to\nunderstand and shape the development of 6G resilience."}
{"id": "2509.09192", "pdf": "https://arxiv.org/pdf/2509.09192", "abs": "https://arxiv.org/abs/2509.09192", "authors": ["Doha Nam", "Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset", "categories": ["cs.SE", "cs.AI"], "comment": "An anonymous link containing the dataset, construction scripts, and\n  experimental code is publicly available for reproducibility:\n  https://figshare.com/s/4f202bc0921e26b41dc2", "summary": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in\nprioritizing risky code changes during code review and continuous integration.\nHowever, existing datasets often suffer from noisy labels and low precision in\nidentifying bug-inducing commits. To address this, we present ReDef\n(Revert-based Defect dataset), a high-confidence benchmark of function-level\nmodifications curated from 22 large-scale C/C++ projects. Defective cases are\nanchored by revert commits, while clean cases are validated through post-hoc\nhistory checks. Ambiguous instances are conservatively filtered out via a\nGPT-assisted triage process involving multiple votes and audits. This pipeline\nyields 3,164 defective and 10,268 clean modifications, offering substantially\nmore reliable labels than prior existing resources. Beyond dataset\nconstruction, we provide the first systematic evaluation of how pre-trained\nlanguage models (PLMs) reason about code modifications -- specifically, which\ninput encodings most effectively expose change information, and whether models\ngenuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder\nunder five encoding strategies, and further probe their sensitivity through\ncounterfactual perturbations that swap added/deleted blocks, invert diff\npolarity, or inject spurious markers. Our results show that compact diff-style\nencodings consistently outperform whole-function formats across all PLMs, with\nstatistical tests confirming large, model-independent effects. However, under\ncounterfactual tests, performance degrades little or not at all -- revealing\nthat what appears to be robustness in fact reflects reliance on superficial\ncues rather than true semantic understanding. These findings indicate that,\nunlike in snapshot-based tasks, current PLMs remain limited in their ability to\ngenuinely comprehend code modifications."}
{"id": "2509.09036", "pdf": "https://arxiv.org/pdf/2509.09036", "abs": "https://arxiv.org/abs/2509.09036", "authors": ["Lorenzo Neil", "Deepthi Mungara", "Laurie Williams", "Yasemin Acar", "Bradley Reaves"], "title": "Extended Version: It Should Be Easy but... New Users Experiences and Challenges with Secret Management Tools", "categories": ["cs.HC"], "comment": null, "summary": "Software developers face risks of leaking their software secrets, such as API\nkeys or passwords, which can result in significant harm. Secret management\ntools (SMTs), such as HashiCorp Vault Secrets or Infisical, are highly\nrecommended by industry, academia, and security guidelines to manage secrets\nsecurely. SMTs are designed to help developers secure their secrets in a\ncentral location, yet secrets leaks are still commonplace, and developers\nreport difficulty in learning how to setup and use SMTs. While SMTs typically\ncome with publicly available help resources (e.g., tool documentation and\ninterfaces), it is unclear if these actually help developers learn to\neffectively use SMTs. Without usable help resources that onboards developers,\nquick adoption and effective use of SMTs may be unrealistic. In a qualitative\ntwo-step study, we observed 21 new users in person while they used SMTs to\nperform two secret management tasks: secret storage and access, then secret\ninjection. We interviewed participants after each task to identify their\nchallenges and experiences using SMTs, with the assistance of help resources.\nWhile our study sample is narrow, it serves as a reasonable proxy for new\ndevelopers who are likely to adopt SMTs early in their careers. We found that\neven in a laboratory setting where new users found tool functionality,\ninterface flexibility helpful, they still experienced increased difficulty to\neffectively use SMTs to securely remediate a hard-coded secret when they felt\ntool documentation was insufficient and it motivated participants to deviate\nfrom official tool documentation to access secondary sources or attempt\nworkaround methods. Specific challenges reported by participants were tool\ndocumentation content quality, navigation difficulties with both tool\ndocumentation and web interfaces for finding helpful content, and supportive\ntool features."}
{"id": "2509.08969", "pdf": "https://arxiv.org/pdf/2509.08969", "abs": "https://arxiv.org/abs/2509.08969", "authors": ["Nima Karimian Kakolaki"], "title": "A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems", "categories": ["cs.DC", "cs.DB"], "comment": null, "summary": "Distributed systems require robust, scalable identifier schemes to ensure\ndata uniqueness and efficient indexing across multiple nodes. This paper\npresents a comprehensive analysis of the evolution of distributed identifiers,\ncomparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We\ncombine mathematical calculation of collision probabilities with empirical\nexperiments measuring generation speed and network transmission overhead in a\nsimulated distributed environment. Results demonstrate that ULIDs significantly\noutperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing\ngeneration speed by 97.32%. statistical analysis further shows ULIDs offer a\n98.42% lower collision risk compared to UUIDv7, while maintaining negligible\ncollision probabilities even at high generation rates. These findings highlight\nULIDs as an optimal choice for high-performance distributed systems, providing\nefficient, time-ordered, and lexicographically sortable identifiers suitable\nfor scalable applications. All source code, datasets, and analysis scripts\nutilized in this research are publicly available in our dedicated repository at\nhttps://github.com/nimakarimiank/uids-comparison. This repository contains\ncomprehensive documentation of the experimental setup, including configuration\nfiles for the distributed environment, producer and consumer implementations,\nand message broker integration. Additionally, it provides the data scripts and\ndatasets. Researchers and practitioners are encouraged to explore the\nrepository for full reproducibility of the experiments and to facilitate\nfurther investigation or extension of the presented work."}
{"id": "2509.09453", "pdf": "https://arxiv.org/pdf/2509.09453", "abs": "https://arxiv.org/abs/2509.09453", "authors": ["Ane Sanz", "Asier Atutxa", "David Franco", "Jasone Astorga", "Eduardo Jacob", "Diego López"], "title": "Toward quantum-safe scalable networks: an open, standards-aware key management framework", "categories": ["cs.NI"], "comment": null, "summary": "With the advent of quantum computing, the increasing threats to security\nposes a great challenge to communication networks. Recent innovations in this\nfield resulted in promising technologies such as Quantum Key Distribution\n(QKD), which enables the generation of unconditionally secure keys,\nestablishing secure communications between remote nodes. Additionally, QKD\nnetworks enable the interconnection of multinode architectures, extending the\npoint-to-point nature of QKD. However, due to the limitations of the current\nstate of technology, the scalability of QKD networks remains a challenge toward\nfeasible implementations. When it comes to long-distance implementations,\ntrusted relay nodes partially solve the distance issue through the forwarding\nof the distributed keys, allowing applications that do not have a direct QKD\nlink to securely share key material. Even though the relay procedure itself has\nbeen extensively studied, the establishment of the relaying node path still\nlacks a solution. This paper proposes an innovative network architecture that\nsolves the challenges of Key Management System (KMS) identification, relay path\ndiscovery, and scalability of QKD networks by integrating Software-Defined\nNetworking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in\neach node and creating a new entity called the Quantum Security Controller\n(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs\nwithin the node and abstracting the user from discovering the correct KMS.\nAdditionally, based on the high-level view of the network topology and status,\nthe QuSeC serves the path discovery requests from vKMSs, computing the\nend-to-end (E2E) relay path and applying security policies. The paper also\nprovides a security analysis of the proposal, identifying the security levels\nof the architecture and analyzing the core networking security properties."}
{"id": "2509.09400", "pdf": "https://arxiv.org/pdf/2509.09400", "abs": "https://arxiv.org/abs/2509.09400", "authors": ["Valerio Besozzi", "Enrico Fiasco", "Marco Danelutto", "Patrizio Dazzi"], "title": "WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge", "categories": ["cs.DC", "cs.PF"], "comment": "Accepted at VHPC25", "summary": "Serverless computing at the edge requires lightweight execution environments\nto minimize cold start latency, especially in Urgent Edge Computing (UEC). This\npaper compares WebAssembly and unikernel-based MicroVMs for serverless\nworkloads. We present Limes, a WebAssembly runtime built on Wasmtime, and\nevaluate it against the Firecracker-based environment used in SPARE. Results\nshow that WebAssembly offers lower cold start times for lightweight functions\nbut suffers with complex workloads, while Firecracker provides higher, but\nstable, cold starts and better execution performance, particularly for\nI/O-heavy tasks."}
{"id": "2509.09349", "pdf": "https://arxiv.org/pdf/2509.09349", "abs": "https://arxiv.org/abs/2509.09349", "authors": ["Ian Nell", "Shane Gilroy"], "title": "Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.RO", "eess.IV"], "comment": null, "summary": "Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behavior classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviors such as excessive lateral movement and erratic trajectory patterns by\nimplementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioral analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions."}
{"id": "2509.09194", "pdf": "https://arxiv.org/pdf/2509.09194", "abs": "https://arxiv.org/abs/2509.09194", "authors": ["Ayelet Berzack", "Guy Katz"], "title": "On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability", "categories": ["cs.SE", "cs.AI", "68N19"], "comment": null, "summary": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper."}
{"id": "2509.09063", "pdf": "https://arxiv.org/pdf/2509.09063", "abs": "https://arxiv.org/abs/2509.09063", "authors": ["Melinda Cohoon"], "title": "Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls", "categories": ["cs.HC", "cs.CY", "cs.SI", "H.5.2; K.6.5; K.4.1"], "comment": "Preprint report. 40 pages, 10 figures. Supported by the Open\n  Technology Fund (OTF) Information Controls Fellowship Program (ICFP)", "summary": "Internet censorship in the Islamic Republic of Iran restricts access to\nglobal platforms and services, forcing users to rely on circumvention\ntechnologies such as VPNs, proxies, and tunneling tools. This report presents\nfindings from a mixed-methods study of 660 Iranian internet users, with a focus\non gamers as a digitally literate and socially networked community. Survey data\nare combined with network measurements of latency and VPN performance to\nidentify both technical and social strategies of circumvention. Results show\nthat while younger users report higher confidence with circumvention, peer\nnetworks, rather than formal training, are the strongest predictors of\nresilience. Gaming communities, particularly those active on platforms such as\nDiscord and Telegram, serve as hubs for sharing tactics and lowering barriers\nto adoption. These findings extend existing work on usable security and\ncensorship circumvention by highlighting the intersection of infrastructural\nconditions and social learning. The study concludes with design and policy\nimplications for developers, researchers, and funders working on digital rights\nand information controls."}
{"id": "2509.09537", "pdf": "https://arxiv.org/pdf/2509.09537", "abs": "https://arxiv.org/abs/2509.09537", "authors": ["Andrea Jimenez-Berenguel", "Celeste Campo", "Marta Moure-Garrido", "Carlos Garcia-Rubio", "Daniel Díaz-Sanchez", "Florina Almenares"], "title": "PARROT: Portable Android Reproducible traffic Observation Tool", "categories": ["cs.NI"], "comment": null, "summary": "The rapid evolution of mobile security protocols and limited availability of\ncurrent datasets constrains research in app traffic analysis. This paper\npresents PARROT, a reproducible and portable traffic capture system for\nsystematic app traffic collection using Android Virtual Devices. The system\nprovides automated environment setup, configurable Android versions, traffic\nrecording management, and labeled captures extraction with human-in-the-loop\napp interaction. PARROT integrates mitmproxy for optional traffic decryption\nwith automated SSL/TLS key extraction, supporting flexible capture modes with\nor without traffic interception. We collected a dataset of 80 apps selected\nfrom the MAppGraph dataset list, providing traffic captures with corresponding\nSSL keys for decryption analysis. Our comparative analysis between the\nMAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern\nevolution across 50 common apps. Key findings include migration from TLSv1.2 to\nTLSv1.3 protocol, with TLSv1.3 comprising 90.0\\% of TCP encrypted traffic in\n2025 compared to 6.7\\% in 2021. QUIC protocol adoption increased substantially,\nwith all 50 common apps generating QUIC traffic under normal network conditions\ncompared to 30 apps in 2021. DNS communications evolved from predominantly\nunencrypted Do53 protocol (91.0\\% in 2021) to encrypted DoT protocol (81.1\\% in\n2025). The open-source PARROT system enables reproducible app traffic capture\nfor research community adoption and provides insights into app security\nprotocol evolution."}
{"id": "2509.09435", "pdf": "https://arxiv.org/pdf/2509.09435", "abs": "https://arxiv.org/abs/2509.09435", "authors": ["Houming Qiu", "Kun Zhu", "Dusit Niyato", "Nguyen Cong Luong", "Changyan Yi", "Chen Dai"], "title": "Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing", "categories": ["cs.DC"], "comment": null, "summary": "Collaborative mobile edge computing (MEC) has emerged as a promising paradigm\nto enable low-capability edge nodes to cooperatively execute\ncomputation-intensive tasks. However, straggling edge nodes (stragglers)\nsignificantly degrade the performance of MEC systems by prolonging computation\nlatency. While coded distributed computing (CDC) as an effective technique is\nwidely adopted to mitigate straggler effects, existing CDC schemes exhibit two\ncritical limitations: (i) They cannot successfully decode the final result\nunless the number of received results reaches a fixed recovery threshold, which\nseriously restricts their flexibility; (ii) They suffer from inherent poles in\ntheir encoding/decoding functions, leading to decoding inaccuracies and\nnumerical instability in the computational results. To address these\nlimitations, this paper proposes an approximated CDC scheme based on\nbarycentric rational interpolation. The proposed CDC scheme offers several\noutstanding advantages. Firstly, it can decode the final result leveraging any\nreturned results from workers. Secondly, it supports computations over both\nfinite and real fields while ensuring numerical stability. Thirdly, its\nencoding/decoding functions are free of poles, which not only enhances\napproximation accuracy but also achieves flexible accuracy tuning. Fourthly, it\nintegrates a novel BRI-based gradient coding algorithm accelerating the\ntraining process while providing robustness against stragglers. Finally,\nexperimental results reveal that the proposed scheme is superior to existing\nCDC schemes in both waiting time and approximate accuracy."}
{"id": "2509.09645", "pdf": "https://arxiv.org/pdf/2509.09645", "abs": "https://arxiv.org/abs/2509.09645", "authors": ["Pranav Khadpe", "Kimi Wenzel", "George Loewenstein", "Geoff Kaufman"], "title": "Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character", "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "To appear at AIES 2025", "summary": "When someone sends us a thoughtful message, we naturally form judgments about\ntheir character. But what happens when that message carries a label indicating\nit was written with the help of AI? This paper investigates how the appearance\nof AI assistance affects our perceptions of message senders. Adding nuance to\nprevious research, through two studies (N=399) featuring vignette scenarios, we\nfind that AI-assistance labels don't necessarily make people view senders\nnegatively. Rather, they dampen the strength of character signals in\ncommunication. We show that when someone sends a warmth-signalling message\n(like thanking or apologizing) without AI help, people more strongly categorize\nthe sender as warm. At the same time, when someone sends a coldness-signalling\nmessage (like bragging or blaming) without assistance, people more confidently\ncategorize them as cold. Interestingly, AI labels weaken both these\nassociations: An AI-assisted apology makes the sender appear less warm than if\nthey had written it themselves, and an AI-assisted blame makes the sender\nappear less cold than if they had composed it independently. This supports our\nsignal diagnosticity explanation: messages labeled as AI-assisted are viewed as\nless diagnostic than messages which seem unassisted. We discuss how our\nfindings shed light on the causal origins of previously reported observations\nin AI-Mediated Communication."}
{"id": "2509.09294", "pdf": "https://arxiv.org/pdf/2509.09294", "abs": "https://arxiv.org/abs/2509.09294", "authors": ["Solal Rapaport", "Laurent Pautet", "Samuel Tardieu", "Stefano Zacchiroli"], "title": "Altered Histories in Version Control System Repositories: Evidence from the Trenches", "categories": ["cs.SE"], "comment": null, "summary": "Version Control Systems (VCS) like Git allow developers to locally rewrite\nrecorded history, e.g., to reorder and suppress commits or specific data in\nthem. These alterations have legitimate use cases, but become problematic when\nperformed on public branches that have downstream users: they break push/pull\nworkflows, challenge the integrity and reproducibility of repositories, and\ncreate opportunities for supply chain attackers to sneak into them nefarious\nchanges. We conduct the first large-scale investigation of Git history\nalterations in public code repositories. We analyze 111 M (millions)\nrepositories archived by Software Heritage, which preserves VCS histories even\nacross alterations. We find history alterations in 1.22 M repositories, for a\ntotal of 8.7 M rewritten histories. We categorize changes by where they happen\n(which repositories, which branches) and what is changed in them (files or\ncommit metadata). Conducting two targeted case studies we show that altered\nhistories recurrently change licenses retroactively, or are used to remove\n''secrets'' (e.g., private keys) committed by mistake. As these behaviors\ncorrespond to bad practices-in terms of project governance or security\nmanagement, respectively-that software recipients might want to avoid, we\nintroduce GitHistorian, an automated tool, that developers can use to spot and\ndescribe history alterations in public Git repositories."}
{"id": "2509.09076", "pdf": "https://arxiv.org/pdf/2509.09076", "abs": "https://arxiv.org/abs/2509.09076", "authors": ["Lindsay Blackwell"], "title": "Content Moderation Futures", "categories": ["cs.HC", "cs.CY"], "comment": "76 pages", "summary": "This study examines the failures and possibilities of contemporary social\nmedia governance through the lived experiences of various content moderation\nprofessionals. Drawing on participatory design workshops with 33 practitioners\nin both the technology industry and broader civil society, this research\nidentifies significant structural misalignments between corporate incentives\nand public interests. While experts agree that successful content moderation is\nprincipled, consistent, contextual, proactive, transparent, and accountable,\ncurrent technology companies fail to achieve these goals, due in part to\nexploitative labor practices, chronic underinvestment in user safety, and\npressures of global scale. I argue that successful governance is undermined by\nthe pursuit of technological novelty and rapid growth, resulting in platforms\nthat necessarily prioritize innovation and expansion over public trust and\nsafety. To counter this dynamic, I revisit the computational history of care\nwork, to motivate present-day solidarity amongst platform governance workers\nand inspire systemic change."}
{"id": "2509.09222", "pdf": "https://arxiv.org/pdf/2509.09222", "abs": "https://arxiv.org/abs/2509.09222", "authors": ["Muhammad Azmi Umer", "Zhan Xuna", "Yan Lin Aung", "Aditya P. Mathur", "Jianying Zhou"], "title": "A Cyber-Twin Based Honeypot for Gathering Threat Intelligence", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "Critical Infrastructure (CI) is prone to cyberattacks. Several techniques\nhave been developed to protect CI against such attacks. In this work, we\ndescribe a honeypot based on a cyber twin for a water treatment plant. The\nhoneypot is intended to serve as a realistic replica of a water treatment plant\nthat attracts potential attackers. The attacks launched on the honeypot are\nrecorded and analyzed for threat intelligence. The intelligence so obtained is\nshared with the management of water treatment plants, who in turn may use it to\nimprove plant protection systems. The honeypot used here is operational and has\nbeen attacked on several occasions using, for example, a ransomware attack that\nis described in detail."}
{"id": "2509.09493", "pdf": "https://arxiv.org/pdf/2509.09493", "abs": "https://arxiv.org/abs/2509.09493", "authors": ["Ignacio Amores-Sesar", "Christian Cachin", "Juan Villacis"], "title": "Weaker Assumptions for Asymmetric Trust", "categories": ["cs.DC"], "comment": null, "summary": "In distributed systems with asymmetric trust, each participant is free to\nmake its own trust assumptions about others, captured by an asymmetric quorum\nsystem. This contrasts with ordinary, symmetric quorum systems and threshold\nmodels, where trust assumptions are uniformly shared among participants.\nFundamental problems like reliable broadcast and consensus are unsolvable in\nthe asymmetric model if quorum systems satisfy only the classical properties of\nconsistency and availability. Existing approaches overcome this by introducing\nstronger assumptions. We show that some of these assumptions are overly\nrestrictive, so much so that they effectively eliminate the benefits of\nasymmetric trust. To address this, we propose a new approach to characterize\nasymmetric problems and, building upon it, present algorithms for reliable\nbroadcast and consensus that require weaker assumptions than previous\nsolutions. Our methods are general and can be extended to other core problems\nin systems with asymmetric trust."}
{"id": "2509.09313", "pdf": "https://arxiv.org/pdf/2509.09313", "abs": "https://arxiv.org/abs/2509.09313", "authors": ["Moritz Mock", "Thomas Forrer", "Barbara Russo"], "title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data", "categories": ["cs.SE"], "comment": "Accepted to the 26th International Conference on Product-Focused\n  Software Process Improvement (PROFES 2025)", "summary": "Deep learning solutions for vulnerability detection proposed in academic\nresearch are not always accessible to developers, and their applicability in\nindustrial settings is rarely addressed. Transferring such technologies from\nacademia to industry presents challenges related to trustworthiness, legacy\nsystems, limited digital literacy, and the gap between academic and industrial\nexpertise. For deep learning in particular, performance and integration into\nexisting workflows are additional concerns. In this work, we first evaluate the\nperformance of CodeBERT for detecting vulnerable functions in industrial and\nopen-source software. We analyse its cross-domain generalisation when\nfine-tuned on open-source data and tested on industrial data, and vice versa,\nalso exploring strategies for handling class imbalance. Based on these results,\nwe develop AI-DO(Automating vulnerability detection Integration for Developers'\nOperations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated\nrecommender system that uses fine-tuned CodeBERT to detect and localise\nvulnerabilities during code review without disrupting workflows. Finally, we\nassess the tool's perceived usefulness through a survey with the company's IT\nprofessionals. Our results show that models trained on industrial data detect\nvulnerabilities accurately within the same domain but lose performance on\nopen-source code, while a deep learner fine-tuned on open data, with\nappropriate undersampling techniques, improves the detection of\nvulnerabilities."}
{"id": "2509.09138", "pdf": "https://arxiv.org/pdf/2509.09138", "abs": "https://arxiv.org/abs/2509.09138", "authors": ["Akira Matsui", "Kazuki Fujikawa", "Ryo Sasaki", "Ryo Adachi"], "title": "User Exploration and Exploitation Behavior Under the Influence of Real-time Interactions in Live Streaming Environments", "categories": ["cs.HC"], "comment": null, "summary": "Live streaming platforms offer a distinctive way for users and content\ncreators to interact with each other through real-time communication. While\nresearch on user behavior in online platforms has explored how users discover\ntheir favorite content from creators and engage with them, the role of\nreal-time features remains unclear. There are open questions as to what\ncommonalities and differences exist in users' relationships with live streaming\nplatforms compared to traditional on-demand style platforms. To understand\nthis, we employ the concept of Exploration/Exploitation (E/E) and analyze a\nlarge-scale dataset from a live streaming platform over two years. Our results\nindicate that even on live streaming platforms, users exhibit E/E behavior but\nexperience a longer exploration period. We also identify external factors, such\nas circadian rhythms, that influence E/E dynamics and user loyalty. The\npresented study emphasizes the importance of balancing E/E in online platform\ndesign, especially for live streaming platforms, providing implications that\nsuggest design strategies for platform developers and content creators to\nfacilitate timely engagement and retention."}
{"id": "2509.09291", "pdf": "https://arxiv.org/pdf/2509.09291", "abs": "https://arxiv.org/abs/2509.09291", "authors": ["Biwei Yan", "Yue Zhang", "Minghui Xu", "Runyu Pan", "Jinku Li", "Xiuzhen Cheng"], "title": "What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains."}
{"id": "2509.09525", "pdf": "https://arxiv.org/pdf/2509.09525", "abs": "https://arxiv.org/abs/2509.09525", "authors": ["Jialiang Huang", "Teng Ma", "Zheng Liu", "Sixing Lin", "Kang Chen", "Jinlei Jiang", "Xia Liao", "Yingdi Shan", "Yongwei Wu", "Ning Zhang", "Mengting Lu", "Tao Ma", "Haifeng Gong", "Mingxing Zhang"], "title": "TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes", "categories": ["cs.DC", "cs.OS"], "comment": "38 pages", "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."}
{"id": "2509.09322", "pdf": "https://arxiv.org/pdf/2509.09322", "abs": "https://arxiv.org/abs/2509.09322", "authors": ["Jacopo Bufalino", "Agathe Blaise", "Stefano Secci"], "title": "ORCA: Unveiling Obscure Containers In The Wild", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Modern software development increasingly depends on open-source libraries and\nthird-party components, which are often encapsulated into containerized\nenvironments. While improving the development and deployment of applications,\nthis approach introduces security risks, particularly when outdated or\nvulnerable components are inadvertently included in production environments.\nSoftware Composition Analysis (SCA) is a critical process that helps identify\nand manage packages and dependencies inside a container. However, unintentional\nmodifications to the container filesystem can lead to incomplete container\nimages, which compromise the reliability of SCA tools. In this paper, we\nexamine the limitations of both cloud-based and open-source SCA tools when\nfaced with such obscure images. An analysis of 600 popular containers revealed\nthat obscure containers exist in well-known registries and trusted images and\nthat many tools fail to analyze such containers. To mitigate these issues, we\npropose an obscuration-resilient methodology for container analysis and\nintroduce ORCA (Obscuration-Resilient Container Analyzer), its open-source\nimplementation. We reported our findings to all vendors using their appropriate\nchannels. Our results demonstrate that ORCA effectively detects the content of\nobscure containers and achieves a median 40% improvement in file coverage\ncompared to Docker Scout and Syft."}
{"id": "2509.09255", "pdf": "https://arxiv.org/pdf/2509.09255", "abs": "https://arxiv.org/abs/2509.09255", "authors": ["Geonsun Lee", "Min Xia", "Nels Numan", "Xun Qian", "David Li", "Yanhe Chen", "Achin Kulshrestha", "Ishan Chatterjee", "Yinda Zhang", "Dinesh Manocha", "David Kim", "Ruofei Du"], "title": "Sensible Agent: A Framework for Unobtrusive Interaction with Proactive AR Agents", "categories": ["cs.HC"], "comment": null, "summary": "Proactive AR agents promise context-aware assistance, but their interactions\noften rely on explicit voice prompts or responses, which can be disruptive or\nsocially awkward. We introduce Sensible Agent, a framework designed for\nunobtrusive interaction with these proactive agents. Sensible Agent dynamically\nadapts both \"what\" assistance to offer and, crucially, \"how\" to deliver it,\nbased on real-time multimodal context sensing. Informed by an expert workshop\n(n=12) and a data annotation study (n=40), the framework leverages egocentric\ncameras, multimodal sensing, and Large Multimodal Models (LMMs) to infer\ncontext and suggest appropriate actions delivered via minimally intrusive\ninteraction modes. We demonstrate our prototype on an XR headset through a user\nstudy (n=10) in both AR and VR scenarios. Results indicate that Sensible Agent\nsignificantly reduces perceived interaction effort compared to voice-prompted\nbaseline, while maintaining high usability and achieving higher preference."}
{"id": "2509.09653", "pdf": "https://arxiv.org/pdf/2509.09653", "abs": "https://arxiv.org/abs/2509.09653", "authors": ["Yufeng Xin", "Liang Zhang"], "title": "Towards A High-Performance Quantum Data Center Network Architecture", "categories": ["quant-ph", "cs.DC", "cs.NI"], "comment": "IEEE International Conference on Communications 2025 (ICC 2025)", "summary": "Quantum Data Centers (QDCs) are needed to support large-scale quantum\nprocessing for both academic and commercial applications. While large-scale\nquantum computers are constrained by technological and financial barriers, a\nmodular approach that clusters small quantum computers offers an alternative.\nThis approach, however, introduces new challenges in network scalability,\nentanglement generation, and quantum memory management. In this paper, we\npropose a three-layer fat-tree network architecture for QDCs, designed to\naddress these challenges. Our architecture features a unique leaf switch and an\nadvanced swapping spine switch design, optimized to handle high volumes of\nentanglement requests as well as a queue scheduling mechanism that efficiently\nmanages quantum memory to prevent decoherence. Through queuing-theoretical\nmodels and simulations in NetSquid, we demonstrate the proposed architecture's\nscalability and effectiveness in maintaining high entanglement fidelity,\noffering a practical path forward for modular QDC networks."}
{"id": "2509.08971", "pdf": "https://arxiv.org/pdf/2509.08971", "abs": "https://arxiv.org/abs/2509.08971", "authors": ["Julien Loiseau", "Hyun Lim", "Andrés Yagüe López", "Mammadbaghir Baghirzade", "Shihab Shahriar Khan", "Yoonsoo Kim", "Sudarshan Neopane", "Alexander Strack", "Farhana Taiyebah", "Benjamin K. Bergen"], "title": "HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework", "categories": ["physics.comp-ph", "astro-ph.IM", "cs.DC"], "comment": "15 pages, 8 figures", "summary": "Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application\nfor high-performance simulations of compressible hydrodynamics with\nradiation-diffusion coupling. Built on the FleCSI (Flexible Computational\nScience Infrastructure) framework, HARD expresses its computational units as\ntasks whose execution can be orchestrated by multiple back-end runtimes,\nincluding Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,\nproviding a single, portable code base that runs efficiently on laptops, small\nhomogeneous clusters, and the largest heterogeneous supercomputers currently\navailable. To ensure scientific reliability, HARD includes a regression-test\nsuite that automatically reproduces canonical verification problems such as the\nSod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical\nsolutions against known analytical results. The project is distributed under an\nOSI-approved license, hosted on GitHub, and accompanied by reproducible build\nscripts and continuous integration workflows. This combination of performance\nportability, verification infrastructure, and community-focused development\nmakes HARD a sustainable platform for advancing radiation hydrodynamics\nresearch across multiple domains."}
{"id": "2509.09614", "pdf": "https://arxiv.org/pdf/2509.09614", "abs": "https://arxiv.org/abs/2509.09614", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": "53 pages", "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench."}
{"id": "2509.09281", "pdf": "https://arxiv.org/pdf/2509.09281", "abs": "https://arxiv.org/abs/2509.09281", "authors": ["Sandeep Banik", "Naira Hovakimyan"], "title": "Flip Co-op: Cooperative Takeovers in Shared Autonomy", "categories": ["cs.HC"], "comment": "11 pages and 4 figures", "summary": "Shared autonomy requires principled mechanisms for allocating and\ntransferring control between a human and an autonomous agent. Existing\napproaches often rely on blending control inputs between human and autonomous\nagent or switching rules, which lack theoretical guarantees. This paper\ndevelops a game-theoretic framework for modeling cooperative takeover in shared\nautonomy. We formulate the switching interaction as a dynamic game in which\nauthority is embedded directly into the system dynamics, resulting in Nash\nequilibrium(NE)-based strategies rather than ad hoc switching rules. We\nestablish the existence and characterization of NE in the space of pure\ntakeover strategies under stochastic human intent. For the class of\nlinear-quadratic systems, we derive closed-form recursions for takeover\nstrategies and saddle-point value functions, providing analytical insight and\nefficient computation of cooperative takeover policies. We further introduce a\nbimatrix potential game reformulation to address scenarios where human and\nautonomy utilities are not perfectly aligned, yielding a unifying potential\nfunction that preserves tractability while capturing intent deviations. The\nframework is applied to a vehicle trajectory tracking problem, demonstrating\nhow equilibrium takeover strategies adapt across straight and curved path\nsegments. The results highlight the trade-off between human adaptability and\nautonomous efficiency and illustrate the practical benefits of grounding shared\nautonomy in cooperative game theory."}
{"id": "2509.09534", "pdf": "https://arxiv.org/pdf/2509.09534", "abs": "https://arxiv.org/abs/2509.09534", "authors": ["Sena Ergisi", "Luis Maßny", "Rawad Bitar"], "title": "ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) emerged as a widely studied paradigm for distributed\nlearning. Despite its many advantages, FL remains vulnerable to adversarial\nattacks, especially under data heterogeneity. We propose a new Byzantine-robust\nFL algorithm called ProDiGy. The key novelty lies in evaluating the client\ngradients using a joint dual scoring system based on the gradients' proximity\nand dissimilarity. We demonstrate through extensive numerical experiments that\nProDiGy outperforms existing defenses in various scenarios. In particular, when\nthe clients' data do not follow an IID distribution, while other defense\nmechanisms fail, ProDiGy maintains strong defense capabilities and model\naccuracy. These findings highlight the effectiveness of a dual perspective\napproach that promotes natural similarity among honest clients while detecting\nsuspicious uniformity as a potential indicator of an attack."}
{"id": "2509.09630", "pdf": "https://arxiv.org/pdf/2509.09630", "abs": "https://arxiv.org/abs/2509.09630", "authors": ["Zhenguang Liu", "Lixun Ma", "Zhongzheng Mu", "Chengkun Wei", "Xiaojun Xu", "Yingying Jiao", "Kui Ren"], "title": "I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Widespread reuse of open-source code in smart contract development boosts\nprogramming efficiency but significantly amplifies bug propagation across\ncontracts, while dedicated methods for detecting similar smart contract\nfunctions remain very limited. Conventional abstract-syntax-tree (AST) based\nmethods for smart contract similarity detection face challenges in handling\nintricate tree structures, which impedes detailed semantic comparison of code.\nRecent deep-learning based approaches tend to overlook code syntax and\ndetection interpretability, resulting in suboptimal performance.\n  To fill this research gap, we introduce SmartDetector, a novel approach for\ncomputing similarity between smart contract functions, explainable at the\nfine-grained statement level. Technically, SmartDetector decomposes the AST of\na smart contract function into a series of smaller statement trees, each\nreflecting a structural element of the source code. Then, SmartDetector uses a\nclassifier to compute the similarity score of two functions by comparing each\npair of their statement trees. To address the infinite hyperparameter space of\nthe classifier, we mathematically derive a cosine-wise diffusion process to\nefficiently search optimal hyperparameters. Extensive experiments conducted on\nthree large real-world datasets demonstrate that SmartDetector outperforms\ncurrent state-of-the-art methods by an average improvement of 14.01% in\nF1-score, achieving an overall average F1-score of 95.88%."}
{"id": "2509.09285", "pdf": "https://arxiv.org/pdf/2509.09285", "abs": "https://arxiv.org/abs/2509.09285", "authors": ["Efe Bozkir", "Babette Bühler", "Xiaoyuan Wu", "Enkelejda Kasneci", "Lujo Bauer", "Lorrie Faith Cranor"], "title": "The Impact of Device Type, Data Practices, and Use Case Scenarios on Privacy Concerns about Eye-tracked Augmented Reality in the United States and Germany", "categories": ["cs.HC"], "comment": null, "summary": "Augmented reality technology will likely be prevalent with more affordable\nhead-mounted displays. Integrating novel interaction modalities such as eye\ntrackers into head-mounted displays could lead to collecting vast amounts of\nbiometric data, which may allow inference of sensitive user attributes like\nhealth status or sexual preference, posing privacy issues. While previous works\nbroadly examined privacy concerns about augmented reality, ours is the first to\nextensively explore privacy concerns on behavioral data, particularly eye\ntracking in augmented reality. We crowdsourced four survey studies in the\nUnited States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand\nthe impact of user attributes, augmented reality devices, use cases, data\npractices, and country on privacy concerns. Our findings indicate that\nparticipants are generally concerned about privacy when they know what\ninferences can be made based on the collected data. Despite the more prominent\nuse of smartphones in daily life than augmented reality glasses, we found no\nindications of differing privacy concerns depending on the device type. In\naddition, our participants are more comfortable when a particular use case\nbenefits them and less comfortable when other humans can consume their data.\nFurthermore, participants in the United States are less concerned about their\nprivacy than those in Germany. Based on our findings, we provide several\nrecommendations to practitioners and policymakers for privacy-aware augmented\nreality."}
{"id": "2509.09653", "pdf": "https://arxiv.org/pdf/2509.09653", "abs": "https://arxiv.org/abs/2509.09653", "authors": ["Yufeng Xin", "Liang Zhang"], "title": "Towards A High-Performance Quantum Data Center Network Architecture", "categories": ["quant-ph", "cs.DC", "cs.NI"], "comment": "IEEE International Conference on Communications 2025 (ICC 2025)", "summary": "Quantum Data Centers (QDCs) are needed to support large-scale quantum\nprocessing for both academic and commercial applications. While large-scale\nquantum computers are constrained by technological and financial barriers, a\nmodular approach that clusters small quantum computers offers an alternative.\nThis approach, however, introduces new challenges in network scalability,\nentanglement generation, and quantum memory management. In this paper, we\npropose a three-layer fat-tree network architecture for QDCs, designed to\naddress these challenges. Our architecture features a unique leaf switch and an\nadvanced swapping spine switch design, optimized to handle high volumes of\nentanglement requests as well as a queue scheduling mechanism that efficiently\nmanages quantum memory to prevent decoherence. Through queuing-theoretical\nmodels and simulations in NetSquid, we demonstrate the proposed architecture's\nscalability and effectiveness in maintaining high entanglement fidelity,\noffering a practical path forward for modular QDC networks."}
{"id": "2509.08847", "pdf": "https://arxiv.org/pdf/2509.08847", "abs": "https://arxiv.org/abs/2509.08847", "authors": ["Amna Hassan"], "title": "Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "This paper presents a novel framework for automated game template generation\nby transforming Game Design Documents (GDDs) into functional Unity game\nprototypes using Natural Language Processing (NLP) and multi-modal Large\nLanguage Models (LLMs). We introduce an end-to-end system that parses GDDs,\nextracts structured game specifications, and synthesizes Unity-compatible C#\ncode that implements the core mechanics, systems, and architecture defined in\nthe design documentation. Our approach combines a fine-tuned LLaMA-3 model\nspecialized for Unity code generation with a custom Unity integration package\nthat streamlines the implementation process. Evaluation results demonstrate\nsignificant improvements over baseline models, with our fine-tuned model\nachieving superior performance (4.8/5.0 average score) compared to\nstate-of-the-art LLMs across compilation success, GDD adherence, best practices\nadoption, and code modularity metrics. The generated templates demonstrate high\nadherence to GDD specifications across multiple game genres. Our system\neffectively addresses critical gaps in AI-assisted game development,\npositioning LLMs as valuable tools in streamlining the transition from game\ndesign to implementation."}
{"id": "2509.09309", "pdf": "https://arxiv.org/pdf/2509.09309", "abs": "https://arxiv.org/abs/2509.09309", "authors": ["Dana Harari", "Ofra Amir"], "title": "Proactive AI Adoption can be Threatening: When Help Backfires", "categories": ["cs.HC"], "comment": null, "summary": "Artificial intelligence (AI) assistants are increasingly embedded in\nworkplace tools, raising the question of how initiative-taking shapes adoption.\nPrior work highlights trust and expectation mismatches as barriers, but the\nunderlying psychological mechanisms remain unclear. Drawing on self-affirmation\nand social exchange theories, we theorize that unsolicited help elicits\nself-threat, reducing willingness to accept assistance, likelihood of future\nuse, and performance expectancy. We report two vignette-based experiments\n(Study~1: $N=761$; Study~2: $N=571$, preregistered). Study~1 compared\nanticipatory and reactive help provided by an AI vs. a human, while Study~2\ndistinguished between \\emph{offering} (suggesting help) and \\emph{providing}\n(acting automatically). In Study 1, AI help was more threatening than human\nhelp. Across both studies, anticipatory help increased perceived threat and\nreduced adoption outcomes. Our findings identify self-threat as a mechanism\nexplaining why proactive AI features may backfire and suggest design\nimplications for AI initiative."}
{"id": "2509.09392", "pdf": "https://arxiv.org/pdf/2509.09392", "abs": "https://arxiv.org/abs/2509.09392", "authors": ["Simon Leistikow", "Thomas Miro", "Adrian Kummerländer", "Ali Nahardani", "Katja Grün", "Markus Franz", "Verena Hoerr", "Mathias J. Krause", "Lars Linsen"], "title": "An Integrated Open Source Software System for the Generation and Analysis of Subject-Specific Blood Flow Simulation Ensembles", "categories": ["physics.med-ph", "cs.SE"], "comment": "21 pages, 7 figures, 2 tables", "summary": "Background and Objective: Hemodynamic analysis of blood flow through arteries\nand veins is critical for diagnosing cardiovascular diseases, such as aneurysms\nand stenoses, and for investigating cardiovascular parameters, such as\nturbulence and wall shear stress. For subject-specific analyses, the anatomy\nand blood flow of the subject can be captured non-invasively using structural\nand 4D Magnetic Resonance Imaging (MRI). Computational Fluid Dynamics (CFD), on\nthe other hand, can be used to generate blood flow simulations by solving the\nNavier-Stokes equations. To generate and analyze subject-specific blood flow\nsimulations, MRI and CFD have to be brought together.\n  Methods: We present an interactive, customizable, and user-oriented visual\nanalysis tool that assists researchers in both medicine and numerical analysis.\nOur open-source tool is applicable to domains such as CFD and MRI, and it\nfacilitates the analysis of simulation results and medical data, especially in\nhemodynamic studies. It enables the creation of simulation ensembles with a\nhigh variety of parameters. Furthermore, it allows for the visual and\nanalytical examination of simulations and measurements through 2D embeddings of\nthe similarity space.\n  Results: To demonstrate the effectiveness of our tool, we applied it to three\nreal-world use cases, showcasing its ability to configure simulation ensembles\nand analyse blood flow dynamics. We evaluated our example cases together with\nMRI and CFD experts to further enhance features and increase the usability.\n  Conclusions: By combining the strengths of both CFD and MRI, our tool\nprovides a more comprehensive understanding of hemodynamic parameters,\nfacilitating more accurate analysis of hemodynamic biomarkers."}
{"id": "2509.09359", "pdf": "https://arxiv.org/pdf/2509.09359", "abs": "https://arxiv.org/abs/2509.09359", "authors": ["Stefan Resch", "André Kousha", "Anna Carroll", "Noah Severinghaus", "Felix Rehberg", "Marco Zatschker", "Yunus Söyleyici", "Daniel Sanchez-Morillo"], "title": "Smart Device Development for Gait Monitoring: Multimodal Feedback in an Interactive Foot Orthosis, Walking Aid, and Mobile Application", "categories": ["cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Smart assistive technologies such as sensor-based footwear and walking aids\noffer promising opportunities to support rehabilitation through real-time\nfeedback and patient-centered monitoring. However, most orthotic devices remain\npassive and lack integrated sensing or feedback functionalities, while existing\nresearch often focuses on isolated prototypes rather than cohesive, interactive\nsystems. In this work, we present the design and implementation of a novel\nmodular sensor system that combines a smart foot orthosis with an instrumented\nforearm crutch. The system integrates plantar pressure and motion sensing,\nvibrotactile feedback, and wireless communication via a smartphone application.\nWe conducted an experimental user study with eight participants to validate the\nfeasibility of the smart foot orthosis for mobile gait detection, explore the\npotential of haptic feedback for user interaction, and assess the usability of\nthe accompanying mobile health application. Our work contributes to the field\nof smart assistive technology in rehabilitation and prevention by demonstrating\na functional and comprehensive system. We further discuss system limitations,\noutline potential application scenarios, and provide recommendations for future\ndevelopment and clinical integration."}
{"id": "2509.09412", "pdf": "https://arxiv.org/pdf/2509.09412", "abs": "https://arxiv.org/abs/2509.09412", "authors": ["Muhannad Ismael", "Maël Cornil"], "title": "Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment", "categories": ["cs.HC"], "comment": "This paper has been accepted as a short paper in VISIGRAPP\n  {https://www.scitepress.org/Papers/2025/131326/131326.pdf}", "summary": "This paper presents an outdoor tracking system using Real-Time Kinematic\n(RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s))\nin urban areas where the accurate tracking of objects is critical and where\ndisplaying occluded information is important for safety reasons. The approach\npresented here replaces 2D screens/tablets and offers distinct advantages,\nparticularly in scenarios demanding hands-free operation. The integration of\nRTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD\nrepresents a promising solution for outdoor applications. This paper provides\nvaluable insights into leveraging the combined potential of RTK and OST-HMD for\noutdoor tracking tasks from the perspectives of systems integration,\nperformance optimization, and usability. The main contributions of this paper\nare: \\textbf{1)} a system for seamlessly merging RTK systems with OST-HMD to\nenable relatively precise and intuitive outdoor tracking, \\textbf{2)} an\napproach to determine a global location to achieve the position relative to the\nworld, \\textbf{3)} an approach referred to as 'semi-dynamic' for system\nassessment. Moreover, we offer insights into several relevant future research\ntopics aimed at improving the OST-HMD and RTK hybrid system for outdoor\ntracking."}
{"id": "2509.09461", "pdf": "https://arxiv.org/pdf/2509.09461", "abs": "https://arxiv.org/abs/2509.09461", "authors": ["Ambre Assor", "Hyeon Jeon", "Sungbok Shin", "Jean-Daniel Fekete"], "title": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries with Human Intervention", "categories": ["cs.HC"], "comment": null, "summary": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world."}
{"id": "2509.09510", "pdf": "https://arxiv.org/pdf/2509.09510", "abs": "https://arxiv.org/abs/2509.09510", "authors": ["Racquel Fygenson", "Lace Padilla", "Enrico Bertini"], "title": "Cognitive Affordances in Visualization: Related Constructs, Design Factors, and Framework", "categories": ["cs.HC"], "comment": null, "summary": "Classically, affordance research investigates how the shape of objects\ncommunicates actions to potential users. Cognitive affordances, a subset of\nthis research, characterize how the design of objects influences cognitive\nactions, such as information processing. Within visualization, cognitive\naffordances inform how graphs' design decisions communicate information to\ntheir readers. Although several related concepts exist in visualization, a\nformal translation of affordance theory to visualization is still lacking. In\nthis paper, we review and translate affordance theory to visualization by\nformalizing how cognitive affordances operate within a visualization context.\nWe also review common methods and terms, and compare related constructs to\ncognitive affordances in visualization. Based on a synthesis of research from\npsychology, human computer interaction, and visualization, we propose a\nframework of cognitive affordances in visualization that enumerates design\ndecisions and reader characteristics that influence a visualization's hierarchy\nof communicated information. Finally, we demonstrate how this framework can\nguide the evaluation and redesign of visualizations."}
{"id": "2509.09645", "pdf": "https://arxiv.org/pdf/2509.09645", "abs": "https://arxiv.org/abs/2509.09645", "authors": ["Pranav Khadpe", "Kimi Wenzel", "George Loewenstein", "Geoff Kaufman"], "title": "Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character", "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "To appear at AIES 2025", "summary": "When someone sends us a thoughtful message, we naturally form judgments about\ntheir character. But what happens when that message carries a label indicating\nit was written with the help of AI? This paper investigates how the appearance\nof AI assistance affects our perceptions of message senders. Adding nuance to\nprevious research, through two studies (N=399) featuring vignette scenarios, we\nfind that AI-assistance labels don't necessarily make people view senders\nnegatively. Rather, they dampen the strength of character signals in\ncommunication. We show that when someone sends a warmth-signalling message\n(like thanking or apologizing) without AI help, people more strongly categorize\nthe sender as warm. At the same time, when someone sends a coldness-signalling\nmessage (like bragging or blaming) without assistance, people more confidently\ncategorize them as cold. Interestingly, AI labels weaken both these\nassociations: An AI-assisted apology makes the sender appear less warm than if\nthey had written it themselves, and an AI-assisted blame makes the sender\nappear less cold than if they had composed it independently. This supports our\nsignal diagnosticity explanation: messages labeled as AI-assisted are viewed as\nless diagnostic than messages which seem unassisted. We discuss how our\nfindings shed light on the causal origins of previously reported observations\nin AI-Mediated Communication."}
{"id": "2509.08857", "pdf": "https://arxiv.org/pdf/2509.08857", "abs": "https://arxiv.org/abs/2509.08857", "authors": ["Marcelino Garcia", "Renato Garcia", "Arthur Parizotto", "Andre Mendes", "Pedro Valle", "Ricardo Vilela", "Renato Balancieri", "Williamson Silva"], "title": "A Systematic Mapping Study on Chatbots in Programming Education", "categories": ["cs.SE", "cs.HC"], "comment": "18 pages, 1 figure, 3 tables", "summary": "Educational chatbots have gained prominence as support tools for teaching\nprogramming, particularly in introductory learning contexts. This paper\npresents a Systematic Mapping Study (SMS) that investigated how such agents\nhave been developed and applied in programming education. From an initial set\nof 3,216 publications, 54 studies were selected and analyzed based on five\nresearch subquestions, addressing chatbot types, programming languages used,\neducational content covered, interaction models, and application contexts. The\nresults reveal a predominance of chatbots designed for Python instruction,\nfocusing on fundamental programming concepts, and employing a wide variety of\npedagogical approaches and technological architectures. In addition to\nidentifying trends and gaps in the literature, this study provides insights to\ninform the development of new educational tools for programming instruction."}
{"id": "2509.08862", "pdf": "https://arxiv.org/pdf/2509.08862", "abs": "https://arxiv.org/abs/2509.08862", "authors": ["Chang Liu", "Loc Hoang", "Andrew Stolman", "Rene F. Kizilcec", "Bo Wu"], "title": "Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Providing students with flexible and timely academic support is a challenge\nat most colleges and universities, leaving many students without help outside\nscheduled hours. Large language models (LLMs) are promising for bridging this\ngap, but interactions between students and LLMs are rarely overseen by\neducators. We developed and studied an LLM-powered course assistant deployed\nacross multiple computer science courses to characterize real-world use and\nunderstand pedagogical implications. By Spring 2024, our system had been\ndeployed to approximately 2,000 students across six courses at three\ninstitutions. Analysis of the interaction data shows that usage remains strong\nin the evenings and nights and is higher in introductory courses, indicating\nthat our system helps address temporal support gaps and novice learner needs.\nWe sampled 200 conversations per course for manual annotation: most sampled\nresponses were judged correct and helpful, with a small share unhelpful or\nerroneous; few responses included dedicated examples. We also examined an\ninquiry-based learning strategy: only around 11% of sampled conversations\ncontained LLM-generated follow-up questions, which were often ignored by\nstudents in advanced courses. A Bloom's taxonomy analysis reveals that current\nLLM capabilities are limited in generating higher-order cognitive questions.\nThese patterns suggest opportunities for pedagogically oriented LLM-based\neducational systems and greater educator involvement in configuring prompts,\ncontent, and policies."}
{"id": "2509.08912", "pdf": "https://arxiv.org/pdf/2509.08912", "abs": "https://arxiv.org/abs/2509.08912", "authors": ["Lingyao Li", "Renkai Ma", "Zhaoqian Xue", "Junjie Xiong"], "title": "Towards Trustworthy AI: Characterizing User-Reported Risks across LLMs \"In the Wild\"", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "While Large Language Models (LLMs) are rapidly integrating into daily life,\nresearch on their risks often remains lab-based and disconnected from the\nproblems users encounter \"in the wild.\" While recent HCI research has begun to\nexplore these user-facing risks, it typically concentrates on a singular LLM\nchatbot like ChatGPT or an isolated risk like privacy. To gain a holistic\nunderstanding of multi-risk across LLM chatbots, we analyze online discussions\non Reddit around seven major LLM chatbots through the U.S. NIST's AI Risk\nManagement Framework. We find that user-reported risks are unevenly distributed\nand platform-specific. While \"Valid and Reliable\" risk is the most frequently\nmentioned, each product also exhibits a unique \"risk fingerprint;\" for\ninstance, user discussions associate GPT more with \"Safe\" and \"Fair\" issues,\nGemini with \"Privacy,\" and Claude with \"Secure and Resilient\" risks.\nFurthermore, the nature of these risks differs by their prevalence: less\nfrequent risks like \"Explainability\" and \"Privacy\" manifest as nuanced user\ntrade-offs, more common ones like \"Fairness\" are experienced as direct personal\nharms. Our findings reveal gaps between risks reported by system-centered\nstudies and by users, highlighting the need for user-centered approaches that\nsupport users in their daily use of LLM chatbots."}
{"id": "2509.09071", "pdf": "https://arxiv.org/pdf/2509.09071", "abs": "https://arxiv.org/abs/2509.09071", "authors": ["Crystal Qian", "Kehang Zhu", "John Horton", "Benjamin S. Manning", "Vivian Tsai", "James Wexler", "Nithum Thain"], "title": "Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games", "categories": ["cs.AI", "cs.GT", "cs.HC"], "comment": null, "summary": "Coordination tasks traditionally performed by humans are increasingly being\ndelegated to autonomous agents. As this pattern progresses, it becomes critical\nto evaluate not only these agents' performance but also the processes through\nwhich they negotiate in dynamic, multi-agent environments. Furthermore,\ndifferent agents exhibit distinct advantages: traditional statistical agents,\nsuch as Bayesian models, may excel under well-specified conditions, whereas\nlarge language models (LLMs) can generalize across contexts. In this work, we\ncompare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in\na dynamic negotiation setting that enables direct, identical-condition\ncomparisons across populations, capturing both outcomes and behavioral\ndynamics. Bayesian agents extract the highest surplus through aggressive\noptimization, at the cost of frequent trade rejections. Humans and LLMs can\nachieve similar overall surplus, but through distinct behaviors: LLMs favor\nconservative, concessionary trades with few rejections, while humans employ\nmore strategic, risk-taking, and fairness-oriented behaviors. Thus, we find\nthat performance parity -- a common benchmark in agent evaluation -- can\nconceal fundamental differences in process and alignment, which are critical\nfor practical deployment in real-world coordination tasks."}
{"id": "2509.09314", "pdf": "https://arxiv.org/pdf/2509.09314", "abs": "https://arxiv.org/abs/2509.09314", "authors": ["Thuy Ngoc Nguyen", "Anita Williams Woolley", "Cleotilde Gonzalez"], "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Coordinated teamwork is essential in fast-paced decision-making environments\nthat require dynamic adaptation, often without an opportunity for explicit\ncommunication. Although implicit coordination has been extensively considered\nin the existing literature, the majority of work has focused on co-located,\nsynchronous teamwork (such as sports teams) or, in distributed teams, primarily\non coordination of knowledge work. However, many teams (firefighters, military,\nlaw enforcement, emergency response) must coordinate their movements in\nphysical space without the benefit of visual cues or extensive explicit\ncommunication. This paper investigates how three dimensions of spatial\ncoordination, namely exploration diversity, movement specialization, and\nadaptive spatial proximity, influence team performance in a collaborative\nonline search and rescue task where explicit communication is restricted and\nteam members rely on movement patterns to infer others' intentions and\ncoordinate actions. Our metrics capture the relational aspects of teamwork by\nmeasuring spatial proximity, distribution patterns, and alignment of movements\nwithin shared environments. We analyze data from 34 four-person teams (136\nparticipants) assigned to specialized roles in a search and rescue task.\nResults show that spatial specialization positively predicts performance, while\nadaptive spatial proximity exhibits a marginal inverted U-shaped relationship,\nsuggesting moderate levels of adaptation are optimal. Furthermore, the temporal\ndynamics of these metrics differentiate high- from low-performing teams over\ntime. These findings provide insights into implicit spatial coordination in\nrole-based teamwork and highlight the importance of balanced adaptive\nstrategies, with implications for training and AI-assisted team support\nsystems."}
{"id": "2509.09508", "pdf": "https://arxiv.org/pdf/2509.09508", "abs": "https://arxiv.org/abs/2509.09508", "authors": ["Avinash Agarwal", "Manisha J. Nene"], "title": "Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "16 pages, 2 figures, 1 table", "summary": "The integration of artificial intelligence (AI) into telecommunications\ninfrastructure introduces novel risks, such as algorithmic bias and\nunpredictable system behavior, that fall outside the scope of traditional\ncybersecurity and data protection frameworks. This paper introduces a precise\ndefinition and a detailed typology of telecommunications AI incidents,\nestablishing them as a distinct category of risk that extends beyond\nconventional cybersecurity and data protection breaches. It argues for their\nrecognition as a distinct regulatory concern. Using India as a case study for\njurisdictions that lack a horizontal AI law, the paper analyzes the country's\nkey digital regulations. The analysis reveals that India's existing legal\ninstruments, including the Telecommunications Act, 2023, the CERT-In Rules, and\nthe Digital Personal Data Protection Act, 2023, focus on cybersecurity and data\nbreaches, creating a significant regulatory gap for AI-specific operational\nincidents, such as performance degradation and algorithmic bias. The paper also\nexamines structural barriers to disclosure and the limitations of existing AI\nincident repositories. Based on these findings, the paper proposes targeted\npolicy recommendations centered on integrating AI incident reporting into\nIndia's existing telecom governance. Key proposals include mandating reporting\nfor high-risk AI failures, designating an existing government body as a nodal\nagency to manage incident data, and developing standardized reporting\nframeworks. These recommendations aim to enhance regulatory clarity and\nstrengthen long-term resilience, offering a pragmatic and replicable blueprint\nfor other nations seeking to govern AI risks within their existing sectoral\nframeworks."}
{"id": "2509.09583", "pdf": "https://arxiv.org/pdf/2509.09583", "abs": "https://arxiv.org/abs/2509.09583", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality."}
{"id": "2509.09638", "pdf": "https://arxiv.org/pdf/2509.09638", "abs": "https://arxiv.org/abs/2509.09638", "authors": ["Amitabh Chakravorty", "Jess Kropczynski", "Nelly Elsayed"], "title": "CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype", "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "With the widespread adoption of cryptocurrencies, cryptojacking has become a\nsignificant security threat to crypto wallet users. This paper presents a\nfront-end prototype of an AI-powered security dashboard, namely, CryptoGuard.\nDeveloped through a user-centered design process, the prototype was constructed\nas a high-fidelity, click-through model from Figma mockups to simulate key user\ninteractions. It is designed to assist users in monitoring their login and\ntransaction activity, identifying any suspicious behavior, and enabling them to\ntake action directly within the wallet interface. The dashboard is designed for\na general audience, prioritizing an intuitive user experience for non-technical\nindividuals. Although its AI functionality is conceptual, the prototype\ndemonstrates features like visual alerts and reporting. This work is positioned\nexplicitly as a design concept, bridging cryptojacking detection research with\nhuman-centered interface design. This paper also demonstrates how usability\nheuristics can directly inform a tool's ability to support rapid and confident\ndecision-making under real-world threats. This paper argues that practical\nsecurity tools require not only robust backend functionality but also a\nuser-centric design that communicates risk and empowers users to take\nmeaningful action."}
