<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 该论文综述了LLM在AIOps中的应用，分析了183篇研究论文，探讨了数据源、任务演进、方法与评估，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在AIOps中的影响与潜力，填补当前研究的空白。

Method: 对2020-2024年的183篇论文进行系统综述，围绕四个研究问题展开分析。

Result: 总结了LLM在AIOps中的最新进展、趋势及现有研究不足。

Conclusion: 提出了未来研究的潜在方向，推动LLM在AIOps领域的进一步发展。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [2] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 该研究探索使用大型语言模型（LLMs）作为灵活的量子SDK间代码转换工具，替代传统基于规则的方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算平台多样性导致编程SDK互操作性差，传统转换工具设计繁琐且维护成本高。

Method: 利用LLMs的预训练知识和上下文推理能力，无需手动定义规则，实现量子程序跨SDK的自动化转换。

Result: LLMs可作为语言无关的转换器，保留功能等效性，提升量子软件的可移植性。

Conclusion: 该工作为量子计算生态中的智能通用代码转换迈出了重要一步。

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [3] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: 论文提出了Kodezi Chronos，一种新型架构，用于解决大语言模型在代码理解和维护中的上下文限制和结构推理不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成和自动化中存在推理上下文有限和缺乏显式代码结构推理的问题，需要一种能处理超长上下文的新方法。

Method: 采用多级嵌入内存引擎，结合向量和图索引，实现对整个代码库的高效理解和维护。

Result: 在真实世界任务中，Kodezi Chronos在错误检测上提升23%，调试周期缩短40%。

Conclusion: Kodezi Chronos标志着软件生态系统向自持续优化的关键进步。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [4] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 该论文提出了首个关于强化学习（RL）在软件工程（SE）中应用的系统性综述，分析了115篇研究，总结了趋势、分类及未来方向。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域系统复杂性增加及自动化需求上升，促使研究者探索强化学习的应用，但缺乏系统性总结。

Method: 综述了115篇同行评审研究，分析了发表趋势、SE主题和RL算法分类，以及数据集、模型设计和评估实践。

Result: 提供了RL在SE中的系统性分类和趋势分析，并提出了开放挑战和未来研究方向。

Conclusion: 该论文填补了RL-for-SE领域的系统性综述空白，为研究者和实践者提供了导航和推动该领域发展的支持。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [5] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 论文提出了一种名为RAGSum的新方法，通过结合检索和生成技术，利用CodeT5模型自动生成代码注释，显著提升了自动化注释的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法中检索和生成通常是独立优化的，导致下游生成受到不相关邻居代码的噪声影响。

Method: RAGSum采用CodeT5模型，结合对比预训练和端到端训练，通过复合损失函数优化检索和生成，并引入轻量级自优化循环提升结果。

Result: 在Java、Python和C三种语言的基准测试中，RAGSum在BLEU、METEOR和ROUTE-L指标上显著优于基线方法。

Conclusion: 紧密耦合检索和生成可以提升自动化注释的上限，并激励未来的复现和开发者定性研究。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [6] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 利用预训练的CodeBERT和CodeT5模型推荐重构方案，CodeT5表现最佳，准确率达96.9%。


<details>
  <summary>Details</summary>
Motivation: 解决现有工具只能检测架构异味但无法推荐修复方案的问题。

Method: 将任务建模为三分类问题，基于2百万重构实例微调CodeBERT和CodeT5。

Result: CodeT5准确率96.9%，F1分数95.2%，优于CodeBERT和传统方法。

Conclusion: Transformer模型能有效连接异味检测与修复建议，支持未来研究。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [7] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 论文通过两种强化学习方法（GRPO和ORPO）微调32B模型，显著提升了量子编程代码生成的准确性，但高级任务仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM（如Granite-20B-Code和StarCoder）在生成量子电路代码时存在错误，需提高其准确性和鲁棒性。

Method: 使用GRPO和ORPO两种强化学习方法微调32B模型，基于标注的合成数据集进行训练。

Result: 在Qiskit HumanEval基准测试中，ORPO达到56.29% Pass@1，GRPO为49%，均优于基线模型；原始HumanEval测试中分别为65.90%和63.00%。

Conclusion: GRPO在基础任务表现优异，ORPO在中级任务表现更好，但两者均未能解决高级任务，表明仍有进步空间。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [8] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 提出了一种结合显式和隐式评估方法的三阶段评估框架，用于智能电网中新建信息与数据模型的设计过程。


<details>
  <summary>Details</summary>
Motivation: 智能电网数字化导致新的信息与数据模型需求增加，现有评估方法在高层次上缺乏具体步骤或仅关注已在使用模型的测试。

Method: 采用设计科学研究方法，设计了三阶段评估框架，结合显式与隐式方法，应用于工业灵活性描述的模型开发。

Result: 通过实际应用验证了评估框架的可行性，并总结了经验教训。

Conclusion: 提出的评估方法填补了智能电网中新模型设计评估的空白，具有实践指导意义。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [9] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 论文提出了一种结合模糊逻辑的新方法，用于更准确地评估项目成功。


<details>
  <summary>Details</summary>
Motivation: 传统的Likert量表方法往往忽视了项目成功的上下文依赖性和多面性，无法全面衡量。

Method: 采用了分层Type-1 Mamdani模糊系统，优先考虑对终端用户的持续积极影响。

Result: 动态方法可能更适合复杂评估，并提供更准确的项目成功测量。

Conclusion: 模糊逻辑在社会科学中的应用具有潜力，未来研究将对此进行实证测试和扩展。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [10] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 提出了一种称为单对话方法论（SCM）的新型软件开发方法，强调通过单一、长上下文对话完成项目全阶段开发。


<details>
  <summary>Details</summary>
Motivation: 纠正当前对大型语言模型（LLM）的被动依赖，突出开发者在智能工具中的主动角色。

Method: 基于认知清晰性、可追溯性、模块化和文档化原则，定义了SCM的各个阶段、最佳实践和哲学立场。

Result: SCM提供了一种结构化、持久的开发对话模式。

Conclusion: SCM是一种实用且必要的软件开发方法，旨在重新确立开发者作为工具架构者和监督者的地位。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [11] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 小型语言模型（SLMs）在自动检测测试异味方面表现出色，尤其是Phi-4性能最佳，能够低成本、高效地提升测试质量。


<details>
  <summary>Details</summary>
Motivation: 手动测试虽然重要，但测试用例常因质量问题（如歧义、冗余）影响可靠性，现有检测工具依赖手动规则且缺乏扩展性，因此探索SLMs的潜力。

Method: 评估Gemma3、Llama3.2和Phi-4在143个Ubuntu测试案例中对七种测试异味的检测能力。

Result: Phi-4表现最佳，检测准确率达97%，而Gemma3和Llama3.2约为91%；SLMs还能自动解释问题并提出改进建议。

Conclusion: SLMs无需依赖规则或语法分析即可高效检测测试异味，有望成为提升测试质量的实用工具，同时保护数据隐私。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [12] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出了一个名为iReDev的知识驱动多智能体框架，用于智能需求开发，通过集成人类知识和人机协作机制，显著提升了需求开发的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 需求开发是一个关键但耗时耗力的过程，现有的多智能体系统在这方面支持有限，且忽视了人类知识的注入和人机协作问题。

Method: iReDev框架包含六个知识驱动的智能体，支持整个需求开发流程，采用事件驱动的通信机制和人类参与回路机制，确保生成的成果符合利益相关者期望。

Result: 评估表明，iReDev在多个方面优于现有基线方法。

Conclusion: iReDev为智能需求开发提供了新的方向，未来可进一步探索其潜力。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [13] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 论文探讨了预训练模型在软件系统中的广泛应用及其对传统需求工程的挑战，提出了一个面向预训练模型系统的需求工程概念框架和研究方向。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在现代软件系统中的广泛应用及其独特特性（如能力边界模糊、行为依赖上下文、持续演化）对传统需求工程的确定性假设提出了挑战。

Method: 提出了一个专门针对预训练模型启用的软件系统的需求工程概念框架，并列举了若干有前景的研究方向。

Result: 论文为研究者和实践者提供了应对预训练模型系统需求工程挑战的指南。

Conclusion: 需要重新思考传统需求工程方法，以适应预训练模型系统的独特特性，并推动相关研究与实践的发展。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [14] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 提出一种从递归下降解析器实现中推断属性语法的新方法，动态分析输入处理的语义。


<details>
  <summary>Details</summary>
Motivation: 结构化输入处理的软件系统通常缺乏完整的最新规范，语法挖掘技术仅关注语法结构，语义处理未充分探索。

Method: 动态分析递归下降解析器的实现，将程序运行时行为映射到语法规则中，提取并嵌入语义动作。

Result: 生成的属性语法能准确再现程序行为，初步实验验证了方法的可行性。

Conclusion: 该方法能够恢复全面的规范，为输入处理的语义推断提供了有效途径。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [15] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文提出了CodeGPTSensor+，通过对抗训练提升对修改后LLM生成代码的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的广泛应用，其代码来源、版权和质量的监管问题日益突出，同时现有检测方法对修改后的代码鲁棒性不足。

Method: 采用对抗训练方法，结合MIST模块生成高质量对抗样本，提升模型对输入扰动的鲁棒性。

Result: 在HMCorp数据集上，CodeGPTSensor+显著提高了对抗测试集的检测准确率，同时保持原始测试集的高准确率。

Conclusion: CodeGPTSensor+优于现有方法，为解决LLM生成代码检测的鲁棒性问题提供了有效方案。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Dual-Numbers Reverse AD for Functional Array Languages](https://arxiv.org/abs/2507.12640)
*Tom Smeding,Mikołaj Konarski,Simon Peyton Jones,Andrew Fitzgibbon*

Main category: cs.PL

TL;DR: 本文提出了一种改进的双数反向模式自动微分方法，支持多维数组且性能损失小，但牺牲了高阶代码的通用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有双数反向模式自动微分在数组程序上性能不佳的问题。

Method: 结合向量化代码转换（BOT）、双数反向AD算法的数组语言扩展和符号解释，实现端到端编译。

Result: 实现了对多维数组的高效支持，但无法完全支持高阶代码。

Conclusion: 通过牺牲高阶通用性，实现了数组程序的高效自动微分。

Abstract: The standard dual-numbers construction works well for forward-mode automatic
differentiation (AD) and is attractive due to its simplicity; recently, it also
has been adapted to reverse-mode AD, but practical performance, especially on
array programs, leaves a lot to be desired. In this paper we introduce
first-class support for multidimensional arrays in dual-numbers reverse-mode AD
with little to no performance overhead. The algorithm consists of three
loosely-coupled components: a semantics-preserving vectorisation code
transformation (the bulk-operation transform or BOT), a fairly straightforward
lifting of the basic dual-numbers reverse AD algorithm to a mostly first-order
array language, and symbolic interpretation to achieve an end-to-end
compilation pipeline. Unfortunately, we lose some of the nice generalisable
aspects of dual-numbers AD in the process, most importantly support for
higher-order code.
  We do support some higher-order array combinators, but only a
carefully-chosen set: 'build' (elementwise array construction), 'gather' and
'scatter'. In return, the BOT can eliminate the essential (for AD)
higher-orderness of the input program, meaning that AD gets essentially
presented with a first-order program. This allows the naive trick of lifting
dual numbers to "dual arrays" to work without much modification.

</details>


### [17] [Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications](https://arxiv.org/abs/2507.13091)
*Aurèle Barrière,Victor Deng,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 提出首个机械化、简洁、实用且完整的现代正则表达式语义，并证明其忠实性，同时展示了两个实际应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决现代正则表达式语言缺乏完整且忠实的形式化语义的问题，并为实际应用提供理论支持。

Method: 通过证明其与ECMAScript官方规范的等价性确保语义的忠实性，并展示了两个应用：上下文等价性证明和PikeVM算法的形式化证明。

Result: 成功实现了一个完整且忠实的形式化语义，并验证了其在实践中的实用性。

Conclusion: 该研究为现代正则表达式提供了可靠的理论基础，并展示了其在实际中的多功能性。

Abstract: We present the first mechanized, succinct, practical, complete, and
proven-faithful semantics for a modern regular expression language with
backtracking semantics. We ensure its faithfulness by proving it equivalent to
a preexisting line-by-line embedding of the official ECMAScript specification
of JavaScript regular expressions. We demonstrate its practicality by
presenting two real-world applications. First, a new notion of contextual
equivalence for modern regular expressions, which we use to prove or disprove
rewrites drawn from previous work. Second, the first formal proof of the PikeVM
algorithm used in many real-world engines. In contrast with the specification
and other formalization work, our semantics captures not only the top-priority
match, but a full backtracking tree recording all possible matches and their
respective priority. All our definitions and results have been mechanized in
the Rocq proof assistant.

</details>


### [18] [Towards Formal Verification of LLM-Generated Code from Natural Language Prompts](https://arxiv.org/abs/2507.13290)
*Aaron Councilman,David Fu,Aryan Gupta,Chengxiao Wang,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.PL

TL;DR: 该论文提出了一种方法，通过结合形式化查询语言和验证工具Astrogator，为LLM生成的代码提供正确性保证，提升AI编程助手的用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的代码常存在错误且用户难以检测的问题，为自然语言编程提供可靠性支持，尤其是对编程知识有限的用户。

Method: 提出了一个形式化查询语言，用于捕获用户意图，并开发了一个验证系统Astrogator，通过符号解释器验证LLM生成的代码是否符合用户意图。

Result: 在21个代码生成任务的测试中，验证器成功验证了83%的正确代码，并识别了92%的错误代码。

Conclusion: 该方法有效提升了LLM生成代码的可信度，为自然语言编程提供了实用工具。

Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by
taking natural language descriptions and generating code based on it. However,
LLMs often generate incorrect code that users need to fix and the literature
suggests users often struggle to detect these errors. In this work we seek to
offer formal guarantees of correctness to LLM generated code; such guarantees
could improve the experience of using AI Code Assistants and potentially enable
natural language programming for users with little or no programming knowledge.
To address this challenge we propose to incorporate a formal query language
that can represent a user's intent in a formally defined but natural
language-like manner that a user can confirm matches their intent. Then, using
such a query we propose to verify LLM generated code to ensure it matches the
user's intent. We implement these ideas in our system, Astrogator, for the
Ansible programming language which includes such a formal query language, a
calculus for representing the behavior of Ansible programs, and a symbolic
interpreter which is used for the verification. On a benchmark suite of 21
code-generation tasks, our verifier is able to verify correct code in 83% of
cases and identify incorrect code in 92%.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [19] [Design and Reliability of a User Space Write-Ahead Log in Rust](https://arxiv.org/abs/2507.13062)
*Vitor K. F. Pellegatti,Gustavo M. D. Vieira*

Main category: cs.OS

TL;DR: 论文探讨了Rust语言如何用于构建高性能、可靠的写前日志（WAL）原型，并发现其性能接近稳定存储设备的预期水平。


<details>
  <summary>Details</summary>
Motivation: 写前日志是计算机科学中重要的容错技术，但确保其高性能和可靠性具有挑战性。本文旨在探索Rust语言构建WAL原型的潜力。

Method: 作者在Rust中创建了一个用户空间WAL原型，利用Rust的易用性、紧凑性和丰富的库支持。

Result: 原型表现出极低的开销，性能接近稳定存储设备的预期水平，表明Rust适合用于构建高效WAL。

Conclusion: Rust语言在实现高性能和可靠的WAL方面具有显著优势，未来可进一步探索其潜力。

Abstract: Write-ahead logs (WALs) are a fundamental fault-tolerance technique found in
many areas of computer science. WALs must be reliable while maintaining high
performance, because all operations will be written to the WAL to ensure their
stability. Without reliability a WAL is useless, because its utility is tied to
its ability to recover data after a failure. In this paper we describe our
experience creating a prototype user space WAL in Rust. We observed that Rust
is easy to use, compact and has a very rich set of libraries. More importantly,
we have found that the overhead is minimal, with the WAL prototype operating at
basically the expected performance of the stable memory device.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2507.12910)
*Xudong Wang,Hongyang Du,Lei Feng,Kaibin Huang*

Main category: cs.NI

TL;DR: 研究了基于RSMA和无人机MEC系统的联合优化问题，提出了一种生成式AI增强的DRL框架以提升能效。


<details>
  <summary>Details</summary>
Motivation: 6G时代对低延迟计算的需求推动了低空MEC系统的应用，但频谱有限导致地面终端上行干扰严重。

Method: 采用生成式AI增强的DRL框架，结合扩散模型和改进的RSMA解码策略，优化无人机的3D轨迹和资源分配。

Result: 仿真表明该方法在能效上优于基线方法，RSMA与扩散模型的结合效果显著。

Conclusion: 提出的方法有效提升了低空MEC系统的性能，为6G低延迟计算提供了可行方案。

Abstract: The growing demand for low-latency computing in 6G is driving the use of
UAV-based low-altitude mobile edge computing (MEC) systems. However, limited
spectrum often leads to severe uplink interference among ground terminals
(GTs). In this paper, we investigate a rate-splitting multiple access
(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists
multiple GTs in concurrently offloading their tasks over a shared uplink. We
formulate a joint optimization problem involving the UAV 3D trajectory, RSMA
decoding order, task offloading decisions, and resource allocation, aiming to
mitigate multi-user interference and maximize energy efficiency. Given the high
dimensionality, non-convex nature, and dynamic characteristics of this
optimization problem, we propose a generative AI-enhanced deep reinforcement
learning (DRL) framework to solve it efficiently. Specifically, we embed a
diffusion model into the actor network to generate high-quality action samples,
improving exploration in hybrid action spaces and avoiding local optima. In
addition, a priority-based RSMA decoding strategy is designed to facilitate
efficient successive interference cancellation with low complexity. Simulation
results demonstrate that the proposed method for low-altitude MEC systems
outperforms baseline methods, and that integrating GDM with RSMA can achieve
significantly improved energy efficiency performance.

</details>


### [21] [RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents](https://arxiv.org/abs/2507.13140)
*Kuiyuan Ding,Caili Guo,Yang Yang,Jianzhang Guo*

Main category: cs.NI

TL;DR: 该论文提出了一种名为RIDAS的多智能体框架，用于优化6G网络中的人工智能集成，通过表示驱动智能体（RDAs）和意图驱动智能体（IDA）实现高效的资源分配。


<details>
  <summary>Details</summary>
Motivation: 6G网络需将AI深度集成到无线接入网（RAN）中，以满足严格的QoS和资源效率要求，但现有解决方案难以弥合高层用户意图与低层参数配置之间的差距。

Method: RIDAS框架由RDAs和IDA组成。RDAs提供可调参数接口，IDA采用基于大型语言模型（LLM）的两阶段规划方案（带宽预分配和重分配），将用户意图和系统状态映射到最优RDA配置。

Result: 实验表明，RIDAS在同等QoS约束下比WirelessAgent支持多44.71%的用户，验证了其在AI RAN环境中高效捕捉用户意图和分配资源的能力。

Conclusion: RIDAS通过多智能体协作有效解决了6G网络中用户意图与资源分配之间的优化问题，展现了在实际应用中的显著优势。

Abstract: Sixth generation (6G) networks demand tight integration of artificial
intelligence (AI) into radio access networks (RANs) to meet stringent quality
of service (QoS) and resource efficiency requirements. Existing solutions
struggle to bridge the gap between high level user intents and the low level,
parameterized configurations required for optimal performance. To address this
challenge, we propose RIDAS, a multi agent framework composed of representation
driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open
interface with tunable control parameters (rank and quantization bits, enabling
explicit trade) offs between distortion and transmission rate. The IDA employs
a two stage planning scheme (bandwidth pre allocation and reallocation) driven
by a large language model (LLM) to map user intents and system state into
optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\%
more users than WirelessAgent under equivalent QoS constraints. These results
validate ability of RIDAS to capture user intent and allocate resources more
efficiently in AI RAN environments.

</details>


### [22] [Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering](https://arxiv.org/abs/2507.13179)
*Ziyu Zhong,Hector A Caltenco,Björn Landfeldt,Günter Alce*

Main category: cs.NI

TL;DR: 6G网络将支持XR应用的计算卸载，降低设备能耗并实现更小设备设计。


<details>
  <summary>Details</summary>
Motivation: 6G网络的低延迟和边缘计算基础设施为XR应用的计算卸载提供了新可能。

Method: 通过将计算密集型功能（如渲染）从用户设备迁移到网络，实现计算卸载。

Result: 降低用户设备的电池需求，并可能设计更小尺寸的设备。

Conclusion: 6G网络的计算卸载能力为XR应用提供了更高效的解决方案。

Abstract: As 6G networks are developed and defined, offloading of XR applications is
emerging as one of the strong new use cases. The reduced 6G latency coupled
with edge processing infrastructure will for the first time provide a realistic
offloading scenario in cellular networks where several computationally
intensive functions, including rendering, can migrate from the user device and
into the network. A key advantage of doing so is the lowering of the battery
needs in the user devices and the possibility to design new devices with
smaller form factors.

</details>


### [23] [Bidirectional Age of Incorrect Information: A Performance Metric for Status Updates in Virtual Dynamic Environments](https://arxiv.org/abs/2507.13312)
*Chiara Schiavo,Manuele Favero,Alessandro Buratto,Leonardo Badia*

Main category: cs.NI

TL;DR: 提出了双向错误信息年龄（BAoII）来量化虚拟动态环境中实体由于信息错误或过时而付出的时间相关代价，并通过马尔可夫链模型推导了长期BAoII的闭式解和最优更新策略。


<details>
  <summary>Details</summary>
Motivation: 在元宇宙和数字孪生等虚拟动态环境中，保持实体表征的准确性和实时性对无缝交互和系统可靠性至关重要。

Method: 扩展了错误信息年龄的概念，提出BAoII来捕捉双向信息交换的需求，并使用连续时间马尔可夫链模型进行建模。

Result: 推导了长期BAoII的闭式解，确定了最优更新策略的传输成本阈值，并通过数值模拟验证了模型。

Conclusion: BAoII有助于评估系统性能，对元宇宙和数字孪生中的实时协作具有重要意义。

Abstract: Virtual dynamic environments (VDEs) such as the Metaverse and digital twins
(DTs) require proper representation of the interacting entities to map their
characteristics within the simulated or augmented space. Keeping these
representations accurate and up-to-date is crucial for seamless interaction and
system reliability. In this paper, we propose bidirectional age of incorrect
information (BAoII) to address this aspect. BAoII quantifies the time-dependent
penalty paid by an entity in a VDE due to incorrect or outdated knowledge about
itself and the overall dynamically changing space. This extends the concept of
age of incorrect information for a bidirectional information exchange,
capturing that a VDE requires mutual awareness of the entity's own
representation, measured in the virtual space, and what the other entities
share about their representations. Using a continuous-time Markov chain model,
we derive a closed-form expression for long-term BAoII and identify a
transmission cost threshold for optimal update strategies. We describe a
trade-off between communication cost and information freshness and validate our
model through numerical simulations, demonstrating the impact of BAoII on
evaluating system performance and highlighting its relevance for real-time
collaboration in the Metaverse and DTs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [Dependency Pairs for Expected Innermost Runtime Complexity and Strong Almost-Sure Termination of Probabilistic Term Rewriting](https://arxiv.org/abs/2507.12918)
*Jan-Christoph Kassing,Leon Spitzer,Jürgen Giesl*

Main category: cs.LO

TL;DR: 提出了第一个依赖对框架，用于分析概率项重写系统的预期复杂度，并证明其正或强几乎必然终止性，工具AProVE验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 概率项重写系统的自动复杂度分析尚未充分探索，现有技术无法有效解决强几乎必然终止性问题。

Method: 结合依赖对框架，分析预期复杂度并证明强几乎必然终止性。

Result: 在AProVE工具中实现，验证了该框架相比现有技术的优势。

Conclusion: 该框架为概率项重写系统提供了一种有效的自动复杂度分析与终止性证明方法。

Abstract: The dependency pair (DP) framework is one of the most powerful techniques for
automatic termination and complexity analysis of term rewrite systems. While
DPs were extended to prove almost-sure termination of probabilistic term
rewrite systems (PTRSs), automatic complexity analysis for PTRSs is largely
unexplored. We introduce the first DP framework for analyzing expected
complexity and for proving positive or strong almost-sure termination (SAST) of
innermost rewriting with PTRSs, i.e., finite expected runtime. We implemented
our framework in the tool AProVE and demonstrate its power compared to existing
techniques for proving SAST.

</details>


### [25] [Cyclic proof theory of positive inductive definitions](https://arxiv.org/abs/2507.13057)
*Gianluca Curzi,Lukas Melgaard*

Main category: cs.LO

TL;DR: 该论文研究了μPA的循环证明系统，证明了循环证明和归纳证明在证明理论强度上相同，并利用了Impredicative二阶算术子系统Π¹₂-CA₀的保守性。


<details>
  <summary>Details</summary>
Motivation: 探索循环证明系统与非良基证明理论在算术理论中的应用，特别是μPA与Π¹₂-CA₀的关系。

Method: 将循环证明转化为基于Sprenger和Dam系统的带注释变体，并在Π¹₂-CA₀中形式化证明。

Result: 循环证明与归纳证明在μPA中具有相同的证明能力，带注释与普通循环证明等价。

Conclusion: 该研究为非良基证明理论在算术分析中提供了进一步的进展，延续了Simpson和Das等人的工作。

Abstract: We study cyclic proof systems for $\mu\mathsf{PA}$, an extension of Peano
arithmetic by positive inductive definitions that is arithmetically equivalent
to the (impredicative) subsystem of second-order arithmetic
$\Pi^1_2$-$\mathsf{CA}_0$ by M\"{o}llefeld. The main result of this paper is
that cyclic and inductive $\mu\mathsf{PA}$ have the same proof-theoretic
strength. First, we translate cyclic proofs into an annotated variant based on
Sprenger and Dam's systems for first-order $\mu$-calculus, whose stronger
validity condition allows for a simpler proof of soundness. We then formalise
this argument within $\Pi^1_2$-$\mathsf{CA}_0$, leveraging M\"{o}llerfeld's
conservativity properties. To this end, we build on prior work by Curzi and Das
on the reverse mathematics of the Knaster-Tarski theorem. As a byproduct of our
proof methods we show that, despite the stronger validity condition, annotated
and "plain" cyclic proofs for $\mu\mathsf{PA}$ prove the same theorems. This
work represents a further step in the non-wellfounded proof-theoretic analysis
of theories of arithmetic via impredicative fragments of second-order
arithmetic, an approach initiated by Simpson's Cyclic Arithmetic, and continued
by Das and Melgaard in the context of arithmetical inductive definitions.

</details>


### [26] [Monotone weak distributive laws over the lifted powerset monad in categories of algebras](https://arxiv.org/abs/2507.13058)
*Quentin Aristote*

Main category: cs.LO

TL;DR: 研究如何在代数范畴中自动从集合的单调弱分配律推广到紧致豪斯多夫空间的部分成功案例，并进一步刻画了幂集幺半群的单调弱分配律的存在条件。


<details>
  <summary>Details</summary>
Motivation: 观察到集合与紧致豪斯多夫空间中两层非确定性的单调弱分配律相似性，探索其自动推广的可能性。

Method: 分析单调弱分配律在代数范畴中的弱提升条件，并探讨幂集幺半群上的分配律存在性。

Result: 部分成功地将集合的分配律推广到紧致豪斯多夫空间，但无法推广到其他代数范畴；刻画了分配律存在的具体条件，并展示了某些成功与失败的案例。

Conclusion: 单调弱分配律在某些特定代数范畴中存在，但推广性有限，需具体场景具体分析。

Abstract: Noticing the similarity between the monotone weak distributive laws combining
two layers of nondeterminism in sets and in compact Hausdorff spaces, we study
whether the latter law can be obtained automatically as a weak lifting of the
former. This holds partially, but does not generalize to other categories of
algebras: we then characterize when exactly monotone weak distributive laws
over powerset monads in categories of algebras exist, exhibiting a law
combining probabilities and non-determinism in compact Hausdorff spaces and
showing on the other hand that such laws do not exist in a lot of other cases.

</details>


### [27] [Impact and Performance of Randomized Test-Generation using Prolog](https://arxiv.org/abs/2507.13178)
*Marcus Gelderie,Maximilian Luff,Maximilian Peltzer*

Main category: cs.LO

TL;DR: 随机生成测试输入序列的研究，探讨Prolog与随机化的结合对测试性能的影响。


<details>
  <summary>Details</summary>
Motivation: Prolog适合生成逻辑复杂的测试序列，但面临大量或无限测试集的问题，随机化是自然选择。

Method: 提出两种随机化策略：一种基于标准Prolog语义，另一种修改SLD选择函数；使用马尔可夫链分析均值性能。

Result: 分析到达测试用例的平均时间和生成测试用例的平均数量，并进行实证比较。

Conclusion: 两种方法各有优劣，为Prolog测试生成提供了新思路。

Abstract: We study randomized generation of sequences of test-inputs to a system using
Prolog. Prolog is a natural fit to generate test-sequences that have complex
logical inter-dependent structure. To counter the problems posed by a large (or
infinite) set of possible tests, randomization is a natural choice. We study
the impact that randomization in conjunction with SLD resolution have on the
test performance. To this end, this paper proposes two strategies to add
randomization to a test-generating program. One strategy works on top of
standard Prolog semantics, whereas the other alters the SLD selection function.
We analyze the mean time to reach a test-case, and the mean number of generated
test-cases in the framework of Markov chains. Finally, we provide an additional
empirical evaluation and comparison between both approaches. Under
consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [28] [Just Verification of Mutual Exclusion Algorithms](https://arxiv.org/abs/2507.13198)
*Rob van Glabbeek,Bas Luttik,Myrthe Spronck*

Main category: cs.LO

TL;DR: 通过模型检查验证多种互斥算法的正确性，涉及原子和非原子共享读/写寄存器，并使用justness作为完整性标准。


<details>
  <summary>Details</summary>
Motivation: 验证互斥算法的正确性并对潜在问题提出改进。

Method: 利用模型检查技术，考虑不同的并发关系模型，并使用justness消除伪反例。

Result: 展示了多个算法违反正确性属性的执行示例，并提出了改进建议。

Conclusion: 模型检查和justness标准可有效验证互斥算法的正确性，并为改进提供了依据。

Abstract: We verify the correctness of a variety of mutual exclusion algorithms through
model checking. We look at algorithms where communication is via shared
read/write registers, where those registers can be atomic or non-atomic. For
the verification of liveness properties, it is necessary to assume a
completeness criterion to eliminate spurious counterexamples. We use justness
as completeness criterion. Justness depends on a concurrency relation; we
consider several such relations, modelling different assumptions on the working
of the shared registers. We present executions demonstrating the violation of
correctness properties by several algorithms, and in some cases suggest
improvements.

</details>


### [29] [Solving SAT By Computing A Stable Set Of Points In Clusters](https://arxiv.org/abs/2507.13282)
*Eugene Goldberg*

Main category: cs.LO

TL;DR: 介绍了稳定点集（SSP）的概念及其与CNF公式不可满足性的关系，提出了一种通过聚类大幅提升计算效率的方法，并讨论了其在实际SAT算法和并行计算中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨如何高效计算SSP以解决CNF公式不可满足性问题，尤其是面对大规模SSP时的计算挑战。

Method: 提出一种基于聚类的计算方法，将SSP划分为多个子集并行处理，从而提升计算效率。

Result: 证明了在聚类基础上计算SSP的可行性，并指出该方法不仅能优化算法效率，还促进了并行计算的应用。

Conclusion: 通过聚类计算SSP是一种有前景的方法，可显著提升SAT求解效率并支持并行化实现。

Abstract: Earlier we introduced the notion of a stable set of points (SSP). We proved
that a CNF formula is unsatisfiable iff there is a set of points (i.e. complete
assignments) that is stable with respect to this formula. Experiments showed
that SSPs for CNF formulas of practical interest are very large. So computing
an SSP for a CNF formula point by point is, in general, infeasible. In this
report, we show how an SSP can be computed in clusters, each cluster being a
large set of points that are processed simultaneously. The appeal of computing
SSPs is twofold. First, it allows one to better take into account formula
structure and hence, arguably, design more efficient SAT algorithms. Second,
SAT solving by SSPs facilitates parallel computing.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [30] ["How to Explore Biases in Speech Emotion AI with Users?" A Speech-Emotion-Acting Study Exploring Age and Language Biases](https://arxiv.org/abs/2507.12580)
*Josephine Beatrice Skovbo Borre,Malene Gorm Wold,Sara Kjær Rasmussen,Ilhan Aslan*

Main category: cs.HC

TL;DR: 研究探讨了年龄和语言如何影响情感的刻意语音表达，关注了青少年和55岁以上成年人在语音情感识别中的表现，发现语言和年龄对模型解释无显著差异。


<details>
  <summary>Details</summary>
Motivation: 旨在解决语音情感识别系统中对青少年和老年人群体以及双语表达的忽视问题。

Method: 开发了结合定制用户界面和实时预测的实验范式，要求参与者刻意表达四种情感目标。

Result: 发现语言和年龄组间无显著差异，模型具有一定跨语言和年龄的鲁棒性，但高唤醒情感识别存在局限。

Conclusion: 呼吁开发更以人为中心的情感识别模型，揭示情感表达与机器解读间的鸿沟。

Abstract: This study explores how age and language shape the deliberate vocal
expression of emotion, addressing underexplored user groups, Teenagers (N = 12)
and Adults 55+ (N = 12), within speech emotion recognition (SER). While most
SER systems are trained on spontaneous, monolingual English data, our research
evaluates how such models interpret intentionally performed emotional speech
across age groups and languages (Danish and English). To support this, we
developed a novel experimental paradigm combining a custom user interface with
a backend for real-time SER prediction and data logging. Participants were
prompted to hit visual targets in valence-arousal space by deliberately
expressing four emotion targets. While limitations include some reliance on
self-managed voice recordings and inconsistent task execution, the results
suggest contrary to expectations, no significant differences between language
or age groups, and a degree of cross-linguistic and age robustness in model
interpretation. Though some limitations in high-arousal emotion recognition
were evident. Our qualitative findings highlight the need to move beyond
system-centered accuracy metrics and embrace more inclusive, human-centered SER
models. By framing emotional expression as a goal-directed act and logging the
real-time gap between human intent and machine interpretation, we expose the
risks of affective misalignment.

</details>


### [31] [NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting](https://arxiv.org/abs/2507.12621)
*Kuangshi Ai,Kaiyuan Tang,Chaoli Wang*

Main category: cs.HC

TL;DR: NLI4VolVis 是一个通过自然语言交互的体数据可视化系统，结合了多视图语义分割和视觉语言模型，提供高效的语义级交互和实时编辑功能。


<details>
  <summary>Details</summary>
Motivation: 传统体可视化方法存在交互设计僵化和计算成本高的问题，而现有方法对非专家用户不友好且缺乏语义级交互支持。

Method: 系统整合了多视图语义分割、视觉语言模型和多智能体大语言模型架构，通过功能调用工具解析用户意图并执行任务。

Result: NLI4VolVis 实现了开放词汇查询、实时场景编辑、最佳视图选择和 2D 风格化，并通过案例研究和用户研究验证了其可用性。

Conclusion: 系统显著提升了体数据探索的可访问性和易用性，建议读者参考案例研究和演示视频了解更多细节。

Abstract: Traditional volume visualization (VolVis) methods, like direct volume
rendering, suffer from rigid transfer function designs and high computational
costs. Although novel view synthesis approaches enhance rendering efficiency,
they require additional learning effort for non-experts and lack support for
semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an
interactive system that enables users to explore, query, and edit volumetric
scenes using natural language. NLI4VolVis integrates multi-view semantic
segmentation and vision-language models to extract and understand semantic
components in a scene. We introduce a multi-agent large language model
architecture equipped with extensive function-calling tools to interpret user
intents and execute visualization tasks. The agents leverage external tools and
declarative VolVis commands to interact with the VolVis engine powered by 3D
editable Gaussians, enabling open-vocabulary object querying, real-time scene
editing, best-view selection, and 2D stylization. We validate our system
through case studies and a user study, highlighting its improved accessibility
and usability in volumetric data exploration. We strongly recommend readers
check our case studies, demo video, and source code at
https://nli4volvis.github.io/.

</details>


### [32] [Design Patterns of Human-AI Interfaces in Healthcare](https://arxiv.org/abs/2507.12721)
*Rui Sheng,Chuhan Shi,Sobhan Lotfi,Shiyi Liu,Adam Perer,Huamin Qu,Furui Cheng*

Main category: cs.HC

TL;DR: 本文提出了一套系统化的设计指南，用于医疗领域的人机交互界面，总结了12种设计模式，并通过访谈和研讨会评估了其效果。


<details>
  <summary>Details</summary>
Motivation: 设计医疗领域的人机交互界面具有挑战性，需要系统化的指导来支持实践和研究。

Method: 通过总结常见信息实体的设计模式，访谈12位医疗专业人士，并举办14人参与的在线研讨会进行评估。

Result: 提出了12种设计模式，并探讨了其在不同应用领域的通用性、局限性和未来研究方向。

Conclusion: 该研究为医疗领域的人机交互设计提供了系统化的指导，同时也指出了未来扩展的可能性和局限性。

Abstract: Human-AI interfaces play a crucial role in advancing practices and research
within the healthcare domain. However, designing such interfaces presents a
substantial challenge for designers. In this paper, we propose systematic
guidance for designing human-AI interfaces in typical healthcare scenarios by
summarizing the design patterns for presenting and interacting with common
information entities. To deepen our understanding of these 12 design patterns,
we interviewed 12 healthcare professionals to explore potential usage scenarios
and important considerations. Furthermore, we conducted workshops with 14
participants recruited online to evaluate our design patterns. Finally, we
discussed the generalizability of the design patterns to other application
domains, the limitations, and the future work.

</details>


### [33] [An Age-based Study into Interactive Narrative Visualization Engagement](https://arxiv.org/abs/2507.12734)
*Nina Errey,Yi Chen,Yu Dong,Quang Vinh Nguyen,Xiaoru Yuan,Tuck Wah Leong,Christy Jie Liang*

Main category: cs.HC

TL;DR: 研究发现，受众年龄影响其对数字媒体的参与度，尤其是在交互式叙事可视化中。年轻群体比年长群体更投入和理解。


<details>
  <summary>Details</summary>
Motivation: 探讨受众年龄如何影响对交互式叙事可视化的参与度，目前这一因素常被忽视。

Method: 通过问卷调查和实验比较不同年龄群体的参与度，并进行定性分析。

Result: 年长群体参与度略低于年轻群体，且对交互式叙事的理解和术语认知较差。

Conclusion: 为设计师提供建议，以更包容性地针对不同年龄群体设计交互式叙事可视化。

Abstract: Research has shown that an audiences' age impacts their engagement in digital
media. Interactive narrative visualization is an increasingly popular form of
digital media that combines data visualization and storytelling to convey
important information. However, audience age is often overlooked by interactive
narrative visualization authors. Using an established visualization engagement
questionnaire, we ran an empirical experiment where we compared end-user
engagement to audience age. We found a small difference in engagement scores
where older age cohorts were less engaged than the youngest age cohort. Our
qualitative analysis revealed that the terminology and overall understanding of
interactive narrative patterns integrated into narrative visualization was more
apparent in the feedback from younger age cohorts relative to the older age
cohorts. We conclude this paper with a series of recommendations for authors of
interactive narrative visualization on how to design inclusively for audiences
according to their age.

</details>


### [34] [Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land](https://arxiv.org/abs/2507.12741)
*Lotfi El Hafi,Kazuma Onishi,Shoichi Hasegawa,Akira Oyama,Tomochika Ishikawa,Masashi Osada,Carl Tornberg,Ryoma Kado,Kento Murata,Saki Hashimoto,Sebastian Carrera Villalobos,Akira Taniguchi,Gustavo Alfonso Garcia Ricardez,Yoshinobu Hagiwara,Tatsuya Aoki,Kensuke Iwata,Takato Horii,Yukiko Horikawa,Takahiro Miyashita,Tadahiro Taniguchi,Hiroshi Ishiguro*

Main category: cs.HC

TL;DR: 研究了公众对完全自主的赛博格化身（CAs）的看法及其社会影响，通过大规模演示和调查发现，尽管人们对CAs在日常生活和工作中的物理支持感兴趣，但对任务执行的可靠性存在担忧。


<details>
  <summary>Details</summary>
Motivation: 探讨完全自主的赛博格化身在现实中的社会接受度和潜在影响，以推动其进一步发展。

Method: 在大阪举行的19天公共活动中，展示完全自主和半自主的CAs，并收集了2,285名参与者的反馈，其中333人详细调查了对完全自主CAs的看法。

Result: 公众对CAs在日常生活和工作中的物理支持感兴趣，但对任务执行的可靠性存在担忧，而对成本和人机交互的担忧较少。

Conclusion: 完全自主的CAs在实际应用中具有潜力，但需提高任务可靠性以增强公众接受度。

Abstract: Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,
enabling individuals to overcome physical limitations through virtual agents
and robotic assistants. While semi-autonomous CAs intermittently require human
teleoperation and supervision, the deployment of fully autonomous CAs remains a
challenge. This study evaluates public perception and potential social impacts
of fully autonomous CAs for physical support in daily life. To this end, we
conducted a large-scale demonstration and survey during Avatar Land, a 19-day
public event in Osaka, Japan, where fully autonomous robotic CAs, alongside
semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we
analyzed responses from 2,285 visitors who engaged with various CAs, including
a subset of 333 participants who interacted with fully autonomous CAs and
shared their perceptions and concerns through a survey questionnaire. The
survey results indicate interest in CAs for physical support in daily life and
at work. However, concerns were raised regarding task execution reliability. In
contrast, cost and human-like interaction were not dominant concerns. Project
page: https://lotfielhafi.github.io/FACA-Survey/.

</details>


### [35] [PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts](https://arxiv.org/abs/2507.12749)
*Xumeng Wang,Xiangxuan Zhang,Zhiqi Gao,Shuangcheng Jiao,Yuxin Ma*

Main category: cs.HC

TL;DR: 本文提出了一种感知模拟模型，用于评估图表的感知效果，并集成到一个原型界面PatternSight中，帮助优化图表设计。


<details>
  <summary>Details</summary>
Motivation: 可视化工具降低了图表创作的难度，但作者缺乏感知理论的支持，难以评估图表效果。

Method: 提出感知模拟模型，预测观众可能注意的图形模式，并将其嵌入PatternSight界面辅助设计优化。

Result: 模型能模拟多数观众的感知行为，PatternSight有效辅助优化图表设计。

Conclusion: 感知模拟模型和PatternSight为图表作者提供了有效的设计和优化工具。

Abstract: The boom in visualization generation tools has significantly lowered the
threshold for chart authoring. Nevertheless, chart authors with an insufficient
understanding of perceptual theories may encounter difficulties in evaluating
the effectiveness of chart representations, thereby struggling to identify the
appropriate chart design to convey the intended data patterns. To address this
issue, we propose a perception simulation model that can assess the perceptual
effectiveness of charts by predicting graphical patterns that chart viewers are
likely to notice. The perception simulation model integrates perceptual theory
into visual feature extraction of chart elements to provide interpretable model
outcomes. Human perceptual results proved that the outcome of our model can
simulate the perceptual grouping behaviors of most chart viewers and cover
diverse perceptual results. We also embed the model into a prototype interface
called PatternSight to facilitate chart authors in assessing whether the chart
design can satisfy their pattern representation requirements as expected and
determining feasible improvements of visual design. According to the results of
a user experiment, PatternSight can effectively assist chart authors in
optimizing chart design for representing data patterns.

</details>


### [36] [Autonomy for Older Adult-Agent Interaction](https://arxiv.org/abs/2507.12767)
*Jiaxin An*

Main category: cs.HC

TL;DR: 论文探讨了AI代理在老年人护理中的自主性问题，提出了四个关键维度，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口老龄化，AI代理可能成为支持老年人护理的工具，但其如何与老年人的自主偏好保持一致仍是关键问题。

Method: 论文基于跨学科的自主性概念，分析了老年人的四个自主性维度，并提出了三项研究方向。

Result: 提出了包括社会责任感、任务视角下的代理自主性操作化及自主性测量开发在内的未来研究方向。

Conclusion: 为AI代理在老年人护理中如何更好地尊重和实现自主性提供了理论框架和研究方向。

Abstract: As the global population ages, artificial intelligence (AI)-powered agents
have emerged as potential tools to support older adults' caregiving. Prior
research has explored agent autonomy by identifying key interaction stages in
task processes and defining the agent's role at each stage. However, ensuring
that agents align with older adults' autonomy preferences remains a critical
challenge. Drawing on interdisciplinary conceptualizations of autonomy, this
paper examines four key dimensions of autonomy for older adults:
decision-making autonomy, goal-oriented autonomy, control autonomy, and social
responsibility autonomy. This paper then proposes the following research
directions: (1) Addressing social responsibility autonomy, which concerns the
ethical and social implications of agent use in communal settings; (2)
Operationalizing agent autonomy from the task perspective; and (3) Developing
autonomy measures.

</details>


### [37] [Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication](https://arxiv.org/abs/2507.13052)
*Tianyu Song,Feng Li,Yuan Bi,Angelos Karlas,Amir Yousefi,Daniela Branzan,Zhongliang Jiang,Ulrich Eck,Nassir Navab*

Main category: cs.HC

TL;DR: 本文介绍了智能虚拟超声医师（IVS）在扩展现实（XR）中的应用，通过整合大语言模型和机器人控制，提升医-机-患交互的效率和信任。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注患者-机器人或医生-机器人交互，但智能虚拟超声医师在沟通医-机-患三方的潜力尚未充分探索。

Method: 开发了一个基于扩展现实的对话虚拟代理，整合大语言模型、语音转文本、文本转语音和机器人控制技术。

Result: 系统提升了机器人超声采集的效率和清晰度，增强了医患交互的可及性和信任。

Conclusion: 该研究为探索智能虚拟超声医师如何弥合医-机-患沟通缺口迈出了第一步，同时改善了患者体验和对机器人超声的接受度。

Abstract: The advancement and maturity of large language models (LLMs) and robotics
have unlocked vast potential for human-computer interaction, particularly in
the field of robotic ultrasound. While existing research primarily focuses on
either patient-robot or physician-robot interaction, the role of an intelligent
virtual sonographer (IVS) bridging physician-robot-patient communication
remains underexplored. This work introduces a conversational virtual agent in
Extended Reality (XR) that facilitates real-time interaction between
physicians, a robotic ultrasound system(RUS), and patients. The IVS agent
communicates with physicians in a professional manner while offering empathetic
explanations and reassurance to patients. Furthermore, it actively controls the
RUS by executing physician commands and transparently relays these actions to
the patient. By integrating LLM-powered dialogue with speech-to-text,
text-to-speech, and robotic control, our system enhances the efficiency,
clarity, and accessibility of robotic ultrasound acquisition. This work
constitutes a first step toward understanding how IVS can bridge communication
gaps in physician-robot-patient interaction, providing more control and
therefore trust into physician-robot interaction while improving patient
experience and acceptance of robotic ultrasound.

</details>


### [38] ["What do you expect? You're part of the internet": Analyzing Celebrities' Experiences as Usees of Deepfake Technology](https://arxiv.org/abs/2507.13065)
*John Twomey,Sarah Foley,Sarah Robinson,Michael Quayle,Matthew Peter Aylett,Conor Linehan,Gillian Murphy*

Main category: cs.HC

TL;DR: 论文探讨了名人女性如何应对非自愿合成的亲密影像（NSII）的问题，分析了她们在寻求法律和社会支持时面临的基础设施和社会障碍。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解名人女性如何构建自己成为深度伪造技术目标的经历，以及她们在寻求支持时遇到的障碍。

Method: 采用Baumer的“Usee”概念，分析八位名人女性和一位非二元性别个体的公开声明，探讨NSII对她们的危害及阻碍因素。

Result: 名人描述了被非自愿深度伪造带来的伤害和痛苦，以及基础设施和社会因素如何阻碍她们的维权行动。

Conclusion: 研究强调了利益相关者在深度伪造滥用基础设施中的作用，并提出了通过人机交互改进现有支持机制的建议，未来需挑战NSII背后的错误价值观。

Abstract: Deepfake technology is often used to create non-consensual synthetic intimate
imagery (NSII), mainly of celebrity women. Through Critical Discursive
Psychological analysis we ask; i) how celebrities construct being targeted by
deepfakes and ii) how they navigate infrastructural and social obstacles when
seeking recourse. In this paper, we adopt Baumers concept of Usees
(stakeholders who are non-consenting, unaware and directly targeted by
technology), to understand public statements made by eight celebrity women and
one non-binary individual targeted with NSII. Celebrities describe harms of
being non-consensually targeted by deepfakes and the distress of becoming aware
of these videos. They describe various infrastructural/social factors (e.g.
blaming/ silencing narratives and the industry behind deepfake abuse) which
hinder activism and recourse. This work has implications in recognizing the
roles of various stakeholders in the infrastructures underlying deepfake abuse
and the potential of human-computer interaction to improve existing recourses
for NSII. We also contribute to understanding how false beliefs online
facilitate deepfake abuse. Future work should involve interventions which
challenge the values and false beliefs which motivate NSII
creation/dissemination.

</details>


### [39] [On tangible user interfaces, humans and spatiality](https://arxiv.org/abs/2507.13167)
*Ehud Sharlin,Benjamin Watson,Yoshifumi Kitamura,Fumio Kishino,Yuichi Itoh*

Main category: cs.HC

TL;DR: 论文探讨了如何通过利用人类对物体的空间技能来设计成功的触觉用户界面（TUI），并提出了一套启发式方法。


<details>
  <summary>Details</summary>
Motivation: 触觉用户界面的成功依赖于人类对物体的空间技能，因此需要深入研究这些技能并将其应用于TUI设计。

Method: 通过分析人类与物理对象的关系及先前研究，提炼出一组观察结果，并将其转化为设计启发式方法，用于空间TUI的设计。

Result: 提出了空间TUI的概念，并通过启发式方法分析了几个现有的空间TUI，验证了方法的有效性。

Conclusion: 空间性是TUI设计的关键因素，提出的启发式方法为未来的TUI设计提供了实用指导。

Abstract: Like the prehistoric twig and stone, tangible user interfaces (TUIs) are
objects manipulated by humans. TUI success will depend on how well they exploit
spatiality, the intuitive spatial skills humans have with the objects they use.
In this paper we carefully examine the relationship between humans and physical
objects, and related previous research. From this examination we distill a set
of observations, and turn these into heuristics for incorporation of spatiality
into TUI application design, a cornerstone for their success. Following this
line of thought, we identify spatial TUIs, the subset of TUIs that mediate
interaction with shape, space and structure. We then examine several existing
spatial TUIs using our heuristics.

</details>


### [40] [Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item](https://arxiv.org/abs/2507.13235)
*Minghao Cai,Guher Gorgun,Carrie Demmans Epp*

Main category: cs.HC

TL;DR: 本研究探讨了使用项目难度参数作为在线学习平台中认知负荷测量代理的可行性，发现其能有效反映内在和外在负荷。


<details>
  <summary>Details</summary>
Motivation: 传统认知负荷测量依赖主观自我报告，研究者寻求更客观的方法。

Method: 利用项目反应理论提取项目难度参数，分析其与认知负荷理论的一致性。

Result: 项目难度参数能有效代表内在负荷，适用于学习游戏中的认知负荷建模。

Conclusion: 项目难度可作为认知负荷测量的有效代理，尤其是在在线学习环境中。

Abstract: Cognitive load is key to ensuring an optimal learning experience. However,
measuring the cognitive load of educational tasks typically relies on
self-report measures which has been criticized by researchers for being
subjective. In this study, we investigated the feasibility of using item
difficulty parameters as a proxy for measuring cognitive load in an online
learning platform. Difficulty values that were derived using item-response
theory were consistent with theories of how intrinsic and extraneous load
contribute to cognitive load. This finding suggests that we can use item
difficulty to represent intrinsic load when modelling cognitive load in
learning games.

</details>


### [41] [RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality](https://arxiv.org/abs/2507.13247)
*Ruohao Li,Jiawei Li,Jia Sun,Zhiqing Wu,Zisu Li,Ziyan Wang,Ge Lin Kan,Mingming Fan*

Main category: cs.HC

TL;DR: RemVerse使用AI和VR技术辅助老年人进行回忆活动，通过生成视觉线索和交互对话，有效触发和深化记忆，提升参与度和自主性。


<details>
  <summary>Details</summary>
Motivation: 城市化导致熟悉的环境消失，传统照片无法完全重建回忆内容，VR和AI技术提供了沉浸式环境和动态内容的能力。

Method: 设计RemVerse原型，结合生成模型和AI代理在VR环境中，提供AI生成的视觉线索和交互对话。

Result: 用户研究表明，RemVerse能有效触发、具体化和深化个人记忆，同时提高老年人的参与度和自主性。

Conclusion: 研究提出设计建议，使AI辅助的VR回忆活动对老年人更易用和吸引人。

Abstract: Reminiscence activities, which involve recalling and sharing past
experiences, have proven beneficial for improving cognitive function, mood, and
overall well-being. However, urbanization has led to the disappearance of
familiar environments, removing visual and audio cues for effective
reminiscence. While old photos can serve as visual cues to aid reminiscence, it
is challenging for people to reconstruct the reminisced content and environment
that are not in the photos. Virtual reality (VR) and artificial intelligence
(AI) offer the ability to reconstruct an immersive environment with dynamic
content and to converse with people to help them gradually reminisce. We
designed RemVerse, an AI-empowered VR prototype aimed to support reminiscence
activities. Integrating generative models and AI agent into a VR environment,
RemVerse helps older adults reminisce with AI-generated visual cues and
interactive dialogues. Our user study with 14 older adults showed that RemVerse
effectively supported reminiscence activities by triggering, concretizing, and
deepening personal memories, while fostering increased engagement and autonomy
among older adults. Based on our findings, we proposed design implications to
make reminiscence activities in AI-assisted VR more accessible and engaging for
older adults.

</details>


### [42] [FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD](https://arxiv.org/abs/2507.13309)
*Hanxiu 'Hazel' Zhu,Ruijia Chen,Yuhang Zhao*

Main category: cs.HC

TL;DR: 研究开发了FocusView，一种为ADHD患者定制的视频界面，显著减少分心，提升观看体验。


<details>
  <summary>Details</summary>
Motivation: ADHD患者在观看信息视频时面临注意力挑战，需要针对性的解决方案。

Method: 设计了FocusView界面，允许ADHD用户从多角度自定义视频，并通过12名ADHD参与者评估效果。

Result: FocusView显著减少分心，揭示了用户对分心的不同感知及定制偏好。

Conclusion: 研究提出了未来ADHD视频定制系统的设计考量。

Abstract: While videos have become increasingly prevalent in delivering information
across different educational and professional contexts, individuals with ADHD
often face attention challenges when watching informational videos due to the
dynamic, multimodal, yet potentially distracting video elements. To understand
and address this critical challenge, we designed \textit{FocusView}, a video
customization interface that allows viewers with ADHD to customize
informational videos from different aspects. We evaluated FocusView with 12
participants with ADHD and found that FocusView significantly improved the
viewability of videos by reducing distractions. Through the study, we uncovered
participants' diverse perceptions of video distractions (e.g., background music
as a distraction vs. stimulation boost) and their customization preferences,
highlighting unique ADHD-relevant needs in designing video customization
interfaces (e.g., reducing the number of options to avoid distraction caused by
customization itself). We further derived design considerations for future
video customization systems for the ADHD community.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition](https://arxiv.org/abs/2507.12498)
*Beizhen Zhao,Yifan Zhou,Sicheng Yu,Zijian Wang,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种基于小波分解的解耦优化框架，提升了3D高斯泼溅在复杂场景重建中的性能，解决了结构完整性和局部光照问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在复杂场景重建中存在结构不完整和局部光照不清晰的问题。

Method: 通过3D小波分解将点云分为高、低频分量，分别优化；低频分量通过体素化捕捉全局结构，高频分量结合重光照模块恢复细节。2D小波分解辅助训练图像模拟辐射变化。

Result: 在多个数据集上实现最优性能，超越现有方法。

Conclusion: 该方法通过解耦优化和小波分解，显著提升了3D场景重建的质量和真实感。

Abstract: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,
which effectively balances rendering quality, efficiency, and speed. However,
existing 3DGS approaches usually generate plausible outputs and face
significant challenges in complex scene reconstruction, manifesting as
incomplete holistic structural outlines and unclear local lighting effects. To
address these issues simultaneously, we propose a novel decoupled optimization
framework, which integrates wavelet decomposition into 3D Gaussian Splatting
and 2D sampling. Technically, through 3D wavelet decomposition, our approach
divides point clouds into high-frequency and low-frequency components, enabling
targeted optimization for each. The low-frequency component captures global
structural outlines and manages the distribution of Gaussians through
voxelization. In contrast, the high-frequency component restores intricate
geometric and textural details while incorporating a relight module to mitigate
lighting artifacts and enhance photorealistic rendering. Additionally, a 2D
wavelet decomposition is applied to the training images, simulating radiance
variations. This provides critical guidance for high-frequency detail
reconstruction, ensuring seamless integration of details with the global
structure. Extensive experiments on challenging datasets demonstrate our method
achieves state-of-the-art performance across various metrics, surpassing
existing approaches and advancing the field of 3D scene reconstruction.

</details>


### [44] [WaFusion: A Wavelet-Enhanced Diffusion Framework for Face Morph Generation](https://arxiv.org/abs/2507.12493)
*Seyed Rasoul Hosseini,Omid Ahmadieh,Jeremy Dawson,Nasser Nasrabadi*

Main category: cs.GR

TL;DR: WaFusion结合小波分解和扩散模型，高效生成高质量人脸融合图像，优于现有方法，提升生物识别系统安全性。


<details>
  <summary>Details</summary>
Motivation: 生物识别中的人脸融合技术对身份验证系统构成威胁，需要高效且高质量的方法来应对。

Method: WaFusion框架整合小波变换的结构细节捕捉能力和扩散模型的生成能力，减少融合图像的伪影。

Result: 在FERET等多个数据集上，WaFusion表现优异，生成了高分辨率且伪影少的融合图像，在APCER、BPCER和EER等指标上领先。

Conclusion: WaFusion为生物识别融合技术设定了新基准，提供了一种高效且先进的解决方案。

Abstract: Biometric face morphing poses a critical challenge to identity verification
systems, undermining their security and robustness. To address this issue, we
propose WaFusion, a novel framework combining wavelet decomposition and
diffusion models to generate high-quality, realistic morphed face images
efficiently. WaFusion leverages the structural details captured by wavelet
transforms and the generative capabilities of diffusion models, producing face
morphs with minimal artifacts. Experiments conducted on FERET, FRGC, FRLL, and
WVU Twin datasets demonstrate WaFusion's superiority over state-of-the-art
methods, producing high-resolution morphs with fewer artifacts. Our framework
excels across key biometric metrics, including the Attack Presentation
Classification Error Rate (APCER), Bona Fide Presentation Classification Error
Rate (BPCER), and Equal Error Rate (EER). This work sets a new benchmark in
biometric morph generation, offering a cutting-edge and efficient solution to
enhance biometric security systems.

</details>


### [45] [HairFormer: Transformer-Based Dynamic Neural Hair Simulation](https://arxiv.org/abs/2507.12600)
*Joy Xiaoji Zhang,Jingsen Zhu,Hanyu Chen,Steve Marschner*

Main category: cs.GR

TL;DR: 提出了一种基于Transformer的两阶段神经方法，用于模拟任意发型、体型和动作的头发动力学。


<details>
  <summary>Details</summary>
Motivation: 模拟适用于任意发型、体型和动作的头发动力学是一个关键挑战。

Method: 采用两阶段神经网络：一个用于预测静态披散形状，另一个通过跨注意力机制融合静态特征与运动输入生成动态效果。

Result: 实现了高保真且通用的头发动态模拟，并能实时推断静态和动态效果。

Conclusion: 该方法在复杂发型和运动中表现出广泛适应性，并能高效解决穿透问题。

Abstract: Simulating hair dynamics that generalize across arbitrary hairstyles, body
shapes, and motions is a critical challenge. Our novel two-stage neural
solution is the first to leverage Transformer-based architectures for such a
broad generalization. We propose a Transformer-powered static network that
predicts static draped shapes for any hairstyle, effectively resolving
hair-body penetrations and preserving hair fidelity. Subsequently, a dynamic
network with a novel cross-attention mechanism fuses static hair features with
kinematic input to generate expressive dynamics and complex secondary motions.
This dynamic network also allows for efficient fine-tuning of challenging
motion sequences, such as abrupt head movements. Our method offers real-time
inference for both static single-frame drapes and dynamic drapes over pose
sequences. Our method demonstrates high-fidelity and generalizable dynamic hair
across various styles, guided by physics-informed losses, and can resolve
penetrations even for complex, unseen long hairstyles, highlighting its broad
generalization.

</details>


### [46] [VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians](https://arxiv.org/abs/2507.12667)
*Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TL;DR: VolSegGS是一个基于高斯溅射的动态体场景交互式分割与跟踪框架，支持实时新视图合成和分割结果跟踪，适用于低计算需求的探索性可视化分析。


<details>
  <summary>Details</summary>
Motivation: 大规模时变模拟数据的可视化对领域科学家分析复杂现象至关重要，但现有方法多关注重建质量而非交互式探索功能。

Method: VolSegGS利用可变形3D高斯表示动态体场景，结合视无关颜色和亲和力场网络实现精确分割，并通过嵌入分割结果跟踪区域随时间变化。

Result: 实验结果展示了VolSegGS在多个时变数据集上的有效性，性能优于现有方法，支持实时交互和灵活分割与跟踪。

Conclusion: VolSegGS为时变体数据分析与可视化提供了低计算需求的高效解决方案，拓展了新的可能性。

Abstract: Visualization of large-scale time-dependent simulation data is crucial for
domain scientists to analyze complex phenomena, but it demands significant I/O
bandwidth, storage, and computational resources. To enable effective
visualization on local, low-end machines, recent advances in view synthesis
techniques, such as neural radiance fields, utilize neural networks to generate
novel visualizations for volumetric scenes. However, these methods focus on
reconstruction quality rather than facilitating interactive visualization
exploration, such as feature extraction and tracking. We introduce VolSegGS, a
novel Gaussian splatting framework that supports interactive segmentation and
tracking in dynamic volumetric scenes for exploratory visualization and
analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic
volumetric scene, allowing for real-time novel view synthesis. For accurate
segmentation, we leverage the view-independent colors of Gaussians for
coarse-level segmentation and refine the results with an affinity field network
for fine-level segmentation. Additionally, by embedding segmentation results
within the Gaussians, we ensure that their deformation enables continuous
tracking of segmented regions over time. We demonstrate the effectiveness of
VolSegGS with several time-varying datasets and compare our solutions against
state-of-the-art methods. With the ability to interact with a dynamic scene in
real time and provide flexible segmentation and tracking capabilities, VolSegGS
offers a powerful solution under low computational demands. This framework
unlocks exciting new possibilities for time-varying volumetric data analysis
and visualization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [47] [Geometric Theory of Ising Machines](https://arxiv.org/abs/2507.12626)
*Andrew G. Moore,Zachary Richey,Isaac K. Martin*

Main category: cs.ET

TL;DR: 本文提出了一种用于低温Ising机的数学设计理论，通过图解法可视化Ising电路的决策边界，并证明其是一类1-NN分类器的推广，同时消除能量景观中的局部极小值可转化为线性规划问题。


<details>
  <summary>Details</summary>
Motivation: 开发低温Ising机的数学设计理论，以解决能量函数设计的难题，实现高效分布式计算。

Method: 引入图解法可视化Ising电路的决策边界，并用于证明Ising电路与1-NN分类器的关系及局部极小值消除的线性规划形式。

Result: 证明Ising电路是一类1-NN分类器的推广，且消除局部极小值的问题可转化为线性规划。

Conclusion: 图解法为Ising机设计提供了理论工具，扩展了其应用范围和设计可能性。

Abstract: We contribute to the mathematical theory of the design of low temperature
Ising machines, a type of experimental probabilistic computing device
implementing the Ising model. Encoding the output of a function in the ground
state of a physical system allows efficient and distributed computation, but
the design of the energy function is a difficult puzzle. We introduce a
diagrammatic device that allows us to visualize the decision boundaries for
Ising circuits. It is then used to prove two results: (1) Ising circuits are a
generalization of 1-NN classifiers with a certain special structure, and (2)
Elimination of local minima in the energy landscape can be formulated as a
linear programming problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Building State Machine Replication Using Practical Network Synchrony](https://arxiv.org/abs/2507.12792)
*Yiliang Wan,Nitin Shivaraman,Akshaye Shenoi,Xiang Liu,Tao Luo,Jialin Li*

Main category: cs.DC

TL;DR: 现代数据中心系统可以通过提供强同步属性优化分布式协议性能，设计了一种名为Chora的新复制协议，显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代分布式协议通常假设网络是部分或完全异步的，但现代数据中心系统可以实现强同步性能，以提高分布式系统的效率。

Method: 结合内核旁路网络、多线程架构和宽松的轮次长度，设计了一种新的复制协议Chora，利用强同步性实现高效的并行复制。

Result: Chora协议在实验中比最新的单领导者和多领导者协议分别提升了255%和109%的吞吐量。

Conclusion: 通过优化网络同步性和协议设计，可以显著提升分布式系统的性能。

Abstract: Distributed systems, such as state machine replication, are critical
infrastructures for modern applications. Practical distributed protocols make
minimum assumptions about the underlying network: They typically assume a
partially synchronous or fully asynchronous network model. In this work, we
argue that modern data center systems can be designed to provide strong
synchrony properties in the common case, where servers move in synchronous
lock-step rounds. We prove this hypothesis by engineering a practical design
that uses a combination of kernel-bypass network, multithreaded architecture,
and loosened round length, achieving a tight round bound under 2us. Leveraging
our engineered networks with strong synchrony, we co-design a new replication
protocol, Chora. Chora exploits the network synchrony property to efficiently
pipeline multiple replication instances, while allowing all replicas to propose
in parallel without extra coordination. Through experiments, we show that Chora
achieves 255% and 109% improvement in throughput over state-of-the-art
single-leader and multi-leader protocols, respectively.

</details>


### [49] [Autonomous Resource Management in Microservice Systems via Reinforcement Learning](https://arxiv.org/abs/2507.12879)
*Yujun Zou,Nia Qi,Yingnan Deng,Zhihao Xue,Ming Gong,Wuyang Zhang*

Main category: cs.DC

TL;DR: 该论文提出了一种基于强化学习的微服务资源调度与优化方法，旨在解决传统微服务架构中资源分配不均、高延迟和吞吐量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着微服务系统中服务数量和负载的增加，高效调度和分配计算、内存和存储等资源成为研究关键问题。

Method: 采用基于强化学习的智能调度算法，通过代理与环境的交互不断优化资源分配策略。

Result: 实验结果表明，该方法在低负载和高并发条件下显著提升了系统响应速度和吞吐量，并优化了资源利用率和能耗。

Conclusion: 与传统静态资源分配方法相比，强化学习模型展现出更强的适应性和优化能力，能够在动态变化的负载和资源环境中实时调整策略，保持系统性能。

Abstract: This paper proposes a reinforcement learning-based method for microservice
resource scheduling and optimization, aiming to address issues such as uneven
resource allocation, high latency, and insufficient throughput in traditional
microservice architectures. In microservice systems, as the number of services
and the load increase, efficiently scheduling and allocating resources such as
computing power, memory, and storage becomes a critical research challenge. To
address this, the paper employs an intelligent scheduling algorithm based on
reinforcement learning. Through the interaction between the agent and the
environment, the resource allocation strategy is continuously optimized. In the
experiments, the paper considers different resource conditions and load
scenarios, evaluating the proposed method across multiple dimensions, including
response time, throughput, resource utilization, and cost efficiency. The
experimental results show that the reinforcement learning-based scheduling
method significantly improves system response speed and throughput under low
load and high concurrency conditions, while also optimizing resource
utilization and reducing energy consumption. Under multi-dimensional resource
conditions, the proposed method can consider multiple objectives and achieve
optimized resource scheduling. Compared to traditional static resource
allocation methods, the reinforcement learning model demonstrates stronger
adaptability and optimization capability. It can adjust resource allocation
strategies in real time, thereby maintaining good system performance in
dynamically changing load and resource environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [Transforming Football Data into Object-centric Event Logs with Spatial Context Information](https://arxiv.org/abs/2507.12504)
*Vito Chan,Lennart Ebert,Paul-Julius Hillmann,Christoffer Rubensson,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: 该论文提出了一个将足球数据转化为对象中心事件日志的框架，并展示其有效性，为足球分析提供首个对象中心事件日志示例。


<details>
  <summary>Details</summary>
Motivation: 现实世界的对象中心事件日志数量有限，需要通过进一步研究验证其有用性。团队运动数据的增加为对象中心流程挖掘提供了机会。

Method: 提出一个框架，将足球数据转化为带有空间维度的对象中心事件日志。基于真实足球数据生成日志，并分析不同流程表示的结果。

Result: 成功生成对象中心事件日志，并展示了其在足球数据分析中的有效性。

Conclusion: 该框架为足球分析提供了首个对象中心事件日志示例，未来工作应关注变体分析和过滤技术以更好地处理变异性。

Abstract: Object-centric event logs expand the conventional single-case notion event
log by considering multiple objects, allowing for the analysis of more complex
and realistic process behavior. However, the number of real-world
object-centric event logs remains limited, and further studies are needed to
test their usefulness. The increasing availability of data from team sports can
facilitate object-centric process mining, leveraging both real-world data and
suitable use cases. In this paper, we present a framework for transforming
football (soccer) data into an object-centric event log, further enhanced with
a spatial dimension. We demonstrate the effectiveness of our framework by
generating object-centric event logs based on real-world football data and
discuss the results for varying process representations. With our paper, we
provide the first example for object-centric event logs in football analytics.
Future work should consider variant analysis and filtering techniques to better
handle variability

</details>


### [51] [Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on Relational Databases](https://arxiv.org/abs/2507.12562)
*Md. Tanvir Alam,Md. Ahasanul Alam,Md Mahmudur Rahman,Md. Mosaddek Khan*

Main category: cs.DB

TL;DR: 提出了一种基于超图的框架rel-HNN，用于解决关系数据库中深度学习模型的输入固定大小问题，并通过多GPU并行训练提高效率。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的结构化数据难以适用于依赖固定大小输入的深度学习模型，现有GNN方法过于简化关系结构。

Method: rel-HNN将每个属性-值对建模为节点，每个元组建模为超边，以捕获细粒度的关系，并采用多GPU并行训练算法。

Result: 在分类和回归任务中表现优于现有方法，并行训练速度提升显著（最高达3.18倍）。

Conclusion: rel-HNN能有效处理关系数据的复杂性，并通过并行化提升效率。

Abstract: Relational databases (RDBs) are ubiquitous in enterprise and real-world
applications. Flattening the database poses challenges for deep learning models
that rely on fixed-size input representations to capture relational semantics
from the structured nature of relational data. Graph neural networks (GNNs)
have been proposed to address this, but they often oversimplify relational
structures by modeling all the tuples as monolithic nodes and ignoring
intra-tuple associations. In this work, we propose a novel hypergraph-based
framework, that we call rel-HNN, which models each unique attribute-value pair
as a node and each tuple as a hyperedge, enabling the capture of fine-grained
intra-tuple relationships. Our approach learns explicit multi-level
representations across attribute-value, tuple, and table levels. To address the
scalability challenges posed by large RDBs, we further introduce a
split-parallel training algorithm that leverages multi-GPU execution for
efficient hypergraph learning. Extensive experiments on real-world and
benchmark datasets demonstrate that rel-HNN significantly outperforms existing
methods in both classification and regression tasks. Moreover, our
split-parallel training achieves substantial speedups -- up to 3.18x for
learning on relational data and up to 2.94x for hypergraph learning -- compared
to conventional single-GPU execution.

</details>


### [52] [Targeted Mining of Time-Interval Related Patterns](https://arxiv.org/abs/2507.12668)
*Shuang Liang,Lili Chen,Wensheng Gan,Philip S. Yu,Shengjie Zhao*

Main category: cs.DB

TL;DR: 本文提出了一种名为TaTIRP的新算法，用于发现目标时间间隔相关模式（TIRP），并通过多种剪枝策略提升大规模数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的序列模式挖掘往往忽略事件的持续时间，而TIRP挖掘能解决这一问题，但现有方法通常计算资源消耗大且效率低。

Method: 提出TaTIRP算法，结合特定标准和多种剪枝策略，以减少冗余扩展操作。

Result: 在真实世界和合成数据集上的实验验证了算法的准确性和效率。

Conclusion: TaTIRP算法在提升TIRP挖掘效率方面表现出色，尤其适用于大规模数据分析。

Abstract: Compared to frequent pattern mining, sequential pattern mining emphasizes the
temporal aspect and finds broad applications across various fields. However,
numerous studies treat temporal events as single time points, neglecting their
durations. Time-interval-related pattern (TIRP) mining is introduced to address
this issue and has been applied to healthcare analytics, stock prediction, etc.
Typically, mining all patterns is not only computationally challenging for
accurate forecasting but also resource-intensive in terms of time and memory.
Targeting the extraction of time-interval-related patterns based on specific
criteria can improve data analysis efficiency and better align with customer
preferences. Therefore, this paper proposes a novel algorithm called TaTIRP to
discover Targeted Time-Interval Related Patterns. Additionally, we develop
multiple pruning strategies to eliminate redundant extension operations,
thereby enhancing performance on large-scale datasets. Finally, we conduct
experiments on various real-world and synthetic datasets to validate the
accuracy and efficiency of the proposed algorithm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Modular SAIL: dream or reality?](https://arxiv.org/abs/2507.12471)
*Petr Kourzanov,Anmol*

Main category: cs.AR

TL;DR: 论文探讨了如何在SAIL-RISCV黄金模型中实现模块化组合，并通过实验验证了模块化对模拟器性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RISC-V ISA的模块化特性，需要在开发流程中实现组合性，从而超越规范层面的模块化。

Method: 提出了模块化SAIL实验，通过调整SAIL-RISCV流程（以及SAIL编译器）来支持模拟器层面的模块化。

Result: 实验表明，模块化模拟器的功能行为与原始单一模拟器一致，且静态和动态绑定的性能表现相当。

Conclusion: 模块化在SAIL-RISCV中是可行的，且不会影响模拟器的功能行为。

Abstract: In order to truly benefit from RISC-V ISA modularity, the community has to
address the issue of compositionality, going beyond modules at the
specification level covering larger subsets of the RISC-V development flow
including emulation, simulation and verification. In this paper we introduce
modular SAIL, an experiment to inject compositionality into the SAIL-RISCV
golden model. We show that it is, in principle, not difficult to adapt the
SAIL-RISCV flow (and ideally the SAIL compiler itself) to support modules at
the emulator level. We back our findings by a comparative study of the
resulting pluggable emulator's performance using both static and dynamic
binding, which both exhibit same functional behavior as the original monolithic
emulator (aka RISC-V ISS).

</details>


### [54] [An ultra-low-power CGRA for accelerating Transformers at the edge](https://arxiv.org/abs/2507.12904)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 本文提出了一种针对边缘设备的低功耗CGRA架构，专门用于加速变压器模型中的GEMM操作，通过4x4 PE阵列和专用MOB优化计算与内存访问。


<details>
  <summary>Details</summary>
Motivation: 变压器模型在边缘设备上部署时面临计算资源和高能耗的挑战，需要一种高效且低功耗的硬件解决方案。

Method: 设计了包含4x4 PE阵列和4x2 MOB的CGRA架构，采用无交换环网互连以减少功耗和延迟。

Result: 该架构优化了内存带宽和数据重用，为边缘设备提供了一种可扩展的变压器加速方案。

Conclusion: 通过异构阵列设计和高效数据流，该CGRA架构成功解决了变压器在边缘设备上的部署难题。

Abstract: Transformers have revolutionized deep learning with applications in natural
language processing, computer vision, and beyond. However, their computational
demands make it challenging to deploy them on low-power edge devices. This
paper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA)
architecture specifically designed to accelerate General Matrix Multiplication
(GEMM) operations in transformer models tailored for the energy and resource
constraints of edge applications. The proposed architecture integrates a 4 x 4
array of Processing Elements (PEs) for efficient parallel computation and
dedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE
operations, reducing memory bandwidth demands and enhancing data reuse. A
switchless mesh torus interconnect network further minimizes power and latency
by enabling direct communication between PEs and MOBs, eliminating the need for
centralized switching. Through its heterogeneous array design and efficient
dataflow, this CGRA architecture addresses the unique computational needs of
transformers, offering a scalable pathway to deploy sophisticated machine
learning models on edge devices.

</details>


### [55] [WIP: Turning Fake Chips into Learning Opportunities](https://arxiv.org/abs/2507.13281)
*Haniye Mehraban,Saad Azmeen-ur-Rahman,John Hu*

Main category: cs.AR

TL;DR: 这篇进展中的论文通过一个案例研究，展示了假冒TL074运算放大器如何在初级电子课程中成为动手学习的机会。


<details>
  <summary>Details</summary>
Motivation: 假冒集成电路（IC）日益普遍，对本科电子实验室的完整性构成威胁。本文旨在探讨如何将其转化为教学机会。

Method: 通过让学生动手进行诊断，测量电流、分析波形和故障排除，利用假冒芯片组件进行学习。

Result: 学生通过接触假冒元件，加深了对模拟电路、供应链安全和实际工程的理解。

Conclusion: 将假冒元件问题转化为教学机会，可以丰富学生的实践经验和对复杂电子系统的理解。

Abstract: This work-in-progress paper presents a case study in which counterfeit TL074
operational amplifiers, discovered in a junior level electronics course, became
the basis for a hands on learning experience. Counterfeit integrated circuits
(IC) are increasingly common, posing a significant threat to the integrity of
undergraduate electronics laboratories. Instead of simply replacing the
counterfeit components, we turned the issue into a teaching moment. Students
engaged in hands-on diagnostics measuring current, analyzing waveforms, and
troubleshooting. By working with fake chip components, they gained deeper
insight into analog circuits, supply chain security, and practical engineering.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [56] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 本文提出了一种改进的LiDAR模拟方法，通过显式建模传感器特性（如滚动快门、激光功率变化和强度衰减），实现了比现有技术更准确的LiDAR模拟。


<details>
  <summary>Details</summary>
Motivation: LiDAR模拟中的传感器特性未被充分解决，现有方法在渲染速度和动态场景处理上已有进展，但LiDAR特定效果仍需改进。

Method: 显式建模传感器特性（如滚动快门、激光功率变化和强度衰减），并通过定量、定性比较以及消融研究验证方法有效性。

Result: 与现有技术相比，该方法在LiDAR模拟中表现更准确，并展示了高级重模拟能力（如生成高分辨率LiDAR扫描）。

Conclusion: 该方法有效解决了LiDAR模拟中的传感器特性问题，为LiDAR仿真提供了更准确和先进的解决方案。

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 本文提出了一种将概率事件微积分（PEC）与马尔可夫决策过程（MDP）结合的方法，弥补了PEC在目标导向推理上的不足。


<details>
  <summary>Details</summary>
Motivation: PEC虽在不确定环境下推理行动及其效果具有优势，但缺乏目标导向的机制。本文旨在通过结合MDP，增强PEC的功能。

Method: 通过开发PEC域到MDP的正式翻译，定义“行动情境”以保留PEC的灵活语义，从而将MDP工具应用于PEC。

Result: PEC-MDP形式主义成功支持了时间推理和目标驱动规划，并将学习策略映射回人类可读的PEC表示，保持了可解释性。

Conclusion: PEC-MDP扩展了PEC的能力，同时保留了其可解释性和表达力，为复杂推理任务提供了新工具。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [58] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 论文提出了一种结合高阶模式和基于最小T-范数的模糊等价关系的统一算法，用于抽象函数的决策任务，并证明了其终止性、可靠性和完备性。


<details>
  <summary>Details</summary>
Motivation: 在涉及抽象函数的决策任务中，高阶理论与模糊逻辑的结合可以解决精确匹配稀少或不必要的问题，但开发高效的推理和计算技术是一个挑战。

Method: 采用高阶模式和基于最小T-范数的模糊等价关系，提出了一种统一算法。

Result: 算法被证明具有终止性、可靠性和完备性，且在可统一时计算最高逼近度的最一般统一子。

Conclusion: 这一方法为高阶模糊逻辑的统一问题提供了一种有效且可靠的解决方案。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [59] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 前沿AI系统在说服、欺骗和影响人类行为方面的能力迅速提升，目前已在某些情境下展现人类水平的说服力和策略性欺骗。本文提出操纵攻击是一个重大威胁，并开发了一个围绕‘能力不足’、‘控制’和‘可信度’的安全案例框架，以帮助AI公司评估和缓解这些风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升，操纵攻击可能对人类监督形成威胁，但相关研究和管理框架仍缺乏。本文旨在填补这一空白。

Method: 通过提出一个基于‘能力不足’、‘控制’和‘可信度’的安全案例框架，为AI公司提供评估方法和实施建议。

Result: 开发了首个系统性方法，将操纵风险纳入AI安全治理，为AI公司提供了评估和缓解风险的具体基础。

Conclusion: 本文为AI公司提供了一个实用的框架，以便在部署前识别和应对操纵攻击的风险，从而增强AI系统的安全性。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [60] [Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic Markers of Unsafe Content Recommended for Children and Teenagers](https://arxiv.org/abs/2507.12571)
*Haoning Xue,Brian Nishimine,Martin Hilbert,Drew Cingel,Samantha Vigil,Jane Shawcroft,Arti Thakur,Zubair Shafiq,Jingwen Zhang*

Main category: cs.CY

TL;DR: 该研究分析了短视频平台推荐给儿童和青少年的内容，发现不安全视频具有视觉特征和有害内容，呼吁加强内容审核和平台监管。


<details>
  <summary>Details</summary>
Motivation: 研究关注短视频平台年龄验证机制失效对儿童和青少年的潜在危害。

Method: 通过多模态特征分析和主题建模，分析了4492个短视频的内容和特征。

Result: 发现不安全视频具有暗色视觉特征和显性/隐性有害内容。

Conclusion: 研究提出了在线危害框架，呼吁对儿童和青少年进行更细致的内容审核和平台监管。

Abstract: The prevalence of short form video platforms, combined with the
ineffectiveness of age verification mechanisms, raises concerns about the
potential harms facing children and teenagers in an algorithm-moderated online
environment. We conducted multimodal feature analysis and thematic topic
modeling of 4,492 short videos recommended to children and teenagers on
Instagram Reels, TikTok, and YouTube Shorts, collected as a part of an
algorithm auditing experiment. This feature-level and content-level analysis
revealed that unsafe (i.e., problematic, mentally distressing) short videos (a)
possess darker visual features and (b) contain explicitly harmful content and
implicit harm from anxiety-inducing ordinary content. We introduce a useful
framework of online harm (i.e., explicit, implicit, unintended), providing a
unique lens for understanding the dynamic, multifaceted online risks facing
children and teenagers. The findings highlight the importance of protecting
younger audiences in critical developmental stages from both explicit and
implicit risks on social media, calling for nuanced content moderation, age
verification, and platform regulation.

</details>


### [61] [Rookie Mistakes: Measuring Software Quality in Student Projects to Guide Educational Enhancement](https://arxiv.org/abs/2507.12488)
*Marco De Luca,Sergio Di Martino,Sergio Di Meglio,Anna Rita Fasolino,Luigi Libero Lucio Starace,Porfirio Tramontana*

Main category: cs.CY

TL;DR: 该研究分析了172名学生在83个面向对象团队项目中的软件质量问题，填补了中等水平学生在复杂项目中质量问题的研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前编程和软件工程教学过于关注功能实现，而忽视了软件质量，课程安排和现有研究的局限性使得中等水平学生在复杂项目中的质量问题尚未明确。

Method: 研究通过SonarQube和ArchUnit静态分析管道，评估83个面向对象团队项目的代码异味和架构反模式。

Result: 研究发现了学生在这一阶段面临的常见质量问题，为优化课程和推广质量导向的开发实践提供了依据。

Conclusion: 研究为教育工作者提供了改进软件工程课程的具体指导，强调了质量导向实践在中等水平项目中的重要性。

Abstract: When teaching Programming and Software Engineering in Bachelor's Degree
programs, the emphasis on creating functional software projects often
overshadows the focus on software quality, a trend that aligns with ACM
curricula recommendations. Software Engineering courses are typically
introduced later in the curriculum, and can generally allocate only limited
time to quality-related topics, leaving educators with the challenge of
deciding which quality aspects to prioritize. In this decision, the literature
offers limited guidance, as most existing studies focus on code written by
novice students and small code units, making it unclear whether those findings
extend to intermediate-level students with foundational object-oriented
programming skills working on more complex software projects. To address this
gap, we analyze 83 object-oriented team projects developed by 172 university
students across 4 different editions of the Object-Oriented Programming course.
We apply a static analysis pipeline used in prior research to assess software
quality, combining SonarQube and ArchUnit to detect code smells and
architectural anti-patterns. Our findings highlight recurring quality issues
and offer concrete evidence of the challenges students face at this stage,
providing valuable guidance for educators aiming to continuously improve
Software Engineering curricula and promote quality-oriented development
practices.

</details>


### [62] [ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle](https://arxiv.org/abs/2507.12674)
*Mihran Miroyan,Rose Niousha,Joseph E. Gonzalez,Gireeja Ranade,Narges Norouzi*

Main category: cs.CY

TL;DR: 这项研究探讨了大型语言模型（LLM）能否生成类似学生的代码（不完美、迭代性强且风格多样），并通过ParaStudent实验表明，微调可以显著改善代码与真实学生学习轨迹的对齐。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能生成真实学生那样的代码，而非仅生成完美代码，以便更好地模拟学生在编程课程中的学习过程。

Method: 使用时间戳记录的学生提交数据集，设计低分辨率和高分辨率实验，从语义、功能和风格维度评估代码输出，并通过微调模型来捕捉学生学习动态。

Result: 微调显著提升了模型输出与真实学生学习轨迹的对齐性，并能捕捉错误模式、逐步改进和风格变化。

Conclusion: 模拟真实学生代码需要结合上下文感知生成、时序建模和多维评估，微调是提升生成结果真实性的关键。

Abstract: Large Language Models (LLMs) have shown strong performance on programming
tasks, but can they generate student-like code like real students - imperfect,
iterative, and stylistically diverse? We present ParaStudent, a systematic
study of LLM-based "student-like" code generation in an introductory
programming course setting. Using a dataset of timestamped student submissions
across multiple semesters, we design low- and high-resolution experiments to
model student progress and evaluate code outputs along semantic, functional,
and stylistic dimensions. Our results show that fine-tuning significantly
improves alignment with real student trajectories and captures error patterns,
incremental improvements, and stylistic variations more faithfully. This study
shows that modeling realistic student code requires capturing learning dynamics
through context-aware generation, temporal modeling, and multi-dimensional
evaluation. Code for experiments and evaluation is available at
\href{https://github.com/mmiroyan/ParaStudent}{\texttt{github.com/mmiroyan/ParaStudent}}.

</details>


### [63] [The Case for Contextual Copyleft: Licensing Open Source Training Data and Generative AI](https://arxiv.org/abs/2507.12713)
*Grant Shanklin,Emmie Hine,Claudio Novelli,Tyler Schroder,Luciano Floridi*

Main category: cs.CY

TL;DR: 本文提出了一种名为CCAI的新型许可机制，将传统Copyleft原则扩展到生成式AI模型，增强开发者控制并促进开源AI发展，但需配套监管以实现风险与收益平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将传统Copyleft原则应用于生成式AI开发，解决开源代码用于训练AI模型时的新挑战。

Method: 提出CCAI许可机制，并通过三部分评估框架验证其可行性：法律合规性、政策合理性及跨情境的收益与风险分析。

Result: CCAI许可在增强开发者控制和促进开源AI方面有显著优势，但需配套监管以应对开源AI的潜在风险。

Conclusion: 在完善的监管环境下，CCAI许可是适应生成式AI发展的可行方案，能够保留并调整FOSS核心原则。

Abstract: The proliferation of generative AI systems has created new challenges for the
Free and Open Source Software (FOSS) community, particularly regarding how
traditional copyleft principles should apply when open source code is used to
train AI models. This article introduces the Contextual Copyleft AI (CCAI)
license, a novel licensing mechanism that extends copyleft requirements from
training data to the resulting generative AI models. The CCAI license offers
significant advantages, including enhanced developer control, incentivization
of open source AI development, and mitigation of openwashing practices. This is
demonstrated through a structured three-part evaluation framework that examines
(1) legal feasibility under current copyright law, (2) policy justification
comparing traditional software and AI contexts, and (3) synthesis of
cross-contextual benefits and risks. However, the increased risk profile of
open source AI, particularly the potential for direct misuse, necessitates
complementary regulatory approaches to achieve an appropriate risk-benefit
balance. The paper concludes that when implemented within a robust regulatory
environment focused on responsible AI usage, the CCAI license provides a viable
mechanism for preserving and adapting core FOSS principles to the evolving
landscape of generative AI development.

</details>


### [64] [Bridging Boundaries: How to Foster Effective Research Collaborations Across Affiliations in the Field of Trust and Safety](https://arxiv.org/abs/2507.13008)
*Amanda Menking,Mona Elswah,David J. Grüning,Lasse H. Hansen,Irene Huang,Julia Kamin,Catrine Normann*

Main category: cs.CY

TL;DR: 该论文探讨了如何在数字信任与安全领域构建跨部门研究合作，以克服不同利益相关者间的激励、时间限制和能力差异，并提出了一个实用框架。


<details>
  <summary>Details</summary>
Motivation: 随着数字信任与安全领域的发展，跨学术、行业、政府和非政府部门的合作变得必要而复杂。论文旨在解决这些部门的激励和优先事项差异问题。

Method: 通过总结作者自身的跨部门合作经验，论文定义了主要的研究合作关系类型，并提出了一个分步框架来建立和管理有效合作，包括建立信任和分配角色等策略。

Result: 论文提出了一种强调清晰表达和协作的框架，并展示了跨部门合作如何推动更伦理、公平和有影响力的研究。

Conclusion: 论文倡导一种强调包容性、透明度和现实相关性的合作模式，以满足信任与安全这一新兴领域的跨学科需求。

Abstract: As the field of Trust and Safety in digital spaces continues to grow, it has
become increasingly necessary - but also increasingly complex - to collaborate
on research across the academic, industry, governmental and non-governmental
sectors. This paper examines how cross-affiliation research partnerships can be
structured to overcome misaligned incentives, timelines and constraints while
delivering on the unique strengths of each stakeholder. Drawing on our own
experience of cross-sector collaboration, we define the main types of
affiliation and highlight the common differences in research priorities,
operational pressures and evaluation metrics across sectors. We then propose a
practical, step-by-step framework for initiating and managing effective
collaborations, including strategies for building trust, aligning goals, and
distributing roles. We emphasize the critical yet often invisible work of
articulation and argue that cross-sector partnerships are essential for
developing more ethical, equitable and impactful research in trust and safety.
Ultimately, we advocate collaborative models that prioritize inclusivity,
transparency and real-world relevance in order to meet the interdisciplinary
demands of this emerging field.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries](https://arxiv.org/abs/2507.12723)
*Minyoung Kim,Sehwan Park,Sungmin Cha,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 该论文提出了一种跨模态水印框架，用于从合成的视听伪造(SAVF)中恢复真实音频并定位篡改，以抵御虚假信息。


<details>
  <summary>Details</summary>
Motivation: 现有的检测或定位伪造的方法无法恢复真实音频，限制了其对抗虚假信息的能力。

Method: 提出跨模态水印框架，将真实音频嵌入到视觉内容中，以便在伪造后恢复音频并定位篡改。

Result: 实验表明，该方法在多种伪造场景下（如语音克隆和唇同步）均表现优异。

Conclusion: 该方法能够有效恢复真实音频并定位篡改，为对抗视听伪造提供了新解决方案。

Abstract: Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.

</details>


### [66] [Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes](https://arxiv.org/abs/2507.12932)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.SD

TL;DR: Enkidu是一种新型用户导向隐私保护框架，通过黑盒知识和少样本训练生成频率域噪声块，实时轻量级保护音频隐私。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造技术快速发展，威胁用户音频隐私，现有防御方法存在适应性差、可扩展性低等问题。

Method: 利用黑盒知识和少量用户数据训练生成频率域噪声块，实现实时轻量级保护。

Result: Enkidu在处理内存和运行时间效率上大幅优于现有方法，实验证明其有效性和实用性。

Conclusion: Enkidu能有效防御语音深度伪造攻击，保护音频隐私，同时保持语音质量和可理解性。

Abstract: The rapid advancement of voice deepfake technologies has raised serious
concerns about user audio privacy, as attackers increasingly exploit publicly
available voice data to generate convincing fake audio for malicious purposes
such as identity theft, financial fraud, and misinformation campaigns. While
existing defense methods offer partial protection, they face critical
limitations, including weak adaptability to unseen user data, poor scalability
to long audio, rigid reliance on white-box knowledge, and high computational
and temporal costs during the encryption process. To address these challenges
and defend against personalized voice deepfake threats, we propose Enkidu, a
novel user-oriented privacy-preserving framework that leverages universal
frequential perturbations generated through black-box knowledge and few-shot
training on a small amount of user data. These highly malleable
frequency-domain noise patches enable real-time, lightweight protection with
strong generalization across variable-length audio and robust resistance to
voice deepfake attacks, all while preserving perceptual quality and speech
intelligibility. Notably, Enkidu achieves over 50 to 200 times processing
memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime
efficiency (real-time coefficient as low as 0.004) compared to six
state-of-the-art countermeasures. Extensive experiments across six mainstream
text-to-speech models and five cutting-edge automated speaker verification
models demonstrate the effectiveness, transferability, and practicality of
Enkidu in defending against both vanilla and adaptive voice deepfake attacks.

</details>


### [67] [Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features](https://arxiv.org/abs/2507.12793)
*J. M. Chan Sri Manukalpa,H. S. Bopage,W. A. M. Jayawardena,P. K. P. G. Panduwawala*

Main category: cs.SD

TL;DR: 提出了一种基于深度学习的非侵入性声学分类框架，用于早期白蚁检测，结合CNN和LSTM架构，实现了高准确性和低假阴性率。


<details>
  <summary>Details</summary>
Motivation: 传统白蚁检测方法具有侵入性和低效性，本研究旨在开发一种非侵入性、自动化的早期检测方案以减少经济损失。

Method: 采用混合卷积神经网络（CNN）和长短时记忆网络（LSTM）架构，提取MFCC特征进行分类训练。

Result: 模型表现出94.5%的准确率、93.2%的精确率和95.8%的召回率，优于单独的CNN和LSTM模型。

Conclusion: 该研究为非侵入性白蚁检测提供了可行方案，未来可结合物联网实现实时警报并扩展到其他害虫检测。

Abstract: Structural pests, such as termites, pose a serious threat to wooden
buildings, resulting in significant economic losses due to their hidden and
progressive damage. Traditional detection methods, such as visual inspections
and chemical treatments, are invasive, labor intensive, and ineffective for
early stage infestations. To bridge this gap, this study proposes a non
invasive deep learning based acoustic classification framework for early
termite detection. We aim to develop a robust, scalable model that
distinguishes termite generated acoustic signals from background noise. We
introduce a hybrid Convolutional Neural Network Long Short Term Memory
architecture that captures both spatial and temporal features of termite
activity. Audio data were collected from termite infested and clean wooden
samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN
LSTM model to classify the signals. Experimental results show high performance,
with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis
reveals that the hybrid model outperforms standalone CNN and LSTM
architectures, underscoring its combined strength. Notably, the model yields
low false-negative rates, which is essential for enabling timely intervention.
This research contributes a non invasive, automated solution for early termite
detection, with practical implications for improved pest monitoring, minimized
structural damage, and better decision making by homeowners and pest control
professionals. Future work may integrate IoT for real time alerts and extend
detection to other structural pests.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [68] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: 论文提出了一个统一框架UniSLU，用于联合建模多个口语理解任务（如ASR、NER和SA），解决了现有方法中任务分离导致的问题，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有口语理解任务方法依赖单独模型，增加了复杂性且限制了跨任务交互，未能充分利用异构数据。

Method: 提出统一表示和生成方法，联合建模ASR、NER和SA任务，并整合大语言模型的生成能力。

Result: 在公开数据集上表现优于基准方法，适用于实际语音多媒体场景。

Conclusion: UniSLU通过统一框架提升了SLU任务的性能，促进了跨任务交互和数据利用。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [69] [Efficiently Constructing Sparse Navigable Graphs](https://arxiv.org/abs/2507.13296)
*Alex Conway,Laxman Dhulipala,Martin Farach-Colton,Rob Johnson,Ben Landrum,Christopher Musco,Yarin Shechter,Torsten Suel,Richard Wen*

Main category: cs.DS

TL;DR: 本文提出了一种快速的、具有理论保证的搜索图构建算法，显著降低了时间复杂度，同时保持了稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有的图基最近邻搜索方法尽管性能优越，但构建稀疏导航图的计算成本高昂，启发式方法缺乏理论保证，因此需要高效的算法。

Method: 通过将问题建模为n个相关的最小集合覆盖实例，结合流式与亚线性时间算法，提出了一种时间复杂度为~O(n²)的算法。

Result: 提出的算法在强指数时间假设下是最优的，且证明了O(log n)近似是NP难的，解决了相关图的构建问题。

Conclusion: 该方法不仅适用于导航图构建，还可推广到其他相关图的构建，为实际应用提供了高效的理论基础。

Abstract: Graph-based nearest neighbor search methods have seen a surge of popularity
in recent years, offering state-of-the-art performance across a wide variety of
applications. Central to these methods is the task of constructing a sparse
navigable search graph for a given dataset endowed with a distance function.
Unfortunately, doing so is computationally expensive, so heuristics are
universally used in practice.
  In this work, we initiate the study of fast algorithms with provable
guarantees for search graph construction. For a dataset with $n$ data points,
the problem of constructing an optimally sparse navigable graph can be framed
as $n$ separate but highly correlated minimum set cover instances. This yields
a naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose
sparsity is at most $O(\log n)$ higher than optimal. We improve significantly
on this baseline, taking advantage of correlation between the set cover
instances to leverage techniques from streaming and sublinear-time set cover
algorithms. Combined with problem-specific pre-processing techniques, we
present an $\tilde{O}(n^2)$ time algorithm for constructing an $O(\log
n)$-approximate sparsest navigable graph under any distance function.
  The runtime of our method is optimal up to logarithmic factors under the
Strong Exponential Time Hypothesis via a reduction from Monochromatic Closest
Pair. Moreover, we prove that, as with general set cover, obtaining better than
an $O(\log n)$-approximation is NP-hard, despite the significant additional
structure present in the navigable graph problem. Finally, we show that our
techniques can also beat cubic time for the closely related and practically
important problems of constructing $\alpha$-shortcut reachable and
$\tau$-monotonic graphs, which are also used for nearest neighbor search. For
such graphs, we obtain $\tilde{O}(n^{2.5})$ time or better algorithms.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [70] [Learning-Based Interface for Semantic Communication with Bit Importance Awareness](https://arxiv.org/abs/2507.12850)
*Wenzheng Kong,Wenyi Zhang*

Main category: cs.IT

TL;DR: 该论文提出了一种基于学习的接口设计（Split DeepJSCC改进版），通过可训练参数优化端到端性能，并引入了重要性感知网络以适应不同信道条件。


<details>
  <summary>Details</summary>
Motivation: 解决现有联合源信道编码（JSCC）方法难以与现有通信网络架构集成的问题，同时保持JSCC的语义保真和信道适应性优势。

Method: 采用可训练的比特级接口设计，并引入重要性感知网络以动态适应不同信道带宽比率和时变信道条件。

Result: 实验结果表明，该方法在无线图像传输任务中表现更优。

Conclusion: 为现有无线网络实现语义通信提供了一种潜在解决方案。

Abstract: Joint source-channel coding (JSCC) is an effective approach for semantic
communication. However, current JSCC methods are difficult to integrate with
existing communication network architectures, where application and network
providers are typically different entities. Recently, a novel paradigm termed
Split DeepJSCC has been under consideration to address this challenge. Split
DeepJSCC employs a bit-level interface that enables separate design of source
and channel codes, ensuring compatibility with existing communication networks
while preserving the advantages of JSCC in terms of semantic fidelity and
channel adaptability. In this paper, we propose a learning-based interface
design by treating its parameters as trainable, achieving improved end-to-end
performance compared to Split DeepJSCC. In particular, the interface enables
specification of bit-level importance at the output of the source code.
Furthermore, we propose an Importance-Aware Net that utilizes the
interface-derived bit importance information, enabling dynamical adaptation to
diverse channel bandwidth ratios and time-varying channel conditions.
Experimental results show that our method improves performance in wireless
image transmission tasks. This work provides a potential solution for realizing
semantic communications in existing wireless networks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 研究探讨了人类如何利用分布式和符号化表征构建定制心理模型来应对新情境，提出了一种结合语言模型和概率程序的'模型合成架构'（MSA），并通过实验验证其在模拟人类判断上的优越性。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何在新情境中整合广泛背景知识并进行连贯推理。

Method: 结合语言模型（实现全局相关检索和模型合成）与概率程序（实现定制化连贯世界模型），构建MSA。

Result: MSA在模拟人类判断上优于仅使用语言模型的基线方法，支持其对人类推理能力的模拟。

Conclusion: MSA提供了一种理解和复制人类在开放领域推理能力的有效路径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [72] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: AutoSteer是一种无需微调即可提升多模态大语言模型安全性的模块化技术，通过安全评分、自适应探测器和轻量级干预模块显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 由于多模态大语言模型面临对抗性输入的安全威胁，作者提出AutoSteer以增强推理时的安全性。

Method: AutoSteer包括安全评分(SAS)、自适应安全探测器和轻量级干预模块，动态识别风险并选择性干预。

Result: 在多种安全基准测试中，AutoSteer显著降低了文本、视觉和跨模态攻击的成功率，同时保持了模型能力。

Conclusion: AutoSteer是一个实用、可解释且有效的框架，适用于多模态AI系统的安全部署。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [73] [Efficient Classical-Processing of Constant-Depth Time Evolution Circuits in Control Hardware](https://arxiv.org/abs/2507.12765)
*Akhil Francis,Abhi D. Rajagopala,Norm M. Tubman,Katherine Klymko,Kasra Nowrouzi*

Main category: quant-ph

TL;DR: 该论文提出了一种通过硬件辅助的参数化电路执行（PCE）来减少经典处理和编译时间的方法，用于计算量子系统的动力学性质，实验显示运行时间减少了50%。


<details>
  <summary>Details</summary>
Motivation: 量子算法的运行时性能可以通过多种策略优化，本研究专注于减少经典处理和编译时间，以缓解近期量子算法中的经典瓶颈问题。

Method: 采用硬件辅助的参数化电路执行（PCE），利用结构等效的时间演化电路计算量子多体系统的动力学性质，特别是使用Cartan分解生成的恒定深度电路计算自旋模型的关联函数。

Result: 在横向场XY（最多6个位点）和海森堡自旋模型（最多3个位点）中，实验观察到运行时间相比标准编译方法减少了50%。

Conclusion: 时间演化电路结合硬件辅助PCE具有适应性，能够显著减少量子算法中的经典瓶颈，提升性能。

Abstract: Improving quantum algorithms run-time performance involves several strategies
such as reducing the quantum gate counts, decreasing the number of
measurements, advancement in QPU technology for faster gate operations, or
optimizing the classical processing. This work focuses on the latter,
specifically reducing classical processing and compilation time via
hardware-assisted parameterized circuit execution (PCE) for computing dynamical
properties of quantum systems. PCE was previously validated for QCVV protocols,
which leverages structural circuit equivalencies. We demonstrate the
applicability of this approach to computing dynamical properties of quantum
many-body systems using structurally equivalent time evolution circuits,
specifically calculating correlation functions of spin models using
constant-depth circuits generated via Cartan decomposition. Implementing this
for spin-spin correlation functions in Transverse field XY (up to 6-sites) and
Heisenberg spin models (up to 3-sites), we observed a run-time reduction of up
to 50\% compared to standard compilation methods. This highlights the
adaptability of time-evolution circuit with hardware-assisted PCE to
potentially mitigate the classical bottlenecks in near-term quantum algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [74] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: 提出了一种名为NeuraLeaf的神经参数模型，用于3D植物叶子的建模与重建，解决了叶子形状多样和变形灵活的问题。


<details>
  <summary>Details</summary>
Motivation: 植物叶子的建模与重建对农业和计算机图形学至关重要，但现有研究多集中于人类和动物，叶子因其形状多样和灵活变形而带来独特挑战。

Method: NeuraLeaf将叶子几何分解为2D基形状和3D变形，利用丰富的2D图像数据集学习基形状，并提出无骨骼的皮肤模型处理3D变形。

Result: NeuraLeaf能够生成多种形状变形的叶子模型，并精确拟合深度图和点云等3D观测数据。

Conclusion: NeuraLeaf有效解决了叶子建模的挑战，为农业和计算机图形学提供了实用工具。

Abstract: We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [75] [Predicting 3D Rigid Body Dynamics with Deep Residual Network](https://arxiv.org/abs/2407.18798)
*Abiodun Finbarrs Oketunji*

Main category: cs.CV

TL;DR: 该研究探讨了深度残差网络在预测三维刚体相互作用动力学中的应用，结合C++物理模拟器和PyTorch深度学习模型，模拟多种物理现象，并在10,000个模拟场景中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索深度残差网络在复杂3D物理系统动力学预测中的潜力，结合物理模拟器和深度学习，以提高预测精度。

Method: 采用C++实现的3D物理模拟器生成训练数据，结合PyTorch构建的深度残差网络（含输入层、多残差块和输出层），处理3D动力学复杂性。

Result: 模型在位置和方向预测上的均方误差分别为0.015和0.022，比基线方法提高了25%，尤其在弹性碰撞和旋转动力学预测中表现突出。

Conclusion: 研究证明了深度残差网络在复杂3D物理系统建模中的巨大潜力，同时讨论了局限性并提出了未来改进方向。

Abstract: This study investigates the application of deep residual networks for
predicting the dynamics of interacting three-dimensional rigid bodies. We
present a framework combining a 3D physics simulator implemented in C++ with a
deep learning model constructed using PyTorch. The simulator generates training
data encompassing linear and angular motion, elastic collisions, fluid
friction, gravitational effects, and damping. Our deep residual network,
consisting of an input layer, multiple residual blocks, and an output layer, is
designed to handle the complexities of 3D dynamics. We evaluate the network's
performance using a datasetof 10,000 simulated scenarios, each involving 3-5
interacting rigid bodies. The model achieves a mean squared error of 0.015 for
position predictions and 0.022 for orientation predictions, representing a 25%
improvement over baseline methods. Our results demonstrate the network's
ability to capture intricate physical interactions, with particular success in
predicting elastic collisions and rotational dynamics. This work significantly
contributes to physics-informed machine learning by showcasing the immense
potential of deep residual networks in modeling complex 3D physical systems. We
discuss our approach's limitations and propose future directions for improving
generalization to more diverse object shapes and materials.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration](https://arxiv.org/abs/2507.12935)
*Shirui Zhao,Jun Yin,Lingyun Yao,Martin Andraud,Wannes Meert,Marian Verhelst*

Main category: cs.LG

TL;DR: MC²A是一种算法-硬件协同设计框架，通过扩展处理器性能屋顶模型并提出灵活的硬件加速器架构，显著提升了MCMC算法的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MCMC加速方案在硬件灵活性或系统级效率方面存在局限，限制了其在大规模问题和实际应用中的可行性。

Method: MC²A通过三维性能屋顶模型分析工作负载多样性，设计参数化硬件加速器架构，并引入新型Gumbel采样器消除指数和归一化操作。

Result: 在端到端案例中，MC²A相比CPU、GPU、TPU和其他MCMC加速器分别实现了307.6倍、1.4倍、2.0倍和84.2倍的加速。

Conclusion: MC²A证明了通用硬件加速的可行性，有助于推广基于MCMC的解决方案。

Abstract: An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.

</details>


### [77] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: 该论文首次深入分析了大型语言模型（LLM）训练中的启动开销问题，提出了优化框架Bootseer，将启动开销减少了50%。


<details>
  <summary>Details</summary>
Motivation: 在工业级LLM训练中，启动开销（从任务提交到实际执行前的延迟）导致大量GPU时间浪费（如3.5%），但现有研究多关注运行时性能，忽视了这一问题。

Method: 通过实际生产数据分析启动开销的组成部分，设计Bootseer框架，针对三大瓶颈（容器镜像加载、运行时依赖安装、模型检查点恢复）分别提出优化技术：热块记录预取、依赖快照和HDFS-FUSE分条存储。

Result: Bootseer在生产环境中部署后，将LLM训练的启动开销减少了50%。

Conclusion: 启动开销是LLM训练中的重要问题，Bootseer通过系统级优化显著提升了效率，为大规模工业应用提供了实用解决方案。

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [78] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: PMKLC是一种新型并行多知识学习压缩器，显著提升基因组数据压缩比、吞吐量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型无损压缩器在压缩比、吞吐量和鲁棒性上不足，限制了其广泛应用。

Method: 提出PMKLC，包含自动化多知识学习框架、GPU加速编码器、并行加速机制及两种压缩模式。

Result: PMKLC-S/M在15个数据集上平均压缩比提升至73.609%/73.480%，吞吐量提升至3.036倍/10.710倍，且鲁棒性最佳。

Conclusion: PMKLC在基因组数据压缩中表现出色，适用于资源受限设备和复杂场景。

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [79] [FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient](https://arxiv.org/abs/2507.12983)
*ShanBin Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FedGA的公平性感知联邦学习算法，通过Gini系数度量性能差异，并动态调整聚合权重以提高公平性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的数据异质性导致客户端性能不平等，需要解决公平性问题。

Method: 使用Gini系数衡量性能差异，建立与全局模型更新规模的关系，动态调整聚合权重。

Result: 在多个数据集上的实验表明，FedGA显著提高了公平性指标（如方差和Gini系数），同时保持整体性能。

Conclusion: FedGA能有效提升联邦学习的公平性，且不影响模型的整体性能。

Abstract: Fairness has emerged as one of the key challenges in federated learning. In
horizontal federated settings, data heterogeneity often leads to substantial
performance disparities across clients, raising concerns about equitable model
behavior. To address this issue, we propose FedGA, a fairness-aware federated
learning algorithm. We first employ the Gini coefficient to measure the
performance disparity among clients. Based on this, we establish a relationship
between the Gini coefficient $G$ and the update scale of the global model
${U_s}$, and use this relationship to adaptively determine the timing of
fairness intervention. Subsequently, we dynamically adjust the aggregation
weights according to the system's real-time fairness status, enabling the
global model to better incorporate information from clients with relatively
poor performance.We conduct extensive experiments on the Office-Caltech-10,
CIFAR-10, and Synthetic datasets. The results show that FedGA effectively
improves fairness metrics such as variance and the Gini coefficient, while
maintaining strong overall performance, demonstrating the effectiveness of our
approach.

</details>


### [80] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: 论文探讨了基于联邦学习的神经解码在开放和闭环场景中的性能与隐私问题，指出了实时应用中性能与隐私的权衡。


<details>
  <summary>Details</summary>
Motivation: 神经接口技术面临数据共享时的隐私挑战，联邦学习作为一种隐私保护框架未被充分探索。

Method: 引入基于联邦学习的神经解码，并在开放和闭环场景中系统评估其性能和隐私表现。

Result: 开放环中联邦学习表现优于本地学习，而闭环中需调整方法，本地学习表现更好但隐私风险更高。

Conclusion: 实时自适应应用中存在性能与隐私的权衡，需设计专门针对单用户的联邦学习方法。

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [81] [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](https://arxiv.org/abs/2507.13092)
*Hyo-Jeong Jang,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 文章提出了一种新型跨模态知识蒸馏框架，用于解决EEG学习中的标签噪声和模态差异问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受噪声和标签错误影响，导致模型性能下降。传统知识蒸馏方法面临模态差异和标签不一致的挑战，需要新的解决方案。

Method: 提出基于原型相似性的特征对齐模块和任务特定的蒸馏头，用于解决模态和标签不一致问题。

Result: 实验显示，该方法在EEG情感回归和分类任务上优于单模态和多模态基线。

Conclusion: 该框架为BCI应用提供了潜在的有效解决方案。

Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state
monitoring in brain-computer interfaces (BCIs). However, it is highly
susceptible to intrinsic signal errors and human-induced labeling errors, which
lead to label noise and ultimately degrade model performance. To enhance EEG
learning, multimodal knowledge distillation (KD) has been explored to transfer
knowledge from visual models with rich representations to EEG-based models.
Nevertheless, KD faces two key challenges: modality gap and soft label
misalignment. The former arises from the heterogeneous nature of EEG and visual
feature spaces, while the latter stems from label inconsistencies that create
discrepancies between ground truth labels and distillation targets. This paper
addresses semantic uncertainty caused by ambiguous features and weakly defined
labels. We propose a novel cross-modal knowledge distillation framework that
mitigates both modality and label inconsistencies. It aligns feature semantics
through a prototype-based similarity module and introduces a task-specific
distillation head to resolve label-induced inconsistency in supervision.
Experimental results demonstrate that our approach improves EEG-based emotion
regression and classification performance, outperforming both unimodal and
multimodal baselines on a public multimodal dataset. These findings highlight
the potential of our framework for BCI applications.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [82] [Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning](https://arxiv.org/abs/2507.12625)
*David Freire-Obregón,Agnieszka Dubiel,Prasoon Kumar Vinodkumar,Gholamreza Anbarjafari,Dorota Kamińska,Modesto Castrillón-Santana*

Main category: q-bio.NC

TL;DR: 提出了一种针对双流EEG分类器的后验可解释性框架，扩展了LIME方法以适应双半球输入，揭示了情绪特异性的半球激活模式。


<details>
  <summary>Details</summary>
Motivation: 现有的基于双半球神经架构的EEG情绪识别方法在可解释性上存在局限，影响了其在情感计算等敏感领域的应用。

Method: 扩展LIME方法以处理结构化的双分支输入，分解预测相关性为跨半球和情绪类别的通道贡献。

Result: 揭示了与已知神经生理现象一致的情绪特异半球激活模式，并支持了情感神经科学中的功能不对称性。

Conclusion: 该框架为双流EEG分类器提供了神经生理学基础的解释，增强了模型的可信度和应用潜力。

Abstract: Recent advances have shown promise in emotion recognition from
electroencephalogram (EEG) signals by employing bi-hemispheric neural
architectures that incorporate neuroscientific priors into deep learning
models. However, interpretability remains a significant limitation for their
application in sensitive fields such as affective computing and cognitive
modeling. In this work, we introduce a post-hoc interpretability framework
tailored to dual-stream EEG classifiers, extending the Local Interpretable
Model-Agnostic Explanations (LIME) approach to accommodate structured,
bi-hemispheric inputs. Our method adapts LIME to handle structured two-branch
inputs corresponding to left and right-hemisphere EEG channel groups. It
decomposes prediction relevance into per-channel contributions across
hemispheres and emotional classes. We apply this framework to a previously
validated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset
of EEG recordings captured during a VR-based emotion elicitation task. The
resulting explanations reveal emotion-specific hemispheric activation patterns
consistent with known neurophysiological phenomena, such as frontal
lateralization in joy and posterior asymmetry in sadness. Furthermore, we
aggregate local explanations across samples to derive global channel importance
profiles, enabling a neurophysiologically grounded interpretation of the
model's decisions. Correlation analysis between symmetric electrodes further
highlights the model's emotion-dependent lateralization behavior, supporting
the functional asymmetries reported in affective neuroscience.

</details>
