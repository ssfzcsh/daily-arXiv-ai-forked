<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study](https://arxiv.org/abs/2509.03541)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: 本文通过系统映射研究发现，移动应用需求工程研究中90%以上的数据来自Google Play和Apple App Store，主要研究需求获取和分析活动，但数据来源单一可能导致偏差，建议扩展数据源和覆盖更多RE活动。


<details>
  <summary>Details</summary>
Motivation: 研究移动应用需求工程(RE)中使用数据集的现状，了解其来源平台及对应的RE活动，以发现潜在偏差和改进空间。

Method: 遵循Kitchenham等人的指南，对43篇相关论文进行系统映射研究。

Result: 90%以上研究数据来自Google Play和Apple App Store；需求获取和分析是最常见的研究活动。

Conclusion: 研究数据来源单一可能导致结果偏差，建议扩展替代数据源及研究更多RE活动。

Abstract: [Background] Research on requirements engineering (RE) for mobile apps
employs datasets formed by app users, developers or vendors. However, little is
known about the sources of these datasets in terms of platforms and the RE
activities that were researched with the help of the respective datasets.
[Aims] The goal of this paper is to investigate the state-of-the-art of the
datasets of mobile apps used in existing RE research. [Method] We carried out a
systematic mapping study by following the guidelines of Kitchenham et al.
[Results] Based on 43 selected papers, we found that Google Play and Apple App
Store provide the datasets for more than 90% of published research in RE for
mobile apps. We also found that the most investigated RE activities - based on
datasets, are requirements elicitation and requirements analysis. [Conclusions]
Our most important conclusions are: (1) there is a growth in the use of
datasets for RE research of mobile apps since 2012, (2) the RE knowledge for
mobile apps might be skewed due to the overuse of Google Play and Apple App
Store, (3) there are attempts to supplement reviews of apps from repositories
with other data sources, (4) there is a need to expand the alternative sources
and experiments with complimentary use of multiple sources, if the community
wants more generalizable results. Plus, it is expected to expand the research
on other RE activities, beyond elicitation and analysis.

</details>


### [2] [A Multi-stage Error Diagnosis for APB Transaction](https://arxiv.org/abs/2509.03554)
*Cheng-Yang Tsai,Tzu-Wei Huang,Jen-Wei Shih,I-Hsiang Wang,Yu-Cheng Lin,Rung-Bin Lin*

Main category: cs.SE

TL;DR: 本文提出了一种基于分层随机森林的自动化错误诊断框架，用于高效检测SoC设计中的APB事务错误，取得了91.36%的准确率，并在ICCAD 2025比赛的beta阶段获得第一名。


<details>
  <summary>Details</summary>
Motivation: 现代SoC设计中，手动检测APB事务错误效率低且易出错，需自动化解决方案提升验证效率。

Method: 采用分层随机森林架构，利用四个预训练二元分类器依次检测不同类型的错误，优先处理高确定性地址错误。

Result: 实验结果显示整体准确率为91.36%，地址错误检测接近完美，数据错误检测表现稳健。

Conclusion: 分层机器学习在硬件调试中具有潜力，可作为EDA中强大的自动化工具。

Abstract: Functional verification and debugging are critical bottlenecks in modern
System-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus
(APB) transaction errors in large Value Change Dump (VCD) files being
inefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this
study proposes an automated error diagnosis framework using a hierarchical
Random Forest-based architecture. The multi-stage error diagnosis employs four
pre-trained binary classifiers to sequentially detect Out-of-Range Access,
Address Corruption, and Data Corruption errors, prioritizing high-certainty
address-related faults before tackling complex data errors to enhance
efficiency. Experimental results show an overall accuracy of 91.36%, with
near-perfect precision and recall for address errors and robust performance for
data errors. Although the final results of the ICCAD 2025 CAD Contest are yet
to be announced as of the submission date, our team achieved first place in the
beta stage, highlighting the method's competitive strength. This research
validates the potential of hierarchical machine learning as a powerful
automated tool for hardware debugging in Electronic Design Automation (EDA).

</details>


### [3] [Parse Tree Tracking Through Time for Programming Process Analysis at Scale](https://arxiv.org/abs/2509.03668)
*Matt Rau,Chris Brown,John Edwards*

Main category: cs.SE

TL;DR: 该研究提出了一种算法，用于跟踪编程过程中解析树节点的变化，并通过分析学生编程数据揭示了新的行为模式和统计结果。


<details>
  <summary>Details</summary>
Motivation: 过去无法自动跟踪高层次的代码表示（如抽象语法树）导致分析学生在特定语境下的行为非常繁琐，本研究旨在解决这一问题。

Method: 使用两种算法跟踪解析树节点并构建非可解析代码状态的树表示，分析公开的学生编程数据。

Result: 研究发现了一些新的统计现象，例如在条件语句和循环内外代码删除率相似，三分之一的注释代码最终被恢复等。

Conclusion: 能够跟踪解析树的变化为理解学生编程行为的新维度（如代码结构的发展、学生最困难的语法结构等）打开了大门。

Abstract: Background and Context: Programming process data can be utilized to
understand the processes students use to write computer programming
assignments. Keystroke- and line-level event logs have been used in the past in
various ways, primarily in high-level descriptive statistics (e.g., timings,
character deletion rate, etc). Analysis of behavior in context (e.g., how much
time students spend working on loops) has been cumbersome because of our
inability to automatically track high-level code representations, such as
abstract syntax trees, through time and unparseable states.
  Objective: Our study has two goals. The first is to design the first
algorithm that tracks parse tree nodes through time. Second, we utilize this
algorithm to perform a partial replication study of prior work that used manual
tracking of code representations, as well as other novel analyses of student
programming behavior that can now be done at scale.
  Method: We use two algorithms presented in this paper to track parse tree
nodes through time and construct tree representations for unparseable code
states. We apply these algorithms to a public keystroke data from student
coursework in a 2021 CS1 course and conduct analysis on the resulting parse
trees.
  Findings: We discover newly observable statistics at scale, including that
code is deleted at similar rates inside and outside of conditionals and loops,
a third of commented out code is eventually restored, and that frequency with
which students jump around in their code may not be indicative of struggle.
  Implications: The ability to track parse trees through time opens the door to
understanding new dimensions of student programming, such as best practices of
structural development of code over time, quantitative measurement of what
syntactic constructs students struggle most with, refactoring behavior, and
attention shifting within the code.

</details>


### [4] [Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems](https://arxiv.org/abs/2509.03848)
*Rodrigo Oliveira Zacarias,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: 该论文提出了SECO-TransDX模型，旨在从开发者体验（DX）的角度理解软件生态系统（SECO）中的透明度，并探讨其如何影响开发者的感知和互动。


<details>
  <summary>Details</summary>
Motivation: 尽管透明度在软件生态系统中对信任、公平和参与至关重要，但其与开发者体验（DX）的关系尚未被系统化研究，因此需要填补这一空白。

Method: 通过Delphi研究与学术界和行业专家的合作，构建并完善了SECO-TransDX模型，该模型包含63个相关概念，涵盖技术、社会和组织层面。

Result: SECO-TransDX模型提供了一个结构化视角，帮助研究人员和实践者理解透明度如何影响开发者体验，并为未来研究和工具开发奠定基础。

Conclusion: SECO-TransDX模型不仅为研究透明度与DX的关系提供了理论框架，还为设计更透明、开发者友好的平台提供了实践指导。

Abstract: Software ecosystems (SECO) have become a dominant paradigm in the software
industry, enabling third-party developers to co-create value through
complementary components and services. While Developer Experience (DX) is
increasingly recognized as critical for sustainable SECO, transparency remains
an underexplored factor shaping how developers perceive and interact with
ecosystems. Existing studies acknowledge transparency as essential for trust,
fairness, and engagement, yet its relationship with DX has not been
systematically conceptualized. Hence, this work aims to advance the
understanding of transparency in SECO from a developer-centered perspective. To
this end, we propose SECO-TransDX (Transparency in Software Ecosystems from a
Developer Experience Perspective), a conceptual model that introduces the
notion of DX-driven transparency. The model identifies 63 interrelated
concepts, including conditioning factors, ecosystem procedures, artifacts, and
relational dynamics that influence how transparency is perceived and
constructed during developer interactions. SECO-TransDX was built upon prior
research and refined through a Delphi study with experts from academia and
industry. It offers a structured lens to examine how transparency mediates DX
across technical, social, and organizational layers. For researchers, it lays
the groundwork for future studies and tool development; for practitioners, it
supports the design of trustworthy, developer-centered platforms that improve
transparency and foster long-term engagement in SECO.

</details>


### [5] [Design and Development of a Web Platform for Blood Donation Management](https://arxiv.org/abs/2509.04423)
*Fatima Zulfiqar Ali,Atrooba Ilyas*

Main category: cs.SE

TL;DR: 本文介绍了一个基于网络的献血平台设计，通过简化献血者注册和血液请求流程，提高紧急情况下的血液获取效率。


<details>
  <summary>Details</summary>
Motivation: 献血对医疗系统至关重要，但在紧急情况下找到合适的献血者面临挑战，因此需要一个集中化的数字化平台。

Method: 平台采用PHP (Laravel框架)、HTML、CSS等技术，结合用例图、数据库设计等，开发了一个动态、用户友好的系统。

Result: 该系统简化了血液请求流程，减少了紧急情况下的延迟，提高了献血服务的效率。

Conclusion: 该献血平台通过数字化管理，有效提升了紧急血液供应的及时性和整体效率。

Abstract: Blood donation is a critical component of healthcare, yet locating suitable
donors in emergencies often presents significant challenges. This paper
presents the design and development of a Blood Donation Web Platform, a
web-based system that connects patients, donors, and administrators within a
centralized digital space. The platform allows interested donors to register
their personal information, including blood group, contact details, and
availability. Patients can search for donors based on blood group and location,
and the system provides a list of nearby donors who are ready to donate. The
platform design was guided by use case, database, class, and sequence diagrams
to ensure a well-structured and efficient system architecture. Modern web
technologies, including PHP (Laravel framework), HTML, CSS, Bootstrap, and
MySQL, supported by XAMPP and Visual Studio Code, were employed to implement a
dynamic, interactive, and user-friendly platform. By streamlining donor
refgistration, blood requests, and communication, the proposed system reduces
delays and complexities in emergencies, improving timely accessibility of blood
and enhancing overall efficiency in blood donation services.

</details>


### [6] [VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report](https://arxiv.org/abs/2509.03875)
*Ziyou Jiang,Mingyang Li,Guowei Yang,Lin Shi,Qing Wang*

Main category: cs.SE

TL;DR: VulRTex是一个利用大型语言模型（LLM）推理能力识别开源软件中漏洞相关问题的自动化方法，通过分析丰富的文本信息实现高效漏洞识别。


<details>
  <summary>Details</summary>
Motivation: 由于开源软件中漏洞问题的报告（IRs）需要安全人员手动筛选，耗时且存在安全隐患，现有方法仅依赖文本描述而忽略丰富文本信息，因此需要更全面的分析方法。

Method: VulRTex利用LLM构建漏洞推理数据库，检索相关案例生成推理指导，通过目标IR的丰富文本信息进行推理分析，实现漏洞识别。

Result: 在973,572个IRs的实验结果显示，VulRTex在不平衡数据集上表现最佳，F1、AUPRC和Macro-F1分别提升11.0%、20.2%和10.5%，且时间成本降低一半。此外，成功识别2024年GitHub中的30个新兴漏洞。

Conclusion: VulRTex通过结合LLM的推理能力和丰富文本分析，高效且实用地识别漏洞相关问题，已在实际项目中验证其有效性。

Abstract: Software vulnerabilities exist in open-source software (OSS), and the
developers who discover these vulnerabilities may submit issue reports (IRs) to
describe their details. Security practitioners need to spend a lot of time
manually identifying vulnerability-related IRs from the community, and the time
gap may be exploited by attackers to harm the system. Previously, researchers
have proposed automatic approaches to facilitate identifying these
vulnerability-related IRs, but these works focus on textual descriptions but
lack the comprehensive analysis of IR's rich-text information. In this paper,
we propose VulRTex, a reasoning-guided approach to identify
vulnerability-related IRs with their rich-text information. In particular,
VulRTex first utilizes the reasoning ability of the Large Language Model (LLM)
to prepare the Vulnerability Reasoning Database with historical IRs. Then, it
retrieves the relevant cases from the prepared reasoning database to generate
reasoning guidance, which guides LLM to identify vulnerabilities by reasoning
analysis on target IRs' rich-text information. To evaluate the performance of
VulRTex, we conduct experiments on 973,572 IRs, and the results show that
VulRTex achieves the highest performance in identifying the
vulnerability-related IRs and predicting CWE-IDs when the dataset is
imbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and
+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.
Furthermore, VulRTex has been applied to identify 30 emerging vulnerabilities
across 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are
successfully assigned CVE-IDs, which illustrates VulRTex's practicality.

</details>


### [7] [Vulnerability-Affected Versions Identification: How Far Are We?](https://arxiv.org/abs/2509.03876)
*Xingchu Chen,Chengwei Liu,Jialun Cao,Yang Xiao,Xinyue Cai,Yeting Li,Jingyi Shi,Tianqi Sun,Haiming Chen ang Wei Huo*

Main category: cs.SE

TL;DR: 该论文首次全面实证研究了漏洞影响版本识别，评估了12种工具在多个维度上的表现，发现现有工具的准确率最高不超过45.0%，并提出需要新方法来提升效果。


<details>
  <summary>Details</summary>
Motivation: 漏洞影响版本的识别对补丁和风险缓解至关重要，但目前工具的实效性因评估范围狭窄和技术过时而未明确。

Method: 研究通过整理1,128个C/C++漏洞的高质量基准，系统评估了12种代表工具在四个维度上的表现。

Result: 研究发现现有工具的准确率最高为45.0%，主要受启发式依赖、语义推理有限和匹配逻辑僵化限制。

Conclusion: 尽管集成策略能提升效果，但总体准确率仍低于60.0%，需开发新方法。研究还提供了可操作的见解并公开了代码和基准数据。

Abstract: Identifying which software versions are affected by a vulnerability is
critical for patching, risk mitigation.Despite a growing body of tools, their
real-world effectiveness remains unclear due to narrow evaluation scopes often
limited to early SZZ variants, outdated techniques, and small or
coarse-graineddatasets. In this paper, we present the first comprehensive
empirical study of vulnerability affected versions identification. We curate a
high quality benchmark of 1,128 real-world C/C++ vulnerabilities and
systematically evaluate 12 representative tools from both tracing and matching
paradigms across four dimensions: effectiveness at both vulnerability and
version levels, root causes of false positives and negatives, sensitivity to
patch characteristics, and ensemble potential. Our findings reveal fundamental
limitations: no tool exceeds 45.0% accuracy, with key challenges stemming from
heuristic dependence, limited semantic reasoning, and rigid matching logic.
Patch structures such as add-only and cross-file changes further hinder
performance. Although ensemble strategies can improve results by up to 10.1%,
overall accuracy remains below 60.0%, highlighting the need for fundamentally
new approaches. Moreover, our study offers actionable insights to guide tool
development, combination strategies, and future research in this critical area.
Finally, we release the replicated code and benchmark on our website to
encourage future contributions.outdated techniques, and small or coarse grained
datasets.

</details>


### [8] [Analyzing Variations in Dependency Distributions Due to Code Smell Interactions](https://arxiv.org/abs/2509.03896)
*Zushuai Zhang,Elliott Wen,Ewan Tempero*

Main category: cs.SE

TL;DR: 研究探讨代码异味间的相互作用如何增加模块间的依赖关系，分析显示交互导致总依赖增加，建议优先处理相互作用的代码异味。


<details>
  <summary>Details</summary>
Motivation: 模块间依赖增加维护复杂性，代码异味的相互作用可能加剧这种依赖，需深入理解其影响。

Method: 对116个开源Java系统进行依赖分析，量化代码异味与非代码异味间的交互。

Result: 代码异味对的交互导致某些依赖增加，总体依赖上升，如Feature Envy与Data Class交互时依赖增至7倍。

Conclusion: 开发者应优先处理相互作用的代码异味而非单独存在的异味，以减少依赖关系。

Abstract: The existence of dependencies between modules, such as classes, can mean that
changing a module triggers ripple effects that make maintenance complex and
costly, so the advice is to minimize dependencies between modules. It is
therefore important to understand the circumstances that can lead to increased
dependencies. Recent studies suggest that code smells, which are
characteristics of code that indicate potential design issues, may interact in
ways that increase dependencies between modules. In this study, we aim to
confirm previous observations and investigate whether and how the distribution
of static dependencies changes in the presence of code smell interactions. We
conducted a dependency analysis on 116 open-source Java systems to quantify the
interactions, comparing interactions among code smells and interactions between
code smells and non-code smells. Our results suggest that while interactions
between code smell pairs are associated with increases in certain dependencies
and decreases in others, overall, they are associated with an increase in total
dependencies. For example, the median number of dependencies between Feature
Envy methods and Data Classes is seven times as many as when the methods are
non-Feature Envy methods, increasing from 1 to 7. This implies that developers
should prioritize addressing code smells that interact with each other, rather
than code smells that exist only in isolation.

</details>


### [9] [The Auth Shim: A Lightweight Architectural Pattern for Integrating Enterprise SSO with Standalone Open-Source Applications](https://arxiv.org/abs/2509.03900)
*Yuvraj Agrawal*

Main category: cs.SE

TL;DR: 论文提出了一种名为Auth Shim的轻量级架构模式，用于解决企业环境中开源软件（OSS）缺乏对SAML或OIDC协议原生支持的安全集成问题。


<details>
  <summary>Details</summary>
Motivation: 企业广泛采用开源软件，但许多工具缺乏对现代身份协议的原生支持，导致安全集成困难。

Method: Auth Shim作为一种外部代理服务，将企业身份提供者（IdP）的请求转换为目标应用程序的原生会话管理机制。

Result: 通过在Adobe的案例研究中实施此模式，成功将一个流行的OSS BI工具与Okta SAML集成，实现了自动化的基于角色的访问控制（RBAC）。

Conclusion: Auth Shim提供了一种可重用、安全且成本效益高的方法，帮助企业在不牺牲安全治理的前提下集成开源工具到企业单点登录（SSO）生态系统中。

Abstract: Open-source software OSS is widely adopted in enterprise settings, but
standalone tools often lack native support for protocols like SAML or OIDC,
creating a critical security integration gap. This paper introduces and
formalizes the Auth Shim, a lightweight architectural pattern designed to solve
this problem. The Auth Shim is a minimal, external proxy service that acts as a
compatibility layer, translating requests from an enterprise Identity Provider
IdP into the native session management mechanism of a target application. A key
prerequisite for this pattern is that the target application must expose a
programmatic, secure administrative API. We present a case study of the
pattern's implementation at Adobe to integrate a popular OSS BI tool with Okta
SAML, which enabled automated Role-Based Access Control RBAC via IAM group
mapping and eliminated manual user provisioning. By defining its components,
interactions, and production deployment considerations, this paper provides a
reusable, secure, and cost-effective blueprint for integrating any standalone
OSS tool into an enterprise SSO ecosystem, thereby enabling organizations to
embrace open-source innovation without compromising on security governance.

</details>


### [10] [RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models](https://arxiv.org/abs/2509.04078)
*Jingjing Liu,Zeming Liu,Zihao Cheng,Mengliang He,Xiaoming Shi,Yuhang Guo,Xiangrong Zhu,Yuanfang Guo,Yunhong Wang,Haifeng Wang*

Main category: cs.SE

TL;DR: RepoDebug是一个多任务、多语言、多错误类型的仓库级代码调试数据集，用于评估LLMs在复杂场景下的调试能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注功能级代码修复，忽略了更复杂、更真实的仓库级场景，导致对LLMs在仓库级调试中的挑战认识不足。

Method: 提出了RepoDebug数据集，支持8种编程语言和3种调试任务，包含22种子类型错误。

Result: 对10种LLMs的评估显示，即使是表现最好的Claude 3.5 Sonnect在仓库级调试中表现不佳。

Conclusion: 仓库级代码调试对LLMs仍具挑战性，需要进一步研究提升其能力。

Abstract: Large Language Models (LLMs) have exhibited significant proficiency in code
debugging, especially in automatic program repair, which may substantially
reduce the time consumption of developers and enhance their efficiency.
Significant advancements in debugging datasets have been made to promote the
development of code debugging. However, these datasets primarily focus on
assessing the LLM's function-level code repair capabilities, neglecting the
more complex and realistic repository-level scenarios, which leads to an
incomplete understanding of the LLM's challenges in repository-level debugging.
While several repository-level datasets have been proposed, they often suffer
from limitations such as limited diversity of tasks, languages, and error
types. To mitigate this challenge, this paper introduces RepoDebug, a
multi-task and multi-language repository-level code debugging dataset with 22
subtypes of errors that supports 8 commonly used programming languages and 3
debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,
where Claude 3.5 Sonnect, the best-performing model, still cannot perform well
in repository-level debugging.

</details>


### [11] [An Empirical Study of Vulnerabilities in Python Packages and Their Detection](https://arxiv.org/abs/2509.04260)
*Haowei Quan,Junjie Wang,Xinzhe Li,Terry Yue Zhuo,Xiao Chen,Xiaoning Du*

Main category: cs.SE

TL;DR: PyVul是一个全面的Python包漏洞基准套件，包含1,157个公开报告的漏洞，并提供了高精度的标注。分析显示，多语言Python包更容易受漏洞影响，现有检测工具的能力与实际需求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: Python包的安全性日益重要，但现有漏洞检测工具的效能未得到充分研究。

Method: 引入PyVul基准套件，采用LLM辅助的数据清洗方法，提供提交和函数级别的标注，并分析漏洞分布。

Result: PyVul在提交级别标注精度达100%，函数级别94%。多语言Python包更易受漏洞影响，现有检测工具能力不足。

Conclusion: PyVul为Python包漏洞研究提供了精确基准，揭示了现有工具的局限性，并强调了未来改进的必要性。

Abstract: In the rapidly evolving software development landscape, Python stands out for
its simplicity, versatility, and extensive ecosystem. Python packages, as units
of organization, reusability, and distribution, have become a pressing concern,
highlighted by the considerable number of vulnerability reports. As a scripting
language, Python often cooperates with other languages for performance or
interoperability. This adds complexity to the vulnerabilities inherent to
Python packages, and the effectiveness of current vulnerability detection tools
remains underexplored. This paper addresses these gaps by introducing PyVul,
the first comprehensive benchmark suite of Python-package vulnerabilities.
PyVul includes 1,157 publicly reported, developer-verified vulnerabilities,
each linked to its affected packages. To accommodate diverse detection
techniques, it provides annotations at both commit and function levels. An
LLM-assisted data cleansing method is incorporated to improve label accuracy,
achieving 100% commit-level and 94% function-level accuracy, establishing PyVul
as the most precise large-scale Python vulnerability benchmark. We further
carry out a distribution analysis of PyVul, which demonstrates that
vulnerabilities in Python packages involve multiple programming languages and
exhibit a wide variety of types. Moreover, our analysis reveals that
multi-lingual Python packages are potentially more susceptible to
vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark
reveals a significant discrepancy between the capabilities of existing tools
and the demands of effectively identifying real-world security issues in Python
packages. Additionally, we conduct an empirical review of the top-ranked CWEs
observed in Python packages, to diagnose the fine-grained limitations of
current detection tools and highlight the necessity for future advancements in
the field.

</details>


### [12] [FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case Study](https://arxiv.org/abs/2509.04328)
*Amine Barrak,Emna Ksontini,Ridouane Atike,Fehmi Jaafar*

Main category: cs.SE

TL;DR: 论文提出了FaaSGuard，一个针对开源无服务器环境的统一DevSecOps管道，通过在开发生命周期的每个阶段嵌入轻量级安全检测，有效解决了无服务器计算的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算的独特特性（如短暂执行和细粒度扩展）带来了独特的安全挑战，尤其是开源平台如OpenFaaS。当前的安全方法缺乏综合性解决方案，因此需要一种集成化策略。

Method: 提出了FaaSGuard，一个统一DevSecOps管道，在规划、编码、构建、部署和监控的每个阶段嵌入轻量级、故障关闭的安全检测。

Result: 通过20个真实无服务器功能的案例研究验证，FaaSGuard表现出高精度（95%）和召回率（91%），且未显著干扰现有CI/CD实践。

Conclusion: FaaSGuard是一种有效的安全策略，能够检测和预防关键漏洞，适用于开源无服务器环境。

Abstract: Serverless computing significantly alters software development by abstracting
infrastructure management and enabling rapid, modular, event-driven
deployments. Despite its benefits, the distinct characteristics of serverless
functions, such as ephemeral execution and fine-grained scalability, pose
unique security challenges, particularly in open-source platforms like
OpenFaaS. Existing approaches typically address isolated phases of the
DevSecOps lifecycle, lacking an integrated and comprehensive security strategy.
To bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline
explicitly designed for open-source serverless environments. FaaSGuard
systematically embeds lightweight, fail-closed security checks into every stage
of the development lifecycle-planning, coding, building, deployment, and
monitoring-effectively addressing threats such as injection attacks, hard-coded
secrets, and resource exhaustion. We validate our approach empirically through
a case study involving 20 real-world serverless functions from public GitHub
repositories. Results indicate that FaaSGuard effectively detects and prevents
critical vulnerabilities, demonstrating high precision (95%) and recall (91%)
without significant disruption to established CI/CD practices.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking](https://arxiv.org/abs/2509.04253)
*Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 本文提出了一种结合区域系统和可达性类型优势的方法，通过静态资源管理实现灵活共享和生命周期控制。


<details>
  <summary>Details</summary>
Motivation: 解决高阶函数语言中静态资源管理的控制、表达性和灵活性之间的矛盾。

Method: 提出两种扩展：A<:用于粗粒度可达性追踪，{A}<:用于词法生命周期控制。

Result: 在Rocq中形式化并验证了类型安全。

Conclusion: 新方法统一了区域系统和可达性类型的优点，实现了灵活共享和生命周期控制。

Abstract: Static resource management in higher-order functional languages remains
elusive due to tensions between control, expressiveness, and flexibility.
Region-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control
over lifetimes and expressive in-region sharing, but restrict resources to
lexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],
offers non-lexical lifetimes and robust safety guarantees, yet its global
invariants make common sharing patterns hard to express. Reachability types
[Wei et al. 2024] enable reasoning about sharing and separation, but lack
practical tools for controlling resource lifetimes.
  In this work, we try to unify their strengths. Our solution enables grouping
resources as arenas for arbitrary sharing and static guarantees of lexically
scoped lifetimes. Crucially, arenas and lexical lifetimes are not the only
choice: users may also manage resources individually, with non-lexical
lifetimes. Regardless of mode, resources share the same type, preserving the
higher-order parametric nature of the language.
  Obtaining static safety guarantee in a higher-order language with flexible
sharing is nontrivial. To this end, we propose two new extensions atop
reachability types [Wei et al. 2024]. First, A<: features a novel
two-dimensional store model to enable coarse-grained reachability tracking for
arbitrarily shared resources within arenas. Building on this, {A}<: establishes
lexical lifetime control with static guarantees. As the first reachability
formalism presented for lifetime control, {A}<: avoids the complication of
flow-sensitive reasoning and retains expressive power and simplicity. Both
calculi are formalized and proven type safe in Rocq.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [14] [Towards Deterministic Sub-0.5 us Response on Linux through Interrupt Isolation](https://arxiv.org/abs/2509.03855)
*Zhouyi Zhou,Zhili Liu,Shancong Zhang,Jiemin Li,Dengke Du,Mengke Sun,Zhiqiang Wang,Hongyan Liu,Guogai Xu*

Main category: cs.OS

TL;DR: 该论文提出了一种中断隔离方法，通过集中化和最小化定时器中断干扰，显著降低了抖动和响应延迟，实现了亚微秒级延迟。


<details>
  <summary>Details</summary>
Motivation: Linux中实时响应常受中断争用和定时器处理开销的限制，难以实现亚微秒级延迟。

Method: 提出了一种中断隔离方法，通过专用API选择性调用定时器处理程序并抑制非关键处理器间中断。

Result: 在ARM多核平台上实验表明，该机制能稳定实现低于0.5微秒的响应时间，优于传统Linux PREEMPT-RT配置。

Conclusion: 中断隔离作为一种轻量且有效的策略，在通用操作系统中为确定性实时任务提供了潜力。

Abstract: Real-time responsiveness in Linux is often constrained by interrupt
contention and timer handling overhead, making it challenging to achieve
sub-microsecond latency. This work introduces an interrupt isolation approach
that centralizes and minimizes timer interrupt interference across CPU cores.
By enabling a dedicated API to selectively invoke timer handling routines and
suppress non-critical inter-processor interrupts, our design significantly
reduces jitter and response latency. Experiments conducted on an ARM-based
multicore platform demonstrate that the proposed mechanism consistently
achieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT
configurations. These results highlight the potential of interrupt isolation as
a lightweight and effective strategy for deterministic real-time workloads in
general-purpose operating systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [Entanglement Purification With Finite Latency Classical Communication in Quantum Networks](https://arxiv.org/abs/2509.03667)
*Vivek Vasan,Alexander Nico-Katz,Boulat A. Bash,Daniel C. Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 研究分析了在非即时经典协调下，基于IP网络的量子纠缠纯化协议（BBPSSW、DEJMPS）的可行性，评估了其在不同网络条件和量子存储技术中的表现，并确定了成功与失败的条件。


<details>
  <summary>Details</summary>
Motivation: 量子网络中纠缠对的高保真度存储面临环境退相干的挑战，而纯化过程中的经典通信延迟会进一步加剧退相干问题，因此需要研究在实际网络条件下的纯化协议性能。

Method: 采用微观Lindblad方法分析量子动力学，结合当前城域IP网络延迟统计和量子存储测试平台的参数，评估BBPSSW和DEJMPS协议的性能。

Result: 确定了纠缠纯化成功与失败的临界条件（通过保真度等值线划分），并量化了完成多轮纯化所需的纠缠对数量及其稳态吞吐量。

Conclusion: 研究为当前及未来网络中部署纠缠纯化提供了延迟预算、存储质量目标和资源开销估算。

Abstract: Quantum networks rely on high fidelity entangled pairs distributed to nodes,
but maintaining their fidelity is challenged by environmental decoherence
during storage. Entanglement purification is used to restore fidelity, but the
idle periods imposed by the associated classical communication delays
counteract this goal by exposing the states to further decoherence. In this
work, we analyze the practical viability of entanglement purification protocols
(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet
protocol (IP) communications networks. We present a comprehensive performance
evaluation of these protocols in various network conditions for a range of
quantum memory technologies. We employ a microscopic Lindblad treatment of the
underlying quantum dynamics, and use current-generation metropolitan IP network
latency statistics and parameters drawn from quantum memory testbeds. In doing
so we identify the regions in which entanglement purification succeeds and
fails, delineated by break-even iso-fidelity contours in the phase space. We
then determine the total number of entangled pairs required to complete a
multi-round purification protocol, and the steady-state throughput of entangled
pairs with purified fidelities that exceed application-specific thresholds.
This provides latency budgets, memory quality targets, and resource-overhead
estimates for deploying purification on current and near-future networks.

</details>


### [16] [Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization](https://arxiv.org/abs/2509.03762)
*Sathwik Chadaga,Eytan Modiano*

Main category: cs.NI

TL;DR: 论文研究了队列网络中未知边传输成本的联合路由和调度问题，提出了一种结合Lyapunov漂移加惩罚和多臂老虎机技术的控制策略，实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决队列网络中未知传输成本下的联合路由和调度问题，同时确保网络稳定性。

Method: 结合Lyapunov漂移加惩罚优化和多臂老虎机技术，设计网络控制策略。

Result: 策略实现了$O(\sqrt{T}\log T)$的次线性遗憾，并通过仿真验证了其有效性。

Conclusion: 提出的策略在未知成本和到达的情况下，实现了接近最优的性能。

Abstract: We consider the problem of joint routing and scheduling in queueing networks,
where the edge transmission costs are unknown. At each time-slot, the network
controller receives noisy observations of transmission costs only for those
edges it selects for transmission. The network controller's objective is to
make routing and scheduling decisions so that the total expected cost is
minimized. This problem exhibits an exploration-exploitation trade-off,
however, previous bandit-style solutions cannot be directly applied to this
problem due to the queueing dynamics. In order to ensure network stability, the
network controller needs to optimize throughput and cost simultaneously. We
show that the best achievable cost is lower bounded by the solution to a static
optimization problem, and develop a network control policy using techniques
from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show
that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as
compared to the best policy that has complete knowledge of arrivals and costs.
Finally, we evaluate the proposed policy using simulations and show that its
regret is indeed sub-linear.

</details>


### [17] [A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements](https://arxiv.org/abs/2509.03818)
*Sherwan Jalal Abdullah,Sravan Reddy Chintareddy,Victor S. Frost,Shawn Keshmiri,Morteza Hashemi*

Main category: cs.NI

TL;DR: 该论文开发了一种基于无人机的移动网络性能测量平台，用于在传统测试方法耗时、费力且危险的地方收集覆盖率和服务质量数据。


<details>
  <summary>Details</summary>
Motivation: 传统众包方法在农村地区因人口密度低和地形复杂而效果不佳，因此需要一种更高效的测量方式。

Method: 平台通过集成机载计算单元和商用蜂窝调制解调器，收集无线接入网络信号和端到端性能数据，并通过地理空间映射和统计分析展示结果。

Result: 实验显示，高空信号功率因视线条件改善而提升，但信号质量因邻区干扰而下降；多数测试区域信号质量和吞吐量表现良好。

Conclusion: 无人机平台能有效收集和分析移动网络性能数据，但信号强度与空间覆盖一致性需进一步研究。

Abstract: In this work, we develop a measurement platform to capture mobile network
performance metrics including coverage and quality of service in regions where
conventional coverage testing approaches are frequently time-intensive,
labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing
methods are used to collect cellular network performance metrics. However,
these approaches are inadequate in rural areas due to low-density population,
and difficult terrain. The platform described here is a UAV-based and is
designed to investigate the mobile network performance through aerial
operations and gather Radio Access Network (RAN) signal alongside end-to-end
network performance metrics. Our platform gathers metrics through the
integration of an onboard computation unit and commercial off-the-shelf
cellular modem. The gathered data are subsequently analyzed and displayed using
geospatial mapping utilities and statistical techniques to deliver key
observations on cellular network performance. Experimental results showed that
the received signal power improves at higher altitudes due to enhanced
line-of-sight (LoS) conditions as expected. However, the signal quality
degrades as a result of increased interference from neighboring cells. The
analysis reveals that for most of the geographic area covered in the initial
experiments the system maintained acceptable signal quality, with adequate
throughput performance for both uplink and downlink communications, while
maintaining satisfactory round-trip time characteristics. Notably, the
experiment showed that a strong radio signal metric for a given cell does not
necessarily translate to consistent spatial coverage across the tested region.

</details>


### [18] [Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research](https://arxiv.org/abs/2509.03901)
*Katarzyna Kosek-Szott,Szymon Szott,Wojciech Ciezobka,Maksymilian Wojnar,Krzysztof Rusek,Jonathan Segev*

Main category: cs.NI

TL;DR: 本文综述了IEEE 802.11mc FTM协议在室内定位中的应用及其最新增强技术，填补了相关研究空白，并总结了重要研究成果和未来方向。


<details>
  <summary>Details</summary>
Motivation: 室内定位技术为家庭、办公室和工业网络用户提供了多种功能，如导航、资产跟踪和紧急服务支持。IEEE 802.11mc FTM协议因其高精度和设备支持潜力巨大，但缺乏系统性综述。

Method: 作者分类并回顾了180多篇研究论文，涵盖FTM实际精度、机器学习改进方法、与其他定位系统的结合、应用及安全问题。

Result: 总结了FTM在室内定位中的重要成果，提出了进一步提升其性能的方法。

Conclusion: 基于综述，论文明确了FTM研究的现状和未来研究方向，为该领域的进一步发展提供了指导。

Abstract: Indoor positioning is an enabling technology for home, office, and industrial
network users because it provides numerous information and communication
technology (ICT) and Internet of things (IoT) functionalities such as indoor
navigation, smart meter localization, asset tracking, support for emergency
services, and detection of hazardous situations. The IEEE 802.11mc fine timing
measurement (FTM) protocol (commercially known as Wi-Fi Location) has great
potential to enable indoor positioning in future generation devices, primarily
because of the high availability of Wi-Fi networks, FTM's high accuracy and
device support. Furthermore, new FTM enhancements are available in the released
(802.11az) and recently completed (802.11bk) amendments. Despite the multitude
of literature reviews on indoor positioning, a survey dedicated to FTM and its
recent enhancements has so far been lacking. We fill this gap by classifying
and reviewing over 180 research papers related to the practical accuracy
achieved with FTM, methods for improving its accuracy (also with machine
learning), combining FTM with other indoor positioning systems, FTM-based
applications, and security issues. Based on the conducted survey, we summarize
the most important research achievements and formulate open areas for further
research.

</details>


### [19] [Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues](https://arxiv.org/abs/2509.03935)
*Sungho Cho,Sung Il Choi,Seung Hyun Oh,Ian P. Roberts,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 本文提出了一种车联网边缘计算（VEC）中的并行任务执行策略，通过任务卸载优化方案减少用户等待延迟，理论与实验验证了其全局最优性。


<details>
  <summary>Details</summary>
Motivation: 车联网边缘计算中，任务卸载的资源利用不均和负载拥堵问题亟需解决，以最小化用户的等待延迟。

Method: 基于网络协作的任务卸载方案，通过预测服务器瞬时处理能力识别过载服务器，并结合队列离散变量进行精确估计。

Result: 理论与实验证明，所提方案在延迟减少上实现全局最优，通过真实地图仿真验证了可行性。

Conclusion: 预测服务器处理能力和精确队列估计是优化延迟的关键，新方案在性能上优于现有方法。

Abstract: This work considers a parallel task execution strategy in vehicular edge
computing (VEC) networks, where edge servers are deployed along the roadside to
process offloaded computational tasks of vehicular users. To minimize the
overall waiting delay among vehicular users, a novel task offloading solution
is implemented based on the network cooperation balancing resource
under-utilization and load congestion. Dual evaluation through theoretical and
numerical ways shows that the developed solution achieves a globally optimal
delay reduction performance compared to existing methods, which is also
approved by the feasibility test over a real-map virtual environment. The
in-depth analysis reveals that predicting the instantaneous processing power of
edge servers facilitates the identification of overloaded servers, which is
critical for determining network delay. By considering discrete variables of
the queue, the proposed technique's precise estimation can effectively address
these combinatorial challenges to achieve optimal performance.

</details>


### [20] [Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding](https://arxiv.org/abs/2509.04219)
*Leandro Márcio Bertholdo,Renan Barreto Paredes,Gabriela de Lima Marin,Cesar A. H. Loureiro,Milton Kaoru Kashiwakura Pedro de Botelho Marcos*

Main category: cs.NI

TL;DR: 研究分析了巴西洪灾对电信网络的影响，构建了包含互联网测量、光纤切断报告等的数据集，揭示了基础设施脆弱性和恢复趋势。


<details>
  <summary>Details</summary>
Motivation: 评估极端气候事件中电信基础设施的韧性，为未来灾难恢复策略和系统开发提供支持。

Method: 通过集成互联网测量、光纤切断报告和路由数据，结合水文与运营因素分析网络中断。

Result: 初步发现包括连接恢复趋势、基础设施脆弱性以及用户行为变化。

Conclusion: 数据集和分析为未来灾难恢复和电信系统韧性研究提供了基础。

Abstract: In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused
widespread damage to infrastructure, impacting over 400 cities and 2.3 million
people. This study presents the construction of comprehensive
telecommunications datasets during this climatic event, encompassing Internet
measurements, fiber cut reports, and Internet Exchange routing data. By
correlating network disruptions with hydrological and operational factors, the
dataset offers insights into the resilience of fiber networks, data centers,
and Internet traffic during critical events. For each scenario, we investigate
failures related to the Information and Communication Technology infrastructure
and highlight the challenges faced when its resilience is critically tested.
Preliminary findings reveal trends in connectivity restoration, infrastructure
vulnerabilities, and user behavior changes. These datasets and pre-analysis aim
to support future research on disaster recovery strategies and the development
of robust telecommunications systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [21] [A Cegar-centric Bounded Reachability Analysis for Compositional Affine Hybrid Systems](https://arxiv.org/abs/2509.03560)
*Atanu Kundu,Pratyay Sarkar,Rajarshi Ray*

Main category: cs.LO

TL;DR: 提出了一种基于CEGAR的有界可达性分析算法，用于处理具有分段仿射动态的组成混合系统，避免了显式计算产品自动机，并通过抽象反例验证和优化提升了效率。


<details>
  <summary>Details</summary>
Motivation: 解决组成混合系统在可达性分析中面临的语义保持和状态爆炸问题。

Method: 结合CEGAR原则，在离散抽象中搜索反例，通过符号可达性分析进行状态空间细化，并采用不同组合语义优化效率。

Result: 在工具SAT-Reach中实现了该算法，展示了其可扩展性优势。

Conclusion: 该方法有效解决了组成混合系统的可达性问题，并通过优化显著提升了效率。

Abstract: Reachability analysis of compositional hybrid systems, where individual
components are modeled as hybrid automata, poses unique challenges. In addition
to preserving the compositional semantics while computing system behaviors,
algorithms have to cater to the explosion in the number of locations in the
parallel product automaton. In this paper, we propose a bounded reachability
analysis algorithm for compositional hybrid systems with piecewise affine
dynamics, based on the principle of counterexample guided abstraction
refinement (CEGAR). In particular, the algorithm searches for a counterexample
in the discrete abstraction of the composition model, without explicitly
computing a product automaton. When a counterexample is discovered in the
abstraction, its validity is verified by a refinement of the state-space guided
by the abstract counterexample. The state-space refinement is through a
symbolic reachability analysis, particularly using a state-of-the-art algorithm
with support functions as the continuous state representation. In addition, the
algorithm mixes different semantics of composition with the objective of
improved efficiency. Step compositional semantics is followed while exploring
the abstract (discrete) state-space, while shallow compositional semantics is
followed during state-space refinement with symbolic reachability analysis.
Optimizations such as caching the results of the symbolic reachability
analysis, which can be later reused, have been proposed. We implement this
algorithm in the tool SAT-Reach and demonstrate the scalability benefits.

</details>


### [22] [Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis](https://arxiv.org/abs/2509.04129)
*Mickael Randour*

Main category: cs.LO

TL;DR: 论文探讨了控制器合成中策略的复杂性，尤其是内存和随机性的影响，并展望了传统复杂性概念的扩展。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索控制器合成中简单策略（如有限内存）的优势及其复杂性，以提高实际控制器的可理解性和经济性。

Method: 采用博弈论方法，将系统与环境互动建模为博弈，研究不同合成背景下的策略复杂性。

Result: 讨论了关于内存和随机性的最新研究成果。

Conclusion: 总结了对策略复杂性研究的现状，并指出了未来可能扩展的方向。

Abstract: In the game-theoretic approach to controller synthesis, we model the
interaction between a system to be controlled and its environment as a game
between these entities, and we seek an appropriate (e.g., winning or optimal)
strategy for the system. This strategy then serves as a formal blueprint for a
real-world controller. A common belief is that simple (e.g., using limited
memory) strategies are better: corresponding controllers are easier to conceive
and understand, and cheaper to produce and maintain.
  This invited contribution focuses on the complexity of strategies in a
variety of synthesis contexts. We discuss recent results concerning memory and
randomness, and take a brief look at what lies beyond our traditional notions
of complexity for strategies.

</details>


### [23] [Janus-faces of temporal constraint languages: a dichotomy of expressivity](https://arxiv.org/abs/2509.04347)
*Johanna Brunar,Michael Pinsker,Moritz Schöbi*

Main category: cs.LO

TL;DR: 本文揭示了Bodirsky-Kára分类中时间约束语言的有限表达能力，并提供了新的代数结果和一致证明。


<details>
  <summary>Details</summary>
Motivation: 探索Bodirsky-Kára分类中时间约束语言的表达能力和复杂性，填补算法和代数不变量的空白。

Method: 分析时间约束语言的pp构造能力，证明其只能pp解释有限的图和超图。

Result: 发现这些语言承认4元伪Siggers多态性，并支持Bodirsky-Pinsker猜想的可能性。

Conclusion: 时间约束语言的表达受限性为更广泛的猜想提供了新的支持。

Abstract: The Bodirsky-K\'ara classification of temporal constraint languages stands as
one of the earliest and most seminal complexity classifications within
infinite-domain Constraint Satisfaction Problems (CSPs), yet it remains one of
the most mysterious in terms of algorithms and algebraic invariants for the
tractable cases. We show that those temporal languages which do not
pp-construct EVERYTHING (and thus by the classification are solvable in
polynomial time) have, in fact, very limited expressive power as measured by
the graphs and hypergraphs they can pp-interpret. This limitation yields many
previously unknown algebraic consequences, while also providing new, uniform
proofs for known invariance properties. In particular, we show that such
temporal constraint languages admit $4$-ary pseudo-Siggers polymorphisms -- a
result that sustains the possibility that the existence of such polymorphisms
extends to the much broader context of the Bodirsky-Pinsker conjecture.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling](https://arxiv.org/abs/2509.03678)
*Xianghan Wang,Chingshuan Hsiao,Shimei Qiu*

Main category: cs.HC

TL;DR: Promisedland是一个结合文化叙事与生态教育的混合现实（MR）叙事景点，通过低成本、高保真的Diorama-to-Virtual流程快速原型化，最终在Meta Quest上实现互动体验。


<details>
  <summary>Details</summary>
Motivation: 解决未来地球元素失衡问题的叙事需求，同时探索物理与数字元素结合的沉浸式叙事框架。

Method: 采用手工制作物理模型、3D扫描后集成到Unreal Engine的流程，结合Stewart平台提供动态反馈。

Result: 开发出支持实时互动和视觉反馈的原型，为未来XR叙事装置提供可复制的设计蓝图。

Conclusion: Promisedland为博物馆和文化展览等场景提出了一种融合物理与数字元素的新型XR叙事框架。

Abstract: Promisedland is a mixed-reality (MR) narrative attraction that combines
cultural storytelling, ecological education, and an innovative hybrid
production workflow. Set in a future Earth suffering from elemental imbalance,
users embark on an interactive journey guided by symbolic characters to restore
harmony through the collection of five classical elements: metal, wood, water,
fire, and earth. To prototype this experience, we introduce a low-cost,
high-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models,
3D scanning, and integrating them into Unreal Engine. This process enables
rapid spatial prototyping while preserving the material expressiveness and
narrative consistency of the physical environment. To further enhance
immersion, the experience incorporates a Stewart Platform to provide motion
feedback synchronized with the virtual ride dynamics, reinforcing spatial
presence and embodied engagement. The final prototype runs on Meta Quest,
supporting dynamic interactions and real-time visual feedback. Promisedland
offers a replicable design blueprint for future XR narrative installations
across museums, cultural exhibitions, and themed entertainment. It proposes a
new framework for XR Narrative Attractions - where physical and digital
elements converge to deepen immersion, agency, and emotional engagement.

</details>


### [25] [Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations](https://arxiv.org/abs/2509.03693)
*Yeaeun Gong,Yifan Liu,Lanyu Shang,Na Wei,Dong Wang*

Main category: cs.HC

TL;DR: 该论文研究了AI解释在帮助用户识别虚假信息方面的有效性，探讨了超越传统内容解释的其他解释方式（如社交解释及其组合），并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI（XAI）方法主要关注内容解释，缺乏对社会背景等因素的考虑，因此需要探索更多解释类型以改进用户识别虚假信息的能力和体验。

Method: 研究通过两种在线众包实验（COVID-19和政治领域），比较内容解释、社交解释及其组合的效果，并考察解释顺序的影响。

Result: AI解释总体有效，但效果受内容与社交解释对齐度及解释顺序的影响，不同领域（如COVID-19与政治）存在差异。

Conclusion: 本研究为AI解释的设计提供了更深入的见解，强调了不同解释类型及其组合在虚假信息检测中的作用。

Abstract: In this paper, we study the problem of AI explanation of misinformation,
where the goal is to identify explanation designs that help improve users'
misinformation detection abilities and their overall user experiences. Our work
is motivated by the limitations of current Explainable AI (XAI) approaches,
which predominantly focus on content explanations that elucidate the linguistic
features and sentence structures of the misinformation. To address this
limitation, we explore various explanations beyond content explanation, such as
"social explanation" that considers the broader social context surrounding
misinformation, as well as a "combined explanation" where both the content and
social explanations are presented in scenarios that are either aligned or
misaligned with each other. To evaluate the comparative effectiveness of these
AI explanations, we conduct two online crowdsourcing experiments in the
COVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our
results show that AI explanations are generally effective in aiding users to
detect misinformation, with effectiveness significantly influenced by the
alignment between content and social explanations. We also find that the order
in which explanation types are presented - specifically, whether a content or
social explanation comes first - can influence detection accuracy, with
differences found between the COVID-19 and Political domains. This work
contributes towards more effective design of AI explanations, fostering a
deeper understanding of how different explanation types and their combinations
influence misinformation detection.

</details>


### [26] [Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support](https://arxiv.org/abs/2509.03741)
*Eduardo Davalos,Yike Zhang,Shruti Jain,Namrata Srivastava,Trieu Truong,Nafees-ul Haque,Tristan Van,Jorge Salas,Sara McFadden,Sun-Joo Cho,Gautam Biswas,Amanda Goodwin*

Main category: cs.HC

TL;DR: 该论文研究了如何通过眼动数据开发适用于课堂的学习分析仪表板，结合用户中心设计和数据叙事原则，展示了其在反思、形成性评估和教学决策中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 眼动数据在课堂教育技术中应用不足，主要由于数据解释和可访问性问题，本文旨在探索如何有效利用眼动数据支持教学。

Method: 通过五项研究迭代设计和评估了基于眼动的学习分析仪表板，结合用户中心设计、数据叙事及大语言模型的对话代理来降低认知负担。

Result: 研究发现，熟悉的可视化、分层次解释和叙事支持可以使眼动数据更具教育价值，大语言模型的引入进一步降低了数据理解的难度。

Conclusion: 本文为未来教育技术系统整合新型数据模态提供了设计启示，强调需结合用户友好性和教学实用性。

Abstract: Eye-tracking offers rich insights into student cognition and engagement, but
remains underutilized in classroom-facing educational technology due to
challenges in data interpretation and accessibility. In this paper, we present
the iterative design and evaluation of a gaze-based learning analytics
dashboard for English Language Arts (ELA), developed through five studies
involving teachers and students. Guided by user-centered design and data
storytelling principles, we explored how gaze data can support reflection,
formative assessment, and instructional decision-making. Our findings
demonstrate that gaze analytics can be approachable and pedagogically valuable
when supported by familiar visualizations, layered explanations, and narrative
scaffolds. We further show how a conversational agent, powered by a large
language model (LLM), can lower cognitive barriers to interpreting gaze data by
enabling natural language interactions with multimodal learning analytics. We
conclude with design implications for future EdTech systems that aim to
integrate novel data modalities in classroom contexts.

</details>


### [27] [Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks](https://arxiv.org/abs/2509.03792)
*Ryo Yonetani,Kotaro Hara*

Main category: cs.HC

TL;DR: Collective Landmark Mapper是一个通过智能手机IMU数据和用户文本输入生成室内语义地标地图的系统，适用于零售和非零售场景。


<details>
  <summary>Details</summary>
Motivation: 通过用户在任务中的自然行为（如记笔记）生成地标地图，满足零售和非零售应用的实际需求。

Method: 利用智能手机IMU数据和用户文本输入识别地标，并通过多用户数据聚合生成统一地图。

Result: 用户研究显示系统可行且在零售和非零售场景中表现优越。

Conclusion: 系统具有广泛适用性，尤其在零售和非零售应用中表现突出。

Abstract: This paper presents Collective Landmark Mapper, a novel map-as-a-by-product
system for generating semantic landmark maps of indoor environments. Consider
users engaged in situated tasks that require them to navigate these
environments and regularly take notes on their smartphones. Collective Landmark
Mapper exploits the smartphone's IMU data and the user's free text input during
these tasks to identify a set of landmarks encountered by the user. The
identified landmarks are then aggregated across multiple users to generate a
unified map representing the positions and semantic information of all
landmarks. In developing the proposed system, we focused specifically on retail
applications and conducted a formative interview with stakeholders to confirm
their practical needs that motivate the map-as-a-byproduct approach. Our user
study demonstrates the feasibility of the proposed system and its superior
mapping performance in two different setups: creating a product availability
map from restocking checklist tasks at a retail store and constructing a room
usage map from office inspection tasks, further demonstrating the potential
applicability to non-retail applications.

</details>


### [28] [Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment](https://arxiv.org/abs/2509.03812)
*Shadeeb Hossain,Natalie Sommer,Neda Adib*

Main category: cs.HC

TL;DR: 提出了一种动态游戏化架构，用于扩展现实虚拟培训环境，提升STEM教育。


<details>
  <summary>Details</summary>
Motivation: 通过沉浸式和适应性学习增强STEM教育。

Method: 四阶段系统构建，涵盖安全隐私措施和风险缓解策略。

Result: 系统设计支持沉浸式学习，但面临技术和成本挑战。

Conclusion: 需解决技术复杂性和成本问题以实现大规模应用。

Abstract: This paper presents a dynamic gamification architecture for an Extended
Reality Artificial Intelligence virtual training environment designed to
enhance STEM education through immersive adaptive, and kinesthetic learning.
The proposed system can be introduced in four phases: Introduction Phase,
Component Development Phase, Fault Introduction and Correction Phase and
Generative AI XR scenarios Phase. Security and privacy are discussed via a
defense-in-depth approach spanning client, middleware, and backend layers,
incorporating AES 256 encryption, multi-factor authentication, role-based
access control and GDPR or FERPA compliance. Risks such as sensor exploitation,
perceptual manipulation, and virtual physical harm are identified, with
mitigation strategies embedded at the design stage. Potential barriers to large
scale adoption-including technical complexity, cost of deployment, and need for
cybersecurity expertise are discussed.

</details>


### [29] ["Low Frequency Tweeters Have More to Say!" A New Approach to Identify Importance of Tweets](https://arxiv.org/abs/2509.03931)
*Gautam Khannaa,Yeliz Yesilada,Sukru Eraslan,Simon Harper*

Main category: cs.HC

TL;DR: 针对Twitter信息过载问题，提出基于用户推文频率的个性化排序方法，发现低频推文用户（每周少于10条）的推文更受关注。


<details>
  <summary>Details</summary>
Motivation: 解决Twitter用户活动流中的信息过载问题，并通过分析推文频率与重要性的关系，确保低频用户的推文不被淹没。

Method: 提出六个新指标，基于推文互动（如转发、点赞、评论）和作者网络互动，评估推文重要性。假设低频推文用户的内容更有价值。

Result: 研究发现，每周推文少于10条的用户更可能被其关注者视为重要，其推文更具价值。

Conclusion: 通过推文频率分带重新排序活动流，可提升用户体验，并为识别重要推文用户提供评分依据。

Abstract: Twitter is one of the most popular social media platforms.With a large number
of tweets, the activity feed of users becomes noisy, challenging to read, and
most importantly tweets often get lost. We present a new approach to
personalise the ranking of the tweets toward solving the problem of information
overload which is achieved by analysing the relationship between the importance
of tweets to the frequency at which the author tweets. The hypothesis tested is
that "low-frequency tweeters have more to say", i.e. if a user who tweets
infrequently actually goes to the effort of tweeting, then it is more likely to
be of more importance or contain more "meaning" than a tweet by a user who
tweets continuously. We propose six new measures to evaluate the importance of
tweets based on the ability of the tweet to drive interaction among its
readers, which is measured through metrics such as retweets, favourites, and
comments, and the extent of the author's network interacting with the tweet.
Our study shows that users who tweeted less than ten tweets per week were more
likely to be perceived as important by their followers and have the most
important messages. This identified tweet-frequency band could be used to
reorder the activity feed of users and such reordering would ensure the
messages of low-frequency tweeters do not get lost in the stream of tweets.
This could also serve as a scoring index for Twitter users to identify users
frequently tweeting important messages.

</details>


### [30] [Spiking Neural Network Decoders of Finger Forces from High-Density Intramuscular Microelectrode Arrays](https://arxiv.org/abs/2509.04088)
*Farah Baracat,Agnese Grison,Dario Farina,Giacomo Indiveri,Elisa Donati*

Main category: cs.HC

TL;DR: 该论文提出了一种基于尖峰神经网络的解码框架，用于恢复辅助技术中的自然手指控制，通过分析运动单元活动实现高精度、高效和鲁棒的解码。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了在辅助技术中实现连续、高精度、高效且鲁棒的手指运动意图解码。

Method: 方法是将尖峰神经网络（SNN）与高密度肌内微电极阵列提取的运动单元活动相结合，解码等长收缩时的单个手指力量。

Result: 结果表明，浅层SNN能够以较低的内存需求和延迟可靠解码手指运动意图，且解码精度具有竞争力。

Conclusion: 该研究为将SNN集成到手指力量解码系统提供了实用框架，输入表示的选择可满足特定应用对精度、鲁棒性和内存效率的需求。

Abstract: Restoring naturalistic finger control in assistive technologies requires the
continuous decoding of motor intent with high accuracy, efficiency, and
robustness. Here, we present a spike-based decoding framework that integrates
spiking neural networks (SNNs) with motor unit activity extracted from
high-density intramuscular microelectrode arrays. We demonstrate simultaneous
and proportional decoding of individual finger forces from motor unit spike
trains during isometric contractions at 15% of maximum voluntary contraction
using SNNs. We systematically evaluated alternative SNN decoder configurations
and compared two possible input modalities: physiologically grounded motor unit
spike trains and spike-encoded intramuscular EMG signals. Through this
comparison, we quantified trade-offs between decoding accuracy, memory
footprint, and robustness to input errors. The results showed that shallow SNNs
can reliably decode finger-level motor intent with competitive accuracy and
minimal latency, while operating with reduced memory requirements and without
the need for external preprocessing buffers. This work provides a practical
blueprint for integrating SNNs into finger-level force decoding systems,
demonstrating how the choice of input representation can be strategically
tailored to meet application-specific requirements for accuracy, robustness,
and memory efficiency.

</details>


### [31] [Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric Similarity Learning of Motion Patterns](https://arxiv.org/abs/2509.04174)
*Christian Merz,Lukas Schach,Marie Luisa Fiedler,Jean-Luc Lugrin,Carolin Wienrich,Marc Erich Latoschik*

Main category: cs.HC

TL;DR: 论文提出了一种基于深度度量相似学习的生物特征用户模型，用于无干扰地检测XR系统中用户行为变化，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究XR环境中用户因化身变化（如身高差异）引发的行为变化（如Proteus效应），需更高效的检测方法。

Method: 使用深度度量相似学习模型，通过高维嵌入向量识别用户行为变化，并与传统运动分析和主观问卷对比。

Result: 模型成功识别化身身高变化引发的行为差异，优于非学习的运动分析和主观评估。

Conclusion: 该方法无需额外用户输入，具有通用性和可扩展性，适用于实时分析XR中的行为变化。

Abstract: This paper introduces an unobtrusive in-situ measurement method to detect
user behavior changes during arbitrary exposures in XR systems. Here, such
behavior changes are typically associated with the Proteus effect or bodily
affordances elicited by different avatars that the users embody in XR. We
present a biometric user model based on deep metric similarity learning, which
uses high-dimensional embeddings as reference vectors to identify behavior
changes of individual users. We evaluate our model against two alternative
approaches: a (non-learned) motion analysis based on central tendencies of
movement patterns and subjective post-exposure embodiment questionnaires
frequently used in various XR exposures. In a within-subject study,
participants performed a fruit collection task while embodying avatars of
different body heights (short, actual-height, and tall). Subjective assessments
confirmed the effective manipulation of perceived body schema, while the
(non-learned) objective analyses of head and hand movements revealed
significant differences across conditions. Our similarity learning model
trained on the motion data successfully identified the elicited behavior change
for various query and reference data pairings of the avatar conditions. The
approach has several advantages in comparison to existing methods: 1) In-situ
measurement without additional user input, 2) generalizable and scalable motion
analysis for various use cases, 3) user-specific analysis on the individual
level, and 4) with a trained model, users can be added and evaluated in real
time to study how avatar changes affect behavior.

</details>


### [32] [Would I regret being different? The influence of social norms on attitudes toward AI usage](https://arxiv.org/abs/2509.04241)
*Jaroslaw Kornowicz,Maurice Pape,Kirsten Thommes*

Main category: cs.HC

TL;DR: 研究表明社会规范能减少算法厌恶，但其形成机制尚不明确。论文探讨了规范来源（同事或上级）如何影响AI使用行为，发现AI选择引发更高后悔，但在AI支持规范下减弱。


<details>
  <summary>Details</summary>
Motivation: 探究社会规范如何影响AI采用，特别是规范来源（同事或上级）的作用，以帮助组织有效推广AI。

Method: 采用在线小实验和定性数据，分析参与者在（反）规范行为后的感受和理由。

Result: 反规范选择引发更高后悔；AI支持规范显著减弱AI选择的后悔；同事和上级影响存在但未显著改变后悔程度。

Conclusion: 社会规范中的后悔厌恶是驱动AI决策模仿的核心机制。

Abstract: Prior research shows that social norms can reduce algorithm aversion, but
little is known about how such norms become established. Most accounts
emphasize technological and individual determinants, yet AI adoption unfolds
within organizational social contexts shaped by peers and supervisors. We ask
whether the source of the norm-peers or supervisors-shapes AI usage behavior.
This question is practically relevant for organizations seeking to promote
effective AI adoption. We conducted an online vignette experiment, complemented
by qualitative data on participants' feelings and justifications after
(counter-)normative behavior. In line with the theory, counter-normative
choices elicited higher regret than norm-adherent choices. On average, choosing
AI increased regret compared to choosing an human. This aversion was weaker
when AI use was presented as the prevailing norm, indicating a statistically
significant interaction between AI use and an AI-favoring norm. Participants
also attributed less blame to technology than to humans, which increased regret
when AI was chosen over human expertise. Both peer and supervisor influence
emerged as relevant factors, though contrary to expectations they did not
significantly affect regret. Our findings suggest that regret aversion,
embedded in social norms, is a central mechanism driving imitation in
AI-related decision-making.

</details>


### [33] [MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals](https://arxiv.org/abs/2509.04254)
*Meisam Jamshidi Seikavandi,Fabricio Batista Narcizo,Ted Vucurevich,Andrew Burke Dittberner,Paolo Burelli*

Main category: cs.HC

TL;DR: MuMTAffect是一种新型多模态多任务情感嵌入网络，用于从生理信号片段中联合进行情绪分类和人格预测。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是基于构建情感理论，设计一个能够分离核心情感编码和高层次概念化的架构，从而提升情感计算的准确性和个性化。

Method: MuMTAffect通过专用的基于Transformer的编码器整合多种生理模态（如瞳孔扩张、视线、面部动作单元和皮肤电反应），并使用融合Transformer建模跨模态交互。

Result: 在AFFEC数据集上的实验表明，MuMTAffect在情绪识别性能上显著提升，且不同模态数据对情绪分类的贡献各有侧重。

Conclusion: MuMTAffect的模块化设计使其具有高度可扩展性和适应性，为未来跨主体泛化的情感计算系统提供了新方向。

Abstract: We present MuMTAffect, a novel Multimodal Multitask Affective Embedding
Network designed for joint emotion classification and personality prediction
(re-identification) from short physiological signal segments. MuMTAffect
integrates multiple physiological modalities pupil dilation, eye gaze, facial
action units, and galvanic skin response using dedicated, transformer-based
encoders for each modality and a fusion transformer to model cross-modal
interactions. Inspired by the Theory of Constructed Emotion, the architecture
explicitly separates core affect encoding (valence/arousal) from higher-level
conceptualization, thereby grounding predictions in contemporary affective
neuroscience. Personality trait prediction is leveraged as an auxiliary task to
generate robust, user-specific affective embeddings, significantly enhancing
emotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset,
demonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin
response substantially improve arousal classification, while pupil and gaze
data enhance valence discrimination. The inherent modularity of MuMTAffect
allows effortless integration of additional modalities, ensuring scalability
and adaptability. Extensive experiments and ablation studies underscore the
efficacy of our multimodal multitask approach in creating personalized,
context-aware affective computing systems, highlighting pathways for further
advancements in cross-subject generalisation.

</details>


### [34] [HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning](https://arxiv.org/abs/2509.04303)
*Georgios Makridis,Georgios Fragiadakis,Jorge Oliveira,Tomaz Saraiva,Philip Mavrepis,Georgios Fatouros,Dimosthenis Kyriazis*

Main category: cs.HC

TL;DR: 该论文介绍了HumAIne-chatbot，一种通过用户画像框架实现个性化回复的对话AI系统。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统通常提供通用反馈，忽略了用户个性化需求，因此需要开发能自适应调整对话管理的系统。

Method: 系统通过预训练GPT生成的虚拟角色建立用户类型先验，并利用在线强化学习结合隐式和显式反馈优化用户模型。

Result: 实验表明，个性化功能显著提高了用户满意度、个性化准确度和任务完成率。

Conclusion: AI驱动的用户画像有效提升了对话系统的个性化能力，为未来实际应用奠定了基础。

Abstract: Current conversational AI systems often provide generic, one-size-fits-all
interactions that overlook individual user characteristics and lack adaptive
dialogue management. To address this gap, we introduce
\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes
responses through a novel user profiling framework. The system is pre-trained
on a diverse set of GPT-generated virtual personas to establish a broad prior
over user types. During live interactions, an online reinforcement learning
agent refines per-user models by combining implicit signals (e.g. typing speed,
sentiment, engagement duration) with explicit feedback (e.g., likes and
dislikes). This profile dynamically informs the chatbot dialogue policy,
enabling real-time adaptation of both content and style. To evaluate the
system, we performed controlled experiments with 50 synthetic personas in
multiple conversation domains. The results showed consistent improvements in
user satisfaction, personalization accuracy, and task achievement when
personalization features were enabled. Statistical analysis confirmed
significant differences between personalized and nonpersonalized conditions,
with large effect sizes across key metrics. These findings highlight the
effectiveness of AI-driven user profiling and provide a strong foundation for
future real-world validation.

</details>


### [35] [Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing Clinical Notes](https://arxiv.org/abs/2509.04340)
*Kristina L. Kupferschmidt,Kieran O'Doherty,Joshua A. Skorburg*

Main category: cs.HC

TL;DR: 大型语言模型（LLMs）在减轻临床文档负担方面具有潜力，但其成功依赖于灵活的整合方式和对临床医生自主性的支持。研究发现，文档挑战具有社会技术性，需要综合考虑工作流程、政策和技术限制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLMs如何支持儿科康复治疗师减轻文档负担，并分析其在实际临床环境中的应用挑战。

Method: 在加拿大安大略省的一家儿科康复机构进行了一项定性研究，20名临床医师参与了两项AI技术的试点项目：一款通用专有LLM和一款基于历史文档定制化的模型。

Result: 研究发现文档挑战具有社会技术性，涉及四个关键主题：工作流程的异质性、文档负担的系统性、对灵活工具的需求以及Clinician与AI系统的相互学习。LLMs的潜力依赖于灵活整合和临床医生的自主权。

Conclusion: LLMs在临床文档减轻方面有潜力，但成功应用需要灵活的集成策略和全面的培训计划，以应对临床环境的复杂性。

Abstract: Large Language Models (LLMs) are often proposed as tools to streamline
clinical documentation, a task viewed as both high-volume and low-risk.
However, even seemingly straightforward applications of LLMs raise complex
sociotechnical considerations to translate into practice. This case study,
conducted at KidsAbility, a pediatric rehabilitation facility in Ontario,
Canada examined the use of LLMs to support occupational therapists in reducing
documentation burden.We conducted a qualitative study involving 20 clinicians
who participated in pilot programs using two AI technologies: a general-purpose
proprietary LLM and a bespoke model fine-tuned on proprietary historical
documentation.
  Our findings reveal that documentation challenges are sociotechnical in
nature, shaped by clinical workflows, organizational policies, and system
constraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)
the documentation burden is systemic and not directly linked to the creation of
any single type of documentation, (3) the need for flexible tools and clinician
autonomy, and (4) effective implementation requires mutual learning between
clinicians and AI systems.
  While LLMs show promise in easing documentation tasks, their success will
depend on flexible, adaptive integration that supports clinician autonomy.
Beyond technical performance, sustained adoption will require training programs
and implementation strategies that reflect the complexity of clinical
environments.

</details>


### [36] [SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars](https://arxiv.org/abs/2509.04356)
*Atikkhan Faridkhan Nilgar,Kristof Van Laerhoven,Ayub Kinoti*

Main category: cs.HC

TL;DR: SRWToolkit是一个开源的Wizard of Oz工具包，用于快速开发基于本地大型语言模型的社交机器人角色，支持多模态交互，并通过用户研究验证了其可用性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 旨在简化基于本地大型语言模型的社交机器人角色的快速开发，避免依赖云端服务，提升模块化和设备功能。

Method: 开发了一个基于网页的工具包，支持文本输入、语音按钮和唤醒词命令，提供实时配置功能。通过用户研究（n=11）评估其效果。

Result: 用户研究显示，工具包在可用性、信任和用户体验方面表现出色，支持多样化的机器人角色开发。

Conclusion: SRWToolkit为研究人员提供了高效、可扩展的社交机器人开发工具，推动了人机交互研究的进展。

Abstract: We present SRWToolkit, an open-source Wizard of Oz toolkit designed to
facilitate the rapid prototyping of social robotic avatars powered by local
large language models (LLMs). Our web-based toolkit enables multimodal
interaction through text input, button-activated speech, and wake-word command.
The toolkit offers real-time configuration of avatar appearance, behavior,
language, and voice via an intuitive control panel. In contrast to prior works
that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and
ensures on-device functionality through local LLM inference. In our small-scale
user study ($n=11$), participants created and interacted with diverse robotic
roles (hospital receptionist, mathematics teacher, and driving assistant),
which demonstrated positive outcomes in the toolkit's usability, trust, and
user experience. The toolkit enables rapid and efficient development of robot
characters customized to researchers' needs, supporting scalable research in
human-robot interaction.

</details>


### [37] [Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity](https://arxiv.org/abs/2509.04358)
*Atikkhan Faridkhan Nilgar,Manuel Dietrich,Kristof Van Laerhoven*

Main category: cs.HC

TL;DR: 研究发现，在社交机器人作为健康教练时，用户对隐私的控制权比透明度和机器人主动性更关键。


<details>
  <summary>Details</summary>
Motivation: 探讨社交机器人中用户隐私感知的关键因素（如透明性、用户控制、机器人行为方式）及其影响。

Method: 通过在线研究（N=200）系统性分析透明度、用户控制和机器人行为方式对隐私感知的影响。

Result: 用户控制显著提高隐私感知和信任度，而透明度和机器人主动性影响较小。

Conclusion: 用户控制是隐私保护的核心，未来需进一步研究如何让用户管理机器人的信息处理。

Abstract: Social robots are increasingly recognized as valuable supporters in the field
of well-being coaching. They can function as independent coaches or provide
support alongside human coaches, and healthcare professionals. In coaching
interactions, these robots often handle sensitive information shared by users,
making privacy a relevant issue. Despite this, little is known about the
factors that shape users' privacy perceptions. This research aims to examine
three key factors systematically: (1) the transparency about information usage,
(2) the level of specific user control over how the robot uses their
information, and (3) the robot's behavioral approach - whether it acts
proactively or only responds on demand. Our results from an online study (N =
200) show that even when users grant the robot general access to personal data,
they additionally expect the ability to explicitly control how that information
is interpreted and shared during sessions. Experimental conditions that
provided such control received significantly higher ratings for perceived
privacy appropriateness and trust. Compared to user control, the effects of
transparency and proactivity on privacy appropriateness perception were low,
and we found no significant impact. The results suggest that merely informing
users or proactive sharing is insufficient without accompanying user control.
These insights underscore the need for further research on mechanisms that
allow users to manage robots' information processing and sharing, especially
when social robots take on more proactive roles alongside humans.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [38] [LuxDiT: Lighting Estimation with Video Diffusion Transformer](https://arxiv.org/abs/2509.03680)
*Ruofan Liang,Kai He,Zan Gojcic,Igor Gilitschenski,Sanja Fidler,Nandita Vijaykumar,Zian Wang*

Main category: cs.GR

TL;DR: LuxDiT是一种基于视频扩散变换器的数据驱动方法，用于从单张图像或视频中估计HDR环境光照，通过合成数据训练和低秩适应微调策略，在定量和定性评估中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉和图形学中从单张图像或视频估计场景光照的长期挑战，克服了数据稀缺和多样性不足的问题。

Method: 提出LuxDiT方法，利用视频扩散变换器生成HDR环境光照图，通过合成数据集训练并结合低秩适应微调策略提升输入与输出的语义对齐。

Result: 模型能够从间接视觉线索推断光照，并在真实场景中有效泛化，生成具有高频细节的准确光照预测。

Conclusion: LuxDiT在光照预测任务中优于现有技术，为场景照明估计提供了高效解决方案。

Abstract: Estimating scene lighting from a single image or video remains a longstanding
challenge in computer vision and graphics. Learning-based approaches are
constrained by the scarcity of ground-truth HDR environment maps, which are
expensive to capture and limited in diversity. While recent generative models
offer strong priors for image synthesis, lighting estimation remains difficult
due to its reliance on indirect visual cues, the need to infer global
(non-local) context, and the recovery of high-dynamic-range outputs. We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.
Trained on a large synthetic dataset with diverse lighting conditions, our
model learns to infer illumination from indirect visual cues and generalizes
effectively to real-world scenes. To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas. Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.

</details>


### [39] [Memory Optimization for Convex Hull Support Point Queries](https://arxiv.org/abs/2509.03753)
*Michael Greer*

Main category: cs.GR

TL;DR: 论文通过优化凸壳内存布局，显著提升了支持点查询的计算速度。


<details>
  <summary>Details</summary>
Motivation: 支持点查询是碰撞算法的基础操作，优化其性能对整体算法效率至关重要。

Method: 改进凸壳的内存布局设计。

Result: 根据凸壳顶点数量的不同，计算时间显著减少。

Conclusion: 内存布局优化是提升支持点查询效率的有效方法。

Abstract: This paper evaluates several improvements to the memory layout of convex
hulls to improve computation times for support point queries. The support point
query is a fundamental part of common collision algorithms, and the work
presented achieves a significant speedup depending on the number of vertices of
the convex hull.

</details>


### [40] [ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction](https://arxiv.org/abs/2509.03775)
*Sankeerth Durvasula,Sharanshangar Muhunthan,Zain Moustafa,Richard Chen,Ruofan Liang,Yushi Guan,Nilesh Ahuja,Nilesh Jain,Selvakumar Panneer,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: ContraGS是一种新方法，通过在训练过程中直接使用压缩的3D高斯表示（不减少高斯数量），显著降低了内存消耗和训练/渲染延迟，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅（3DGS）技术需要大量GPU内存存储模型参数，导致训练和渲染效率低下。ContraGS旨在解决这一问题，通过压缩表示减少内存占用。

Method: ContraGS利用码本紧凑存储高斯参数向量，并通过贝叶斯推断框架，使用MCMC采样学习不可微分的压缩表示参数。

Result: ContraGS平均减少3.49倍训练峰值内存，加速训练和渲染1.36倍和1.88倍，同时保持接近现有技术的质量。

Conclusion: ContraGS提供了一种高效训练压缩3DGS模型的方法，显著降低了资源需求，且质量接近最优。

Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.

</details>


### [41] [TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media](https://arxiv.org/abs/2509.04047)
*Ashish Tiwari,Satyam Bhardwaj,Yash Bachwana,Parag Sarvoday Sahu,T. M. Feroz Ali,Bhargava Chintalapati,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 本文提出了一种基于学习的框架TensoIS，用于从稀疏多视角图像中估计异质散射参数，通过Fractal Perlin噪声模拟真实世界中的异质性，并创建了合成数据集HeteroSynth进行验证。


<details>
  <summary>Details</summary>
Motivation: 目前大多数方法假设介质是均匀的，而真实世界中异质介质的散射参数缺乏明确的分布模型。本文旨在通过Perlin噪声模拟这种异质性，并提出一种逆散射的学习方法。

Method: 首先创建了合成数据集HeteroSynth，其中异质散射参数通过Fractal Perlin噪声建模。随后提出了TensoIS框架，利用可学习的低秩张量分量来表示散射体积。

Result: TensoIS在HeteroSynth测试集、烟雾和云几何形状的模拟数据以及真实样本上验证了其有效性。

Conclusion: 本文探索了Perlin噪声分布对异质散射参数的建模能力，并通过TensoIS框架实现了高效的逆散射估计。

Abstract: Estimating scattering parameters of heterogeneous media from images is a
severely under-constrained and challenging problem. Most of the existing
approaches model BSSRDF either through an analysis-by-synthesis approach,
approximating complex path integrals, or using differentiable volume rendering
techniques to account for heterogeneity. However, only a few studies have
applied learning-based methods to estimate subsurface scattering parameters,
but they assume homogeneous media. Interestingly, no specific distribution is
known to us that can explicitly model the heterogeneous scattering parameters
in the real world. Notably, procedural noise models such as Perlin and Fractal
Perlin noise have been effective in representing intricate heterogeneities of
natural, organic, and inorganic surfaces. Leveraging this, we first create
HeteroSynth, a synthetic dataset comprising photorealistic images of
heterogeneous media whose scattering parameters are modeled using Fractal
Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a
learning-based feed-forward framework to estimate these Perlin-distributed
heterogeneous scattering parameters from sparse multi-view image observations.
Instead of directly predicting the 3D scattering parameter volume, TensoIS uses
learnable low-rank tensor components to represent the scattering volume. We
evaluate TensoIS on unseen heterogeneous variations over shapes from the
HeteroSynth test set, smoke and cloud geometries obtained from open-source
realistic volumetric simulations, and some real-world samples to establish its
effectiveness for inverse scattering. Overall, this study is an attempt to
explore Perlin noise distribution, given the lack of any such well-defined
distribution in literature, to potentially model real-world heterogeneous
scattering in a feed-forward manner.

</details>


### [42] [SMooGPT: Stylized Motion Generation using Large Language Models](https://arxiv.org/abs/2509.04058)
*Lei Zhong,Yi Yang,Changjian Li*

Main category: cs.GR

TL;DR: 本文提出了一种基于自然语言和LLM的新方法SMooGPT，通过身体部分文本空间生成风格化运动，解决了现有方法在可解释性、控制和泛化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有风格化运动生成方法在可解释性、控制和泛化能力上存在局限，且对新风格的适应能力不足。

Method: 采用推理-组合-生成的新视角，利用LLM在身体部分文本空间中进行运动生成，提出SMooGPT模型。

Result: 该方法显著提高了可解释性和控制能力，并能有效泛化到新风格。

Conclusion: SMooGPT在纯文本驱动的风格化运动生成中表现出色，解决了现有方法的局限性。

Abstract: Stylized motion generation is actively studied in computer graphics,
especially benefiting from the rapid advances in diffusion models. The goal of
this task is to produce a novel motion respecting both the motion content and
the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing
research attempts to address this problem via motion style transfer or
conditional motion generation. They typically embed the motion style into a
latent space and guide the motion implicitly in a latent space as well. Despite
the progress, their methods suffer from low interpretability and control,
limited generalization to new styles, and fail to produce motions other than
``walking'' due to the strong bias in the public stylization dataset. In this
paper, we propose to solve the stylized motion generation problem from a new
perspective of reasoning-composition-generation, based on our observations: i)
human motion can often be effectively described using natural language in a
body-part centric manner, ii) LLMs exhibit a strong ability to understand and
reason about human motion, and iii) human motion has an inherently
compositional nature, facilitating the new motion content or style generation
via effective recomposing. We thus propose utilizing body-part text space as an
intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a
reasoner, composer, and generator when generating the desired stylized motion.
Our method executes in the body-part text space with much higher
interpretability, enabling fine-grained motion control, effectively resolving
potential conflicts between motion content and style, and generalizes well to
new styles thanks to the open-vocabulary ability of LLMs. Comprehensive
experiments and evaluations, and a user perceptual study, demonstrate the
effectiveness of our approach, especially under the pure text-driven stylized
motion generation.

</details>


### [43] [Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion](https://arxiv.org/abs/2509.04145)
*Dongliang Cao,Guoxing Sun,Marc Habermann,Florian Bernard*

Main category: cs.GR

TL;DR: 本文提出了一种结合特定人物渲染和扩散生成模型的新方法，用于生成高真实度和逼真姿态依赖变形的动态人类头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人类头像时，要么局限于单一身份的特定渲染模型，要么生成的静态卡通头像质量较低且无法捕捉姿态依赖的变形（如衣物褶皱）。

Method: 采用两阶段流程：1) 优化一组特定人物的UNet网络以捕捉姿态依赖变形；2) 训练一个超扩散模型在权重空间生成网络权重，实现实时可控的动态头像生成。

Result: 实验表明，该方法在大规模跨身份多视角视频数据集上优于现有技术。

Conclusion: 该方法成功结合了特定人物渲染的高真实度和扩散模型的泛化能力，实现了高质量动态人类头像的生成。

Abstract: Creating human avatars is a highly desirable yet challenging task. Recent
advancements in radiance field rendering have achieved unprecedented
photorealism and real-time performance for personalized dynamic human avatars.
However, these approaches are typically limited to person-specific rendering
models trained on multi-view video data for a single individual, limiting their
ability to generalize across different identities. On the other hand,
generative approaches leveraging prior knowledge from pre-trained 2D diffusion
models can produce cartoonish, static human avatars, which are animated through
simple skeleton-based articulation. Therefore, the avatars generated by these
methods suffer from lower rendering quality compared to person-specific
rendering methods and fail to capture pose-dependent deformations such as cloth
wrinkles. In this paper, we propose a novel approach that unites the strengths
of person-specific rendering and diffusion-based generative modeling to enable
dynamic human avatar generation with both high photorealism and realistic
pose-dependent deformations. Our method follows a two-stage pipeline: first, we
optimize a set of person-specific UNets, with each network representing a
dynamic human avatar that captures intricate pose-dependent deformations. In
the second stage, we train a hyper diffusion model over the optimized network
weights. During inference, our method generates network weights for real-time,
controllable rendering of dynamic human avatars. Using a large-scale,
cross-identity, multi-view video dataset, we demonstrate that our approach
outperforms state-of-the-art human avatar generation methods.

</details>


### [44] [Massively-Parallel Implementation of Inextensible Elastic Rods Using Inter-block GPU Synchronization](https://arxiv.org/abs/2509.04277)
*Przemyslaw Korzeniowski,Niels Hald,Fernando Bello*

Main category: cs.GR

TL;DR: 论文提出了一种基于GPU的大规模并行弹性杆模拟方法，显著提升了计算速度，适用于实时物理模拟。


<details>
  <summary>Details</summary>
Motivation: 弹性杆（如Cosserat杆）在物理模拟中有广泛应用，但其计算复杂度高，尤其是在实时应用中。因此，需要高效的并行实现以满足性能需求。

Method: 通过优化CUDA可扩展编程模型并利用块间同步，实现了多物理时间步的单次内核启动，充分利用GPU的流式多处理器。

Result: 优化后的实现在10个时间步的单次内核启动中，原CoRdE模型加速40倍；不可拉伸版本的GPU实现平均加速15.11倍，心血管模拟中实现了13.5倍的性能提升。

Conclusion: 该方法显著提升了Cosserat杆模拟的效率，支持高精度的实时交互式模拟。

Abstract: An elastic rod is a long and thin body able to sustain large global
deformations, even if local strains are small. The Cosserat rod is a non-linear
elastic rod with an oriented centreline, which enables modelling of bending,
stretching and twisting deformations. It can be used for physically-based
computer simulation of threads, wires, ropes, as well as flexible surgical
instruments such as catheters, guidewires or sutures. We present a
massively-parallel implementation of the original CoRdE model as well as our
inextensible variation. By superseding the CUDA Scalable Programming Model and
using inter-block synchronization, we managed to simulate multiple physics
time-steps per single kernel launch utilizing all the GPU's streaming
multiprocessors. Under some constraints, this results in nearly constant
computation time, regardless of the number of Cosserat elements simulated. When
executing 10 time-steps per single kernel launch, our implementation of the
original, extensible CoRdE was x40.0 faster. In a number of tests, the GPU
implementation of our inextensible CoRdE modification achieved an average
speed-up of x15.11 over the corresponding CPU version. Simulating a
catheter/guidewire pair (2x512 Cosserat elements) in a cardiovascular
application resulted in a 13.5 fold performance boost, enabling for accurate
real-time simulation at haptic interactive rates (0.5-1kHz).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 本文提出了一种基于区块链的数字市场，用于可持续建筑材料的再利用，结合IPFS确保透明度和可追溯性，旨在解决建筑行业中的材料浪费和可持续性问题。


<details>
  <summary>Details</summary>
Motivation: 建筑行业在材料浪费和可持续实践方面面临重大挑战，需要创新解决方案来整合自动化、可追溯性和分散决策，以实现高效的材料再利用。

Method: 通过区块链技术和IPFS，开发了一个数字市场框架，确保材料交换的透明度和可追溯性。

Result: 展示了该市场的操作流程，证明了其在工业自动化和循环供应链中的有效性。

Conclusion: 该数字市场能够促进高效且可信的建筑材料再利用，是迈向更可持续建筑实践的重要一步。

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [46] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: HPEC Graph Challenge 通过新的基准测试和 GraphBLAS 技术展示了使用开源工具（如 NVIDIA RAPIDS）对图形处理的显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试（如 LINPACK）无法充分测试 HPC 系统的硬件和软件性能，因此需要更复杂的图形处理工作负载来评估和改进系统性能。

Method: 利用 GraphBLAS 和 RAPIDS 生态系统中的工具（如 cuDF 和 cupy）重新实现图形挑战，替代传统的 CPU 代码（如 Pandas）。

Result: 在 NVIDIA GPU（A100、H100 和 H200）上实现了显著的加速效果，速度提升范围从 147 倍到 2185 倍不等。

Conclusion: 使用现成的 ETL 工具（如 RAPIDS）可以在不编写专门 HPC 代码的情况下显著加速图形处理任务，展示了开源工具在高性能计算中的潜力。

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [47] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 研究了异步通信网络中分布式数据检索问题，针对崩溃和拜占庭故障模型提出了查询复杂度最优的确定性解决方案，并扩展了随机协议的下界。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在同步通信网络上，本文首次扩展到异步网络，解决了数据检索中的最小查询复杂度和最大容错能力问题。

Method: 在异步网络中，提出了确定性协议处理崩溃故障，并针对拜占庭故障模型扩展了随机协议的下界，给出了随机协议的存在条件。

Result: 在崩溃故障模型中实现了查询复杂度最优的解决方案；在拜占庭模型中，针对β<1/2提出了近优随机协议。

Conclusion: 本研究首次在异步网络中解决了数据检索问题，为分布式系统中的容错和效率提供了新的理论支持。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [48] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本文研究了在有限可见性条件下，解决连续圆环上自主、匿名且同质机器人聚集问题的算法。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决在π-可见性模型下，机器人具有有限通信能力（FCOM）且非刚性移动时的聚集问题。

Method: 提出了一种算法，适用于π-可见性模型，机器人具备有限通信能力和非刚性移动，且工作在完全异步调度器下。

Result: 结果表明该算法能够有效解决聚集问题。

Conclusion: 结论表明，有限通信能力和非刚性移动的结合在π-可见性模型下是可实现的。

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [49] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 论文提出了一种基于不确定性松弛的算法，用于高效并行计算，显著提升了具有burnout变量系统的反事实估计的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在具有burnout变量的大规模系统中，模拟假设场景计算成本高昂，传统方法难以扩展，尤其是在在线广告等领域。

Method: 引入了不确定性松弛算法，通过并行计算来解决传统方法的扩展性问题。

Result: 新算法显著提高了反事实估计的可扩展性。

Conclusion: 不确定性松弛算法为具有burnout变量的系统提供了一种高效且可扩展的反事实估计方法。

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [50] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff是一个高效的频繁检查点框架，通过重用压缩梯度作为差异检查点来降低成本，并优化存储写入，动态调整检查点频率和批量大小以提升性能。


<details>
  <summary>Details</summary>
Motivation: 分布式训练大型深度学习模型常因失败而中断，现有的频繁检查点方法成本高且性能下降，差分检查点方法又限于推荐系统。

Method: LowDiff重用压缩梯度作为差异化检查点，采用批量梯度写入优化存储，动态调整检查点频率和批量大小，并引入层级梯度重用和CPU异步持久化策略。

Result: 实验表明，LowDiff能以每迭代一次的频率进行检查点，运行时开销低于3.1%。

Conclusion: LowDiff通过多种优化策略实现了高效频繁检查点，显著降低了成本并提升了分布式训练的性能。

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


### [51] [On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs](https://arxiv.org/abs/2509.04383)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: OBLOT模型中，同步机器人在有限图上利用无限计算能力，设计了一种最小化移动和轮次的解决算法。


<details>
  <summary>Details</summary>
Motivation: 以往研究中OBLOT模型的计算能力未被充分考量，本研究填补了这一空白。

Method: 利用同步机器人的无限计算能力，设计适用于广泛问题的算法。

Result: 证明了无限计算能力对算法效率的显著影响，并提出了一种高效算法。

Conclusion: 研究中无限计算能力的引入显著提升了OBLOT模型中任务的解决效率。

Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics.
It assumes weak capabilities for the involved mobile robots, such as they are
anonymous, disoriented, no memory of past events (oblivious), and silent. Their
only means of (implicit) communication is transferred to their positioning,
i.e., stigmergic information. These limited capabilities make the design of
distributed algorithms a challenging task. Over the last two decades, numerous
research papers have addressed the question of which tasks can be accomplished
within this model. Nevertheless, as it usually happens in distributed
computing, also in OBLOT the computational power available to the robots is
neglected as the main cost measures for the designed algorithms refer to the
number of movements or the number of rounds required. In this paper, we prove
that for synchronous robots moving on finite graphs, the unlimited
computational power (other than finite time) has a significant impact. In fact,
by exploiting it, we provide a definitive resolution algorithm that applies to
a wide class of problems while guaranteeing the minimum number of moves and
rounds.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [52] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 一个深度学习推理框架，通过提前规划计算和通信，减少对I/O、片外存储和主机控制的依赖，提高硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 为了减少深度学习推理中对I/O、片外存储和主机控制的依赖，提高计算效率和硬件利用率。

Method: 利用可预测的神经网络行为，生成统一的指令和数据流，采用细粒度消息传递技术，如静态权重重用、阵列内多播和分级归约。

Result: 在VGG-19上实现高利用率（88-92%），内部消息生成占比97%，片内传输时间占比89%，计算吞吐量超过1 TFLOP/s。

Conclusion: 基于流的计算方式有效，框架通过紧密协调数据和指令流实现了高效执行。

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [53] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 综述了FPGA上部署CNN用于目标检测、分类和跟踪的最新进展，强调了FPGA在实时计算机视觉应用中的优势及其优化策略。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶、机器人和监控等领域对实时计算机视觉应用的需求增长，FPGA因其可重构性、低功耗和确定性延迟成为GPU和ASIC的有力替代品。

Method: 综述了最新的FPGA实现，包括算法创新、硬件加速技术，以及剪枝、量化和稀疏感知方法等优化策略，同时探讨了现代FPGA平台和软件开发工具。

Result: 总结了FPGA在深度学习任务中的性能优势，并提出了GPU和FPGA混合架构的协作加速方案。

Conclusion: 本文为研究人员和工程师提供了开发下一代高效能、高性能视觉系统的见解，特别适用于边缘和嵌入式应用。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [54] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文综述了FPGA上部署Transformer和视觉语言模型的设计权衡、优化策略及实现挑战，探讨了硬件与算法的协同设计趋势，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: Transformer和视觉语言模型在计算复杂性和内存占用方面的高要求，限制了其在低延迟和低功耗环境中的应用，FPGA因其可重构性和高效能成为理想平台。

Method: 通过分析设备选择、内存子系统约束、数据流编排、量化策略、稀疏性利用及工具链选择等关键因素，结合模态特定的异质计算平衡和跨注意力内存管理，提出优化方案。

Result: 总结了FPGA加速Transformer和VLMs的现状，强调了硬件算法协同设计的创新，如注意力机制和改进的压缩技术。

Conclusion: 未来需发展可扩展、可移植且可重构的FPGA解决方案，以适应不断演进的模型架构，并提供高效且可预测的性能。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [55] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 这篇综述论文调查了自动驾驶汽车中实时目标检测算法及其硬件加速器，分析了AI算法（如CNN）如何取代传统方法，并探讨了硬件平台（如GPU和ASIC）的应用。文章还指出了该领域的研究障碍，如商业机密限制学术研究，并旨在弥合学术与商业技术之间的差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车需要高效的实时目标检测算法以确保安全。尽管AI算法（如CNN）和硬件加速器（如GPU）已取得显著进展，但商业机密和有限的研究条件仍阻碍了学术研究的深入发展。本文旨在填补现有研究空白，为未来完全自动驾驶汽车的设计提供参考。

Method: 论文采用综述方法，总结了当前最先进的实时目标检测算法及其硬件加速器（如GPU和ASIC），并分析了商业自动驾驶系统中的技术细节及其对学术研究的影响。

Result: 研究发现，尽管CNN等AI算法和GPU/ASIC硬件加速器能够实现每秒数百帧的处理速度，但处理多摄像头数据仍需硬件和算法的进一步优化。商业与学术间的信息不对称限制了研究的进展。

Conclusion: 本文通过综述当前技术和商业应用，为未来自动驾驶汽车的设计提供了重要参考，并呼吁加强学术与产业界的合作以推动技术进步。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [56] [The MolecularWeb Universe: Web-Based, Immersive, Multiuser Molecular Graphics And Modeling, for Education and Work in Chemistry, Structural Biology, and Materials Sciences](https://arxiv.org/abs/2509.04056)
*Luciano A. Abriata*

Main category: physics.chem-ph

TL;DR: MolecularWeb生态系统是一套基于网络的工具，支持沉浸式分子可视化和建模，已广泛应用于教育和科研。


<details>
  <summary>Details</summary>
Motivation: 解决2D设备在3D分子可视化中的局限性，利用XR技术提供更直观的交互方式。

Method: 开发了多个工具，如moleculARweb、MolecularWebXR和PDB2AR，支持AR/VR教育、科研和多用户协作。

Result: 成功实现了从教育到科研的应用扩展，并展示了未来分子科学的交互潜力。

Conclusion: MolecularWeb展示了网络环境下分子科学的可访问性和交互性前景。

Abstract: Molecular visualization software has long supported research and education in
chemical and structural sciences, but consumer devices constrained to 2D inputs
and outputs pose two major challenges: they poorly convey 3D nature, and 3D
manipulation is very difficult. eXtended Reality (XR, including AR and VR)
offers new ways to see and interact with molecules in three dimensions. This
chapter presents the "MolecularWeb" ecosystem (https://molecularweb.org), a set
of web-based tools for immersive visualization, modeling, and simulations,
already widely used in education and science communication and now expanding
toward research applications. We cover moleculARweb, which provides AR
educational activities via phones, tablets, and computers; MolecularWebXR, a
multiuser WebXR platform accessible from both headsets and simpler devices,
supporting immersive education, outreach, and scientific discussion; and
PDB2AR, which enables users to generate custom content for MolecularWebXR and
standalone AR/VR. Finally, we introduce a prototype and an upcoming version of
HandMol, our latest WebXR software which allows concurrent multiuser immersive
visualization and modeling of molecules with bare hands supported by real-time
molecular mechanics, natural language input via a language model, and access
through both high-end headsets or consumer devices like smartphones and
laptops. Together, these tools demonstrate the present and near-future of
accessible, interactive molecular science on the web.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [57] [Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post Quantum Cryptography](https://arxiv.org/abs/2509.04070)
*Paresh Baidya,Rourab Paul,Vikas Srivastava,Sumit Kumar Debnath*

Main category: cs.CR

TL;DR: 该论文提出了三种基于重新计算的轻量级故障检测方法，用于Kyber的CT-BU中的Barrett Reduction，并以RESWO算法为创新点。


<details>
  <summary>Details</summary>
Motivation: 由于后量子密码（PQC）算法的硬件加速器可能因故障注入而泄露敏感信息，尤其是在NIST标准化的Kyber算法中，因此需要高效的故障检测方法以确保其可靠性。

Method: 论文提出了三种故障检测方法（RESWO、RENO、RESO），其中RESWO是新算法，用于检测Barrett Reduction中的故障，并在FPGA上实现。

Result: RESWO在延迟性能上优于RENO和RESO，三种方法的故障检测效率均接近100%。

Conclusion: RESWO是一种高效且轻量级的故障检测方法，适用于PQC算法的硬件实现，展现了在Barrett Reduction中的实用性和优越性能。

Abstract: A fault can occur naturally or intentionally. However, intentionally
injecting faults into hardware accelerators of Post-Quantum Cryptographic (PQC)
algorithms may leak sensitive information. This intentional fault injection in
side-channel attacks compromises the reliability of PQC implementations. The
recently NIST-standardized key encapsulation mechanism (KEM), Kyber may also
leak information at the hardware implementation level. This work proposes three
efficient and lightweight recomputation-based fault detection methods for
Barrett Reduction in the Cooley-Tukey Butterfly Unit (CT-BU) of Kyber on a
Field Programmable Gate Array (FPGA). The CT-BU and Barrett Reduction are
fundamental components in structured lattice-based PQC algorithms, including
Kyber, NTRU, Falcon, CRYSTALS-Dilithium, etc. This paper introduces a new
algorithm, Recomputation with Swapped Operand (RESWO), for fault detection.
While Recomputation with Negated Operand (RENO) and Recomputation with Shifted
Operand (RESO) are existing methods used in other PQC hardware algorithms. To
the best of our knowledge, RENO and RESO have never been used in Barrett
Reduction before. The proposed RESWO method consumes a similar number of slices
compared to RENO and RESO. However, RESWO shows lesser delay compared to both
RENO and RESO. The fault detection efficiency of RESWO, RENO, and RESO is
nearly 100%.

</details>


### [58] [Reactive Bottom-Up Testing](https://arxiv.org/abs/2509.03711)
*Siddharth Muralee,Sourag Cherupattamoolayil,James C. Davis,Antonio Bianchi,Aravind Machiry*

Main category: cs.CR

TL;DR: 论文提出了一种名为Reactive Bottom-Up Testing的新范式，通过三阶段的测试方案（识别潜在漏洞函数、模糊测试与符号执行、验证排除假阳性）来优化动态测试的局限性和假阳性问题。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统中软件漏洞普遍存在，传统动态测试方法难以深入调用图底层的函数，且易产生假阳性或输入不符合程序上下文的问题。

Method: 提出Reactive Bottom-Up Testing三阶段方案：1) 识别潜在漏洞函数并生成上下文感知的测试工具；2) 通过模糊测试和符号执行提取输入约束；3) 结合约束验证漏洞以排除假阳性。

Result: 在48个已知漏洞的基准测试中成功检测到28个，并在Pacman等真实应用中发现6个未知漏洞。

Conclusion: Reactive Bottom-Up Testing能显著提升复杂系统中漏洞检测能力，为更健壮的安全实践铺平道路。

Abstract: Modern computing systems remain rife with software vulnerabilities. Engineers
apply many means to detect them, of which dynamic testing is one of the most
common and effective. However, most dynamic testing techniques follow a
top-down paradigm, and struggle to reach and exercise functions deep within the
call graph. While recent works have proposed Bottom-Up approaches to address
these limitations, they face challenges with false positives and generating
valid inputs that adhere to the context of the entire program.
  In this work, we introduce a new paradigm that we call Reactive Bottom-Up
Testing. Our insight is that function-level testing is necessary but not
sufficient for the validation of vulnerabilities in functions. What we need is
a systematic approach that not only tests functions in isolation but also
validates their behavior within the broader program context, ensuring that
detected vulnerabilities are both reachable and triggerable. We develop a
three-stage bottom-up testing scheme: (1) identify likely-vulnerable functions
and generate type- and context-aware harnesses; (2) fuzz to find crashes and
extract input constraints via symbolic execution; (3) verify crashes by
combining constraints to remove false positives. We implemented an automated
prototype, which we call Griller. We evaluated Griller in a controlled setting
using a benchmark of 48 known vulnerabilities across 5 open-source projects,
where we successfully detected 28 known vulnerabilities. Additionally, we
evaluated Griller on several real-world applications such as Pacman, and it
discovered 6 previously unknown vulnerabilities. Our findings suggest that
Reactive Bottom-Up Testing can significantly enhance the detection of
vulnerabilities in complex systems, paving the way for more robust security
practices.

</details>


### [59] [BIDO: A Unified Approach to Address Obfuscation and Concept Drift Challenges in Image-based Malware Detection](https://arxiv.org/abs/2509.03807)
*Junhui Li,Chengbin Feng,Zhiwei Yang,Qi Mo,Wei Wang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为BIDO的混合图像恶意软件检测器，通过局部特征选择模块、跨模态依赖建模和学习度量设计，显著提升了模型对混淆和概念漂移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图像恶意软件检测方法在混淆和概念漂移面前性能下降，而现有解决方案往往独立处理这两个问题，忽略了它们共同的统计根源——分布外问题。

Method: 设计局部特征选择模块提升特征判别力；在外部乘积空间建模跨模态依赖关系以提取稳定共现模式；设计学习度量确保特征紧凑性。

Result: 在真实数据集上的实验表明，BIDO显著优于现有基线，对概念漂移和混淆均表现出更高的鲁棒性。

Conclusion: BIDO通过统一处理混淆和概念漂移的共同根源，提供了一种高效且鲁棒的图像恶意软件检测方案。

Abstract: To identify malicious Android applications, various malware detection
techniques have been proposed. Among them, image-based approaches are
considered potential alternatives due to their efficiency and scalability.
Recent studies have reported that these approaches suffer significant
performance declines when confronted with obfuscation or concept drift.
However, existing solutions often treat these two challenges as different
problems, offering independent solutions. These techniques overlook the fact
that both challenges share a common statistical root, out-of-distribution, and
research from this perspective remains limited. In response, we propose BIDO, a
hybrid image-based malware detector designed to enhance robustness against both
obfuscation and concept drift simultaneously. Specifically, to improve the
discriminative power of image features, we introduce a local feature selection
module that identifies informative subregions within malware images. Second, to
enhance feature robustness, we model pairwise cross-modal dependencies in an
outer product space, enabling the extraction of stable co-occurrence patterns.
Third, to ensure feature compactness, we design a learnable metric that pulls
samples with identical labels closer while pushing apart those with different
labels, regardless of obfuscation or concept drift. Extensive experiments on
the real-world datasets demonstrate that BIDO significantly outperforms
existing baselines, achieving higher robustness against both concept drift and
obfuscation. The source code is available at:
https://github.com/whatishope/BIDO/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Semi-decentralized Federated Time Series Prediction with Client Availability Budgets](https://arxiv.org/abs/2509.03660)
*Yunkai Bao,Reza Safarzadeh,Xin Wang,Steve Drew*

Main category: cs.LG

TL;DR: 该论文提出了一种半去中心化的联邦学习方法FedDeCAB，通过概率排名选择客户端，并利用邻近客户端参数优化离线模型，提升了在数据异构性和通信限制下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习在物联网场景中如何应对客户端数据异构性、能量限制和可用性动态变化的问题，尤其是时间序列数据对模型收敛的影响。

Method: 设计了FedDeCAB方法，通过概率排名选择可用客户端，并在客户端离线时利用邻近客户端参数进行联合优化。

Result: 实验表明，FedDeCAB在高度异构数据、有限通信预算和动态客户端变化下表现优异。

Conclusion: FedDeCAB有效提升了联邦学习在动态和资源受限环境中的性能，减少了通信开销。

Abstract: Federated learning (FL) effectively promotes collaborative training among
distributed clients with privacy considerations in the Internet of Things (IoT)
scenarios. Despite of data heterogeneity, FL clients may also be constrained by
limited energy and availability budgets. Therefore, effective selection of
clients participating in training is of vital importance for the convergence of
the global model and the balance of client contributions. In this paper, we
discuss the performance impact of client availability with time-series data on
federated learning. We set up three different scenarios that affect the
availability of time-series data and propose FedDeCAB, a novel,
semi-decentralized client selection method applying probabilistic rankings of
available clients. When a client is disconnected from the server, FedDeCAB
allows obtaining partial model parameters from the nearest neighbor clients for
joint optimization, improving the performance of offline models and reducing
communication overhead. Experiments based on real-world large-scale taxi and
vessel trajectory datasets show that FedDeCAB is effective under highly
heterogeneous data distribution, limited communication budget, and dynamic
client offline or rejoining.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [61] [Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation](https://arxiv.org/abs/2509.04095)
*Achilleas Santi Seisa,Viswa Narayanan Sankaranarayanan,Gerasimos Damigos,Sumeet Gajanan Satpute,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种基于容器化技术的可扩展框架，用于测试云和边缘机器人系统，解决了网络延迟和安全问题。


<details>
  <summary>Details</summary>
Motivation: 云机器人技术具有计算任务卸载和数据共享的优势，但其集成面临网络延迟、安全性和资源管理等问题。

Method: 使用容器化技术构建了两个主要组件：容器化云集群和机器人仿真环境，并通过UDP隧道实现双向通信，模拟实际网络条件。

Result: 该框架成功实现了云辅助无人机远程控制的用例，并模拟了实际部署中的网络延迟和抖动。

Conclusion: 提出的框架为云机器人系统的测试提供了一种高效且直观的解决方案。

Abstract: Cloud robotics has emerged as a promising technology for robotics
applications due to its advantages of offloading computationally intensive
tasks, facilitating data sharing, and enhancing robot coordination. However,
integrating cloud computing with robotics remains a complex challenge due to
network latency, security concerns, and the need for efficient resource
management. In this work, we present a scalable and intuitive framework for
testing cloud and edge robotic systems. The framework consists of two main
components enabled by containerized technology: (a) a containerized cloud
cluster and (b) the containerized robot simulation environment. The system
incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling
bidirectional communication between the cloud cluster container and the robot
simulation environment, while simulating realistic network conditions. To
achieve this, we consider the use case of cloud-assisted remote control for
aerial robots, while utilizing Linux-based traffic control to introduce
artificial delay and jitter, replicating variable network conditions
encountered in practical cloud-robot deployments.

</details>


### [62] [DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation](https://arxiv.org/abs/2509.04441)
*Hao-Shu Fang,Branden Romero,Yichen Xie,Arthur Hu,Bo-Ruei Huang,Juan Alvarez,Matthew Kim,Gabriel Margolis,Kavya Anbarasu,Masayoshi Tomizuka,Edward Adelson,Pulkit Agrawal*

Main category: cs.RO

TL;DR: 介绍了perioperation和DEXOP，一种通过被动手外骨骼收集人类操作数据的新范式，旨在最大化数据对机器人的可转移性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作数据收集的挑战，通过人类自然的操作行为提高数据的质量和实用性。

Method: 开发了DEXOP被动手外骨骼，通过力反馈和姿态镜像技术，收集多样化的灵巧操作任务数据。

Result: DEXOP能够高效收集高质量数据，相比远程操作显著提高了任务表现。

Conclusion: DEXOP是提升机器人灵巧性的强大工具，适用于多样化任务的数据收集。

Abstract: We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [63] [The Chaotic Art: Quantum Representation and Manipulation of Color](https://arxiv.org/abs/2509.03542)
*Guosheng Hu*

Main category: quant-ph

TL;DR: 量子计算技术因其独特原理将深刻改变色彩艺术。本文通过实验探索量子比特表示、色彩通道处理和量子计算生成图像，提出了一种在量子计算环境中进行色彩计算的新技术路径。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算技术在色彩艺术中的应用潜力，探索量子比特如何表示和操控色彩，为艺术创造提供新工具。

Method: 通过在Qiskit和IBM Q中进行编程实验，提出了一种量子比特表示和操作色彩的方法，并将其结果还原至经典计算机。

Result: 实验证明该方法可行，为量子计算在信息可视化、图像处理等色彩计算任务中的应用奠定了基础。

Conclusion: 量子计算有望促进新色彩理论和艺术概念的发展，为艺术与技术融合开辟新途径。

Abstract: Due to its unique computing principles, quantum computing technology will
profoundly change the spectacle of color art. Focusing on experimental
exploration of color qubit representation, color channel processing, and color
image generation via quantum computing, this article proposes a new technical
path for color computing in quantum computing environment, by which digital
color is represented, operated, and measured in quantum bits, and then restored
for classical computers as computing results. This method has been proved
practicable as an artistic technique of color qubit representation and quantum
computing via programming experiments in Qiskit and IBM Q. By building a bridge
between classical chromatics and quantum graphics, quantum computers can be
used for information visualization, image processing, and more color computing
tasks. Furthermore, quantum computing can be expected to facilitate new color
theories and artistic concepts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
*Qi Chen,Jingxuan Wei,Zhuoya Yao,Haiguang Wang,Gaowei Wu,Bihui Yu,Siyuan Li,Cheng Tan*

Main category: cs.CL

TL;DR: 本文提出了一种名为“多文档科学推理”的新任务，旨在通过提取和对齐相关论文的动机、方法和实验结果来重构研究发展链。作者开发了一个名为ResearchPulse的框架，包含三个协调的agent，并在实验中展示了其优于GPT-4o等基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解科学思想的演变过程，需要一个结构化的方法来分析和对齐相关研究论文的动机、方法和实验结果。

Method: 提出了一个基于agent的框架ResearchPulse，包含Plan Agent、Mmap-Agent和Lchart-Agent，分别用于任务分解、构建动机-方法思维导图和合成实验结果折线图。

Result: 实验表明，尽管使用了7B规模的agent，ResearchPulse在语义对齐、结构一致性和视觉保真度方面均优于GPT-4o等基线模型。

Conclusion: ResearchPulse为多文档科学推理任务提供了一个有效的解决方案，并通过实验结果证明了其优越性。

Abstract: Understanding how scientific ideas evolve requires more than summarizing
individual papers-it demands structured, cross-document reasoning over
thematically related research. In this work, we formalize multi-document
scientific inference, a new task that extracts and aligns motivation,
methodology, and experimental results across related papers to reconstruct
research development chains. This task introduces key challenges, including
temporally aligning loosely structured methods and standardizing heterogeneous
experimental tables. We present ResearchPulse, an agent-based framework that
integrates instruction planning, scientific content extraction, and structured
visualization. It consists of three coordinated agents: a Plan Agent for task
decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a
Lchart-Agent that synthesizes experimental line charts. To support this task,
we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper
clusters. Experiments show that our system, despite using 7B-scale agents,
consistently outperforms strong baselines like GPT-4o in semantic alignment,
structural consistency, and visual fidelity. The dataset are available in
https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


### [65] [Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue](https://arxiv.org/abs/2509.04104)
*Keara Schaaij,Roel Boumans,Tibor Bosse,Iris Hendrickx*

Main category: cs.CL

TL;DR: 研究探讨如何通过构建个性化词汇配置文件实现对话代理中的词汇对齐，发现较小且紧凑的配置文件在10分钟转录语音和特定词性项目数量下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 词汇对齐对沟通成功至关重要，但如何应用于对话代理仍待探索，尤其是在大型语言模型（LLMs）背景下。

Method: 研究通过调整转录语音数据量和词性类别项目数量，构建个性化词汇配置文件，并评估其性能。

Result: 较小且紧凑的配置文件（10分钟转录语音，特定词性项目数量）在性能和效率上表现最佳。

Conclusion: 研究为构建稳定、个性化的词汇配置文件提供实用指导，为对话代理中的词汇对齐策略奠定基础。

Abstract: Lexical alignment, where speakers start to use similar words across
conversation, is known to contribute to successful communication. However, its
implementation in conversational agents remains underexplored, particularly
considering the recent advancements in large language models (LLMs). As a first
step towards enabling lexical alignment in human-agent dialogue, this study
draws on strategies for personalising conversational agents and investigates
the construction of stable, personalised lexical profiles as a basis for
lexical alignment. Specifically, we varied the amounts of transcribed spoken
data used for construction as well as the number of items included in the
profiles per part-of-speech (POS) category and evaluated profile performance
across time using recall, coverage, and cosine similarity metrics. It was shown
that smaller and more compact profiles, created after 10 min of transcribed
speech containing 5 items for adjectives, 5 items for conjunctions, and 10
items for adverbs, nouns, pronouns, and verbs each, offered the best balance in
both performance and data efficiency. In conclusion, this study offers
practical insights into constructing stable, personalised lexical profiles,
taking into account minimal data requirements, serving as a foundational step
toward lexical alignment strategies in conversational agents.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music](https://arxiv.org/abs/2509.04215)
*Hayeon Bang,Eunjin Choi,Seungheon Doh,Juhan Nam*

Main category: cs.SD

TL;DR: PianoBind是一种针对钢琴音乐的多模态联合嵌入模型，旨在捕捉小型和同质数据集中的细微语义区别，优于通用音乐模型。


<details>
  <summary>Details</summary>
Motivation: 尽管钢琴音乐具有丰富的表现力，但现有通用模型难以捕捉其细微语义区别，且钢琴特定模型多为单模态。

Method: 提出PianoBind，通过多源训练和模态利用策略，优化联合嵌入框架。

Result: 实验显示PianoBind能有效捕捉钢琴音乐的细微差异，在文本到音乐的检索任务中表现优于通用模型。

Conclusion: PianoBind为多模态表示学习提供了可复用的设计思路，适用于其他同质数据集。

Abstract: Solo piano music, despite being a single-instrument medium, possesses
significant expressive capabilities, conveying rich semantic information across
genres, moods, and styles. However, current general-purpose music
representation models, predominantly trained on large-scale datasets, often
struggle to captures subtle semantic distinctions within homogeneous solo piano
music. Furthermore, existing piano-specific representation models are typically
unimodal, failing to capture the inherently multimodal nature of piano music,
expressed through audio, symbolic, and textual modalities. To address these
limitations, we propose PianoBind, a piano-specific multimodal joint embedding
model. We systematically investigate strategies for multi-source training and
modality utilization within a joint embedding framework optimized for capturing
fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano
datasets. Our experimental results demonstrate that PianoBind learns multimodal
representations that effectively capture subtle nuances of piano music,
achieving superior text-to-music retrieval performance on in-domain and
out-of-domain piano datasets compared to general-purpose music joint embedding
models. Moreover, our design choices offer reusable insights for multimodal
representation learning with homogeneous datasets beyond piano music.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [67] [Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for Differential Equations](https://arxiv.org/abs/2509.03945)
*Guglielmo Gattiglio,Lyudmila Grigoryeva,Massimiliano Tamborrino*

Main category: stat.CO

TL;DR: Prob-GParareal是一种基于概率的GParareal扩展算法，用于量化并行时间解决方案的不确定性，支持概率初始条件，并与传统数值求解器兼容。


<details>
  <summary>Details</summary>
Motivation: 为并行时间（PinT）求解微分方程的不确定性提供量化方法，填补现有PinT方法在概率对应方面的空白。

Method: 利用高斯过程（GPs）建模Parareal校正函数，支持数值不确定性的传播，并引入Prob-nnGParareal变体以提升性能。

Result: 理论分析了计算复杂度和误差界限，数值实验验证了算法在多种ODE系统和PDE问题中的准确性和鲁棒性。

Conclusion: Prob-GParareal成功填补了概率PinT方法的空白，展示了其在复杂系统中的灵活性和扩展潜力。

Abstract: We introduce Prob-GParareal, a probabilistic extension of the GParareal
algorithm designed to provide uncertainty quantification for the
Parallel-in-Time (PinT) solution of (ordinary and partial) differential
equations (ODEs, PDEs). The method employs Gaussian processes (GPs) to model
the Parareal correction function, as GParareal does, further enabling the
propagation of numerical uncertainty across time and yielding probabilistic
forecasts of system's evolution. Furthermore, Prob-GParareal accommodates
probabilistic initial conditions and maintains compatibility with classical
numerical solvers, ensuring its straightforward integration into existing
Parareal frameworks. Here, we first conduct a theoretical analysis of the
computational complexity and derive error bounds of Prob-GParareal. Then, we
numerically demonstrate the accuracy and robustness of the proposed algorithm
on five benchmark ODE systems, including chaotic, stiff, and bifurcation
problems. To showcase the flexibility and potential scalability of the proposed
algorithm, we also consider Prob-nnGParareal, a variant obtained by replacing
the GPs in Parareal with the nearest-neighbors GPs, illustrating its increased
performance on an additional PDE example. This work bridges a critical gap in
the development of probabilistic counterparts to established PinT methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [68] [lifeXplore at the Lifelog Search Challenge 2021](https://arxiv.org/abs/2509.03692)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.IR

TL;DR: 文章介绍了自2018年起举办的Lifelog Search Challenge (LSC)比赛的背景和目标，并展示了作者团队改进的lifeXplore检索系统。


<details>
  <summary>Details</summary>
Motivation: LSC比赛旨在测试团队在有限时间内使用专门开发的工具检索大量生命日志数据的能力。作者团队希望通过改进lifeXplore系统提升检索效率。

Method: 改进的lifeXplore系统结合了按时间顺序浏览日摘要和交互式可组合概念过滤功能，并增加了时间查询、高级日摘要功能和用户体验优化。

Result: 系统相比之前版本功能更强大，用户体验更好。

Conclusion: 改进后的lifeXplore系统为LSC比赛提供了更高效的检索工具，展现了作者团队在生命日志数据检索领域的进展。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC)
continues to rise in popularity as an interactive lifelog data retrieval
competition, co-located at the ACM International Conference on Multimedia
Retrieval (ICMR). The goal of this annual live event is to search a large
corpus of lifelogging data for specifically announced memories using a
purposefully developed tool within a limited amount of time. As long-standing
participants, we present our improved lifeXplore - a retrieval system combining
chronologic day summary browsing with interactive combinable concept filtering.
Compared to previous versions, the tool is improved by incorporating temporal
queries, advanced day summary features as well as usability improvements.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [69] [No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening](https://arxiv.org/abs/2509.04404)
*Kyra Wilson,Mattea Sim,Anna-Maria Gueorguieva,Aylin Caliskan*

Main category: cs.CY

TL;DR: 研究通过简历筛选实验（N=528）探讨人类与模拟AI协作时的种族偏见影响。结果显示，AI偏见会显著改变人类决策行为，而隐性关联测试（IAT）可部分缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示人类与AI协作中种族偏见的传染性及其对决策的影响，为AI-人协作系统的设计与监管提供依据。

Method: 使用模拟AI模型（含种族偏见）与人类协作筛选简历，分析1,526种场景中的选择偏好，并结合IAT测量隐性种族-地位关联。

Result: 人类在无偏见AI辅助时公平选择候选人，但在偏见AI影响下倾向于AI偏好的群体（高达90%）。IAT可将反刻板印象选择率提升13%。

Conclusion: AI偏见可显著影响人类决策，需在政策、教育和系统设计中重视AI-人协作的复杂性及偏见缓解策略。

Abstract: In this study, we conduct a resume-screening experiment (N=528) where people
collaborate with simulated AI models exhibiting race-based preferences (bias)
to evaluate candidates for 16 high and low status occupations. Simulated AI
bias approximates factual and counterfactual estimates of racial bias in
real-world AI systems. We investigate people's preferences for White, Black,
Hispanic, and Asian candidates (represented through names and affinity groups
on quality-controlled resumes) across 1,526 scenarios and measure their
unconscious associations between race and status using implicit association
tests (IATs), which predict discriminatory hiring decisions but have not been
investigated in human-AI collaboration. When making decisions without AI or
with AI that exhibits no race-based preferences, people select all candidates
at equal rates. However, when interacting with AI favoring a particular group,
people also favor those candidates up to 90% of the time, indicating a
significant behavioral shift. The likelihood of selecting candidates whose
identities do not align with common race-status stereotypes can increase by 13%
if people complete an IAT before conducting resume screening. Finally, even if
people think AI recommendations are low quality or not important, their
decisions are still vulnerable to AI bias under certain circumstances. This
work has implications for people's autonomy in AI-HITL scenarios, AI and work,
design and evaluation of AI hiring systems, and strategies for mitigating bias
in collaborative decision-making tasks. In particular, organizational and
regulatory policy should acknowledge the complex nature of AI-HITL decision
making when implementing these systems, educating people who use them, and
determining which are subject to oversight.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [70] [How many patients could we save with LLM priors?](https://arxiv.org/abs/2509.04250)
*Shota Arai,David Selby,Andrew Vargo,Sebastian Vollmer*

Main category: stat.ME

TL;DR: 提出了一种利用大语言模型（LLM）生成的先验分布改进分层贝叶斯模型的方法，显著提升了多中心临床试验中不良事件预测的效能。


<details>
  <summary>Details</summary>
Motivation: 通过LLM编码的知识减少临床试验所需患者数量，同时保持统计效能。

Method: 开发了一个分层贝叶斯模型框架，直接从LLM获取参数化先验分布，而非生成合成数据。

Result: LLM生成的先验分布优于传统元分析方法，提高了预测性能。

Conclusion: 该方法为临床试验设计提供了更高效和基于专家知识的途径，有望降低患者数量需求并提升药物安全性监测。

Abstract: Imagine a world where clinical trials need far fewer patients to achieve the
same statistical power, thanks to the knowledge encoded in large language
models (LLMs). We present a novel framework for hierarchical Bayesian modeling
of adverse events in multi-center clinical trials, leveraging LLM-informed
prior distributions. Unlike data augmentation approaches that generate
synthetic data points, our methodology directly obtains parametric priors from
the model. Our approach systematically elicits informative priors for
hyperparameters in hierarchical Bayesian models using a pre-trained LLM,
enabling the incorporation of external clinical expertise directly into
Bayesian safety modeling. Through comprehensive temperature sensitivity
analysis and rigorous cross-validation on real-world clinical trial data, we
demonstrate that LLM-derived priors consistently improve predictive performance
compared to traditional meta-analytical approaches. This methodology paves the
way for more efficient and expert-informed clinical trial design, enabling
substantial reductions in the number of patients required to achieve robust
safety assessment and with the potential to transform drug safety monitoring
and regulatory decision making.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 这篇综述论文填补了人类运动视频生成领域的空白，详细介绍了生成过程的五个关键阶段，并讨论了大型语言模型的潜力，涵盖了三种主要模态和超过200篇论文的研究。


<details>
  <summary>Details</summary>
Motivation: 现有的研究综述仅关注个别方法，缺乏对整个生成过程的全面概述，因此本文旨在提供一个深入的综述，涵盖多个子任务和关键技术阶段。

Method: 论文通过分析人类运动视频生成的五个阶段（输入、运动规划、运动视频生成、细化和输出），并回顾了视觉、文本和音频三种模态下的最新研究和技术趋势。

Result: 提供了涵盖200多篇论文的全面综述，指出了里程碑式的技术突破，并讨论了大型语言模型的潜在应用。

Conclusion: 这篇综述揭示了人类运动视频生成的潜力，并为数字人类的综合应用提供了宝贵的资源。

Abstract: Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [72] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种结合双向文本融合模块和类别感知时间图模块的方法，用于改进弱监督下的音频-视觉视频解析任务，通过语义注入和动态校准减少噪声，实现了在两个基准数据集上的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在弱监督音频-视觉视频解析任务中，存在噪声伪标签被误认为可靠监督或注意力机制无差别传播误差的问题，导致训练中错误被放大。本文旨在解决这些问题。

Method: 论文提出了一种结合双向文本融合模块（BiT）和类别感知时间图模块（CATS）的方法，BiT模块用于语义注入和动态校准，CATS模块用于语义传播和连接，以实现精确的语义信息传播。

Result: 实验结果表明，该方法在两个基准数据集（LLP和UnAV-100）的多个关键指标上达到了最先进的性能。

Conclusion: 通过结合BiT和CATS模块的优势，本文成功解决了噪声伪标签和注意力传播带来的问题，显著提升了弱监督音频-视觉视频解析任务的性能。

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [73] [TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection](https://arxiv.org/abs/2509.04448)
*Zehong Yan,Peng Qi,Wynne Hsu,Mong Li Lee*

Main category: cs.CV

TL;DR: TRUST-VL是一个统一且可解释的多模态视觉语言模型，用于检测多模态虚假信息，通过联合训练和任务特定特征提取实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息（如文本、视觉和跨模态扭曲）对社会构成威胁，现有方法通常只关注单一类型的扭曲且难以泛化。

Method: 提出TRUST-VL模型，引入Question-Aware Visual Amplifier模块提取任务特定视觉特征，并构建TRUST-Instruct数据集（198K样本）支持训练。

Result: 在领域内和零样本基准测试中，TRUST-VL表现出卓越性能，具备强泛化能力和可解释性。

Conclusion: TRUST-VL通过联合训练和结构化推理实现了多模态虚假信息的有效检测与泛化。

Abstract: Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: 本文介绍了Oruga系统，该系统基于表示系统理论（RST），旨在赋予机器灵活的表示能力，包括核心数据结构、通信语言和结构转移方法。


<details>
  <summary>Details</summary>
Motivation: 人类能够灵活使用各种表示方法（如绘图、类比等），作者希望赋予机器类似的能力，使其更符合人类使用习惯。

Method: Oruga系统包含RST理论的核心数据结构、通信语言以及结构转移引擎，通过结构转移方法实现表示转换。

Result: 论文概述了Oruga系统的核心和语言，并展示了结构转移方法的转换示例。

Conclusion: Oruga系统为机器提供了灵活的表示能力，有望提高机器与人类的兼容性。

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [75] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 研究了马尔可夫逻辑网络（MLN）在无限域大小下的随机结构特性，通过三个具体例子展示了不同权重约束对极限行为的影响，并比较了MLN与提升贝叶斯网络的渐近不可比性。


<details>
  <summary>Details</summary>
Motivation: 探讨MLN在无限域下的概率分布特性，以及不同约束条件对结构极限行为的影响，填补理论与实际应用间的空白。

Method: 分析了三种不同类型的MLN案例，通过理论推导和数学证明，研究了随机结构的极限行为及其与权重约束的关系。

Result: 发现不同约束条件的MLN在极限行为上有显著差异，权重约束可能影响也可能不影响结果；同时证明了MLN与贝叶斯网络的渐近不可比性。

Conclusion: MLN在大域上的分布与均匀分布完全不同，展示了其在复杂概率建模中的独特潜力。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [76] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种将GUI代理的多步操作序列转换为页面图的方法，结合RAG技术和多代理框架PG-Agent，显著提升了代理在新场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理通常依赖多步操作序列作为先验知识，但难以捕捉页面间的复杂转移关系，限制了代理对新场景的感知和泛化能力。

Method: 设计自动化流程，将操作序列转换为页面图；引入RAG技术检索GUI感知指南；提出PG-Agent框架，结合任务分解策略。

Result: 在各种基准测试中，PG-Agent表现出色，即使页面图构建的样本有限。

Conclusion: 页面图和PG-Agent的结合有效提升了GUI代理在新场景中的泛化能力。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [77] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 论文提出一种新方法PersonaTeaming，通过在对抗性提示生成过程中引入角色，以提高AI模型的风险发现能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动化红队方法未考虑身份背景，而人类红队因身份不同可能发现不同风险，因此需结合身份因素改进自动化方法。

Method: 开发PersonaTeaming方法，包括基于专家或普通用户角色的提示变异策略，动态生成适应性角色，并引入新指标衡量变异距离。

Result: 实验表明，角色变异使对抗性提示攻击成功率提升高达144.1%，同时保持提示多样性。

Conclusion: PersonaTeaming展示了结合身份背景的自动化红队潜力，未来可探索与人类红队的互补性。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [78] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: MBTI-in-Thoughts框架通过心理人格条件提升LLM代理的效能，利用MBTI类型引导行为，并验证其通用性。


<details>
  <summary>Details</summary>
Motivation: 结合心理学理论（如MBTI）增强LLM的行为设计，无需微调即可实现人格一致性。

Method: 通过提示工程为LLM注入特定人格原型，控制认知与情感行为，并集成16Personalities测试验证特质持续性。

Result: 人格引导在不同任务中产生稳定行为偏差，如情感化代理在叙事生成中表现更优，分析性代理在博弈论中策略更稳定。

Conclusion: 框架为心理学增强的AI代理奠定基础，且可推广至其他心理模型（如Big Five）。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>
