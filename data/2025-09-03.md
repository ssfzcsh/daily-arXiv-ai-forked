<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 27]
- [cs.PL](#cs.PL) [Total: 6]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 27]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LO](#cs.LO) [Total: 7]
- [cs.HC](#cs.HC) [Total: 26]
- [cs.GR](#cs.GR) [Total: 13]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 21]
- [cs.DB](#cs.DB) [Total: 14]
- [cs.AR](#cs.AR) [Total: 11]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 15]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards](https://arxiv.org/abs/2509.00140)
*Songhui Yue*

Main category: cs.SE

TL;DR: 提出了一种基于大型语言模型（LLM）的关系三元组提取方法，用于软件工程标准（SES）的自动化本体生成（AOG），并通过多粒度基准测试验证其优于传统OpenIE方法。


<details>
  <summary>Details</summary>
Motivation: 由于软件工程标准（SES）文本冗长且非结构化，传统本体生成方法难以高效处理。本研究旨在利用LLM辅助关系三元组提取（RTE），提升自动化本体生成（AOG）的效率和准确性。

Method: 采用LLM辅助的RTE方法，包括文档分割、候选术语挖掘、LLM关系推断、术语归一化和跨部分对齐。通过Prompt Engineering结合LLM能力，构建了一个有效的AOG工作流。

Result: 生成的本体在三粒度基准测试中表现优异，其性能与OpenIE方法相当甚至更优。

Conclusion: LLM辅助的RTE方法为SES的自动化本体生成提供了高效解决方案，展示了LLM在知识表示领域的潜力。

Abstract: Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.

</details>


### [2] [LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers](https://arxiv.org/abs/2509.00256)
*Yutong Wang,Cindy Rubio-González*

Main category: cs.SE

TL;DR: LLM4FP利用大语言模型生成浮动点程序，以检测编译器不一致性，效果优于现有工具Varity。


<details>
  <summary>Details</summary>
Motivation: 解决不同编译器间浮动点不一致性对数值软件可靠性的影响。

Method: 结合基于语法的生成和基于反馈的变异，生成多样化且有效的程序。

Result: LLM4FP检测到的不一致性是Varity的两倍以上，且多数为实际数值差异。

Conclusion: LLM引导的程序生成显著提升了数值不一致性的检测能力。

Abstract: Floating-point inconsistencies across compilers can undermine the reliability
of numerical software. We present LLM4FP, the first framework that uses Large
Language Models (LLMs) to generate floating-point programs specifically
designed to trigger such inconsistencies. LLM4FP combines Grammar-Based
Generation and Feedback-Based Mutation to produce diverse and valid programs.
We evaluate LLM4FP across multiple compilers and optimization levels, measuring
inconsistency rate, time cost, and program diversity. LLM4FP detects over twice
as many inconsistencies compared to the state-of-the-art tool, Varity. Notably,
most of the inconsistencies involve real-valued differences, rather than
extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies
across a wider range of optimization levels, and finds the most mismatches
between host and device compilers. These results show that LLM-guided program
generation improves the detection of numerical inconsistencies.

</details>


### [3] [JS-TOD: Detecting Order-Dependent Flaky Tests in Jest](https://arxiv.org/abs/2509.00466)
*Negar Hashemi,Amjed Tahir,Shawn Rasheed,August Shi,Rachel Blagojevic*

Main category: cs.SE

TL;DR: JS-TOD 是一个用于检测 JavaScript 测试顺序依赖性的工具，通过随机化测试顺序和多次运行来揭示测试的不可靠性。


<details>
  <summary>Details</summary>
Motivation: 测试顺序依赖性是一种常见的测试不可靠性原因，理想情况下，测试应在独立环境下运行且结果一致。JS-TOD 旨在解决这一问题。

Method: JS-TOD 采用系统化方法，随机化测试、测试套件和描述块的执行顺序，并支持多次重排序和重运行以检测问题。

Result: 评估发现，测试顺序依赖性的主要原因是测试间的共享文件和模拟状态。

Conclusion: JS-TOD 是一种有效工具，可帮助开发者识别并解决测试顺序依赖性导致的不可靠性问题。

Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that
can extract, reorder, and rerun Jest tests to reveal possible order-dependent
test flakiness. Test order dependency is one of the leading causes of test
flakiness. Ideally, each test should operate in isolation and yield consistent
results no matter the sequence in which tests are run. However, in practice,
test outcomes can vary depending on their execution order. JS-TOD employed a
systematic approach to randomising tests, test suites, and describe blocks. The
tool is highly customisable, as one can set the number of orders and reruns
required (the default setting is 10 reorder and 10 reruns for each test and
test suite). Our evaluation using JS-TOD reveals two main causes of test order
dependency flakiness: shared files and shared mocking state between tests.

</details>


### [4] [Bug Whispering: Towards Audio Bug Reporting](https://arxiv.org/abs/2509.00785)
*Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 研究探讨了通过音频记录让用户直接报告移动应用问题的方法，分析了其优势和现有技术面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 希望通过音频记录简化用户反馈流程，增加开发团队收集的问题报告数量，从而更快发现和修复问题。

Method: 提出用户通过音频消息即时报告问题，并通过初步实验验证其可行性和挑战。

Result: 音频报告易于实现，但存在特定技术挑战，影响现有问题重现方法的有效性。

Conclusion: 音频报告有潜力改进问题反馈效率，但需进一步研究如何克服其带来的技术挑战。

Abstract: Bug reporting is a key feature of mobile applications, as it enables
developers to collect information about faults that escaped testing and thus
affected end-users. This paper explores the idea of allowing end-users to
immediately report the problems that they experience by recording and
submitting audio messages. Audio recording is simple to implement and has the
potential to increase the number of bug reports that development teams can
gather, thus potentially improving the rate at which bugs are identified and
fixed. However, audio bug reports exhibit specific characteristics that
challenge existing techniques for reproducing bugs. This paper discusses these
challenges based on a preliminary experiment, and motivates further research on
the collection and analysis of audio-based bug reports

</details>


### [5] [REConnect: Participatory RE that Matters](https://arxiv.org/abs/2509.01006)
*Daniela Damian,Bachan Ghimire,Ze Shi Li*

Main category: cs.SE

TL;DR: REConnect提出了一种以人类连接为中心的需求工程方法，强调参与式设计，确保与利益相关者的价值观和愿景一致。通过三个案例研究，展示了其关键原则和实践。


<details>
  <summary>Details</summary>
Motivation: 当前的需求工程实践（如CrowdRE和AI辅助策略）可能忽视了文化、社会和政治背景，导致与用户需求和价值观脱节。

Method: REConnect采用参与式方法，注重建立信任关系、共同设计和赋能用户，并通过REActions嵌入持续的利益相关者参与。

Result: 案例研究表明，REConnect能够产生文化扎根、社会合法且可持续的需求解决方案。

Conclusion: REConnect为需求工程提供了以人为本的框架，尤其是在生成式AI时代，需要确保AI的整合不会削弱人类的核心作用。

Abstract: Software increasingly shapes the infrastructures of daily life, making
requirements engineering (RE) central to ensuring that systems align with human
values and lived experiences. Yet, current popular practices such as CrowdRE
and AI-assisted elicitation strategies risk detaching requirements work from
the cultural, social, and political contexts that shape lived experiences,
human values, and real user needs. In this paper, we introduce REConnect that
re-centers RE on the human connection as central to the understanding of lived
experiences where impact is sought. REConnect advocates for a human-centered
participatory approach "that matters" to the communities and beneficiaries
involved, ensuring alignment with their values and aspirations. Drawing on
three case studies of societal impact: BloodSync in rural Nepal, Herluma
supporting women at risk of homelessness in Canada, and BridgingRoots to
revitalize Indigenous languages in the Canadian Arctic. REConnect argues that
three key principles and enablers: building trusting relationships,
co-designing with and alongside stakeholders, and empowering users as agents of
change, can yield requirements that are culturally grounded, socially
legitimate, and sustainable beyond system delivery. REConnect also proposes a
set of actionable practices (REActions) that embed relationality and ongoing
stakeholder engagement throughout requirements elicitation, analysis, and
validation of solution development. Finally, we situate REConnect in the era of
Generative AI. While AI can accelerate and scale certain RE tasks, its
integration must be guided by participatory practices that not only preserve
human agency but also empower humans' roles to become guardians of values and
ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in
iterative review cycles.

</details>


### [6] [Generative Goal Modeling](https://arxiv.org/abs/2509.01048)
*Ateeq Sharfuddin,Travis Breaux*

Main category: cs.SE

TL;DR: 这篇论文提出了一种使用文本蕴含技术从访谈记录中提取目标并构建目标模型的方法。该方法在29个应用领域的15份访谈记录上进行了评估，结果显示GPT-4o能够可靠地提取目标（准确率62.0%）和目标溯源（准确率98.7%），同时目标模型细化关系的生成准确率为72.2%。


<details>
  <summary>Details</summary>
Motivation: 在软件工程中，需求通常通过访谈等方法从利益相关者那里获取，但手动分析和记录需求耗时且容易出错。因此，需要一种自动化方法来高效准确地提取目标并构建目标模型。

Method: 该方法利用文本蕴含技术从访谈记录中提取目标，并使用GPT-4o自动构建目标模型。实验评估了GPT-4o在目标提取、溯源和目标模型细化关系生成上的表现。

Result: 实验结果显示，GPT-4o在目标提取上与人工提取的目标匹配率为62.0%，目标溯源准确率为98.7%，目标模型细化关系的生成准确率为72.2%。

Conclusion: 该方法能够自动化地从访谈记录中提取目标并构建目标模型，具有良好的准确性和可靠性，为需求工程提供了高效的支持。

Abstract: In software engineering, requirements may be acquired from stakeholders
through elicitation methods, such as interviews, observational studies, and
focus groups. When supporting acquisition from interviews, business analysts
must review transcripts to identify and document requirements. Goal modeling is
a popular technique for representing early stakeholder requirements as it lends
itself to various analyses, including refinement to map high-level goals into
software operations, and conflict and obstacle analysis. In this paper, we
describe an approach to use textual entailment to reliably extract goals from
interview transcripts and to construct goal models. The approach has been
evaluated on 15 interview transcripts across 29 application domains. The
findings show that GPT-4o can reliably extract goals from interview
transcripts, matching 62.0% of goals acquired by humans from the same
transcripts, and that GPT-4o can trace goals to originating text in the
transcript with 98.7% accuracy. In addition, when evaluated by human
annotators, GPT-4o generates goal model refinement relationships among
extracted goals with 72.2% accuracy.

</details>


### [7] [A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps](https://arxiv.org/abs/2509.01068)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: 该研究通过系统映射方法分析了73篇论文，探讨了移动应用自动化需求获取与分析的技术和工具现状，发现半自动技术、开源非自研工具及需求分析、挖掘与分类是主要特点和任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解移动应用自动化需求获取与分析中技术和工具的特点及其支持的需求工程任务。

Method: 采用Kitchenham等人的指南进行系统映射研究，分析了73篇相关论文。

Result: 发现半自动技术最常用，工具多为开源非自研且主要用于需求分析和文本预处理；需求分析、挖掘与分类是三大主要任务。

Conclusion: 研究总结出技术和工具使用增长的趋势，半自动技术的主导地位，三大主要需求任务，以及开源非自研工具的流行。

Abstract: [Background:] Research on automated requirements elicitation and analysis of
mobile apps employed lots of techniques and tools proposed by RE researchers
and practitioners. However, little is known about the characteristics of these
techniques and tools as well as the RE tasks in requirements elicitation and
analysis that got supported with the help of respective techniques and tools.
[Aims:] The goal of this paper is to investigate the state-of-the-art of the
techniques and tools used in automated requirements elicitation and analysis of
mobile apps. [Method:] We carried out a systematic mapping study by following
the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we
found the most frequently used techniques - semi-automatic techniques, and the
main characteristics of the tools - open-sourced and non-self-developed tools
for requirements analysis and text pre-processing. Plus, the most three
investigated RE tasks are requirements analysis, mining and classification.
[Conclusions:] Our most important conclusions are: (1) there is a growth in the
use of techniques and tools in automated requirements elicitation and analysis
of mobile apps, (2) semi-automatic techniques are mainly used in the
publications on this research topic, (3) requirements analysis, mining and
classification are the top three RE tasks with the support of automatic
techniques and tools, and (4) the most popular tools are open-sourced and
non-self-developed, and they are mainly used in requirements analysis and text
processing.

</details>


### [8] [Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound](https://arxiv.org/abs/2509.01149)
*Hui Zeng,Zhihao Xu,Hui Li,Siwen Wang,Qian Ma*

Main category: cs.SE

TL;DR: Lin-Hunter 是一个新型测试框架，旨在通过系统生成多样化的 HDL 测试用例和优化 FPGA 逻辑验证效率，解决了现有方法的多样性不足问题。


<details>
  <summary>Details</summary>
Motivation: FPGA 逻辑综合工具的正确性至关重要，但现有方法依赖随机策略，测试用例的结构多样性不足，导致工具功能空间探索不充分。

Method: Lin-Hunter 采用了基于变形规则的测试用例生成方法，并结合 LinUCB 自适应策略选择机制，动态优化测试策略以提高错误发现效率。

Result: 在三个月实验中，Lin-Hunter 发现了 18 个独特错误（包括 10 个未知错误），并在多样性和错误发现效率上优于现有方法。

Conclusion: Lin-Hunter 显著提升了 FPGA 工具验证的多样性和效率，被官方开发者确认有效。

Abstract: Field-Programmable Gate Arrays (FPGAs) play an indispensable role in
Electronic Design Automation (EDA), translating Register-Transfer Level (RTL)
designs into gate-level netlists. The correctness and reliability of FPGA logic
synthesis tools are critically important, as unnoticed bugs in these tools may
infect the final hardware implementations. However, recent approaches often
rely heavily on random selection strategies, limiting the structural diversity
of the generated HDL test cases and resulting in inadequate exploration of the
tool's feature space. To address this limitation, we propose Lin-Hunter, a
novel testing framework designed to systematically enhance the diversity of HDL
test cases and the efficiency of FPGA logic synthesis tool validation.
Specifically, Lin-Hunter introduces a principled set of metamorphic
transformation rules to generate functionally equivalent yet structurally
diverse HDL test case variants, effectively addressing the limited diversity of
existing test inputs. To further enhance bug discovery efficiency, Lin-Hunter
integrates an adaptive strategy selection mechanism based on the Linear Upper
Confidence Bound (LinUCB) method. This method leverages feedback from synthesis
logs of previously executed test cases to dynamically prioritize transformation
strategies that have empirically demonstrated a higher likelihood of triggering
synthesis bugs. Comprehensive experiments conducted over a three-month period
demonstrate the practical effectiveness of Lin-Hunter. Our method has
discovered 18 unique bugs, including 10 previously unreported defects, which
have been confirmed by official developers. Moreover, our method outperforms
state-of-the-art testing methods in both test-case diversity and bug-discovery
efficiency.

</details>


### [9] [Policy-driven Software Bill of Materials on GitHub: An Empirical Study](https://arxiv.org/abs/2509.01255)
*Oleksii Novikov,Davide Fucci,Oleksandr Adamov,Daniel Mendez*

Main category: cs.SE

TL;DR: 研究发现，仅有0.56%的GitHub热门仓库包含策略驱动的SBOM，这些SBOM中存在大量漏洞，且22%未报告许可证信息。


<details>
  <summary>Details</summary>
Motivation: SBOM有助于软件供应链安全，但实际研究和应用仍处于早期阶段，研究旨在了解开源项目中策略驱动SBOM的现状。

Method: 通过挖掘GitHub仓库中的SBOM文件，分析其包含的信息和相关漏洞，使用描述性统计方法。

Result: 结果显示策略驱动SBOM的普及率极低，存在2202个唯一漏洞，22%的依赖项未提供许可证信息。

Conclusion: 研究结果为SBOM的安全评估和许可证管理提供了重要参考。

Abstract: Background. The Software Bill of Materials (SBOM) is a machine-readable list
of all the software dependencies included in a software. SBOM emerged as way to
assist securing the software supply chain. However, despite mandates from
governments to use SBOM, research on this artifact is still in its early
stages. Aims. We want to understand the current state of SBOM in open-source
projects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to
achieve security goals, such as enhancing project transparency and ensuring
compliance, rather than being used as fixtures for tools or artificially
generated for benchmarking or academic research purposes. Method. We performed
a mining software repository study to collect and carefully select SBOM files
hosted on GitHub. We analyzed the information reported in policy-driven SBOMs
and the vulnerabilities associated with the declared dependencies by means of
descriptive statistics. Results. We show that only 0.56% of popular GitHub
repositories contain policy-driven SBOM. The declared dependencies contain
2,202 unique vulnerabilities, while 22% of them do not report licensing
information. Conclusion. Our findings provide insights for SBOM usage to
support security assessment and licensing.

</details>


### [10] [Metamorphic Testing of Multimodal Human Trajectory Prediction](https://arxiv.org/abs/2509.01294)
*Helge Spieker,Nadjib Lazaar,Arnaud Gotlieb,Nassim Belmecheri*

Main category: cs.SE

TL;DR: 论文提出了一种基于变态测试（MT）的系统方法来测试多模态人类轨迹预测（HTP）模型，解决了缺乏确定性测试预言的问题。


<details>
  <summary>Details</summary>
Motivation: 由于人类轨迹预测模型的输入多源且输出随机，缺乏明确的测试预言，导致测试困难。

Method: 提出五种变态关系（MRs），包括轨迹和地图输入的标签保留几何变换（镜像、旋转、缩放）以及地图修改变换（改变语义标签、引入障碍物）。

Result: 通过概率违反标准（如Wasserstein或Hellinger距离）评估模型对输入变换的鲁棒性。

Conclusion: 该MT框架可在无需地面真实轨迹的情况下测试多模态随机HTP系统，评估模型鲁棒性。

Abstract: Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.

</details>


### [11] [Aligning Requirement for Large Language Model's Code Generation](https://arxiv.org/abs/2509.01313)
*Zhao Tian,Junjie Chen*

Main category: cs.SE

TL;DR: 该论文提出了一种名为Specine的技术，旨在解决大语言模型（LLM）在代码生成中对输入规范感知不足的问题，从而提高生成代码与规范的匹配度。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界问题的复杂性，LLM生成的代码往往无法完全符合给定的编程规范，而现有技术忽视了规范感知这一关键问题。

Method: 论文借鉴软件需求工程方法，提出Specine技术，通过识别未对齐的输入规范、提取LLM感知的规范并进行对齐，以提升代码生成性能。

Result: 在四个先进LLM和五个挑战性基准测试中，Specine的平均Pass@1性能提升了29.60%，优于十种现有基线方法。

Conclusion: Specine通过改进规范对齐，显著提升了LLM的代码生成能力，为解决规范感知不足问题提供了有效解决方案。

Abstract: Code generation refers to the automatic generation of source code based on a
given programming specification, which has garnered significant attention
particularly with the advancement of large language models (LLMs). However, due
to the inherent complexity of real-world problems, the LLM-generated code often
fails to fully align with the provided specification. While state-of-the-art
agent-based techniques have been proposed to enhance LLM code generation, they
overlook the critical issue of specification perception, resulting in
persistent misalignment issues. Given that accurate perception of programming
specifications serves as the foundation of the LLM-based code generation
paradigm, ensuring specification alignment is particularly crucial. In this
work, we draw on software requirements engineering to propose Specine, a novel
specification alignment technique for LLM code generation. Its key idea is to
identify misaligned input specifications, lift LLM-perceived specifications,
and align them to enhance the code generation performance of LLMs. Our
comprehensive experiments on four state-of-the-art LLMs across five challenging
competitive benchmarks by comparing with ten state-of-the-art baselines,
demonstrate the effectiveness of Specine. For example, Specine outperforms the
most effective baseline, achieving an average improvement of 29.60\% across all
subjects in terms of Pass@1.

</details>


### [12] [Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing](https://arxiv.org/abs/2509.01318)
*Chiara Ghinami,Jonas Winzer,Nils Bosbach,Lennart M. Reimann,Lukas Jünger,Simon Wörner,Rainer Leupers*

Main category: cs.SE

TL;DR: SystemC虚拟原型和模糊测试的结合框架，用于嵌入式软件测试。


<details>
  <summary>Details</summary>
Motivation: 嵌入式软件测试中模糊测试的应用受限，现有解决方案缺乏灵活性和对外设的支持。

Method: 开发一个框架，将AFL模糊测试与SystemC模拟器解耦，拦截外设访问并模糊测试值。

Result: 框架支持不同模拟器和虚拟原型的灵活集成，测试了多种软件。

Conclusion: 该框架提升了嵌入式软件模糊测试的灵活性和效率。

Abstract: SystemC-based virtual prototypes have emerged as widely adopted tools to test
software ahead of hardware availability, reducing the time-to-market and
improving software reliability. Recently, fuzzing has become a popular method
for automated software testing due to its ability to quickly identify
corner-case errors. However, its application to embedded software is still
limited. Simulator tools can help bridge this gap by providing a more powerful
and controlled execution environment for testing. Existing solutions, however,
often tightly couple fuzzers with built-in simulators that lack support for
hardware peripherals and of- fer limited flexibility, restricting their ability
to test embedded software. To address these limitations, we present a framework
that allows the integration of American-Fuzzy-Lop-based fuzzers and
SystemC-based simulators. The framework provides a harness to decouple the
adopted fuzzer and simulator. In addition, it intercepts peripheral accesses
and queries the fuzzer for values, effectively linking peripheral behavior to
the fuzzer. This solution enables flexible interchangeability of peripher- als
within the simulation environment and supports the interfacing of different
SystemC-based virtual prototypes. The flexibility of the pro- posed solution is
demonstrated by integrating the harness with different simulators and by
testing various softwares.

</details>


### [13] [Towards Multi-Platform Mutation Testing of Task-based Chatbots](https://arxiv.org/abs/2509.01389)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文提出了MUTABOT方法的扩展，支持多平台（Dialogflow和Rasa），并通过实验展示如何利用突变测试揭示Botium测试套件的弱点。


<details>
  <summary>Details</summary>
Motivation: 由于任务型聊天机器人在测试中存在难以覆盖所有对话场景的问题，导致潜在缺陷未被发现，因此需要一种方法改进测试效果。

Method: 扩展MUTABOT至多平台（Dialogflow和Rasa），利用突变测试注入故障并生成有缺陷的聊天机器人。

Result: 实验证明突变测试可以有效揭示Botium生成的测试套件的不足。

Conclusion: MUTABOT扩展支持多平台，为任务型聊天机器人的测试提供了更高效的工具。

Abstract: Chatbots, also known as conversational agents, have become ubiquitous,
offering services for a multitude of domains. Unlike general-purpose chatbots,
task-based chatbots are software designed to prioritize the completion of tasks
of the domain they handle (e.g., flight booking). Given the growing popularity
of chatbots, testing techniques that can generate full conversations as test
cases have emerged. Still, thoroughly testing all the possible conversational
scenarios implemented by a task-based chatbot is challenging, resulting in
incorrect behaviors that may remain unnoticed. To address this challenge, we
proposed MUTABOT, a mutation testing approach for injecting faults in
conversations and producing faulty chatbots that emulate defects that may
affect the conversational aspects. In this paper, we present our extension of
MUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments
that show how mutation testing can be used to reveal weaknesses in test suites
generated by the Botium state-of-the-art test generator.

</details>


### [14] [Non Technical Debt in Agile Software Development](https://arxiv.org/abs/2509.01445)
*Muhammad Ovais Ahmad,Tomas Gustavsson*

Main category: cs.SE

TL;DR: 非技术债务（NTD）是敏捷软件开发中的常见问题，分为流程债务、社会债务、人员债务和组织债务。研究发现，高效团队和心理安全对创新至关重要，而低效流程和角色模糊会降低满意度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示非技术债务（NTD）在敏捷开发中的影响，帮助团队和领导者通过科学方法减少债务，提升效率。

Method: 采用问卷调查、深度访谈和统计分析，结合工业合作伙伴的实际经验，分析NTD的驱动因素及其影响。

Result: 发现高效团队、心理安全、清晰角色和流程优化对减少NTD至关重要；社交碎片化和人力资源问题则会降低生产力。

Conclusion: 通过优化团队结构、明确角色、提升心理安全和流程效率，组织可以减少NTD，释放团队潜力。

Abstract: NonTechnical Debt (NTD) is a common challenge in agile software development,
manifesting in four critical forms, Process Debt, Social Debt, People Debt,
Organizational debt. NODLA project is a collaboration between Karlstad
University and four leading Swedish industrial partners, reveals how various
debt types disrupt large scale Agile Software Development (ASD) environments.
Through extensive surveys, indepth interviews, and statistical analyses
involving a diverse group of software professionals, we identified key drivers
of NTD and their impacts. Our findings emphasize (1) Well structured, highly
cohesive teams learn faster, adapt more effectively, and innovate consistently.
(2) Psychological safety, fostered by proactive leadership, is essential for
innovation, experimentation, and keeping employees. (3) Inefficient processes
and unclear roles contribute significantly to drops in job satisfaction,
productivity and team morale. (4) Social fragmentation, particularly in remote
and hybrid settings, breeds rework, delays, and increased costs. (5) Neglected
human resource needs, such as delayed hiring or insufficient training, limit an
organization ability to meet growing demands. This white paper distils these
insights into practical, evidence based strategies, such as refining team
composition, clarifying roles, fostering psychological safety, streamlining
workflows, and embracing failure as a learning tool. By implementing these
strategies, organizations can reduce NTD, reclaim agility, and unlock their
teams full potential.

</details>


### [15] [Benchmarking and Studying the LLM-based Code Review](https://arxiv.org/abs/2509.01494)
*Zhengran Zeng,Ruikai Shi,Keke Han,Yixin Li,Kaicheng Sun,Yidong Wang,Zhuohao Yu,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 介绍了一个名为SWRBench的新基准测试，用于评估自动化代码审查，解决了现有基准测试的不足，并提出了改进性能的策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法反映真实世界复杂性，限制了现代大型语言模型（LLMs）的评估。

Method: 基于1000个手动验证的GitHub拉取请求（PRs），提出PR中心化的审查方法，使用LLM客观评估。

Result: 当前ACR工具表现不佳，但更擅长检测功能错误；多审查聚合策略显著提升了性能。

Conclusion: SWRBench为ACR研究提供了有价值的基准、评估方法和改进策略。

Abstract: Automated Code Review (ACR) is crucial for software quality, yet existing
benchmarks often fail to reflect real-world complexities, hindering the
evaluation of modern Large Language Models (LLMs). Current benchmarks
frequently focus on fine-grained code units, lack complete project context, and
use inadequate evaluation metrics. To address these limitations, we introduce
SWRBench , a new benchmark comprising 1000 manually verified Pull Requests
(PRs) from GitHub, offering PR-centric review with full project context.
SWRBench employs an objective LLM-based evaluation method that aligns strongly
with human judgment (~90 agreement) by verifying if issues from a structured
ground truth are covered in generated reviews. Our systematic evaluation of
mainstream ACR tools and LLMs on SWRBench reveals that current systems
underperform, and ACR tools are more adept at detecting functional errors.
Subsequently, we propose and validate a simple multi-review aggregation
strategy that significantly boosts ACR performance, increasing F1 scores by up
to 43.67%. Our contributions include the SWRBench benchmark, its objective
evaluation method, a comprehensive study of current ACR capabilities, and an
effective enhancement approach, offering valuable insights for advancing ACR
research.

</details>


### [16] [A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model](https://arxiv.org/abs/2509.01527)
*Amirreza Nayyeri,Abbas Rasoolzadegan*

Main category: cs.SE

TL;DR: 本文介绍了一种基于大型语言模型的隐私保护推荐工具，用于本地化生成有效的网页表单测试用例，避免数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 网页表单测试中手动生成测试用例耗时且易出错，现有云基大型语言模型工具存在数据泄露风险，需要一个本地化解决方案。

Method: 工具通过分析HTML结构、检测输入类型并提取约束条件，基于本地化大型语言模型生成有效的字段填充建议。

Result: 开发出一种本地化操作的工具，能在保护隐私的同时高效生成测试用例。

Conclusion: 该工具为网页表单测试提供了一种安全且智能的解决方案，解决了数据泄露问题并提升了测试效率。

Abstract: Web applications are increasingly used in critical domains such as education,
finance, and e-commerce. This highlights the need to ensure their failure-free
performance. One effective method for evaluating failure-free performance is
web form testing, where defining effective test scenarios is key to a complete
and accurate evaluation. A core aspect of this process involves filling form
fields with suitable values to create effective test cases. However, manually
generating these values is time-consuming and prone to errors. To address this,
various tools have been developed to assist testers. With the appearance of
large language models (LLMs), a new generation of tools seeks to handle this
task more intelligently. Although many LLM-based tools have been introduced, as
these models typically rely on cloud infrastructure, their use in testing
confidential web forms raises concerns about unintended data leakage and
breaches of confidentiality. This paper introduces a privacy-preserving
recommender that operates locally using a large language model. The tool
assists testers in web form testing by suggesting effective field values. This
tool analyzes the HTML structure of forms, detects input types, and extracts
constraints based on each field's type and contextual content, guiding proper
field filling.

</details>


### [17] [WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing](https://arxiv.org/abs/2509.01612)
*Omur Sahin,Man Zhang,Andrea Arcuri*

Main category: cs.SE

TL;DR: 本文提出了Web Fuzzing Commons (WFC)和Web Fuzzing Dataset (WFD)，以解决REST API模糊测试中的三大挑战：API认证、故障类型分类与比较、以及公平比较用的案例研究。


<details>
  <summary>Details</summary>
Motivation: 解决REST API模糊测试中关于认证、故障分类和公平比较的三大问题，推动该领域的研究进展。

Method: 提出了WFC（开源的库和模式定义）和WFD（36个开源API集合），支持实验运行和故障分类。通过EvoMaster等工具的实验验证其有效性。

Result: WFC和WFD为模糊测试提供了统一的框架和数据集，解决了三大挑战，并展示了与其他工具的对比结果。

Conclusion: WFC和WFD为REST API模糊测试提供了实用工具，支持公平比较和标准化研究，推动了领域发展。

Abstract: Fuzzing REST APIs is an important research problem, with practical
applications and impact in industry. As such, a lot of research work has been
carried out on this topic in the last few years. However, there are three major
issues that hinder further progress: how to deal with API authentication; how
to catalog and compare different fault types found by different fuzzers; and
what to use as case study to facilitate fair comparisons among fuzzers. To
address these important challenges, we present Web Fuzzing Commons (WFC) and
Web Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema
definitions to declaratively specify authentication info and catalog different
types of faults that fuzzers can automatically detect. WFD is a collection of
36 open-source APIs with all necessary scaffolding to easily run experiments
with fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of
experiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web
APIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster
with other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest,
RESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as
well as providing guidelines with support of WFC/WFD to avoid them.

</details>


### [18] [Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing](https://arxiv.org/abs/2509.01616)
*Konstantinos Kitsios,Marco Castelluccio,Alberto Bacchelli*

Main category: cs.SE

TL;DR: BLAST是一种结合LLM和SBST的工具，用于自动从问题-补丁对中生成问题复现测试，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 开发者常在不带测试的情况下提交补丁，因此需要自动化生成问题复现测试以提高开发效率和代码质量。

Method: BLAST通过LLM结合git历史分析、静态分析和SBST生成的测试，补充问题描述和补丁信息，并利用SBST优化生成测试。

Result: BLAST在426个Python问题中成功生成了151个测试（35.4%），在32个PR-问题对中生成了11个测试。

Conclusion: BLAST展示了自动化测试生成的潜力，但仍需进一步优化和改进以适应开发者需求。

Abstract: Issue-reproducing tests fail on buggy code and pass once a patch is applied,
thus increasing developers' confidence that the issue has been resolved and
will not be re-introduced. However, past research has shown that developers
often commit patches without such tests, making the automated generation of
issue-reproducing tests an area of interest. We propose BLAST, a tool for
automatically generating issue-reproducing tests from issue-patch pairs by
combining LLMs and search-based software testing (SBST). For the LLM part, we
complement the issue description and the patch by extracting relevant context
through git history analysis, static analysis, and SBST-generated tests. For
the SBST part, we adapt SBST for generating issue-reproducing tests; the issue
description and the patch are fed into the SBST optimization through an
intermediate LLM-generated seed, which we deserialize into SBST-compatible
form. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%)
of the issues from a curated Python benchmark, outperforming the
state-of-the-art (23.5%). Additionally, to measure the real-world impact of
BLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR)
linked to an issue is opened, and if BLAST generates an issue-reproducing test,
the bot proposes it as a comment in the PR. We deployed the bot in three
open-source repositories for three months, gathering data from 32 PRs-issue
pairs. BLAST generated an issue-reproducing test in 11 of these cases, which we
proposed to the developers. By analyzing the developers' feedback, we discuss
challenges and opportunities for researchers and tool builders. Data and
material: https://doi.org/10.5281/zenodo.16949042

</details>


### [19] [Tether: A Personalized Support Assistant for Software Engineers with ADHD](https://arxiv.org/abs/2509.01946)
*Aarsh Shah,Cleyton Magalhaes,Kiev Gama,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Tether是一款专为ADHD开发者设计的LLM桌面应用，通过实时监控、检索增强生成和游戏化提供个性化支持，初步验证表明其上下文准确性有所提升，未来有望成为神经多样性工具的基础。


<details>
  <summary>Details</summary>
Motivation: 软件开发中的多样性与包容性常忽视神经多样性，尤其是ADHD开发者的认知挑战。现有工具较少针对其特定需求设计，因此开发了Tether以填补这一空白。

Method: 结合本地活动监控、检索增强生成（RAG）和游戏化，Tether提供实时专注支持和个性化对话，集成操作系统级跟踪以提示任务参与。

Result: 初步自我验证表明，通过迭代优化提示和RAG增强，系统的上下文准确性有所改善。

Conclusion: Tether为未来神经多样性工具奠定了基础，展示了LLM作为个性化支持系统的潜力，尤其适用于未被充分代表的认知需求。

Abstract: Equity, diversity, and inclusion in software engineering often overlook
neurodiversity, particularly the experiences of developers with Attention
Deficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that
population in SE, few tools are designed to support their cognitive challenges
(e.g., sustained attention, task initiation, self-regulation) within
development workflows. We present Tether, an LLM-powered desktop application
designed to support software engineers with ADHD by delivering adaptive,
context-aware assistance. Drawing from engineering research methodology, Tether
combines local activity monitoring, retrieval-augmented generation (RAG), and
gamification to offer real-time focus support and personalized dialogue. The
system integrates operating system level system tracking to prompt engagement
and its chatbot leverages ADHD-specific resources to offer relevant responses.
Preliminary validation through self-use revealed improved contextual accuracy
following iterative prompt refinements and RAG enhancements. Tether
differentiates itself from generic tools by being adaptable and aligned with
software-specific workflows and ADHD-related challenges. While not yet
evaluated by target users, this work lays the foundation for future
neurodiversity-aware tools in SE and highlights the potential of LLMs as
personalized support systems for underrepresented cognitive needs.

</details>


### [20] [Automated Repair of C Programs Using Large Language Models](https://arxiv.org/abs/2509.01947)
*Mahdi Farzandway,Fatemeh Ghassemi*

Main category: cs.SE

TL;DR: 本研究提出了一种结合频谱式故障定位、运行时反馈和思维链提示的框架，用于自动修复C程序，实现了44.93%的修复准确率。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在C程序自动修复中的潜力，结合统计程序分析和LLM推理，改进现有方法。

Method: 通过迭代修复循环，利用结构化思维链提示，结合失败测试、可疑代码区域和先前补丁结果生成新补丁。

Result: 在Codeflaws基准测试中修复了44.93%的漏洞，比现有基线（如GPT-4加思维链）提升了3.61%。

Conclusion: 研究表明统计程序分析与生成式AI结合是自动调试的可行方向。

Abstract: This study explores the potential of Large Language Models (LLMs) in
automating the repair of C programs. We present a framework that integrates
spectrum-based fault localization (SBFL), runtime feedback, and
Chain-of-Thought-structured prompting into an autonomous repair loop. Unlike
prior approaches, our method explicitly combines statistical program analysis
with LLM reasoning. The iterative repair cycle leverages a structured
Chain-of-Thought (CoT) prompting approach, where the model reasons over failing
tests, suspicious code regions, and prior patch outcomes, before generating new
candidate patches. The model iteratively changes the code, evaluates the
results, and incorporates reasoning from previous attempts into subsequent
modifications, reducing repeated errors and clarifying why some bugs remain
unresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where
our approach achieves 44.93% repair accuracy, representing a 3.61% absolute
improvement over strong state-of-the-art APR baselines such as GPT-4 with CoT.
This outcome highlights a practical pathway toward integrating statistical
program analysis with generative AI in automated debugging.

</details>


### [21] [ProbTest: Unit Testing for Probabilistic Programs (Extended Version)](https://arxiv.org/abs/2509.02012)
*Katrine Christensen,Mahsa Varshosaz,Raúl Pardo*

Main category: cs.SE

TL;DR: ProbTest是一种基于优惠券收集问题理论的黑盒单元测试方法，用于测试概率程序的输出结果，开发者可以像平常一样编写单元测试，无需额外努力，同时自动确定所需的测试执行次数并提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 由于概率程序的随机性，测试其输出结果需要多次执行，而传统确定性程序只需单次执行。因此，如何确定运行次数以有效测试概率程序成为一个关键问题。

Method: 提出了一种名为ProbTest的黑盒单元测试方法，基于优惠券收集问题理论，自动确定测试执行次数并提供统计保证。该方法通过PyTest插件实现，开发者可以像编写普通Python程序一样编写测试。

Result: 在Gymnasium强化学习库和随机化数据结构的案例研究中，ProbTest被验证为有效的方法。

Conclusion: ProbTest提供了一种简便且统计可靠的方式来测试概率程序，减少了开发者的手动负担，并能自动确定测试次数。

Abstract: Testing probabilistic programs is non-trivial due to their stochastic nature.
Given an input, the program may produce different outcomes depending on the
underlying stochastic choices in the program. This means testing the expected
outcomes of probabilistic programs requires repeated test executions unlike
deterministic programs where a single execution may suffice for each test
input. This raises the following question: how many times should we run a
probabilistic program to effectively test it? This work proposes a novel
black-box unit testing method, ProbTest, for testing the outcomes of
probabilistic programs. Our method is founded on the theory surrounding a
well-known combinatorial problem, the coupon collector's problem. Using this
method, developers can write unit tests as usual without extra effort while the
number of required test executions is determined automatically with statistical
guarantees for the results. We implement ProbTest as a plug-in for PyTest, a
well-known unit testing tool for python programs. Using this plug-in,
developers can write unit tests similar to any other Python program and the
necessary test executions are handled automatically. We evaluate the method on
case studies from the Gymnasium reinforcement learning library and a randomized
data structure.

</details>


### [22] [Scalable Thread-Safety Analysis of Java Classes with CodeQL](https://arxiv.org/abs/2509.02022)
*Bjørnar Haugstad Jåtten,Simon Boye Jørgensen,Rasmus Petersen,Raúl Pardo*

Main category: cs.SE

TL;DR: 本文提出了一种高度可扩展的方法来分析Java类的线程安全性，基于Java内存模型的正确性原则定义了线程安全，并通过静态分析工具CodeQL自动检测代码。


<details>
  <summary>Details</summary>
Motivation: 在面向对象语言中，开发人员依赖线程安全的类来实现并发应用，但判断一个类是否线程安全是一项挑战。

Method: 设计了确保线程安全的一组属性，并将其编码到CodeQL中，对GitHub前1000个仓库中的Java类进行了分析。

Result: 在3632865个Java类中检测到数千个线程安全错误，运行时间短，开发者对提交的错误修复PR反应积极。

Conclusion: 该方法适用于现实世界的代码库，具有高度的可扩展性和实用性。

Abstract: In object-oriented languages software developers rely on thread-safe classes
to implement concurrent applications. However, determining whether a class is
thread-safe is a challenging task. This paper presents a highly scalable method
to analyze thread-safety in Java classes. We provide a definition of
thread-safety for Java classes founded on the correctness principle of the Java
memory model, data race freedom. We devise a set of properties for Java classes
that are proven to ensure thread-safety. We encode these properties in the
static analysis tool CodeQL to automatically analyze Java source code. We
perform an evaluation on the top 1000 GitHub repositories. The evaluation
comprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from
71 repositories. These repositories include highly popular software such as
Apache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k
starts), and gRPC (11.6k starts). Our queries detected thousands of
thread-safety errors. The running time of our queries is below 2 minutes for
repositories up to 200k lines of code, 20k methods, 6000 fields, and 1200
classes. We have submitted a selection of detected concurrency errors as PRs,
and developers positively reacted to these PRs. We have submitted our CodeQL
queries to the main CodeQL repository, and they are currently in the process of
becoming available as part of GitHub actions. The results demonstrate the
applicability and scalability of our method to analyze thread-safety in
real-world code bases.

</details>


### [23] [Curiosity-Driven Testing for Sequential Decision-Making Process](https://arxiv.org/abs/2509.02025)
*Junda He,Zhou Yang,Jieke Shi,Chengran Yang,Kisub Kim,Bowen Xu,Xin Zhou,David Lo*

Main category: cs.SE

TL;DR: 提出了一种名为CureFuzz的新方法，用于测试顺序决策过程（SDMs）的安全性，通过好奇心驱动和多目标种子选择技术，有效发现多样化的触发崩溃场景。


<details>
  <summary>Details</summary>
Motivation: 顺序决策过程（SDMs）在复杂现实问题中至关重要，但现有深度学习方法容易产生不安全行为，需要开发一种能够识别多样化崩溃触发场景的测试框架。

Method: 提出CureFuzz方法，结合好奇心机制和多目标种子选择技术，以优化黑盒模糊测试过程，提高崩溃场景的检测能力。

Result: CureFuzz在多种SDMs上表现优异，比现有方法发现更多故障和多样化的崩溃触发场景，并能修复SDMs。

Conclusion: CureFuzz是一种有效的测试工具，可用于优化SDMs的性能和安全性。

Abstract: Sequential decision-making processes (SDPs) are fundamental for complex
real-world challenges, such as autonomous driving, robotic control, and traffic
management. While recent advances in Deep Learning (DL) have led to mature
solutions for solving these complex problems, SDMs remain vulnerable to
learning unsafe behaviors, posing significant risks in safety-critical
applications. However, developing a testing framework for SDMs that can
identify a diverse set of crash-triggering scenarios remains an open challenge.
To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz
testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows
a fuzzer to effectively explore novel and diverse scenarios, leading to
improved detection of crashtriggering scenarios. Additionally, we introduce a
multi-objective seed selection technique to balance the exploration of novel
scenarios and the generation of crash-triggering scenarios, thereby optimizing
the fuzzing process. We evaluate CureFuzz on various SDMs and experimental
results demonstrate that CureFuzz outperforms the state-of-the-art method by a
substantial margin in the total number of faults and distinct types of
crash-triggering scenarios. We also demonstrate that the crash-triggering
scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a
valuable tool for testing SDMs and optimizing their performance.

</details>


### [24] [Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports](https://arxiv.org/abs/2509.02150)
*Pin Ji,Yang Feng,Zongtai Li,Xiangchi Zhou,Jia Liu,Jun Sun,Zhihong Zhao*

Main category: cs.SE

TL;DR: Txt2Sce是一种基于文本事故报告生成OpenSCENARIO格式测试场景的方法，利用LLM转换文本并生成多样化的场景文件树，有效检测自动驾驶系统（如Autoware）的异常行为。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂场景中仍存在挑战，现有测试方法依赖视觉数据且缺乏标准化格式，限制了可扩展性和可移植性。

Method: Txt2Sce通过LLM将文本事故报告转换为OpenSCENARIO文件，然后通过场景分解、块变异和组装生成场景文件树。

Result: 实验生成了33个场景文件树（共4,373个文件），成功检测到Autoware在安全性、智能性和平稳性方面的异常行为。

Conclusion: Txt2Sce能高效生成标准化测试场景，提升自动驾驶系统测试的多样性和效果。

Abstract: With the rapid advancement of deep learning and related technologies,
Autonomous Driving Systems (ADSs) have made significant progress and are
gradually being widely applied in safety-critical fields. However, numerous
accident reports show that ADSs still encounter challenges in complex
scenarios. As a result, scenario-based testing has become essential for
identifying defects and ensuring reliable performance. In particular,
real-world accident reports offer valuable high-risk scenarios for more
targeted ADS testing. Despite their potential, existing methods often rely on
visual data, which demands large memory and manual annotation. Additionally,
since existing methods do not adopt standardized scenario formats (e.g.,
OpenSCENARIO), the generated scenarios are often tied to specific platforms and
ADS implementations, limiting their scalability and portability. To address
these challenges, we propose Txt2Sce, a method for generating test scenarios in
OpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM
to convert textual accident reports into corresponding OpenSCENARIO scenario
files. It then generates a derivation-based scenario file tree through scenario
disassembly, scenario block mutation, and scenario assembly. By utilizing the
derivation relationships between nodes in the scenario tree, Txt2Sce helps
developers identify the scenario conditions that trigger unexpected behaviors
of ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file
trees, resulting in a total of 4,373 scenario files for testing the open-source
ADS, Autoware. The experimental results show that Txt2Sce successfully converts
textual reports into valid OpenSCENARIO files, enhances scenario diversity
through mutation, and effectively detects unexpected behaviors of Autoware in
terms of safety, smartness, and smoothness.

</details>


### [25] [Formalizing Operational Design Domains with the Pkl Language](https://arxiv.org/abs/2509.02221)
*Martin Skoglund,Fredrik Warg,Anders Thorsén,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: 本文提出了一种使用Pkl语言形式化操作设计域（ODD）的方法，以解决ODD规范中的挑战，并通过汽车示例展示了其广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 随着自动化功能的部署，传统依赖人工操作的安全评估框架已不适用，需要新的方法来证明自动化功能在真实条件下不引入不可接受的风险。

Method: 采用Pkl语言形式化ODD，利用其专门的配置语言特性提高可用性，同时解决规范中的灵活性和一致性问题。

Result: 通过汽车示例验证了方法的有效性，展示了如何确保对操作环境的严格评估。

Conclusion: 该方法不仅适用于汽车领域，还可广泛用于其他需要严格评估操作环境的场景。

Abstract: The deployment of automated functions that can operate without direct human
supervision has changed safety evaluation in domains seeking higher levels of
automation. Unlike conventional systems that rely on human operators, these
functions require new assessment frameworks to demonstrate that they do not
introduce unacceptable risks under real-world conditions. To make a convincing
safety claim, the developer must present a thorough justification argument,
supported by evidence, that a function is free from unreasonable risk when
operated in its intended context. The key concept relevant to the presented
work is the intended context, often captured by an Operational Design Domain
specification (ODD). ODD formalization is challenging due to the need to
maintain flexibility in adopting diverse specification formats while preserving
consistency and traceability and integrating seamlessly into the development,
validation, and assessment. This paper presents a way to formalize an ODD in
the Pkl language, addressing central challenges in specifying ODDs while
improving usability through specialized configuration language features. The
approach is illustrated with an automotive example but can be broadly applied
to ensure rigorous assessments of operational contexts.

</details>


### [26] [Methodology for Test Case Allocation based on a Formalized ODD](https://arxiv.org/abs/2509.02311)
*Martin Skoglund,Fredrik Warg,Anders Thoren,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: 论文提出了一种通过扩展ODD形式化方法来评估测试案例在不同测试环境中的适用性，以实现自动化测试案例分配。


<details>
  <summary>Details</summary>
Motivation: 随着CCAM系统的出现，传统的安全评估方法已无法满足需求，需要新的方法来评估其安全性。不同测试环境的能力和需求存在差异，如何对齐这些差异是一项挑战。

Method: 通过扩展现有的ODD形式化方法，整合ODD参数和测试属性，以捕获测试环境的相关能力。该方法支持自动化的适用性评估。

Result: 通过一个自动倒车卡车功能的案例研究验证了该方法的有效性，系统实现精度与ODD参数绑定，支持基于环境能力的测试案例自动分配。

Conclusion: 提出的方法为CCAM系统的安全评估提供了一种有效的测试案例分配解决方案，解决了测试环境多样性带来的挑战。

Abstract: The emergence of Connected, Cooperative, and Automated Mobility (CCAM)
systems has significantly transformed the safety assessment landscape. Because
they integrate automated vehicle functions beyond those managed by a human
driver, new methods are required to evaluate their safety. Approaches that
compile evidence from multiple test environments have been proposed for
type-approval and similar evaluations, emphasizing scenario coverage within the
systems Operational Design Domain (ODD). However, aligning diverse test
environment requirements with distinct testing capabilities remains
challenging. This paper presents a method for evaluating the suitability of
test case allocation to various test environments by drawing on and extending
an existing ODD formalization with key testing attributes. The resulting
construct integrates ODD parameters and additional test attributes to capture a
given test environments relevant capabilities. This approach supports automatic
suitability evaluation and is demonstrated through a case study on an automated
reversing truck function. The system's implementation fidelity is tied to ODD
parameters, facilitating automated test case allocation based on each
environments capacity for object-detection sensor assessment.

</details>


### [27] [ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation](https://arxiv.org/abs/2509.02330)
*Yicong Zhao,Shisong Chen,Jiacheng Zhang,Zhixu Li*

Main category: cs.SE

TL;DR: ReCode是一个细粒度的检索增强学习框架，通过算法感知检索和模块化双编码器架构，实现了高效且准确的代码修复，显著降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法存在高训练成本或计算昂贵的问题，而传统检索策略无法捕捉代码的结构细节。

Method: ReCode引入了算法感知检索策略和模块化双编码器架构，实现对输入和上下文的细粒度语义匹配。

Result: 实验表明，ReCode在实际代码修复场景中显著提高了修复准确性并降低了推理成本。

Conclusion: ReCode为现实世界的代码修复提供了实用的解决方案，特别是在高效性和准确性方面表现优越。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in code-related tasks, such as code generation and automated
program repair. Despite their promising performance, most existing approaches
for code repair suffer from high training costs or computationally expensive
inference. Retrieval-augmented generation (RAG), with its efficient in-context
learning paradigm, offers a more scalable alternative. However, conventional
retrieval strategies, which are often based on holistic code-text embeddings,
fail to capture the structural intricacies of code, resulting in suboptimal
retrieval quality. To address the above limitations, we propose ReCode, a
fine-grained retrieval-augmented in-context learning framework designed for
accurate and efficient code repair. Specifically, ReCode introduces two key
innovations: (1) an algorithm-aware retrieval strategy that narrows the search
space using preliminary algorithm type predictions; and (2) a modular
dual-encoder architecture that separately processes code and textual inputs,
enabling fine-grained semantic matching between input and retrieved contexts.
Furthermore, we propose RACodeBench, a new benchmark constructed from
real-world user-submitted buggy code, which addresses the limitations of
synthetic benchmarks and supports realistic evaluation. Experimental results on
RACodeBench and competitive programming datasets demonstrate that ReCode
achieves higher repair accuracy with significantly reduced inference cost,
highlighting its practical value for real-world code repair scenarios.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [28] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop是一个可编程的语义约束解码框架，通过抽象程序结构和正则共数据的可实现性验证，确保语言模型生成的代码满足丰富语义属性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型生成的代码难以保证正确性，当前约束解码方法仅限于浅层语法约束或脆弱的语义编码。

Method: ChopChop结合基于共诱导的形式化方法，将语义约束转化为正则共数据的可实现性问题。

Result: 在类型安全和程序等价约束下，ChopChop提高了语言模型的生成成功率，同时保持解码延迟在可接受范围。

Conclusion: ChopChop将语义约束解码从小众技术转变为系统性扩展，为LM驱动的代码生成提供了理论和实践基础。

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [29] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: 论文提出了支持对称性属性验证的Hoare风格逻辑和工具SymVerif，用于验证程序中的对称性，并在基准测试中发现了文献中的错误。


<details>
  <summary>Details</summary>
Motivation: 现有形式化方法缺乏对对称性属性的支持，因此需要开发一种方法来验证这些属性。

Method: 设计了一种描述群动作的语法，开发了Hoare风格的逻辑，并实现了工具SymVerif。

Result: 工具成功验证了一系列手写基准测试的对称性属性，并发现了文献中的错误。

Conclusion: 提出的方法能够有效验证程序中的对称性属性，且工具在实际应用中表现出色。

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [30] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新算法，将G代码转换为立方体集合并定义点云表示，用于3D打印中的错误检测和工具评估。


<details>
  <summary>Details</summary>
Motivation: 现有3D打印流程缺乏类似传统编译器的程序不变量检查方法，亟需新工具提升可靠性。

Method: 通过将G代码转换为立方体集合，并定义近似点云表示，实现高效操作。

Result: 工具GlitchFinder能在58个真实CAD模型中有效定位小特征引起的切片问题，比较切片工具差异并检测网格修复工具的错误。

Conclusion: 该算法为3D打印流程提供了新的错误检测和工具评估手段。

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [31] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: 论文提出了一个字符串序列理论，研究了其可满足性问题，并通过限制为直线片段恢复可判定性。该方法通过将字符串序列编码为字符串，并在OSTRICH框架中实现，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器通常不支持字符串序列的常见操作（如正则匹配、分割等），而这些操作在字符串处理程序中频繁使用。因此，论文提出了一种字符串序列理论以弥补这一不足。

Method: 将字符串序列编码为字符串，并将字符串序列操作映射为相应的字符串操作。通过限制为直线片段恢复可判定性，并在OSTRICH框架中实现预图像计算。

Result: 实验验证了方法的有效性，包括从实际JavaScript程序生成的基准约束、手工模板和单元测试。

Conclusion: 提出的字符串序列理论在直线片段下是可判定的，且通过实现工具展示了其实际应用价值。

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [32] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 本文探讨了覆盖率类型与不正确性逻辑框架之间的联系，提出了一种将不正确性推理更系统地集成到细化类型系统中的机制。


<details>
  <summary>Details</summary>
Motivation: 动机在于利用覆盖率类型的不完全推理能力，验证测试生成器的完整性和安全性，并探索其与不正确性逻辑框架的深层联系。

Method: 方法是通过探索覆盖率类型与不正确性逻辑框架的连接，提出一种机制将它们集成到表达性细化类型系统中。

Result: 结果为该方法为函数式程序员、程序验证者和分析工具提供了新的机会。

Conclusion: 结论是这种集成可以为程序验证和分析工具带来更多可能性。

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [33] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 本文提出了一个基于类型理论的框架，用于分析功能程序与库API交互时的错误行为，利用符号正则表达式（SREs）和符号有限自动机（SFAs）进行推理。


<details>
  <summary>Details</summary>
Motivation: 研究功能程序在调用外部库API时可能出现的错误行为，提供系统化的分析方法。

Method: 基于痕迹（API调用的时间序列）和SREs，设计了一种类型推断算法，通过SFAs组合推理痕迹。

Result: 算法成功时，推断的类型能够证明ADT实现可能表现出指定的错误行为。

Conclusion: 该方法首次系统化地实现了基于痕迹的错误行为推理，为组合分析提供了新工具。

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [34] [Improving Nonpreemptive Multiserver Job Scheduling with Quickswap](https://arxiv.org/abs/2509.01893)
*Zhongrui Chen,Adityo Anggraito,Diletta Olliaro,Andrea Marin,Marco Ajmone Marsan,Benjamin Berg,Isaac Grosof*

Main category: cs.PF

TL;DR: 论文提出了一种名为MSFQ的调度策略，旨在优化多服务器作业的响应时间，通过减少作业等待时间的变异性，显著优于传统的MSF和FCFS策略。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心的多服务器作业需要多个CPU核心，且通常是状态化的，抢占作业会导致高开销。为避免抢占，需设计非抢占式调度策略，以实现高系统利用率和低平均响应时间。

Method: 论文提出MSFQ策略，通过周期性赋予其他作业优先级，减少作业等待时间的变异性。理论分析和仿真评估验证了其性能。

Result: MSFQ在作业请求单核或所有核心时显著优于MSF；通过优化，其变体在复杂场景下也优于MSF和FCFS。

Conclusion: MSFQ策略能有效降低多服务器作业的响应时间，提升系统性能，适用于实际工作负载。

Abstract: Modern data center workloads are composed of multiserver jobs, computational
jobs that require multiple CPU cores in order to run. A data center server can
run many multiserver jobs in parallel, as long as it has sufficient resources
to meet their demands. However, multiserver jobs are generally stateful,
meaning that job preemptions incur significant overhead from saving and
reloading the state associated with running jobs. Hence, most systems try to
avoid these costly job preemptions altogether. Given these constraints, a
scheduling policy must determine what set of jobs to run in parallel at each
moment in time to minimize the mean response time across a stream of arriving
jobs. Unfortunately, simple non-preemptive policies such as FCFS may leave many
cores idle, resulting in high mean response times or even system instability.
Our goal is to design and analyze non-preemptive scheduling policies for
multiserver jobs that maintain high system utilization to achieve low mean
response time.
  One well-known non-preemptive policy, Most Servers First (MSF), prioritizes
jobs with higher core requirements and achieves high resource utilization.
However, MSF causes extreme variability in job waiting times, and can perform
significantly worse than FCFS in practice. To address this, we propose and
analyze a class of scheduling policies called MSF-Quick Swap (MSFQ) that
performs well. MSFQ reduces the variability of job waiting times by
periodically granting priority to other jobs in the system. We provide both
stability results and an analysis of mean response time under MSFQ to prove
that our policy dramatically outperforms MSF in the case where jobs request one
core or all the cores. In more complex cases, we evaluate MSFQ in simulation.
We show that, with some additional optimization, variants of the MSFQ policy
can greatly outperform MSF and FCFS on real-world multiserver job workloads.

</details>


### [35] [Non-Asymptotic Performance Analysis of DOA Estimation Based on Real-Valued Root-MUSIC](https://arxiv.org/abs/2509.01999)
*Junyang Liu,Weicheng Zhao,Qingping Wang,Xiangtian Meng,Maria Greco,Fulvio Gini*

Main category: cs.PF

TL;DR: 该论文对RV-root-MUSIC算法在非渐近条件下进行了系统性理论性能分析，解决了镜像根估计模糊问题，并结合CBF技术研究了噪声子空间扰动、真实根扰动和镜像根扰动特性，提供了参数优化的理论基础。


<details>
  <summary>Details</summary>
Motivation: RV-root-MUSIC算法在DOA估计中存在镜像根估计模糊问题，需要理论分析以优化实际应用中的参数选择，特别是在雷达和通信系统中。

Method: 通过基于共轭扩展方法的等效子空间和扰动等价性，研究了RV-root-MUSIC算法中的噪声子空间扰动、真实根扰动和镜像根扰动特性。

Result: 仿真结果表明，所建立的统计模型和广义扰动表达式具有正确性和有效性。

Conclusion: 研究结果为DOA估计的参数优化提供了坚实的理论基础，适用于雷达、通信网络和智能传感技术等领域。

Abstract: This paper presents a systematic theoretical performance analysis of the
Real-Valued root-MUSIC (RV-root-MUSIC) algorithm under non-asymptotic
conditions. However, RV-root-MUSIC suffers from the problem of estimation
ambiguity for the mirror roots, therefore the conventional beamforming (CBF)
technique is typically employed to filter out the mirror roots. Through the
equivalent subspace based on the conjugate extension method and the equivalence
of perturbations for both true roots and mirror roots , this paper provides a
comprehensive investigation of three critical aspects: noise subspace
perturbation, true root perturbation, and mirror root perturbation
characteristics in the RV-root-MUSIC algorithm. The statistical model is
established and the generalized expression of perturbation is further
developed. The simulation results show the correctness and validity of the
derived statistical characteristics. The results provide a solid theoretical
foundation for optimizing the parameter selection of DOA estimation in
practical applications, particularly in radar systems, communication networks,
and intelligent sensing technologies.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [36] [AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving](https://arxiv.org/abs/2509.00105)
*Shaoting Feng,Hanchen Li,Kuntai Du,Zhuohan Gu,Yuhan Liu,Jiayi Yao,Siddhant Ray,Samuel Shen,Yihua Cheng,Ganesh Ananthanarayanan,Junchen Jiang*

Main category: cs.OS

TL;DR: 该论文提出了一种动态调整压缩算法和速率的系统AdaptCache，以提高KV缓存的DRAM命中率并减少加载延迟，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM服务系统中KV缓存因规模过大导致的高延迟问题，特别是SSD加载速度慢的问题。

Method: 设计了一种动态损失压缩系统，根据KV缓存条目选择压缩算法、压缩率和设备放置位置，以优化DRAM命中率和延迟。

Result: 与静态压缩方法相比，AdaptCache在相同质量下延迟减少1.43-2.4倍，在相同延迟下质量提升6-55%。

Conclusion: 动态损失压缩是优化KV缓存存储和加载效率的有效方法。

Abstract: Large language model (LLM) applications often reuse previously processed
context, such as chat history and documents, which introduces significant
redundant computation. Existing LLM serving systems address such redundant
computation by storing the KV caches of processed context and loading the
corresponding KV cache when a new request reuses the context. Further, as these
LLM applications scale, the total size of KV caches becomes excessively large
and requires both DRAM and SSD for full storage.
  However, prior work that stores KV caches in DRAM and SSD suffers from high
loading delays, as most KV cache hits come from SSD, which is slow to load. To
increase the KV cache hit rate on DRAM, we identify lossy KV cache compression
as a promising approach. We design a lossy compression system that decides the
compression algorithm, compression rate and device placement for each KV cache
entry to maximise DRAM hits and minimise loading delay without significantly
degrading generation quality. Compared to various static compression baselines
across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at
the same quality and 6--55% quality improvements at the same delay.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [37] [VOTA: Parallelizing 6G-RAN Experimentation with Virtualized Over-The-Air Workloads](https://arxiv.org/abs/2509.00130)
*Chang Liu,T. D. Khoa Le,Rahul Saini,Kishor C. Joshi,George Exarchakos*

Main category: cs.NI

TL;DR: VOTA是一种开源、纯软件的测试床扩展方法，通过实时虚拟化和频率调谐最大化并行实验，同时控制干扰，提高测试床共享效率。


<details>
  <summary>Details</summary>
Motivation: 测试床共享在无线实验研究中普遍存在，但其主要缺点是实验不便，如延迟实验或容忍对实验保真度有害的计算和射频干扰。

Method: 提出了VOTA，利用实时虚拟化和频率调谐，控制干扰并最大化并行实验。

Result: 在两个干扰敏感的6G用例中展示了VOTA的能力，实现了类似专用的结果，同时增加了2.67倍的共享机会。

Conclusion: VOTA有效解决了测试床共享中的干扰问题，提高了实验效率。

Abstract: Testbed sharing, a practice in which different researchers concurrently
develop independent use cases on top of the same testbed, is ubiquitous in
wireless experimental research. Its key drawback is experimental inconvenience:
one must delay experiments or tolerate compute and RF interference that harms
experimental fidelity. In this paper, we propose \textbf{VOTA}, an open-source,
software-only testbed scaling method that leverages real-time virtualization
and frequency tuning to maximize parallel experiments while controlling
interference. In a demonstration of two interference-sensitive 6G use cases --
\textit{MIMO iDFT/DFT Offloading} and \textit{O-RAN DoS Attack} -- running
side-by-side on a 32-core host, we showcase VOTA capabilities:
\textbf{dedicated-like} results while allowing \textbf{2.67$\times$} more
sharing opportunities.

</details>


### [38] [Intelligent Spectrum Management in Satellite Communications](https://arxiv.org/abs/2509.00286)
*Rakshitha De Silva,Shiva Raj Pokhrel,Jonathan Kua,Sithamparanathan Kandeepan*

Main category: cs.NI

TL;DR: 本文探讨了认知卫星（CogSat）网络中动态频谱管理（DSM）的智能方法，解决了传统卫星频谱分配的局限性。通过AI/ML技术优化DSM，并分析了其在实际应用中的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着高带宽需求的增长和大型卫星星座的普及，传统的频谱分配方法难以满足需求，认知无线电（CR）和动态频谱管理（DSM）成为解决频谱稀缺问题的有效途径。

Method: 本文综述了智能DSM方法在卫星通信（SatCom）中的应用，结合AI/ML技术优化频谱管理，并讨论了相关法规与标准化的挑战。

Result: 研究发现，智能DSM技术能够显著提升卫星网络的频谱利用率和性能，但仍需解决法规、网络架构和系统优化等方面的挑战。

Conclusion: 认知卫星网络结合智能DSM技术为全球连接提供了可持续和可扩展的解决方案，未来需进一步研究法规框架和技术优化。

Abstract: Satellite Communication (SatCom) networks represent a fundamental pillar in
modern global connectivity, facilitating reliable service and extensive
coverage across a plethora of applications. The expanding demand for
high-bandwidth services and the proliferation of mega satellite constellations
highlight the limitations of traditional exclusive satellite spectrum
allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite
(CogSat) networks through Dynamic Spectrum Management (DSM), which enables the
dynamic adaptability of radio equipment to environmental conditions for optimal
performance, presents a promising solution for the emerging spectrum scarcity.
In this survey, we explore the adaptation of intelligent DSM methodologies to
SatCom, leveraging satellite network integrations. We discuss contributions and
hurdles in regulations and standardizations in realizing intelligent DSM in
SatCom, and deep dive into DSM techniques, which enable CogSat networks.
Furthermore, we extensively evaluate and categorize state-of-the-art Artificial
Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while
exploring operational resilience and robustness of such integrations. In
addition, performance evaluation metrics critical for adaptive resource
management and system optimization in CogSat networks are thoroughly
investigated. This survey also identifies open challenges and outlines future
research directions in regulatory frameworks, network architectures, and
intelligent spectrum management, paving the way for sustainable and scalable
SatCom networks for enhanced global connectivity.

</details>


### [39] [SpliDT: Partitioned Decision Trees for Scalable Stateful Inference at Line Rate](https://arxiv.org/abs/2509.00397)
*Murayyiam Parvez,Annus Zulfiqar,Roman Beltiukov,Shir Landau Feibish,Walter Willinger,Arpit Gupta,Muhammad Shahbaz*

Main category: cs.NI

TL;DR: SLPIDT是一种在数据平面中实现分区推断的系统，通过滑动窗口和分区技术显著提升了决策树的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有数据平面中的决策树实现由于需要预先计算所有输入特征，限制了模型的准确性和扩展性。

Method: SPLIDT采用分区推断和滑动窗口技术，结合自定义训练和设计空间探索框架，优化特征分配和树分区。

Result: SPLIDT在多个实际数据集上实现了更高准确性，支持多达5倍的状态特征，同时保持低检测时间。

Conclusion: SPLIDT通过创新设计显著提升了决策树在数据平面中的性能和应用范围。

Abstract: Machine learning (ML) is increasingly being deployed in programmable data
planes (switches and SmartNICs) to enable real-time traffic analysis, security
monitoring, and in-network decision-making. Decision trees (DTs) are
particularly well-suited for these tasks due to their interpretability and
compatibility with data-plane architectures, i.e., match-action tables (MATs).
However, existing in-network DT implementations are constrained by the need to
compute all input features upfront, forcing models to rely on a small, fixed
set of features per flow. This significantly limits model accuracy and
scalability under stringent hardware resource constraints.
  We present SPLIDT, a system that rethinks DT deployment in the data plane by
enabling partitioned inference over sliding windows of packets. SPLIDT
introduces two key innovations: (1) it assigns distinct, variable feature sets
to individual sub-trees of a DT, grouped into partitions, and (2) it leverages
an in-band control channel (via recirculation) to reuse data-plane resources
(both stateful registers and match keys) across partitions at line rate. These
insights allow SPLIDT to scale the number of stateful features a model can use
without exceeding hardware limits. To support this architecture, SPLIDT
incorporates a custom training and design-space exploration (DSE) framework
that jointly optimizes feature allocation, tree partitioning, and DT model
depth. Evaluation across multiple real-world datasets shows that SPLIDT
achieves higher accuracy while supporting up to 5x more stateful features than
prior approaches (e.g., NetBeacon and Leo). It maintains the same low
time-to-detection (TTD) as these systems, while scaling to millions of flows
with minimal recirculation overhead (<0.05%).

</details>


### [40] [Interference Between FM Cell Sites and CDMA Cell Sites](https://arxiv.org/abs/2509.00567)
*P. Kumar*

Main category: cs.NI

TL;DR: 本文探讨了CDMA基站与FM基站之间的干扰问题，分析了干扰类型和观察结果。


<details>
  <summary>Details</summary>
Motivation: 解决电信领域中的FM与CDMA基站干扰问题，以提高通信质量。

Method: 分析干扰类型和干扰过程中的各种现象。

Result: 总结出干扰的类型和相关观察结果。

Conclusion: 干扰问题是电信领域的重要挑战，需进一步研究解决方案。

Abstract: Interference is the major problem now days in telecommunication sector. One
type of interference which is very common now days is FM Cell sites
interference between CDMA Cell sites. Which are the types of interference and
various observations during this interference is discussed below in this paper.

</details>


### [41] [SmartFLow: A Communication-Efficient SDN Framework for Cross-Silo Federated Learning](https://arxiv.org/abs/2509.00603)
*Osama Abu Hamdan,Hao Che,Engin Arslan,Md Arifuzzaman*

Main category: cs.NI

TL;DR: SmartFLow 是一种基于 SDN 的框架，旨在提升跨组织联邦学习中的通信效率，通过动态调整路由路径减少拥塞，实验显示其参数同步时间比最短路径路由减少 47%。


<details>
  <summary>Details</summary>
Motivation: 跨组织联邦学习中，传统路由方法无法有效减少拥塞，导致通信延迟增加和训练时间延长，而 SDN 提供了集中化、可编程的网络资源控制方式，为解决问题提供了可能。

Method: 提出了 SmartFLow 框架，利用 SDN 动态调整路由路径，以应对网络条件变化，从而降低拥塞并提升同步效率。

Result: 实验表明，SmartFLow 将参数同步时间缩短了 47%（相比最短路径路由）和 41%（相比容量感知路由），同时计算开销极小，并能扩展到 50 个客户端的网络中。

Conclusion: SmartFLow 在提升联邦学习通信效率和同步速度方面表现出色，具备实际部署的可行性和扩展性。

Abstract: Cross-silo Federated Learning (FL) enables multiple institutions to
collaboratively train machine learning models while preserving data privacy. In
such settings, clients repeatedly exchange model weights with a central server,
making the overall training time highly sensitive to network performance.
However, conventional routing methods often fail to prevent congestion, leading
to increased communication latency and prolonged training. Software-Defined
Networking (SDN), which provides centralized and programmable control over
network resources, offers a promising way to address this limitation. To this
end, we propose SmartFLow, an SDN-based framework designed to enhance
communication efficiency in cross-silo FL. SmartFLow dynamically adjusts
routing paths in response to changing network conditions, thereby reducing
congestion and improving synchronization efficiency. Experimental results show
that SmartFLow decreases parameter synchronization time by up to 47% compared
to shortest-path routing and 41% compared to capacity-aware routing.
Furthermore, it achieves these gains with minimal computational overhead and
scales effectively to networks of up to 50 clients, demonstrating its
practicality for real-world FL deployments.

</details>


### [42] [FLEET: A Federated Learning Emulation and Evaluation Testbed for Holistic Research](https://arxiv.org/abs/2509.00621)
*Osama Abu Hamdan,Hao Che,Engin Arslan,Md Arifuzzaman*

Main category: cs.NI

TL;DR: FLEET是一个用于评估联邦学习（FL）算法的测试平台，解决了现有工具无法模拟真实网络条件的问题，支持多样化的网络环境和机器学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习评估工具常忽略实际网络条件，导致算法理论与实际性能之间存在差距，需要更全面的测试平台。

Method: 开发FLEET平台，整合高保真网络模拟器和通用学习组件，支持自定义网络拓扑和动态流量生成。

Result: FLEET能系统评估网络约束对FL算法的影响，提供可复现的实验环境。

Conclusion: FLEET填补了FL理论与实际网络条件间的鸿沟，促进联邦学习系统的全面评估。

Abstract: Federated Learning (FL) presents a robust paradigm for privacy-preserving,
decentralized machine learning. However, a significant gap persists between the
theoretical design of FL algorithms and their practical performance, largely
because existing evaluation tools often fail to model realistic operational
conditions. Many testbeds oversimplify the critical dynamics among algorithmic
efficiency, client-level heterogeneity, and continuously evolving network
infrastructure. To address this challenge, we introduce the Federated Learning
Emulation and Evaluation Testbed (FLEET). This comprehensive platform provides
a scalable and configurable environment by integrating a versatile,
framework-agnostic learning component with a high-fidelity network emulator.
FLEET supports diverse machine learning frameworks, customizable real-world
network topologies, and dynamic background traffic generation. The testbed
collects holistic metrics that correlate algorithmic outcomes with detailed
network statistics. By unifying the entire experiment configuration, FLEET
enables researchers to systematically investigate how network constraints, such
as limited bandwidth, high latency, and packet loss, affect the convergence and
efficiency of FL algorithms. This work provides the research community with a
robust tool to bridge the gap between algorithmic theory and real-world network
conditions, promoting the holistic and reproducible evaluation of federated
learning systems.

</details>


### [43] [Unsupervised Dataset Cleaning Framework for Encrypted Traffic Classification](https://arxiv.org/abs/2509.00701)
*Kun Qiu,Ying Wang,Baoqian Li,Wenjun Zhu*

Main category: cs.NI

TL;DR: 论文提出了一种无监督框架，用于自动清理加密移动流量数据，替代传统耗时的手动清理，实验结果显示其分类准确率仅降低2%~2.5%。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备加密流量的普及，传统深度包检测方法失效，而机器学习方法依赖的数据清理过程成本高昂且耗时，亟需自动化解决方案。

Method: 提出了一种无监督框架，自动识别并清理无关流量（如无关协议、后台活动等），无需人工逐包检查。

Result: 在真实数据集上的评估显示，该框架的自动清理仅使分类准确率降低2%~2.5%，与手动清理效果接近。

Conclusion: 该框架为基于机器学习的加密流量分类提供了高效且有效的预处理步骤。

Abstract: Traffic classification, a technique for assigning network flows to predefined
categories, has been widely deployed in enterprise and carrier networks. With
the massive adoption of mobile devices, encryption is increasingly used in
mobile applications to address privacy concerns. Consequently, traditional
methods such as Deep Packet Inspection (DPI) fail to distinguish encrypted
traffic. To tackle this challenge, Artificial Intelligence (AI), in particular
Machine Learning (ML), has emerged as a promising solution for encrypted
traffic classification. A crucial prerequisite for any ML-based approach is
traffic data cleaning, which removes flows that are not useful for training
(e.g., irrelevant protocols, background activity, control-plane messages, and
long-lived sessions). Existing cleaning solutions depend on manual inspection
of every captured packet, making the process both costly and time-consuming. In
this poster, we present an unsupervised framework that automatically cleans
encrypted mobile traffic. Evaluation on real-world datasets shows that our
framework incurs only a 2%~2.5% reduction in classification accuracy compared
with manual cleaning. These results demonstrate that our method offers an
efficient and effective preprocessing step for ML-based encrypted traffic
classification.

</details>


### [44] [ReWeave: Traffic Engineering with Robust Path Weaving for Localized Link Failure Recover](https://arxiv.org/abs/2509.00708)
*Jingyi Guan,Kun Qiu,Jin Zhao*

Main category: cs.NI

TL;DR: ReWeave是一种高效、可扩展的链路级流量工程方案，通过局部路由器和紧凑的备份路径实现快速故障恢复，显著提升了性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ISP网络中链路故障频繁，现有TE方案或因静态路径导致性能下降，或因预计算备份路径引入高开销。需要一种低开销、高效的故障恢复机制。

Method: ReWeave为每个链路配备紧凑的仅相邻备份路径，利用SRv6动态绕行，无需控制器干预或全路径重计算。

Result: 在大规模骨干网络中，ReWeave平均最大链路利用率降低10.5%~20.1%，最坏情况降低29.5%~40.9%，且90%的故障情况下丢包率接近Flexile。

Conclusion: ReWeave在性能和开销上优于现有方案，适合实际网络部署。

Abstract: Link failures occur frequently in Internet Service Provider (ISP) networks
and pose significant challenges for Traffic Engineering (TE). Existing TE
schemes either reroute traffic over vulnerable static paths, leading to
performance degradation, or precompute backup routes for a broad range of
failure scenarios, which introduces high overhead and limits scalability.
Hence, an effective failure recovery mechanism is required to offer sufficient
path diversity under constrained overhead, thereby ensuring robust and
performant network operation. This paper presents ReWeave, a scalable and
efficient link-level TE scheme that enables localized rerouting by equipping
each link with a compact set of adjacent-only backup paths. Upon detecting a
failure, only the routers at both ends of the failed link reroute traffic
dynamically using SRv6-based detours, without controller intervention or
full-path recomputation. Evaluation results on large-scale backbone networks
demonstrate that ReWeave outperforms existing TE schemes in link failure
scenarios. Compared to HARP, the state-of-the-art failure recovery scheme based
on centralized control and dynamic traffic reallocation, our approach reduces
the average maximum link utilization by 10.5%~20.1%, and lowers the worst-case
utilization by 29.5%~40.9%. When compared with Flexile, a protection-based
scheme that precomputes routes for multi-failure scenarios, ReWeave achieves a
similarly low packet loss rate in 90% of failure cases, while maintaining a
response speed comparable to the fastest router-based local rerouting schemes.

</details>


### [45] [FCT O-RAN: Design and Deployment of a Multi-Vendor End-to-End Private 5G Testbed](https://arxiv.org/abs/2509.01891)
*Amogh PC,Nagamuthu Vignesh,Pei Yiyang,Neelakantam Venkatarayalu,Pedro Henrique Amorim Rezende,Shyam Babu Mahato,Sumei Sun*

Main category: cs.NI

TL;DR: 摘要探讨了5G网络向软件定义和云原生架构的转型，介绍了新加坡FCP项目和FCT实验室在5G技术部署中的应用，以及通过数字孪生和性能评估优化的结果。


<details>
  <summary>Details</summary>
Motivation: 5G网络需要向软件定义和智能化架构转型，以实现更高的性能和灵活性，FCP项目旨在推动新加坡在通信技术领域的发展。

Method: 采用多厂商私有5G平台（FCT O-RAN），利用数字孪生技术优化部署，并通过QualiPoc智能手机测量网络性能。

Result: 室内外环境均实现了优化的带宽分配和高吞吐量（室内下行713 Mbps，上行66 Mbps；室外下行371 Mbps，上行55 Mbps）。

Conclusion: FCT实验室的5G部署展示了云原生和多厂商架构的实际应用潜力，为行业提供了可行的技术参考。

Abstract: The transformation of 5G networks into software-defined, agile, intelligent
and programmable architectures necessitates a paradigm shift in deployment
strategies. To deliver superior performance and surpass traditional systems,
public and private 5G networks must adopt software-centric cloud native
frameworks that enable flexibility through tailored configurations and
optimized deployment approaches. In Singapore, the Infocomm Media Development
Authority (IMDA) and the National Research Foundation Singapore (NRF) launched
the Future Communications Research and Development Programme (FCP) to advance
the nation's communications and connectivity landscape. At the core of this
initiative is the Future Communications Translation Lab (FCT) at the Singapore
Institute of Technology (SIT), which focuses on advancing 5G technologies to
higher readiness levels, facilitating their adoption across various industries.
A key component is the deployment of FCT O-RAN, a state-of-the-art multi-vendor
private 5G platform. The setup includes a 5G core network powered by Microsoft
Affirmed and ENEA, O-RAN Centralized and Distributed Units from Radisys. Indoor
Remote Units are deployed with Foxconn, while outdoor RUs are deployed with
Benetel. To optimize the deployment of remote units, a digital twin was created
using Wireless InSite, and performance evaluations were conducted for both the
digital twin and the private 5G deployment. Smartphones equipped with QualiPoc
were used to measure network performance. The testbed demonstrated effective
performance with optimized bandwidth allocations for both indoor and outdoor
environments. In the indoor setup, utilizing 50 MHz of bandwidth, a downlink
throughput of 713 Mbps and an uplink throughput of 66 Mbps were achieved.
Meanwhile, the outdoor setup, utilizing 40 MHz of bandwidth, achieved a
downlink throughput of 371 Mbps and an uplink throughput of 55 Mbps.

</details>


### [46] [A Modular and Scalable Simulator for Connected-UAVs Communication in 5G Networks](https://arxiv.org/abs/2509.00868)
*Yong Su,Yiyi Chen,Shenghong Yi,Hui Feng,Yuedong Xu,Wang Xiang,Bo Hu*

Main category: cs.NI

TL;DR: 开发了一个模块化、可扩展的仿真平台，用于研究无人机通信中的频繁切换和传统传输协议效率问题。


<details>
  <summary>Details</summary>
Motivation: 无人机通信系统面临频繁切换和传统传输协议效率低的挑战，需要专门的仿真平台进行研究。

Method: 利用MATLAB无线通信研究生态，开发支持5G NR节点部署、自定义无人机移动模型和多网络接口扩展的平台。

Result: 平台支持多种传输协议和切换管理模块，可用于评估不同协议和切换策略的性能。

Conclusion: 该平台可作为蜂窝连接无人机系统高级传输策略开发和评估的测试平台。

Abstract: Cellular-connected UAV systems have enabled a wide range of low-altitude
aerial services. However, these systems still face many challenges, such as
frequent handovers and the inefficiency of traditional transport protocols. To
better study these issues, we develop a modular and scalable simulation
platform specifically designed for UAVs communication leveraging the research
ecology in wireless communication of MATLAB. The platform supports flexible 5G
NR node deployment, customizable UAVs mobility models, and
multi-network-interface extensions. It also supports multiple transport
protocols including TCP, UDP, QUIC, etc., allowing to investigate how different
transport protocols affect UAVs communication performance.In addition, the
platform includes a handover management module, enabling the evaluation of both
traditional and learning-based handover strategies. Our platform can serve as a
testbed for the development and evaluation of advanced transmission strategies
in cellular-connected UAV systems.

</details>


### [47] [Efficient Multichannel Rendezvous Algorithms without Global Channel Enumeration](https://arxiv.org/abs/2509.00885)
*Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: 提出了一种基于局部敏感哈希（LSH）的低复杂度多通道会合算法，适用于无全局通道枚举系统的场景，性能与现有算法相当。


<details>
  <summary>Details</summary>
Motivation: 解决在多通道会合问题中缺乏全局通道枚举系统时的挑战，提升物联网设备发现效率。

Method: 利用LSH和一致性哈希技术，设计了LC-LSH和LC-LSH4算法，分别用于同步和异步场景，并通过嵌入多集增强模块时钟和准随机技术确保MTTR有界。

Result: 模拟实验显示，算法性能与现有LSH算法相当，且复杂度更低。

Conclusion: 提出的算法在无全局通道枚举系统下仍能高效解决多通道会合问题，适用于物联网应用。

Abstract: The multichannel rendezvous problem (MRP) is a critical challenge for
neighbor discovery in IoT applications, requiring two users to find each other
by hopping among available channels over time. This paper addresses the MRP in
scenarios where a global channel enumeration system is unavailable. To tackle
this challenge, we propose a suite of low-complexity multichannel rendezvous
algorithms based on locality-sensitive hashing (LSH), tailored for environments
where channel labels are unique L-bit identifiers rather than globally
coordinated indices. Inspired by consistent hashing techniques in distributed
systems, we develop the LC-LSH and LC-LSH4 algorithms for synchronous and
asynchronous settings, respectively. These algorithms significantly reduce
implementation complexity while maintaining expected time-to-rendezvous (ETTR)
performance comparable to state-of-the-art methods that require global channel
enumeration. To ensure bounded maximum time-to-rendezvous (MTTR) in the
asynchronous setting, we further introduce the ASYM-LC-LSH4 and QR-LC-LSH4
algorithms by embedding multiset-enhanced modular clock and quasi-random
techniques into our framework. Extensive simulations demonstrate that the
proposed algorithms achieve performance comparable to state-of-the-art LSH
algorithms in both synchronous and asynchronous settings, even without a global
channel enumeration system.

</details>


### [48] [FlexNGIA 2.0: Redesigning the Internet with Agentic AI - Protocols, Services, and Traffic Engineering Designed, Deployed, and Managed by AI](https://arxiv.org/abs/2509.02124)
*Mohamed Faten Zhani,Younes Korbi,Yamen Mkadem*

Main category: cs.NI

TL;DR: FlexNGIA 2.0是一种基于LLM AI驱动的互联网架构，通过AI代理自主管理和优化网络协议、功能及资源分配，提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 随着沉浸式通信需求的增长和网络软化的进步，重新设计未来互联网架构的需求日益迫切。

Method: 提出FlexNGIA 2.0架构，利用LLM AI代理动态设计、部署和优化网络协议及资源分配策略。

Result: 实验证明了这些代理在自动化设计、部署和性能评估方面的能力。

Conclusion: FlexNGIA 2.0为AI驱动的自主网络奠定了基础，但仍需解决关键研究挑战。

Abstract: The escalating demands of immersive communications, alongside advances in
network softwarization and AI-driven cognition and generative reasoning, create
a pivotal opportunity to rethink and reshape the future Internet. In this
context, we introduce in this paper, FlexNGIA 2.0, an Agentic AI-driven
Internet architecture that leverages LLM-based AI agents to autonomously
orchestrate, configure, and evolve the network. These agents can, at runtime,
perceive, reason, coordinate among themselves to dynamically design, implement,
deploy, and adapt communication protocols, Service Function Chains (SFCs),
network functions, resource allocation strategies, congestion control, and
traffic engineering schemes, thereby ensuring optimal performance, reliability,
and efficiency under evolving conditions.
  The paper first outlines the overall architecture of FlexNGIA 2.0 and its
constituent LLM-Based AI agents. For each agent, we detail its design,
implementation, inputs and outputs, prompt structures, interactions with tools
and other agents, followed by preliminary proof-of-concept experiments
demonstrating its operation and potential. The results clearly highlight the
ability of these LLM-based AI agents to automate the design, the
implementation, the deployment, and the performance evaluation of transport
protocols, service function chains, network functions, congestion control
schemes, and resource allocation strategies.
  FlexNGIA 2.0 paves the way for a new class of Agentic AI-Driven networks,
where fully cognitive, self-evolving AI agents can autonomously design,
implement, adapt and optimize the network's protocols, algorithms, and
behaviors to efficiently operate across complex, dynamic, and heterogeneous
environments. To bring this vision to reality, we also identify key research
challenges toward achieving fully autonomous, adaptive, and agentic AI-driven
networks.

</details>


### [49] [BUBBLE-BLUE a multihop private network based on Bluetooth](https://arxiv.org/abs/2509.00967)
*Nadjib Achir,Philippe Jacquet*

Main category: cs.NI

TL;DR: BB项目利用智能手机蓝牙技术构建私有网络，不依赖运营商，基于动态CDS路由策略。


<details>
  <summary>Details</summary>
Motivation: 旨在实现用户通过智能手机蓝牙自主通信，避免依赖运营商网络。

Method: 采用动态Connected Dominant Sets（CDS）路由策略。

Result: 模拟结果显示其路由性能良好。

Conclusion: BB项目展示了蓝牙技术在私有通信网络中的潜力。

Abstract: The BUBBLE-BLUE (BB) project aims to create private Bluetooth bubbles on top
of smartphones and to create a kind of terrestrial STARLINK network based on
users smartphones.. In each private bubble, participants will be able to
communicate autonomously, without recourse to private operator networks,
neither data nor cellular, relying solely on the Bluetooth technology of
smartphones. The routing strategy is based on dynamic Connected Dominant Sets
(CDS). We present the specific features of a BB network as well as some
simulation results on their routing performance.

</details>


### [50] [Quantum-based QoE Optimization in Advanced Cellular Networks: Integration and Cloud Gaming Use Case](https://arxiv.org/abs/2509.01008)
*Fatma Chaouech,Javier Villegas,António Pereira,Carlos Baena,Sergio Fortes,Raquel Barco,Dominic Gribben,Mohammad Dib,Alba Villarino,Aser Cortines,Román Orús*

Main category: cs.NI

TL;DR: 该研究探讨了量子机器学习（QML）和量子启发（QI）技术在5G及以上网络中优化端到端服务的应用，结果表明QML在估计准确性和速度上优于传统机器学习。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用QML和QI技术优化电信网络中的服务质量（QoE），尤其是在5G及更高网络环境下。

Method: 采用混合框架结合量子与经典计算，通过QML和QI优化网络配置；具体实现针对云游戏服务的QoE优化。

Result: QML模型在估计准确性上与传统ML相当或更优，且推理和加载时间更短；高维数据下表现更优。

Conclusion: QML在网络优化中展现出潜力，但数据可用性和量子与经典ML的集成复杂性仍需进一步研究。

Abstract: This work explores the integration of Quantum Machine Learning (QML) and
Quantum-Inspired (QI) techniques for optimizing end-to-end (E2E) network
services in telecommunication systems, particularly focusing on 5G networks and
beyond. The application of QML and QI algorithms is investigated, comparing
their performance with classical Machine Learning (ML) approaches. The present
study employs a hybrid framework combining quantum and classical computing
leveraging the strengths of QML and QI, without the penalty of quantum hardware
availability. This is particularized for the optimization of the Quality of
Experience (QoE) over cellular networks. The framework comprises an estimator
for obtaining the expected QoE based on user metrics, service settings, and
cell configuration, and an optimizer that uses the estimation to choose the
best cell and service configuration. Although the approach is applicable to any
QoE-based network management, its implementation is particularized for the
optimization of network configurations for Cloud Gaming services. Then, it is
evaluated via performance metrics such as accuracy and model loading and
inference times for the estimator, and time to solution and solution score for
the optimizer. The results indicate that QML models achieve similar or superior
accuracy to classical ML models for estimation, while decreasing inference and
loading times. Furthermore, potential for better performance is observed for
higher-dimensional data, highlighting promising results for higher complexity
problems. Thus, the results demonstrate the promising potential of QML in
advancing network optimization, although challenges related to data
availability and integration complexities between quantum and classical ML are
identified as future research lines.

</details>


### [51] [Modeling and Analysis of Coexistence Between MLO NSTR-based Wi-Fi 7 and Legacy Wi-Fi](https://arxiv.org/abs/2509.01201)
*Suhwan Jung,Seokwoo Choi,Youngkeun Yoon,Ho-kyung Son,Hyoil Kim*

Main category: cs.NI

TL;DR: 该论文研究了Wi-Fi 7的多链路操作（MLO）性能，提出了一种基于Markov链的模型来分析其共存性能，并通过仿真验证了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi 7的MLO技术虽能提升性能，但与旧版Wi-Fi设备的共存性能尚未充分研究，需要开发标准化的分析框架。

Method: 提出了基于Markov链的模型，分别针对AP和非AP多链路设备（MLD），推导了传输和碰撞概率，并开发了ns-3仿真器验证模型。

Result: 模型在饱和流量条件下准确预测了设备吞吐量，揭示了不同设备类型（AP MLD、非AP MLD、旧版设备）的共存动态。

Conclusion: 提出的分析模型能有效评估Wi-Fi 7 MLO与旧版Wi-Fi的共存性能，为实际应用提供了理论支持。

Abstract: Wi-Fi 7 introduces Multi-link operation (MLO) to enhance throughput and
latency performance compared to legacy Wi-Fi standards. MLO enables
simultaneous transmission and reception through multiple links, departing from
conventional single-link operations (SLO). To fully exploit MLO's potential, it
is essential to investigate Wi-Fi 7's coexistence performance with legacy Wi-Fi
devices. Existing approaches, however, have overlooked some crucial aspects of
MLO, necessitating the development of a standards-compliant analytical
framework to model the actual channel access mechanism of MLO. Therefore, this
paper tries to fill the gap by proposing a set of novel Markov chains (MC) to
accurately model the MLO operation aligned with multi-link backoff behaviors
specified by the standard. Specifically, we design two separate MCs for AP and
non-AP multi-link devices (MLD) respectively, based on which transmit and
collision probabilities are derived under the saturated traffic condition.
Then, we also derive closed-form expressions for the throughput of various
device types in the coexistence scenario between Wi-Fi 7 and legacy Wi-Fi,
including AP MLD, non- AP MLD, and legacy devices. To validate the accuracy of
our proposed models, we developed an ns-3 based simulator by implementing both
STR(simultaneous transmission and reception) and NSTR(non-STR) based MLO
operations. Our ns-3 based extensive simulations have demonstrated that the
proposed analytic model provides accurate estimates on the per device
throughput performance, while also revealing the dynamics of inter-WLAN
coexistence scenarios.

</details>


### [52] [A Real-time Data Collection Approach for 6G AI-native Networks](https://arxiv.org/abs/2509.01276)
*He Shiwen,Dong Haolei,Wang Liangpeng,An Zhenyu*

Main category: cs.NI

TL;DR: 该论文提出了一种在6G网络中实时采集和处理数据的方法，通过部署数据探针和构建数据支持系统，解决了现有研究中实时数据获取的挑战，并通过案例展示了系统的功能。


<details>
  <summary>Details</summary>
Motivation: 在6G网络中，AI的集成需要高质量数据支持，但实时数据获取和处理仍存在挑战。

Method: 论文提出了一种并行于比特流处理的综合数据采集方法，并部署数据探针捕获实时网络状态数据，同时构建数据支持系统集成异构数据。

Result: 在Kubernetes上搭建了6G通信测试平台，并通过网络流量预测案例验证了系统的功能。

Conclusion: 该方法为AI原生网络的实现提供了实时数据支持，展现了其在6G网络中的潜力。

Abstract: During the development of the Sixth Generation (6G) networks, the integration
of Artificial Intelligence (AI) into network systems has become a focal point,
leading to the concept of AI-native networks. High quality data is essential
for developing such networks. Although some studies have explored data
collection and analysis in 6G networks, significant challenges remain,
particularly in real-time data acquisition and processing. This paper proposes
a comprehensive data collection method that operates in parallel with bitstream
processing for wireless communication networks. By deploying data probes, the
system captures real-time network and system status data in software-defined
wireless communication networks. Furthermore, a data support system is
implemented to integrate heterogeneous data and provide automatic support for
AI model training and decision making. Finally, a 6G communication testbed
using OpenAirInterface5G and Open5GS is built on Kubernetes, as well as the
system's functionality is demonstrated via a network traffic prediction case
study.

</details>


### [53] [Multi-AAV-enabled Distributed Beamforming in Low-Altitude Wireless Networking for AoI-Sensitive IoT Data Forwarding](https://arxiv.org/abs/2509.01427)
*Zifan Lang,Guixia Liu,Jiahui Li,Geng Sun,Zemin Sun,Jiacheng Wang,Dusit Niyato,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 本文提出了一种基于分布式波束成形的AAV辅助转发系统，以优化物联网中的信息时效性（AoI）和AAV能耗。


<details>
  <summary>Details</summary>
Motivation: 传统AAV辅助数据传输受限于通信覆盖范围和周期性返程飞行，导致AoI增加和服务可靠性下降。

Method: 提出使用分布式波束成形技术协同AAV收集和中继数据，并结合深度强化学习优化AAV轨迹和通信调度。

Result: 仿真结果表明，提出的SAC-TLS算法在收敛性、平均AoI和AAV能耗方面优于基线算法。

Conclusion: 该方法有效提升了信息时效性并降低了AAV能耗，适用于物联网网络。

Abstract: With the rapid development of low-altitude wireless networking, autonomous
aerial vehicles (AAVs) have emerged as critical enablers for timely and
reliable data delivery, particularly in remote or underserved areas. In this
context, the age of information (AoI) has emerged as a critical performance
metric for evaluating the freshness and timeliness of transmitted information
in Internet of things (IoT) networks. However, conventional AAV-assisted data
transmission is fundamentally limited by finite communication coverage ranges,
which requires periodic return flights for data relay operations. This
propulsion-repositioning cycle inevitably introduces latency spikes that raise
the AoI while degrading service reliability. To address these challenges, this
paper proposes a AAV-assisted forwarding system based on distributed
beamforming to enhance the AoI in IoT. Specifically, AAVs collaborate via
distributed beamforming to collect and relay data between the sensor nodes and
remote base station. Then, we formulate an optimization problem to minimize the
AoI and AAV energy consumption, by jointly optimizing the AAV trajectories and
communication schedules. Due to the non-convex nature of the problem and its
pronounced temporal variability, we introduce a deep reinforcement learning
solution that incorporates temporal sequence input, layer normalization gated
recurrent unit, and a squeeze-and-excitation block to capture long-term
dependencies, thereby improving decision-making stability and accuracy, and
reducing computational complexity. Simulation results demonstrate that the
proposed SAC-TLS algorithm outperforms baseline algorithms in terms of
convergence, time average AoI, and energy consumption of AAVs.

</details>


### [54] [A QoS Framework for Service Provision in Multi-Infrastructure-Sharing Networks](https://arxiv.org/abs/2509.01694)
*Quang Minh Nguyen,Eytan Modiano*

Main category: cs.NI

TL;DR: 提出了一个在共享基础设施网络中提供QoS保证的资源调配框架，使用改进的Dirft-plus-Penalty (MDP)策略，实现了长期稳定性和短期的概率性服务保证。


<details>
  <summary>Details</summary>
Motivation: 为了解决共享基础设施网络中资源调配的QoS保证问题，特别是在吞吐量和延迟方面的概率性服务需求。

Method: 使用改进的Dirft-plus-Penalty (MDP)策略，结合线性化的上置信界，确保长期稳定性和短期服务保证。

Result: MDP策略实现了平均速率稳定性和最优性差距，且随着服务保证的时间帧增大而消失。

Conclusion: 通过实证模拟验证了理论，表明该算法在多基础设施网络中处理QoS时表现出色。

Abstract: We propose a framework for resource provisioning with QoS guarantees in
shared infrastructure networks. Our novel framework provides tunable
probabilistic service guarantees for throughput and delay. Key to our approach
is a Modified Dirft-plus-Penalty (MDP) policy that ensures long-term stability
while capturing short-term probabilistic service guarantees using linearized
upper-confidence bounds. We characterize the feasible region of service
guarantees and show that our MDP procedure achieves mean rate stability and an
optimality gap that vanishes with the frame size over which service guarantees
are provided. Finally, empirical simulations validate our theory and
demonstrate the favorable performance of our algorithm in handling QoS in
multi-infrastructure networks.

</details>


### [55] [Adaptive AI Model Partitioning over 5G Networks](https://arxiv.org/abs/2509.01906)
*Tam Thanh Nguyen,Tuan Van Ngo,Long Thanh Le,Yong Hao Pua,Mao Van Ngo,Binbin Chen,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 论文提出了一种基于实时AI估计的自适应DNN分割方案，用于优化移动设备与边缘服务器之间的计算任务分配，以平衡延迟、能耗和隐私。


<details>
  <summary>Details</summary>
Motivation: 移动设备运行完整DNN模型能耗高，而全卸载到云端或边缘服务器则存在隐私和带宽问题。固定分割方案在时变5G信道中表现不佳。

Method: 开发了一种自适应DNN分割方案，基于实时AI吞吐量估计，动态调整模型分割点。

Result: 在多种场景下验证了该方案在端到端延迟、能耗和隐私方面的显著性能提升。

Conclusion: 自适应分割方案能够有效平衡性能与隐私需求，优于固定分割策略。

Abstract: Mobile devices increasingly rely on deep neural networks (DNNs) for complex
inference tasks, but running entire models locally drains the device battery
quickly. Offloading computation entirely to cloud or edge servers reduces
processing load at devices but poses privacy risks and can incur high network
bandwidth consumption and long delays. Split computing (SC) mitigates these
challenges by partitioning DNNs between user equipment (UE) and edge servers.
However, 5G wireless channels are time-varying and a fixed splitting scheme can
lead to sub-optimal solutions. This paper addresses the limitations of fixed
model partitioning in privacy-focused image processing and explores trade-offs
in key performance metrics, including end-to-end (E2E) latency, energy
consumption, and privacy, by developing an adaptive ML partitioning scheme
based on real-time AI-powered throughput estimation. Evaluation in multiple
scenarios demonstrates significant performance gains of our scheme.

</details>


### [56] [AoI-based Scheduling of Correlated Sources for Timely Inference](https://arxiv.org/abs/2509.01926)
*Md Kamran Chowdhury Shisher,Vishrant Tripathi,Mung Chiang,Christopher G. Brinton*

Main category: cs.NI

TL;DR: 研究了多源实时远程推断系统，采用AoI度量数据新鲜度，提出信号无关调度策略，解决相关性带来的RMAB问题，并通过模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在多源实时远程推断系统中，通信资源有限导致数据不新鲜，需设计不需要目标值或源观测知识的调度策略以最小化推断误差。

Method: 提出基于AoI的信号无关调度策略，近似惩罚函数并分析误差界限，针对已知/未知惩罚函数场景分别设计策略，开发在线学习方法。

Result: 模拟结果显示所提策略能有效最小化推断误差，并在源数量增加时保持可扩展性。

Conclusion: 提出的方法在多源相关性系统中表现优越，为AoI和RMAB结合提供了新思路。

Abstract: We investigate a real-time remote inference system where multiple correlated
sources transmit observations over a communication channel to a receiver. The
receiver utilizes these observations to infer multiple time-varying targets.
Due to limited communication resources, the delivered observations may not be
fresh. To quantify data freshness, we employ the Age of Information (AoI)
metric. To minimize the inference error, we aim to design a signal-agnostic
scheduling policy that leverages AoI without requiring knowledge of the actual
target values or the source observations. This scheduling problem is a restless
multi-armed bandit (RMAB) problem with a non-separable penalty function. Unlike
traditional RMABs, the correlation among sources introduces a unique challenge:
the penalty function of each source depends on the AoI of other correlated
sources, preventing decomposition of the problem into multiple independent
Markov Decision Processes (MDPs), a key step in applying traditional RMAB
solutions. To address this, we propose a novel approach by approximating the
penalty function of each source and establish an analytical bound on the
approximation error. We then develop scheduling policies for two scenarios: (i)
full knowledge of the penalty functions and (ii) no knowledge of the penalty
functions. For the case of known penalty functions, we present an upper bound
on the optimality gap of our policy in the asymptotic regime. For the case of
unknown penalty functions and signal distributions, we develop an online
learning approach that utilizes bandit feedback to learn an online Maximum Gain
First (MGF) policy. Simulation results demonstrate the effectiveness of our
proposed policies in minimizing inference error and achieving scalability in
the number of sources.

</details>


### [57] [Federated Foundation Models in Harsh Wireless Environments: Prospects, Challenges, and Future Directions](https://arxiv.org/abs/2509.01957)
*Evan Chen,Seyyedali Hosseinalipour,Christopher G. Brinton,David J. Love*

Main category: cs.NI

TL;DR: 论文探讨了联邦基础模型（FFMs）在恶劣环境中部署的潜力，结合了基础模型的泛化能力和去中心化的联邦学习框架，以解决现有方法在资源有限、不稳定的环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）虽具有广泛的应用潜力，但在恶劣环境中的部署仍面临挑战，现有分布式学习方法（如联邦学习）因依赖稳定基础设施而表现不佳。

Method: 提出联邦基础模型（FFMs），结合基础模型的泛化能力和去中心化、通信感知的联邦学习框架，以适应恶劣环境中的需求。

Result: 论文分析了恶劣环境中的系统级约束，并探讨了通信设计、模型鲁棒性和能效个性化等方面的研究挑战。

Conclusion: FFMs为解决恶劣环境中的智能部署问题提供了新思路，但仍需进一步研究。

Abstract: Foundation models (FMs) have shown remarkable capabilities in generalized
intelligence, multimodal understanding, and adaptive learning across a wide
range of domains. However, their deployment in harsh or austere environments --
characterized by intermittent connectivity, limited computation, noisy data,
and dynamically changing network topologies -- remains an open challenge.
Existing distributed learning methods such as federated learning (FL) struggle
to adapt in such settings due to their reliance on stable infrastructure,
synchronized updates, and resource-intensive training. In this work, we explore
the potential of Federated Foundation Models (FFMs) as a promising paradigm to
address these limitations. By integrating the scalability and generalization
power of FMs with novel decentralized, communication-aware FL frameworks, we
aim to enable robust, energy-efficient, and adaptive intelligence in extreme
and adversarial conditions. We present a detailed breakdown of system-level
constraints in harsh environments, and discuss the open research challenges in
communication design, model robustness, and energy-efficient personalization
for these unique settings.

</details>


### [58] [Green Traffic Engineering for Satellite Networks Using Segment Routing Flexible Algorithm](https://arxiv.org/abs/2509.02149)
*Jintao Liang,Pablo G. Madoery,Chung-Horng Lung,Halim Yanikomeroglu,Gunes Karabulut Kurt*

Main category: cs.NI

TL;DR: 研究提出了一种基于SRv6 Flex-Algo的框架，用于低地球轨道（LEO）卫星网络的能效、可靠性和低延迟路由优化，通过MILP统一目标实现资源高效分配。


<details>
  <summary>Details</summary>
Motivation: 解决大规模LEO星座网络中同时优化能源消耗、确保高可靠性和满足低延迟需求的挑战。

Method: 采用SRv6 Flex-Algo框架，分为能效、高可靠性和低延迟三个逻辑切片，结合MILP统一优化目标，并通过SDN控制器实现流量调度。

Result: 在Telesat Lightspeed星座的仿真中，平均CPU使用率降低73%，突发流量下PDR保持在91%以上，关键流延迟减少18ms。

Conclusion: Flex-Algo作为一种基于切片的TE工具，在资源受限的卫星网络中表现出显著优势。

Abstract: Large-scale low-Earth-orbit (LEO) constellations demand routing that
simultaneously minimizes energy, guarantees delivery under congestion, and
meets latency requirements for time-critical flows. We present a segment
routing over IPv6 (SRv6) flexible algorithm (Flex-Algo) framework that consists
of three logical slices: an energy-efficient slice (Algo 130), a
high-reliability slice (Algo 129), and a latency-sensitive slice (Algo 128).
The framework provides a unified mixed-integer linear program (MILP) that
combines satellite CPU power, packet delivery rate (PDR), and end-to-end
latency into a single objective, allowing a lightweight software-defined
network (SDN) controller to steer traffic from the source node. Emulation of
Telesat's Lightspeed constellation shows that, compared with different routing
schemes, the proposed design reduces the average CPU usage by 73%, maintains a
PDR above 91% during traffic bursts, and decreases urgent flow delay by 18 ms
between Ottawa and Vancouver. The results confirm Flex-Algo's value as a
slice-based traffic engineering (TE) tool for resource-constrained satellite
networks.

</details>


### [59] [AI Agent Communication from Internet Architecture Perspective: Challenges and Opportunities](https://arxiv.org/abs/2509.02317)
*Chenguang Du,Chuyi Wang,Yihan Chao,Xiaohui Xie,Yong Cui*

Main category: cs.NI

TL;DR: 本文从互联网架构的角度首次系统分析了AI代理通信，提出了五个关键要素（可扩展性、安全性、实时性能、高性能和管理性），以指导可持续的多代理生态系统发展。


<details>
  <summary>Details</summary>
Motivation: AI代理的快速发展带来了通信需求的激增，但现有的框架和协议存在碎片化和互操作性问题，需要从系统化的角度指导可持续的生态系统发展。

Method: 从历史上最成功的全球分布式系统——互联网架构中提炼出五个关键要素（可扩展性、安全性、实时性能、高性能和管理性），并将其应用于AI代理通信的分析。

Result: 通过互联网架构的视角，揭示了开发稳健多代理生态系统的机遇和瓶颈。

Conclusion: 本文首次将互联网架构与AI代理通信联系起来，为可持续的AI代理通信生态系统发展提供了新的指导视角。

Abstract: The rapid development of AI agents leads to a surge in communication demands.
Alongside this rise, a variety of frameworks and protocols emerge. While these
efforts demonstrate the vitality of the field, they also highlight increasing
fragmentation, with redundant innovation and siloed designs hindering
cross-domain interoperability. These challenges underscore the need for a
systematic perspective to guide the development of scalable, secure, and
sustainable AI agent ecosystems. To address this need, this paper provides the
first systematic analysis of AI agent communication from the standpoint of
Internet architecture-the most successful global-scale distributed system in
history. Specifically, we distill decades of Internet evolution into five key
elements that are directly relevant to agent communication: scalability,
security, real-time performance, high performance, and manageability. We then
use these elements to examine both the opportunities and the bottlenecks in
developing robust multi-agent ecosystems. Overall, this paper bridges Internet
architecture and AI agent communication for the first time, providing a new
lens for guiding the sustainable growth of AI agent communication ecosystems.

</details>


### [60] [Towards Intelligent Battery Management via A Five-Tier Digital Twin Framework](https://arxiv.org/abs/2509.02366)
*Tianwen Zhu,Hao Wang,Zhiwei Cao,Jiarong Xi,Yonggang Wen*

Main category: cs.NI

TL;DR: 提出了一种五层数字孪生框架，用于智能电池管理，通过预测建模和自主操作优化电池生命周期管理。


<details>
  <summary>Details</summary>
Motivation: 传统反应式电池管理系统在健康预测和高级维护管理方面存在局限，增加了安全风险和经济成本。

Method: 采用五层数字孪生框架，结合贝叶斯优化校准的电化学模型和物理信息神经网络（PINN）进行健康状态预测。

Result: 电化学模型的电压和温度预测误差分别低于1.57%和0.39%，PINN的健康状态预测误差低于3%。

Conclusion: 该框架将电池管理系统提升为能够主动管理和自主优化的智能系统，提升了关键应用的安全性和可靠性。

Abstract: Battery management systems (BMSs) are critical to ensuring safety,
efficiency, and longevity across electronics, transportation, and energy
storage. However, with the rapid growth of lithium-ion batteries, conventional
reactive BMS approaches face limitations in health prediction and advanced
maintenance management, resulting in increased safety risks and economic costs.
To address these challenges, we propose a five-tier digital twin framework for
intelligent battery management. The framework spans geometric visualization,
predictive modeling, prescriptive optimization, and autonomous operation,
enabling full lifecycle optimization. In validation, an electrochemical model
calibrated via Bayesian optimization achieved strong alignment with measured
voltage and temperature, with Mean Absolute Percentage Errors (MAPE) below
1.57\% and 0.39\%. A Physics-Informed Neural Network (PINN) then combined data
and simulations to predict State of Health (SOH), attaining MAPE under 3\% with
quantified uncertainty. This framework elevates BMSs into intelligent systems
capable of proactive management and autonomous optimization, advancing safety
and reliability in critical applications.

</details>


### [61] [Tree algorithms for set reconciliation](https://arxiv.org/abs/2509.02373)
*Francisco Lázaro,Čedomir Stefanović*

Main category: cs.NI

TL;DR: 提出了一种名为EPSR的新型集合协调方案，通过分区策略和树算法改进通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统分区集合协调(PSR)中通信成本较高的问题，提出更高效的方法。

Method: 采用分区策略和树算法，设计增强分区集合协调(EPSR)方案，并通过事件驱动模拟器评估性能。

Result: EPSR将PSR的通信成本降低近一半，同时保持相同的时间复杂度。

Conclusion: EPSR在保持效率的同时显著降低了通信成本，适用于集合协调场景。

Abstract: In this work, a set reconciliation setting is considered in which two parties
have similar sets that they would like to reconcile. In particular, we focus on
a divide-and-conquer strategy known as partitioned set reconciliation (PSR), in
which the sets to be reconciled are successively partitioned until they contain
a number of differences below some predetermined value. Borrowing techniques
from tree-algorithms for random-access protocols, we present and analyze a
novel set reconciliation scheme that we term enhanced partitioned set
reconciliation (EPSR). This approach improves the efficiency in terms of
overhead, i.e., it yields a lower communication cost, while keeping the same
time and communication round complexity as PSR. Additionally, we simulate the
performance of the proposed algorithm in an event-driven simulator. Our
findings indicate that this novel protocol nearly halves the communication cost
of PSR while maintaining the same time complexity.

</details>


### [62] [Inter-DU Load Balancing in an Experimental Over-the-Air 5G Open Radio Access Network](https://arxiv.org/abs/2509.02420)
*Fahim Bashar,Asheesh Tripathi,Mayukh Roy Chowdhury,Aloizio Da Silva,Alexandre Huff*

Main category: cs.NI

TL;DR: 本文首次实现了基于O-RAN架构的5G NR SA网络中的负载均衡（LB）全开源方案，展示了闭环控制的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为了验证在开放的5G网络中实现负载均衡的可行性，同时推动O-RAN架构的实际应用。

Method: 利用O-RAN SC的Near-RT RIC、srsRAN协议栈、Open5GS核心网及SDR设备，扩展了srsRAN以支持E2SM-RC和E2SM-KPM的负载均衡功能。

Result: 成功实现了基于网络负载指标的移动负载均衡（MLB）xApp，在两个频段不同的O-DU之间进行切换决策。

Conclusion: 实验证明了O-RAN架构下负载均衡的可行性和有效性，为未来研究提供了开源基础。

Abstract: This paper presents the first ever fully open-source implementation of Load
Balancing (LB) in an experimental Fifth Generation (5G) New Radio (NR)
Standalone (SA) network using Open Radio Access Network (O-RAN) architecture.
The deployment leverages the O-RAN Software Community (SC) Near Real-Time RAN
Intelligent Controller (Near-RT RIC), srsRAN stack, Open5GS core, and
Software-Defined Radios (SDRs), with Commercial Off-The-Shelf (COTS) User
Equipments (UEs). The implementation extends the srsRAN stack to support E2
Service Model for RAN Control (E2SM-RC) Style 3 Action 1 to facilitate
Handovers (HOs) and adds Medium Access Control (MAC) downlink (DL) buffer
volume reporting to srsRAN's E2 Service Model for Key Performance Measurement
(E2SM-KPM). The deployment demonstrates Near-RT RIC closed-loop control where
our Mobility Load Balancing (MLB) xApp makes HO decisions based on network load
metrics for LB between two Open Distributed Units (O-DUs) operating at
different frequencies in the same band.

</details>


### [63] [On Transferring, Merging, and Splitting Task-Oriented Network Digital Twins](https://arxiv.org/abs/2509.02551)
*Zifan Zhang,Minghong Fang,Mingzhe Chen,Yuchen Liu*

Main category: cs.NI

TL;DR: 该论文探讨了在网络数字孪生（NDTs）中通过统一孪生转换（UTT）框架实现NDTs之间的互操作，以提高资源利用率和降低创建成本，同时确保模型一致性。


<details>
  <summary>Details</summary>
Motivation: 当前网络数字孪生技术面临数据整合、属性映射和可扩展性等挑战，需要一种新方法来优化NDTs的互操作效率。

Method: 提出统一孪生转换（UTT）框架，支持NDTs的高效转移、合并和拆分，并利用多模态和分布式映射机制。

Result: 理论分析和实际应用（如轨迹重建、人员定位）验证了UTT框架的有效性和可行性。

Conclusion: UTT框架为任务导向的数字孪生提供了一种新的计算范式，显著提升了NDTs的互操作性和实用性。

Abstract: The integration of digital twinning technologies is driving next-generation
networks toward new capabilities, allowing operators to thoroughly understand
network conditions, efficiently analyze valuable radio data, and innovate
applications through user-friendly, immersive interfaces. Building on this
foundation, network digital twins (NDTs) accurately depict the operational
processes and attributes of network infrastructures, facilitating predictive
management through real-time analysis and measurement. However, constructing
precise NDTs poses challenges, such as integrating diverse data sources,
mapping necessary attributes from physical networks, and maintaining
scalability for various downstream tasks. Unlike previous works that focused on
the creation and mapping of NDTs from scratch, we explore intra- and
inter-operations among NDTs within a Unified Twin Transformation (UTT)
framework, which uncovers a new computing paradigm for efficient transfer,
merging, and splitting of NDTs to create task-oriented twins. By leveraging
joint multi-modal and distributed mapping mechanisms, UTT optimizes resource
utilization and reduces the cost of creating NDTs, while ensuring twin model
consistency. A theoretical analysis of the distributed mapping problem is
conducted to establish convergence bounds for this multi-modal gated
aggregation process. Evaluations on real-world twin-assisted applications, such
as trajectory reconstruction, human localization, and sensory data generation,
demonstrate the feasibility and effectiveness of interoperability among NDTs
for corresponding task development.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [64] [Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?](https://arxiv.org/abs/2509.00053)
*Shuo Liu,Di Yao,Yan Lin,Gao Cong,Jingping Bi*

Main category: cs.MM

TL;DR: 提出了Traj-MLLM，一种基于多模态大语言模型（MLLMs）的通用框架，首次应用于轨迹数据挖掘。该框架通过多视角上下文将原始轨迹转化为图像-文本序列，并利用MLLMs的推理能力进行分析，在多个任务上表现优异，且无需训练数据或微调模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有轨迹分析方法泛化能力不足的问题，探索MLLMs在轨迹数据挖掘中的应用潜力。

Method: 引入Traj-MLLM框架，通过多视角上下文转换轨迹为图像-文本序列，并提出提示优化方法以适应不同任务。

Result: 在旅行时间估计、移动预测、异常检测和交通方式识别等任务上超越现有方法，分别提升48.05%、15.52%、51.52%和1.83%。

Conclusion: Traj-MLLM展示了MLLMs在轨迹数据挖掘中的强大能力，无需额外训练即可实现高效泛化。

Abstract: Building a general model capable of analyzing human trajectories across
different geographic regions and different tasks becomes an emergent yet
important problem for various applications. However, existing works suffer from
the generalization problem, \ie, they are either restricted to train for
specific regions or only suitable for a few tasks. Given the recent advances of
multimodal large language models (MLLMs), we raise the question: can MLLMs
reform current trajectory data mining and solve the problem? Nevertheless, due
to the modality gap of trajectory, how to generate task-independent multimodal
trajectory representations and how to adapt flexibly to different tasks remain
the foundational challenges. In this paper, we propose \texttt{Traj-MLLM}},
which is the first general framework using MLLMs for trajectory data mining. By
integrating multiview contexts, \texttt{Traj-MLLM}} transforms raw trajectories
into interleaved image-text sequences while preserving key spatial-temporal
characteristics, and directly utilizes the reasoning ability of MLLMs for
trajectory analysis. Additionally, a prompt optimization method is proposed to
finalize data-invariant prompts for task adaptation. Extensive experiments on
four publicly available datasets show that \texttt{Traj-MLLM}} outperforms
state-of-the-art baselines by $48.05\%$, $15.52\%$, $51.52\%$, $1.83\%$ on
travel time estimation, mobility prediction, anomaly detection and
transportation mode identification, respectively. \texttt{Traj-MLLM}} achieves
these superior performances without requiring any training data or fine-tuning
the MLLM backbones.

</details>


### [65] [LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition](https://arxiv.org/abs/2509.01337)
*Qianrui Zhou,Hua Xu,Yifan Wang,Xinzhi Dong,Hanlei Zhang*

Main category: cs.MM

TL;DR: 论文提出了一种LGSRR方法，利用大语言模型（LLM）提升小模型的关系推理能力，用于多模态意图理解。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在模态依赖和细粒度语义推理上的局限，提升复杂意图理解的准确性。

Method: 采用LLM提取细粒度语义作为指导，通过浅层到深层的Chain-of-Thought（CoT）自动发现和排序语义线索，并建模三种基础语义关系。

Result: 在多模态意图和对话行为识别任务中表现优异，超越现有方法。

Conclusion: LGSRR方法显著提升了语义理解和推理能力，适用于多样化场景。

Abstract: Understanding human intents from multimodal signals is critical for analyzing
human behaviors and enhancing human-machine interactions in real-world
scenarios. However, existing methods exhibit limitations in their
modality-level reliance, constraining relational reasoning over fine-grained
semantics for complex intent understanding. This paper proposes a novel
LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the
expansive knowledge of large language models (LLMs) to establish semantic
foundations that boost smaller models' relational reasoning performance.
Specifically, an LLM-based strategy is proposed to extract fine-grained
semantics as guidance for subsequent reasoning, driven by a shallow-to-deep
Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks
semantic cues by their importance without relying on manually defined priors.
Besides, we formally model three fundamental types of semantic relations
grounded in logical principles and analyze their nuanced interplay to enable
more effective relational reasoning. Extensive experiments on multimodal intent
and dialogue act recognition tasks demonstrate LGSRR's superiority over
state-of-the-art methods, with consistent performance gains across diverse
semantic understanding scenarios. The complete data and code are available at
https://github.com/thuiar/LGSRR.

</details>


### [66] [Efficient Geometry Compression and Communication for 3D Gaussian Splatting Point Clouds](https://arxiv.org/abs/2509.02232)
*Liang Xie,Yanting Li,Luyang Tang,Wei Gao*

Main category: cs.MM

TL;DR: 本文提出了一种结合AVS PCRM和i3DV平台的动态3D场景压缩方案，有效降低了存储和传输成本，同时保持高质量的渲染效果。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯数据量的爆炸式增长，存储和传输成为动态3D场景表示的主要挑战。

Method: 采用AVS PCRM软件对高斯点云几何数据进行高效压缩，并结合i3DV平台的哈希表机制优化率失真性能。

Result: 实验表明，该联合框架在40 Mbps带宽约束下实现高保真传输，并节省10%-25%的比特率。

Conclusion: 该方案为3D视频的存储、传输和交互提供了优越的率失真权衡解决方案。

Abstract: Storage and transmission challenges in dynamic 3D scene representation based
on the i3DV platform, With increasing scene complexity, the explosive growth of
3D Gaussian data volume causes excessive storage space occupancy. To address
this issue, we propose adopting the AVS PCRM reference software for efficient
compression of Gaussian point cloud geometry data. The strategy deeply
integrates the advanced encoding capabilities of AVS PCRM into the i3DV
platform, forming technical complementarity with the original rate-distortion
optimization mechanism based on binary hash tables. On one hand, the hash table
efficiently caches inter-frame Gaussian point transformation relationships,
which allows for high-fidelity transmission within a 40 Mbps bandwidth
constraint. On the other hand, AVS PCRM performs precise compression on
geometry data. Experimental results demonstrate that the joint framework
maintains the advantages of fast rendering and high-quality synthesis in 3D
Gaussian technology while achieving significant 10\%-25\% bitrate savings on
universal test sets. It provides a superior rate-distortion tradeoff solution
for the storage, transmission, and interaction of 3D volumetric video.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [67] [Undecidability of Linear Logics without Weakening](https://arxiv.org/abs/2509.00644)
*Jun Suzuki,Katsuhiko Sano*

Main category: cs.LO

TL;DR: 证明了在两种线性逻辑系统中（CLLR和CLLRR）的可证序列问题是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究线性逻辑中省略指数模态削弱规则的系统是否仍保持不可判定性。

Method: 通过省略CLL中的指数模态削弱规则定义CLLR，并通过省略单位元定义CLLRR，分别证明其不可判定性。

Result: 证实CLLR和CLLRR的不可判定性，前者通过归约到CLL，后者通过模拟Minsky的双计数器机。

Conclusion: 两种系统在省略特定规则后仍保持不可判定性，凸显了线性逻辑中某些结构的必要性。

Abstract: The goal of this paper is to establish that it remains undecidable whether a
sequent is provable in two systems in which a weakening rule for an exponential
modality is completely omitted from classical propositional linear logic
$\mathbf{CLL}$ introduced by Girard (1987), which is shown to be undecidable by
Lincoln et al. (1992). We introduce two logical systems, $\mathbf{CLLR}$ and
$\mathbf{CLLRR}$. The first system, $\mathbf{CLLR}$, is obtained by omitting
the weakening rule for the exponential modality of $\mathbf{CLL}$. The system
$\mathbf{CLLR}$ has been studied by several authors, including
Meli\`es-Tabareau (2010), but its undecidability was unknown. This paper shows
the undecidability of $\mathbf{CLLR}$ by reducing it to the undecidability of
$\mathbf{CLL}$, where the units $\mathbf{1}$ and $\bot$ play a crucial role in
simulating the weakening rule. We also omit these units from the syntax and
inference rules of $\mathbf{CLLR}$ in order to define the second system,
$\mathbf{CLLRR}$. The undecidability of $\mathbf{CLLRR}$ is established by
showing that the system can simulate any two-counter machine proposed by Minsky
(1961).

</details>


### [68] [Formal Verification of Isothermal Chemical Reactors](https://arxiv.org/abs/2509.01130)
*Parivash Feyzishendi,Sophia Hamer,Jinyu Huang,Tyler R. Josephson*

Main category: cs.LO

TL;DR: 该论文利用微分动态逻辑（dL）和定理证明工具KeYmaera X，为等温化学反应器中的状态可达性提供数学保证，并验证了某些条件（如出口浓度不超标）。方法适用于简单至复杂的反应动力学模型，但受限于不变量的发现，尤其是对CSTRs的复杂反应网络。尽管dL的结果范围较广，但相对数值模拟仍有限。


<details>
  <summary>Details</summary>
Motivation: 化学反应器的安全性和经济性依赖于状态的可达性评估，传统方法通常依赖数值模拟。本研究旨在通过符号逻辑方法（dL）为这些评估提供数学证明，确保某些条件（如浓度限制）得到满足。

Method: 使用微分动态逻辑（dL）和自动化定理证明工具KeYmaera X，对等温化学反应器中的状态可达性进行符号分析。方法覆盖了从闭式解的简单反应（如一级批反应器）到复杂动力学（如Michaelis-Menten），并依赖于不变量的识别（如质量守恒）。

Result: dL成功为简单至复杂反应动力学模型提供了可达性证明，但受限于不变量的发现，尤其是CSTRs的复杂反应网络。其结果的边界较数值模拟更广。

Conclusion: dL为化学反应器中的可达性问题提供了一种新颖的符号逻辑方法，但其在实际应用中的广泛性和精确性仍需要进一步优化，尤其是在复杂反应网络中不变量的发现方面。

Abstract: Chemical reactors are dynamic systems that can be described by systems of
ordinary differential equations (ODEs). Reactor safety, regulatory compliance,
and economics depend on whether certain states are reachable by the reactor,
and are generally assessed using numerical simulation. In this work, we show
how differential dynamic logic (dL), as implemented in the automated theorem
prover KeYmaera X, can be used to symbolically determine reachability in
isothermal chemical reactors, providing mathematical guarantees that certain
conditions are satisfied (for example, that an outlet concentration never
exceeds a regulatory threshold). First, we apply dL to systems whose dynamics
can be solved in closed form, such as first-order reactions in batch reactors,
proving that such reactors cannot exceed specified concentration limits. We
extend this method to reaction models as complex as Michaelis-Menten kinetics,
whose dynamics require approximations or numerical solutions. In all cases,
proofs are facilitated by identification of invariants; we find that
conservation of mass is both a principle proved from the ODEs describing mass
action kinetics as well as a useful relationship for proving other properties.
Useful invariants for continuous stirred tank reactors (CSTRs) were not found,
which limited the complexity of reaction networks that could be proved with dL.
While dL provides an interesting symbolic logic approach for reachability in
chemical reactions, the bounds we obtained are quite broad relative to those
typically achieved via numerical reachability analyses.

</details>


### [69] [Quantum Petri Nets with Event Structure semantics](https://arxiv.org/abs/2509.01423)
*Julien Saan Joachim,Marc de Visme,Stefan Haar*

Main category: cs.LO

TL;DR: 该论文提出了量子Petri网(QPNs)，填补了量子并发建模的空白，提供了严格的语义和展开理论。


<details>
  <summary>Details</summary>
Motivation: 为量子并发提供一个与经典Petri网类似的严格模型，解决现有量子Petri网缺乏并发性和量子语义的问题。

Method: 提出局部定义的量子发生网(LQONs)，并构建具有良好展开语义的QPNs，同时提供一个组合框架。

Result: 建立了一个语义明确的量子并发模型，连接了Petri网理论和量子编程。

Conclusion: QPNs为量子并发提供了一个理论基础和实用工具，填补了该领域的空白。

Abstract: Classical Petri nets provide a canonical model of concurrency, with unfolding
semantics linking nets, occurrence nets, and event structures. No comparable
framework exists for quantum concurrency: existing ''quantum Petri nets'' lack
rigorous concurrent and sound quantum semantics, analysis tools, and unfolding
theory. We introduce Quantum Petri Nets (QPNs), Petri nets equipped with a
quantum valuation compatible with the quantum event structure semantics of
Clairambault, De Visme, and Winskel (2019). Our contributions are: (i) a local
definition of Quantum Occurrence Nets (LQONs) compatible with quantum event
structures, (ii) a construction of QPNs with a well-defined unfolding
semantics, (iii) a compositional framework for QPNs. This establishes a
semantically well grounded model of quantum concurrency, bridging Petri net
theory and quantum programming.

</details>


### [70] [TREBL -- A Relative Complete Temporal Event-B Logic. Part I: Theory](https://arxiv.org/abs/2509.01462)
*Klaus-Dieter Schewe,Flavio Ferrarotti,Peter Rivière,Neeraj Kumar Singh,Guillaume Dupont,Yamine Aït Ameur*

Main category: cs.LO

TL;DR: 本文扩展了Event-B逻辑，提出了一种称为TREBL的片段，用于表达活性条件，并定义了其推导规则和相对完备性。


<details>
  <summary>Details</summary>
Motivation: 验证活性条件是状态基严格方法中的重要方面，现有Event-B逻辑需要扩展以支持轨迹属性的表达。

Method: 提出了TREBL逻辑片段，定义了一套推导规则，并证明其相对完备性，要求机器足够细化。

Result: 理论支持通过细化机器来满足推导规则的应用条件，且这种细化总是可行的。

Conclusion: 文章通过安全领域的示例验证了TREBL逻辑及其推导规则的有效性和实用性。

Abstract: The verification of liveness conditions is an important aspect of state-based
rigorous methods. This article addresses the extension of the logic of Event-B
to a powerful logic, in which properties of traces of an Event-B machine can be
expressed. However, all formulae of this logic are still interpreted over
states of an Event-B machine rather than traces. The logic exploits that for an
Event-B machine $M$ a state $S$ determines all traces of $M$ starting in $S$.
We identify a fragment called TREBL of this logic, in which all liveness
conditions of interest can be expressed, and define a set of sound derivation
rules for the fragment. We further show relative completeness of these
derivation rules in the sense that for every valid entailment of a formula
$\varphi$ one can find a derivation, provided the machine $M$ is sufficiently
refined. The decisive property is that certain variant terms must be definable
in the refined machine. We show that such refinements always exist. Throughout
the article several examples from the field of security are used to illustrate
the theory.

</details>


### [71] [An Information-Flow Perspective on Explainability Requirements: Specification and Verification](https://arxiv.org/abs/2509.01479)
*Bernd Finkbeiner,Hadar Frenkel,Julian Siber*

Main category: cs.LO

TL;DR: 该论文讨论了可解释系统如何通过信息流实现解释性，并提出了一种基于认知时序逻辑的方法来验证系统是否满足解释性要求。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于平衡可解释系统中的信息流，确保既能提供足够的解释信息，又不侵犯隐私。

Method: 使用扩展的认知时序逻辑，结合反事实因果量化，提出了一种算法来验证有限状态模型的解释性。

Result: 通过原型实现和基准测试，证明该方法能区分可解释与不可解释系统，并能附加隐私要求。

Conclusion: 论文提出了一种形式化方法，用于验证系统是否满足解释性和隐私的双重要求。

Abstract: Explainable systems expose information about why certain observed effects are
happening to the agents interacting with them. We argue that this constitutes a
positive flow of information that needs to be specified, verified, and balanced
against negative information flow that may, e.g., violate privacy guarantees.
Since both explainability and privacy require reasoning about knowledge, we
tackle these tasks with epistemic temporal logic extended with quantification
over counterfactual causes. This allows us to specify that a multi-agent system
exposes enough information such that agents acquire knowledge on why some
effect occurred. We show how this principle can be used to specify
explainability as a system-level requirement and provide an algorithm for
checking finite-state models against such specifications. We present a
prototype implementation of the algorithm and evaluate it on several
benchmarks, illustrating how our approach distinguishes between explainable and
unexplainable systems, and how it allows to pose additional privacy
requirements.

</details>


### [72] [Derivation and Verification of Array Sorting by Merging, and its Certification in Dafny](https://arxiv.org/abs/2509.01758)
*Juan Pablo Carbonell,José E. Solsona,Nora Szasz,Álvaro Tasistro*

Main category: cs.LO

TL;DR: 该论文对Dafny语言中的两种归并排序版本进行了形式化验证，展示了如何通过分治法生成循环不变式并实现迭代版本。


<details>
  <summary>Details</summary>
Motivation: 通过形式化验证提升归并排序算法的可靠性与正确性。

Method: 使用分治法设计模式，并通过Dafny语言实现递归和迭代版本的验证。

Result: 成功验证了递归和迭代版本的归并排序，并展示了方法的通用性。

Conclusion: 该方法不仅适用于归并排序，还可推广到其他排序算法如快速排序。

Abstract: We provide full certifications of two versions of merge sort of arrays in the
verification-aware programming language Dafny. We start by considering schemas
for applying the divide-and-conquer or partition method of solution to
specifications given by pre- and post-conditions involving linear arrays. We
then derive the merge sort and merging algorithms as instances of these
schemas, thereby arriving at a fully recursive formulation. Further, the
analysis of the tree of subproblems arising from the partition facilitates the
design of loop invariants that allow to derive a fully iterative version
(sometimes called bottom-up merge sort) that does not employ a stack. We show
how the use of the provided schemas conveniently conducts the formalization and
actual verification in Dafny. The whole method is also applicable to deriving
variants of quicksort, which we sketch.

</details>


### [73] [Probabilistically stable revision and comparative probability: a representation theorem and applications](https://arxiv.org/abs/2509.02495)
*Krzysztof Mierzewski*

Main category: cs.LO

TL;DR: 论文提出了一个关于信念稳定性的规则，并通过概率稳定的命题来描述理性的信念接受，证明了表示定理，并探讨了其在非单调逻辑和比较概率理论中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过概率稳定的命题和信念修订操作符，捕捉信念的动态变化，同时提供一个完整的表示定理来定性描述此类操作符。

Method: 通过概率稳定的命题和贝叶斯条件更新，结合比较概率理论，提出并证明了表示定理，并提供了选择函数语义。

Result: 证明了表示定理，完整描述了概率稳定的信念修订操作符，并展示了其在非单调逻辑中的强单调性特性，但未能满足AGM信念修订公设。

Conclusion: 研究结果为概率稳定的信念修订提供了理论基础，并揭示了其在比较概率理论、简单投票游戏和偏好理论中的潜在应用。

Abstract: The stability rule for belief, advocated by Leitgeb [Annals of Pure and
Applied Logic 164, 2013], is a rule for rational acceptance that captures
categorical belief in terms of $\textit{probabilistically stable
propositions}$: propositions to which the agent assigns resiliently high
credence. The stability rule generates a class of $\textit{probabilistically
stable belief revision}$ operators, which capture the dynamics of belief that
result from an agent updating their credences through Bayesian conditioning
while complying with the stability rule for their all-or-nothing beliefs. In
this paper, we prove a representation theorem that yields a complete
characterisation of such probabilistically stable revision operators and
provides a `qualitative' selection function semantics for the (non-monotonic)
logic of probabilistically stable belief revision. Drawing on the theory of
comparative probability orders, this result gives necessary and sufficient
conditions for a selection function to be representable as a
strongest-stable-set operator on a finite probability space. The resulting
logic of probabilistically stable belief revision exhibits strong monotonicity
properties while failing the AGM belief revision postulates and satisfying only
very weak forms of case reasoning. In showing the main theorem, we prove two
results of independent interest to the theory of comparative probability: the
first provides necessary and sufficient conditions for the joint representation
of a pair of (respectively, strict and non-strict) comparative probability
orders. The second result provides a method for axiomatising the logic of ratio
comparisons of the form ``event $A$ is at least $k$ times more likely than
event $B$''. In addition to these measurement-theoretic applications, we point
out two applications of our main result to the theory of simple voting games
and to revealed preference theory.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [74] [Unlocking Mixed Reality for Medical Education: A See-Through Perspective on Head Anatomy](https://arxiv.org/abs/2509.00352)
*Yuqing Wei,Yupeng Wang,Jiayi Zhao,Yanjun Liu,Huxin Gao,Jiewen Lai*

Main category: cs.HC

TL;DR: 该研究开发了一种混合现实（MR）应用，用于头部解剖学教育，通过手势和控制器交互，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统医学教育方法（如课本、物理模型）缺乏交互性和空间关系的有效传达，MR技术可以弥补这些不足。

Method: 研究设计了一个MR应用，结合分层信息设计和自动校准模块，支持交互式学习。

Result: 实验表明，该系统能有效匹配虚拟解剖模型与实时场景，增强医学教育的交互性和沉浸感。

Conclusion: MR技术为解剖学教育提供了创新的教学工具，具有显著的潜力。

Abstract: Extended reality (XR), encompassing Virtual Reality (VR), Augmented Reality
(AR), and Mixed Reality (MR), is emerging as a transformative platform for
medical education. Traditional methods such as textbooks, physical models, and
cadaveric dissections often lack interactivity and fail to convey complex
spatial relationships effectively. The emerging MR technology addresses these
limitations by providing immersive environments that blend virtual elements
with real-world contexts. This study presents an MR application for head
anatomy education, enabling learners to intuitively interact with see-through
3D anatomical structures via hand gestures and controllers. Our hierarchical
information design supports progressive learning, guiding users from basic
anatomical labels to detailed structural insights. Additionally, the system
incorporates an automatic calibration module that aligns virtual anatomical
models with a real human head, thereby facilitating realistic human-model
interactions. Experiments show that the system can effectively match the
anatomical model with real-time scenes, thus enhancing the interactivity and
immersion of medical education, providing an innovative tool for teaching
anatomy.

</details>


### [75] [Data Humanism Decoded: A Characterization of its Principles to Bridge Data Visualization Researchers and Practitioners](https://arxiv.org/abs/2509.00440)
*Ibrahim Al-Hazwani,Ke Er Zhang,Laura Garrison,Jürgen Bernard*

Main category: cs.HC

TL;DR: 该论文通过混合方法研究，定义了Data Humanism的13条原则，为可视化研究提供了具体指导，并通过专家验证和实际应用验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Data Humanism作为一种以人为本的设计方法，虽在设计实践中逐渐流行，但其原则在研究中定义模糊，导致设计与系统应用之间存在鸿沟。

Method: 采用混合方法，包括系统性文献综述、多媒体分析和专家访谈，对Data Humanism原则进行定义和验证。

Result: 论文提出了Data Humanism原则的具体定义，保持了设计选择的操作灵活性，并通过专家验证和实际案例映射验证了其有效性。

Conclusion: 研究为以人为本的可视化设计建立了共同语言，弥合了实践与研究之间的鸿沟，为未来应用和评估提供了基础。

Abstract: Data Humanism is a human-centered design approach that emphasizes the
personal, contextual, and imperfect nature of data. Despite its growing
influence among practitioners, the 13 principles outlined in Giorgia Lupi's
visual manifesto remain loosely defined in research contexts, creating a gap
between design practice and systematic application. Through a mixed-methods
approach, including a systematic literature review, multimedia analysis, and
expert interviews, we present a characterization of Data Humanism principles
for visualization researchers. Our characterization provides concrete
definitions that maintain interpretive flexibility in operationalizing design
choices. We validate our work through direct consultation with Lupi. Moreover,
we leverage the characterization to decode a visualization work, mapping Data
Humanism principles to specific visual design choices. Our work creates a
common language for human-centered visualization, bridging the gap between
practice and research for future applications and evaluations.

</details>


### [76] [How to Make Museums More Interactive? Case Study of Artistic Chatbot](https://arxiv.org/abs/2509.00572)
*Filip J. Kucia,Bartosz Grabek,Szymon D. Trochimiak,Anna Wróblewska*

Main category: cs.HC

TL;DR: 该论文提出了一个名为Artistic Chatbot的声音交互聊天机器人，用于在艺术展览中支持非正式学习，增强参观者互动，并在实际部署中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）在实体文化场所（如艺术展览）中的应用潜力，提升参观者的互动体验。

Method: 开发了一个基于RAG技术的语音问答聊天机器人，使用策展人提供的226份领域知识文档作为知识库。

Result: 聊天机器人在面对不可预测的查询时，60%的回答直接相关，显示出其在文化场所中的实用性。

Conclusion: Artistic Chatbot展示了在公共文化场所中增加互动性的潜力，但也指出了实际部署中的挑战。

Abstract: Conversational agents powered by Large Language Models (LLMs) are
increasingly utilized in educational settings, in particular in individual
closed digital environments, yet their potential adoption in the physical
learning environments like cultural heritage sites, museums, and art galleries
remains relatively unexplored. In this study, we present Artistic Chatbot, a
voice-to-voice RAG-powered chat system to support informal learning and enhance
visitor engagement during a live art exhibition celebrating the 15th
anniversary of the Faculty of Media Art at the Warsaw Academy of Fine Arts,
Poland. The question answering (QA) chatbot responded to free-form spoken
questions in Polish using the context retrieved from a curated, domain-specific
knowledge base consisting of 226 documents provided by the organizers,
including faculty information, art magazines, books, and journals. We describe
the key aspects of the system architecture and user interaction design, as well
as discuss the practical challenges associated with deploying chatbots at
public cultural sites. Our findings, based on interaction analysis, demonstrate
that chatbots such as Artistic Chatbot effectively maintain responses grounded
in exhibition content (60\% of responses directly relevant), even when faced
with unpredictable queries outside the target domain, showing their potential
for increasing interactivity in public cultural sites.
  GitHub project page: https://github.com/cinekucia/artistic-chatbot-cikm2025

</details>


### [77] [Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse](https://arxiv.org/abs/2509.00696)
*Akriti Verma,Shama Islam,Valeh Moghaddam,Adnan Anwar*

Main category: cs.HC

TL;DR: 本文提出了一种基于图的框架和评论排队机制，用于实时调节在线对话中的情绪，减少毒性和愤怒传播。


<details>
  <summary>Details</summary>
Motivation: 在线毒性和情绪动态影响数字互动，现有研究多关注事后审核，忽视了实时情绪调节。

Method: 采用图框架识别情绪调节需求，引入评论排队机制延迟发布评论。

Result: 实验显示毒性减少12%，愤怒传播降低15%，仅4%评论被临时延迟。

Conclusion: 实时情绪调节与延迟审核结合可显著提升在线环境健康。

Abstract: The pervasiveness of online toxicity, including hate speech and trolling,
disrupts digital interactions and online well-being. Previous research has
mainly focused on post-hoc moderation, overlooking the real-time emotional
dynamics of online conversations and the impact of users' emotions on others.
This paper presents a graph-based framework to identify the need for emotion
regulation within online conversations. This framework promotes self-reflection
to manage emotional responses and encourage responsible behaviour in real time.
Additionally, a comment queuing mechanism is proposed to address intentional
trolls who exploit emotions to inflame conversations. This mechanism introduces
a delay in publishing comments, giving users time to self-regulate before
further engaging in the conversation and helping maintain emotional balance.
Analysis of social media data from Twitter and Reddit demonstrates that the
graph-based framework reduced toxicity by 12%, while the comment queuing
mechanism decreased the spread of anger by 15%, with only 4% of comments being
temporarily held on average. These findings indicate that combining real-time
emotion regulation with delayed moderation can significantly improve well-being
in online environments.

</details>


### [78] [Why it is worth making an effort with GenAI](https://arxiv.org/abs/2509.00852)
*Yvonne Rogers*

Main category: cs.HC

TL;DR: 探讨如何通过调整学生对生成式AI（如ChatGPT）的使用方式，使其在学习中投入更多努力，从而避免过度依赖导致的写作和批判性思维能力退化，并设计新工具以提升学习体验。


<details>
  <summary>Details</summary>
Motivation: 当前学生过度依赖生成式AI完成作业，可能导致其写作和批判性思维能力发展受阻，需要探索如何通过工具设计让学生在AI辅助学习中投入更多努力，以实现更深入的学习效果。

Method: 提出让学生在使用生成式AI时增加额外努力（如结合传统学习方法），并设计新工具以促进元认知和反思能力的培养。

Result: 初步提出了一些工具设计思路，例如结合生成式AI与传统学习方式，以增强学生的学习体验和技能发展。

Conclusion: 通过有意识地设计需要更多努力的AI工具，可以平衡学生对AI的依赖，同时促进其写作、批判性思维和元认知能力的全面发展。

Abstract: Students routinely use ChatGPT and the like now to help them with their
homework, such as writing an essay. It takes less effort to complete and is
easier to do than by hand. It can even produce as good if not better output
than the student's own work. However, there is a growing concern that
over-reliance on using GenAI in this way will stifle the development of
learning writing and critical thinking skills. How might this trend be
reversed? What if students were required to make more effort when using GenAI
to do their homework? It might be more challenging, but the additional effort
involved could result in them learning more and having a greater sense of
achievement. This tension can be viewed as a form of effort paradox; where
effort is both viewed as something to be avoided but at the same time is
valued. Is it possible to let students learn sometimes with less and other
times more effort? Students are already adept at the former but what about the
latter? Could we design new kinds of AI tools that deliberately require more
effort to use to deepen the learning experience? In this paper, I begin to
outline what form these might take, for example, asking students to use a
combination of GenAI tools with traditional learning approaches (e.g.
note-taking while reading). I also discuss how else to design tools to think
with that augments human cognition; where students learn more the skills of
metacognition and reflection.

</details>


### [79] [Through the Expert's Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR](https://arxiv.org/abs/2509.00944)
*Clara Sayffaerth,Annika Köhler,Julian Rasch,Albrecht Schmidt,Florian Müller*

Main category: cs.HC

TL;DR: 研究表明，第一人称视角在复杂手动任务的教学中表现优于第三人称视角，且更受用户青睐。


<details>
  <summary>Details</summary>
Motivation: 解决跨代传递复杂实践技能的问题，探索XR和AI技术在保留专家知识和提供个性化教学方面的潜力。

Method: 通过36人实验，比较异步第一人称和第三人称视角及视线可视化对效率、体现感和连接性的影响。

Result: 第一人称视角在量化指标上表现更好，且用户偏好度更高。

Conclusion: 提供了展示保留知识的最佳实践和未来系统设计指南。

Abstract: Transferring knowledge across generations is fundamental to human
civilization, yet the challenge of passing on complex practical skills
persists. Methods without a physically present instructor, such as videos,
often fail to explain complex manual tasks, where spatial and social factors
are critical. Technologies such as eXtended Reality and Artificial Intelligence
hold the potential to retain expert knowledge and facilitate the creation of
tailored, contextualized, and asynchronous explanations regardless of time and
place. In contrast to videos, the learner's perspective can be different from
the recorded perspective in XR. This paper investigates the impact of
asynchronous first- and third-person perspectives and gaze visualizations on
efficiency, feeling of embodiment, and connectedness during manual tasks. The
empirical results of our study (N=36) show that the first-person perspective is
better in quantitative measures and preferred by users. We identify best
practices for presenting preserved knowledge and provide guidelines for
designing future systems.

</details>


### [80] [The State of the Art in Visualization Literacy](https://arxiv.org/abs/2509.01018)
*Matthew Varona,Karen Bonilla,Maryam Hedayati,Alark Joshi,Lane Harrison,Matthew Kay,Carolina Nobre*

Main category: cs.HC

TL;DR: 本文综述了可视化素养领域的研究现状，提出了一个分类法，并提供了操作框架来定义和分类相关能力。


<details>
  <summary>Details</summary>
Motivation: 解决‘可视化素养’概念的模糊性，为领域研究提供清晰的组织框架。

Method: 提出基于应用情景和能力的分类法，将研究贡献分为五类。

Result: 总结了关键趋势、开放挑战及各类别间相互促进的关系。

Conclusion: 该框架推动了可视化素养领域的进步，并为未来研究指明了方向。

Abstract: Research in visualization literacy explores the skills required to engage
with visualizations. This state-of-the-art report surveys the current
literature in visualization literacy to provide a comprehensive overview of the
field. We propose a taxonomy of visualization literacy that organizes the field
into competency themes and research categories. To address ambiguity
surrounding the term ``visualization literacy'', we provide a framework for
operationalizing visualization literacy based on application contexts
(including domain, scenario, and audience) and relevant competencies, which are
categorized under consumption, construction, critique, and connection. Research
contributions are organized into five categories: ontology, assessment,
mechanisms, populiteracy, and intervention. For each category, we identify key
trends, discuss which competencies are addressed, highlight open challenges,
and examine how advancements within these areas inform and reinforce each
other, driving progress in the field.

</details>


### [81] [Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces](https://arxiv.org/abs/2509.01051)
*Matte Lim,Catherine Yeh,Martin Wattenberg,Fernanda Viégas,Panagiotis Michalatos*

Main category: cs.HC

TL;DR: 一种结合力导向投影和流式聚类的可视化技术，用于探索时间数据的演变主题。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法难以捕捉时间数据中的语义变化。

Method: 结合力导向投影和流式聚类，构建时空嵌入地图。

Result: 开发了交互式工具Chronotome，成功应用于文本和图像数据。

Conclusion: 该方法为理解时间数据集的美学和语义提供了新视角。

Abstract: Many real-world datasets -- from an artist's body of work to a person's
social media history -- exhibit meaningful semantic changes over time that are
difficult to capture with existing dimensionality reduction methods. To address
this gap, we introduce a visualization technique that combines force-based
projection and streaming clustering methods to build a spatial-temporal map of
embeddings. Applying this technique, we create Chronotome, a tool for
interactively exploring evolving themes in time-based data -- in real time. We
demonstrate the utility of our approach through use cases on text and image
data, showing how it offers a new lens for understanding the aesthetics and
semantics of temporal datasets.

</details>


### [82] [CosinorAge: Unified Python and Web Platform for Biological Age Estimation from Wearable- and Smartwatch-Based Activity Rhythms](https://arxiv.org/abs/2509.01089)
*Jinjoo Shim,Jacob Hunecke,Elgar Fleisch,Filipe Barata*

Main category: cs.HC

TL;DR: CosinorAge是一个开源框架，通过智能设备数据估算生物年龄，解决了数据分析工具分散、专有和难以访问的问题。


<details>
  <summary>Details</summary>
Motivation: 将智能设备收集的个人健康数据转化为有意义的健康洞察，帮助用户理解并影响其生物年龄。

Method: 提供端到端的工作流，从原始数据预处理到特征计算和生物年龄估算，支持多种设备数据输入，并提供预训练模型参数。

Result: CosinorAge通过统一的、可复现的管道分析昼夜节律、活动和睡眠数据，并链接到健康结果。

Conclusion: 该工具结合透明可复现的分析和广泛的可访问性，推动了个性化健康监控的发展，并连接了数字健康技术与生物衰老研究。

Abstract: Every day, millions of people worldwide track their steps, sleep, and
activity rhythms with smartwatches and fitness trackers. These continuously
collected data streams present a remarkable opportunity to transform routine
self-tracking into meaningful health insights that enable individuals to
understand -- and potentially influence -- their biological aging. Yet most
tools for analyzing wearable data remain fragmented, proprietary, and
inaccessible, creating a major barrier between this vast reservoir of personal
health information and its translation into actionable insights on aging.
CosinorAge is an open-source framework that estimates biological age from
wearable-derived circadian, physical activity, and sleep metrics. It addresses
the lack of unified, reproducible pipelines for jointly analyzing
rest--activity rhythmicity, physical activity, and sleep, and linking them to
health outcomes. The Python package provides an end-to-end workflow from raw
data ingestion and preprocessing to feature computation and biological age
estimation, supporting multiple input sources across wearables and smartwatch.
It also makes available trained model parameters (open weights) derived from
large-scale population datasets such as UK Biobank, enabling reproducibility,
transparency, and generalizability across studies. Its companion web-based
CosinorAge Calculator enables non-technical users to access identical
analytical capabilities through an intuitive interface. By combining
transparent, reproducible analysis with broad accessibility, CosinorAge
advances scalable, personalized health monitoring and bridges digital health
technologies with biological aging research.

</details>


### [83] [Unpacking Personal(?!) Health Informatics: An Investigation of Awareness, Understanding, And Leveraged Utility in India](https://arxiv.org/abs/2509.01231)
*Shyama Sastha Krishnamoorthy Srinivasan,Mohan Kumar,Pushpendra Singh*

Main category: cs.HC

TL;DR: 研究探讨了印度背景下个人健康信息学（PHI）的采用障碍，通过多方法研究发现低健康素养、可用性挑战和数字健康平台的不信任是主要障碍，并提出了用户控制的设计建议。


<details>
  <summary>Details</summary>
Motivation: 个人健康信息学（PHI）在支持健康评估和自我护理方面具有潜力，但其在印度的采用障碍尚未充分研究。

Method: 采用多方法研究，包括调查（n=87）和半结构化访谈（n=22），分析PHI的使用动机、模式和障碍，并通过原型设计和评估提出改进方案。

Result: 研究发现PHI的健康监测价值被认可，但低健康素养、可用性问题和信任问题阻碍了其采用。提出的原型设计得到了验证。

Conclusion: 研究强调了印度PHI采用的社会技术挑战，建议开发可靠、以用户为中心的解决方案。

Abstract: Personal Health Informatics (PHI), which leverages digital tools and
information systems to support health assessment and self-care, holds promise
for empowering individuals and transforming healthcare delivery. However,
barriers to its adoption remain underexplored in the Indian context. This study
investigates PHI adoption among Indian users and stakeholders using a
multi-method approach. An awareness survey (n = 87) examined the usage of
wearables and general PHI engagement, followed by semi-structured interviews (n
= 22) that explored motivations, usage patterns, and health information
sources. Qualitative analysis revealed that while PHI is valued for health
monitoring and shared/collective care, its adoption is hindered by factors such
as low health literacy, usability challenges, and mistrust in digital health
platforms. Further stakeholder interviews and co-design workshops informed the
development of a Figma-based prototype, which was evaluated for usability.
Based on these findings, we offer design recommendations for an integrated,
user-controlled PHI platform featuring accessible analytics and verifiable
health information. Our insights highlight the socio-technical challenges of
PHI adoption in India and underscore the need for reliable, user-centric
solutions to support proactive healthcare.

</details>


### [84] [An AI-Based Shopping Assistant System to Support the Visually Impaired](https://arxiv.org/abs/2509.01246)
*Larissa R. de S. Shibata,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.HC

TL;DR: 该论文提出了一种基于AI的购物助手原型，旨在帮助视障人士在超市环境中提升自主性和包容性，结合了计算机视觉、语音识别等技术。


<details>
  <summary>Details</summary>
Motivation: 购物在塑造消费者身份和社会融合中起重要作用，但对视障人士来说，超市购物具有挑战性。研究旨在提升他们的购物体验。

Method: 系统整合了计算机视觉、语音识别、文本转语音合成和室内导航技术，通过ArUco标记检测和实时环境扫描帮助用户。

Result: 实验表明系统能有效引导用户并改善购物体验。

Conclusion: 该研究为开发包容性AI辅助技术提供了贡献，旨在提升购物体验的可访问性和用户独立性。

Abstract: Shopping plays a significant role in shaping consumer identity and social
integration. However, for individuals with visual impairments, navigating in
supermarkets and identifying products can be an overwhelming and challenging
experience. This paper presents an AI-based shopping assistant prototype
designed to enhance the autonomy and inclusivity of visually impaired
individuals in supermarket environments. The system integrates multiple
technologies, including computer vision, speech recognition, text-to-speech
synthesis, and indoor navigation, into a single, user-friendly platform. Using
cameras for ArUco marker detection and real-time environmental scanning, the
system helps users navigate the store, identify product locations, provide
real-time auditory guidance, and gain context about their surroundings. The
assistant interacts with the user through voice commands and multimodal
feedback, promoting a more dynamic and engaging shopping experience. The system
was evaluated through experiments, which demonstrated its ability to guide
users effectively and improve their shopping experience. This paper contributes
to the development of inclusive AI-driven assistive technologies aimed at
enhancing accessibility and user independence for the shopping experience.

</details>


### [85] [MetaRoundWorm: A Virtual Reality Escape Room Game for Learning the Lifecycle and Immune Response to Parasitic Infections](https://arxiv.org/abs/2509.01367)
*Xuanru Cheng,Xian Wang,Chi-lok Tai,Lik-Hang Lee*

Main category: cs.HC

TL;DR: MetaRoundWorm是一款沉浸式VR逃脱游戏，旨在通过游戏化学习提升对寄生虫感染和宿主免疫反应的理解。研究显示，与传统互动幻灯片相比，它能显著提高学习效果和参与度。


<details>
  <summary>Details</summary>
Motivation: 公共卫生教育具有抽象性且易引发抵触情绪，亟需创新教学方法。元宇宙和游戏化被视为改善免疫系统学习体验的新途径。

Method: 开发了MetaRoundWorm VR游戏，模拟蛔虫生命周期及人体免疫反应。结合任务驱动、探索解谜等机制，并与传统互动幻灯片进行对比研究。

Result: MetaRoundWorm显著提升了即时学习效果、认知参与度和情感体验，且知识保留率稳定。

Conclusion: 沉浸式VR游戏化是传播复杂生物医学概念和推动数字健康教育的有效教学工具。

Abstract: Promoting public health is challenging owing to its abstract nature, and
individuals may be apprehensive about confronting it. Recently, there has been
an increasing interest in using the metaverse and gamification as novel
educational techniques to improve learning experiences related to the immune
system. Thus, we present MetaRoundWorm, an immersive virtual reality (VR)
escape room game designed to enhance the understanding of parasitic infections
and host immune responses through interactive, gamified learning. The
application simulates the lifecycle of Ascaris lumbricoides and corresponding
immunological mechanisms across anatomically accurate environments within the
human body. Integrating serious game mechanics with embodied learning
principles, MetaRoundWorm offers players a task-driven experience combining
exploration, puzzle-solving, and immune system simulation. To evaluate the
educational efficacy and user engagement, we conducted a controlled study
comparing MetaRoundWorm against a traditional approach, i.e., interactive
slides. Results indicate that MetaRoundWorm significantly improves immediate
learning outcomes, cognitive engagement, and emotional experience, while
maintaining knowledge retention over time. Our findings suggest that immersive
VR gamification holds promise as an effective pedagogical tool for
communicating complex biomedical concepts and advancing digital health
education.

</details>


### [86] [AttenTrack: Mobile User Attention Awareness Based on Context and External Distractions](https://arxiv.org/abs/2509.01414)
*Yutong Lin,Suyuan Liu,Kaiwen Guo,Haohua Du,Chao Liu,Xiang-Yang Li*

Main category: cs.HC

TL;DR: 该论文提出了一种名为AttenTrack的轻量级注意力感知模型，通过分析用户的响应行为和上下文信息来推断注意力状态，无需依赖个性化历史数据或复杂主观输入，具有较强的冷启动能力和跨上下文适应性。


<details>
  <summary>Details</summary>
Motivation: 当前基于可穿戴设备或个性化数据的注意力感知系统可扩展性和跨上下文适应性有限，论文试图通过心理学理论，将移动通知视为自然的外部干扰，构建不依赖个性化数据的注意力感知模型。

Method: 设计了结合主观和客观数据的现场研究框架，围绕当前上下文-外部干扰-主观注意力的关系构建细粒度数据集，并提出基于非隐私敏感数据的AttenTrack模型。

Result: 通过现场研究深入分析用户响应行为、动机、上下文和注意力状态的关系，证明了模型的冷启动能力和适用性。

Conclusion: 论文提出的AttenTrack模型具有轻量、隐私友好和广泛适用性，并将公开数据集以推动移动注意力感知领域的研究。

Abstract: In the mobile internet era, managing limited attention amid information
overload is crucial for enhancing collaboration and information delivery.
However, current attention-aware systems often depend on wearables or
personalized data, limiting their scalability and cross-context adaptability.
Inspired by psychological theories, we attempt to treat mobile notifications as
naturally occurring external distractions and infer users' attention states
based on their response behaviors and contextual information. Our goal is to
build an attention-aware model that does not rely on personalized historical
data or complex subjective input, while ensuring strong cold-start capability
and cross-context adaptability. To this end, We design a field study framework
integrating subjective and objective data, closely aligned with real-world
external distractions (i.e., mobile notifications). Through field studies, we
construct a fine-grained and interpretable dataset centered on the relationship
among current context - external distractions - subjective attention. Through
our field studies, we conduct an in-depth analysis of the relationships among
users' response behaviors, response motivations, contextual information, and
attention states. Building on our findings, we propose AttenTrack, a
lightweight, privacy-friendly attention awareness model with strong cold-start
capability. The model relies solely on non-privacy-sensitive objective data
available on mobile devices, and can be applied to a variety of attention
management tasks. In addition, we will publicly release the constructed dataset
to support future research and advance the field of mobile attention awareness.

</details>


### [87] [Body Ownership Affects the Processing of Sensorimotor Contingencies in Virtual Reality](https://arxiv.org/abs/2509.01420)
*Evan G. Center,Matti Pouke,Alessandro Nardi,Lukas Gehrke,Klaus Gramann,Timo Ojala,Steven M. LaValle*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Presence in virtual reality (VR), the subjective sense of "being there" in a
virtual environment, is notoriously difficult to measure.
Electroencephalography (EEG) may offer a promising, unobtrusive means of
assessing a user's momentary state of presence. Unlike traditional
questionnaires, EEG does not interrupt the experience or rely on users'
retrospective self-reports, thereby avoiding interference with the very state
it aims to capture. Previous research has attempted to quantify presence in
virtual environments using event-related potentials (ERPs). We contend,
however, that previous efforts have fallen short of fully realizing this goal,
failing to either A) independently manipulate presence, B) validate their
measure of presence against traditional techniques, C) adequately separate the
constructs of presence and attention, and/or D) implement a realistic and
immersive environment and task. We address these shortcomings in a
preregistered ERP experiment in which participants play an engaging target
shooting game in VR. ERPs are time-locked to the release of a ball from a
sling. We induce breaks in presence (BIPs) by freezing the ball's release on a
minority of trials. Embodiment is manipulated by allowing manual manipulation
of the sling with a realistic avatar in one condition (embodied condition) and
passive manipulation with only controllers in another (non-embodied condition).
We support our predictions that the N2, the P3b, and the N400, are selectively
sensitive towards specific components of these manipulations. The pattern of
findings carries significant implications for theories of presence, which have
been seldom addressed in previous ERP investigations on this topic.

</details>


### [88] [Dissecting Atomic Facts: Visual Analytics for Improving Fact Annotations in Language Model Evaluation](https://arxiv.org/abs/2509.01460)
*Manuel Schmidt,Daniel A. Keim,Frederik L. Dennig*

Main category: cs.HC

TL;DR: 提出了一种可视化分析方法，用于暴露和分析事实提取中的标注不一致问题，旨在改善LLM的事实性评估。


<details>
  <summary>Details</summary>
Motivation: 现有原子事实定义不明确，导致人工和模型标注者之间高分歧，影响LLM事实性评估的准确性。

Method: 通过可视化语义对齐、粒度和指代依赖性，系统性检查提取的事实，并通过引导修订循环促进一致性。

Result: 提出了一种更稳定的事实性评估基准方法，有助于改善LLM的事实性评估效果。

Conclusion: 该可视化分析工具能有效减少标注不一致性，为LLM事实性评估提供更可靠的基础。

Abstract: Factuality evaluation of large language model (LLM) outputs requires
decomposing text into discrete "atomic" facts. However, existing definitions of
atomicity are underspecified, with empirical results showing high disagreement
among annotators, both human and model-based, due to unresolved ambiguity in
fact decomposition. We present a visual analytics concept to expose and analyze
annotation inconsistencies in fact extraction. By visualizing semantic
alignment, granularity and referential dependencies, our approach aims to
enable systematic inspection of extracted facts and facilitate convergence
through guided revision loops, establishing a more stable foundation for
factuality evaluation benchmarks and improving LLM evaluation.

</details>


### [89] [Quantifying the Effect of Thermal Illusions in Virtual Reality](https://arxiv.org/abs/2509.01609)
*Yannick Weiss,Marlene Eder,Oguzhan Cesur,Steeven Villa*

Main category: cs.HC

TL;DR: 该论文研究了热错觉（通过视觉和听觉线索模拟温度感知）的效果，发现其对主观感知有明显影响，但实际温度感知变化较小。


<details>
  <summary>Details</summary>
Motivation: 虚拟和扩展现实系统难以有效模拟热感，硬件设备笨重且限制用户活动，因此探索热错觉作为一种替代方法。

Method: 进行了两项用户研究，通过心理物理程序评估视觉和听觉线索对温度感知的影响。

Result: 热错觉能显著改变主观评价，但实际温度感知变化较小（约±0.5°C），且听觉线索可作为视觉技术的补充。

Conclusion: 未来热错觉设计需重新考虑评估方法，以更好地模拟温度感知。

Abstract: Thermal sensations are central to how we experience the world, yet most
virtual and extended reality systems fail to simulate them effectively. While
hardware-based thermal displays can provide accurate temperature changes, they
are often bulky, power-intensive, and restrict user mobility. Consequently,
recent works have explored thermal illusions, perceptual effects that rely on
cross-modal interactions, to achieve thermal experiences without physical
heating or cooling. While thermal illusions have been shown to consistently
alter subjective ratings, the actual extent of their effect on the perceived
temperature of interacted objects remains unexplored. To address this, we
contribute the findings of two user studies following psychophysical
procedures. We first ordered and scaled the effects of a variety of visual and
auditory cues (N=20) and subsequently quantified their isolated and combined
efficacy in offsetting physical temperature changes (N=24). We found that
thermal illusions elicited robust changes in subjective judgments, and auditory
cues showed potential as an alternative or complementary approach to
established visual techniques. However, the actual effects induced by thermal
illusions were relatively small (+-0.5{\deg}C) and did not consistently align
with abstract ratings, suggesting a need to reconsider how future thermal
illusions or experiences are designed and evaluated.

</details>


### [90] [An Interactive Google Earth Engine Application for Global Multi-Scale Vegetation Analysis Using NDVI Thresholding](https://arxiv.org/abs/2509.01628)
*Md. Moktader Moula,Israt Jahan Shonom,Azharul Islam,Mohammad Mosharraf Hossain*

Main category: cs.HC

TL;DR: 开发了一个基于Google Earth Engine的交互式云应用程序，简化全球植被分析，无需复杂技术知识，支持实时植被覆盖量化。


<details>
  <summary>Details</summary>
Motivation: 传统遥感方法监测植被动态复杂且资源密集，需要简化技术流程，为决策者和研究人员提供易用工具。

Method: 利用GEE平台，通过NDVI指数和Sentinel-2、Landsat影像，自动化计算植被面积，支持多尺度分析和自定义区域。

Result: 在孟加拉国两个保护区验证中，准确率达97%以上，工具可扩展且实用。

Conclusion: 该工具简化了地理空间分析，支持政策制定和环境保护决策。

Abstract: Monitoring vegetation dynamics is crucial for addressing global environmental
challenges like degradation and deforestation, but traditional remote sensing
methods are often complex and resource-intensive. To overcome these barriers,
we developed an interactive, cloud-based application on the Google Earth Engine
(GEE) platform for few clicks on-demand global vegetation analysis without
complex technical knowledge. The application automates the calculation of
vegetated areas using the Normalized Difference Vegetation Index (NDVI) derived
from Sentinel-2 and Landsat imagery. It utilizes a median composite of images
over a selected period to create a single, robust, cloud-free image, minimizing
atmospheric noise and other artifacts. It offers a flexible, global multi-scale
analytical platform, allowing users to define regions of interest based on
administrative boundaries, protected areas, or custom-drawn polygons. The
user-friendly interface enables the selection of specific time periods and NDVI
thresholds to quantify vegetation cover in real time, eliminating the need for
manual and time intensive data handling and processing. A validation of the
platform was conducted for two protected areas in Bangladesh which demonstrated
high accuracy, with area estimates showing over 97% agreement with published
reference data. By simplifying access to powerful geospatial analytics to
general people, this tool provides a scalable and practical solution for
researchers, land managers, policymakers, and any interested person to monitor
vegetation trends, support conservation efforts, to inform decision making in
spatial context where policy maker need to use insights in few clicks and
inform environmental policy.

</details>


### [91] [EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras](https://arxiv.org/abs/2509.01786)
*Vimal Mollyn,Chris Harrison*

Main category: cs.HC

TL;DR: 摘要提出了一种基于RGB摄像头的高精度裸手皮肤输入方法，适用于AR/VR环境，无需特殊设备，且在各种光照、肤色和身体运动下表现稳健。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决现有AR/VR中空中输入界面的速度、精度和人体工学问题，探索更便捷的皮肤输入方式。

Method: 利用现代XR头显内置的RGB摄像头，开发了一种高精度裸手皮肤输入技术，支持多种输入元数据（如触摸力、手指识别等）。

Result: 实验结果显示该方法精度高，能适应不同光照、肤色和身体运动条件，并提供了丰富的输入元数据。

Conclusion: 该技术为皮肤输入界面提供了实用且稳健的解决方案，有望推动其在HCI领域的广泛应用。

Abstract: In augmented and virtual reality (AR/VR) experiences, a user's arms and hands
can provide a convenient and tactile surface for touch input. Prior work has
shown on-body input to have significant speed, accuracy, and ergonomic benefits
over in-air interfaces, which are common today. In this work, we demonstrate
high accuracy, bare hands (i.e., no special instrumentation of the user) skin
input using just an RGB camera, like those already integrated into all modern
XR headsets. Our results show this approach can be accurate, and robust across
diverse lighting conditions, skin tones, and body motion (e.g., input while
walking). Finally, our pipeline also provides rich input metadata including
touch force, finger identification, angle of attack, and rotation. We believe
these are the requisite technical ingredients to more fully unlock on-skin
interfaces that have been well motivated in the HCI literature but have lacked
robust and practical methods.

</details>


### [92] [Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore](https://arxiv.org/abs/2509.01845)
*Gabriel Spadon,Oladapo Oyebode,Camilo M. Botero,Tushar Sharma,Floris Goerlandt,Ronald Pelot*

Main category: cs.HC

TL;DR: 该论文概述了一项以人为中心的计划，旨在增强新斯科舍省东部海岸的气候韧性。通过多学科合作和社区参与，项目构建了一个数字化档案，帮助传统社区应对气候变化。


<details>
  <summary>Details</summary>
Motivation: 东部海岸的乡村社区因气候变化面临生存威胁，项目旨在通过技术和社区知识的结合增强其气候韧性。

Method: 结合计算机科学、工业工程和海岸地理学的专业知识，与社区共同创建工具，并通过公民科学网络整合居民知识。

Result: 项目建立了一个数字化档案，并提供了一套可复制的技术模型，支持传统社区更有效地应对气候变化。

Conclusion: 该项目展示了技术如何与传统社区知识结合，为气候变化的适应提供有效支持。

Abstract: This paper presents an overview of a human-centered initiative aimed at
strengthening climate resilience along Nova Scotia's Eastern Shore. This
region, a collection of rural villages with deep ties to the sea, faces
existential threats from climate change that endanger its way of life. Our
project moves beyond a purely technical response, weaving together expertise
from Computer Science, Industrial Engineering, and Coastal Geography to
co-create tools with the community. By integrating generational knowledge of
residents, particularly elders, through the Eastern Shore Citizen Science
Coastal Monitoring Network, this project aims to collaborate in building a
living digital archive. This effort is hosted under Dalhousie University's
Transforming Climate Action (TCA) initiative, specifically through its
Transformative Adaptations to Social-Ecological Climate Change Trajectories
(TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is
driven by a collaboration model in which student teams work directly with
residents. We present a detailed project timeline and a replicable model for
how technology can support traditional communities, enabling them to navigate
climate transformation more effectively.

</details>


### [93] [E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI](https://arxiv.org/abs/2509.02100)
*Sharjeel Tahir,Judith Johnson,Jumana Abu-Khalaf,Syed Afaq Ali Shah*

Main category: cs.HC

TL;DR: 该论文提出了E-THER数据集，旨在解决现有AI系统在共情能力上的不足，通过多维度标注的言语-视觉不一致性检测，训练AI实现更真实的共情能力。


<details>
  <summary>Details</summary>
Motivation: 当前共情AI系统无法识别言语表达与情感状态的不一致，因为现有数据集仅关注表面情感识别，缺乏对言语-视觉不一致模式的关注。

Method: 提出基于以人为中心疗法的多模态数据集E-THER，包含言语-视觉不一致性标注，并利用该数据集训练视觉语言模型。

Result: 实验表明，基于不一致性训练的模型在共情和对话质量上优于通用模型，尤其在维持治疗参与度和减少虚假语言模式方面表现突出。

Conclusion: E-THER数据集为AI系统的共情能力提供了新框架，验证了言语-视觉不一致性检测对提升AI共情效果的重要性。

Abstract: A prevalent shortfall among current empathic AI systems is their inability to
recognize when verbal expressions may not fully reflect underlying emotional
states. This is because the existing datasets, used for the training of these
systems, focus on surface-level emotion recognition without addressing the
complex verbal-visual incongruence (mismatch) patterns useful for empathic
understanding. In this paper, we present E-THER, the first Person-Centered
Therapy-grounded multimodal dataset with multidimensional annotations for
verbal-visual incongruence detection, enabling training of AI systems that
develop genuine rather than performative empathic capabilities. The annotations
included in the dataset are drawn from humanistic approach, i.e., identifying
verbal-visual emotional misalignment in client-counsellor interactions -
forming a framework for training and evaluating AI on empathy tasks. Additional
engagement scores provide behavioral annotations for research applications.
Notable gains in empathic and therapeutic conversational qualities are observed
in state-of-the-art vision-language models (VLMs), such as IDEFICS and
VideoLLAVA, using evaluation metrics grounded in empathic and therapeutic
principles. Empirical findings indicate that our incongruence-trained models
outperform general-purpose models in critical traits, such as sustaining
therapeutic engagement, minimizing artificial or exaggerated linguistic
patterns, and maintaining fidelity to PCT theoretical framework.

</details>


### [94] [Shared Control for Game Accessibility: Understanding Current Human Cooperation Practices to Inform the Design of Partial Automation Solutions](https://arxiv.org/abs/2509.02132)
*Dragan Ahmetovic,Matteo Manzoni,Filippo Corti,Sergio Mascetti*

Main category: cs.HC

TL;DR: 研究探讨了共享控制在残疾玩家游戏中的实践应用及其自动化需求。


<details>
  <summary>Details</summary>
Motivation: 共享控制技术帮助残疾玩家解决游戏操作障碍，但依赖人力支持是主要限制，研究旨在探索如何通过自动化改进。

Method: 通过14位残疾玩家的访谈，分析共享控制的使用实践、挑战及自动化设计需求。

Result: 共享控制对游戏可访问性至关重要，但自动化支持是未来发展方向，需满足特定设计需求。

Conclusion: 研究为开发自动化共享控制系统提供了实践洞察和设计指南。

Abstract: Shared control is a form of video gaming accessibility support that allows
players with disabilities to delegate inaccessible controls to another person.
Through interviews involving 14 individuals with lived experience of accessible
gaming in shared control, we explore the ways in which shared control
technologies are adopted in practice, the accessibility challenges they
address, and how the support currently provided in shared control can be
automated to remove the need for a human assistant. Findings indicate that
shared control is essential for enabling access to otherwise inaccessible
games, but its reliance on human support is a key limitation. Participants
welcomed the idea of automating the support with software agents, while also
identifying limitations and design requirements. Accordingly, this work
contributes insights into current practices and proposes guidelines for
developing automated support systems.

</details>


### [95] [A Theoretical Framework of the Processes of Change in Psychotherapy Delivered by Artificial Agents](https://arxiv.org/abs/2509.02144)
*Arthur Bran Herbener,Malene Flensborg Damholdt*

Main category: cs.HC

TL;DR: 本文提出了首个关于由人工智能代理提供的心理治疗变化过程的理论框架，探讨了人类治疗师与AI代理在治疗过程中的差异及其影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的推出，人工智能代理是否能替代人类治疗师引发关注，但目前缺乏对AI代理提供心理治疗变化过程的理解。

Method: 通过概念分析，研究了人类治疗师的某些特质（如本体论和社会文化地位）如何影响治疗效果，并提出了“真诚性差距”和“可信性差距”的概念。

Result: 研究发现，缺乏人类治疗师的本体论和社会文化地位可能导致治疗效果的关键变化过程受阻。

Conclusion: 文章提出了未来科学研究和实践应用的方向，强调结合AI代理和人类治疗师的各自优势，并讨论了AI代理的复杂性对普遍适用性命题的挑战。

Abstract: The question of whether artificial agents (e.g., chatbots and social robots)
can replace human therapists has received notable attention following the
recent launch of large language models. However, little is known about the
processes of change in psychotherapy delivered by artificial agents. To
facilitate hypothesis development and stimulate scientific debate, the present
article offers the first theoretical framework of the processes of change in
psychotherapy delivered by artificial agents. The theoretical framework rests
upon a conceptual analysis of what active ingredients may be inherently linked
to the presence of human therapists. We propose that human therapists'
ontological status as human beings and sociocultural status as socially
sanctioned healthcare professionals play crucial roles in promoting treatment
outcomes. In the absence of the ontological and sociocultural status of human
therapists, we propose what we coin the genuineness gap and credibility gap can
emerge and undermine key processes of change in psychotherapy. Based on these
propositions, we propose avenues for scientific investigations and practical
applications aimed at leveraging the strengths of artificial agents and human
therapists respectively. We also highlight the intricate agentic nature of
artificial agents and discuss how this complicates endeavors to establish
universally applicable propositions regarding the processes of change in these
interventions.

</details>


### [96] [Look: AI at Work! - Analysing Key Aspects of AI-support at the Work Place](https://arxiv.org/abs/2509.02274)
*Stefan Schiffer,Anna Milena Rothermel,Alexander Ferrein,Astrid Rosenthal-von der Pütten*

Main category: cs.HC

TL;DR: 本文分析了在工作场所应用人工智能（AI）的技术和心理因素，基于12个案例研究，提出了开发AI应用和提升AI素养的建议，并探讨了AI支持的未来工作系统中需研究的心理问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨AI在工作场所应用时涉及的技术挑战（如高质量数据和人类专业知识整合）及心理因素（如接受度与信任），为未来工作系统设计提供参考。

Method: 通过12个案例研究，从技术和心理两个角度分析AI在工作场所的应用，重点关注AI相关技术和心理影响因素。

Result: 总结了AI应用开发中的关键技术和AI素养提升建议，并提出了需进一步研究的心理问题（如接受度、开放性和信任）。

Conclusion: 研究为AI在工作场所的集成提供了技术和心理层面的实用建议，强调了数据质量、人类参与和心理因素的重要性。

Abstract: In this paper we present an analysis of technological and psychological
factors of applying artificial intelligence (AI) at the work place. We do so
for a number of twelve application cases in the context of a project where AI
is integrated at work places and in work systems of the future. From a
technological point of view we mainly look at the areas of AI that the
applications are concerned with. This allows to formulate recommendations in
terms of what to look at in developing an AI application and what to pay
attention to with regards to building AI literacy with different stakeholders
using the system. This includes the importance of high-quality data for
training learning-based systems as well as the integration of human expertise,
especially with knowledge-based systems. In terms of the psychological factors
we derive research questions to investigate in the development of AI supported
work systems and to consider in future work, mainly concerned with topics such
as acceptance, openness, and trust in an AI system.

</details>


### [97] [Balaton Borders: Data Ceramics for Ecological Reflection](https://arxiv.org/abs/2509.02284)
*Hajnal Gyeviki,Mihály Minkó,Mary Karyda,Damla Çay*

Main category: cs.HC

TL;DR: 将生态数据转化为陶瓷餐具，通过共享饮食体验引发对生态破坏的反思。


<details>
  <summary>Details</summary>
Motivation: 通过艺术与生态数据的结合，唤起人们对人类活动对自然景观影响的关注。

Method: 将Lake Balaton的生态数据（如芦苇减少、海岸线变化等）转化为陶瓷餐具，并在共享餐饮中使用。

Result: 创造出一种多感官体验，促进对生态破坏的集体反思。

Conclusion: 艺术与生态数据的结合能有效引发公众对环境问题的关注和讨论。

Abstract: Balaton Borders translates ecological data from Lake Balaton into ceramic
tableware that represents human impact on the landscape, from reedbed reduction
to shoreline modification and land erosion. Designed for performative dining,
the pieces turn shared meals into multisensory encounters where food and data
ceramics spark collective reflection on ecological disruption.

</details>


### [98] [Talking Spell: A Wearable System Enabling Real-Time Anthropomorphic Voice Interaction with Everyday Objects](https://arxiv.org/abs/2509.02367)
*Xuetong Wang,Ching Christie Pang,Pan Hui*

Main category: cs.HC

TL;DR: Talking Spell是一种可穿戴系统，通过计算机视觉和大语言模型技术，赋予日常物品语音和拟人化特征，帮助用户与物品建立情感连接。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智能伴侣通常嵌入特定物品中，用户需与陌生物品建立情感联系，导致参与度降低和疏离感。Talking Spell旨在解决这一问题。

Method: 利用YOLOv11进行物体检测，QWEN-VL生成拟人化角色，结合语音技术，系统通过三个阶段（认识、熟悉、绑定）引导用户建立情感连接。

Result: 用户研究表明，Talking Spell在娱乐、陪伴、实用和创造力四个方面有效促进了用户与物品的互动和情感联系。

Conclusion: Talking Spell通过个性化体验增强了用户与日常物品的情感互动，展示了其广泛的应用潜力。

Abstract: Virtual assistants (VAs) have become ubiquitous in daily life, integrated
into smartphones and smart devices, sparking interest in AI companions that
enhance user experiences and foster emotional connections. However, existing
companions are often embedded in specific objects-such as glasses, home
assistants, or dolls-requiring users to form emotional bonds with unfamiliar
items, which can lead to reduced engagement and feelings of detachment. To
address this, we introduce Talking Spell, a wearable system that empowers users
to imbue any everyday object with speech and anthropomorphic personas through a
user-centric radiative network. Leveraging advanced computer vision (e.g.,
YOLOv11 for object detection), large vision-language models (e.g., QWEN-VL for
persona generation), speech-to-text and text-to-speech technologies, Talking
Spell guides users through three stages of emotional connection: acquaintance,
familiarization, and bonding. We validated our system through a user study
involving 12 participants, utilizing Talking Spell to explore four interaction
intentions: entertainment, companionship, utility, and creativity. The results
demonstrate its effectiveness in fostering meaningful interactions and
emotional significance with everyday objects. Our findings indicate that
Talking Spell creates engaging and personalized experiences, as demonstrated
through various devices, ranging from accessories to essential wearables.

</details>


### [99] [Octo's Heartland: Supporting Children with Congenital Heart Disease through Digital Health Education](https://arxiv.org/abs/2509.02537)
*Irene Zeng,Neda Barbazi,Ji Youn Shin,Gurumurthy Hiremath,Carlye Anne Lauff*

Main category: cs.HC

TL;DR: 研究探讨了利用隐喻游戏设计数字应用，帮助先天性心脏病儿童理解医学信息，并通过用户测试优化设计。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病儿童需要从早期理解复杂医学信息，但目前缺乏针对他们的数字健康工具设计。

Method: 采用参与式设计（PD）和用户测试，开发了一款基于隐喻游戏的数字应用，并通过30名儿童测试优化。

Result: 应用在可用性、参与度和理解度方面表现良好，展现了通过严肃游戏提升健康素养的潜力。

Conclusion: 研究结果支持进一步测试和优化，以推广至家庭和临床环境。

Abstract: Children with congenital heart disease (CHD) often face challenges that
require them to understand complex medical information from an early age in
order to support lifelong care and improve health outcomes. However, prior
research has rarely included young children in designing and evaluating digital
tools to support health education using developmentally appropriate strategies.
This study is part of a multi-phase research involving participatory design
(PD), user testing, and iterative development. We present the design and
refinement of a digital application that introduces basic information about
CHD, including heart anatomy and healthy habits, through metaphor-based
gameplay. User testing sessions with 30 children informed the redesign of
interactive activities aligned with specific health conditions. Findings
highlight usability, engagement, and comprehension outcomes and reveal design
opportunities for supporting health literacy through serious game (SG)
principles. These results inform the next phase, including further testing,
refinement, and deployment in home and clinical settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [100] [Curve-based slicer for multi-axis DLP 3D printing](https://arxiv.org/abs/2509.00040)
*Chengkai Dai,Tao Liu,Dezhao Guo,Binzhi Sun,Guoxin Fang,Yeung Yam,Charlie C. L. Wang*

Main category: cs.GR

TL;DR: 提出了一种基于曲线的切片方法，用于数字光处理（DLP）3D打印中动态变化方向的平面层生成，解决了大悬垂区域和阶梯伪影等问题。


<details>
  <summary>Details</summary>
Motivation: 解决DLP打印中大悬垂区域和阶梯伪影的挑战，同时保持高分辨率和快速打印的优势。

Method: 将切片问题建模为优化任务，通过参数曲线定义切片层和模型分区，优化以实现无碰撞运动和浮动沉积。

Result: 在机器人多轴DLP打印装置上进行了物理实验，验证了优化曲线能够稳健地指导高质量复杂几何体的打印。

Conclusion: 该方法通过优化曲线实现了高质量、平滑的DLP打印，适用于复杂几何体的制造。

Abstract: This paper introduces a novel curve-based slicing method for generating
planar layers with dynamically varying orientations in digital light processing
(DLP) 3D printing. Our approach effectively addresses key challenges in DLP
printing, such as regions with large overhangs and staircase artifacts, while
preserving its intrinsic advantages of high resolution and fast printing
speeds. We formulate the slicing problem as an optimization task, in which
parametric curves are computed to define both the slicing layers and the model
partitioning through their tangent planes. These curves inherently define
motion trajectories for the build platform and can be optimized to meet
critical manufacturing objectives, including collision-free motion and
floating-free deposition. We validate our method through physical experiments
on a robotic multi-axis DLP printing setup, demonstrating that the optimized
curves can robustly guide smooth, high-quality fabrication of complex
geometries.

</details>


### [101] [Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation](https://arxiv.org/abs/2509.00052)
*Jianzhi Long,Wenhao Sun,Rongcheng Tu,Dacheng Tao*

Main category: cs.GR

TL;DR: 针对扩散式说话头模型的推理速度慢问题，本文提出了一种任务专用框架，通过缓存静态特征并行预测及优化注意力计算，显著提升了推理速度且不影响视频质量。


<details>
  <summary>Details</summary>
Motivation: 扩散基说话头模型生成高质量视频但推理速度慢，现有通用加速方法未能利用其时空冗余特性。

Method: 提出LightningCP缓存静态特征并行预测，以及DFA优化注意力计算，专注于动态前景区域。

Result: 实验表明，该框架显著加快推理速度并保持视频质量。

Conclusion: 该方法高效解决了扩散式说话头模型的推理瓶颈，具有实际应用潜力。

Abstract: Diffusion-based talking head models generate high-quality, photorealistic
videos but suffer from slow inference, limiting practical applications.
Existing acceleration methods for general diffusion models fail to exploit the
temporal and spatial redundancies unique to talking head generation. In this
paper, we propose a task-specific framework addressing these inefficiencies
through two key innovations. First, we introduce Lightning-fast Caching-based
Parallel denoising prediction (LightningCP), caching static features to bypass
most model layers in inference time. We also enable parallel prediction using
cached features and estimated noisy latents as inputs, efficiently bypassing
sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to
further accelerate attention computations, exploiting the spatial decoupling in
talking head videos to restrict attention to dynamic foreground regions.
Additionally, we remove reference features in certain layers to bring extra
speedup. Extensive experiments demonstrate that our framework significantly
improves inference speed while preserving video quality.

</details>


### [102] [Evaluate Neighbor Search for Curve-based Vector Field Processing](https://arxiv.org/abs/2509.00180)
*Nguyen Phan,Guoning Chen*

Main category: cs.GR

TL;DR: 研究评估了两种邻近搜索策略在不同距离度量下的表现，用于基于点的向量场重建和段显著性估计，提出了多种衡量指标，并揭示了被忽视的发现。


<details>
  <summary>Details</summary>
Motivation: 由于积分曲线可能无法完全代表原始流动行为且分布不均，现有邻近搜索策略常返回偏斜和冗余结果，缺乏系统性研究。

Method: 结合不同距离度量，评估两种邻近搜索策略在向量场重建和显著性估计中的表现，提出平均距离和均匀性等衡量指标。

Result: 部分证实了理想邻近配置的预期，同时揭示了社区忽视的新发现。

Conclusion: 研究填补了邻近搜索策略配置影响的系统性研究空白，并为后续任务提供了参考。

Abstract: Curve-based representations, particularly integral curves, are often used to
represent large-scale computational fluid dynamic simulations. Processing and
analyzing curve-based vector field data sets often involves searching for
neighboring segments given a query point or curve segment. However, because the
original flow behavior may not be fully represented by the set of integral
curves and the input integral curves may not be evenly distributed in space,
popular neighbor search strategies often return skewed and redundant
neighboring segments. Yet, there is a lack of systematic and comprehensive
research on how different configurations of neighboring segments returned by
specific neighbor search strategies affect subsequent tasks. To fill this gap,
this study evaluates the performance of two popular neighbor search strategies
combined with different distance metrics on a point-based vector field
reconstruction task and a segment saliency estimation using input integral
curves. A large number of reconstruction tests and saliency calculations are
conducted for the study. To characterize the configurations of neighboring
segments for an effective comparison of different search strategies, a number
of measures, like average neighbor distance and uniformity, are proposed. Our
study leads to a few observations that partially confirm our expectations about
the ideal configurations of a neighborhood while revealing additional findings
that were overlooked by the community.

</details>


### [103] [3D-LATTE: Latent Space 3D Editing from Textual Instructions](https://arxiv.org/abs/2509.00269)
*Maria Parelli,Michael Oechsle,Michael Niemeyer,Federico Tombari,Andreas Geiger*

Main category: cs.GR

TL;DR: 提出了一种基于原生3D扩散模型的训练免费编辑方法，通过3D注意力图和几何感知正则化等方法，实现了高质量的3D资产编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D先验的3D资产编辑方法存在视角不一致的问题，导致编辑质量较低。

Method: 在原生3D扩散模型的潜在空间中操作，通过3D注意力图混合、几何感知正则化、傅里叶域谱调制和3D增强细化步骤实现编辑。

Result: 方法在多种形状和语义操作中表现出色，优于现有3D编辑方法。

Conclusion: 该方法实现了高保真、精确且鲁棒的3D编辑，解决了视角不一致问题。

Abstract: Despite the recent success of multi-view diffusion models for
text/image-based 3D asset generation, instruction-based editing of 3D assets
lacks surprisingly far behind the quality of generation models. The main reason
is that recent approaches using 2D priors suffer from view-inconsistent editing
signals. Going beyond 2D prior distillation methods and multi-view editing
strategies, we propose a training-free editing method that operates within the
latent space of a native 3D diffusion model, allowing us to directly manipulate
3D geometry. We guide the edit synthesis by blending 3D attention maps from the
generation with the source object. Coupled with geometry-aware regularization
guidance, a spectral modulation strategy in the Fourier domain and a refinement
step for 3D enhancement, our method outperforms previous 3D editing methods
enabling high-fidelity, precise, and robust edits across a wide range of shapes
and semantic manipulations.

</details>


### [104] [Quantum Brush: A quantum computing-based tool for digital painting](https://arxiv.org/abs/2509.01442)
*João S. Ferreira,Arianna Crippa,Astryd Park,Daniel Bultrini,Pierre Fromholz,Roman Lipski,Karl Jansen,James R. Wootton*

Main category: cs.GR

TL;DR: Quantum Brush是一款开源数字绘画工具，利用量子计算生成新颖艺术表达，包含四种不同画笔，将笔画转化为独特的量子算法，展示量子效应如何创造新美学，兼容当前NISQ设备。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在艺术表达中的应用，通过开发工具展示量子效应如何产生独特美学效果。

Method: 开发Quantum Brush工具，包含四种画笔，将绘画动作转化为量子算法，并在IQM的Sirius设备上执行。

Result: 工具成功展示了量子效应在艺术创作中的潜力，并在现有NISQ设备上实现。

Conclusion: Quantum Brush为量子计算与艺术的结合提供了新工具和可能性，展示了量子效应在美学领域的潜力。

Abstract: We present Quantum Brush, an open-source digital painting tool that harnesses
quantum computing to generate novel artistic expressions. The tool includes
four different brushes that translate strokes into unique quantum algorithms,
each highlighting a different way in which quantum effects can produce novel
aesthetics. Each brush is designed to be compatible with the current noisy
intermediate-scale quantum (NISQ) devices, as demonstrated by executing them on
IQM's Sirius device.

</details>


### [105] [Locality-Aware Automatic Differentiation on the GPU for Mesh-Based Computations](https://arxiv.org/abs/2509.00406)
*Ahmed H. Mahmoud,Jonathan Ragan-Kelley,Justin Solomon*

Main category: cs.GR

TL;DR: 这篇论文介绍了一种高性能的自动微分系统，特别适用于三角形网格上的函数，利用GPU实现快速梯度和Hessian计算。


<details>
  <summary>Details</summary>
Motivation: 传统反向模式自动微分方法在处理网格能量函数时存在全局计算图和同步的开销，本研究旨在通过局部计算和GPU优化提升计算效率。

Method: 采用基于元素的前向模式自动微分，所有局部计算保持在GPU寄存器或共享内存中，避免全局同步和内存频繁访问。

Result: 在二阶导数计算中，比优化后的PyTorch实现快6.2倍；Hessian向量积计算快2.76倍；一阶导数计算也显著优于其他工具。

Conclusion: 该系统在保持与手写导数相当性能的同时，显著提升了自动微分在网格计算中的效率。

Abstract: We present a high-performance system for automatic differentiation (AD) of
functions defined on triangle meshes that exploits the inherent sparsity and
locality of mesh-based energy functions to achieve fast gradient and Hessian
computation on the GPU. Our system is designed around per-element forward-mode
differentiation, enabling all local computations to remain in GPU registers or
shared memory. Unlike reverse-mode approaches that construct and traverse
global computation graphs, our method performs differentiation on the fly,
minimizing memory traffic and avoiding global synchronization. Our programming
model allows users to define local energy terms while the system handles
parallel evaluation, derivative computation, and sparse Hessian assembly. We
benchmark our system on a range of applications--cloth simulation, surface
parameterization, mesh smoothing, and spherical manifold optimization. We
achieve a geometric mean speedup of 6.2x over optimized PyTorch implementations
for second-order derivatives, and 2.76x speedup for Hessian-vector products.
For first-order derivatives, our system is 6.38x, 2.89x, and 1.98x faster than
Warp, JAX, and Dr.JIT, respectively, while remaining on par with hand-written
derivatives.

</details>


### [106] [LatentEdit: Adaptive Latent Control for Consistent Semantic Editing](https://arxiv.org/abs/2509.00541)
*Siyi Liu,Weiming Chen,Yushun Tang,Zhihai He*

Main category: cs.GR

TL;DR: LatentEdit是一种自适应的潜在融合框架，通过动态结合当前潜在代码和参考潜在代码，在保持背景相似性的同时实现高质量的图像编辑。其无需模型修改或复杂注意力机制，轻量且兼容多种架构。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散基图像编辑方法在保持背景相似性和高效性方面的挑战，提供更精细可控的编辑能力。

Method: 通过自适应潜在融合框架动态结合当前和参考潜在代码，选择性保留高相似度区域的源特征，并生成目标内容。

Result: 在PIE-Bench数据集上表现优异，平衡了保真度和可编辑性，超越现有方法且在8-15步内高效完成。

Conclusion: LatentEdit提供了一种轻量级、即插即用的高效图像编辑解决方案，适用于实时部署。

Abstract: Diffusion-based Image Editing has achieved significant success in recent
years. However, it remains challenging to achieve high-quality image editing
while maintaining the background similarity without sacrificing speed or memory
efficiency. In this work, we introduce LatentEdit, an adaptive latent fusion
framework that dynamically combines the current latent code with a reference
latent code inverted from the source image. By selectively preserving source
features in high-similarity, semantically important regions while generating
target content in other regions guided by the target prompt, LatentEdit enables
fine-grained, controllable editing. Critically, the method requires no internal
model modifications or complex attention mechanisms, offering a lightweight,
plug-and-play solution compatible with both UNet-based and DiT-based
architectures. Extensive experiments on the PIE-Bench dataset demonstrate that
our proposed LatentEdit achieves an optimal balance between fidelity and
editability, outperforming the state-of-the-art method even in 8-15 steps.
Additionally, its inversion-free variant further halves the number of neural
function evaluations and eliminates the need for storing any intermediate
variables, substantially enhancing real-time deployment efficiency.

</details>


### [107] [IntrinsicReal: Adapting IntrinsicAnything from Synthetic to Real Objects](https://arxiv.org/abs/2509.00777)
*Xiaokang Wei,Zizheng Yan,Zhangyang Xiong,Yiming Hao,Yipeng Qin,Xiaoguang Han*

Main category: cs.GR

TL;DR: IntrinsicReal 是一个新的领域自适应框架，通过双伪标签策略将 IntrinsicAnything 适应于真实世界，显著提升了真实世界图像的反射率估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在合成数据上训练并直接应用于真实世界图像，忽略了合成与真实数据的领域差异，导致泛化性能不足。

Method: 提出 IntrinsicReal，使用双伪标签策略（绝对置信阈值和相对偏好排序）对 IntrinsicAnything 进行微调。

Result: 在合成和真实数据集上实现了最先进的反射率估计性能。

Conclusion: IntrinsicReal 成功弥合了合成与真实数据的领域差距，提升了真实世界图像的内在图像分解性能。

Abstract: Estimating albedo (a.k.a., intrinsic image decomposition) from single RGB
images captured in real-world environments (e.g., the MVImgNet dataset)
presents a significant challenge due to the absence of paired images and their
ground truth albedos. Therefore, while recent methods (e.g., IntrinsicAnything)
have achieved breakthroughs by harnessing powerful diffusion priors, they
remain predominantly trained on large-scale synthetic datasets (e.g.,
Objaverse) and applied directly to real-world RGB images, which ignores the
large domain gap between synthetic and real-world data and leads to suboptimal
generalization performance. In this work, we address this gap by proposing
IntrinsicReal, a novel domain adaptation framework that bridges the
above-mentioned domain gap for real-world intrinsic image decomposition.
Specifically, our IntrinsicReal adapts IntrinsicAnything to the real domain by
fine-tuning it using its high-quality output albedos selected by a novel dual
pseudo-labeling strategy: i) pseudo-labeling with an absolute confidence
threshold on classifier predictions, and ii) pseudo-labeling using the relative
preference ranking of classifier predictions for individual input objects. This
strategy is inspired by human evaluation, where identifying the highest-quality
outputs is straightforward, but absolute scores become less reliable for
sub-optimal cases. In these situations, relative comparisons of outputs become
more accurate. To implement this, we propose a novel two-phase pipeline that
sequentially applies these pseudo-labeling techniques to effectively adapt
IntrinsicAnything to the real domain. Experimental results show that our
IntrinsicReal significantly outperforms existing methods, achieving
state-of-the-art results for albedo estimation on both synthetic and real-world
datasets.

</details>


### [108] [RealMat: Realistic Materials with Diffusion and Reinforcement Learning](https://arxiv.org/abs/2509.01134)
*Xilong Zhou,Pedro Figueiredo,Miloš Hašan,Valentin Deschaintre,Paul Guerrero,Yiwei Hu,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: 提出一种基于扩散模型的材质生成方法RealMat，结合合成数据与真实照片，通过强化学习提高生成材质的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有材质生成方法主要依赖合成数据，存在与真实材质的视觉差距；而小规模真实照片数据缺乏多样性和规模。为此，提出RealMat以解决这一问题。

Method: 先微调预训练的Stable Diffusion XL（SDXL）处理合成材质网格，再通过强化学习进一步微调模型，利用大规模真实材质照片数据集设计真实感奖励函数。

Result: RealMat生成的材质比基础模型及相关工作更具真实感。

Conclusion: RealMat通过结合合成数据与真实照片的扩散模型，辅以强化学习优化，显著提升了生成材质的真实感。

Abstract: Generative models for high-quality materials are particularly desirable to
make 3D content authoring more accessible. However, the majority of material
generation methods are trained on synthetic data. Synthetic data provides
precise supervision for material maps, which is convenient but also tends to
create a significant visual gap with real-world materials. Alternatively,
recent work used a small dataset of real flash photographs to guarantee
realism, however such data is limited in scale and diversity. To address these
limitations, we propose RealMat, a diffusion-based material generator that
leverages realistic priors, including a text-to-image model and a dataset of
realistic material photos under natural lighting. In RealMat, we first finetune
a pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged
in $2 \times 2$ grids. This way, our model inherits some realism of SDXL while
learning the data distribution of the synthetic material grids. Still, this
creates a realism gap, with some generated materials appearing synthetic. We
propose to further finetune our model through reinforcement learning (RL),
encouraging the generation of realistic materials. We develop a realism reward
function for any material image under natural lighting, by collecting a
large-scale dataset of realistic material images. We show that this approach
increases generated materials' realism compared to our base model and related
work.

</details>


### [109] [HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices](https://arxiv.org/abs/2509.01839)
*Akis Nousias,Stavros Nousias*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Currently, prominent Transformer architectures applied on graphs and meshes
for shape analysis tasks employ traditional attention layers that heavily
utilize spectral features requiring costly eigenvalue decomposition-based
methods. To encode the mesh structure, these methods derive positional
embeddings, that heavily rely on eigenvalue decomposition based operations,
e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then
concatenated to the input features. This paper proposes a novel approach
inspired by the explicit construction of the Hodge Laplacian operator in
Discrete Exterior Calculus as a product of discrete Hodge operators and
exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust
the Transformer architecture in a novel deep learning layer that utilizes the
multi-head attention mechanism to approximate Hodge matrices $\star_0$,
$\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act
on mesh vertices, edges and faces. Our approach results in a
computationally-efficient architecture that achieves comparable performance in
mesh segmentation and classification tasks, through a direct learning
framework, while eliminating the need for costly eigenvalue decomposition
operations or complex preprocessing operations.

</details>


### [110] [GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals](https://arxiv.org/abs/2509.02141)
*Mohit Mendiratta,Mayur Deshmukh,Kartik Teotia,Vladislav Golyanik,Adam Kortylewski,Christian Theobalt*

Main category: cs.GR

TL;DR: GRMM是一种基于高斯3D可变形模型的全头模型，通过添加残差几何和外观组件来增强传统3DMM，实现高分辨率和细节的表现。它提供了实时渲染（75 FPS）和高保真度的表达能力，超越了现有方法。EXPRESS-50数据集支持了身份与表情的鲁棒解耦。


<details>
  <summary>Details</summary>
Motivation: 传统PCA基础的3DMM模型在分辨率、细节和真实感上受限；神经体积方法虽然提升了真实感但速度慢；而基于高斯溅射的面部模型依赖于3DMM先验，无法捕捉细粒度几何和表情。因此，需要一种既能实现高保真又能实时渲染的解决方案。

Method: GRMM通过残差几何和外观组件增强基础3DMM，使用粗解码器生成网格变形，细解码器表示高斯级别的外观，并利用轻量CNN增强渲染图像的真实感。EXPRESS-50数据集用于训练和解耦身份与表情。

Result: GRMM在单目3D人脸重建、新视角合成和表情迁移中表现优于现有方法，实现了高保真度和表情精确度，同时保持75 FPS的实时渲染性能。

Conclusion: GRMM结合了3DMM和高斯溅射的优点，实现了高分辨率、细节丰富的面部建模和实时渲染，为解决现有模型的局限性提供了有效方案。

Abstract: 3D Morphable Models (3DMMs) enable controllable facial geometry and
expression editing for reconstruction, animation, and AR/VR, but traditional
PCA-based mesh models are limited in resolution, detail, and photorealism.
Neural volumetric methods improve realism but remain too slow for interactive
use. Recent Gaussian Splatting (3DGS) based facial models achieve fast,
high-quality rendering but still depend solely on a mesh-based 3DMM prior for
expression control, limiting their ability to capture fine-grained geometry,
expressions, and full-head coverage. We introduce GRMM, the first full-head
Gaussian 3D morphable model that augments a base 3DMM with residual geometry
and appearance components, additive refinements that recover high-frequency
details such as wrinkles, fine skin texture, and hairline variations. GRMM
provides disentangled control through low-dimensional, interpretable parameters
(e.g., identity shape, facial expressions) while separately modelling residuals
that capture subject- and expression-specific detail beyond the base model's
capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders
represent per-Gaussian appearance, and a lightweight CNN refines rasterised
images for enhanced realism, all while maintaining 75 FPS real-time rendering.
To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first
dataset with 60 aligned expressions across 50 identities, enabling robust
disentanglement of identity and expression in Gaussian-based 3DMMs. Across
monocular 3D face reconstruction, novel-view synthesis, and expression
transfer, GRMM surpasses state-of-the-art methods in fidelity and expression
accuracy while delivering interactive real-time performance.

</details>


### [111] [Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation](https://arxiv.org/abs/2509.02278)
*Zikai Huang,Yihan Zhou,Xuemiao Xu,Cheng Xu,Xiaofen Xing,Jing Qin,Shengfeng He*

Main category: cs.GR

TL;DR: Think2Sing是一种基于扩散的框架，通过结合预训练的大型语言模型和歌词与声学条件，生成语义一致、时间连贯的3D头部动画，用于唱歌驱动的动画任务。


<details>
  <summary>Details</summary>
Motivation: 现有的语音驱动方法在唱歌动画中效果不佳，表现为过度简化、情感平淡和语义不一致。唱歌涉及更丰富的情感、动态韵律和歌词语义，需要更精细的面部运动合成。

Method: 提出Think2Sing框架，引入运动字幕作为辅助语义表示，通过唱歌链式推理和声学指导检索生成运动强度预测问题，从而精细控制面部区域和改进表达运动的建模。

Result: 大量实验表明，Think2Sing在真实性、表达力和情感保真度上优于现有方法，并支持灵活的动画编辑。

Conclusion: Think2Sing通过创新的运动字幕和语义建模，解决了唱歌驱动动画中语义和情感表达的挑战。

Abstract: Singing-driven 3D head animation is a challenging yet promising task with
applications in virtual avatars, entertainment, and education. Unlike speech,
singing involves richer emotional nuance, dynamic prosody, and lyric-based
semantics, requiring the synthesis of fine-grained, temporally coherent facial
motion. Existing speech-driven approaches often produce oversimplified,
emotionally flat, and semantically inconsistent results, which are insufficient
for singing animation. To address this, we propose Think2Sing, a
diffusion-based framework that leverages pretrained large language models to
generate semantically coherent and temporally consistent 3D head animations,
conditioned on both lyrics and acoustics. A key innovation is the introduction
of motion subtitles, an auxiliary semantic representation derived through a
novel Singing Chain-of-Thought reasoning process combined with acoustic-guided
retrieval. These subtitles contain precise timestamps and region-specific
motion descriptions, serving as interpretable motion priors. We frame the task
as a motion intensity prediction problem, enabling finer control over facial
regions and improving the modeling of expressive motion. To support this, we
create a multimodal singing dataset with synchronized video, acoustic
descriptors, and motion subtitles, enabling diverse and expressive motion
learning. Extensive experiments show that Think2Sing outperforms
state-of-the-art methods in realism, expressiveness, and emotional fidelity,
while also offering flexible, user-controllable animation editing.

</details>


### [112] [Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework](https://arxiv.org/abs/2509.02474)
*Nina Wiedemann,Sainan Liu,Quentin Leboutet,Katelyn Gao,Benjamin Ummenhofer,Michael Paulitsch,Kai Yuan*

Main category: cs.GR

TL;DR: 论文提出了一个统一评估框架，用于比较不同3D表示方法在重建和生成中的性能，重点关注质量、计算效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着文本和图像生成的快速发展，3D生成研究逐渐兴起，但3D表示方法多样且分散，缺乏统一评估标准。

Method: 开发了一个统一的评估框架，对比了多种3D表示方法（如体素网格、神经辐射场等），并从预处理、重建、压缩到生成进行了全面实验。

Result: 研究发现重建错误对整体性能影响显著，生成和重建需要联合评估。

Conclusion: 该框架为不同应用场景的3D模型选择提供了指导，有助于开发更鲁棒且专用的3D生成解决方案。

Abstract: Following rapid advancements in text and image generation, research has
increasingly shifted towards 3D generation. Unlike the well-established
pixel-based representation in images, 3D representations remain diverse and
fragmented, encompassing a wide variety of approaches such as voxel grids,
neural radiance fields, signed distance functions, point clouds, or octrees,
each offering distinct advantages and limitations. In this work, we present a
unified evaluation framework designed to assess the performance of 3D
representations in reconstruction and generation. We compare these
representations based on multiple criteria: quality, computational efficiency,
and generalization performance. Beyond standard model benchmarking, our
experiments aim to derive best practices over all steps involved in the 3D
generation pipeline, including preprocessing, mesh reconstruction, compression
with autoencoders, and generation. Our findings highlight that reconstruction
errors significantly impact overall performance, underscoring the need to
evaluate generation and reconstruction jointly. We provide insights that can
inform the selection of suitable 3D models for various applications,
facilitating the development of more robust and application-specific solutions
in 3D generation. The code for our framework is available at
https://github.com/isl-org/unifi3d.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [113] [IoT-based Noise Monitoring using Mobile Nodes for Smart Cities](https://arxiv.org/abs/2509.00979)
*Bhima Sankar Manthina,Shreyash Gujar,Sachin Chaudhari,Kavita Vemuri1,Shivam Chhirolya*

Main category: cs.ET

TL;DR: 论文提出了一种基于移动车辆的IoT噪声监测系统，通过机器学习校准传感器，并在实际移动环境中验证性能，结果表明随机森林回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 城市噪声污染对公共健康构成威胁，现有监测基础设施覆盖范围和适应性有限，需低成本、可扩展的解决方案。

Method: 使用低成本声音传感器和GPS模块收集噪声数据，并通过多种ML算法（如RFR）校准传感器。

Result: 移动校准中RFR表现最佳（R2=0.937），系统在印度海得拉巴部署，捕捉到43.6万数据点，显示噪声时空变化。

Conclusion: 该系统支持智能城市广泛部署IoT噪声监测网络，有效管理噪声污染。

Abstract: Urban noise pollution poses a significant threat to public health, yet
existing monitoring infrastructures offer limited spatial coverage and
adaptability. This paper presents a scalable, low-cost, IoT-based, real-time
environmental noise monitoring solution using mobile nodes (sensor nodes on a
moving vehicle). The system utilizes a low-cost sound sensor integrated with
GPS-enabled modules to collect geotagged noise data at one-second intervals.
The sound nodes are calibrated against a reference sound level meter in a
laboratory setting to ensure accuracy using various machine learning (ML)
algorithms, such as Simple Linear Regression (SLR), Multiple Linear Regression
(MLR), Polynomial Regression (PR), Segmented Regression (SR), Support Vector
Regression (SVR), Decision Tree (DT), and Random Forest Regression (RFR). While
laboratory calibration demonstrates high accuracy, it is shown that the
performance of the nodes degrades during data collection in a moving vehicle.
To address this, it is demonstrated that the calibration must be performed on
the IoT-based node based on the data collected in a moving environment along
with the reference device. Among the employed ML models, RFR achieved the best
performance with an R2 of 0.937 and RMSE of 1.09 for mobile calibration. The
system was deployed in Hyderabad, India, through three measurement campaigns
across 27 days, capturing 436,420 data points. Results highlight temporal and
spatial noise variations across weekdays, weekends, and during Diwali.
Incorporating vehicular velocity into the calibration significantly improves
accuracy. The proposed system demonstrates the potential for widespread
deployment of IoT-based noise sensing networks in smart cities, enabling
effective noise pollution management and urban planning.

</details>


### [114] [5-axis Multi-material Desktop Additive Manufacturing of Conformal Antennas](https://arxiv.org/abs/2509.01448)
*Ivan Revenga Riesco,Borut Lampret,Connor Myant,David Boyle*

Main category: cs.ET

TL;DR: 论文探讨了低成本5轴多材料3D打印技术制造复杂共形天线的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用多轴多材料3D打印技术，克服传统天线制造的局限性，提升阻抗匹配并降低成本。

Method: 使用定制化开源5轴桌面打印机和导电材料，制造了S波段贴片天线和超宽带天线，并与平面打印天线及电磁仿真结果对比。

Result: 结果表明该方法在阻抗匹配、制造时间和成本上具有优势，适用于复杂几何形状天线的快速原型制作。

Conclusion: 多轴多材料3D打印技术为复杂天线的制造提供了高效、低成本的解决方案。

Abstract: This paper describes the novel use of low-cost, 5-axis, multi-material
additive manufacturing to fabricate functional, complex conformal antennas.
Using a customised open source 5-axis desktop printer incorporating conductive
filaments, conformal S-band patch and Ultra-Wide Band antennas were fabricated
and compared against planar-printed counterparts and electromagnetic
simulations. Results show the potential of the approach for superior impedance
matching, reduced fabrication time, and cost savings; highlighting the
applicability of multi-axis multi-material prototyping of antennas with complex
geometries.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [115] [KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache](https://arxiv.org/abs/2509.00579)
*Bo Jiang,Taolue Yang,Youyuan Liu,Chengming Zhang,Xubin He,Sian Jin*

Main category: cs.DC

TL;DR: KVComp是一个高效的KV缓存管理框架，通过无损压缩技术减少内存需求，适用于长文本生成，保持高计算效率且不影响模型精度。


<details>
  <summary>Details</summary>
Motivation: 长上下文推断中，KV缓存的高内存需求是主要挑战，需优化以减少内存占用。

Method: 采用专为KV缓存设计的无损压缩技术，结合压缩算法和系统架构的协同设计。

Result: KVComp平均减少47%内存占用，最高达83%，无明显精度损失，且执行吞吐量高。

Conclusion: KVComp有效解决了长文本生成中的内存需求问题，提升了性能和效率。

Abstract: Transformer-based large language models (LLMs) demonstrate impressive
potential in various practical applications. However, long context inference
poses a significant challenge due to the enormous memory requirements of the
key-value (KV) cache, which can scale to multiple gigabytes as sequence length
and batch size increase. In this paper, we present KVComp, a generic and
efficient KV cache management framework optimized for long-text generation that
synergistically works with both latency-critical and throughput-critical
inference systems. KVComp employs novel lossy compression techniques
specifically designed for KV cache data characteristics, featuring careful
co-design of compression algorithms and system architecture. Our approach
maintains compatibility with the growing nature of KV cache while preserving
high computational efficiency. Experimental results show that KVComp achieves
on average 47\% and up to 83\% higher memory reduction rate compared to
existing methods with little/no model accuracy degradation. Furthermore, KVComp
achieves extremely high execution throughput, effectively reducing
decompression overhead and, in some cases, even accelerating the matrix-vector
multiplication operation and outperform cuBLAS-based attention kernels with
less data movement.

</details>


### [116] [HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation](https://arxiv.org/abs/2509.00642)
*Qizheng Yang,Tung-I Chen,Siyu Zhao,Ramesh K. Sitaraman,Hui Guan*

Main category: cs.DC

TL;DR: HADIS是一个自适应扩散模型服务系统，通过优化级联模型选择、查询路由和资源分配，显著提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽视觉质量高，但计算成本大，难以实时扩展。现有系统固定级联配置，浪费资源。

Method: HADIS采用基于规则的路由器和离线分析，选择最优级联配置和GPU分配。

Result: HADIS提升了35%响应质量，降低了2.7-45倍的延迟违规率。

Conclusion: HADIS有效解决了高计算成本和资源浪费问题，优于现有系统。

Abstract: Text-to-image diffusion models have achieved remarkable visual quality but
incur high computational costs, making real-time, scalable deployment
challenging. Existing query-aware serving systems mitigate the cost by
cascading lightweight and heavyweight models, but most rely on a fixed cascade
configuration and route all prompts through an initial lightweight stage,
wasting resources on complex queries. We present HADIS, a hybrid adaptive
diffusion model serving system that jointly optimizes cascade model selection,
query routing, and resource allocation. HADIS employs a rule-based prompt
router to send clearly hard queries directly to heavyweight models, bypassing
the overhead of the lightweight stage. To reduce the complexity of resource
management, HADIS uses an offline profiling phase to produce a Pareto-optimal
cascade configuration table. At runtime, HADIS selects the best cascade
configuration and GPU allocation given latency and workload constraints.
Empirical evaluations on real-world traces demonstrate that HADIS improves
response quality by up to 35% while reducing latency violation rates by
2.7-45$\times$ compared to state-of-the-art model serving systems.

</details>


### [117] [Accelerating Latency-Critical Applications with AI-Powered Semi-Automatic Fine-Grained Parallelization on SMT Processors](https://arxiv.org/abs/2509.00883)
*Denis Los,Igor Petushkov*

Main category: cs.DC

TL;DR: 论文提出Aira，一个AI驱动的并行化顾问，用于在SMT核心上优化延迟关键应用的细粒度并行化，实现17%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 延迟关键应用在高性能超标量处理器中由于缓存未命中和预测错误导致功能单元利用率低，而SMT技术很少用于此类应用的粗粒度线程。

Method: 通过扩展Cursor IDE中的AI编码代理，结合模型上下文协议，开发了Aira。Aira具备LLM指导的热点检测、动态依赖收集和SMT感知性能模拟功能，并采用Relic框架实现细粒度任务并行。

Result: 实验表明，Aira结合Relic框架在延迟关键基准测试中实现了17%的几何平均性能提升。

Conclusion: Aira展示了AI技术在优化SMT核心上延迟关键应用并行化的潜力，显著提升性能。

Abstract: Latency-critical applications tend to show low utilization of functional
units due to frequent cache misses and mispredictions during speculative
execution in high-performance superscalar processors. However, due to
significant impact on single-thread performance, Simultaneous Multithreading
(SMT) technology is rarely used with heavy threads of latency-critical
applications. In this paper, we explore utilization of SMT technology to
support fine-grained parallelization of latency-critical applications.
Following the advancements in the development of Large Language Models (LLMs),
we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we
extend AI Coding Agent in Cursor IDE with additional tools connected through
Model Context Protocol, enabling end-to-end AI Agent for parallelization.
Additional connected tools enable LLM-guided hotspot detection, collection of
dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance
simulation to estimate performance gains. We apply Aira with Relic parallel
framework for fine-grained task parallelism on SMT cores to parallelize
latency-critical benchmarks representing real-world applications used in
industry. We show 17% geomean performance gain from parallelization of
latency-critical benchmarks using Aira with Relic framework.

</details>


### [118] [Parallelizing Drug Discovery: HPC Pipelines for Alzheimer's Molecular Docking and Simulation](https://arxiv.org/abs/2509.00937)
*Paul Ruiz Alliata,Diana Rubaga,Daniel Kumlin,Alberto Puliga*

Main category: cs.DC

TL;DR: 该论文探讨了高性能计算（HPC）如何通过虚拟筛选、分子对接和分子动力学模拟加速阿尔茨海默病药物发现，展示了并行化工作流程的优势及局限性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用HPC技术提高药物发现的效率，特别是在阿尔茨海默病等神经退行性疾病的治疗中，通过大规模分子模拟加速筛选和优化过程。

Method: 采用了基于GROMACS的混合MPI-OpenMP并行化工作流程，并开发了使用Python多进程库的分子对接原型，测试了在能量最小化、平衡和生产阶段的性能扩展。

Result: 结果表明，并行化显著提高了计算效率，尤其是在分子对接和动力学模拟中，但仍面临数据管理、计算成本和扩展效率的挑战。

Conclusion: HPC技术在神经退行性疾病药物发现中展现出巨大潜力，但需进一步优化以克服当前的技术和成本限制。

Abstract: High-performance computing (HPC) is reshaping computational drug discovery by
enabling large-scale, time-efficient molecular simulations. In this work, we
explore HPC-driven pipelines for Alzheimer's disease drug discovery, focusing
on virtual screening, molecular docking, and molecular dynamics simulations. We
implemented a parallelised workflow using GROMACS with hybrid MPI-OpenMP
strategies, benchmarking scaling performance across energy minimisation,
equilibration, and production stages. Additionally, we developed a docking
prototype that demonstrates significant runtime gains when moving from
sequential execution to process-based parallelism using Python's
multiprocessing library. Case studies on prolinamide derivatives and baicalein
highlight the biological relevance of these workflows in targeting amyloid-beta
and tau proteins. While limitations remain in data management, computational
costs, and scaling efficiency, our results underline the potential of HPC to
accelerate neurodegenerative drug discovery.

</details>


### [119] [DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving](https://arxiv.org/abs/2509.01083)
*Mingyu Yang,Jae-Young Choi,Kihyo Moon,Minsung Jang,Eunjoo Joen*

Main category: cs.DC

TL;DR: 动态推测解码引擎（DSDE）通过KL散度方差和自适应推测长度提升大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码固定推测长度的方式在大批量多样请求环境下表现不佳，需动态适应方案。

Method: 提出DSDE框架，包含基于KL散度方差的预测信号和自适应推测长度上限组件。

Result: 实验显示KLD稳定性信号有效，算法在延迟和鲁棒性上优于基线，特别适用于低接受率场景。

Conclusion: 事后信号为动态推测长度适应提供了新研究方向，有望构建更鲁棒的LLM推理系统。

Abstract: Speculative decoding accelerates large language model inference, but its
reliance on a fixed speculation length is suboptimal in large-batch serving
environments with diverse requests. This paper explores a new direction for
dynamic adaptation by investigating a novel class of post-hoc, diagnostic
signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free
framework built on two primary components: (1) a predictive signal based on the
variance of the Kullback-Leibler (KLD) divergence, which diagnoses the
generation's regional stability, and (2) an adaptive speculation length cap to
mitigate the straggler problem in per-sequence decoding. Experiments
demonstrate the potential of using KLD-based stability signals for dynamic
adaptation. An algorithm guided by these signals achieves end-to-end latency
competitive with leading baselines and exhibits superior robustness across
diverse workloads. This robustness is particularly valuable in challenging
low-acceptance-rate regimes, where the proposed signal maintains its diagnostic
utility. Collectively, these findings validate post-hoc signals as a valuable
component for building more robust and intelligent LLM inference systems, and
highlight a promising direction for future research on dynamic speculation
length adaptation.

</details>


### [120] [Ocior: Ultra-Fast Asynchronous Leaderless Consensus with Two-Round Finality, Linear Overhead, and Adaptive Security](https://arxiv.org/abs/2509.01118)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: Ocior是一种异步拜占庭容错共识协议，通过并行共识实例处理交易，达到在容错、通信、计算和轮复杂度上的最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统拜占庭容错共识协议依赖指定领导者，性能受限；Ocior旨在实现无领导者设计，确保稳定活力并优化各种性能指标。

Method: Ocior采用并行共识实例处理交易，引入新型非交互式阈值签名方案OciorBLSts，支持实时签名聚合。

Result: Ocior在容错（容忍最多t个故障节点）、通信（O(n)）、计算（O(n)或O(n log² n)）和轮复杂度（最优两轮）方面均达到最优。

Conclusion: Ocior通过无领导者设计和OciorBLSts签名方案，显著提升了异步拜占庭容错共识的性能和效率，适用于实时交易处理。

Abstract: In this work, we propose Ocior, a practical asynchronous Byzantine
fault-tolerant (BFT) consensus protocol that achieves the optimal performance
in resilience, communication, computation, and round complexity. Unlike
traditional BFT consensus protocols, Ocior processes incoming transactions
individually and concurrently using parallel instances of consensus. While
leader-based consensus protocols rely on a designated leader to propose
transactions, Ocior is a leaderless consensus protocol that guarantees stable
liveness. Ocior achieves: 1) Optimal resilience: Ocior tolerates up to $t$
faulty nodes controlled by an adaptive adversary, for $n\geq 3t+1$. 2) Optimal
communication complexity: The total expected communication per transaction is
$O(n)$. 3) Optimal (or near-optimal) computation complexity: The total
computation per transaction is $O(n)$ in the best case, or $O(n \log^2 n)$ in
the worst case. 4) Optimal round complexity: A legitimate two-party transaction
can be finalized with a good-case latency of two asynchronous rounds, for any
$n\geq 3t+1$. The good case in terms of latency refers to the scenario where
the transaction is proposed by any (not necessarily designated) honest node. A
two-party transaction involves the transfer of digital assets from one user (or
group of users) to one or more recipients. To support efficient consensus, we
introduce a novel non-interactive threshold signature (TS) scheme called
OciorBLSts. It offers fast signature aggregation, and is adaptively secure.
OciorBLSts achieves a signature aggregation computation cost of only $O(n)$ for
the best case. Moreover, OciorBLSts supports the property of Instantaneous TS
Aggregation. This enables real-time aggregation of partial signatures as they
arrive, reducing waiting time and improving responsiveness.

</details>


### [121] [Optimal Parallel Scheduling under Concave Speedup Functions](https://arxiv.org/abs/2509.01811)
*Chengzhang Li,Peizhong Ju,Atilla Eryilmaz,Ness Shroff*

Main category: cs.DC

TL;DR: 本文解决了在任意凹加速函数下并行作业的最优调度问题，提出了CDR规则和GWF方法，并设计了SmartFill算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代云计算系统中，如何高效分配资源以加速多个并行作业的完成是一个核心问题，但现有方法仅适用于特定加速函数，无法处理更普遍的凹加速函数。

Method: 提出了CDR规则作为调度基础，开发了GWF方法计算最优分配，并设计了SmartFill算法，根据作业需求动态选择资源分配策略。

Result: SmartFill算法在广泛的凹加速函数下显著优于heSRPT，能够提供封闭形式的解或高效计算最优解。

Conclusion: 本文通过CDR规则和SmartFill算法解决了通用凹加速函数下的调度问题，为实际系统提供了更灵活的解决方案。

Abstract: Efficient scheduling of parallel computation resources across multiple jobs
is a fundamental problem in modern cloud/edge computing systems for many
AI-based applications. Allocating more resources to a job accelerates its
completion, but with diminishing returns. Prior work (heSRPT) solved this
problem only for some specific speedup functions with an exponential form,
providing a closed-form solution. However, the general case with arbitrary
concave speedup functions -- which more accurately capture real-world workloads
-- has remained open.
  In this paper, we solve this open problem by developing optimal scheduling
algorithms for parallel jobs under general concave speedup functions. We first
discover a fundamental and broadly-applicable rule for optimal parallel
scheduling, namely the Consistent Derivative Ratio (CDR) Rule, which states
that the ratio of the derivatives of the speedup functions across active jobs
remains constant over time. To efficiently compute the optimal allocations that
satisfy the CDR Rule, we propose the General Water-Filling (GWF) method, a more
general version of classical water-filling in wireless communications.
Combining these insights, we design the SmartFill Algorithm to solve the
general scheduling problem. Unlike heSRPT, which always allocates resources to
all active jobs, SmartFill selectively determines which jobs should receive
resources and how much they should be allocated. For a broad class of so-called
\emph{regular} speedup functions, SmartFill yields closed-form optimal
solutions, while for non-regular functions it efficiently computes the optimum
with low complexity. Numerical evaluations show that SmartFill can
substantially outperform heSRPT across a wide range of concave speedup
functions.

</details>


### [122] [Detecting Rug Pulls in Decentralized Exchanges: Machine Learning Evidence from the TON Blockchain](https://arxiv.org/abs/2509.01168)
*Dmitry Yaremus,Jianghai Li,Alisa Kalacheva,Igor Vodolazov,Yury Yanovich*

Main category: cs.DC

TL;DR: 论文提出了一种机器学习框架，用于早期检测TON区块链上DEX的rug pull骗局，重点比较了两种定义方法（TVL和idle）的性能。


<details>
  <summary>Details</summary>
Motivation: TON区块链的独特架构和快速增长的使用环境，亟需一种有效的欺诈检测方法以保护投资者。

Method: 结合TVL和idle两种定义，训练梯度提升模型，并在TON的两大DEX（Ston.Fi和DeDust）上进行验证。

Result: TVL方法在AUC表现最佳（0.891），而idle方法在召回率上更优。数据分布差异显著，需平台感知模型。

Conclusion: 研究为投资者提供了早期预警机制，增强了TON DeFi生态的安全性。

Abstract: This paper presents a machine learning framework for the early detection of
rug pull scams on decentralized exchanges (DEXs) within The Open Network (TON)
blockchain. TON's unique architecture, characterized by asynchronous execution
and a massive web2 user base from Telegram, presents a novel and critical
environment for fraud analysis. We conduct a comprehensive study on the two
largest TON DEXs, Ston.Fi and DeDust, fusing data from both platforms to train
our models. A key contribution is the implementation and comparative analysis
of two distinct rug pull definitions--TVL-based (a catastrophic liquidity
withdrawal) and idle-based (a sudden cessation of all trading activity)--within
a single, unified study. We demonstrate that Gradient Boosting models can
effectively identify rug pulls within the first five minutes of trading, with
the TVL-based method achieving superior AUC (up to 0.891) while the idle-based
method excels at recall. Our analysis reveals that while feature sets are
consistent across exchanges, their underlying distributions differ
significantly, challenging straightforward data fusion and highlighting the
need for robust, platform-aware models. This work provides a crucial
early-warning mechanism for investors and enhances the security infrastructure
of the rapidly growing TON DeFi ecosystem.

</details>


### [123] [LobRA: Multi-tenant Fine-tuning over Heterogeneous Data](https://arxiv.org/abs/2509.01193)
*Sheng Lin,Fangcheng Fu,Haoyang Li,Hao Ge,Xuanyu Wang,Jiawen Niu,Yaofeng Tu,Bin Cui*

Main category: cs.DC

TL;DR: LobRA框架通过异构资源分配和数据调度，解决了多任务联合微调中的效率和资源利用率问题，显著降低了GPU时间消耗。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型微调需求的增加，如何高效处理多任务联合微调成为关键。LoRA技术虽能支持多任务，但因训练数据的异质性（序列长度差异和偏斜）导致效率低下。

Method: 提出了LobRA框架：1) 根据序列长度变化部署异构资源使用的微调副本；2) 根据序列长度偏斜动态调度训练数据以实现负载均衡。

Result: 实验证明，LobRA将联合微调的GPU时间减少了45.03%-60.67%。

Conclusion: LobRA通过针对性设计有效解决了多任务微调中的异质性问题，显著提升了效率，为服务提供商降低微调成本提供了可行方案。

Abstract: With the breakthrough of Transformer-based pre-trained models, the demand for
fine-tuning (FT) to adapt the base pre-trained models to downstream
applications continues to grow, so it is essential for service providers to
reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely
used FT technique that only trains small-scale adapters and keeps the base
model unaltered, conveying the possibility of processing multiple FT tasks by
jointly training different LoRA adapters with a shared base model.
  Nevertheless, through in-depth analysis, we reveal the efficiency of joint FT
is dampened by two heterogeneity issues in the training data -- the sequence
length variation and skewness. To tackle these issues, we develop LobRA, a
brand new framework that supports processing multiple FT tasks by jointly
training LoRA adapters. Two innovative designs are introduced. Firstly, LobRA
deploys the FT replicas (i.e., model replicas for FT) with heterogeneous
resource usages and parallel configurations, matching the diverse workloads
caused by the sequence length variation. Secondly, for each training step,
LobRA takes account of the sequence length skewness and dispatches the training
data among the heterogeneous FT replicas to achieve workload balance. We
conduct experiments to assess the performance of LobRA, validating that it
significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.

</details>


### [124] [LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM Serving](https://arxiv.org/abs/2509.01229)
*Huanqi Hu,Bowen Xiao,Shixuan Sun,Jianian Yin,Zhexi Zhang,Xiang Luo,Chengquan Jiang,Weiqi Xu,Xiaoying Jia,Xin Liu,Minyi Guo*

Main category: cs.DC

TL;DR: LiquidGEMM是一种高效的W4A8 GEMM核心，通过LiquidQuant和隐式细粒度管道技术，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有W4A8 GEMM核心因CUDA Core效率不足，无法充分利用Tensor Core高吞吐量，限制了量化加速的潜力。

Method: 设计了LiquidQuant硬件高效量化方法和隐式细粒度管道技术，实现无软件同步的权重加载、反量化和MMA重叠。

Result: LiquidGEMM比现有W4A8核心快2.90倍，系统级加速达4.94倍；在TensorRT-LLM中性能提升1.12-1.63倍。

Conclusion: LiquidGEMM通过硬件优化显著提升了量化计算效率，为高效LLM服务提供了新解决方案。

Abstract: Quantization is a critical technique for accelerating LLM inference by
reducing memory footprint and improving computational efficiency. Among various
schemes, 4-bit weight and 8-bit activation quantization (W4A8) offers a strong
balance between accuracy and performance. However, existing W4A8 GEMM kernels
fall short in practice due to inefficient dequantization on CUDA Cores, which
cannot keep pace with the high throughput of Tensor Cores. In this paper, we
present LiquidGEMM, a hardware-efficient W4A8 GEMM kernel for efficient LLM
serving. LiquidGEMM designs two key techniques: LiquidQuant, a
hardware-efficient quantization method that enables fast, overflow-safe
dequantization using just two arithmetic instructions per four elements; and an
implicit fine-grained pipeline that fully overlaps weight loading,
dequantization, and MMA across warp groups without software synchronization or
redundant memory traffic. Experimental results show that LiquidGEMM achieves up
to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x end-to-end
system-level speedup. Compared to various quantized GEMM kernels in NVIDIA
TensorRT-LLM, LiquidGEMM delivers 1.12-1.63x performance gains, and achieves up
to 1.63x system-level speedup.

</details>


### [125] [Safe Memory Reclamation Techniques](https://arxiv.org/abs/2509.02457)
*Ajay Singh*

Main category: cs.DC

TL;DR: 安全内存回收对非垃圾收集语言中的乐观和无锁并发数据结构至关重要，但设计理想算法面临速度、可扩展性、易用性、适配性等多重挑战。


<details>
  <summary>Details</summary>
Motivation: 在非垃圾收集语言中，安全内存回收对确保乐观和无锁并发数据结构的内存安全至关重要。

Method: 通过跨硬件软件栈的混合设计和工具利用，研究多种安全内存回收算法。

Result: 解决了算法设计中的多重挑战，包括高性能、易用性和广泛适用性。

Conclusion: 跨层解决方案可以有效应对安全内存回收的复杂挑战。

Abstract: Safe memory reclamation is crucial to memory safety for optimistic and
lock-free concurrent data structures in non garbage collected programming
languages. However, several challenges arise in designing an ideal safe memory
reclamation algorithm, including achieving high speed and scalability, easy of
use for programmers, applicability to wide class of data structures, managing
the large memory footprint caused by delayed freeing of memory for safety and
performance, and avoiding asymmetric overhead on data structure operations.
Several approaches to designing safe memory reclamation algorithms are studied
by blending ideas and tools from across the hardware-software stack. These
solutions cross traditional boundaries and exploit features exposed at
different layers.

</details>


### [126] [HiCR, an Abstract Model for Distributed Heterogeneous Programming](https://arxiv.org/abs/2509.01425)
*Sergio Miguel Martin,Luca Terracciano,Kiril Dichev,Noah Baumann,Jiashu Lin,Albert-Jan Yzelman*

Main category: cs.DC

TL;DR: HiCR是一种用于描述分布式异构应用和运行时系统语义的模型，通过抽象操作支持硬件拓扑发现、内核执行、内存管理等功能，旨在实现跨平台兼容性。


<details>
  <summary>Details</summary>
Motivation: 为分布式异构系统提供一种通用的运行时支持层，避免因硬件或编程范式变化而需要大规模重构。

Method: 采用最小抽象操作集和插件化方法实现设备特定的细节管理。

Result: HiCR模型能够在多种平台上无缝运行，支持不同编程范式。

Conclusion: HiCR作为运行时支持层，为分布式异构应用提供了灵活且可扩展的解决方案。

Abstract: We present HiCR, a model to represent the semantics of distributed
heterogeneous applications and runtime systems. The model describes a minimal
set of abstract operations to enable hardware topology discovery, kernel
execution, memory management, communication, and instance management, without
prescribing any implementation decisions. The goal of the model is to enable
execution in current and future systems without the need for significant
refactoring, while also being able to serve any governing parallel programming
paradigm. In terms of software abstraction, HiCR is naturally located between
distributed heterogeneous systems and runtime systems. We coin the phrase
\emph{Runtime Support Layer} for this level of abstraction. We explain how the
model's components and operations are realized by a plugin-based approach that
takes care of device-specific implementation details, and present examples of
HiCR-based applications that operate equally on a diversity of platforms.

</details>


### [127] [STZ: A High Quality and High Speed Streaming Lossy Compression Framework for Scientific Data](https://arxiv.org/abs/2509.01626)
*Daoce Wang,Pascal Grosset,Jesus Pulido,Jiannan Tian,Tushar M. Athawale,Jinda Jia,Baixi Sun,Boyuan Zhang,Sian Jin,Kai Zhao,James Ahrens,Fengguang Song*

Main category: cs.DC

TL;DR: 该论文提出了一种新颖的流式压缩框架，支持渐进式解压缩和随机访问解压缩，同时保持高质量的压缩速度和效率。


<details>
  <summary>Details</summary>
Motivation: 科学数据量大，现有的无损压缩方法在处理渐进式解压缩和随机访问解压缩时效果不佳。需要一种高效的支持这两种特性的压缩方法。

Method: 设计了一种支持渐进式和随机访问解压缩的压缩框架，采用分层分区策略和分层预测机制，提高压缩质量和速度。

Result: 提出的框架压缩质量与SZ3相当，压缩和解压缩速度比SZ3快6.7倍。

Conclusion: 该框架高效解决了流式压缩中的渐进式和随机访问解压缩问题，同时保持了高质量和速度。

Abstract: Error-bounded lossy compression is one of the most efficient solutions to
reduce the volume of scientific data. For lossy compression, progressive
decompression and random-access decompression are critical features that enable
on-demand data access and flexible analysis workflows. However, these features
can severely degrade compression quality and speed. To address these
limitations, we propose a novel streaming compression framework that supports
both progressive decompression and random-access decompression while
maintaining high compression quality and speed. Our contributions are
three-fold: (1) we design the first compression framework that simultaneously
enables both progressive decompression and random-access decompression; (2) we
introduce a hierarchical partitioning strategy to enable both streaming
features, along with a hierarchical prediction mechanism that mitigates the
impact of partitioning and achieves high compression quality -- even comparable
to state-of-the-art (SOTA) non-streaming compressor SZ3; and (3) our framework
delivers high compression and decompression speed, up to 6.7$\times$ faster
than SZ3.

</details>


### [128] [Energy-Efficient Split Learning for Resource-Constrained Environments: A Smart Farming Solution](https://arxiv.org/abs/2509.02549)
*Keiwan Soltani,Vishesh Kumar Tanwar,Ashish Gupta,Sajal K. Das*

Main category: cs.DC

TL;DR: eEnergy-Split是一个能效框架，通过分割学习（SL）减少边缘设备的能量消耗并保护数据隐私，相比联邦学习（FL）降低了86%的能量使用，同时提高了分类准确率。结合优化边缘部署和无人机轨迹规划，在农业害虫数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决智能农业系统中的资源限制、数据隐私需求和农村地区连接差等问题。

Method: 利用分割学习（SL）分配模型训练任务，结合边缘设备和中央服务器，提出边缘部署算法和无人机轨迹规划策略。

Result: 相比FL，能量消耗降低86%，分类准确率提升6.2%（ResNet-18），整体准确率最高提升17%。

Conclusion: eEnergy-Split通过SL和能量感知设计，为资源受限的智能农业环境提供了可扩展且隐私保护的解决方案。

Abstract: Smart farming systems encounter significant challenges, including limited
resources, the need for data privacy, and poor connectivity in rural areas. To
address these issues, we present eEnergy-Split, an energy-efficient framework
that utilizes split learning (SL) to enable collaborative model training
without direct data sharing or heavy computation on edge devices. By
distributing the model between edge devices and a central server, eEnergy-Split
reduces on-device energy usage by up to 86 percent compared to federated
learning (FL) while safeguarding data privacy. Moreover, SL improves
classification accuracy by up to 6.2 percent over FL on ResNet-18 and by more
modest amounts on GoogleNet and MobileNetV2. We propose an optimal edge
deployment algorithm and a UAV trajectory planning strategy that solves the
Traveling Salesman Problem (TSP) exactly to minimize flight cost and extend and
maximize communication rounds. Comprehensive evaluations on agricultural pest
datasets reveal that eEnergy-Split lowers UAV energy consumption compared to
baseline methods and boosts overall accuracy by up to 17 percent. Notably, the
energy efficiency of SL is shown to be model-dependent-yielding substantial
savings in lightweight models like MobileNet, while communication and memory
overheads may reduce efficiency gains in deeper networks. These results
highlight the potential of combining SL with energy-aware design to deliver a
scalable, privacy-preserving solution for resource-constrained smart farming
environments.

</details>


### [129] [A Continuous Energy Ising Machine Leveraging Difference-of-Convex Programming](https://arxiv.org/abs/2509.01928)
*Debraj Banerjee,Santanu Mahapatra,Kunal Narayan Chaudhury*

Main category: cs.DC

TL;DR: 该论文提出了一种新的伊辛问题求解方法，通过将二元自旋松弛为连续变量并引入势函数，设计出高效且具有收敛保证的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的伊辛求解器（如模拟退火法）缺乏收敛保证且对冷却计划敏感，因此需要一种更高效且可靠的替代方法。

Method: 将二元自旋松弛为连续变量，引入势函数引导解向二元自旋配置，利用凸差函数设计高效迭代算法。

Result: 在多种GPU平台上实现，性能优于现有求解器，适用于小规模（10^3自旋）到超大规模（10^8自旋）问题。

Conclusion: 该方法提供了高效、可扩展且具有收敛保证的伊辛问题求解方案。

Abstract: Many combinatorial optimization problems can be reformulated as the task of
finding the ground state of a physical system, such as the Ising model. Most
existing Ising solvers are inspired by simulated annealing. Although annealing
techniques offer scalability, they lack convergence guarantees and are
sensitive to the cooling schedule. We propose to solve the Ising problem by
relaxing the binary spins to continuous variables and introducing a potential
function (attractor) that steers the solution toward binary spin
configurations. The resulting Hamiltonian can be expressed as a difference of
convex functions, enabling the design of efficient iterative algorithms that
require a single matrix-vector multiplication per iteration and are backed by
convergence guarantees. We implement our Ising solver across a range of GPU
platforms: from edge devices to high-performance computing clusters and
demonstrate that it consistently outperforms existing solvers across problem
sizes ranging from small ($10^3$ spins) to ultra-large ($10^8$ spins).

</details>


### [130] [Fault-Tolerant Decentralized Distributed Asynchronous Federated Learning with Adaptive Termination Detection](https://arxiv.org/abs/2509.02186)
*Phani Sahasra Akkinepally,Manaswini Piduguralla,Sushant Joshi,Sathya Peri,Sandeep Kulkarni*

Main category: cs.DC

TL;DR: 该研究提出了一种异步去中心化的联邦学习框架，分为两个阶段：首先允许客户端独立学习和更新；其次引入容错机制，确保在不可预测条件下的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习依赖中心化服务器，可能产生瓶颈和单点故障；而去中心化异步联邦学习能提高可扩展性和响应性。

Method: 分两个阶段：第一阶段开发异步联邦学习框架；第二阶段引入容错机制和自主终止技术。

Result: 提出的方法确保了客户端的高效终止和可靠收敛，适应异步通信和故障。

Conclusion: 该框架在异构环境中表现优异，解决了传统联邦学习的局限性。

Abstract: Federated Learning (FL) facilitates collaborative model training across
distributed clients while ensuring data privacy. Traditionally, FL relies on a
centralized server to coordinate learning, which creates bottlenecks and a
single point of failure. Decentralized FL architectures eliminate the need for
a central server and can operate in either synchronous or asynchronous modes.
Synchronous FL requires all clients to compute updates and wait for one another
before aggregation, guaranteeing consistency but often suffering from delays
due to slower participants. Asynchronous FL addresses this by allowing clients
to update independently, offering better scalability and responsiveness in
heterogeneous environments.
  Our research develops an asynchronous decentralized FL approach in two
progressive phases. (a) In Phase 1, we develop an asynchronous FL framework
that enables clients to learn and update independently, removing the need for
strict synchronization. (b) In Phase 2, we extend this framework with fault
tolerance mechanisms to handle client failures and message drops, ensuring
robust performance even under unpredictable conditions. As a central
contribution, we propose Client-Confident Convergence and Client-Responsive
Termination novel techniques that provide each client with the ability to
autonomously determine appropriate termination points. These methods ensure
that all active clients conclude meaningfully and efficiently, maintaining
reliable convergence despite the challenges of asynchronous communication and
faults.

</details>


### [131] [Near-Optimal Stability for Distributed Transaction Processing in Blockchain Sharding](https://arxiv.org/abs/2509.02421)
*Ramesh Adhikari,Costas Busch,Dariusz R. Kowalski*

Main category: cs.DC

TL;DR: 本文研究了区块链分片系统中的稳定性问题，提出了单领导和多领导调度器，以在一定交易注入率下保证系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 确保区块链分片系统在任意交易模式下保持稳定是关键挑战，尤其是对抗高注入率和突发交易的情况。

Method: 设计了单领导和多领导调度器，分别在不同交易注入率下保证队列和延迟的有界性。

Result: 单领导调度器在注入率ρ≤1/(16k)或1/(16⌈√s⌉)时稳定，多领导调度器在ρ≤1/(16c₁logDlogs)max{1/k,1/⌈√s⌉}时稳定。

Conclusion: 多领导调度器的性能接近最优，显著优于已有结果，为分布式区块链分片提供了有效的稳定性保障。

Abstract: In blockchain sharding, $n$ processing nodes are divided into $s$ shards, and
each shard processes transactions in parallel. A key challenge in such a system
is to ensure system stability for any ``tractable'' pattern of generated
transactions; this is modeled by an adversary generating transactions with a
certain rate of at most $\rho$ and burstiness $b$. This model captures
worst-case scenarios and even some attacks on transactions' processing, e.g.,
DoS. A stable system ensures bounded transaction queue sizes and bounded
transaction latency. It is known that the absolute upper bound on the maximum
injection rate for which any scheduler could guarantee bounded queues and
latency of transactions is $\max\left\{ \frac{2}{k+1}, \frac{2}{
\left\lfloor\sqrt{2s}\right\rfloor}\right\}$, where $k$ is the maximum number
of shards that each transaction accesses. Here, we first provide a single
leader scheduler that guarantees stability under injection rate $\rho \leq
\max\left\{ \frac{1}{16k}, \frac{1}{16\lceil \sqrt{s} \rceil}\right\}$.
Moreover, we also give a distributed scheduler with multiple leaders that
guarantees stability under injection rate $\rho \leq \frac{1}{16c_1 \log D \log
s}\max\left\{ \frac{1}{k}, \frac{1}{\lceil \sqrt{s} \rceil} \right\}$, where
$c_1$ is some positive constant and $D$ is the diameter of shard graph $G_s$.
This bound is within a poly-log factor from the optimal injection rate, and
significantly improves the best previous known result for the distributed
setting by Adhikari et al., SPAA 2024.

</details>


### [132] [Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster](https://arxiv.org/abs/2509.02440)
*Marie Reinbigler,Rishi Sharma,Rafael Pires,Elisabeth Brunet,Anne-Marie Kermarrec,Catalin Fetita*

Main category: cs.DC

TL;DR: PyramidAI是一种降低计算成本的千兆像素图像分析方法，通过逐步分析图像分辨率并优化计算负载，显著减少数据处理量。


<details>
  <summary>Details</summary>
Motivation: 千兆像素图像分析通常计算量大，PyramidAI旨在减少计算成本，同时保持准确性，并在普通计算机上实现高效分析。

Method: 采用逐步分析策略，从低分辨率开始，逐步聚焦感兴趣区域进行高分辨率分析。研究了两种自适应分辨率选择策略，并在Camelyon16数据集上验证。

Result: PyramidAI减少多达2.65倍的数据处理量，分析时间从超过一小时缩短到几分钟（使用12个工作节点）。

Conclusion: PyramidAI提供了一种高效、实用的千兆像素图像分析解决方案，适用于大规模图像处理。

Abstract: Analyzing gigapixel images is recognized as computationally demanding. In
this paper, we introduce PyramidAI, a technique for analyzing gigapixel images
with reduced computational cost. The proposed approach adopts a gradual
analysis of the image, beginning with lower resolutions and progressively
concentrating on regions of interest for detailed examination at higher
resolutions. We investigated two strategies for tuning the accuracy-computation
performance trade-off when implementing the adaptive resolution selection,
validated against the Camelyon16 dataset of biomedical images. Our results
demonstrate that PyramidAI substantially decreases the amount of processed data
required for analysis by up to 2.65x, while preserving the accuracy in
identifying relevant sections on a single computer. To ensure democratization
of gigapixel image analysis, we evaluated the potential to use mainstream
computers to perform the computation by exploiting the parallelism potential of
the approach. Using a simulator, we estimated the best data distribution and
load balancing algorithm according to the number of workers. The selected
algorithms were implemented and highlighted the same conclusions in a
real-world setting. Analysis time is reduced from more than an hour to a few
minutes using 12 modest workers, offering a practical solution for efficient
large-scale image analysis.

</details>


### [133] [An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447)
*Xinrui Zhong,Xinze Feng,Jingwei Zuo,Fanjiang Ye,Yi Mu,Junfeng Guo,Heng Huang,Myungjin Lee,Yuke Wang*

Main category: cs.DC

TL;DR: QRMark 是一种高效、自适应的端到端方法，用于检测嵌入的图像水印，通过结合 QR 码纠错和分块技术来提高检测效率，同时保持准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的水印检测方法主要关注准确性和鲁棒性，但忽视了在大规模图像集合中的效率问题。

Method: QRMark 采用了 Reed-Solomon 纠错机制和基于分块的工作负载交错策略，并实现了资源感知的流分配策略。

Result: QRMark 的端到端评估显示，其推理速度平均比基线方法快 2.43 倍。

Conclusion: QRMark 在保持高检测准确性和鲁棒性的同时，显著提升了水印检测的效率。

Abstract: Efficient and reliable detection of generated images is critical for the
responsible deployment of generative models. Existing approaches primarily
focus on improving detection accuracy and robustness under various image
transformations and adversarial manipulations, yet they largely overlook the
efficiency challenges of watermark detection across large-scale image
collections. To address this gap, we propose QRMark, an efficient and adaptive
end-to-end method for detecting embedded image watermarks. The core idea of
QRMark is to combine QR Code inspired error correction with tailored tiling
techniques to improve detection efficiency while preserving accuracy and
robustness. At the algorithmic level, QRMark employs a Reed-Solomon error
correction mechanism to mitigate the accuracy degradation introduced by tiling.
At the system level, QRMark implements a resource-aware stream allocation
policy that adaptively assigns more streams to GPU-intensive stages of the
detection pipeline. It further employs a tile-based workload interleaving
strategy to overlap data-loading overhead with computation and schedules
kernels across stages to maximize efficiency. End-to-end evaluations show that
QRMark achieves an average 2.43x inference speedup over the sequential
baseline.

</details>


### [134] [KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End Kubernetes Management](https://arxiv.org/abs/2509.02449)
*Mohsen Seyedkazemi Ardebili,Andrea Bartolini*

Main category: cs.DC

TL;DR: KubeIntellect是一个基于大型语言模型的智能Kubernetes控制系统，支持自然语言交互，提高了操作效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: Kubernetes管理复杂且分散，现有工具不足以满足全谱API操作的需求，需要更智能的解决方案。

Method: KubeIntellect通过模块化代理和代码生成代理实现自然语言交互，整合了检查点和动态任务排序。

Result: 系统在200个自然语言查询中工具合成成功率达93%，可靠性100%。

Conclusion: KubeIntellect为复杂基础设施管理提供了一种可解释、可扩展的新型LLM驱动系统。

Abstract: Kubernetes has become the foundation of modern cloud-native infrastructure,
yet its management remains complex and fragmented. Administrators must navigate
a vast API surface, manage heterogeneous workloads, and coordinate tasks across
disconnected tools - often requiring precise commands, YAML configuration, and
contextual expertise.
  This paper presents KubeIntellect, a Large Language Model (LLM)-powered
system for intelligent, end-to-end Kubernetes control. Unlike existing tools
that focus on observability or static automation, KubeIntellect supports
natural language interaction across the full spectrum of Kubernetes API
operations, including read, write, delete, exec, access control, lifecycle, and
advanced verbs. The system uses modular agents aligned with functional domains
(e.g., logs, metrics, RBAC), orchestrated by a supervisor that interprets user
queries, maintains workflow memory, invokes reusable tools, or synthesizes new
ones via a secure Code Generator Agent.
  KubeIntellect integrates memory checkpoints, human-in-the-loop clarification,
and dynamic task sequencing into a structured orchestration framework.
Evaluation results show a 93% tool synthesis success rate and 100% reliability
across 200 natural language queries, demonstrating the system's ability to
operate efficiently under diverse workloads. An automated demo environment is
provided on Azure, with additional support for local testing via kind. This
work introduces a new class of interpretable, extensible, and LLM-driven
systems for managing complex infrastructure.

</details>


### [135] [MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to Break the GPU Memory Wall](https://arxiv.org/abs/2509.02480)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: 论文提出了MLP-Offload技术，通过多级多路径卸载引擎优化资源受限环境下的LLM训练，显著减少I/O瓶颈，实现比现有技术快2.5倍的迭代速度。


<details>
  <summary>Details</summary>
Motivation: 由于LLM规模增长速度远快于GPU内存容量，训练超大型LLM需要跨多GPU内存或磁盘卸载技术，但现有卸载策略在关键训练路径中产生显著的I/O开销，影响训练迭代速度。

Method: 提出MLP-Offload，一种多级多路径卸载引擎，通过高效缓存和并发控制，在反向传播和更新阶段跨多层级卸载优化器状态，以减少I/O瓶颈。

Result: 在280B参数的模型上评估，MLP-Offload实现了比现有技术快2.5倍的迭代速度。

Conclusion: MLP-Offload通过创新设计有效解决了LLM训练中的I/O瓶颈问题，显著提升了训练效率。

Abstract: Training LLMs larger than the aggregated memory of multiple GPUs is
increasingly necessary due to the faster growth of LLM sizes compared to GPU
memory. To this end, multi-tier host memory or disk offloading techniques are
proposed by state of art. Despite advanced asynchronous multi-tier read/write
strategies, such offloading strategies result in significant I/O overheads in
the critical path of training, resulting in slower iterations. To this end, we
propose MLP-Offload, a novel multi-level, multi-path offloading engine
specifically designed for optimizing LLM training on resource-constrained
setups by mitigating I/O bottlenecks. We make several key observations that
drive the design of MLP-Offload, such as I/O overheads during the update
dominate the iteration time; I/O bandwidth of the third-level remote storage
tier remains unutilized; and, contention due to concurrent offloading amplifies
I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload
to offload the optimizer states across multiple tiers in a cache-efficient and
concurrency-controlled fashion to mitigate I/O bottlenecks during the backward
and update phases. Evaluations on models up to 280B parameters shows that
MLP-Offload achieves 2.5$\times$ faster iterations compared to the
state-of-the-art LLM training runtimes.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [136] [Efficient Computation of Trip-based Group Nearest Neighbor Queries (Full Version)](https://arxiv.org/abs/2509.00173)
*Shahiduz Zaman,Tanzima Hashem,Sukarna Barua*

Main category: cs.DB

TL;DR: 论文提出了一种新型查询方法T-GNN，用于在用户现有行程中找到最佳集合点POI，并通过优化总行程额外距离来提高效率。


<details>
  <summary>Details</summary>
Motivation: 为满足人们在繁忙日程中高效组织团体活动的需求。

Method: 提出了T-GNN查询，结合三种剪枝技术和实时处理算法优化搜索空间。

Result: 实验验证了算法的性能。

Conclusion: T-GNN查询有效减少了总行程额外距离，适用于实时场景。

Abstract: In recent years, organizing group meetups for entertainment or other
necessities has gained significant importance, especially given the busy nature
of daily schedules. People often combine multiple activities, such as dropping
kids off at school, commuting to work, and grocery shopping, while seeking
opportunities to meet others. To address this need, we propose a novel query
type, the Trip-based Group Nearest Neighbor (T-GNN) query, which identifies the
optimal meetup Point of Interest (POI) that aligns with users' existing trips.
An individual trip consists of a sequence of locations, allowing users the
flexibility to detour to the meetup POI at any location within the sequence,
known as a detour location. Given a set of trips for the users, the query
identifies the optimal meetup POI (e.g., restaurants or movie theaters) and
detour locations from each user's trip that minimize the total trip overhead
distance. The trip overhead distance refers to the additional distance a user
must travel to visit the meetup POI before returning to the next location in
their trip. The sum of these overhead distances for all users constitutes the
total trip overhead distance. The computation time for processing T-GNN queries
increases with the number of POIs. To address this, we introduce three
techniques to prune the POIs that cannot contribute to the optimal solution,
and thus refine the search space. We also develop an efficient approach for
processing T-GNN queries in real-time. Extensive experiments validate the
performance of the proposed algorithm.

</details>


### [137] [SABER: A SQL-Compatible Semantic Document Processing System Based on Extended Relational Algebra](https://arxiv.org/abs/2509.00277)
*Changjae Lee,Zhuoyue Zhao,Jinjun Xiong*

Main category: cs.DB

TL;DR: 论文提出了一种名为SABER的语义代数，旨在为语义数据处理系统（SDPSs）提供统一的代数基础，支持逻辑计划构建、优化和形式化正确性保证。


<details>
  <summary>Details</summary>
Motivation: 现有SDPSs缺乏统一的代数基础，导致查询难以组合、推理和优化，限制了其发展和应用。

Method: 提出基于扩展关系代数的语义代数SABER，并采用SQL兼容语法实现，支持混合结构化/非结构化数据处理。

Result: SABER展示了为现有SDPSs提供统一接口的可行性，能够有效组合任何语义兼容的运算符实现，扩展了社区贡献的适用性。

Conclusion: SABER为SDPSs提供了新的代数基础，解决了现有系统缺乏统一性的问题，为未来研究和应用奠定了基础。

Abstract: The emergence of large-language models (LLMs) has enabled a new class of
semantic data processing systems (SDPSs) to support declarative queries against
unstructured documents. Existing SDPSs are, however, lacking a unified
algebraic foundation, making their queries difficult to compose, reason, and
optimize. We propose a new semantic algebra, SABER (Semantic Algebra Based on
Extended Relational algebra), opening the possibility of semantic operations'
logical plan construction, optimization, and formal correctness guarantees. We
further propose to implement SABER in a SQL-compatible syntax so that it
natively supports mixed structured/unstructured data processing. With SABER, we
showcase the feasibility of providing a unified interface for existing SDPSs so
that it can effectively mix and match any semantically-compatible operator
implementation from any SDPS, greatly enhancing SABER's applicability for
community contributions.

</details>


### [138] [Illuminating Patterns of Divergence: DataDios SmartDiff for Large-Scale Data Difference Analysis](https://arxiv.org/abs/2509.00293)
*Aryan Poduri,Yashwant Tailor*

Main category: cs.DB

TL;DR: SmartDiff是一个统一的系统，用于高效可靠地比较数据，支持模式漂移和异构类型，提供解释性标签，显著提升性能和诊断速度。


<details>
  <summary>Details</summary>
Motivation: 现有工具在处理模式漂移、异构类型和解释性不足时表现不佳，需要一种更高效且易用的解决方案。

Method: SmartDiff结合模式感知映射、类型特定比较器和并行执行，支持结构化与半结构化数据对齐，并生成解释性标签。

Result: 在多百万行数据集上，SmartDiff达到95%以上精确率和召回率，速度提升30-40%，内存使用减少30-50%，根因分析时间从10小时缩短至12分钟。

Conclusion: SmartDiff在数据迁移验证、回归测试、合规审计及持续数据质量监测中具有显著应用价值。

Abstract: Data engineering workflows require reliable differencing across files,
databases, and query outputs, yet existing tools falter under schema drift,
heterogeneous types, and limited explainability. SmartDiff is a unified system
that combines schema-aware mapping, type-specific comparators, and parallel
execution. It aligns evolving schemas, compares structured and semi-structured
data (strings, numbers, dates, JSON/XML), and clusters results with labels that
explain how and why differences occur. On multi-million-row datasets, SmartDiff
achieves over 95 percent precision and recall, runs 30 to 40 percent faster,
and uses 30 to 50 percent less memory than baselines; in user studies, it
reduces root-cause analysis time from 10 hours to 12 minutes. An LLM-assisted
labeling pipeline produces deterministic, schema-valid multilabel explanations
using retrieval augmentation and constrained decoding; ablations show further
gains in label accuracy and time to diagnosis over rules-only baselines. These
results indicate SmartDiff's utility for migration validation, regression
testing, compliance auditing, and continuous data quality monitoring. Index
Terms: data differencing, schema evolution, data quality, parallel processing,
clustering, explainable validation, big data

</details>


### [139] [Access Paths for Efficient Ordering with Large Language Models](https://arxiv.org/abs/2509.00303)
*Fuheng Zhao,Jiayue Chen,Yiming Pan,Tahseen Rabbani,Divyakant Agrawal,Amr El Abbadi*

Main category: cs.DB

TL;DR: 本文介绍了LLM ORDER BY操作符的逻辑抽象及其物理实现，研究发现不同方法的效果因查询特性和数据而异，提出了三种新设计，并通过实验验证了它们的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM ORDER BY操作符的实现，以探索如何在不同查询和数据条件下优化排序性能。

Method: 提出了三种新设计：基于一致性的批量大小策略、多数表决机制和双向外部归并排序，并在统一评估框架下进行实验。

Result: 实验表明，基于一致性的策略能有效决定批量大小，多数表决机制增强了GPT-4o的排序准确性，外部归并排序在数据集和模型间实现了高效权衡。

Conclusion: 研究发现计算成本与排序质量呈对数线性关系，为LLM驱动的数据系统提供了成本模型的理论基础。

Abstract: We present the LLM ORDER BY operator as a logical abstraction and study its
physical implementations within a unified evaluation framework. Our experiments
show that no single approach is universally optimal, with effectiveness
depending on query characteristics and data. We introduce three new designs: an
agreement-based batch-size policy, a majority voting mechanism for pairwise
sorting, and a two-way external merge sort adapted for LLMs. With extensive
experiments, our agreement-based procedure is effective at determining batch
size for value-based methods, the majority-voting mechanism consistently
strengthens pairwise comparisons on GPT-4o, and external merge sort achieves
high accuracy-efficiency trade-offs across datasets and models. We further
observe a log-linear scaling between compute cost and ordering quality,
offering the first step toward principled cost models for LLM powered data
systems.

</details>


### [140] [CRouting: Reducing Expensive Distance Calls in Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2509.00365)
*Zhenxin Li,Shuibing He,Jiahao Guo,Xuechen Zhang,Xian-He Sun,Gang Chen*

Main category: cs.DB

TL;DR: 本文提出了一种名为CRouting的新路由策略，通过利用高维向量的角度分布来绕过不必要的距离计算，从而加速图基近似最近邻搜索（ANNS）。实验表明，CRouting减少了41.5%的距离计算，并且在HNSW和NSG两种主流图索引上提升了1.48倍的查询速度。


<details>
  <summary>Details</summary>
Motivation: 高维空间中重复的距离计算是图基ANNS算法的主要时间成本，限制了其效率。

Method: 提出CRouting路由策略，基于高维向量角度分布绕过不必要的距离计算，可轻松集成到现有图基搜索算法中。

Result: CRouting将距离计算减少了41.5%，在HNSW和NSG上查询速度提升1.48倍。

Conclusion: CRouting是一种高效且易于集成的优化策略，显著提升了图基ANNS的性能。

Abstract: Approximate nearest neighbor search (ANNS) is a crucial problem in
information retrieval and AI applications. Recently, there has been a surge of
interest in graph-based ANNS algorithms due to their superior efficiency and
accuracy. However, the repeated computation of distances in high-dimensional
spaces constitutes the primary time cost of graph-based methods. To accelerate
the search, we propose a novel routing strategy named CRouting, which bypasses
unnecessary distance computations by exploiting the angle distributions of
high-dimensional vectors. CRouting is designed as a plugin to optimize existing
graph-based search with minimal code modifications. Our experiments show that
CRouting reduces the number of distance computations by up to 41.5% and boosts
queries per second by up to 1.48$\times$ on two predominant graph indexes, HNSW
and NSG. Code is publicly available at https://github.com/ISCS-ZJU/CRouting.

</details>


### [141] [BPI: A Novel Efficient and Reliable Search Structure for Hybrid Storage Blockchain](https://arxiv.org/abs/2509.00480)
*Xinkui Zhao,Rengrong Xiong,Guanjie Cheng,Xinhao Jin,Shawn Shi,Xiubo Liang,Gongsheng Yuan,Xiaoye Miao,Jianwei Yin,Shuiguang Deng*

Main category: cs.DB

TL;DR: 本文提出了一种轻量级框架BPI，用于解决区块链混合存储中的数据查询问题，通过“Articulated Search”模式和验证模型提高了查询效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 区块链混合存储依赖中心化存储服务提供商（SPs），可能导致查询结果遗漏问题，现有认证数据结构（ADS）无法完全解决这一缺陷。

Method: 提出BPI框架，引入“Articulated Search”查询模式，结合轻量级验证模型，确保查询结果完整且高效。

Result: 实验证明BPI在区块链中的关键词查询性能和扩展性优于EthMB+及其他主流混合存储区块链的搜索数据库。

Conclusion: BPI通过创新的查询和验证机制，显著提升了区块链数据查询的效率和可靠性。

Abstract: Hybrid storage solutions have emerged as potent strategies to alleviate the
data storage bottlenecks prevalent in blockchain systems. These solutions
harness off-chain Storage Services Providers (SPs) in conjunction with
Authenticated Data Structures (ADS) to ensure data integrity and accuracy.
Despite these advancements, the reliance on centralized SPs raises concerns
about query correctness. Although ADS can verify the existence of individual
query results, they fall short of preventing SPs from omitting valid results.
  In this paper, we delineate the fundamental distinctions between data search
in blockchains and traditional database systems. Drawing upon these insights,
we introduce BPI, a lightweight framework that enables efficient keyword
queries and maintenance with low overhead. We propose "Articulated Search", a
query pattern specifically designed for blockchain environments that enhances
search efficiency while significantly reducing costs during data user updates.
Furthermore, BPI employs a suite of validation models to ensure the inclusion
of all valid content in search results while maintaining low overhead.
  Extensive experimental evaluations demonstrate that the BPI framework
achieves outstanding scalability and performance in keyword searches within
blockchain, surpassing EthMB+ and state of the art search databases commonly
used in mainstream hybrid storage blockchains (HSB).

</details>


### [142] [SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction](https://arxiv.org/abs/2509.00581)
*Saumya Chaturvedi,Aman Chadha,Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 该研究提出了一种名为SQL-of-Thought的多智能体框架，通过分解Text2SQL任务并引入动态错误修正方法，在Spider数据集上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决将自然语言查询转换为SQL查询的挑战，以提高对数据库和大规模应用的访问性。

Method: 采用SQL-of-Thought框架，将任务分解为模式链接、子问题识别、查询计划生成、SQL生成和动态错误修正。

Result: 在Spider数据集及其变体上实现了最佳性能。

Conclusion: 该方法通过动态错误修正和推理式查询规划，显著提升了文本到SQL系统的性能。

Abstract: Converting natural language queries into SQL queries is a crucial challenge
in both industry and academia, aiming to increase access to databases and
large-scale applications. This work examines how in-context learning and
chain-of-thought can be utilized to develop a robust solution for text-to-SQL
systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the
Text2SQL task into schema linking, subproblem identification, query plan
generation, SQL generation, and a guided correction loop. Unlike prior systems
that rely only on execution-based static correction, we introduce
taxonomy-guided dynamic error modification informed by in-context learning.
SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its
variants, combining guided error taxonomy with reasoning-based query planning.

</details>


### [143] [Near-Duplicate Text Alignment under Weighted Jaccard Similarity](https://arxiv.org/abs/2509.00627)
*Yuheng Zhang,Miao Qiao,Zhencan Peng,Dong Deng*

Main category: cs.DB

TL;DR: MONO提出了一种支持加权Jaccard相似度的近重复文本对齐方法，通过一致的加权采样优化了现有方法的局限性，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法缺乏准确性保证且参数难以调优，而现有方法未能考虑词的重要性或频率，限制了实际应用。

Method: MONO利用一致的加权采样支持加权Jaccard相似度，实现了哈希框架内的最优性。

Result: MONO在索引构建时间、索引大小和查询延迟方面显著优于现有方法，最高提升26倍效率和30%存储空间。

Conclusion: MONO解决了加权相似度的问题，并在性能和效率上表现出色，适用于实际场景。

Abstract: Near-duplicate text alignment is the task of identifying, among the texts in
a corpus, all the subsequences (substrings) that are similar to a given query.
Traditional approaches rely on seeding-extension-filtering heuristics, which
lack accuracy guarantees and require many hard-to-tune parameters. Recent
methods leverage min-hash techniques under a hash-based framework: group
subsequences by their min-hash, and for any query, find all sketches similar to
the query's sketch. These methods guarantee to report all subsequences whose
estimated unweighted Jaccard similarity with the query exceeds a user-provided
threshold and are efficient. However, they fail to account for token importance
or frequency, which limits their use in real scenarios where tokens carry
weights, such as TF-IDF. To address this, we propose MONO, an approach that
supports weighted Jaccard similarity using consistent weighted sampling. MONO
achieves optimality within the hash-based framework. For example, when token
weights are proportional to frequencies, MONO generates O(n + n log f) groups
in expectation for a text of length n, where f is the maximum token frequency.
Each group takes O(1) space and represents a few subsequences sharing the same
sampling. We prove this bound is tight: any algorithm must produce Omega(n + n
log f) groups in expectation in the worst case. Experiments show that MONO
outperforms the state of the art by up to 26x in index construction time,
reduces index size by up to 30 percent, and improves query latency by up to 3x,
while scaling well.

</details>


### [144] [Diverse Unionable Tuple Search: Novelty-Driven Discovery in Data Lakes [Technical Report]](https://arxiv.org/abs/2509.01012)
*Aamod Khatiwada,Roee Shraga,Renée J. Miller*

Main category: cs.DB

TL;DR: 论文提出了一个解决数据湖中与查询表多样性的问题的算法DUST，通过聚类和嵌入模型提高了效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的联合表搜索技术可能会返回与查询表几乎相同的表，缺乏新信息。

Method: 提出了一种基于聚类的新算法DUST，使用新型嵌入模型表示联合表。

Result: DUST在效率和多样性方面显著优于现有算法，速度提升了6倍以上，嵌入模型性能提高了15%以上。

Conclusion: DUST在联合表多样性和效率方面表现出色，是一种有效的解决方案。

Abstract: Unionable table search techniques input a query table from a user and search
for data lake tables that can contribute additional rows to the query table.
The definition of unionability is generally based on similarity measures which
may include similarity between columns (e.g., value overlap or semantic
similarity of the values in the columns) or tables (e.g., similarity of table
embeddings). Due to this and the large redundancy in many data lakes (which can
contain many copies and versions of the same table), the most unionable tables
may be identical or nearly identical to the query table and may contain little
new information. Hence, we introduce the problem of identifying unionable
tuples from a data lake that are diverse with respect to the tuples already
present in a query table. We perform an extensive experimental analysis of
well-known diversity algorithms applied to this novel problem and identify a
gap that we address with a novel, clustering-based tuple diversity algorithm
called DUST. DUST uses a novel embedding model to represent unionable tuples
that outperforms other tuple representation models by at least 15 % when
representing unionable tuples. Using real data lake benchmarks, we show that
our diversification algorithm is more than six times faster than the most
efficient diversification baseline. We also show that it is more effective in
diversifying unionable tuples than existing diversification algorithms.

</details>


### [145] [Disentangling the schema turn: Restoring the information base to conceptual modelling](https://arxiv.org/abs/2509.01617)
*Chris Partridge,Andrew Mitchell,Sergio de Cesare,Oscar Xiberta Soto*

Main category: cs.DB

TL;DR: 论文分析了计算机科学中概念建模的‘模式转向’现象，探讨其起源并展示其非必要性，提出了结合模式与信息库的更自动化方法。


<details>
  <summary>Details</summary>
Motivation: 探讨当代主流概念建模实践中‘模式转向’现象的成因及其局限性，展示更广泛的可能性。

Method: 通过分析历史和技术发展，论证结合模式与信息库的可行性，并以bCLEARer为例说明新技术的实现。

Result: 现代技术可实现模式与信息库结合的概念建模，方法比当前假设更丰富。

Conclusion: ‘模式转向’可能只是暂时的进化弯路，未来或有更包容的建模方法。

Abstract: If one looks at contemporary mainstream development practices for conceptual
modelling in computer science, these so clearly focus on a conceptual schema
completely separated from its information base that the conceptual schema is
often just called the conceptual model. These schema-centric practices are
crystallized in almost every database textbook. We call this strong, almost
universal, bias towards conceptual schemas the schema turn. The focus of this
paper is on disentangling this turn within (computer science) conceptual
modeling. It aims to shed some light on how it emerged and so show that it is
not fundamental. To show that modern technology enables the adoption of an
inclusive schema-and-base conceptual modelling approach, which in turn enables
more automated, and empirically motivated practices. And to show, more
generally, the space of possible conceptual modelling practices is wider than
currently assumed. It also uses the example of bCLEARer to show that the
implementations in this wider space will probably need to rely on new
pipeline-based conceptual modelling techniques. So, it is possible that the
schema turn's complete exclusion of the information base could be merely a
temporary evolutionary detour.

</details>


### [146] [OASIS: Object-based Analytics Storage for Intelligent SQL Query Offloading in Scientific Tabular Workloads](https://arxiv.org/abs/2509.01966)
*Soon Hwang,Junhyeok Park,Junghyun Ryu,Seonghoon Ahn,Jeoungahn Park,Jeongjin Lee,Soonyeal Yang,Jungki Noh,Woosuk Chung,Hoshik Kim,Youngjae Kim*

Main category: cs.DB

TL;DR: OASIS是一种新型COS系统，解决了现有COS系统在输出格式、操作符支持和存储层次优化方面的局限性，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有COS系统在灵活性、操作符支持和存储层次优化方面存在不足，限制了其在复杂分析任务中的应用。

Method: 提出OASIS系统，支持多样化输出格式、复杂操作符和表达式，并在存储层次中动态选择最优执行路径。

Result: 在HPC工作流的真实科学查询中，OASIS比现有COS系统性能提升达32.7%。

Conclusion: OASIS通过灵活性和优化设计，显著提升了COS系统的性能和应用范围。

Abstract: Computation-Enabled Object Storage (COS) systems, such as MinIO and Ceph,
have recently emerged as promising storage solutions for post hoc, SQL-based
analysis on large-scale datasets in High-Performance Computing (HPC)
environments. By supporting object-granular layouts, COS facilitates
column-oriented access and supports in-storage execution of data reduction
operators, such as filters, close to where the data resides. Despite growing
interest and adoption, existing COS systems exhibit several fundamental
limitations that hinder their effectiveness. First, they impose rigid
constraints on output data formats, limiting flexibility and interoperability.
Second, they support offloading for only a narrow set of operators and
expressions, restricting their applicability to more complex analytical tasks.
Third--and perhaps most critically--they fail to incorporate design strategies
that enable compute offloading optimized for the characteristics of deep
storage hierarchies. To address these challenges, this paper proposes OASIS, a
novel COS system that features: (i) flexible and interoperable output delivery
through diverse formats, including columnar layouts such as Arrow; (ii) broad
support for complex operators (e.g., aggregate, sort) and array-aware
expressions, including element-wise predicates over array structures; and (iii)
dynamic selection of optimal execution paths across internal storage layers,
guided by operator characteristics and data movement costs. We implemented a
prototype of OASIS and integrated it into the Spark analytics framework.
Through extensive evaluation using real-world scientific queries from HPC
workflows, OASIS achieves up to a 32.7% performance improvement over Spark
configured with existing COS-based storage systems.

</details>


### [147] [GeoLayer: Towards Low-Latency and Cost-Efficient Geo-Distributed Graph Stores with Layered Graph](https://arxiv.org/abs/2509.02106)
*Feng Yao,Xiaokang Yang,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: GeoLayer是一个地理分布式图存储框架，通过分层图架构和重叠中心副本放置优化图形副本放置和请求路由，显著提高了响应时间和分析性能。


<details>
  <summary>Details</summary>
Motivation: 图数据的固有连通性和依赖性以及其独特的拓扑驱动访问模式，使得传统的复制和请求路由策略在地理分布式云存储系统中面临挑战。

Method: 提出分层图架构以减少优化问题的复杂性，引入重叠中心副本放置方案和定向热扩散模型指导数据分配，开发逐步分层路由策略高效检索数据。

Result: 与现有技术相比，GeoLayer使在线图形模式请求的响应时间提高了1.34x - 3.67x，离线图形分析性能提高了1.28x - 3.56x。

Conclusion: GeoLayer通过联合优化副本放置和请求路由，有效应对了地理分布式环境中图数据存储和访问的挑战，显著提升了性能。

Abstract: The inherent connectivity and dependency of graph-structured data, combined
with its unique topology-driven access patterns, pose fundamental challenges to
conventional data replication and request routing strategies in geo-distributed
cloud storage systems. In this paper, we propose GeoLayer, a geo-distributed
graph storage framework that jointly optimizes graph replica placement and
pattern request routing. We first construct a latency-aware layered graph
architecture that decomposes the graph topology into multiple layers, aiming to
reduce the decision space and computational complexity of the optimization
problem, while mitigating the impact of network heterogeneity in
geo-distributed environments. Building on the layered graph, we introduce an
overlap-centric replica placement scheme to accommodate the diversity of graph
pattern accesses, along with a directed heat diffusion model that captures heat
conduction and superposition effects to guide data allocation. For request
routing, we develop a stepwise layered routing strategy that performs
progressive expansion over the layered graph to efficiently retrieve the
required data. Experimental results show that, compared to state-of-the-art
replica placement and routing schemes, GeoLayer achieves a 1.34x - 3.67x
improvement in response times for online graph pattern requests and a 1.28x -
3.56x speedup in offline graph analysis performance.

</details>


### [148] [Batch Query Processing and Optimization for Agentic Workflows](https://arxiv.org/abs/2509.02121)
*Junyi Shen,Noppanat Wadlom,Yao Lu*

Main category: cs.DB

TL;DR: Halo系统通过优化批处理查询和资源共享，显著提升了多代理LLM工作流的执行效率和硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务引擎在多代理工作流中存在冗余执行和GPU利用率低的问题，尤其是在批处理分析场景中。

Method: Halo将工作流表示为结构化查询计划DAG，通过成本模型和运行时优化技术（如自适应批处理、KV缓存共享）减少冗余。

Result: Halo在批处理推理和在线服务中分别实现了18.6x和4.7x的性能提升，且不影响输出质量。

Conclusion: Halo通过统一查询优化和LLM服务，为数据分析和决策应用提供了高效的多代理工作流支持。

Abstract: Large Language Models (LLMs) in agentic workflows combine multi-step
reasoning, tool use, and collaboration across multiple specialized agents.
Existing LLM serving engines optimize indi- vidual calls in isolation, while
multi-agent frameworks focus on orchestration without system-level performance
planning. As a result, repeated prompts, overlapping contexts, and concurrent
ex- ecutions create substantial redundancy and poor GPU utilization, especially
in batch analytics scenarios. We introduce Halo, a system that brings batch
query processing and optimization into agentic LLM workflows. Halo represents
each workflow as a structured query plan DAG and constructs a consoli- dated
graph for batched queries that exposes shared computation. Guided by a cost
model that jointly considers prefill and decode costs, cache reuse, and GPU
placement, Halo performs plan-level op- timization to minimize redundant
execution. Its runtime integrates adaptive batching, KV-cache sharing and
migration, along with compute-communication overlap to maximize hardware
efficiency. Evaluation across six benchmarks shows that Halo achieves up to
18.6x speedup for batch inference and 4.7x throughput im- provement under
online serving, scaling to workloads of tens of thousands of queries and
complex graphs. These gains are achieved without compromising output quality.
By unifying query optimiza- tion with LLM serving, Halo enables efficient
agentic workflows in data analytics and decision-making applications.

</details>


### [149] [FDABench: A Benchmark for Data Agents on Analytical Queries over Heterogeneous Data](https://arxiv.org/abs/2509.02473)
*Ziting Wang,Shize Zhang,Haitao Yuan,Jinwei Zhu,Shifu Li,Wei Dong,Gao Cong*

Main category: cs.DB

TL;DR: FDABench是一个专为多源数据分析场景设计的数据代理基准测试，解决了现有基准测试在全面性、可靠性和通用性上的不足。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的决策需求增长推动了数据代理的发展，但现有基准测试存在局限性，如缺乏全面性、测试用例构建复杂和适应性不足。

Method: 设计了FDABench基准测试，包含2,007个多样化任务，采用代理-专家协作框架构建，并具备通用性。

Result: FDABench评估了多个数据代理系统，揭示了各系统在响应质量、准确性、延迟和成本方面的优缺点。

Conclusion: FDABench为多源数据代理评估提供了全面且可靠的基准，推动了该领域的发展。

Abstract: The growing demand for data-driven decision-making has created an urgent need
for data agents that can integrate structured and unstructured data for
analysis. While data agents show promise for enabling users to perform complex
analytics tasks, this field still suffers from three critical limitations:
first, comprehensive data agent benchmarks remain absent due to the difficulty
of designing test cases that evaluate agents' abilities across multi-source
analytical tasks; second, constructing reliable test cases that combine
structured and unstructured data remains costly and prohibitively complex;
third, existing benchmarks exhibit limited adaptability and generalizability,
resulting in narrow evaluation scope.
  To address these challenges, we present FDABench, the first data agent
benchmark specifically designed for evaluating agents in multi-source data
analytical scenarios. Our contributions include: (i) we construct a
standardized benchmark with 2,007 diverse tasks across different data sources,
domains, difficulty levels, and task types to comprehensively evaluate data
agent performance; (ii) we design an agent-expert collaboration framework
ensuring reliable and efficient benchmark construction over heterogeneous data;
(iii) we equip FDABench with robust generalization capabilities across diverse
target systems and frameworks. We use FDABench to evaluate various data agent
systems, revealing that each system exhibits distinct advantages and
limitations regarding response quality, accuracy, latency, and token cost.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [150] [AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection](https://arxiv.org/abs/2509.00433)
*Houshu He,Naifeng Jing,Li Jiang,Xiaoyao Liang,Zhuoran Song*

Main category: cs.AR

TL;DR: 提出了一种名为AGS的算法-硬件协同设计框架，通过利用相邻帧的高相似性，优化了3D高斯泼溅（3DGS）SLAM系统的效率，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM系统由于每帧需要多次训练迭代和高斯数量庞大，导致吞吐量不足。本文旨在通过算法和硬件协同设计解决这一问题。

Method: 软件层面提出粗到细的位姿跟踪方法，并共享高斯贡献信息以减少冗余计算；硬件层面设计了帧共视检测引擎、位姿跟踪引擎和映射引擎，结合工作负载调度器高效部署AGS算法。

Result: AGS在移动和高性能GPU以及GSCore加速器上的性能分别提升了17.12倍、6.71倍和5.41倍。

Conclusion: AGS通过算法和硬件优化，显著提升了3DGS-SLAM的效率，为实时应用提供了可行的解决方案。

Abstract: Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.

</details>


### [151] [Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator](https://arxiv.org/abs/2509.00500)
*Yizhi Chen,Jingwei Li,Wenyao Zhu,Zhonghai Lu*

Main category: cs.AR

TL;DR: 提出了一种基于'1'位数统计的排序方法，用于在NoC-based DNN加速器中减少Bit Transition (BT)，从而降低链路功耗。


<details>
  <summary>Details</summary>
Motivation: 随着DNN的普及，NoC-based DNN加速器越来越重要，减少BT以节省链路功耗成为研究热点。

Method: 提出了两种数据排序方法（affiliated-ordering和separated-ordering），并提供了数学证明其有效性。

Result: 实验表明，该方法在不使用NoC时，BT减少最高达20.38%（FP32）和55.71%（INT8）；在NoC加速器中，BT减少最高达32.01%（FP32）和40.85%（INT8）。

Conclusion: 所提出的排序方法能有效减少BT，显著降低链路功耗，适用于多种DNN模型和配置。

Abstract: As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip
(NoC)-based DNN accelerators gained increasing popularity. To save link power
in NoC, many researchers focus on reducing the Bit Transition (BT). We propose
'1'-bit count-based ordering method to reduce BT for DNN workloads. We provide
a mathematical proof of the efficacy of proposed ordering. We evaluate our
method through experiments without NoC and with NoC. Without NoC, our proposed
ordering method achieves up to 20.38% BT reduction for floating-point-32 data
and 55.71% for fixed-point-8 data, respectively. We propose two data ordering
methods, affiliated-ordering and separated-ordering to process weight and input
jointly or individually and apply them to run full DNNs in NoC-based DNN
accelerator. We evaluate our approaches under various configurations, including
different DNN models such as LeNet and DarkNet, various NoC sizes with
different numbers of memory controllers, random weights and trained weights,
and different data precision. Our approach efficiently reduces the link power
by achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT
reduction for fixed-point-8 data.

</details>


### [152] [Real-Time Piano Note Frequency Detection Using FPGA and FFT Core](https://arxiv.org/abs/2509.00589)
*Shafayet M. Anik,D. G. Perera*

Main category: cs.AR

TL;DR: 该论文研究了基于FPGA的实时FFT系统用于钢琴音频信号的频率分析，强调其在速度和确定性上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统软件DSP方法存在延迟和计算资源需求高的问题，而FPGA因其并行处理能力可提供更高效的解决方案。

Method: 使用基于FPGA的实时FFT系统对数字钢琴的模拟音频信号进行频率分析。

Result: 未明确提及具体结果，但暗示FPGA在实时性和效率上优于传统方法。

Conclusion: FPGA平台为实时音频频率分析提供了一种高效且低延迟的选择。

Abstract: Real-time frequency analysis of musical instruments, such as the piano, is an
essential feature in areas like electronic tuners, music visualizers, and live
sound monitoring. Traditional methods often rely on software-based digital
signal processing (DSP), which may introduce latency and require significant
computational power. In contrast, hardware platforms such as FPGAs (Field
Programmable Gate Arrays) offer the ability to perform such analyses with
greater speed and determinism due to their parallel processing capabilities.
The primary objective of this project was to analyze analog audio signals from
a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT)
system.

</details>


### [153] [COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives](https://arxiv.org/abs/2509.00599)
*Shubham Negi,Manik Singhal,Aayush Ankit,Sudeep Bhoja,Kaushik Roy*

Main category: cs.AR

TL;DR: COMET提出了一种针对机器学习加速器中复合操作的建模与优化框架，通过显式建模空间集群间的集体通信及其成本，提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现代DNN模型依赖复合操作，现有优化框架缺乏对集体通信成本的显式建模，限制了其适用性。

Method: COMET引入了一种新表示法，显式建模集体通信和操作依赖，优化数据流。

Result: 优化后的数据流在多个复合操作（如GEMM-Softmax）中显著提升了速度和能效。

Conclusion: COMET通过集体通信感知的建模，拓展了优化空间，为现代DNN模型提供了更高效的加速方案。

Abstract: Modern machine learning accelerators are designed to efficiently execute deep
neural networks (DNNs) by optimizing data movement, memory hierarchy, and
compute throughput. However, emerging DNN models such as large language models,
state space models increasingly rely on compound operations-structured
compositions of multiple basic operations-which introduce new challenges for
dataflow optimization and minimizing off-chip memory traffic. Moreover, as
model size continues to grow, deployment across spatially distributed compute
clusters becomes essential, requiring frequent and complex collective
communication. Existing dataflow optimization frameworks and performance models
either focus on single operations or lack explicit modeling of collective
communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and
optimizing dataflow for compound operations on machine learning accelerators.
COMET introduces a novel representation that explicitly models collective
communication across spatial clusters, along with latency and energy cost
models that account for both GEMM and non-GEMM operation level dependencies
within compound operations. We demonstrate COMET's capabilities to analyze and
optimize dataflows for compound operations such as GEMM--Softmax,
GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator
configurations. Our collective-aware modeling enables exploration of a broader
mapping space, leading to improved performance and energy efficiency.
Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for
GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for
self-attention compared to unfused baselines.

</details>


### [154] [On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures](https://arxiv.org/abs/2509.00633)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed A. Badawy,Ahmad Patooghy*

Main category: cs.AR

TL;DR: 3D堆叠高带宽内存（HBM）架构易受热攻击，攻击者通过注入热脉冲影响相邻内存库，绕过安全检测。


<details>
  <summary>Details</summary>
Motivation: 为解决内存墙问题，HBM架构提供了高性能内存交互，但其垂直堆叠结构易受热攻击。

Method: 攻击者通过相邻内存库注入短而强烈的热脉冲，形成热波以延迟受害应用访问数据。

Result: 攻击模仿合法负载，绕过设计时安全测试和操作系统内存管理策略，难以检测。

Conclusion: HBM架构的热攻击风险需引起重视，未来需开发新的安全机制应对此类攻击。

Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance
memory interactions to address the well-known performance challenge, namely the
memory wall. However, these architectures are susceptible to thermal
vulnerabilities due to the inherent vertical adjacency that occurs during the
manufacturing process of HBM architectures. We anticipate that adversaries may
exploit the intense vertical and lateral adjacency to design and develop
thermal performance degradation attacks on the memory banks that host
data/instructions from victim applications. In such attacks, the adversary
manages to inject short and intense heat pulses from vertically and/or
laterally adjacent memory banks, creating a convergent thermal wave that
maximizes impact and delays the victim application from accessing its
data/instructions. As the attacking application does not access any
out-of-range memory locations, it can bypass both design-time security tests
and the operating system's memory management policies. In other words, since
the attack mimics legitimate workloads, it will be challenging to detect.

</details>


### [155] [GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs](https://arxiv.org/abs/2509.01020)
*Elena Espinosa,Rubén Rodríguez Álvarez,José Miranda,Rafael Larrosa,Miguel Peón-Quirós,Oscar Plata,David Atienza*

Main category: cs.AR

TL;DR: 该论文提出了一种基于FPGA的可扩展加速器模板GeneTEK，用于基因组序列比对，显著提升了速度和能效，克服了现有FPGA方法的扩展限制。


<details>
  <summary>Details</summary>
Motivation: 下一代测序技术的高通量数据生成带来了巨大的计算挑战，尤其是序列比对步骤耗时耗能，需要高效且可扩展的解决方案。

Method: 论文采用高级综合和基于工作者的架构，在FPGA上实现Myers算法，构建了GeneTEK加速器模板。

Result: GeneTEK在速度和能效上均优于现有的CPU和GPU实现，执行速度提高了至少19.4%，能耗降低高达62倍，同时支持更大的比对矩阵。

Conclusion: FPGA在可扩展基因组工作负载中展现出高效的潜力，GeneTEK为未来研究提供了可行的解决方案。

Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic
research by enabling high-throughput data generation through parallel
sequencing of a diverse range of organisms at significantly reduced costs. This
breakthrough has unleashed a "Cambrian explosion" in genomic data volume and
diversity. This volume of workloads places genomics among the top four big data
challenges anticipated for this decade. In this context, pairwise sequence
alignment represents a very time- and energy-consuming step in common
bioinformatics pipelines. Speeding up this step requires the implementation of
heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated
significant performance gains, recent field programmable gate array (FPGA)
implementations have shown improved energy efficiency. However, the latter
often suffer from limited scalability due to constraints on hardware resources
when aligning longer sequences. In this work, we present a scalable and
flexible FPGA-based accelerator template that implements Myers's algorithm
using high-level synthesis and a worker-based architecture. GeneTEK, an
instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,
outperforms state-of-the-art CPU and GPU implementations in both speed and
energy efficiency, while overcoming scalability limitations of current FPGA
approaches. Specifically, GeneTEK achieves at least a 19.4% increase in
execution speed and up to 62x reduction in energy consumption compared to
leading CPU and GPU solutions, while fitting comparison matrices up to 72%
larger compared to previous FPGA solutions. These results reaffirm the
potential of FPGAs as an energy-efficient platform for scalable genomic
workloads.

</details>


### [156] [Low Power Approximate Multiplier Architecture for Deep Neural Networks](https://arxiv.org/abs/2509.00764)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出了一种低功耗近似乘法器架构，用于深度神经网络应用，通过设计一种仅引入单一组合错误的4:2压缩器，实现了能耗节省30.24%，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在低功耗硬件上的高效实现需要平衡能耗与计算精度，而传统乘法器能耗较高。

Method: 设计了一种仅引入单一组合错误的4:2压缩器，并将其集成到8x8无符号乘法器中，降低了精确压缩器的使用。

Result: 在图像去噪任务中提升了PSNR和SSIM，在手写数字识别中保持了高分类精度，能耗节省达30.24%。

Conclusion: 该架构在能耗与计算精度之间取得了良好平衡，适用于低功耗AI硬件实现。

Abstract: This paper proposes an low power approximate multiplier architecture for deep
neural network (DNN) applications. A 4:2 compressor, introducing only a single
combination error, is designed and integrated into an 8x8 unsigned multiplier.
This integration significantly reduces the usage of exact compressors while
preserving low error rates. The proposed multiplier is employed within a custom
convolution layer and evaluated on neural network tasks, including image
recognition and denoising. Hardware evaluation demonstrates that the proposed
design achieves up to 30.24% energy savings compared to the best among existing
multipliers. In image denoising, the custom approximate convolution layer
achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity
Index Measure (SSIM) compared to other approximate designs. Additionally, when
applied to handwritten digit recognition, the model maintains high
classification accuracy. These results demonstrate that the proposed
architecture offers a favorable balance between energy efficiency and
computational precision, making it suitable for low-power AI hardware
implementations.

</details>


### [157] [Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication](https://arxiv.org/abs/2509.00778)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出了一种结合精确和近似处理单元的脉动阵列架构，显著提升了能效并保持了输出质量，适用于图像和视觉处理。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要高效的矩阵乘法引擎，以支持复杂计算，现有设计在能效和输出质量上仍有改进空间。

Method: 采用新型正负部分积单元（PPC和NPPC）的8位精确和近似PE，构建8x8脉动阵列，应用于DCT和卷积计算。

Result: 能效提升22%和32%，DCT计算PSNR达38.21dB，卷积边缘检测PSNR达30.45dB。

Conclusion: 该设计在能效和输出质量上表现优异，适用于容错的图像和视觉处理应用。

Abstract: Deep Neural Networks (DNNs) require highly efficient matrix multiplication
engines for complex computations. This paper presents a systolic array
architecture incorporating novel exact and approximate processing elements
(PEs), designed using energy-efficient positive partial product and negative
partial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit
exact and approximate PE designs are employed in a 8x8 systolic array, which
achieves a energy savings of 22% and 32%, respectively, compared to the
existing design. To demonstrate their effectiveness, the proposed PEs are
integrated into a systolic array (SA) for Discrete Cosine Transform (DCT)
computation, achieving high output quality with a PSNR of 38.21,dB.
Furthermore, in an edge detection application using convolution, the
approximate PE achieves a PSNR of 30.45,dB. These results highlight the
potential of the proposed design to deliver significant energy efficiency while
maintaining competitive output quality, making it well-suited for
error-resilient image and vision processing applications.

</details>


### [158] [GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency](https://arxiv.org/abs/2509.00911)
*Joongho Jo,Jongsun Park*

Main category: cs.AR

TL;DR: 3D高斯喷绘（3D-GS）是一种高效的视图合成方法，但仍需提升实时性。GS-TG通过优化排序和光栅化操作，显著提升了渲染速度。


<details>
  <summary>Details</summary>
Motivation: 3D-GS虽快速但无法满足实时应用的FPS需求，GS-TG旨在解决排序与光栅化的冗余计算问题。

Method: GS-TG通过分组小图块共享排序操作，并使用位掩码标识相关小图块，优化排序与光栅化流程。

Result: 实验显示，GS-TG平均提速1.54倍，且无需重新训练或微调。

Conclusion: GS-TG是一种无损优化方法，可与现有技术无缝结合，显著提升3D-GS的实时性能。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to
neural radiance fields (NeRF) as it offers high speed as well as high image
quality in novel view synthesis. Despite these advancements, 3D-GS still
struggles to meet the frames per second (FPS) demands of real-time
applications. In this paper, we introduce GS-TG, a tile-grouping-based
accelerator that enhances 3D-GS rendering speed by reducing redundant sorting
operations and preserving rasterization efficiency. GS-TG addresses a critical
trade-off issue in 3D-GS rendering: increasing the tile size effectively
reduces redundant sorting operations, but it concurrently increases unnecessary
rasterization computations. So, during sorting of the proposed approach, GS-TG
groups small tiles (for making large tiles) to share sorting operations across
tiles within each group, significantly reducing redundant computations. During
rasterization, a bitmask assigned to each Gaussian identifies relevant small
tiles, to enable efficient sharing of sorting results. Consequently, GS-TG
enables sorting to be performed as if a large tile size is used by grouping
tiles during the sorting stage, while allowing rasterization to proceed with
the original small tiles by using bitmasks in the rasterization stage. GS-TG is
a lossless method requiring no retraining or fine-tuning and it can be
seamlessly integrated with previous 3D-GS optimization techniques. Experimental
results show that GS-TG achieves an average speed-up of 1.54 times over
state-of-the-art 3D-GS accelerators.

</details>


### [159] [LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems](https://arxiv.org/abs/2509.01339)
*Bochen Ye,Gustavo Naspolini,Kimmo Salo,Manil Dev Gomony*

Main category: cs.AR

TL;DR: LinkBo是一种创新的单线通信协议，提供低延迟、高吞吐和强鲁棒性，支持可变距离的芯片间通信。其性能显著优于现有商业方案，适用于成本敏感的嵌入式系统。


<details>
  <summary>Details</summary>
Motivation: 当前单线通信协议存在延迟高、吞吐受限和鲁棒性不足的问题，无法满足低成本嵌入式系统的需求，因此需要一种更高效的单线通信方案。

Method: 提出LinkBo协议，采用硬件中断技术，支持可变距离通信，并设计了相应的硬件架构，在两块FPGA上进行了性能评估。

Result: LinkBo协议实现了50.4微秒的低延迟数据传输，支持15米线长下的300 kbps速率和11厘米线长下的7.5 Mbps速率，性能显著优于现有方案。

Conclusion: LinkBo是一种高效、可靠的单线通信协议，适用于多种距离的芯片间通信，尤其在成本敏感的嵌入式系统中具有显著优势。

Abstract: Cost-effective embedded systems necessitate utilizing the single-wire
communication protocol for inter-chip communication, thanks to its reduced pin
count in comparison to the multi-wire I2C or SPI protocols. However, current
single-wire protocols suffer from increased latency, restricted throughput, and
lack of robustness. This paper presents LinkBo, an innovative single-wire
protocol that offers reduced latency, enhanced throughput, and greater
robustness with hardware-interrupt for variable-distance inter-chip
communication. The LinkBo protocol-level guarantees that high-priority messages
are delivered with an error detection feature in just 50.4 $\mu$s, surpassing
current commercial options, 1-wire and UNI/O by at least 20X and 6.3X,
respectively. In addition, we present the hardware architecture for this new
protocol and its performance evaluation on a hardware platform consisting of
two FPGAs. Our findings demonstrate that the protocol reliably supports wire
lengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum
data rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for
varying inter-chip communication distances.

</details>


### [160] [Guidance and Control Neural Network Acceleration using Memristors](https://arxiv.org/abs/2509.02369)
*Zacharia A. Rudge,Dario Izzo,Moritz Fieback,Anteneh Gebregiorgis,Said Hamdioui,Dominik Dold*

Main category: cs.AR

TL;DR: 本文探讨了在太空应用中使用相变存储器（PCM）和电阻随机存取存储器（RRAM）忆阻器进行内存计算AI加速的可行性，分析了其性能及挑战。


<details>
  <summary>Details</summary>
Motivation: 小型卫星和立方卫星的有限能量预算及辐射问题限制了AI在太空应用中的发展，需要研究能满足计算和性能需求的神经网络加速器。

Method: 通过模拟使用PCM和RRAM忆阻器加速的制导与控制神经网络（G&CNET），评估其在多种场景下和非理想条件下的性能。

Result: 忆阻加速器能够学习专家动作，但噪声对准确性仍有影响；重新训练可在性能下降后恢复至正常水平。

Conclusion: 研究为未来太空应用中基于忆阻器的AI加速器研究奠定了基础，展示了其潜力及进一步研究的必要性。

Abstract: In recent years, the space community has been exploring the possibilities of
Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),
for a variety of on board applications. However, this development is limited by
the restricted energy budget of smallsats and cubesats as well as radiation
concerns plaguing modern chips. This necessitates research into neural network
accelerators capable of meeting these requirements whilst satisfying the
compute and performance needs of the application. This paper explores the use
of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)
memristors for on-board in-memory computing AI acceleration in space
applications. A guidance and control neural network (G\&CNET) accelerated using
memristors is simulated in a variety of scenarios and with both device types to
evaluate the performance of memristor-based accelerators, considering device
non-idealities such as noise and conductance drift. We show that the memristive
accelerator is able to learn the expert actions, though challenges remain with
the impact of noise on accuracy. We also show that re-training after
degradation is able to restore performance to nominal levels. This study
provides a foundation for future research into memristor-based AI accelerators
for space, highlighting their potential and the need for further investigation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [161] [From Sound to Sight: Towards AI-authored Music Videos](https://arxiv.org/abs/2509.00029)
*Leo Vitasovic,Stella Graßhof,Agnes Mercedes Kloft,Ville V. Lehtola,Martin Cunneen,Justyna Starostka,Glenn McGarry,Kun Li,Sami S. Brandt*

Main category: cs.SD

TL;DR: 本文提出两条新流程，利用现成的深度学习模型自动生成音乐视频，通过潜特征技术和生成模型增强音乐可视化的表现力。


<details>
  <summary>Details</summary>
Motivation: 传统音乐可视化系统依赖手工制作的特定形状和颜色转换，表现力有限。本文旨在探索潜特征技术和深度生成模型在音乐可视化中的潜力。

Method: 采用潜特征技术分析音频，检测音乐情感和乐器模式，并用语言模型生成文本场景描述；随后利用生成模型生成相应视频片段。通过用户评估验证效果。

Result: 初步用户评估表明，生成的视频在叙事潜力、视觉连贯性和情感与音乐的一致性方面表现良好。

Conclusion: 潜特征技术和深度生成模型有望超越传统方法，提升音乐可视化的表现力和多样性。

Abstract: Conventional music visualisation systems rely on handcrafted ad hoc
transformations of shapes and colours that offer only limited expressiveness.
We propose two novel pipelines for automatically generating music videos from
any user-specified, vocal or instrumental song using off-the-shelf deep
learning models. Inspired by the manual workflows of music video producers, we
experiment on how well latent feature-based techniques can analyse audio to
detect musical qualities, such as emotional cues and instrumental patterns, and
distil them into textual scene descriptions using a language model. Next, we
employ a generative model to produce the corresponding video clips. To assess
the generated videos, we identify several critical aspects and design and
conduct a preliminary user evaluation that demonstrates storytelling potential,
visual coherency and emotional alignment with the music. Our findings
underscore the potential of latent feature techniques and deep generative
models to expand music visualisation beyond traditional approaches.

</details>


### [162] [A Survey on Evaluation Metrics for Music Generation](https://arxiv.org/abs/2509.00051)
*Faria Binte Kader,Santu Karmaker*

Main category: cs.SD

TL;DR: 论文讨论了音乐生成系统评估方法的不足，提出了评估指标的详细分类，并指出了当前方法的局限性，如客观指标与人类感知的低相关性、跨文化偏见和缺乏标准化。


<details>
  <summary>Details</summary>
Motivation: 由于音乐的复杂性（如结构、连贯性、创造力和情感表达），现有音乐生成系统的评估方法进展缓慢，亟需改进。

Method: 论文提出了音频和符号音乐表示的评估指标分类，并对当前方法的局限性进行了批判性回顾。

Result: 指出了当前评估方法的问题，如客观指标与人类感知相关性低、跨文化偏见和标准化不足。

Conclusion: 建议未来研究方向，以构建全面的音乐生成评估框架。

Abstract: Despite significant advancements in music generation systems, the
methodologies for evaluating generated music have not progressed as expected
due to the complex nature of music, with aspects such as structure, coherence,
creativity, and emotional expressiveness. In this paper, we shed light on this
research gap, introducing a detailed taxonomy for evaluation metrics for both
audio and symbolic music representations. We include a critical review
identifying major limitations in current evaluation methodologies which
includes poor correlation between objective metrics and human perception,
cross-cultural bias, and lack of standardization that hinders cross-model
comparisons. Addressing these gaps, we further propose future research
directions towards building a comprehensive evaluation framework for music
generation evaluation.

</details>


### [163] [CoComposer: LLM Multi-agent Collaborative Music Composition](https://arxiv.org/abs/2509.00132)
*Peiwen Xing,Aske Plaat,Niki van Stein*

Main category: cs.SD

TL;DR: CoComposer是一个多智能体系统，用于音乐创作，性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐工具在生成时长、音乐质量和可控性上受限。

Method: 通过五个协作智能体模拟传统作曲流程，使用AudioBox-Aesthetics系统评估。

Result: CoComposer在音乐质量和创作复杂度上优于其他多智能体系统，但音乐质量仍不及MusicLM。

Conclusion: CoComposer在多智能体系统中有优势，但音乐质量需进一步提升。

Abstract: Existing AI Music composition tools are limited in generation duration,
musical quality, and controllability. We introduce CoComposer, a multi-agent
system that consists of five collaborating agents, each with a task based on
the traditional music composition workflow. Using the AudioBox-Aesthetics
system, we experimentally evaluate CoComposer on four compositional criteria.
We test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find
(1) that CoComposer outperforms existing multi-agent LLM-based systems in music
quality, and (2) compared to a single-agent system, in production complexity.
Compared to non- LLM MusicLM, CoComposer has better interpretability and
editability, although MusicLM still produces better music.

</details>


### [164] [The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation](https://arxiv.org/abs/2509.00654)
*Ashwin Nagarajan,Hao-Wen Dong*

Main category: cs.SD

TL;DR: 研究探讨大型语言模型生成的轻量级、易于理解的修饰符是否能替代需要重训练或专用条件的现有方法，用于音乐生成中的细粒度风格控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有风格化方法中因需要重训练或专用条件而难以复现或受限的问题，尤其是在艺术家名称被限制时无法合规使用。

Method: 基于MusicGen-small模型，比较基线提示、艺术家名称提示和描述符集合，利用VGGish和CLAP嵌入评估风格控制效果。

Result: 艺术家名称是最强的控制信号，但描述符能部分替代；跨艺术家传递效果减弱，显示描述符针对特定风格线索。

Conclusion: 研究表明，单纯限制艺术家名称可能无法完全防止风格模仿，需开发可复现的评测协议来填补无名称间隙。

Abstract: Text-to-music models capture broad attributes such as instrumentation or
mood, but fine-grained stylistic control remains an open challenge. Existing
stylization methods typically require retraining or specialized conditioning,
which complicates reproducibility and limits policy compliance when artist
names are restricted. We study whether lightweight, human-readable modifiers
sampled from a large language model can provide a policy-robust alternative for
stylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish
(vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use
fifteen reference excerpts and evaluate matched seeds under three conditions:
baseline prompts, artist-name prompts, and five descriptor sets. All prompts
are generated using a large language model. Evaluation uses both VGGish and
CLAP embeddings with distributional and per-clip similarity measures, including
a new min-distance attribution metric. Results show that artist names are the
strongest control signal across both artists, while name-free descriptors
recover much of this effect. This highlights that existing safeguards such as
the restriction of artist names in music generation prompts may not fully
prevent style imitation. Cross-artist transfers reduce alignment, showing that
descriptors encode targeted stylistic cues. We also present a descriptor table
across ten contemporary artists to illustrate the breadth of the tokens.
Together these findings define the name-free gap, the controllability
difference between artist-name prompts and policy-compliant descriptors, shown
through a reproducible evaluation protocol for prompt-level controllability.

</details>


### [165] [From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation](https://arxiv.org/abs/2509.01588)
*Andrea Poltronieri,Xavier Serra,Martín Rocamora*

Main category: cs.SD

TL;DR: 本文评估了音频和弦标注者间一致性，并提出了一种基于协和度的距离度量。进一步设计了一种新的Conformer模型，通过协和度标签平滑和分解输出解决类不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决音频和弦标注中标注者主观性和数据集类不平衡问题，提升模型性能。

Method: 提出协和度距离度量评估标注一致性，并设计Conformer模型结合协和度标签平滑与分解输出方法。

Result: 协和度度量更有效捕捉标注间音乐意义一致性，新模型性能优于现有方法。

Conclusion: 协和度概念和弦分解策略为解决ACE任务中的标注分歧和类不平衡提供了有效途径。

Abstract: Audio Chord Estimation (ACE) holds a pivotal role in music information
research, having garnered attention for over two decades due to its relevance
for music transcription and analysis. Despite notable advancements, challenges
persist in the task, particularly concerning unique characteristics of harmonic
content, which have resulted in existing systems' performances reaching a glass
ceiling. These challenges include annotator subjectivity, where varying
interpretations among annotators lead to inconsistencies, and class imbalance
within chord datasets, where certain chord classes are over-represented
compared to others, posing difficulties in model training and evaluation. As a
first contribution, this paper presents an evaluation of inter-annotator
agreement in chord annotations, using metrics that extend beyond traditional
binary measures. In addition, we propose a consonance-informed distance metric
that reflects the perceptual similarity between harmonic annotations. Our
analysis suggests that consonance-based distance metrics more effectively
capture musically meaningful agreement between annotations. Expanding on these
findings, we introduce a novel ACE conformer-based model that integrates
consonance concepts into the model through consonance-based label smoothing.
The proposed model also addresses class imbalance by separately estimating
root, bass, and all note activations, enabling the reconstruction of chord
labels from decomposed outputs.

</details>


### [166] [CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays](https://arxiv.org/abs/2509.01399)
*Runduo Han,Yanxin Hu,Yihui Fu,Zihan Zhang,Yukai Jv,Li Chen,Lei Xie*

Main category: cs.SD

TL;DR: CabinSep是一种轻量级的语音分离方法，用于减少语音识别错误，通过结合空间特征、MVDR技术和数据增强，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 提高人机交互中重叠语音的分离效果，减少后端语音识别的错误。

Method: 利用通道信息提取空间特征、应用MVDR减少语音失真、结合模拟和真实录音的数据增强方法。

Result: 在计算复杂度仅为0.4 GMACs时，与现有最佳模型相比，语音识别错误率相对降低了17.5%。

Conclusion: CabinSep在轻量化和性能上均表现出色，适用于实际场景，尤其是车载环境。

Abstract: Separating overlapping speech from multiple speakers is crucial for effective
human-vehicle interaction. This paper proposes CabinSep, a lightweight neural
mask-based minimum variance distortionless response (MVDR) speech separation
approach, to reduce speech recognition errors in back-end automatic speech
recognition (ASR) models. Our contributions are threefold: First, we utilize
channel information to extract spatial features, which improves the estimation
of speech and noise masks. Second, we employ MVDR during inference, reducing
speech distortion to make it more ASR-friendly. Third, we introduce a data
augmentation method combining simulated and real-recorded impulse responses
(IRs), improving speaker localization at zone boundaries and further reducing
speech recognition errors. With a computational complexity of only 0.4 GMACs,
CabinSep achieves a 17.5% relative reduction in speech recognition error rate
in a real-recorded dataset compared to the state-of-the-art DualSep model.
Demos are available at: https://cabinsep.github.io/cabinsep/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [167] [A Survey on Open Dataset Search in the LLM Era: Retrospectives and Perspectives](https://arxiv.org/abs/2509.00728)
*Pengyue Li,Sheng Wang,Hua Dai,Zhiyu Chen,Zhifeng Bao,Brian D. Davison*

Main category: cs.IR

TL;DR: 本文综述了开放数据集搜索的最新进展，强调基于示例的搜索、高级相似性测量和搜索加速技术，并探讨了大规模语言模型（LLMs）在此领域的应用与互惠关系。


<details>
  <summary>Details</summary>
Motivation: 高质量数据集对数据驱动任务至关重要，但如何高效准确地满足用户需求是一个重大挑战，因此需要对开放数据集搜索进行全面综述。

Method: 聚焦于超越传统方法的开放数据集搜索技术，包括基于示例的搜索、内容相似性测量和搜索加速技术。

Result: LLMs在查询理解、语义建模和交互指导方面展现了潜力，同时数据集搜索的进步可支持LLMs的提升。

Conclusion: 总结了开放数据集搜索的研究现状，提出了未来方向，为研究者和实践者提供了结构化参考。

Abstract: High-quality datasets are typically required for accomplishing data-driven
tasks, such as training medical diagnosis models, predicting real-time traffic
conditions, or conducting experiments to validate research hypotheses.
Consequently, open dataset search, which aims to ensure the efficient and
accurate fulfillment of users' dataset requirements, has emerged as a critical
research challenge and has attracted widespread interest. Recent studies have
made notable progress in enhancing the flexibility and intelligence of open
dataset search, and large language models (LLMs) have demonstrated strong
potential in addressing long-standing challenges in this area. Therefore, a
systematic and comprehensive review of the open dataset search problem is
essential, detailing the current state of research and exploring future
directions. In this survey, we focus on recent advances in open dataset search
beyond traditional approaches that rely on metadata and keywords. From the
perspective of dataset modalities, we place particular emphasis on
example-based dataset search, advanced similarity measurement techniques based
on dataset content, and efficient search acceleration techniques. In addition,
we emphasize the mutually beneficial relationship between LLMs and open dataset
search. On the one hand, LLMs help address complex challenges in query
understanding, semantic modeling, and interactive guidance within open dataset
search. In turn, advances in dataset search can support LLMs by enabling more
effective integration into retrieval-augmented generation (RAG) frameworks and
data selection processes, thereby enhancing downstream task performance.
Finally, we summarize open research problems and outline promising directions
for future work. This work aims to offer a structured reference for researchers
and practitioners in the field of open dataset search.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [168] [Agentic Workflow for Education: Concepts and Applications](https://arxiv.org/abs/2509.01517)
*Yuan-Hao Jiang,Yijie Lu,Ling Dai,Jiatong Wang,Ruijia Li,Bo Jiang*

Main category: cs.CY

TL;DR: 论文提出了一种教育代理工作流（AWE）模型，通过动态非线性流程提升教育任务的个性化和协作性，并在数学测试生成案例中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）和AI代理的快速发展，代理工作流在教育中展现出变革潜力。作者旨在从传统的静态提示-响应系统转向动态非线性工作流。

Method: 提出AWE模型，包含四个组件：自我反思、工具调用、任务规划和多代理协作，并基于冯·诺伊曼多代理系统（MAS）架构构建理论框架。

Result: 通过数学测试生成案例验证，AWE生成的题目与真实考试题在统计上具有可比性。

Conclusion: AWE为减少教师工作量、提升教学质量和推动教育创新提供了可行路径。

Abstract: With the rapid advancement of Large Language Models (LLMs) and Artificial
Intelligence (AI) agents, agentic workflows are showing transformative
potential in education. This study introduces the Agentic Workflow for
Education (AWE), a four-component model comprising self-reflection, tool
invocation, task planning, and multi-agent collaboration. We distinguish AWE
from traditional LLM-based linear interactions and propose a theoretical
framework grounded in the von Neumann Multi-Agent System (MAS) architecture.
Through a paradigm shift from static prompt-response systems to dynamic,
nonlinear workflows, AWE enables scalable, personalized, and collaborative task
execution. We further identify four core application domains: integrated
learning environments, personalized AI-assisted learning, simulation-based
experimentation, and data-driven decision-making. A case study on automated
math test generation shows that AWE-generated items are statistically
comparable to real exam questions, validating the model's effectiveness. AWE
offers a promising path toward reducing teacher workload, enhancing
instructional quality, and enabling broader educational innovation.

</details>


### [169] [The Living Library of Trees: Mapping Knowledge Ecology in the Arnold Arboretum](https://arxiv.org/abs/2509.00114)
*Johan Malmstedt,Giacomo Nanni,Dario Rodighiero*

Main category: cs.CY

TL;DR: 本项目探讨哈佛大学阿诺德植物园在生物多样性与气候变化背景下的研究价值，结合历史数据与计算方法，揭示植物与人之间的传记关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示植物园作为基础设施的重要作用，通过数据可视化探索植物与人类活动的多维度联系。

Method: 结合历史分析与计算方法，运用人工智能、地理空间绘图与信息设计技术，分析植物园数据。

Result: 构建的平台揭示了植物护理与科学观察的模式，以及植物数据中的集体维度。

Conclusion: 研究表明植物园是一个共享代理系统，展示了设计在连接档案记录与未来护理中的潜力。

Abstract: As biodiversity loss and climate change accelerate, botanical gardens serve
as vital infrastructures for research, education, and conservation. This
project focuses on the Arnold Arboretum of Harvard University, a 281-acre
living museum founded in 1872 in Boston. Drawing on more than a century of
curatorial data, the research combines historical analysis with computational
methods to visualize the biographies of plants and people. The resulting
platform reveals patterns of care and scientific observations, along with the
collective dimensions embedded in botanical data. Using techniques from
artificial intelligence, geospatial mapping, and information design, the
project frames the arboretum as a system of shared agency--an active archive of
more-than-human affinities that records the layered memory of curatorial labor,
the situated nature of knowledge production, and the potential of design to
bridge archival record and future care.

</details>


### [170] [Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms](https://arxiv.org/abs/2509.00167)
*W. F. Lamberti,S. R. Lawrence,D. White,S. Kim,S. Abdullah*

Main category: cs.CY

TL;DR: 生成式AI在教育中的应用日益广泛，但其对学生批判性思维的培养作用仍有待探索。本研究通过学生对AI生成内容的评估，初步探讨了他们在计算与数据科学课程中的批判性思维能力。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在学生批判性思维培养中的潜在作用，尤其是在评估AI生成内容的准确性和适用性方面。

Method: 设计学习活动，要求学生分析、批评并修改AI生成的解决方案，测试其批判性思维能力。

Result: 初步发现学生在批判性评估AI内容方面具有一定能力，为未来更全面研究奠定了基础。

Conclusion: 生成式AI在教育中具有潜力，但仍需更多研究来优化其在批判性思维培养中的应用。

Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings,
yet their role in fostering critical thinking remains underexplored. While
previous studies have examined GAI as a tutor for specific lessons or as a tool
for completing assignments, few have addressed how students critically evaluate
the accuracy and appropriateness of GAI-generated responses. This pilot study
investigates students' ability to apply structured critical thinking when
assessing Generative AI outputs in introductory Computational and Data Science
courses. Given that GAI tools often produce contextually flawed or factually
incorrect answers, we designed learning activities that require students to
analyze, critique, and revise AI-generated solutions. Our findings offer
initial insights into students' ability to engage critically with GAI content
and lay the groundwork for more comprehensive studies in future semesters.

</details>


### [171] [Can AI be Auditable?](https://arxiv.org/abs/2509.00575)
*Himanshu Verma,Kirtan Path,Eva Thelisson*

Main category: cs.CY

TL;DR: AI系统的可审计性需要独立评估其合规性，受法规推动但仍面临技术和管理挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过法规和协作确保AI系统在伦理、法律和技术上的合规性。

Method: 分析现有法规（如欧盟AI法案）和挑战（如技术不透明、缺乏标准工具）。

Result: 需要清晰的指南、国际协调和多方协作来实现有效的AI审计生态。

Conclusion: 可审计性必须融入AI开发和治理，以确保系统功能和伦理法律一致。

Abstract: Auditability is defined as the capacity of AI systems to be independently
assessed for compliance with ethical, legal, and technical standards throughout
their lifecycle. The chapter explores how auditability is being formalized
through emerging regulatory frameworks, such as the EU AI Act, which mandate
documentation, risk assessments, and governance structures. It analyzes the
diverse challenges facing AI auditability, including technical opacity,
inconsistent documentation practices, lack of standardized audit tools and
metrics, and conflicting principles within existing responsible AI frameworks.
The discussion highlights the need for clear guidelines, harmonized
international regulations, and robust socio-technical methodologies to
operationalize auditability at scale. The chapter concludes by emphasizing the
importance of multi-stakeholder collaboration and auditor empowerment in
building an effective AI audit ecosystem. It argues that auditability must be
embedded in AI development practices and governance infrastructures to ensure
that AI systems are not only functional but also ethically and legally aligned.

</details>


### [172] [Understanding Fanchuan in Livestreaming Platforms: A New Form of Online Antisocial Behavior](https://arxiv.org/abs/2509.00780)
*Yiluo Wei,Jiahui He,Gareth Tyson*

Main category: cs.CY

TL;DR: 该论文研究了新兴的在线反社会行为'反串'（fanchuan），通过分析Bilibili平台的直播聊天数据，揭示了其特点和传播方式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了理解并应对'反串'这种具有隐蔽性和长期战略目标的恶意行为，其独特性和对内容审核的新挑战促使了这项研究。

Method: 研究方法是基于Bilibili平台的2.7百万直播会话和3.6亿条聊天消息的数据集，识别并分析了13万次'反串'行为。

Result: 研究结果揭示了'反串'行为的特点，并提供了关于其传播方式和行为者的重要见解。

Conclusion: 结论强调了理解'反串'行为的重要性，并提出了对抗这一新兴恶意行为的必要性。

Abstract: Recently, a distinct form of online antisocial behavior, known as "fanchuan",
has emerged across online platforms, particularly in livestreaming chats.
Fanchuan is an indirect attack on a specific entity, such as a celebrity, video
game, or brand. It entails two main actions: (i) individuals first feign
support for the entity, and exhibit this allegiance widely; (ii) they then
engage in offensive or irritating behavior, attempting to undermine the entity
by association. This deceptive conduct is designed to tarnish the reputation of
the target and/or its fan community. Fanchuan is a novel, covert and indirect
form of social attack, occurring outside the targeted community (often in a
similar or broader community), with strategic long-term objectives. This
distinguishes fanchuan from other types of antisocial behavior and presents
significant new challenges in moderation. We argue it is crucial to understand
and combat this new malicious behavior. Therefore, we conduct the first
empirical study on fanchuan behavior in livestreaming chats, focusing on
Bilibili, a leading livestreaming platform in China. Our dataset covers 2.7
million livestreaming sessions on Bilibili, featuring 3.6 billion chat
messages. We identify 130k instances of fanchuan behavior across 37.4k
livestreaming sessions. Through various types of analysis, our research offers
valuable insights into fanchuan behavior and its perpetrators.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [173] [BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators](https://arxiv.org/abs/2509.01742)
*Yitong Guo,Hongbo Chen,Haobin Hiroki Chen,Yukui Luo,XiaoFeng Wang,Chenghong Wang*

Main category: cs.CR

TL;DR: BOLT是一种基于HBM的高性能无感知地图（OMAP）加速器，通过创新的算法和架构设计，显著降低了带宽开销和性能损耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统OMAP因随机重映射和最坏情况填充导致的高开销问题，利用现代加速器的HBM特性，提出了一种新的解决方案。

Method: 结合隔离HBM作为不可观测缓存的新算法、自我托管架构以及算法-架构协同设计，实现了高效的OMAP加速。

Result: BOLT在初始化和查询时间上分别比现有OMAP实现快279倍和480倍，首次达到O(1) + O((log log N)^2)带宽开销。

Conclusion: BOLT通过HBM和创新的设计，显著提升了OMAP的性能，克服了传统方法的性能限制。

Abstract: While Trusted Execution Environments provide a strong foundation for secure
cloud computing, they remain vulnerable to access pattern leakages. Oblivious
Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high
overhead due to randomized remapping and worst-case padding. We argue these
costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory
(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that
eavesdropping on HBM is difficult -- even for physical attackers -- as its
memory channels are sealed together with processor cores inside the same
physical package. Later, Hunt et al. [NSDI'20] show that, with proper
isolation, HBM can be turned into an unobservable region where both data and
memory traces are hidden. This motivates a rethink of OMAP design with
HBM-backed solutions to finally overcome their traditional performance limits.
Building on these insights, we present BOLT, a Bandwidth Optimized,
Lightning-fast OMAP accelerator that, for the first time, achieves O(1) +
O((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)
a new OMAP algorithm that leverages isolated HBM as an unobservable cache to
accelerate oblivious access to large host memory; (ii) a self-hosted
architecture that offloads execution and memory control from the host to
mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs
that maximize resource efficiency. We implement a prototype BOLT on a Xilinx
U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in
initialization and query time, respectively, over state-of-the-art OMAPs,
including an industry implementation from Facebook.

</details>


### [174] [Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves](https://arxiv.org/abs/2509.00615)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: 研究如何用差分隐私保护的患者数据计算多区域Kaplan-Meier曲线，评估了四种平滑技术，显示全变差方法最优，且在隐私预算较高时临床信息可安全共享。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保护患者隐私（通过节点级差分隐私）的情况下，计算多个医疗管辖区的Kaplan-Meier生存曲线。

Method: 每个站点仅披露一次其曲线，并在通用时间网格长度决定的尺度上添加Laplace噪声；服务器对这些带噪声的曲线进行平均，从而保持总体隐私预算不变。研究测试了四种一次平滑技术：离散余弦变换、Haar小波收缩、自适应全变差去噪和参数Weibull拟合，并在五种隐私级别和三种分区场景下进行了基准测试。

Result: 全变差方法在平均精度上表现最佳，频域平滑方法在最坏情况下具有更强的鲁棒性，而Weibull模型在最严格的隐私设置下表现最稳定。所有方法在隐私预算为0.5及以上时，释放的曲线将经验对数秩I型误差控制在15%以下。

Conclusion: 研究表明，无需迭代训练或复杂加密技术，即可共享具有临床价值的生存信息。

Abstract: We investigate how to calculate Kaplan-Meier survival curves across multiple
health-care jurisdictions while protecting patient privacy with node-level
differential privacy. Each site discloses its curve only once, adding Laplace
noise whose scale is determined by the length of the common time grid; the
server then averages the noisy curves, so the overall privacy budget remains
unchanged. We benchmark four one-shot smoothing techniques: Discrete Cosine
Transform, Haar Wavelet shrinkage, adaptive Total-Variation denoising, and a
parametric Weibull fit on the NCCTG lung-cancer cohort under five privacy
levels and three partition scenarios (uniform, moderately skewed, highly
imbalanced). Total-Variation gives the best mean accuracy, whereas the
frequency-domain smoothers offer stronger worst-case robustness and the Weibull
model shows the most stable behaviour at the strictest privacy setting. Across
all methods the released curves keep the empirical log-rank type-I error below
fifteen percent for privacy budgets of 0.5 and higher, demonstrating that
clinically useful survival information can be shared without iterative training
or heavy cryptography.

</details>


### [175] [LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT](https://arxiv.org/abs/2509.01434)
*Handi Chen,Jing Deng,Xiuzhe Wu,Zhihan Jiang,Xinchen Zhang,Xianhao Chen,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: LiFeChain是一种专为联邦终身学习（FLL）设计的轻量级区块链，通过最小化链上披露和双向验证，提供防篡改账本，解决物联网系统中长期攻击和数据异构性问题。


<details>
  <summary>Details</summary>
Motivation: 物联网设备生成异构数据流，需要持续去中心化智能解决方案。FLL结合联邦学习和终身学习以克服灾难性遗忘，但容易受到长期攻击，且标准单服务器架构存在单点故障风险。

Method: LiFeChain结合两种机制：服务器上的Proof-of-Model-Correlation（PoMC）共识，耦合学习和遗忘机制以减少负迁移；客户端的分段零知识仲裁（Seg-ZA），检测异常行为而不泄露隐私。

Result: 实验表明，LiFeChain不仅能抵御两种长期攻击提升模型性能，同时保持高效率和可扩展性。

Conclusion: LiFeChain是首个专为FLL设计的区块链，可作为即插即用组件无缝集成现有算法，提升安全性与效率。

Abstract: The expansion of Internet of Things (IoT) devices constantly generates
heterogeneous data streams, driving demand for continuous, decentralized
intelligence. Federated Lifelong Learning (FLL) provides an ideal solution by
incorporating federated and lifelong learning to overcome catastrophic
forgetting. The extended lifecycle of FLL in IoT systems increases their
vulnerability to persistent attacks, and these risks may be obscured by
performance degradation caused by spatial-temporal data heterogeneity.
Moreover, this problem is exacerbated by the standard single-server
architecture, as its single point of failure makes it difficult to maintain a
reliable audit trail for long-term threats. Blockchain provides a tamper-proof
foundation for trustworthy FLL systems. Nevertheless, directly applying
blockchain to FLL significantly increases computational and retrieval costs
with the expansion of the knowledge base, slowing down the training on IoT
devices. To address these challenges, we propose LiFeChain, a lightweight
blockchain for secure and efficient federated lifelong learning by providing a
tamper-resistant ledger with minimal on-chain disclosure and bidirectional
verification. To the best of our knowledge, LiFeChain is the first blockchain
tailored for FLL. LiFeChain incorporates two complementary mechanisms: the
proof-of-model-correlation (PoMC) consensus on the server, which couples
learning and unlearning mechanisms to mitigate negative transfer, and segmented
zero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates
abnormal committee behavior without compromising privacy. LiFeChain is designed
as a plug-and-play component that can be seamlessly integrated into existing
FLL algorithms. Experimental results demonstrate that LiFeChain not only
enhances model performance against two long-term attacks but also sustains high
efficiency and scalability.

</details>


### [176] [Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs](https://arxiv.org/abs/2509.02372)
*Zhiyang Chen,Tara Saba,Xun Deng,Xujie Si,Fan Long*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have become critical to modern software
development, but their reliance on internet datasets for training introduces a
significant security risk: the absorption and reproduction of malicious
content. To evaluate this threat, this paper introduces a scalable, automated
audit framework that synthesizes innocuous, developer-style prompts from known
scam databases to query production LLMs and determine if they generate code
containing harmful URLs. We conducted a large-scale evaluation across four
production LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and
found a systemic vulnerability, with all tested models generating malicious
code at a non-negligible rate. On average, 4.2\% of programs generated in our
experiments contained malicious URLs. Crucially, this malicious code is often
generated in response to benign prompts. We manually validate the prompts which
cause all four LLMs to generate malicious code, and resulting in 177 innocuous
prompts that trigger all models to produce harmful outputs. These results
provide strong empirical evidence that the training data of production LLMs has
been successfully poisoned at scale, underscoring the urgent need for more
robust defense mechanisms and post-generation safety checks to mitigate the
propagation of hidden security threats.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [177] [KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation](https://arxiv.org/abs/2509.00366)
*Ziyi Guan,Jason Chun Lok Li,Zhijian Hou,Pingping Zhang,Donglai Xu,Yuzhi Zhao,Mengyang Wu,Jinpeng Chen,Thanh-Toan Nguyen,Pengfei Xian,Wenao Ma,Shengchao Qin,Graziano Chesi,Ngai Wong*

Main category: cs.MA

TL;DR: KG-RAG是一个基于知识图谱的检索增强生成框架，通过优化UI转换图的提取与集成，显著提升GUI代理在复杂移动任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理在复杂移动任务中表现不佳，主要由于缺乏应用特定知识且UI转换图的利用率低。

Method: 提出KG-RAG框架，将碎片化的UI转换图转化为结构化向量数据库，采用意图引导的LLM搜索方法生成可操作的导航路径。

Result: 实验表明，KG-RAG在多个移动应用中成功率提升8.9%，决策准确率提升8.1%，并减少了任务步骤数。

Conclusion: KG-RAG不仅能提升移动应用中的任务完成率，还能扩展到Web和桌面环境，为实际部署提供了可行的权衡方案。

Abstract: Despite recent progress, Graphic User Interface (GUI) agents powered by Large
Language Models (LLMs) struggle with complex mobile tasks due to limited
app-specific knowledge. While UI Transition Graphs (UTGs) offer structured
navigation representations, they are underutilized due to poor extraction and
inefficient integration. We introduce KG-RAG, a Knowledge Graph-driven
Retrieval-Augmented Generation framework that transforms fragmented UTGs into
structured vector databases for efficient real-time retrieval. By leveraging an
intent-guided LLM search method, KG-RAG generates actionable navigation paths,
enhancing agent decision-making. Experiments across diverse mobile apps show
that KG-RAG outperforms existing methods, achieving a 75.8% success rate (8.9%
improvement over AutoDroid), 84.6% decision accuracy (8.1% improvement), and
reducing average task steps from 4.5 to 4.1. Additionally, we present
KG-Android-Bench and KG-Harmony-Bench, two benchmarks tailored to the Chinese
mobile ecosystem for future research. Finally, KG-RAG transfers to web/desktop
(+40% SR on Weibo-web; +20% on QQ Music-desktop), and a UTG cost ablation shows
accuracy saturates at ~4h per complex app, enabling practical deployment
trade-offs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [178] [U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks](https://arxiv.org/abs/2509.00055)
*Tongtong Feng,Xin Wang,Feilin Han,Leping Zhang,Wenwu Zhu*

Main category: cs.RO

TL;DR: 论文提出了首个针对长时任务的大规模无人机集群自主飞行数据集U2UData-2，并开发了一个可扩展的数据收集与算法验证平台，支持多样化的场景和任务定制。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注基础任务，受限于数据集，无法实现长时任务的真实部署，需要处理长期依赖、状态保持和目标动态变化等挑战。

Method: 提出U2UData-2数据集和在线收集与闭环验证平台，包含12个场景、720条轨迹、120小时飞行数据，支持多种传感器和任务定制。

Result: 数据集规模大、覆盖全面，平台支持高效算法验证，并针对野生动物保护任务提供了基准测试。

Conclusion: U2UData-2为长时任务的研究提供了重要资源，推动了无人机集群自主飞行技术的发展。

Abstract: Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for
advancing the low-altitude economy. However, existing methods focus only on
specific basic tasks due to dataset limitations, failing in real-world
deployment for LH tasks. LH tasks are not mere concatenations of basic tasks,
requiring handling long-term dependencies, maintaining persistent states, and
adapting to dynamic goal shifts. This paper presents U2UData-2, the first
large-scale swarm UAV autonomous flight dataset for LH tasks and the first
scalable swarm UAV data online collection and algorithm closed-loop
verification platform. The dataset is captured by 15 UAVs in autonomous
collaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120
hours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.
This dataset also includes brightness, temperature, humidity, smoke, and
airflow values covering all flight routes. The platform supports the
customization of simulators, UAVs, sensors, flight algorithms, formation modes,
and LH tasks. Through a visual control window, this platform allows users to
collect customized datasets through one-click deployment online and to verify
algorithms by closed-loop simulation. U2UData-2 also introduces an LH task for
wildlife conservation and provides comprehensive benchmarks with 9 SOTA models.
U2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.

</details>


### [179] [Constrained Decoding for Robotics Foundation Models](https://arxiv.org/abs/2509.01728)
*Parv Kapoor,Akila Ganlath,Changliu Liu,Sebastian Scherer,Eunsuk Kang*

Main category: cs.RO

TL;DR: 该论文提出了一种约束解码框架，确保机器人基础模型生成的行动在运行时满足信号时序逻辑规范，无需重新训练，同时保持对基础模型的独立性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人基础模型在多任务泛化方面表现优异，但它们缺乏对行为正确性和安全约束的明确考虑，因此存在局限性。

Method: 通过引入约束解码框架，在动态系统中对行动轨迹强制执行逻辑约束。

Result: 实验表明，该方法不仅能过滤不安全行动，还能实现条件行动生成。

Conclusion: 该方法在运行时显著提升了机器人基础模型的安全性和行为正确性。

Abstract: Recent advances in the development of robotic foundation models have led to
promising end-to-end and general-purpose capabilities in robotic systems. These
models are pretrained on vast datasets of robot trajectories to process multi-
modal inputs and directly output a sequence of action that the system then
executes in the real world. Although this approach is attractive from the
perspective of im- proved generalization across diverse tasks, these models are
still data-driven and, therefore, lack explicit notions of behavioral
correctness and safety constraints. We address these limitations by introducing
a constrained decoding framework for robotics foundation models that enforces
logical constraints on action trajec- tories in dynamical systems. Our method
ensures that generated actions provably satisfy signal temporal logic (STL)
specifications at runtime without retraining, while remaining agnostic of the
underlying foundation model. We perform com- prehensive evaluation of our
approach across state-of-the-art navigation founda- tion models and we show
that our decoding-time interventions are useful not only for filtering unsafe
actions but also for conditional action-generation. Videos available on our
website: https://constrained-robot-fms.github.io

</details>


### [180] [CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction](https://arxiv.org/abs/2509.00660)
*Felipe Arias-Russi,Yuanchen Bai,Angelique Taylor*

Main category: cs.RO

TL;DR: 论文介绍了CARIS系统，一种上下文自适应的人机交互工具，解决了现有WoZ工具在不同场景下的适应性问题。通过两个案例展示了其潜力，并指出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 解决传统WoZ工具在不同情境、用户和机器人平台上的适应性不足问题。

Method: 开发了CARIS系统，结合远程操控、人类感知、人机对话和多模态数据记录等功能。

Result: 在心理健康伴侣和导游两个场景中验证了CARIS的有效性，并提出了改进建议。

Conclusion: CARIS为HRI领域提供了一个公开可用的上下文自适应工具，支持数据驱动的智能机器人行为研究。

Abstract: The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz
(WoZ) controlled robots to explore navigation, conversational dynamics,
human-in-the-loop interactions, and more to explore appropriate robot behaviors
in everyday settings. However, existing WoZ tools are often limited to one
context, making them less adaptable across different settings, users, and
robotic platforms. To mitigate these issues, we introduce a Context-Adaptable
Robot Interface System (CARIS) that combines advanced robotic capabilities such
teleoperation, human perception, human-robot dialogue, and multimodal data
recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ
control a robot in two contexts: 1) mental health companion and as a 2) tour
guide. Furthermore, we identified areas of improvement for CARIS, including
smoother integration between movement and communication, clearer functionality
separation, recommended prompts, and one-click communication options to enhance
the usability wizard control of CARIS. This project offers a publicly
available, context-adaptable tool for the HRI community, enabling researchers
to streamline data-driven approaches to intelligent robot behavior.

</details>


### [181] [Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC](https://arxiv.org/abs/2509.01450)
*Ane San Martin,Michael Hagenow,Julie Shah,Johan Kildal,Elena Lazkano*

Main category: cs.RO

TL;DR: 研究探讨人与机器人协作中，未来辅助代理的设计关键，通过用户实验分析远程人类辅助及未来非人类代理的接受度。


<details>
  <summary>Details</summary>
Motivation: 随着机器人技术进步，人机协作在工业任务中日益普遍，研究目标是设计更有效的用户辅助代理。

Method: 通过定性与定量数据分析用户研究中远程人类辅助对人机协作装配任务的影响，评估用户在求助及接受帮助时的体验。

Result: 研究揭示了用户对非人类辅助代理的接受度，以及辅助请求方式（需求驱动与非请求式）对情绪、生产力和偏好的影响。

Conclusion: 未来辅助代理设计需平衡用户接受度与任务效率，明确辅助方式的选择对协作效果的重要性。

Abstract: As robot technology advances, collaboration between humans and robots will
become more prevalent in industrial tasks. When humans run into issues in such
scenarios, a likely future involves relying on artificial agents or robots for
aid. This study identifies key aspects for the design of future user-assisting
agents. We analyze quantitative and qualitative data from a user study
examining the impact of on-demand assistance received from a remote human in a
human-robot collaboration (HRC) assembly task. We study scenarios in which
users require help and we assess their experiences in requesting and receiving
assistance. Additionally, we investigate participants' perceptions of future
non-human assisting agents and whether assistance should be on-demand or
unsolicited. Through a user study, we analyze the impact that such design
decisions (human or artificial assistant, on-demand or unsolicited help) can
have on elicited emotional responses, productivity, and preferences of humans
engaged in HRC tasks.

</details>


### [182] [Speculative Design of Equitable Robotics: Queer Fictions and Futures](https://arxiv.org/abs/2509.01643)
*Minja Axelsson*

Main category: cs.RO

TL;DR: 本文通过探索性论文形式探讨了公平机器人这一话题，特别关注LGBTQ+群体设计和使用的机器人，旨在引发关于理想化酷儿机器人未来的思考。


<details>
  <summary>Details</summary>
Motivation: 激发关于酷儿机器人未来在艺术和科学领域的讨论，探索其潜力和伦理问题。

Method: 通过三个推测性设计提案：1）反映酷儿用户的身份；2）用酷儿机器人身份表演减少偏见；3）建立酷儿机器人网络共享资源。

Result: 提出了酷儿机器人的潜在角色和未来发展方向，并探讨了伦理问题。

Conclusion: 文章总结了酷儿机器人的未来可能性，并提出了实现这一愿景所需的努力。

Abstract: This paper examines the speculative topic of equitable robots through an
exploratory essay format. It focuses specifically on robots by and for LGBTQ+
populations. It aims to provoke thought and conversations in the field about
what aspirational queer robotics futures may look like, both in the arts and
sciences. First, it briefly reviews the state-of-the-art of queer robotics in
fiction and science, drawing together threads from each. Then, it discusses
queering robots through three speculative design proposals for queer robot
roles: 1) reflecting the queerness of their ''in-group'' queer users, building
and celebrating ''in-group'' identity, 2) a new kind of queer activism by
implementing queer robot identity performance to interact with ''out-group''
users, with a goal of reducing bigotry through familiarisation, and 3) a
network of queer-owned robots, through which the community could reach each
other, and distribute and access important resources. The paper then questions
whether robots should be queered, and what ethical implications this raises.
Finally, the paper makes suggestions for what aspirational queer robotics
futures may look like, and what would be required to get there.

</details>


### [183] [MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation](https://arxiv.org/abs/2509.01996)
*Chi Sun,Xian Wang,Abhishek Kumar,Chengbin Cui,Lik-Hang Lee*

Main category: cs.RO

TL;DR: 论文提出了一个结合虚拟导纳模型和多模态CNN网络的共享控制框架，用于提升多物体遥操作中的人机交互性能和用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟现实环境中多物体遥操作任务中的感知模糊性和单模态意图识别的局限性。

Method: 使用虚拟导纳模型优化运动轨迹，并结合多模态CNN网络（MMIPN）处理多模态输入以估计人类抓取意图。

Result: 用户研究表明，MMIPN显著提高了抓取成功率，虚拟导纳模型减少了路径长度。

Conclusion: 结合多模态线索和隐式指导在多物体抓取任务中表现出色，为未来更自然的交互提供了解决方案。

Abstract: Effective human-robot interaction (HRI) in multi-object teleoperation tasks
faces significant challenges due to perceptual ambiguities in virtual reality
(VR) environments and the limitations of single-modality intention recognition.
This paper proposes a shared control framework that combines a virtual
admittance (VA) model with a Multimodal-CNN-based Human Intention Perception
Network (MMIPN) to enhance teleoperation performance and user experience. The
VA model employs artificial potential fields to guide operators toward target
objects by adjusting admittance force and optimizing motion trajectories. MMIPN
processes multimodal inputs, including gaze movement, robot motions, and
environmental context, to estimate human grasping intentions, helping to
overcome depth perception challenges in VR. Our user study evaluated four
conditions across two factors, and the results showed that MMIPN significantly
improved grasp success rates, while the VA model enhanced movement efficiency
by reducing path lengths. Gaze data emerged as the most crucial input modality.
These findings demonstrate the effectiveness of combining multimodal cues with
implicit guidance in VR-based teleoperation, providing a robust solution for
multi-object grasping tasks and enabling more natural interactions across
various applications in the future.

</details>


### [184] [OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments](https://arxiv.org/abs/2509.02425)
*Yifan Xu,Qianwei Wang,Vineet Kamat,Carol Menassa*

Main category: cs.RO

TL;DR: OpenGuide是一个结合自然语言理解和视觉语言模型的辅助机器人系统，用于帮助视障人士在复杂室内环境中高效搜索多对象。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术多关注基础导航或障碍避让，但在复杂、部分可观察环境中缺乏高效的多对象搜索能力。

Method: 系统整合自然语言理解、视觉语言模型、前沿探索和POMDP规划器，支持开放词汇请求和适应性导航。

Result: 实验显示OpenGuide在搜索成功率和效率上显著优于现有方法。

Conclusion: 研究为辅助生活环境中的规模化、人本化机器人协助奠定基础。

Abstract: Indoor built environments like homes and offices often present complex and
cluttered layouts that pose significant challenges for individuals who are
blind or visually impaired, especially when performing tasks that involve
locating and gathering multiple objects. While many existing assistive
technologies focus on basic navigation or obstacle avoidance, few systems
provide scalable and efficient multi-object search capabilities in real-world,
partially observable settings. To address this gap, we introduce OpenGuide, an
assistive mobile robot system that combines natural language understanding with
vision-language foundation models (VLM), frontier-based exploration, and a
Partially Observable Markov Decision Process (POMDP) planner. OpenGuide
interprets open-vocabulary requests, reasons about object-scene relationships,
and adaptively navigates and localizes multiple target items in novel
environments. Our approach enables robust recovery from missed detections
through value decay and belief-space reasoning, resulting in more effective
exploration and object localization. We validate OpenGuide in simulated and
real-world experiments, demonstrating substantial improvements in task success
rate and search efficiency over prior methods. This work establishes a
foundation for scalable, human-centered robotic assistance in assisted living
environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [185] [Performance is not All You Need: Sustainability Considerations for Algorithms](https://arxiv.org/abs/2509.00045)
*Xiang Li,Chong Zhang,Hongpeng Wang,Shreyank Narayana Gowda,Yushi Li,Xiaobo Jin*

Main category: cs.CV

TL;DR: 该研究提出了一种创新的二维可持续性评估系统，用于平衡深度学习模型训练的性能与能耗，并引入了FMS和ASC两个量化指标。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型训练产生的高碳排放问题，平衡算法性能与能源消耗。

Method: 提出可持续性谐波均值（FMS）和可持续性曲线下面积（ASC）两个指标，构建多模态任务基准验证其普适性。

Result: 实验表明该系统可为跨任务算法评估提供量化依据，推动绿色AI研究从理论转向实践。

Conclusion: 该框架为行业建立算法能效标准提供了方法支持。

Abstract: This work focuses on the high carbon emissions generated by deep learning
model training, specifically addressing the core challenge of balancing
algorithm performance and energy consumption. It proposes an innovative
two-dimensional sustainability evaluation system. Different from the
traditional single performance-oriented evaluation paradigm, this study
pioneered two quantitative indicators that integrate energy efficiency ratio
and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy
consumption and performance parameters through the harmonic mean to reveal the
algorithm performance under unit energy consumption; the area under the
sustainability curve (ASC) constructs a performance-power consumption curve to
characterize the energy efficiency characteristics of the algorithm throughout
the cycle. To verify the universality of the indicator system, the study
constructed benchmarks in various multimodal tasks, including image
classification, segmentation, pose estimation, and batch and online learning.
Experiments demonstrate that the system can provide a quantitative basis for
evaluating cross-task algorithms and promote the transition of green AI
research from theory to practice. Our sustainability evaluation framework code
can be found here, providing methodological support for the industry to
establish algorithm energy efficiency standards.

</details>


### [186] [A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation](https://arxiv.org/abs/2509.01919)
*Seohyun Kim,Junyoung Lee,Jongho Park,Jinhyung Koo,Sungjin Lee,Yeseong Kim*

Main category: cs.CV

TL;DR: DiTTO是一个基于扩散的新框架，用于生成高保真、多样化的多设备存储追踪数据。


<details>
  <summary>Details</summary>
Motivation: 现有的存储追踪数据生成方法难以满足高保真和多样性需求。

Method: 利用先进的扩散技术生成连续追踪数据，捕捉时间动态和设备间依赖关系。

Result: 实验显示DiTTO生成的追踪数据保真度高且多样化，配置误差仅8%。

Conclusion: DiTTO为多设备存储追踪数据生成提供了高效且可配置的解决方案。

Abstract: We propose DiTTO, a novel diffusion-based framework for generating realistic,
precisely configurable, and diverse multi-device storage traces. Leveraging
advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity
continuous traces that capture temporal dynamics and inter-device dependencies
with user-defined configurations. Our experimental results demonstrate that
DiTTO can generate traces with high fidelity and diversity while aligning
closely with guided configurations with only 8% errors.

</details>


### [187] [PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity](https://arxiv.org/abs/2509.01214)
*Yizhe Yuan,Bingsen Xue,Bangzheng Pu,Chengxiang Wang,Cheng Jin*

Main category: cs.CV

TL;DR: PRINTER是一个弱监督框架，通过原型驱动和变形感知对抗学习策略，实现了H&E和IHC染色模式的精确虚拟染色，解决了空间错位问题。


<details>
  <summary>Details</summary>
Motivation: 肿瘤空间异质性分析需要H&E形态学与IHC生物标志物表达的精确定位，但现有方法因空间错位问题限制了病理解释的准确性。

Method: PRINTER结合原型驱动的内容-染色模式解耦和变形感知对抗学习策略，通过GapBridge框架实现H&E与IHC域的对齐与虚拟染色。

Result: 实验表明，PRINTER在保留H&E细节和虚拟染色保真度方面优于现有方法。

Conclusion: PRINTER为虚拟染色提供了可靠且可扩展的解决方案，推动了计算病理学的发展。

Abstract: Tumor spatial heterogeneity analysis requires precise correlation between
Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker
expression, yet current methods suffer from spatial misalignment in consecutive
sections, severely compromising in situ pathological interpretation. In order
to obtain a more accurate virtual staining pattern, We propose PRINTER, a
weakly-supervised framework that integrates PRototype-drIven content and
staiNing patTERn decoupling and deformation-aware adversarial learning
strategies designed to accurately learn IHC staining patterns while preserving
H&E staining details. Our approach introduces three key innovations: (1) A
prototype-driven staining pattern transfer with explicit content-style
decoupling; and (2) A cyclic registration-synthesis framework GapBridge that
bridges H&E and IHC domains through deformable structural alignment, where
registered features guide cross-modal style transfer while synthesized outputs
iteratively refine the registration;(3) Deformation-Aware Adversarial Learning:
We propose a training framework where a generator and deformation-aware
registration network jointly adversarially optimize a style-focused
discriminator. Extensive experiments demonstrate that PRINTER effectively
achieves superior performance in preserving H&E staining details and virtual
staining fidelity, outperforming state-of-the-art methods. Our work provides a
robust and scalable solution for virtual staining, advancing the field of
computational pathology.

</details>


### [188] [Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement](https://arxiv.org/abs/2509.01362)
*Jiayi Gao,Changcheng Hua,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出一种无训练的提示、图像和引导增强（TPIGE）框架，用于提升身份保持文本到视频（IPT2V）生成的质量，解决数据和调优成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大量数据和昂贵调优，限制了IPT2V的广泛改进。

Method: 通过GPT-4o增强文本提示，用图像生成器优化参考图像，并在生成过程中使用统一梯度优化。

Result: 在1000个视频测试集上表现优异，赢得了ACM Multimedia 2025竞赛。

Conclusion: TPIGE框架在不需额外训练的情况下，显著提升了IPT2V的性能和通用性。

Abstract: Identity-preserving text-to-video (IPT2V) generation creates videos faithful
to both a reference subject image and a text prompt. While fine-tuning large
pretrained video diffusion models on ID-matched data achieves state-of-the-art
results on IPT2V, data scarcity and high tuning costs hinder broader
improvement. We thus introduce a Training-Free Prompt, Image, and Guidance
Enhancement (TPIGE) framework that bridges the semantic gap between the video
description and the reference image and design sampling guidance that enhances
identity preservation and video quality, achieving performance gains at minimal
cost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o
to enhance the text prompt with facial details derived from the reference
image. We then propose Prompt Aware Reference Image Enhancement, leveraging an
identity-preserving image generator to refine the reference image, rectifying
conflicts with the text prompt. The above mutual refinement significantly
improves input quality before video generation. Finally, we propose ID-Aware
Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize
identity preservation and video quality jointly during generation.Our method
outperforms prior work and is validated by automatic and human evaluations on a
1000 video test set, winning first place in the ACM Multimedia 2025
Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art
performance and strong generality. The code is available at
https://github.com/Andyplus1/IPT2V.git.

</details>


### [189] [Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning](https://arxiv.org/abs/2509.01383)
*Long Zhang,Peipei Song,Jianfeng Dong,Kun Li,Xun Yang*

Main category: cs.CV

TL;DR: RAL框架通过概率建模和动态权重相似性来解决PRVR中的数据不确定性问题，提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: PRVR的核心挑战在于学习鲁棒的查询-视频对齐，以避免由数据不确定性引起的虚假语义相关性。

Method: RAL框架采用多元高斯分布编码视频和查询，并通过可学习的置信门动态加权相似性。

Result: 实验表明RAL可以有效提升多种检索架构的性能。

Conclusion: RAL作为一种即插即用的解决方案，能够显著提升PRVR任务的性能。

Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos
partially relevant to a given query. The core challenge lies in learning robust
query-video alignment against spurious semantic correlations arising from
inherent data uncertainty: 1) query ambiguity, where the query incompletely
characterizes the target video and often contains uninformative tokens, and 2)
partial video relevance, where abundant query-irrelevant segments introduce
contextual noise in cross-modal alignment. Existing methods often focus on
enhancing multi-scale clip representations and retrieving the most relevant
clip. However, the inherent data uncertainty in PRVR renders them vulnerable to
distractor videos with spurious similarities, leading to suboptimal
performance. To fill this research gap, we propose Robust Alignment Learning
(RAL) framework, which explicitly models the uncertainty in data. Key
innovations include: 1) we pioneer probabilistic modeling for PRVR by encoding
videos and queries as multivariate Gaussian distributions. This not only
quantifies data uncertainty but also enables proxy-level matching to capture
the variability in cross-modal correspondences; 2) we consider the
heterogeneous informativeness of query words and introduce learnable confidence
gates to dynamically weight similarity. As a plug-and-play solution, RAL can be
seamlessly integrated into the existing architectures. Extensive experiments
across diverse retrieval backbones demonstrate its effectiveness.

</details>


### [190] [SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization](https://arxiv.org/abs/2509.01439)
*Artur Díaz-Juan,Coloma Ballester,Gloria Haro*

Main category: cs.CV

TL;DR: 该论文提出了一种针对足球视频摘要的公开数据集和基线模型，填补了现有数据的不足，并引入了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开可用的数据集，开发体育视频摘要的鲁棒模型面临挑战，研究旨在填补这一空白。

Method: 提供237场足球比赛的镜头边界数据集，并提出一个基线模型和新评估指标。

Result: 基线模型在测试集上F1得分为0.3956，数据集和代码已公开。

Conclusion: 该研究为足球视频摘要提供了基准数据集和工具，推动了相关领域的发展。

Abstract: Video summarization aims to extract key shots from longer videos to produce
concise and informative summaries. One of its most common applications is in
sports, where highlight reels capture the most important moments of a game,
along with notable reactions and specific contextual events. Automatic summary
generation can support video editors in the sports media industry by reducing
the time and effort required to identify key segments. However, the lack of
publicly available datasets poses a challenge in developing robust models for
sports highlight generation. In this paper, we address this gap by introducing
a curated dataset for soccer video summarization, designed to serve as a
benchmark for the task. The dataset includes shot boundaries for 237 matches
from the Spanish, French, and Italian leagues, using broadcast footage sourced
from the SoccerNet dataset. Alongside the dataset, we propose a baseline model
specifically designed for this task, which achieves an F1 score of 0.3956 in
the test set. Furthermore, we propose a new metric constrained by the length of
each target summary, enabling a more objective evaluation of the generated
content. The dataset and code are available at
https://ipcv.github.io/SoccerHigh/.

</details>


### [191] [Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction](https://arxiv.org/abs/2509.00381)
*Runtong Wu,Jiayao Song,Fei Teng,Xianhao Ren,Yuyan Gao,Kailun Yang*

Main category: cs.CV

TL;DR: 论文提出了一种新范式NAME，将研究文档转化为故事图像，减轻了叙事分析中数据解读的负担，并通过模块化设计和评估指标提升了生成图像的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统叙事分析需要大量手动处理文本数据，增加了研究者和参与者的负担，亟需更高效且友好的叙事生成方法。

Method: 提出了NAME范式，包含角色定位与形状生成模块，设计了三个维度的评估指标来衡量生成图像的感知质量和叙事一致性。

Result: 方法在数据量仅为基线0.96%的情况下，仍显著降低了FID分数，并在不同数据划分方案中均表现出色。

Conclusion: NAME范式显著提升了叙事分析的效率和生成图像的质量，为领域提供了新的研究方向。

Abstract: Narrative inquiry has been one of the prominent application domains for the
analysis of human experience, aiming to know more about the complexity of human
society. However, researchers are often required to transform various forms of
data into coherent hand-drafted narratives in storied form throughout narrative
analysis, which brings an immense burden of data analysis. Participants, too,
are expected to engage in member checking and presentation of these narrative
products, which involves reviewing and responding to large volumes of
documents. Given the dual burden and the need for more efficient and
participant-friendly approaches to narrative making and representation, we made
a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt
to push the field of narrative inquiry. Name is able to transfer research
documents into coherent story images, alleviating the cognitive burden of
interpreting extensive text-based materials during member checking for both
researchers and participants. (ii) We develop an actor location and shape
module to facilitate plausible image generation. (iii) We have designed a set
of robust evaluation metrics comprising three key dimensions to objectively
measure the perceptual quality and narrative consistency of generated
characters. Our approach consistently demonstrates state-of-the-art performance
across different data partitioning schemes. Remarkably, while the baseline
relies on the full 100% of the available data, our method requires only 0.96%
yet still reduces the FID score from 195 to 152. Under identical data volumes,
our method delivers substantial improvements: for the 70:30 split, the FID
score decreases from 175 to 152, and for the 95:5 split, it is nearly halved
from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the
newly introduced metric, surpassing the baseline score of 2.66.

</details>


### [192] [Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation](https://arxiv.org/abs/2509.01868)
*Komala Subramanyam Cherukuri,Kewei Sha,Zhenhua Huang*

Main category: cs.CV

TL;DR: 论文研究了在连接自动驾驶车辆（CAVs）中使用联邦学习（FL）进行目标检测的可行性和挑战，提出了一个全面的部署导向评估方法，结合了模型性能、系统资源分析和环境鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统集中式目标检测方法在扩展性、适应性和隐私保护方面存在不足，而联邦学习提供了分布式协作训练的可能性，但实际部署仍面临计算资源限制和环境多样性等挑战。

Method: 研究采用YOLOv5、YOLOv8、YOLOv11和Deformable DETR等先进检测器，在KITTI、BDD100K和nuScenes数据集上，通过多分辨率、批量大小、环境条件和动态客户端参与下的实验分析FL部署的权衡。

Result: 研究揭示了在目标检测精度、计算成本和资源使用之间的权衡关系，为CAVs中FL的实际部署提供了参考。

Conclusion: 通过系统化的评估框架，本研究为在CAVs中实现鲁棒的联邦学习目标检测提供了实践路径，强调了多方权衡的重要性。

Abstract: Object detection is crucial for Connected Autonomous Vehicles (CAVs) to
perceive their surroundings and make safe driving decisions. Centralized
training of object detection models often achieves promising accuracy, fast
convergence, and simplified training process, but it falls short in
scalability, adaptability, and privacy-preservation. Federated learning (FL),
by contrast, enables collaborative, privacy-preserving, and continuous training
across naturally distributed CAV fleets. However, deploying FL in real-world
CAVs remains challenging due to the substantial computational demands of
training and inference, coupled with highly diverse operating conditions.
Practical deployment must address three critical factors: (i) heterogeneity
from non-IID data distributions, (ii) constrained onboard computing hardware,
and (iii) environmental variability such as lighting and weather, alongside
systematic evaluation to ensure reliable performance. This work introduces the
first holistic deployment-oriented evaluation of FL-based object detection in
CAVs, integrating model performance, system-level resource profiling, and
environmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,
YOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes
datasets, we analyze trade-offs between detection accuracy, computational cost,
and resource usage under diverse resolutions, batch sizes, weather and lighting
conditions, and dynamic client participation, paving the way for robust FL
deployment in CAVs.

</details>


### [193] [DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion](https://arxiv.org/abs/2509.01177)
*Junxiang Liu,Junming Lin,Jiangtong Li,Jie Li*

Main category: cs.CV

TL;DR: DynaMind通过联合建模神经动态和语义特征，实现了从EEG信号重建动态视觉场景的创新框架，显著提升了重建视频的准确性和质量。


<details>
  <summary>Details</summary>
Motivation: EEG信号的低空间分辨率、神经记录与视频动态的时间不匹配，以及脑活动中语义信息的不足，限制了动态视觉场景的重建效果。

Method: DynaMind框架包含三个核心模块：区域感知语义映射器（RSM）、时间感知动态对齐器（TDA）和双引导视频重建器（DGVR），共同建模神经动态和语义特征。

Result: 在SEED-DV数据集上，DynaMind实现了新的SOTA，视频和帧级别准确率分别提升了12.5和10.3个百分点，像素级质量和时间一致性也有显著提升。

Conclusion: DynaMind在神经动态和高保真视觉语义之间架起了桥梁，标志着重要的技术进步。

Abstract: Reconstruction dynamic visual scenes from electroencephalography (EEG)
signals remains a primary challenge in brain decoding, limited by the low
spatial resolution of EEG, a temporal mismatch between neural recordings and
video dynamics, and the insufficient use of semantic information within brain
activity. Therefore, existing methods often inadequately resolve both the
dynamic coherence and the complex semantic context of the perceived visual
stimuli. To overcome these limitations, we introduce DynaMind, a novel
framework that reconstructs video by jointly modeling neural dynamics and
semantic features via three core modules: a Regional-aware Semantic Mapper
(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video
Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to
extract multimodal semantic features from EEG signals across distinct brain
regions, aggregating them into a unified diffusion prior. In the mean time, the
TDA generates a dynamic latent sequence, or blueprint, to enforce temporal
consistency between the feature representations and the original neural
recordings. Together, guided by the semantic diffusion prior, the DGVR
translates the temporal-aware blueprint into a high-fidelity video
reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art
(SOTA), boosting reconstructed video accuracies (video- and frame-based) by
12.5 and 10.3 percentage points, respectively. It also achieves a leap in
pixel-level quality, showing exceptional visual fidelity and temporal coherence
with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical
advancement, bridging the gap between neural dynamics and high-fidelity visual
semantics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [194] [ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor Decomposition](https://arxiv.org/abs/2509.00280)
*Ahmed E. Helal,Fabio Checconi,Jan Laukemann,Yongseok Soh,Jesmin Jahan Tithi,Fabrizio Petrini,Jee Choi*

Main category: cs.LG

TL;DR: ReLATE框架通过强化学习自动构建高效稀疏张量表示，适应不规则张量形状和数据分布，性能优于专家设计格式。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏张量格式无法适应不规则张量形状和高度变化的数据分布，需要一种自适应方法。

Method: 采用强化学习框架，结合模型无关和基于模型的算法，通过动作掩码和过滤确保功能性。

Result: 在多种稀疏张量数据集上，ReLATE性能最优，最高提速2倍，几何平均提速1.4-1.46倍。

Conclusion: ReLATE为高维稀疏数据分析提供了一种自适应的强化学习方法，显著提升了性能。

Abstract: Tensor decomposition (TD) is essential for analyzing high-dimensional sparse
data, yet its irregular computations and memory-access patterns pose major
performance challenges on modern parallel processors. Prior works rely on
expert-designed sparse tensor formats that fail to adapt to irregular tensor
shapes and/or highly variable data distributions. We present the
reinforcement-learned adaptive tensor encoding (ReLATE) framework, a novel
learning-augmented method that automatically constructs efficient sparse tensor
representations without labeled training samples. ReLATE employs an autonomous
agent that discovers optimized tensor encodings through direct interaction with
the TD environment, leveraging a hybrid model-free and model-based algorithm to
learn from both real and imagined actions. Moreover, ReLATE introduces
rule-driven action masking and dynamics-informed action filtering mechanisms
that ensure functionally correct tensor encoding with bounded execution time,
even during early learning stages. By automatically adapting to both irregular
tensor shapes and data distributions, ReLATE generates sparse tensor
representations that consistently outperform expert-designed formats across
diverse sparse tensor data sets, achieving up to 2X speedup compared to the
best sparse format, with a geometric-mean speedup of 1.4-1.46X.

</details>


### [195] [An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment](https://arxiv.org/abs/2509.00560)
*Can Cui,Zilong Fu,Penghe Huang,Yuanyuan Li,Wu Deng,Dongyan Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为SA-DSD的创新框架，通过从图神经网络（GNN）到Kolmogorov-Arnold网络（KAN）的知识蒸馏，改进了Fourier KAN（FR-KAN），并将其作为学生模型。该方法在降低计算复杂度的同时，显著提升了非线性拟合能力。实验证明SA-DSD在多个数据集上性能优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 资源受限的边缘环境（如智能家居设备、可穿戴技术和移动终端）对模型压缩和推理速度提出了更高要求。由于多层感知机（MLP）难以捕捉图神经网络的复杂邻域依赖性，限制了其在边缘环境中的性能。因此，需要一种新的知识蒸馏方法来克服这些限制。

Method: 论文提出了一种名为Self Attention Dynamic Sampling Distillation（SA-DSD）的框架，改进了FR-KAN并将其作为学生模型。通过引入可学习的频率基础和相位偏移机制，结合算法优化，提升了非线性拟合能力并降低了计算复杂度。此外，基于师生预测一致性构建了采样概率矩阵，并设计了自适应加权损失机制。

Result: 在六个真实数据集上的实验表明，SA-DSD在性能上比GNN教师模型提高了3.05%-3.62%，比FR-KAN+模型提高了15.61%。同时，与关键基准模型相比，SA-DSD实现了16.96倍的参数量减少和55.75%的推理时间降低。

Conclusion: SA-DSF框架通过改进的知识蒸馏方法，显著提升了学生模型的性能，同时大幅降低了计算资源需求，适用于资源受限的边缘环境。

Abstract: Knowledge distillation (KD) is crucial for deploying deep learning models in
resource-constrained edge environments, particularly within the consumer
electronics sector, including smart home devices, wearable technology, and
mobile terminals. These applications place higher demands on model compression
and inference speed, necessitating the transfer of knowledge from Graph Neural
Networks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However,
due to their fixed activation functions and fully connected architecture, MLPs
face challenges in rapidly capturing the complex neighborhood dependencies
learned by GNNs, thereby limiting their performance in edge environments. To
address these limitations, this paper introduces an innovative from GNNs to
Kolmogorov-Arnold Networks (KANs) knowledge distillation framework-Self
Attention Dynamic Sampling Distillation (SA-DSD). This study improved Fourier
KAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model.
Through the incorporation of learnable frequency bases and phase-shift
mechanisms, along with algorithmic optimization, FR-KAN significantly improves
its nonlinear fitting capability while effectively reducing computational
complexity. Building on this, a margin-level sampling probability matrix, based
on teacher-student prediction consistency, is constructed, and an adaptive
weighted loss mechanism is designed to mitigate performance degradation in the
student model due to the lack of explicit neighborhood aggregation. Extensive
experiments conducted on six real-world datasets demonstrate that SA-DSD
achieves performance improvements of 3.05%-3.62% over three GNN teacher models
and 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmark
models, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75%
decrease in inference time.

</details>


### [196] [REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis](https://arxiv.org/abs/2509.01082)
*Madhav Kanda,Shubham Ugare,Sasa Misailovic*

Main category: cs.LG

TL;DR: RefineStat是一种基于语言模型的框架，用于生成符合语义约束的概率程序，并通过诊断感知的细化方法提升程序的可靠性。


<details>
  <summary>Details</summary>
Motivation: 概率编程领域中，小型语言模型生成的程序常存在语法和语义错误，需要一种方法来确保程序的正确性和可靠性。

Method: RefineStat通过强制执行语义约束和诊断感知的细化方法（如重新采样先验或似然组件）来改进程序生成。

Result: 在多个任务中，RefineStat生成的程序不仅语法正确且统计可靠，表现优于或匹敌大型语言模型。

Conclusion: RefineStat为小型语言模型在概率编程领域提供了一种高效且可靠的程序生成方法。

Abstract: Probabilistic programming offers a powerful framework for modeling
uncertainty, yet statistical model discovery in this domain entails navigating
an immense search space under strict domain-specific constraints. When small
language models are tasked with generating probabilistic programs, they
frequently produce outputs that suffer from both syntactic and semantic errors,
such as flawed inference constructs. Motivated by probabilistic programmers'
domain expertise and debugging strategies, we introduce RefineStat, a language
model--driven framework that enforces semantic constraints ensuring synthesized
programs contain valid distributions and well-formed parameters, and then
applies diagnostic-aware refinement by resampling prior or likelihood
components whenever reliability checks fail. We evaluate RefineStat on multiple
probabilistic-programming code-generation tasks using smaller language models
(SLMs) and find that it produces programs that are both syntactically sound and
statistically reliable, often matching or surpassing those from closed-source
large language models (e.g., OpenAI o3).

</details>


### [197] [DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing](https://arxiv.org/abs/2509.02197)
*Afif Boudaoud,Alexandru Calotoiu,Marcin Copik,Torsten Hoefler*

Main category: cs.LG

TL;DR: DaCe AD是一种高效、通用的自动微分引擎，无需代码修改，通过新颖的ILP算法优化存储与重计算的权衡，在HPC基准测试中平均性能超过JAX 92倍。


<details>
  <summary>Details</summary>
Motivation: 现有自动微分框架存在语言支持有限、需代码修改、科学计算性能不足等问题，导致科学家需手动计算梯度，限制了其应用。

Method: 提出DaCe AD，采用ILP算法优化存储与重计算的管理，无需修改代码即可高效计算梯度。

Result: 在NPBench基准测试中，DaCe AD平均性能超过JAX 92倍，无需任何代码修改。

Conclusion: DaCe AD解决了现有框架的多项限制，为科学计算和机器学习提供了高效的自动微分解决方案。

Abstract: Automatic differentiation (AD) is a set of techniques that systematically
applies the chain rule to compute the gradients of functions without requiring
human intervention. Although the fundamentals of this technology were
established decades ago, it is experiencing a renaissance as it plays a key
role in efficiently computing gradients for backpropagation in machine learning
algorithms. AD is also crucial for many applications in scientific computing
domains, particularly emerging techniques that integrate machine learning
models within scientific simulations and schemes. Existing AD frameworks have
four main limitations: limited support of programming languages, requiring code
modifications for AD compatibility, limited performance on scientific computing
codes, and a naive store-all solution for forward-pass data required for
gradient calculations. These limitations force domain scientists to manually
compute the gradients for large problems. This work presents DaCe AD, a
general, efficient automatic differentiation engine that requires no code
modifications. DaCe AD uses a novel ILP-based algorithm to optimize the
trade-off between storing and recomputing to achieve maximum performance within
a given memory constraint. We showcase the generality of our method by applying
it to NPBench, a suite of HPC benchmarks with diverse scientific computing
patterns, where we outperform JAX, a Python framework with state-of-the-art
general AD capabilities, by more than 92 times on average without requiring any
code changes.

</details>


### [198] [RNN Generalization to Omega-Regular Languages](https://arxiv.org/abs/2509.02491)
*Charles Pert,Dalal Alrajeh,Alessandra Russo*

Main category: cs.LG

TL;DR: 该研究首次探讨了RNNs是否能泛化到LTL公式衍生的ω-regular语言，实验结果显示RNNs在训练数据外的长序列上也能高准确率地学习复杂语言。


<details>
  <summary>Details</summary>
Motivation: Büchi自动机在处理复杂系统行为时面临可扩展性问题，神经网络的引入为解决这一问题提供了可能，但其泛化能力尚未被充分研究。

Method: 通过在周期性ω词序列上训练RNNs以模拟目标Büchi自动机的行为，并评估其在分布外序列上的泛化能力。

Result: 实验表明，RNNs在比训练数据长8倍的序列上仍能高准确率地学习复杂ω-regular语言，92.6%的任务实现了完美或接近完美的泛化。

Conclusion: 研究证实了神经网络学习复杂ω-regular语言的可行性，为神经符号化验证方法提供了潜在组件。

Abstract: B\"uchi automata (BAs) recognize $\omega$-regular languages defined by formal
specifications like linear temporal logic (LTL) and are commonly used in the
verification of reactive systems. However, BAs face scalability challenges when
handling and manipulating complex system behaviors. As neural networks are
increasingly used to address these scalability challenges in areas like model
checking, investigating their ability to generalize beyond training data
becomes necessary. This work presents the first study investigating whether
recurrent neural networks (RNNs) can generalize to $\omega$-regular languages
derived from LTL formulas. We train RNNs on ultimately periodic $\omega$-word
sequences to replicate target BA behavior and evaluate how well they generalize
to out-of-distribution sequences. Through experiments on LTL formulas
corresponding to deterministic automata of varying structural complexity, from
3 to over 100 states, we show that RNNs achieve high accuracy on their target
$\omega$-regular languages when evaluated on sequences up to $8 \times$ longer
than training examples, with $92.6\%$ of tasks achieving perfect or
near-perfect generalization. These results establish the feasibility of neural
approaches for learning complex $\omega$-regular languages, suggesting their
potential as components in neurosymbolic verification methods.

</details>


### [199] [T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation](https://arxiv.org/abs/2509.00066)
*Chuanxiang Yang,Yuanfeng Zhou,Guangshun Wei,Siyu Ren,Yuan Liu,Junhui Hou,Wenping Wang*

Main category: cs.LG

TL;DR: 提出了一种新型神经网络架构T-MLP，通过为MLP添加多输出分支（尾部），实现了多尺度建模，显著提升了信号的LOD表示性能。


<details>
  <summary>Details</summary>
Motivation: 为了高效建模和传输信号（如图像和3D形状），需要支持多尺度（LoD）的表示方法。传统MLP因单尺度操作无法满足需求。

Method: 提出T-MLP架构，为MLP隐藏层添加多个输出分支，通过直接监督不同深度实现多尺度学习。

Result: 实验表明，T-MLP在多种信号表示任务中优于其他神经网络LoD基线。

Conclusion: T-MLP通过多分支设计有效实现了信号的多尺度表示，为LoD建模提供了新思路。

Abstract: Level-of-detail (LoD) representation is critical for efficiently modeling and
transmitting various types of signals, such as images and 3D shapes. In this
work, we present a novel neural architecture that supports LoD signal
representation. Our architecture is based on an elaborate modification of the
widely used Multi-Layer Perceptron (MLP), which inherently operates at a single
scale and therefore lacks native support for LoD. Specifically, we introduce
the Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching
multiple output branches, also called tails, to its hidden layers, enabling
direct supervision at multiple depths. Our loss formulation and training
strategy allow each hidden layer to effectively learn a target signal at a
specific LoD, thus enabling multi-scale modeling. Extensive experimental
results show that our T-MLP outperforms other neural LoD baselines across a
variety of signal representation tasks.

</details>


### [200] [Robust Detection of Synthetic Tabular Data under Schema Variability](https://arxiv.org/abs/2509.00092)
*G. Charbel N. Kindji,Elisa Fromont,Lina Maria Rojas-Barahona,Tanguy Urvoy*

Main category: cs.LG

TL;DR: 提出了一种用于检测合成表格数据的新方法，通过创新的Transformer架构显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型的普及引发了对数据真实性的担忧，但表格数据由于其异构性和测试时的未知格式，检测合成表格数据的任务尚未得到充分研究。

Method: 引入了一种新型的datum-wise Transformer架构，并加入了表格适应组件，以处理具有变量和未知模式的表格数据。

Result: 新方法在AUC和准确率上均比现有基线提升了7个百分点，进一步提升了模型的鲁棒性。

Conclusion: 研究首次证实，在真实条件下检测合成表格数据不仅可行，而且可以高可靠性实现。

Abstract: The rise of powerful generative models has sparked concerns over data
authenticity. While detection methods have been extensively developed for
images and text, the case of tabular data, despite its ubiquity, has been
largely overlooked. Yet, detecting synthetic tabular data is especially
challenging due to its heterogeneous structure and unseen formats at test time.
We address the underexplored task of detecting synthetic tabular data in the
wild, where tables have variable and previously unseen schemas. We introduce a
novel datum-wise transformer architecture that significantly outperforms the
only previously published baseline, improving both AUC and accuracy by 7
points. By incorporating a table-adaptation component, our model gains an
additional 7 accuracy points, demonstrating enhanced robustness. This work
provides the first strong evidence that detecting synthetic tabular data in
real-world conditions is not only feasible, but can be done with high
reliability.

</details>


### [201] [Balanced Multimodal Learning: An Unidirectional Dynamic Interaction Perspective](https://arxiv.org/abs/2509.02281)
*Shijie Wang,Li Zhang,Xinyan Liang,Yuhua Qian,Shen Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新的多模态学习方法UDI，通过放弃传统的联合损失，采用顺序训练方案来解决模态不平衡问题，提高了多模态学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态学习方法通常使用联合损失，但会导致模态不平衡，强模态压制弱模态，限制了信息的有效利用。因此，探索一种不依赖联合损失的新策略是必要的。

Method: 提出了Unidirectional Dynamic Interaction (UDI)策略，首先训练锚定模态至收敛，再用其表示通过无监督损失指导其他模态，动态调整模态交互以优化各自贡献。

Result: 实验表明，UDI在解决模态不平衡问题上优于现有方法，提高了多模态学习任务的性能。

Conclusion: UDI通过解耦模态优化和定向信息流，避免了单一模态的支配，促进了跨模态特征学习的有效性。

Abstract: Multimodal learning typically utilizes multimodal joint loss to integrate
different modalities and enhance model performance. However, this joint
learning strategy can induce modality imbalance, where strong modalities
overwhelm weaker ones and limit exploitation of individual information from
each modality and the inter-modality interaction information.Existing
strategies such as dynamic loss weighting, auxiliary objectives and gradient
modulation mitigate modality imbalance based on joint loss. These methods
remain fundamentally reactive, detecting and correcting imbalance after it
arises, while leaving the competitive nature of the joint loss untouched. This
limitation drives us to explore a new strategy for multimodal imbalance
learning that does not rely on the joint loss, enabling more effective
interactions between modalities and better utilization of information from
individual modalities and their interactions. In this paper, we introduce
Unidirectional Dynamic Interaction (UDI), a novel strategy that abandons the
conventional joint loss in favor of a proactive, sequential training scheme.
UDI first trains the anchor modality to convergence, then uses its learned
representations to guide the other modality via unsupervised loss. Furthermore,
the dynamic adjustment of modality interactions allows the model to adapt to
the task at hand, ensuring that each modality contributes optimally. By
decoupling modality optimization and enabling directed information flow, UDI
prevents domination by any single modality and fosters effective cross-modal
feature learning. Our experimental results demonstrate that UDI outperforms
existing methods in handling modality imbalance, leading to performance
improvement in multimodal learning tasks.

</details>


### [202] [Learning to Shard: RL for Co-optimizing the Parallelism Degrees and Per-operator Sharding Dimensions in Distributed LLM Inference](https://arxiv.org/abs/2509.00217)
*Ruokai Yin,Sattwik Deb Mishra,Xuan Zuo,Hokchhay Tann,Preyas Shah,Apala Guha*

Main category: cs.LG

TL;DR: 论文提出了一种名为Learn to Shard的RL方法，用于优化分布式LLM推理中的并行性策略，相比现有方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统（如Megatron-LM）依赖静态启发式方法，难以适应模型规模和硬件拓扑的多样性，导致性能不足。

Method: 采用基于强化学习的方法，联合优化粗粒度并行度与细粒度分片维度，并通过基于注意力的策略从高性能历史中学习。

Result: 在H100集群上测试，使用1.6T参数的MoE模型，Learn to Shard的吞吐量比元启发式基线提升3.5倍，比Megatron启发式提升1.06倍。

Conclusion: Learn to Shard通过动态优化策略显著提升了分布式LLM推理的性能，为大规模模型部署提供了有效解决方案。

Abstract: Distributed LLM inference requires careful coordination of parallelization
strategies across hundreds to thousands of NPUs to meet production SLOs.
Current systems like Megatron-LM rely on static heuristics that separately
configure parallelism degrees and per-operator sharding dimensions, leaving
significant performance on the table as models scale and hardware topologies
diversify. We introduce Learn to Shard, to our knowledge, the first RL-based
approach to co-optimize both coarse-grained parallelism degrees and
fine-grained per-operator sharding dimensions for distributed LLM inference.
Our method employs an attention-based policy over an elite history that learns
from high-performing strategies to efficiently navigate the vast combinatorial
search space. Evaluated on H100 clusters with MoE models up to 1.6T parameters,
Learn to Shard achieves up to 3.5x throughput improvement over metaheuristic
baselines and 1.06x over Megatron heuristics.

</details>


### [203] [Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks](https://arxiv.org/abs/2509.01257)
*Andrea Fox,Francesco De Pellegrini,Eitan Altman*

Main category: cs.LG

TL;DR: 本文提出了一种去中心化框架，通过共享约束向量实现边缘计算系统中自主代理的隐式协调，解决了现有方法在有限可观测性和通信限制下的不足。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算系统中，自主代理需要在竞争共享资源时快速做出本地决策，但现有多智能体强化学习方法依赖集中式评估或频繁通信，无法满足受限环境和通信条件。

Method: 每个代理通过解决约束马尔可夫决策过程（CMDP），利用共享约束向量实现隐式协调，通过安全强化学习同时满足本地和全局目标。

Result: 理论和实验证明，该方法在大型系统中优于集中式和独立基线方法，尤其在资源利用率方面表现突出。

Conclusion: 去中心化的轻量级协调机制有效解决了边缘计算中的资源分配问题，兼具高性能和低通信开销。

Abstract: In edge computing systems, autonomous agents must make fast local decisions
while competing for shared resources. Existing MARL methods often resume to
centralized critics or frequent communication, which fail under limited
observability and communication constraints. We propose a decentralized
framework in which each agent solves a constrained Markov decision process
(CMDP), coordinating implicitly through a shared constraint vector. For the
specific case of offloading, e.g., constraints prevent overloading shared
server resources. Coordination constraints are updated infrequently and act as
a lightweight coordination mechanism. They enable agents to align with global
resource usage objectives but require little direct communication. Using safe
reinforcement learning, agents learn policies that meet both local and global
goals. We establish theoretical guarantees under mild assumptions and validate
our approach experimentally, showing improved performance over centralized and
independent baselines, especially in large-scale settings.

</details>


### [204] [Online Identification of IT Systems through Active Causal Learning](https://arxiv.org/abs/2509.02130)
*Kim Hammar,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文提出了一种名为主动因果学习的在线数据驱动方法，用于构建IT系统的因果模型，通过高斯过程回归和基于干预的策略，实现了高效且低干扰的模型识别。


<details>
  <summary>Details</summary>
Motivation: 传统的因果模型依赖于专家设计，但随着IT系统日益复杂和动态，这种方法变得越来越困难。因此，需要一个自动化、数据驱动的方法来构建因果模型，以支持系统预测、优化和故障诊断等任务。

Method: 方法采用主动因果学习，通过高斯过程回归迭代估计系统变量间的因果函数，并使用基于干预的策略收集数据。

Result: 实验证明，该方法在贝叶斯意义下最优，并能生成有效的干预措施，同时在实际测试中展现出高准确性和低干扰性。

Conclusion: 主动因果学习方法为复杂动态IT系统的建模提供了一种高效、自动化的解决方案，具有广阔的应用前景。

Abstract: Identifying a causal model of an IT system is fundamental to many branches of
systems engineering and operation. Such a model can be used to predict the
effects of control actions, optimize operations, diagnose failures, detect
intrusions, etc., which is central to achieving the longstanding goal of
automating network and system management tasks. Traditionally, causal models
have been designed and maintained by domain experts. This, however, proves
increasingly challenging with the growing complexity and dynamism of modern IT
systems. In this paper, we present the first principled method for online,
data-driven identification of an IT system in the form of a causal model. The
method, which we call active causal learning, estimates causal functions that
capture the dependencies among system variables in an iterative fashion using
Gaussian process regression based on system measurements, which are collected
through a rollout-based intervention policy. We prove that this method is
optimal in the Bayesian sense and that it produces effective interventions.
Experimental validation on a testbed shows that our method enables accurate
identification of a causal system model while inducing low interference with
system operations.

</details>


### [205] [TimeCopilot](https://arxiv.org/abs/2509.00616)
*Azul Garza,Reneé Rosillo*

Main category: cs.LG

TL;DR: TimeCopilot是首个开源的时间序列预测框架，结合了时间序列基础模型与大型语言模型，提供自动化预测生成与自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统通常缺乏透明度与可解释性，TimeCopilot旨在通过统一API提供自动化、可解释且可访问的预测解决方案。

Method: TimeCopilot整合多个时间序列基础模型与大型语言模型，支持特征分析、模型选择、交叉验证等功能，并通过自然语言解释预测结果。

Result: 在GIFT-Eval基准测试中，TimeCopilot以低成本实现了最先进的概率预测性能。

Conclusion: TimeCopilot为可重复、可解释且可访问的预测系统提供了实用框架。

Abstract: We introduce TimeCopilot, the first open-source agentic framework for
forecasting that combines multiple Time Series Foundation Models (TSFMs) with
Large Language Models (LLMs) through a single unified API. TimeCopilot
automates the forecasting pipeline: feature analysis, model selection,
cross-validation, and forecast generation, while providing natural language
explanations and supporting direct queries about the future. The framework is
LLM-agnostic, compatible with both commercial and open-source models, and
supports ensembles across diverse forecasting families. Results on the
large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art
probabilistic forecasting performance at low cost. Our framework provides a
practical foundation for reproducible, explainable, and accessible agentic
forecasting systems.

</details>


### [206] [HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction](https://arxiv.org/abs/2509.02481)
*Aishwarya Sarkar,Autrin Hakimi,Xiaoqiong Chen,Hai Huang,Chaoqun Lu,Ibrahim Demir,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了一种基于异构图和动态注意力的HydroGAT模型，用于高分辨率河流网络中时空交互的洪水预测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统洪水预测方法忽视拓扑信息或时空交互，现有GNN方法因计算成本难以处理高分辨率数据。

Method: 构建异质流域图，提出HydroGAT结合动态注意力和分布式训练，同时建模时空依赖关系。

Result: 在两个美国中西部流域的实验中，模型在NSE、KGE和PBIAS指标上显著优于基线，并支持高分辨率训练。

Conclusion: HydroGAT通过自适应时空建模和高效分布式训练，为洪水预测提供了高精度和可解释性解决方案。

Abstract: Accurate flood forecasting remains a challenge for water-resource management,
as it demands modeling of local, time-varying runoff drivers (e.g.,
rainfall-induced peaks, baseflow trends) and complex spatial interactions
across a river network. Traditional data-driven approaches, such as
convolutional networks and sequence-based models, ignore topological
information about the region. Graph Neural Networks (GNNs) propagate
information exactly along the river network, which is ideal for learning
hydrological routing. However, state-of-the-art GNN-based flood prediction
models collapse pixels to coarse catchment polygons as the cost of training
explodes with graph size and higher resolution. Furthermore, most existing
methods treat spatial and temporal dependencies separately, either applying
GNNs solely on spatial graphs or transformers purely on temporal sequences,
thus failing to simultaneously capture spatiotemporal interactions critical for
accurate flood prediction. We introduce a heterogenous basin graph where every
land and river pixel is a node connected by physical hydrological flow
directions and inter-catchment relationships. We propose HydroGAT, a
spatiotemporal network that adaptively learns local temporal importance and the
most influential upstream locations. Evaluated in two Midwestern US basins and
across five baseline architectures, our model achieves higher NSE (up to 0.97),
improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourly
discharge prediction, while offering interpretable attention maps that reveal
sparse, structured intercatchment influences. To support high-resolution
basin-scale training, we develop a distributed data-parallel pipeline that
scales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer,
demonstrating up to 15x speedup across machines. Our code is available at
https://github.com/swapp-lab/HydroGAT.

</details>


### [207] [Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition](https://arxiv.org/abs/2509.01031)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: TPRL-DG提出了一种基于强化学习的框架，用于解决穿戴式传感器在人类活动识别中的跨用户泛化问题，通过时序保留和标签自由的设计，显著提升模型在未见过用户上的表现。


<details>
  <summary>Details</summary>
Motivation: 穿戴式传感器在健康监测和智能环境中应用广泛，但用户间的运动模式、传感器位置和生理特征差异导致模型泛化性能差。现有方法常忽略时序依赖或依赖不现实的领域标签。

Method: TPRL-DG结合强化学习和Transformer架构，通过自回归生成时序令牌捕获用户不变特征，并使用多目标奖励函数平衡类区分和跨用户不变性。

Result: 在DSADS和PAMAP2数据集上，TPRL-DG在跨用户泛化性能上优于现有方法，无需用户特定校准即实现更高精度。

Conclusion: TPRL-DG通过学习鲁棒的用户不变时序模式，推动了可扩展的人类活动识别系统的发展，有助于个性化医疗和智能环境的应用。

Abstract: Human Activity Recognition (HAR) using wearable sensors is crucial for
healthcare, fitness tracking, and smart environments, yet cross-user
variability -- stemming from diverse motion patterns, sensor placements, and
physiological traits -- hampers generalization in real-world settings.
Conventional supervised learning methods often overfit to user-specific
patterns, leading to poor performance on unseen users. Existing domain
generalization approaches, while promising, frequently overlook temporal
dependencies or depend on impractical domain-specific labels. We propose
Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a
novel framework that redefines feature extraction as a sequential
decision-making process driven by reinforcement learning. TPRL-DG leverages a
Transformer-based autoregressive generator to produce temporal tokens that
capture user-invariant activity dynamics, optimized via a multi-objective
reward function balancing class discrimination and cross-user invariance. Key
innovations include: (1) an RL-driven approach for domain generalization, (2)
autoregressive tokenization to preserve temporal coherence, and (3) a
label-free reward design eliminating the need for target user annotations.
Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses
state-of-the-art methods in cross-user generalization, achieving superior
accuracy without per-user calibration. By learning robust, user-invariant
temporal patterns, TPRL-DG enables scalable HAR systems, facilitating
advancements in personalized healthcare, adaptive fitness tracking, and
context-aware environments.

</details>


### [208] [Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology](https://arxiv.org/abs/2509.02355)
*Caterina Fuster-Barcelo,Gonzalo R. Rios-Munoz,Arrate Munoz-Barrutia*

Main category: cs.LG

TL;DR: 研究通过整合数字协作工具和结构化同伴评估，改进了生物医学图像处理课程的教学效果，提升了学生参与度和评估公平性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过数字工具和结构化评估机制提升STEM教育中的学习效果和公平性。

Method: 结合Google Colab实时编程、Weights & Biases实验追踪与报告，以及基于量规的同伴评估。

Result: 实施后成绩离散度和最终项目得分熵值增加，调查显示学生参与度提高。

Conclusion: 数字协作工具和结构化评估机制有助于优化STEM教育的学习效果和公平性。

Abstract: This study examines the integration of digital collaborative tools and
structured peer evaluation in the Machine Learning for Health master's program,
through the redesign of a Biomedical Image Processing course over two academic
years. The pedagogical framework combines real-time programming with Google
Colab, experiment tracking and reporting via Weights & Biases, and
rubric-guided peer assessment to foster student engagement, transparency, and
fair evaluation. Compared to a pre-intervention cohort, the two implementation
years showed increased grade dispersion and higher entropy in final project
scores, suggesting improved differentiation and fairness in assessment. The
survey results further indicate greater student engagement with the subject and
their own learning process. These findings highlight the potential of
integrating tool-supported collaboration and structured evaluation mechanisms
to enhance both learning outcomes and equity in STEM education.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [209] [The Impact of Sequential versus Parallel Clearing Mechanisms in Agent-Based Simulations of Artificial Limit Order Book Exchanges](https://arxiv.org/abs/2509.01683)
*Matej Steinbacher,Mitja Steinbacher,Matjaz Steinbacher*

Main category: q-fin.TR

TL;DR: 研究比较了不同清算机制对多资产价格动态的影响，发现顺序处理订单簿会引入系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨清算机制如何影响市场中资产价格的动态和资金分配。

Method: 在人工股票市场中模拟不同清算机制，分析顺序处理订单簿的影响。

Result: 顺序清算会导致早期处理的资产获得资金优先权，扭曲价格轨迹。

Conclusion: 清算机制在市场中起关键作用，需谨慎设计和验证以准确模拟金融行为。

Abstract: This study examines the impact of different computing implementations of
clearing mechanisms on multi-asset price dynamics within an artificial stock
market framework. We show that sequential processing of order books introduces
a systematic and significant bias by affecting the allocation of traders'
capital within a single time step. This occurs because applying budget
constraints sequentially grants assets processed earlier preferential access to
funds, distorting individual asset demand and consequently their price
trajectories. The findings highlight that while the overall price level is
primarily driven by macro factors like the money-to-stock ratio, the market's
microstructural clearing mechanism plays a critical role in the allocation of
value among individual assets. This underscores the necessity for careful
consideration and validation of clearing mechanisms in artificial markets to
accurately model complex financial behaviors.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [210] [afspm: A Framework for Manufacturer-Agnostic Automation in Scanning Probe Microscopy](https://arxiv.org/abs/2509.00113)
*Nicholas J. Sullivan,Julio J. Valdés,Kirk H. Bevan,Peter Grutter*

Main category: physics.ins-det

TL;DR: 提出了一个SPM自动化框架，旨在提高代码共享和组件重用性，支持多语言和多计算机协作，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: SPM技术的时间和知识门槛限制了其广泛应用，现有自动化方案难以跨设备兼容。

Method: 设计了通用的控制和数据结构模式，通过独立软件组件和SPM特定翻译器实现自动化操作。

Result: 成功集成并测试了两款不同厂商的SPM设备，运行了热漂移校正实验。

Conclusion: 该框架有效解决了SPM自动化中的兼容性和可扩展性问题。

Abstract: Scanning probe microscopy (SPM) is a valuable technique by which one can
investigate the physical characteristics of the surfaces of materials. However,
its widespread use is hampered by the time-consuming nature of running an
experiment and the significant domain knowledge required. Recent studies have
shown the value of multiple forms of automation in improving this, but their
use is limited due to the difficulty of integrating them with SPMs other than
the one it was developed for. With this in mind, we propose an automation
framework for SPMs aimed toward facilitating code sharing and reusability of
developed components. Our framework defines generic control and data structure
schemas which are passed among independent software processes (components),
with the final SPM commands sent after passing through an SPM-specific
translator. This approach permits multi-language support and allows for
experimental components to be decoupled among multiple computers. Our mediation
logic limits access to the SPM to a single component at a time, with a simple
override mechanism in order to correct detected experiment problems. To
validate our proposal, we integrated and tested it with two SPMs from separate
manufacturers, and ran an experiment involving a thermal drift correction
component.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [211] [Enabling Down Syndrome Research through a Knowledge Graph-Driven Analytical Framework](https://arxiv.org/abs/2509.01565)
*Madan Krishnamurthy,Surya Saha,Pierrette Lo,Patricia L. Whetzel,Tursynay Issabekova,Jamed Ferreris Vargas,Jack DiGiovanna,Melissa A Haendel*

Main category: q-bio.QM

TL;DR: 摘要介绍了NIH INCLUDE计划整合唐氏综合症数据的知识图谱平台，通过语义关联实现跨研究和AI分析。


<details>
  <summary>Details</summary>
Motivation: 唐氏综合症的临床表型多样且数据分散，需要整合工具以促进研究和转化发现。

Method: 开发了一个知识图谱平台，整合了9项研究的数据，涵盖7148名参与者、456种疾病和37000多个生物样本，并通过Monarch Initiative扩展了数据覆盖范围。

Result: 知识图谱包含160万条语义关联，支持AI分析和自然语言查询，促进跨研究模式识别和基因型-表型关系探索。

Conclusion: 该平台将静态数据转化为动态发现环境，为唐氏综合症研究提供了新工具。

Abstract: Trisomy 21 results in Down syndrome, a multifaceted genetic disorder with
diverse clinical phenotypes, including heart defects, immune dysfunction,
neurodevelopmental differences, and early-onset dementia risk. Heterogeneity
and fragmented data across studies challenge comprehensive research and
translational discovery. The NIH INCLUDE (INvestigation of Co-occurring
conditions across the Lifespan to Understand Down syndromE) initiative has
assembled harmonized participant-level datasets, yet realizing their potential
requires integrative analytical frameworks. We developed a knowledge
graph-driven platform transforming nine INCLUDE studies, comprising 7,148
participants, 456 conditions, 501 phenotypes, and over 37,000 biospecimens,
into a unified semantic infrastructure. Cross-resource enrichment with Monarch
Initiative data expands coverage to 4,281 genes and 7,077 variants. The
resulting knowledge graph contains over 1.6 million semantic associations,
enabling AI-ready analysis with graph embeddings and path-based reasoning for
hypothesis generation. Researchers can query the graph via SPARQL or natural
language interfaces. This framework converts static data repositories into
dynamic discovery environments, supporting cross-study pattern recognition,
predictive modeling, and systematic exploration of genotype-phenotype
relationships in Down syndrome.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [212] [Generalised Möbius Categories and Convolution Kleene Algebras](https://arxiv.org/abs/2509.00168)
*James Cranch,Georg Struth,Jana Wagemaker*

Main category: cs.FL

TL;DR: 本文提出了一种基于广义Möbius范畴和形式幂级数的星的广义定义，用于构建卷积Kleene代数，解决了此前因缺乏合适的星定义而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 卷积代数在数学和科学中广泛存在，但在构建广泛的卷积Kleene代数时，缺乏合适的星定义是一个障碍。本文旨在解决这一问题。

Method: 通过结合广义Möbius范畴和形式幂级数的星的广义定义，构建卷积Kleene代数，并在多种结构中实例化，如带测试的卷积Kleene代数、模态卷积Kleene代数等。

Result: 成功构建了适用于多种结构的卷积Kleene代数，并在加权和概率程序验证、高阶维度重写等领域展示了其应用潜力。

Conclusion: 本文提出的方法扩展了卷积Kleene代数的适用范围，并为未来应用提供了具体的示例结构。

Abstract: Convolution algebras on maps from structures such as monoids, groups or
categories into semirings, rings or fields abound in mathematics and the
sciences. Of special interest in computing are convolution algebras based on
variants of Kleene algebras, which are additively idempotent semirings equipped
with a Kleene star. Yet an obstacle to the construction of convolution Kleene
algebras on a wide class of structures has so far been the definition of a
suitable star. We show that a generalisation of M\"obius categories combined
with a generalisation of a classical definition of a star for formal power
series allow such a construction. We discuss several instances of this
construction on generalised M\"obius categories: convolution Kleene algebras
with tests, modal convolution Kleene algebras, concurrent convolution Kleene
algebras and higher convolution Kleene algebras (e.g. on strict higher
categories and higher relational monoids). These are relevant to the
verification of weighted and probabilistic sequential and concurrent programs,
using quantitative Hoare logics or predicate transformer algebras, as well as
for algebraic reasoning in higher-dimensional rewriting. We also adapt the
convolution Kleene algebra construction to Conway semirings, which is widely
studied in the context of weighted automata. Finally, we compare the
convolution Kleene algebra construction with a previous construction of
convolution quantales and present concrete example structures in preparation
for future applications.

</details>


### [213] [DTMC Model Checking by Path Abstraction Revisited (extended version)](https://arxiv.org/abs/2509.02393)
*Arnd Hartmanns,Robert Modderman*

Main category: cs.FL

TL;DR: 论文证明了在离散时间马尔可夫链中，通过路径抽象计算子路径的概率与直接方法结果一致，且无需遵循SCC结构，支持任意有限序列的非目标状态集合划分。


<details>
  <summary>Details</summary>
Motivation: 研究如何在概率模型检验中计算子路径的概率质量，补充并扩展了2010年Abraham等人的工作。

Method: 将DTMC解释为状态空间自由幺半群的结构，通过路径抽象划分计算。

Result: 证明了路径抽象方法的正确性及灵活性，并提供PARI/GP的参考实现。

Conclusion: 路径抽象方法在理论和实现上均可行，为概率模型检验提供了新工具。

Abstract: Computing the probability of reaching a set of goal states G in a
discrete-time Markov chain (DTMC) is a core task of probabilistic model
checking. We can do so by directly computing the probability mass of the set of
all finite paths from the initial state to G; however, when refining
counterexamples, it is also interesting to compute the probability mass of
subsets of paths. This can be achieved by splitting the computation into path
abstractions that calculate "local" reachability probabilities as shown by
\'Abrah\'am et al. in 2010. In this paper, we complete and extend their work:
We prove that splitting the computation into path abstractions indeed yields
the same result as the direct approach, and that the splitting does not need to
follow the SCC structure. In particular, we prove that path abstraction can be
performed along any finite sequence of sets of non-goal states. Our proofs
proceed in a novel way by interpreting the DTMC as a structure on the free
monoid on its state space, which makes them clean and concise. Additionally, we
provide a compact reference implementation of path abstraction in PARI/GP.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [214] [Computation of Feasible Assume-Guarantee Contracts: A Resilience-based Approach](https://arxiv.org/abs/2509.01832)
*Negar Monir,Youssef Ait Si,Ratnangshu Das,Pushpak Jagtap,Adnane Saoud,Sadegh Soudjani*

Main category: eess.SY

TL;DR: 提出了一个基于韧性的框架，用于计算离散时间系统中满足时序规范的假设-保证合同，并通过迭代方法优化子系统间的假设和保证。


<details>
  <summary>Details</summary>
Motivation: 解决互联系统中时序规范的满足问题，通过韧性度量量化扰动的影响，实现子系统间的协同验证。

Method: 使用韧性度量（最大扰动）迭代优化假设和保证，扩展到L个子系统使用加权组合建模互联效应。在线性和非线性系统中分别验证有限时间安全性和可达性规范。

Result: 展示了方法的正确性、单调性和最大性假设，并通过数值模拟和非线性微电网案例验证了框架的有效性。

Conclusion: 该框架提供了一种有效的组合验证方法，能够处理复杂系统中的时序规范需求。

Abstract: We propose a resilience-based framework for computing feasible
assume-guarantee contracts that ensure the satisfaction of temporal
specifications in interconnected discrete-time systems. Interconnection effects
are modeled as structured disturbances. We use a resilience metric, the maximum
disturbance under which local specifications hold, to refine assumptions and
guarantees across subsystems iteratively. For two subsystems, we demonstrate
correctness, monotone refinement of guarantees, and that the resulting
assumptions are maximal within ball-shaped sets. Additionally, we extend our
approach to general networks of L subsystems using weighted combinations of
interconnection effects. We instantiate the framework on linear systems by
meeting finite-horizon safety, exact-time reachability, and finite-time
reachability specifications, and on nonlinear systems by fulfilling general
finite-horizon specifications. Our approach is demonstrated through numerical
linear examples, and a nonlinear DC Microgrid case study, showcasing the impact
of our framework in verifying temporal logic specifications with compositional
reasoning.

</details>


### [215] [Semantic Technologies in Practical Demand Response: An Informational Requirement-based Roadmap](https://arxiv.org/abs/2509.01459)
*Ozan Baris Mulayim,Yuvraj Agarwal,Mario Bergés,Steve Schaefer,Mitali Shah,Derek Supple*

Main category: eess.SY

TL;DR: 论文通过形式化本体评估/开发方法，明确了商业建筑激励型需求响应（DR）中语义互操作性的信息需求（IRs），并评估现有本体（Brick、DELTA、EFOnt）的支持程度，揭示了其与实际需求的偏差，提出了扩展路径。


<details>
  <summary>Details</summary>
Motivation: 未来电网将高度复杂和分散化，需支持异构系统的语义互操作性；当前本体缺乏形式化框架且整合方案多为概念性，无法满足现实DR需求。

Method: 采用形式化本体评估/开发方法，识别商业建筑DR的信息需求（IRs），评估现有本体（Brick、DELTA、EFOnt）对其的支持程度。

Result: 发现现有本体与DR实际需求存在显著偏差，提出了扩展和整合的路线图。

Conclusion: 研究为提升当前及未来智能电网的互操作性、促进DR系统规模化集成奠定了基础。

Abstract: The future grid will be highly complex and decentralized, requiring
sophisticated coordination across numerous human and software agents that
manage distributed resources such as Demand Response (DR). Realizing this
vision demands significant advances in semantic interoperability, which enables
scalable and cost-effective automation across heterogeneous systems. While
semantic technologies have progressed in commercial building and DR domains,
current ontologies have two critical limitations: they are often developed
without a formal framework that reflects real-world DR requirements, and
proposals for integrating general and application-specific ontologies remain
mostly conceptual, lacking formalization or empirical validation.
  In this paper, we address these gaps by applying a formal ontology
evaluation/development approach to define the informational requirements (IRs)
necessary for semantic interoperability in the area of incentive-based DR for
commercial buildings. We identify the IRs associated with each stage of the
wholesale incentive-based DR process, focusing on the perspective of building
owners. Using these IRs, we evaluate how well existing ontologies (Brick,
DELTA, and EFOnt) support the operational needs of DR participation. Our
findings reveal substantial misalignments between current ontologies and
practical DR requirements. Based on our assessments, we propose a roadmap of
necessary extensions and integrations for these ontologies. This work
ultimately aims to enhance the interoperability of today's and future smart
grid, thereby facilitating scalable integration of DR systems into the grid's
complex operational framework.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [216] [Traq: Estimating the Quantum Cost of Classical Programs](https://arxiv.org/abs/2509.01508)
*Anurudh Peduri,Gilles Barthe,Michael Walter*

Main category: quant-ph

TL;DR: Traq是一种自动化、可证明的方法，用于估计量子计算机对经典程序的加速效果，包括语言、成本分析和量子程序编译。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手动分析和数值模拟，效率低且针对性单一。Traq旨在提供一种自动化、通用的量化加速评估工具。

Method: Traq采用一种经典语言，包含适于量子加速的高级原语，进行成本分析，并编译为低层量子程序。成本分析提供细粒度的非渐近复杂性上界。

Result: 通过概念验证实现和案例研究（AND-OR树），证明Traq能够有效量化量子程序的加速潜力。

Conclusion: Traq为量子加速的实际应用提供了一种可扩展和自动化的分析框架，具有理论和实践价值。

Abstract: Predicting practical speedups offered by future quantum computers has become
a major focus of the quantum computing community. Typically, these predictions
are supported by lengthy manual analyses and numerical simulations and are
carried out for one specific application at a time. In this paper, we present
Traq, a principled approach towards estimating the quantum speedup of classical
programs fully automatically and with provable guarantees. It consists of a
classical language that includes high-level primitives amenable to quantum
speedups, a cost analysis, and a compilation to low-level quantum programs. Our
cost analysis upper bounds the complexity of the resulting quantum program in a
fine-grained way: it captures non-asymptotic information and is sensitive to
the input of the program (rather than providing worst-case costs). We also
provide a proof-of-concept implementation and a case study inspired by AND-OR
trees.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [217] [Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic](https://arxiv.org/abs/2509.01954)
*Nirmalya Thakur,Madeline D Hartel,Lane Michael Boden,Dallas Enriquez,Boston Joyner Ricks*

Main category: cs.SI

TL;DR: 研究了2023年至2024年间的1万条COVID-19相关YouTube视频，分析了时间、词汇、语言和结构因素对受众参与的影响。


<details>
  <summary>Details</summary>
Motivation: 探索疫情后期YouTube视频的受众参与模式，揭示发布计划、标题词汇、主题和视频时长等因素对观看量的影响。

Method: 分析了视频的发布时间、标题词汇频率、情感分析和视频时长等数据，并结合统计方法评估了相关性。

Result: 发现发布时间的周内效应、特定词汇（如"shorts"）的高观看量，以及视频时长与类别的显著影响。情感分析的相关系数在排除异常值后增强。

Conclusion: 疫情后期YouTube视频的受众参与模式受多种因素驱动，包括发布时间、标题词汇和视频类别与时长的组合效应。

Abstract: This work investigated about 10,000 COVID-19-related YouTube videos published
between January 2023 and October 2024 to evaluate how temporal, lexical,
linguistic, and structural factors influenced engagement during the late
pandemic period. Publishing activity showed consistent weekday effects: in the
first window, average views peaked on Mondays at 92,658; in the second, on
Wednesdays at 115,479; and in the third, on Fridays at 84,874, reflecting a
shift in audience attention toward mid- and late week. Lexical analysis of
video titles revealed recurring high-frequency keywords related to COVID-19 and
YouTube features, including COVID, coronavirus, shorts, and live. Frequency
analysis revealed sharp spikes, with COVID appearing in 799 video titles in
August 2024, while engagement analysis showed that videos titled with shorts
attracted very high views, peaking at 2.16 million average views per video in
June 2023. Analysis of sentiment of video descriptions in English showed weak
correlation with views in the raw data (Pearson r = 0.0154, p = 0.2987), but
stronger correlations emerged once outliers were addressed, with Spearman r =
0.110 (p < 0.001) and Pearson r = 0.0925 (p < 0.001). Category-level analysis
of video durations revealed contrasting outcomes: long videos focusing on
people and blogs averaged 209,114 views, short entertainment videos averaged
288,675 views, and medium-to-long news and politics videos averaged 51,309 and
59,226 views, respectively. These results demonstrate that engagement patterns
of COVID-19-related videos on YouTube during the late pandemic followed
distinct characteristics driven by publishing schedules, title vocabulary,
topics, and genre-specific duration effects.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [218] [PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces](https://arxiv.org/abs/2509.00670)
*Gursimran Singh,Aviral Chharia,Rahul Upadhyay,Vinay Kumar,Luca Longo*

Main category: eess.SP

TL;DR: PyNoetic是一个模块化的脑机接口（BCI）框架，旨在解决现有BCI工具的局限性，提供全流程设计功能和无代码GUI，适合不同水平的研究者。


<details>
  <summary>Details</summary>
Motivation: 现有BCI框架缺乏灵活性、学习曲线陡峭、依赖专有软件且功能不全面，PyNoetic旨在填补这些空白。

Method: PyNoetic提供从刺激呈现到数据可视化的完整流程，支持无代码设计和自定义功能集成。

Result: PyNoetic支持离线和实时BCI开发，简化设计流程，加速研究进展。

Conclusion: PyNoetic是一个多功能且易于使用的BCI框架，适合广泛的研究需求。

Abstract: Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have
emerged as a transformative technology with applications spanning robotics,
virtual reality, medicine, and rehabilitation. However, existing BCI frameworks
face several limitations, including a lack of stage-wise flexibility essential
for experimental research, steep learning curves for researchers without
programming expertise, elevated costs due to reliance on proprietary software,
and a lack of all-inclusive features leading to the use of multiple external
tools affecting research outcomes. To address these challenges, we present
PyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI
research. PyNoetic is one of the very few frameworks in Python that encompasses
the entire BCI design pipeline, from stimulus presentation and data acquisition
to channel selection, filtering, feature extraction, artifact removal, and
finally simulation and visualization. Notably, PyNoetic introduces an intuitive
and end-to-end GUI coupled with a unique pick-and-place configurable flowchart
for no-code BCI design, making it accessible to researchers with minimal
programming experience. For advanced users, it facilitates the seamless
integration of custom functionalities and novel algorithms with minimal coding,
ensuring adaptability at each design stage. PyNoetic also includes a rich array
of analytical tools such as machine learning models, brain-connectivity
indices, systematic testing functionalities via simulation, and evaluation
methods of novel paradigms. PyNoetic's strengths lie in its versatility for
both offline and real-time BCI development, which streamlines the design
process, allowing researchers to focus on more intricate aspects of BCI
development and thus accelerate their research endeavors. Project Website:
https://neurodiag.github.io/PyNoetic

</details>


### [219] [Know What, Know Why: Semantic Hazard Communication for Intelligent V2X Systems](https://arxiv.org/abs/2509.02442)
*Chen Sun,Wenqi Zhang,Bizhu Wang,Xiaodong Xu,Chau Yuen,Yan Zhang,Ping Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种语义增强且可解释的V2X（SEE-V2X）系统，通过提供上下文感知的警告信息，帮助驾驶员做出更明智的驾驶决策。


<details>
  <summary>Details</summary>
Motivation: 当前的V2X通信系统中，路边单元（RSUs）广播的警告信息缺乏上下文，导致驾驶员过度谨慎或驾驶行为低效。

Method: 提出SEE-V2X系统，RSUs配备智能摄像头检测障碍物，并传输上下文感知信息；通过实地演示展示“透视”功能。

Result: 仿真结果表明，SEE-V2X显著提高了交通效率并减少了不必要的减速。

Conclusion: SEE-V2X通过增强信息的语义和可解释性，优化了驾驶决策和交通流。

Abstract: In current vehicle-to-everything (V2X) communication systems, roadside units
(RSUs) broadcast brief warning messages that alert nearby vehicles to avoid
potential hazards. However, these messages lack contextual information on why a
warning is issued, leading to excessive caution or inefficient driving
behaviors. To avoid such a situation, we propose a semantic-enhanced and
explainable V2X (SEE-V2X) system. In the proposed system, RSUs equipped with
smart cameras detect obstructions and transmit context-aware messages to
vehicles. By understanding both what the hazard is and why it occurs, drivers
can make more intelligent decisions based on their specific driving situation.
Furthermore, through a real-field demonstration, we show the new "see-through"
feature in the proposed system, which enables drivers to visualize hidden
pedestrians behind obstacles. We also perform simulations to compare
traditional V2X with SEE-V2X under different traffic conditions. The results
show that SEE-V2X significantly improves traffic efficiency and reduces
unnecessary deceleration.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [220] [Towards Agentic OS: An LLM Agent Framework for Linux Schedulers](https://arxiv.org/abs/2509.01245)
*Yusheng Zheng,Yanpeng Hu,Wei Zhang,Andi Quinn*

Main category: cs.AI

TL;DR: SchedCP 是一个框架，利用大型语言模型（LLM）自主优化 Linux 调度器，解决了内核策略与应用程序需求之间的语义鸿沟问题，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的操作系统调度器存在语义鸿沟问题，无法充分理解应用程序的特定需求，导致性能不佳。需要一种自主化、高效且安全的优化方法。

Method: SchedCP 设计了名为 Model Context Protocol (MCP) 的服务器，提供三个关键服务：工作负载分析引擎、动态演化的调度策略库和执行验证器。通过多代理系统 sched-agent 分析工作负载并生成定制的 eBPF 调度策略。

Result: SchedCP 实现了最高 1.79 倍的性能提升和 13 倍的成本降低，同时保持了高成功率。

Conclusion: SchedCP 通过桥接语义鸿沟，实现了专家级别的系统优化，并为创建真正自适应的操作系统迈出了重要一步。

Abstract: Operating system schedulers suffer from a fundamental semantic gap, where
kernel policies fail to understand application-specific needs, leading to
suboptimal performance. We introduce SchedCP, the first framework that enables
fully autonomous Large Language Model (LLM) agents to safely and efficiently
optimize Linux schedulers without human involvement. Our core insight is that
the challenge is not merely to apply a better LLM, but to architect a decoupled
control plane that separates the AI's role of semantic reasoning ("what to
optimize") from the system's role of execution ("how to observe and act").
Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable
interface with three key services: a Workload Analysis Engine, an evolving
Scheduler Policy Repository, and an Execution Verifier that validates all
AI-generated code and configure before deployment with static and dynamic
analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent
system that autonomously analyzes workloads, synthesizes custom eBPF scheduling
policies, and deploys them via the sched\_ext infrastructure. Our evaluation
shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x
cost reduction compared to naive agentic approaches, all while maintaining high
success rate. By bridging the semantic gap, SchedCP democratizes expert-level
system optimization and represents a step towards creating truly
self-optimizing, application-aware operating systems. The code is open-sourced
in https://github.com/eunomia-bpf/schedcp

</details>


### [221] [Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)](https://arxiv.org/abs/2509.00184)
*Alexandru Baltag,Malvin Gattinger,Djanira Gomes*

Main category: cs.AI

TL;DR: 本文研究了多智能体证据模型中的群体知识与信念，通过拓扑语义扩展了个体的证据基础信念与可错知识到群体层面。


<details>
  <summary>Details</summary>
Motivation: 探索群体在多智能体系统中的知识与信念形式化问题。

Method: 通过拓扑语义扩展个体证据模型到群体层面，并提出动态证据共享算子。

Result: 完全公理化并证明了群体证据逻辑及片段的可判定性，动态扩展与原体系等价。

Conclusion: 证明动态证据共享逻辑与静态基础逻辑的表达能力相同。

Abstract: We study notions of (virtual) group knowledge and group belief within
multi-agent evidence models, obtained by extending the topological semantics of
evidence-based belief and fallible knowledge from individuals to groups. We
completely axiomatize and show the decidability of the logic of ("hard" and
"soft") group evidence, and do the same for an especially interesting fragment
of it: the logic of group knowledge and group belief. We also extend these
languages with dynamic evidence-sharing operators, and completely axiomatize
the corresponding logics, showing that they are co-expressive with their static
bases.

</details>


### [222] [Neuro-Symbolic Predictive Process Monitoring](https://arxiv.org/abs/2509.00834)
*Axel Mezini,Elena Umili,Ivan Donadello,Fabrizio Maria Maggi,Matteo Mancanelli,Fabio Patrizi*

Main category: cs.AI

TL;DR: 该论文提出了一种结合数据驱动学习和时序逻辑先验知识的神经符号预测过程监控方法，以提高后缀预测的准确性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在业务过程管理中预测后缀时，往往因缺乏领域知识的显式整合而无法满足基本逻辑约束。

Method: 提出了一种将有限轨迹上的线性时序逻辑（LTLf）整合到自回归序列预测器训练中的方法，通过可微的逻辑损失函数实现逻辑一致性。

Result: 在三个真实数据集上的实验表明，该方法提高了后缀预测的准确性和时序约束的符合性。

Conclusion: 该方法为神经符号AI的进步提供了新思路，适用于任何符号序列生成任务。

Abstract: This paper addresses the problem of suffix prediction in Business Process
Management (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring
(PPM) approach that integrates data-driven learning with temporal logic-based
prior knowledge. While recent approaches leverage deep learning models for
suffix prediction, they often fail to satisfy even basic logical constraints
due to the absence of explicit integration of domain knowledge during training.
We propose a novel method to incorporate Linear Temporal Logic over finite
traces (LTLf) into the training process of autoregressive sequence predictors.
Our approach introduces a differentiable logical loss function, defined using a
soft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be
combined with standard predictive losses. This ensures the model learns to
generate suffixes that are both accurate and logically consistent. Experimental
evaluation on three real-world datasets shows that our method improves suffix
prediction accuracy and compliance with temporal constraints. We also introduce
two variants of the logic loss (local and global) and demonstrate their
effectiveness under noisy and realistic settings. While developed in the
context of BPM, our framework is applicable to any symbolic sequence generation
task and contributes toward advancing Neuro-Symbolic AI.

</details>


### [223] [OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination](https://arxiv.org/abs/2509.00723)
*Junzhe Chen,Tianshu Zhang,Shiyu Huang,Yuwei Niu,Chao Sun,Rongzhou Zhang,Guanyu Zhou,Lijie Wen,Xuming Hu*

Main category: cs.AI

TL;DR: OmniDPO框架通过偏好对齐策略解决OLLMs中的多模态幻觉问题，提升音频-视频理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有OLLMs在多模态任务中仍存在幻觉问题，文本模态主导导致忽视视觉和音频信息，且缺乏模态间相关性建模。

Method: 提出OmniDPO框架，通过构建文本偏好和多模态偏好样本对，增强模型对音频-视频交互的理解及对视觉和听觉信息的关注。

Result: 实验表明，OmniDPO不仅有效减少幻觉，还显著提升跨模态推理能力。

Conclusion: OmniDPO为解决OLLMs中的多模态幻觉问题提供了一种高效框架。

Abstract: Recently, Omni-modal large language models (OLLMs) have sparked a new wave of
research, achieving impressive results in tasks such as audio-video
understanding and real-time environment perception. However, hallucination
issues still persist. Similar to the bimodal setting, the priors from the text
modality tend to dominate, leading OLLMs to rely more heavily on textual cues
while neglecting visual and audio information. In addition, fully multimodal
scenarios introduce new challenges. Most existing models align visual or
auditory modalities with text independently during training, while ignoring the
intrinsic correlations between video and its corresponding audio. This
oversight results in hallucinations when reasoning requires interpreting hidden
audio cues embedded in video content. To address these challenges, we propose
OmniDPO, a preference-alignment framework designed to mitigate hallucinations
in OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing
text-preference sample pairs to enhance the model's understanding of
audio-video interactions; and (2) constructing multimodal-preference sample
pairs to strengthen the model's attention to visual and auditory information.
By tackling both challenges, OmniDPO effectively improves multimodal grounding
and reduces hallucination. Experiments conducted on two OLLMs demonstrate that
OmniDPO not only effectively mitigates multimodal hallucinations but also
significantly enhances the models' reasoning capabilities across modalities.
All code and datasets will be released upon paper acceptance.

</details>


### [224] [Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First](https://arxiv.org/abs/2509.00997)
*Shu Liu,Soujanya Ponnapalli,Shreya Shankar,Sepanta Zeighami,Alan Zhu,Shubham Agarwal,Ruiqi Chen,Samion Suwito,Shuo Yuan,Ion Stoica,Matei Zaharia,Alvin Cheung,Natacha Crooks,Joseph E. Gonzalez,Aditya G. Parameswaran*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）代理将成为未来数据系统的主要工作负载，但其探索和解决问题的过程（称为代理推测）的高吞吐量和低效性对现有系统提出挑战。数据系统需要原生支持代理工作负载，并利用代理推测的特性（如规模、异构性、冗余和可操控性）来设计新的架构。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代理在数据处理中的主导作用，及其对现有数据系统的挑战，提出如何优化系统以支持代理工作负载。

Method: 通过分析代理推测的特性（规模、异构性、冗余、可操控性），提出适应代理优先的数据系统架构的研究方向。

Result: 识别了代理推测的挑战和机会，为设计新查询接口、处理技术和存储方案提供了方向。

Conclusion: 数据系统需为LLM代理的工作负载进行优化，以提升效率并支持其独特需求。

Abstract: Large Language Model (LLM) agents, acting on their users' behalf to
manipulate and analyze data, are likely to become the dominant workload for
data systems in the future. When working with data, agents employ a
high-throughput process of exploration and solution formulation for the given
task, one we call agentic speculation. The sheer volume and inefficiencies of
agentic speculation can pose challenges for present-day data systems. We argue
that data systems need to adapt to more natively support agentic workloads. We
take advantage of the characteristics of agentic speculation that we identify,
i.e., scale, heterogeneity, redundancy, and steerability - to outline a number
of new research opportunities for a new agent-first data systems architecture,
ranging from new query interfaces, to new query processing techniques, to new
agentic memory stores.

</details>


### [225] [GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models](https://arxiv.org/abs/2509.01308)
*Mattia Tritto,Giuseppe Farano,Dario Di Palma,Gaetano Rossiello,Fedelucio Narducci,Dharmashankar Subramanian,Tommaso Di Noia*

Main category: cs.AI

TL;DR: 该论文研究了使用结果奖励模型（ORMs）作为Text-to-SQL任务中的启发式方法，比传统的Best-of-N和Majority Voting方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL任务中取得了进展，但复杂查询中用户意图与数据库模式的精确对齐仍存在挑战，传统方法依赖于表面启发式。

Method: 提出了一种训练ORMs的框架，并将其与ex-BoN和Maj方法进行比较，同时在BIRD和SPIDER基准上评估性能。

Result: ORMs在BIRD和Spider基准上的执行准确率分别比ex-BoN高出4.33%和2.10%，比Maj高出2.91%和0.93%。

Conclusion: ORMs显著优于传统方法，特别是在已对齐SQL生成的模型上表现更佳，且在简单查询和候选生成数量增加时表现更优。

Abstract: Text-to-SQL, the task of translating natural language questions into SQL
queries, has significantly advanced with the introduction of Large Language
Models (LLMs), broadening database accessibility for a wide range of users.
Despite substantial progress in generating valid SQL, current LLMs still
struggle with complex queries that require precise alignment between user
intent and the database schema. To mitigate this, test-time strategies such as
Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the
assumption that LLMs can generate correct answers but may require multiple
attempts. However, these methods rely on surface-level heuristics, selecting
either the syntactically correct query through execution-based BoN (ex-BoN) or
the most frequently generated query with Maj. Recently, Outcome Reward Models
(ORMs), which assign utility scores to generated outputs based on semantic
correctness, have emerged as a promising approach for better aligning model
predictions with user intent. Nevertheless, their application to Text-to-SQL
remains largely underexplored.
  In this work, we evaluate ORMs as an effective heuristic for BoN, compare
them with ex-BoN and Maj, and introduce a framework for training ORMs for the
Text-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks,
finetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3
model families. Our results show that ORMs outperform ex-BoN and Maj, achieving
execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and
+2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that
finetuning models already aligned with SQL generation, such as OmniSQL, yields
superior ORM performance. Additionally, we observe that ORMs achieve
competitive results on simple queries and benefit more from an increased number
of candidates compared to ex-BoN and Maj.

</details>


### [226] [SHERPA: A Model-Driven Framework for Large Language Model Execution](https://arxiv.org/abs/2509.00272)
*Boqi Chen,Kua Chen,José Antonio Hernández López,Gunter Mussbacher,Dániel Varró,Amir Feizpour*

Main category: cs.AI

TL;DR: SHERPA是一个通过分层状态机将领域特定最佳实践融入LLM的框架，显著提升了复杂任务中LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多个领域表现优异，但其在复杂任务中缺乏结构化推理能力，尤其是缺乏领域特定最佳实践的训练数据。

Method: SHERPA通过分层状态机结构化LLM的执行过程，结合机器学习方法或规则进行行为控制。

Result: 实验表明SHERPA在代码生成、类名生成和问答等任务中显著提升了性能，尤其是在缺乏训练数据的复杂任务中。

Conclusion: SHERPA为LLMs提供了一种有效的控制机制，提升了其在复杂任务中的表现和适应性。

Abstract: Recently, large language models (LLMs) have achieved widespread application
across various fields. Despite their impressive capabilities, LLMs suffer from
a lack of structured reasoning ability, particularly for complex tasks
requiring domain-specific best practices, which are often unavailable in the
training data. Although multi-step prompting methods incorporating human best
practices, such as chain-of-thought and tree-of-thought, have gained
popularity, they lack a general mechanism to control LLM behavior. In this
paper, we propose SHERPA, a model-driven framework to improve the LLM
performance on complex tasks by explicitly incorporating domain-specific best
practices into hierarchical state machines. By structuring the LLM execution
processes using state machines, SHERPA enables more fine-grained control over
their behavior via rules or decisions driven by machine learning-based
approaches, including LLMs. We show that SHERPA is applicable to a wide variety
of tasks-specifically, code generation, class name generation, and question
answering-replicating previously proposed approaches while further improving
the performance. We demonstrate the effectiveness of SHERPA for the
aforementioned tasks using various LLMs. Our systematic evaluation compares
different state machine configurations against baseline approaches without
state machines. Results show that integrating well-designed state machines
significantly improves the quality of LLM outputs, and is particularly
beneficial for complex tasks with well-established human best practices but
lacking data used for training LLMs.

</details>


### [227] [When Agents go Astray: Course-Correcting SWE Agents with PRMs](https://arxiv.org/abs/2509.02360)
*Shubham Gandhi,Jason Tsay,Jatin Ganhotra,Kiran Kate,Yara Rizk*

Main category: cs.AI

TL;DR: 论文提出了一种推理时过程奖励模型（SWE-PRM），用于即时检测并纠正大型语言模型在软件工程任务中的轨迹错误，显著提高了任务解决率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在执行复杂软件工程任务时存在效率低下的问题，如冗余探索和无法及时终止，而现有方法多为事后诊断。

Method: 引入SWE-PRM模型，基于常见低效行为的分类，提供轻量级且可解释的反馈，无需修改底层策略。

Result: 在SWE-bench验证集上，PRM将任务解决率从40.0%提升至50.6%，并在中等和困难任务中表现最佳。

Conclusion: SWE-PRM是一种实用且可扩展的机制，能够显著提升代理的可靠性和效率，且推理成本低。

Abstract: Large Language Model (LLM) agents are increasingly deployed for complex,
multi-step software engineering (SWE) tasks. However, their trajectories often
contain costly inefficiencies, such as redundant exploration, looping, and
failure to terminate once a solution is reached. Prior work has largely treated
these errors in a post-hoc manner, diagnosing failures only after execution. In
this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM)
that intervenes during execution to detect and course-correct trajectory-level
errors. Our PRM design leverages a taxonomy of common inefficiencies and
delivers lightweight, interpretable feedback without modifying the underlying
policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0%
to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among
feedback strategies, taxonomy-guided PRMs outperform unguided or explicit
action-prescriptive variants, increasing success rate while reducing trajectory
length. These benefits come at an acceptable added inference cost of as low as
$0.2, making PRMs a practical and scalable mechanism for improving SWE agents'
reliability and efficiency.

</details>


### [228] [Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping](https://arxiv.org/abs/2509.01182)
*Wonduk Seo,Taesub Shin,Hyunjin An,Dokyun Kim,Seunghyun Lee*

Main category: cs.AI

TL;DR: Q2K 是一个多代理框架，利用大语言模型（LLMs）解决电商中 SKU 映射的挑战，通过推理、知识获取和去重代理实现高准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 电商中缺少明确标识和商品名称差异大导致 SKU 映射困难，传统基于规则的方法容易因忽略品牌、规格等细节而误分类。

Method: Q2K 包含推理代理生成消歧问题、知识代理通过搜索解决问题、去重代理复用已验证推理以减少冗余，并结合人工反馈优化。

Result: 实验表明 Q2K 在真实商品数据集上优于基线，尤其在捆绑识别和品牌来源消歧等复杂场景中表现更准确和鲁棒。

Conclusion: Q2K 通过复用推理而非重复搜索，平衡了准确性和效率，提供了可扩展且可解释的产品集成解决方案。

Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit
(SKU) is a persistent challenge in ecommerce, especially when explicit
identifiers are missing and product names vary widely across platforms. Rule
based heuristics and keyword similarity often misclassify products by
overlooking subtle distinctions in brand, specification, or bundle
configuration. To overcome these limitations, we propose Question to Knowledge
(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for
reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates
targeted disambiguation questions, (2) a Knowledge Agent that resolves them via
focused web searches, and (3) a Deduplication Agent that reuses validated
reasoning traces to reduce redundancy and ensure consistency. A human in the
loop mechanism further refines uncertain cases. Experiments on real world
consumer goods datasets show that Q2K surpasses strong baselines, achieving
higher accuracy and robustness in difficult scenarios such as bundle
identification and brand origin disambiguation. By reusing retrieved reasoning
instead of issuing repeated searches, Q2K balances accuracy with efficiency,
offering a scalable and interpretable solution for product integration.

</details>


### [229] [Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models](https://arxiv.org/abs/2509.01909)
*Ranjie Duan,Jiexi Liu,Xiaojun Jia,Shiji Zhao,Ruoxi Cheng,Fengxiang Wang,Cheng Wei,Yong Xie,Chang Liu,Defeng Li,Yinpeng Dong,Yichi Zhang,Yuefeng Chen,Chongwen Wang,Xingjun Ma,Xingxing Wei,Yang Liu,Hang Su,Jun Zhu,Xinfeng Li,Yitong Sun,Jie Zhang,Jinzhao Hu,Sha Xu,Yitong Yang,Jialing Tao,Hui Xue*

Main category: cs.AI

TL;DR: 论文提出了Constructive Safety Alignment (CSA)，一种以人为中心的安全范式，不仅防止恶意使用，还主动引导心理困扰的用户。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制主要关注对抗性风险，忽视了非恶意用户的心理需求，简单的拒绝可能带来负面后果。

Method: CSA结合博弈论的用户反应预测、细粒度风险边界发现和可解释推理控制，通过Oy1模型实现。

Result: Oy1在安全性和通用能力上表现优异，接近GPT-5的引导效果，且在Strata-Sword数据集上接近GPT-o1的鲁棒性。

Conclusion: CSA从拒绝优先转向引导优先的安全机制，重塑模型与用户关系，推动AI的负责任发展。

Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent
harmful content generation. Most current approaches focus narrowly on risks
posed by malicious actors, often framing risks as adversarial events and
relying on defensive refusals. However, in real-world settings, risks also come
from non-malicious users seeking help while under psychological distress (e.g.,
self-harm intentions). In such cases, the model's response can strongly
influence the user's next actions. Simple refusals may lead them to repeat,
escalate, or move to unsafe platforms, creating worse outcomes. We introduce
Constructive Safety Alignment (CSA), a human-centric paradigm that protects
against malicious misuse while actively guiding vulnerable users toward safe
and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic
anticipation of user reactions, fine-grained risk boundary discovery, and
interpretable reasoning control, turning safety into a trust-building process.
Oy1 achieves state-of-the-art safety among open models while retaining high
general capabilities. On our Constructive Benchmark, it shows strong
constructive engagement, close to GPT-5, and unmatched robustness on the
Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from
refusal-first to guidance-first safety, CSA redefines the model-user
relationship, aiming for systems that are not just safe, but meaningfully
helpful. We release Oy1, code, and the benchmark to support responsible,
user-centered AI.

</details>


### [230] [AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent](https://arxiv.org/abs/2509.02444)
*Jingru Fan,Yufan Dang,Jingyao Wu,Huatao Li,Runde Yang,Xiyuan Yang,Yuheng Wang,Zhong Zhang,Yaxi Lu,Yankai Lin,Zhiyuan Liu,Dahai Li,Chen Qian*

Main category: cs.AI

TL;DR: 论文提出了AppCopilot，一个多模态、多代理的通用设备助手，解决了移动代理在泛化性、准确性、长远任务能力和效率方面的四大核心问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和多模态基础模型的快速发展，移动代理领域面临的挑战尚未收敛。论文旨在解决泛化性、准确性、长远任务能力和效率四大核心问题，以实现实用化、可扩展的移动代理应用。

Method: AppCopilot采用端到端自主管道，整合多模态基础模型和链式思维推理、分层任务规划与多代理协作，并通过个性化配置和异构硬件优化实现高效运行。

Result: 实验表明，AppCopilot在四大核心问题上均取得显著改进：更强的泛化性、更高精度的屏幕操作、更可靠的长远任务完成能力以及更高效的运行。

Conclusion: AppCopilot通过系统设计和优化，成功解决了移动代理的关键挑战，为实际应用提供了可行的解决方案。

Abstract: With the raid evolution of large language models and multimodal foundation
models, the mobile-agent landscape has proliferated without converging on the
fundamental challenges. This paper identifies four core problems that must be
solved for mobile agents to deliver practical, scalable impact: (1)
generalization across tasks, modalities, apps, and devices; (2) accuracy,
specifically precise on-screen interaction and click targeting; (3)
long-horizon capability for sustained, multi-step goals; and (4) efficiency,
specifically high-performance runtime on resource-constrained devices. We
present AppCopilot, a multimodal, multi-agent, general-purpose on-device
assistant that operates across applications and constitutes a full-stack,
closed-loop system from data to deployment. AppCopilot operationalizes this
position through an end-to-end autonomous pipeline spanning data collection,
training, deployment, high-quality and efficient inference, and mobile
application development. At the model layer, it integrates multimodal
foundation models with robust Chinese-English support. At the reasoning and
control layer, it combines chain-of-thought reasoning, hierarchical task
planning and decomposition, and multi-agent collaboration. At the execution
layer, it enables user personalization and experiential adaptation, voice
interaction, function calling, cross-app and cross-device orchestration, and
comprehensive mobile app support. The system design incorporates
profiling-driven optimization for latency, memory, and energy across
heterogeneous hardware. Empirically, AppCopilot achieves significant
improvements along all four dimensions: stronger generalization,
higher-precision on-screen actions, more reliable long-horizon task completion,
and faster, more resource-efficient runtime.

</details>


### [231] [UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.02544)
*Haoming Wang,Haoyang Zou,Huatong Song,Jiazhan Feng,Junjie Fang,Junting Lu,Longxiang Liu,Qinyu Luo,Shihao Liang,Shijue Huang,Wanjun Zhong,Yining Ye,Yujia Qin,Yuwen Xiong,Yuxin Song,Zhiyong Wu,Bo Li,Chen Dun,Chong Liu,Fuxing Leng,Hanbin Wang,Hao Yu,Haobin Chen,Hongyi Guo,Jing Su,Jingjia Huang,Kai Shen,Kaiyu Shi,Lin Yan,Peiyao Zhao,Pengfei Liu,Qinghao Ye,Renjie Zheng,Wayne Xin Zhao,Wen Heng,Wenhao Huang,Wenqian Wang,Xiaobo Qin,Yi Lin,Youbin Wu,Zehui Chen,Zihao Wang,Baoquan Zhong,Xinchun Zhang,Xujing Li,Yuanfan Li,Zhongkai Zhao,Chengquan Jiang,Faming Wu,Haotian Zhou,Jinlin Pang,Li Han,Qianli Ma,Siyao Liu,Songhua Cai,Wenqi Fu,Xin Liu,Zhi Zhang,Bo Zhou,Guoliang Li,Jiajun Shi,Jiale Yang,Jie Tang,Li Li,Taoran Lu,Woyu Lin,Xiaokang Tong,Xinyao Li,Yichi Zhang,Yu Miao,Zhengxuan Jiang,Zili Li,Ziyuan Zhao,Chenxin Li,Dehua Ma,Feng Lin,Ge Zhang,Haihua Yang,Hangyu Guo,Hongda Zhu,Jiaheng Liu,Junda Du,Kai Cai,Kuanye Li,Lichen Yuan,Meilan Han,Minchao Wang,Shuyue Guo,Tianhao Cheng,Xiaobo Ma,Xiaojun Xiao,Xiaolong Huang,Xinjie Chen,Yidi Du,Yilin Chen,Yiwen Wang,Zhaojian Li,Zhenzhu Yang,Zhiyuan Zeng,Chaolin Jin,Chen Li,Hao Chen,Haoli Chen,Jian Chen,Qinghao Zhao,Guang Shi*

Main category: cs.AI

TL;DR: UI-TARS-2是一个面向图形用户界面（GUI）的自主代理模型，通过系统化的训练方法（如数据飞轮、多回合强化学习框架等）解决了数据扩展性、环境稳定性等问题，并在多项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发自主GUI代理面临数据扩展性、多回合强化学习等挑战，UI-TARS-2旨在解决这些问题。

Method: 采用数据飞轮生成可扩展数据、稳定的多回合RL框架、混合GUI环境及统一沙盒平台。

Result: 在GUI基准测试中表现优异，如Online-Mind2Web（88.2分），并在游戏环境和长时信息任务中展现泛化能力。

Conclusion: UI-TARS-2显著推进了GUI代理的现状，并在实际交互场景中表现出强泛化能力。

Abstract: The development of autonomous agents for graphical user interfaces (GUIs)
presents major challenges in artificial intelligence. While recent advances in
native agent models have shown promise by unifying perception, reasoning,
action, and memory through end-to-end learning, open problems remain in data
scalability, multi-turn reinforcement learning (RL), the limitations of
GUI-only operation, and environment stability. In this technical report, we
present UI-TARS-2, a native GUI-centered agent model that addresses these
challenges through a systematic training methodology: a data flywheel for
scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI
environment that integrates file systems and terminals, and a unified sandbox
platform for large-scale rollouts. Empirical evaluation demonstrates that
UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.
On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on
WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines
such as Claude and OpenAI agents. In game environments, it attains a mean
normalized score of 59.8 across a 15-game suite-roughly 60% of human-level
performance-and remains competitive with frontier proprietary models (e.g.,
OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to
long-horizon information-seeking tasks and software engineering benchmarks,
highlighting its robustness across diverse agent tasks. Detailed analyses of
training dynamics further provide insights into achieving stability and
efficiency in large-scale agent RL. These results underscore UI-TARS-2's
potential to advance the state of GUI agents and exhibit strong generalization
to real-world interactive scenarios.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [232] [Next-Generation Sustainable Wireless Systems: Energy Efficiency Meets Environmental Impact](https://arxiv.org/abs/2509.02395)
*Christo Kurisummoottil Thomas,Omar Hashash,Kimia Ehsani,Walid Saad*

Main category: cs.IT

TL;DR: 提出了一种新的可持续性指标（每比特排放量），并通过多目标强化学习优化6G网络的资源分配，平衡环境可持续性与网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有无线网络的可持续性设计仅关注能效，未能全面衡量环境影响，因此需要一种新指标和方法来整合可持续性。

Method: 结合多目标强化学习（MORL）优化能源、计算和通信资源的分配，以每比特排放量为关键指标。

Result: 仿真结果显示，该方法比现有技术平均减少26%的每比特排放量。

Conclusion: 通过新指标和MORL框架，成功在6G网络中实现了环境可持续性与性能的平衡。

Abstract: Aligning with the global mandates pushing towards advanced technologies with
reduced resource consumption and environmental impacts, the sustainability of
wireless networks becomes a significant concern in 6G systems. To address this
concern, a native integration of sustainability into the operations of
next-generation networks through novel designs and metrics is necessary.
Nevertheless, existing wireless sustainability efforts remain limited to
energy-efficient network designs which fail to capture the environmental impact
of such systems. In this paper, a novel sustainability metric is proposed that
captures emissions per bit, providing a rigorous measure of the environmental
foot- print associated with energy consumption in 6G networks. This metric also
captures how energy, computing, and communication resource parameters influence
the reduction of emissions per bit. Then, the problem of allocating the energy,
computing and communication resources is posed as a multi-objective (MO)
optimization problem. To solve the resulting non-convex problem, our framework
leverages MO reinforcement learning (MORL) to maximize the novel sustainability
metric alongside minimizing energy consumption and average delays in
successfully delivering the data, all while adhering to constraints on energy
resource capacity. The proposed MORL methodology computes a global policy that
achieves a Pareto-optimal tradeoff among multiple objectives, thereby balancing
environmental sustainability with network performance. Simulation results show
that the proposed approach reduces the average emissions per bit by around 26%
compared to state-of-the-art methods that do not explicitly integrate carbon
emissions into their control objectives.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [233] [Triangle Counting in Hypergraph Streams: A Complete and Practical Approach](https://arxiv.org/abs/2509.00674)
*Lingkai Meng,Long Yuan,Xuemin Lin,Wenjie Zhang,Ying Zhang*

Main category: cs.DS

TL;DR: 论文提出了一种新的超图流中三角形计数方法HTCount和HTCount-P，解决了现有方法在超顶点三角形分类和不灵活采样方案上的局限性，显著提高了内存利用效率和计数准确性。


<details>
  <summary>Details</summary>
Motivation: 超图中的三角形计数（包括超顶点和超边三角形）是一个基础性问题，广泛应用于超图分析。现有方法存在超顶点三角形分类不完整和采样方案不灵活的局限性。

Method: 首先完整分类超顶点三角形（内、混合和外三角形），然后开发了基于水库采样的HTCount算法，动态调整样本大小；进一步提出HTCount-P，通过自适应分区提高内存利用率。

Result: 实验表明，新算法在严格内存限制下实现了高精度三角形计数，相对误差比现有方法低1-2个数量级，且保持了高吞吐量。

Conclusion: HTCount和HTCount-P在内存约束下显著提高了三角形计数的准确性和效率，扩展了超图分析的应用范围。

Abstract: Triangle counting in hypergraph streams, including both hyper-vertex and
hyper-edge triangles, is a fundamental problem in hypergraph analytics, with
broad applications. However, existing methods face two key limitations: (i) an
incomplete classification of hyper-vertex triangle structures, typically
considering only inner or outer triangles; and (ii) inflexible sampling schemes
that predefine the number of sampled hyperedges, which is impractical under
strict memory constraints due to highly variable hyperedge sizes. To address
these challenges, we first introduce a complete classification of hyper-vertex
triangles, including inner, hybrid, and outer triangles. Based on this, we
develop HTCount, a reservoir-based algorithm that dynamically adjusts the
sample size based on the available memory M. To further improve memory
utilization and reduce estimation error, we develop HTCount-P, a
partition-based variant that adaptively partitions unused memory into
independent sample subsets. We provide theoretical analysis of the unbiasedness
and variance bounds of the proposed algorithms. Case studies demonstrate the
expressiveness of our triangle structures in revealing meaningful interaction
patterns. Extensive experiments on real-world hypergraphs show that both our
algorithms achieve highly accurate triangle count estimates under strict memory
constraints, with relative errors that are 1 to 2 orders of magnitude lower
than those of existing methods and consistently high throughput.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [234] [Self-Organising Memristive Networks as Physical Learning Systems](https://arxiv.org/abs/2509.00747)
*Francesco Caravelli,Gianluca Milano,Adam Z. Stieg,Carlo Ricciardi,Simon Anthony Brown,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 利用物理系统的非线性动力学进行学习的新范式，重点关注自组织忆阻网络（SOMNs）在实现高效、类脑持续学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统基于晶体管的硬件在人工智能计算中的不可持续性促使转向物理硬件的学习方法。

Method: 通过实验和理论方法（如平均场理论、图论等）研究自组织忆阻网络的非线性动力学及其学习能力。

Result: SOMNs展现出集体非线性、自适应动力学，并能在资源受限环境中实现嵌入式学习和实时决策。

Conclusion: SOMNs为新型物理智能技术的发展提供了独特机会，尤其在嵌入式边缘智能领域具有广阔应用前景。

Abstract: Learning with physical systems is an emerging paradigm that seeks to harness
the intrinsic nonlinear dynamics of physical substrates for learning. The
impetus for a paradigm shift in how hardware is used for computational
intelligence stems largely from the unsustainability of artificial neural
network software implemented on conventional transistor-based hardware. This
Perspective highlights one promising approach using physical networks comprised
of resistive memory nanoscale components with dynamically reconfigurable,
self-organising electrical circuitry. Experimental advances have revealed the
non-trivial interactions within these Self-Organising Memristive Networks
(SOMNs), offering insights into their collective nonlinear and adaptive
dynamics, and how these properties can be harnessed for learning using
different hardware implementations. Theoretical approaches, including
mean-field theory, graph theory, and concepts from disordered systems, reveal
deeper insights into the dynamics of SOMNs, especially during transitions
between different conductance states where criticality and other dynamical
phase transitions emerge in both experiments and models. Furthermore, parallels
between adaptive dynamics in SOMNs and plasticity in biological neuronal
networks suggest the potential for realising energy-efficient, brain-like
continual learning. SOMNs thus offer a promising route toward embedded edge
intelligence, unlocking real-time decision-making for autonomous systems,
dynamic sensing, and personalised healthcare, by enabling embedded learning in
resource-constrained environments. The overarching aim of this Perspective is
to show how the convergence of nanotechnology, statistical physics, complex
systems, and self-organising principles offers a unique opportunity to advance
a new generation of physical intelligence technologies.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [235] [Embodying computation in nonlinear perturbative metamaterials](https://arxiv.org/abs/2509.01625)
*Sima Zahedi Fard,Paolo Tiso,Parisa Omidvar,Marc Serra-Garcia*

Main category: cond-mat.mes-hall

TL;DR: 该论文提出了一种设计具有信息处理功能的超材料的方法，通过非线性坐标变换将紧束缚模型的功能映射到超材料几何结构中，展示了三种不同的计算范式应用。


<details>
  <summary>Details</summary>
Motivation: 设计能够执行高级计算的超材料具有挑战性，关键在于将非线性功能从离散模型准确映射到几何结构中。

Method: 采用两步策略：首先在紧束缚模型中编码功能，其次通过非线性坐标变换将功能映射到超材料几何结构中。

Result: 成功设计了三种基于不同计算范式的信息处理超材料：相干伊辛机、机械赛道存储器和模拟神经形态计算的语音分类材料。

Conclusion: 该方法为通过紧束缚模型设计广泛的超材料计算器件提供了可行路径。

Abstract: Designing metamaterials that carry out advanced computations poses a
significant challenge. A powerful design strategy splits the problem into two
steps: First, encoding the desired functionality in a discrete or tight-binding
model, and second, identifying a metamaterial geometry that conforms to the
model. Applying this approach to information-processing tasks requires
accurately mapping nonlinearity -- an essential element for computation -- from
discrete models to geometries. Here we formulate this mapping through a
nonlinear coordinate transformation that accurately connects tight-binding
degrees of freedom to metamaterial excitations in the nonlinear regime. This
transformation allows us to design information-processing metamaterials across
the broad range of computations that can be expressed as tight-binding models,
a capability we showcase with three examples based on three different computing
paradigms: a coherent Ising machine that approximates combinatorial
optimization problems through energy minimization, a mechanical racetrack
memory exemplifying in-memory computing, and a speech classification
metamaterial based on analog neuromorphic computing.

</details>


### [236] [Racetrack computing with a topological boundary ratchet](https://arxiv.org/abs/2509.01706)
*Parisa Omidvar,Markus Bestler,Sima Zahedi Fard,Oded Zilberberg,Marc Serra-Garcia*

Main category: cond-mat.mes-hall

TL;DR: 该论文通过实验实现了一种弹性超材料中的拓扑边界棘轮，展示了编码在屈曲域中的数字信息如何通过循环加载以量子化方式传输。


<details>
  <summary>Details</summary>
Motivation: 解决中性系统中信息传递稳定性的问题，提出了基于拓扑边界模式的解决方案。

Method: 通过实验和数值模拟，研究了屈曲域中的信息传输机制，并探索了可调机械约束对信息传播方向的控制。

Result: 成功实现了信息在弹性超材料中的量子化传输，并展示了可调的传播方向。

Conclusion: 该方法为中性系统中的赛道存储器提供了一种通用途径，具有潜在的应用价值。

Abstract: Multistable order parameters provide a natural means of encoding non-volatile
information in spatial domains, a concept that forms the foundation of magnetic
memory devices. However, this stability inherently conflicts with the need to
move information around the device for processing and readout. While in
magnetic systems, domains can be transported using currents or external fields,
mechanisms to robustly shuttle information-bearing domains across neutral
systems are scarce. Here, we experimentally realize a topological boundary
ratchet in an elastic metamaterial, where digital information is encoded in
buckling domains and transported in a quantized manner via cyclic loading. The
transport is topological in origin: neighboring domains act as different
topological pumps for their Bogoliubov excitations, so their interface hosts
topological boundary modes. Cyclic loading renders these modes unstable through
inter-domain pressure, which in turn drives the motion of the domain wall. We
demonstrate that the direction of information propagation can be controlled
through adjustable mechanical constraints on the buckling beams, and
numerically investigate buckling-based domain-wall logic circuits in an elastic
metamaterial network. The underlying tight-binding structure with low-order
nonlinearities makes this approach a general pathway toward racetrack memories
in neutral systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [237] [LongCat-Flash Technical Report](https://arxiv.org/abs/2509.01322)
*Meituan LongCat Team,Bayan,Bei Li,Bingye Lei,Bo Wang,Bolin Rong,Chao Wang,Chao Zhang,Chen Gao,Chen Zhang,Cheng Sun,Chengcheng Han,Chenguang Xi,Chi Zhang,Chong Peng,Chuan Qin,Chuyu Zhang,Cong Chen,Congkui Wang,Dan Ma,Daoru Pan,Defei Bu,Dengchang Zhao,Deyang Kong,Dishan Liu,Feiye Huo,Fengcun Li,Fubao Zhang,Gan Dong,Gang Liu,Gang Xu,Ge Li,Guoqiang Tan,Guoyuan Lin,Haihang Jing,Haomin Fu,Haonan Yan,Haoxing Wen,Haozhe Zhao,Hong Liu,Hongmei Shi,Hongyan Hao,Hongyin Tang,Huantian Lv,Hui Su,Jiacheng Li,Jiahao Liu,Jiahuan Li,Jiajun Yang,Jiaming Wang,Jian Yang,Jianchao Tan,Jiaqi Sun,Jiaqi Zhang,Jiawei Fu,Jiawei Yang,Jiaxi Hu,Jiayu Qin,Jingang Wang,Jiyuan He,Jun Kuang,Junhui Mei,Kai Liang,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Liang Gao,Liang Shi,Lianhui Ma,Lin Qiu,Lingbin Kong,Lingtong Si,Linkun Lyu,Linsen Guo,Liqi Yang,Lizhi Yan,Mai Xia,Man Gao,Manyuan Zhang,Meng Zhou,Mengxia Shen,Mingxiang Tuo,Mingyang Zhu,Peiguang Li,Peng Pei,Peng Zhao,Pengcheng Jia,Pingwei Sun,Qi Gu,Qianyun Li,Qingyuan Li,Qiong Huang,Qiyuan Duan,Ran Meng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shizhe Wu,Shuai Liang,Shuo Wang,Suogui Dang,Tao Fang,Tao Li,Tefeng Chen,Tianhao Bai,Tianhao Zhou,Tingwen Xie,Wei He,Wei Huang,Wei Liu,Wei Shi,Wei Wang,Wei Wu,Weikang Zhao,Wen Zan,Wenjie Shi,Xi Nan,Xi Su,Xiang Li,Xiang Mei,Xiangyang Ji,Xiangyu Xi,Xiangzhou Huang,Xianpeng Li,Xiao Fu,Xiao Liu,Xiao Wei,Xiaodong Cai,Xiaolong Chen,Xiaoqing Liu,Xiaotong Li,Xiaowei Shi,Xiaoyu Li,Xili Wang,Xin Chen,Xing Hu,Xingyu Miao,Xinyan He,Xuemiao Zhang,Xueyuan Hao,Xuezhi Cao,Xunliang Cai,Xurui Yang,Yan Feng,Yang Bai,Yang Chen,Yang Yang,Yaqi Huo,Yerui Sun,Yifan Lu,Yifan Zhang,Yipeng Zang,Yitao Zhai,Yiyang Li,Yongjing Yin,Yongkang Lv,Yongwei Zhou,Yu Yang,Yuchen Xie,Yueqing Sun,Yuewen Zheng,Yuhua Wei,Yulei Qian,Yunfan Liang,Yunfang Tai,Yunke Zhao,Zeyang Yu,Zhao Zhang,Zhaohua Yang,Zhenchao Zhang,Zhikang Xia,Zhiye Zou,Zhizhao Zeng,Zhongda Su,Zhuofan Chen,Zijian Zhang,Ziwen Wang,Zixu Jiang,Zizhe Zhao,Zongyu Wang,Zunhai Su*

Main category: cs.CL

TL;DR: LongCat-Flash是一个5600亿参数的混合专家语言模型，强调计算效率和智能代理能力，通过动态资源分配和高效推理设计实现了稳定训练和高性能推断。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模语言模型的效率与可扩展性问题，同时提升智能代理能力，研究团队开发了LongCat-Flash模型。

Method: 采用零计算专家和快捷连接MoE设计，结合超参数转移、模型增长初始化等技术，实现了高效训练与推理。

Result: 模型在30天内完成了20万亿token的训练，推断性能达到每秒100token，并在智能代理任务中表现突出。

Conclusion: LongCat-Flash为社区提供了高性能的开源模型，特别适用于智能代理任务。

Abstract: We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)
language model designed for both computational efficiency and advanced agentic
capabilities. Stemming from the need for scalable efficiency, LongCat-Flash
adopts two novel designs: (a) Zero-computation Experts, which enables dynamic
computational budget allocation and activates 18.6B-31.3B (27B on average) per
token depending on contextual demands, optimizing resource usage. (b)
Shortcut-connected MoE, which enlarges the computation-communication overlap
window, demonstrating notable gains in inference efficiency and throughput
compared to models of a comparable scale. We develop a comprehensive scaling
framework for large models that combines hyperparameter transfer, model-growth
initialization, a multi-pronged stability suite, and deterministic computation
to achieve stable and reproducible training. Notably, leveraging the synergy
among scalable architectural design and infrastructure efforts, we complete
model training on more than 20 trillion tokens within 30 days, while achieving
over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million
output tokens. To cultivate LongCat-Flash towards agentic intelligence, we
conduct a large-scale pre-training on optimized mixtures, followed by targeted
mid- and post-training on reasoning, code, and instructions, with further
augmentation from synthetic data and tool use tasks. Comprehensive evaluations
demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers
highly competitive performance among other leading models, with exceptional
strengths in agentic tasks. The model checkpoint of LongCat-Flash is
open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat

</details>


### [238] [Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting](https://arxiv.org/abs/2509.00482)
*Saksorn Ruangtanusak,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 该报告探讨了四种提示方法，以优化工具增强大型语言模型在角色扮演对话中的表现，其中基于规则的提示方法（RRP）效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决角色扮演对话代理中过度发言和工具使用不足的问题。

Method: 研究了四种提示方法：基本角色提示、人工制作角色提示、自动提示优化（APO）和基于规则的角色提示（RRP）。

Result: RRP方法通过新颖技术（角色卡/场景合同设计和严格函数调用）取得最佳效果，优于零样本基线。

Conclusion: RRP设计显著提高了角色扮演对话代理的效率和可靠性，优于更复杂的方法如APO。

Abstract: This report investigates approaches for prompting a tool-augmented large
language model (LLM) to act as a role-playing dialogue agent in the API track
of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this
setting, dialogue agents often produce overly long in-character responses
(over-speaking) while failing to use tools effectively according to the persona
(under-acting), such as generating function calls that do not exist or making
unnecessary tool calls before answering. We explore four prompting approaches
to address these issues: 1) basic role prompting, 2) human-crafted role
prompting, 3) automatic prompt optimization (APO), and 4) rule-based role
prompting. The rule-based role prompting (RRP) approach achieved the best
performance through two novel techniques--character-card/scene-contract design
and strict enforcement of function calling--which led to an overall score of
0.571, improving on the zero-shot baseline score of 0.519. These findings
demonstrate that RRP design can substantially improve the effectiveness and
reliability of role-playing dialogue agents compared with more elaborate
methods such as APO. To support future efforts in developing persona prompts,
we are open-sourcing all of our best-performing prompts and the APO tool.
Source code is available at https://github.com/scb-10x/apo.

</details>


### [239] [Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts](https://arxiv.org/abs/2509.01814)
*Shreyas Tirumala,Nishant Jain,Danny D. Leybzon,Trent D. Buskirk*

Main category: cs.CL

TL;DR: Transformer大语言模型（LLM）催生了“AI面试官”，能够实时进行语音调查。本文探讨了AI面试系统在定量和定性研究中的适用性，分析其输入/输出性能与语言推理能力。AI面试官在数据收集方面已优于传统IVR系统，但在转录错误率、情感检测和后续问题质量上仍有局限，适用性需结合具体情境。


<details>
  <summary>Details</summary>
Motivation: 评估AI面试系统的适用性，以确定其在定量和定性研究中的潜力。

Method: 通过比较AI面试官与传统IVR系统在输入/输出性能和语言推理能力上的表现，结合实地研究数据进行分析。

Result: AI面试官在数据收集方面优于IVR系统，但在实时转录错误率、情感检测和后续问题质量上存在不足。

Conclusion: 当前AI面试技术效果显著，尤其在定量研究中，但在定性研究中适用性受限于其技术缺陷，需进一步改进。

Abstract: Transformer-based Large Language Models (LLMs) have paved the way for "AI
interviewers" that can administer voice-based surveys with respondents in
real-time. This position paper reviews emerging evidence to understand when
such AI interviewing systems are fit for purpose for collecting data within
quantitative and qualitative research contexts. We evaluate the capabilities of
AI interviewers as well as current Interactive Voice Response (IVR) systems
across two dimensions: input/output performance (i.e., speech recognition,
answer recording, emotion handling) and verbal reasoning (i.e., ability to
probe, clarify, and handle branching logic). Field studies suggest that AI
interviewers already exceed IVR capabilities for both quantitative and
qualitative data collection, but real-time transcription error rates, limited
emotion detection abilities, and uneven follow-up quality indicate that the
utility, use and adoption of current AI interviewer technology may be
context-dependent for qualitative data collection efforts.

</details>
