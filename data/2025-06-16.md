<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 68]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]
- [eess.SP](#eess.SP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [math.LO](#math.LO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality](https://arxiv.org/abs/2506.10984)
*Ahilan Ayyachamy Nadar Ponnusamy*

Main category: cs.SE

TL;DR: 论文探讨了AI辅助代码生成工具的问题，提出了结合LLM代码推理与生成能力及人类专业知识的框架，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助代码生成中的安全漏洞、可靠性问题和不一致性，以释放其潜力。

Method: 提出一个结合LLM的代码推理与生成能力及人类专业知识的框架，并通过案例研究验证。

Result: 案例研究展示了框架在应用现代化中的实际效用。

Conclusion: 强调人类参与的重要性，为未来研究提供基础框架和实用洞见。

Abstract: AI-assisted code generation tools have revolutionized software development,
offering unprecedented efficiency and scalability. However, multiple studies
have consistently highlighted challenges such as security vulnerabilities,
reliability issues, and inconsistencies in the generated code. Addressing these
concerns is crucial to unlocking the full potential of this transformative
technology. While advancements in foundational and code-specialized language
models have made notable progress in mitigating some of these issues,
significant gaps remain, particularly in ensuring high-quality, trustworthy
outputs.
  This paper builds upon existing research on leveraging large language models
(LLMs) for application modernization. It explores an opinionated approach that
emphasizes two core capabilities of LLMs: code reasoning and code generation.
The proposed framework integrates these capabilities with human expertise to
tackle application modernization challenges effectively. It highlights the
indispensable role of human involvement and guidance in ensuring the success of
AI-assisted processes.
  To demonstrate the framework's utility, this paper presents a detailed case
study, walking through its application in a real-world scenario. The analysis
includes a step-by-step breakdown, assessing alternative approaches where
applicable. This work aims to provide actionable insights and a robust
foundation for future research in AI-driven application modernization. The
reference implementation created for this paper is available on GitHub.

</details>


### [2] [Collaboration Tools and their Role in Agile Software Projects](https://arxiv.org/abs/2506.10985)
*Raman Mohammed Hussein,Bryar A. Hassan*

Main category: cs.SE

TL;DR: 这篇综述探讨了协作工具（如Slack、Microsoft Teams和Confluence）在敏捷和软件项目中的重要性，强调了它们如何促进团队协作和沟通。


<details>
  <summary>Details</summary>
Motivation: 由于团队协作和沟通在远程工作中仍存在挑战，研究这些工具如何支持敏捷方法论的实施变得至关重要。

Method: 论文通过分析Slack、Microsoft Teams和Confluence如何符合敏捷原则，并促进迭代开发和任务管理。

Result: 这些工具改善了任务协调、知识共享，并帮助团队在跨职能环境中更好地采用敏捷价值观。

Conclusion: 协作工具对提升敏捷项目的效率和团队协作具有显著作用。

Abstract: The purpose of this review is to understand the importance of collaboration
tools which are Slack, Microsoft Teams, Confluence in Agile and software
projects. Agile methodologies rely on flexibility, using cycles and integration
throughout various levels of developing cycles. However, it is still a great
problem for many teams to collaborate and communicate even if staff members and
teams are working remotely. In terms of collaboration, the applications and
technologies mean better organization of work, increased mutually
understandable openness and fast and efficient inter team and interpersonal
interactions to enhance results of projects into productivity. This paper
examines how these tools fit the Agile principles, how they facilitate
iterative development, and encouraging effective initiation and tracking of
tasks in small and large projects. The insights focus on how Slack, Microsoft
Teams, and Confluence are essential for gaining better task coordination,
supporting knowledge sharing, and adopting agile values across cross-functional
contexts.

</details>


### [3] [CoMRAT: Commit Message Rationale Analysis Tool](https://arxiv.org/abs/2506.10986)
*Mouna Dhaouadi,Bentley James Oakes,Michalis Famelis*

Main category: cs.SE

TL;DR: CoMRAT 是一款用于分析提交消息中决策和理由句的工具，旨在帮助研究人员和开发者更好地理解和利用提交消息中的理由信息。初步评估表明该工具在研究与开发中均有实用性和可用性。


<details>
  <summary>Details</summary>
Motivation: 提交消息中常包含代码变更的理由，但关于这一主题的研究却较为有限。

Method: 开发了 CoMRAT 工具，用于分析 GitHub 模块中提交消息中的理由信息。

Result: 初步评估表明工具在研究（提供指标和分析）和开发（检查提交消息中的理由量）中均有用。

Conclusion: CoMRAT 为研究提交消息中的理由提供了新工具，并展示了其潜在价值。

Abstract: In collaborative open-source development, the rationale for code changes is
often captured in commit messages, making them a rich source of valuable
information. However, research on rationale in commit messages remains limited.
In this paper, we present CoMRAT, a tool for analyzing decision and rationale
sentences rationale in commit messages. CoMRAT enables a) researchers to
produce metrics and analyses on rationale information in any Github module, and
b) developers to check the amount of rationale in their commit messages. A
preliminary evaluation suggests the tool's usefulness and usability in both
these research and development contexts.

</details>


### [4] [Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks](https://arxiv.org/abs/2506.10987)
*Shaoyi Yang*

Main category: cs.SE

TL;DR: 该研究将Chain of Draft（CoD）方法扩展到软件工程，发现CoD变体在代码任务中使用比Chain of Thought（CoT）更少的令牌数，显著提高了效率，同时保持了90%以上的代码质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在软件开发中至关重要，但复杂代码任务需要冗长的中间推理，导致高延迟和成本。研究旨在通过CoD方法提高效率。

Method: 研究设计和评估了针对代码任务的多种CoD变体，并在SWE-bench的300个样本上进行了全面实验，比较了CoD与CoT的性能。

Result: CoD变体使用显著少于CoT的令牌数（Baseline CoD仅占55.4%），效率提升约为45%，同时代码质量保持在90%以上。

Conclusion: 研究表明，领域特定特性会影响提示策略的有效性，并为软件工程应用提供平衡效率与质量的框架，优化基于LLM的开发工作流。

Abstract: Large language models (LLMs) have become vital tools for software
development, but they often require verbose intermediate reasoning for complex
code tasks, leading to high latency and costs. This research extends the Chain
of Draft (CoD) method to software engineering, designing and evaluating
multiple CoD variants tailored for code tasks. Through comprehensive
experiments on all 300 samples from the SWE-bench benchmark, we found that all
CoD variants used significantly fewer tokens than Chain of Thought (CoT), with
Baseline CoD being most efficient at 55.4% of CoT's tokens. While this
represents substantial efficiency gains - translating to approximately 45%
reduction in processing time and API costs - it differs from the extreme 7.6%
reported in the original CoD paper for mathematical reasoning. This difference
stems from the inherent complexity and context-dependency of software tasks,
which require more detailed reasoning to maintain solution quality. Our
multi-dimensional quality assessment revealed that CoD variants maintain over
90% of CoT's code quality across key metrics including correctness,
compatibility, and maintainability, making them practical alternatives for
real-world development scenarios where efficiency matters. This research
demonstrates how domain-specific characteristics influence prompting strategy
effectiveness and provides a framework for balancing efficiency with solution
quality in software engineering applications. Our findings offer practical
guidance for optimizing LLM-based development workflows through appropriate
prompting strategy selection based on project requirements.

</details>


### [5] [You Only Train Once: A Flexible Training Framework for Code Vulnerability Detection Driven by Vul-Vector](https://arxiv.org/abs/2506.10988)
*Bowen Tian,Zhengyang Xu,Mingqiang Wu,Songning Lai,Yutai Yue*

Main category: cs.SE

TL;DR: 论文提出YOTO框架，通过参数融合整合多种漏洞检测模型，避免联合训练，快速适应新漏洞，减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程复杂多样，传统深度学习方法需要大量标注数据和长时间训练，难以应对新漏洞的快速出现。

Method: 引入YOTO框架，利用参数融合技术整合多类型漏洞检测模型，无需联合训练。

Result: YOTO能快速适配新漏洞，显著降低模型更新的时间和计算资源需求。

Conclusion: YOTO为漏洞检测提供了一种高效、灵活的新方法，适用于快速变化的软件环境。

Abstract: With the pervasive integration of computer applications across industries,
the presence of vulnerabilities within code bases poses significant risks. The
diversity of software ecosystems coupled with the intricate nature of modern
software engineering has led to a shift from manual code vulnerability
identification towards the adoption of automated tools. Among these, deep
learning-based approaches have risen to prominence due to their superior
accuracy; however, these methodologies encounter several obstacles. Primarily,
they necessitate extensive labeled datasets and prolonged training periods, and
given the rapid emergence of new vulnerabilities, the frequent retraining of
models becomes a resource-intensive endeavor, thereby limiting their
applicability in cutting-edge scenarios. To mitigate these challenges, this
paper introduces the \underline{\textbf{YOTO}}--\underline{\textbf{Y}}ou
\underline{\textbf{O}}nly \underline{\textbf{T}}rain \underline{\textbf{O}}nce
framework. This innovative approach facilitates the integration of multiple
types of vulnerability detection models via parameter fusion, eliminating the
need for joint training. Consequently, YOTO enables swift adaptation to newly
discovered vulnerabilities, significantly reducing both the time and
computational resources required for model updates.

</details>


### [6] [LeanExplore: A search engine for Lean 4 declarations](https://arxiv.org/abs/2506.11085)
*Justin Asher*

Main category: cs.SE

TL;DR: LeanExplore是一个为Lean 4生态系统设计的语义搜索引擎，支持跨多个包的形式化与非形式化搜索。


<details>
  <summary>Details</summary>
Motivation: 随着Lean 4生态系统的扩展，其庞大的库使得导航变得困难，因此需要一个高效的搜索工具。

Method: 采用混合排序策略，结合多源语义嵌入模型、BM25+关键词评分和基于PageRank的声明重要性评分。

Result: 实现了LeanExplore搜索引擎，提供网站和Python API访问，支持LLM集成以增强交互性和研究效率。

Conclusion: LeanExplore能够显著提升Lean 4工作流和AI驱动的数学研究效率。

Abstract: The expanding Lean 4 ecosystem poses challenges for navigating its vast
libraries. This paper introduces LeanExplore, a search engine for Lean 4
declarations. LeanExplore enables users to semantically search for statements,
both formally and informally, across select Lean 4 packages (including
Batteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is
powered by a hybrid ranking strategy, integrating scores from a multi-source
semantic embedding model (capturing conceptual meaning from formal Lean code,
docstrings, AI-generated informal translations, and declaration titles), BM25+
for keyword-based lexical relevance, and a PageRank-based score reflecting
declaration importance and interconnectedness. The search engine is accessible
via a dedicated website (https://www.leanexplore.com/) and a Python API
(https://github.com/justincasher/lean-explore). Furthermore, the database can
be downloaded, allowing users to self-host the service. LeanExplore integrates
easily with LLMs via the model context protocol (MCP), enabling users to chat
with an AI assistant about Lean declarations or utilize the search engine for
building theorem-proving agents. This work details LeanExplore's architecture,
data processing, functionalities, and its potential to enhance Lean 4 workflows
and AI-driven mathematical research

</details>


### [7] [Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs](https://arxiv.org/abs/2506.10989)
*Rogelio Cruz,Jonatan Contreras,Francisco Guerrero,Ezequiel Rodriguez,Carlos Valdez,Citlali Carrillo*

Main category: cs.SE

TL;DR: 提出了一种新的提示方法，旨在提升大语言模型生成准确Python代码的能力。


<details>
  <summary>Details</summary>
Motivation: 增强大语言模型生成代码的准确性和可靠性。

Method: 设计了一种提示模板，优化代码生成质量，并通过HumanEval数据集在两种先进LLM上进行实验。

Result: 该方法在Pass@k指标上优于零样本和思维链方法，且显著减少token使用量。

Conclusion: 定制化提示策略能优化代码生成性能，降低计算需求，拓宽AI编程应用。

Abstract: In this paper, we propose a novel prompting approach aimed at enhancing the
ability of Large Language Models (LLMs) to generate accurate Python code.
Specifically, we introduce a prompt template designed to improve the quality
and correctness of generated code snippets, enabling them to pass tests and
produce reliable results. Through experiments conducted on two state-of-the-art
LLMs using the HumanEval dataset, we demonstrate that our approach outperforms
widely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the
Pass@k metric. Furthermore, our method achieves these improvements with
significantly reduced token usage compared to the CoT approach, making it both
effective and resource-efficient, thereby lowering the computational demands
and improving the eco-footprint of LLM capabilities. These findings highlight
the potential of tailored prompting strategies to optimize code generation
performance, paving the way for broader applications in AI-driven programming
tasks.

</details>


### [8] [From over-reliance to smart integration: using Large-Language Models as translators between specialized modeling and simulation tools](https://arxiv.org/abs/2506.11141)
*Philippe J. Giabbanelli,John Beverley,Istvan David,Andreas Tolk*

Main category: cs.SE

TL;DR: 论文探讨将大语言模型（LLMs）作为中间件或翻译工具，以简化建模与仿真（M&S）任务，同时避免过度依赖带来的质量问题。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的自然语言接口简化M&S工作流，但需避免因其模糊性、逻辑捷径和幻觉问题而影响质量。

Method: 将LLMs作为中间件或翻译工具集成到专门工具中，开发高效软件架构，并采用低秩适应（Low-Rank Adaptation）方法进行任务适配。

Result: LLMs能够提升不同形式、语义和范式系统间的互操作性，同时优化M&S任务的工作效率。

Conclusion: LLMs应作为专门工具的补充而非替代，以确保高质量和可靠的M&S流程。

Abstract: Large Language Models (LLMs) offer transformative potential for Modeling &
Simulation (M&S) through natural language interfaces that simplify workflows.
However, over-reliance risks compromising quality due to ambiguities, logical
shortcuts, and hallucinations. This paper advocates integrating LLMs as
middleware or translators between specialized tools to mitigate complexity in
M&S tasks. Acting as translators, LLMs can enhance interoperability across
multi-formalism, multi-semantics, and multi-paradigm systems. We address two
key challenges: identifying appropriate languages and tools for modeling and
simulation tasks, and developing efficient software architectures that
integrate LLMs without performance bottlenecks. To this end, the paper explores
LLM-mediated workflows, emphasizes structured tool integration, and recommends
Low-Rank Adaptation-based architectures for efficient task-specific
adaptations. This approach ensures LLMs complement rather than replace
specialized tools, fostering high-quality, reliable M&S processes.

</details>


### [9] [On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances](https://arxiv.org/abs/2506.10990)
*Roberto Vergallo,Luís Cruz,Alessio Errico,Luca Mainetti*

Main category: cs.SE

TL;DR: 论文研究了'Follow-the-Sun' (FtS)策略在降低AI工作负载碳足迹方面的效果，通过实验验证其优于其他策略。


<details>
  <summary>Details</summary>
Motivation: AI的高能耗问题引发广泛讨论，但缺乏科学证据支持FtS策略在减少碳足迹方面的优势。

Method: 在部分合成场景下，对四种异常检测算法进行了实验，比较了无策略、FtS及两种现有策略的碳排放差异。

Result: FtS策略平均减少14.6%碳排放（峰值达16.3%），同时保持训练时间不变。

Conclusion: FtS策略能有效降低AI工作负载的碳足迹，为可持续发展提供支持。

Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at
minimizing the carbon footprint of computer workloads. It involves dynamically
moving workloads to regions with cleaner energy sources as demand increases and
energy production relies more on fossil fuels. With the significant power
consumption of Artificial Intelligence (AI) being a subject of extensive
debate, FtS is proposed as a strategy to mitigate the carbon footprint of
training AI models. However, the literature lacks scientific evidence on the
advantages of FtS to mitigate the carbon footprint of AI workloads. In this
paper, we present the results of an experiment conducted in a partial synthetic
scenario to address this research gap. We benchmarked four AI algorithms in the
anomaly detection domain and measured the differences in carbon emissions in
four cases: no strategy, FtS, and two strategies previously introduced in the
state of the art, namely Flexible Start and Pause and Resume. To conduct our
experiment, we utilized historical carbon intensity data from the year 2021 for
seven European cities. Our results demonstrate that the FtS strategy not only
achieves average reductions of up to 14.6% in carbon emissions (with peaks of
16.3%) but also helps in preserving the time needed for training.

</details>


### [10] [Model Discovery and Graph Simulation: A Lightweight Alternative to Chaos Engineering](https://arxiv.org/abs/2506.11176)
*Anatoly A. Krasnovsky,Alexander Zorkin*

Main category: cs.SE

TL;DR: 该论文提出了一种自动化模型发现方法，通过从跟踪数据中提取实时依赖图来预测微服务应用的弹性，实验证明该方法准确且高效。


<details>
  <summary>Details</summary>
Motivation: 微服务应用因密集的服务间依赖容易发生级联故障，传统弹性测试需在生产环境中进行故障注入实验，过程复杂且耗时。

Method: 论文提出自动化模型发现技术，从跟踪数据中提取依赖图，并通过蒙特卡洛模拟预测弹性，再通过实际混沌实验验证。

Result: 实验结果显示，依赖图模型的预测与实际观测结果高度吻合，无复制情况下预测弹性为0.161，实际为0.186；有复制时两者均为0.305。

Conclusion: 研究表明，即使是简单的自动发现依赖图也能高精度预测微服务可用性，为设计阶段提供快速洞察，无需大规模故障测试。

Abstract: Microservice applications are prone to cascading failures because of dense
inter-service dependencies. Ensuring resilience usually demands fault-injection
experiments in production-like setups. We propose \textit{model discovery} --
an automated CI/CD step that extracts a live dependency graph from trace data
-- and show that this lightweight representation is sufficient for accurate
resilience prediction. Using the DeathStarBench Social Network, we build the
graph, simulate failures via Monte-Carlo, and run matching chaos experiments on
the real system. The graph model closely matches reality: with no replication,
16 trials yield an observed resilience of 0.186 versus a predicted 0.161; with
replication, both observed and predicted values converge to 0.305 (mean
absolute error \leq 0.0004). These results indicate that even a simple,
automatically discovered graph can estimate microservice availability with high
fidelity, offering rapid design-time insight without full-scale failure
testing.

</details>


### [11] [What is Business Process Automation Anyway?](https://arxiv.org/abs/2506.10991)
*Hoang Vu,Henrik Leopold,Han van der Aa*

Main category: cs.SE

TL;DR: 本文通过分析18家主要业务流程自动化供应商的市场情况，全面概述了当前工业界的自动化能力及其未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 学术界主要关注机器人流程自动化（RPA），而忽略了工业界提供的更广泛的自动化能力，因此本文旨在填补这一研究空白。

Method: 通过结构化的市场分析，研究了Gartner认定的18家主要业务流程自动化供应商的解决方案。

Result: 提供了当前工业界业务流程自动化能力的全面概述，并分析了自动化类型及其未来潜力。

Conclusion: 本文揭示了业务流程自动化的多样性，并指出了未来研究与实践的潜在方向。

Abstract: Many organizations strive to increase the level of automation in their
business processes. While automation historically was mainly concerned with
automating physical labor, current automation efforts mostly focus on
automation in a digital manner, thus targeting work that is related to the
interaction between humans and computers. This type of automation, commonly
referred to as business process automation, has many facets. Yet, academic
literature mainly focuses on Robotic Process Automation, a specific automation
capability. Recognizing that leading vendors offer automation capabilities
going way beyond that, we use this paper to develop a detailed understanding of
business process automation in industry. To this end, we conduct a structured
market analysis of the 18 predominant vendors of business process automation
solutions as identified by Gartner. As a result, we provide a comprehensive
overview of the business process automation capabilities currently offered by
industrial vendors. We show which types and facets of automation exist and
which aspects represent promising directions for the future.

</details>


### [12] [Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing](https://arxiv.org/abs/2506.11180)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Felix Gehlhoff*

Main category: cs.SE

TL;DR: 论文提出了一种基于模型上下文协议（MCP）的新方法，通过标准化接口使功能对LLM更易访问，无需复杂的手动建模。


<details>
  <summary>Details</summary>
Motivation: 传统的能力和技能建模方法需要大量手动工作且难以被LLM访问，因此需要更高效的方式。

Method: 使用MCP标准化接口，将资源功能直接暴露给LLM代理，并在实验室规模的制造系统中进行原型评估。

Result: 实验表明，该方法实现了灵活工业自动化，无需依赖显式语义模型。

Conclusion: 为LLM驱动的生产系统中外部工具集成奠定了基础。

Abstract: Explicit modeling of capabilities and skills -- whether based on ontologies,
Asset Administration Shells, or other technologies -- requires considerable
manual effort and often results in representations that are not easily
accessible to Large Language Models (LLMs). In this work-in-progress paper, we
present an alternative approach based on the recently introduced Model Context
Protocol (MCP). MCP allows systems to expose functionality through a
standardized interface that is directly consumable by LLM-based agents. We
conduct a prototypical evaluation on a laboratory-scale manufacturing system,
where resource functions are made available via MCP. A general-purpose LLM is
then tasked with planning and executing a multi-step process, including
constraint handling and the invocation of resource functions via MCP. The
results indicate that such an approach can enable flexible industrial
automation without relying on explicit semantic models. This work lays the
basis for further exploration of external tool integration in LLM-driven
production systems.

</details>


### [13] [Towards a Theory on Process Automation Effects](https://arxiv.org/abs/2506.10992)
*Hoang Vu,Jennifer Haase,Henrik Leopold,Jan Mendling*

Main category: cs.SE

TL;DR: 探讨自动化对业务流程的影响，提出优化自动化使用的方法。


<details>
  <summary>Details</summary>
Motivation: 研究自动化技术的实际效果及其对人类工作的影响。

Method: 通过文献综述分析人机交互，提出技术、参与者、管理者和开发者之间的互动模型。

Result: 提出了优化自动化的建议并提出了新的研究问题。

Conclusion: 自动化需要更好的互动模型以提升效果，并鼓励进一步研究。

Abstract: Process automation is a crucial strategy for improving business processes,
but little attention has been paid to the effects that automation has once it
is operational. This paper addresses this research problem by reviewing the
literature on human-automation interaction. Although many of the studies in
this field have been conducted in different domains, they provide a foundation
for developing propositions about process automation effects. Our analysis
focuses on how humans perceive automation technology when working within a
process, allowing us to propose an effective engagement model between
technology, process participants, process managers, and software developers.
This paper offers insights and recommendations that can help organizations
optimize their use of process automation. We further derive novel research
questions for a discourse within the process automation community.

</details>


### [14] [Contract-based Verification of Digital Twins](https://arxiv.org/abs/2506.10993)
*Muhammad Naeem,Cristina Seceleanu*

Main category: cs.SE

TL;DR: 论文提出一种通过集成模型检查来验证神经网络数字孪生模型的创新方法，利用系统级合约和UPPAAL模型检查器自动识别模型行为不符合合约的场景。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在工业应用中日益重要，但其模型的验证因大数据集而面临挑战，需要一种无需了解技术细节的黑盒验证方法。

Method: 提出结合模型检查的方法，通过在Simulink中实现数字孪生模型，并基于UPPAAL模型检查器验证其行为是否符合系统级合约。

Result: 在锅炉系统案例中成功识别预测错误，验证了方法的有效性。

Conclusion: 模型检查与数字孪生模型的结合可实现持续改进，提升模型可靠性。

Abstract: Digital twins are becoming powerful tools in industrial applications,
offering virtual representations of cyber-physical systems. However,
verification of these models remains a significant challenge due to the
potentially large datasets used by the digital twin. This paper introduces an
innovative methodology for verifying neural network-based digital twin models,
in a black-box fashion, by integrating model checking into the process. The
latter relies on defining and applying system-level contracts that capture the
system's requirements, to verify the behavior of digital twin models,
implemented in Simulink. We develop an automated solution that simulates the
digital twin model for certain inputs, and feeds the predicted outputs together
with the inputs to the contract model described as a network of timed automata
in the UPPAAL model checker. The latter verifies whether the predicted outputs
fulfill the specified contracts. This approach allows us to identify scenarios
where the digital twin's behavior fails to meet the contracts, without
requiring the digital twin's design technicalities. We apply our method to a
boiler system case study for which we identify prediction errors via contract
verification. Our work demonstrates the effectiveness of integrating model
checking with digital twin models for continuous improvement.

</details>


### [15] [Improving Software Team Communication Through Social Interventions in Project Management Tools](https://arxiv.org/abs/2506.10994)
*April Clarke*

Main category: cs.SE

TL;DR: 利用社交网络分析技术开发项目管理工具功能，帮助学生改善软件工程团队项目中的沟通与协调行为。


<details>
  <summary>Details</summary>
Motivation: 提高软件工程团队成员间的沟通效率和贡献平衡，促进项目成功，尤其是在学生团队项目中。

Method: 首先评估社交网络分析技术对识别团队沟通改进领域的适用性，随后开发项目管理工具功能并评估其效果。

Result: 尚未明确（研究进行中）。

Conclusion: 社交网络分析可能成为改善学生团队沟通与协调的有效工具，需进一步开发并验证其项目管理功能。

Abstract: Productive software engineering teams require effective communication and
balanced contributions between team members. However, teams are often
ineffective at these skills, which is detrimental to project success.
Project-based university courses are an opportunity for students to practise
these skills, but we have yet to establish how we can guide students towards
improving their communication and coordination. We aim to develop project
management tool features, informed by social network analysis, that nudge
students in software engineering group projects towards beneficial behaviours.
To do this, we will first evaluate the suitability of social network analysis
techniques for identifying areas of improvement in teams' communication. Then,
we will develop features in a project management tool that aid students in
identifying and addressing these areas of improvement, and evaluate them in the
context of a software engineering group project.

</details>


### [16] [Evaluating Small-Scale Code Models for Code Clone Detection](https://arxiv.org/abs/2506.10995)
*Jorge Martinez-Gil*

Main category: cs.SE

TL;DR: 论文研究了小型代码模型在检测代码克隆中的性能，结果表明大部分模型表现良好，但某些相似但功能不同的代码仍难以检测。


<details>
  <summary>Details</summary>
Motivation: 代码克隆检测对软件维护和重构很重要，但现有方法在结构相似但功能不等同的情况下仍有挑战。

Method: 系统评估了六种小型代码模型在五个数据集上的性能，包括准确率、精确率、召回率和F1分数等指标。

Result: 多数模型在标准指标上表现良好，但相似但功能不同的代码克隆仍难以检测。

Conclusion: 小型代码模型在代码克隆检测中表现优异，但需进一步改进以解决特定挑战。

Abstract: Detecting code clones is relevant to software maintenance and code
refactoring. This challenge still presents unresolved cases, mainly when
structural similarity does not reflect functional equivalence, though recent
code models show promise. Therefore, this research aims to systematically
measure the performance of several newly introduced small code models in
classifying code pairs as clones or non-clones. The evaluation is based on five
datasets: BigCloneBench, CodeJam, Karnalim, POJ104, and PoolC, as well as six
code models: CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and
Polycoder. Most models performed well across standard metrics, including
accuracy, precision, recall, and F1-score. However, a marginal fraction of
clones remains challenging to detect, especially when the code looks similar
but performs different operations. The source code that illustrates our
approach is available at:
https://github.com/jorge-martinez-gil/small-code-models

</details>


### [17] [Evaluating LLMs for Visualization Tasks](https://arxiv.org/abs/2506.10996)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在生成可视化代码和理解常见图表方面的能力，揭示了其优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在信息可视化中的应用潜力，探索其能否通过简单提示生成可视化代码并理解图表。

Method: 通过实验评估多种流行LLMs，测试其生成可视化代码和回答图表相关问题的能力。

Result: LLMs能够生成部分可视化代码并回答相关问题，但仍存在局限性。

Conclusion: 研究结果为改进LLMs和信息可视化系统提供了有价值的见解。

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language Models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
simple questions. Our study shows that LLMs could generate code for some
visualizations as well as answer questions about them. However, LLMs also have
several limitations. We believe that our insights can be used to improve both
LLMs and Information Visualization systems.

</details>


### [18] [A Theory-driven Interpretation and Elaboration of Verification and Validation](https://arxiv.org/abs/2506.10997)
*Hanumanthrao Kannan,Alejandro Salado*

Main category: cs.SE

TL;DR: 论文提出了一种基于知识构建的验证与确认（V&V）形式化理论，利用动态认知模态逻辑定义V&V，并阐明其在系统知识中的作用。


<details>
  <summary>Details</summary>
Motivation: 传统V&V实践中存在模糊性，需要通过形式化方法提高系统工程方法的精确性和一致性。

Method: 使用动态认知模态逻辑，形式化V&V的定义和角色，分析认知状态、证据和推理过程的互动。

Result: 建立了V&V的形式化基础，推导出澄清其概念基础的定理，为系统工程提供结构化框架。

Conclusion: 该理论不仅解决了实践中的模糊性，还促进了V&V作为工程知识生成关键组成部分的深入理解。

Abstract: This paper presents a formal theory of verification and validation (V&V)
within systems engineering, grounded in the axiom that V&V are fundamentally
knowledge-building activities. Using dynamic epistemic modal logic, we develop
precise definitions of verification and validation, articulating their roles in
confirming and contextualizing knowledge about systems. The theory formalizes
the interplay between epistemic states, evidence, and reasoning processes,
allowing for the derivation of theorems that clarify the conceptual
underpinnings of V&V. By providing a formal foundation, this work addresses
ambiguities in traditional V&V practices, offering a structured framework to
enhance precision and consistency in systems engineering methodologies. The
insights gained have implications for both academic research and practical
applications, fostering a deeper understanding of V&V as critical components of
engineering knowledge generation.

</details>


### [19] [Towards Automated Formal Verification of Backend Systems with LLMs](https://arxiv.org/abs/2506.10998)
*Kangping Xu,Yifan Luo,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.SE

TL;DR: 该论文提出了一种利用函数式编程和类型系统将Scala后端代码转换为形式化Lean表示的框架，自动生成并验证API行为定理，大幅提升测试效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试方法在测试局部性、通用可靠性和业务逻辑识别方面存在局限，无法媲美人工测试能力。

Method: 通过功能编程和类型系统将代码转为形式化表示，自动生成定理并使用LLM验证，无法验证时需人工干预。

Result: 实验显示，该方法能验证50%以上的测试需求，每API平均成本仅2.19美元，显著优于人工测试。

Conclusion: 该框架为可扩展的AI驱动软件测试提供了新方向，有望显著提升工程效率。

Abstract: Software testing plays a critical role in ensuring that systems behave as
intended. However, existing automated testing approaches struggle to match the
capabilities of human engineers due to key limitations such as test locality,
lack of general reliability, and business logic blindness. In this work, we
propose a novel framework that leverages functional programming and type
systems to translate Scala backend code into formal Lean representations. Our
pipeline automatically generates theorems that specify the intended behavior of
APIs and database operations, and uses LLM-based provers to verify them. When a
theorem is proved, the corresponding logic is guaranteed to be correct and no
further testing is needed. If the negation of a theorem is proved instead, it
confirms a bug. In cases where neither can be proved, human intervention is
required. We evaluate our method on realistic backend systems and find that it
can formally verify over 50% of the test requirements, which suggests that half
of a testing engineer's workload can be automated. Additionally, with an
average cost of only $2.19 per API, LLM-based verification is significantly
more cost-effective than manual testing and can be scaled easily through
parallel execution. Our results indicate a promising direction for scalable,
AI-powered software testing, with the potential to greatly improve engineering
productivity as models continue to advance.

</details>


### [20] [Automated Validation of COBOL to Java Transformation](https://arxiv.org/abs/2506.10999)
*Atul Kumar,Diptikalyan Saha,Toshikai Yasue,Kohichi Ono,Saravanan Krishnan,Sandeep Hans,Fumiko Satoh,Gerald Mitchell,Sachin Kumar*

Main category: cs.SE

TL;DR: 论文提出了一种框架和工具，用于验证COBOL代码与翻译后的Java代码的等效性，并通过符号执行生成的单元测试来修复问题并提供模型反馈。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的自动代码转换结果虽令人鼓舞，但翻译后的代码无法保证准确性，因此需要验证和修复工具。

Method: 使用符号执行生成COBOL程序的单元测试，并生成等效的JUnit测试用例，通过运行测试来验证语义等效性。

Result: 工具可自动生成测试用例，验证并修复翻译后的代码，同时为AI模型提供反馈以改进翻译效果。

Conclusion: 该方法有效解决了LLM代码转换中的可信问题，并提供了可扩展的验证框架。

Abstract: Recent advances in Large Language Model (LLM) based Generative AI techniques
have made it feasible to translate enterpriselevel code from legacy languages
such as COBOL to modern languages such as Java or Python. While the results of
LLM-based automatic transformation are encouraging, the resulting code cannot
be trusted to correctly translate the original code. We propose a framework and
a tool to help validate the equivalence of COBOL and translated Java. The
results can also help repair the code if there are some issues and provide
feedback to the AI model to improve. We have developed a
symbolic-execution-based test generation to automatically generate unit tests
for the source COBOL programs which also mocks the external resource calls. We
generate equivalent JUnit test cases with equivalent mocking as COBOL and run
them to check semantic equivalence between original and translated programs.

</details>


### [21] [Ever-Improving Test Suite by Leveraging Large Language Models](https://arxiv.org/abs/2506.11000)
*Ketai Qiu*

Main category: cs.SE

TL;DR: E-Test通过利用大型语言模型识别并补充测试套件中未覆盖的生产行为，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 维持长期软件系统的质量需要测试套件能够反映实际使用情况。

Method: E-Test逐步补充测试套件，利用大型语言模型识别已测试、未测试和易出错的行为。

Result: 实验表明E-Test在识别测试不足行为和优化测试套件方面优于其他先进方法。

Conclusion: E-Test能有效提升测试套件的覆盖率，有助于软件质量的持续改进。

Abstract: Augmenting test suites with test cases that reflect the actual usage of the
software system is extremely important to sustain the quality of long lasting
software systems. In this paper, we propose E-Test, an approach that
incrementally augments a test suite with test cases that exercise behaviors
that emerge in production and that are not been tested yet. E-Test leverages
Large Language Models to identify already-tested, not-yet-tested, and
error-prone unit execution scenarios, and augment the test suite accordingly.
Our experimental evaluation shows that E-Test outperforms the main
state-of-the-art approaches to identify inadequately tested behaviors and
optimize test suites.

</details>


### [22] [Rethinking Technological Readiness in the Era of AI Uncertainty](https://arxiv.org/abs/2506.11001)
*S. Tucker Browne,Mark M. Bailey*

Main category: cs.SE

TL;DR: 提出了一个针对军事AI系统的新评估框架，以弥补现有技术准备度评估的不足。


<details>
  <summary>Details</summary>
Motivation: 现有技术准备度评估未能涵盖AI特有因素，可能导致军事部署中的风险。

Method: 提出AI Readiness Framework，类似传统TRL但专为AI扩展，评估成熟度和可信度。

Result: 框架能有效评估AI系统的可靠性、安全性和适用性，为决策者提供清晰依据。

Conclusion: 该框架有助于军事AI系统的安全部署，推动国防技术管理和风险评估的进步。

Abstract: Artificial intelligence (AI) is poised to revolutionize military combat
systems, but ensuring these AI-enabled capabilities are truly mission-ready
presents new challenges. We argue that current technology readiness assessments
fail to capture critical AI-specific factors, leading to potential risks in
deployment. We propose a new AI Readiness Framework to evaluate the maturity
and trustworthiness of AI components in military systems. The central thesis is
that a tailored framework - analogous to traditional Technology Readiness
Levels (TRL) but expanded for AI - can better gauge an AI system's reliability,
safety, and suitability for combat use. Using current data evaluation tools and
testing practices, we demonstrate the framework's feasibility for near-term
implementation. This structured approach provides military decision-makers with
clearer insight into whether an AI-enabled system has met the necessary
standards of performance, transparency, and human integration to be deployed
with confidence, thus advancing the field of defense technology management and
risk assessment.

</details>


### [23] [Notes On Writing Effective Empirical Software Engineering Papers: An Opinionated Primer](https://arxiv.org/abs/2506.11002)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: 本文旨在为实证软件工程领域的论文写作提供指导，帮助初学者和有一定经验的研究者。


<details>
  <summary>Details</summary>
Motivation: 由于实证软件工程研究中良好的科学写作实践较少被讨论，作者希望通过本文为感到困惑或压力大的研究者提供帮助。

Method: 作者基于主观和个人愿景，总结了写作实证软件工程论文的建议。

Result: 虽然这些建议不具备完全客观性或普遍性，但对作者团队和学生已经有效。

Conclusion: 本文提供的指导有望帮助其他研究者在实证软件工程领域的论文写作中受益。

Abstract: While mastered by some, good scientific writing practices within Empirical
Software Engineering (ESE) research appear to be seldom discussed and
documented. Despite this, these practices are implicit or even explicit
evaluation criteria of typical software engineering conferences and journals.
In this pragmatic, educational-first document, we want to provide guidance to
those who may feel overwhelmed or confused by writing ESE papers, but also
those more experienced who still might find an opinionated collection of
writing advice useful. The primary audience we had in mind for this paper were
our own BSc, MSc, and PhD students, but also students of others. Our documented
advice therefore reflects a subjective and personal vision of writing ESE
papers. By no means do we claim to be fully objective, generalizable, or
representative of the whole discipline. With that being said, writing papers in
this way has worked pretty well for us so far. We hope that this guide can at
least partially do the same for others.

</details>


### [24] [EmbedAgent: Benchmarking Large Language Models in Embedded System Development](https://arxiv.org/abs/2506.11003)
*Ruiyang Xu,Jialun Cao,Mingyuan Wu,Wenliang Zhong,Yaojie Lu,Ben He,Xianpei Han,Shing-Chi Cheung,Le Sun*

Main category: cs.SE

TL;DR: 论文介绍了EmbedAgent和Embedbench，用于评估大语言模型在嵌入式系统开发中的能力，发现模型表现不一，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试较少评估LLMs在嵌入式系统开发中的能力，需填补这一空白。

Method: 提出EmbedAgent模拟嵌入式开发角色，Embedbench包含126个测试案例，评估10种主流LLMs。

Result: LLMs在简单任务中表现不佳（如DeepSeek-R1仅55.6%通过率），但在部分任务（如MicroPython）中表现较好。提出的改进策略显著提升了性能。

Conclusion: LLMs在嵌入式系统开发中潜力待挖掘，需针对性优化策略。

Abstract: Large Language Models (LLMs) have shown promise in various tasks, yet few
benchmarks assess their capabilities in embedded system development.In this
paper, we introduce EmbedAgent, a paradigm designed to simulate real-world
roles in embedded system development, such as Embedded System Programmer,
Architect, and Integrator. This paradigm enables LLMs to be tested in tasks
that bridge the gap between digital and physical systems, allowing for a more
comprehensive assessment of their capabilities. To evaluate LLMs on these
tasks, we propose Embedbench, the first comprehensive benchmark for embedded
system programming, circuit design, and cross-platform migration.Embedbench
consists of 126 cases, covering 9 electronic components across 3 hardware
platforms. Through extensive experiments on 10 mainstream LLMs, we uncover
several key findings. Surprisingly, despite the simplicity of the cases,
DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic
information, and 50.0% when tasked with generating the schematics itself. In
the cross-platform migration tasks, LLMs show relatively strong performance
with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8%
pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4%
pass@1.Interestingly, we observe that general-purpose chat LLMs like
DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this
domain, while reasoning LLMs tend to overthink and overlook efficient knowledge
during pretraining. Based on these insights, we propose two strategies:
retrieval augmented generation and compiler feedback-to enhance LLM
performance. These strategies result in significant improvements, with
Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without.
Additionally, the accuracy of the Arduino to ESP32 migration task improves from
21.4% to 27.8%.

</details>


### [25] [Automated Extraction and Analysis of Developer's Rationale in Open Source Software](https://arxiv.org/abs/2506.11005)
*Mouna Dhaouadi,Bentley Oakes,Michalis Famelis*

Main category: cs.SE

TL;DR: 该论文提出了一种自动化方法，用于开源项目的理性分析，帮助开发者理解历史决策并避免冲突。


<details>
  <summary>Details</summary>
Motivation: 开源贡献者需要理解项目历史以避免决策冲突，但目前缺乏自动化工具支持。

Method: 基于Kantara架构，利用预训练模型和大语言模型，检测设计冲突和推理问题。

Result: 在Linux内核等项目上验证了方法的可行性，能有效提取理性和检测冲突。

Conclusion: 自动化工具可帮助开发者主动解决隐藏问题，确保新变更与历史一致。

Abstract: Contributors to open source software must deeply understand a project's
history to make coherent decisions which do not conflict with past reasoning.
However, inspecting all related changes to a proposed contribution requires
intensive manual effort, and previous research has not yet produced an
automated mechanism to expose and analyze these conflicts. In this article, we
propose such an automated approach for rationale analyses, based on an
instantiation of Kantara, an existing high-level rationale extraction and
management architecture. Our implementation leverages pre-trained models and
Large Language Models, and includes structure-based mechanisms to detect
reasoning conflicts and problems which could cause design erosion in a project
over time. We show the feasibility of our extraction and analysis approach
using the OOM-Killer module of the Linux Kernel project, and investigate the
approach's generalization to five other highly active open source projects. The
results confirm that our automated approach can support rationale analyses with
reasonable performance, by finding interesting relationships and to detect
potential conflicts and reasoning problems. We also show the effectiveness of
the automated extraction of decision and rationale sentences and the prospects
for generalizing this to other open source projects. This automated approach
could therefore be used by open source software developers to proactively
address hidden issues and to ensure that new changes do not conflict with past
decisions.

</details>


### [26] [Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs](https://arxiv.org/abs/2506.11006)
*Sai Krishna,Balvinder Singh,Sujoy Roychowdhury,Giriprasad Sridhara,Sourav Mazumdar,Magnus Sandelin,Dimitris Rentas,Maciej Nalepa,Karol Sawicki,Jakub Gajda*

Main category: cs.SE

TL;DR: 论文描述了在爱立信中使用大型语言模型（LLM）生成测试代码的方法，通过改进提示工程和模型微调，显著提高了生成的测试代码与原始开发者代码的匹配度。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言描述的测试步骤直接通过简单提示生成Java代码时，LLM假设的函数和签名与代码库不匹配的问题。

Method: 结合检索增强生成（RAG）和提示工程，通过静态程序分析扩展提示信息，并基于自定义提示模板对LLM进行微调。

Result: 微调后的8x7b混合专家（MoE）模型在F1分数上平均提升了8%，与更大的8x22b MoE模型表现相当。

Conclusion: 通过RAG和模型微调，可以有效提升LLM生成的测试代码的质量和匹配度。

Abstract: We describe test code generation using Large Language Models (LLMs) in
Ericsson. Our input is a test step in natural language (English) and our output
is code (Java) which accomplishes the test step. We describe how straight
forward prompting does not suffice and results in LLM assuming functions and
signatures which are not present in the code repository. We then show how we
alleviate the problem by a combination of Retrieval Augmented Generation (RAG)
along with prompt engineering that expanded the simple prompt with additional
contextual information using static program analysis. We then describe further
improvements that we obtained by fine-tuning the underlying LLM. The fine
tuning is done based on a custom designed prompt template which has
pre-dependent classes, their public methods as well two exemplar outputs
obtained from RAG. Our results establish that our fine tuned models help
improve the correspondence or conformity with the original developer written
test code as measured by the traditional metrics of F1-score based on the
methods used in the generated code. Fine tuning of a 8x7b Mixture of Experts
(MoE) model leads to an average improvement of 8\% over the base model and is
comparable to the scores on a much larger 8x22b MoE model.

</details>


### [27] [Impact of Comments on LLM Comprehension of Legacy Code](https://arxiv.org/abs/2506.11007)
*Rock Sabetto,Emily Escamilla,Devesh Agarwal,Sujay Kandwal,Justin F. Brunelle,Scott Rosen,Nitin Naik,Samruddhi Thaker,Eric O. Scott,Jacob Zimmer,Amit Madan,Arun Sridharan,Doug Wendt,Michael Doyle,Christopher Glasz,Jasper Phillips,William Macke,Colin Diggs,Michael Bartholf,Zachary Robin,Paul Ursino*

Main category: cs.SE

TL;DR: 总结了大语言模型（LLMs）在软件工程和维护任务中的应用，探讨了其在理解遗留语言代码方面的研究空白，并提出了一种通过多项选择题回答（MCQA）方法评估LLM对遗留代码理解的新途径。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在理解遗留语言代码方面的能力，尤其是在缺乏或文档不准确的情况下，填补现有研究的空白。

Method: 利用多项选择题回答（MCQA）作为评估工具，量化LLM对遗留代码的理解能力，并分析文档注释的普遍性和准确性对理解的影响。

Result: 初步研究了文档对LLM理解遗留代码的影响，并提出了未来工作的战略目标。

Conclusion: MCQA方法为评估LLM对遗留代码的理解提供了新视角，未来研究需进一步验证和优化该方法。

Abstract: Large language models (LLMs) have been increasingly integrated into software
engineering and maintenance tasks due to their high performance with software
engineering tasks and robust understanding of modern programming languages.
However, the ability of LLMs to comprehend code written with legacy languages
remains a research gap challenged by real-world legacy systems lacking or
containing inaccurate documentation that may impact LLM comprehension. To
assess LLM comprehension of legacy languages, there is a need for objective LLM
evaluation. In order to objectively measure LLM comprehension of legacy
languages, we need an efficient, quantitative evaluation method. We leverage
multiple-choice question answering (MCQA), an emerging LLM evaluation
methodology, to evaluate LLM comprehension of legacy code and the impact of
comment prevalence and inaccurate comments. In this work, we present
preliminary findings on the impact of documentation on LLM comprehension of
legacy code and outline strategic objectives for future work.

</details>


### [28] [Encoding Software For Perpetuity: A Compact Representation Of Apollo 11 Guidance Code](https://arxiv.org/abs/2506.11008)
*David Noever*

Main category: cs.SE

TL;DR: 本文介绍了一种将阿波罗11号登月舱引导计算机代码压缩为QR码的新方法，使其可通过现代移动设备访问且无需专门硬件或互联网连接。


<details>
  <summary>Details</summary>
Motivation: 目标是为历史性软件遗产提供一种便捷且易于保存的数字化方法，使其在现代移动设备上可访问。

Method: 通过令牌化、选择性内容保留和最小化HTML/JavaScript技术，将原始汇编语言代码压缩为3KB的QR码。

Result: 成功将关键组件压缩为可分享、保存和扫描的图像，同时评估了不同压缩策略的权衡。

Conclusion: 该方法为计算遗产保护提供了一种补充传统存档的技术，展示了如何通过现代移动技术即时访问重要软件。

Abstract: This brief note presents a novel method for encoding historic Apollo 11 Lunar
Module guidance computer code into a single, compact Quick Response Code (QR
code) format, creating an accessible digital artifact for transmission and
archival purposes. By applying tokenization, selective content preservation,
and minimal HTML/JavaScript techniques, we successfully compressed key
components of the original Assembly Language Code (AGC) into a shareable,
preservable, and scannable 3 kilobyte (KB) image. We evaluate multiple
compression strategies and their tradeoffs in terms of size, readability, and
historical significance. This method addresses the challenge of making
historically significant software artifacts available through modern mobile
devices without requiring specialized hardware or internet connectivity. While
numerous digital preservation methods exist for historic software, this
approach balances accessibility with historical significance, offering a
complementary method to traditional archival techniques. This work contributes
to the broader field of computing heritage preservation by demonstrating how
landmark software can be made accessible instantly through contemporary mobile
technologies.

</details>


### [29] [Human-In-The-Loop Software Development Agents: Challenges and Future Directions](https://arxiv.org/abs/2506.11009)
*Jirat Pasuksmit,Wannita Takerngsaksiri,Patanamon Thongtanunam,Chakkrit Tantithamthavorn,Ruixiong Zhang,Shiyan Wang,Fan Jiang,Jing Li,Evan Cook,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: 论文探讨了多智能体LLM驱动系统在软件开发中的应用，重点分析了计算成本高和评估不一致的挑战，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用多智能体LLM系统提升软件开发效率，特别是在Jira工作项中的实际应用。

Method: 通过功能正确性测试和基于GPT的相似性评分评估生成代码质量，并分析了现有方法的局限性。

Result: 发现单元测试的高计算成本和LLM评估的变异性是主要挑战。

Conclusion: 未来研究需改进评估框架，以优化人机协同的软件开发工具。

Abstract: Multi-agent LLM-driven systems for software development are rapidly gaining
traction, offering new opportunities to enhance productivity. At Atlassian, we
deployed Human-in-the-Loop Software Development Agents to resolve Jira work
items and evaluated the generated code quality using functional correctness
testing and GPT-based similarity scoring. This paper highlights two major
challenges: the high computational costs of unit testing and the variability in
LLM-based evaluations. We also propose future research directions to improve
evaluation frameworks for Human-In-The-Loop software development tools.

</details>


### [30] [Enhancing Inventory Management with Progressive Web Applications (PWAs): A Scalable Solution for Small and Large Enterprises](https://arxiv.org/abs/2506.11011)
*Abhi Desai*

Main category: cs.SE

TL;DR: 论文探讨了渐进式Web应用（PWA）在库存管理中的开发与实施，旨在优化操作流程并降低成本。


<details>
  <summary>Details</summary>
Motivation: 通过整合条码扫描、地理位置识别等功能，提升库存管理效率，研究PWA技术的潜力与挑战。

Method: 开发一款PWA应用，支持离线功能、跨平台适配及响应式用户体验，并分析其性能。

Result: PWA在提供跨平台和离线功能上表现优越，但在性能上可能逊于原生应用，为未来开发提供参考。

Conclusion: PWA为库存管理提供了一种可扩展且经济的替代方案，未来可进一步优化性能。

Abstract: Efficient inventory management is crucial for both small and large
enterprises to optimize operational workflows and reduce overhead costs. This
paper explores the development and implementation of a Progressive Web
Application (PWA) designed to enhance the inventory management experience. The
application integrates key functionalities such as barcode and QR code
scanning, geolocation-based warehouse identification, and cross-device
accessibility. By leveraging PWA technology, the solution ensures offline
capabilities, responsive user experience, and seamless adaptability across
various platforms. The study discusses the challenges and benefits of
implementing PWA in inventory management systems, including its limitations in
performance compared to native applications. Insights from the development
process provide a roadmap for future developers looking to integrate PWA
technology into enterprise applications. This research contributes to the
growing domain of web-based inventory solutions, offering a scalable and
cost-effective alternative to traditional inventory management software.

</details>


### [31] [Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.11013)
*Filipe Fernandes,Cláudia Werner*

Main category: cs.SE

TL;DR: 本文分析了量子软件工程（QSE）的研究现状，提出巴西研究议程以填补该领域的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管QSE领域发展迅速，但知识碎片化且缺乏标准化方法，巴西等国的参与度较低。

Method: 通过系统性映射研究，分析文献的分类、研究类型及其与SWEBOK知识领域的联系。

Result: 多数研究集中在软件工程模型、架构和测试，但实证验证较少；巴西的研究贡献稀缺。

Conclusion: QSE是一个有前景但仍在发展的领域，需标准化实践、扩大实证研究并提升发展中国家参与度。

Abstract: Context: Quantum Software Engineering (QSE) has emerged as a key field to
support the development of reliable, maintainable, and scalable quantum
applications, bridging advances in quantum computing with established practices
in software engineering. Problem: Despite its growth, the field still suffers
from fragmented knowledge, with a lack of standardized methodologies, tools,
and guidelines tailored to the unique features of the quantum paradigm.
Additionally, countries like Brazil have had limited participation in the
development of this emerging domain. Objective: This study aims to map the
state of the art in QSE by identifying current research trends, recurring
contributions, and existing gaps that can guide future investigations and
strategic initiatives. Methodology: A systematic mapping study was conducted
analyzing selected publications based on inclusion and exclusion criteria.
Articles were categorized by study type, research type, and alignment with the
SWEBOK knowledge areas. Results: Most of the reviewed studies are primary
research articles written in English, with a strong focus on Software
Engineering Models and Methods, Software Architecture, and Software Testing.
Conceptual proposals and technical solutions predominate, while empirical
validations remain limited. Conclusions: Findings confirm that QSE is a
promising but still maturing field. The standardization of practices, expansion
of empirical studies, and inclusion of researchers from developing countries
are crucial for advancing the discipline. Additionally, Brazilian contributions
are still scarce, highlighting the urgent need to establish a national research
agenda. As a main contribution, this study proposes a Brazilian Research Agenda
in QSE, outlining priority areas and opportunities to foster a local scientific
community and accelerate progress in this emerging field.

</details>


### [32] [MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants](https://arxiv.org/abs/2506.11014)
*Benedetta Donato,Leonardo Mariani,Daniela Micucci,Oliviero Riganelli,Marco Somaschini*

Main category: cs.SE

TL;DR: MultiMind是一个VS Code插件，旨在简化AI辅助开发任务，解决IDE中嵌入AI助手时的挑战。


<details>
  <summary>Details</summary>
Motivation: AI助手在软件开发中越来越重要，但在IDE中嵌入AI助手仍面临诸多挑战，如适时调用、协调交互和处理输出等。

Method: 提出了MultiMind插件，提供一个模块化和可扩展的框架，方便开发者低成本实现和实验新的AI交互功能。

Result: MultiMind在代码注释自动生成和AI聊天功能两个用例中进行了测试，展示了其有效性。

Conclusion: MultiMind通过模块化设计，为开发者提供了一个高效且灵活的AI辅助开发工具。

Abstract: The integration of AI assistants into software development workflows is
rapidly evolving, shifting from automation-assisted tasks to collaborative
interactions between developers and AI. Large Language Models (LLMs) have
demonstrated their effectiveness in several development activities, including
code completion, test case generation, and documentation production. However,
embedding AI-assisted tasks within Integrated Development Environments (IDEs)
presents significant challenges. It requires designing mechanisms to invoke AI
assistants at the appropriate time, coordinate interactions with multiple
assistants, process the generated outputs, and present feedback in a way that
seamlessly integrates with the development workflow. To address these issues,
we introduce MultiMind, a Visual Studio Code plug-in that streamlines the
creation of AI-assisted development tasks. MultiMind provides a modular and
extensible framework, enabling developers to cost-effectively implement and
experiment with new AI-powered interactions without the need for complex IDE
customizations. MultiMind has been tested in two use cases: one for the
automatic generation of code comments and the other about the definition of
AI-powered chat.

</details>


### [33] [ZjsComponent: A Pragmatic Approach to Modular, Reusable UI Fragments for Web Development](https://arxiv.org/abs/2506.11016)
*Lelanthran Manickum*

Main category: cs.SE

TL;DR: ZjsComponent是一个轻量、框架无关的Web组件，用于创建模块化和可重用的UI元素，无需构建步骤或依赖项。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种简单、无依赖的方法来创建和复用Web组件，减少开发者的负担。

Method: 通过动态加载和隔离HTML+JS片段，ZjsComponent实现了无构建步骤、无预编译的组件开发。

Result: 提供了一种轻量、独立的组件解决方案，支持代码隔离和生命周期钩子。

Conclusion: ZjsComponent为开发者提供了一种高效、简单的Web组件开发方法，适用于无需复杂工具的轻量级场景。

Abstract: In this paper, I present ZjsComponent, a lightweight and framework-agnostic
web component designed for creating modular, reusable UI elements with minimal
developer overhead. ZjsComponent is an example implementation of an approach to
creating components and object instances that can be used purely from HTML.
Unlike traditional approaches to components, the approach implemented by
ZjsComponent does not require build-steps, transpiling, pre-compilation, any
specific ecosystem or any other dependency. All that is required is that the
browser can load and execute Javascript as needed by Web Components.
ZjsComponent allows dynamic loading and isolation of HTML+JS fragments,
offering developers a simple way to build reusable interfaces with ease. This
approach is dependency-free, provides significant DOM and code isolation, and
supports simple lifecycle hooks as well as traditional methods expected of an
instance of a class.

</details>


### [34] [Formation of requirements traceability in the process of information systems design](https://arxiv.org/abs/2506.11018)
*Grigory Tsiperman*

Main category: cs.SE

TL;DR: 论文探讨了需求跟踪在信息系统设计中的重要性，并提出了基于自适应聚类方法（ACM）的解决方案，以实现需求跟踪与设计过程的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 需求跟踪是信息系统设计的关键质量特性，但如何将其无缝集成到设计过程中是一个重大挑战。

Method: 采用作者开发的自适应聚类方法（ACM），通过无缝系统架构实现不同抽象级别项目工件的显式互联。

Result: ACM方法能够显式连接项目工件，从而简化系统验证与验证过程，并减少对开发人员的依赖。

Conclusion: ACM方法为需求跟踪与设计过程的集成提供了有效的解决方案，提升了系统的可维护性和可验证性。

Abstract: The traceability of requirements in the information system design process is
considered an essential property of the project, one of its quality
characteristics. The point here is that traceability provides the methods of
validation and verification of software systems, and that the system model
based on requirements traceability reduces the system's dependence on
developers and, in general, makes it as straightforward as possible. One of the
challenges of the traceability process, dubbed "The grand challenge of
traceability" among traceability researchers, is its integration into the
design process. In this paper, to achieve this goal, we propose the application
of the Adaptive Clustering Method (ACM) of Information Systems developed by the
author, which is based on the idea of a seamless system architecture that
provides explicit interconnection of project artifacts of different levels of
abstraction.

</details>


### [35] [Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)](https://arxiv.org/abs/2506.11019)
*Vincent Koc,Jacques Verre,Douglas Blank,Abigail Morgan*

Main category: cs.SE

TL;DR: 该论文介绍了基于模型上下文协议（MCP）的遥测感知集成开发环境（IDEs），支持实时优化和集成多种框架，展示了开源MCP服务器Opik，并为未来的提示优化和LLMOps研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: AI开发环境正朝着以可观察性为首的平台发展，需要整合实时遥测、提示追踪和评估反馈以优化开发流程。

Method: 通过模型上下文协议（MCP）连接IDEs与提示指标、追踪日志和版本控制，提出本地提示迭代、基于CI的优化和自适应代理的设计模式。

Result: 展示了开源MCP服务器Opik，并验证了架构与多种框架的集成能力，为LLMOps生态系统提供支持。

Conclusion: 该工作为未来提示优化、IDE代理工具和遥测丰富的AI开发流程的实证研究奠定了基础。

Abstract: AI development environments are evolving into observability first platforms
that integrate real time telemetry, prompt traces, and evaluation feedback into
the developer workflow. This paper introduces telemetry aware integrated
development environments (IDEs) enabled by the Model Context Protocol (MCP), a
system that connects IDEs with prompt metrics, trace logs, and versioned
control for real time refinement. We present design patterns for local prompt
iteration, CI based optimization, and autonomous agents that adapt behavior
using telemetry. Rather than focusing on a single algorithm, we describe an
architecture that supports integration with frameworks like DSPy, PromptWizard,
and Prompts as Programs. We demonstrate this through Opik, an open source MCP
server for LLM telemetry, and position our approach within the emerging LLMOps
ecosystem. This work lays a foundation for future research on prompt
optimization, IDE agent tooling, and empirical benchmarking in telemetry rich
AI development workflows.

</details>


### [36] [Extracting Knowledge Graphs from User Stories using LangChain](https://arxiv.org/abs/2506.11020)
*Thayná Camargo da Silva*

Main category: cs.SE

TL;DR: 提出一种利用大语言模型从用户故事自动生成知识图谱的新方法，通过LangChain框架开发模块，实现全自动提取和评估。


<details>
  <summary>Details</summary>
Motivation: 提升用户需求和领域概念的可视化与理解，以更好地对齐软件功能和用户期望。

Method: 利用LangChain框架和大语言模型开发User Story Graph Transformer模块，自动化提取知识图谱节点和关系。

Result: 实现了全自动的知识图谱提取和评估，增强了软件开发的用户中心性。

Conclusion: 该方法有效改进了软件功能与用户期望的对齐，促进更高效的用户中心开发流程。

Abstract: This thesis introduces a novel methodology for the automated generation of
knowledge graphs from user stories by leveraging the advanced capabilities of
Large Language Models. Utilizing the LangChain framework as a basis, the User
Story Graph Transformer module was developed to extract nodes and relationships
from user stories using an LLM to construct accurate knowledge graphs.This
innovative technique was implemented in a script to fully automate the
knowledge graph extraction process. Additionally, the evaluation was automated
through a dedicated evaluation script, utilizing an annotated dataset for
assessment. By enhancing the visualization and understanding of user
requirements and domain concepts, this method fosters better alignment between
software functionalities and user expectations, ultimately contributing to more
effective and user-centric software development processes.

</details>


### [37] [Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering](https://arxiv.org/abs/2506.11021)
*Chaitanya Ravuri,Saman Amarasinghe*

Main category: cs.SE

TL;DR: 论文提出了一种名为功能性聚类的黑盒包装器，有效减少LLM生成代码中的幻觉错误，并提供可调置信度。方法通过采样、测试和聚类候选程序，显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 虽然现代代码生成LLM能解决许多编程问题，但仍有幻觉导致的细微错误，影响代码的自主部署安全性。

Method: 通过采样多个候选程序，运行自生成测试套件，聚类I/O行为相同的候选程序，利用最大聚类的经验质量作为置信度估计。

Result: 在LiveCodeBench上，该方法保持基线性能的同时，将错误率从65%降至2%，保守阈值下可降至0%。

Conclusion: 功能性聚类是一种实用方法，适用于闭源API和未来模型，为自主代码生成提供了可靠路径。

Abstract: Modern code-generation LLMs can already solve a large fraction of programming
problems, yet they still hallucinate subtle bugs that make their outputs unsafe
for autonomous deployment. We present functional clustering, a black-box
wrapper that eliminates nearly all hallucination-induced errors while providing
a tunable confidence score. The wrapper samples many candidate programs,
executes each on a self-generated test suite, and clusters candidates whose I/O
behavior is identical; the empirical mass of the largest cluster serves as an
exact confidence estimate. A single scalar threshold on this estimate lets
users trade coverage for reliability with exponential guarantees. On
LiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet
slashes the error rate of returned answers from ~65% to 2%, and drives it to 0%
at a conservative threshold while still answering 15.6% of prompts. Manual
audits show that the few residual mistakes stem from prompt misinterpretation,
not random generation noise, narrowing future work to specification clarity.
Because the method requires only sampling and sandbox execution, it applies
unchanged to closed-source APIs and future models, offering a practical path
toward dependable, autonomous code generation. Our code is available on Github
(https://github.com/20ChaituR/functional-clustering).

</details>


### [38] [Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox](https://arxiv.org/abs/2506.11022)
*Shivani Shukla,Himanshu Joshi,Romilla Syed*

Main category: cs.SE

TL;DR: 研究发现通过LLM迭代反馈生成的代码中安全漏洞显著增加，挑战了迭代改进能提升代码安全性的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型(LLM)在代码生成过程中迭代反馈如何导致安全漏洞的增加。

Method: 通过控制实验分析400个代码样本在40轮“改进”中使用四种不同提示策略的影响。

Result: 在五轮迭代后，关键漏洞增加了37.6%，且不同提示策略导致独特漏洞模式。

Conclusion: 强调人类专家在迭代过程中的必要性，并提出开发者的实践指南以减少风险。

Abstract: The rapid adoption of Large Language Models(LLMs) for code generation has
transformed software development, yet little attention has been given to how
security vulnerabilities evolve through iterative LLM feedback. This paper
analyzes security degradation in AI-generated code through a controlled
experiment with 400 code samples across 40 rounds of "improvements" using four
distinct prompting strategies. Our findings show a 37.6% increase in critical
vulnerabilities after just five iterations, with distinct vulnerability
patterns emerging across different prompting approaches. This evidence
challenges the assumption that iterative LLM refinement improves code security
and highlights the essential role of human expertise in the loop. We propose
practical guidelines for developers to mitigate these risks, emphasizing the
need for robust human validation between LLM iterations to prevent the
paradoxical introduction of new security issues during supposedly beneficial
code "improvements".

</details>


### [39] [Software Security Mapping Framework: Operationalization of Security Requirements](https://arxiv.org/abs/2506.11051)
*Sung Une Lee,Liming Dong,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 该论文提出了一个软件安全映射框架，将抽象的安全原则转化为具体的操作步骤，覆盖从监管标准到技术活动的多层次需求，并通过Log4j漏洞案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发环境的复杂性增加了供应链安全的风险，现有框架难以将抽象安全原则转化为具体实践，亟需一个结构化的解决方案。

Method: 通过结合KAOS目标建模方法，开发了一个分层映射框架，包含131项安全需求和400多个操作步骤，并提供基于Web的导航工具和机器可读的OSCAL模型。

Result: 框架成功将安全目标与操作步骤关联，并通过Log4j案例展示了其实际应用价值，支持自动化实施和合规流程简化。

Conclusion: 该框架为组织提供了一种清晰、可操作的方法来应对供应链安全挑战，并通过工具支持促进了广泛采用。

Abstract: The escalating complexity of modern software development environments has
heightened concerns around supply chain security. However, existing frameworks
often fall short in translating abstract security principles into concrete,
actionable practices. This paper introduces the Software Security Mapping
Framework, a structured solution designed to operationalize security
requirements across hierarchical levels -- from high-level regulatory standards
(e.g., ISM, Australia cybersecurity standard published by the Australian
Signals Directorate), through mid-level frameworks (e.g., NIST SSDF, the U.S.
Secure Software Development Framework), to fine-grained technical activities
(e.g., SLSA, a software supply chain security framework). Developed through
collaborative research with academic experts and industry practitioners, the
framework systematically maps 131 refined security requirements to over 400
actionable operational steps spanning the software development lifecycle. It is
grounded in four core security goals: Secure Software Environment, Secure
Software Development, Software Traceability, and Vulnerability Management. Our
approach leverages the KAOS goal modeling methodology to establish traceable
linkages between strategic goals and tactical operations, enhancing clarity,
accountability, and practical implementation. To facilitate adoption, we
provide a web-based navigation tool for interactive exploration of the
framework. A real-world case study based on the Log4j vulnerability illustrates
the framework's utility by generating a tailored checklist aligned with
industry best practices. Additionally, we offer a structured, machine-readable
OSCAL Catalog Model of the Software Security Mapping Framework, enabling
organizations to automate implementation, streamline compliance processes, and
respond effectively to evolving security risks.

</details>


### [40] [Refactoring Codebases through Library Design](https://arxiv.org/abs/2506.11058)
*Ziga Kovacic,Celine Lee,Justin Chiu,Wenting Zhao,Kevin Ellis*

Main category: cs.SE

TL;DR: 研究者提出了一种名为Librarian的方法，用于生成可重用的代码库，并创建了Minicode基准测试来评估代码代理的代码重构能力。Librarian在压缩率和正确性上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着代码代理在解决独立编程问题上的准确性提高，如何通过重构支持代码的增长和可重用性成为重要问题。

Method: 提出了Librarian方法（一种抽样-重新排序技术）和Minicode基准测试，用于评估代码代理的重构能力。

Result: Librarian在压缩率和正确性上均优于现有代码代理，压缩率提高了1.6-2倍。

Conclusion: 该方法及基准测试为代码重构提供了有效工具，支持代码的可重用性和维护性。

Abstract: Maintainable and general software allows developers to build robust
applications efficiently, yet achieving these qualities often requires
refactoring specialized solutions into reusable components. This challenge
becomes particularly relevant as code agents become increasingly accurate at
solving isolated programming problems. We investigate code agents' capacity to
refactor code in ways supporting growth and reusability. We present both a
method and a benchmark for refactoring: Librarian, a sample-and-rerank method
for generating reusable libraries, and Minicode, a benchmark where code agents
must minimize and refactor multiple independent solutions into a joint library.
Compared to state-of-the-art code agents, Librarian achieves strong results on
both compression and correctness on Minicode, obtaining compression rates
1.6-2x better than coding agents while also improving correctness. We
open-source our code and benchmark at https://code-refactor.github.io/.

</details>


### [41] [CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs](https://arxiv.org/abs/2506.11059)
*Hanxi Guo,Siyuan Cheng,Kaiyuan Zhang,Guangyu Shen,Xiangyu Zhang*

Main category: cs.SE

TL;DR: CodeMirage是一个全面的基准测试，用于评估AI生成代码的检测器，覆盖10种编程语言和10种先进LLM，揭示了当前检测器的优缺点。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，AI生成代码的风险（如抄袭、许可证违规和不安全程序）日益凸显，需要有效的检测器。现有基准测试覆盖范围有限且依赖较弱生成模型，无法满足需求。

Method: 提出CodeMirage基准，包括10种编程语言的原始和改写代码样本，整合了6家提供商的10种先进LLM输出。评估10种检测器，采用4种方法范式、4种配置和3种指标。

Result: 分析揭示了9个关键发现，展示当前检测器的优缺点，并提出未来研究的挑战。

Conclusion: CodeMirage为开发鲁棒且通用的AI生成代码检测器提供了严格实用的测试平台。

Abstract: Large language models (LLMs) have become integral to modern software
development, producing vast amounts of AI-generated source code. While these
models boost programming productivity, their misuse introduces critical risks,
including code plagiarism, license violations, and the propagation of insecure
programs. As a result, robust detection of AI-generated code is essential. To
support the development of such detectors, a comprehensive benchmark that
reflects real-world conditions is crucial. However, existing benchmarks fall
short -- most cover only a limited set of programming languages and rely on
less capable generative models. In this paper, we present CodeMirage, a
comprehensive benchmark that addresses these limitations through three major
advancements: (1) it spans ten widely used programming languages, (2) includes
both original and paraphrased code samples, and (3) incorporates outputs from
ten state-of-the-art production-level LLMs, including both reasoning and
non-reasoning models from six major providers. Using CodeMirage, we evaluate
ten representative detectors across four methodological paradigms under four
realistic evaluation configurations, reporting results using three
complementary metrics. Our analysis reveals nine key findings that uncover the
strengths and weaknesses of current detectors, and identify critical challenges
for future work. We believe CodeMirage offers a rigorous and practical testbed
to advance the development of robust and generalizable AI-generated code
detectors.

</details>


### [42] [Code Researcher: Deep Research Agent for Large Systems Code and Commit History](https://arxiv.org/abs/2506.11060)
*Ramneet Singh,Sathvik Joel,Abhav Mehrotra,Nalin Wadhwa,Ramakrishna B Bairi,Aditya Kanade,Nagarajan Natarajan*

Main category: cs.SE

TL;DR: 论文研究了基于LLM的编码代理在系统代码中的表现，并提出了一种名为Code Researcher的深度研究代理，用于修复系统代码中的崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理在编码基准测试中表现良好，但其在系统代码中的应用仍未充分探索。系统代码的规模和复杂性使其修改成为巨大挑战。

Method: 设计了一个名为Code Researcher的深度研究代理，通过多步骤推理收集代码的语义、模式和提交历史等上下文，并利用结构化记忆合成补丁。

Result: 在Linux内核崩溃基准测试中，Code Researcher的修复率（58%）显著优于基线（37.5%），且能深度探索代码库。

Conclusion: 研究表明，全局上下文收集和多面推理对大型代码库至关重要，Code Researcher展示了在该领域的优越性和通用性。

Abstract: Large Language Model (LLM)-based coding agents have shown promising results
on coding benchmarks, but their effectiveness on systems code remains
underexplored. Due to the size and complexities of systems code, making changes
to a systems codebase is a daunting task, even for humans. It requires
researching about many pieces of context, derived from the large codebase and
its massive commit history, before making changes. Inspired by the recent
progress on deep research agents, we design the first deep research agent for
code, called Code Researcher, and apply it to the problem of generating patches
for mitigating crashes reported in systems code. Code Researcher performs
multi-step reasoning about semantics, patterns, and commit history of code to
gather sufficient context. The context is stored in a structured memory which
is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a
benchmark of Linux kernel crashes, and show that it significantly outperforms
strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5%
by SWE-agent. On an average, Code Researcher explores 10 files in each
trajectory whereas SWE-agent explores only 1.33 files, highlighting Code
Researcher's ability to deeply explore the codebase. Through another experiment
on an open-source multimedia software, we show the generalizability of Code
Researcher. Our experiments highlight the importance of global context
gathering and multi-faceted reasoning for large codebases.

</details>


### [43] [CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval](https://arxiv.org/abs/2506.11066)
*Jiahui Geng,Fengyu Cai,Shaobo Cui,Qing Li,Liangwei Chen,Chenyang Lyu,Haonan Li,Derui Zhu,Walter Pretschner,Heinz Koeppl,Fakhri Karray*

Main category: cs.SE

TL;DR: 论文提出了CoQuIR，一个多语言、质量感知的代码检索基准，用于评估代码的正确性、效率、安全性和可维护性。


<details>
  <summary>Details</summary>
Motivation: 现有的代码检索基准主要关注功能相关性，忽略了代码质量的关键维度。

Method: 引入CoQuIR基准，包含42,725个查询和134,907个代码片段的质量标注，并使用两种质量中心评估指标。

Result: 测试了23种检索模型，发现即使表现最好的模型也难以区分有缺陷或不安全的代码。通过合成数据集训练，提升了质量感知能力而不牺牲语义相关性。

Conclusion: 研究强调了将质量信号整合到代码检索系统中的重要性，为更可信和鲁棒的软件开发工具奠定了基础。

Abstract: Code retrieval is essential in modern software development, as it boosts code
reuse and accelerates debugging. However, current benchmarks primarily
emphasize functional relevance while neglecting critical dimensions of software
quality. Motivated by this gap, we introduce CoQuIR, the first large-scale,
multilingual benchmark specifically designed to evaluate quality-aware code
retrieval across four key dimensions: correctness, efficiency, security, and
maintainability. CoQuIR provides fine-grained quality annotations for 42,725
queries and 134,907 code snippets in 11 programming languages, and is
accompanied by two quality-centric evaluation metrics: Pairwise Preference
Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23
retrieval models, covering both open-source and proprietary systems, and find
that even top-performing models frequently fail to distinguish buggy or
insecure code from their more robust counterparts. Furthermore, we conduct
preliminary investigations into training methods that explicitly encourage
retrievers to recognize code quality. Using synthetic datasets, we demonstrate
promising improvements in quality-aware metrics across various models, without
sacrificing semantic relevance. Downstream code generation experiments further
validate the effectiveness of our approach. Overall, our work highlights the
importance of integrating quality signals into code retrieval systems, laying
the groundwork for more trustworthy and robust software development tools.

</details>


### [44] [DCE-LLM: Dead Code Elimination with Large Language Models](https://arxiv.org/abs/2506.11076)
*Minyu Chen,Guoqiang Li,Ling-I Wu,Ruibang Liu*

Main category: cs.SE

TL;DR: DCE-LLM是一个基于LLM的自动化死代码消除框架，通过高效的代码定位和修正，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 死代码会带来二进制大小、维护和安全性问题，现有工具难以全面解决，需要自动化方案。

Method: 结合CodeBERT模型和LLM，通过标注数据集训练，实现代码分类、定位、解释和修正。

Result: DCE-LLM的F1分数超过94%，比GPT-4o提高30%，支持多语言。

Conclusion: DCE-LLM在死代码检测和修正方面表现优异，为开发提供了高效解决方案。

Abstract: Dead code introduces several challenges in software development, such as
increased binary size and maintenance difficulties. It can also obscure logical
errors and be exploited for obfuscation in malware. For LLM-based code-related
tasks, dead code introduces vulnerabilities that can mislead these models,
raising security concerns. Although modern compilers and IDEs offer dead code
elimination, sophisticated patterns can bypass these tools. A universal
approach that includes classification, location, explanation, and correction is
needed, yet current tools often require significant manual effort. We present
DCE-LLM, a framework for automated dead code elimination using a small CodeBERT
model with an attribution-based line selector to efficiently locate suspect
code. LLMs then generate judgments and explanations, fine-tuned on a
large-scale, annotated dead code dataset to provide detailed explanations and
patches. DCE-LLM outperforms existing tools, with advanced unreachability
detection, automated correction, and support for multiple programming
languages. Experimental results show DCE-LLM achieves over 94% F1 scores for
unused and unreachable code, significantly surpassing GPT-4o by 30%.

</details>


### [45] [Research and Analysis of Employers' Opinion on the Necessary Skills that Students in the Field of Web Programming Should Possess](https://arxiv.org/abs/2506.11084)
*Yordan Kalmukov*

Main category: cs.SE

TL;DR: 研究探讨AI时代雇主对毕业生的技能需求变化，重点分析学生应学习使用现成工具还是掌握基础开发原则。


<details>
  <summary>Details</summary>
Motivation: 探讨现代IT环境下雇主对毕业生技能的需求变化，以确定教育方向。

Method: 通过问卷调查IT雇主，了解其对毕业生技能的需求。

Result: 雇主更看重哪些技能，以帮助学生快速适应工作。

Conclusion: 研究为教育者提供指导，帮助学生掌握雇主所需的实用技能。

Abstract: In the era of artificial intelligence (AI) and chatbots, based on large
language models that can generate programming code in any language, write texts
and summarize information, it is obvious that the requirements of employers for
graduating students have already changed. The modern IT world offers
significant automation of programming through software frameworks and a huge
set of third-party libraries and application programming interfaces (APIs). All
these tools provide most of the necessary functionality out of the box (already
implemented), and quite naturally the question arises as to what is more useful
for students - to teach how to use these ready-made tools or the basic
principles of working and development of web applications from scratch. This
paper analyzes the results of a survey conducted among IT employers, aimed to
identify what, in their opinion, are the necessary technical skills that
graduating students in the field of Web Programming should possess in order to
join the company's work as quickly and effectively as possible.

</details>


### [46] [Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor](https://arxiv.org/abs/2506.11107)
*Weibo Gao,Qi Liu,Rui Li,Yuze Zhao,Hao Wang,Linan Yre,Fangzhou Yao,Zheng Zhang*

Main category: cs.SE

TL;DR: Coda通过将松散代码序列转换为紧凑代码图，并利用语义相似性和聚类感知GCN识别并减轻噪声影响，从而提升PKT模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前PKT研究主要关注代码内容与知识评估的隐式关系，忽略了长期编程活动中的噪声信号（如无关提交和微小修改），限制了模型性能。

Method: 提出Coda框架，包括代码图转换、噪声信号识别（基于语义相似性）、弱信号聚类（使用GCN）及轻量适配器优化。

Result: 在四个真实数据集上，Coda显著优于基线模型，有效处理了噪声编程记录。

Conclusion: Coda是一个模型无关的框架，能提升现有PKT解决方案的性能，尤其在噪声环境下表现突出。

Abstract: Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners'
mastery levels of programming knowledge based on their coding activities,
facilitating more effective and personalized programming education. However,
current PKT studies primarily focus on the implicit relationship between code
content and knowledge assessment, often overlooking two types of noise signals
in long-term programming activities: unwanted signals from unrelated
submissions and weak signals from minor modifications. This practical challenge
significantly limits model performance and application. To address this issue,
we propose Coda, a Code graph-based tuning adaptor designed to enhance existing
PKT models by identifying and mitigating the impact of noise. Specifically,
Coda first transforms the loose code sequences submitted by each learner into a
compact code graph. By leveraging this code graph, unwanted signals can be
identified from a semantic similarity perspective. We then apply a
cluster-aware GCN to the code graph, which improves the discrimination of weak
signals and enables their clustering for identification. Finally, a lightweight
yet effective adaptor is incorporated into the PKT task through optimization
with two noise feature-based constraints and a navigational regularization
term, to correct knowledge states affected by noise. It is worth mentioning
that the Coda framework is model-agnostic and can be adapted to most existing
PKT solutions. Extensive experimental results on four real-world datasets
demonstrate that Coda effectively performs the PKT task in the presence of
noisy programming records, outperforming typical baselines.

</details>


### [47] [Mutual-Supervised Learning for Sequential-to-Parallel Code Translation](https://arxiv.org/abs/2506.11153)
*Changxin Ke,Rui Zhang,Shuo Wang,Li Ding,Guangli Li,Yuanbo Wen,Shuoming Zhang,Ruiyuan Xu,Jin Qin,Jiaming Guo,Chenxi Wang,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的Mutual-Supervised Learning框架（MSL），通过Translator和Tester的协同验证和进化，解决了序列代码到并行代码翻译中的功能等价性问题。


<details>
  <summary>Details</summary>
Motivation: 由于并行编程的复杂性以及数据稀缺性挑战，现有的序列到并行代码翻译方法难以保证功能等价性，因此需要一种新方法来解决这一问题。

Method: MSL框架包含Translator和Tester两个模型，通过迭代的Co-verify和Co-evolve步骤，模型相互生成数据并共同提升。Tester生成单元测试验证功能等价性，Translator生成翻译代码作为Tester的输入。

Result: 实验表明，MSL显著提升了基础模型的性能，Pass@1提高了28.91%，Tester性能提升68.90%，且在BLEU和CodeBLEU分数上优于现有方法。

Conclusion: MSL框架有效解决了序列到并行代码翻译中的功能等价性问题，并在性能上优于现有方法，具有实际应用潜力。

Abstract: The rise of GPU-based high-performance computing (HPC) has driven the
widespread adoption of parallel programming models such as CUDA. Yet, the
inherent complexity of parallel programming creates a demand for the automated
sequential-to-parallel approaches. However, data scarcity poses a significant
challenge for machine learning-based sequential-to-parallel code translation.
Although recent back-translation methods show promise, they still fail to
ensure functional equivalence in the translated code. In this paper, we propose
a novel Mutual-Supervised Learning (MSL) framework for sequential-to-parallel
code translation to address the functional equivalence issue. MSL consists of
two models, a Translator and a Tester. Through an iterative loop consisting of
Co-verify and Co-evolve steps, the Translator and the Tester mutually generate
data for each other and improve collectively. The Tester generates unit tests
to verify and filter functionally equivalent translated code, thereby evolving
the Translator, while the Translator generates translated code as augmented
input to evolve the Tester. Experimental results demonstrate that MuSL
significantly enhances the performance of the base model: when applied to
Qwen2.5-Coder, it not only improves Pass@1 by up to 28.91% and boosts Tester
performance by 68.90%, but also outperforms the previous state-of-the-art
method CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while
achieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is
available at https://github.com/kcxain/musl.

</details>


### [48] [LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation](https://arxiv.org/abs/2506.11237)
*Ngoc Phuoc An Vo,Brent Paulovicks,Vadim Sheinin*

Main category: cs.SE

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: In an effort to automatically evaluate and select the best model and improve
code quality for automatic incident remediation in IT Automation, it is crucial
to verify if the generated code for remediation action is syntactically and
semantically correct and whether it can be executed correctly as intended.
There are three approaches: 1) conventional methods use surface form similarity
metrics (token match, exact match, etc.) which have numerous limitations, 2)
execution-based evaluation focuses more on code functionality based on
pass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs
for automated evaluation to judge if it is a correct answer for a given problem
based on pre-defined metrics. In this work, we focused on enhancing
LLM-as-a-Judge using bidirectional functionality matching and logic
representation for reference-less automatic validation and refinement for Bash
code generation to select the best model for automatic incident remediation in
IT Automation. We used execution-based evaluation as ground-truth to evaluate
our LLM-as-a-Judge metrics. Results show high accuracy and agreement with
execution-based evaluation (and up to 8% over baseline). Finally, we built
Reflection code agents to utilize judgments and feedback from our evaluation
metrics which achieved significant improvement (up to 24% increase in accuracy)
for automatic code refinement.

</details>


### [49] [Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation](https://arxiv.org/abs/2506.11266)
*Benjamin Elder,Anupama Murthi,Jungkoo Kang,Ankita Rajaram Naik,Kiran Kate,Kinjal Basu,Danish Contractor*

Main category: cs.SE

TL;DR: 该文探讨了利用NL2SQL数据集自动生成NL2API数据集的方法，通过SQL语法构建等效的API调用序列，并测试了10种LLM的性能，发现其工具调用能力有限，仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在企业环境中与复杂API集合交互的能力，通过NL2SQL数据集生成NL2API数据集，以评估LLM的工具调用表现。

Method: 开发了一种数据生成管道，利用SQL查询的语法构建等效的API调用序列，并将其应用于BIRD-SQL数据集，生成2500多个可调用的API工具。

Result: 测试了10种公共LLM的性能，发现它们的任务完成率较低（7-47%），通过ReACT代理与环境交互后仅提升至50%。模型在SQL生成上的表现优于API调用。

Conclusion: 当前工具调用的LLM性能不足，需要进一步改进，以适应通用工具调用代理的需求。

Abstract: Large language models (LLMs) are routinely deployed as agentic systems, with
access to tools that interact with live environments to accomplish tasks. In
enterprise deployments these systems need to interact with API collections that
can be extremely large and complex, often backed by databases. In order to
create datasets with such characteristics, we explore how existing NL2SQL
(Natural Language to SQL query) datasets can be used to automatically create
NL2API datasets. Specifically, this work describes a novel data generation
pipeline that exploits the syntax of SQL queries to construct a functionally
equivalent sequence of API calls. We apply this pipeline to one of the largest
NL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be
served as invocable tools or REST-endpoints. We pair natural language queries
from BIRD-SQL to ground-truth API sequences based on this API pool. We use this
collection to study the performance of 10 public LLMs and find that all models
struggle to determine the right set of tools (consisting of tasks of intent
detection, sequencing with nested function calls, and slot-filling). We find
that models have extremely low task completion rates (7-47 percent - depending
on the dataset) which marginally improves to 50 percent when models are
employed as ReACT agents that interact with the live API environment. The best
task completion rates are far below what may be required for effective
general-use tool-calling agents, suggesting substantial scope for improvement
in current state-of-the-art tool-calling LLMs. We also conduct detailed
ablation studies, such as assessing the impact of the number of tools available
as well as the impact of tool and slot-name obfuscation. We compare the
performance of models on the original SQL generation tasks and find that
current models are sometimes able to exploit SQL better than APIs.

</details>


### [50] [A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.11295)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 研究旨在通过基于指标的架构模型来管理ML系统的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂性如何影响ML系统，并为架构决策提供支持。

Method: 引入基于指标的架构模型，并以SPIRA和Ocean Guard MLES为案例研究。

Result: 提出了一种评估ML系统复杂性的模型。

Conclusion: 该模型有助于指导ML系统的设计和扩展。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper brings, side-by-side, the
architecture representation of two systems that can be used as case studies for
creating the metrics-based architectural model: the SPIRA and the Ocean Guard
MLES.

</details>


### [51] [A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline](https://arxiv.org/abs/2506.11400)
*Yupeng Jiang,Yao Deng,Sebastian Schroder,Linfeng Liang,Suhaas Gambhir,Alice James,Avishkar Seth,James Pirrie,Yihao Zhang,Xi Zheng*

Main category: cs.SE

TL;DR: 本文提供了一个自主无人机测试管道的逐步指南，涵盖从模拟到实际测试的关键阶段，以确保安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着自主无人机从研究原型转向关键任务平台，确保其安全、可靠和高效至关重要。

Method: 提出分阶段的测试管道，包括SIL模拟、HIL测试、受控实际测试和现场测试，并结合实例演示。

Result: 通过系统验证和性能优化，减少了部署风险，为无人机实际操作做好准备。

Conclusion: 该测试管道为开发者提供了全面验证方法，助力无人机安全可靠地应用于现实世界。

Abstract: Autonomous drones are rapidly reshaping industries ranging from aerial
delivery and infrastructure inspection to environmental monitoring and disaster
response. Ensuring the safety, reliability, and efficiency of these systems is
paramount as they transition from research prototypes to mission-critical
platforms. This paper presents a step-by-step guide to establishing a robust
autonomous drone testing pipeline, covering each critical stage:
Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)
Testing, Controlled Real-World Testing, and In-Field Testing. Using practical
examples, including the marker-based autonomous landing system, we demonstrate
how to systematically verify drone system behaviors, identify integration
issues, and optimize performance. Furthermore, we highlight emerging trends
shaping the future of drone testing, including the integration of Neurosymbolic
and LLMs, creating co-simulation environments, and Digital Twin-enabled
simulation-based testing techniques. By following this pipeline, developers and
researchers can achieve comprehensive validation, minimize deployment risks,
and prepare autonomous drones for safe and reliable real-world operations.

</details>


### [52] [ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification](https://arxiv.org/abs/2506.11442)
*Yiyang Jin,Kunzhao Xu,Hang Li,Xueting Han,Yanmin Zhou,Cheng Li,Jing Bai*

Main category: cs.SE

TL;DR: 提出了ReVeal框架，通过多轮强化学习结合代码生成与自我验证，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏真实的验证信号和对验证的明确优化，导致自我验证不可靠。

Method: ReVeal框架通过多轮强化学习，结合代码生成、测试用例自动生成、外部工具调用及定制RL算法优化验证能力。

Result: 在LiveCodeBench上Pass@k显著提升，推理深度增加时生成代码质量持续提升，超越DeepSeek-R1-Zero-Qwen-32B。

Conclusion: ReVeal作为一种可扩展的范式，能够构建更稳健和自主的AI智能体。

Abstract: Recent advances in reinforcement learning (RL) with verifiable outcome
rewards have significantly improved the reasoning capabilities of large
language models (LLMs), especially when combined with multi-turn tool
interactions. However, existing methods lack both meaningful verification
signals from realistic environments and explicit optimization for verification,
leading to unreliable self-verification. To address these limitations, we
propose ReVeal, a multi-turn reinforcement learning framework that interleaves
code generation with explicit self-verification and tool-based evaluation.
ReVeal enables LLMs to autonomously generate test cases, invoke external tools
for precise feedback, and improves performance via a customized RL algorithm
with dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a
model's generation and verification capabilities through RL training, expanding
the reasoning boundaries of the base model, demonstrated by significant gains
in Pass@k on LiveCodeBench. It also enables test-time scaling into deeper
inference regimes, with code consistently evolving as the number of turns
increases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.
These findings highlight the promise of ReVeal as a scalable and effective
paradigm for building more robust and autonomous AI agents.

</details>


### [53] [Understanding the Issue Types in Open Source Blockchain-based Software Projects with the Transformer-based BERTopic](https://arxiv.org/abs/2506.11451)
*Md Nahidul Islam Opu,Md Shahidul Islam,Sara Rouhani,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 对1,209个区块链项目的497,742个问题进行了大规模实证研究，发现钱包管理和UI增强是最常见的问题，且钱包问题解决时间最长。


<details>
  <summary>Details</summary>
Motivation: 理解区块链软件开发中的挑战，以改进其鲁棒性和可维护性。

Method: 使用BERTopic对GitHub上的开源区块链项目问题进行分类和分析。

Result: 识别出49个问题主题，分为11个子类，钱包问题频率最高且解决最慢。

Conclusion: 研究结果为区块链软件维护提供了见解，有助于开发专门工具和实践。

Abstract: Blockchain-based software systems are increasingly deployed across diverse
domains, yet a systematic understanding of their development challenges remains
limited. This paper presents a large-scale empirical study of 497,742 issues
mined from 1,209 open-source blockchain projects hosted on GitHub. Employing
BERTopic, a transformer-based topic modeling technique, we identify 49 distinct
issue topics and organize them hierarchically into 11 major subcategories. Our
analysis reveals that both general software development issues and
blockchain-specific concerns are nearly equally represented, with Wallet
Management and UI Enhancement emerging as the most prominent topics. We further
examine the temporal evolution of issue categories and resolution times,
finding that Wallet issues not only dominate in frequency but also exhibit the
longest resolution time. Conversely, Mechanisms issues are resolved
significantly faster. Issue frequency surged after 2016 with the rise of
Ethereum and decentralized applications, but declined after 2022. These
findings enhance our understanding of blockchain software maintenance,
informing the development of specialized tools and practices to improve
robustness and maintainability.

</details>


### [54] [VulStamp: Vulnerability Assessment using Large Language Model](https://arxiv.org/abs/2506.11484)
*Haoshen,Ming Hu,Xiaofei Xie,Jiaye Li,Mingsong Chen*

Main category: cs.SE

TL;DR: 论文提出VulStamp框架，通过大语言模型提取漏洞代码意图，并结合强化学习优化评估模型，解决现有方法依赖人工描述的局限性和数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞评估方法依赖人工描述，但描述质量不一且主观性强，导致性能受限。需要一种无需人工描述的漏洞评估方法。

Method: VulStamp结合静态分析和大语言模型提取漏洞代码意图，使用提示调优模型进行评估，并通过强化学习解决数据不平衡问题。

Result: VulStamp实现了无需人工描述的漏洞严重性评估，优化了开发效率。

Conclusion: VulStamp为漏洞评估提供了新思路，解决了现有方法的局限性，提升了评估性能。

Abstract: Although modern vulnerability detection tools enable developers to
efficiently identify numerous security flaws, indiscriminate remediation
efforts often lead to superfluous development expenses. This is particularly
true given that a substantial portion of detected vulnerabilities either
possess low exploitability or would incur negligible impact in practical
operational environments. Consequently, vulnerability severity assessment has
emerged as a critical component in optimizing software development efficiency.
Existing vulnerability assessment methods typically rely on manually crafted
descriptions associated with source code artifacts. However, due to variability
in description quality and subjectivity in intention interpretation, the
performance of these methods is seriously limited. To address this issue, this
paper introduces VulStamp, a novel intention-guided framework, to facilitate
description-free vulnerability assessment. Specifically, VulStamp adopts static
analysis together with Large Language Model (LLM) to extract the intention
information of vulnerable code. Based on the intention information, VulStamp
uses a prompt-tuned model for vulnerability assessment. Furthermore, to
mitigate the problem of imbalanced data associated with vulnerability types,
VulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to
train the assessment model.

</details>


### [55] [A Procedural Framework for Assessing the Desirability of Process Deviations](https://arxiv.org/abs/2506.11525)
*Michael Grohs,Nadine Cordes,Jana-Rebecca Rehse*

Main category: cs.SE

TL;DR: 提出了一种系统评估流程偏差可取性的框架，结合文献和实践经验，帮助分析师更高效地分类偏差并提出行动建议。


<details>
  <summary>Details</summary>
Motivation: 现有一致性检查技术无法评估流程偏差的可取性，手动评估方法效率低且主观性强。

Method: 基于文献综述和实证访谈，提出了逐步评估框架，指导分析师系统分类偏差并链接行动建议。

Result: 实证评估表明，该框架能有效简化评估过程，实现全面且简洁的偏差分析。

Conclusion: 该框架为流程偏差的可取性评估提供了实用工具，有望提升流程分析的效率和一致性。

Abstract: Conformance checking techniques help process analysts to identify where and
how process executions deviate from a process model. However, they cannot
determine the desirability of these deviations, i.e., whether they are
problematic, acceptable or even beneficial for the process. Such desirability
assessments are crucial to derive actions, but process analysts typically
conduct them in a manual, ad-hoc way, which can be time-consuming, subjective,
and irreplicable. To address this problem, this paper presents a procedural
framework to guide process analysts in systematically assessing deviation
desirability. It provides a step-by-step approach for identifying which input
factors to consider in what order to categorize deviations into mutually
exclusive desirability categories, each linked to action recommendations. The
framework is based on a review and conceptualization of existing literature on
deviation desirability, which is complemented by empirical insights from
interviews with process analysis practitioners and researchers. We evaluate the
framework through a desirability assessment task conducted with practitioners,
indicating that the framework effectively enables them to streamline the
assessment for a thorough yet concise evaluation.

</details>


### [56] [Augmenting the Generality and Performance of Large Language Models for Software Engineering](https://arxiv.org/abs/2506.11548)
*Fabian C. Peña*

Main category: cs.SE

TL;DR: 研究旨在提升大型语言模型（LLMs）在软件工程（SE）中的通用性和性能，特别关注非代码任务、基础知识和幻觉检测。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成和分析方面已取得显著进展，但其在SE中的非代码任务（如概念化和设计）应用仍不足。

Method: 研究通过分析不同LLMs在非代码任务的表现、评估其作为SE基础知识来源的能力，以及检测SE陈述中的幻觉来实现目标。

Result: 初步结果显示在多种非代码任务上的性能提升。

Conclusion: 研究有望提供领域特定的LLMs、新基准和幻觉检测方法，推动LLMs在SE中的广泛应用。

Abstract: Large Language Models (LLMs) are revolutionizing software engineering (SE),
with special emphasis on code generation and analysis. However, their
applications to broader SE practices including conceptualization, design, and
other non-code tasks, remain partially underexplored. This research aims to
augment the generality and performance of LLMs for SE by (1) advancing the
understanding of how LLMs with different characteristics perform on various
non-code tasks, (2) evaluating them as sources of foundational knowledge in SE,
and (3) effectively detecting hallucinations on SE statements. The expected
contributions include a variety of LLMs trained and evaluated on
domain-specific datasets, new benchmarks on foundational knowledge in SE, and
methods for detecting hallucinations. Initial results in terms of performance
improvements on various non-code tasks are promising.

</details>


### [57] [Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation](https://arxiv.org/abs/2506.11559)
*Gábor Antal,Dénes Bán,Martin Isztin,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 论文研究了GPT-4在自动生成单元测试方面的能力，特别关注漏洞修复。结果表明，GPT-4能够生成语法正确的测试用例，但在语义正确性方面表现一般。


<details>
  <summary>Details</summary>
Motivation: 为了提高软件测试的效率和覆盖度，减少手动生成测试用例的复杂性，探索GPT-4在漏洞修复方面的测试生成能力。

Method: 使用VUL4J数据集中的真实漏洞及其修复代码，评估GPT-4生成的测试用例的语法和语义正确性，并考察其自我修正能力。

Result: GPT-4在66.5%的情况下能生成语法正确的测试用例，但语义正确性仅占7.5%。尽管如此，生成的测试模板可进一步开发为功能完整的测试。

Conclusion: GPT-4在部分自动化测试生成中具有潜力，虽需人工介入，但能显著减少工作量。

Abstract: In the life-cycle of software development, testing plays a crucial role in
quality assurance. Proper testing not only increases code coverage and prevents
regressions but it can also ensure that any potential vulnerabilities in the
software are identified and effectively fixed. However, creating such tests is
a complex, resource-consuming manual process. To help developers and security
experts, this paper explores the automatic unit test generation capability of
one of the most widely used large language models, GPT-4, from the perspective
of vulnerabilities. We examine a subset of the VUL4J dataset containing real
vulnerabilities and their corresponding fixes to determine whether GPT-4 can
generate syntactically and/or semantically correct unit tests based on the code
before and after the fixes as evidence of vulnerability mitigation. We focus on
the impact of code contexts, the effectiveness of GPT-4's self-correction
ability, and the subjective usability of the generated test cases. Our results
indicate that GPT-4 can generate syntactically correct test cases 66.5\% of the
time without domain-specific pre-training. Although the semantic correctness of
the fixes could be automatically validated in only 7. 5\% of the cases, our
subjective evaluation shows that GPT-4 generally produces test templates that
can be further developed into fully functional vulnerability-witnessing tests
with relatively minimal manual effort.
  Therefore, despite the limited data, our initial findings suggest that GPT-4
can be effectively used in the generation of vulnerability-witnessing tests. It
may not operate entirely autonomously, but it certainly plays a significant
role in a partially automated process.

</details>


### [58] [Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study](https://arxiv.org/abs/2506.11561)
*Gábor Antal,Bence Bogenfürst,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 研究发现，GPT-4o在修复Java漏洞时性能较GPT-4下降11.9%，但能修复更多漏洞；CVE信息显著提升修复率，而任务描述长度影响较小。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（如GPT-4o）在漏洞修复中的性能表现，以及不同上下文信息对其能力的影响。

Method: 使用Vul4J数据集，比较GPT-4o和GPT-4在相同提示下的表现，并测试9种包含不同上下文信息的提示对修复能力的影响。

Result: GPT-4o平均性能下降11.9%，但能修复更多漏洞；CVE信息和手动提取的代码上下文结合效果最佳。

Conclusion: 零样本场景下，集成多种提示策略能显著提升漏洞修复能力。

Abstract: Recent advancements in large language models (LLMs) have shown promise for
automated vulnerability detection and repair in software systems. This paper
investigates the performance of GPT-4o in repairing Java vulnerabilities from a
widely used dataset (Vul4J), exploring how different contextual information
affects automated vulnerability repair (AVR) capabilities. We compare the
latest GPT-4o's performance against previous results with GPT-4 using identical
prompts. We evaluated nine additional prompts crafted by us that contain
various contextual information such as CWE or CVE information, and manually
extracted code contexts. Each prompt was executed three times on 42
vulnerabilities, and the resulting fix candidates were validated using Vul4J's
automated testing framework.
  Our results show that GPT-4o performed 11.9\% worse on average than GPT-4
with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities
in the three runs together. CVE information significantly improved repair
rates, while the length of the task description had minimal impact. Combining
CVE guidance with manually extracted code context resulted in the best
performance. Using our \textsc{Top}-3 prompts together, GPT-4o repaired 26
(62\%) vulnerabilities at least once, outperforming both the original baseline
(40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies
could improve vulnerability repair in zero-shot settings.

</details>


### [59] [MBSR at Work: Perspectives from an Instructor and Software Developers](https://arxiv.org/abs/2506.11588)
*Simone Romano,Alberto Conforti,Gloria Guidetti,Sara Viotti,Rachele Ceschin,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 定性研究发现，软件开发者在参与正念减压课程（MBSR）后虽持怀疑态度，但认可个人改善；然而工作环境中整合MBSR技术仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 探索MBSR在软件开发（SD）这一高压工作环境中的应用效果，填补该领域的研究空白。

Method: 采用半结构化访谈，收集软件开发者和MBSR导师的第一手经验。

Result: 开发者对MBSR持怀疑态度但承认个人获益；工作环境中实践MBSR技术仍有难度。

Conclusion: MBSR对软件开发者的减压有效，但需进一步研究如何更好地融入工作环境。

Abstract: In this paper, we present the preliminary findings from a qualitative study
(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction
(MBSR) program, carried out in the Software Development (SD) working context,
is perceived by the software developers of a multinational company who
participated in the MBSR program and by the instructor who led it. MBSR is a
deeply personal and experiential practice in helping individuals manage stress,
particularly in high-pressure environments such as workplaces, healthcare
settings, education, and other demanding professional or personal situations.
Although MBSR has been experimented in different working contexts;
surprisingly, it has never been studied in the SD working context where there
are several stress factors that developers experience (e.g., time pressure and
uncertainty about the content of a particular task and its outcome). In this
respect, qualitative research can generate valuable insights into the
application of MBSR in the SD working context that cannot be captured by
standardized quantitative measures. Being MBSR instructors and software
developers the key stakeholders in delivering an MBSR program in the SD working
context, understanding their first-hand experiences can provide a more detailed
picture of the investigated phenomenon. The most important takeaway result of
our research can be summarized as follows: despite initial skepticism, the
developers recognized personal improvements due to the MBSR practice, though
the integration of MBSR techniques in the working context remained challenging.

</details>


### [60] [Retrieval-Augmented Code Review Comment Generation](https://arxiv.org/abs/2506.11591)
*Hyunsun Hong,Jongmoon Baik*

Main category: cs.SE

TL;DR: 该论文提出了基于检索增强生成（RAG）的自动化代码审查评论生成方法，结合生成式与检索式方法的优势，显著提升了评论生成的质量与低频语义标记的覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，生成式模型难以处理低频语义标记，而检索式方法缺乏对新代码的适应性。因此，论文旨在通过结合两种方法提升自动化代码审查评论生成的性能。

Method: 采用检索增强生成（RAG）框架，利用检索到的代码审查示例作为上下文输入，指导预训练语言模型生成更准确的审查评论。

Result: 在Tufano基准测试中，RAG方法比生成式方法提高了1.67%的精确匹配率和4.25%的BLEU分数，并在低频标记生成上提升了24.01%。

Conclusion: RAG方法有效结合了生成与检索的优势，显著提升了代码审查评论生成的质量，尤其是对低频语义标记的覆盖。

Abstract: Automated code review comment generation (RCG) aims to assist developers by
automatically producing natural language feedback for code changes. Existing
approaches are primarily either generation-based, using pretrained language
models, or information retrieval-based (IR), reusing comments from similar past
examples. While generation-based methods leverage code-specific pretraining on
large code-natural language corpora to learn semantic relationships between
code and natural language, they often struggle to generate low-frequency but
semantically important tokens due to their probabilistic nature. In contrast,
IR-based methods excel at recovering such rare tokens by copying from existing
examples but lack flexibility in adapting to new code contexts-for example,
when input code contains identifiers or structures not found in the retrieval
database. To bridge the gap between generation-based and IR-based methods, this
work proposes to leverage retrieval-augmented generation (RAG) for RCG by
conditioning pretrained language models on retrieved code-review exemplars. By
providing relevant examples that illustrate how similar code has been
previously reviewed, the model is better guided to generate accurate review
comments. Our evaluation on the Tufano et al. benchmark shows that RAG-based
RCG outperforms both generation-based and IR-based RCG. It achieves up to
+1.67% higher exact match and +4.25% higher BLEU scores compared to
generation-based RCG. It also improves the generation of low-frequency
ground-truth tokens by up to 24.01%. We additionally find that performance
improves as the number of retrieved exemplars increases.

</details>


### [61] [Further Evidence on a Controversial Topic about Human-Based Experiments: Professionals vs. Students](https://arxiv.org/abs/2506.11597)
*Simone Romano,Francesco Paolo Sferratore,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 论文探讨学生在软件工程实验中的表现是否优于专业开发者，引发关于实验外部有效性的讨论。


<details>
  <summary>Details</summary>
Motivation: 验证学生作为实验参与者与软件行业专业人士在Bug修复任务中的表现差异，以评估实验结果的现实适用性。

Method: 比较62名计算机科学本科生与42名来自跨国公司的专业人士，在同一Java程序Bug修复任务中的表现。专业人士实验环境更贴近现实（如中断压力）。

Result: 学生表现优于专业人士，与过往研究结果部分冲突。

Conclusion: 研究呼吁进一步探讨影响软件工程任务的多因素，并建议未来实验设计应更贴近现实。

Abstract: Most Software Engineering (SE) human-based controlled experiments rely on
students as participants, raising concerns about their external validity.
Specifically, the realism of results obtained from students and their
applicability to the software industry remains in question. In this short
paper, we bring further evidence on this controversial point. To do so, we
compare 62 students and 42 software professionals on a bug-fixing task on the
same Java program. The students were enrolled in a Bachelor's program in
Computer Science, while the professionals were employed by two multinational
companies (for one of them, the professionals were from two offices). Some
variations in the experimental settings of the two groups (students and
professionals) were present. For instance, the experimental environment of the
experiment with professionals was more realistic; i.e., they faced some stress
factors such as interruptions during the bug-fixing task. Considering the
differences between the two groups of participants, the gathered data show that
the students outperformed the professionals in fixing bugs. This diverges to
some extent from past empirical evidence. Rather than presenting definitive
conclusions, our results aim to catalyze the discussion on the use of students
in experiments and pave the way for future investigations. Specifically, our
results encourage us to examine the complex factors influencing SE tasks,
making experiments as more realistic as possible.

</details>


### [62] [Understanding API Usage and Testing: An Empirical Study of C Libraries](https://arxiv.org/abs/2506.11598)
*Ahmed Zaki,Cristian Cadar*

Main category: cs.SE

TL;DR: 研究分析了21个C库的API使用情况，发现开发者未根据客户端使用情况优先测试API，并提出利用客户端测试提升库测试覆盖率的框架LibProbe。


<details>
  <summary>Details</summary>
Motivation: 帮助库开发者通过了解API的实际使用情况，做出数据驱动的决策，如优化测试和优先处理问题。

Method: 对21个C库及其3061个客户端进行实证研究，比较API使用与测试覆盖率，并开发了LibProbe框架。

Result: 发现库开发者未按客户端使用情况测试API，客户测试可提升库测试覆盖率（如LMDB提升了14.7%）。

Conclusion: 建议库开发者利用客户端测试数据优化测试策略，LibProbe提供了一种实用工具。

Abstract: For library developers, understanding how their Application Programming
Interfaces (APIs) are used in the field can be invaluable. Knowing how clients
are using their APIs allows for data-driven decisions on prioritising bug
reports, feature requests, and testing activities. For example, the priority of
a bug report concerning an API can be partly determined by how widely that API
is used.
  In this paper, we present an empirical study in which we analyse API usage
across 21 popular open-source C libraries, such as OpenSSL and SQLite, with a
combined total of 3,061 C/C++ clients. We compare API usage by clients with how
well library test suites exercise the APIs to offer actionable insights for
library developers. To our knowledge, this is the first study that compares API
usage and API testing at scale for the C/C++ ecosystem. Our study shows that
library developers do not prioritise their effort based on how clients use
their API, with popular APIs often poorly tested. For example, in LMDB, a
popular key-value store, 45% of the APIs are used by clients but not tested by
the library test suite. We further show that client test suites can be
leveraged to improve library testing e.g., improving coverage in LMDB by 14.7%
with the important advantage that those tests are representative of how the
APIs are used in the field.
  For our empirical study, we have developed LibProbe, a framework that can be
used to analyse a large corpus of clients for a given library and produce
various metrics useful to library developers.

</details>


### [63] [Accelerating Delta Debugging through Probabilistic Monotonicity Assessment](https://arxiv.org/abs/2506.11614)
*Yonggang Tao,Jingling Xue*

Main category: cs.SE

TL;DR: PMA通过概率单调性评估提升DDMIN算法的效率，减少冗余测试，同时保持效果，显著优于CHISEL和ProbDD。


<details>
  <summary>Details</summary>
Motivation: 实践中Delta调试的单调性假设不成立，PMA通过动态评估单调性提升效率。

Method: PMA动态建模并评估搜索空间的单调性，使用置信函数量化，概率性排除非故障诱导子集。

Result: PMA较CHISEL处理时间减少59.2%，速度提升3.32x，结果程序大小减少6.7%；较ProbDD时间减少22.0%，速度提升1.34x，程序大小减少3.0%。

Conclusion: PMA显著提升Delta调试效率，同时保持或提高效果。

Abstract: Delta debugging assumes search space monotonicity: if a program causes a
failure, any supersets of that program will also induce the same failure,
permitting the exclusion of subsets of non-failure-inducing programs. However,
this assumption does not always hold in practice. This paper introduces
Probabilistic Monotonicity Assessment (PMA), enhancing the efficiency of
DDMIN-style algorithms without sacrificing effectiveness. PMA dynamically
models and assesses the search space's monotonicity based on prior tests tried
during the debugging process and uses a confidence function to quantify
monotonicity, thereby enabling the probabilistic exclusion of subsets of
non-failure-inducing programs. Our approach significantly reduces redundant
tests that would otherwise be performed, without compromising the quality of
the reduction.
  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.
Our findings indicate that PMA cuts processing time by 59.2% compared to
CHISEL, accelerates the reduction process (i.e., the number of tokens deleted
per second) by 3.32x, and decreases the sizes of the final reduced programs by
6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x
speedup in the reduction process, and further decreases the sizes of the final
reduced programs by 3.0%. These findings affirm PMA's role in significantly
improving delta debugging's efficiency while maintaining or enhancing its
effectiveness.

</details>


### [64] [An Empirical study on LLM-based Log Retrieval for Software Engineering Metadata Management](https://arxiv.org/abs/2506.11659)
*Simin Sun,Yuchuan Jin,Miroslaw Staron*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的方法，结合信号日志数据和测试驾驶的视频记录，支持自然语言场景搜索，减少了对专业知识的依赖，并通过量化指标评估查询结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: 开发自动驾驶系统（ADS）时，大量高频日志数据难以查询和定位特定驾驶场景，传统基于SQL的查询方法需要专业知识和数据库技能，结果准确性难以验证。

Method: 结合信号日志数据和视频记录，利用场景距离图和相对间隙指标，提供量化指标评估查询可靠性，实现为API以高效查询和检索记录。

Result: 在开放工业数据集上的评估显示，该方法提高了场景检索的效率和可靠性，减少了对单一数据源和传统SQL的依赖。

Conclusion: 该方法通过自然语言搜索和直观可视化，有效解决了自动驾驶日志数据查询的挑战，提升了开发者的工作效率。

Abstract: Developing autonomous driving systems (ADSs) involves generating and storing
extensive log data from test drives, which is essential for verification,
research, and simulation. However, these high-frequency logs, recorded over
varying durations, pose challenges for developers attempting to locate specific
driving scenarios. This difficulty arises due to the wide range of signals
representing various vehicle components and driving conditions, as well as
unfamiliarity of some developers' with the detailed meaning of these signals.
Traditional SQL-based querying exacerbates this challenge by demanding both
domain expertise and database knowledge, often yielding results that are
difficult to verify for accuracy.
  This paper introduces a Large Language Model (LLM)-supported approach that
combines signal log data with video recordings from test drives, enabling
natural language based scenario searches while reducing the need for
specialized knowledge. By leveraging scenario distance graphs and relative gap
indicators, it provides quantifiable metrics to evaluate the reliability of
query results. The method is implemented as an API for efficient database
querying and retrieval of relevant records, paired with video frames for
intuitive visualization. Evaluation on an open industrial dataset demonstrates
improved efficiency and reliability in scenario retrieval, eliminating
dependency on a single data source and conventional SQL.

</details>


### [65] [SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments](https://arxiv.org/abs/2506.11697)
*Yiwei Hu,Zhen Li,Kedie Shu,Shenghua Guan,Deqing Zou,Shouhuai Xu,Bin Yuan,Hai Jin*

Main category: cs.SE

TL;DR: 该论文系统化总结了自动化漏洞修复（AVR）的方法，提出首个C/C++漏洞修复基准数据集Vul4C，并评估了多种AVR工具，同时探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软件复杂性增加导致漏洞增多，手动修复效率低，自动化漏洞修复（AVR）成为重要研究方向。

Method: 通过漏洞分析、补丁生成和补丁验证三个步骤系统化AVR方法，构建Vul4C数据集并评估多款AVR工具。

Result: Vul4C数据集包含144个漏洞，用于评估7款C/C++ AVR工具；Vul4J数据集评估2款Java AVR工具。

Conclusion: 论文为AVR研究提供系统和数据支持，提出未来研究方向。

Abstract: The increasing complexity of software has led to the steady growth of
vulnerabilities. Vulnerability repair investigates how to fix software
vulnerabilities. Manual vulnerability repair is labor-intensive and
time-consuming because it relies on human experts, highlighting the importance
of Automated Vulnerability Repair (AVR). In this SoK, we present the
systematization of AVR methods through the three steps of AVR workflow:
vulnerability analysis, patch generation, and patch validation. We assess AVR
tools for C/C++ and Java programs as they have been widely studied by the
community. Since existing AVR tools for C/C++ programs are evaluated with
different datasets, which often consist of a few vulnerabilities, we construct
the first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which
contains 144 vulnerabilities as well as their exploits and patches. We use
Vul4C to evaluate seven AVR tools for C/C++ programs and use the third-party
Vul4J dataset to evaluate two AVR tools for Java programs. We also discuss
future research directions.

</details>


### [66] [Classification of Quality Characteristics in Online User Feedback using Linguistic Analysis, Crowdsourcing and LLMs](https://arxiv.org/abs/2506.11722)
*Eduard C. Groen,Fabiano Dalpiaz,Martijn van Vliet,Boris Winter,Joerg Doerr,Sjaak Brinkkemper*

Main category: cs.SE

TL;DR: 研究探讨了在低数据环境下，如何自动识别移动应用用户反馈中的质量特征，比较了基于语言模式、众包和大型语言模型的三种方法。


<details>
  <summary>Details</summary>
Motivation: 移动应用的用户满意度受软件质量（如可用性和可靠性）影响，用户反馈是重要的质量信息来源，但数据的异质性和缺乏训练集限制了监督学习的应用。

Method: 研究了三种低数据环境下的方法：基于质量关键词的语言模式、众包微任务和大型语言模型提示，并比较了它们的准确性。

Result: 语言模式方法的精确度因质量特征而异（0.38-0.92），但召回率低；众包方法在两阶段中表现最佳（0.63, 0.72），而大型语言模型的表现接近（0.66）。

Conclusion: 在低数据环境下，众包和大型语言模型能够实现准确的分类，而语言模式方法潜力有限；众包和大型语言模型还可用于构建训练集。

Abstract: Software qualities such as usability or reliability are among the strongest
determinants of mobile app user satisfaction and constitute a significant
portion of online user feedback on software products, making it a valuable
source of quality-related feedback to guide the development process. The
abundance of online user feedback warrants the automated identification of
quality characteristics, but the online user feedback's heterogeneity and the
lack of appropriate training corpora limit the applicability of supervised
machine learning. We therefore investigate the viability of three approaches
that could be effective in low-data settings: language patterns (LPs) based on
quality-related keywords, instructions for crowdsourced micro-tasks, and large
language model (LLM) prompts. We determined the feasibility of each approach
and then compared their accuracy. For the complex multiclass classification of
quality characteristics, the LP-based approach achieved a varied precision
(0.38-0.92) depending on the quality characteristic, and low recall;
crowdsourcing achieved the best average accuracy in two consecutive phases
(0.63, 0.72), which could be matched by the best-performing LLM condition
(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings
show that in this low-data setting, the two approaches that use crowdsourcing
or LLMs instead of involving experts achieve accurate classifications, while
the LP-based approach has only limited potential. The promise of crowdsourcing
and LLMs in this context might even extend to building training corpora.

</details>


### [67] [A Short Survey on Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.11874)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文通过文献综述探讨了大型语言模型（LLM）在辅助编写软件形式化规范中的应用，总结了35篇关键论文，并提供了相关案例。


<details>
  <summary>Details</summary>
Motivation: 研究源于VERIFAI项目，旨在解决从自然语言需求中编写形式化规范的挑战。

Method: 使用多个学术数据库筛选相关研究，并借助AI工具Elicit进行初步选择，最终人工筛选确定了35篇关键论文。

Result: 综述提供了利用LLM形式化软件需求的宝贵见解及未来研究方向。

Conclusion: LLM在辅助形式化规范方面具有潜力，为未来研究提供了方向。

Abstract: This paper presents a focused literature survey on the use of large language
models (LLM) to assist in writing formal specifications for software. A summary
of thirty-five key papers is presented, including examples for specifying
programs written in Dafny, C and Java. This paper arose from the project
VERIFAI - Traceability and verification of natural language requirements that
addresses the challenges in writing formal specifications from requirements
that are expressed in natural language. Our methodology employed multiple
academic databases to identify relevant research. The AI-assisted tool Elicit
facilitated the initial paper selection, which were manually screened for final
selection. The survey provides valuable insights and future directions for
utilising LLMs while formalising software requirements.

</details>


### [68] [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?](https://arxiv.org/abs/2506.11928)
*Zihan Zheng,Zerui Cheng,Zeyu Shen,Shang Zhou,Kaiyuan Liu,Hansen He,Dongruixuan Li,Stanley Wei,Hangyi Hao,Jianzhu Yao,Peiyao Sheng,Zixuan Wang,Wenhao Chai,Aleksandra Korolova,Peter Henderson,Sanjeev Arora,Pramod Viswanath,Jingbo Shang,Saining Xie*

Main category: cs.SE

TL;DR: 大型语言模型（LLM）在编程竞赛中表现优异，但仍存在局限性，尤其是在复杂算法推理方面与人类专家差距显著。


<details>
  <summary>Details</summary>
Motivation: 重新评估LLM在编程竞赛中的表现，比较其与人类专家的差异，并识别其局限性。

Method: 引入LiveCodeBench Pro基准测试，由国际竞赛奖牌得主进行问题标注和分析，评估LLM在不同难度问题的表现。

Result: 前沿LLM在中等难度问题中通过率为53%，高难度问题为0%，且依赖工具辅助而非推理能力。

Conclusion: LLM在实现细节上表现良好，但在复杂算法推理上仍有显著不足，需进一步改进。

Abstract: Recent reports claim that large language models (LLMs) now outperform elite
humans in competitive programming. Drawing on knowledge from a group of
medalists in international algorithmic contests, we revisit this claim,
examining how LLMs differ from human experts and where limitations still
remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from
Codeforces, ICPC, and IOI that are continuously updated to reduce the
likelihood of data contamination. A team of Olympiad medalists annotates every
problem for algorithmic categories and conducts a line-by-line analysis of
failed model-generated submissions. Using this new data and benchmark, we find
that frontier models still have significant limitations: without external
tools, the best model achieves only 53% pass@1 on medium-difficulty problems
and 0% on hard problems, domains where expert humans still excel. We also find
that LLMs succeed at implementation-heavy problems but struggle with nuanced
algorithmic reasoning and complex case analysis, often generating confidently
incorrect justifications. High performance appears largely driven by
implementation precision and tool augmentation, not superior reasoning.
LiveCodeBench Pro thus highlights the significant gap to human grandmaster
levels, while offering fine-grained diagnostics to steer future improvements in
code-centric LLM reasoning.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [69] [A Performance Model for Warp Specialization Kernels](https://arxiv.org/abs/2506.11209)
*Zhengyang Liu,Vinod Grover*

Main category: cs.PL

TL;DR: 提出了一种针对warp specialization kernels的性能模型，通过微分方程准确预测执行时间，对GPU加速应用优化有实际意义。


<details>
  <summary>Details</summary>
Motivation: 旨在理解和优化warp specialization技术在GPU加速应用中的表现，提升执行效率。

Method: 利用微分方程并结合仿真和实验验证，分析warp大小、tilling大小等影响因素。

Result: 模型能准确预测执行时间，为编译器优化、参数调整和算法设计提供依据。

Conclusion: 模型不仅深化了对warp specialization的理解，还为GPU应用优化提供了实用工具。

Abstract: This paper presents a performance model tailored for warp specialization
kernels, focusing on factors such as warp size, tilling size, input matrix
size, memory bandwidth, and thread divergence. Our model offers accurate
predictions of execution time by leveraging differential equations validated
through simulations and experiments. The insights gained from this model not
only enhance our understanding of warp specialization techniques but also have
practical implications for optimizing GPU-accelerated applications through
compiler optimizations, kernel parameter tuning, and algorithm design.

</details>


### [70] [PermRust: A Token-based Permission System for Rust](https://arxiv.org/abs/2506.11701)
*Lukas Gehring,Sebastian Rehms,Florian Tschorsch*

Main category: cs.PL

TL;DR: 本文提出了一种针对Rust编程语言的库级权限系统PermRust，基于能力系统概念，实现了零成本抽象。


<details>
  <summary>Details</summary>
Motivation: 现代软件常使用第三方库代码，但操作系统权限系统仅能管理进程级访问，因此需要库级权限系统以增强安全性。

Method: 借鉴能力系统概念，构建编程语言级别的权限系统理论框架，并在Rust类型系统上实现零成本抽象的令牌权限系统PermRust。

Result: 实现了PermRust系统，能够按库管理对系统资源的访问。

Conclusion: PermRust为编程语言级别的权限管理提供了理论基础和实践方案，提升了安全性与灵活性。

Abstract: Permission systems which restrict access to system resources are a
well-established technology in operating systems, especially for smartphones.
However, as such systems are implemented in the operating system they can at
most manage access on the process-level. Since moderns software often (re)uses
code from third-parties libraries, a permission system for libraries can be
desirable to enhance security. In this short-paper, we adapt concepts from
capability systems building a novel theoretical foundation for permission
system at the level of the programming language. This leads to PermRust, a
token-based permission system for the Rust programming language as a zero cost
abstraction on top of its type-system. With it access to system resources can
be managed per library.

</details>


### [71] [ALEA IACTA EST: A Declarative Domain-Specific Language for Manually Performable Random Experiments](https://arxiv.org/abs/2506.11794)
*Baltasar Trancón y Widemann,Markus Lepper*

Main category: cs.PL

TL;DR: Alea是一种用于指定随机实验的领域特定语言，适合非专业程序员使用，支持静态分析和动态执行。


<details>
  <summary>Details</summary>
Motivation: 随机实验在教学和游戏中广泛应用，需要一种简单且直观的方式来描述和分析这些实验。

Method: 设计并实现Alea语言，支持静态分析概率分布和动态执行模拟，注重功能性和数学基础的结合。

Result: Alea语言为初学者和游戏设计者提供了一种易于使用的工具，但目前仍在开发和改进中。

Conclusion: Alea语言有望成为教学和游戏设计中描述随机实验的有用工具，但需要进一步优化和扩展。

Abstract: Random experiments that are simple and clear enough to be performed by human
agents feature prominently in the teaching of elementary stochastics as well as
in games. We present Alea, a domain-specific language for the specification of
random experiments. Alea code can either be analyzed statically to obtain and
inspect probability distributions of outcomes, or be executed with a source
pseudo-randomness for simulation or as a game assistant. The language is
intended for ease of use by non-expert programmers, in particular students of
elementary stochastics, and players and designers of games of chance, by
focusing on concepts common to functional programming and basic mathematics.
Both the design of the language and the implementation of runtime environments
are work in progress.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [72] [Efficient Traffic Classification using HW-NAS: Advanced Analysis and Optimization for Cybersecurity on Resource-Constrained Devices](https://arxiv.org/abs/2506.11319)
*Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino*

Main category: cs.NI

TL;DR: 本文提出了一种硬件高效的深度神经网络（DNN），通过硬件感知的神经架构搜索（HW-NAS）优化，用于资源受限的物联网（IoT）和边缘设备上的加密流量分类。优化的1D CNN模型在ISCX VPN-nonVPN数据集上表现出色，参数和计算量大幅减少，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的IoT和边缘设备，设计一种硬件高效的DNN模型，以支持加密流量分类，从而在不牺牲性能的情况下减少内存和计算资源占用。

Method: 采用硬件感知的神经架构搜索（HW-NAS）优化1D CNN模型，对ISCX VPN-nonVPN数据集进行分类，并通过预处理策略和会话长度调整提升效率。

Result: 优化模型实现了96.59%的准确率，参数和计算量大幅减少（最多分别减少444倍和312倍），同时在多种分类任务中表现出色（如VPN分类准确率达99.64%）。预处理和会话长度调整对性能有显著影响。

Conclusion: 该方法为IoT网络中的加密流量实时分析提供了高效、可扩展的解决方案，但预处理和会话长度选择仍需谨慎，以避免性能损失。

Abstract: This paper presents a hardware-efficient deep neural network (DNN), optimized
through hardware-aware neural architecture search (HW-NAS); the DNN supports
the classification of session-level encrypted traffic on resource-constrained
Internet of Things (IoT) and edge devices. Thanks to HW-NAS, a 1D convolutional
neural network (CNN) is tailored on the ISCX VPN-nonVPN dataset to meet strict
memory and computational limits while achieving robust performance. The
optimized model attains an accuracy of 96.59% with just 88.26K parameters,
10.08M FLOPs, and a maximum tensor size of 20.12K. Compared to state-of-the-art
models, it achieves reductions of up to 444-fold, 312-fold, and 15.6-fold in
these metrics, respectively, significantly minimizing memory footprint and
runtime requirements. The model also demonstrates versatility in classification
tasks, achieving accuracies of up to 99.64% in VPN differentiation, VPN-type
classification, broader traffic categories, and application identification. In
addition, an in-depth approach to header-level preprocessing strategies
confirms that the optimized model can provide notable performances across a
wide range of configurations, even in scenarios with stricter privacy
considerations. Likewise, a reduction in the length of sessions of up to 75%
yields significant improvements in efficiency, while maintaining high accuracy
with only a negligible drop of 1-2%. However, the importance of careful
preprocessing and session length selection in the classification of raw traffic
data is still present, as improper settings or aggressive reductions can bring
about a 7% reduction in overall accuracy. Those results highlight the method's
effectiveness in enforcing cybersecurity for IoT networks, by providing
scalable, efficient solutions for the real-time analysis of encrypted traffic
within strict hardware limitations.

</details>


### [73] [Scheduling Agile Earth Observation Satellites with Onboard Processing and Real-Time Monitoring](https://arxiv.org/abs/2506.11556)
*Antonio M. Mercado-Martínez,Beatriz Soret,Antonio Jurado-Navas*

Main category: cs.NI

TL;DR: 本文研究了敏捷地球观测卫星调度问题（AEOSSP），提出了一种结合实时数据处理和多卫星优化的方法，通过优先级指标和局部搜索策略，显著提高了观测分辨率和目标监测频率的均匀性。


<details>
  <summary>Details</summary>
Motivation: 敏捷地球观测卫星（AEOSs）的出现提高了数据采集的灵活性，但如何优化观测序列以最大化收益仍是一个挑战。本文旨在解决这一问题，并结合实时数据处理技术优化多卫星调度。

Method: 定义优先级指标，开发构造性启发式方法，并辅以局部搜索策略，优化多卫星观测序列。

Result: 算法将采集帧分辨率平均提高了10%，目标监测频率的方差减少了83%，优于FIFO方法。

Conclusion: 提出的方法显著提升了敏捷卫星的观测效率和实时性，为多卫星调度问题提供了有效解决方案。

Abstract: The emergence of Agile Earth Observation Satellites (AEOSs) has marked a
significant turning point in the field of Earth Observation (EO), offering
enhanced flexibility in data acquisition. Concurrently, advancements in onboard
satellite computing and communication technologies have greatly enhanced data
compression efficiency, reducing network latency and congestion while
supporting near real-time information delivery. In this paper, we address the
Agile Earth Observation Satellite Scheduling Problem (AEOSSP), which involves
determining the optimal sequence of target observations to maximize overall
observation profit. Our approach integrates onboard data processing for
real-time remote monitoring into the multi-satellite optimization problem. To
this end, we define a set of priority indicators and develop a constructive
heuristic method, further enhanced with a Local Search (LS) strategy. The
results show that the proposed algorithm provides high-quality information by
increasing the resolution of the collected frames by up to 10% on average,
while reducing the variance in the monitoring frequency of the targets within
the instance by up to 83%, ensuring more up-to-date information across the
entire set compared to a First-In First-Out (FIFO) method.

</details>


### [74] [Generalised Rate Control Approach For Stream Processing Applications](https://arxiv.org/abs/2506.11710)
*Ziren Xiao*

Main category: cs.NI

TL;DR: 该论文提出了一种基于图神经网络的深度强化学习方法，用于分布式流处理系统中的数据发射率控制，以避免系统过载，提升吞吐量和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 分布式流处理系统在处理实时数据时容易因过载导致系统不稳定和资源浪费，传统的多层感知机方法存在不足，需要更高效的控制方法。

Method: 采用图神经网络处理流处理引擎的系统指标，通过深度强化学习协同控制数据发射率，避免存储过去状态和长时间等待动作反馈。

Result: 实验表明，该方法在三个应用中使吞吐量和端到端延迟分别提升至多13.5%和30%。

Conclusion: 图神经网络与深度强化学习的结合能有效优化流处理系统的性能，适应多场景多应用需求。

Abstract: Distributed stream processing systems are widely deployed to process
real-time data generated by various devices, such as sensors and software
systems. A key challenge in the system is overloading, which leads to an
unstable system status and consumes additional system resources. In this paper,
we use a graph neural network-based deep reinforcement learning to
collaboratively control the data emission rate at which the data is generated
in the stream source to proactively avoid overloading scenarios. Instead of
using a traditional multi-layer perceptron-styled network to control the rate,
the graph neural network is used to process system metrics collected from the
stream processing engine. Consequently, the learning agent (i) avoids storing
past states where previous actions may affect the current state, (ii) is
without waiting a long interval until the current action has been fully
effective and reflected in the system's specific metrics, and more importantly,
(iii) is able to adapt multiple stream applications in multiple scenarios. We
deploy the rate control approach on three applications, and the experimental
results demonstrate that the throughput and end-to-end latency are improved by
up to 13.5% and 30%, respectively.

</details>


### [75] [Adaptive determinantal scheduling with fairness in wireless networks](https://arxiv.org/abs/2506.11738)
*H. P. Keeler,B. Błaszczyszyn*

Main category: cs.NI

TL;DR: 提出了一种基于行列式点过程的无线网络调度框架，结合公平性，通过凸优化问题解决资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 传统的Aloha协议独立调度传输，缺乏公平性。行列式点过程的排斥特性适合网络资源调度。

Method: 将调度问题转化为针对行列式点过程的凸优化问题，使用$L$-ensembles进行统计和数值处理。

Result: 证明了行列式调度在基于SINR的网络模型中的适用性，并展示了其公平性潜力。

Conclusion: 该框架将机器学习与无线通信结合，提供了数学优雅且计算高效的网络调度方法。

Abstract: We propose a novel framework for wireless network scheduling with fairness
using determinantal (point) processes. Our approach incorporates the repulsive
nature of determinantal processes, generalizing traditional Aloha protocols
that schedule transmissions independently. We formulate the scheduling problem
with an utility function representing fairness. We then recast this formulation
as a convex optimization problem over a certain class of determinantal point
processes called $L$-ensembles, which are particularly suited for statistical
and numerical treatments. These determinantal processes, which have already
proven valuable in subset learning, offer an attractive approach to network
resource scheduling and allocating. We demonstrate the suitability of
determinantal processes for network models based on the
signal-to-interference-plus-noise ratio (SINR). Our results highlight the
potential of determinantal scheduling coupled with fairness. This work bridges
recent advances in machine learning with wireless communications, providing a
mathematically elegant and computationally tractable approach to network
scheduling.

</details>


### [76] [Enabling Next-Generation Cloud-Connected Bionic Limbs Through 5G Connectivity](https://arxiv.org/abs/2506.11744)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.NI

TL;DR: 5G和边缘/云计算赋能智能假肢，解决现有设备的延迟和计算能力问题，通过分层计算架构实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 当前假肢存在计算能力不足和高延迟问题，作者希望通过5G和云计算技术提升假肢性能，改善用户体验。

Method: 采用分层分布式计算架构，结合本地、边缘和云计算层，利用5G网络实现低延迟和高数据率。

Result: 在5G测试环境中验证了该架构能够满足自然假肢控制的需求，实现计算任务的边缘/云端卸载。

Conclusion: 研究为云端互联智能假肢系统提供了初步验证，展示了未来应用潜力。

Abstract: Despite the recent advancements in human-machine interfacing, contemporary
assistive bionic limbs face critical challenges, including limited
computational capabilities, high latency, and unintuitive control mechanisms,
leading to suboptimal user experience and abandonment rates. Addressing these
challenges requires a shift toward intelligent, interconnected solutions
powered by advances in Internet of Things systems, particularly wireless
connectivity and edge/cloud computing. This article presents a conceptual
approach to transform bionic limbs by harnessing the pervasive connectivity of
5G and the significant computational power of cloud and edge servers, equipping
them with capabilities not available hitherto. The system leverages a
hierarchical distributed-computing architecture that integrates local, edge,
and cloud computing layers. Time-critical tasks are handled by a local
processing unit, while compute-intensive tasks are offloaded to edge and cloud
servers, leveraging the high data rate, reliable and low latency capabilities
of advanced cellular networks. We perform a proof-of-concept validation in a 5G
testbed showing that such networks are capable of achieving data rates and
fulfilling latency requirements for a natural prosthetic control, allowing for
offloading of compute-intensive jobs to the edge/cloud servers. This is the
first step towards the realization and real-world validation of cloud-connected
bionic limb systems.

</details>


### [77] [The Throughput Gain of Hypercycle-level Resource Reservation for Time-Triggered Ethernet](https://arxiv.org/abs/2506.11745)
*Peng Wang,Suman Sourav,Binbin Chen,Hongyan Li,Feng Wang,Fan Zhang*

Main category: cs.NI

TL;DR: 本文提出了一种名为超周期级别灵活调度（HFS）的新方法，显著提升了时间触发通信中可接受的流数量，相比固定周期调度（FCS）有高达6倍的提升。


<details>
  <summary>Details</summary>
Motivation: 时间触发通信在安全关键系统中至关重要，但现有固定周期调度方法因资源分配不兼容限制了可接受的流数量，需要更灵活的调度方案。

Method: 提出HFS方案，允许流在每个超周期内跨周期灵活分配资源，将其建模为ILP问题并提出启发式算法HFS-LLF，通过最短路径问题序列解决。

Result: HFS理论上可提供无限容量增益，实际测试中可接受流数量是FCS的6倍，且HFS-LLF运行速度比通用求解器快104倍。

Conclusion: HFS显著提升了时间触发通信系统的灵活性，兼容现有技术，为安全关键系统提供了更高效的解决方案。

Abstract: Time-Triggered Communication is a key technology for many safety-critical
systems, with applications spanning the areas of aerospace and industrial
control. Such communication relies on time-triggered flows, with each flow
consisting of periodic packets originating from a source and destined for a
destination node. Each packet needs to reach its destination before its
deadline. Different flows can have different cycle lengths. To achieve assured
transmission of time-triggered flows, existing efforts constrain the packets of
a flow to be cyclically transmitted along the same path. Under such Fixed
Cyclic Scheduling (FCS), reservation for flows with different cycle lengths can
become incompatible over a shared link, limiting the total number of admissible
flows. Considering the cycle lengths of different flows, a hyper-cycle has
length equal to their least common multiple (LCM). It determines the time
duration over which the scheduling compatibility of the different flows can be
checked. In this work, we propose a more flexible schedule scheme called the
Hypercycle-level Flexible Schedule (HFS) scheme, where a flow's resource
reservation can change across cycles within a hypercycle. HFS can significantly
increase the number of admitted flows by providing more scheduling options
while remaining perfectly compatible with existing Time-Triggered Ethernet
system. We show that, theoretically the possible capacity gain provided by HFS
over FCS can be unbounded. We formulate the joint pathfinding and scheduling
problem under HFS as an ILP problem which we prove to be NP-Hard. To solve HFS
efficiently, we further propose a least-load-first heuristic (HFS-LLF), solving
HFS as a sequence of shortest path problems. Extensive study shows that HFS
admits up to 6 times the number of flows achieved by FCS. Moreover, our
proposed HFS-LLF can run 104 times faster than solving HFS using a generic
solver.

</details>


### [78] [Distributed Learning for Reliable and Timely Communication in 6G Industrial Subnetworks](https://arxiv.org/abs/2506.11749)
*Samira Abdelrahman,Hossam Farag,Gilberto Berardinelli*

Main category: cs.NI

TL;DR: 提出了一种分布式、基于学习的随机接入协议，用于6G工业网络中自主子网络的协调，以提高事件驱动控制流量的及时传输。


<details>
  <summary>Details</summary>
Motivation: 在6G工业网络中，由于无线资源有限、设备活动动态性和高移动性，支持事件驱动的关键控制流量的及时传输具有挑战性。

Method: 使用轻量级神经模型和在线训练，基于竞争签名信号分布式学习选择接入配置，以减少碰撞概率。

Result: 仿真结果显示，该方法显著提升了数据包及时交付的概率，在高密度和高负载场景下表现尤为突出。

Conclusion: 提出的方法在6G工业网络中表现优异，相比传统方法（如MAB）有显著改进。

Abstract: Emerging 6G industrial networks envision autonomous in-X subnetworks to
support efficient and cost-effective short range, localized connectivity for
autonomous control operations. Supporting timely transmission of event-driven,
critical control traffic is challenging in such networks is challenging due to
limited radio resources, dynamic device activity, and high mobility. In this
paper, we propose a distributed, learning-based random access protocol that
establishes implicit inter-subnetwork coordination to minimize the collision
probability and improves timely delivery. Each subnetwork independently learns
and selects access configurations based on a contention signature signal
broadcast by a central access point, enabling adaptive, collision-aware access
under dynamic traffic and mobility conditions. The proposed approach features
lightweight neural models and online training, making it suitable for
deployment in constrained industrial subnetworks. Simulation results show that
our method significantly improves the probability of timely packet delivery
compared to baseline methods, particularly in dense and high-load scenarios.
For instance, our proposed method achieves 21% gain in the probability of
timely packet delivery compared to a classical Multi-Armed Bandit (MAB) for an
industrial setting of 60 subnetworks and 5 radio channels.

</details>


### [79] [A Tale of Two Mobile Generations: 5G-Advanced and 6G in 3GPP Release 20](https://arxiv.org/abs/2506.11828)
*Xingqin Lin*

Main category: cs.NI

TL;DR: 本文探讨了3GPP Release 20在5G和6G过渡阶段的关键作用，强调了其平衡5G-Advanced与6G发展的双重目标。


<details>
  <summary>Details</summary>
Motivation: 电信行业正面临5G向6G过渡的关键时期，3GPP Release 20作为转折点，需要平衡5G-Advanced的增强与6G的基础建设。

Method: 文章通过分析Release 20的关键改进，探讨其设计动机及其对移动通信未来的影响。

Result: Release 20为未来移动通信标准和部署提供了重要基础。

Conclusion: Release 20在5G和6G过渡阶段发挥了桥梁作用，为未来技术发展奠定了基础。

Abstract: As the telecommunications industry stands at the crossroads between the fifth
generation (5G) and sixth generation (6G) of mobile communications, the 3rd
generation partnership project (3GPP) Release 20 emerges as a pivotal point of
transition. By striking a balance between enhancing 5G-Advanced capabilities
and setting the stage for 6G, Release 20 provides the crucial foundation upon
which future mobile communication standards and deployments will be built. This
article examines these dual objectives, outlining the key enhancements, the
motivations behind them, and their implications for the future of mobile
communications.

</details>


### [80] [Intractable Cookie Crumbs: Unveiling the Nexus of Stateful Banner Interaction and Tracking Cookies](https://arxiv.org/abs/2506.11947)
*Ali Rasaii,Ha Dao,Anja Feldmann,Mohammadmadi Javid,Oliver Gasser,Devashish Gosain*

Main category: cs.NI

TL;DR: 研究揭示了网站如何在用户未同意的情况下通过技术漏洞进行跨站追踪，尤其是利用持久性Cookie。


<details>
  <summary>Details</summary>
Motivation: 调查ePrivacy指令和GDPR框架下，网站如何绕过用户同意进行数据追踪。

Method: 对Tranco排行榜上的2万多个域名进行有状态爬取和分析。

Result: 约50%的网站发送至少一个持久性Cookie，启用GPC信号可显著减少数量。

Conclusion: 当前的隐私保护措施存在漏洞，需要更严格的技术和政策干预。

Abstract: In response to the ePrivacy Directive and the consent requirements introduced
by the GDPR, websites began deploying consent banners to obtain user permission
for data collection and processing. However, due to shared third-party services
and technical loopholes, non-consensual cross-site tracking can still occur. In
fact, contrary to user expectations of seemingly isolated consent, a user's
decision on one website may affect tracking behavior on others. In this study,
we investigate the technical and behavioral mechanisms behind these
discrepancies. Specifically, we disclose a persistent tracking mechanism
exploiting web cookies. These cookies, which we refer to as intractable, are
initially set on websites with accepted banners, persist in the browser, and
are subsequently sent to trackers before the user provides explicit consent on
other websites. To meticulously analyze this covert tracking behavior, we
conduct an extensive measurement study performing stateful crawls on over 20k
domains from the Tranco top list, strategically accepting banners in the first
half of domains and measuring intractable cookies in the second half. Our
findings reveal that around 50% of websites send at least one intractable
cookie, with the majority set to expire after more than 10 days. In addition,
enabling the Global Privacy Control (GPC) signal initially reduces the number
of intractable cookies by 30% on average, with a further 32% reduction possible
on subsequent visits by rejecting the banners. Moreover, websites with Consent
Management Platform (CMP) banners, on average, send 6.9 times more intractable
cookies compared to those with native banners. Our research further reveals
that even if users reject all other banners, they still receive a large number
of intractable cookies set by websites with cookie paywalls.

</details>


### [81] [Minimum-hop Constellation Design for Low Earth Orbit Satellite Networks](https://arxiv.org/abs/2506.11995)
*Chirag Rao,Eytan Modiano*

Main category: cs.NI

TL;DR: 研究了低地球轨道卫星网络中优化星间链路（ISL）拓扑以最小化平均最短路径长度（ASPL），并分析了对称和非对称拓扑的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升卫星网络的通信效率，需要优化ISL的拓扑结构以减少数据传输的延迟。

Method: 通过理论分析建立ASPL的下界，并设计了两种拓扑（顶点对称和一般规则拓扑）以验证下界可达到性。

Result: 仿真表明，通过合理设计拓扑，ASPL可接近理论下界，网格结构在ASPL和直径上表现不佳。

Conclusion: 优化后的ISL拓扑能显著提升卫星网络的性能，尤其是在高密度网络中接近理论最优。

Abstract: We consider a Low Earth Orbit (LEO) satellite network with each satellite
capable of establishing inter-satellite link (ISL) connections for
satellite-to-satellite communication. Since ISLs can be reoriented to change
the topology, we optimize the topology to minimize the average shortest path
length (ASPL). We characterize the optimal ASPL ISL topology in two families of
topologies, 1) vertex-symmetric in which the ISL connections at a satellite
node represent a motif that is repeated at all other satellite nodes, and 2)
general regular topologies in which no such repeating pattern need exist. We
establish ASPL lower bounds for both scenarios and show constructions for which
they are achievable assuming each satellite makes 3 or 4 ISL connections. For
the symmetric case, we show that the mesh grid is suboptimal in both ASPL and
diameter. Additionally, we show there are constructions that maintain
intra-orbital ISL connections while still achieving near-optimal ASPL
performance. For the general case we show it is possible to construct networks
with ASPL close to the general lower bound when the network is sufficiently
dense. Simulation results show that for both scenarios, one can find topologies
that are very close to the lower bounds as the network size scales.

</details>


### [82] [Upgrade or Switch: Do We Need a New Registry Architecture for the Internet of AI Agents?](https://arxiv.org/abs/2506.12003)
*Ramesh Raskar,Pradyumna Chari,Jared James Grogan,Mahesh Lambe,Robert Lincourt,Raghu Bala,Abhishek Singh,Ayush Chopra,Rajesh Ranjan,Shailja Gupta,Dimitris Stripelis,Maria Gorskikh,Sichao Wang*

Main category: cs.NI

TL;DR: 论文探讨了现有的网络基础设施是否能够支持新兴的AI代理互联网，分析了升级现有系统或构建新架构的可行性，最终建议采用混合方案。


<details>
  <summary>Details</summary>
Motivation: 现有的网络基础设施是为人类交互设计的，无法满足自主AI代理对速度、规模和安全性（如毫秒级发现、即时凭证撤销等）的高要求。

Method: 分析现有DNS/PKI的不足，评估三种解决方案：升级现有系统、替换为全新架构、采用混合注册表架构。

Result: 升级提供兼容性和快速部署，但全新架构性能更好；由于需求是质变而非量变，混合方案更可能成为主流。

Conclusion: 建议采用混合注册表架构，集中化处理关键代理，分权化应对特定用例。

Abstract: The emerging Internet of AI Agents challenges existing web infrastructure
designed for human-scale, reactive interactions. Unlike traditional web
resources, autonomous AI agents initiate actions, maintain persistent state,
spawn sub-agents, and negotiate directly with peers: demanding
millisecond-level discovery, instant credential revocation, and cryptographic
behavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes
whether to upgrade existing infrastructure or implement purpose-built registry
architectures for autonomous agents. We identify critical failure points: DNS
propagation (24-48 hours vs. required milliseconds), certificate revocation
unable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate
for agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)
Switch options, (3) Hybrid registries. Drawing parallels to dialup-to-broadband
transitions, we find that agent requirements constitute qualitative, and not
incremental, changes. While upgrades offer compatibility and faster deployment,
clean-slate solutions provide better performance but require longer for
adoption. Our analysis suggests hybrid approaches will emerge, with centralized
registries for critical agents and federated meshes for specialized use cases.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [83] [Decidable Reversible Equivalences for Finite Petri Nets](https://arxiv.org/abs/2506.11517)
*Roberto Gorrieri,Ivan Lanese*

Main category: cs.LO

TL;DR: 论文证明了因果网双模拟和遗传因果网双模拟的等价性，展示其可逆行为等价性，并提供了两种可判定的并发行为等价性替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究目标是验证因果网双模拟的可逆性，并提供可判定的并发行为等价性解决方案，弥补现有方法的不可判定性缺陷。

Method: 通过在Petri网中分析因果网双模拟与结构保持双模拟的等价性，并结合已有的双模拟理论进行比较。

Result: 证明因果网双模拟是可逆的行为等价性，且在有限有界Petri网上是可判定的；同时验证了位置双模拟的可判定性。

Conclusion: 研究提供了两种可判定的并发行为等价性方法，替代了不可判定的遗传历史保持双模拟，拓展了Petri网的行为等价性理论。

Abstract: In the setting of Petri nets, we prove that {\em causal-net bisimilarity}
\cite{G15,Gor22,Gor25a}, which is a refinement of history-preserving
bisimilarity \cite{RT88,vGG89,DDM89}, and the novel {\em hereditary} causal-net
bisimilarity, which is a refinement of hereditary history-preserving
bisimilarity \cite{Bed91,JNW96}, do coincide. This means that causal-net
bisimilarity is a {\em reversible behavioral equivalence}, as causal-net
bisimilar markings not only are able to match each other's forward transitions,
but also backward transitions by undoing performed events. Causal-net
bisimilarity can be equivalently formulated as {\em structure-preserving
bisimilarity} \cite{G15,Gor25a}, that is decidable on finite bounded Petri nets
\cite{CG21a}. Moreover, place bisimilarity \cite{ABS91}, that we prove to be
finer than causal-net bisimilarity, is also reversible and it was proved
decidable for finite Petri nets in \cite{Gor21decid,Gor25a}. These results
offer two decidable reversible behavioral equivalences in the true concurrency
spectrum, which are alternative to the coarser hereditary history-preserving
bisimilarity \cite{Bed91,JNW96}, that, unfortunately, is undecidable even for
safe Petri nets \cite{JNS03}.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [84] [Needling Through the Threads: A Visualization Tool for Navigating Threaded Online Discussions](https://arxiv.org/abs/2506.11276)
*Yijun Liu,Frederick Choi,Eshwar Chandrasekharan*

Main category: cs.HC

TL;DR: 论文提出了一个名为Needle的交互式系统，通过视觉分析帮助管理复杂的在线讨论，尤其是嵌套式评论。


<details>
  <summary>Details</summary>
Motivation: 大型在线讨论因内容量大且速度快，导致管理员难以跟踪和有效管理。现有的线程结构仍存在问题，无法清晰展示讨论的全貌。

Method: Needle利用视觉分析技术，总结关键对话指标（如活跃度、毒性水平、投票趋势），并提供高层次的洞察和详细的分析。通过10位Reddit管理员进行用户研究验证效果。

Result: 研究表明，Needle能减轻认知负担，帮助管理员快速定位需关注的区域，并提供决策支持。

Conclusion: 基于研究结果，论文提出了一套设计指南，并认为Needle是首个将交互式视觉分析与人工审核结合的在线讨论管理系统。

Abstract: Navigating large-scale online discussions is difficult due to the rapid pace
and large volume of user-generated content. Prior work in CSCW has shown that
moderators often struggle to follow multiple simultaneous discussions, track
evolving conversations, and maintain contextual understanding--all of which
hinder timely and effective moderation. While platforms like Reddit use
threaded structures to organize discourse, deeply nested threads can still
obscure discussions and make it difficult to grasp the overall trajectory of
conversations. In this paper, we present an interactive system called Needle to
support better navigation and comprehension of complex discourse within
threaded discussions. Needle uses visual analytics to summarize key
conversational metrics--such as activity, toxicity levels, and voting
trends--over time, offering both high-level insights and detailed breakdowns of
discussion threads. Through a user study with ten Reddit moderators, we find
that Needle supports moderation by reducing cognitive load in making sense of
large discussion, helping prioritize areas that need attention, and providing
decision-making supports. Based on our findings, we provide a set of design
guidelines to inform future visualization-driven moderation tools and
sociotechnical systems. To the best of our knowledge, Needle is one of the
first systems to combine interactive visual analytics with human-in-the-loop
moderation for threaded online discussions.

</details>


### [85] [Combining Log Data and Collaborative Dialogue Features to Predict Project Quality in Middle School AI Education](https://arxiv.org/abs/2506.11326)
*Conrad Borchers,Xiaoyi Tian,Kristy Elizabeth Boyer,Maya Israel*

Main category: cs.HC

TL;DR: 研究通过对话和系统交互日志预测中学生AI项目学习中的项目质量，发现不同数据模态对预测特定学习结果有独特价值。


<details>
  <summary>Details</summary>
Motivation: 由于项目学习的开放性，追踪和评估项目进展具有挑战性，研究旨在探索如何利用多模态数据预测项目质量。

Method: 分析94名中学生在结对学习中的对话和系统日志数据，预测项目质量的三个指标（生产力、内容丰富度和词汇多样性）。

Result: 日志数据更擅长预测生产力，对话数据对内容丰富度更有效，多模态融合对生产力和词汇多样性预测有提升。

Conclusion: 多模态数据的价值取决于具体学习目标，研究为开放式AI学习环境中的学习分析提供了新见解。

Abstract: Project-based learning plays a crucial role in computing education. However,
its open-ended nature makes tracking project development and assessing success
challenging. We investigate how dialogue and system interaction logs predict
project quality during collaborative, project-based AI learning of 94 middle
school students working in pairs. We used linguistic features from dialogue
transcripts and behavioral features from system logs to predict three project
quality outcomes: productivity (number of training phrases), content richness
(word density), and lexical variation (word diversity) of chatbot training
phrases. We compared the predictive accuracy of each modality and a fusion of
the modalities. Results indicate log data better predicts productivity, while
dialogue data is more effective for content richness. Both modalities modestly
predict lexical variation. Multimodal fusion improved predictions for
productivity and lexical variation of training phrases but not content
richness. These findings suggest that the value of multimodal fusion depends on
the specific learning outcome. The study contributes to multimodal learning
analytics by demonstrating the nuanced interplay between behavioral and
linguistic data in assessing student learning progress in open-ended AI
learning environments.

</details>


### [86] [Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities](https://arxiv.org/abs/2506.11366)
*Alemitu Bezabih,Shadi Nourriz,Anne-Marie Snider,Rosalie Rauenzahn,C. Estelle Smith*

Main category: cs.HC

TL;DR: 该研究探讨如何将精神护理扩展到在线空间，通过混合方法研究22名牧师，发现在线精神护理社区（OSCCs）的潜在好处，但也揭示了技术限制和未来设计需求，提出了“护理循环”模型和设计建议。


<details>
  <summary>Details</summary>
Motivation: 精神护理在美国需求增长但服务不足，且CSCW/HCI研究少有涉及专业牧师和精神护理提供者，研究旨在理解如何将精神护理拓展到在线空间。

Method: 采用混合方法，包括访谈和用户测试，对22名牧师进行探索性研究，使用扎根理论方法分析数据。

Result: 发现OSCCs的好处（如可访问性、可扩展性）和挑战（如技术限制），提出“护理循环”模型和新干预措施的设计需求。

Conclusion: 研究为在线精神护理提供了理论基础和设计方向，强调了技术与社区护理结合的重要性。

Abstract: Despite a growing need for spiritual care in the US, it is often
under-served, inaccessible, or misunderstood, while almost no prior work in
CSCW/HCI research has engaged with professional chaplains and spiritual care
providers. This interdisciplinary study aims to develop a foundational
understanding of how spiritual care may (or may not) be expanded into online
spaces -- especially focusing on anonymous, asynchronous, and text-based online
communities. We conducted an exploratory mixed-methods study with chaplains
(N=22) involving interviews and user testing sessions centered around Reddit
support communities to understand participants' perspectives on technology and
their ideations about the role of chaplaincy in prospective Online Spiritual
Care Communities (OSCCs). Our Grounded Theory Method analysis highlighted
benefits of OSCCs including: meeting patients where they are at; accessibility
and scalability; and facilitating patient-initiated care. Chaplains highlighted
how their presence in OSCCs could help with shaping peer interactions,
moderation, synchronous chats for group care, and redirecting to external
resources, while also raising important feasibility concerns, risks, and needs
for future design and research. We used an existing taxonomy of chaplaincy
techniques to show that some spiritual care strategies may be amenable to
online spaces, yet we also exposed the limitations of technology to fully
mediate spiritual care and the need to develop new online chaplaincy
interventions. Based on these findings, we contribute the model of a ``Care
Loop'' between institutionally-based formal care and platform-based community
care to expand access and drive greater awareness and utilization of spiritual
care. We also contribute design implications to guide future work in online
spiritual care.

</details>


### [87] [Co-Designing a Chatbot for Culturally Competent Clinical Communication: Experience and Reflections](https://arxiv.org/abs/2506.11393)
*Sandro Radovanović,Shuangyu Li*

Main category: cs.HC

TL;DR: 该论文探讨使用AI聊天机器人支持跨文化沟通培训，初步测试显示其潜力与局限性并存。


<details>
  <summary>Details</summary>
Motivation: 传统模拟病人训练资源密集且难以扩展，尤其在资源匮乏地区，AI聊天机器人提供了一种替代方案。

Method: 设计了基于ACT文化能力模型的AI聊天机器人，模拟真实患者对话并提供结构化反馈，并在医学生中试点。

Result: 聊天机器人帮助学生反思沟通技巧，但未能覆盖系统性问题和历史背景等复杂议题，且缺乏非语言线索。

Conclusion: AI工具在沟通培训中具有潜力，但需进一步改进以提升学习效果。

Abstract: Clinical communication skills are essential for preparing healthcare
professionals to provide equitable care across cultures. However, traditional
training with simulated patients can be resource intensive and difficult to
scale, especially in under-resourced settings. In this project, we explore the
use of an AI-driven chatbot to support culturally competent communication
training for medical students. The chatbot was designed to simulate realistic
patient conversations and provide structured feedback based on the ACT Cultural
Competence model. We piloted the chatbot with a small group of third-year
medical students at a UK medical school in 2024. Although we did not follow a
formal experimental design, our experience suggests that the chatbot offered
useful opportunities for students to reflect on their communication,
particularly around empathy and interpersonal understanding. More challenging
areas included addressing systemic issues and historical context. Although this
early version of the chatbot helped surface some interesting patterns,
limitations were also clear, such as the absence of nonverbal cues and the
tendency for virtual patients to be overly agreeable. In general, this
reflection highlights both the potential and the current limitations of AI
tools in communication training. More work is needed to better understand their
impact and improve the learning experience.

</details>


### [88] [Do Not Immerse and Drive? Prolonged Effects of Cybersickness on Physiological Stress Markers And Cognitive Performance](https://arxiv.org/abs/2506.11536)
*Daniel Zielasko,Ben Rehling,Bernadette von Dawans,Gregor Domes*

Main category: cs.HC

TL;DR: 研究探讨虚拟现实（VR）引发的晕动症后效，发现其显著增加主观和生理压力指标，并导致工作记忆能力下降，症状延迟进展且压力恢复时间较长，提示需延长XR研究的洗脱期。


<details>
  <summary>Details</summary>
Motivation: 探究VR诱发的晕动症对生理压力和认知能力的持续影响，为XR研究的安全性提供依据。

Method: 通过旋转模拟诱发晕动症，测量主观不适、生理压力指标（唾液皮质醇等）和认知任务表现，观察90分钟后效。

Result: VR暴露后主观和生理压力显著增加，工作记忆下降，症状延迟出现，唾液皮质醇持续升高。

Conclusion: 需延长XR研究的洗脱期，并关注专业应用中的后效安全性。

Abstract: Extended exposure to virtual reality environments can induce motion sickness,
often referred to as cybersickness, which may lead to physiological stress
responses and impaired cognitive performance. This study investigates the
aftereffects of VR-induced motion sickness with a focus on physiological stress
markers and working memory performance. Using a carousel simulation to elicit
cybersickness, we assessed subjective discomfort (SSQ, FMS), physiological
stress (salivary cortisol, alpha-amylase, electrodermal activity, heart rate),
and cognitive performance (n-Back task) over a 90-minute post-exposure period.
Our findings demonstrate a significant increase in both subjective and
physiological stress indicators following VR exposure, accompanied by a decline
in working memory performance. Notably, delayed symptom progression was
observed in a substantial proportion of participants, with some reporting peak
symptoms up to 90 minutes post-stimulation. Salivary cortisol levels remained
elevated throughout the observation period, indicating prolonged stress
recovery. These results highlight the need for longer washout phases in XR
research and raise safety concerns for professional applications involving
post-exposure task performance.

</details>


### [89] ["If we misunderstand the client, we misspend 100 hours": Exploring conversational AI and response types for information elicitation](https://arxiv.org/abs/2506.11610)
*Daniel Hove Paludan,Julie Fredsgård,Kasper Patrick Bährentz,Ilhan Aslan*

Main category: cs.HC

TL;DR: 研究了数字技术（如对话AI和选择式响应）如何改善客户与设计师的合作，发现尽管用户体验评分降低，但客户输入的清晰度提高。


<details>
  <summary>Details</summary>
Motivation: 填补数字技术对客户与设计师协作影响的空白研究。

Method: 分三阶段研究：访谈10家设计公司、50名模拟用户的2x2实验、系统反馈收集。

Result: 对话AI和选择式响应降低了用户体验评分，但提升了客户输入的清晰度。

Conclusion: 设计建议：整合对话AI和选择式响应工具以促进初期合作的相互理解。

Abstract: Client-designer alignment is crucial to the success of design projects, yet
little research has explored how digital technologies might influence this
alignment. To address this gap, this paper presents a three-phase study
investigating how digital systems can support requirements elicitation in
professional design practice. Specifically, it examines how integrating a
conversational agent and choice-based response formats into a digital
elicitation tool affects early-stage client-designer collaboration. The first
phase of the study inquired into the current practices of 10 design companies
through semi-structured interviews, informing the system's design. The second
phase evaluated the system using a 2x2 factorial design with 50 mock clients,
quantifying the effects of conversational AI and response type on user
experience and perceived preparedness. In phase three, the system was presented
to seven of the original 10 companies to gather reflections on its value,
limitations, and potential integration into practice. Findings show that both
conversational AI and choice-based responses lead to lower dependability scores
on the User Experience Questionnaire, yet result in client input with greater
clarity. We contribute design implications for integrating conversational AI
and choice-based responses into elicitation tools to support mutual
understanding in early-stage client-designer collaboration.

</details>


### [90] [Perspectives on Explanation Formats From Two Stakeholder Groups in Germany: Software Providers and Dairy Farmers](https://arxiv.org/abs/2506.11665)
*Mengisti Berihu Girmay,Felix Möhrle*

Main category: cs.HC

TL;DR: 研究比较了德国奶农和软件提供商对数字决策支持系统解释格式的看法，发现两者之间存在差异。


<details>
  <summary>Details</summary>
Motivation: 探讨奶农和软件提供商对解释格式的不同观点，以找出数字系统采用率低的原因。

Method: 设计四种解释格式（文本、规则、畜群比较、时间序列），分别调查奶农和软件提供商的反馈。

Result: 软件提供商对奶农偏好的假设不准确，表明需更深入的用户需求分析。

Conclusion: 研究强调用户需求分析对改进软件适应性和用户接受度的重要性。

Abstract: This paper examines the views of software providers in the German dairy
industry with regard to dairy farmers' needs for explanation of digital
decision support systems. The study is based on mastitis detection in dairy
cows using a hypothetical herd management system. We designed four exemplary
explanation formats for mastitis assessments with different types of
presentation (textual, rule-based, herd comparison, and time series). In our
previous study, 14 dairy farmers in Germany had rated these formats in terms of
comprehensibility and the trust they would have in a system providing each
format. In this study, we repeat the survey with 13 software providers active
in the German dairy industry. We ask them how well they think the formats would
be received by farmers. We hypothesized that there may be discrepancies between
the views of both groups that are worth investigating, partly to find reasons
for the reluctance to adopt digital systems. A comparison of the feedback from
both groups supports the hypothesis and calls for further investigation. The
results show that software providers tend to make assumptions about farmers'
preferences that are not necessarily accurate. Our study, although not
representative due to the small sample size, highlights the potential benefits
of a thorough user requirements analysis (farmers' needs) to improve software
adaptation and user acceptance.

</details>


### [91] [Interaction, Process, Infrastructure: A Unified Architecture for Human-Agent Collaboration](https://arxiv.org/abs/2506.11718)
*Yun Wang,Yan Lu*

Main category: cs.HC

TL;DR: 论文提出了一种分层框架，整合了交互、流程和基础设施三个维度，以解决当前AI工具在专业工作中协作不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI工具虽然功能强大，但缺乏持续的、适应性强的协作框架，难以满足专业知识的长期协作需求。

Method: 提出了一种分层框架，重点关注流程的显式化、可检查和可适应性，以促进人类与AI的长期协作。

Result: 该框架揭示了当前工具的局限性，统一了系统设计方法，并为研究者提供了新的研究方向。

Conclusion: 通过结构化协作，人类与AI的协作不再是任务特定增强，而是实现现实工作中一致的、对齐的系统。

Abstract: As AI tools proliferate across domains, from chatbots and copilots to
emerging agents, they increasingly support professional knowledge work. Yet
despite their growing capabilities, these systems remain fragmented: they
assist with isolated tasks but lack the architectural scaffolding for
sustained, adaptive collaboration. We propose a layered framework for
human-agent systems that integrates three interdependent dimensions:
interaction, process, and infrastructure. Crucially, our architecture elevates
process to a primary focus by making it explicit, inspectable, and adaptable,
enabling humans and agents to align with evolving goals and coordinate over
time. This model clarifies limitations of current tools, unifies emerging
system design approaches, and reveals new opportunities for researchers and AI
system builders. By grounding intelligent behavior in structured collaboration,
we reimagine human-agent collaboration not as task-specific augmentation, but
as a form of coherent and aligned system for real-world work.

</details>


### [92] [GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant](https://arxiv.org/abs/2506.11781)
*Gaspard Merten,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.HC

TL;DR: GeoPandas-AI通过集成LLM到GeoPandas工作流中，简化了地理空间数据分析，为专家和初学者提供了智能化的代码生成和分析工具。


<details>
  <summary>Details</summary>
Motivation: 针对地理空间数据分析工具（如GeoPandas）使用复杂、需要专业知识的问题，提出利用LLM技术简化这一过程。

Method: 设计并实现GeoPandas-AI，通过智能化的GeoDataFrame类和对话界面，利用LLM生成代码和分析数据。

Result: 开源了GeoPandas-AI的PyPI包，为地理空间开发提供了一种新的代码辅助范式。

Conclusion: GeoPandas-AI通过结合对话界面和LLM技术，开创了地理空间开发的新方式，降低了技术门槛。

Abstract: Geospatial data analysis plays a crucial role in tackling intricate societal
challenges such as urban planning and climate modeling. However, employing
tools like GeoPandas, a prominent Python library for geospatial data
manipulation, necessitates expertise in complex domain-specific syntax and
workflows. GeoPandas-AI addresses this gap by integrating LLMs directly into
the GeoPandas workflow, transforming the GeoDataFrame class into an
intelligent, stateful class for both data analysis and geospatial code
development. This paper formalizes the design of such a smart class and
provides an open-source implementation of GeoPandas-AI in PyPI package manager.
Through its innovative combination of conversational interfaces and stateful
exploitation of LLMs for code generation and data analysis, GeoPandas-AI
introduces a new paradigm for code-copilots and instantiates it for geospatial
development.

</details>


### [93] [Digital Labor: Challenges, Ethical Insights, and Implications](https://arxiv.org/abs/2506.11788)
*ATM Mizanur Rahman,Sharifa Sultana*

Main category: cs.HC

TL;DR: 本文分析了2015年至2024年间300多篇关于数字劳动的研究论文，重点关注了其中的143篇，揭示了数字工人面临的低报酬、不公平条件及缺乏认可等问题，并提出了对研究者、平台设计者和政策制定者的新见解。


<details>
  <summary>Details</summary>
Motivation: 数字工人在AI系统中扮演关键角色，但其待遇和认可度不足，研究旨在系统梳理相关问题。

Method: 筛选并详细分析了143篇关于数字零工劳动的论文，综合了领域内的关键问题和趋势。

Result: 揭示了数字零工劳动的现状和问题，提出了需要干预和未来研究的重点领域。

Conclusion: 论文为理解和改善数字劳动在AI生态系统中的角色提供了批判性视角。

Abstract: Digital workers on crowdsourcing platforms (e.g., Amazon Mechanical Turk,
Appen, Clickworker, Prolific) play a crucial role in training and improving AI
systems, yet they often face low pay, unfair conditions, and a lack of
recognition for their contributions. To map these issues in the existing
literature of computer science, AI, and related scholarship, we selected over
300 research papers on digital labor published between 2015 and 2024, narrowing
them down to 143 on digital gig-labor for a detailed analysis. This analysis
provides a broad overview of the key challenges, concerns, and trends in the
field. Our synthesis reveals how the persistent patterns of representation and
voices of gig workers in digital labor are structured and governed. We offer
new insights for researchers, platform designers, and policymakers, helping
them better understand the experiences of digital workers and pointing to key
areas where interventions and future investigations are promptly needed. By
mapping the findings from the past ten years' growth of the domain and possible
implications, this paper contributes to a more coherent and critical
understanding of digital labor in contemporary and future AI ecosystems.

</details>


### [94] [Conversational AI as a Catalyst for Informal Learning: An Empirical Large-Scale Study on LLM Use in Everyday Learning](https://arxiv.org/abs/2506.11789)
*Nađa Terzimehić,Babette Bühler,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 大型语言模型（LLMs）已成为自导向学习的重要工具，88%的调查参与者已将其融入日常学习。年轻人是主要使用者，但用户对其准确性和隐私保护表现出矛盾态度。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs如何改变非正式学习方式，了解用户采纳或避免使用的原因，以及新兴的学习模式。

Method: 通过776名参与者的大规模调查，分析LLMs在不同学习任务和设备中的使用情况。

Result: 88%的受访者已将LLMs用于日常学习，年轻人是主要用户。用户对LLMs的信任和隐私保护行为表现出矛盾。

Conclusion: 未来设计LLMs学习工具时，需考虑多样化媒体支持、协作学习、信息来源以及满足不同学习者需求。

Abstract: Large language models have not only captivated the public imagination but
have also sparked a profound rethinking of how we learn. In the third year
following the breakthrough launch of ChatGPT, everyday informal learning has
been transformed as diverse user groups explore these novel tools. Who is
embracing LLMs for self-directed learning, and who remains hesitant? What are
their reasons for adoption or avoidance? What learning patterns emerge with
this novel technological landscape? We present an in-depth analysis from a
large-scale survey of 776 participants, showcasing that 88% of our respondents
already incorporate LLMs into their everyday learning routines for a wide
variety of (learning) tasks. Young adults are at the forefront of adopting
LLMs, primarily to enhance their learning experiences independently of time and
space. Four types of learners emerge across learning contexts, depending on the
tasks they perform with LLMs and the devices they use to access them.
Interestingly, our respondents exhibit paradoxical behaviours regarding their
trust in LLMs' accuracy and privacy protection measures. Our implications
emphasize the importance of including different media types for learning,
enabling collaborative learning, providing sources and meeting the needs of
different types of learners and learning by design.

</details>


### [95] [Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training](https://arxiv.org/abs/2506.11890)
*Judson Leroy Dean Haynes IV*

Main category: cs.HC

TL;DR: 探究VR教师培训中AI虚拟学生最佳逼真度的研究，提出基于教学理论的分阶段逼真度框架（Graduated Realism），并设计高效的单调用架构Crazy Slots。


<details>
  <summary>Details</summary>
Motivation: VR仿真技术为教师培训提供了强大工具，但AI驱动的虚拟学生逼真度对教学效果的影响尚不明确，需平衡技术追求与教学需求。

Method: 通过文献综述分析VR教师培训中虚拟逼真度的演变，结合认知负荷理论，提出分阶段逼真度框架，并设计Crazy Slots架构实现实时响应。

Result: 研究支持逼真度并非越高越好，分阶段提升逼真度有助于减少新手认知负荷，Crazy Slots架构有效降低了延迟和成本。

Conclusion: 提出教学驱动的逼真度设计原则，Graduated Realism框架和Crazy Slots架构为下一代AI模拟器的开发提供了可扩展且高效的解决方案。

Abstract: Virtual Reality simulators offer a powerful tool for teacher training, yet
the integration of AI-powered student avatars presents a critical challenge:
determining the optimal level of avatar realism for effective pedagogy. This
literature review examines the evolution of avatar realism in VR teacher
training, synthesizes its theoretical implications, and proposes a new
pedagogical framework to guide future design. Through a systematic review, this
paper traces the progression from human-controlled avatars to generative AI
prototypes. Applying learning theories like Cognitive Load Theory, we argue
that hyper-realism is not always optimal, as high-fidelity avatars can impose
excessive extraneous cognitive load on novices, a stance supported by recent
empirical findings. A significant gap exists between the technological drive
for photorealism and the pedagogical need for scaffolded learning. To address
this gap, we propose Graduated Realism, a framework advocating for starting
trainees with lower-fidelity avatars and progressively increasing behavioral
complexity as skills develop. To make this computationally feasible, we outline
a novel single-call architecture, Crazy Slots, which uses a probabilistic
engine and a Retrieval-Augmented Generation database to generate authentic,
real-time responses without the latency and cost of multi-step reasoning
models. This review provides evidence-based principles for designing the next
generation of AI simulators, arguing that a pedagogically grounded approach to
realism is essential for creating scalable and effective teacher education
tools.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [96] [Anti-Aliased 2D Gaussian Splatting](https://arxiv.org/abs/2506.11252)
*Mae Younes,Adnane Boukhayma*

Main category: cs.GR

TL;DR: AA-2DGS改进2D高斯溅射方法，解决多尺度渲染中的走样问题，提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 2DGS在多尺度渲染中因缺乏频率约束和无效的屏幕空间夹紧方法而出现走样问题。

Method: 引入世界空间平滑核和对象空间Mip滤波器来约束频率内容并消除高频伪影。

Result: AA-2DGS在保持几何精度的同时显著提升多尺度渲染质量。

Conclusion: AA-2DGS有效解决了2DGS的走样问题，扩展了其实际应用场景。

Abstract: 2D Gaussian Splatting (2DGS) has recently emerged as a promising method for
novel view synthesis and surface reconstruction, offering better
view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS
suffers from severe aliasing artifacts when rendering at different sampling
rates than those used during training, limiting its practical applications in
scenarios requiring camera zoom or varying fields of view. We identify that
these artifacts stem from two key limitations: the lack of frequency
constraints in the representation and an ineffective screen-space clamping
approach. To address these issues, we present AA-2DGS, an antialiased
formulation of 2D Gaussian Splatting that maintains its geometric benefits
while significantly enhancing rendering quality across different scales. Our
method introduces a world space flat smoothing kernel that constrains the
frequency content of 2D Gaussian primitives based on the maximal sampling
frequency from training views, effectively eliminating high-frequency artifacts
when zooming in. Additionally, we derive a novel object space Mip filter by
leveraging an affine approximation of the ray-splat intersection mapping, which
allows us to efficiently apply proper anti-aliasing directly in the local space
of each splat.

</details>


### [97] [On Ray Reordering Techniques for Faster GPU Ray Tracing](https://arxiv.org/abs/2506.11273)
*Daniel Meister,Jakub Bokšanský,Michael Guthe,Jiří Bittner*

Main category: cs.GR

TL;DR: 研究如何通过光线重排序提升GPU光线追踪的性能，提出了一种改进的方法，并评估了其在波前路径追踪中的效果。


<details>
  <summary>Details</summary>
Motivation: 探讨光线重排序对现有GPU光线追踪实现的性能提升潜力，尤其是对二次光线的适用性。

Method: 总结了现有的光线排序方法，提出了一种基于终止点估计的新改进方法，并在RTX追踪内核中评估了这些技术。

Result: 光线重排序在最新GPU上显著提升了追踪速度（1.3-2.0倍），但硬件加速阶段的排序开销恢复存在问题。

Conclusion: 光线重排序可以显著提升性能，但在实际应用中需权衡排序开销与性能增益。

Abstract: We study ray reordering as a tool for increasing the performance of existing
GPU ray tracing implementations. We focus on ray reordering that is fully
agnostic to the particular trace kernel. We summarize the existing methods for
computing the ray sorting keys and discuss their properties. We propose a novel
modification of a previously proposed method using the termination point
estimation that is well-suited to tracing secondary rays. We evaluate the ray
reordering techniques in the context of the wavefront path tracing using the
RTX trace kernels. We show that ray reordering yields significantly higher
trace speed on recent GPUs (1.3-2.0x), but to recover the reordering overhead
in the hardware-accelerated trace phase is problematic.

</details>


### [98] [Adaptive Tetrahedral Grids for Volumetric Path-Tracing](https://arxiv.org/abs/2506.11510)
*Anis Benyoub,Jonathan Dupuy*

Main category: cs.GR

TL;DR: 使用最长边二分法构建的四面体网格用于体积数据路径追踪，GPU实现性能提升达30倍。


<details>
  <summary>Details</summary>
Motivation: 解决体积数据渲染中内存占用高和计算效率低的问题。

Method: 基于最长边二分法构建自适应四面体网格，优化GPU上的路径追踪算法和数据结构。

Result: GPU实现性能比规则网格提升30倍，支持实时渲染32样本/像素的生产资产。

Conclusion: 四面体网格在体积数据路径追踪中高效且节省内存，适用于实时渲染。

Abstract: We advertise the use of tetrahedral grids constructed via the longest edge
bisection algorithm for rendering volumetric data with path tracing. The key
benefits of such grids is two-fold. First, they provide a highly adaptive
space-partitioning representation that limits the memory footprint of
volumetric assets. Second, each (tetrahedral) cell has exactly 4 neighbors
within the volume (one per face of each tetrahedron) or less at boundaries. We
leverage these properties to devise optimized algorithms and data-structures to
compute and path-trace adaptive tetrahedral grids on the GPU. In practice, our
GPU implementation outperforms regular grids by up to x30 and renders
production assets in real time at 32 samples per pixel.

</details>


### [99] [CGVQM+D: Computer Graphics Video Quality Metric and Dataset](https://arxiv.org/abs/2506.11546)
*Akshay Jindal,Nabil Sadaka,Manu Mathew Thomas,Anton Sochenov,Anton Kaplanyan*

Main category: cs.GR

TL;DR: 提出了一个针对高级渲染技术引入的失真的视频质量数据集，并开发了新的质量度量标准CGVQM，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索合成内容和现代渲染失真的视觉质量，填补现有数据集在自然视频和传统失真之外的空白。

Method: 构建新数据集，评估现有指标，提出基于预训练3D CNN特征的CGVQM度量标准。

Result: CGVQM显著优于现有指标，Pearson相关系数最高达0.78。

Conclusion: 新型数据集和CGVQM为合成内容的质量评估提供了有效工具。

Abstract: While existing video and image quality datasets have extensively studied
natural videos and traditional distortions, the perception of synthetic content
and modern rendering artifacts remains underexplored. We present a novel video
quality dataset focused on distortions introduced by advanced rendering
techniques, including neural supersampling, novel-view synthesis, path tracing,
neural denoising, frame interpolation, and variable rate shading. Our
evaluations show that existing full-reference quality metrics perform
sub-optimally on these distortions, with a maximum Pearson correlation of 0.78.
Additionally, we find that the feature space of pre-trained 3D CNNs aligns
strongly with human perception of visual quality. We propose CGVQM, a
full-reference video quality metric that significantly outperforms existing
metrics while generating both per-pixel error maps and global quality scores.
Our dataset and metric implementation is available at
https://github.com/IntelLabs/CGVQM.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [100] [Gradients of unitary optical neural networks using parameter-shift rule](https://arxiv.org/abs/2506.11565)
*Jinzhe Jiang,Yaqian Zhao,Xin Zhang,Chen Li,Yunlong Yu,Hailing Liu*

Main category: cs.ET

TL;DR: 本文研究了参数偏移规则（PSR）在酉光学神经网络（UONN）中计算梯度的应用，提出了一种克服光学系统物理限制的有效训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播在光学神经网络中实现困难，PSR为这一问题提供了解决方案。

Method: 通过调整参数值计算梯度，利用马赫-曾德尔干涉仪网格构建UONN，直接从硬件测量中获取精确解析梯度。

Result: 该方法避免了有限差分近似和全光学反向传播的限制，为光学计算系统的高效训练提供了潜力。

Conclusion: PSR为光学神经网络的硬件训练策略提供了理论和实践基础，有望推动光学计算系统的发展。

Abstract: This paper explores the application of the parameter-shift rule (PSR) for
computing gradients in unitary optical neural networks (UONNs). While
backpropagation has been fundamental to training conventional neural networks,
its implementation in optical neural networks faces significant challenges due
to the physical constraints of optical systems. We demonstrate how PSR, which
calculates gradients by evaluating functions at shifted parameter values, can
be effectively adapted for training UONNs constructed from Mach-Zehnder
interferometer meshes. The method leverages the inherent Fourier series nature
of optical interference in these systems to compute exact analytical gradients
directly from hardware measurements. This approach offers a promising
alternative to traditional in silico training methods and circumvents the
limitations of both finite difference approximations and all-optical
backpropagation implementations. We present the theoretical framework and
practical methodology for applying PSR to optimize phase parameters in optical
neural networks, potentially advancing the development of efficient
hardware-based training strategies for optical computing systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [101] [SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous Speculative Decoding](https://arxiv.org/abs/2506.11309)
*Ziyi Zhang,Ziheng Jiang,Chengquan Jiang,Menghan Yu,Size Zheng,Haibin Lin,Henry Hoffmann,Xin Liu*

Main category: cs.DC

TL;DR: SwiftSpec通过异步和分散设计的推测解码技术大幅降低大型语言模型的解码延迟，相比现有技术实现了1.75倍加速，并在8块GPU上达到348 tokens/s的高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 为提高大型语言模型（如聊天机器人和代码助手）在单查询设置中生成长输出的速度，现有的推测解码技术和张量并行方法因计算需求不平衡和通信开销等问题无法同时应用。SwiftSpec旨在解决这些问题，实现超低延迟解码。

Method: SwiftSpec重新设计推测解码流程，采用异步和分散式方法，提出了并行树生成、树感知KV缓存管理以及融合的延迟优化内核，以灵活扩展各组件并减少关键路径上的开销。

Result: 在5个模型家族和6个数据集上进行测试，SwiftSpec平均比现有推测解码系统快1.75倍，尤其在8块Nvidia Hopper GPU上实现了348 tokens/s的高吞吐量。

Conclusion: SwiftSpec是目前已知的在低延迟大型语言模型服务中最快的系统，显著提升了解码效率。

Abstract: Low-latency decoding for large language models (LLMs) is crucial for
applications like chatbots and code assistants, yet generating long outputs
remains slow in single-query settings. Prior work on speculative decoding
(which combines a small draft model with a larger target model) and tensor
parallelism has each accelerated decoding. However, conventional approaches
fail to apply both simultaneously due to imbalanced compute requirements
(between draft and target models), KV-cache inconsistencies, and communication
overheads under small-batch tensor-parallelism. This paper introduces
SwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec
redesigns the speculative decoding pipeline in an asynchronous and
disaggregated manner, so that each component can be scaled flexibly and remove
draft overhead from the critical path. To realize this design, SwiftSpec
proposes parallel tree generation, tree-aware KV cache management, and fused,
latency-optimized kernels to overcome the challenges listed above. Across 5
model families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup
over state-of-the-art speculative decoding systems and, as a highlight, serves
Llama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known
system for low-latency LLM serving at this scale.

</details>


### [102] [Capsule: Efficient Player Isolation for Datacenters](https://arxiv.org/abs/2506.11483)
*Zhouheng Du,Nima Davari,Li Li,Nodir Kodirov*

Main category: cs.DC

TL;DR: Capsule是一种机制，允许多个玩家共享一个GPU，显著提高数据中心资源利用率。


<details>
  <summary>Details</summary>
Motivation: 云游戏日益流行，但数据中心资源利用率低，尤其是在GPU上，因为游戏引擎通常只支持单玩家运行。

Method: 开发了Capsule机制，并在开源游戏引擎O3DE中实现，支持多玩家共享GPU。

Result: 实验表明，Capsule能容纳最多2.25倍玩家，提升资源利用率且不影响游戏体验。

Conclusion: Capsule设计可推广至其他游戏引擎，帮助云提供商提高数据中心利用率。

Abstract: Cloud gaming is increasingly popular. A challenge for cloud provider is to
keep datacenter utilization high: a non-trivial task due to application
variety. These applications come in different shapes and sizes. So do cloud
datacenter resources, e.g., CPUs, GPUs, NPUs.
  Part of the challenge stems from game engines being predominantly designed to
run only one player. One player in a lightweight game might utilize only a
fraction of the cloud server GPU. The remaining GPU capacity will be left
underutilized, an undesired outcome for the cloud provider. We introduce
Capsule, a mechanism that allows multiple players to seamlessly share one GPU.
  We implemented Capsule in O3DE, a popular open source game engine. Our
evaluations show that Capsule can increase datacenter resource utilization by
accommodating up to 2.25x more players, without degrading player gaming
experience. Capsule is also application agnostic. We ran four applications on
Capsule-based O3DE with no application changes. Our experiences show that
Capsule design can be adopted by other game engines to increase datacenter
utilization across cloud providers.

</details>


### [103] [Bounded Memory in Distributed Networks](https://arxiv.org/abs/2506.11644)
*Ran Ben Basat,Keren Censor-Hillel,Yi-Jun Chang,Wenchen Han,Dean Leitersdorf,Gregory Schwartzman*

Main category: cs.DC

TL;DR: 该论文介绍了μ-CONGEST模型，解决了CONGEST算法在现实部署中因内存限制引起的问题，提出了两种快速算法：改进内存密集型算法和高效模拟流算法。


<details>
  <summary>Details</summary>
Motivation: 现实数据中心网络中的可编程交换机虽然便于部署分布式算法，但CONGEST算法仍面临理论与实践的差距，尤其是内存限制问题。

Method: 提出μ-CONGEST模型，结合带宽和内存限制（μ字）；设计改进的内存密集型算法，并建立下限证明；高效模拟多种流算法。

Result: 提供了算法运行时间与节点内存之间的紧致权衡关系；展示了流算法在μ-CONGEST中的高效模拟能力。

Conclusion: 通过μ-CONGEST模型及相应算法，解决了内存限制问题，并扩展了流算法在网络统计中的应用。

Abstract: The recent advent of programmable switches makes distributed algorithms
readily deployable in real-world datacenter networks. However, there are still
gaps between theory and practice that prevent the smooth adaptation of CONGEST
algorithms to these environments. In this paper, we focus on the memory
restrictions that arise in real-world deployments. We introduce the
$\mu$-CONGEST model where on top of the bandwidth restriction, the memory of
nodes is also limited to $\mu$ words, in line with real-world systems. We
provide fast algorithms of two main flavors.
  First, we observe that many algorithms in the CONGEST model are
memory-intensive and do not work in $\mu$-CONGEST. A prime example of a family
of algorithms that use large memory is clique-listing algorithms. We show that
the memory issue that arises here cannot be resolved without incurring a cost
in the round complexity, by establishing a lower bound on the round complexity
of listing cliques in $\mu$-CONGEST. We introduce novel techniques to overcome
these issues and generalize the algorithms to work within a given memory bound.
Combined with our lower bound, these provide tight tradeoffs between the
running time and memory of nodes.
  Second, we show that it is possible to efficiently simulate various families
of streaming algorithms in $\mu$-CONGEST. These include fast simulations of
$p$-pass algorithms, random order streams, and various types of mergeable
streaming algorithms.
  Combining our contributions, we show that we can use streaming algorithms to
efficiently generate statistics regarding combinatorial structures in the
network. An example of an end result of this type is that we can efficiently
identify and provide the per-color frequencies of the frequent monochromatic
triangles in $\mu$-CONGEST.

</details>


### [104] [A retrospective on DISPEED -- Leveraging heterogeneity in a drone swarm for IDS execution](https://arxiv.org/abs/2506.11800)
*Vincent Lannurien,Camélia Slimani,Louis Morge-Rollet,Laurent Lemarchand,David Espes,Frédéric Le Roy,Jalil Boukhobza*

Main category: cs.DC

TL;DR: 无人机群的自主性和任务效率提升，但面临网络安全威胁。DISPEED项目通过利用无人机异构性部署轻量级NIDS，包括性能评估和策略选择两个阶段，成果已发表多篇论文。


<details>
  <summary>Details</summary>
Motivation: 无人机群面临安全威胁，传统NIDS因资源消耗大难以部署，需开发适合异构平台的轻量级解决方案。

Method: 项目分为两阶段：1. 在不同嵌入式平台上评估36种IDS实现；2. 设计独立和分布式策略，根据上下文选择最优NIDS。

Result: 在树莓派、Jetson Xavier和Pynq-Z2平台上验证了36种IDS，并提出了策略选择方法。项目成果已发表于国际会议和期刊。

Conclusion: DISPEED项目成功解决了无人机群部署NIDS的挑战，为异构平台提供了轻量级安全方案。

Abstract: Swarms of drones are gaining more and more autonomy and efficiency during
their missions. However, security threats can disrupt their missions'
progression. To overcome this problem, Network Intrusion Detection Systems
((N)IDS) are promising solutions to detect malicious behavior on network
traffic. However, modern NIDS rely on resource-hungry machine learning
techniques, that can be difficult to deploy on a swarm of drones. The goal of
the DISPEED project is to leverage the heterogeneity (execution platforms,
memory) of the drones composing a swarm to deploy NIDS. It is decomposed in two
phases: (1) a characterization phase that consists in characterizing various
IDS implementations on diverse embedded platforms, and (2) an IDS
implementation mapping phase that seeks to develop selection strategies to
choose the most relevant NIDS depending on the context. On the one hand, the
characterization phase allowed us to identify 36 relevant IDS implementations
on three different embedded platforms: a Raspberry Pi 4B, a Jetson Xavier, and
a Pynq-Z2. On the other hand, the IDS implementation mapping phase allowed us
to design both standalone and distributed strategies to choose the best NIDSs
to deploy depending on the context. The results of the project have led to
three publications in international conferences, and one publication in a
journal.

</details>


### [105] [Secure API-Driven Research Automation to Accelerate Scientific Discovery](https://arxiv.org/abs/2506.11950)
*Tyler J. Skluzacek,Paul Bryant,A. J. Ruckman,Daniel Rosendo,Suzanne Prentice,Michael J. Brim,Ryan Adamson,Sarp Oral,Mallikarjun Shankar,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: S3M是一种通过服务网格架构整合实时流、智能工作流编排和细粒度授权的框架，旨在加速科学研究并保障安全性。


<details>
  <summary>Details</summary>
Motivation: 传统科学计算中存在研究者、计算资源和实验设施之间的壁垒，S3M旨在消除这些障碍，加速实验生命周期。

Method: 通过服务网格架构整合实时流、智能工作流编排和细粒度授权，动态提供资源并执行复杂工作流。

Result: S3M实现了对高性能计算的程序化访问，提升AI增强自主科学的潜力。

Conclusion: S3M标志着科学计算基础设施的新时代，消除了传统壁垒，推动了科研的自动化与高效性。

Abstract: The Secure Scientific Service Mesh (S3M) provides API-driven infrastructure
to accelerate scientific discovery through automated research workflows. By
integrating near real-time streaming capabilities, intelligent workflow
orchestration, and fine-grained authorization within a service mesh
architecture, S3M revolutionizes programmatic access to high performance
computing (HPC) while maintaining uncompromising security. This framework
allows intelligent agents and experimental facilities to dynamically provision
resources and execute complex workflows, accelerating experimental lifecycles,
and unlocking the full potential of AI-augmented autonomous science. S3M
signals a new era in scientific computing infrastructure that eliminates
traditional barriers between researchers, computational resources, and
experimental facilities.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [106] [Jelly: a fast and convenient RDF serialization format](https://arxiv.org/abs/2506.11298)
*Piotr Sowinski,Karolina Bogacka,Anastasiya Danilenka,Nikita Kozlov*

Main category: cs.DB

TL;DR: Jelly是一种高效的二进制RDF序列化格式，解决了现有格式性能不足、压缩率低和缺乏流支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RDF序列化格式（如Turtle、N-Triples和JSON-LD）在性能、压缩比及流支持方面存在局限性，亟需改进。

Method: 基于Protocol Buffers设计Jelly，支持批量和流式处理，优化序列化吞吐量、文件大小和计算资源使用，并提供开源实现和工具。

Result: Jelly提升了效率，支持现代编程语言与RDF库的集成，并通过具体用例展示了其实用性。

Conclusion: Jelly结合实用性与高效性，是语义网工具栈的重要贡献。

Abstract: Existing RDF serialization formats such as Turtle, N-Triples, and JSON-LD are
widely used for communication and storage in knowledge graph and Semantic Web
applications. However, they suffer from limitations in performance, compression
ratio, and lack of native support for RDF streams. To address these
shortcomings, we introduce Jelly, a fast and convenient binary serialization
format for RDF data that supports both batch and streaming use cases. Jelly is
designed to maximize serialization throughput, reduce file size with
lightweight streaming compression, and minimize compute resource usage. Built
on Protocol Buffers, Jelly is easy to integrate with modern programming
languages and RDF libraries. To maximize reusability, Jelly has an open
protocol specification, open-source implementations in Java and Python
integrated with popular RDF libraries, and a versatile command-line tool. To
illustrate its usefulness, we outline concrete use cases where Jelly can
provide tangible benefits. By combining practical usability with
state-of-the-art efficiency, Jelly is an important contribution to the Semantic
Web tool stack.

</details>


### [107] [OCPQ: Object-Centric Process Querying & Constraints](https://arxiv.org/abs/2506.11541)
*Aaron Küsters,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 本文提出了一种名为OCPQ的新型对象中心流程查询方法，解决了传统流程挖掘技术不适用于对象中心事件数据的问题，支持广泛的应用场景，并在性能和易用性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统流程挖掘技术基于案例中心事件数据，无法适用于对象中心过程挖掘（OCPM），因此需要开发新的查询和约束检查方法。

Method: 提出OCPQ方法，支持对象中心事件数据的查询和约束检查，包括高性能执行引擎和易用编辑器。

Result: 在真实数据集上的评估显示，OCPQ在表达能力和运行时性能上优于传统方案（如SQLite和Neo4j），与性能优化的DuckDB相当。

Conclusion: OCPQ是一种高度表达力的对象中心流程查询方法，具有实际应用价值和优越性能。

Abstract: Process querying is used to extract information and insights from process
execution data. Similarly, process constraints can be checked against input
data, yielding information on which process instances violate them.
Traditionally, such process mining techniques use case-centric event data as
input. However, with the uptake of Object-Centric Process Mining (OCPM),
existing querying and constraint checking techniques are no longer applicable.
Object-Centric Event Data (OCED) removes the requirement to pick a single case
notion (i.e., requiring that events belong to exactly one case) and can thus
represent many real-life processes much more accurately. In this paper, we
present a novel highly-expressive approach for object-centric process querying,
called OCPQ. It supports a wide variety of applications, including OCED-based
constraint checking and filtering. The visual representation of nested queries
in OCPQ allows users to intuitively read and create queries and constraints. We
implemented our approach using (1) a high-performance execution engine backend
and (2) an easy-to-use editor frontend. Additionally, we evaluated our approach
on a real-life dataset, showing the lack in expressiveness of prior work and
runtime performance significantly better than the general querying solutions
SQLite and Neo4j, as well as comparable to the performance-focused DuckDB.

</details>


### [108] [LLM-based Dynamic Differential Testing for Database Connectors with Reinforcement Learning-Guided Prompt Selection](https://arxiv.org/abs/2506.11870)
*Ce Lyu,Minghao Zhao,Yanhao Wang,Liang Jie*

Main category: cs.DB

TL;DR: 提出了一种基于强化学习（RL）引导的LLM测试用例生成方法，用于检测数据库连接器漏洞。


<details>
  <summary>Details</summary>
Motivation: 数据库连接器的安全漏洞常被忽视，传统模糊测试和LLM生成测试用例方法难以有效检测。

Method: 通过参数化提示模板为LLM提供领域知识，结合动态差异测试和RL优化提示选择，最大化控制流覆盖率。

Result: 在MySQL和OceanBase的JDBC连接器上发现了16个漏洞，其中10个被官方确认。

Conclusion: RL引导的LLM测试方法能有效检测连接器漏洞，弥补传统方法的不足。

Abstract: Database connectors are critical components enabling applications to interact
with underlying database management systems (DBMS), yet their security
vulnerabilities often remain overlooked. Unlike traditional software defects,
connector vulnerabilities exhibit subtle behavioral patterns and are inherently
challenging to detect. Besides, nonstandardized implementation of connectors
leaves potential risks (a.k.a. unsafe implementations) but is more elusive. As
a result, traditional fuzzing methods are incapable of finding such
vulnerabilities. Even for LLM-enable test case generation, due to a lack of
domain knowledge, they are also incapable of generating test cases that invoke
all interface and internal logic of connectors. In this paper, we propose
reinforcement learning (RL)-guided LLM test-case generation for database
connector testing. Specifically, to equip the LLM with sufficient and
appropriate domain knowledge, a parameterized prompt template is composed which
can be utilized to generate numerous prompts. Test cases are generated via LLM
with a prompt, and are dynamically evaluated through differential testing
across multiple connectors. The testing is iteratively conducted, with each
round RL is adopted to select optimal prompt based on prior-round behavioral
feedback, so as to maximize control flow coverage. We implement aforementioned
methodology in a practical tool and evaluate it on two widely used JDBC
connectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported
16 bugs, among them 10 are officially confirmed and the rest are acknowledged
as unsafe implementations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [109] [Design and Implementation of Washing Machine HUD Using FPGAs](https://arxiv.org/abs/2506.11287)
*Norman Stites,D. G. Perera*

Main category: cs.AR

TL;DR: 该项目通过FPGA开发板模拟家用洗衣机控制器，重点是图形显示功能。


<details>
  <summary>Details</summary>
Motivation: 将数字设计教育的理论与实践结合，通过实际项目增强学习效果。

Method: 使用Xilinx Spartan-3E开发板，设计硬件模拟洗衣机控制器，并结合VGA接口实现图形显示。

Result: 成功开发出能够实时显示洗衣机操作状态和循环选择的硬件控制系统。

Conclusion: FPGA项目是数字设计教育中理论与实践结合的有效工具。

Abstract: In contemporary digital design education, practical field programmable gate
array (FPGA) projects are indispensable for bridging theoretical concepts with
real-world applications. This project focuses on developing a hardware-based
simulation of a domestic washing machine controller using the Xilinx Spartan-3E
development board. A critical component of the design is the graphical heads-up
display (HUD), which renders real-time information about the machine's
operational state and cycle selections via a VGA interface.

</details>


### [110] [A4: Microarchitecture-Aware LLC Management for Datacenter Servers with Emerging I/O Devices](https://arxiv.org/abs/2506.11329)
*Haneul Park,Jiaqi Lou,Sangjin Lee,Yifan Yuan,Kyoung Soo Park,Yongseok Son,Ipoom Jeong,Nam Sung Kim*

Main category: cs.AR

TL;DR: 本文研究了现代服务器CPU中LLC的隐藏问题，提出了一种运行时管理框架\design，显著提升了高优先级工作负载的性能。


<details>
  <summary>Details</summary>
Motivation: 探索Intel Xeon CPU中因高带宽I/O设备引发的LLC竞争问题，尤其是此前未被发现的两类隐藏竞争（C1和C2）。

Method: 通过分析微架构特性，提出\design框架，利用隐藏的硬件功能动态管理LLC资源。

Result: \design显著提升了高优先级任务的性能（51%），同时对低优先级任务影响较小。

Conclusion: \design有效解决了LLC竞争问题，为多工作负载环境提供了优化方案。

Abstract: In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim
cache for higher-level private caches but also as a buffer for low-latency DMA
transfers between CPU cores and I/O devices through Direct Cache Access (DCA).
However, prior work has shown that high-bandwidth network-I/O devices can
rapidly flood the LLC with packets, often causing significant contention with
co-running workloads. One step further, this work explores hidden
microarchitectural properties of the Intel Xeon CPUs, uncovering two previously
unrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.
Specifically, (C1) DMA-written cache lines in LLC ways designated for DCA
(referred to as DCA ways) are migrated to certain LLC ways (denoted as
inclusive ways) when accessed by CPU cores, unexpectedly contending with
non-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth
storage-I/O devices, which are increasingly common in datacenter servers,
benefit little from DCA while contending with (latency-sensitive) network-I/O
devices within DCA ways. To this end, we present \design, a runtime LLC
management framework designed to alleviate both (C1) and (C2) among diverse
co-running workloads, using a hidden knob and other hardware features
implemented in those CPUs. Additionally, we demonstrate that \design can also
alleviate other previously known network-I/O-driven LLC contentions. Overall,
it improves the performance of latency-sensitive, high-priority workloads by
51\% without notably compromising that of low-priority workloads.

</details>


### [111] [DPUV4E: High-Throughput DPU Architecture Design for CNN on Versal ACAP](https://arxiv.org/abs/2506.11441)
*Guoyu Li,Pengbo Zheng,Jian Weng,Enshan Yang*

Main category: cs.AR

TL;DR: 论文提出了一种名为DPUV4E的新设计，针对AMD Versal ACAP架构，通过优化计算单元和数据流，显著提升了性能与能效比。


<details>
  <summary>Details</summary>
Motivation: 传统FPGA在性能和灵活性之间存在资源限制，而AMD Versal ACAP架构的AI Engines虽具备高计算能力，却受限于内存带宽不足。因此，需要一种新的设计来充分发挥其潜力。

Method: 设计了两种计算单元（Conv PE和DWC PE）以支持不同计算模式，并通过高效数据流减少带宽瓶颈。同时，扩展了PE的功能以利用AIEs执行非卷积操作。

Result: 实验表明，新设计在能效比（TOPS/W）上提升了8.6倍，资源占用显著降低（DSP减少95.8%，LUT减少44.7%），并缩短了延迟。端到端推理吞吐量提升了1.3至2.2倍。

Conclusion: DPUV4E设计成功解决了AMD Versal ACAP架构的内存带宽问题，显著提升了性能和能效，适用于广泛的计算机视觉应用。

Abstract: Convolutional Neural Networks (CNNs) remain prevalent in computer vision
applications, and FPGAs, known for their flexibility and energy efficiency,
have become essential components in heterogeneous acceleration systems.
However, traditional FPGAs face challenges in balancing performance and
versatility due to limited on-chip resources. AMD's Versal ACAP architecture,
tailored for AI applications, incorporates AI Engines (AIEs) to deliver high
computational power. Nevertheless, the platform suffers from insufficient
memory bandwidth, hindering the full utilization of the AIEs' theoretical
performance. In this paper, we present DPUV4E for the Versal architecture,
providing configurations ranging from 2PE ($32.6$ TOPS) to 8PE ($131.0$ TOPS).
We design two computation units, Conv PE and DWC PE, to support different
computational patterns. Each computation unit's data flow efficiently utilizes
the data reuse opportunities to mitigate bandwidth bottlenecks. Additionally,
we extend the functionality of each PE to utilize AIEs for non-convolutional
operations, reducing resource overhead. Experiments on over 50 models show that
compared to previous designs, our design provides $8.6\times$ the TOPS/W of
traditional FPGA-based DPU designs, while reducing DSP usage by $95.8\%$, LUT
usage by $44.7\%$, and latency to $68.5\%$ under single-batch conditions. For
end-to-end inference, our design improving throughput by up to $2.2\times$ for
depth-wise convolution models and up to $1.3\times$ for standard models.

</details>


### [112] [Topology-Aware Virtualization over Inter-Core Connected Neural Processing Units](https://arxiv.org/abs/2506.11446)
*Dahu Feng,Erhu Feng,Dong Du,Pinjie Xu,Yubin Xia,Haibo Chen,Rong Zhao*

Main category: cs.AR

TL;DR: 本文介绍了vNPU，一种针对互联核心NPU的虚拟化设计，通过路由、内存虚拟化和最佳拓扑映射技术，显著提升了性能并降低了硬件成本。


<details>
  <summary>Details</summary>
Motivation: 现有的NPU虚拟化技术未考虑互联核心NPU的硬件拓扑问题，导致资源利用不足。

Method: vNPU提出三种技术：路由虚拟化、内存虚拟化和最佳拓扑映射，以优化资源分配和性能。

Result: 相比其他方法，vNPU性能提升2倍，硬件成本仅增加2%。

Conclusion: vNPU为互联核心NPU提供了高效虚拟化解决方案，平衡了资源利用与性能。

Abstract: With the rapid development of artificial intelligence (AI) applications, an
emerging class of AI accelerators, termed Inter-core Connected Neural
Processing Units (NPU), has been adopted in both cloud and edge computing
environments, like Graphcore IPU, Tenstorrent, etc. Despite their innovative
design, these NPUs often demand substantial hardware resources, leading to
suboptimal resource utilization due to the imbalance of hardware requirements
across various tasks. To address this issue, prior research has explored
virtualization techniques for monolithic NPUs, but has neglected inter-core
connected NPUs with the hardware topology.
  This paper introduces vNPU, the first comprehensive virtualization design for
inter-core connected NPUs, integrating three novel techniques: (1) NPU route
virtualization, which redirects instruction and data flow from virtual NPU
cores to physical ones, creating a virtual topology; (2) NPU memory
virtualization, designed to minimize translation stalls for SRAM-centric and
NoC-equipped NPU cores, thereby maximizing the memory bandwidth; and (3)
Best-effort topology mapping, which determines the optimal mapping from all
candidate virtual topologies, balancing resource utilization with end-to-end
performance. We have developed a prototype of vNPU on both an FPGA platform
(Chipyard+FireSim) and a simulator (DCRA). Evaluation results indicate that,
compared to other virtualization approaches such as unified virtual memory and
MIG, vNPU achieves up to a 2x performance improvement across various ML models,
with only 2% hardware cost.

</details>


### [113] [FractalSync: Lightweight Scalable Global Synchronization of Massive Bulk Synchronous Parallel AI Accelerators](https://arxiv.org/abs/2506.11668)
*Victor Isachi,Alessandro Nadalini,Riccardo Fiorani Gallotta,Angelo Garofalo,Francesco Conti,Davide Rossi*

Main category: cs.AR

TL;DR: 提出了FractalSync，一种用于BSP系统的硬件加速同步机制，解决了大规模异构多核平台中处理元素同步的挑战。


<details>
  <summary>Details</summary>
Motivation: 随技术进步放缓及AI工作负载增加，计算机架构师需利用并行化和硬件加速提升性能，但同步处理元素是一大挑战。

Method: 在MAGIA（一种可扩展的基于瓦片的AI加速器）中集成了FractalSync，研究了其在不同规模瓦片网格上的可扩展性。

Result: 相比基于软件原子内存操作的同步方案，FractalSync同步速度提升高达43倍，且面积开销极小（<0.01%）。

Conclusion: FractalSync在1GHz频率下实现了时序闭合，是一种高效的硬件加速同步方案。

Abstract: The slow-down of technology scaling and the emergence of Artificial
Intelligence (AI) workloads have led computer architects to increasingly
exploit parallelization coupled with hardware acceleration to keep pushing the
performance envelope. However, this solution comes with the challenge of
synchronization of processing elements (PEs) in massive heterogeneous many-core
platforms. To address this challenge, we propose FractalSync, a hardware
accelerated synchronization mechanism for Bulk Synchronous Parallel (BSP)
systems. We integrate FractalSync in MAGIA, a scalable tile-based AI
accelerator, with each tile featuring a RISC-V-coupled matrix-multiplication
(MatMul) accelerator, scratchpad memory (SPM), and a DMA connected to a global
mesh Network-on-Chip (NoC). We study the scalability of the proposed barrier
synchronization scheme on tile meshes ranging from 2x2 PEs to 16x16 PEs to
evaluate its design boundaries. Compared to a synchronization scheme based on
software atomic memory operations (AMOs), the proposed solution achieves up to
43x speedup on synchronization, introducing a negligible area overhead
(<0.01%). FractalSync closes timing at MAGIA's target 1GHz frequency.

</details>


### [114] [Real-World Deployment of a Lane Change Prediction Architecture Based on Knowledge Graph Embeddings and Bayesian Inference](https://arxiv.org/abs/2506.11925)
*M. Manzour,Catherine M. Elias,Omar M. Shehata,R. Izquierdo,M. A. Sotelo*

Main category: cs.AR

TL;DR: 该论文提出了一种基于知识图谱嵌入（KGE）和贝叶斯推断的车道变换预测系统，通过实际硬件验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有车道变换预测研究多局限于仿真或数据集，缺乏实际部署验证，本文旨在填补这一空白。

Method: 采用感知模块提取环境特征并转换为语言类别，预测模块结合KGE和贝叶斯推断预测目标车辆行为，并通过纵向制动确保安全。

Result: 实验显示系统能提前3到4秒预测车道变换，为目标车辆和自车提供充足反应时间。

Conclusion: 该系统在实际硬件上实现了高效的车道变换预测和安全性保障。

Abstract: Research on lane change prediction has gained a lot of momentum in the last
couple of years. However, most research is confined to simulation or results
obtained from datasets, leaving a gap between algorithmic advances and on-road
deployment. This work closes that gap by demonstrating, on real hardware, a
lane-change prediction system based on Knowledge Graph Embeddings (KGEs) and
Bayesian inference. Moreover, the ego-vehicle employs a longitudinal braking
action to ensure the safety of both itself and the surrounding vehicles. Our
architecture consists of two modules: (i) a perception module that senses the
environment, derives input numerical features, and converts them into
linguistic categories; and communicates them to the prediction module; (ii) a
pretrained prediction module that executes a KGE and Bayesian inference model
to anticipate the target vehicle's maneuver and transforms the prediction into
longitudinal braking action. Real-world hardware experimental validation
demonstrates that our prediction system anticipates the target vehicle's lane
change three to four seconds in advance, providing the ego vehicle sufficient
time to react and allowing the target vehicle to make the lane change safely.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [115] [The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI](https://arxiv.org/abs/2506.11015)
*Barbara Oakley,Michael Johnston,Ken-Zen Chen,Eulho Jung,Terrence J. Sejnowski*

Main category: cs.CY

TL;DR: 本文探讨AI工具过度依赖对人类认知记忆系统的潜在负面影响，强调强健内部模型的重要性及其政策意义。


<details>
  <summary>Details</summary>
Motivation: 研究在AI普遍应用的背景下，人类记忆系统可能因外部工具依赖而退化的结构性矛盾。

Method: 结合神经科学与认知心理学理论，分析AI工具如何干扰记忆巩固过程，并通过实证研究验证影响。

Result: 发现AI工具依赖会削弱长期记忆和直觉掌握能力，强调内部模型对AI输出的评估与指导作用。

Conclusion: 提出教育和工作培训中应培养强健内部认知模型，以优化人类与AI的协作。

Abstract: In the age of generative AI and ubiquitous digital tools, human cognition
faces a structural paradox: as external aids become more capable, internal
memory systems risk atrophy. Drawing on neuroscience and cognitive psychology,
this paper examines how heavy reliance on AI systems and discovery-based
pedagogies may impair the consolidation of declarative and procedural memory --
systems essential for expertise, critical thinking, and long-term retention. We
review how tools like ChatGPT and calculators can short-circuit the retrieval,
error correction, and schema-building processes necessary for robust neural
encoding. Notably, we highlight striking parallels between deep learning
phenomena such as "grokking" and the neuroscience of overlearning and
intuition. Empirical studies are discussed showing how premature reliance on AI
during learning inhibits proceduralization and intuitive mastery. We argue that
effective human-AI interaction depends on strong internal models -- biological
"schemata" and neural manifolds -- that enable users to evaluate, refine, and
guide AI output. The paper concludes with policy implications for education and
workforce training in the age of large language models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [116] [Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification](https://arxiv.org/abs/2506.11036)
*Yang Qin,Chao Chen,Zhihang Fu,Dezhong Peng,Xi Peng,Peng Hu*

Main category: cs.LG

TL;DR: 提出了一种交互式跨模态学习框架（ICL），通过人机交互和多模态学习提升文本到图像人物重识别的准确性，并引入数据增强策略优化训练文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像人物重识别方法因网络结构和数据质量的限制，难以区分复杂候选图像，需通过交互和多模态知识增强查询意图对齐。

Method: 提出ICL框架，包含Test-time Humane-centered Interaction（THI）模块和多轮交互的MLLM模型，以及Reorganization Data Augmentation（RDA）数据增强策略。

Result: 在四个基准数据集上的实验表明，该方法性能显著提升。

Conclusion: ICL通过交互和数据增强显著提升了文本到图像人物重识别的效果。

Abstract: Despite remarkable advancements in text-to-image person re-identification
(TIReID) facilitated by the breakthrough of cross-modal embedding models,
existing methods often struggle to distinguish challenging candidate images due
to intrinsic limitations, such as network architecture and data quality. To
address these issues, we propose an Interactive Cross-modal Learning framework
(ICL), which leverages human-centered interaction to enhance the
discriminability of text queries through external multimodal knowledge. To
achieve this, we propose a plug-and-play Test-time Humane-centered Interaction
(THI) module, which performs visual question answering focused on human
characteristics, facilitating multi-round interactions with a multimodal large
language model (MLLM) to align query intent with latent target images.
Specifically, THI refines user queries based on the MLLM responses to reduce
the gap to the best-matching images, thereby boosting ranking accuracy.
Additionally, to address the limitation of low-quality training texts, we
introduce a novel Reorganization Data Augmentation (RDA) strategy based on
information enrichment and diversity enhancement to enhance query
discriminability by enriching, decomposing, and reorganizing person
descriptions. Extensive experiments on four TIReID benchmarks, i.e.,
CUHK-PEDES, ICFG-PEDES, RSTPReid, and UFine6926, demonstrate that our method
achieves remarkable performance with substantial improvement.

</details>


### [117] [Large Language models for Time Series Analysis: Techniques, Applications, and Challenges](https://arxiv.org/abs/2506.11040)
*Feifei Shi,Xueyan Yin,Kang Wang,Wanyu Tu,Qifu Sun,Huansheng Ning*

Main category: cs.LG

TL;DR: LLMs在时间序列分析中展现出跨模态知识和注意力机制的优势，但由于数据多样性、标注稀缺和计算需求，通用LLMs的开发仍受限。本文系统回顾了预训练LLMs驱动的时间序列分析，梳理技术路线和应用挑战，为未来发展提供方向。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析方法在非线性特征表示和长期依赖捕捉方面受限，而LLMs的跨模态知识和注意力机制为时间序列分析带来新潜力。本文旨在系统回顾和总结这一新兴领域的技术、应用及挑战。

Method: 1. 建立AI驱动的时间序列分析发展路线图；2. 从输入、优化和轻量化三阶段梳理技术；3. 分析实际应用和开放挑战。

Result: 揭示了LLMs驱动的时间序列分析的技术框架、应用潜力和关键挑战，为未来研究奠定了基础。

Conclusion: 本文为学术界和工业界提供了LLMs驱动时间序列分析的全面参考，推动更高效、泛化和可解释系统的开发。

Abstract: Time series analysis is pivotal in domains like financial forecasting and
biomedical monitoring, yet traditional methods are constrained by limited
nonlinear feature representation and long-term dependency capture. The
emergence of Large Language Models (LLMs) offers transformative potential by
leveraging their cross-modal knowledge integration and inherent attention
mechanisms for time series analysis. However, the development of
general-purpose LLMs for time series from scratch is still hindered by data
diversity, annotation scarcity, and computational requirements. This paper
presents a systematic review of pre-trained LLM-driven time series analysis,
focusing on enabling techniques, potential applications, and open challenges.
First, it establishes an evolutionary roadmap of AI-driven time series
analysis, from the early machine learning era, through the emerging LLM-driven
paradigm, to the development of native temporal foundation models. Second, it
organizes and systematizes the technical landscape of LLM-driven time series
analysis from a workflow perspective, covering LLMs' input, optimization, and
lightweight stages. Finally, it critically examines novel real-world
applications and highlights key open challenges that can guide future research
and innovation. The work not only provides valuable insights into current
advances but also outlines promising directions for future development. It
serves as a foundational reference for both academic and industrial
researchers, paving the way for the development of more efficient,
generalizable, and interpretable systems of LLM-driven time series analysis.

</details>


### [118] [From Reasoning to Code: GRPO Optimization for Underrepresented Languages](https://arxiv.org/abs/2506.11027)
*Federico Pennino,Bianca Raimondi,Massimo Rondelli,Andrea Gurioli,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: 该论文提出了一种通过结合小规模代码模型和GRPO方法，为训练数据有限的编程语言（如Prolog）生成高质量可执行代码的通用方法。


<details>
  <summary>Details</summary>
Motivation: 解决因训练数据不足而难以使用大型语言模型为小众编程语言生成准确可执行代码的问题。

Method: 使用Qwen 2.5的小规模代码模型结合GRPO方法，通过显式推理步骤生成代码，并在强化学习循环中整合推理驱动的反馈。

Result: 实验证明，该方法在数学逻辑问题基准测试中显著提高了代码的准确性、逻辑正确性和推理质量。

Conclusion: 该方法为缺乏大规模训练资源的编程语言提供了一种有效的代码生成解决方案，具有广泛的应用潜力。

Abstract: Generating accurate and executable code using large language models (LLMs) is
challenging for languages with limited public training data compared to popular
languages such as Python. This paper introduces a generalizable approach that
uses small-scale code versions of the Qwen 2.5 model combined with Group
Relative Policy Optimization (GRPO) to enable effective code generation through
explicit reasoning steps, which is particularly beneficial for languages with
smaller source code databases. Using Prolog as a representative use case --
given its limited online presence -- the initial model faced challenges in
generating executable code. After some training steps, the model successfully
produces logically consistent and syntactically accurate code by directly
integrating reasoning-driven feedback into the reinforcement learning loop.
Experimental evaluations using mathematical logic problem benchmarks illustrate
significant improvements in reasoning quality, code accuracy, and logical
correctness, underscoring the potential of this approach to benefit a wide
range of programming languages lacking extensive training resources.

</details>


### [119] [Data Science: a Natural Ecosystem](https://arxiv.org/abs/2506.11010)
*Emilio Porcu,Roy El Moukari,Laurent Najman,Francisco Herrera,Horst Simon*

Main category: cs.LG

TL;DR: 该论文提出了一种整体性的数据科学视角，强调数据科学的生态系统中存在的挑战与使命，并提出了评估数据发现有用性的方法以避免理论与实践的分歧。


<details>
  <summary>Details</summary>
Motivation: 探讨数据科学的本质及其在多维复杂性（数据结构、领域、基数、因果性和伦理）与数据生命周期阶段中的自然生态，同时关注计算与基础数据科学之间的潜在分歧。

Method: 提出了一种基于数据代理和逻辑组织的数据科学框架，通过定义特定学科驱动的数据科学与泛数据科学来整合学科与基本数据科学。

Result: 指出计算与基础数据科学可能存在的分歧，并提出了评估数据发现有用性的方法以减少这种分歧。

Conclusion: 强调了评估数据发现的有用性对统一数据科学理论与实践的重要性，以促进数据科学的健康发展。

Abstract: This manuscript provides a holistic (data-centric) view of what we term
essential data science, as a natural ecosystem with challenges and missions
stemming from the data universe with its multiple combinations of the 5D
complexities (data structure, domain, cardinality, causality, and ethics) with
the phases of the data life cycle. Data agents perform tasks driven by specific
goals. The data scientist is an abstract entity that comes from the logical
organization of data agents with their actions. Data scientists face challenges
that are defined according to the missions. We define specific
discipline-induced data science, which in turn allows for the definition of
pan-data science, a natural ecosystem that integrates specific disciplines with
the essential data science. We semantically split the essential data science
into computational, and foundational. We claim that there is a serious threat
of divergence between computational and foundational data science. Especially,
if no approach is taken to rate whether a data universe discovery should be
useful or not. We suggest that rigorous approaches to measure the usefulness of
data universe discoveries might mitigate such a divergence.

</details>


### [120] [Not All Clients Are Equal: Personalized Federated Learning on Heterogeneous Multi-Modal Clients](https://arxiv.org/abs/2506.11024)
*Minhyuk Seo,Taeheon Kim,Hankook Lee,Jonghyun Choi,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 该论文探讨了如何在个性化联邦学习（PFL）中处理数据和模型异质性，提出了一种任务相似性感知的模型聚合方法和维度不变模块以提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决集中式训练的隐私和传输成本问题，个性化联邦学习（PFL）提供了一种分布式替代方案，但其在现实场景中的数据和模型异质性未得到充分研究。

Method: 提出任务相似性感知的模型聚合方法处理数据异质性，以及维度不变模块解决模型异质性。

Result: 实验证明该方法在40个多模态PFL任务中表现优异，优于现有技术。

Conclusion: 该方法在PFL中有效应对了现实场景中的异质性挑战，提升了模型的个性化和泛化能力。

Abstract: Foundation models have shown remarkable capabilities across diverse
multi-modal tasks, but their centralized training raises privacy concerns and
induces high transmission costs. In contrast, federated learning (FL) offers a
distributed alternative without the need to share data. Recently, for the
growing demand for personalizing AI models for different user purposes,
personalized federated learning (PFL) has emerged. PFL allows each client to
leverage the knowledge of other clients for further adaptation to individual
user preferences, again without the need to share data. Despite its potential,
most PFL studies remain confined to simulated environments, overlooking the
data and model heterogeneity that arise in real-world scenarios. In contrast,
we first consider large data heterogeneity, evaluating on a new benchmark for
multi-modal PFL, spanning 40 distinct tasks with realistic data distribution
shifts. We then consider model heterogeneity in that we do not assume that all
clients share similar model architectures. To address data heterogeneity, we
propose a task-similarity-aware model aggregation method that provides
customized global models to each client. For model heterogeneity, we propose a
dimension-invariant module that enables knowledge sharing across heterogeneous
models. Empirical validations demonstrate that the proposed approach
outperforms the state-of-the-art, excelling in both personalization and
generalization capabilities.

</details>


### [121] [An Active Learning-Based Streaming Pipeline for Reduced Data Training of Structure Finding Models in Neutron Diffractometry](https://arxiv.org/abs/2506.11100)
*Tianle Wang,Jorge Ramirez,Cristina Garcia-Cardona,Thomas Proffen,Shantenu Jha,Sudip K. Seal*

Main category: cs.LG

TL;DR: 论文提出了一种基于主动学习的批次模式策略，显著减少中子衍射结构分析所需的训练数据量，同时提升准确性，并设计了一种高效流式训练流程。


<details>
  <summary>Details</summary>
Motivation: 中子衍射结构分析计算成本高，传统方法耗时较长，而现有机器学习方法需要大量模拟数据，数据量随结构参数增长而指数级增加。

Method: 引入了一种基于不确定性采样的批次模式主动学习策略，优先选择模型最不确定的标记数据进行训练，并设计了高效的流式训练流程。

Result: 实验表明，该方法减少约75%的训练数据需求，同时提高准确性；流式训练流程在异构平台上缩短20%训练时间，且无准确率损失。

Conclusion: 提出的主动学习策略和流式训练流程显著降低了中子衍射结构分析的训练成本和时间，为相关领域提供了高效解决方案。

Abstract: Structure determination workloads in neutron diffractometry are
computationally expensive and routinely require several hours to many days to
determine the structure of a material from its neutron diffraction patterns.
The potential for machine learning models trained on simulated neutron
scattering patterns to significantly speed up these tasks have been reported
recently. However, the amount of simulated data needed to train these models
grows exponentially with the number of structural parameters to be predicted
and poses a significant computational challenge. To overcome this challenge, we
introduce a novel batch-mode active learning (AL) policy that uses uncertainty
sampling to simulate training data drawn from a probability distribution that
prefers labelled examples about which the model is least certain. We confirm
its efficacy in training the same models with about 75% less training data
while improving the accuracy. We then discuss the design of an efficient
stream-based training workflow that uses this AL policy and present a
performance study on two heterogeneous platforms to demonstrate that, compared
with a conventional training workflow, the streaming workflow delivers about
20% shorter training time without any loss of accuracy.

</details>


### [122] [Developing a Dyslexia Indicator Using Eye Tracking](https://arxiv.org/abs/2506.11004)
*Kevin Cogan,Vuong M. Ngo,Mark Roantree*

Main category: cs.LG

TL;DR: 研究探讨了结合眼动追踪技术和机器学习算法作为经济高效的早期阅读障碍诊断方法，通过分析眼动模式，提出了一种改进的解决方案。


<details>
  <summary>Details</summary>
Motivation: 全球10%至20%的人口受到阅读障碍的影响，亟需创新的诊断方法。

Method: 利用随机森林分类器和层次聚类方法分析眼动模式（如注视时长和扫视异常）以检测阅读障碍及其严重程度。

Result: 随机森林分类器的准确率达到88.58%，并能识别不同程度阅读障碍。

Conclusion: 眼动追踪技术与机器学习的结合为阅读障碍诊断提供了高精度、非侵入性的解决方案。

Abstract: Dyslexia, affecting an estimated 10% to 20% of the global population,
significantly impairs learning capabilities, highlighting the need for
innovative and accessible diagnostic methods. This paper investigates the
effectiveness of eye-tracking technology combined with machine learning
algorithms as a cost-effective alternative for early dyslexia detection. By
analyzing general eye movement patterns, including prolonged fixation durations
and erratic saccades, we proposed an enhanced solution for determining
eye-tracking-based dyslexia features. A Random Forest Classifier was then
employed to detect dyslexia, achieving an accuracy of 88.58\%. Additionally,
hierarchical clustering methods were applied to identify varying severity
levels of dyslexia. The analysis incorporates diverse methodologies across
various populations and settings, demonstrating the potential of this
technology to identify individuals with dyslexia, including those with
borderline traits, through non-invasive means. Integrating eye-tracking with
machine learning represents a significant advancement in the diagnostic
process, offering a highly accurate and accessible method in clinical research.

</details>


### [123] [Perception-Driven Bias Detection in Machine Learning via Crowdsourced Visual Judgment](https://arxiv.org/abs/2506.11047)
*Chirudeep Tupakula,Rittika Shamsuddin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于众包人类判断的偏见检测新框架，通过非专家用户的视觉感知信号发现潜在的系统偏见，提供了一种标签高效且可解释的公平性评估方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在高风险领域部署时存在偏见问题，传统检测方法依赖敏感标签或僵化的公平性指标，适用性受限。本文旨在探索一种更灵活、可扩展的偏见检测方法。

Method: 开发了一个轻量级网络平台，展示简化后的数据可视化（如薪资分布），并收集用户对群体相似性的二元判断。通过分析用户的视觉感知信号（如布局、间距等），结合统计测试和机器学习交叉验证，检测潜在偏见。

Result: 研究发现，非专家用户的感知信号与已知偏见案例显著相关，表明视觉直觉可以作为公平性审计的有效代理。

Conclusion: 该方法为公平性诊断提供了一种无需标签依赖、易于理解的新思路，有望推动基于众包的偏见检测流程发展。

Abstract: Machine learning systems are increasingly deployed in high-stakes domains,
yet they remain vulnerable to bias systematic disparities that
disproportionately impact specific demographic groups. Traditional bias
detection methods often depend on access to sensitive labels or rely on rigid
fairness metrics, limiting their applicability in real-world settings. This
paper introduces a novel, perception-driven framework for bias detection that
leverages crowdsourced human judgment. Inspired by reCAPTCHA and other
crowd-powered systems, we present a lightweight web platform that displays
stripped-down visualizations of numeric data (for example-salary distributions
across demographic clusters) and collects binary judgments on group similarity.
We explore how users' visual perception-shaped by layout, spacing, and question
phrasing can signal potential disparities. User feedback is aggregated to flag
data segments as biased, which are then validated through statistical tests and
machine learning cross-evaluations. Our findings show that perceptual signals
from non-expert users reliably correlate with known bias cases, suggesting that
visual intuition can serve as a powerful, scalable proxy for fairness auditing.
This approach offers a label-efficient, interpretable alternative to
conventional fairness diagnostics, paving the way toward human-aligned,
crowdsourced bias detection pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [124] [LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic](https://arxiv.org/abs/2506.11221)
*Weibing Zheng,Laurah Turner,Jess Kropczynski,Murat Ozer,Tri Nguyen,Shane Halse*

Main category: cs.AI

TL;DR: 本文提出了一种结合模糊逻辑和大语言模型（LLM）的方法LLM-as-a-Fuzzy-Judge，用于自动化评估医学生的临床沟通技能，并使其更符合医生的主观判断。


<details>
  <summary>Details</summary>
Motivation: 临床沟通技能对医学教育至关重要，但大规模实践和评估这些技能具有挑战性。现有的LLM驱动的临床场景模拟虽有一定效果，但在提供符合医生主观判断的自动化评估方面仍存在困难。

Method: 通过数据收集、基于多维模糊集的数据标注、提示工程和监督微调（SFT）预训练的LLM，构建了LLM-as-a-Fuzzy-Judge模型。

Result: 结果表明，该模型在主要评估项目上准确率超过90%，整体准确率超过80%，能够提供可解释且符合人类偏好的评估。

Conclusion: 该方法证实了结合模糊逻辑和LLM的可行性，推动了医学教育中的自动化评估，支持更稳健的评估实践。

Abstract: Clinical communication skills are critical in medical education, and
practicing and assessing clinical communication skills on a scale is
challenging. Although LLM-powered clinical scenario simulations have shown
promise in enhancing medical students' clinical practice, providing automated
and scalable clinical evaluation that follows nuanced physician judgment is
difficult. This paper combines fuzzy logic and Large Language Model (LLM) and
proposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the
automated evaluation of medical students' clinical skills with subjective
physicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is
fine-tuned to evaluate medical students' utterances within student-AI patient
conversation scripts based on human annotations from four fuzzy sets, including
Professionalism, Medical Relevance, Ethical Behavior, and Contextual
Distraction. The methodology of this paper started from data collection from
the LLM-powered medical education system, data annotation based on
multidimensional fuzzy sets, followed by prompt engineering and the supervised
fine-tuning (SFT) of the pre-trained LLMs using these human annotations. The
results show that the LLM-as-a-Fuzzy-Judge achieves over 80\% accuracy, with
major criteria items over 90\%, effectively leveraging fuzzy logic and LLM as a
solution to deliver interpretable, human-aligned assessment. This work suggests
the viability of leveraging fuzzy logic and LLM to align with human
preferences, advances automated evaluation in medical education, and supports
more robust assessment and judgment practices. The GitHub repository of this
work is available at https://github.com/2sigmaEdTech/LLMAsAJudge

</details>


### [125] [Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task](https://arxiv.org/abs/2506.11986)
*Wuzhenghong Wen,Su Pan,yuwei Sun*

Main category: cs.AI

TL;DR: 论文提出了一种基于强化学习的Schema-R1模型，用于改进Text-to-SQL任务中的模式链接，通过构造高质量推理样本和规则强化学习，提升了模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前模式链接模型的微调方法过度依赖死记硬背，缺乏推理能力，难以获取高质量推理样本。

Method: Schema-R1通过构造高质量推理样本、监督微调初始化和规则强化学习三个步骤实现改进。

Result: 相较于现有方法，Schema-R1在过滤准确率上提升了10%。

Conclusion: Schema-R1有效提升了模式链接模型的推理能力，为Text-to-SQL任务提供了更好的解决方案。

Abstract: Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.

</details>


### [126] [Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning](https://arxiv.org/abs/2506.11376)
*Liying Wang,Ph. D.,Daffodil Carrington,M. S.,Daniil Filienko,M. S.,Caroline El Jazmi,M. S.,Serena Jinchen Xie,M. S.,Martine De Cock,Ph. D.,Sarah Iribarren,Ph. D.,Weichao Yuwen,Ph. D*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLM）驱动的对话代理在提供基于证据的心理健康支持（如PST、MI和BCA疗法）给家庭护理人员中的潜力。实验表明，结合Few-Shot和RAG提示技术的模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 家庭护理人员因角色复杂且资源有限而面临心理健康挑战，亟需有效支持。

Method: 通过28名护理人员与四种LLM配置交互的实验，评估共情和治疗联盟。

Result: 最佳模型能提升情境理解和个性化支持，参与者对其情感验证和策略提供表示认可。

Conclusion: LLM在提供共情和定制化支持方面具有潜力，但需平衡评估与建议效率。

Abstract: Family caregivers often face substantial mental health challenges due to
their multifaceted roles and limited resources. This study explored the
potential of a large language model (LLM)-powered conversational agent to
deliver evidence-based mental health support for caregivers, specifically
Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)
and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted
with 28 caregivers interacting with four LLM configurations to evaluate empathy
and therapeutic alliance. The best-performing models incorporated Few-Shot and
Retrieval-Augmented Generation (RAG) prompting techniques, alongside
clinician-curated examples. The models showed improved contextual understanding
and personalized support, as reflected by qualitative responses and
quantitative ratings on perceived empathy and therapeutic alliances.
Participants valued the model's ability to validate emotions, explore
unexpressed feelings, and provide actionable strategies. However, balancing
thorough assessment with efficient advice delivery remains a challenge. This
work highlights the potential of LLMs in delivering empathetic and tailored
support for family caregivers.

</details>


### [127] [OntoGSN: An Ontology for Dynamic Management of Assurance Cases](https://arxiv.org/abs/2506.11023)
*Tomas Bueno Momcilovic,Barbara Gallina,Ingmar Kessler,Dian Balta*

Main category: cs.AI

TL;DR: OntoGSN是一个基于GSN标准的本体论和中间件，用于管理保证案例（ACs），提供知识表示和可查询图，支持自动填充、评估和更新。


<details>
  <summary>Details</summary>
Motivation: 管理保证案例（ACs）需要大量精力，现有工具难以应对动态环境的变化，可能导致虚假信心。

Method: 提出OntoGSN，包括OWL本体、辅助工具、设计文档、SPARQL查询库和原型界面，严格遵循GSN标准并经过多种评估。

Result: OntoGSN能够动态管理ACs，并通过示例展示了其在大型语言模型对抗鲁棒性保证中的应用。

Conclusion: OntoGSN为动态环境下ACs的管理提供了有效工具，支持自动化维护和更新，具有实际应用价值。

Abstract: Assurance cases (ACs) are a common artifact for building and maintaining
confidence in system properties such as safety or robustness. Constructing an
AC can be challenging, although existing tools provide support in static,
document-centric applications and methods for dynamic contexts (e.g.,
autonomous driving) are emerging. Unfortunately, managing ACs remains a
challenge, since maintaining the embedded knowledge in the face of changes
requires substantial effort, in the process deterring developers - or worse,
producing poorly managed cases that instill false confidence. To address this,
we present OntoGSN: an ontology and supporting middleware for managing ACs in
the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge
representation and a queryable graph that can be automatically populated,
evaluated, and updated. Our contributions include: a 1:1 formalization of the
GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology
and parser for integration with a widely used AC tool; a repository and
documentation of design decisions for OntoGSN maintenance; a SPARQL query
library with automation patterns; and a prototypical interface. The ontology
strictly adheres to the standard's text and has been evaluated according to
FAIR principles, the OOPS framework, competency questions, and community
feedback. The development of other middleware elements is guided by the
community needs and subject to ongoing evaluations. To demonstrate the utility
of our contributions, we illustrate dynamic AC management in an example
involving assurance of adversarial robustness in large language models.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [128] [Decentralized Uplink Adaptive Compression for Cell-Free MIMO with Limited Fronthaul](https://arxiv.org/abs/2506.11284)
*Zehua Li,Jingjie Wei,Raviraj Adve*

Main category: cs.IT

TL;DR: 研究在有限前传容量下，小区多输入多输出网络的上行压缩问题，提出一种基于速率的自适应压缩方法，优于传统变换方法。


<details>
  <summary>Details</summary>
Motivation: 传统的变换矩阵方法在压缩高维信号时缺乏灵活性，无法适应网络流量和前传容量的变化。

Method: 提出基于速率的自适应压缩方法，结合互信息优化理论网络容量，并通过信道统计和用户流量密度高效生成全局信道状态信息的侧信息。

Result: 去中心化实现的自适应压缩方法在网络性能上与集中式方法相当，同时保持低信息交换开销。

Conclusion: 自适应压缩方法在灵活性和性能上优于传统方法，适合动态网络环境。

Abstract: We study the problem of uplink compression for cell-free multi-input
multi-output networks with limited fronthaul capacity. In compress-forward
mode, remote radio heads (RRHs) compress the received signal and forward it to
a central unit for joint processing. While previous work has focused on a
transform-based approach, which optimizes the transform matrix that reduces
signals of high dimension to a static pre-determined lower dimension, we
propose a rate-based approach that simultaneously finds both dimension and
compression adaptively. Our approach accommodates for changes to network
traffic and fronthaul limits. Using mutual information as the objective, we
obtain the theoretical network capacity for adaptive compression and decouple
the expression to enable decentralization. Furthermore, using channel
statistics and user traffic density, we show different approaches to compute an
efficient representation of side information that summarizes global channel
state information and is shared with RRHs to assist compression. While keeping
the information exchange overhead low, our decentralized implementation of
adaptive compression shows competitive overall network performance compared to
a centralized approach.

</details>


### [129] [Black-Box Edge AI Model Selection with Conformal Latency and Accuracy Guarantees](https://arxiv.org/abs/2506.11391)
*Anders E. Kalør,Tomoaki Ohtsuki*

Main category: cs.IT

TL;DR: 本文提出了一种新颖的黑盒模型选择框架，旨在满足预定义的延迟违规概率和预期损失要求，为6G中的边缘AI提供可靠实时服务。


<details>
  <summary>Details</summary>
Motivation: 6G应用（如自动驾驶和机器人技术）需要低延迟和高精度的边缘AI服务，但由于ML模型的黑盒性、任务复杂性及无线信道的随机性，实现这一目标具有挑战性。

Method: 利用共形风险控制和非参数统计，从一组不同复杂度和计算时间的黑盒特征提取与推理模型中智能选择最优组合。包括固定（依赖信道统计）和动态（信道自适应）两种模型选择方案。

Result: 数值实验验证了该框架在满足最大误分类概率要求的同时，完成了具有截止时间约束的图像分类任务。

Conclusion: 该框架有望为6G提供可靠的实时边缘AI服务。

Abstract: Edge artificial intelligence (AI) will be a central part of 6G, with powerful
edge servers supporting devices in performing machine learning (ML) inference.
However, it is challenging to deliver the latency and accuracy guarantees
required by 6G applications, such as automated driving and robotics. This stems
from the black-box nature of ML models, the complexities of the tasks, and the
interplay between transmitted data quality, chosen inference model, and the
random wireless channel. This paper proposes a novel black-box model selection
framework for reliable real-time wireless edge AI designed to meet predefined
requirements on both deadline violation probability and expected loss.
Leveraging conformal risk control and non-parametric statistics, our framework
intelligently selects the optimal model combination from a collection of
black-box feature-extraction and inference models of varying complexities and
computation times. We present both a fixed (relying on channel statistics) and
a dynamic (channel-adaptive) model selection scheme. Numerical results validate
the framework on a deadline-constrained image classification task while
satisfying a maximum misclassification probability requirement. These results
indicate that the proposed framework has the potential to provide reliable
real-time edge AI services in 6G.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [130] [Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models](https://arxiv.org/abs/2506.11521)
*Jinming Wen,Xinyi Wu,Shuai Zhao,Yanhao Jia,Yuwen Li*

Main category: cs.CR

TL;DR: 该论文综述了音频-视觉多模态大语言模型（MLLMs）的安全漏洞及攻击类型，填补了现有研究中缺乏统一全面分析的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在音频-视觉任务中表现出色，但其依赖第三方数据和开源模型带来了安全隐患，需要通过系统研究揭示安全漏洞。

Method: 论文对音频-视觉攻击（如对抗攻击、后门攻击和越狱攻击）进行全面系统回顾，并分析最新MLLMs的攻击类型。

Result: 研究发现MLLMs易受指令或输入操纵产生恶意内容，现有研究对攻击类型覆盖不足。

Conclusion: 论文总结了音频-视觉攻击的挑战与趋势，为未来防御研究提供了方向。

Abstract: Multimodal large language models (MLLMs), which bridge the gap between
audio-visual and natural language processing, achieve state-of-the-art
performance on several audio-visual tasks. Despite the superior performance of
MLLMs, the scarcity of high-quality audio-visual training data and
computational resources necessitates the utilization of third-party data and
open-source MLLMs, a trend that is increasingly observed in contemporary
research. This prosperity masks significant security risks. Empirical studies
demonstrate that the latest MLLMs can be manipulated to produce malicious or
harmful content. This manipulation is facilitated exclusively through
instructions or inputs, including adversarial perturbations and malevolent
queries, effectively bypassing the internal security mechanisms embedded within
the models. To gain a deeper comprehension of the inherent security
vulnerabilities associated with audio-visual-based multimodal models, a series
of surveys investigates various types of attacks, including adversarial and
backdoor attacks. While existing surveys on audio-visual attacks provide a
comprehensive overview, they are limited to specific types of attacks, which
lack a unified review of various types of attacks. To address this issue and
gain insights into the latest trends in the field, this paper presents a
comprehensive and systematic review of audio-visual attacks, which include
adversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this
paper also reviews various types of attacks in the latest audio-visual-based
MLLMs, a dimension notably absent in existing surveys. Drawing upon
comprehensive insights from a substantial review, this paper delineates both
challenges and emergent trends for future research on audio-visual attacks and
defense.

</details>


### [131] [Bhatt Conjectures: On Necessary-But-Not-Sufficient Benchmark Tautology for Human Like Reasoning](https://arxiv.org/abs/2506.11423)
*Manish Bhatt*

Main category: cs.CR

TL;DR: 摘要讨论了大型语言或推理模型（LLMs/LRMs）是真正推理还是模式匹配的辩论，并提出两个分析性基准来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过明确表达其思维模型，为解决LLMs/LRMs是否真正推理的争论提供清晰的分析框架。

Method: 作者提出两个“分析性”（即“重言式”）的基准，用于区分推理与模式匹配。

Result: 虽然未具体描述结果，但作者认为这两个基准能够穿透模糊性，提供明确的分析工具。

Conclusion: 作者总结认为，通过这两个基准可以更清晰地分析LLMs/LRMs的行为，从而解决相关争论。

Abstract: Debates about whether Large Language or Reasoning Models (LLMs/LRMs) truly
reason or merely pattern-match suffer from shifting goal posts. In my personal
opinion, two analytic--hence "tautological"--benchmarks cut through that fog in
my mental model. In this paper, I attempt to write down my mental model in
concrete terms.

</details>


### [132] [User Perceptions and Attitudes Toward Untraceability in Messaging Platforms](https://arxiv.org/abs/2506.11212)
*Carla F. Griggio,Boel Nelson,Zefan Sramek,Aslan Askarov*

Main category: cs.CR

TL;DR: 研究用户对消息平台中“不可追踪性”的感知，探讨其对隐私增强功能和匿名通信协议设计的启示。


<details>
  <summary>Details</summary>
Motivation: 消息平台通过现有功能保护内容隐私，但元数据仍可能泄露通信关系。研究用户对“不可追踪性”的理解，以指导未来隐私功能设计。

Method: 通过虚构平台Texty和Chatty的案例调查189名参与者，分析其对“不可追踪性”功能的建议和威胁模型的多样性。

Result: 用户对“不可追踪性”的理解多样，包括匿名性和隐私威胁防范，但与技术协议的实际威胁模型存在差距。

Conclusion: 用户认知与隐私技术设计存在差异，为消息平台集成不可追踪工具提供了挑战与机遇。

Abstract: Mainstream messaging platforms offer a variety of features designed to
enhance user privacy, such as disappearing messages, password-protected chats,
and end-to-end encryption (E2EE), which primarily protect message contents.
Beyond contents, the transmission of messages generates metadata that can
reveal who communicates with whom, when and how often. In this paper, we study
user perceptions of "untraceability", i.e., preventing third parties from
tracing who communicates with whom, with the goal of informing the design of
privacy-enhancing features in messaging platforms and untraceable communication
protocols that depend on large anonymity sets and widespread user adoption. We
explore this from a broad conceptual standpoint: rather than studying mental
models of a particular solution, we analyze how users reason about what
features should be incorporated by two fictitious platforms, Texty and Chatty,
to prevent third parties from knowing who communicates with whom. Through a
vignette-based survey with 189 participants, we found that users associate the
concept of untraceability with a wide range of privacy enhancing technologies,
implying a diverse set of threat models. Overall, the features suggested by
participants show awareness of privacy threats stemming from forms of
surveillance and unauthorized access to message contents. Many participants
also associated untraceability with the notion of anonymity, but interpreted it
as senders and receivers concealing their identity from each other rather than
only from third parties. We discuss the gap between users' perceptions of
untraceability and the threat models addressed by untraceable communication
protocols, as well as how different privacy attitudes point to challenges and
opportunities for the adoption of untraceable communication tools in messaging
platforms.

</details>


### [133] [KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis](https://arxiv.org/abs/2506.11612)
*Zhijie Liu,Qiyi Tang,Sen Nie,Shi Wu,Liang Feng Zhang,Yutian Tang*

Main category: cs.CR

TL;DR: KEENHash是一种基于大型语言模型生成的函数嵌入的哈希方法，用于高效和大规模的二进制代码相似性分析，显著超越了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的二进制代码相似性分析方法（尤其是函数级比对工具）在处理大规模场景时存在高时间复杂度的问题，无法有效扩展到如1/n-to-n搜索等大规模应用中。

Method: 提出KEENHash，通过大型语言模型生成函数嵌入，并利用K-Means和特征哈希将二进制代码压缩为固定长度的程序级表示，从而实现高效的大规模二进制代码相似性分析。

Result: 实验结果显示，KEENHash比现有函数匹配工具快至少215倍，同时在53亿次相似性评估中仅耗时395.83秒，而现有工具至少需要56天。在20多万个二进制文件的程序克隆搜索中，KEENHash比4种最先进方法效果提升至少23.16%。

Conclusion: KEENHash在大规模二进制代码相似性分析中展现出显著的高效性和有效性，尤其在恶意软件检测等安全场景中表现卓越。

Abstract: Binary code similarity analysis (BCSA) is a crucial research area in many
fields such as cybersecurity. Specifically, function-level diffing tools are
the most widely used in BCSA: they perform function matching one by one for
evaluating the similarity between binary programs. However, such methods need a
high time complexity, making them unscalable in large-scale scenarios (e.g.,
1/n-to-n search). Towards effective and efficient program-level BCSA, we
propose KEENHash, a novel hashing approach that hashes binaries into
program-level representations through large language model (LLM)-generated
function embeddings. KEENHash condenses a binary into one compact and
fixed-length program embedding using K-Means and Feature Hashing, allowing us
to do effective and efficient large-scale program-level BCSA, surpassing the
previous state-of-the-art methods. The experimental results show that KEENHash
is at least 215 times faster than the state-of-the-art function matching tools
while maintaining effectiveness. Furthermore, in a large-scale scenario with
5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while
these tools will cost at least 56 days. We also evaluate KEENHash on the
program clone search of large-scale BCSA across extensive datasets in 202,305
binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of
them by at least 23.16%, and displays remarkable superiority over them in the
large-scale BCSA security scenario of malware detection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [134] [Auditory-Tactile Congruence for Synthesis of Adaptive Pain Expressions in RoboPatients](https://arxiv.org/abs/2506.11827)
*Saitarun Nadipineni,Chapa Sirithunge,Yue Xie,Fumiya Iida,Thilina Dulantha Lalitharatne*

Main category: cs.RO

TL;DR: RoboPatient是一个基于触觉和听觉反馈的医疗机器人模拟器，用于培训临床医生诊断罕见、微妙或复杂的病例，以减少误诊。研究显示，声音的振幅和音高显著影响对疼痛表达的感知。


<details>
  <summary>Details</summary>
Motivation: 误诊可能导致延误治疗和伤害。通过机器人患者提供可控的训练和评估环境，减少诊断错误。

Method: 开发RoboPatient，通过触觉和听觉反馈模拟疼痛表达，并对7680次试验进行研究，分析参与者对疼痛声音的感知。

Result: 振幅和音高是感知疼痛声音的关键因素，音高是最具影响力的线索。更强的触压力度引发更高的感知一致性。

Conclusion: 这一方法为临床教育和诊断模拟中的高保真机器人患者奠定了基础。

Abstract: Misdiagnosis can lead to delayed treatments and harm. Robotic patients offer
a controlled way to train and evaluate clinicians in rare, subtle, or complex
cases, reducing diagnostic errors. We present RoboPatient, a medical robotic
simulator aimed at multimodal pain synthesis based on haptic and auditory
feedback during palpation-based training scenarios. The robopatient functions
as an adaptive intermediary, capable of synthesizing plausible pain expressions
vocal and facial in response to tactile stimuli generated during palpation.
Using an abdominal phantom, robopatient captures and processes haptic input via
an internal palpation-to-pain mapping model. To evaluate perceptual congruence
between palpation and the corresponding auditory output, we conducted a study
involving 7680 trials across 20 participants, where they evaluated pain
intensity through sound. Results show that amplitude and pitch significantly
influence agreement with the robot's pain expressions, irrespective of pain
sounds. Stronger palpation forces elicited stronger agreement, aligning with
psychophysical patterns. The study revealed two key dimensions: pitch and
amplitude are central to how people perceive pain sounds, with pitch being the
most influential cue. These acoustic features shape how well the sound matches
the applied force during palpation, impacting perceived realism. This approach
lays the groundwork for high-fidelity robotic patients in clinical education
and diagnostic simulation.

</details>


### [135] [The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions](https://arxiv.org/abs/2506.11829)
*Ana Müller,Anja Richert*

Main category: cs.RO

TL;DR: 本文提出了一种多方法框架，结合心理距离和社交绑定理论，研究真实世界中人与智能体的空间和社交动态。


<details>
  <summary>Details</summary>
Motivation: 解决人与智能体互动中感知与行为一致性的挑战。

Method: 结合主观自我报告和客观空间追踪，在博物馆的两个现场研究中测试机器人及虚拟智能体。

Result: 提供了一个开源、可扩展且经过现场测试的工具包。

Conclusion: 该框架为未来研究提供了实用工具，支持进一步探索人机互动中的空间与社交动态。

Abstract: This paper introduces a multimethod framework for studying spatial and social
dynamics in real-world group-agent interactions with socially interactive
agents. Drawing on proxemics and bonding theories, the method combines
subjective self-reports and objective spatial tracking. Applied in two field
studies in a museum (N = 187) with a robot and a virtual agent, the paper
addresses the challenges in aligning human perception and behavior. We focus on
presenting an open source, scalable, and field-tested toolkit for future
studies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [136] [Reimagining Dance: Real-time Music Co-creation between Dancers and AI](https://arxiv.org/abs/2506.12008)
*Olga Vechtomova,Jeff Bos*

Main category: cs.SD

TL;DR: 提出一种多模态系统，让舞者通过动作动态塑造音乐环境，实现舞蹈与音乐的双向互动。


<details>
  <summary>Details</summary>
Motivation: 传统舞蹈表演中动作单向响应音乐，缺乏双向互动，希望利用AI扩展舞蹈与音乐的创造性合作。

Method: 采用多模态架构，通过舞者动作智能组合预录音乐片段，形成连贯的音乐创作。

Result: 通过性能数据分析，展示了动作特性与音频特征之间的新兴通信模式。

Conclusion: AI可作为响应性合作者，重新定义其在表演艺术中的角色，扩展舞蹈表演和即兴艺术表达的可能性。

Abstract: Dance performance traditionally follows a unidirectional relationship where
movement responds to music. While AI has advanced in various creative domains,
its application in dance has primarily focused on generating choreography from
musical input. We present a system that enables dancers to dynamically shape
musical environments through their movements. Our multi-modal architecture
creates a coherent musical composition by intelligently combining pre-recorded
musical clips in response to dance movements, establishing a bidirectional
creative partnership where dancers function as both performers and composers.
Through correlation analysis of performance data, we demonstrate emergent
communication patterns between movement qualities and audio features. This
approach reconceptualizes the role of AI in performing arts as a responsive
collaborator that expands possibilities for both professional dance performance
and improvisational artistic expression across broader populations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [137] [Forgetful by Design? A Critical Audit of YouTube's Search API for Academic Research](https://arxiv.org/abs/2506.11727)
*Bernhard Rieder,Adrian Padilla,Oscar Coromina*

Main category: cs.IR

TL;DR: 该研究批判性地分析了YouTube Data API (v3) 的搜索端点，发现其在完整性、代表性、一致性和偏见方面存在显著问题，并揭示了排名参数对视频召回率和精确度的影响以及时间衰减现象。


<details>
  <summary>Details</summary>
Motivation: 研究者希望通过系统评估YouTube API的搜索功能，揭示其在学术研究中的局限性，尤其是对欧盟《数字服务法》要求的潜在不适应性。

Method: 通过六个月的系统性每周搜索，使用11个查询词，对API的搜索结果的完整性、一致性、时间衰减等指标进行分析。

Result: 研究发现，API的搜索结果存在时间衰减（20-60天内视频可检索性大幅下降）、一致性差（相同查询返回不同结果）、参数选择（如相关性）导致大量无关视频。

Conclusion: 尽管提出了缓解策略，但研究者认为YouTube API的搜索功能因优先考虑“新鲜度”而非全面检索，无法满足学术研究的严谨性要求。

Abstract: This paper critically audits the search endpoint of YouTube's Data API (v3),
a common tool for academic research. Through systematic weekly searches over
six months using eleven queries, we identify major limitations regarding
completeness, representativeness, consistency, and bias. Our findings reveal
substantial differences between ranking parameters like relevance and date in
terms of video recall and precision, with relevance often retrieving numerous
off-topic videos. We also find severe temporal decay, as the number of findable
videos for a specific period dramatically decreases after just 20-60 days from
the publication date, potentially hampering many different research designs.
Furthermore, search results lack consistency, with identical queries yielding
different video sets over time, compromising replicability. A case study on the
European Parliament elections highlights how these issues impact research
outcomes. While the paper offers several mitigation strategies, it concludes
that the API's search function, potentially prioritizing "freshness" over
comprehensive retrieval, is not adequate for robust academic research,
especially concerning Digital Services Act requirements.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [138] [5G-Enabled Smart Prosthetic Hand: Connectivity Analysis and Assessment](https://arxiv.org/abs/2506.11729)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: eess.SY

TL;DR: 提出了一个边缘连接的假肢系统框架，通过5G或WiFi连接实现低延迟控制。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够实时处理环境信息的边缘连接假肢系统，以提升用户体验。

Method: 使用配备摄像头的人体假肢与Jetson设备连接，通过5G或WiFi将视频流传输到边缘服务器进行处理。

Result: 系统延迟低于125毫秒，满足自然控制需求。

Conclusion: 首次展示了基于5G的假肢系统的可行性。

Abstract: In this paper, we demonstrate a proof-of-concept implementation of a
framework for the development of edge-connected prosthetic systems. The
framework is composed of a bionic hand equipped with a camera and connected to
a Jetson device that establishes a wireless connection to the edge server,
processing the received video stream and feeding back the inferred information
about the environment. The hand-edge server connection is obtained either
through a direct 5G link, where the edge server also functions as a 5G base
station, or through a WiFi link. We evaluate the latency of closing the control
loop in the system, showing that, in a realistic usage scenario, the
connectivity and computation delays combined are well below 125 ms, which falls
into the natural control range. To the best of our knowledge, this is the first
analysis showcasing the feasibility of a 5G-enabled prosthetic system.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [139] [Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model](https://arxiv.org/abs/2506.11737)
*Dinh Viet Cuong,Hoang-Bao Le,An Pham Ngoc Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.CV

TL;DR: 论文展示了LLaVA-NeXT-interleave在22个数据集上的优异表现，并比较了加入DCI模块后的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLaVA-NeXT-interleave在多模态任务中的表现，以及DCI模块对其性能的影响。

Method: 在标准模型中加入Dense Channel Integration (DCI)连接器，并在22个数据集上测试性能。

Result: 标准模型在视觉任务表现最佳，DCI版本在语义连贯性和结构化变化理解上更强。

Conclusion: 结合基础模型与即插即用技术在多模态任务中潜力巨大。

Abstract: This paper addresses two main objectives. Firstly, we demonstrate the
impressive performance of the LLaVA-NeXT-interleave on 22 datasets across three
different tasks: Multi-Image Reasoning, Documents and Knowledge-Based
Understanding and Interactive Multi-Modal Communication. Secondly, we add the
Dense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and
compare its performance against the standard model. We find that the standard
model achieves the highest overall accuracy, excelling in vision-heavy tasks
like VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows
particular strength on datasets requiring deeper semantic coherence or
structured change understanding such as MIT-States_PropertyCoherence and
SlideVQA. Our results highlight the potential of combining powerful foundation
models with plug-and-play techniques for Interleave tasks. The code is
available at https://github.com/dinhvietcuong1996/icme25-inova.

</details>


### [140] [Monocular 3D Hand Pose Estimation with Implicit Camera Alignment](https://arxiv.org/abs/2506.11133)
*Christos Pantazopoulos,Spyridon Thermos,Gerasimos Potamianos*

Main category: cs.CV

TL;DR: 该论文提出了一种优化流程，通过2D关键点输入估计3D手部关节，无需相机参数，并在公开数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 单幅彩色图像估计3D手部关节在AR、VR、HCI和机器人领域具有广泛应用，但深度信息缺失、遮挡、关节复杂性和相机参数需求等问题增加了挑战。

Method: 提出了一个优化流程，包括关键点对齐步骤和指尖损失函数，摆脱了对相机参数的依赖。

Result: 在EgoDexter和Dexter+Object基准测试中表现出色，且在处理"野外"图像时显示出鲁棒性。

Conclusion: 尽管使用了手部先验，2D关键点估计的准确性对结果较为敏感。代码已开源。

Abstract: Estimating the 3D hand articulation from a single color image is a
continuously investigated problem with applications in Augmented Reality (AR),
Virtual Reality (VR), Human-Computer Interaction (HCI), and robotics. Apart
from the absence of depth information, occlusions, articulation complexity, and
the need for camera parameters knowledge pose additional challenges. In this
work, we propose an optimization pipeline for estimating the 3D hand
articulation from 2D keypoint input, which includes a keypoint alignment step
and a fingertip loss to overcome the need to know or estimate the camera
parameters. We evaluate our approach on the EgoDexter and Dexter+Object
benchmarks to showcase that our approach performs competitively with the SotA,
while also demonstrating its robustness when processing "in-the-wild" images
without any prior camera knowledge. Our quantitative analysis highlights the
sensitivity of the 2D keypoint estimation accuracy, despite the use of hand
priors. Code is available at https://github.com/cpantazop/HandRepo

</details>


### [141] [Teleoperated Driving: a New Challenge for 3D Object Detection in Compressed Point Clouds](https://arxiv.org/abs/2506.11804)
*Filippo Bragato,Michael Neri,Paolo Testolina,Marco Giordani,Federica Battisti*

Main category: cs.CV

TL;DR: 论文研究了通过点云数据检测车辆和行人以支持远程驾驶（TD），利用扩展的SELMA数据集评估了压缩算法和目标检测器的性能及其对V2X网络的影响。


<details>
  <summary>Details</summary>
Motivation: 互联设备的快速发展为远程驾驶（TD）提供了技术支持，但需要高效的点云数据处理和目标检测以确保安全性。

Method: 利用扩展的SELMA数据集，评估了多种压缩算法和目标检测器的性能，包括压缩效率、处理时间、检测精度及对V2X网络的影响。

Result: 分析了压缩和目标检测对V2X网络数据速率和延迟的影响，并比较了不同算法的表现。

Conclusion: 研究为远程驾驶中的点云数据处理和目标检测提供了可行性分析，支持未来TD应用的技术优化。

Abstract: In recent years, the development of interconnected devices has expanded in
many fields, from infotainment to education and industrial applications. This
trend has been accelerated by the increased number of sensors and accessibility
to powerful hardware and software. One area that significantly benefits from
these advancements is Teleoperated Driving (TD). In this scenario, a controller
drives safely a vehicle from remote leveraging sensors data generated onboard
the vehicle, and exchanged via Vehicle-to-Everything (V2X) communications. In
this work, we tackle the problem of detecting the presence of cars and
pedestrians from point cloud data to enable safe TD operations. More
specifically, we exploit the SELMA dataset, a multimodal, open-source,
synthetic dataset for autonomous driving, that we expanded by including the
ground-truth bounding boxes of 3D objects to support object detection. We
analyze the performance of state-of-the-art compression algorithms and object
detectors under several metrics, including compression efficiency,
(de)compression and inference time, and detection accuracy. Moreover, we
measure the impact of compression and detection on the V2X network in terms of
data rate and latency with respect to 3GPP requirements for TD applications.

</details>


### [142] [Self-Calibrating BCIs: Ranking and Recovery of Mental Targets Without Labels](https://arxiv.org/abs/2506.11151)
*Jonathan Grizou,Carlos de la Torre-Ortiz,Tuukka Ruotsalo*

Main category: cs.CV

TL;DR: 论文提出了一种名为CURSOR的算法，首次在没有标注数据或预训练解码器的情况下，通过自校准方法从EEG和图像数据中恢复未知的心理目标。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖标注数据，但实际场景中标注数据可能不可用。本文旨在解决在没有标注信息的情况下，如何从脑电信号中恢复心理目标的问题。

Method: 提出了CURSOR框架和算法，通过自校准学习从无标注的EEG和图像数据中恢复心理目标。

Result: CURSOR能够预测与人类感知判断相关的图像相似性分数，并生成与未知心理目标难以区分的新刺激（验证通过N=53的用户研究）。

Conclusion: CURSOR在无监督条件下展示了从脑电信号中恢复心理目标的潜力，为未来研究提供了新方向。

Abstract: We consider the problem of recovering a mental target (e.g., an image of a
face) that a participant has in mind from paired EEG (i.e., brain responses)
and image (i.e., perceived faces) data collected during interactive sessions
without access to labeled information. The problem has been previously explored
with labeled data but not via self-calibration, where labeled data is
unavailable. Here, we present the first framework and an algorithm, CURSOR,
that learns to recover unknown mental targets without access to labeled data or
pre-trained decoders. Our experiments on naturalistic images of faces
demonstrate that CURSOR can (1) predict image similarity scores that correlate
with human perceptual judgments without any label information, (2) use these
scores to rank stimuli against an unknown mental target, and (3) generate new
stimuli indistinguishable from the unknown mental target (validated via a user
study, N=53).

</details>


### [143] [AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments](https://arxiv.org/abs/2506.11773)
*Zikang Leng,Megha Thukral,Yaqi Liu,Hrudhai Rajasekhar,Shruthi K. Hiremath,Thomas Plötz*

Main category: cs.CV

TL;DR: AgentSense利用大型语言模型生成虚拟数据集，解决智能家居中人类活动识别数据不足的问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 智能家居中人类活动识别系统因缺乏大规模多样化标注数据而难以泛化，需解决数据多样性和环境差异问题。

Method: 开发AgentSense流程，通过虚拟角色生成日常行为序列，在模拟环境中记录传感器数据。

Result: 虚拟数据与少量真实数据结合训练模型，性能接近完全真实数据训练的结果。

Conclusion: 虚拟数据可有效缓解大规模标注数据不足的挑战，无需手动收集数据。

Abstract: A major obstacle in developing robust and generalizable smart home-based
Human Activity Recognition (HAR) systems is the lack of large-scale, diverse
labeled datasets. Variability in home layouts, sensor configurations, and user
behavior adds further complexity, as individuals follow varied routines and
perform activities in distinct ways. Building HAR systems that generalize well
requires training data that captures the diversity across users and
environments. To address these challenges, we introduce AgentSense, a virtual
data generation pipeline where diverse personas are generated by leveraging
Large Language Models. These personas are used to create daily routines, which
are then decomposed into low-level action sequences. Subsequently, the actions
are executed in a simulated home environment called VirtualHome that we
extended with virtual ambient sensors capable of recording the agents
activities as they unfold. Overall, AgentSense enables the generation of rich,
virtual sensor datasets that represent a wide range of users and home settings.
Across five benchmark HAR datasets, we show that leveraging our virtual sensor
data substantially improves performance, particularly when real data are
limited. Notably, models trained on a combination of virtual data and just a
few days of real data achieve performance comparable to those trained on the
entire real datasets. These results demonstrate and prove the potential of
virtual data to address one of the most pressing challenges in ambient sensing,
which is the distinct lack of large-scale, annotated datasets without requiring
any manual data collection efforts.

</details>


### [144] [Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation](https://arxiv.org/abs/2506.11774)
*Abhishek Jaiswal,Armeet Singh Luthra,Purav Jangir,Bhavya Garg,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: 开发实时反馈系统，评估等长运动姿势，发布大型数据集并引入新指标，提升智能家庭健身系统的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决等长运动依赖不可靠数字内容导致的风险，如错误姿势和受伤。

Method: 发布大型多类别视频数据集，评估最先进模型，引入三部分新指标。

Result: 增强智能家庭健身系统的可行性，扩展至康复和物理治疗等领域。

Conclusion: 该系统提供专家级诊断，适用于多种健身和医疗场景。

Abstract: Isometric exercises appeal to individuals seeking convenience, privacy, and
minimal dependence on equipments. However, such fitness training is often
overdependent on unreliable digital media content instead of expert
supervision, introducing serious risks, including incorrect posture, injury,
and disengagement due to lack of corrective feedback. To address these
challenges, we present a real-time feedback system for assessing isometric
poses. Our contributions include the release of the largest multiclass
isometric exercise video dataset to date, comprising over 3,600 clips across
six poses with correct and incorrect variations. To support robust evaluation,
we benchmark state-of-the-art models-including graph-based networks-on this
dataset and introduce a novel three-part metric that captures classification
accuracy, mistake localization, and model confidence. Our results enhance the
feasibility of intelligent and personalized exercise training systems for home
workouts. This expert-level diagnosis, delivered directly to the users, also
expands the potential applications of these systems to rehabilitation,
physiotherapy, and various other fitness disciplines that involve physical
motion.

</details>


### [145] [Evaluating Sensitivity Parameters in Smartphone-Based Gaze Estimation: A Comparative Study of Appearance-Based and Infrared Eye Trackers](https://arxiv.org/abs/2506.11932)
*Nishan Gunawardena,Gough Yumu Lui,Jeewani Anupama Ginige,Bahman Javadi*

Main category: cs.CV

TL;DR: 比较智能手机深度学习眼动算法与商用红外眼动仪的性能，结果显示深度学习模型在多种条件下具有潜力，但对光照、年龄和视力矫正更敏感。


<details>
  <summary>Details</summary>
Motivation: 研究基于外观的视线估计在移动设备使用条件下的可行性。

Method: 通过轻量级卷积神经网络（MobileNet-V3）和长短期记忆网络（LSTM）从灰度面部图像预测视线坐标，并与商用设备Tobii Pro Nano对比。

Result: 深度学习模型均误差17.76 mm，略高于Tobii的16.53 mm，但对光照、视力矫正和年龄更敏感。

Conclusion: 外观基方法在移动眼动追踪中具有潜力，但需针对不同使用条件优化。

Abstract: This study evaluates a smartphone-based, deep-learning eye-tracking algorithm
by comparing its performance against a commercial infrared-based eye tracker,
the Tobii Pro Nano. The aim is to investigate the feasibility of
appearance-based gaze estimation under realistic mobile usage conditions. Key
sensitivity factors, including age, gender, vision correction, lighting
conditions, device type, and head position, were systematically analysed. The
appearance-based algorithm integrates a lightweight convolutional neural
network (MobileNet-V3) with a recurrent structure (Long Short-Term Memory) to
predict gaze coordinates from grayscale facial images. Gaze data were collected
from 51 participants using dynamic visual stimuli, and accuracy was measured
using Euclidean distance. The deep learning model produced a mean error of
17.76 mm, compared to 16.53 mm for the Tobii Pro Nano. While overall accuracy
differences were small, the deep learning-based method was more sensitive to
factors such as lighting, vision correction, and age, with higher failure rates
observed under low-light conditions among participants using glasses and in
older age groups. Device-specific and positional factors also influenced
tracking performance. These results highlight the potential of appearance-based
approaches for mobile eye tracking and offer a reference framework for
evaluating gaze estimation systems across varied usage conditions.

</details>


### [146] [Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting](https://arxiv.org/abs/2506.11124)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: 本文提出了RefAV框架的两项改进：容错迭代代码生成机制和专门提示工程，以解决LLMs在场景挖掘中的运行时错误和参数解释不准确问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决RefAV框架中LLMs生成代码的运行时错误和复杂空间关系参数解释不准确的问题，提高场景挖掘的可靠性和精度。

Method: 引入了容错迭代代码生成机制和专门提示工程，优化了LLMs的代码生成和参数理解能力。

Result: 在Argoverse 2验证集上的实验显示，系统在多指标上表现一致提升，HOTA-Temporal得分达到52.37。

Conclusion: 提出方法的有效性和高精度验证了其在可靠场景挖掘中的实用性。

Abstract: Scenario mining from extensive autonomous driving datasets, such as Argoverse
2, is crucial for the development and validation of self-driving systems. The
RefAV framework represents a promising approach by employing Large Language
Models (LLMs) to translate natural-language queries into executable code for
identifying relevant scenarios. However, this method faces challenges,
including runtime errors stemming from LLM-generated code and inaccuracies in
interpreting parameters for functions that describe complex multi-object
spatial relationships. This technical report introduces two key enhancements to
address these limitations: (1) a fault-tolerant iterative code-generation
mechanism that refines code by re-prompting the LLM with error feedback, and
(2) specialized prompt engineering that improves the LLM's comprehension and
correct application of spatial-relationship functions. Experiments on the
Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash,
and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably,
the proposed system achieves a HOTA-Temporal score of 52.37 on the official
test set using Gemini 2.5 Pro. These results underline the efficacy of the
proposed techniques for reliable, high-precision scenario mining.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [147] [Vector Representations of Vessel Trees](https://arxiv.org/abs/2506.11163)
*James Batten,Michiel Schaap,Matthew Sinclair,Ying Bai,Ben Glocker*

Main category: eess.IV

TL;DR: VeTTA框架通过两个Transformer自编码器分阶段学习3D血管网络的几何和拓扑特征，降低了GPU内存需求，实现了高保真重建和拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 为医疗影像中的树状解剖结构提供一种精确、灵活且拓扑一致的建模方法。

Method: 采用两阶段Transformer自编码器：第一阶段的Vessel Autoencoder学习血管段的几何特征，第二阶段的Vessel Tree Autoencoder编码整个血管网络的拓扑结构。

Result: 在2D合成树数据集和3D冠状动脉数据集上展示了优于3D卷积模型的重建保真度、拓扑准确性和潜在空间插值效果。

Conclusion: VeTTA框架高效且可扩展，适用于大规模训练和实际医疗影像应用。

Abstract: We introduce a novel framework for learning vector representations of
tree-structured geometric data focusing on 3D vascular networks. Our approach
employs two sequentially trained Transformer-based autoencoders. In the first
stage, the Vessel Autoencoder captures continuous geometric details of
individual vessel segments by learning embeddings from sampled points along
each curve. In the second stage, the Vessel Tree Autoencoder encodes the
topology of the vascular network as a single vector representation, leveraging
the segment-level embeddings from the first model. A recursive decoding process
ensures that the reconstructed topology is a valid tree structure. Compared to
3D convolutional models, this proposed approach substantially lowers GPU memory
requirements, facilitating large-scale training. Experimental results on a 2D
synthetic tree dataset and a 3D coronary artery dataset demonstrate superior
reconstruction fidelity, accurate topology preservation, and realistic
interpolations in latent space. Our scalable framework, named VeTTA, offers
precise, flexible, and topologically consistent modeling of anatomical tree
structures in medical imaging.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [148] [TeleEval-OS: Performance evaluations of large language models for operations scheduling](https://arxiv.org/abs/2506.11017)
*Yanyan Wang,Yingying Wang,Junli Liang,Yin Xu,Yunlong Liu,Yiming Xu,Zhengwang Jiang,Zhehe Li,Fei Li,Long Zhao,Kuang Xu,Qi Song,Xiangyang Li*

Main category: cs.CL

TL;DR: 该论文提出了首个电信运营调度评估基准（TeleEval-OS），包含15个数据集和13个子任务，用于评估大语言模型在电信领域的潜力。实验表明，开源模型在特定场景下可优于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 电信运营调度的复杂性和缺乏评估基准限制了大语言模型在这一领域的应用潜力。

Method: 提出TeleEval-OS基准，涵盖四个关键操作阶段，并通过零样本和少样本方法评估14种语言模型。

Result: 开源大语言模型在特定场景下表现优于闭源模型。

Conclusion: TeleEval-OS为大语言模型在电信运营调度中的应用提供了评估基础，展示了开源模型的潜力。

Abstract: The rapid advancement of large language models (LLMs) has significantly
propelled progress in artificial intelligence, demonstrating substantial
application potential across multiple specialized domains. Telecommunications
operation scheduling (OS) is a critical aspect of the telecommunications
industry, involving the coordinated management of networks, services, risks,
and human resources to optimize production scheduling and ensure unified
service control. However, the inherent complexity and domain-specific nature of
OS tasks, coupled with the absence of comprehensive evaluation benchmarks, have
hindered thorough exploration of LLMs' application potential in this critical
field. To address this research gap, we propose the first Telecommunications
Operation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this
benchmark comprises 15 datasets across 13 subtasks, comprehensively simulating
four key operational stages: intelligent ticket creation, intelligent ticket
handling, intelligent ticket closure, and intelligent evaluation. To
systematically assess the performance of LLMs on tasks of varying complexity,
we categorize their capabilities in telecommunications operation scheduling
into four hierarchical levels, arranged in ascending order of difficulty: basic
NLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we
leverage zero-shot and few-shot evaluation methods to comprehensively assess 10
open-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o)
across diverse scenarios. Experimental results demonstrate that open-source
LLMs can outperform closed-source LLMs in specific scenarios, highlighting
their significant potential and value in the field of telecommunications
operation scheduling.

</details>


### [149] [Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation](https://arxiv.org/abs/2506.11105)
*Uttej Kallakurik,Edward Humes,Rithvik Jonna,Xiaomin Lin,Tinoosh Mohsenin*

Main category: cs.CL

TL;DR: 提出一种针对大型语言模型（LLMs）的通用压缩框架，通过剪枝和量化技术，在不损失性能的情况下显著减小模型尺寸，使其适用于医疗领域的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域潜力巨大，但因模型过大难以在资源有限的边缘设备上实时运行。

Method: 使用领域特定数据测量神经元重要性，进行剪枝和量化，以减小模型尺寸并保持性能。

Result: 成功压缩Gemma和LLaMA3模型，在Jetson Orin Nano和Raspberry Pi 5上实现了实时高效的推理。

Conclusion: 提出的压缩方法有效支持了大型语言模型在资源受限环境中的实际部署。

Abstract: Large Language Models (LLMs) have significant impact on the healthcare
scenarios but remain prohibitively large for deployment in real-time,
resource-constrained environments such as edge devices. In this work, we
introduce a novel medical assistant system, optimized through our
general-purpose compression framework, which tailors Large Language Models
(LLMs) for deployment in specialized domains. By measuring neuron saliency on
domain-specific data, our method can aggressively prune irrelevant neurons,
reducing model size while preserving performance. Following pruning, we apply
post-training quantization to further reduce the memory footprint, and evaluate
the compressed model across medical benchmarks including MedMCQA, MedQA, and
PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed
LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),
achieving real-time, energy-efficient inference under hardware constraints.

</details>


### [150] [Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation](https://arxiv.org/abs/2506.11092)
*Jubin Abhishek Soni,Amit Anand,Rajesh Kumar Pandey,Aniket Abhishek Soni*

Main category: cs.CL

TL;DR: 本文介绍了一种名为动态上下文调整（DCT）的轻量级框架，扩展了检索增强生成（RAG）以支持多轮对话和动态工具环境。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统通常受限于静态、单轮交互和固定工具集，无法适应动态领域（如医疗和智能家居）中用户意图和工具的演变。

Method: DCT通过基于注意力的上下文缓存跟踪历史信息，使用LoRA动态选择工具，并压缩输入以保持在LLM上下文限制内。

Result: 实验表明，DCT在计划准确性上提升14%，减少幻觉37%，且在成本更低的情况下达到GPT-4性能。

Conclusion: DCT能够泛化到未见工具，适用于广泛的动态环境，提供了可扩展和适应性强的AI助手解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has significantly advanced large
language models (LLMs) by grounding their outputs in external tools and
knowledge sources. However, existing RAG systems are typically constrained to
static, single-turn interactions with fixed toolsets, making them ill-suited
for dynamic domains such as healthcare and smart homes, where user intent,
available tools, and contextual factors evolve over time. We present Dynamic
Context Tuning (DCT), a lightweight framework that extends RAG to support
multi-turn dialogue and evolving tool environments without requiring
retraining. DCT integrates an attention-based context cache to track relevant
past information, LoRA-based retrieval to dynamically select domain-specific
tools, and efficient context compression to maintain inputs within LLM context
limits. Experiments on both synthetic and real-world benchmarks show that DCT
improves plan accuracy by 14% and reduces hallucinations by 37%, while matching
GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to
previously unseen tools, enabling scalable and adaptable AI assistants across a
wide range of dynamic environments.

</details>


### [151] [Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)](https://arxiv.org/abs/2506.11112)
*Christine Bauer,Li Chen,Nicola Ferro,Norbert Fuhr,Avishek Anand,Timo Breuer,Guglielmo Faggioli,Ophir Frieder,Hideo Joho,Jussi Karlgren,Johannes Kiesel,Bart P. Knijnenburg,Aldo Lipani,Lien Michiels,Andrea Papenmeier,Maria Soledad Pera,Mark Sanderson,Scott Sanner,Benno Stein,Johanne R. Trippas,Karin Verspoor,Martijn C Willemsen*

Main category: cs.CL

TL;DR: 讨论CONIAC定义及其世界模型，提出评估框架CAFE，包含六大组件。


<details>
  <summary>Details</summary>
Motivation: 明确CONIAC的独特性并建立系统评估标准。

Method: 提出CAFE框架，包含目标、任务、用户特性、标准、方法论和量化指标。

Result: 定义了CONIAC的评估框架CAFE。

Conclusion: CAFE为CONIAC系统提供了全面的评估工具。

Abstract: During the workshop, we deeply discussed what CONversational Information
ACcess (CONIAC) is and its unique features, proposing a world model abstracting
it, and defined the Conversational Agents Framework for Evaluation (CAFE) for
the evaluation of CONIAC systems, consisting of six major components: 1) goals
of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)
aspects of the users carrying out the tasks, 4) evaluation criteria to be
considered, 5) evaluation methodology to be applied, and 6) measures for the
quantitative criteria chosen.

</details>


### [152] [code_transformed: The Influence of Large Language Models on Code](https://arxiv.org/abs/2506.12014)
*Yuliang Xu,Siming Huang,Mingmeng Geng,Yao Wan,Xuanhua Shi,Dongping Chen*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLM）对代码风格的影响，包括命名规范、复杂性和维护性，并基于GitHub仓库数据提供了实证。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否改变了代码风格及其如何被表征，以填补这一领域的空白。

Method: 分析2020至2025年间19,000多个与arXiv论文关联的GitHub仓库中的代码，考察命名、复杂性等指标。

Result: 研究发现LLM影响了代码风格，例如Python中snake_case变量名的比例从47%上升至51%。

Conclusion: LLM确实影响了实际编程风格，但具体比例难以精确量化。

Abstract: Coding remains one of the most fundamental modes of interaction between
humans and machines. With the rapid advancement of Large Language Models
(LLMs), code generation capabilities have begun to significantly reshape
programming practices. This development prompts a central question: Have LLMs
transformed code style, and how can such transformation be characterized? In
this paper, we present a pioneering study that investigates the impact of LLMs
on code style, with a focus on naming conventions, complexity, maintainability,
and similarity. By analyzing code from over 19,000 GitHub repositories linked
to arXiv papers published between 2020 and 2025, we identify measurable trends
in the evolution of coding style that align with characteristics of
LLM-generated code. For instance, the proportion of snake\_case variable names
in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we
investigate how LLMs approach algorithmic problems by examining their reasoning
processes. Given the diversity of LLMs and usage scenarios, among other
factors, it is difficult or even impossible to precisely estimate the
proportion of code generated or assisted by LLMs. Our experimental results
provide the first large-scale empirical evidence that LLMs affect real-world
programming style.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [153] [Brain2Vec: A Deep Learning Framework for EEG-Based Stress Detection Using CNN-LSTM-Attention](https://arxiv.org/abs/2506.11179)
*Md Mynoddin,Troyee Dev,Rishita Chakma*

Main category: eess.SP

TL;DR: Brain2Vec是一种结合卷积、LSTM和注意力机制的深度学习工具，用于从原始EEG信号分类心理压力状态，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 精神压力影响认知健康和整体福祉，需要开发非侵入性诊断工具。EEG信号因非平稳性和高维度特性建模挑战大。

Method: Brain2Vec采用卷积层捕获空间依赖，LSTM建模时间模式，注意力机制突出关键时间区域。预处理包括带通滤波、z-score标准化和分段。

Result: 在DEAP数据集上，Brain2Vec的AUC为0.68，验证准确率为81.25%，优于传统CNN-LSTM基线。

Conclusion: Brain2Vec展现出在可穿戴压力监测和个性化医疗系统中的潜力。

Abstract: Mental stress has become a pervasive factor affecting cognitive health and
overall well-being, necessitating the development of robust, non-invasive
diagnostic tools. Electroencephalogram (EEG) signals provide a direct window
into neural activity, yet their non-stationary and high-dimensional nature
poses significant modeling challenges. Here we introduce Brain2Vec, a new deep
learning tool that classifies stress states from raw EEG recordings using a
hybrid architecture of convolutional, recurrent, and attention mechanisms. The
model begins with a series of convolutional layers to capture localized spatial
dependencies, followed by an LSTM layer to model sequential temporal patterns,
and concludes with an attention mechanism to emphasize informative temporal
regions. We evaluate Brain2Vec on the DEAP dataset, applying bandpass
filtering, z-score normalization, and epoch segmentation as part of a
comprehensive preprocessing pipeline. Compared to traditional CNN-LSTM
baselines, our proposed model achieves an AUC score of 0.68 and a validation
accuracy of 81.25%. These findings demonstrate Brain2Vec's potential for
integration into wearable stress monitoring platforms and personalized
healthcare systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [154] [Knapsack and Shortest Path Problems Generalizations From A Quantum-Inspired Tensor Network Perspective](https://arxiv.org/abs/2506.11711)
*Sergio Muñiz Subiñas,Jorge Martínez Martín,Alejandro Mata Ali,Javier Sedano,Ángel Miguel García-Vico*

Main category: quant-ph

TL;DR: 本文提出两种基于张量网络的量子启发算法，用于解决背包问题和最短路径问题及其变体，提供精确的最优解。方法结合虚时间演化和张量网络的限制条件，并通过对称性和中间计算重用降低复杂度。性能实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 解决组合优化问题（如背包问题和最短路径问题）的经典方法计算复杂度高，希望通过量子启发算法提高效率。

Method: 基于张量网络的算法，结合虚时间演化和限制条件，利用对称性和中间计算结果重用降低复杂度。

Result: 算法能够精确求解最优解，并通过实验验证了相对于经典方法的性能优势。

Conclusion: 量子启发张量网络算法为解决组合优化问题提供了一种高效且精确的途径。

Abstract: In this paper, we present two tensor network quantum-inspired algorithms to
solve the knapsack and the shortest path problems, and enables to solve some of
its variations. These methods provide an exact equation which returns the
optimal solution of the problems. As in other tensor network algorithms for
combinatorial optimization problems, the method is based on imaginary time
evolution and the implementation of restrictions in the tensor network. In
addition, we introduce the use of symmetries and the reutilization of
intermediate calculations, reducing the computational complexity for both
problems. To show the efficiency of our implementations, we carry out some
performance experiments and compare the results with those obtained by other
classical algorithms.

</details>


### [155] [Controlling quantum chaos via Parrondo strategies on NISQ hardware](https://arxiv.org/abs/2506.11225)
*Aditi Rath,Dinesh Kumar Panda,Colin Benjamin*

Main category: quant-ph

TL;DR: 论文研究了在NISQ系统中利用离散时间量子行走（DTQW）探索和控制量子混沌，通过量子傅里叶变换优化电路实现，并在实验中观察到量子混沌到有序的转变。


<details>
  <summary>Details</summary>
Motivation: 随着NISQ计算的进步，研究如何在这种系统中有效控制量子混沌，以探索其潜在应用。

Method: 使用DTQW和量子傅里叶变换（QFT）优化电路实现，实验验证在3-和4-循环图上的量子行走。

Result: 4-循环图实现高保真度量子演化，而3-循环图通过动态解耦脉冲显著提升了保真度。

Conclusion: 研究为未来基于量子行走的算法和密码协议提供了实用方法。

Abstract: Advancements in Noisy Intermediate-Scale Quantum (NISQ) computing are
steadily pushing these systems toward outperforming classical supercomputers on
specific, well-defined computational tasks. In this work, we explore and
control quantum chaos in NISQ systems using discrete-time quantum walks (DTQW)
on cyclic graphs. To efficiently implement quantum walks on NISQ hardware, we
employ the quantum Fourier transform (QFT) to diagonalize the conditional shift
operator, optimizing circuit depth and fidelity. We experimentally realize the
transition from quantum chaos to order via DTQW dynamics on both odd and even
cyclic graphs, specifically 3- and 4-cycle graphs, using the counterintuitive
Parrondo's paradox strategy across three different NISQ devices. While the
4-cycle graphs exhibit high-fidelity quantum evolution, the 3-cycle
implementation shows significant fidelity improvement when augmented with
dynamical decoupling pulses. Our results demonstrate a practical approach to
probing and harnessing controlled chaotic dynamics on real quantum hardware,
laying the groundwork for future quantum algorithms and cryptographic protocols
based on quantum walks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [156] [Temporal Dynamics of Emotions in Italian Online Soccer Fandoms](https://arxiv.org/abs/2506.11934)
*Salvatore Citraro,Giovanni Mauro,Emanuele Ferragina*

Main category: cs.SI

TL;DR: 通过计算分析2023-24赛季意大利足球联赛粉丝在Instagram上的评论，研究发现情绪动态与球队表现相关，愤怒情绪爆发性显著相关。


<details>
  <summary>Details</summary>
Motivation: 探究足球粉丝情绪动态与球队表现的关系，以及社交媒体上的情感表达模式。

Method: 对83支球队官方Instagram的粉丝评论进行情感分析，提取情绪时间分布模式，并结合复杂系统理论分析。

Result: 愤怒情绪爆发性与球队表现显著相关，排除此参数后统计模型解释力下降32%。

Conclusion: 粉丝情绪表达与球队成绩相关，为体育分析和粉丝互动研究提供新视角。

Abstract: This study investigates the emotional dynamics of Italian soccer fandoms
through computational analysis of user-generated content from official
Instagram accounts of 83 teams across Serie A, Serie B, and Lega Pro during the
2023-24 season. By applying sentiment analysis to fan comments, we extract
temporal emotional patterns and identify distinct clusters of fan bases with
similar preseason expectations. Drawing from complex systems theory, we
characterize joy as displaying anti-bursty temporal distributions, while anger
is marked by pronounced bursty patterns. Our analysis reveals significant
correlations between these emotional signals, preseason expectations,
socioeconomic factors, and final league rankings. In particular, the burstiness
metric emerges as a meaningful correlate of team performance; statistical
models excluding this parameter show a decrease in the coefficient of
determination of 32%. These findings offer novel insights into the relationship
between fan emotional expression and team outcomes, suggesting potential
avenues for research in sports analytics, social media dynamics, and fan
engagement studies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [157] [FeNN: A RISC-V vector processor for Spiking Neural Network acceleration](https://arxiv.org/abs/2506.11760)
*Zainab Aizaz,James C. Knight,Thomas Nowotny*

Main category: cs.NE

TL;DR: 本文提出了一种基于RISC-V的软向量处理器（FeNN），用于在FPGA上高效模拟脉冲神经网络（SNNs），展示了其在能耗和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）能显著降低AI系统的能耗，但主流加速器（如GPU和TPU）不适合SNN模拟。FPGA因其高内存带宽和片上存储容量，更适用于低算术强度的SNN模拟。

Method: 开发了基于RISC-V的软向量处理器FeNN，结合随机舍入和饱和技术，实现了高数值精度和低硬件利用率。

Result: 实验表明，单个FeNN核心在模拟SNN分类器时，速度优于嵌入式GPU和Loihi神经形态系统。

Conclusion: FeNN是一种高效且可编程的SNN模拟解决方案，适用于从边缘到云端的各种应用。

Abstract: Spiking Neural Networks (SNNs) have the potential to drastically reduce the
energy requirements of AI systems. However, mainstream accelerators like GPUs
and TPUs are designed for the high arithmetic intensity of standard ANNs so are
not well-suited to SNN simulation. FPGAs are well-suited to applications with
low arithmetic intensity as they have high off-chip memory bandwidth and large
amounts of on-chip memory. Here, we present a novel RISC-V-based soft vector
processor (FeNN), tailored to simulating SNNs on FPGAs. Unlike most dedicated
neuromorphic hardware, FeNN is fully programmable and designed to be integrated
with applications running on standard computers from the edge to the cloud. We
demonstrate that, by using stochastic rounding and saturation, FeNN can achieve
high numerical precision with low hardware utilisation and that a single FeNN
core can simulate an SNN classifier faster than both an embedded GPU and the
Loihi neuromorphic system.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [158] [On Effective Banach-Mazur Games and an application to the Poincaré Recurrence Theorem for Category](https://arxiv.org/abs/2506.11118)
*Prajval Koul,Satyadev Nandakumar*

Main category: math.LO

TL;DR: 论文通过有效化Banach-Mazur游戏，证明了有效第一范畴集的描述，并应用于动态系统中的有效Poincaré回归定理。


<details>
  <summary>Details</summary>
Motivation: 研究如何将经典拓扑空间中的范畴理论扩展到有效计算领域，特别是动态系统中的回归性质。

Method: 通过有效化Banach-Mazur游戏，并结合动态系统理论进行分析。

Result: 证明了有效Banach范畴定理及有效Poincaré回归定理的有效版本。

Conclusion: 有效化游戏理论为动态系统中的范畴性质提供了新的研究工具。

Abstract: The classical Banach-Mazur game characterizes sets of first category in a
topological space. In this work, we show that an effectivized version of the
game yields a characterization of sets of effective first category. Using this,
we give a proof for the effective Banach Category Theorem. Further, we provide
a game-theoretic proof of an effective theorem in dynamical systems, namely the
category version of Poincar\'e Recurrence. The Poincar\'e Recurrence Theorem
for category states that for a homeomorphism without open wandering sets, the
set of non recurrent points forms a first category (meager) set. As an
application of the effectivization of the Banach-Mazur game, we show that such
a result holds true in effective settings as well.

</details>
