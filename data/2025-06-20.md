<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 20]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 7]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: 论文介绍了OS-Harm，一个用于评估基于LLM的计算机代理安全性的新基准，覆盖了滥用、攻击和误行为三类危害，并提出了自动化评估方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的计算机代理（如处理截图或可访问性树的代理）日益流行，但其安全性问题被忽视，亟需评估其潜在危害以促进广泛应用。

Method: 在OSWorld环境上构建OS-Harm基准，设计了150个任务，涵盖多种安全违规场景（如骚扰、侵权、虚假信息等），并开发了自动化评估工具。

Result: 测试显示，前沿模型（如o4-mini、Claude 3.7 Sonnet等）普遍易受滥用和攻击，且可能执行不安全操作。评估工具与人工标注一致性高（F1分数0.76-0.79）。

Conclusion: OS-Harm为计算机代理的安全性评估提供了标准化工具，揭示了现有模型的安全隐患，有助于未来研究和改进。

Abstract: Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.

</details>


### [2] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 这篇论文首次全面分析了数据可视化库中的错误，研究了564个来自五个常用库的bug，总结了症状和根本原因，并提出了分类法。研究发现错误/不准确的绘图普遍存在，且主要原因是图形计算错误。此外，研究还提出了触发这些错误的八个关键步骤和两个测试预言，并探索了视觉语言模型（VLMs）在检测错误中的可行性。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库的准确性对用户体验和信息传达至关重要，错误的可视化可能导致用户误解和决策失误。然而，这些库中的视觉错误往往不易察觉，因此需要深入研究其特性以改进检测和修复方法。

Method: 研究通过收集和分析564个来自五个常用数据可视化库的bug，系统地分类了它们的症状和根本原因，并探索了视觉语言模型在检测不准确绘图中的应用。

Result: 研究发现错误/不准确的绘图普遍存在，主要原因是图形计算错误。提出八个触发步骤和两个测试预言，并发现VLMs的检测效果在29%至57%之间，取决于提示设计。

Conclusion: 数据可视化库中的错误普遍且具有独特性，需要进一步开发自动化测试方法。视觉语言模型在检测错误方面显示出一定的潜力，但效果仍有提升空间。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation, data analysis, and application development, underscoring the importance of their accuracy in transforming data into visual representations. Incorrect visualizations can adversely impact user experience, distort information conveyance, and influence user perception and decision-making processes. Visual bugs in these libraries can be particularly insidious as they may not cause obvious errors like crashes, but instead mislead users of the underlying data graphically, resulting in wrong decision making. Consequently, a good understanding of the unique characteristics of bugs in DataViz libraries is essential for researchers and developers to detect and fix bugs in DataViz libraries.
  This study presents the first comprehensive analysis of bugs in DataViz libraries, examining 564 bugs collected from five widely-used libraries. Our study systematically analyzes their symptoms and root causes, and provides a detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in DataViz libraries and incorrect graphic computation is the major root cause, which necessitates further automated testing methods for DataViz libraries. Moreover, we identified eight key steps to trigger such bugs and two test oracles specific to DataViz libraries, which may inspire future research in designing effective automated testing techniques. Furthermore, with the recent advancements in Vision Language Models (VLMs), we explored the feasibility of applying these models to detect incorrect/inaccurate plots. The results show that the effectiveness of VLMs in bug detection varies from 29% to 57%, depending on the prompts, and adding more information in prompts does not necessarily increase the effectiveness. More findings can be found in our manuscript.

</details>


### [3] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: 本文提出了一种新的基准测试，通过可配置的细粒度程序特征来评估模糊测试的有效性，填补了传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试评估通常关注整体性能，而忽略了细粒度程序特征对效果的影响。本文旨在填补这一空白。

Method: 通过分析25项最近的研究，提取了7个影响模糊测试性能的程序特征，生成了包含153个程序的基准测试，并评估了11种流行的模糊器。

Result: 结果显示，模糊器性能受程序特征及其强度显著影响，强调了将程序特征纳入评估的重要性。

Conclusion: 本文的基准测试为模糊测试提供了更细致的评估工具，有助于提升测试的针对性和有效性。

Abstract: Fuzzing is a powerful software testing technique renowned for its effectiveness in identifying software vulnerabilities. Traditional fuzzing evaluations typically focus on overall fuzzer performance across a set of target programs, yet few benchmarks consider how fine-grained program features influence fuzzing effectiveness. To bridge this gap, we introduce a novel benchmark designed to generate programs with configurable, fine-grained program features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing studies, extracting 7 program features related to control-flow and data-flow that can impact fuzzer performance. Using these features, we generated a benchmark consisting of 153 programs controlled by 10 fine-grained configurable parameters. We evaluated 11 popular fuzzers using this benchmark. The results indicate that fuzzer performance varies significantly based on the program features and their strengths, highlighting the importance of incorporating program characteristics into fuzzing evaluations.

</details>


### [4] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: 研究评估了18种大语言模型（LLM）在增强报告审批预测（ERAP）中的应用，发现LoRA微调的Llama 3.1 8B Instruct表现最佳，准确率达79%，优于传统方法5%。


<details>
  <summary>Details</summary>
Motivation: 手动处理增强报告（ERs）效率低且易丢失有价值信息，因此研究者希望通过机器学习（尤其是LLM）提升自动化决策的准确性。

Method: 系统评估了18种LLM（如BERT、GPT-3.5-turbo等）与传统方法（CNN/LSTM-BERT/GloVe）的对比，并引入创建者特征和LoRA微调。

Result: 未微调的仅解码器模型加入创建者特征后准确率提升10.8%；LoRA微调的Llama 3.1 8B Instruct准确率达79%，召回率显著优于传统方法。

Conclusion: LLM在ERAP中表现优异，可优化软件维护流程，并为未来研究提供了改进方向。

Abstract: Enhancement reports (ERs) serve as a critical communication channel between users and developers, capturing valuable suggestions for software improvement. However, manually processing these reports is resource-intensive, leading to delays and potential loss of valuable insights. To address this challenge, enhancement report approval prediction (ERAP) has emerged as a research focus, leveraging machine learning techniques to automate decision-making. While traditional approaches have employed feature-based classifiers and deep learning models, recent advancements in large language models (LLM) present new opportunities for enhancing prediction accuracy. This study systematically evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1 8B Instruct and DeepSeek-V3 for decoder models) against traditional methods (CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1) Incorporating creator profiles increases unfine-tuned decoder-only models' overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79 percent accuracy and significantly enhancing recall for approved reports (76.1 percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5 percent under strict chronological evaluation and effectively addressing class imbalance issues. These findings establish LLM as a superior solution for ERAP, demonstrating their potential to streamline software maintenance workflows and improve decision-making in real-world development environments. We also investigated and summarized the ER cases where the large models underperformed, providing valuable directions for future research.

</details>


### [5] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 该论文提出了一种静态验证框架，用于证明使用Go语言子集的分布式程序中不存在通信竞争。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的并发问题（如通信竞争）可能导致接收者接收到错误的消息或无法接收消息，从而引发程序错误。现有的研究主要集中在共享内存系统的数据竞争检测上，而通信竞争的类似问题尚未充分解决。

Method: 作者扩展了happens-before顺序，以适应缓冲和非缓冲通道，并通过静态分析分布式程序的执行流程来检测通信竞争。

Result: 该方法能够证明使用Go子集的分布式程序中不存在通信竞争。

Conclusion: 通过静态验证框架，可以有效避免分布式程序中的通信竞争问题，提升程序的可靠性。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid races. However, reasoning about concurrency is difficult, and unexpected races show up as bugs. Data race detection in shared memory systems is well-studied (dynamic data race detection [13], behavioral types [15], dynamic race detection [31]). Similar to how a data race consists of reads and writes not related by happens-before at a shared memory location, a communication race consists of receives and sends not related by happens-before on a shared channel. Communication races are problematic: a receiver expects a specific message from a specific sender, but with a communication race, the receiver can receive a message meant for another receiver, or not receive anything at all. In this work, we describe a verification framework that can prove the absence of communication races for distributed programs that use a subset of the Go programming language, where synchronization is mainly achieved via message passing. We statically reason about how a distributed program executes, using a happens-before order, extended to buffered and unbuffered channels.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: 介绍了一个基于智能技术的工具RetroAI++，用于自动化优化Agile/Scrum开发中的冲刺计划和回顾分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件开发者在Agile/Scrum项目中的冲刺计划和回顾活动，提高效率。

Method: 开发原型工具RetroAI++，利用AI技术自动化优化冲刺计划、开发和回顾阶段流程。

Result: 工具提供智能冲刺组织建议和有意义的回顾反思见解。

Conclusion: RetroAI++能有效提升Agile/Scrum开发过程中的管理效率。

Abstract: In Agile/Scrum software development, sprint planning and retrospective analysis are the key elements of project management. The aim of our work is to support software developers in these activities. In this paper, we present our prototype tool RetroAI++, based on emerging intelligent technologies. In our RetroAI++ prototype, we aim to automate and refine the practical application of Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI insights, our prototype aims to automate and refine the many processes involved in the Sprint Planning, Development and Retrospective stages of Agile/Scrum development projects, offering intelligent suggestions for sprint organisation as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本文对大语言模型（LLMs）在单元测试中的应用进行了首次系统文献综述，总结了现有成果、开放挑战及未来机遇。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在单元测试中的广泛应用，研究者需要了解现有成就、挑战及机会，以推动未来研究。

Method: 本文通过分析相关论文，从单元测试和LLMs的角度对任务进行分类，并讨论了LLMs在单元测试中的应用策略。

Result: 总结了LLMs在单元测试中的关键挑战和潜在研究方向，为研究者提供了全面的概述。

Conclusion: 本文为单元测试社区提供了系统性的研究概况，旨在推动未来相关研究的发展。

Abstract: Unit testing is a fundamental practice in modern software engineering, with the aim of ensuring the correctness, maintainability, and reliability of individual software components. Very recently, with the advances in Large Language Models (LLMs), a rapidly growing body of research has leveraged LLMs to automate various unit testing tasks, demonstrating remarkable performance and significantly reducing manual effort. However, due to ongoing explorations in the LLM-based unit testing field, it is challenging for researchers to understand existing achievements, open challenges, and future opportunities. This paper presents the first systematic literature review on the application of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant papers from the perspectives of both unit testing and LLMs. We first categorize existing unit testing tasks that benefit from LLMs, e.g., test generation and oracle generation. We then discuss several critical aspects of integrating LLMs into unit testing research, including model usage, adaptation strategies, and hybrid approaches. We further summarize key challenges that remain unresolved and outline promising directions to guide future research in this area. Overall, our paper provides a systematic overview of the research landscape to the unit testing community, helping researchers gain a comprehensive understanding of achievements and promote future research. Our artifacts are publicly available at the GitHub repository: https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该研究探讨了代码片段的文档描述类型及其通过大型语言模型（如Llama）生成的效果，发现大多数描述基于示例使用，且LLM在识别和生成相关描述时表现良好但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解开发者常用的代码片段描述类型，并评估LLM（如Llama）在生成这些描述时的能力，以支持更清晰的文档化需求。

Method: 研究使用了NPM代码片段库中的185,412个包和1,024,579个代码片段，从中选取400个样本进行手动分类和LLM分析。

Result: 手动分类显示55.5%的描述为示例使用；LLM正确识别出79.75%的描述为“示例”，生成描述的平均相似度为0.7173，表明相关但需改进。

Conclusion: 代码片段的文档意图因任务而异，LLM在描述生成方面表现良好但仍有改进空间，尤其是细节表达和相关性提升。

Abstract: Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as "Example" (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: RAG在代码生成中依赖语义连贯的块选择，现有方法破坏语义结构，本文提出基于AST的分块方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道中的分块方法（如基于行的分块）会破坏代码的语义结构，影响生成质量。

Method: 提出一种基于抽象语法树（AST）的结构感知分块方法，递归分解大节点并合并兄弟节点，同时限制块大小。

Result: 该方法在RepoEval检索任务中Recall@5提升4.3点，在SWE-bench生成任务中Pass@1提升2.67点。

Conclusion: 结构感知分块对提升检索增强的代码智能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: 为解决稀疏数据结构在GPU上的低效问题，论文提出了一种新编译器转换方法，显著提升了稀疏矩阵乘法的性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏数据结构虽能减少内存占用，但因随机内存访问导致效率低下，尤其在GPU上表现不佳。

Method: 提出enumerate-and-sparse-coarsen编译器转换，通过增加数据重用和平衡负载优化稀疏矩阵乘法。

Result: 在A100 GPU上，该方法相比基线（cuBLAS和cuSPARSE）带来了1.84倍到2.27倍的几何平均加速。

Conclusion: 该方法有效提升了GPU上稀疏矩阵乘法的效率，适用于卷积和Transformer模型。

Abstract: Sparse data structures are commonly used in neural networks to reduce the memory footprint. These data structures are compact but cause irregularities such as random memory accesses, which prevent efficient use of the memory hierarchy. GPUs are a common platform for machine learning practitioners, but running compact data structures on these devices often leads to slow-downs due to inefficient use of computing and memory resources. This paper proposes a new compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse matrix-matrix multiplication (SPMM) on GPU devices. The transformation increases data reuse in registers and caches while creating more balanced workloads for GPU computing resources. The transformation is tested on sparse neural networks in convolutional and transformer models. On an A100 GPU and across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to 128, the transformation yields a geometric mean speedup of 1.84$\times$ to 2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [11] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: 本文提出了一种新的Haskell内存类型上下文PSM，结合了并行性和确定性，解决了资源共享时的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Haskell并行编程抽象（如IVar和LVar）与并发抽象（如MVar和TVar）无法同时满足确定性和破坏性更新的需求，需要更高层次的内存抽象。

Method: 引入PSM（Policy Synchronised Memory）类型上下文，支持持久状态访问和破坏性更新，同时通过策略协调保证并发访问的无竞争性和确定性。

Result: PSM上下文中的事务可以构建抽象数据结构，这些结构具有命令式、可并发共享且确定性的特点。

Conclusion: PSM提供了一种兼具并行性和确定性的解决方案，适用于需要共享资源且保持确定性的编程模式。

Abstract: Concurrency and determinacy do not go well with each other when resources must be shared. Haskell provides parallel programming abstractions such as IVar and LVar in the Par monad and concurrent abstractions such as MVar and TVar in the in IO and STM monads, respectively. The former are determinate but have no destructive updates and the latter have destructive updates but do not guarantee determinacy. Programming patterns that are both concurrent and determinate, such as those provided by Kahn or Berry require memory abstractions at a higher level than is currently available. In this paper we describe a new type context PSM for policy synchronised memory in Haskell. Like STM and IO, the computations in PSM can access persistent state and, as a side-effect, update the memory in imperative style. Like the Par and IO monads, PSM supports concurrent threads and shared state. However, in contrast to IO, our PSM contexts are race-free since concurrent accesses are policy coordinated which guarantees determinacy.Well-typed transactions in the PSM context can accommodate abstract data structures that are imperative, concurrently shareable and still behave deterministically, by construction.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Atys: An Efficient Profiling Framework for Identifying Hotspot Functions in Large-scale Cloud Microservices](https://arxiv.org/abs/2506.15523)
*Jiaqi Sun,Dingyu Yang,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.PF

TL;DR: 本文介绍了Atys1，一个高效的大规模分布式服务热点函数分析框架，具有多语言适配、两级聚合、选择性剪枝和动态调整采样频率等核心特性，显著降低了分析成本并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模服务通常由数千个云实例组成，性能微小提升可带来显著成本节约。因此，需要一种高效的性能分析框架来识别服务中的热点函数以优化性能。

Method: Atys1框架包括：1)语言无关的多语言微服务适配机制；2)两级聚合方法生成全面的火焰图；3)函数选择性剪枝策略(FSP)；4)基于服务状态的动态频率调整方案(FDA)。

Result: 实验表明，FSP策略减少了6.8%的时间，MAPE仅为0.58%。FDA方案在保持MSE相同的同时，成本降低了87.6%。

Conclusion: Atys1通过其核心特性，有效识别热点函数，减少了分析成本并保持了高精度，适用于大规模分布式服务的性能优化。

Abstract: To handle the high volume of requests, large-scale services are comprised of thousands of instances deployed in clouds. These services utilize diverse programming languages and are distributed across various nodes as encapsulated containers. Given their vast scale, even minor performance enhancements can lead to significant cost reductions. In this paper, we introduce Atys1, an efficient profiling framework specifically designed to identify hotspot functions within large-scale distributed services. Atys presents four key features. First, it implements a language-agnostic adaptation mechanism for multilingual microservices. Second, a two-level aggregation method is introduced to provide a comprehensive overview of flamegraphs. Third, we propose a function selective pruning (FSP) strategy to enhance the efficiency of aggregating profiling results. Finally, we develop a frequency dynamic adjustment (FDA) scheme that dynamically modifies sampling frequency based on service status, effectively minimizing profiling cost while maintaining accuracy. Cluster-scale experiments on two benchmarks show that the FSP strategy achieves a 6.8% reduction in time with a mere 0.58% mean average percentage error (MAPE) in stack traces aggregation. Additionally, the FDA scheme ensures that the mean squared error (MSE) remains on par with that at high sampling rates, while achieving an 87.6% reduction in cost.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC](https://arxiv.org/abs/2506.14987)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.NI

TL;DR: 论文提出了一种基于CNN的动态优先级预测机制，改进了LDP算法，用于多小区多信道网络中的干扰协调，显著提升了网络性能和调度能力。


<details>
  <summary>Details</summary>
Motivation: 在大规模工业无线网络中，确保包级通信质量对URLLC至关重要，传统LDP的静态优先级分配方法无法满足动态需求。

Method: 结合CNN和图着色算法，动态分配链路优先级，根据实时流量、传输机会和网络条件自适应调整。

Result: 仿真结果显示，在三种网络配置中，SINR增益分别高达113%、94%和49%。

Conclusion: 该方法在复杂URLLC场景中表现出色，显著提升了网络容量、SINR和调度效率。

Abstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a CNN-based dynamic priority prediction mechanism for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach uses a Convolutional Neural Network and graph coloring to adaptively assign link priorities based on real-time traffic, transmission opportunities, and network conditions. Assuming that first training phase is performed offline, our approach introduced minimal overhead, while enabling more efficient resource allocation, boosting network capacity, SINR, and schedulability. Simulation results show SINR gains of up to 113\%, 94\%, and 49\% over LDP across three network configurations, highlighting its effectiveness for complex URLLC scenarios.

</details>


### [14] [GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC](https://arxiv.org/abs/2506.15011)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.NI

TL;DR: 论文提出了一种结合GCN和DQN的方法，改进了LDP算法，以动态学习链接优先级，优化多细胞、多通道网络中的干扰协调，显著提升了SINR性能。


<details>
  <summary>Details</summary>
Motivation: 为确保大规模工业无线网络中URLLC的包级通信质量，需要解决传统静态优先级方法（如LDP）无法适应动态需求的问题。

Method: 结合GCN（捕捉空间依赖）和DQN（自适应调度决策），动态学习链接优先级。

Result: 相比LDP，GCN-DQN在三种网络配置中平均SINR分别提升了179.6%、197.4%和175.2%；相比之前的CNN方法，提升了31.5%、53.0%和84.7%。

Conclusion: GCN-DQN能高效满足URLLC需求，显著提升网络性能，且开销低。

Abstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph Convolutional Network (GCN) integrated with a Deep Q-Network (DQN) reinforcement learning framework for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach dynamically learns link priorities based on real-time traffic demand, network topology, remaining transmission opportunities, and interference patterns. The GCN captures spatial dependencies, while the DQN enables adaptive scheduling decisions through reward-guided exploration. Simulation results show that our GCN-DQN model achieves mean SINR improvements of 179.6\%, 197.4\%, and 175.2\% over LDP across three network configurations. Additionally, the GCN-DQN model demonstrates mean SINR improvements of 31.5\%, 53.0\%, and 84.7\% over our previous CNN-based approach across the same configurations. These results underscore the effectiveness of our GCN-DQN model in addressing complex URLLC requirements with minimal overhead and superior network performance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [Omnidirectional Video Super-Resolution using Deep Learning](https://arxiv.org/abs/2506.14803)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter W. Eklund,Sunil Aryal*

Main category: cs.MM

TL;DR: 论文提出了一个针对360°视频超分辨率的新方法S3PO，并创建了新的数据集360VDS，解决了传统VSR技术中的失真问题。


<details>
  <summary>Details</summary>
Motivation: 360°视频的有限空间分辨率影响了其沉浸式体验的视觉质量，而传统VSR技术未能解决其投影失真问题。

Method: 提出了S3PO模型，采用循环建模和注意力机制，设计了针对球形失真的特征提取器和损失函数。

Result: S3PO在360°视频数据集上表现优于传统VSR和专为360°视频设计的超分辨率模型。

Conclusion: S3PO为360°视频超分辨率提供了有效解决方案，并通过消融研究验证了其架构设计的有效性。

Abstract: Omnidirectional Videos (or 360° videos) are widely used in Virtual Reality (VR) to facilitate immersive and interactive viewing experiences. However, the limited spatial resolution in 360° videos does not allow for each degree of view to be represented with adequate pixels, limiting the visual quality offered in the immersive experience. Deep learning Video Super-Resolution (VSR) techniques used for conventional videos could provide a promising software-based solution; however, these techniques do not tackle the distortion present in equirectangular projections of 360° video signals. An additional obstacle is the limited availability of 360° video datasets for study. To address these issues, this paper creates a novel 360° Video Dataset (360VDS) with a study of the extensibility of conventional VSR models to 360° videos. This paper further proposes a novel deep learning model for 360° Video Super-Resolution (360° VSR), called Spherical Signal Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent modelling with an attention mechanism, unbound from conventional VSR techniques like alignment. With a purpose-built feature extractor and a novel loss function addressing spherical distortion, S3PO outperforms most state-of-the-art conventional VSR models and 360°~specific super-resolution models on 360° video datasets. A step-wise ablation study is presented to understand and demonstrate the impact of the chosen architectural sub-components, targeted training and optimisation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [16] [See What I Mean? CUE: A Cognitive Model of Understanding Explanations](https://arxiv.org/abs/2506.14775)
*Tobias Labarta,Nhi Hoang,Katharina Weitz,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Main category: cs.HC

TL;DR: 介绍了CUE模型，通过链接解释属性和认知子过程（易读性、可读性和可解释性）来提高AI解释的认知可访问性。研究发现，视觉障碍用户在任务表现上与非障碍用户相当，但在信心和努力上较低，且特定颜色映射并未改善这种差异。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在关键决策中的应用增加，需要更易理解的AI解释，特别是针对视觉障碍用户。现有评估常忽视认知可访问性。

Method: 提出CUE模型，将解释属性（易读性、可读性、可解释性）与认知子过程联系起来。通过实验（N=455）测试不同颜色映射的热图，分析用户表现。

Result: 任务表现无显著差异，但视觉障碍用户信心/努力更低。特定颜色映射（如Cividis）未改善甚至加剧差异。结果表明需要自适应XAI界面。

Conclusion: 结果挑战了感知优化的假设，支持自适应XAI界面的必要性。CUE模型通过改变易读性影响理解性，为人类中心解释属性提供了定义和实证支持。

Abstract: As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI.

</details>


### [17] [WebXAII: an open-source web framework to study human-XAI interaction](https://arxiv.org/abs/2506.14777)
*Jules Leguy,Pierre-Antoine Jean,Felipe Torres Figueroa,Sébastien Harispe*

Main category: cs.HC

TL;DR: WebXAII是一个开源框架，旨在简化人类与可解释人工智能（XAI）系统交互的研究，提供实验协议的完整实现。


<details>
  <summary>Details</summary>
Motivation: 由于AI广泛应用的社会影响，XAI研究需求增长，但现有实验接口缺乏共享和可复用性。WebXAII旨在解决这一问题。

Method: 通过基于Web的平台设计通用视图和模块的复合架构，实验协议可通过结构化配置文件实现，无需高编程技能。

Result: WebXAII成功复现了一项前沿研究的协议，证明其有效性。

Conclusion: WebXAII为XAI交互研究提供了灵活且易于使用的工具，提升实验可复用性和可重复性。

Abstract: This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature. The framework is available at https://github.com/PAJEAN/WebXAII.

</details>


### [18] [Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust](https://arxiv.org/abs/2506.14799)
*Evdoxia Taka,Debadyuti Bhattacharya,Joanne Garde-Hansen,Sanjay Sharma,Tanaya Guha*

Main category: cs.HC

TL;DR: 论文提出了一种基于CLIP模型的工具，用于分析和可视化影视内容中的角色表征（如年龄和性别），并通过用户研究探讨了公众对AI生成结果的信任度和工具的有用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决公众对AI生成的性别和年龄表征数据的信任度和实用性疑问，提出一种新的分析和可视化工具。

Method: 利用CLIP模型分析视觉数据，设计可视化工具，并通过用户研究评估工具的有用性和信任度。

Result: 用户认为工具总体有用，但对AI模型的信任度为中低水平，同时希望增加更多人口统计类别和上下文信息。

Conclusion: 工具在角色表征分析上有效，但需进一步提升信任度和功能多样性。

Abstract: Recent advances in AI has enabled automated analysis of complex media content at scale and generate actionable insights regarding character representation along such dimensions as gender and age. Past work focused on quantifying representation from audio/video/text using various ML models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are they to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these questions through a user study, while proposing a new AI-based character representation and visualization tool. Our tool based on the Contrastive Language Image Pretraining (CLIP) foundation model to analyze visual screen data to quantify character representation across dimensions of age and gender. We also designed effective visualizations suitable for presenting such analytics to lay audience. Next, we conducted a user study to seek empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We note that participants were able to understand the analytics from our visualization, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and data from the user study can be found here: https://anonymous.4open.science/r/Character-Representation-Media-FF7B

</details>


### [19] [Impact of a Deployed LLM Survey Creation Tool through the IS Success Model](https://arxiv.org/abs/2506.14809)
*Peng Jiang,Vinicius Cezar Monteiro de Lira,Antonio Maiorino*

Main category: cs.HC

TL;DR: 论文探讨了利用大型语言模型（LLMs）自动化生成高质量调查问卷的实践，并通过实际部署评估其对信息系统的核心方法的影响。


<details>
  <summary>Details</summary>
Motivation: 当前人工创建高质量调查问卷需要大量时间和专业知识，LLMs的发展为自动化这一过程提供了新机会。

Method: 采用DeLone和McLean的信息系统成功模型评估LLM驱动的调查生成系统，并提出混合评估框架和安全措施。

Result: 首次将信息系统成功模型应用于生成AI调查系统，并提出一个结合自动化和人工评估的混合框架。

Conclusion: 生成AI系统能够加速数据收集并保持调查质量，但需结合人类评估和安全措施以确保有效集成。

Abstract: Surveys are a cornerstone of Information Systems (IS) research, yet creating high-quality surveys remains labor-intensive, requiring both domain expertise and methodological rigor. With the evolution of large language models (LLMs), new opportunities emerge to automate survey generation. This paper presents the real-world deployment of an LLM-powered system designed to accelerate data collection while maintaining survey quality. Deploying such systems in production introduces real-world complexity, including diverse user needs and quality control. We evaluate the system using the DeLone and McLean IS Success Model to understand how generative AI can reshape a core IS method. This study makes three key contributions. To our knowledge, this is the first application of the IS Success Model to a generative AI system for survey creation. In addition, we propose a hybrid evaluation framework combining automated and human assessments. Finally, we implement safeguards that mitigate post-deployment risks and support responsible integration into IS workflows.

</details>


### [20] [Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction](https://arxiv.org/abs/2506.14820)
*Hyeon Jeon,Hyunwook Lee,Yun-Hsin Kuo,Taehyun Yang,Daniel Archambault,Sungahn Ko,Takanori Fujiwara,Kwan-Liu Ma,Jinwook Seo*

Main category: cs.HC

TL;DR: 提出了一份指南，帮助初学者和实践者通过分类文献来可靠地使用降维（DR）进行视觉分析。


<details>
  <summary>Details</summary>
Motivation: 由于降维技术在视觉分析中可能存在失真等问题，文献中方法多样且广泛，初学者和研究人员往往不知从何入手。

Method: 基于先前文献分类，设计了一份指南，帮助用户评估自身DR知识水平并选择相关论文学习。

Result: 通过与三位DR和数据可视化专家的访谈，验证了指南的重要性、全面性和实用性。

Conclusion: 该指南为初学者和实践者提供了一个系统化的学习路径，有助于提升DR在视觉分析中的可靠性。

Abstract: Visual analytics using dimensionality reduction (DR) can easily be unreliable for various reasons, e.g., inherent distortions in representing the original data. The literature has thus proposed a wide range of methodologies to make DR-based visual analytics reliable. However, the diversity and extensiveness of the literature can leave novice analysts and researchers uncertain about where to begin and proceed. To address this problem, we propose a guide for reading papers for reliable visual analytics with DR. Relying on the previous classification of the relevant literature, our guide helps both practitioners to (1) assess their current DR expertise and (2) identify papers that will further enhance their understanding. Interview studies with three experts in DR and data visualizations validate the significance, comprehensiveness, and usefulness of our guide.

</details>


### [21] [The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities](https://arxiv.org/abs/2506.14829)
*Aditya Majumdar,Wenbo Zhang,Kashvi Prawal,Amulya Yadav*

Main category: cs.HC

TL;DR: 该论文分析了AI4SI研究中阻碍实际影响的多种因素，并提出了一些最佳实践和策略。


<details>
  <summary>Details</summary>
Motivation: 解决AI4SI项目难以在现实世界中实现规模化应用的挑战，并提升其社会影响力。

Method: 通过半结构化访谈和作者自身经验，采用主题分析法识别主要障碍。

Result: 识别出结构性、组织性、沟通、协作和运营等方面的问题，并提出应对策略。

Conclusion: 论文为AI4SI研究者和合作伙伴提供了一份实用指南，帮助更有效地开展社会影响力强的AI合作。

Abstract: In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects focus on harnessing AI to address societal issues in areas such as healthcare, social justice, etc. Unfortunately, despite growing interest in AI4SI, achieving tangible, on-the-ground impact remains a significant challenge. For example, identifying and engaging motivated collaborators who are willing to co-design and deploy AI based solutions in real-world settings is often difficult. Even when such partnerships are established, many AI4SI projects "fail" to progress beyond the proof-of-concept stage, and hence, are unable to transition to at-scale production-level solutions. Furthermore, the unique challenges faced by AI4SI researchers are not always fully recognized within the broader AI community, where such work is sometimes viewed as primarily applied and not aligning with the traditional criteria for novelty emphasized in core AI venues. This paper attempts to shine a light on the diverse challenges faced in AI4SI research by diagnosing a multitude of factors that prevent AI4SI partnerships from achieving real-world impact on the ground. Drawing on semi-structured interviews with six leading AI4SI researchers - complemented by the authors' own lived experiences in conducting AI4SI research - this paper attempts to understand the day-to-day difficulties faced in developing and deploying socially impactful AI solutions. Through thematic analysis, we identify structural and organizational, communication, collaboration, and operational challenges as key barriers to deployment. While there are no easy fixes, we synthesize best practices and actionable strategies drawn from these interviews and our own work in this space. In doing so, we hope this paper serves as a practical reference guide for AI4SI researchers and partner organizations seeking to engage more effectively in socially impactful AI collaborations.

</details>


### [22] [Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework](https://arxiv.org/abs/2506.14948)
*Mohna Chakraborty,Lu Wang,David Jurgens*

Main category: cs.HC

TL;DR: 提出了一个基于价值的框架来评估和改进大语言模型的道德推理能力，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的道德推理能力存在局限性，缺乏与人类道德推理的一致性，因此需要开发一种方法提升其道德决策能力。

Method: 通过基于价值系统、伦理理论和认知策略的分类提示基准，评估12个开源模型，并结合监督蒸馏方法将道德能力从大模型迁移到小模型。

Result: 具有明确道德结构的提示显著提升了模型的准确性和一致性，第一原理推理和特定伦理框架效果最佳；监督蒸馏方法成功实现了道德能力的迁移。

Conclusion: 该研究为大语言模型的可解释性和价值基础的道德推理提供了一条可扩展的路径。

Abstract: Large language models (LLMs) are increasingly deployed in domains requiring moral understanding, yet their reasoning often remains shallow, and misaligned with human reasoning. Unlike humans, whose moral reasoning integrates contextual trade-offs, value systems, and ethical theories, LLMs often rely on surface patterns, leading to biased decisions in morally and ethically complex scenarios. To address this gap, we present a value-grounded framework for evaluating and distilling structured moral reasoning in LLMs. We benchmark 12 open-source models across four moral datasets using a taxonomy of prompts grounded in value systems, ethical theories, and cognitive reasoning strategies. Our evaluation is guided by four questions: (1) Does reasoning improve LLM decision-making over direct prompting? (2) Which types of value/ethical frameworks most effectively guide LLM reasoning? (3) Which cognitive reasoning strategies lead to better moral performance? (4) Can small-sized LLMs acquire moral competence through distillation? We find that prompting with explicit moral structure consistently improves accuracy and coherence, with first-principles reasoning and Schwartz's + care-ethics scaffolds yielding the strongest gains. Furthermore, our supervised distillation approach transfers moral competence from large to small models without additional inference cost. Together, our results offer a scalable path toward interpretable and value-grounded models.

</details>


### [23] [Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output](https://arxiv.org/abs/2506.15008)
*Richa Gupta,Alexander Htet Kyaw*

Main category: cs.HC

TL;DR: 论文提出了一种结合DALL-E 3和材料数据集的新方法，通过在AI生成的设计中添加可持续性指标和材料使用数据，帮助设计师评估环境影响。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的生成式AI可以快速生成视觉效果，但缺乏设计师所需的可操作数据，尤其是可持续性方面的信息。

Method: 提出了一种流程，利用后处理模块从AI生成的图像中识别材料，并与二氧化碳当量（CO2e）数据匹配，为设计师提供环境影响的即时评估。

Result: 用户测试显示，引入可持续性指标可以促进更明智的设计决策，但也可能导致决策疲劳和满意度下降。

Conclusion: 研究证明了在AI辅助设计中平衡设计自由与可持续性约束的重要性，为更全面的数据驱动解决方案提供了路径。

Abstract: Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts. While generative AI can produce visually appealing images they often lack actionable data for designers In this work, we propose a novel pipeline that integrates DALL-E 3 with a materials dataset to enrich AI-generated designs with sustainability metrics and material usage insights. After the model generates an interior design image, a post-processing module identifies the top ten materials present and pairs them with carbon dioxide equivalent (CO2e) values from a general materials dictionary. This approach allows designers to immediately evaluate environmental impacts and refine prompts accordingly. We evaluate the system through three user tests: (1) no mention of sustainability to the user prior to the prompting process with generative AI, (2) sustainability goals communicated to the user before prompting, and (3) sustainability goals communicated along with quantitative CO2e data included in the generative AI outputs. Our qualitative and quantitative analyses reveal that the introduction of sustainability metrics in the third test leads to more informed design decisions, however, it can also trigger decision fatigue and lower overall satisfaction. Nevertheless, the majority of participants reported incorporating sustainability principles into their workflows in the third test, underscoring the potential of integrated metrics to guide more ecologically responsible practices. Our findings showcase the importance of balancing design freedom with practical constraints, offering a clear path toward holistic, data-driven solutions in AI-assisted architectural design.

</details>


### [24] [Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.15047)
*Jiayue Melissa Shi,Dong Whi Yoo,Keran Wang,Violeta J. Rodriguez,Ravi Karkar,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究开发了一款基于GPT-4o的聊天机器人Carey，用于支持阿尔茨海默病及相关痴呆症（AD/ADRD）患者家属的心理健康，并通过访谈分析了六类需求及设计建议。


<details>
  <summary>Details</summary>
Motivation: AD/ADRD患者家属面临巨大的情绪和后勤压力，现有生成式AI（如大语言模型）未被充分探索如何支持其心理健康。

Method: 开发了基于GPT-4o的聊天机器人Carey作为技术探针，通过半结构化访谈和情景驱动的交互，对16名家属进行调研，采用归纳编码和反射性主题分析。

Result: 提取了六类需求：实时信息获取、情感支持、安全披露空间、危机管理、个性化服务和数据隐私，并分析了需求与AI能力间的张力。

Conclusion: 研究为设计更主动、可信且以家属为中心的AI系统提供了理论与实践依据。

Abstract: Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers.

</details>


### [25] [Data Verbalisation: What is Text Doing in a Data Visualisation?](https://arxiv.org/abs/2506.15129)
*Paul Murrell*

Main category: cs.HC

TL;DR: 本文探讨了文本元素在数据可视化中的作用，并提出了一个易于理解和应用的框架来评估文本的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据可视化研究主要关注非文本元素（如条形、点和线），而缺乏对文本元素的系统解释。

Method: 通过分析文本在数据可视化中的应用案例，并结合现有知识和评估技术，研究文本的有效性。

Result: 提出了一种易于理解和应用的框架，用于评估数据可视化中文本元素的目的和效果。

Conclusion: 该框架为理解和优化数据可视化中的文本元素提供了实用工具。

Abstract: This article discusses the role that text elements play in a data visualisation. We argue that there is a need for a simple, coherent explanation of text elements similar to the understanding that already exists for non-text elements like bars, points, and lines. We explore examples of how text is used within a data visualisation and use existing knowledge and assessment techniques to evaluate when text is effective and when it is not. The result is a framework that aims to be easy to understand and easy to apply in order to understand the purpose and effectiveness of the text elements in any data visualisation.

</details>


### [26] [Accessible Gesture-Driven Augmented Reality Interaction System](https://arxiv.org/abs/2506.15189)
*Yikan Wang*

Main category: cs.HC

TL;DR: 本文提出一种基于手势的AR交互系统，通过深度学习识别手势并优化界面，显著提升运动障碍用户的使用效率和满意度。


<details>
  <summary>Details</summary>
Motivation: AR交互依赖精确输入方式，对运动障碍用户不友好。本研究旨在开发一种手势识别系统，提升AR的可访问性和适用性。

Method: 结合ViTs、TCNs和GATs处理手势数据，利用联邦学习保护隐私，强化学习优化界面布局和交互模式。

Result: 实验显示，系统比基准AR系统提升了20%的任务完成效率和25%的用户满意度。

Conclusion: 该方法有效提升了AR的可访问性和扩展性，适用于运动障碍用户。

Abstract: Augmented reality (AR) offers immersive interaction but remains inaccessible for users with motor impairments or limited dexterity due to reliance on precise input methods. This study proposes a gesture-based interaction system for AR environments, leveraging deep learning to recognize hand and body gestures from wearable sensors and cameras, adapting interfaces to user capabilities. The system employs vision transformers (ViTs), temporal convolutional networks (TCNs), and graph attention networks (GATs) for gesture processing, with federated learning ensuring privacy-preserving model training across diverse users. Reinforcement learning optimizes interface elements like menu layouts and interaction modes. Experiments demonstrate a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems. This approach enhances AR accessibility and scalability. Keywords: Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction

</details>


### [27] [Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces](https://arxiv.org/abs/2506.15293)
*Francesco Chiossi,Julian Rasch,Robin Welsch,Albrecht Schmidt,Florian Michahelles*

Main category: cs.HC

TL;DR: 论文提出了一种结构化方法，通过多维度设计空间（意图内容、规划范围和模态）指导动态协作场景中的意图通信设计，为透明人机交互奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着机器人进入协作工作空间，确保人与机器人系统之间的相互理解成为信任、安全和效率的前提。

Method: 基于情境感知代理透明度（SAT）框架和任务抽象层次的概念，提出一个多维设计空间（意图内容、规划范围和模态），并指导多模态通信策略的设计。

Result: 论文为未来的透明人机交互设计工具包奠定了基础，并提出了共享议程。

Conclusion: 研究强调了关键开放问题和设计挑战，提出了在混合工作环境中实现多模态、自适应和可信赖的机器人协作的共享议程。

Abstract: As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments.

</details>


### [28] [UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments](https://arxiv.org/abs/2506.15294)
*Jonas Lau,Annie Tran*

Main category: cs.HC

TL;DR: 论文探讨了使用UXR PoV Playbook框架进行功能优先级排序的UX研究方法，提出用MaxDiff（多项逻辑回归）解决传统调查技术的不足，并通过案例研究验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统调查技术在功能优先级排序中存在不足，需要一种更可靠且样本量合理的方法来提升用户体验研究的效率。

Method: 采用MaxDiff方法生成可靠的功能偏好列表，并通过调整方法减少一半的问卷回答量，减轻受访者负担。

Result: 案例研究表明，调整后的MaxDiff方法能有效用于用户（包括残障人士）的功能优先级研究。

Conclusion: MaxDiff方法是一种高效且用户友好的功能优先级排序工具，适用于消费产品开发。

Abstract: This paper discusses a popular UX research activity, feature prioritization, using the User Experience Research Point of View (UXR PoV) Playbook framework. We describe an application of multinomial logistic regression, frequently marketed as MaxDiff, for prioritizing product features in consumer product development. It addresses challenges of traditional surveying techniques. We propose a solution using MaxDiff to generate a reliable preference list with a reasonable sample size. We also adapt the MaxDiff method to reduce the number of survey responses in half, making it less tedious from the survey takers' perspective. We present a case study using the adapted MaxDiff method for tablet feature prioritization research involving users with disabilities.

</details>


### [29] [Case Study for Developing a UXR Point of View for FinOps Product Innovation](https://arxiv.org/abs/2506.15314)
*Jason Dong,Anna Wu*

Main category: cs.HC

TL;DR: 案例研究探讨了用户体验研究（UXR）观点在推动FinOps产品创新中的应用，结合定性与定量方法解决客户需求理解问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决云财务管理中客户需求多样化和资源有限的问题。

Method: 采用多阶段研究方法，结合定性和定量分析，识别机会、量化痛点并细分客户群体。

Result: 最终形成UXR观点，指导开发了一站式仪表盘，为FinOps从业者提供实用工具和洞察。

Conclusion: 研究表明混合方法研究能有效揭示驱动产品创新的关键洞察。

Abstract: In the dynamic landscape of Cloud financial management, we are sharing a case study exploring the development of a User Experience Research (UXR) Point of View (PoV) to drive FinOps product innovation. We demonstrate how qualitative and quantitative research methods working together to navigate the challenges of understanding customer needs, aligning cross-functional teams, and prioritizing limited resources. Through a multi-phased research approach, the research team identifies opportunities, quantifies pain points, and segments diverse customer cohorts. This culminated in a UXR PoV that informed the creation of a differentiated product strategy, a 'one-stop shop' dashboard empowering FinOps practitioners with actionable insights and tools. This case study highlights the power of mixed-methods research in uncovering actionable insights that drive impactful product innovation.

</details>


### [30] [Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook](https://arxiv.org/abs/2506.15325)
*Festus Adedoyin,Huseyin Dogan*

Main category: cs.HC

TL;DR: 该论文探讨了以人为中心的人工智能（HCAI）在金融行业中的个性化服务和产品开发中的应用及影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示HCAI如何通过数据分析、机器学习和自然语言处理等技术，帮助金融机构更好地理解客户需求并提升用户体验。

Method: 论文通过分析HCAI在数据分析和机器学习中的应用，结合行业实例，探讨了其在个性化金融解决方案、投资顾问服务和风险管理中的作用。

Result: 研究发现HCAI显著提升了金融产品的个性化程度，改善了用户体验，并增强了欺诈检测和风险管理能力。

Conclusion: HCAI为金融机构提供了一个战略框架，通过结合用户研究视角，可以更好地将AI技术与用户需求和业务目标对齐。

Abstract: Advancements in Artificial Intelligence (AI) have significantly transformed the financial industry, enabling the development of more personalised and adaptable financial products and services. This research paper explores various instances where Human-Centred AI (HCAI) has facilitated these advancements, drawing from contemporary studies and industry progress. The paper examines how the application of HCAI-powered data analytics, machine learning, and natural language processing enables financial institutions to gain a deeper understanding of their customers' unique needs, preferences, and behavioural patterns. This, in turn, allows for the creation of tailored financial solutions that address individual consumer requirements, ultimately enhancing overall user experience and satisfaction. Additionally, the study highlights the integration of AI-powered robo-advisory services, which offer customised investment recommendations and portfolio management tailored to diverse risk profiles and investment goals. Moreover, the paper underscores the role of AI in strengthening fraud detection, risk assessment, and regulatory compliance, leading to a more secure and adaptable financial landscape. The findings of this research demonstrate the substantial impact of Human-Centred AI on the financial industry, offering a strategic framework for financial institutions to leverage these technologies. By incorporating a User Experience Research (UXR) Point of View (PoV), financial institutions can ensure that AI-driven solutions align with user needs and business objectives.

</details>


### [31] [Building Blocks of a User Experience Research Point of View](https://arxiv.org/abs/2506.15332)
*Patricia Diaz*

Main category: cs.HC

TL;DR: 论文提出了三种基于数据、证据和洞察的用户体验研究（UXR）视角（POV），展示了在企业环境中构建POV的策略和方法如何发挥作用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在企业环境中利用数据和AI技术解决实际问题，并验证这些看似超前的POV如何被采纳并产生持久影响。

Method: 通过三个具体案例（智能视觉、可访问代码编辑器和机会景观）展示POV的构建和应用，结合AI技术实现高效率的用户反馈和需求挖掘。

Result: 所有提出的POV均被企业采纳，并实现了长期的实际影响，验证了其有效性。

Conclusion: 研究表明，看似超前的POV通过数据驱动和AI技术的结合，能够在企业环境中实现高效的用户体验改进和业务价值提升。

Abstract: This paper presents three User Experience Research (UXR) perspectives based on data, evidence and insights - known as Point of View (POV) - showcasing how the strategies and methods of building a POV work in an enterprise setting. The POV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in videos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the learner as it is the loop that requires the least effort for the highest impact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at the intersection of emergent technical capabilities that unlock novel approaches to critical user needs while addressing business strategic priorities (2019). They all seemed far-fetched and went against common practice. All were adopted and had long-lasting impact.

</details>


### [32] [Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI](https://arxiv.org/abs/2506.15468)
*Ryota Okumura,Tadahiro Taniguchi,Akira Taniguchi,Yoshinobu Hagiwara*

Main category: cs.HC

TL;DR: 论文提出了一种新的共同创造性学习范式，通过人类与AI的互动构建共享外部表示，解决了不同模态信息整合的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统AI教学基于单向知识传递，难以整合不同模态的信息。因此，作者提出共同创造性学习，以弥合人类与AI之间的感知差异。

Method: 使用基于Metropolis-Hastings命名游戏（MHNG）的分散式贝叶斯推理机制，进行人类-AI互动实验。69名参与者在部分可观测环境下与三种计算机代理类型之一互动。

Result: 基于MHNG的代理显著提高了分类准确性，并实现了更强的共享符号系统收敛。人类接受行为与MHNG推导的接受概率高度一致。

Conclusion: 研究为人类-AI共同创造性学习提供了首个实证证据，展示了通过动态对齐感知体验实现共生AI系统的潜力。

Abstract: We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shared external representations, a process we interpret as symbol emergence. Unlike traditional AI teaching based on unilateral knowledge transfer, this addresses the challenge of integrating information from inherently different modalities. We empirically test this framework using a human-AI interaction model based on the Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference mechanism. In an online experiment, 69 participants played a joint attention naming game (JA-NG) with one of three computer agent types (MH-based, always-accept, or always-reject) under partial observability. Results show that human-AI pairs with an MH-based agent significantly improved categorization accuracy through interaction and achieved stronger convergence toward a shared sign system. Furthermore, human acceptance behavior aligned closely with the MH-derived acceptance probability. These findings provide the first empirical evidence for co-creative learning emerging in human-AI dyads via MHNG-based interaction. This suggests a promising path toward symbiotic AI systems that learn with humans, rather than from them, by dynamically aligning perceptual experiences, opening a new venue for symbiotic AI alignment.

</details>


### [33] [Foundation of Affective Computing and Interaction](https://arxiv.org/abs/2506.15497)
*Changzeng Fu*

Main category: cs.HC

TL;DR: 本书全面探讨了情感计算与人机交互技术，涵盖历史发展、理论基础、关键技术、应用领域及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 旨在提供关于情感计算与人机交互的全面知识，促进技术发展并解决伦理挑战。

Method: 通过多模态技术（如视觉、语音、脑机接口）和心理学、神经科学理论，分析情感识别与交互。

Result: 详细介绍了情感计算的关键技术、应用场景及未来发展方向。

Conclusion: 强调技术创新与伦理平衡的重要性，推动情感计算技术的负责任发展与应用。

Abstract: This book provides a comprehensive exploration of affective computing and human-computer interaction technologies. It begins with the historical development and basic concepts of human-computer interaction, delving into the technical frameworks and practical applications of emotional computing, visual interaction, voice interaction, brain-computer interfaces, physiological electrical signal analysis, and social robotics. The book covers a wide range of topics, including the psychological and neuroscience foundations of emotion, multimodal emotion recognition, emotional expression mechanisms, and the principles of brain-computer interfaces.
  Key technologies such as affective computing based on discrete emotion theory and dimensional models, visual perception principles, speech recognition and synthesis, EEG signal acquisition and processing, and multimodal emotion recognition are explained in detail. This book also addresses the technical challenges in the field, including multimodal data fusion, privacy and security, and ethical considerations in human-machine relationships. It discusses the applications of these technologies across various domains such as education, healthcare, entertainment, and intelligent assistance.
  Looking to the future, the book anticipates trends such as the deep integration of artificial intelligence with emotion recognition, the advancement of multimodal interaction technologies, and the development of more personalized and adaptive emotion recognition systems. It emphasizes the importance of balancing technological innovation with ethical considerations to ensure the responsible development and application of affective computing technologies.

</details>


### [34] [Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach](https://arxiv.org/abs/2506.15512)
*Wenqi Guan,Yang Fang*

Main category: cs.HC

TL;DR: 本文提出了一种基于GPT和LangChain框架的远程学习资源检索改进方法，通过CoT推理和提示工程提升检索结果的精准度和相关性，并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 当前远程学习资源的检索缺乏深度的上下文信息，难以满足学生对复杂查询的需求。

Method: 利用GPT模型和LangChain框架，结合CoT推理与提示工程，优化检索系统。

Result: 与现有LLM相比，该方法在用户满意度和学习效果上均有显著提升。

Conclusion: 该方法能够提供更精准、上下文丰富的远程学习资源，满足学生个性化需求。

Abstract: Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each student's needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes.

</details>


### [35] ["How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students](https://arxiv.org/abs/2506.15525)
*Isabella Pu,Prerna Ravi,Linh Dieu Dinh,Chelsea Joe,Caitlin Ogoe,Zixuan Li,Cynthia Breazeal,Anastasia K. Ostrowski*

Main category: cs.HC

TL;DR: 论文探讨高中生对生成式AI（GenAI）的看法，通过参与式设计工作坊收集学生意见，提出针对教育技术设计的指南，并呼吁在学校AI政策制定中更多融入学生声音。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（GenAI）对高中环境有重大影响，但现有研究很少关注学生的观点。本文旨在填补这一空白，了解学生的需求和担忧。

Method: 通过参与式设计工作坊，17名高中生参与了设计GenAI工具和学校政策的讨论，提出问题和解决方案。

Result: 学生关注的四大问题空间包括：偏见与错误信息、犯罪与抄袭、过度依赖AI、学术不诚信的误判。基于此，提出针对教育技术设计的指南。

Conclusion: 应更多地纳入学生声音以优化GenAI工具和政策，为高中环境提供更有针对性的解决方案。

Abstract: As generative AI (GenAI) emerges as a transformative force, clear understanding of high school students' perspectives is essential for GenAI's meaningful integration in high school environments. In this work, we draw insights from a participatory design workshop where we engaged 17 high school students -- a group rarely involved in prior research in this area -- through the design of novel GenAI tools and school policies addressing their key concerns. Students identified challenges and developed solutions outlining their ideal features in GenAI tools, appropriate school use, and regulations. These centered around the problem spaces of combating bias & misinformation, tackling crime & plagiarism, preventing over-reliance on AI, and handling false accusations of academic dishonesty. Building on our participants' underrepresented perspectives, we propose new guidelines targeted at educational technology designers for development of GenAI technologies in high schools. We also argue for further incorporation of student voices in development of AI policies in their schools.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [36] [You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality](https://arxiv.org/abs/2506.15183)
*Xingyu Chen,Xinmin Fang,Shuting Zhang,Xinyu Zhang,Liang He,Zhengxiong Li*

Main category: cs.GR

TL;DR: EffVR提出了一种针对移动VR的渲染优化方法，通过单次渲染生成双眼图像，显著节省计算资源和能耗。


<details>
  <summary>Details</summary>
Motivation: 当前VR技术需要分别渲染双眼图像，对计算能力和电源有限的移动设备造成瓶颈。

Method: 利用像素属性，EffVR通过单次渲染从单眼图像生成双眼VR图像。

Result: 相比现有技术，EffVR平均节省27%能耗，图像质量高（SSIM 0.9679，PSNR 34.09），帧率提升115.2%。

Conclusion: EffVR在计算/节能方面表现卓越，为可持续移动VR发展铺平道路。

Abstract: Mobile Virtual Reality (VR) is essential to achieving convenient and immersive human-computer interaction and realizing emerging applications such as Metaverse. However, existing VR technologies require two separate renderings of binocular images, causing a significant bottleneck for mobile devices with limited computing capability and power supply. This paper proposes an approach to rendering optimization for mobile VR called EffVR. By utilizing the per-pixel attribute, EffVR can generate binocular VR images from the monocular image through genuinely one rendering, saving half the computation over conventional approaches. Our evaluation indicates that, compared with the state-of-art, EffVRcan save 27% power consumption on average while achieving high binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR applications. Additionally, EffVR can increase the frame rate by 115.2%. These results corroborate EffVRsuperior computation/energy-saving performance, paving the road to a sustainable mobile VR. The source code, demo video, android app, and more are released anonymously at https://yoro-vr.github.io/

</details>


### [37] [Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models](https://arxiv.org/abs/2506.15290)
*Andela Ilic,Jiaxi Jiang,Paul Streli,Xintong Liu,Christian Holz*

Main category: cs.GR

TL;DR: 本文提出了使用稀疏、松散固定的惯性测量单元（IMU）传感器进行全身人体姿态估计的新任务，并开发了基于Transformer的扩散模型来解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设IMU传感器紧贴人体，但现实中这一假设常不成立。因此，本文探索了松散固定IMU的实用场景。

Method: 通过从现有服装感知运动数据集中模拟IMU记录，开发了基于Transformer的扩散模型来合成松散IMU数据并估计人体姿态。同时引入服装相关参数以提升模型表达能力。

Result: 实验表明，所提出的方法在模拟和合成数据上训练后，优于现有方法，在定量和定性上均表现更优。

Conclusion: 本文为松散IMU传感器的姿态估计开辟了新方向，展示了其在未来研究中的潜力。

Abstract: Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present a new task of full-body human pose estimation using sparse, loosely attached IMU sensors. To solve this task, we simulate IMU recordings from an existing garment-aware human motion dataset. We developed transformer-based diffusion models to synthesize loose IMU data and estimate human poses based on this challenging loose IMU data. In addition, we show that incorporating garment-related parameters while training the model on simulated loose data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Experiments show that our proposed diffusion methods trained on simulated and synthetic data outperformed the state-of-the-art methods quantitatively and qualitatively, opening up a promising direction for future research.

</details>


### [38] [One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning](https://arxiv.org/abs/2506.15312)
*Han Wu,Junyao Li,Kangbo Zhao,Sen Zhang,Yukai Shi,Liang Lin*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的单次人脸素描合成方法，通过优化文本指令实现，并引入新的基准数据集OS-Sketch进行验证。


<details>
  <summary>Details</summary>
Motivation: 解决现有大规模判别学习方法因数据稀缺和人工成本高导致的生成性能下降问题。

Method: 基于扩散模型，通过梯度优化文本指令，使用单对照片-素描图像进行训练和推理。

Result: 方法能够在单次上下文中将多种照片转换为逼真且高度一致的素描，优于其他方法。

Conclusion: 该方法提供了更便捷和广泛的适用性，数据集将公开供研究使用。

Abstract: Face sketch synthesis is a technique aimed at converting face photos into sketches. Existing face sketch synthesis research mainly relies on training with numerous photo-sketch sample pairs from existing datasets. However, these large-scale discriminative learning methods will have to face problems such as data scarcity and high human labor costs. Once the training data becomes scarce, their generative performance significantly degrades. In this paper, we propose a one-shot face sketch synthesis method based on diffusion models. We optimize text instructions on a diffusion model using face photo-sketch image pairs. Then, the instructions derived through gradient-based optimization are used for inference. To simulate real-world scenarios more accurately and evaluate method effectiveness more comprehensively, we introduce a new benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark consists of 400 pairs of face photo-sketch images, including sketches with different styles and photos with different backgrounds, ages, sexes, expressions, illumination, etc. For a solid out-of-distribution evaluation, we select only one pair of images for training at each time, with the rest used for inference. Extensive experiments demonstrate that the proposed method can convert various photos into realistic and highly consistent sketches in a one-shot context. Compared to other methods, our approach offers greater convenience and broader applicability. The dataset will be available at: https://github.com/HanWu3125/OS-Sketch

</details>


### [39] [Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards](https://arxiv.org/abs/2506.15684)
*Qingming Liu,Zhen Liu,Dinghuai Zhang,Kui Jia*

Main category: cs.GR

TL;DR: 论文提出了一种名为Nabla-R2D3的强化学习对齐框架，用于改进3D扩散模型的质量和样本效率，通过2D奖励信号实现高效调整。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成模型在遵循指令、符合人类偏好及生成逼真纹理、几何和物理属性方面存在不足，需要改进。

Method: 基于Nabla-GFlowNet方法，提出Nabla-R2D3框架，利用2D奖励信号对3D扩散模型进行高效调整。

Result: 实验表明，Nabla-R2D3在少量调整步骤中能稳定获得更高奖励并减少先验遗忘，优于基线方法。

Conclusion: Nabla-R2D3为3D生成提供了一种高效且有效的强化学习对齐方法，显著提升了模型性能。

Abstract: Generating high-quality and photorealistic 3D assets remains a longstanding challenge in 3D vision and computer graphics. Although state-of-the-art generative models, such as diffusion models, have made significant progress in 3D generation, they often fall short of human-designed content due to limited ability to follow instructions, align with human preferences, or produce realistic textures, geometries, and physical attributes. In this paper, we introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement learning alignment framework for 3D-native diffusion models using 2D rewards. Built upon the recently proposed Nabla-GFlowNet method, which matches the score function to reward gradients in a principled manner for reward finetuning, our Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D reward signals. Extensive experiments show that, unlike vanilla finetuning baselines which either struggle to converge or suffer from reward hacking, Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting within a few finetuning steps.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Efficient Serving of LLM Applications with Probabilistic Demand Modeling](https://arxiv.org/abs/2506.14851)
*Yifei Liu,Zuo Gan,Zhenghao Gan,Weiye Wang,Chen Chen,Yizhou Shan,Xusheng Chen,Zhenhua Han,Yifei Zhu,Shixuan Sun,Minyi Guo*

Main category: cs.DC

TL;DR: 论文提出了一种基于概率需求图（PDGraph）的LLM应用服务系统Hermes，显著提高了服务效率。


<details>
  <summary>Details</summary>
Motivation: 现有系统将LLM应用的资源需求视为黑箱，导致排队顺序不当和后端预热延迟，影响效率。

Method: 使用PDGraph建模LLM应用的资源需求，并基于Gittins策略确定调度顺序以减少完成时间，同时利用PDGraph预热后端。

Result: 实验表明，Hermes能将平均完成时间降低70%以上，P95完成时间降低80%以上。

Conclusion: PDGraph和Hermes有效提升了LLM应用的资源调度和服务效率。

Abstract: Applications based on Large Language Models (LLMs) contains a series of tasks to address real-world problems with boosted capability, which have dynamic demand volumes on diverse backends. Existing serving systems treat the resource demands of LLM applications as a blackbox, compromising end-to-end efficiency due to improper queuing order and backend warm up latency. We find that the resource demands of LLM applications can be modeled in a general and accurate manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which leverages PDGraph for efficient serving of LLM applications. Confronting probabilistic demand description, Hermes applies the Gittins policy to determine the scheduling order that can minimize the average application completion time. It also uses the PDGraph model to help prewarm cold backends at proper moments. Experiments with diverse LLM applications confirm that Hermes can effectively improve the application serving efficiency, reducing the average completion time by over 70% and the P95 completion time by over 80%.

</details>


### [41] [Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching](https://arxiv.org/abs/2506.14852)
*Qizheng Zhang,Michael Wornow,Kunle Olukotun*

Main category: cs.DC

TL;DR: 本文提出了一种针对LLM代理应用的新缓存方法——代理计划缓存，通过提取、存储和重用结构化的计划模板，显著降低了服务成本（平均46.62%），同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM缓存技术（如上下文缓存和语义缓存）主要为聊天机器人设计，无法满足代理应用中依赖外部数据或环境上下文的复杂需求。因此，需要一种更高效的缓存方法。

Method: 提出代理计划缓存，从已完成的代理执行中提取计划模板，通过关键词匹配新请求，并利用轻量级模型将模板适配到任务特定计划中。

Result: 评估表明，该方法平均可降低46.62%的服务成本，同时保持性能。

Conclusion: 代理计划缓存是一个高效的解决方案，补充了现有LLM服务基础设施，适用于复杂的代理应用。

Abstract: LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.

</details>


### [42] [Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions](https://arxiv.org/abs/2506.14981)
*Hailiang Zhang,Dieu My T. Nguyen,Christine Smit,Mahabal Hegde*

Main category: cs.DC

TL;DR: 论文提出了一种高效处理大规模多维数据的通用方法，通过生成小型的累积和补充数据集，显著提升了计算性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 处理大规模多维数据时，传统的全扫描方法计算成本高，亟需一种高效且经济的替代方案。

Method: 提出了一种通用方法，通过生成可调大小的累积和补充数据集，优化计算过程。

Result: 在AWS上实现的方法性能比传统方法快3-4个数量级，且仅需5%的额外存储。

Conclusion: 该方法特别适合云环境中的分块数据格式，显著提升了大规模数据处理的效率和成本效益。

Abstract: Data analysis on massive multi-dimensional data, such as high-resolution large-region time averaging or area averaging for geospatial data, often involves calculations over a significant number of data points. While performing calculations in scalable and flexible distributed or cloud environments is a viable option, a full scan of large data volumes still serves as a computationally intensive bottleneck, leading to significant cost. This paper introduces a generic and comprehensive method to address these computational challenges. This method generates a small, size-tunable supplementary dataset that stores the cumulative sums along specific subset dimensions on top of the raw data. This minor addition unlocks rapid and cheap high-resolution large-region data analysis, making calculations over large numbers of data points feasible with small instances or even microservices in the cloud. This method is general-purpose, but is particularly well-suited for data stored in chunked, cloud-optimized formats and for services running in distributed or cloud environments. We present a Zarr extension proposal to integrate the specifications of this method and facilitate its straightforward implementation in general-purpose software applications. Benchmark tests demonstrate that this method, implemented in Amazon Web services (AWS), significantly outperforms the brute-force approach used in on-premises services. With just 5% supplemental storage, this method achieves a performance that is 3-4 orders of magnitude (~10,000 times) faster than the brute-force approach, while incurring significantly reduced computational costs.

</details>


### [43] [Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library](https://arxiv.org/abs/2506.15114)
*Youjia Li,Robert Latham,Robert Ross,Ankit Agrawal,Alok Choudhary,Wei-Keng Liao*

Main category: cs.DC

TL;DR: 高层次的I/O库（如HDF5和PnetCDF）在大规模科学应用中用于并行I/O任务，但对多进程独立创建大量数据对象的效率不高。本文基于PnetCDF提出了一种新的文件头部格式，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有高层次的I/O库在多进程独立创建大量数据对象时效率低下，尤其是需要保证元数据一致性的场景。本文旨在解决这一局限性。

Method: 提出了一种新的文件头部格式，包含索引表和元数据块列表，支持独立或集体创建数据对象。

Result: 新设计在4096个MPI进程下并行创建5,684,800个数据对象时，性能提升达582倍，并减少了内存占用。

Conclusion: 新方法显著提升了多进程独立创建数据对象的性能，且内存占用更低。

Abstract: High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by large-scale scientific applications to perform I/O tasks in parallel. These I/O libraries store the metadata such as data types and dimensionality along with the raw data in the same files. While these libraries are well-optimized for concurrent access to the raw data, they are designed neither to handle a large number of data objects efficiently nor to create different data objects independently by multiple processes, as they require applications to call data object creation APIs collectively with consistent metadata among all processes. Applications that process data gathered from remote sensors, such as particle collision experiments in high-energy physics, may generate data of different sizes from different sensors and desire to store them as separate data objects. For such applications, the I/O library's requirement on collective data object creation can become very expensive, as the cost of metadata consistency check increases with the metadata volume as well as the number of processes. To address this limitation, using PnetCDF as an experimental platform, we investigate solutions in this paper that abide the netCDF file format, as well as propose a new file header format that enables independent data object creation. The proposed file header consists of two sections, an index table and a list of metadata blocks. The index table contains the reference to the metadata blocks and each block stores metadata of objects that can be created collectively or independently. The new design achieves a scalable performance, cutting data object creation times by up to 582x when running on 4096 MPI processes to create 5,684,800 data objects in parallel. Additionally, the new method reduces the memory footprints, with each process requiring an amount of memory space inversely proportional to the number of processes.

</details>


### [44] [eLLM: Elastic Memory Management Framework for Efficient LLM Serving](https://arxiv.org/abs/2506.15155)
*Jiale Xu,Rui Zhang,Yi Xiong,Cong Guo,Zihan Liu,Yangjie Zhou,Weiming Hu,Hao Wu,Changxu Shao,Ziqing Wang,Yongjie Yuan,Junping Zhao,Minyi Guo,Jingwen Leng*

Main category: cs.DC

TL;DR: 为解决大语言模型在数据中心部署时的内存管理问题，提出了一个弹性内存管理框架eLLM，显著提升了内存利用率和解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内存管理中，动态组件（激活和KV缓存）和静态权重分离管理导致内存利用率低，影响性能。

Method: 提出eLLM框架，包括虚拟张量抽象、弹性内存机制和轻量级调度策略，动态调整内存分配并优化利用。

Result: eLLM在解码吞吐量上比现有系统高2.32倍，支持3倍大的批量处理128K-token输入。

Conclusion: eLLM通过统一内存管理和动态调整机制，显著提升了内存利用率和处理效率。

Abstract: Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput.
  To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs.

</details>


### [45] [RISC-V for HPC: An update of where we are and main action points](https://arxiv.org/abs/2506.15418)
*Nick Brown*

Main category: cs.DC

TL;DR: 分析RISC-V在HPC领域的现状与限制，强调未来的努力方向。


<details>
  <summary>Details</summary>
Motivation: 评估RISC-V生态系统在高性能计算（HPC）中的当前状态与不足，以指导未来发展。

Method: 由RISC-V HPC SIG牵头进行分析。

Result: 尽管近年有显著进展，但仍存在限制，需进一步努力。

Conclusion: 需关注RISC-V在HPC中的短板，集中资源推动生态完善。

Abstract: This extended abstract is submitted on behalf of the RISC-V HPC SIG who have been undertaking an analysis to explore the current state and limitations of the RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has been great progress made in recent years, we also highlight limitations and where effort should be focussed.

</details>


### [46] [Exploring Fast Fourier Transforms on the Tenstorrent Wormhole](https://arxiv.org/abs/2506.15437)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 本文探讨了将Cooley-Tukey FFT算法移植到RISC-V加速器上的效果，展示了其在功耗和能源效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管RISC-V ISA在多个计算领域广泛采用，但尚未在HPC中普及。RISC-V加速器为HPC提供了无需大规模生态变化的专业化方案。

Method: 研究人员将FFT算法移植到Tenstorrent Wormhole PCIe RISC-V加速器上，优化数据移动的瓶颈问题。

Result: 虽然Wormhole n300在处理2D FFT时速度不及24核Xeon Platinum CPU，但其功耗和能源消耗分别降低8倍和2.8倍。

Conclusion: RISC-V加速器在HPC中具有显著的能效优势，适合特定场景的应用。

Abstract: Whilst numerous areas of computing have adopted the RISC-V Instruction Set Architecture (ISA) wholesale in recent years, it is yet to become widespread in HPC. RISC-V accelerators offer a compelling option where the HPC community can benefit from the specialisation offered by the open nature of the standard but without the extensive ecosystem changes required when adopting RISC-V CPUs. In this paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT) algorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon Tenstorrent's Tensix architecture, this technology decouples the movement of data from compute, potentially offering increased control to the programmer. Exploring different optimisation techniques to address the bottlenecks inherent in data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is slower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around 8 times less power and consumes around 2.8 times less energy than the CPU when computing the Fourier transform.

</details>


### [47] [Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA](https://arxiv.org/abs/2506.15454)
*Nizar ALHafez,Ahmad Kurdi*

Main category: cs.DC

TL;DR: 本文比较了HPC中的三种主流并行编程模型：MPI、OpenMP和CUDA，分析了它们的优缺点及适用场景，并指出混合方法在异构环境中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现代异构HPC架构中如何选择最优编程方法变得越来越重要。

Method: 系统对比MPI、OpenMP和CUDA在架构基础、性能特点、领域适用性、编程复杂度和最新进展等方面的表现。

Result: MPI在分布式内存环境中表现优异，OpenMP擅长共享内存系统，CUDA适用于GPU任务，而混合方法通常最优。

Conclusion: 最佳编程模型取决于应用需求、硬件条件和开发限制，混合方法更适合现代HPC应用。

Abstract: This paper presents a comprehensive comparison of three dominant parallel programming models in High Performance Computing (HPC): Message Passing Interface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device Architecture (CUDA). Selecting optimal programming approaches for modern heterogeneous HPC architectures has become increasingly critical. We systematically analyze these models across multiple dimensions: architectural foundations, performance characteristics, domain-specific suitability, programming complexity, and recent advancements. We examine each model's strengths, weaknesses, and optimization techniques. Our investigation demonstrates that MPI excels in distributed memory environments with near-linear scalability for communication-intensive applications, but faces communication overhead challenges. OpenMP provides strong performance and usability in shared-memory systems and loop-centric tasks, though it is limited by shared memory contention. CUDA offers substantial performance gains for data-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires specialized expertise. Performance evaluations across scientific simulations, machine learning, and data analytics reveal that hybrid approaches combining two or more models often yield optimal results in heterogeneous environments. The paper also discusses implementation challenges, optimization best practices, and emerging trends such as performance portability frameworks, task-based programming, and the convergence of HPC and Big Data. This research helps developers and researchers make informed decisions when selecting programming models for modern HPC applications, emphasizing that the best choice depends on application requirements, hardware, and development constraints.

</details>


### [48] [All is Not Lost: LLM Recovery without Checkpoints](https://arxiv.org/abs/2506.15461)
*Nikolay Blagoev,Oğuzhan Ersoy,Lydia Yiyu Chen*

Main category: cs.DC

TL;DR: 提出了一种名为CheckFree的高效恢复方法，通过加权平均邻近阶段来替代故障阶段，无需额外计算或存储。


<details>
  <summary>Details</summary>
Motivation: 解决在分布式弱计算节点上训练大语言模型时，节点故障导致的阶段丢失问题，降低传统恢复方法的开销。

Method: CheckFree通过加权平均邻近阶段来恢复故障阶段；CheckFree+进一步结合乱序流水线执行以容忍首末阶段的故障。

Result: 在低至中等故障率（5-10%）下，CheckFree和CheckFree+在收敛时间上优于传统方法，提升超过12%。

Conclusion: CheckFree和CheckFree+为分布式训练提供了高效且低开销的故障恢复方案，尤其适用于大模型场景。

Abstract: Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree.

</details>


### [49] [Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation](https://arxiv.org/abs/2506.15488)
*Hussam Al Daas,Grey Ballard,Laura Grigori,Suraj Kumar,Kathryn Rouse,Mathieu Vérité*

Main category: cs.DC

TL;DR: 该论文研究了3维对称张量的两种模式上乘同一向量的并行通信成本，建立了通信下界，并展示了最优算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了在高阶幂方法和对称CP分解的梯度方法中，优化3维对称张量的计算效率。

Method: 方法是通过扩展3维对称计算的关键几何不等式，建立通信下界，并设计了一个基于三角形块分配的最优算法。

Result: 结果表明，提出的通信下界是紧的，并通过算法验证了其最优性。

Conclusion: 结论是，扩展的三角形块分配方案能够有效满足3维对称张量计算的通信需求。

Abstract: In this article, we focus on the parallel communication cost of multiplying the same vector along two modes of a $3$-dimensional symmetric tensor. This is a key computation in the higher-order power method for determining eigenpairs of a $3$-dimensional symmetric tensor and in gradient-based methods for computing a symmetric CP decomposition. We establish communication lower bounds that determine how much data movement is required to perform the specified computation in parallel. The core idea of the proof relies on extending a key geometric inequality for $3$-dimensional symmetric computations. We demonstrate that the communication lower bounds are tight by presenting an optimal algorithm where the data distribution is a natural extension of the triangle block partition scheme for symmetric matrices to 3-dimensional symmetric tensors.

</details>


### [50] [Automatic Metadata Capture and Processing for High-Performance Workflows](https://arxiv.org/abs/2506.15537)
*Polina Shpilker,Line Pouchard*

Main category: cs.DC

TL;DR: 该论文旨在通过开发软件收集HPC系统上工作流的元数据注释，应用FAIR原则提升研究可重复性。


<details>
  <summary>Details</summary>
Motivation: 异构计算架构增加了工作流的复杂性，需要统一方法来管理和分析工作流性能。

Method: 试验了两种元数据存储格式，并优化其结构以便研究人员更容易使用。

Result: 开发了一种软件工具，能够收集并优化HPC工作流的元数据。

Conclusion: 通过统一的元数据收集和优化，有助于提升工作流性能研究的效率和可重复性。

Abstract: Modern workflows run on increasingly heterogeneous computing architectures and with this heterogeneity comes additional complexity. We aim to apply the FAIR principles for research reproducibility by developing software to collect metadata annotations for workflows run on HPC systems. We experiment with two possible formats to uniformly store these metadata, and reorganize the collected metadata to be as easy to use as possible for researchers studying their workflow performance.

</details>


### [51] [LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters](https://arxiv.org/abs/2506.15595)
*Kunming Zhang,Hanlong Liao,Guoming Tang*

Main category: cs.DC

TL;DR: LiteGD是一个动态GPU调度系统，通过全局视角优化多GPU并行计算中的通信延迟问题，在大规模异构集群中显著提升带宽利用率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于物理邻近性的GPU调度方法在大规模异构集群中存在局限性，通信带宽分布不均导致效率低下。LiteGD旨在解决这一问题。

Method: LiteGD采用轻量级Transformer网络存储GPU拓扑信息，并结合双向树搜索算法动态优化调度决策。

Result: 实验表明，LiteGD在不同集群配置下带宽利用率达90%（真实H100集群中为80%），优于传统方法。

Conclusion: LiteGD通过轻量级全局优化设计，显著提升大规模异构环境下的GPU通信效率。

Abstract: Parallel computing with multiple GPUs has become the dominant paradigm for machine learning tasks, especially those of large language models (LLMs). To reduce the latency incurred by inter-GPU communication, a common practice for parallel tasks has been to allocate GPUs based on their physical proximity. However, this long-standing assumption has notable limitations, particularly in large-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs is irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU dispatching system based on global perspectives. To tackle the difficulty of storing massive GPU topology information, LiteGD adopts a computation-aware design that leverages a lightweight Transformer network trained on sampled data. Our customized design for network structure ensures both transferability and scalability. LiteGD also employs a bidirectional tree search approach to find the optimal GPU dispatching in the data generated in the previous step, which can identify near-optimal solutions while reducing search overhead. We implement and evaluate LiteGD in both real and simulated GPU clusters with homogeneous and heterogeneous interconnects, respectively. Experimental results demonstrate that LiteGD consistently achieves high GPU bandwidth efficacy (approximately 90\%) across various cluster configurations and 80\% in real-world H100 cluster, significantly outperforming conventional default and interconnect topology-aware dispatching methods, particularly in large-scale heterogeneous environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [SimBank: from Simulation to Solution in Prescriptive Process Monitoring](https://arxiv.org/abs/2506.14772)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.DB

TL;DR: 介绍了一种名为SimBank的模拟器，用于对Prescriptive Process Monitoring（PresPM）方法进行准确基准测试，填补了现有文献中的技术比较和评估方法不足的空白。


<details>
  <summary>Details</summary>
Motivation: PresPM领域缺乏技术间的广泛比较和评估方法，亟需工具来解决这些问题。

Method: 设计了一个模拟银行贷款流程的模拟器SimBank，支持对在线和离线PresPM方法进行广泛比较，并包含不同复杂度的干预优化问题。

Result: SimBank能够生成每种干预下的真实结果，支持评估方法对数据混杂的鲁棒性，并通过实验证明了其基准测试能力。

Conclusion: SimBank是一个公开可用的模拟器，有助于推动PresPM领域的研究和实践。

Abstract: Prescriptive Process Monitoring (PresPM) is an emerging area within Process Mining, focused on optimizing processes through real-time interventions for effective decision-making. PresPM holds significant promise for organizations seeking enhanced operational performance. However, the current literature faces two key limitations: a lack of extensive comparisons between techniques and insufficient evaluation approaches. To address these gaps, we introduce SimBank: a simulator designed for accurate benchmarking of PresPM methods. Modeled after a bank's loan application process, SimBank enables extensive comparisons of both online and offline PresPM methods. It incorporates a variety of intervention optimization problems with differing levels of complexity and supports experiments on key causal machine learning challenges, such as assessing a method's robustness to confounding in data. SimBank additionally offers a comprehensive evaluation capability: for each test case, it can generate the true outcome under each intervention action, which is not possible using recorded datasets. The simulator incorporates parallel activities and loops, drawing from common logs to generate cases that closely resemble real-life process instances. Our proof of concept demonstrates SimBank's benchmarking capabilities through experiments with various PresPM methods across different interventions, highlighting its value as a publicly available simulator for advancing research and practice in PresPM.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Scaling Intelligence: Designing Data Centers for Next-Gen Language Models](https://arxiv.org/abs/2506.15006)
*Jesmin Jahan Tithi,Hanjiang Wu,Avishaii Abuhatzera,Fabrizio Petrini*

Main category: cs.AR

TL;DR: 论文探讨了面向大型语言模型（LLM）的数据中心架构优化，提出了一种联合设计框架，通过分析FLOPS、HBM带宽与容量、网络拓扑结构等，评估了FullFlat网络架构对性能和可扩展性的显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模的爆炸式增长（如GPT-4达1.8万亿参数），传统数据中心架构难以满足可扩展性、效率和成本效益的需求，亟需重新设计。

Method: 提出了一种联合设计框架，研究FLOPS、HBM带宽与容量、网络拓扑结构等，并引入FullFlat网络架构，评估其对性能的影响。通过敏感性分析量化了计算与通信重叠、硬件加速集体通信等技术的作用。

Result: FullFlat网络架构显著提升了性能和可扩展性。性能建模工具预测误差在10%以内，为支持万亿参数模型提供了可行方案。

Conclusion: 研究为高效支持超大参数LLM的数据中心设计提供了实践指导和优化方向，降低了复杂性并适应AI能力快速演进的需求。

Abstract: The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8 trillion parameters - demands a radical rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, wider scale-out domains, and larger memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens per sec / Peak flops of the hardware) and overall throughput. For the co-design study, we extended and validated a performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.

</details>


### [54] [ChatModel: Automating Reference Model Design and Verification with LLMs](https://arxiv.org/abs/2506.15066)
*Jianmin Ye,Tianyang Liu,Qi Tian,Shengchu Su,Zhe Jiang,Xi Wang*

Main category: cs.AR

TL;DR: 论文介绍了ChatModel，一个基于大语言模型的敏捷参考模型生成与验证平台，显著提升了参考模型生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路设计复杂度增加，功能验证变得更具挑战性，传统参考模型开发耗时且复杂。

Method: ChatModel通过设计标准化和层次化敏捷建模，采用模块化生成策略，优化LLM的参考模型生成能力。

Result: 在300个不同复杂度设计中，ChatModel效率提升55.02%，生成稳定性增强，参考模型设计能力提升9.18倍，迭代速度提升5.90倍。

Conclusion: ChatModel展示了在参考模型生成与验证自动化方面的巨大潜力。

Abstract: As the complexity of integrated circuit designs continues to escalate, the functional verification becomes increasingly challenging. Reference models, critical for accelerating the verification process, are themselves becoming more intricate and time-consuming to develop. Despite the promise shown by large language models (LLMs) in code programming, effectively generating complex reference models remains a significant hurdle. To address these challenges, we introduce ChatModel, the first LLM-aided agile reference model generation and verification platform. ChatModel streamlines the transition from design specifications to fully functional reference models by integrating design standardization and hierarchical agile modeling. Employing a building-block generation strategy, it not only enhances the design capabilities of LLMs for reference models but also significantly boosts verification efficiency. We evaluated ChatModel on 300 designs of varying complexity, demonstrating substantial improvements in both efficiency and quality of reference model generation. ChatModel achieved a peak performance improvement of 55.02% compared to alternative methods, with notable enhancements in generation stability, and delivered a 9.18x increase in its capacity to produce reference model designs. Furthermore, it accelerated the iterative process of reference model design and validation by an average of 5.90x compared to traditional approaches. These results highlight the potential of ChatModel to significantly advance the automation of reference model generation and validation.

</details>


### [55] [J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor](https://arxiv.org/abs/2506.15316)
*Benoit Tain,Raphael Millet,Romain Lemaire,Michal Szczepanski,Laurent Alacoque,Emmanuel Pluchart,Sylvain Choisnet,Rohit Prasad,Jerome Chossat,Pascal Pierunek,Pascal Vivet,Sebastien Thuries*

Main category: cs.AR

TL;DR: J3DAI是一种基于微小深度神经网络的硬件加速器，专为3D堆叠CMOS图像传感器设计，具备高性能、低功耗和小面积特点，结合Aidge软件框架实现高效边缘AI处理。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种高效的边缘AI硬件加速器，以满足实时、低延迟和低功耗的AI处理需求，特别是在CMOS图像传感器上的应用。

Method: 方法包括设计一个基于DNN的硬件加速器，利用Aidge软件框架支持模型部署，并采用训练后量化技术减少计算复杂度和内存占用。

Result: 实验结果显示J3DAI在边缘AI领域表现出高效和多功能性，能够处理从简单到计算密集的任务。

Conclusion: J3DAI展示了在边缘AI领域的潜力，未来工作将优化架构并探索新应用，以适应边缘AI的快速发展需求。

Abstract: This paper presents J3DAI, a tiny deep neural network-based hardware accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial intelligence (AI) chip integrating a Deep Neural Network (DNN)-based accelerator. The DNN accelerator is designed to efficiently perform neural network tasks such as image classification and segmentation. This paper focuses on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA) characteristics and showcasing advanced edge AI capabilities on a CMOS image sensor. To support hardware, we utilized the Aidge comprehensive software framework, which enables the programming of both the host processor and the DNN accelerator. Aidge supports post-training quantization, significantly reducing memory footprint and computational complexity, making it crucial for deploying models on resource-constrained hardware like J3DAI. Our experimental results demonstrate the versatility and efficiency of this innovative design in the field of edge AI, showcasing its potential to handle both simple and computationally intensive tasks. Future work will focus on further optimizing the architecture and exploring new applications to fully leverage the capabilities of J3DAI. As edge AI continues to grow in importance, innovations like J3DAI will play a crucial role in enabling real-time, low-latency, and energy-efficient AI processing at the edge.

</details>


### [56] [Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration](https://arxiv.org/abs/2506.15440)
*Omar Numan,Gaurav Singh,Kazybek Adam,Jelin Leslin,Aleksi Korsman,Otto Simola,Marko Kosunen,Jussi Ryynänen,Martin Andraud*

Main category: cs.AR

TL;DR: 本文提出了一种自校准混合信号计算内存（CIM）加速器SoC，解决电阻和非易失性存储器（eNVM）CIM核心的集成与可靠性挑战。


<details>
  <summary>Details</summary>
Motivation: 为加速AI任务，尤其是深度神经网络（DNNs），需要高效的计算内存架构，但现有混合信号CIM核心面临集成和可靠性问题。

Method: 采用22纳米FDSOI技术，结合SRAM的存储优势与线性电阻的多位计算，并通过RISC-V控制芯片校准提升可靠性。

Result: 计算信噪比（SNR）提高了25%到45%，达到18-24 dB。

Conclusion: 该CIM架构为高性能AI加速提供了可行的解决方案，并可扩展到高密度线性电阻技术。

Abstract: Developing accurate and reliable Compute-In-Memory (CIM) architectures is becoming a key research focus to accelerate Artificial Intelligence (AI) tasks on hardware, particularly Deep Neural Networks (DNNs). In that regard, there has been significant interest in analog and mixed-signal CIM architectures aimed at increasing the efficiency of data storage and computation to handle the massive amount of data needed by DNNs. Specifically, resistive mixed-signal CIM cores are pushed by recent progresses in emerging Non-Volatile Memory (eNVM) solutions. Yet, mixed-signal CIM computing cores still face several integration and reliability challenges that hinder their large-scale adoption into end-to-end AI computing systems. In terms of integration, resistive and eNVM-based CIM cores need to be integrated with a control processor to realize end-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still more efficient and easier to program than their eNVM counterparts. In terms of reliability, analog circuits are more susceptible to variations, leading to computation errors and degraded accuracy. This work addresses these two challenges by proposing a self-calibrated mixed-signal CIM accelerator SoC, fabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the CIM architecture, combining the density and ease of SRAM-based weight storage with multi-bit computation using linear resistors, and (2) an open-source programming and testing strategy for CIM systems. The accuracy and reliability are enabled through an automated RISC-V controlled on-chip calibration, allowing us to improve the compute SNR by 25 to 45% across multiple columns to reach 18-24 dB. To showcase further integration possibilities, we show how our proof-of-concept SoC can be extended to recent high-density linear resistor technologies for enhanced computing performance.

</details>


### [57] [CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies](https://arxiv.org/abs/2506.15601)
*Donghyun Gouk,Seungkwan Kang,Seungjun Lee,Jiseon Kim,Kyungkuk Nam,Eojin Ryu,Sangwon Lee,Dongpyung Kim,Junhyeok Jang,Hanyeoreum Bae,Myoungsoo Jung*

Main category: cs.AR

TL;DR: 本文介绍了一种利用CXL的GPU存储扩展解决方案，提出了多CXL根端口的GPU系统设计，首次实现两位纳秒级往返延迟，并通过读预测和确定性存储机制优化读写性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决GPU存储扩展的需求，并克服现有技术中性能和延迟的局限性，研究提出了基于CXL的创新方案。

Method: 设计了多CXL根端口的GPU系统，开发了硬件RTL级别的CXL控制器，并采用了读预测和确定性存储机制。

Result: 性能评估显示，该方法显著优于现有技术，实现了两位纳秒级的往返延迟。

Conclusion: 这项研究为GPU存储技术带来了重要进展，展示了CXL在GPU存储扩展中的潜力。

Abstract: This work introduces a GPU storage expansion solution utilizing CXL, featuring a novel GPU system design with multiple CXL root ports for integrating diverse storage media (DRAMs and/or SSDs). We developed and siliconized a custom CXL controller integrated at the hardware RTL level, achieving two-digit nanosecond roundtrip latency, the first in the field. This study also includes speculative read and deterministic store mechanisms to efficiently manage read and write operations to hide the endpoint's backend media latency variation. Performance evaluations reveal our approach significantly outperforms existing methods, marking a substantial advancement in GPU storage technology.

</details>


### [58] [From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation](https://arxiv.org/abs/2506.15613)
*Miryeong Kwon,Donghyun Gouk,Junhyeok Jang,Jinwoo Baek,Hyunwoo You,Sangyoon Ji,Hongjoo Jung,Junseok Moon,Seungkwan Kang,Seungjun Lee,Myoungsoo Jung*

Main category: cs.AR

TL;DR: 探讨如何通过CXL将PCIe块存储扩展为可伸缩的字节寻址工作内存，通过CXL-SSD和注释机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决块存储适应CXL内存中心模型的挑战，推动内存-存储融合。

Method: 采用FPGA原型和注释机制（Determinism和Bufferability）优化性能。

Result: CXL-SSD性能比PCIe内存扩展器高10.9倍，延迟降低5.4倍；高效缓存使其接近DRAM性能。

Conclusion: CXL-SSD为内存-存储融合提供可行方案，奠定未来研究基础。

Abstract: This paper explores how Compute Express Link (CXL) can transform PCIe-based block storage into a scalable, byte-addressable working memory. We address the challenges of adapting block storage to CXL's memory-centric model by emphasizing cacheability as a key enabler and advocating for Type 3 endpoint devices, referred to as CXL-SSDs. To validate our approach, we prototype a CXL-SSD on a custom FPGA platform and propose annotation mechanisms, Determinism and Bufferability, to enhance performance while preserving data persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves 10.9x better performance than PCIe-based memory expanders and further reduces latency by 5.4x with annotation enhancements. In workloads with high locality, CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This work highlights the feasibility of integrating block storage into CXL's ecosystem and provides a foundation for future memory-storage convergence.

</details>


### [59] [SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy](https://arxiv.org/abs/2506.15634)
*Hasnain A. Ziad,Alexander C. Bodoh,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出了一种基于选择性冗余的新型容错NCL架构，相比现有的基于复制的NCL设计，在面积和能耗方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 完全冗余的QDI异步电路设计虽然能实现完全弹性，但带来显著的能耗、延迟和面积开销。

Method: 采用选择性冗余的NCL架构。

Result: 在图像处理应用中，该方法在面积和能耗利用率上优于现有方法。

Conclusion: 选择性冗余的NCL架构是一种高效的容错方案。

Abstract: Duplication-based redundancy schemes have proven to be effective in designing fully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The complete resiliency, however, is accompanied by significant energy, latency, and area overhead. This paper presents a novel error-tolerant Null Convention Logic (NCL) architecture based on selective redundancy. Results demonstrate the efficacy of the proposed method in terms of area and energy utilization as compared to existing duplication-based NCL designs, targeting an image processing application.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [60] [On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing](https://arxiv.org/abs/2506.14928)
*Dyk Chung Nguyen,Thomas Chetaille,Yuan-Hang Zhang,Yuriy V. Pershin,Massimiliano Di Ventra*

Main category: nlin.CD

TL;DR: 本文研究了数字内存计算机器（DMMs）在数值噪声和物理噪声下的性能表现，重点分析了噪声如何导致系统从成功解决问题的混沌转变。通过功率谱和李雅普诺夫指数的分析，发现噪声强度与系统可解性相关。


<details>
  <summary>Details</summary>
Motivation: 研究DMMs在实际应用中的性能，需要了解数值误差和物理噪声如何影响其解决问题的能力。

Method: 通过调整积分时间步长（数值噪声）和引入随机扰动（物理噪声）分析DMMs的性能，重点观察功率谱和李雅普诺夫指数的变化。

Result: 研究发现噪声强度与系统可解性相关，且在某些情况下，即使李雅普诺夫指数为正值，系统仍能保持可解性。功率谱能有效区分系统的规则和混沌行为。

Conclusion: 数值噪声和物理噪声对DMMs的定性影响相似，功率谱可用于优化DMMs的动态工作状态。

Abstract: Digital memcomputing machines (DMMs) have been designed to solve complex combinatorial optimization problems. Since DMMs are fundamentally classical dynamical systems, their ordinary differential equations (ODEs) can be efficiently simulated on modern computers. This provides a unique platform to study their performance under various conditions. An aspect that has received little attention so far is how their performance is affected by the numerical errors in the solution of their ODEs and the physical noise they would be naturally subject to if built in hardware. Here, we analyze these two aspects in detail by varying the integration time step (numerical noise) and adding stochastic perturbations (physical noise) into the equations of DMMs. We are particularly interested in understanding how noise induces a chaotic transition that marks the shift from successful problem-solving to failure in these systems. Our study includes an analysis of power spectra and Lyapunov exponents depending on the noise strength. The results reveal a correlation between the instance solvability and the sign of the ensemble averaged mean largest Lyapunov exponent. Interestingly, we find a regime in which DMMs with positive mean largest Lyapunov exponents still exhibit solvability. Furthermore, the power spectra provide additional information about our system by distinguishing between regular behavior (peaks) and chaotic behavior (broadband spectrum). Therefore, power spectra could be utilized to control whether a DMM operates in the optimal dynamical regime. Overall, we find that the qualitative effects of numerical and physical noise are mostly similar, despite their fundamentally different origin.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders](https://arxiv.org/abs/2506.14937)
*Luan Gonçalves Miranda,Pedro Ivo da Cruz,Murilo Bellezoni Loiola*

Main category: cs.LG

TL;DR: 论文提出使用机器学习算法自动定义自编码器的异常检测阈值，评估了KNN、K-Means和SVM三种算法。


<details>
  <summary>Details</summary>
Motivation: 自编码器在异常检测中表现出潜力，但分类阈值的非标准化问题直接影响检测性能，因此需要自动化阈值定义方法。

Method: 评估了K-Nearest Neighbors、K-Means和Support Vector Machine三种算法，用于自动定义自编码器的重构误差分类阈值。

Result: 未明确提及具体结果，但指出通过算法自动定义阈值可以优化检测性能。

Conclusion: 自动定义阈值为自编码器异常检测提供了一种可行的方法，未来可以进一步优化算法选择。

Abstract: Currently, digital security mechanisms like Anomaly Detection Systems using Autoencoders (AE) show great potential for bypassing problems intrinsic to the data, such as data imbalance. Because AE use a non-trivial and nonstandardized separation threshold to classify the extracted reconstruction error, the definition of this threshold directly impacts the performance of the detection process. Thus, this work proposes the automatic definition of this threshold using some machine learning algorithms. For this, three algorithms were evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.

</details>


### [62] [FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models](https://arxiv.org/abs/2506.14824)
*Yao Zhang,Hewei Gao,Haokun Chen,Weiguo Li,Yunpu Ma,Volker Tresp*

Main category: cs.LG

TL;DR: FedNano是一种新的联邦学习框架，用于解决多模态大语言模型在分布式场景下的部署问题，通过中央服务器和轻量级客户端模块NanoEdge，显著降低了存储和通信开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在实际应用中面临分布式数据存储和隐私保护等挑战，而传统的联邦学习方法无法直接用于大规模模型。

Method: 提出FedNano框架，中央服务器托管大语言模型，客户端使用轻量级模块NanoEdge，结合低秩适配器NanoAdapters，减少存储和通信需求。

Result: 实验显示，FedNano将客户端存储减少95%，通信开销仅为模型参数的0.01%，并在性能上优于现有联邦学习方法。

Conclusion: FedNano为多模态大语言模型的大规模联邦学习提供了可行方案，平衡了模型规模与计算资源需求。

Abstract: Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.

</details>


### [63] [MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing](https://arxiv.org/abs/2506.15571)
*Le Vu Anh,Nguyen Viet Anh,Mehmet Dik,Tu Nguyen Thi Ngoc*

Main category: cs.LG

TL;DR: MicroRicci 是一种自适应的局部 Ricci 流求解器，通过结合编码理论和轻量级神经网络模块，显著提升了实时网格平滑的性能和质量。


<details>
  <summary>Details</summary>
Motivation: 传统 Ricci 流求解器需要进行全局更新，计算成本高，而贪心方法则收敛缓慢或需要繁琐调参。MicroRicci 旨在实现高效、自适应的局部求解。

Method: MicroRicci 结合了贪婪的综合征解码步骤（在 O(E) 时间内修正最大曲率误差）和两个小型神经模块（动态选择顶点和步长）。

Result: 在 110 个 SJTU-TMQA 网格上，MicroRicci 将迭代次数减少 2.4 倍，曲率范围改善为 0.185，UV 失真与 MOS 相关性达 -0.93，每次迭代仅增加 0.25 毫秒，整体加速 1.8 倍。

Conclusion: MicroRicci 以其线性时间更新、自动超参数调整和高性能，适用于图形、模拟等资源受限的实时应用。

Abstract: Real-time mesh smoothing at scale remains a formidable challenge: classical Ricci-flow solvers demand costly global updates, while greedy heuristics suffer from slow convergence or brittle tuning. We present MicroRicci, the first truly self-tuning, local Ricci-flow solver that borrows ideas from coding theory and packs them into just 1K + 200 parameters. Its primary core is a greedy syndrome-decoding step that pinpoints and corrects the largest curvature error in O(E) time, augmented by two tiny neural modules that adaptively choose vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes, MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup), tightens curvature spread from 0.19 to 0.185, and achieves a remarkable UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration over state-of-the-art methods. MicroRicci's combination of linear-time updates, automatic hyperparameter adaptation, and high-quality geometric and perceptual results makes it well suited for real-time, resource-limited applications in graphics, simulation, and related fields.

</details>


### [64] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen,Ruoxin Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 提出的混合BiGRU-MHA模型结合多头注意力机制，提升了SSD健康状态分类的准确性和稳定性，具有优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: SSD健康状态预测对数据可靠性至关重要，但传统模型泛化能力不足。

Method: 模型结合双向GRU网络和多头注意力机制，捕捉时间特征并动态分配权重。

Result: 训练集和测试集准确率分别为92.70%和92.44%，AUC为0.94，泛化性能优秀。

Conclusion: 该方法为SSD健康预测提供了新思路，能显著降低数据丢失风险，具有实际应用价值。

Abstract: Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a hybrid BiGRU-MHA model that incorporates a multi-head attention mechanism to enhance the accuracy and stability of storage device health classification. The model innovatively integrates temporal feature extraction and key information focusing capabilities. Specifically, it leverages the bidirectional timing modeling advantages of the BiGRU network to capture both forward and backward dependencies of SSD degradation features. Simultaneously, the multi-head attention mechanism dynamically assigns feature weights, improving the model's sensitivity to critical health indicators. Experimental results show that the proposed model achieves classification accuracies of 92.70% on the training set and 92.44% on the test set, with a minimal performance gap of only 0.26%, demonstrating excellent generalization ability. Further analysis using the receiver operating characteristic (ROC) curve shows an area under the curve (AUC) of 0.94 on the test set, confirming the model's robust binary classification performance. This work not only presents a new technical approach for SSD health prediction but also addresses the generalization bottleneck of traditional models, offering a verifiable method with practical value for preventive maintenance of industrial-grade storage systems. The results show the model can significantly reduce data loss risks by providing early failure warnings and help optimize maintenance costs, supporting intelligent decision-making in building reliable storage systems for cloud computing data centers and edge storage environments.

</details>


### [65] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang,Boyu Wang,Bin Gu,Charles Ling*

Main category: cs.LG

TL;DR: 垂直联邦学习（VFL）中在线学习更适应现实场景，但异步数据流和事件驱动特性带来挑战。作者提出事件驱动的在线VFL框架，动态局部遗憾（DLR）应对非凸与非稳态环境，实验证明其稳定性和低成本。


<details>
  <summary>Details</summary>
Motivation: 现实场景中VFL客户端的数据流常异步且事件驱动，传统方法忽视这一问题。为此，研究提出事件驱动的在线VFL框架以解决异步性和非稳态性挑战。

Method: 提出事件驱动的在线VFL框架，仅激活相关客户端，其余被动协作；引入动态局部遗憾（DLR）处理非凸与非稳态问题。

Result: 实验显示框架在非稳态数据下更稳定，显著降低通信与计算成本。

Conclusion: 事件驱动的在线VFL框架有效解决异步数据流问题，结合DLR提升了非稳态环境下的性能与效率。

Abstract: Online learning is more adaptable to real-world scenarios in Vertical Federated Learning (VFL) compared to offline learning. However, integrating online learning into VFL presents challenges due to the unique nature of VFL, where clients possess non-intersecting feature sets for the same sample. In real-world scenarios, the clients may not receive data streaming for the disjoint features for the same entity synchronously. Instead, the data are typically generated by an \emph{event} relevant to only a subset of clients. We are the first to identify these challenges in online VFL, which have been overlooked by previous research. To address these challenges, we proposed an event-driven online VFL framework. In this framework, only a subset of clients were activated during each event, while the remaining clients passively collaborated in the learning process. Furthermore, we incorporated \emph{dynamic local regret (DLR)} into VFL to address the challenges posed by online learning problems with non-convex models within a non-stationary environment. We conducted a comprehensive regret analysis of our proposed framework, specifically examining the DLR under non-convex conditions with event-driven online VFL. Extensive experiments demonstrated that our proposed framework was more stable than the existing online VFL framework under non-stationary data conditions while also significantly reducing communication and computation costs.

</details>


### [66] [Centroid Approximation for Byzantine-Tolerant Federated Learning](https://arxiv.org/abs/2506.15264)
*Mélanie Cambus,Darya Melnyk,Tijana Milentijević,Stefan Schmid*

Main category: cs.LG

TL;DR: 该论文探讨了联邦学习在存在拜占庭行为时的鲁棒性，提出了关于中心点近似的新下界和上界，并开发了新的算法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习在分布式环境中如何应对拜占庭行为，确保模型的收敛性和有效性。

Method: 通过理论分析提出了中心点近似的下界和上界，并设计新算法在凸有效性条件下实现近似。

Result: 首次证明了在文献常见的盒有效性条件下中心点近似的下界，同时提供了新的上界和算法。

Conclusion: 论文提出了关于联邦学习拜占鲁棒性的新理论结果和算法，并验证了其在实验中的有效性。

Abstract: Federated learning allows each client to keep its data locally when training machine learning models in a distributed setting. Significant recent research established the requirements that the input must satisfy in order to guarantee convergence of the training loop. This line of work uses averaging as the aggregation rule for the training models. In particular, we are interested in whether federated learning is robust to Byzantine behavior, and observe and investigate a tradeoff between the average/centroid and the validity conditions from distributed computing. We show that the various validity conditions alone do not guarantee a good approximation of the average. Furthermore, we show that reaching good approximation does not give good results in experimental settings due to possible Byzantine outliers. Our main contribution is the first lower bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under box validity that is often considered in the literature, where $n$ is the number of clients, $t$ the upper bound on the number of Byzantine faults, and $d$ is the dimension of the machine learning model. We complement this lower bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis for the case $n<d$. In addition, we present a new algorithm that achieves a $\sqrt{2d}$-approximation under convex validity, which also proves that the existing lower bound in the literature is tight. We show that all presented bounds can also be achieved in the distributed peer-to-peer setting. We complement our analytical results with empirical evaluations in federated stochastic gradient descent and federated averaging settings.

</details>


### [67] [Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction](https://arxiv.org/abs/2506.15626)
*Vincent Roca,Marc Tommasi,Paul Andrey,Aurélien Bellet,Markus D. Schirmer,Hilde Henon,Laurent Puy,Julien Ramon,Grégory Kuchcinski,Martin Bretzner,Renaud Lopes*

Main category: cs.LG

TL;DR: 研究评估了联邦学习（FL）在缺血性中风患者中用于BrainAGE估计的性能，并探讨了其与临床表型及功能结果的关联，结果表明FL在不集中数据的情况下仍能提供准确预测。


<details>
  <summary>Details</summary>
Motivation: BrainAGE是反映大脑健康的生物标志物，但其训练通常需要大量数据，且隐私问题限制了数据共享。研究旨在探索FL的可行性及其临床应用价值。

Method: 使用1674名中风患者的FLAIR大脑图像，采用集中学习、FL和单点学习三种策略训练模型，并分析BrainAGE与血管风险因素及功能结果的关系。

Result: FL表现优于单点学习，且BrainAGE与糖尿病等血管风险因素及中风后功能恢复显著相关。

Conclusion: FL无需数据集中即可提供准确预测，BrainAGE在脑卒中预后模型中具有潜在应用价值。

Abstract: $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.

</details>


### [68] [MedSyn: Enhancing Diagnostics with Human-AI Collaboration](https://arxiv.org/abs/2506.14774)
*Burcu Sayin,Ipek Baris Schlicht,Ngoc Vo Hong,Sara Allievi,Jacopo Staiano,Pasquale Minervini,Andrea Passerini*

Main category: cs.LG

TL;DR: 论文摘要介绍了MedSyn，一种结合人类医生与大型语言模型（LLMs）的多步交互框架，用于辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: 临床决策复杂且易受认知偏差、信息不完整和病例模糊性的影响，现有的LLMs使用方式通常为一次性或有限交互，未能充分体现现实医疗实践的复杂性。

Method: 提出MedSyn框架，通过医生与LLMs的多步交互对话来优化诊断和治疗决策，支持动态交流。

Result: 实验表明，开源LLMs在模拟医生-LLM交互中表现良好，有潜力成为实际医疗中的助手。

Conclusion: 未来计划通过真实医生互动进一步验证MedSyn在诊断准确性和患者结果中的实用性。

Abstract: Clinical decision-making is inherently complex, often influenced by cognitive biases, incomplete information, and case ambiguity. Large Language Models (LLMs) have shown promise as tools for supporting clinical decision-making, yet their typical one-shot or limited-interaction usage may overlook the complexities of real-world medical practice. In this work, we propose a hybrid human-AI framework, MedSyn, where physicians and LLMs engage in multi-step, interactive dialogues to refine diagnoses and treatment decisions. Unlike static decision-support tools, MedSyn enables dynamic exchanges, allowing physicians to challenge LLM suggestions while the LLM highlights alternative perspectives. Through simulated physician-LLM interactions, we assess the potential of open-source LLMs as physician assistants. Results show open-source LLMs are promising as physician assistants in the real world. Future work will involve real physician interactions to further validate MedSyn's usefulness in diagnostic accuracy and patient outcomes.

</details>


### [69] [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
*Mohamed Masry,Mohamed Amen,Mohamed Elzyat,Mohamed Hamed,Norhan Magdy,Maram Khaled*

Main category: cs.LG

TL;DR: 论文提出ETS框架，结合EEG和眼动数据，提升开放词汇文本生成和情感分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决非侵入式EEG在开放词汇场景下因噪声和变异性导致的性能不佳问题。

Method: 集成EEG与同步眼动数据，用于开放词汇文本生成和情感分类。

Result: 在BLEU和Rouge分数上表现优异，情感分类F1分数提升10%，支持多源数据。

Conclusion: ETS框架在开放词汇EEG到文本系统中具有高性能潜力。

Abstract: Decoding natural language from brain activity using non-invasive electroencephalography (EEG) remains a significant challenge in neuroscience and machine learning, particularly for open-vocabulary scenarios where traditional methods struggle with noise and variability. Previous studies have achieved high accuracy on small-closed vocabularies, but it still struggles on open vocabularies. In this study, we propose ETS, a framework that integrates EEG with synchronized eye-tracking data to address two critical tasks: (1) open-vocabulary text generation and (2) sentiment classification of perceived language. Our model achieves a superior performance on BLEU and Rouge score for EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment classification, which significantly outperforms supervised baselines. Furthermore, we show that our proposed model can handle data from various subjects and sources, showing great potential for high performance open vocabulary eeg-to-text system.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [70] [Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices](https://arxiv.org/abs/2506.15028)
*Gargi Mitra,Mohammadreza Hallajiyan,Inji Kim,Athish Pranav Dharmalingam,Mohammed Elnawawy,Shahrear Iqbal,Karthik Pattabiraman,Homa Alemzadeh*

Main category: cs.CR

TL;DR: 该论文探讨了AI/ML医疗设备的网络安全风险及其对患者安全的影响，提出了一套工具和技术以支持上市前的风险评估。


<details>
  <summary>Details</summary>
Motivation: 由于AI/ML医疗设备的复杂性和互联性，网络安全问题严重威胁患者安全，需从设计阶段保障其安全性。

Method: 通过分析设备召回、不良事件和已知漏洞数据，设计了一套工具帮助安全分析师进行上市前风险评估。

Result: 提出的工具有助于制造商在设计阶段嵌入网络安全原则，提升设备安全性。

Conclusion: 强调在AI/ML医疗设备上市前解决网络安全问题的紧迫性，以确保患者安全。

Abstract: The integration of AI/ML into medical devices is rapidly transforming healthcare by enhancing diagnostic and treatment facilities. However, this advancement also introduces serious cybersecurity risks due to the use of complex and often opaque models, extensive interconnectivity, interoperability with third-party peripheral devices, Internet connectivity, and vulnerabilities in the underlying technologies. These factors contribute to a broad attack surface and make threat prevention, detection, and mitigation challenging. Given the highly safety-critical nature of these devices, a cyberattack on these devices can cause the ML models to mispredict, thereby posing significant safety risks to patients. Therefore, ensuring the security of these devices from the time of design is essential. This paper underscores the urgency of addressing the cybersecurity challenges in ML-enabled medical devices at the pre-market phase. We begin by analyzing publicly available data on device recalls and adverse events, and known vulnerabilities, to understand the threat landscape of AI/ML-enabled medical devices and their repercussions on patient safety. Building on this analysis, we introduce a suite of tools and techniques designed by us to assist security analysts in conducting comprehensive premarket risk assessments. Our work aims to empower manufacturers to embed cybersecurity as a core design principle in AI/ML-enabled medical devices, thereby making them safe for patients.

</details>


### [71] [Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine](https://arxiv.org/abs/2506.15070)
*Rasha Karakchi,Rye Stahle-Smith,Nishant Chinnasami,Tiffany Yu*

Main category: cs.CR

TL;DR: SPiME是一种轻量级、可扩展且与FPGA兼容的安全内存处理器加密架构，集成AES-128至PiM框架，支持分布式加密，高效且低延迟。


<details>
  <summary>Details</summary>
Motivation: 物联网应用需求增长，传统CPU加密方法在高吞吐和低延迟环境中表现不佳。

Method: 提出SPiME架构，将AES-128直接集成到PiM框架中，以并行PiM单元阵列实现分布式加密。

Result: 在高端FPGA上支持4000+并行单元，资源利用率低至5%，加密吞吐量达25Gbps。

Conclusion: SPiME在安全边缘计算和嵌入式加密系统中表现出色，具备高效、可配置和可移植特性。

Abstract: The exponential growth of Internet of Things (IoT) applications has intensified the demand for efficient, high-throughput, and energy-efficient data processing at the edge. Conventional CPU-centric encryption methods suffer from performance bottlenecks and excessive data movement, especially in latency-sensitive and resource-constrained environments. In this paper, we present SPiME, a lightweight, scalable, and FPGA-compatible Secure Processor-in-Memory Encryption architecture that integrates the Advanced Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM) framework. SPiME is designed as a modular array of parallel PiM units, each combining an AES core with a minimal control unit to enable distributed in-place encryption with minimal overhead. The architecture is fully implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+ FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units while maintaining less than 5\% utilization of key FPGA resources on high-end devices. It delivers over 25~Gbps in sustained encryption throughput with predictable, low-latency performance. The design's portability, configurability, and resource efficiency make it a compelling solution for secure edge computing, embedded cryptographic systems, and customizable hardware accelerators.

</details>


### [72] [deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses](https://arxiv.org/abs/2506.15648)
*Georgios Androutsopoulos,Antonio Bianchi*

Main category: cs.CR

TL;DR: deepSURF是一种结合静态分析和LLM引导的模糊测试工具，用于有效检测Rust库中的内存安全漏洞，尤其针对不安全代码。


<details>
  <summary>Details</summary>
Motivation: 尽管Rust默认提供内存安全性，但不安全代码的使用可能引入漏洞，现有工具检测能力有限。

Method: deepSURF通过自定义类型替代泛型、生成特定特质实现，并结合LLM动态增强模糊测试工具链。

Result: 在27个真实Rust库中，成功重现20个已知漏洞并发现6个新漏洞。

Conclusion: deepSURF在检测能力上显著优于现有工具，具有实用价值。

Abstract: Although Rust ensures memory safety by default, it also permits the use of unsafe code, which can introduce memory safety vulnerabilities if misused. Unfortunately, existing tools for detecting memory bugs in Rust typically exhibit limited detection capabilities, inadequately handle Rust-specific types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates static analysis with Large Language Model (LLM)-guided fuzzing harness generation to effectively identify memory safety vulnerabilities in Rust libraries, specifically targeting unsafe code. deepSURF introduces a novel approach for handling generics by substituting them with custom types and generating tailored implementations for the required traits, enabling the fuzzer to simulate user-defined behaviors within the fuzzed library. Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically, facilitating exploration of complex API interactions and significantly increasing the likelihood of exposing memory safety vulnerabilities. We evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20 known memory safety bugs and uncovering 6 previously unknown vulnerabilities, demonstrating clear improvements over state-of-the-art tools.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [73] [Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing](https://arxiv.org/abs/2506.15278)
*Reuben Binns,Jake Stein,Siddhartha Datta,Max Van Kleek,Nigel Shadbolt*

Main category: cs.CY

TL;DR: 研究分析了Uber的动态定价对司机收入和分配的影响，发现其导致收入下降、平台抽成增加、分配不均等问题。


<details>
  <summary>Details</summary>
Motivation: 探讨Uber平台宣称的‘灵活性’是否实际改善了司机的工作条件，特别是动态定价对司机收入和工作分配的影响。

Method: 通过参与式行动研究与司机和工会合作，对Uber的算法薪酬和工作分配进行审核，分析了英国258名司机的150万次行程数据。

Result: 动态定价后，司机收入下降，平台抽成增加，工作分配和薪酬更不可预测，司机之间的不平等加剧，等待时间更长。

Conclusion: 动态定价对司机不利，研究为算法审核、零工经济以及工人数据科学提供了方法论和理论贡献。

Abstract: Ride-sharing platforms like Uber market themselves as enabling `flexibility' for their workforce, meaning that drivers are expected to anticipate when and where the algorithm will allocate them jobs, and how well remunerated those jobs will be. In this work we describe our process of participatory action research with drivers and trade union organisers, culminating in a participatory audit of Uber's algorithmic pay and work allocation, before and after the introduction of dynamic pricing. Through longitudinal analysis of 1.5 million trips from 258 drivers in the UK, we find that after dynamic pricing, pay has decreased, Uber's cut has increased, job allocation and pay is less predictable, inequality between drivers is increased, and drivers spend more time waiting for jobs. In addition to these findings, we provide methodological and theoretical contributions to algorithm auditing, gig work, and the emerging practice of worker data science.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [74] [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154)
*Anuradha Chopra,Abhinaba Roy,Dorien Herremans*

Main category: cs.SD

TL;DR: SonicVerse是一个多任务音乐描述模型，通过辅助音乐特征检测任务提升音乐描述的丰富性和准确性。


<details>
  <summary>Details</summary>
Motivation: 丰富音乐数据库并推动音乐AI研究，需要准确反映音乐特征的详细描述。

Method: 提出投影式架构，将音频输入转换为语言标记，并通过辅助任务检测音乐特征，增强描述输入。

Result: 实验表明，整合音乐特征提升了生成描述的质量和细节。

Conclusion: SonicVerse能生成丰富且时间感知的音乐描述，为音乐AI研究提供新工具。

Abstract: Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [75] [A theory of Lending Protocols in DeFi](https://arxiv.org/abs/2506.15295)
*Massimo Bartoletti,Enrico Lipparini*

Main category: cs.GT

TL;DR: 总结去中心化金融（DeFi）借贷协议的激励机制复杂性及其潜在风险，并提出一种形式化模型来分析其经济和战略动态。


<details>
  <summary>Details</summary>
Motivation: 研究借贷协议的激励机制，评估其结构性和经济性，防止因机制缺陷导致的不良行为或攻击。

Method: 提出一种形式化模型，捕捉主流平台的核心特征，并验证其经济和战略动态的关键属性。

Result: 通过模型识别并证实了借贷协议的关键属性，揭示了潜在的经济和战略问题。

Conclusion: 形式化模型为借贷协议的经济和战略动态提供了一种系统化分析方法，有助于识别和预防潜在风险。

Abstract: Lending protocols are one of the main applications of Decentralized Finance (DeFi), enabling crypto-assets loan markets with a total value estimated in the tens of billions of dollars. Unlike traditional lending systems, these protocols operate without relying on trusted authorities or off-chain enforcement mechanisms. To achieve key economic goals such as stability of the loan market, they devise instead trustless on-chain mechanisms, such as rewarding liquidators who repay the loans of under-collateralized borrowers by awarding them part of the borrower's collateral. The complexity of these incentive mechanisms, combined with their entanglement in low-level implementation details, makes it challenging to precisely assess the structural and economic properties of lending protocols, as well as to analyze user strategies and attacks. Crucially, since participation is open to anyone, any weaknesses in the incentive mechanism may give rise to unintended emergent behaviours, or even enable adversarial strategies aimed at making profits to the detriment of legit users, or at undermining the stability of the protocol. In this work, we propose a formal model of lending protocols that captures the essential features of mainstream platforms, enabling us to identify and prove key properties related to their economic and strategic dynamics.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [76] [Empirical Studies of Large Scale Environment Scanning by Consumer Electronics](https://arxiv.org/abs/2506.14771)
*Mengyuan Wang,Yang Liu,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: eess.IV

TL;DR: 对消费级3D扫描设备Matterport Pro3进行大规模环境重建的评估，结果显示其在高密度点云和对齐精度方面优于iPhone，适用于大规模应用。


<details>
  <summary>Details</summary>
Motivation: 评估Matterport Pro3在大型建筑扫描中的效果、限制及性能优化潜力。

Method: 对六层建筑进行详细扫描（1,099个点），并与iPhone进行对比分析。

Result: Pro3生成的点云更密集（1,877,324点vs. 506,961点），对齐精度更高（RMSE 0.0118米）。

Conclusion: Pro3在成本效益与性能之间取得平衡，适合大规模3D建模。

Abstract: This paper presents an empirical evaluation of the Matterport Pro3, a consumer-grade 3D scanning device, for large-scale environment reconstruction. We conduct detailed scanning (1,099 scanning points) of a six-floor building (17,567 square meters) and assess the device's effectiveness, limitations, and performance enhancements in diverse scenarios. Challenges encountered during the scanning are addressed through proposed solutions, while we also explore advanced methods to overcome them more effectively. Comparative analysis with another consumer-grade device (iPhone) highlights the Pro3's balance between cost-effectiveness and performance. The Matterport Pro3 achieves a denser point cloud with 1,877,324 points compared to the iPhone's 506,961 points and higher alignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C) average distance error between the two point cloud models is 0.0408 meters, with a standard deviation of 0.0715 meters. The study demonstrates the Pro3's ability to generate high-quality 3D models suitable for large-scale applications, leveraging features such as LiDAR and advanced alignment techniques.

</details>


### [77] [ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression](https://arxiv.org/abs/2506.15228)
*Yufeng Zhang,Wenrui Dai,Hang Yu,Shizhan Liu,Junhui Hou,Jianguo Li,Weiyao Lin*

Main category: eess.IV

TL;DR: ABC框架通过贝叶斯网络结构学习实现神经图像压缩的计算可扩展性，解决了传统方法在计算复杂性控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 神经图像压缩因其高计算需求难以广泛应用，现有方法缺乏对计算复杂性的全面控制。

Method: ABC框架通过异构二分贝叶斯网络管理神经主干计算，同构多分贝叶斯网络优化自回归单元处理，以及自适应控制模块动态调整结构。

Result: 实验证明ABC在保持压缩性能的同时，实现了更好的计算适应性和更广的复杂性控制范围。

Conclusion: ABC是一种鲁棒的解决方案，适用于各种基于贝叶斯网络的神经图像压缩架构。

Abstract: Neural Image Compression (NIC) has revolutionized image compression with its superior rate-distortion performance and multi-task capabilities, supporting both human visual perception and machine vision tasks. However, its widespread adoption is hindered by substantial computational demands. While existing approaches attempt to address this challenge through module-specific optimizations or pre-defined complexity levels, they lack comprehensive control over computational complexity. We present ABC (Adaptive BayesNet structure learning for computational scalable multi-task image Compression), a novel, comprehensive framework that achieves computational scalability across all NIC components through Bayesian network (BayesNet) structure learning. ABC introduces three key innovations: (i) a heterogeneous bipartite BayesNet (inter-node structure) for managing neural backbone computations; (ii) a homogeneous multipartite BayesNet (intra-node structure) for optimizing autoregressive unit processing; and (iii) an adaptive control module that dynamically adjusts the BayesNet structure based on device capabilities, input data complexity, and downstream task requirements. Experiments demonstrate that ABC enables full computational scalability with better complexity adaptivity and broader complexity control span, while maintaining competitive compression performance. Furthermore, the framework's versatility allows integration with various NIC architectures that employ BayesNet representations, making it a robust solution for ensuring computational scalability in NIC applications. Code is available in https://github.com/worldlife123/cbench_BaSIC.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [78] [EmojiVoice: Towards long-term controllable expressivity in robot speech](https://arxiv.org/abs/2506.15085)
*Paige Tuttösí,Shivam Mehta,Zachary Syvenky,Bermet Burkanova,Gustav Eje Henter,Angelica Lim*

Main category: cs.RO

TL;DR: EmojiVoice是一个免费的、可定制的文本转语音（TTS）工具包，旨在为社交机器人提供长期可变的表达性语音。


<details>
  <summary>Details</summary>
Motivation: 社交机器人通常使用单调的‘高兴’语音，缺乏人类语音中的长期变化。现有的基础模型TTS系统难以在机器人上离线部署。

Method: 使用轻量级Matcha-TTS框架和表情符号提示（emoji-prompting）实现细粒度控制，并在三个案例中进行测试。

Result: 在讲故事任务中，多样化的表情符号提示提高了语音的感知和表达性，但在助手任务中未被优先选择。

Conclusion: EmojiVoice在特定场景下可提升机器人语音的表达性，但需根据用途调整。

Abstract: Humans vary their expressivity when speaking for extended periods to maintain engagement with their listener. Although social robots tend to be deployed with ``expressive'' joyful voices, they lack this long-term variation found in human speech. Foundation model text-to-speech systems are beginning to mimic the expressivity in human speech, but they are difficult to deploy offline on robots. We present EmojiVoice, a free, customizable text-to-speech (TTS) toolkit that allows social roboticists to build temporally variable, expressive speech on social robots. We introduce emoji-prompting to allow fine-grained control of expressivity on a phase level and use the lightweight Matcha-TTS backbone to generate speech in real-time. We explore three case studies: (1) a scripted conversation with a robot assistant, (2) a storytelling robot, and (3) an autonomous speech-to-speech interactive agent. We found that using varied emoji prompting improved the perception and expressivity of speech over a long period in a storytelling task, but expressive voice was not preferred in the assistant use case.

</details>


### [79] [I Know You're Listening: Adaptive Voice for HRI](https://arxiv.org/abs/2506.15107)
*Paige Tuttösí*

Main category: cs.RO

TL;DR: 论文探讨了社交机器人在语言教学中的应用，重点解决了任务特定合成语音的缺失问题，通过轻量级表达性语音、环境适应性语音调整和针对L2学习者的清晰语音系统三个方面进行了改进。


<details>
  <summary>Details</summary>
Motivation: 语言是口头任务，社交机器人缺乏针对语言教学的特定语音系统可能影响教学效果，因此需要开发适合的语音技术。

Method: 1. 使用Matcha-TTS的微调版本，通过表情符号提示创建轻量级且表达丰富的语音；2. 根据物理和社会环境调整语音的音高和音速；3. 基于L2学习者对元音时长的感知，创建更清晰的英语TTS系统。

Result: 1. 表达性语音更生动且适合长时间讲述；2. 环境适应性调整使语音更贴合环境；3. "L2清晰模式"显著提高了L2学习者的理解能力和减少错误。

Conclusion: 通过改进语音系统的表达性、环境适应性和清晰度，社交机器人在语言教学中的效果得到显著提升。

Abstract: While the use of social robots for language teaching has been explored, there remains limited work on a task-specific synthesized voices for language teaching robots. Given that language is a verbal task, this gap may have severe consequences for the effectiveness of robots for language teaching tasks. We address this lack of L2 teaching robot voices through three contributions: 1. We address the need for a lightweight and expressive robot voice. Using a fine-tuned version of Matcha-TTS, we use emoji prompting to create an expressive voice that shows a range of expressivity over time. The voice can run in real time with limited compute resources. Through case studies, we found this voice more expressive, socially appropriate, and suitable for long periods of expressive speech, such as storytelling. 2. We explore how to adapt a robot's voice to physical and social ambient environments to deploy our voices in various locations. We found that increasing pitch and pitch rate in noisy and high-energy environments makes the robot's voice appear more appropriate and makes it seem more aware of its current environment. 3. We create an English TTS system with improved clarity for L2 listeners using known linguistic properties of vowels that are difficult for these listeners. We used a data-driven, perception-based approach to understand how L2 speakers use duration cues to interpret challenging words with minimal tense (long) and lax (short) vowels in English. We found that the duration of vowels strongly influences the perception for L2 listeners and created an "L2 clarity mode" for Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels unchanged. Our clarity mode was found to be more respectful, intelligible, and encouraging than base Matcha-TTS while reducing transcription errors in these challenging tense/lax minimal pairs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [80] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI代理范式——Embodied Web Agents，旨在融合物理世界感知与在线知识推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理往往局限于数字信息处理或物理世界交互，缺乏两者结合的能力，而许多任务需要跨领域的智能协作。

Method: 构建了一个统一仿真平台，集成3D环境与功能化网络接口，并设计了涵盖多种跨领域任务的基准测试（如烹饪、导航等）。

Result: 实验表明，现有AI系统与人类能力相比存在显著差距，凸显了跨领域认知与知识访问的挑战与机遇。

Conclusion: 研究为物理与数字智能融合提供了新方向，并公开了数据集与平台以供社区研究。

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.

</details>


### [81] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: 该立场论文主张在基础模型评估中需要更严格和透明的人类基线，以更好地比较人类与AI性能，并提供相关建议和报告清单。


<details>
  <summary>Details</summary>
Motivation: 人类性能基线对机器学习社区、下游用户和政策制定者理解AI评估至关重要，但现有方法在严谨性和文档化方面不足。

Method: 通过元综述测量理论和AI评估文献，提出了设计、执行和报告人类基线的框架和建议，并转化为清单形式。

Result: 作者用清单系统审查了115项人类基线研究，发现了现有基线方法的不足，同时清单也能帮助研究者进行更规范的基线研究和结果报告。

Conclusion: 该工作旨在推动更严格的AI评估实践，以更好地服务于研究社区和政策制定者。

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
*Yang Yao,Lingyu Li,Jiaxin Song,Chiyu Chen,Zhenqi He,Yixu Wang,Xin Wang,Tianle Gu,Jie Li,Yan Teng,Yingchun Wang*

Main category: cs.CV

TL;DR: 论文提出了一种多模态基准Argus Inspection和框架Eye of Panoptes，用于评估多模态大语言模型在细粒度视觉感知和常识因果推理中的能力，实验显示现有模型仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）的认知和推理能力有所提升，但在细粒度视觉感知和常识因果推理方面仍存在挑战，需要更全面的评估工具。

Method: 引入Argus Inspection多模态基准和Eye of Panoptes框架，结合二元参数化Sigmoid指标和指示函数，全面评估MLLMs在意见型推理任务中的表现。

Result: 对26个主流MLLMs的实验表明，视觉细粒度推理的最高性能仅为0.46，显示出模型在这些任务中有显著改进空间。

Conclusion: 研究为MLLMs的进一步优化提供了有价值的视角。

Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.

</details>


### [83] [MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion](https://arxiv.org/abs/2506.15276)
*Jun Zhu,Xinfeng Zhang,Lv Tang,JunHao Jiang*

Main category: cs.CV

TL;DR: 提出多尺度特征融合框架MSNeRV，增强神经视频表示能力，解决现有方法细节和快速变化内容表现不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有INR方法在细节密集和快速变化的视频内容中表现不佳，源于网络特征利用不足和缺乏视频特定设计。

Method: 提出MSNeRV框架，包括时间窗口增强一致性、GoP级网格背景表示、多尺度空间解码器及自适应损失函数，多尺度特征块优化特征提取。

Result: 在HEVC ClassB和UVG数据集上评估，MSNeRV表现优于其他INR方法，在动态场景中压缩效率超越VTM-23.7。

Conclusion: MSNeRV通过多尺度特征融合显著提升视频表示和压缩性能，尤其在动态内容中表现突出。

Abstract: Implicit Neural representations (INRs) have emerged as a promising approach for video compression, and have achieved comparable performance to the state-of-the-art codecs such as H.266/VVC. However, existing INR-based methods struggle to effectively represent detail-intensive and fast-changing video content. This limitation mainly stems from the underutilization of internal network features and the absence of video-specific considerations in network design. To address these challenges, we propose a multi-scale feature fusion framework, MSNeRV, for neural video representation. In the encoding stage, we enhance temporal consistency by employing temporal windows, and divide the video into multiple Groups of Pictures (GoPs), where a GoP-level grid is used for background representation. Additionally, we design a multi-scale spatial decoder with a scale-adaptive loss function to integrate multi-resolution and multi-frequency information. To further improve feature extraction, we introduce a multi-scale feature block that fully leverages hidden features. We evaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and compression. Experimental results demonstrate that our model exhibits superior representation capability among INR-based approaches and surpasses VTM-23.7 (Random Access) in dynamic scenarios in terms of compression efficiency.

</details>


### [84] [MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering](https://arxiv.org/abs/2506.15298)
*Xinqi Fan,Jingting Li,John See,Moi Hoon Yap,Wen-Huang Cheng,Xiaobai Li,Xiaopeng Hong,Su-Jing Wang,Adrian K. Davision*

Main category: cs.CV

TL;DR: MEGC 2025提出两个新任务：ME-STR（统一微表情检测与识别）和ME-VQA（利用多模态大模型进行视觉问答），以改进微表情分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法将微表情检测与识别分开处理，不适用于长视频分析，而多模态大模型为新方法提供了可能。

Method: 通过ME-STR任务统一检测与识别流程，利用ME-VQA任务结合多模态大模型进行问答式理解。

Result: MEGC 2025为微表情分析提供了新方向，并设立公开测试集与排行榜。

Conclusion: 多模态大模型的引入有望提升微表情分析的效率和准确性，推动实际应用。

Abstract: Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. In recent years, substantial advancements have been made in the areas of ME recognition, spotting, and generation. However, conventional approaches that treat spotting and recognition as separate tasks are suboptimal, particularly for analyzing long-duration videos in realistic settings. Concurrently, the emergence of multimodal large language models (MLLMs) and large vision-language models (LVLMs) offers promising new avenues for enhancing ME analysis through their powerful multimodal reasoning capabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that reflect these evolving research directions: (1) ME spot-then-recognize (ME-STR), which integrates ME spotting and subsequent recognition in a unified sequential pipeline; and (2) ME visual question answering (ME-VQA), which explores ME understanding through visual question answering, leveraging MLLMs or LVLMs to address diverse question types related to MEs. All participating algorithms are required to run on this test set and submit their results on a leaderboard. More details are available at https://megc2025.github.io.

</details>


### [85] [Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis](https://arxiv.org/abs/2506.14854)
*Varun Mannam,Zhenyu Shi*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的零售视频自动标注方法，显著降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 零售视频标注对顾客行为分析等应用至关重要，但传统手工标注耗时且低效。

Method: 利用深度神经网络学习视频帧特征，结合针对零售环境的目标检测技术，自动化标注。

Result: 方法在准确性上媲美人工标注，节省50%成本，同时仅需人工验证5%的帧。

Conclusion: 该方法高效且经济，适用于多种零售应用，如顾客旅程分析和安全监控。

Abstract: Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.

</details>
