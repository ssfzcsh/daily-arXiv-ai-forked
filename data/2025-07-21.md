<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [math.LO](#math.LO) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence](https://arxiv.org/abs/2507.13481)
*Arthur Bueno,Bruno Cafeo,Maria Cagnin,Awdren Fontão*

Main category: cs.SE

TL;DR: 研究探讨了开源生态系统中代码样本的技术（如代码异味）和社区（如沟通障碍）问题如何相互作用，发现社区问题常导致技术退化。


<details>
  <summary>Details</summary>
Motivation: 代码样本对开源生态系统至关重要，但其管理和维护往往不规范，导致技术和社区问题的累积。

Method: 采用多声文献综述协议，分析了30篇同行评审论文和17篇实践导向文献（2013-2024年），并进行了主题合成。

Result: 识别出9种模式，表明社区问题（如沉默寡言）常先于或加剧技术退化；缺乏持续重构和非正式协作是问题积累的主因。

Conclusion: 开源生态系统中代码样本的社区问题与技术退化相关，需开发轻量级治理机制和指标以改善。

Abstract: Code samples play a pivotal role in open-source ecosystems (OSSECO), serving
as lightweight artifacts that support knowledge transfer, onboarding, and
framework adoption. Despite their instructional relevance, these samples are
often governed informally, with minimal review and unclear ownership, which
increases their exposure to socio-technical degradation. In this context, the
co-occurrence and longitudinal interplay of code smells (e.g., large classes,
poor modularity) and community smells (e.g., lone contributors, fragmented
communication) become particularly critical. While each type of smell has been
studied in isolation, little is known about how community-level dysfunctions
anticipate or exacerbate technical anomalies in code samples over time. This
study investigates how code and community smells emerge, co-occur, and evolve
within code samples maintained in OSSECOs. A Multivocal Literature Review
protocol was applied, encompassing 30 peer-reviewed papers and 17
practitioner-oriented sources (2013-2024). Thematic synthesis was conducted to
identify recurring socio-technical patterns related to smell dynamics. Nine
patterns were identified, showing that community smells often precede or
reinforce technical degradation in code samples. Symptoms such as "radio
silence" and centralized ownership were frequently associated with persistent
structural anomalies. Additionally, limited onboarding, the absence of
continuous refactoring, and informal collaboration emerged as recurring
conditions for smell accumulation. Conclusion: In OSSECOs, particularly within
code samples, community-level dysfunctions not only correlate with but often
signal maintainability decay. These findings underscore the need for
socio-technical quality indicators and lightweight governance mechanisms
tailored to shared instructional artifacts.

</details>


### [2] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: Meta开发了AI辅助代码审查工具MetaMateCR，通过微调Llama模型提升修复效率，并在安全试验和实际生产中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模代码审查中人工修复效率低下的问题。

Method: 使用64k数据点微调Llama模型，并通过离线测试和安全试验验证模型效果。

Result: 大模型LargeLSFT在离线测试中表现优于GPT-4o，但在安全试验中初始版本导致审查时间增加，调整后效果显著。

Conclusion: MetaMateCR展示了AI工具在大规模代码审查中的潜力，但需通过安全试验避免负面影响。

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>


### [3] [Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software](https://arxiv.org/abs/2507.13553)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 研究探讨了开源软件中功能请求的自然语言缺陷（模糊或不完整）及开发者在澄清过程中如何应对，发现开发者更关注与项目目标的一致性而非文本模糊性，澄清时重视用户意图而非技术细节。


<details>
  <summary>Details</summary>
Motivation: 功能请求常因自然语言的模糊或不完整导致误解和实现错误，影响软件质量，但开发者如何实际进行澄清尚不明确。

Method: 通过分析开源软件平台上的功能请求及其澄清对话，研究自然语言缺陷的类型和澄清的动态过程。

Result: 发现功能请求常存在模糊或不完整问题，开发者更倾向于与项目目标对齐，而非直接解决文本模糊性；澄清时注重用户意图而非技术细节。

Conclusion: 研究揭示了开源协作中功能请求澄清的模式，有助于改进用户与开发者合作方式，并为有效处理功能请求提供最佳实践。

Abstract: As user demands evolve, effectively incorporating feature requests is crucial
for maintaining software relevance and user satisfaction. Feature requests,
typically expressed in natural language, often suffer from ambiguity or
incomplete information due to communication gaps or the requester's limited
technical expertise. These issues can lead to misinterpretation, faulty
implementation, and reduced software quality. While seeking clarification from
requesters is a common strategy to mitigate these risks, little is known about
how developers engage in this clarification process in practice-how they
formulate clarifying questions, seek technical or contextual details, align on
goals and use cases, or decide to close requests without attempting
clarification. This study investigates how feature requests are prone to NL
defects (i.e. ambiguous or incomplete) and the conversational dynamics of
clarification in open-source software (OSS) development, aiming to understand
how developers handle ambiguous or incomplete feature requests. Our findings
suggest that feature requests published on the OSS platforms do possess
ambiguity and incompleteness, and in some cases, both. We also find that
explicit clarification for the resolution of these defects is uncommon;
developers usually focus on aligning with project goals rather than resolving
unclear text. When clarification occurs, it emphasizes understanding user
intent/goal and feasibility, rather than technical details. By characterizing
the dynamics of clarification in open-source issue trackers, this work
identifies patterns that can improve user-developer collaboration and inform
best practices for handling feature requests effectively.

</details>


### [4] [Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software](https://arxiv.org/abs/2507.13555)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 论文提出了一种利用大型语言模型（LLM）来自动检测和优化自然语言中缺陷的方法，以提高功能请求的清晰度和完整性。


<details>
  <summary>Details</summary>
Motivation: 随着软件应用的普及和行业快速发展，用户通过自然语言提出的功能请求常常存在模糊和不完整的问题，传统验证方法在去中心化环境中效率低。

Method: 采用LLM自动识别功能请求中的缺陷，并生成澄清问题（CQs），通过真实OSS请求和人工标注对比评估效果，并采访开发者了解其处理策略。

Result: 方法在真实场景中表现良好，能够有效提升功能请求的清晰度，并得到开发者正面反馈。

Conclusion: LLM在自动化优化自然语言缺陷方面具有潜力，可显著提升功能请求的可用性，但对开发者的需求仍需进一步研究。

Abstract: The growing popularity and widespread use of software applications (apps)
across various domains have driven rapid industry growth. Along with this
growth, fast-paced market changes have led to constantly evolving software
requirements. Such requirements are often grounded in feature requests and
enhancement suggestions, typically provided by users in natural language (NL).
However, these requests often suffer from defects such as ambiguity and
incompleteness, making them challenging to interpret. Traditional validation
methods (e.g., interviews and workshops) help clarify such defects but are
impractical in decentralized environments like open-source software (OSS),
where change requests originate from diverse users on platforms like GitHub.
This paper proposes a novel approach leveraging Large Language Models (LLMs) to
detect and refine NL defects in feature requests. Our approach automates the
identification of ambiguous and incomplete requests and generates clarification
questions (CQs) to enhance their usefulness for developers. To evaluate its
effectiveness, we apply our method to real-world OSS feature requests and
compare its performance against human annotations. In addition, we conduct
interviews with GitHub developers to gain deeper insights into their
perceptions of NL defects, the strategies they use to address these defects,
and the impact of defects on downstream software engineering (SE) tasks.

</details>


### [5] [Testing Autonomous Driving Systems -- What Really Matters and What Doesn't](https://arxiv.org/abs/2507.13661)
*Changwen Li,Joseph Sifakis,Rongjie Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 论文探讨了自动驾驶系统测试方法的有效性及有效性依赖的设计原则，发现现有方法大多不符合要求，并建议开发时考虑理性与确定性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统测试领域缺乏统一评估基础，论文旨在填补这一空白，评估现有方法的有效性和有效性。

Method: 提出一个框架比较测试方法的内在有效性和有效性，分析其对自动驾驶设计原则的依赖性。

Result: 多数测试方法不满足要求，且自动驾驶设计中的理性和确定性对测试有效性至关重要。

Conclusion: 建议开发自动驾驶系统时考虑理性和确定性，以提升测试方法的有效性。

Abstract: Despite extensive research, the testing of autonomous driving systems (ADS)
landscape remains fragmented, and there is currently no basis for an informed
technical assessment of the importance and contribution of the current state of
the art. This paper attempts to address this problem by exploring two
complementary aspects.
  First, it proposes a framework for comparing existing test methods in terms
of their intrinsic effectiveness and validity. It shows that many methods do
not meet both of these requirements. Either because they are based on criteria
that do not allow for rapid, inexpensive, and comprehensive detection of
failures, or because the degree of validity of the properties tested cannot be
accurately estimated. In particular, it is shown that most critical test
methods do not take into account the nominal operational capabilities of
autopilots and generate scenarios that are impossible for the tested vehicles
to handle, resulting in unjustified rejections.
  Secondly, the paper shows that test effectiveness and validity are highly
dependent on how autopilots are designed: how they choose between different
control policies to perform maneuvers, as well as on the reproducibility of the
results. In fact, most test methods take for granted two principles underlying
traditional methods, but do not generally apply to ADS. We maintain that the
absence of rationality and determinacy significantly impairs the effectiveness
and validity of test methods, and provide test results on eight open
autopilots, in which most do not satisfy these properties, thereby illustrating
this fact.
  We conclude that under the current state of the art, it is impossible to
obtain strong enough guarantees for essential autopilot properties and
recommend that autopilots be developed with a view to both rationality and
determinacy.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Random Variate Generation with Formal Guarantees](https://arxiv.org/abs/2507.13494)
*Feras A. Saad,Wonyeol Lee*

Main category: cs.PL

TL;DR: 提出了一种精确且高效生成随机变量的新方法，通过有限精度数值程序定义CDF，自动合成精确生成器，避免溢出和高成本运算，提供高精度和熵效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在随机变量生成中常面临精度和效率问题，需要一种既能保证精确性又能高效运行的新方法。

Method: 基于有限精度数值程序定义CDF，自动合成生成器，采用空间时间最优实现，确保信息理论最优的熵率。

Result: 在C语言中实现的库在多样分布上表现优异，运行时与GNU科学库竞争，同时提供更高精度和熵效率。

Conclusion: 新方法在随机变量生成中实现了自动化、高精度和效率，适用于多种分布类型。

Abstract: This article introduces a new approach to principled and practical random
variate generation with formal guarantees. The key idea is to first specify the
desired probability distribution in terms of a finite-precision numerical
program that defines its cumulative distribution function (CDF), and then
generate exact random variates according to this CDF. We present a universal
and fully automated method to synthesize exact random variate generators given
any numerical CDF implemented in any binary number format, such as
floating-point, fixed-point, and posits. The method is guaranteed to operate
with the same precision used to specify the CDF, does not overflow, avoids
expensive arbitrary-precision arithmetic, and exposes a consistent API. The
method rests on a novel space-time optimal implementation for the class of
generators that attain the information-theoretically optimal Knuth and Yao
entropy rate, consuming the least possible number of input random bits per
output variate. We develop a random variate generation library using our method
in C and evaluate it on a diverse set of ``continuous'' and ``discrete''
distributions, showing competitive runtime with the state-of-the-art GNU
Scientific Library while delivering higher accuracy, entropy efficiency, and
automation.

</details>


### [7] [Increasing the Expressiveness of a Gradual Verifier](https://arxiv.org/abs/2507.13533)
*Priyam Gupta*

Main category: cs.PL

TL;DR: 本文介绍了一种扩展Gradual C0的方法，通过支持展开表达式来更直观地指定递归堆数据结构。


<details>
  <summary>Details</summary>
Motivation: 静态验证虽然能提供强大的代码正确性保证，但完整规范程序的静态验证过程复杂且繁琐。逐步验证的引入旨在通过部分规范程序来简化这一过程。现有的逐步验证工具Gradual C0在堆操作程序验证上表现良好，但其规范语言表达能力有限。

Method: 本文设计并实现了对Gradual C0的扩展，增加了支持展开表达式的功能，从而提升了对递归堆数据结构的规范表达能力。

Result: 扩展后的Gradual C0能够更直观地指定递归堆数据结构，增强了规范语言的表达能力。

Conclusion: 通过支持展开表达式，扩展的Gradual C0在保持逐步验证优势的同时，提高了对复杂数据结构的规范能力。

Abstract: Static verification provides strong correctness guarantees for code; however,
fully specifying programs for static verification is a complex, burdensome
process for users. Gradual verification was introduced to make this process
easier by supporting the verification of partially specified programs. The only
currently working gradual verifier, Gradual C0, successfully verifies heap
manipulating programs, but lacks expressiveness in its specification language.
This paper describes the design and implementation of an extension to Gradual
C0 that supports unfolding expressions, which allow more intuitive
specifications of recursive heap data structures.

</details>


### [8] [AdapTT: Functoriality for Dependent Type Casts](https://arxiv.org/abs/2507.13774)
*Arthur Adjedj,Meven Lennon-Bertrand,Thibaut Benjamin,Kenji Maillard*

Main category: cs.PL

TL;DR: AdapTT是一种类型理论，通过抽象的适配器概念系统地研究了类型构造器的函子性，从而为相关类型之间的转换提供了通用方法。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于统一和分析依赖类型理论中不同类型转换（如观测类型理论、子类型或渐进类型中的强制转换）的共同结构行为。

Method: 提出了AdapTT类型理论，利用类型构造器的函子性，并通过抽象的适配器关系来精确描述类型转换。

Result: 通过AdapTT，可以推导出通用归纳类型构造器上的类型转换结构规律。

Conclusion: AdapTT为依赖类型系统中的类型转换提供了一种系统化和精确的理论框架。

Abstract: The ability to cast values between related types is a leitmotiv of many
flavors of dependent type theory, such as observational type theories,
subtyping, or cast calculi for gradual typing. These casts all exhibit a common
structural behavior that boils down to the pervasive functoriality of type
formers. We propose and extensively study a type theory, called AdapTT, which
makes systematic and precise this idea of functorial type formers, with respect
to an abstract notion of adapters relating types. Leveraging descriptions for
functorial inductive types in AdapTT, we derive structural laws for type casts
on general inductive type formers.

</details>


### [9] [Don't exhaust, don't waste](https://arxiv.org/abs/2507.13792)
*Riccardo Bianchini,Francesco Dagnino,Paola Giannini,Elena Zucca*

Main category: cs.PL

TL;DR: 这篇论文扩展了一个带有常见构造的λ演算的语义和类型系统，使其具备资源感知能力。语义跟踪资源的使用情况，并在资源耗尽或浪费时停止计算。类型系统确保良好类型的程序在不耗尽或浪费资源的情况下运行。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过类型系统和语义跟踪资源使用，以防止资源耗尽或浪费，从而保证程序的可靠性和效率。

Method: 采用基于大步骤语义的形式化方法，并利用任意的等级代数进行参数化扩展，避免对底层语言进行特定修改。使用共归纳推理技术证明资源感知的合理性。

Result: 扩展的类型系统能够在资源不耗尽或浪费的情况下保证程序的正确运行，且方法具有通用性。

Conclusion: 通过共归纳推理和大步骤语义，成功构建了一个资源感知的λ演算系统，为资源管理提供了理论支持。

Abstract: We extend the semantics and type system of a lambda calculus equipped with
common constructs to be resource-aware. That is, the semantics keep tracks of
the usage of resources, and is stuck, besides in case of type errors, if either
a needed resource is exhausted, or a provided resource would be wasted. In such
way, the type system guarantees, besides standard soundness, that for
well-typed programs there is a computation where no resource gets either
exhausted or wasted.
  The no-waste extension is parametric on an arbitrary grade algebra, modeling
an arbitrary assortment of possible usages, and does not require ad-hoc changes
to the underlying language. To this end, the semantics needs to be formalized
in big-step style; as a consequence, expressing and proving (resource-aware)
soundness is challenging, and is achieved by applying recent techniques based
on coinductive reasoning.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [10] [Photonic Fabric Platform for AI Accelerators](https://arxiv.org/abs/2507.14000)
*Jing Ding,Trung Diep*

Main category: cs.PF

TL;DR: 介绍了Photonic Fabric和Photonic Fabric Appliance（PFA），这是一种光子技术支持的交换和内存子系统，提供低延迟、高带宽和低能耗的特性，支持分布式AI训练和推理。


<details>
  <summary>Details</summary>
Motivation: 解决当前XPU加速器设计中固定内存与计算比例的限制，通过光子技术扩展内存容量和带宽。

Method: 使用2.5D电光系统封装技术，整合HBM3E内存、光交换模块和DDR5内存，并提供CelestiSim模拟器验证性能。

Result: 在LLM推理和训练中，实现最高7.04倍吞吐量提升、1.41倍延迟改善，及60-90%的数据移动能耗节省。

Conclusion: PFA技术可显著提升AI加速器性能，适用于多种XPU设计，突破现有内存与计算比例的限制。

Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM
(PFA), a photonic-enabled switch and memory subsystem that delivers low
latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth
HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D
electro-optical system-in-package, the PFA offers up to 32 TB of shared memory
alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM
enables distributed AI training and inference to execute parallelism strategies
more efficiently. The Photonic Fabric removes the silicon beachfront constraint
that limits the fixed memory-to-compute ratio observed in virtually all current
XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet
that connects to the Photonic Fabric increases its memory capacity and
correspondingly its memory bandwidth by offering a flexible path to scaling
well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a
lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It
is used to evaluate the performance of LLM reference and energy savings on PFA,
without any significant change to the GPU core design. With the PFA, the
simulation results show that up to 3.66x throughput and 1.40x latency
improvements in LLM inference at 405B parameters, up to 7.04x throughput and
1.41x latency improvements at 1T parameters, and 60-90% energy savings in data
movement for heavy collective operations in all LLM training scenarios. While
these results are shown for NVIDIA GPUs, they can be applied similarly to other
AI accelerator designs (XPUs) that share the same fundamental limitation of
fixed memory to compute.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica](https://arxiv.org/abs/2507.13476)
*Jaber Daneshamooz,Jessica Nguyen,William Chen,Sanjay Chandrasekaran,Satyandra Guthula,Ankit Gupta,Arpit Gupta,Walter Willinger*

Main category: cs.NI

TL;DR: NetReplica通过生成具有真实性和可控性的训练数据集，解决了网络机器学习模型的领域适应问题，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 网络机器学习模型在跨领域部署时表现不佳，需要解决领域适应问题。

Method: NetReplica将网络建模为具有特定属性的瓶颈链路集合，利用生产网络痕迹实现真实性，并通过精细控制链路属性实现可控性。

Result: NetReplica生成的样本不仅匹配现有数据特性，还能补充缺乏的数据，训练出的模型在挑战性网络条件下预测误差降低47%。

Conclusion: NetReplica为解决基于机器学习的网络系统领域适应问题迈出了重要一步。

Abstract: Machine learning models in networking suffer from the domain adaptation
problem; models trained in one domain often fail when deployed in different
production environments. This paper presents the design and implementation of
NetReplica, a system that addresses this challenge by generating training
datasets with two critical properties: realism in protocol dynamics and
controllability of network conditions. NetReplica models networks as
collections of bottleneck links with specific attributes, achieves realism by
leveraging production network traces, and enables controllability through fine
grained control knobs for each link attribute. Our evaluation using Puffer
demonstrates that NetReplica not only matches existing data characteristics but
generates realistic samples that are underrepresented in or absent from Puffer
data. Models trained on NetReplica augmented datasets show substantially
improved generalizability, reducing transmission time prediction error by up to
47% for challenging network conditions compared to models trained solely on
Puffer data. This work represents a significant step toward solving the domain
adaptation problem that has limited the effectiveness of ML based networking
systems.

</details>


### [12] [CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC](https://arxiv.org/abs/2507.13676)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Jiawei Hu,Chun Tung Chou,Wen Hu*

Main category: cs.NI

TL;DR: CARTS是一种自适应5G上行链路感知方案，通过融合DMRS和SRS两种CSI流，提升CSI更新频率和用户感知机会，显著提升系统性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代5G网络中，上行链路CSI通常来自DMRS和SRS，但现有基站将这两种信息流分开处理，限制了CSI更新频率和感知机会。CARTS旨在通过融合这两种流，提升ISAC性能。

Method: CARTS提出了一种新颖的信道拼接与补偿方法，整合异步的DMRS和SRS CSI估计，并结合实时SRS触发算法，确保用户获得足够的感知机会。

Result: 实验表明，CARTS显著提升了扩展性，支持的用户数是仅使用周期性SRS的基线的两倍，同时实现0.167的NMSE误差和85 cm的UE跟踪精度。

Conclusion: CARTS通过融合DMRS和SRS，无需额外资源即可提升ISAC的CSI可用性，是一种实用且符合标准的高效解决方案。

Abstract: This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to
provide Integrated Sensing and Communication (ISAC) services. The performance
of both communication and sensing fundamentally depends on the availability of
accurate and up-to-date channel state information (CSI). In modern 5G networks,
uplink CSI is derived from two reference signals: the demodulation reference
signal (DMRS) and the sounding reference signal (SRS). However, current base
station implementations treat these CSI measurements as separate information
streams. The key innovation of CARTS is to fuse these two CSI streams, thereby
increasing the frequency of CSI updates and extending sensing opportunities to
more users. CARTS addresses two key challenges: (i) a novel channel stitching
and compensation method that integrates asynchronous CSI estimates from DMRS
and SRS, despite their different time and frequency allocations, and (ii) a
real-time SRS triggering algorithm that complements the inherently
uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing
opportunities for all users. Our trace-driven evaluation shows that CARTS
significantly improves scalability, achieving a channel estimation error (NMSE)
of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of
users as a periodic SRS-only baseline with similar performance. By
opportunistically combining DMRS and SRS, CARTS therefore provides a practical,
standard-compliant solution to improve CSI availability for ISAC without
requiring additional radio resources.

</details>


### [13] [ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks](https://arxiv.org/abs/2507.13717)
*Yingming Mao,Qiaozhu Zhai,Zhen Yao,Xia Zhu,Ximeng Liu,Xinchi Han*

Main category: cs.NI

TL;DR: ATRO是一个解决可重构数据中心网络（DCN）中拓扑与路由优化问题的无求解器框架，通过交替优化拓扑和路由，显著提高了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在平衡解决方案质量和运行时间效率上的不足，尤其是在拓扑决策复杂的情况下。

Method: 引入了交替拓扑和路由优化（ATRO）框架，利用交替更新步骤单调降低最大链路利用率（MLU），并通过高效的加速二分搜索法（ABSM）解决拓扑优化子问题。

Result: ATRO在单跳场景中达到全局最优，在多跳场景中显著优于基线方法，表现出良好的可扩展性和鲁棒性。

Conclusion: ATRO框架有效地解决了可重构DCN中的拓扑和路由优化问题，为大规模复杂网络提供了一种高效的解决方案。

Abstract: The growing scale and complexity of reconfigurable data center networks
(DCNs) demand more scalable and efficient algorithms for computing logical
topologies and routing. Reconfigurable DCNs typically operate in two modes:
one-hop configurations that require frequent topology optimization (TO), and
multi-hop scenarios that involve joint topology and routing optimization (TRO).
In both cases, the combinatorial nature of topology decisions makes it
difficult for existing methods to balance solution quality and runtime
efficiency. To address this, we introduce Alternating Topology and Routing
Optimization (ATRO), a solver-free framework that alternates between TO and
routing optimization (RO). This decomposition exploits two key insights: first,
each alternating update step monotonically reduces maximum link utilization
(MLU), ensuring consistent performance improvement across iterations; second,
the TO subproblem, equivalent to one-hop optimization, exhibits a monotonic
structure that enables optimal solutions via an efficient Accelerated Binary
Search Method (ABSM). To preserve the solver-free design, RO is solved using
existing Traffic Engineering accelerators. ATRO attains the global optimum in
one-hop scenarios and significantly outperforms baselines in multi-hop settings
in terms of both runtime and solution quality. Evaluations confirm its
scalability and robustness across diverse DCNs.

</details>


### [14] [On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies](https://arxiv.org/abs/2507.13889)
*Bilal Karaman,Ilhan Basturk,Ferdi Kara,Metin Ozturk,Sezai Taskin,Halil Yanikomeroglu*

Main category: cs.NI

TL;DR: 研究探讨了将主动可重构智能表面（RIS）与高空平台站（HAPS）结合以提升非地面网络性能的方法，表明主动RIS在长距离通信中优于被动RIS。


<details>
  <summary>Details</summary>
Motivation: 由于HAPS链路存在严重的路径损耗和双重衰落，传统被动RIS架构性能受限，研究旨在利用主动RIS的信号放大能力提升网络性能。

Method: 通过联合优化功率分配和RIS单元分配，提出了基于HAPS的主动RIS辅助通信系统，并探讨了多种子连接架构以减少功耗和硬件复杂度。

Result: 仿真显示主动RIS在服务质量（QoS）上显著优于被动RIS，且子连接架构在能效上表现更佳。

Conclusion: 主动RIS支持的HAPS系统有望满足未来超蜂窝覆盖和绿色网络的需求。

Abstract: This paper investigates the integration of active reconfigurable intelligent
surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance
non-terrestrial network (NTN) performance in next-generation wireless systems.
While prior studies focused on passive RIS architectures, the severe path loss
and double fading in long-distance HAPS links make active RIS a more suitable
alternative due to its inherent signal amplification capabilities. We formulate
a sum-rate maximization problem to jointly optimize power allocation and RIS
element assignment for ground user equipments (UEs) supported by a HAPS-based
active RIS-assisted communication system. To reduce power consumption and
hardware complexity, several sub-connected active RIS architectures are also
explored. Simulation results reveal that active RIS configurations
significantly outperform passive RIS in terms of quality of service (QoS).
Moreover, although fully-connected architectures achieve the highest
throughput, sub-connected schemes demonstrate superior energy efficiency under
practical power constraints. These findings highlight the potential of active
RIS-enabled HAPS systems to meet the growing demands of beyond-cellular
coverage and green networking.

</details>


### [15] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 论文探讨了由大型语言模型（LLM）生成的“LLM主导”内容的不可靠性和伦理问题，提出了一种高可靠、可扩展的分类方法，能够对整个网站进行检测。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的内容存在剽窃和幻觉问题，且网站很少披露此类内容，人类读者难以辨别，因此需要开发可靠检测LLM主导内容的方法。

Method: 提出了一种基于LLM文本检测器输出的多页文本分类的可靠管道，通过收集两个不同的真实数据集（总计120个网站）进行训练和评估。

Result: 在真实数据集上检测准确率达到100%，并在搜索引擎和Common Crawl存档中发现了大量LLM主导的网站。

Conclusion: LLM主导网站在搜索结果中排名较高且呈增长趋势，对其对最终用户和整个网络生态系统的影响提出了疑问。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [16] [SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.13415)
*Peican Zhu,Yubo Jing,Le Cheng,Bin Chen,Xiaodong Cui,Lianwei Wu,Keke Tang*

Main category: cs.MM

TL;DR: 提出了一种新颖的多模态假新闻检测方法SEER，通过语义增强和情感推理提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了大型多模态模型的语义增强和新闻的情感特征，而假新闻往往包含更多负面情绪。

Method: 结合大型多模态模型生成图像摘要以增强语义理解，并设计情感推理模块优化情感特征以推断新闻真实性。

Result: 在两个真实数据集上的实验显示SEER优于现有基准方法。

Conclusion: SEER通过语义增强和情感推理有效提升了多模态假新闻检测的性能。

Abstract: Previous studies on multimodal fake news detection mainly focus on the
alignment and integration of cross-modal features, as well as the application
of text-image consistency. However, they overlook the semantic enhancement
effects of large multimodal models and pay little attention to the emotional
features of news. In addition, people find that fake news is more inclined to
contain negative emotions than real ones. Therefore, we propose a novel
Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake
news detection. We generate summarized captions for image semantic
understanding and utilize the products of large multimodal models for semantic
enhancement. Inspired by the perceived relationship between news authenticity
and emotional tendencies, we propose an expert emotional reasoning module that
simulates real-life scenarios to optimize emotional features and infer the
authenticity of news. Extensive experiments on two real-world datasets
demonstrate the superiority of our SEER over state-of-the-art baselines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [17] [Complexity of Abduction in Łukasiewicz Logic](https://arxiv.org/abs/2507.13847)
*Katsumi Inoue,Daniil Kozhemiachenko*

Main category: cs.LO

TL;DR: 该论文探讨了在涉及模糊逻辑的语境中解释观察结果的问题，研究了在Łukasiewicz模糊逻辑中的溯因问题及其复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决涉及真理程度的语句（如‘电梯已满载’、‘症状严重’等）的解释问题，填补模糊逻辑中溯因推理的空白。

Method: 方法是通过定义和分析Łukasiewicz逻辑中的溯因问题和解释，使用包含区间文字的扩展语言，并对其复杂性进行分类。

Result: 结果表明，与经典命题逻辑不同，Łukasiewicz逻辑中的溯因问题在子句片段中的复杂性低于一般情况。

Conclusion: 结论是指出了模糊逻辑中溯因推理的复杂性差异，为未来研究提供了方向。

Abstract: We explore the problem of explaining observations in contexts involving
statements with truth degrees such as `the lift is loaded', `the symptoms are
severe', etc. To formalise these contexts, we consider infinitely-valued
{\L}ukasiewicz fuzzy logic. We define and motivate the notions of abduction
problems and explanations in the language of {\L}ukasiewicz logic expanded with
`interval literals' of the form $p\geq\mathbf{c}$, $p\leq\mathbf{c}$, and their
negations that express the set of values a variable can have. We analyse the
complexity of standard abductive reasoning tasks (solution recognition,
solution existence, and relevance / necessity of hypotheses) in {\L}ukasiewicz
logic for the case of the full language and for the case of theories containing
only disjunctive clauses and show that in contrast to classical propositional
logic, the abduction in the clausal fragment has lower complexity than in the
general case.

</details>


### [18] [Application Placement with Constraint Relaxation](https://arxiv.org/abs/2507.13895)
*Damiano Azzolini,Marco Duca,Stefano Forti,Francesco Gallo,Antonio Ielo*

Main category: cs.LO

TL;DR: 论文提出了一种基于答案集编程优化方法，解决云计算边缘网络中服务放置的不可满足性和偏好问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法处理不可满足的问题实例或DevOps的偏好需求，因此需要一种新的优化方法。

Method: 利用答案集编程（ASP）的优化能力，处理服务放置的约束和偏好问题。

Result: 在模拟环境中，该方法在真实网络和应用程序中表现有效。

Conclusion: 答案集编程为解决云计算边缘网络中的服务放置问题提供了有效工具。

Abstract: Novel utility computing paradigms rely upon the deployment of multi-service
applications to pervasive and highly distributed cloud-edge infrastructure
resources. Deciding onto which computational nodes to place services in
cloud-edge networks, as per their functional and non-functional constraints,
can be formulated as a combinatorial optimisation problem. Most existing
solutions in this space are not able to deal with \emph{unsatisfiable} problem
instances, nor preferences, i.e. requirements that DevOps may agree to relax to
obtain a solution. In this article, we exploit Answer Set Programming
optimisation capabilities to tackle this problem. Experimental results in
simulated settings show that our approach is effective on lifelike networks and
applications.

</details>


### [19] [Bounded Inquisitive Logics: Sequent Calculi and Schematic Validity](https://arxiv.org/abs/2507.13946)
*Tadeusz Litak,Katsuhiko Sano*

Main category: cs.LO

TL;DR: 该论文探讨了命题探究逻辑与谓词探究逻辑在有限近似中的差异，并提出了无切割的标号序列演算方法。论文还通过实例分析了模式有效性的复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解命题和谓词探究逻辑在有限近似中的表现差异，并开发相应的标号序列演算方法。

Method: 方法包括引入无切割的标号序列演算系统，并通过分析Casari公式的模式有效性来展示系统的复杂性。

Result: 结果显示，命题探究逻辑的有限近似极限成立，但谓词逻辑中不成立；同时，开发了标号序列演算系统，并展示了模式有效性的特殊案例。

Conclusion: 结论指出，谓词探究逻辑的有限近似表现与命题逻辑不同，且模式有效性在某些情况下需要特殊规则才能保证。

Abstract: Propositional inquisitive logic is the limit of its $n$-bounded
approximations. In the predicate setting, however, this does not hold anymore,
as discovered by Ciardelli and Grilletti, who also found complete
axiomatizations of $n$-bounded inquisitive logics $\mathsf{InqBQ}_{n}$, for
every fixed $n$. We introduce cut-free labelled sequent calculi for these
logics. We illustrate the intricacies of \textit{schematic validity} in such
systems by showing that the well-known Casari formula is \textit{atomically}
valid in (a weak sublogic of) predicate inquisitive logic $\mathsf{InqBQ}$,
fails to be schematically valid in it, and yet is schematically valid under the
finite boundedness assumption. The derivations in our calculi, however, are
guaranteed to be schematically valid whenever a single specific rule is not
used.

</details>


### [20] [ChemLog: Making MSOL Viable for Ontological Classification and Learning](https://arxiv.org/abs/2507.13987)
*Simon Flügel,Martin Glauer,Till Mossakowski,Fabian Neuhaus*

Main category: cs.LO

TL;DR: 论文提出了一种使用单子二阶逻辑形式化方法进行本体分类的途径，通过在化学本体ChEBI中的肽类进行案例研究，并结合深度学习模型提升分类性能。


<details>
  <summary>Details</summary>
Motivation: OWL在本体分类中的表达性不足，需要一种更强大的逻辑方法来弥补这一缺陷。

Method: 采用单子二阶逻辑形式化方法应用于ChEBI中的14个肽类，并进一步与深度学习模型结合。

Result: 逻辑方法显著提升了深度学习模型在ChEBI本体分类中的性能。

Conclusion: 单子二阶逻辑形式化方法结合深度学习，有效提升了本体分类的能力和规模。

Abstract: Despite its prevalence, in many domains, OWL is not expressive enough to
define ontology classes. In this paper, we present an approach that allows to
use monadic second-order formalisations for ontology classification. As a case
study, we have applied our approach to 14 peptide-related classes from the
chemistry ontology ChEBI. For these classes, a monadic second-order logic
formalisation has been developed and applied both to ChEBI as well as to 119
million molecules from the chemistry database PubChem. While this logical
approach alone is limited to classification for the specified classes (in our
case, (sub)classes of peptides), transformer deep learning models scale
classification to the whole of the ChEBI ontology. We show that when using the
classifications obtained by the logical approach as training data, the
performance of the deep learning models can be significantly enhanced.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [Humans learn to prefer trustworthy AI over human partners](https://arxiv.org/abs/2507.13524)
*Yaomin Jiang,Levin Brinkmann,Anne-Marie Nussberger,Ivan Soraperra,Jean-François Bonnefon,Iyad Rahwan*

Main category: cs.HC

TL;DR: 研究探讨人类在AI竞争压力下如何选择合作伙伴，发现隐藏身份时人类会误判AI行为，而公开身份虽减少AI初期被选率但最终因其高亲社会性胜出。


<details>
  <summary>Details</summary>
Motivation: 研究AI与人类在合作中的竞争动态，揭示AI如何重塑混合社会的社交互动。

Method: 设计基于沟通的伙伴选择游戏，通过三个实验（共975人）研究人类与LLM驱动的机器人在混合小社会中的互动。

Result: 隐藏身份时人类误判AI行为，公开身份后AI虽初期被选率低，但最终因其亲社会性胜出。

Conclusion: 研究展示了AI如何影响混合社会互动，为设计更有效的混合合作系统提供参考。

Abstract: Partner selection is crucial for cooperation and hinges on communication. As
artificial agents, especially those powered by large language models (LLMs),
become more autonomous, intelligent, and persuasive, they compete with humans
for partnerships. Yet little is known about how humans select between human and
AI partners and adapt under AI-induced competition pressure. We constructed a
communication-based partner selection game and examined the dynamics in hybrid
mini-societies of humans and bots powered by a state-of-the-art LLM. Through
three experiments (N = 975), we found that bots, though more prosocial than
humans and linguistically distinguishable, were not selected preferentially
when their identity was hidden. Instead, humans misattributed bots' behaviour
to humans and vice versa. Disclosing bots' identity induced a dual effect: it
reduced bots' initial chances of being selected but allowed them to gradually
outcompete humans by facilitating human learning about the behaviour of each
partner type. These findings show how AI can reshape social interaction in
mixed societies and inform the design of more effective and cooperative hybrid
systems.

</details>


### [22] [Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface](https://arxiv.org/abs/2507.13528)
*Daniele Masti,Stefano Menchetti,Çağrı Erdem,Giorgio Gnecco,Davide Rocchesso*

Main category: cs.HC

TL;DR: TickTacking是一种基于节奏的界面，通过双按钮敲击控制二维指针。论文研究了使用预测方法生成类人轨迹，并在目标跟踪任务中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 开发更直观的节奏人机界面，提升用户表现和减少交互挫折。

Method: 分析用户轨迹，提取关键行为特征，设计模仿这些行为的控制器。

Result: 人类启发控制器优于基线最优控制代理，验证了特定特征对类人交互的重要性。

Conclusion: 研究为设计更人性化的节奏界面提供了见解，提升用户体验。

Abstract: TickTacking is a rhythm-based interface that allows users to control a
pointer in a two-dimensional space through dual-button tapping. This paper
investigates the generation of human-like trajectories using a receding horizon
approach applied to the TickTacking interface in a target-tracking task. By
analyzing user-generated trajectories, we identify key human behavioral
features and incorporate them in a controller that mimics these behaviors. The
performance of this human-inspired controller is evaluated against a baseline
optimal-control-based agent, demonstrating the importance of specific control
features for achieving human-like interaction. These findings contribute to the
broader goal of developing rhythm-based human-machine interfaces by offering
design insights that enhance user performance, improve intuitiveness, and
reduce interaction frustration

</details>


### [23] [From Firms to Computation: AI Governance and the Evolution of Institutions](https://arxiv.org/abs/2507.13616)
*Michael S. Harre*

Main category: cs.HC

TL;DR: 本文通过整合多层级选择理论、Aoki的企业计算过程观点和Ostrom的稳健制度设计原则，提出一个框架，研究AI代理在社会经济系统中的选择和治理机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于重新审视AI代理融入社会经济系统时，如何通过选择和治理机制共同决定经济结果。

Method: 方法包括多层级Price方程、分布式推断框架和Ostrom规则的应用。

Result: 结果展示了选择和治理如何共同影响经济结果，并通过案例验证了框架的解释力。

Conclusion: 结论提出了一套设计原则，以实现人类与AI在制度层面的对齐，并提出实际政策建议。

Abstract: The integration of agential artificial intelligence into socioeconomic
systems requires us to reexamine the evolutionary processes that describe
changes in our economic institutions. This article synthesizes three
frameworks: multi-level selection theory, Aoki's view of firms as computational
processes, and Ostrom's design principles for robust institutions. We develop a
framework where selection operates concurrently across organizational levels,
firms implement distributed inference via game-theoretic architectures, and
Ostrom-style rules evolve as alignment mechanisms that address AI-related
risks. This synthesis yields a multi-level Price equation expressed over nested
games, providing quantitative metrics for how selection and governance
co-determine economic outcomes. We examine connections to Acemoglu's work on
inclusive institutions, analyze how institutional structures shape AI
deployment, and demonstrate the framework's explanatory power via case studies.
We conclude by proposing a set of design principles that operationalize
alignment between humans and AI across institutional layers, enabling scalable,
adaptive, and inclusive governance of agential AI systems. We conclude with
practical policy recommendations and further research to extend these
principles into real-world implementation.

</details>


### [24] [In-Home Social Robots Design for Cognitive Stimulation Therapy in Dementia Care](https://arxiv.org/abs/2507.13578)
*Emmanuel Akinrintoyo,Nicole Salomons*

Main category: cs.HC

TL;DR: 研究设计了社交辅助机器人系统，用于为痴呆症患者提供个体化认知刺激治疗（iCST），以解决家庭成员参与度低的问题。用户中心设计和评估显示，患者乐于使用该系统且愿意长期使用，但语音转文字功能存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有的个体化认知刺激治疗（iCST）因家庭成员参与度低而效果有限，因此研究探索了社交辅助机器人系统作为替代方案。

Method: 通过与16名痴呆症护理人员和专业人士的咨询，制定设计指南并开发原型，随后由3名专业人士和5名痴呆症患者进行测试。

Result: 痴呆症患者乐于使用该系统并愿意长期采用，但系统的语音转文字功能存在频繁识别失败的问题。

Conclusion: 社交辅助机器人系统有望弥补iCST治疗中家庭成员参与不足的缺陷，但需改进语音识别技术以提高实用性。

Abstract: Individual cognitive stimulation therapy (iCST) is a non-pharmacological
intervention for improving the cognition and quality of life of persons with
dementia (PwDs); however, its effectiveness is limited by low adherence to
delivery by their family members. In this work, we present the user-centered
design and evaluation of a novel socially assistive robotic system to provide
iCST therapy to PwDs in their homes for long-term use. We consulted with 16
dementia caregivers and professionals. Through these consultations, we gathered
design guidelines and developed the prototype. The prototype was validated by
testing it with three dementia professionals and five PwDs. The evaluation
revealed PwDs enjoyed using the system and are willing to adopt its use over
the long term. One shortcoming was the system's speech-to-text capabilities,
where it frequently failed to understand the PwDs.

</details>


### [25] [Managing level of detail through peripheral degradation: Effects on search performance with a head-mounted display](https://arxiv.org/abs/2507.13660)
*Benjamin Watson,Neff Walker,Larry F Hodges,Aileen Worden*

Main category: cs.HC

TL;DR: 论文通过两项用户研究评估了头戴显示器外围细节层次（LOD）降低对视觉搜索性能的影响。一项研究降低分辨率，另一项研究使用灰度化。结果显示，外围LOD降级可在不显著影响性能的情况下减少视觉复杂度。


<details>
  <summary>Details</summary>
Motivation: 探讨头戴显示器外围细节层次降级对视觉搜索性能的潜在影响，以优化用户体验。

Method: 进行两项用户研究，分别通过降低分辨率和灰度化来降级外围细节，10名受试者完成复杂搜索任务。

Result: 外围LOD降级可将颜色或空间视觉复杂度降低近一半，同时性能未显著下降。

Conclusion: 外围细节层次降级是优化头戴显示器视觉复杂度的有效方法。

Abstract: Two user studies were performed to evaluate the effect of level-of-detail
(LOD) degradation in the periphery of head-mounted displays on visual search
performance. In the first study, spatial detail was degraded by reducing
resolution. In the second study, detail was degraded in the color domain by
using grayscale in the periphery. In each study, 10 subjects were given a
complex search task that required users to indicate whether or not a target
object was present among distracters. Subjects used several different displays
varying in the amount of detail presented. Frame rate, object location, subject
input method, and order of display use were all controlled. The primary
dependent measures were search time on correctly performed trials and the
percentage of all trials correctly performed. Results indicated that peripheral
LOD degradation can be used to reduce color or spatial visual complexity by
almost half in some search tasks with out significantly reducing performance.

</details>


### [26] [Regression-Based Approach to Anxiety Estimation of Spider Phobics During Behavioural Avoidance Tasks](https://arxiv.org/abs/2507.13795)
*Florian Grensing,Vanessa Schmücker,Anne Sophie Hildebrand,Tim Klucken,Maria Maleshkova*

Main category: cs.HC

TL;DR: 利用手腕穿戴传感器收集的生理数据训练回归模型，预测焦虑评分，结合上下文任务信息后模型效果提升。


<details>
  <summary>Details</summary>
Motivation: 评估焦虑反应的现有方法（如问卷和行为回避测试）仅提供瞬间数据，穿戴设备可以帮助持续监测焦虑强度，辅助治疗。

Method: 25名参与者进行四种行为回避测试时，收集心率、心率变异性、皮肤电活动和皮肤温度等生理数据，训练三种回归模型（仅生理信号、添加特征、结合上下文信息）。

Result: 结合上下文信息的模型效果最佳（RMSE=0.197，MAE=0.041），表明穿戴设备数据可连续预测焦虑强度。

Conclusion: 腕戴传感器数据能持续提供有意义的焦虑评估，为个性化治疗和方案制定提供支持。

Abstract: Phobias significantly impact the quality of life of affected persons. Two
methods of assessing anxiety responses are questionnaires and behavioural
avoidance tests (BAT). While these can be used in a clinical environment they
only record momentary insights into anxiety measures. In this study, we
estimate the intensity of anxiety during these BATs, using physiological data
collected from unobtrusive, wrist-worn sensors. Twenty-five participants
performed four different BATs in a single session, while periodically being
asked how anxious they currently are. Using heart rate, heart rate variability,
electrodermal activity, and skin temperature, we trained regression models to
predict anxiety ratings from three types of input data: (1) using only
physiological signals, (2) adding computed features (e.g., min, max, range,
variability), and (3) computed features combined with contextual task
information. Adding contextual information increased the effectiveness of the
model, leading to a root mean squared error (RMSE) of 0.197 and a mean absolute
error (MAE) of 0.041. Overall, this study shows, that data obtained from
wearables can continuously provide meaningful estimations of anxiety, which can
assist in therapy planning and enable more personalised treatment.

</details>


### [27] [Effects of Cognitive Distraction and Driving Environment Complexity on Adaptive Cruise Control Use and Its Impact on Driving Performance: A Simulator Study](https://arxiv.org/abs/2507.13886)
*Anaïs Halin,Marc Van Droogenbroeck,Christel Devue*

Main category: cs.HC

TL;DR: 研究表明，驾驶环境复杂性会降低驾驶员对自适应巡航控制（ACC）的依赖，而认知负荷对ACC使用无显著影响。ACC使用对变道次数无影响，但能提高车速合规性和横向控制。


<details>
  <summary>Details</summary>
Motivation: 探讨驾驶员的认知状态和驾驶环境复杂性是否以及如何影响他们对驾驶自动化功能的依赖，并分析这种依赖对驾驶性能的影响。

Method: 采用模拟器研究，参与者操作配备ACC的车辆，在六种不同交通条件下进行驾驶，部分参与者同时进行认知任务（心算）。记录ACC使用情况及驾驶表现。

Result: 复杂驾驶环境中ACC使用时间较少；认知负荷对ACC使用无显著影响；ACC使用不影响变道次数，但提高车速合规性和横向控制。

Conclusion: 驾驶环境复杂性是影响ACC依赖的主要因素，而ACC能提升部分驾驶性能，但对变道行为无影响。

Abstract: In this simulator study, we adopt a human-centered approach to explore
whether and how drivers' cognitive state and driving environment complexity
influence reliance on driving automation features. Besides, we examine whether
such reliance affects driving performance. Participants operated a vehicle
equipped with adaptive cruise control (ACC) in a simulator across six
predefined driving scenarios varying in traffic conditions while either
performing a cognitively demanding task (i.e., responding to mental
calculations) or not. Throughout the experiment, participants had to respect
speed limits and were free to activate or deactivate ACC. In complex driving
environments, we found that the overall ACC engagement time was lower compared
to less complex driving environments. We observed no significant effect of
cognitive load on ACC use. Furthermore, while ACC use had no effect on the
number of lane changes, it impacted the speed limits compliance and improved
lateral control.

</details>


### [28] [Initiating and Replicating the Observations of Interactional Properties by User Studies Optimizing Applicative Prototypes](https://arxiv.org/abs/2507.13923)
*Guillaume Rivière*

Main category: cs.HC

TL;DR: 本文提出了一种形式化的用户交互观察方法（交互环衍射），支持在不同条件下复现交互属性，旨在优化应用原型并推动交互科学的理论建设。


<details>
  <summary>Details</summary>
Motivation: HCI领域的研究通常局限于特定技术、设计和任务的孤立发现，缺乏普适性和复现性。

Method: 通过形式化用户交互观察和交互环衍射方法，研究可校准的交互属性。

Result: 交互属性能在不同条件下复现，优化应用原型，并为交互科学提供理论和实证基础。

Conclusion: 该方法有助于提升泛在用户界面的交互体验，并推动HCI领域的理论积累和科学化。

Abstract: The science of Human-Computer Interaction (HCI) is populated by isolated
empirical findings, often tied to specific technologies, designs, and tasks.
This paper proposes a formalization of user interaction observations (instead
of user interfaces) and an associated revealing method (interaction loop
diffraction). The resulting interactional properties that are studied in a
calibrated manner, are well suited to replication across various conditions
(prototypes, technologies, tasks, and user profiles). In particular,
interactional properties can emerge and be replicated within the workflow of
applicative cases, which in return benefit from the optimization of applicative
prototypes. Applicative cases' publications will then contribute to
demonstrating technology utility, along with providing empirical results that
will lead future work to theory consolidation and theory building, and finally
to a catalog and a science of relevant interactional properties. These
properties will contribute to better user interactions, especially for the
variety of ubiquitous user interfaces.

</details>


### [29] [Democratizing Game Modding with GenAI: A Case Study of StarCharM, a Stardew Valley Character Maker](https://arxiv.org/abs/2507.13951)
*Hamid Zand Miralvand,Mohammad Ronagh Nikghalb,Mohammad Darandeh,Abidullah Khan,Ian Arawjo,Jinghui Cheng*

Main category: cs.HC

TL;DR: StarCharM是一款基于GenAI的NPC创建工具，旨在降低游戏模组的技术门槛，让更多玩家参与模组制作。用户研究表明，虽然工具激发了创造力，但也存在内容生成不足和原创性担忧的问题。


<details>
  <summary>Details</summary>
Motivation: 为降低游戏模组制作的技术门槛，让更多玩家能够轻松创作个性化的NPC模组。

Method: 设计并开发了StarCharM工具，通过GenAI技术简化NPC模组创建过程，支持用户迭代调整，并进行了一项包含10名用户的研究。

Result: 用户对工具能快速实现角色创意表示兴奋，但也指出其在复杂内容生成上的局限性。此外，GenAI工具可能影响原创性和社区互动。

Conclusion: 研究为未来GenAI驱动的模组工具提供了设计启示，强调了平衡易用性与内容质量、维护社区参与的重要性。

Abstract: Game modding offers unique and personalized gaming experiences, but the
technical complexity of creating mods often limits participation to skilled
users. We envision a future where every player can create personalized mods for
their games. To explore this space, we designed StarCharM, a GenAI-based
non-player character (NPC) creator for Stardew Valley. Our tool enables players
to iteratively create new NPC mods, requiring minimal user input while allowing
for fine-grained adjustments through user control. We conducted a user study
with ten Stardew Valley players who had varied mod usage experiences to
understand the impacts of StarCharM and provide insights into how GenAI tools
may reshape modding, particularly in NPC creation. Participants expressed
excitement in bringing their character ideas to life, although they noted
challenges in generating rich content to fulfill complex visions. While they
believed GenAI tools like StarCharM can foster a more diverse modding
community, some voiced concerns about diminished originality and community
engagement that may come with such technology. Our findings provided
implications and guidelines for the future of GenAI-powered modding tools and
co-creative modding practices.

</details>


### [30] [Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy (fNIRS) Signals using Machine Learning](https://arxiv.org/abs/2507.13952)
*Shayla Sharmin,Roghayeh Leila Barmaki*

Main category: cs.HC

TL;DR: 论文通过机器学习模型基于fNIRS数据预测认知努力，结合神经激活和行为表现，提升教育游戏的效果。


<details>
  <summary>Details</summary>
Motivation: 研究认知努力的评估，以优化教育材料，提高学习效果和学生参与度。

Method: 使用fNIRS收集前额叶皮层的氧合血红蛋白数据，结合游戏任务行为表现，训练机器学习模型预测表现，计算认知努力。

Result: 模型预测准确率为58%-67%，但标准化后的神经效率和参与度指标显示了稳健的认知努力趋势。

Conclusion: 尽管预测准确率中等，但计算出的认知努力指标具有稳健性，为教育游戏优化提供了有效工具。

Abstract: The estimation of cognitive effort could potentially help educators to modify
material to enhance learning effectiveness and student engagement. Where
cognitive load refers how much work the brain is doing while someone is
learning or doing a task cognitive effort consider both load and behavioral
performance. Cognitive effort can be captured by measuring oxygen flow and
behavioral performance during a task. This study infers cognitive effort
metrics using machine learning models based on oxygenated hemoglobin collected
by using functional near-infrared spectroscopy from the prefrontal cortex
during an educational gameplay. In our study, sixteen participants responded to
sixteen questions in an in-house Unity-based educational game. The quiz was
divided into two sessions, each session consisting of two task segments. We
extracted temporal statistical and functional connectivity features from
collected oxygenated hemoglobin and analyzed their correlation with quiz
performance. We trained multiple machine learning models to predict quiz
performance from oxygenated hemoglobin features and achieved accuracies ranging
from 58\% to 67\% accuracy. These predictions were used to calculate cognitive
effort via relative neural involvement and efficiency, which consider both
brain activation and behavioral performance. Although quiz score predictions
achieved moderate accuracy, the derived relative neural efficiency and
involvement values remained robust. Since both metrics are based on the
relative positions of standardized brain activation and performance scores,
even small misclassifications in predicted scores preserved the overall
cognitive effort trends observed during gameplay.

</details>


### [31] [Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors](https://arxiv.org/abs/2507.14034)
*Jochen Wulf,Jurg Meierhofer,Frank Hannich*

Main category: cs.HC

TL;DR: 该论文提出了一种六模式的人类与代理AI合作分类法，旨在指导在不同自主级别下的任务协作，基于任务复杂性、操作风险等因素为技术服务的自动化与控制提供了系统化的设计框架。


<details>
  <summary>Details</summary>
Motivation: 目前由大型语言模型（LLMs）驱动的代理AI系统在技术服务中具有巨大的价值共创潜力，但其幻觉问题和操作脆弱性限制了其自主应用，因此需要一种稳健的框架来指导人机协作。

Method: 基于人类与AI协作的研究及自主驾驶等领域类比，结合技术支持平台的案例研究，提出了一种六模式的分类法，涵盖从全自动到被动辅助的合作模式。

Result: 提出了一个全面的框架，将合作模式与任务复杂性、操作风险等关键因素联系起来，为设计和选择适当的人类监督级别提供了系统化方法。

Conclusion: 该框架为从业者提供了在自动化与控制之间权衡的工具，有助于开发更安全、有效和情境感知的技术服务系统。

Abstract: Agentic AI systems, powered by Large Language Models (LLMs), offer
transformative potential for value co-creation in technical services. However,
persistent challenges like hallucinations and operational brittleness limit
their autonomous use, creating a critical need for robust frameworks to guide
human-AI collaboration. Drawing on established Human-AI teaming research and
analogies from fields like autonomous driving, this paper develops a structured
taxonomy of human-agent interaction. Based on case study research within
technical support platforms, we propose a six-mode taxonomy that organizes
collaboration across a spectrum of AI autonomy. This spectrum is anchored by
the Human-Out-of-the-Loop (HOOTL) model for full automation and the
Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the
framework specifies four distinct intermediate structures. These include the
Human-in-Command (HIC) model, where AI proposals re-quire mandatory human
approval, and the Human-in-the-Process (HITP) model for structured work-flows
with deterministic human tasks. The taxonomy further delineates the
Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation
upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables
discretionary human oversight of an autonomous AI. The primary contribution of
this work is a comprehensive framework that connects this taxonomy to key
contingency factors -- such as task complexity, operational risk, and system
reliability -- and their corresponding conceptual architectures. By providing a
systematic method for selecting and designing an appropriate level of human
oversight, our framework offers practitioners a crucial tool to navigate the
trade-offs between automation and control, thereby fostering the development of
safer, more effective, and context-aware technical service systems.

</details>


### [32] [The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?](https://arxiv.org/abs/2507.14084)
*Maria Tsfasman,Ramin Ghorbani,Catholijn M. Jonker,Bernd Dudzik*

Main category: cs.HC

TL;DR: 研究探讨了群体情绪与记忆性之间的关系，发现情绪标注与记忆性之间无显著关联。


<details>
  <summary>Details</summary>
Motivation: 智能系统需准确建模用户记忆性，传统认为情绪与记忆性相关，但第三方情绪标注可能不准确。

Method: 在动态非结构化群体对话中，连续标注情绪（愉悦-唤醒）和记忆性，模拟现实应用场景。

Result: 情绪与记忆性标注之间的关系无法显著区别于随机结果。

Conclusion: 情绪标注作为记忆性代理的假设不成立，为情感计算技术发展提供了新视角，并指明未来研究方向。

Abstract: Humans have a selective memory, remembering relevant episodes and forgetting
the less relevant information. Possessing awareness of event memorability for a
user could help intelligent systems in more accurate user modelling, especially
for such applications as meeting support systems, memory augmentation, and
meeting summarisation. Emotion recognition has been widely studied, since
emotions are thought to signal moments of high personal relevance to users. The
emotional experience of situations and their memorability have traditionally
been considered to be closely tied to one another: moments that are experienced
as highly emotional are considered to also be highly memorable. This
relationship suggests that emotional annotations could serve as proxies for
memorability. However, existing emotion recognition systems rely heavily on
third-party annotations, which may not accurately represent the first-person
experience of emotional relevance and memorability. This is why, in this study,
we empirically examine the relationship between perceived group emotions
(Pleasure-Arousal) and group memorability in the context of conversational
interactions. Our investigation involves continuous time-based annotations of
both emotions and memorability in dynamic, unstructured group settings,
approximating conditions of real-world conversational AI applications such as
online meeting support systems. Our results show that the observed relationship
between affect and memorability annotations cannot be reliably distinguished
from what might be expected under random chance. We discuss the implications of
this surprising finding for the development and applications of Affective
Computing technology. In addition, we contextualise our findings in broader
discourses in the Affective Computing and point out important targets for
future research efforts.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [33] [StructInbet: Integrating Explicit Structural Guidance into Inbetween Frame Generation](https://arxiv.org/abs/2507.13377)
*Zhenglin Pan,Haoran Xie*

Main category: cs.GR

TL;DR: StructInbet是一个基于显式结构指导的中间帧生成系统，旨在通过结构指导减少像素轨迹的模糊性，并采用时间注意力机制保持角色外观一致性。


<details>
  <summary>Details</summary>
Motivation: 解决中间帧生成中的像素轨迹模糊性问题，并提供可控的过渡效果。

Method: 提出显式结构指导以减少模糊性，并采用时间注意力机制整合前后关键帧的视觉特征。

Result: 系统能够生成可控且一致的中间帧过渡。

Conclusion: StructInbet通过结构指导和时间注意力机制有效提升了中间帧生成的质量和可控性。

Abstract: In this paper, we propose StructInbet, an inbetweening system designed to
generate controllable transitions over explicit structural guidance.
StructInbet introduces two key contributions. First, we propose explicit
structural guidance to the inbetweening problem to reduce the ambiguity
inherent in pixel trajectories. Second, we adopt a temporal attention mechanism
that incorporates visual identity from both the preceding and succeeding
keyframes, ensuring consistency in character appearance.

</details>


### [34] [DLSF: Dual-Layer Synergistic Fusion for High-Fidelity Image Syn-thesis](https://arxiv.org/abs/2507.13388)
*Zhen-Qi Chen,Yuan-Fu Yang*

Main category: cs.GR

TL;DR: 提出了一种新的双潜在集成框架，通过特征拼接和自适应融合模块增强基础潜在和精细潜在表示之间的特征交互，解决了Stable Diffusion模型中特征聚合不理想的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Stable Diffusion模型在特征聚合上表现不佳，导致语义对齐不完整和细粒度细节丢失，尤其在复杂纹理场景中。

Method: 采用双潜在集成框架，结合特征拼接和自适应融合模块（适应性全局融合AGF或动态空间融合DSF），以增强跨潜在通信。

Result: 新框架提升了全局一致性和局部纹理保真度，改善了图像的语义对齐和细节保留。

Conclusion: 提出的方法有效解决了Stable Diffusion模型的特征聚合问题，为高保真图像合成提供了更好的解决方案。

Abstract: With the rapid advancement of diffusion-based generative models, Stable
Diffusion (SD) has emerged as a state-of-the-art framework for high-fidelity
im-age synthesis. However, existing SD models suffer from suboptimal feature
aggregation, leading to in-complete semantic alignment and loss of fine-grained
details, especially in highly textured and complex scenes. To address these
limitations, we propose a novel dual-latent integration framework that
en-hances feature interactions between the base latent and refined latent
representations. Our approach em-ploys a feature concatenation strategy
followed by an adaptive fusion module, which can be instantiated as either (i)
an Adaptive Global Fusion (AGF) for hier-archical feature harmonization, or
(ii) a Dynamic Spatial Fusion (DSF) for spatially-aware refinement. This design
enables more effective cross-latent com-munication, preserving both global
coherence and local texture fidelity. Our GitHub page:
https://anonymous.4open.science/r/MVA2025-22 .

</details>


### [35] [Lab-Scale Gantry Crane Digital Twin Exemplar](https://arxiv.org/abs/2507.13419)
*Joost Mertens,Joachim Denil*

Main category: cs.GR

TL;DR: 论文展示了一个公开可用的实验室规模门式起重机及其数字孪生系统，旨在促进可重复的科学研究和教育。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术虽受广泛关注，但公开可用的实例稀缺，作者希望通过提供开放和可重复的示例推动相关研究和教育。

Method: 研究包含物理门式起重机及其控制器，以及数字侧的CAD模型、运动学模型、最优控制服务、历史数据记录、数据可视化和持续验证功能。

Result: 系统已在多篇先前发表的研究中得到功能验证，且完全基于免费开源软件，便于未来研究和教育使用。

Conclusion: 该数字孪生系统为数字孪生领域提供了一个公开、可重复的研究案例，适合用于教学和进一步研究。

Abstract: The research topic of digital twins has attracted a large amount of interest
over the past decade. However, publicly available exemplars remain scarce. In
the interest of open and reproducible science, in this exemplar paper we
present a lab-scale gantry crane and its digital twin. The exemplar comprises
both the physical and digital side of the twin system. The physical side
consists of the physical crane and its controller. The digital side covers the
CAD models and kinematic model of the crane, and provides services for optimal
control, historical data logging, data visualization and continuous validation.
We used this setup as use case in several previous publications where its
functionality was validated. It is publicly available and only relies on other
freely available and commonly used software, this way we hope it can be used
for future research or education on the topic of digital twins.

</details>


### [36] [TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting](https://arxiv.org/abs/2507.13586)
*Kaiyuan Tang,Kuangshi Ai,Jun Han,Chaoli Wang*

Main category: cs.GR

TL;DR: 该论文提出了一种基于纹理高斯泼溅的体可视化框架TexGS-VolVis，解决了传统方法灵活性不足和风格单一的问题，实现了高质量、几何一致性的风格化和实时渲染。


<details>
  <summary>Details</summary>
Motivation: 传统体可视化方法依赖于复杂的预定义规则且仅支持单一风格传输，限制了灵活性。作者希望通过结合可微分高斯基元和预训练大模型，实现任意风格传输和实时渲染。

Method: 提出TexGS-VolVis框架，使用2D高斯基元并扩展纹理和着色属性，结合图像和文本驱动的非真实感场景编辑技术，实现高质量风格化和细粒度控制。

Result: TexGS-VolVis在效率、视觉质量和编辑灵活性方面均优于现有方法，支持几何一致的高质量风格化和部分场景编辑。

Conclusion: TexGS-VolVis为体可视化提供了更灵活、高效的解决方案，拓展了风格化和场景编辑的能力。

Abstract: Advancements in volume visualization (VolVis) focus on extracting insights
from 3D volumetric data by generating visually compelling renderings that
reveal complex internal structures. Existing VolVis approaches have explored
non-photorealistic rendering techniques to enhance the clarity, expressiveness,
and informativeness of visual communication. While effective, these methods
often rely on complex predefined rules and are limited to transferring a single
style, restricting their flexibility. To overcome these limitations, we
advocate the representation of VolVis scenes using differentiable Gaussian
primitives combined with pretrained large models to enable arbitrary style
transfer and real-time rendering. However, conventional 3D Gaussian primitives
tightly couple geometry and appearance, leading to suboptimal stylization
results. To address this, we introduce TexGS-VolVis, a textured Gaussian
splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,
extending each Gaussian with additional texture and shading attributes,
resulting in higher-quality, geometry-consistent stylization and enhanced
lighting control during inference. Despite these improvements, achieving
flexible and controllable scene editing remains challenging. To further enhance
stylization, we develop image- and text-driven non-photorealistic scene editing
tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing
with fine-grained control. We evaluate TexGS-VolVis both qualitatively and
quantitatively across various volume rendering scenes, demonstrating its
superiority over existing methods in terms of efficiency, visual quality, and
editing flexibility.

</details>


### [37] [Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading](https://arxiv.org/abs/2507.13917)
*Efstratios Geronikolakis,Manos Kamarianakis,Antonis Protopsaltis,George Papagiannakis*

Main category: cs.GR

TL;DR: Neural-GASh是一种新型实时着色管线，利用神经辐射场架构和共形几何代数（CGA）编码的顶点信息，实现无需预计算的动态场景着色，适用于Unity引擎下的动画和变形3D网格。


<details>
  <summary>Details</summary>
Motivation: 传统Precomputed Radiance Transfer（PRT）方法需要昂贵的离线预计算，限制了动态场景的灵活性。Neural-GASh旨在通过学习模型直接处理CGA编码的顶点数据，实现实时动态着色。

Method: 采用神经辐射场架构，以CGA编码的顶点位置和法线为输入，实现实时着色；集成到Unity引擎中，支持动态场景灯光的球谐旋转优化。

Result: 在复杂几何下，Neural-GASh展示了与传统PRT竞争的渲染速度，并在3D高斯溅射生成的场景中验证了其灵活性和鲁棒性。

Conclusion: Neural-GASh为动态、交互式环境提供了高效、高质量的实时着色解决方案，适用于包括移动和VR在内的多种平台。

Abstract: This paper presents Neural-GASh, a novel real-time shading pipeline for 3D
meshes, that leverages a neural radiance field architecture to perform
image-based rendering (IBR) using Conformal Geometric Algebra (CGA)-encoded
vertex information as input. Unlike traditional Precomputed Radiance Transfer
(PRT) methods, that require expensive offline precomputations, our learned
model directly consumes CGA-based representations of vertex positions and
normals, enabling dynamic scene shading without precomputation. Integrated
seamlessly into the Unity engine, Neural-GASh facilitates accurate shading of
animated and deformed 3D meshes - capabilities essential for dynamic,
interactive environments. The shading of the scene is implemented within Unity,
where rotation of scene lights in terms of Spherical Harmonics is also
performed optimally using CGA. This neural field approach is designed to
deliver fast and efficient light transport simulation across diverse platforms,
including mobile and VR, while preserving high rendering quality. Additionally,
we evaluate our method on scenes generated via 3D Gaussian splats, further
demonstrating the flexibility and robustness of Neural-GASh in diverse
scenarios. Performance is evaluated in comparison to conventional PRT,
demonstrating competitive rendering speeds even with complex geometries.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [38] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: 该论文探讨了NVIDIA MIG技术在多任务调度中的潜力，提出了一种名为FAR的三阶段算法，通过动态重新配置GPU资源来最小化任务完成时间，展示了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 研究MIG技术在任务调度中的未开发潜力，解决多任务执行下的GPU资源分配问题。

Method: 提出了FAR算法，包含三个阶段：经典任务可塑性方法、结合LPT和列表调度的新启发式方法，以及局部搜索优化。

Result: 在不考虑重新配置成本的情况下，理论近似比为7/4（A30）和2（A100/H100）；实验结果显示实际完成时间接近最优解的1.22倍（基准测试）和1.10倍（合成输入）。

Conclusion: 论文证明了MIG技术的研究潜力，并提出了未来工作的有用指标和方法。

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [39] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: Checkmate系统通过利用数据并行训练中的梯度信息，实现无训练减速的每迭代检查点，显著提升检查点频率并减少失败时重复工作。


<details>
  <summary>Details</summary>
Motivation: 传统检查点方法需要在训练暂停时复制模型状态，存在检查点频率与失败成本的权衡，而Checkmate旨在避免这种权衡。

Method: 提出了一种新的多播抽象，将梯度同时传递给基于CPU的影子集群，影子集群通过应用梯度维护模型副本以实现检查点。

Result: Checkmate实现每迭代检查点，训练吞吐量与无检查点基线相当，检查点频率提升5至34.5倍，重复工作减少80%至97.1%。

Conclusion: Checkmate在相同检查点频率下，吞吐量比其他系统高1.3至6.5倍，显著提升了训练效率和容错能力。

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [40] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: DistFlow是一个新颖的分布式强化学习框架，解决了现有系统中负载不平衡导致的扩展瓶颈问题，实现了数千GPU的近线性扩展。


<details>
  <summary>Details</summary>
Motivation: 现有主流框架通常采用混合控制器架构，负载不平衡会导致显著瓶颈，限制了系统的扩展性。

Method: 引入多控制器范式，将数据传输和执行任务分配给所有工作节点，消除中心节点，每个工作节点独立运行。

Result: DistFlow实现了近线性扩展，比现有最优框架提升了7倍的端到端吞吐量。

Conclusion: DistFlow通过去中心化设计和资源解耦，显著提升了强化学习框架的扩展性和效率。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [41] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 论文综述了基于脉冲神经网络（SNN）的边缘智能（EdgeSNN），探讨其在资源受限设备上的潜力，包括学习、推理和安全问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习的资源需求和隐私问题引发了对边缘智能的需求，SNN因其低功耗和事件驱动特性成为有前景的替代方案。

Method: 系统分类EdgeSNN的基础（神经元模型、学习算法、硬件平台），深入讨论其在实际中的三个关键问题（轻量推理、资源感知训练、隐私保护）。

Result: 提出双轨基准测试策略以公平评估EdgeSNN，并总结了当前进展、挑战和未来方向。

Conclusion: EdgeSNN有望填补脑启发学习与边缘部署之间的鸿沟，为研究和实践提供重要参考。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [42] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: Shipwright是一个验证框架，用于证明分布式系统在存在恶意参与者时的正确性和活跃性，并成功应用于PBFT的部分实现。


<details>
  <summary>Details</summary>
Motivation: 确保去中心化系统（如PBFT）的活跃性至关重要，但传统方法难以验证活跃性。Shipwright旨在填补这一空白。

Method: Shipwright引入三种技术：形式化推理、模块化分解和密码签名验证，支持对恶意参与者的分布式系统进行验证。

Result: Shipwright成功实现并验证了PBFT的部分原型，并在Go中生成可执行代码，展示了其在常见和故障场景下的活跃性。

Conclusion: Shipwright为分布式系统的活跃性和正确性验证提供了新方法，并在PBFT中展示了其可行性。

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [43] [CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation](https://arxiv.org/abs/2507.13710)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: 论文提出了一种名为CogniQ-H的框架，利用分层强化学习（HRL）和贝叶斯推理改进数据准备过程，实现了比现有RL方法更高质量的自动化和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 数据准备是机器学习生命周期中的关键但复杂环节，现有强化学习方法效率低下且未能捕捉问题的结构化层次性。

Method: CogniQ-H结合了分层强化学习、LLM生成的高层策略先验、LTR模型的细粒度操作评分以及Q函数的长期价值估计，形成了一种软层次范式。

Result: 在18个多样化数据集上的实验表明，CogniQ-H在管道质量和收敛速度上分别提升了13.9%和2.8倍。

Conclusion: CogniQ-H通过软层次结构和多模型协同，显著提升了数据准备的自动化效果，为未来研究提供了新方向。

Abstract: Data preparation is a foundational yet notoriously challenging component of
the machine learning lifecycle, characterized by a vast combinatorial search
space of potential operator sequences. While reinforcement learning (RL) offers
a promising direction, existing approaches are inefficient as they fail to
capture the structured, hierarchical nature of the problem. We argue that
Hierarchical Reinforcement Learning (HRL), a paradigm that has been successful
in other domains, provides a conceptually ideal yet previously unexplored
framework for this task. However, a naive HRL implementation with a `hard
hierarchy' is prone to suboptimal, irreversible decisions. To address this, we
introduce CogniQ-H, the first framework to implement a soft hierarchical
paradigm for robust, end-to-end automated data preparation. CogniQ-H formulates
action selection as a Bayesian inference problem. A high-level strategic prior,
generated by a Large Language Model (LLM), guides exploration
probabilistically. This prior is synergistically combined with a fine-grained
operator quality score from a supervised Learning-to-Rank (LTR) model and a
long-term value estimate from the agent's own Q-function. This hybrid
architecture allows CogniQ-H to balance strategic guidance with adaptive,
evidence-based decision-making. Through extensive experiments on 18 diverse
datasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to
13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence
compared to state-of-the-art RL-based methods.

</details>


### [44] [LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction](https://arxiv.org/abs/2507.13712)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: LLaPipe利用大语言模型（LLMs）作为智能策略顾问，优化自动化数据准备的探索效率，通过语义理解和经验蒸馏提升预处理管道质量，同时控制计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（RL）的自动化数据准备方法在预处理管道探索中效率低下，导致探索瓶颈。

Method: LLaPipe通过LLM Policy Advisor、Experience Distillation和Adaptive Advisor Triggering策略，结合语义理解和历史经验优化探索。

Result: 在18个数据集上，LLaPipe比现有RL方法提升22.4%的管道质量，收敛速度快2.3倍，且计算效率高。

Conclusion: LLaPipe通过智能探索机制显著提升自动化数据准备的效率和效果。

Abstract: Automated data preparation is crucial for democratizing machine learning, yet
existing reinforcement learning (RL) based approaches suffer from inefficient
exploration in the vast space of possible preprocessing pipelines. We present
LLaPipe, a novel framework that addresses this exploration bottleneck by
integrating Large Language Models (LLMs) as intelligent policy advisors. Unlike
traditional methods that rely solely on statistical features and blind
trial-and-error, LLaPipe leverages the semantic understanding capabilities of
LLMs to provide contextually relevant exploration guidance. Our framework
introduces three key innovations: (1) an LLM Policy Advisor that analyzes
dataset semantics and pipeline history to suggest promising preprocessing
operations, (2) an Experience Distillation mechanism that mines successful
patterns from past pipelines and transfers this knowledge to guide future
exploration, and (3) an Adaptive Advisor Triggering strategy
(Advisor\textsuperscript{+}) that dynamically determines when LLM intervention
is most beneficial, balancing exploration effectiveness with computational
cost. Through extensive experiments on 18 diverse datasets spanning multiple
domains, we demonstrate that LLaPipe achieves up to 22.4\% improvement in
pipeline quality and 2.3$\times$ faster convergence compared to
state-of-the-art RL-based methods, while maintaining computational efficiency
through selective LLM usage (averaging only 19.0\% of total exploration steps).

</details>


### [45] [Efficient and Scalable Self-Healing Databases Using Meta-Learning and Dependency-Driven Recovery](https://arxiv.org/abs/2507.13757)
*Joydeep Chandra,Prabal Manhas*

Main category: cs.DB

TL;DR: 提出了一种结合元学习和强化学习的数据库自愈框架，旨在提升动态工作负载环境中的实时适应性和减少重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决动态数据库环境中实时适应性和最小化重新训练的挑战。

Method: 整合MAML和强化学习，利用多目标优化、GNN建模数据库组件关系，并通过合成任务增强和数据效率提升训练效果。

Result: 框架显著提升了适应性、效率和可靠性。

Conclusion: 该方法在数据库管理和自愈系统中具有重要进展。

Abstract: This study explored the development of a novel self-healing framework for
databases using meta-learning and reinforcement learning techniques. The
primary objective was to address the challenges of real-time adaptability and
minimal retraining in dynamic workload environments. The proposed approach
integrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning to
enable anomaly detection and corrective actions that adapted swiftly to
evolving database conditions. Multi-objective optimization was employed to
balance performance, resource utilization, and cost efficiency during the
healing process. Graph Neural Networks (GNNs) were incorporated to model
interdependencies within database components, ensuring holistic recovery
strategies. Data efficiency was enhanced through synthetic task augmentation
and self-supervised learning, enabling effective training in sparse data
regimes. To promote trust and transparency, explainable AI techniques were
integrated to provide interpretable insights into anomaly detection and healing
actions. Federated meta-learning further enabled privacy-preserving
adaptability in distributed database environments. The framework demonstrated
significant improvements in adaptability, efficiency, and reliability,
contributing to advancements in database management and self-healing systems.

</details>


### [46] [Towards Next Generation Data Engineering Pipelines](https://arxiv.org/abs/2507.13892)
*Kevin M. Kramer,Valerie Restat,Sebastian Strasser,Uta Störl,Meike Klettke*

Main category: cs.DB

TL;DR: 论文提出下一代数据工程管道的三个层次：优化管道、自感知管道和自适应管道，以解决现有管道在数据质量和反应性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的数据工程管道在数据质量和反应性方面存在问题，无法适应数据变化，导致崩溃或不良结果。

Method: 提出三个层次的改进：优化操作组合与参数、实现管道状态监控、以及自动适应数据变化。

Result: 展示了每个层次的实现方法，旨在提升管道的稳定性和数据质量。

Conclusion: 优化的、自感知和自适应的数据工程管道是解决现有挑战的未来方向。

Abstract: Data engineering pipelines are a widespread way to provide high-quality data
for all kinds of data science applications. However, numerous challenges still
remain in the composition and operation of such pipelines. Data engineering
pipelines do not always deliver high-quality data. By default, they are also
not reactive to changes. When new data is coming in which deviates from prior
data, the pipeline could crash or output undesired results. We therefore
envision three levels of next generation data engineering pipelines: optimized
data pipelines, self-aware data pipelines, and self-adapting data pipelines.
Pipeline optimization addresses the composition of operators and their
parametrization in order to achieve the highest possible data quality.
Self-aware data engineering pipelines enable a continuous monitoring of its
current state, notifying data engineers on significant changes. Self-adapting
data engineering pipelines are then even able to automatically react to those
changes. We propose approaches to achieve each of these levels.

</details>


### [47] [Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries](https://arxiv.org/abs/2507.14101)
*Diego Figueira,Cibele Freire*

Main category: cs.DB

TL;DR: 提出了一种名为 'project-connex' 树宽的新度量，用于评估半环聚合查询的易处理性，并通过统一的算法框架解释现有研究的可解结果。


<details>
  <summary>Details</summary>
Motivation: 为半环聚合查询的计数和枚举提供一种统一且直观的结构化度量，简化复杂性分析并解释现有研究的可解性。

Method: 定义 'project-connex' 树分解作为 'free-connex' 分解的扩展，并利用经典树分解算法计算。

Result: 证明了该度量能够统一解释多个领域的易处理结果，包括计数查询和聚合查询的高效评估。

Conclusion: 'project-connex' 树宽为半环聚合查询的复杂性分析提供了简洁且通用的框架，推动了相关领域的理论研究。

Abstract: We introduce 'project-connex' tree-width as a measure of tractability for
counting and aggregate conjunctive queries over semirings with 'group-by'
projection (also known as 'AJAR' or 'FAQ' queries). This elementary measure
allows to obtain comparable complexity bounds to the ones obtained by previous
structural conditions tailored for efficient evaluation of semiring aggregate
queries, enumeration algorithms of conjunctive queries, and tractability of
counting answers to conjunctive queries.
  Project-connex tree decompositions are defined as the natural extension of
the known notion of 'free-connex' decompositions. They allow for a unified,
simple and intuitive algorithmic manipulation for evaluation of aggregate
queries and explain some existing tractability results on conjunctive query
enumeration, counting conjunctive query evaluation, and evaluation of semiring
aggregate queries. Using this measure we also recover results relating
tractable classes of counting conjunctive queries and bounded free-connex
tree-width, or the constant-time delay enumeration of semiring aggregate
queries over bounded project-connex classes. We further show that
project-connex tree decompositions can be obtained via algorithms for computing
classical tree decompositions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [48] [PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning](https://arxiv.org/abs/2507.13355)
*Riadul Islam,Dhandeep Challagundla*

Main category: cs.AR

TL;DR: 论文提出了一种无需监督的DRC违规预测方法，通过单类不平衡数据集建模，显著提高了预测准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习和神经网络的模型需要大量平衡数据集和训练时间，这限制了其在实际应用中的效率。

Method: 提出了一种无监督学习方法，利用单类数据集建模，并通过设定阈值对新数据进行分类。

Result: 新方法在28纳米CMOS技术中实现了99.95%的预测准确率，训练时间比SVM和NN模型分别低26.3倍和6003倍。

Conclusion: 该方法为电子设计自动化提供了更高效、更灵活的解决方案，具有显著的性能优势。

Abstract: Leveraging artificial intelligence (AI)-driven electronic design and
automation (EDA) tools, high-performance computing, and parallelized algorithms
are essential for next-generation microprocessor innovation, ensuring continued
progress in computing, AI, and semiconductor technology. Machine learning-based
design rule checking (DRC) and lithography hotspot detection can improve
first-pass silicon success. However, conventional ML and neural network
(NN)-based models use supervised learning and require a large balanced dataset
(in terms of positive and negative classes) and training time. This research
addresses those key challenges by proposing the first-ever unsupervised DRC
violation prediction methodology. The proposed model can be built using any
unbalanced dataset using only one class and set a threshold for it, then
fitting any new data querying if they are within the boundary of the model for
classification. This research verified the proposed model by implementing
different computational cores using CMOS 28 nm technology and Synopsys Design
Compiler and IC Compiler II tools. Then, layouts were divided into virtual
grids to collect about 60k data for analysis and verification. The proposed
method has 99.95% prediction test accuracy, while the existing support vector
machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy,
respectively. In addition, the proposed methodology has about 26.3x and up to
6003x lower training times compared to SVM and NN-models, respectively.

</details>


### [49] [VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation](https://arxiv.org/abs/2507.13369)
*Paul E. Calzada,Zahin Ibnat,Tanvir Rahman,Kamal Kandula,Danyu Lu,Sujan Kumar Saha,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: 论文探讨了利用大型语言模型（LLM）进行硬件设计自动化的RTL代码生成，提出了构建高质量Verilog数据集的三步自动流程，生成了20,392个Verilog样本（751MB），是目前最大的用于LLM微调的高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为LLM在硬件设计自动化中的应用提供高质量的训练和微调数据集，填补当前文献空白。

Method: 方法包括三步：1) 使用PostgreSQL创建和管理数据库；2) 从OpenCores和GitHub收集数据；3) 数据预处理（语法验证、逻辑合成和元数据提取）。

Result: 生成了一个包含20,392个Verilog样本（751MB）的高质量数据集，是目前最大的同类数据集。

Conclusion: 结论是这一数据集为未来基于LLM的硬件生成研究提供了基础，并探讨了潜在应用和挑战。

Abstract: Large Language Models (LLMs) are gaining popularity for hardware design
automation, particularly through Register Transfer Level (RTL) code generation.
In this work, we examine the current literature on RTL generation using LLMs
and identify key requirements for training and fine-tuning datasets. We
construct a robust Verilog dataset through an automated three-pronged process
involving database (DB) creation and management with PostgreSQL, data
collection from code hosting sites like OpenCores and GitHub, and data
preprocessing to verify the codes' syntax, run logic synthesis, and extract
relevant module metadata. We implement a scalable and efficient DB
infrastructure to support analysis and detail our preprocessing pipeline to
enforce high-quality data before DB insertion. The resulting dataset comprises
20,392 Verilog samples, 751 MB of Verilog code data, which is the largest
high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further
evaluate the dataset, address associated challenges, and explore potential
applications for future research and development in LLM-based hardware
generation.

</details>


### [50] [GAP-LA: GPU-Accelerated Performance-Driven Layer Assignment](https://arxiv.org/abs/2507.13375)
*Chunyuan Zhao,Zizheng Guo,Zuodong Zhang,Yibo Lin*

Main category: cs.AR

TL;DR: 该论文提出了一种名为GAP-LA的GPU加速性能驱动层分配框架，用于同时优化时序、功耗和拥塞问题，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着设计复杂度的增加，同时优化时序、功耗和拥塞变得越来越困难，现有研究通常只能优化部分目标。

Method: 提出GPU加速的性能驱动层分配框架GAP-LA，实现全局优化。

Result: 实验显示，GAP-LA在WNS和TNS上分别提升0.3%-9.9%和2.0%-5.4%，同时保持功耗和拥塞竞争力。

Conclusion: GAP-LA是一种高效且全面的层分配解决方案，适用于大规模设计。

Abstract: Layer assignment is critical for global routing of VLSI circuits. It converts
2D routing paths into 3D routing solutions by determining the proper metal
layer for each routing segments to minimize congestion and via count. As
different layers have different unit resistance and capacitance, layer
assignment also has significant impacts to timing and power. With growing
design complexity, it becomes increasingly challenging to simultaneously
optimize timing, power, and congestion efficiently. Existing studies are mostly
limited to a subset of objectives. In this paper, we propose a GPU-accelerated
performance-driven layer assignment framework, GAP-LA, for holistic
optimization the aforementioned objectives. Experimental results demonstrate
that we can achieve 0.3%-9.9% better worst negative slack (WNS) and 2.0%-5.4%
better total negative slack (TNS) while maintaining power and congestion with
competitive runtime compared with ISPD 2025 contest winners, especially on
designs with up to 12 millions of nets.

</details>


### [51] [4T2R X-ReRAM CiM Array for Variation-tolerant, Low-power, Massively Parallel MAC Operation](https://arxiv.org/abs/2507.13631)
*Fuyuki Kihara,Seiji Uenohara,Satoshi Awamura,Naoko Misawa,Chihiro Matsui,Ken Takeuchi*

Main category: cs.AR

TL;DR: 提出了一种适用于计算内存 (CiM) 的 4T2R ReRAM 单元和 8T SRAM CiM，旨在解决随着行并行性增加而导致的功耗和设备衍生错误问题。


<details>
  <summary>Details</summary>
Motivation: 计算内存 (CiM) 技术在实现 AI 加速器所需的高速低功耗 MAC 计算方面具有潜力，但随着行并行性增加，功耗和设备衍生错误问题突出。

Method: 提出并采用 4T2R ReRAM 单元和 8T SRAM CiM 作为解决方案。

Result: 4T2R ReRAM 单元相比传统 4T4R ReRAM 单元减少了因 ReRAM 设备变异引起的错误。

Conclusion: 所提出的 4T2R ReRAM 单元和 8T SRAM CiM 在减少设备错误和优化功耗方面具有优势。

Abstract: Computation-in-Memory (CiM) is attracting attention as a technology that can
perform MAC calculations required for AI accelerators, at high speed with low
power consumption. However, there is a problem regarding power consumption and
device-derived errors that increase as row parallelism increases. In this
paper, a 4T2R ReRAM cell and an 8T SRAM CiM suitable for CiM is proposed. It is
shown that adopting the proposed 4T2R ReRAM cell reduces the errors due to
variation in ReRAM devices compared to conventional 4T4R ReRAM cells.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 提出了一种基于SpiNNaker2芯片的端到端DNN调度框架，支持从PyTorch模型到推理的全流程。


<details>
  <summary>Details</summary>
Motivation: 为了在神经拟态平台SpiNNaker2上实现复杂DNN的边缘计算。

Method: 扩展OctopuScheduler，结合量化和降阶步骤的前端。

Result: 能够在SpiNNaker2芯片上执行复杂DNN，包括Transformer级模型。

Conclusion: 该框架为神经拟态平台上的边缘计算提供了高效解决方案。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [53] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin是一种基于轻量级服务器端数字孪生的客户端跳过算法，通过预测客户端的梯度更新，减少通信开销。在非独立同分布数据下，实验表明其能减少12-15.5%的通信量，同时提高模型精度0.5个百分点。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信开销是主要瓶颈，尤其在移动和物联网设备带宽受限的情况下，需要高效且资源节约的解决方案。

Method: 使用服务器端LSTM数字孪生预测客户端梯度更新的幅度和不确定性，仅在预测值超过阈值时请求通信，否则跳过该轮。

Result: 在UCI-HAR和MNIST数据集上，与非独立同分布数据的10个客户端实验中，FedSkipTwin减少了12-15.5%的通信量，并提高了0.5百分点的模型精度。

Conclusion: 预测引导的跳过策略是带宽受限边缘环境中联邦学习的实用且高效的方法。

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [54] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
*Lizhi Ma,Tong Zhao,Shuai Zhang,Nirui Song,Hongliang He,Anqi Li,Ran Feng,Huachuan Qiu,Jingsong Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 研究探讨了中文心理咨询中语言表达与抑郁和焦虑状态的关系，发现负面情感词与心理状态严重程度显著正相关，但第一人称单数代词的使用频率与心理状态无关，可能与中西方文化差异和心理咨询互动特点有关。


<details>
  <summary>Details</summary>
Motivation: 旨在探究中文心理咨询中语言表达（如负面情感词和第一人称代词）与心理状态的关系，并与西方研究结果进行对比，揭示文化差异的影响。

Method: 基于735次在线心理咨询会话语料库，使用LIWC软件量化语言模式，采用广义线性混合效应模型分析数据。

Result: 负面情感词的使用频率与抑郁和焦虑的严重程度显著正相关，但第一人称单数代词的使用频率与心理状态无显著关联。

Conclusion: 研究发现文化背景和心理咨询互动特点对语言使用有重要影响，对中文心理健康交流的实践和标记具有启示意义。

Abstract: This study explores the relationship between linguistic expressions and
psychological states of depression and anxiety within Chinese psycho-counseling
interactions, focusing specifically on the usage of first-person singular
pronouns and negative emotional words. Utilizing a corpus derived from 735
online counseling sessions, the analysis employed a general linear mixed-effect
model to assess linguistic patterns quantified by the Linguistic Inquiry and
Word Count (LIWC) software. Results indicate a significant positive correlation
between the frequency of negative emotional words and the severity of both
depressive and anxious states among clients. However, contrary to prior
findings predominantly derived from English-language contexts, the usage
frequency of first-person singular pronouns did not vary significantly with the
clients' psychological conditions. These outcomes are discussed within the
framework of cultural distinctions between collectivist Chinese contexts and
individualistic Western settings, as well as the interactive dynamics unique to
psycho-counseling conversations. The findings highlight the nuanced influence
of cultural and conversational contexts on language use in mental health
communications, providing insights into psycholinguistic markers relevant to
therapeutic practices in Chinese-speaking populations.

</details>


### [55] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: 研究发现，当前和未来AI的说服力主要来自后训练和提示方法，而非个性化或模型规模扩大，且这些方法虽然提高了说服力，但降低了事实准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨对话AI对人类信念的潜在影响，评估其在政治议题上的说服力和事实准确性。

Method: 通过三个大型实验（N=76,977），测试19个LLM在707个政治议题上的说服力，并分析466,769条生成的LLM声明的准确性。

Result: 后训练和提示方法能显著提升AI说服力（分别增加51%和27%），但同时降低了事实准确性。

Conclusion: AI的说服力更多源于其信息处理能力，而非规模或个性化，但需警惕其在提升说服力时可能牺牲事实准确性。

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [56] [SAQR-QC: A Logic for Scalable but Approximate Quantitative Reasoning about Quantum Circuits](https://arxiv.org/abs/2507.13635)
*Nengkun Yu,Jens Palsberg,Thomas Reps*

Main category: quant-ph

TL;DR: SAQR-QC是一种用于量子电路的可扩展但近似定量推理逻辑，旨在解决现有验证技术在时间和空间上效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的量子程序验证技术效率低下，尤其对于多量子比特电路，需要指数级的时间和空间，因此提出一种新的推理逻辑SAQR-QC。

Method: SAQR-QC通过故意引入精度损失、控制累积误差和局部推理（每次涉及少量量子比特）来实现可扩展性。

Result: 通过两个案例研究（GHZ电路和非Clifford门的验证，以及量子相位估计的分析）证明了SAQR-QC的有效性。

Conclusion: SAQR-QC为大规模量子电路的验证提供了一种实用且可扩展的方法。

Abstract: Reasoning about quantum programs remains a fundamental challenge, regardless
of the programming model or computational paradigm. Despite extensive research,
existing verification techniques are insufficient--even for quantum circuits, a
deliberately restricted model that lacks classical control, but still underpins
many current quantum algorithms. Many existing formal methods require
exponential time and space to represent and manipulate (representations of)
assertions and judgments, making them impractical for quantum circuits with
many qubits. This paper presents a logic for reasoning in such settings, called
SAQR-QC. The logic supports Scalable but Approximate Quantitative Reasoning
about Quantum Circuits, whence the name. SAQR-QC has three characteristics: (i)
some (deliberate) loss of precision is built into it; (ii) it has a mechanism
to help the accumulated loss of precision during a sequence of reasoning steps
remain small; and (iii) most importantly, to make reasoning scalable, all
reasoning steps are local--i.e., they each involve just a small number of
qubits. We demonstrate the effectiveness of SAQR-QC via two case studies: the
verification of GHZ circuits involving non-Clifford gates, and the analysis of
quantum phase estimation--a core subroutine in Shor's factoring algorithm.

</details>


### [57] [Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification](https://arxiv.org/abs/2507.14116)
*Daniëlle Schuman,Mark V. Seebode,Tobias Rohe,Maximilian Balthasar Mansky,Michael Schroedl-Baumann,Jonas Stein,Claudia Linnhoff-Popien,Florian Krellner*

Main category: quant-ph

TL;DR: 论文提出了一种改进的并行量子退火方法，用于在监督学习中训练量子玻尔兹曼机（QBMs），并在MedMNIST医学图像数据集上测试，达到了与类似规模卷积神经网络（CNNs）相当的效果，且训练速度提升了近70%。


<details>
  <summary>Details</summary>
Motivation: 量子玻尔兹曼机（QBMs）因其潜在的量子加速优势而受到关注，但当前训练成本高，限制了其在NISQ时代的应用。论文旨在通过改进的并行量子退火方法降低训练成本。

Method: 采用改进的并行量子退火技术，在监督学习场景中训练QBMs，并通过MedMNIST数据集验证其性能。

Result: 实验表明，改进方法在更少的训练轮次下实现了与类似规模CNNs相当的结果，且速度提升了近70%。

Conclusion: 论文的并行退火技术显著提升了QBMs的训练效率，为其在实际应用中的推广提供了可能。

Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently
follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann
Machines (QBMs) have gained increasing popularity in the quantum research
community. While they harbor great promises for quantum speed-up, their usage
currently stays a costly endeavor, as large amounts of QPU time are required to
train them. This limits their applicability in the NISQ era. Following the idea
of No\`e et al. (2024), who tried to alleviate this cost by incorporating
parallel quantum annealing into their unsupervised training of QBMs, this paper
presents an improved version of parallel quantum annealing that we employ to
train QBMs in a supervised setting. Saving qubits to encode the inputs, the
latter setting allows us to test our approach on medical images from the
MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world
applicability of the technology. Our experiments show that QBMs using our
approach already achieve reasonable results, comparable to those of
similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller
numbers of epochs than these classical models. Our parallel annealing technique
leads to a speed-up of almost 70 % compared to regular annealing-based BM
executions.

</details>


### [58] [The Proportional Fair Scheduler in Wavelength-Multiplexed Quantum Networks](https://arxiv.org/abs/2507.13999)
*Sanidhay Bhambay,Siddarth Koduru Joshi,Thirupathaiah Vasantam,Neil Walton*

Main category: quant-ph

TL;DR: 该论文研究了量子网络中优化泵浦策略的问题，提出了分布式算法和比例公平泵浦策略（PF-PS），以平衡公平性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 量子网络通过分发纠缠光子对实现安全通信，但实际应用中存在资源竞争和网络性能优化的挑战。

Method: 提出了自适应泵浦策略，特别是比例公平泵浦策略（PF-PS），动态优先分配低平均密钥率的用户。

Result: 理论分析和数值模拟显示PF-PS在纠缠态分发中表现最优，解决了量子网络的资源分配问题。

Conclusion: 比例公平泵浦策略是量子网络中高效资源分配的强有力候选方案。

Abstract: We address the problem of optimal pumping strategies in quantum networks.
These networks enable secure communication by distributing entangled photon
pairs to user (or node) pairs. Quantum Key Distribution (QKD) protocols, like
BBM92, generate secret keys from entangled photons. While secure communication
and error correction are essential for any quantum communication channel,
resource contention, optimization, and fairness issues are critical for
networks. In this article, we analyze the performance of quantum networks,
proposing simple distributed algorithms for QKD networks generating secret
keys.
  There are significant advantages of pumping entangled photons in QKD
networks, but challenges arise in practical implementations. The underlying
channels are inherently time-varying, and thus data rates fluctuate between
nodes. Moreover, multiple edges (node pairs) can be pumped simultaneously,
albeit at the cost of a reduced secret key rate (SKR). These temporal and
spatial constraints yield a complex decision-making problem whose solutions may
favor a small set of user pairs to the detriment of overall, long-run network
performance.
  We design adaptive pumping strategies that address these challenges in QKD
networks. In particular, we find that a proportional fairness pumping strategy
(PF-PS) stands out by dynamically prioritizing users with lower average secret
key rates and optimally balancing fairness with throughput. The proposed
algorithm is a natural extension to quantum networks of the Proportional Fair
Scheduler deployed in 4G LTE and 5G mobile networks. Both theoretical analysis
and numerical simulations confirm that PF-PS is optimal for entangled state
distribution, and thus, when adapted appropriately, proportional fair pumping
is a strong candidate for efficient resource allocation in quantum networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security](https://arxiv.org/abs/2507.13367)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CR

TL;DR: 该研究提出了一种结合自适应像素值差分（APVD）和伪随机像素选择的新型隐写方法，有效解决了传统方法的“未使用块”问题，提升了安全性、嵌入容量和图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统APVD方法存在“未使用块”问题，导致安全性下降、嵌入容量减少和视觉质量降低，需要一种改进方法来解决这些问题。

Method: 研究提出了一种新策略，将APVD与伪随机像素选择相结合，以提高性能并解决未使用块问题。

Result: 新方法在安全性、数据隐藏容量和图像质量方面优于现有技术，显著提升了PSNR、UIQ和SSIM等关键指标。

Conclusion: 新型隐写方法具有多功能性，适用于多种载体和秘密图像格式，实现了安全传输且不影响图像美观性。

Abstract: Steganography is the process of embedding secret information discreetly
within a carrier, ensuring secure exchange of confidential data. The Adaptive
Pixel Value Differencing (APVD) steganography method, while effective,
encounters certain challenges like the "unused blocks" issue. This problem can
cause a decrease in security, compromise the embedding capacity, and lead to
lower visual quality. This research presents a novel steganographic strategy
that integrates APVD with pseudorandom pixel selection to effectively mitigate
these issues. The results indicate that the new method outperforms existing
techniques in aspects of security, data hiding capacity, and the preservation
of image quality. Empirical results reveal that the combination of APVD with
pseudorandom pixel selection significantly enhances key image quality metrics
such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),
and Structural Similarity Index (SSIM), surpassing other contemporary methods
in performance. The newly proposed method is versatile, able to handle a
variety of cover and secret images in both color and grayscale, thereby
ensuring secure data transmission without compromising the aesthetic quality of
the image.

</details>


### [60] [Quantum Blockchain Survey: Foundations, Trends, and Gaps](https://arxiv.org/abs/2507.13720)
*Saurav Ghosh*

Main category: cs.CR

TL;DR: 量子计算对传统区块链系统构成威胁，促使其发展为后量子区块链和量子区块链两大方向。本文综述了这两类技术的密码学基础、设计架构及实现挑战，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁传统区块链安全性，催生了对量子安全区块链的研究需求。

Method: 综述后量子区块链和量子区块链的技术发展，分析其密码学基础和架构设计。

Result: 比较了不同方案在安全性、可扩展性和部署上的权衡，明确了硬件、共识和网络设计中的开放问题。

Conclusion: 本文为量子时代安全区块链的发展提供了结构化、全面的参考。

Abstract: Quantum computing poses fundamental risks to classical blockchain systems by
undermining widely used cryptographic primitives. In response, two major
research directions have emerged: post-quantum blockchains, which integrate
quantum-resistant algorithms, and quantum blockchains, which leverage quantum
properties such as entanglement and quantum key distribution. This survey
reviews key developments in both areas, analyzing their cryptographic
foundations, architectural designs, and implementation challenges. This work
provides a comparative overview of technical proposals, highlight trade-offs in
security, scalability, and deployment, and identify open research problems
across hardware, consensus, and network design. The goal is to offer a
structured and comprehensive reference for advancing secure blockchain systems
in the quantum era.

</details>


### [61] [The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems](https://arxiv.org/abs/2507.14007)
*Serhan W. Bahar*

Main category: cs.CR

TL;DR: 该论文介绍了CryptoNeo威胁建模框架（CNTMF），旨在应对区块链和加密货币生态系统中的风险，如预言机操纵和跨链攻击，扩展了现有威胁建模方法。


<details>
  <summary>Details</summary>
Motivation: 随着区块链、加密货币和Web3技术快速融入数字银行和金融科技操作，传统金融系统与去中心化元素的融合带来了新的风险。

Method: CNTMF结合了STRIDE、OWASP Top 10等方法，并引入了混合层分析、CRYPTOQ助记符和AI增强反馈循环等定制组件。

Result: 基于2025年真实事件数据，CNTMF成功减少了因安全事件造成的损失（上半年约24.7亿美元）。

Conclusion: CNTMF通过资产映射、风险分析和迭代反馈，为应对不断演变的威胁（如国家支持攻击）提供了有效支持。

Abstract: The rapid integration of blockchain, cryptocurrency, and Web3 technologies
into digital banks and fintech operations has created an integrated environment
blending traditional financial systems with decentralised elements. This paper
introduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed
framework designed to address the risks in these ecosystems, such as oracle
manipulation and cross-chain exploits. CNTMF represents a proposed extension of
established methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN,
and PASTA, while incorporating tailored components including Hybrid Layer
Analysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an
AI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents,
CNTMF supports data-driven mitigation to reduce losses, which totalled
approximately $2.47 billion in the first half of 2025 across 344 security
events (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its
phases guide asset mapping, risk profiling, prioritisation, mitigation, and
iterative feedback. This supports security against evolving risks like
state-sponsored attacks.

</details>


### [62] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: PHASE是一个机器学习框架，用于通过Zeek连接日志评估网络活动中的人类行为真实性，准确率超过90%，并提出改进合成用户行为的配置。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏量化评估合成用户行为真实性的方法，影响网络安全仿真环境的有效性。

Method: PHASE通过分析Zeek连接日志，利用机器学习和DNS记录分类网络流量，结合SHAP分析揭示人类行为特征。

Result: PHASE能区分人类和非人类活动，改进合成用户行为配置，显著提升行为真实性。

Conclusion: PHASE提供了一种被动有效的评估方法，可提升网络安全仿真环境中合成用户行为的真实性。

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [63] [Nonlinear Distortion Equalization in Multi-Span Optical Links Via a Feed-Forward Photonic Neural Network](https://arxiv.org/abs/2507.13775)
*Emiliano Staffoli,Elisabetta Ferri,Stefano Gretter,Lorenzo Pavesi*

Main category: physics.optics

TL;DR: 使用集成的光子神经网络（PNN）均衡光通信信号中的线性和非线性失真，PNN由线性阶段（8抽头FIR滤波器）和非线性阶段（平方模运算）组成。实验验证了其在10 Gbps信号中的性能，并探索了100 Gbps调制下的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决光通信信号中由多段传播引起的线性和非线性失真问题，通过全光处理实现低延迟和低功耗的信号均衡。

Method: 采用基于8抽头FIR滤波器的线性阶段和平方模运算的非线性阶段构建PNN，并应用于IMDD系统中的PAM2信号。实验使用硅基器件验证性能，仿真探索更高调制速率的应用。

Result: 实验显示PNN可实现200公里色散均衡和450公里自相位调制（无色散）均衡，仿真表明其适应100 Gbps调制和交叉相位调制均衡的潜力。

Conclusion: PNN在光通信信号均衡中表现出色，具备低功耗、低延迟和适应高调制速率的潜力。

Abstract: Linear and nonlinear distortions in optical communication signals are
equalized using an integrated feed-forward Photonic Neural Network (PNN). The
PNN is based on a linear stage made of an 8-tap Finite Impulse Response (FIR)
filter, featuring tunable amplitude and phase weights at each tap, and of a
nonlinear stage achieved through the square modulus operation at the
end-of-line photodetector. Within an Intensity Modulation/Direct Detection
(IMDD) system, the PNN is applied to 2-level Pulse Amplitude Modulated (PAM2)
optical signals undergoing multi-span propagation. Each 50 km segment includes
fiber transmission, optical power restoration, and optional chromatic
dispersion compensation via a Tunable Dispersion Compensator. Positioned at the
receiver, the PNN enables fully optical signal processing with minimal latency
and power consumption. Experimental validation is conducted using a
Silicon-On-Insulator device operating on 10 Gbps signals. It demonstrates
chromatic dispersion equalization over distances up to 200 km and self-phase
modulation (with dispersion removed) up to 450 km. Simulations explore PNN
adaptation for 100 Gbps modulations and its potential for cross-phase
modulation equalization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse是一种统一框架，用于处理V2X协同感知中异构传感器配置的挑战，通过分层融合机制和自适应空间分辨率调整模块实现高效特征融合，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中V2X系统的传感器配置通常异构，导致特征融合和感知可靠性问题，需一种通用解决方案。

Method: 提出HeCoFuse框架，结合通道和空间注意力机制分层融合特征，并通过自适应空间分辨率调整和学习策略优化性能。

Result: 在TUMTraf-V2X数据集上，HeCoFuse在多种异构配置下表现优异，3D mAP达43.38%，并获CVPR 2025 DriveX挑战赛第一名。

Conclusion: HeCoFuse在异构传感器配置下展现出鲁棒性和高效性，成为当前V2X协同感知的最先进方法。

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [65] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF是一种可泛化的神经渲染方法，能够在任意视角和时间下渲染新视图，即使输入视图很少。它结合多视角立体、神经辐射场和解耦策略，无需逐场景优化即可生成逼真的时间过渡效果。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，多视图采集成本高且效率低，而数字领域（如元宇宙）需要支持昼夜自然过渡的3D环境建模，现有NeRF方法在时间建模方面仍有局限。

Method: TimeNeRF融合多视角立体、神经辐射场和解耦技术，构建隐式内容辐射场，支持任意时间点的辐射场生成，并通过体积渲染合成新视图。

Result: 实验表明，TimeNeRF在少样本设置下无需逐场景优化即可渲染新视图，并能逼真捕捉从黎明到黄昏的自然场景变化。

Conclusion: TimeNeRF为时间敏感的3D场景建模提供了高效且灵活的解决方案，尤其适用于需要动态时间过渡的应用场景。

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

</details>


### [66] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: QuantEIT是一种基于量子电路的轻量级EIT图像重建框架，通过结合并行2量子比特电路生成隐式非线性先验，显著降低了模型复杂度，无监督且无需训练数据，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: EIT技术虽然低成本、非侵入且高时间分辨率，但其逆问题病态性导致图像重建困难。现有深度学习方法依赖复杂架构且参数庞大，限制了效率和可扩展性，因此需要更高效轻量的解决方案。

Method: 提出QuantEIT框架，利用量子辅助网络（QA-Net）的并行2量子比特电路生成隐式非线性先验表示，结合单一线性层重建电导率，模型复杂度大幅降低，且无需监督或训练数据。

Result: 在2D和3D肺部EIT成像实验中，QuantEIT仅用0.2%参数即达到或超越传统方法的精度，且抗噪能力更强。

Conclusion: QuantEIT首次将量子电路集成到EIT重建中，显著简化模型的同时提高性能，为轻量级和高鲁棒性EIT成像提供了新方向。

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [67] [Faster Multi-Source Reachability and Approximate Distances via Shortcuts, Hopsets and Matrix Multiplication](https://arxiv.org/abs/2507.13470)
*Michael Elkin,Chhaya Trehan*

Main category: cs.DS

TL;DR: 论文提出了一种新的集中式算法和改进的并行算法，用于解决有向图中的S×V可达性问题，显著提升了现有算法的运行时间和复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的S×V可达性算法在时间和复杂度上存在局限性，尤其是在处理大规模图时。因此，需要开发更高效的算法来优化性能。

Method: 利用Kogan和Parter的快捷构造技术，提出了一种集中式算法，其时间复杂度为O(n^(1+(2/3)ω(σ)))，并扩展了Cohen的并行算法，适用于具有小递归分隔器的图。

Result: 新算法的时间复杂度优于现有算法的最佳界限，尤其在σ的特定范围内，且并行算法在广泛参数范围内降低了工作复杂度。

Conclusion: 论文通过引入新技术和优化现有方法，显著提升了S×V可达性问题的计算效率，适用于多种图结构和近似距离计算。

Abstract: Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a subset $S \subseteq V$
of $|S| = n^{\sigma}$ (for some $0 \le \sigma \le 1$) designated sources, the
$S \times V$ reachability problem is to compute the sets $\mathcal V_s$ of
vertices reachable from $s$, for every $s \in S$. Naive centralized algorithms
run BFS/DFS from each source in $O(m \cdot n^{\sigma})$ time or compute $G$'s
transitive closure in $\hat O(n^{\omega})$ time, where $\omega \le
2.371552\ldots$ is the matrix multiplication exponent. Thus, the best known
bound is $\hat O(n^{\min \{ 2 + \sigma, \omega\}})$. Leveraging shortcut
constructions by Kogan and Parter [SODA 2022, ICALP 2022], we develop a
centralized algorithm with running time $\hat O(n^{1 + \frac{2}{3}
\omega(\sigma)})$, where $\omega(\sigma)$ is the rectangular matrix
multiplication exponent. Using current estimates on $\omega(\sigma)$, our
exponent improves upon $\min \{2 + \sigma, \omega \}$ for $\tilde \sigma \leq
\sigma \leq 0.53$, where $1/3 < \tilde \sigma < 0.3336$ is a universal
constant.
  In a classical result, Cohen [Journal of Algorithms, 1996] devised parallel
algorithms for $S \times V$ reachability on graphs admitting balanced recursive
separators of size $n^{\rho}$ for $\rho < 1$, requiring polylogarithmic time
and work $n^{\max \{\omega \rho, 2\rho + \sigma \} + o(1)}$. We significantly
improve, extend, and generalize Cohen's result. First, our parallel algorithm
for graphs with small recursive separators has lower work complexity than
Cohen's in boraod paramater ranges. Second, we generalize our algorithm to
graphs of treewidth at most $n^{\rho}$ ($\rho < 1$) and provide a centralized
algorithm that outperforms existing bounds for $S \times V$ reachability on
such graphs. We also do this for some other graph familes with small
separators. Finally, we extend these results to $(1 + \epsilon)$-approximate
distance computation.

</details>


### [68] [Weighted Matching in a Poly-Streaming Model](https://arxiv.org/abs/2507.14114)
*Ahammed Ullah,S. M. Ferdous,Alex Pothen*

Main category: cs.DS

TL;DR: 提出了一种称为“多流模型”的并行流计算模型，设计了一种单遍算法用于近似求解最大权重匹配问题，并在共享内存并行环境中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的流计算模型在处理大规模数据时效率有限，因此需要一种更高效的并行流计算模型，以支持多处理器协作处理数据流。

Method: 通过引入多流模型（poly-streaming model），允许多个处理器并行处理多个数据流。设计了一种单遍算法，用于近似求解最大权重匹配（MWM）问题，并分析了其在共享内存架构中的性能。

Result: 算法在共享内存系统中表现出色，能够显著减少运行时间和内存占用。例如，在包含数万亿边的图上，算法能够将运行时间减少近两个数量级，内存占用减少五个数量级。

Conclusion: 多流模型及其设计的算法为大规模数据处理提供了一种高效的解决方案，尤其是在近似匹配问题中表现出优越的性能和可扩展性。

Abstract: We introduce the poly-streaming model, a generalization of streaming models
of computation in which $k$ processors process $k$ data streams containing a
total of $N$ items. The algorithm is allowed $O\left(f(k)\cdot M_1\right)$
space, where $M_1$ is either $o\left(N\right)$ or the space bound for a
sequential streaming algorithm. Processors may communicate as needed.
Algorithms are assessed by the number of passes, per-item processing time,
total runtime, space usage, communication cost, and solution quality.
  We design a single-pass algorithm in this model for approximating the maximum
weight matching (MWM) problem. Given $k$ edge streams and a parameter
$\varepsilon > 0$, the algorithm computes a
$\left(2+\epsilon\right)$-approximate MWM. We analyze its performance in a
shared-memory parallel setting: for any constant $\varepsilon > 0$, it runs in
time $\widetilde{O}\left(L_{\max}+n\right)$, where $n$ is the number of
vertices and $L_{\max}$ is the maximum stream length. It supports
$O\left(1\right)$ per-edge processing time using $\widetilde{O}\left(k\cdot
n\right)$ space. We further generalize the design to hierarchical
architectures, in which $k$ processors are partitioned into $r$ groups, each
with its own shared local memory. The total intergroup communication is
$\widetilde{O}\left(r \cdot n\right)$ bits, while all other performance
guarantees are preserved.
  We evaluate the algorithm on a shared-memory system using graphs with
trillions of edges. It achieves substantial speedups as $k$ increases and
produces matchings with weights significantly exceeding the theoretical
guarantee. On our largest test graph, it reduces runtime by nearly two orders
of magnitude and memory usage by five orders of magnitude compared to an
offline algorithm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM是一种基于轻量级LLM框架的活动日志生成和摘要系统，通过整合位置、运动、环境和生理四个维度的上下文信息，显著提高了日志生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富性上存在不足，亟需改进。

Method: DailyLLM采用轻量级LLM框架，结合结构化提示和高效特征提取，实现高级活动理解。

Result: DailyLLM在BERTScore精确度上比70B参数的SOTA基线提升17%，且推理速度提高近10倍。

Conclusion: DailyLLM通过高效整合多维度信息，显著提升了活动日志生成的性能，适用于智能手机和智能手表等常见设备。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [70] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 提出了一种名为ADPC的新型视觉语言因果干预框架，用于阿尔茨海默病的诊断辅助，结合多模态数据和因果推理，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 早期识别和干预轻度认知障碍（MCI）可以延缓阿尔茨海默病（AD）的进展，但诊断中存在由多模态数据选择和变量间复杂关系引起的混杂因素。

Method: ADPC框架利用大语言模型（LLM）总结临床数据，结合MRI和fMRI图像，通过因果干预消除混杂因素。

Result: 实验结果显示该方法在区分CN/MCI/AD病例上表现优异，达到了当前最优性能。

Conclusion: 研究表明，将因果推理与多模态学习结合在神经系统疾病诊断中具有巨大潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [71] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 论文提出了一种新颖的时间和约束扩展方法，用于逻辑编程（ASP）中的非单调时间推理，解决动态系统的高分辨率建模问题。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑编程方法在处理高分辨率的动态系统时面临挑战，需要支持时间和数值约束的非单调推理能力。

Method: 通过结合线性时间逻辑（Here-and-There）与约束逻辑，开发了一种新的逻辑框架，支持时间和数值约束的非单调推理。

Result: 成功构建了一个适用于ASP的表达能力强的系统，为复杂动态系统的高分辨率建模提供了理论基础。

Conclusion: 该研究为ASP范式下处理高分辨率动态系统提供了首个专门的非单调时间推理方法，奠定了重要的逻辑框架基础。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [72] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升了CUDA内核的性能，并在多种GPU架构上表现出良好的可移植性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速发展，对GPU计算资源的需求激增，急需自动化CUDA优化策略。现有的代码生成模型在CUDA优化方面成功率低，因此需要更高效的解决方案。

Method: 提出了CUDA-L1框架，通过强化学习训练，使用速度提升作为奖励信号，无需人工干预即可优化CUDA内核。

Result: 在NVIDIA A100上，CUDA-L1平均提速17.7倍，峰值达449倍，同时在多种GPU架构上表现良好。还发现了多种优化技术并揭示了CUDA优化的基本原理。

Conclusion: CUDA-L1证明了强化学习可以有效地将性能较差的LLM转化为CUDA优化工具，为自动化CUDA优化和提升GPU效率提供了新途径。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [73] [Intuitionistic monotone modal logic via translation](https://arxiv.org/abs/2507.13746)
*Jim de Groot*

Main category: math.LO

TL;DR: 该论文介绍了一种单调模态逻辑，类似于直觉主义模态逻辑IK，并通过翻译成合适的一阶逻辑进行构建。作者为该逻辑提供了公理化和直觉主义邻域模型语义，并与其他直觉主义单调模态逻辑进行了比较，展示了如何将其嵌入多模态版本的IK中。


<details>
  <summary>Details</summary>
Motivation: 研究直觉主义模态逻辑的单调模态变种，以扩展直觉主义模态逻辑的理论框架和应用范围。

Method: 将单调模态逻辑翻译成直觉主义一阶逻辑，并设计直觉主义邻域模型作为语义支持。

Result: 成功构建了单调模态逻辑的语法和语义框架，并将其与其他直觉主义模态逻辑进行了比较和嵌入验证。

Conclusion: 该研究为直觉主义模态逻辑提供了新的单调模态变种，验证了其可行性和理论一致性。

Abstract: We introduce a monotone modal analogue of the intuitionistic (normal) modal
logic IK using a translation into a suitable (intuitionistic) first-order
logic. We axiomatise the logic and give a semantics by means of intuitionistic
neighbourhood models, which contain neighbourhoods whose value can change when
moving along the intuitionistic accessibility relation. We compare the
resulting logic with other intuitionistic monotone modal logics and show how it
can be embedded into a multimodal version of IK.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [74] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 论文提出利用多模态数据集检测LLM驱动的对话机器人失败，并通过ERR@HRI 2.0挑战推动相关模型的开发。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的对话机器人易出错（如误解用户意图或中断对话），检测这些错误对维持用户体验和信任至关重要。

Method: 提供包含16小时人机对话的多模态数据集，标注了机器人错误和用户意图，邀请团队开发基于多模态数据的机器学习模型。

Result: 挑战旨在评测模型的失败检测性能，如准确率和误报率。

Conclusion: 该挑战通过社会信号分析推动人机交互中失败检测的改进。

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [75] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: 本文扩展了低成本GELLO远程操作系统，加入了力反馈和力信息的数据收集，改善了用户操作体验和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 扩展GELLO系统以支持力反馈和力信息收集，提升远程操作的直观性和任务成功率。

Method: 实现了力反馈功能，并将力信息加入数据收集与模仿学习模型的训练中，通过用户实验和任务性能对比验证效果。

Result: 有机器人经验的用户更青睐新控制器，力信息的加入显著提高了多数任务的完成率。

Conclusion: 力反馈和力信息的引入有效提升了远程操作系统的性能和用户体验。

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>
