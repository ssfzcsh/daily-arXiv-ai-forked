{"id": "2508.14104", "pdf": "https://arxiv.org/pdf/2508.14104", "abs": "https://arxiv.org/abs/2508.14104", "authors": ["Yutong Bian", "Xianhao Lin", "Yupeng Xie", "Tianyang Liu", "Mingchen Zhuge", "Siyuan Lu", "Haoming Tang", "Jinlin Wang", "Jiayi Zhang", "Jiaqi Chen", "Xiangru Tang", "Yongxin Ni", "Sirui Hong", "Chenglin Wu"], "title": "You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) and code agents in software development are\nrapidly evolving from generating isolated code snippets to producing\nfull-fledged software applications with graphical interfaces, interactive\nlogic, and dynamic behaviors. However, current benchmarks fall short in\nevaluating such production-ready software, as they often rely on static checks\nor binary pass/fail scripts, failing to capture the interactive behaviors and\nruntime dynamics that define real-world usability - qualities that only emerge\nwhen an application is actively used. This is the blind spot of current\nevaluation: you don't know if an app works until you click through it, interact\nwith it, and observe how it responds. To bridge this gap, we introduce\nRealDevWorld, a novel evaluation framework for automated end-to-end assessment\nof LLMs' ability to generate production-ready repositories from scratch. It\nfeatures two key components: (1) RealDevBench, a diverse collection of 194\nopen-ended software engineering tasks across multiple domains, incorporating\nmultimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a\nnew agent-as-a-judge evaluation system that simulates realistic, GUI-based user\ninteractions to automatically and holistically assess software functional\ncorrectness, visual fidelity, and runtime behavior. The framework delivers\nfine-grained, task-specific diagnostic feedback, supporting nuanced evaluation\nbeyond simple success/failure judgments. Empirical results show that\nRealDevWorld delivers effective, automatic, and human-aligned evaluations,\nachieving an accuracy of 0.92 and a correlation of 0.85 with expert human\nassessments, while significantly reducing the reliance on manual review. This\nenables scalable, human-aligned assessment of production-level software\ngenerated by LLMs. Our code is available on GitHub."}
{"id": "2508.14114", "pdf": "https://arxiv.org/pdf/2508.14114", "abs": "https://arxiv.org/abs/2508.14114", "authors": ["Aditey Nandan", "Viraj Kumar"], "title": "Ambiguity Resolution with Human Feedback for Code Writing Tasks", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted at the Proceedings of the 33rd International Conference on\n  Computers in Education (ICCE 2025), Asia-Pacific Society for Computers in\n  Education (APSCE)", "summary": "Specifications for code writing tasks are usually expressed in natural\nlanguage and may be ambiguous. Programmers must therefore develop the ability\nto recognize ambiguities in task specifications and resolve them by asking\nclarifying questions. We present and evaluate a prototype system, based on a\nnovel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)\nsuggests specific inputs on which a given task specification may be ambiguous,\n(2) seeks limited human feedback about the code's desired behavior on those\ninputs, and (3) uses this feedback to generate code that resolves these\nambiguities. We evaluate the efficacy of our prototype, and we discuss the\nimplications of such assistive systems on Computer Science education."}
{"id": "2508.14288", "pdf": "https://arxiv.org/pdf/2508.14288", "abs": "https://arxiv.org/abs/2508.14288", "authors": ["Yewei Song", "Tiezhu Sun", "Xunzhu Tang", "Prateek Rajput", "Tegawende F. Bissyande", "Jacques Klein"], "title": "Measuring LLM Code Generation Stability via Structural Entropy", "categories": ["cs.SE", "cs.CL"], "comment": "ASE-NIER", "summary": "Assessing the stability of code generation from large language models (LLMs)\nis essential for judging their reliability in real-world development. We extend\nprior \"structural-entropy concepts\" to the program domain by pairing entropy\nwith abstract syntax tree (AST) analysis. For any fixed prompt, we collect the\nmultiset of depth-bounded subtrees of AST in each generated program and treat\ntheir relative frequencies as a probability distribution. We then measure\nstability in two complementary ways: (i) Jensen-Shannon divergence, a\nsymmetric, bounded indicator of structural overlap, and (ii) a Structural\nCross-Entropy ratio that highlights missing high-probability patterns. Both\nmetrics admit structural-only and token-aware variants, enabling separate views\non control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or\nCodeBLEU, our metrics are reference-free, language-agnostic, and\nexecution-independent. We benchmark several leading LLMs on standard code\ngeneration tasks, demonstrating that AST-driven structural entropy reveals\nnuances in model consistency and robustness. The method runs in O(n,d) time\nwith no external tests, providing a lightweight addition to the code-generation\nevaluation toolkit."}
{"id": "2508.14419", "pdf": "https://arxiv.org/pdf/2508.14419", "abs": "https://arxiv.org/abs/2508.14419", "authors": ["Scott Blyth", "Sherlock A. Licorish", "Christoph Treude", "Markus Wagner"], "title": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, achieving high scores on benchmarks such as HumanEval and\nMBPP. However, these benchmarks primarily assess functional correctness and\nneglect broader dimensions of code quality, including security, reliability,\nreadability, and maintainability. In this work, we systematically evaluate the\nability of LLMs to generate high-quality code across multiple dimensions using\nthe PythonSecurityEval benchmark. We introduce an iterative static\nanalysis-driven prompting algorithm that leverages Bandit and Pylint to\nidentify and resolve code quality issues. Our experiments with GPT-4o show\nsubstantial improvements: security issues reduced from >40% to 13%, readability\nviolations from >80% to 11%, and reliability warnings from >50% to 11% within\nten iterations. These results demonstrate that LLMs, when guided by static\nanalysis feedback, can significantly enhance code quality beyond functional\ncorrectness."}
{"id": "2508.14394", "pdf": "https://arxiv.org/pdf/2508.14394", "abs": "https://arxiv.org/abs/2508.14394", "authors": ["Ryan Tjoa", "Poorva Garg", "Harrison Goldstein", "Todd Millstein", "Benjamin Pierce", "Guy Van den Broeck"], "title": "Tuning Random Generators: Property-Based Testing as Probabilistic Programming", "categories": ["cs.PL", "cs.SE", "D.3; D.2.5; G.3"], "comment": "Extended version of OOPSLA '25 paper", "summary": "Property-based testing validates software against an executable specification\nby evaluating it on randomly generated inputs. The standard way that PBT users\ngenerate test inputs is via generators that describe how to sample test inputs\nthrough random choices. To achieve a good distribution over test inputs, users\nmust tune their generators, i.e., decide on the weights of these individual\nrandom choices. Unfortunately, it is very difficult to understand how to choose\nindividual generator weights in order to achieve a desired distribution, so\ntoday this process is tedious and limits the distributions that can be\npractically achieved.\n  In this paper, we develop techniques for the automatic and offline tuning of\ngenerators. Given a generator with undetermined symbolic weights and an\nobjective function, our approach automatically learns values for these weights\nthat optimize for the objective. We describe useful objective functions that\nallow users to (1) target desired distributions and (2) improve the diversity\nand validity of their test cases. We have implemented our approach in a novel\ndiscrete probabilistic programming system, Loaded Dice, that supports\ndifferentiation and parameter learning, and use it as a language for\ngenerators. We empirically demonstrate that our approach is effective at\noptimizing generator distributions according to the specified objective\nfunctions. We also perform a thorough evaluation on PBT benchmarks,\ndemonstrating that, when automatically tuned for diversity and validity, the\ngenerators exhibit a 3.1-7.4x speedup in bug finding."}
{"id": "2508.14411", "pdf": "https://arxiv.org/pdf/2508.14411", "abs": "https://arxiv.org/abs/2508.14411", "authors": ["Seokjun Choi", "Hoon-Gyu Chung", "Yujin Jeon", "Giljoo Nam", "Seung-Hwan Baek"], "title": "A Real-world Display Inverse Rendering Dataset", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Inverse rendering aims to reconstruct geometry and reflectance from captured\nimages. Display-camera imaging systems offer unique advantages for this task:\neach pixel can easily function as a programmable point light source, and the\npolarized light emitted by LCD displays facilitates diffuse-specular\nseparation. Despite these benefits, there is currently no public real-world\ndataset captured using display-camera systems, unlike other setups such as\nlight stages. This absence hinders the development and evaluation of\ndisplay-based inverse rendering methods. In this paper, we introduce the first\nreal-world dataset for display-based inverse rendering. To achieve this, we\nconstruct and calibrate an imaging system comprising an LCD display and stereo\npolarization cameras. We then capture a diverse set of objects with diverse\ngeometry and reflectance under one-light-at-a-time (OLAT) display patterns. We\nalso provide high-quality ground-truth geometry. Our dataset enables the\nsynthesis of captured images under arbitrary display patterns and different\nnoise levels. Using this dataset, we evaluate the performance of existing\nphotometric stereo and inverse rendering methods, and provide a simple, yet\neffective baseline for display inverse rendering, outperforming\nstate-of-the-art inverse rendering methods. Code and dataset are available on\nour project page at https://michaelcsj.github.io/DIR/"}
{"id": "2508.14581", "pdf": "https://arxiv.org/pdf/2508.14581", "abs": "https://arxiv.org/abs/2508.14581", "authors": ["Chen Chen", "Runze Li", "Zejun Zhang", "Pukun Zhao", "Fanqing Zhou", "Longxiang Wang", "Haojian Huang"], "title": "FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics", "categories": ["cs.MM", "eess.IV"], "comment": null, "summary": "FakeHunter is a multimodal deepfake detection framework that combines\nmemory-guided retrieval, chain-of-thought (Observation-Thought-Action)\nreasoning, and tool-augmented verification to provide accurate and\ninterpretable video forensics. FakeHunter encodes visual content using CLIP and\naudio using CLAP, generating joint audio-visual embeddings that retrieve\nsemantically similar real exemplars from a FAISS-indexed memory bank for\ncontextual grounding. Guided by the retrieved context, the system iteratively\nreasons over evidence to localize manipulations and explain them. When\nconfidence is low, it automatically invokes specialized tools-such as zoom-in\nimage forensics or mel-spectrogram inspection-for fine-grained verification.\nBuilt on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that\nspecify what was modified, where it occurs, and why it is judged fake. We also\nintroduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos\n(950+ min) annotated with manipulation type, region/entity, violated reasoning\ncategory, and free-form justification. On X-AVFake, FakeHunter achieves an\naccuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87\npercentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies\nreveal that memory retrieval contributes a 7.75 percentage point gain, and\ntool-based inspection improves low-confidence cases to 46.50%. Despite its\nmulti-stage design, the pipeline processes a 10-minute clip in 8 minutes on a\nsingle NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x),\ndemonstrating practical deployability."}
{"id": "2508.14249", "pdf": "https://arxiv.org/pdf/2508.14249", "abs": "https://arxiv.org/abs/2508.14249", "authors": ["Matthias Hetzenberger", "Georg Moser", "Florian Zuleger"], "title": "To Zip Through the Cost Analysis of Probabilistic Programs", "categories": ["cs.LO"], "comment": null, "summary": "Probabilistic programming and the formal analysis of probabilistic algorithms\nare active areas of research, driven by the widespread use of randomness to\nimprove performance. While functional correctness has seen substantial\nprogress, automated reasoning about expected runtime remains comparatively\nlimited. In this work, we address this challenge by introducing a\nrefinement-typed probability monad in Liquid Haskell. Our monad enables\nautomated reasoning about expected values and costs by encoding probabilistic\nbehaviour directly in types. Initially defined for discrete distributions over\nfinite support, it is extended to support infinite distributions via an\naxiomatic approach. By leveraging Liquid Haskell's SMT-based refinement type\nchecking, our framework provides a high degree of automation. We evaluate our\napproach through four case studies: meldable heaps, coupon collector,\nrandomised quicksort, and zip trees. The first two demonstrate automation with\nminimal annotation overhead. The latter two showcase how our monad integrates\nwith interactive proofs, including the first formal verification of the\nexpected runtime of zip trees."}
{"id": "2508.14222", "pdf": "https://arxiv.org/pdf/2508.14222", "abs": "https://arxiv.org/abs/2508.14222", "authors": ["Miao Zhang", "Jiaxing Li", "Haoyuan Zhao", "Linfeng Shen", "Jiangchuan Liu"], "title": "StarStream: Live Video Analytics over Space Networking", "categories": ["cs.NI", "cs.MM"], "comment": "Accepted by MM'24", "summary": "Streaming videos from resource-constrained front-end devices over networks to\nresource-rich cloud servers has long been a common practice for surveillance\nand analytics. Most existing live video analytics (LVA) systems, however, have\nbeen built over terrestrial networks, limiting their applications during\nnatural disasters and in remote areas that desperately call for real-time\nvisual data delivery and scene analysis. With the recent advent of space\nnetworking, in particular, Low Earth Orbit (LEO) satellite constellations such\nas Starlink, high-speed truly global Internet access is becoming available and\naffordable. This paper examines the challenges and potentials of LVA over\nmodern LEO satellite networking (LSN). Using Starlink as the testbed, we have\ncarried out extensive in-the-wild measurements to gain insights into its\nachievable performance for LVA. The results reveal that the uplink bottleneck\nin today's LSN, together with the volatile network conditions, can\nsignificantly affect the service quality of LVA and necessitate prompt\nadaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming\nframework for LVA. At its core, StarStream is empowered by a Transformer-based\nnetwork performance predictor tailored for LSN and a content-aware\nconfiguration optimizer. We discuss a series of key design and implementation\nissues of StarStream and demonstrate its effectiveness and superiority through\ntrace-driven experiments with real-world network and video processing data."}
{"id": "2508.14257", "pdf": "https://arxiv.org/pdf/2508.14257", "abs": "https://arxiv.org/abs/2508.14257", "authors": ["Jonathan Zong"], "title": "Using Real Names of Disabled Participant-Contributors to Practice Citational Justice in Accessibility", "categories": ["cs.HC"], "comment": "4 pages, 0 figures. IEEE VIS Accessibility Workshop", "summary": "In accessibility research involving human subjects, researchers\nconventionally anonymize their research participants to protect privacy.\nHowever, a lack of intentionality about who to publicly acknowledge for\nintellectual contributions to research can lead to the erasure of disabled\nindividuals' work and knowledge. In this paper, I propose identifying disabled\nresearch participants by name (with consent) as a practice of citational\njustice. I share observations from examples of this practice in accessible\nvisualization research, and offer considerations for when it may be appropriate\nto de-anonymize. Intentional practices of citation offer researchers an\nopportunity to acknowledge the expertise and intellectual contributions of\ndisabled people in our communities."}
{"id": "2508.14084", "pdf": "https://arxiv.org/pdf/2508.14084", "abs": "https://arxiv.org/abs/2508.14084", "authors": ["Minh Chung", "Yaknan Gambo", "Burak Mete", "Xiao-Ting Michelle To", "Florian Krötz", "Korbinian Staudacher", "Martin Letras", "Xiaolong Deng", "Mounika Vavilala", "Amir Raoofy", "Jorge Echavarria", "Luigi Iapichino", "Laura Schulz", "Josef Weidendorfer", "Martin Schulz"], "title": "Q-BEAST: A Practical Course on Experimental Evaluation and Characterization of Quantum Computing Systems", "categories": ["physics.ed-ph", "cs.ET", "math.QA", "quant-ph"], "comment": "This paper is submitted and accepted in the Fourth Annual Quantum\n  Science and Engineering Education Conference (QSEEC25), which is collocated\n  with the IEEE International Conference on Quantum Computing & Engineering\n  (QCE25), part of IEEE Quantum Week 2025", "summary": "Quantum computing (QC) promises to be a transformative technology with impact\non various application domains, such as optimization, cryptography, and\nmaterial science. However, the technology has a sharp learning curve, and\npractical evaluation and characterization of quantum systems remains complex\nand challenging, particularly for students and newcomers from computer science\nto the field of quantum computing. To address this educational gap, we\nintroduce Q-BEAST, a practical course designed to provide structured training\nin the experimental analysis of quantum computing systems. Q-BEAST offers a\ncurriculum that combines foundational concepts in quantum computing with\npractical methodologies and use cases for benchmarking and performance\nevaluation on actual quantum systems. Through theoretical instruction and\nhands-on experimentation, students gain experience in assessing the advantages\nand limitations of real quantum technologies. With that, Q-BEAST supports the\neducation of a future generation of quantum computing users and developers.\nFurthermore, it also explicitly promotes a deeper integration of High\nPerformance Computing (HPC) and QC in research and education."}
{"id": "2508.14247", "pdf": "https://arxiv.org/pdf/2508.14247", "abs": "https://arxiv.org/abs/2508.14247", "authors": ["Saswata Jana", "Subhajit Pramanick", "Adri Bhattacharya", "Partha Sarathi Mandal"], "title": "Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots", "categories": ["cs.DC"], "comment": null, "summary": "In a connected graph with an autonomous robot swarm with limited visibility,\nit is natural to ask whether the robots can be deployed to certain vertices\nsatisfying a given property using only local knowledge. This paper\naffirmatively answers the question with a set of \\emph{myopic} (finite\nvisibility range) luminous robots with the aim of \\emph{filling a minimal\nvertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special\nvertices, called \\emph{doors}, through which robots enter sequentially.\nStarting from the doors, the goal of the robots is to settle on a set of\nvertices that forms a minimal vertex cover of $G$ under the asynchronous\n($\\mathcal{ASYNC}$) scheduler. We are also interested in achieving the\n\\emph{minimum vertex cover} (MinVC, which is NP-hard \\cite{Karp1972} for\ngeneral graphs) for a specific graph class using the myopic robots. We\nestablish lower bounds on the visibility range for the robots and on the time\ncomplexity (which is $\\Omega(|E|)$). We present two algorithms for trees: one\nfor single door, which is both time and memory-optimal, and the other for\nmultiple doors, which is memory-optimal and achieves time-optimality when the\nnumber of doors is a constant. Interestingly, our technique achieves MinVC on\ntrees with a single door. We then move to the general graph, where we present\ntwo algorithms, one for the single door and the other for the multiple doors\nwith an extra memory of $O(\\log \\Delta)$ for the robots, where $\\Delta$ is the\nmaximum degree of $G$. All our algorithms run in $O(|E|)$ epochs."}
{"id": "2508.14147", "pdf": "https://arxiv.org/pdf/2508.14147", "abs": "https://arxiv.org/abs/2508.14147", "authors": ["Zhuo Ma", "Dong Wen", "Hanchen Wang", "Wentao Li", "Wenjie Zhang", "Xuemin Lin"], "title": "Accelerating K-Core Computation in Temporal Graphs", "categories": ["cs.DB"], "comment": null, "summary": "We address the problem of enumerating all temporal k-cores given a query time\nrange and a temporal graph, which suffers from poor efficiency and scalability\nin the state-of-the-art solution. Motivated by an existing concept called core\ntimes, we propose a novel algorithm to compute all temporal $k$-cores based on\ncore times and prove that the algorithmic running time is bounded by the size\nof all resulting temporal k-cores, which is optimal in this scenario.\nMeanwhile, we show that the cost of computing core times is much lower, which\ndemonstrates the close relevance between our overall running time and the\nresult size."}
{"id": "2508.14053", "pdf": "https://arxiv.org/pdf/2508.14053", "abs": "https://arxiv.org/abs/2508.14053", "authors": ["Jinwei Tang", "Jiayin Qin", "Nuo Xu", "Pragnya Sudershan Nalla", "Yu Cao", "Yang", "Zhao", "Caiwen Ding"], "title": "MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging", "categories": ["cs.AR", "cs.AI", "cs.MA"], "comment": null, "summary": "As program workloads (e.g., AI) increase in size and algorithmic complexity,\nthe primary challenge lies in their high dimensionality, encompassing computing\ncores, array sizes, and memory hierarchies. To overcome these obstacles,\ninnovative approaches are required. Agile chip design has already benefited\nfrom machine learning integration at various stages, including logic synthesis,\nplacement, and routing. With Large Language Models (LLMs) recently\ndemonstrating impressive proficiency in Hardware Description Language (HDL)\ngeneration, it is promising to extend their abilities to 2.5D integration, an\nadvanced technique that saves area overhead and development costs. However,\nLLM-driven chiplet design faces challenges such as flatten design, high\nvalidation cost and imprecise parameter optimization, which limit its chiplet\ndesign capability. To address this, we propose MAHL, a hierarchical LLM-based\nchiplet design generation framework that features six agents which\ncollaboratively enable AI algorithm-hardware mapping, including hierarchical\ndescription generation, retrieval-augmented code generation, diverseflow-based\nvalidation, and multi-granularity design space exploration. These components\ntogether enhance the efficient generation of chiplet design with optimized\nPower, Performance and Area (PPA). Experiments show that MAHL not only\nsignificantly improves the generation accuracy of simple RTL design, but also\nincreases the generation accuracy of real-world chiplet design, evaluated by\nPass@5, from 0 to 0.72 compared to conventional LLMs under the best-case\nscenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves\ncomparable or even superior PPA results under certain optimization objectives."}
{"id": "2508.14451", "pdf": "https://arxiv.org/pdf/2508.14451", "abs": "https://arxiv.org/abs/2508.14451", "authors": ["Richard Sserujongi", "Daniel Ogenrwot", "Nicholas Niwamanya", "Noah Nsimbe", "Martin Bbaale", "Benjamin Ssempala", "Noble Mutabazi", "Raja Fidel Wabinyai", "Deo Okure", "Engineer Bainomugisha"], "title": "Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air Quality Monitoring in Low-Resource Settings", "categories": ["cs.SE", "K.6.3; E.0"], "comment": "15 pages, 11 figures, 34th International Conference on Software\n  Engineering and Data Engineering", "summary": "The increasing adoption of low-cost environmental sensors and AI-enabled\napplications has accelerated the demand for scalable and resilient data\ninfrastructures, particularly in data-scarce and resource-constrained regions.\nThis paper presents the design, implementation, and evaluation of the AirQo\ndata pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system\nengineered to support both real-time and batch processing of heterogeneous air\nquality data across urban deployments in Africa. It is Built using open-source\ntechnologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The\npipeline integrates diverse data streams from low-cost sensors, third-party\nweather APIs, and reference-grade monitors to enable automated calibration,\nforecasting, and accessible analytics. We demonstrate the pipeline's ability to\ningest, transform, and distribute millions of air quality measurements monthly\nfrom over 400 monitoring devices while achieving low latency, high throughput,\nand robust data availability, even under constrained power and connectivity\nconditions. The paper details key architectural features, including workflow\norchestration, decoupled ingestion layers, machine learning-driven sensor\ncalibration, and observability frameworks. Performance is evaluated across\noperational metrics such as resource utilization, ingestion throughput,\ncalibration accuracy, and data availability, offering practical insights into\nbuilding sustainable environmental data platforms. By open-sourcing the\nplatform and documenting deployment experiences, this work contributes a\nreusable blueprint for similar initiatives seeking to advance environmental\nintelligence through data engineering in low-resource settings."}
{"id": "2508.14614", "pdf": "https://arxiv.org/pdf/2508.14614", "abs": "https://arxiv.org/abs/2508.14614", "authors": ["Ashish Mishra", "Suresh Jagannathan"], "title": "Close is Good Enough: Component-Based Synthesis Modulo Logical Similarity", "categories": ["cs.PL", "D.3.0; D.3.1"], "comment": null, "summary": "Component-based synthesis (CBS) aims to generate loop-free programs from a\nset of libraries whose methods are annotated with specifications and whose\noutput must satisfy a set of logical constraints, expressed as a query. The\neffectiveness of a CBS algorithm critically depends on the severity of the\nconstraints imposed by the query. The more exact these constraints are, the\nsparser the space of feasible solutions. This maxim also applies when we enrich\nthe expressiveness of the specifications affixed to library methods. In both\ncases, the search must now contend with constraints that may only hold over a\nsmall number of the possible execution paths that can be enumerated by a CBS\nprocedure.\n  In this paper, we address this challenge by equipping CBS search with the\nability to reason about logical similarities among the paths it explores. Our\nsetting considers library methods equipped with refinement-type specifications\nthat enrich ordinary base types with a set of rich logical qualifiers to\nconstrain the set of values accepted by that type. We perform a search over a\ntree automata variant called Qualified Tree Automata that intelligently records\ninformation about enumerated terms, leveraging subtyping constraints over the\nrefinement types associated with these terms to enable reasoning about\nsimilarity among candidate solutions as search proceeds, thereby avoiding\nexploration of semantically similar paths.\n  We present an implementation of this idea in a tool called \\name, and provide\na comprehensive evaluation that demonstrates \\name's ability to synthesize\nsolutions to complex CBS queries that go well-beyond the capabilities of the\nexisting state-of-the-art."}
{"id": "2508.14879", "pdf": "https://arxiv.org/pdf/2508.14879", "abs": "https://arxiv.org/abs/2508.14879", "authors": ["Bingquan Dai", "Li Ray Luo", "Qihong Tang", "Jie Wang", "Xinyu Lian", "Hao Xu", "Minghan Qin", "Xudong Xu", "Bo Dai", "Haoqian Wang", "Zhaoyang Lyu", "Jiangmiao Pang"], "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding."}
{"id": "2508.14222", "pdf": "https://arxiv.org/pdf/2508.14222", "abs": "https://arxiv.org/abs/2508.14222", "authors": ["Miao Zhang", "Jiaxing Li", "Haoyuan Zhao", "Linfeng Shen", "Jiangchuan Liu"], "title": "StarStream: Live Video Analytics over Space Networking", "categories": ["cs.NI", "cs.MM"], "comment": "Accepted by MM'24", "summary": "Streaming videos from resource-constrained front-end devices over networks to\nresource-rich cloud servers has long been a common practice for surveillance\nand analytics. Most existing live video analytics (LVA) systems, however, have\nbeen built over terrestrial networks, limiting their applications during\nnatural disasters and in remote areas that desperately call for real-time\nvisual data delivery and scene analysis. With the recent advent of space\nnetworking, in particular, Low Earth Orbit (LEO) satellite constellations such\nas Starlink, high-speed truly global Internet access is becoming available and\naffordable. This paper examines the challenges and potentials of LVA over\nmodern LEO satellite networking (LSN). Using Starlink as the testbed, we have\ncarried out extensive in-the-wild measurements to gain insights into its\nachievable performance for LVA. The results reveal that the uplink bottleneck\nin today's LSN, together with the volatile network conditions, can\nsignificantly affect the service quality of LVA and necessitate prompt\nadaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming\nframework for LVA. At its core, StarStream is empowered by a Transformer-based\nnetwork performance predictor tailored for LSN and a content-aware\nconfiguration optimizer. We discuss a series of key design and implementation\nissues of StarStream and demonstrate its effectiveness and superiority through\ntrace-driven experiments with real-world network and video processing data."}
{"id": "2508.14531", "pdf": "https://arxiv.org/pdf/2508.14531", "abs": "https://arxiv.org/abs/2508.14531", "authors": ["Julien Saan Joachim", "Marc de Visme", "Stefan Haar"], "title": "Quantum Petri Nets with Event Structures semantics", "categories": ["cs.LO", "quant-ph"], "comment": null, "summary": "Classical Petri nets provide a canonical model of concurrency, with unfolding\nsemantics linking nets, occurrence nets, and event structures. No comparable\nframework exists for quantum concurrency: existing ''quantum Petri nets'' lack\nrigorous concurrent and sound quantum semantics, analysis tools, and unfolding\ntheory. We introduce Quantum Petri Nets (QPNs), Petri nets equipped with a\nquantum valuation compatible with the quantum event structure semantics of\nClairambault, De Visme, and Winskel (2019). Our contributions are: (i) a local\ndefinition of Quantum Occurrence Nets (LQONs) compatible with quantum event\nstructures, (ii) a construction of QPNs with a well-defined unfolding\nsemantics, (iii) a compositional framework for QPNs. This establishes a\nsemantically well grounded model of quantum concurrency, bridging Petri net\ntheory and quantum programming."}
{"id": "2508.14237", "pdf": "https://arxiv.org/pdf/2508.14237", "abs": "https://arxiv.org/abs/2508.14237", "authors": ["Miao Zhang", "Yifei Zhu", "Linfeng Shen", "Fangxin Wang", "Jiangchuan Liu"], "title": "OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos", "categories": ["cs.NI", "cs.CV", "cs.MM", "eess.IV"], "comment": "10 pages; Accepted by INFOCOM'23", "summary": "With the reduced hardware costs of omnidirectional cameras and the\nproliferation of various extended reality applications, more and more\n$360^\\circ$ videos are being captured. To fully unleash their potential,\nadvanced video analytics is expected to extract actionable insights and\nsituational knowledge without blind spots from the videos. In this paper, we\npresent OmniSense, a novel edge-assisted framework for online immersive video\nanalytics. OmniSense achieves both low latency and high accuracy, combating the\nsignificant computation and network resource challenges of analyzing\n$360^\\circ$ videos. Motivated by our measurement insights into $360^\\circ$\nvideos, OmniSense introduces a lightweight spherical region of interest (SRoI)\nprediction algorithm to prune redundant information in $360^\\circ$ frames.\nIncorporating the video content and network dynamics, it then smartly scales\nvision models to analyze the predicted SRoIs with optimized resource\nutilization. We implement a prototype of OmniSense with commodity devices and\nevaluate it on diverse real-world collected $360^\\circ$ videos. Extensive\nevaluation results show that compared to resource-agnostic baselines, it\nimproves the accuracy by $19.8\\%$ -- $114.6\\%$ with similar end-to-end\nlatencies. Meanwhile, it hits $2.0\\times$ -- $2.4\\times$ speedups while keeping\nthe accuracy on par with the highest accuracy of baselines."}
{"id": "2508.14289", "pdf": "https://arxiv.org/pdf/2508.14289", "abs": "https://arxiv.org/abs/2508.14289", "authors": ["Areen Khalaila", "Lane Harrison", "Nam Wook Kim", "Dylan Cashman"], "title": "\"They Aren't Built For Me\": An Exploratory Study of Strategies for Measurement of Graphical Primitives in Tactile Graphics", "categories": ["cs.HC"], "comment": null, "summary": "Advancements in accessibility technologies such as low-cost swell form\nprinters or refreshable tactile displays promise to allow blind or low-vision\n(BLV) people to analyze data by transforming visual representations directly to\ntactile representations. However, it is possible that design guidelines derived\nfrom experiments on the visual perception system may not be suited for the\ntactile perception system. We investigate the potential mismatch between\nfamiliar visual encodings and tactile perception in an exploratory study into\nthe strategies employed by BLV people to measure common graphical primitives\nconverted to tactile representations. First, we replicate the Cleveland and\nMcGill study on graphical perception using swell form printing with eleven BLV\nsubjects. Then, we present results from a group interview in which we describe\nthe strategies used by our subjects to read four common chart types. While our\nresults suggest that familiar encodings based on visual perception studies can\nbe useful in tactile graphics, our subjects also expressed a desire to use\nencodings designed explicitly for BLV people. Based on this study, we identify\ngaps between the perceptual expectations of common charts and the perceptual\ntools available in tactile perception. Then, we present a set of guidelines for\nthe design of tactile graphics that accounts for these gaps. Supplemental\nmaterial is available at\nhttps://osf.io/3nsfp/?view_only=7b7b8dcbae1d4c9a8bb4325053d13d9f."}
{"id": "2508.14109", "pdf": "https://arxiv.org/pdf/2508.14109", "abs": "https://arxiv.org/abs/2508.14109", "authors": ["Shayan Bafandkar", "Sungyong Chung", "Homa Khosravian", "Alireza Talebpour"], "title": "PAPPL: Personalized AI-Powered Progressive Learning Platform", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Engineering education has historically been constrained by rigid,\nstandardized frameworks, often neglecting students' diverse learning needs and\ninterests. While significant advancements have been made in online and\npersonalized education within K-12 and foundational sciences, engineering\neducation at both undergraduate and graduate levels continues to lag in\nadopting similar innovations. Traditional evaluation methods, such as exams and\nhomework assignments, frequently overlook individual student requirements,\nimpeding personalized educational experiences. To address these limitations,\nthis paper introduces the Personalized AI-Powered Progressive Learning (PAPPL)\nplatform, an advanced Intelligent Tutoring System (ITS) designed specifically\nfor engineering education. It highlights the development of a scalable,\ndata-driven tutoring environment leveraging cutting-edge AI technology to\nenhance personalized learning across diverse academic disciplines, particularly\nin STEM fields. PAPPL integrates core ITS components including the expert\nmodule, student module, tutor module, and user interface, and utilizes GPT-4o,\na sophisticated large language model (LLM), to deliver context-sensitive and\npedagogically sound hints based on students' interactions. The system uniquely\nrecords student attempts, detects recurring misconceptions, and generates\nprogressively targeted feedback, providing personalized assistance that adapts\ndynamically to each student's learning profile. Additionally, PAPPL offers\ninstructors detailed analytics, empowering evidence-based adjustments to\nteaching strategies. This study provides a fundamental framework for the\nprogression of Generative ITSs scalable to all education levels, delivering\nimportant perspectives on personalized progressive learning and the wider\npossibilities of Generative AI in the field of education."}
{"id": "2508.14271", "pdf": "https://arxiv.org/pdf/2508.14271", "abs": "https://arxiv.org/abs/2508.14271", "authors": ["Saul Youssef"], "title": "Pure Data Spaces", "categories": ["cs.DC", "math.LO", "03F50, 03B70, 68Q42, 16Y60, 20M35", "F.1.1; F.4.1; F.4.2; G.2.1; G.2.2"], "comment": "26 pages", "summary": "In a previous work, \"pure data\" is proposed as an axiomatic foundation for\nmathematics and computing, based on \"finite sequence\" as the foundational\nconcept rather than based on logic or type. Within this framework, objects with\nmathematical meaning are \"data\" and collections of mathematical objects must\nthen be associative data, called a \"space.\" A space is then the basic\ncollection in this framework analogous to sets in Set Theory or objects in\nCategory Theory. A theory of spaces is developed,where spaces are studied via\ntheir semiring of endomorphisms. To illustrate these concepts, and as a way of\nexploring the implications of the framework, pure data spaces are \"grown\norganically\" from the substrate of pure data with minimal combinatoric\ndefinitions. Familiar objects from classical mathematics emerge this way,\nincluding natural numbers, integers, rational numbers, boolean spaces, matrix\nalgebras, Gaussian Integers, Quaternions, and non-associative algebras like the\nInteger Octonions. Insights from these examples are discussed with a view\ntowards new directions in theory and new exploration."}
{"id": "2508.14356", "pdf": "https://arxiv.org/pdf/2508.14356", "abs": "https://arxiv.org/abs/2508.14356", "authors": ["Xinjian Zhang", "Lu Chen", "Chengfei Liu", "Rui Zhou", "Bo Ning"], "title": "Efficient Size Constraint Community Search over Heterogeneous Information Networks", "categories": ["cs.DB"], "comment": null, "summary": "The goal of community search in heterogeneous information networks (HINs) is\nto identify a set of closely related target nodes that includes a query target\nnode. In practice, a size constraint is often imposed due to limited resources,\nwhich has been overlooked by most existing HIN community search works. In this\npaper, we introduce the size-bounded community search problem to HIN data.\nSpecifically, we propose a refined (k, P)-truss model to measure community\ncohesiveness, aiming to identify the most cohesive community of size s that\ncontains the query node. We prove that this problem is NP-hard. To solve this\nproblem, we develop a novel B\\&B framework that efficiently generates target\nnode sets of size s. We then tailor novel bounding, branching, total ordering,\nand candidate reduction optimisations, which enable the framework to\nefficiently lead to an optimum result. We also design a heuristic algorithm\nleveraging structural properties of HINs to efficiently obtain a high-quality\ninitial solution, which serves as a global lower bound to further enhance the\nabove optimisations. Building upon these, we propose two exact algorithms that\nenumerate combinations of edges and nodes, respectively. Extensive experiments\non real-world datasets demonstrate the effectiveness and efficiency of the\nproposed methods."}
{"id": "2508.14068", "pdf": "https://arxiv.org/pdf/2508.14068", "abs": "https://arxiv.org/abs/2508.14068", "authors": ["Chen Chen", "Jiaqi Yin", "Cunxi Yu"], "title": "Revisit Choice Network for Synthesis and Technology Mapping", "categories": ["cs.AR", "cs.AI"], "comment": "Accepted by ICCAD 2025", "summary": "Choice network construction is a critical technique for alleviating\nstructural bias issues in Boolean optimization, equivalence checking, and\ntechnology mapping. Previous works on lossless synthesis utilize independent\noptimization to generate multiple snapshots, and use simulation and SAT solvers\nto identify functionally equivalent nodes. These nodes are then merged into a\nsubject graph with choice nodes. However, such methods often neglect the\nquality of these choices, raising the question of whether they truly contribute\nto effective technology mapping.\n  This paper introduces Cristal, a novel methodology and framework for\nconstructing Boolean choice networks. Specifically, Cristal introduces a new\nflow of choice network-based synthesis and mapping, including representative\nlogic cone search, structural mutation for generating diverse choice structures\nvia equality saturation, and priority-ranking choice selection along with\nchoice network construction and validation. Through these techniques, Cristal\nconstructs fewer but higher-quality choices.\n  Our experimental results demonstrate that Cristal outperforms the\nstate-of-the-art Boolean choice network construction implemented in ABC in the\npost-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in\ndelay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime\nreduction on large-scale cases across a diverse set of combinational circuits\nfrom the IWLS 2005, ISCAS'89, and EPFL benchmark suites."}
{"id": "2508.14511", "pdf": "https://arxiv.org/pdf/2508.14511", "abs": "https://arxiv.org/abs/2508.14511", "authors": ["Eagon Meng", "Daniel Jackson"], "title": "What You See Is What It Does: A Structural Pattern for Legible Software", "categories": ["cs.SE", "D.2.11"], "comment": "16 pages. Appearing in Onward! at SPLASH 2025", "summary": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach."}
{"id": "2508.14851", "pdf": "https://arxiv.org/pdf/2508.14851", "abs": "https://arxiv.org/abs/2508.14851", "authors": ["Radosław Jan Rowicki", "Adrian Francalanza", "Alceste Scalas"], "title": "Correct Black-Box Monitors for Distributed Deadlock Detection: Formalisation and Implementation (Technical Report)", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "Many software applications rely on concurrent and distributed (micro)services\nthat interact via message-passing and various forms of remote procedure calls\n(RPC). As these systems organically evolve and grow in scale and complexity,\nthe risk of introducing deadlocks increases and their impact may worsen: even\nif only a few services deadlock, many other services may block while awaiting\nresponses from the deadlocked ones. As a result, the \"core\" of the deadlock can\nbe obfuscated by its consequences on the rest of the system, and diagnosing and\nfixing the problem can be challenging.\n  In this work we tackle the challenge by proposing distributed black-box\nmonitors that are deployed alongside each service and detect deadlocks by only\nobserving the incoming and outgoing messages, and exchanging probes with other\nmonitors. We present a formal model that captures popular RPC-based application\nstyles (e.g., gen_servers in Erlang/OTP), and a distributed black-box\nmonitoring algorithm that we prove sound and complete (i.e., identifies\ndeadlocked services with neither false positives nor false negatives). We\nimplement our results in a tool called DDMon for the monitoring of Erlang/OTP\napplications, and we evaluate its performance.\n  This is the first work that formalises, proves the correctness, and\nimplements distributed black-box monitors for deadlock detection. Our results\nare mechanised in Coq. DDMon is the companion artifact of this paper."}
{"id": "2508.14892", "pdf": "https://arxiv.org/pdf/2508.14892", "abs": "https://arxiv.org/abs/2508.14892", "authors": ["Jia Lu", "Taoran Yi", "Jiemin Fang", "Chen Yang", "Chuiyun Wu", "Wei Shen", "Wenyu Liu", "Qi Tian", "Xinggang Wang"], "title": "Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hustvl.github.io/Snap-Snap/", "summary": "Reconstructing 3D human bodies from sparse views has been an appealing topic,\nwhich is crucial to broader the related applications. In this paper, we propose\na quite challenging but valuable task to reconstruct the human body from only\ntwo images, i.e., the front and back view, which can largely lower the barrier\nfor users to create their own 3D digital humans. The main challenges lie in the\ndifficulty of building 3D consistency and recovering missing information from\nthe highly sparse input. We redesign a geometry reconstruction model based on\nfoundation reconstruction models to predict consistent point clouds even input\nimages have scarce overlaps with extensive human data training. Furthermore, an\nenhancement algorithm is applied to supplement the missing color information,\nand then the complete human point clouds with colors can be obtained, which are\ndirectly transformed into 3D Gaussians for better rendering quality.\nExperiments show that our method can reconstruct the entire human in 190 ms on\na single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,\ndemonstrating state-of-the-art performance on the THuman2.0 and cross-domain\ndatasets. Additionally, our method can complete human reconstruction even with\nimages captured by low-cost mobile devices, reducing the requirements for data\ncollection. Demos and code are available at\nhttps://hustvl.github.io/Snap-Snap/."}
{"id": "2508.14237", "pdf": "https://arxiv.org/pdf/2508.14237", "abs": "https://arxiv.org/abs/2508.14237", "authors": ["Miao Zhang", "Yifei Zhu", "Linfeng Shen", "Fangxin Wang", "Jiangchuan Liu"], "title": "OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos", "categories": ["cs.NI", "cs.CV", "cs.MM", "eess.IV"], "comment": "10 pages; Accepted by INFOCOM'23", "summary": "With the reduced hardware costs of omnidirectional cameras and the\nproliferation of various extended reality applications, more and more\n$360^\\circ$ videos are being captured. To fully unleash their potential,\nadvanced video analytics is expected to extract actionable insights and\nsituational knowledge without blind spots from the videos. In this paper, we\npresent OmniSense, a novel edge-assisted framework for online immersive video\nanalytics. OmniSense achieves both low latency and high accuracy, combating the\nsignificant computation and network resource challenges of analyzing\n$360^\\circ$ videos. Motivated by our measurement insights into $360^\\circ$\nvideos, OmniSense introduces a lightweight spherical region of interest (SRoI)\nprediction algorithm to prune redundant information in $360^\\circ$ frames.\nIncorporating the video content and network dynamics, it then smartly scales\nvision models to analyze the predicted SRoIs with optimized resource\nutilization. We implement a prototype of OmniSense with commodity devices and\nevaluate it on diverse real-world collected $360^\\circ$ videos. Extensive\nevaluation results show that compared to resource-agnostic baselines, it\nimproves the accuracy by $19.8\\%$ -- $114.6\\%$ with similar end-to-end\nlatencies. Meanwhile, it hits $2.0\\times$ -- $2.4\\times$ speedups while keeping\nthe accuracy on par with the highest accuracy of baselines."}
{"id": "2508.14670", "pdf": "https://arxiv.org/pdf/2508.14670", "abs": "https://arxiv.org/abs/2508.14670", "authors": ["Sarah Meng Li", "Michele Mosca", "Neil J. Ross", "John van de Wetering", "Yuming Zhao"], "title": "A Complete and Natural Rule Set for Multi-Qutrit Clifford Circuits", "categories": ["cs.LO"], "comment": "In Proceedings QPL 2025, arXiv:2508.13619", "summary": "We present a complete set of rewrite rules for n-qutrit Clifford circuits\nwhere n is any non-negative integer. This is the first completeness result for\nany fragment of quantum circuits in odd prime dimensions. We first generalize\nSelinger's normal form for n-qubit Clifford circuits to the qutrit setting.\nThen, we present a rewrite system by which any Clifford circuit can be reduced\nto this normal form. We then simplify the rewrite rules in this procedure to a\nsmall natural set of rules, giving a clean presentation of the group of qutrit\nClifford unitaries in terms of generators and relations."}
{"id": "2508.14239", "pdf": "https://arxiv.org/pdf/2508.14239", "abs": "https://arxiv.org/abs/2508.14239", "authors": ["Shengze Wang", "Yi Liu", "Xiaoxue Zhang", "Liting Hu", "Chen Qian"], "title": "A Distributed Learned Hash Table", "categories": ["cs.NI"], "comment": null, "summary": "Distributed Hash Tables (DHTs) are pivotal in numerous high-impact key-value\napplications built on distributed networked systems, offering a decentralized\narchitecture that avoids single points of failure and improves data\navailability. Despite their widespread utility, DHTs face substantial\nchallenges in handling range queries, which are crucial for applications such\nas LLM serving, distributed storage, databases, content delivery networks, and\nblockchains. To address this limitation, we present LEAD, a novel system\nincorporating learned models within DHT structures to significantly optimize\nrange query performance. LEAD utilizes a recursive machine learning model to\nmap and retrieve data across a distributed system while preserving the inherent\norder of data. LEAD includes the designs to minimize range query latency and\nmessage cost while maintaining high scalability and resilience to network\nchurn. Our comprehensive evaluations, conducted in both testbed implementation\nand simulations, demonstrate that LEAD achieves tremendous advantages in system\nefficiency compared to existing range query methods in large-scale distributed\nsystems, reducing query latency and message cost by 80% to 90%+. Furthermore,\nLEAD exhibits remarkable scalability and robustness against system churn,\nproviding a robust, scalable solution for efficient data retrieval in\ndistributed key-value systems."}
{"id": "2508.14346", "pdf": "https://arxiv.org/pdf/2508.14346", "abs": "https://arxiv.org/abs/2508.14346", "authors": ["Sungwon In", "Ayush Roy", "Eric Krokos", "Kirsten Whitley", "Chris North", "Yalong Yang"], "title": "Exploring Organizational Strategies in Immersive Computational Notebooks", "categories": ["cs.HC"], "comment": "10 pages, 8 figures, IEEE International Symposium on Mixed and\n  Augmented Reality (ISMAR)", "summary": "Computational notebooks, which integrate code, documentation, tags, and\nvisualizations into a single document, have become increasingly popular for\ndata analysis tasks. With the advent of immersive technologies, these notebooks\nhave evolved into a new paradigm, enabling more interactive and intuitive ways\nto perform data analysis. An immersive computational notebook, which integrates\ncomputational notebooks within an immersive environment, significantly enhances\nnavigation performance with embodied interactions. However, despite recognizing\nthe significance of organizational strategies in the immersive data science\nprocess, the organizational strategies for using immersive notebooks remain\nlargely unexplored. In response, our research aims to deepen our understanding\nof organizations, especially focusing on spatial structures for computational\nnotebooks, and to examine how various execution orders can be visualized in an\nimmersive context. Through an exploratory user study, we found participants\npreferred organizing notebooks in half-cylindrical structures and engaged\nsignificantly more in non-linear analysis. Notably, as the scale of the\nnotebooks increased (i.e., more code cells), users increasingly adopted\nmultiple, concurrent non-linear analytical approaches."}
{"id": "2508.14533", "pdf": "https://arxiv.org/pdf/2508.14533", "abs": "https://arxiv.org/abs/2508.14533", "authors": ["Theodoros Trochatos", "Christopher Kang", "Andrew Wang", "Frederic T. Chong", "Jakub Szefer"], "title": "Trace-Based Reconstruction of Quantum Circuit Dataflow in Surface Codes", "categories": ["quant-ph", "cs.AR", "cs.ET", "cs.SE"], "comment": null, "summary": "Practical applications of quantum computing depend on fault-tolerant devices\nthat employ error correction. A promising quantum error-correcting code for\nlarge-scale quantum computing is the surface code. For this code,\nFault-Tolerant Quantum Computing (FTQC) can be performed via lattice surgery,\ni.e. merging and splitting of encoded qubit patches on a 2D grid. Lattice\nsurgery operations result in space-time patterns of activity that are defined\nin this work as access traces. This work demonstrates that the access traces\nreveal when, where, and how logical qubits interact. Leveraging this\nformulation, this work further introduces TraceQ, a trace-based reconstruction\nframework that is able to reconstruct the quantum circuit dataflow just by\nobserving the patch activity at each trace entry. The framework is supported by\nheuristics for handling inherent ambiguity in the traces, and demonstrates its\neffectiveness on a range of synthetic fault-tolerant quantum benchmarks. The\naccess traces can have applications in a wide range of scenarios, enabling\nanalysis and profiling of execution of quantum programs and the hardware they\nrun on. As one example use of TraceQ, this work investigates whether such\ntraces can act as a side channel through which an observer can recover the\ncircuit's structure and identify known subroutines in a larger program or even\nwhole programs. The findings show that indeed the minimal access traces can be\nused to recover subroutines or even whole quantum programs with very high\naccuracy. Only a single trace per program execution is needed and the\nprocessing can be done fully offline. Along with the custom heuristics,\nadvanced subgraph matching algorithms used in this work enable a high rate of\nlocating the subroutines while executing in minimal time."}
{"id": "2508.14319", "pdf": "https://arxiv.org/pdf/2508.14319", "abs": "https://arxiv.org/abs/2508.14319", "authors": ["Parshan Javanrood", "Matei Ripeanu"], "title": "SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path", "categories": ["cs.DC"], "comment": "Submitted to the IA^3 Workshop at SC25. Under review", "summary": "Modern graphs are both large and dynamic, presenting significant challenges\nfor fundamental queries, such as the Single-Source Shortest Path (SSSP)\nproblem. Naively recomputing the SSSP tree after each topology change is\nprohibitively expensive, causing on-demand computation to suffer from high\nlatency. Existing dynamic SSSP algorithms often cannot simultaneously handle\nboth edge additions and deletions, operate in distributed memory, and provide\nlow-latency query results. To address these challenges, this paper presents\nSSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm\nfor dynamic SSSP. Operating in a shared-nothing architecture, our algorithm\nprocesses streams of both edge insertions and deletions. We conduct a\ncomprehensive evaluation on large real-world and synthetic graphs with millions\nof vertices, and provide a thorough analysis by evaluating result latency,\nsolution stability, and throughput."}
{"id": "2508.14608", "pdf": "https://arxiv.org/pdf/2508.14608", "abs": "https://arxiv.org/abs/2508.14608", "authors": ["Paulo Pintor", "Rogério Costa", "José Moreira"], "title": "A DBMS-independent approach for capturing provenance polynomials through query rewriting", "categories": ["cs.DB"], "comment": null, "summary": "In today's data-driven ecosystems, ensuring data integrity, traceability and\naccountability is important. Provenance polynomials constitute a powerful\nformalism for tracing the origin and the derivations made to produce database\nquery results. Despite their theoretical expressiveness, current\nimplementations have limitations in handling aggregations and nested queries,\nand some of them and tightly coupled to a single Database Management System\n(DBMS), hindering interoperability and broader applicability.\n  This paper presents a query rewriting-based approach for annotating\nStructured Query Language (SQL) queries with provenance polynomials. The\nproposed methods are DBMS-independent and support\nSelect-Projection-Join-Union-Aggregation (SPJUA) operations and nested queries,\nthrough recursive propagation of provenance annotations. This constitutes the\nfirst full implementation of semiring-based theory for provenance polynomials\nextended with semimodule structures. It also presents an experimental\nevaluation to assess the validity of the proposed methods and compare the\nperformance against state-of-the-art systems using benchmark data and queries.\nThe results indicate that our solution delivers a comprehensive implementation\nof the theoretical formalisms proposed in the literature, and demonstrates\nimproved performance and scalability, outperforming existing methods."}
{"id": "2508.14123", "pdf": "https://arxiv.org/pdf/2508.14123", "abs": "https://arxiv.org/abs/2508.14123", "authors": ["Ankita Sharma", "YuQi Fu", "Vahid Ansari", "Rishabh Iyer", "Fiona Kuang", "Kashish Mistry", "Raisa Islam Aishy", "Sara Ahmad", "Joaquin Matres", "Dirk R. Englund", "Joyce K. S. Poon"], "title": "AI Agents for Photonic Integrated Circuit Design Automation", "categories": ["cs.AR", "cs.AI", "physics.app-ph", "physics.optics"], "comment": null, "summary": "We present Photonics Intelligent Design and Optimization (PhIDO), a\nmulti-agent framework that converts natural-language photonic integrated\ncircuit (PIC) design requests into layout mask files. We compare 7 reasoning\nlarge language models for PhIDO using a testbench of 102 design descriptions\nthat ranged from single devices to 112-component PICs. The success rate for\nsingle-device designs was up to 91%. For design queries with less than or equal\nto 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest\nend-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro\nrequiring the fewest output tokens and lowest cost. The next steps toward\nautonomous PIC development include standardized knowledge representations,\nexpanded datasets, extended verification, and robotic automation."}
{"id": "2508.14532", "pdf": "https://arxiv.org/pdf/2508.14532", "abs": "https://arxiv.org/abs/2508.14532", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Mingqi Yang", "Haokun Li", "Xiao Yi", "Shengchao Qin", "Jianwei Yin"], "title": "Preguss: It Analyzes, It Specifies, It Verifies", "categories": ["cs.SE", "cs.LO"], "comment": "Position paper to appear in the 1st International Workshop on\n  Language Models and Programming Languages (LMPL '25)", "summary": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs."}
{"id": "2508.14187", "pdf": "https://arxiv.org/pdf/2508.14187", "abs": "https://arxiv.org/abs/2508.14187", "authors": ["Md Ashiqur Rahman", "Chiao-An Yang", "Michael N. Cheng", "Lim Jun Hao", "Jeremiah Jiang", "Teck-Yian Lim", "Raymond A. Yeh"], "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Scale variation is a fundamental challenge in computer vision. Objects of the\nsame class can have different sizes, and their perceived size is further\naffected by the distance from the camera. These variations are local to the\nobjects, i.e., different object sizes may change differently within the same\nimage. To effectively handle scale variations, we present a deep equilibrium\ncanonicalizer (DEC) to improve the local scale equivariance of a model. DEC can\nbe easily incorporated into existing network architectures and can be adapted\nto a pre-trained model. Notably, we show that on the competitive ImageNet\nbenchmark, DEC improves both model performance and local scale consistency\nacross four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our\ncode is available at https://github.com/ashiq24/local-scale-equivariance."}
{"id": "2508.14475", "pdf": "https://arxiv.org/pdf/2508.14475", "abs": "https://arxiv.org/abs/2508.14475", "authors": ["Xiangfei Sheng", "Xiaofeng Pan", "Zhichao Yang", "Pengfei Chen", "Leida Li"], "title": "Fine-grained Image Quality Assessment for Perceptual Image Restoration", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "9 pages,6 figures", "summary": "Recent years have witnessed remarkable achievements in perceptual image\nrestoration (IR), creating an urgent demand for accurate image quality\nassessment (IQA), which is essential for both performance comparison and\nalgorithm optimization. Unfortunately, the existing IQA metrics exhibit\ninherent weakness for IR task, particularly when distinguishing fine-grained\nquality differences among restored images. To address this dilemma, we\ncontribute the first-of-its-kind fine-grained image quality assessment dataset\nfor image restoration, termed FGRestore, comprising 18,408 restored images\nacross six common IR tasks. Beyond conventional scalar quality scores,\nFGRestore was also annotated with 30,886 fine-grained pairwise preferences.\nBased on FGRestore, a comprehensive benchmark was conducted on the existing IQA\nmetrics, which reveal significant inconsistencies between score-based IQA\nevaluations and the fine-grained restoration quality. Motivated by these\nfindings, we further propose FGResQ, a new IQA model specifically designed for\nimage restoration, which features both coarse-grained score regression and\nfine-grained quality ranking. Extensive experiments and comparisons demonstrate\nthat FGResQ significantly outperforms state-of-the-art IQA metrics. Codes and\nmodel weights have been released in https://pxf0429.github.io/FGResQ/"}
{"id": "2508.14725", "pdf": "https://arxiv.org/pdf/2508.14725", "abs": "https://arxiv.org/abs/2508.14725", "authors": ["Daniel Hausmann", "Shufang Zhu", "Gianmarco Parretti", "Christoph Weinhuber", "Giuseppe De Giacomo", "Nir Piterman"], "title": "Emerson-Lei and Manna-Pnueli Games for LTLf+ and PPLTL+ Synthesis", "categories": ["cs.LO", "cs.AI", "cs.FL"], "comment": null, "summary": "Recently, the Manna-Pnueli Hierarchy has been used to define the temporal\nlogics LTLfp and PPLTLp, which allow to use finite-trace LTLf/PPLTL techniques\nin infinite-trace settings while achieving the expressiveness of full LTL. In\nthis paper, we present the first actual solvers for reactive synthesis in these\nlogics. These are based on games on graphs that leverage DFA-based techniques\nfrom LTLf/PPLTL to construct the game arena. We start with a symbolic solver\nbased on Emerson-Lei games, which reduces lower-class properties (guarantee,\nsafety) to higher ones (recurrence, persistence) before solving the game. We\nthen introduce Manna-Pnueli games, which natively embed Manna-Pnueli objectives\ninto the arena. These games are solved by composing solutions to a DAG of\nsimpler Emerson-Lei games, resulting in a provably more efficient approach. We\nimplemented the solvers and practically evaluated their performance on a range\nof representative formulas. The results show that Manna-Pnueli games often\noffer significant advantages, though not universally, indicating that combining\nboth approaches could further enhance practical performance."}
{"id": "2508.14281", "pdf": "https://arxiv.org/pdf/2508.14281", "abs": "https://arxiv.org/abs/2508.14281", "authors": ["Zhun Yin", "Xiaotian Li", "Lifan Mei", "Yong Liu", "Zhong-Ping Jiang"], "title": "DeeP-TE: Data-enabled Predictive Traffic Engineering", "categories": ["cs.NI"], "comment": null, "summary": "Routing configurations of a network should constantly adapt to traffic\nvariations to achieve good network performance. Adaptive routing faces two main\nchallenges: 1) how to accurately measure/estimate time-varying traffic\nmatrices? 2) how to control the network and application performance degradation\ncaused by frequent route changes? In this paper, we develop a novel\ndata-enabled predictive traffic engineering (DeeP-TE) algorithm that minimizes\nthe network congestion by gracefully adapting routing configurations over time.\nOur control algorithm can generate routing updates directly from the historical\nrouting data and the corresponding link rate data, without direct traffic\nmatrix measurement or estimation. Numerical experiments on real network\ntopologies with real traffic matrices demonstrate that the proposed DeeP-TE\nrouting adaptation algorithm can achieve close-to-optimal control effectiveness\nwith significantly lower routing variations than the baseline methods."}
{"id": "2508.14395", "pdf": "https://arxiv.org/pdf/2508.14395", "abs": "https://arxiv.org/abs/2508.14395", "authors": ["Running Zhao", "Zhihan Jiang", "Xinchen Zhang", "Chirui Chang", "Handi Chen", "Weipeng Deng", "Luyao Jin", "Xiaojuan Qi", "Xun Qian", "Edith C. H. Ngai"], "title": "NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to UIST 2025. Project website:\n  https://zhaorunning.github.io/NoteIt/", "summary": "Users often take notes for instructional videos to access key knowledge later\nwithout revisiting long videos. Automated note generation tools enable users to\nobtain informative notes efficiently. However, notes generated by existing\nresearch or off-the-shelf tools fail to preserve the information conveyed in\nthe original videos comprehensively, nor can they satisfy users' expectations\nfor diverse presentation formats and interactive features when using notes\ndigitally. In this work, we present NoteIt, a system, which automatically\nconverts instructional videos to interactable notes using a novel pipeline that\nfaithfully extracts hierarchical structure and multimodal key information from\nvideos. With NoteIt's interface, users can interact with the system to further\ncustomize the content and presentation formats of the notes according to their\npreferences. We conducted both a technical evaluation and a comparison user\nstudy (N=36). The solid performance in objective metrics and the positive user\nfeedback demonstrated the effectiveness of the pipeline and the overall\nusability of NoteIt. Project website: https://zhaorunning.github.io/NoteIt/"}
{"id": "2508.14457", "pdf": "https://arxiv.org/pdf/2508.14457", "abs": "https://arxiv.org/abs/2508.14457", "authors": ["Yongrae Jo", "Chanik Park"], "title": "A Hierarchical Sharded Blockchain Balancing Performance and Availability", "categories": ["cs.DC"], "comment": null, "summary": "Blockchain networks offer decentralization, transparency, and immutability\nfor managing critical data but encounter scalability problems as the number of\nnetwork members and transaction issuers grows. Sharding is considered a\npromising solution to enhance blockchain scalability. However, most existing\nblockchain sharding techniques prioritize performance at the cost of\navailability (e.g., a failure in a few servers holding a shard leads to data\nunavailability). In this paper, we propose PyloChain, a hierarchical sharded\nblockchain that balances availability and performance. PyloChain consists of\nmultiple lower-level local chains and one higher-level main chain. Each local\nchain speculatively executes local transactions to achieve high parallelism\nacross multiple local chains. The main chain leverages a directed-acyclic-graph\n(DAG)-based mempool to guarantee local block availability and to enable\nefficient Byzantine Fault Tolerance (BFT) consensus to execute global (or\ncross-shard) transactions within a collocated sharding. PyloChain speculatively\nexecutes local transactions across multiple local chains to achieve high\nparallelism. In order to reduce the number of aborted local transactions,\nPyloChain applies a simple scheduling technique to handle global transactions\nin the main chain. PyloChain provides a fine-grained auditing mechanism to\nmitigate faulty higher-level members by externalizing main chain operations to\nlower-level local members. We implemented and evaluated PyloChain,\ndemonstrating its performance scalability with 1.49x higher throughput and\n2.63x faster latency compared to the state-of-the-art balanced hierarchical\nsharded blockchain."}
{"id": "2508.14056", "pdf": "https://arxiv.org/pdf/2508.14056", "abs": "https://arxiv.org/abs/2508.14056", "authors": ["Sepideh Entezari Maleki", "Mohammadreza Pourreza", "Davood Rafiei"], "title": "Confidence Estimation for Text-to-SQL in Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Confidence estimation for text-to-SQL aims to assess the reliability of\nmodel-generated SQL queries without having access to gold answers. We study\nthis problem in the context of large language models (LLMs), where access to\nmodel weights and gradients is often constrained. We explore both black-box and\nwhite-box confidence estimation strategies, evaluating their effectiveness on\ncross-domain text-to-SQL benchmarks. Our evaluation highlights the superior\nperformance of consistency-based methods among black-box models and the\nadvantage of SQL-syntax-aware approaches for interpreting LLM logits in\nwhite-box settings. Furthermore, we show that execution-based grounding of\nqueries provides a valuable supplementary signal, improving the effectiveness\nof both approaches."}
{"id": "2508.14245", "pdf": "https://arxiv.org/pdf/2508.14245", "abs": "https://arxiv.org/abs/2508.14245", "authors": ["Shuting Du", "Mohamed Ibrahim", "Zishen Wan", "Luqi Zheng", "Boheng Zhao", "Zhenkun Fan", "Che-Kai Liu", "Tushar Krishna", "Arijit Raychowdhury", "Haitong Li"], "title": "Cross-Layer Design of Vector-Symbolic Computing: Bridging Cognition and Brain-Inspired Hardware Acceleration", "categories": ["cs.AR"], "comment": null, "summary": "Vector Symbolic Architectures (VSAs) have been widely deployed in various\ncognitive applications due to their simple and efficient operations. The\nwidespread adoption of VSAs has, in turn, spurred the development of numerous\nhardware solutions aimed at optimizing their performance. Despite these\nadvancements, a comprehensive and unified discourse on the convergence of\nhardware and algorithms in the context of VSAs remains somewhat limited. The\npaper aims to bridge the gap between theoretical software-level explorations\nand the development of efficient hardware architectures and emerging technology\nfabrics for VSAs, providing insights from the co-design aspect for researchers\nfrom either side. First, we introduce the principles of vector-symbolic\ncomputing, including its core mathematical operations and learning paradigms.\nSecond, we provide an in-depth discussion on hardware technologies for VSAs,\nanalyzing analog, mixed-signal, and digital circuit design styles. We compare\nhardware implementations of VSAs by carrying out detailed analysis of their\nperformance characteristics and tradeoffs, allowing us to extract design\nguidelines for the development of arbitrary VSA formulations. Third, we discuss\na methodology for cross-layer design of VSAs that identifies synergies across\nlayers and explores key ingredients for hardware/software co-design of VSAs.\nFinally, as a concrete demonstration of this methodology, we propose the first\nin-memory computing hierarchical cognition hardware system, showcasing the\nefficiency, flexibility, and scalability of this co-design approach. The paper\nconcludes with a discussion of open research challenges for future\nexplorations."}
{"id": "2508.14540", "pdf": "https://arxiv.org/pdf/2508.14540", "abs": "https://arxiv.org/abs/2508.14540", "authors": ["Dennis Schiese", "Andreas Both"], "title": "Post-hoc LLM-Supported Debugging of Distributed Processes", "categories": ["cs.SE", "cs.AI"], "comment": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)", "summary": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users."}
{"id": "2508.14706", "pdf": "https://arxiv.org/pdf/2508.14706", "abs": "https://arxiv.org/abs/2508.14706", "authors": ["Junying Chen", "Zhenyang Cai", "Zhiheng Liu", "Yunjin Yang", "Rongsheng Wang", "Qingying Xiao", "Xiangyi Feng", "Zhan Su", "Jing Guo", "Xiang Wan", "Guangjun Yu", "Haizhou Li", "Benyou Wang"], "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field."}
{"id": "2508.14838", "pdf": "https://arxiv.org/pdf/2508.14838", "abs": "https://arxiv.org/abs/2508.14838", "authors": ["Claude Tardif"], "title": "Constraint satisfaction problems, compactness and non-measurable sets", "categories": ["cs.LO", "03E65, 68Q19", "G.0"], "comment": "7 pages", "summary": "A finite relational structure A is called compact if for any infinite\nrelational structure B of the same type, the existence of a homomorphism from B\nto A is equivalent to the existence of homomorphisms from all finite\nsubstructures of B to A. We show that if A has width one, then the compactness\nof A can be proved in the axiom system of Zermelo and Fraenkel, but otherwise,\nthe compactness of A implies the existence of non-measurable sets in 3-space."}
{"id": "2508.14305", "pdf": "https://arxiv.org/pdf/2508.14305", "abs": "https://arxiv.org/abs/2508.14305", "authors": ["Terlumun Gbaden", "Mterorga Ukor", "Grace Erdoo Ateata"], "title": "Design and Simulation of Fault-Tolerant Network Switching System Using Python-Based Algorithms", "categories": ["cs.NI"], "comment": "8 pages, 6 figures", "summary": "Ensuring uninterrupted data flow in modern networks requires robust\nfault-tolerant mechanisms, especially in environments where reliability and\nresponsiveness are critical. This paper presents the design and simulation of a\nfault-tolerant network switching system using Python-based algorithms. A\nsimulated enterprise-level Local Area Network (LAN) was modeled using NetworkX\nto represent switch-router interconnectivity with redundant links. Fault\nscenarios, including link failure and congestion, were injected using Scapy,\nwhile automatic failover and rerouting were implemented via custom Python\nlogic. The system demonstrates resilience by dynamically detecting path\nfailures, redistributing network traffic through redundant links, and\nminimizing downtime. Performance evaluations reveal significant improvements in\npacket delivery continuity, faster recovery times, and reduced packet loss\ncompared to non-fault-tolerant baselines. The implementation provides a\nscalable and lightweight approach to integrating fault-tolerance features into\nmid-scale networks, with potential application in enterprise information\ntechnology infrastructures and academic simulations."}
{"id": "2508.14442", "pdf": "https://arxiv.org/pdf/2508.14442", "abs": "https://arxiv.org/abs/2508.14442", "authors": ["Haojun Zhuang", "Dünya Baradari", "Nataliya Kosmyna", "Arnav Balyan", "Constanze Albrecht", "Stephanie Chen", "Pattie Maes"], "title": "Detecting Reading-Induced Confusion Using EEG and Eye Tracking", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Humans regularly navigate an overwhelming amount of information via text\nmedia, whether reading articles, browsing social media, or interacting with\nchatbots. Confusion naturally arises when new information conflicts with or\nexceeds a reader's comprehension or prior knowledge, posing a challenge for\nlearning. In this study, we present a multimodal investigation of\nreading-induced confusion using EEG and eye tracking. We collected neural and\ngaze data from 11 adult participants as they read short paragraphs sampled from\ndiverse, real-world sources. By isolating the N400 event-related potential\n(ERP), a well-established neural marker of semantic incongruence, and\nintegrating behavioral markers from eye tracking, we provide a detailed\nanalysis of the neural and behavioral correlates of confusion during\nnaturalistic reading. Using machine learning, we show that multimodal (EEG +\neye tracking) models improve classification accuracy by 4-22% over unimodal\nbaselines, reaching an average weighted participant accuracy of 77.3% and a\nbest accuracy of 89.6%. Our results highlight the dominance of the brain's\ntemporal regions in these neural signatures of confusion, suggesting avenues\nfor wearable, low-electrode brain-computer interfaces (BCI) for real-time\nmonitoring. These findings lay the foundation for developing adaptive systems\nthat dynamically detect and respond to user confusion, with potential\napplications in personalized learning, human-computer interaction, and\naccessibility."}
{"id": "2508.14506", "pdf": "https://arxiv.org/pdf/2508.14506", "abs": "https://arxiv.org/abs/2508.14506", "authors": ["Hagit Attiya", "Antonio Fernández Anta", "Alessia Milani", "Alexandre Rapetti", "Corentin Travers"], "title": "Auditable Shared Objects: From Registers to Synchronization Primitives", "categories": ["cs.DC", "cs.DB", "cs.DS"], "comment": null, "summary": "Auditability allows to track operations performed on a shared object,\nrecording who accessed which information. This gives data owners more control\non their data. Initially studied in the context of single-writer registers,\nthis work extends the notion of auditability to other shared objects, and\nstudies their properties.\n  We start by moving from single-writer to multi-writer registers, and provide\nan implementation of an auditable $n$-writer $m$-reader read / write register,\nwith $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding\nregisters, which have consensus number $m+n$. We show that this consensus\nnumber is necessary. The implementation extends naturally to support an\nauditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a\nprimitive that supports efficient implementation of many shared objects.\nFinally, we relate auditable registers to other access control objects, by\nimplementing an anti-flickering deny list from auditable registers."}
{"id": "2508.14506", "pdf": "https://arxiv.org/pdf/2508.14506", "abs": "https://arxiv.org/abs/2508.14506", "authors": ["Hagit Attiya", "Antonio Fernández Anta", "Alessia Milani", "Alexandre Rapetti", "Corentin Travers"], "title": "Auditable Shared Objects: From Registers to Synchronization Primitives", "categories": ["cs.DC", "cs.DB", "cs.DS"], "comment": null, "summary": "Auditability allows to track operations performed on a shared object,\nrecording who accessed which information. This gives data owners more control\non their data. Initially studied in the context of single-writer registers,\nthis work extends the notion of auditability to other shared objects, and\nstudies their properties.\n  We start by moving from single-writer to multi-writer registers, and provide\nan implementation of an auditable $n$-writer $m$-reader read / write register,\nwith $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding\nregisters, which have consensus number $m+n$. We show that this consensus\nnumber is necessary. The implementation extends naturally to support an\nauditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a\nprimitive that supports efficient implementation of many shared objects.\nFinally, we relate auditable registers to other access control objects, by\nimplementing an anti-flickering deny list from auditable registers."}
{"id": "2508.14318", "pdf": "https://arxiv.org/pdf/2508.14318", "abs": "https://arxiv.org/abs/2508.14318", "authors": ["Esha Choukse", "Brijesh Warrier", "Scot Heath", "Luz Belmont", "April Zhao", "Hassan Ali Khan", "Brian Harry", "Matthew Kappel", "Russell J. Hewett", "Kushal Datta", "Yu Pei", "Caroline Lichtenberger", "John Siegler", "David Lukofsky", "Zaid Kahn", "Gurpreet Sahota", "Andy Sullivan", "Charles Frederick", "Hien Thai", "Rebecca Naughton", "Daniel Jurnove", "Justin Harp", "Reid Carper", "Nithish Mahalingam", "Srini Varkala", "Alok Gautam Kumbhare", "Satyajit Desai", "Venkatesh Ramamurthy", "Praneeth Gottumukkala", "Girish Bhatia", "Kelsey Wildstone", "Laurentiu Olariu", "Mohammed Ayna", "Mike Kendrick", "Ricardo Bianchini", "Aaron Hurst", "Reza Zamani", "Xin Li", "Gene Oden", "Rory Carmichael", "Tom Li", "Apoorv Gupta", "Nilesh Dattani", "Lawrence Marwong", "Rob Nertney", "Jeff Liott", "Miro Enev", "Divya Ramakrishnan", "Ian Buck", "Jonah Alben"], "title": "Power Stabilization for AI Training Datacenters", "categories": ["cs.AR", "cs.AI", "cs.DC"], "comment": null, "summary": "Large Artificial Intelligence (AI) training workloads spanning several tens\nof thousands of GPUs present unique power management challenges. These arise\ndue to the high variability in power consumption during the training. Given the\nsynchronous nature of these jobs, during every iteration there is a\ncomputation-heavy phase, where each GPU works on the local data, and a\ncommunication-heavy phase where all the GPUs synchronize on the data. Because\ncompute-heavy phases require much more power than communication phases, large\npower swings occur. The amplitude of these power swings is ever increasing with\nthe increase in the size of training jobs. An even bigger challenge arises from\nthe frequency spectrum of these power swings which, if harmonized with critical\nfrequencies of utilities, can cause physical damage to the power grid\ninfrastructure. Therefore, to continue scaling AI training workloads safely, we\nneed to stabilize the power of such workloads. This paper introduces the\nchallenge with production data and explores innovative solutions across the\nstack: software, GPU hardware, and datacenter infrastructure. We present the\npros and cons of each of these approaches and finally present a multi-pronged\napproach to solving the challenge. The proposed solutions are rigorously tested\nusing a combination of real hardware and Microsoft's in-house cloud power\nsimulator, providing critical insights into the efficacy of these interventions\nunder real-world conditions."}
{"id": "2508.14553", "pdf": "https://arxiv.org/pdf/2508.14553", "abs": "https://arxiv.org/abs/2508.14553", "authors": ["Dennis Schiese", "Aleksandr Perevalov", "Andreas Both"], "title": "Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems", "categories": ["cs.SE", "cs.AI"], "comment": "Presented at ICWI 2024, Zagreb. Released with ISBN:\n  978-989-8704-62-7. Data source:\n  https://figshare.com/articles/dataset/Towards_LLM-generated_explanations_for_component-based_knowledge_graph_question_answering_systems/27079687", "summary": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations."}
{"id": "2508.14851", "pdf": "https://arxiv.org/pdf/2508.14851", "abs": "https://arxiv.org/abs/2508.14851", "authors": ["Radosław Jan Rowicki", "Adrian Francalanza", "Alceste Scalas"], "title": "Correct Black-Box Monitors for Distributed Deadlock Detection: Formalisation and Implementation (Technical Report)", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "Many software applications rely on concurrent and distributed (micro)services\nthat interact via message-passing and various forms of remote procedure calls\n(RPC). As these systems organically evolve and grow in scale and complexity,\nthe risk of introducing deadlocks increases and their impact may worsen: even\nif only a few services deadlock, many other services may block while awaiting\nresponses from the deadlocked ones. As a result, the \"core\" of the deadlock can\nbe obfuscated by its consequences on the rest of the system, and diagnosing and\nfixing the problem can be challenging.\n  In this work we tackle the challenge by proposing distributed black-box\nmonitors that are deployed alongside each service and detect deadlocks by only\nobserving the incoming and outgoing messages, and exchanging probes with other\nmonitors. We present a formal model that captures popular RPC-based application\nstyles (e.g., gen_servers in Erlang/OTP), and a distributed black-box\nmonitoring algorithm that we prove sound and complete (i.e., identifies\ndeadlocked services with neither false positives nor false negatives). We\nimplement our results in a tool called DDMon for the monitoring of Erlang/OTP\napplications, and we evaluate its performance.\n  This is the first work that formalises, proves the correctness, and\nimplements distributed black-box monitors for deadlock detection. Our results\nare mechanised in Coq. DDMon is the companion artifact of this paper."}
{"id": "2508.14335", "pdf": "https://arxiv.org/pdf/2508.14335", "abs": "https://arxiv.org/abs/2508.14335", "authors": ["Hailong Su", "Jinshu Su", "Yusheng Xia", "Haibin Li"], "title": "The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations", "categories": ["cs.NI", "cs.SI"], "comment": null, "summary": "In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite\nconstellations has introduced unprecedented opportunities for global\nconnectivity. However, routing efficiency and inter-shell communication remain\nkey challenges in multi-shell architectures. This paper investigates the\nstructural properties and network dynamics of a representative six-shell\nmega-constellation composed of 10,956 satellites and 198 gateway stations\n(GSs). Leveraging tools from complex network analysis, we identify several\ncritical findings: (1) the constellation exhibits strong small-world\ncharacteristics, enabling efficient routing despite large network diameters;\n(2) GS relays play a pivotal role in enhancing inter-shell connectivity by\nbridging otherwise disconnected components; (3) feeder links significantly\nreduce average path length, making long-haul communication more feasible; (4)\nbetweenness analysis reveals load imbalances among GSs, indicating the need for\ntraffic-aware management strategies; (5) the architecture offers excellent\nspatial coverage and resilience, maintaining connectivity and low routing costs\neven under GS failures. These insights not only explain the design rationale\nbehind current mega-constellations like SpaceX Starlink, but also provide\nvaluable guidance for the evolution of future satellite network\ninfrastructures."}
{"id": "2508.14580", "pdf": "https://arxiv.org/pdf/2508.14580", "abs": "https://arxiv.org/abs/2508.14580", "authors": ["Huizhong Cao", "Henrik Söderlund", "Qi Fang", "Siyuan Chen", "Lejla Erdal", "Ammar Gubartalla", "Paulo Victor Lopes", "Guodong Shao", "Per Lonnehed", "Henri Putto", "Abbe Ahmed", "Sven Ekered", "Björn Johansson"], "title": "Towards AI-based Sustainable and XR-based human-centric manufacturing: Implementation of ISO 23247 for digital twins of production systems", "categories": ["cs.HC"], "comment": "Journal paper", "summary": "Since the introduction of Industry 4.0, digital twin technology has\nsignificantly evolved, laying the groundwork for a transition toward Industry\n5.0 principles centered on human-centricity, sustainability, and resilience.\nThrough digital twins, real-time connected production systems are anticipated\nto be more efficient, resilient, and sustainable, facilitating communication\nand connectivity between digital and physical systems. However, environmental\nperformance and integration with virtual reality (VR) and artificial\nintelligence (AI) of such systems remain challenging. Further exploration of\ndigital twin technologies is needed to validate the real-world impact and\nbenefits. This paper investigates these challenges by implementing a real-time\ndigital twin based on the ISO 23247 standard, connecting the physical factory\nand simulation software with VR capabilities. This digital twin system provides\ncognitive assistance and a user-friendly interface for operators, thereby\nimproving cognitive ergonomics. The connection of the Internet of Things (IoT)\nplatform allows the digital twin to have real-time bidirectional communication,\ncollaboration, monitoring, and assistance. A lab-scale drone factory was used\nas the digital twin application to test and evaluate the ISO 23247 standard and\nits potential benefits. Additionally, AI integration and environmental\nperformance Key Performance Indicators (KPIs) have been considered as the next\nstages in improving VR-integrated digital twins. With a solid theoretical\nfoundation and a demonstration of the VR-integrated digital twins, this paper\naddresses integration issues between various technologies and advances the\nframework of digital twins based on ISO 23247."}
{"id": "2508.14524", "pdf": "https://arxiv.org/pdf/2508.14524", "abs": "https://arxiv.org/abs/2508.14524", "authors": ["Krishnendu Chatterjee", "Jan Matyáš Křišťan", "Stefan Schmid", "Jakub Svoboda", "Michelle Yeo"], "title": "Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection", "categories": ["cs.DC", "cs.CR"], "comment": null, "summary": "Payment channel networks (PCNs) are a promising technology that alleviates\nblockchain scalability by shifting the transaction load from the blockchain to\nthe PCN. Nevertheless, the network topology has to be carefully designed to\nmaximise the transaction throughput in PCNs. Additionally, users in PCNs also\nhave to make optimal decisions on which transactions to forward and which to\nreject to prolong the lifetime of their channels. In this work, we consider an\ninput sequence of transactions over $p$ parties. Each transaction consists of a\ntransaction size, source, and target, and can be either accepted or rejected\n(entailing a cost). The goal is to design a PCN topology among the $p$\ncooperating parties, along with the channel capacities, and then output a\ndecision for each transaction in the sequence to minimise the cost of creating\nand augmenting channels, as well as the cost of rejecting transactions. Our\nmain contribution is an $\\mathcal{O}(p)$ approximation algorithm for the\nproblem with $p$ parties. We further show that with some assumptions on the\ndistribution of transactions, we can reduce the approximation ratio to\n$\\mathcal{O}(\\sqrt{p})$. We complement our theoretical analysis with an\nempirical study of our assumptions and approach in the context of the Lightning\nNetwork."}
{"id": "2508.14375", "pdf": "https://arxiv.org/pdf/2508.14375", "abs": "https://arxiv.org/abs/2508.14375", "authors": ["Choongseok Song", "Doo Seok Jeong"], "title": "Computing-In-Memory Dataflow for Minimal Buffer Traffic", "categories": ["cs.AR", "cs.AI"], "comment": "IEEE International Conference on Computer Design", "summary": "Computing-In-Memory (CIM) offers a potential solution to the memory wall\nissue and can achieve high energy efficiency by minimizing data movement,\nmaking it a promising architecture for edge AI devices. Lightweight models like\nMobileNet and EfficientNet, which utilize depthwise convolution for feature\nextraction, have been developed for these devices. However, CIM macros often\nface challenges in accelerating depthwise convolution, including\nunderutilization of CIM memory and heavy buffer traffic. The latter, in\nparticular, has been overlooked despite its significant impact on latency and\nenergy consumption. To address this, we introduce a novel CIM dataflow that\nsignificantly reduces buffer traffic by maximizing data reuse and improving\nmemory utilization during depthwise convolution. The proposed dataflow is\ngrounded in solid theoretical principles, fully demonstrated in this paper.\nWhen applied to MobileNet and EfficientNet models, our dataflow reduces buffer\ntraffic by 77.4-87.0%, leading to a total reduction in data traffic energy and\nlatency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline\n(conventional weight-stationary dataflow)."}
{"id": "2508.14631", "pdf": "https://arxiv.org/pdf/2508.14631", "abs": "https://arxiv.org/abs/2508.14631", "authors": ["Marcos Gomez-Vazquez", "Jordi Cabot"], "title": "Towards a DSL to Formalize Multimodal Requirements", "categories": ["cs.SE"], "comment": null, "summary": "Multimodal systems, which process multiple input types such as text, audio,\nand images, are becoming increasingly prevalent in software systems, enabled by\nthe huge advancements in Machine Learning. This triggers the need to easily\ndefine the requirements linked to these new types of user interactions,\npotentially involving more than one modality at the same time. This remains an\nopen challenge due to the lack of languages and methods adapted to the diverse\nnature of multimodal interactions, with the risk of implementing AI-enhanced\nsystems that do not properly satisfy the user needs.\n  In this sense, this paper presents MERLAN, a Domain-Specific Language (DSL)\nto specify the requirements for these new types of multimodal interfaces. We\npresent the metamodel for such language together with a textual syntax\nimplemented as an ANTLR grammar. A prototype tool enabling requirements\nengineers to write such requirements and automatically generate a possible\nimplementation of a system compliant with them on top of an agentic framework\nis also provided."}
{"id": "2508.14091", "pdf": "https://arxiv.org/pdf/2508.14091", "abs": "https://arxiv.org/abs/2508.14091", "authors": ["Matthew Morris", "David J. Tena Cucala", "Bernardo Cuenca Grau"], "title": "Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions", "categories": ["cs.LG", "cs.AI", "cs.LO", "03B70", "I.2.6; G.2.2; I.2.4; I.2.3"], "comment": "Full version (with appendices) of paper accepted to KR 2025 (22nd\n  International Conference on Principles of Knowledge Representation and\n  Reasoning)", "summary": "Graph neural networks (GNNs) are often used for the task of link prediction:\npredicting missing binary facts in knowledge graphs (KGs). To address the lack\nof explainability of GNNs on KGs, recent works extract Datalog rules from GNNs\nwith provable correspondence guarantees. The extracted rules can be used to\nexplain the GNN's predictions; furthermore, they can help characterise the\nexpressive power of various GNN models. However, these works address only a\nform of link prediction based on a restricted, low-expressivity graph\nencoding/decoding method. In this paper, we consider a more general and popular\napproach for link prediction where a scoring function is used to decode the GNN\noutput into fact predictions. We show how GNNs and scoring functions can be\nadapted to be monotonic, use the monotonicity to extract sound rules for\nexplaining predictions, and leverage existing results about the kind of rules\nthat scoring functions can capture. We also define procedures for obtaining\nequivalent Datalog programs for certain classes of monotonic GNNs with scoring\nfunctions. Our experiments show that, on link prediction benchmarks, monotonic\nGNNs and scoring functions perform well in practice and yield many sound rules."}
{"id": "2508.14435", "pdf": "https://arxiv.org/pdf/2508.14435", "abs": "https://arxiv.org/abs/2508.14435", "authors": ["Aqsa Sayeed", "Samaresh Bera"], "title": "Availability-Aware VNF Placement and Request Routing in MEC-Enabled 5G Networks", "categories": ["cs.NI"], "comment": "11 pages", "summary": "In this paper, we study the virtual network function (VNF) placement problem\nin mobile edge computing (MEC)-enabled 5G networks to meet the stringent\nreliability and latency requirements of uRLLC applications. We pose it as a\nconstrained optimization problem, which is NP-hard, to maximize the total\nreward obtained by a network service provider by serving uRLLC service\nrequests. We propose an approximated randomized rounding approach to solve the\nNP-hard optimization problem in polynomial time. We prove that the proposed\nrandomized approach achieves performance guarantees while violating the\nresource constraints boundedly. Furthermore, we present a greedy-heuristic\napproach to tackle the violations of resource constraints.\n  Simulation results show that the proposed randomized rounding and greedy\napproaches achieve a total reward which is within 5% and 10% of the optimal\nsolution, respectively. Furthermore, we compare the proposed greedy approach\nwith the existing schemes that do not consider the availability requirements.\nWe observe that the existing schemes perform poorly in terms of total reward,\nas negligence to the availability requirements negatively impacts the number of\nsuccessfully served requests. These findings highlight the trade-off between\navailability and resource efficiency in latency-sensitive uRLLC applications.\nWe also implement a software prototype of a 5G network using open-source\nsoftware platforms with redundant placement of VNFs. The results on packet\ndelivery ratio and latency obtained from the prototype implementation are also\nimproved in the redundant VNFs with different failure probabilities."}
{"id": "2508.14719", "pdf": "https://arxiv.org/pdf/2508.14719", "abs": "https://arxiv.org/abs/2508.14719", "authors": ["Mohit Sharma", "Emma Nilsson", "Martin Falk", "Talha Bin Masood", "Lee Jollans", "Anders Persson", "Tino Ebbers", "Ingrid Hotz"], "title": "Topology-Aware Volume Fusion for Spectral Computed Tomography via Histograms and Extremum Graph", "categories": ["cs.HC"], "comment": null, "summary": "Photon-Counting Computed Tomography (PCCT) is a novel imaging modality that\nsimultaneously acquires volumetric data at multiple X-ray energy levels,\ngenerating separate volumes that capture energy-dependent attenuation\nproperties. Attenuation refers to the reduction in X-ray intensity as it passes\nthrough different tissues or materials. This spectral information enhances\ntissue and material differentiation, enabling more accurate diagnosis and\nanalysis. However, the resulting multivolume datasets are often complex and\nredundant, making visualization and interpretation challenging. To address\nthese challenges, we propose a method for fusing spectral PCCT data into a\nsingle representative volume that enables direct volume rendering and\nsegmentation by leveraging both shared and complementary information across\ndifferent channels. Our approach starts by computing 2D histograms between\npairs of volumes to identify those that exhibit prominent structural features.\nThese histograms reveal relationships and variations that may be difficult to\ndiscern from individual volumes alone. Next, we construct an extremum graph\nfrom the 2D histogram of two minimally correlated yet complementary\nvolumes-selected to capture both shared and distinct features-thereby\nmaximizing the information content. The graph captures the topological\ndistribution of histogram extrema. By extracting prominent structure within\nthis graph and projecting each grid point in histogram space onto it, we reduce\nthe dimensionality to one, producing a unified volume. This representative\nvolume retains key structural and material characteristics from the original\nspectral data while significantly reducing the analysis scope from multiple\nvolumes to one. The result is a topology-aware, information-rich fusion of\nmulti-energy CT datasets that facilitates more effective visualization and\nsegmentation."}
{"id": "2508.14625", "pdf": "https://arxiv.org/pdf/2508.14625", "abs": "https://arxiv.org/abs/2508.14625", "authors": ["Kathleen West", "Youssef Moawad", "Fabian Lehmann", "Vasilis Bountris", "Ulf Leser", "Yehia Elkhatib", "Lauritz Thamsen"], "title": "A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows", "categories": ["cs.DC"], "comment": "This is a pre-print of our paper currently under review", "summary": "Scientific workflows are widely used to automate scientific data analysis and\noften involve computationally intensive processing of large datasets on compute\nclusters. As such, their execution tends to be long-running and\nresource-intensive, resulting in substantial energy consumption and, depending\non the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware\ncomputing methods have been proposed, yet little work has focused specifically\non scientific workflows, even though they present a substantial opportunity for\ncarbon-aware computing because they are often significantly delay tolerant,\nefficiently interruptible, highly scalable and widely heterogeneous. In this\nstudy, we first exemplify the problem of carbon emissions associated with\nrunning scientific workflows, and then show the potential for carbon-aware\nworkflow execution. For this, we estimate the carbon footprint of seven\nreal-world Nextflow workflows executed on different cluster infrastructures\nusing both average and marginal carbon intensity data. Furthermore, we\nsystematically evaluate the impact of carbon-aware temporal shifting, and the\npausing and resuming of the workflow. Moreover, we apply resource scaling to\nworkflows and workflow tasks. Finally, we report the potential reduction in\noverall carbon emissions, with temporal shifting capable of decreasing\nemissions by over 80%, and resource scaling capable of decreasing emissions by\n67%."}
{"id": "2508.14414", "pdf": "https://arxiv.org/pdf/2508.14414", "abs": "https://arxiv.org/abs/2508.14414", "authors": ["Ruiyang Ma", "Daikang Kuang", "Ziqian Liu", "Jiaxi Zhang", "Ping Fan", "Guojie Luo"], "title": "Wit-HW: Bug Localization in Hardware Design Code via Witness Test Case Generation", "categories": ["cs.AR"], "comment": "Accepted by ICCAD'2025", "summary": "Debugging hardware designs requires significant manual effort during hardware\ndevelopment. After engineers identify a bug-triggering test case in\nsimulation-based hardware verification, they usually spend considerable time\nanalyzing the execution trace to localize the bug. Although numerous automated\nhardware debugging techniques exist, they are not applicable to large designs\nand deep bugs. A primary reason for their limitations is that these techniques\nonly utilize the information of a single bug-triggering test case for bug\nlocalization, which prevents them from effectively analyzing intricate hardware\nsystems and figure out the root cause of bugs. To solve this problem, in this\npaper, we transform the hardware bug localization problem into a test\ngeneration problem, aiming to find a set of effective witness test cases beyond\nthe initial bug-triggering test case to enhance hardware bug localization.\nWitness test cases refer to the cases that do not trigger the bug in the faulty\ndesign. By analyzing the execution differences between passing and failing test\ncases with spectrum-based method, we can eliminate innocent design statements\nand localize the buggy ones. To further refine the suspicious area, we define\nthe criteria for effective witness test cases and use a mutation-based strategy\nto generate such test cases. Based on this approach, we propose an automated\nhardware bug localization framework named Wit-HW. We evaluate Wit-HW on 41 bugs\nfrom various hardware designs. The experimental results show that Wit-HW\neffectively localize 49%, 73%, 88% bugs within Top-1, Top-5, Top-10 ranks,\nsignificantly outperforming state-of-the-art bug localization techniques.\nAdditionally, we evaluate Wit-HW on 13 real-world bugs collected from\nopen-source hardware projects, showcasing the robust performance of our method."}
{"id": "2508.14727", "pdf": "https://arxiv.org/pdf/2508.14727", "abs": "https://arxiv.org/abs/2508.14727", "authors": ["Abbas Sabra", "Olivier Schmitt", "Joseph Tyler"], "title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment."}
{"id": "2508.14532", "pdf": "https://arxiv.org/pdf/2508.14532", "abs": "https://arxiv.org/abs/2508.14532", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Mingqi Yang", "Haokun Li", "Xiao Yi", "Shengchao Qin", "Jianwei Yin"], "title": "Preguss: It Analyzes, It Specifies, It Verifies", "categories": ["cs.SE", "cs.LO"], "comment": "Position paper to appear in the 1st International Workshop on\n  Language Models and Programming Languages (LMPL '25)", "summary": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs."}
{"id": "2508.14445", "pdf": "https://arxiv.org/pdf/2508.14445", "abs": "https://arxiv.org/abs/2508.14445", "authors": ["M. Umar Khan"], "title": "Transforming Next-generation Network Planning assisted by Data Acquisition of Top Three Spanish MNOs", "categories": ["cs.NI"], "comment": null, "summary": "In this paper, we address the necessity of data related to mobile traffic of\nthe legacy infrastructure to extract useful information and perform network\ndimensioning for 5G. These data can help us achieve a more efficient network\nplanning design, especially in terms of topology and cost. To that end, a real\nopen database of top three Spanish mobile network operators (MNOs) is used to\nestimate the traffic and to identify the area of highest user density for the\ndeployment of new services. We propose the data acquisition procedure described\nto clean the database, to extract meaningful traffic information and to\nvisualize traffic density patterns for new gNB deployments. We present the\nstate of the art in Network Data. We describe the considered network database\nin detail. The Network Data Acquisition entity along with the proposed\nprocedure is explained. The corresponding results are discussed, following the\nconclusions."}
{"id": "2508.14787", "pdf": "https://arxiv.org/pdf/2508.14787", "abs": "https://arxiv.org/abs/2508.14787", "authors": ["Natalia Kucirkova", "Alexis Hiniker", "Megumi Ishikawa", "Sho Tsuji", "Aayushi Dangol", "Robert Wolfe"], "title": "Challenges and Opportunities for Participatory Design of Conversational Agents for Young People's Wellbeing", "categories": ["cs.HC", "cs.CY"], "comment": "Presented at the AI4CW workshop at ACM IDC 2025", "summary": "This paper outlines the challenges and opportunities of research on\nconversational agents with children and young people across four countries,\nexploring the ways AI technologies can support children's well-being across\nsocial and cultural contexts."}
{"id": "2508.14716", "pdf": "https://arxiv.org/pdf/2508.14716", "abs": "https://arxiv.org/abs/2508.14716", "authors": ["Amores-Sesar Ignacio", "Grøndal Viktor", "Holmgård Adam", "Ottendal Mads"], "title": "DAG it off: Latency Prefers No Common Coins", "categories": ["cs.DC"], "comment": null, "summary": "We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based\nByzantine atomic broadcast protocol in a partially synchronous setting that\nsuccessfully forgoes the reliable broadcast and common coin primitives. Black\nMarlin achieves the optimal latency of 3 rounds of communication (4.25 with\nByzantine faults) while maintaining optimal communication and amortized\ncommunication complexities. We present a formal security analysis of the\nprotocol, accompanied by empirical evidence that Black Marlin outperforms\nstate-of-the-art DAG-based protocols in both throughput and latency."}
{"id": "2508.14582", "pdf": "https://arxiv.org/pdf/2508.14582", "abs": "https://arxiv.org/abs/2508.14582", "authors": ["Ryan Albert Antonio", "Joren Dumoulin", "Xiaoling Yi", "Josse Van Delm", "Yunhao Deng", "Guilherme Paim", "Marian Verhelst"], "title": "An Open-Source HW-SW Co-Development Framework Enabling Efficient Multi-Accelerator Systems", "categories": ["cs.AR", "cs.AI"], "comment": "7 pages, 10 figures, 1 table, to be published in ISLPED 2025", "summary": "Heterogeneous accelerator-centric compute clusters are emerging as efficient\nsolutions for diverse AI workloads. However, current integration strategies\noften compromise data movement efficiency and encounter compatibility issues in\nhardware and software. This prevents a unified approach that balances\nperformance and ease of use. To this end, we present SNAX, an open-source\nintegrated HW-SW framework enabling efficient multi-accelerator platforms\nthrough a novel hybrid-coupling scheme, consisting of loosely coupled\nasynchronous control and tightly coupled data access. SNAX brings reusable\nhardware modules designed to enhance compute accelerator utilization, and its\ncustomizable MLIR-based compiler to automate key system management tasks,\njointly enabling rapid development and deployment of customized\nmulti-accelerator compute clusters. Through extensive experimentation, we\ndemonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.\nAccelerators can easily be integrated and programmed to achieve > 10x\nimprovement in neural network performance compared to other accelerator systems\nwhile maintaining accelerator utilization of > 90% in full system operation."}
{"id": "2508.14747", "pdf": "https://arxiv.org/pdf/2508.14747", "abs": "https://arxiv.org/abs/2508.14747", "authors": ["Beatriz Cabrero-Daniel", "Mazen Mohamad"], "title": "Challenges of Virtual Validation and Verification for Automotive Functions", "categories": ["cs.SE"], "comment": "This work is supported by Sweden's innovation agency, Vinnova, under\n  Grant No. 2021-05043 entitled \"Enabling Virtual Validation and Verification\n  for ADAS and AD Features (EVIDENT).\"", "summary": "Verification and validation of vehicles is a complex yet critical process,\nparticularly for ensuring safety and coverage through simulations. However,\nachieving realistic and useful simulations comes with significant challenges.\nTo explore these challenges, we conducted a workshop with experts in the field,\nallowing them to brainstorm key obstacles. Following this, we distributed a\nsurvey to consolidate findings and gain further insights into potential\nsolutions. The experts identified 17 key challenges, along with proposed\nsolutions, an assessment of whether they represent next steps for research, and\nthe roadblocks to their implementation. While a lack of resources was not\ninitially highlighted as a major challenge, utilizing more resources emerged as\na critical necessity when experts discussed solutions. Interestingly, we\nexpected some of these challenges to have already been addressed or to have\nsystematic solutions readily available, given the collective expertise in the\nfield. Many of the identified problems already have known solutions, allowing\nus to shift focus towards unresolved challenges and share the next steps with\nthe broader community."}
{"id": "2508.14671", "pdf": "https://arxiv.org/pdf/2508.14671", "abs": "https://arxiv.org/abs/2508.14671", "authors": ["Miriam Backens", "Thomas Perez"], "title": "Inserting Planar-Measured Qubits into MBQC Patterns while Preserving Flow", "categories": ["quant-ph", "cs.LO"], "comment": "In Proceedings QPL 2025, arXiv:2508.13619", "summary": "In the one-way model of measurement-based quantum computation (MBQC),\ncomputation proceeds via single-qubit measurements on a resource state. Flow\nconditions ensure that the overall computation is deterministic in a suitable\nsense, and are required for efficient translation into quantum circuits.\nProcedures that rewrite MBQC patterns -- e.g. for optimisation, or adapting to\nhardware constraints -- thus need to preserve the existence of flow. Most\nprevious work has focused on rewrites that reduce the number of qubits in the\ncomputation, or that introduce new Pauli-measured qubits. Here, we consider the\ninsertion of planar-measured qubits into MBQC patterns, i.e. arbitrary\nmeasurements in a plane of the Bloch sphere spanned by a pair of Pauli\noperators; such measurements are necessary for universal MBQC. We extend the\ndefinition of causal flow, previously restricted to XY -measurements only, to\nalso permit YZ-measurements and derive the conditions under which a\nYZ-insertion preserves causal flow. Then we derive conditions for YZ-insertion\ninto patterns with gflow or Pauli flow, in which case the argument\nstraightforwardly extends to XZ-insertions as well. We also show that the\n'vertex splitting' or 'neighbour unfusion' rule previously used in the\nliterature can be derived from YZ-insertion and pivoting. This work contributes\nto understanding the broad properties of flow-preserving rewriting in MBQC and\nin the ZX-calculus more broadly, and it will enable more efficient\noptimisation, obfuscation, or routing."}
{"id": "2508.14471", "pdf": "https://arxiv.org/pdf/2508.14471", "abs": "https://arxiv.org/abs/2508.14471", "authors": ["Muhammad Z. Haq", "Nadia N. Qadri", "Omer Chughtai", "Sadiq A. Ahmad", "Waqas Khalid", "Heejung Yu"], "title": "Adaptive Network Selection for Latency-Aware V2X Systems under Varying Network and Vehicle Densities", "categories": ["cs.NI"], "comment": "Accepted for IEEE Access", "summary": "This paper presents ANS-V2X, an Adaptive Network Selection framework tailored\nfor latency-aware V2X systems operating under varying vehicle densities and\nheterogeneous network conditions. Modern vehicular environments demand\nlow-latency and high-throughput communication, yet real-time network selection\nis hindered by diverse application requirements and the coexistence of multiple\nRadio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X\nemploys a heuristic-driven approach to assign vehicles to networks by\nconsidering application sensitivity, latency, computational load, and\ndirectionality constraints. The framework is benchmarked against a\nMixed-Integer Linear Programming (MILP) formulation for optimal solutions and a\nQ-learning-based method representing reinforcement learning. Simulation results\ndemonstrate that ANS-V2X achieves near-optimal performance, typically within 5\nto 10% of the utility achieved by MILP-V2X, while reducing execution time by\nmore than 85%. Although MILP-V2X offers globally optimal results, its\ncomputation time often exceeds 100 milliseconds, making it unsuitable for\nreal-time applications. The Q-learning-based method is more adaptable but\nrequires extensive training and converges slowly in dynamic scenarios. In\ncontrast, ANS-V2X completes decisions in under 15 milliseconds and consistently\ndelivers lower latency than both alternatives. This confirms its suitability\nfor real-time, edge-level deployment in latency-critical V2X systems"}
{"id": "2508.14825", "pdf": "https://arxiv.org/pdf/2508.14825", "abs": "https://arxiv.org/abs/2508.14825", "authors": ["Lixiang Yan"], "title": "From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The role of Artificial Intelligence (AI) in education is undergoing a rapid\ntransformation, moving beyond its historical function as an instructional tool\ntowards a new potential as an active participant in the learning process. This\nshift is driven by the emergence of agentic AI, autonomous systems capable of\nproactive, goal-directed action. However, the field lacks a robust conceptual\nframework to understand, design, and evaluate this new paradigm of human-AI\ninteraction in learning. This paper addresses this gap by proposing a novel\nconceptual framework (the APCP framework) that charts the transition from AI as\na tool to AI as a collaborative partner. We present a four-level model of\nescalating AI agency within human-AI collaborative learning: (1) the AI as an\nAdaptive Instrument, (2) the AI as a Proactive Assistant, (3) the AI as a\nCo-Learner, and (4) the AI as a Peer Collaborator. Grounded in sociocultural\ntheories of learning and Computer-Supported Collaborative Learning (CSCL), this\nframework provides a structured vocabulary for analysing the shifting roles and\nresponsibilities between human and AI agents. The paper further engages in a\ncritical discussion of the philosophical underpinnings of collaboration,\nexamining whether an AI, lacking genuine consciousness or shared\nintentionality, can be considered a true collaborator. We conclude that while\nAI may not achieve authentic phenomenological partnership, it can be designed\nas a highly effective functional collaborator. This distinction has significant\nimplications for pedagogy, instructional design, and the future research agenda\nfor AI in education, urging a shift in focus towards creating learning\nenvironments that harness the complementary strengths of both human and AI."}
{"id": "2508.14830", "pdf": "https://arxiv.org/pdf/2508.14830", "abs": "https://arxiv.org/abs/2508.14830", "authors": ["Kushagra Agrawal", "Polat Goktas", "Anjan Bandopadhyay", "Debolina Ghosh", "Junali Jasmine Jena", "Mahendra Kumar Gourisaria"], "title": "MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems", "categories": ["cs.DC", "cs.GT", "cs.NE", "cs.NI"], "comment": null, "summary": "The rapid growth of Internet of Things (IoT) ecosystems has intensified the\nchallenge of efficiently allocating heterogeneous resources in highly dynamic,\ndistributed environments. Conventional centralized mechanisms and\nsingle-objective auction models, focusing solely on metrics such as cost\nminimization or revenue maximization, struggle to deliver balanced system\nperformance. This paper proposes the Multi-Objective Hierarchical Auction\nFramework (MOHAF), a distributed resource allocation mechanism that jointly\noptimizes cost, Quality of Service (QoS), energy efficiency, and fairness.\nMOHAF integrates hierarchical clustering to reduce computational complexity\nwith a greedy, submodular optimization strategy that guarantees a (1-1/e)\napproximation ratio. A dynamic pricing mechanism adapts in real time to\nresource utilization, enhancing market stability and allocation quality.\nExtensive experiments on the Google Cluster Data trace, comprising 3,553\nrequests and 888 resources, demonstrate MOHAF's superior allocation efficiency\n(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)\nauctions, while achieving perfect fairness (Jain's index = 1.000). Ablation\nstudies reveal the critical influence of cost and QoS components in sustaining\nbalanced multi-objective outcomes. With near-linear scalability, theoretical\nguarantees, and robust empirical performance, MOHAF offers a practical and\nadaptable solution for large-scale IoT deployments, effectively reconciling\nefficiency, equity, and sustainability in distributed resource coordination."}
{"id": "2508.14798", "pdf": "https://arxiv.org/pdf/2508.14798", "abs": "https://arxiv.org/abs/2508.14798", "authors": ["Soumyo Bhattacharjee", "Federico Villani", "Christian Vogt", "Andrea Cossettini", "Luca Benini"], "title": "ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems", "categories": ["cs.AR", "eess.SP"], "comment": "This work has been accepted for publication in IEEE IWASI Conference\n  proceedings. The final published version will be available via IEEE Xplore", "summary": "The demand for hundreds of tightly synchronized channels operating at tens of\nMSPS in ultrasound systems exceeds conventional low-voltage differential\nsignaling links' bandwidth, pin count, and latency. Although the JESD204B\nserial interface mitigates these limitations, commercial FPGA IP cores are\nproprietary, costly, and resource-intensive. We present ListenToJESD204B, an\nopen-source receiver IP core released under a permissive Solderpad 0.51 license\nfor AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable\nSystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides\ncycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It\noccupies only 107 configurable logic blocks (approximately 437 LUTs),\nrepresenting a 79\\% reduction compared to comparable commercially available IP.\nA modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC\ngeneration, and optional LFSR descrambling facilitates scaling to high lane\ncounts. We verified protocol compliance through simulation against the Xilinx\nJESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block\nstability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s\nlinks for 30 minutes with no errors."}
{"id": "2508.14394", "pdf": "https://arxiv.org/pdf/2508.14394", "abs": "https://arxiv.org/abs/2508.14394", "authors": ["Ryan Tjoa", "Poorva Garg", "Harrison Goldstein", "Todd Millstein", "Benjamin Pierce", "Guy Van den Broeck"], "title": "Tuning Random Generators: Property-Based Testing as Probabilistic Programming", "categories": ["cs.PL", "cs.SE", "D.3; D.2.5; G.3"], "comment": "Extended version of OOPSLA '25 paper", "summary": "Property-based testing validates software against an executable specification\nby evaluating it on randomly generated inputs. The standard way that PBT users\ngenerate test inputs is via generators that describe how to sample test inputs\nthrough random choices. To achieve a good distribution over test inputs, users\nmust tune their generators, i.e., decide on the weights of these individual\nrandom choices. Unfortunately, it is very difficult to understand how to choose\nindividual generator weights in order to achieve a desired distribution, so\ntoday this process is tedious and limits the distributions that can be\npractically achieved.\n  In this paper, we develop techniques for the automatic and offline tuning of\ngenerators. Given a generator with undetermined symbolic weights and an\nobjective function, our approach automatically learns values for these weights\nthat optimize for the objective. We describe useful objective functions that\nallow users to (1) target desired distributions and (2) improve the diversity\nand validity of their test cases. We have implemented our approach in a novel\ndiscrete probabilistic programming system, Loaded Dice, that supports\ndifferentiation and parameter learning, and use it as a language for\ngenerators. We empirically demonstrate that our approach is effective at\noptimizing generator distributions according to the specified objective\nfunctions. We also perform a thorough evaluation on PBT benchmarks,\ndemonstrating that, when automatically tuned for diversity and validity, the\ngenerators exhibit a 3.1-7.4x speedup in bug finding."}
{"id": "2508.14601", "pdf": "https://arxiv.org/pdf/2508.14601", "abs": "https://arxiv.org/abs/2508.14601", "authors": ["Yufei Ye", "Shijian Gao", "Xinhu Zheng", "Liuqing Yang"], "title": "Multi-Tier UAV Edge Computing for Low Altitude Networks Towards Long-Term Energy Stability", "categories": ["cs.NI"], "comment": null, "summary": "This paper presents a novel multi-tier UAV-assisted edge computing system\ndesigned for low-altitude networks. The system comprises vehicle users,\nlightweight Low-Tier UAVs (L-UAVs), and High-Tier UAV (H-UAV). L-UAVs function\nas small-scale edge servers positioned closer to vehicle users, while the\nH-UAV, equipped with more powerful server and larger-capacity battery, serves\nas mobile backup server to address the limitations in endurance and computing\nresources of L-UAVs. The primary objective is to minimize task execution delays\nwhile ensuring long-term energy stability for L-UAVs. To address this\nchallenge, the problem is first decoupled into a series of deterministic\nproblems for each time slot using Lyapunov optimization. The priorities of task\ndelay and energy consumption for L-UAVs are adaptively adjusted based on\nreal-time energy status. The optimization tasks include assignment of tasks,\nallocation of computing resources, and trajectory planning for both L-UAVs and\nH-UAV. Simulation results demonstrate that the proposed approach achieves a\nreduction of at least 26% in transmission energy for L-UAVs and exhibits\nsuperior energy stability compared to existing benchmarks."}
{"id": "2508.14060", "pdf": "https://arxiv.org/pdf/2508.14060", "abs": "https://arxiv.org/abs/2508.14060", "authors": ["Kartik Pandey", "Arun Balasubramanian", "Debasis Samanta"], "title": "Activity Coefficient-based Channel Selection for Electroencephalogram: A Task-Independent Approach", "categories": ["q-bio.NC", "cs.CV", "cs.HC", "cs.LG", "eess.SP"], "comment": null, "summary": "Electroencephalogram (EEG) signals have gained widespread adoption in\nbrain-computer interface (BCI) applications due to their non-invasive,\nlow-cost, and relatively simple acquisition process. The demand for higher\nspatial resolution, particularly in clinical settings, has led to the\ndevelopment of high-density electrode arrays. However, increasing the number of\nchannels introduces challenges such as cross-channel interference and\ncomputational overhead. To address these issues, modern BCI systems often\nemploy channel selection algorithms. Existing methods, however, are typically\ntask-specific and require re-optimization for each new application. This work\nproposes a task-agnostic channel selection method, Activity Coefficient-based\nChannel Selection (ACCS), which uses a novel metric called the Channel Activity\nCoefficient (CAC) to quantify channel utility based on activity levels. By\nselecting the top 16 channels ranked by CAC, ACCS achieves up to 34.97%\nimprovement in multi-class classification accuracy. Unlike traditional\napproaches, ACCS identifies a reusable set of informative channels independent\nof the downstream task or model, making it highly adaptable for diverse\nEEG-based applications."}
{"id": "2508.14848", "pdf": "https://arxiv.org/pdf/2508.14848", "abs": "https://arxiv.org/abs/2508.14848", "authors": ["Qiao Zhang", "Rabab Alomairy", "Dali Wang", "Zhuowei Gu", "Qinglei Cao"], "title": "Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach", "categories": ["cs.DC"], "comment": null, "summary": "General Matrix Multiplication (GEMM) is a critical operation underpinning a\nwide range of applications in high-performance computing (HPC) and artificial\nintelligence (AI). The emergence of hardware optimized for low-precision\narithmetic necessitates a reevaluation of numerical algorithms to leverage\nmixed-precision computations, achieving improved performance and energy\nefficiency. This research introduces an adaptive mixed-precision GEMM framework\nthat supports different precision formats at fine-grained tile/block levels. We\nutilize the PaRSEC runtime system to balance workloads across various\narchitectures. The performance scales well on ARM CPU-based Fugaku\nsupercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier\nsupercomputer. This research aims to enhance computational efficiency and\naccuracy by bridging algorithmic advancements and hardware innovations, driving\ntransformative progress in various applications."}
{"id": "2508.14533", "pdf": "https://arxiv.org/pdf/2508.14533", "abs": "https://arxiv.org/abs/2508.14533", "authors": ["Theodoros Trochatos", "Christopher Kang", "Andrew Wang", "Frederic T. Chong", "Jakub Szefer"], "title": "Trace-Based Reconstruction of Quantum Circuit Dataflow in Surface Codes", "categories": ["quant-ph", "cs.AR", "cs.ET", "cs.SE"], "comment": null, "summary": "Practical applications of quantum computing depend on fault-tolerant devices\nthat employ error correction. A promising quantum error-correcting code for\nlarge-scale quantum computing is the surface code. For this code,\nFault-Tolerant Quantum Computing (FTQC) can be performed via lattice surgery,\ni.e. merging and splitting of encoded qubit patches on a 2D grid. Lattice\nsurgery operations result in space-time patterns of activity that are defined\nin this work as access traces. This work demonstrates that the access traces\nreveal when, where, and how logical qubits interact. Leveraging this\nformulation, this work further introduces TraceQ, a trace-based reconstruction\nframework that is able to reconstruct the quantum circuit dataflow just by\nobserving the patch activity at each trace entry. The framework is supported by\nheuristics for handling inherent ambiguity in the traces, and demonstrates its\neffectiveness on a range of synthetic fault-tolerant quantum benchmarks. The\naccess traces can have applications in a wide range of scenarios, enabling\nanalysis and profiling of execution of quantum programs and the hardware they\nrun on. As one example use of TraceQ, this work investigates whether such\ntraces can act as a side channel through which an observer can recover the\ncircuit's structure and identify known subroutines in a larger program or even\nwhole programs. The findings show that indeed the minimal access traces can be\nused to recover subroutines or even whole quantum programs with very high\naccuracy. Only a single trace per program execution is needed and the\nprocessing can be done fully offline. Along with the custom heuristics,\nadvanced subgraph matching algorithms used in this work enable a high rate of\nlocating the subroutines while executing in minimal time."}
{"id": "2508.14533", "pdf": "https://arxiv.org/pdf/2508.14533", "abs": "https://arxiv.org/abs/2508.14533", "authors": ["Theodoros Trochatos", "Christopher Kang", "Andrew Wang", "Frederic T. Chong", "Jakub Szefer"], "title": "Trace-Based Reconstruction of Quantum Circuit Dataflow in Surface Codes", "categories": ["quant-ph", "cs.AR", "cs.ET", "cs.SE"], "comment": null, "summary": "Practical applications of quantum computing depend on fault-tolerant devices\nthat employ error correction. A promising quantum error-correcting code for\nlarge-scale quantum computing is the surface code. For this code,\nFault-Tolerant Quantum Computing (FTQC) can be performed via lattice surgery,\ni.e. merging and splitting of encoded qubit patches on a 2D grid. Lattice\nsurgery operations result in space-time patterns of activity that are defined\nin this work as access traces. This work demonstrates that the access traces\nreveal when, where, and how logical qubits interact. Leveraging this\nformulation, this work further introduces TraceQ, a trace-based reconstruction\nframework that is able to reconstruct the quantum circuit dataflow just by\nobserving the patch activity at each trace entry. The framework is supported by\nheuristics for handling inherent ambiguity in the traces, and demonstrates its\neffectiveness on a range of synthetic fault-tolerant quantum benchmarks. The\naccess traces can have applications in a wide range of scenarios, enabling\nanalysis and profiling of execution of quantum programs and the hardware they\nrun on. As one example use of TraceQ, this work investigates whether such\ntraces can act as a side channel through which an observer can recover the\ncircuit's structure and identify known subroutines in a larger program or even\nwhole programs. The findings show that indeed the minimal access traces can be\nused to recover subroutines or even whole quantum programs with very high\naccuracy. Only a single trace per program execution is needed and the\nprocessing can be done fully offline. Along with the custom heuristics,\nadvanced subgraph matching algorithms used in this work enable a high rate of\nlocating the subroutines while executing in minimal time."}
{"id": "2508.14676", "pdf": "https://arxiv.org/pdf/2508.14676", "abs": "https://arxiv.org/abs/2508.14676", "authors": ["Parham Soltani", "Mehrshad Eskandarpour", "Sina Heidari", "Farnaz Alizadeh", "Hossein Soleimani"], "title": "Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach", "categories": ["cs.NI"], "comment": null, "summary": "Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of\nthe target area, network size, and sensor coverage to determine initial\ndeployment. This often results in significant overlap to ensure continued\nnetwork operation despite sensor energy depletion. With the emergence of Mobile\nWireless Sensor Networks (MWSNs), issues such as sensor failure and static\ncoverage limitations can be more effectively addressed through mobility. This\npaper proposes a novel deployment strategy in which mobile sensors autonomously\nposition themselves to maximize area coverage, eliminating the need for\npredefined policies. A live camera system, combined with deep reinforcement\nlearning (DRL), monitors the network by detecting sensor LED indicators and\nevaluating real-time coverage. Rewards based on coverage efficiency and sensor\nmovement are computed at each learning step and shared across the network\nthrough a Multi-Agent Reinforcement Learning (MARL) framework, enabling\ndecentralized, cooperative sensor control. Key contributions include a\nvision-based, low-cost coverage evaluation method; a scalable MARL-DRL\nframework for autonomous deployment; and a self-reconfigurable system that\nadjusts sensor positioning in response to energy depletion. Compared to\ntraditional distance-based localization, the proposed method achieves a 26.5%\nimprovement in coverage, a 32% reduction in energy consumption, and a 22%\ndecrease in redundancy, extending network lifetime by 45%. This approach\nsignificantly enhances adaptability, energy efficiency, and robustness in\nMWSNs, offering a practical deployment solution within the IoT framework."}
{"id": "2508.14113", "pdf": "https://arxiv.org/pdf/2508.14113", "abs": "https://arxiv.org/abs/2508.14113", "authors": ["Vinit Hegiste", "Vidit Goyal", "Tatjana Legler", "Martin Ruskowski"], "title": "Federated Action Recognition for Smart Worker Assistance Using FastPose", "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.HC"], "comment": "8 pages and submitted to FLTA2025 conference", "summary": "In smart manufacturing environments, accurate and real-time recognition of\nworker actions is essential for productivity, safety, and human-machine\ncollaboration. While skeleton-based human activity recognition (HAR) offers\nrobustness to lighting, viewpoint, and background variations, most existing\napproaches rely on centralized datasets, which are impractical in\nprivacy-sensitive industrial scenarios. This paper presents a federated\nlearning (FL) framework for pose-based HAR using a custom skeletal dataset of\neight industrially relevant upper-body gestures, captured from five\nparticipants and processed using a modified FastPose model. Two temporal\nbackbones, an LSTM and a Transformer encoder, are trained and evaluated under\nfour paradigms: centralized, local (per-client), FL with weighted federated\naveraging (FedAvg), and federated ensemble learning (FedEnsemble). On the\nglobal test set, the FL Transformer improves over centralized training by +12.4\npercentage points, with FedEnsemble delivering a +16.3 percentage points gain.\nOn an unseen external client, FL and FedEnsemble exceed centralized accuracy by\n+52.6 and +58.3 percentage points, respectively. These results demonstrate that\nFL not only preserves privacy but also substantially enhances cross-user\ngeneralization, establishing it as a practical solution for scalable,\nprivacy-aware HAR in heterogeneous industrial settings."}
{"id": "2508.14883", "pdf": "https://arxiv.org/pdf/2508.14883", "abs": "https://arxiv.org/abs/2508.14883", "authors": ["Benedikt Pittl", "Werner Mach", "Erich Schikuta"], "title": "The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace", "categories": ["cs.DC", "cs.GT", "91-08", "J.1; H.1.m"], "comment": null, "summary": "In recent years, cloud providers have introduced novel approaches for trading\nvirtual machines. For example, Virtustream introduced so-called muVMs to charge\ncloud computing resources while other providers such as Google, Microsoft, or\nAmazon re-invented their marketspaces. Today, the market leader Amazon runs six\nmarketspaces for trading virtual machines. Consumers can purchase bundles of\nvirtual machines, which are called cloud-portfolios, from multiple marketspaces\nand providers. An industry-relevant field of research is to identify best\npractices and guidelines on how such optimal portfolios are created. In the\npaper at hand, a cost analysis of cloud portfolios is presented. Therefore,\npricing data from Amazon was used as well as a real virtual machine utilization\ndataset from the Bitbrains datacenter. The results show that a cost optimum can\nonly be reached if heterogeneous portfolios are created where virtual machines\nare purchased from different marketspaces. Additionally, the cost-benefit of\nmigrating virtual machines to different marketplaces during runtime is\npresented. Such migrations are especially cost-effective for virtual machines\nof cloud-portfolios which run between 6 hours and 1 year. The paper further\nshows that most of the resources of virtual machines are never utilized by\nconsumers, which represents a significant future potential for cost\noptimization. For the validation of the results, a second dataset of the\nBitbrains datacenter was used, which contains utility data of virtual machines\nfrom a different domain of application."}
{"id": "2508.14796", "pdf": "https://arxiv.org/pdf/2508.14796", "abs": "https://arxiv.org/abs/2508.14796", "authors": ["James C Davis", "Sophie Chen", "Huiyun Peng", "Paschal C Amusuo", "Kelechi G Kalu"], "title": "A Guide to Stakeholder Analysis for Cybersecurity Researchers", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Stakeholder-based ethics analysis is now a formal requirement for submissions\nto top cybersecurity research venues. This requirement reflects a growing\nconsensus that cybersecurity researchers must go beyond providing capabilities\nto anticipating and mitigating the potential harms thereof. However, many\ncybersecurity researchers may be uncertain about how to proceed in an ethics\nanalysis. In this guide, we provide practical support for that requirement by\nenumerating stakeholder types and mapping them to common empirical research\nmethods. We also offer worked examples to demonstrate how researchers can\nidentify likely stakeholder exposures in real-world projects. Our goal is to\nhelp research teams meet new ethics mandates with confidence and clarity, not\nconfusion."}
{"id": "2508.14679", "pdf": "https://arxiv.org/pdf/2508.14679", "abs": "https://arxiv.org/abs/2508.14679", "authors": ["Parham Soltani", "Mehrshad Eskandarpour", "Amir Ahmadizad", "Hossein Soleimani"], "title": "Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach", "categories": ["cs.NI"], "comment": null, "summary": "Efficient energy management is essential in Wireless Sensor Networks (WSNs)\nto extend network lifetime and ensure reliable data transmission. This paper\npresents a novel method using reinforcement learning-based cluster-head\nselection and a hybrid multi-hop routing algorithm, which leverages Q-learning\nwithin a multi-agent system to dynamically adapt transmission paths based on\nthe energy distribution across sensor nodes. Each sensor node is modeled as an\nautonomous agent that observes local state parameters, such as residual energy,\ndistance to sink, hop count, and hotspot proximity, and selects routing actions\nthat maximize long-term energy efficiency. After computing the optimal paths,\neach sensor aggregates sensed data and forwards it through intermediate nodes\nto a selected transmitter node, chosen based on the highest remaining State of\nCharge (SoC), thereby avoiding premature node depletion. To promote efficient\nlearning, a carefully designed reward function incentivizes balanced load\ndistribution, hotspot avoidance, and energy-aware forwarding while maintaining\nsignal quality. The learning process occurs either in a decentralized manner or\nvia a cloud-based controller that offloads computation in large-scale\ndeployments. Moreover, the RL-driven routing decisions are fused with classical\ngraph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum\nSpanning Tree (MST), to optimize energy consumption and load balancing.\nSimulations confirm that the proposed approach significantly improves node\nsurvival rate, reduces SoC variance, and enhances network resilience, making it\na scalable and adaptive solution for energy-constrained WSNs in dynamic sensor\ndeployments and IoT applications."}
{"id": "2508.14119", "pdf": "https://arxiv.org/pdf/2508.14119", "abs": "https://arxiv.org/abs/2508.14119", "authors": ["Mackenzie Jorgensen", "Kendall Brogle", "Katherine M. Collins", "Lujain Ibrahim", "Arina Shah", "Petra Ivanovic", "Noah Broestl", "Gabriel Piles", "Paul Dongha", "Hatim Abdulhussein", "Adrian Weller", "Jillian Powers", "Umang Bhatt"], "title": "Documenting Deployment with Fabric: A Repository of Real-World AI Governance", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "AIES 2025", "summary": "Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance."}
{"id": "2508.14113", "pdf": "https://arxiv.org/pdf/2508.14113", "abs": "https://arxiv.org/abs/2508.14113", "authors": ["Vinit Hegiste", "Vidit Goyal", "Tatjana Legler", "Martin Ruskowski"], "title": "Federated Action Recognition for Smart Worker Assistance Using FastPose", "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.HC"], "comment": "8 pages and submitted to FLTA2025 conference", "summary": "In smart manufacturing environments, accurate and real-time recognition of\nworker actions is essential for productivity, safety, and human-machine\ncollaboration. While skeleton-based human activity recognition (HAR) offers\nrobustness to lighting, viewpoint, and background variations, most existing\napproaches rely on centralized datasets, which are impractical in\nprivacy-sensitive industrial scenarios. This paper presents a federated\nlearning (FL) framework for pose-based HAR using a custom skeletal dataset of\neight industrially relevant upper-body gestures, captured from five\nparticipants and processed using a modified FastPose model. Two temporal\nbackbones, an LSTM and a Transformer encoder, are trained and evaluated under\nfour paradigms: centralized, local (per-client), FL with weighted federated\naveraging (FedAvg), and federated ensemble learning (FedEnsemble). On the\nglobal test set, the FL Transformer improves over centralized training by +12.4\npercentage points, with FedEnsemble delivering a +16.3 percentage points gain.\nOn an unseen external client, FL and FedEnsemble exceed centralized accuracy by\n+52.6 and +58.3 percentage points, respectively. These results demonstrate that\nFL not only preserves privacy but also substantially enhances cross-user\ngeneralization, establishing it as a practical solution for scalable,\nprivacy-aware HAR in heterogeneous industrial settings."}
{"id": "2508.14300", "pdf": "https://arxiv.org/pdf/2508.14300", "abs": "https://arxiv.org/abs/2508.14300", "authors": ["Youssef Maklad", "Fares Wael", "Ali Hamdi", "Wael Elsersy", "Khaled Shaban"], "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "categories": ["cs.CR", "cs.CL", "cs.MA", "cs.NI"], "comment": null, "summary": "Traditional protocol fuzzing techniques, such as those employed by AFL-based\nsystems, often lack effectiveness due to a limited semantic understanding of\ncomplex protocol grammars and rigid seed mutation strategies. Recent works,\nsuch as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol\nfuzzing and address these limitations, pushing protocol fuzzers to wider\nexploration of the protocol state space. But ChatAFL still faces issues like\nunreliable output, LLM hallucinations, and assumptions of LLM knowledge about\nprotocol specifications. This paper introduces MultiFuzz, a novel dense\nretrieval-based multi-agent system designed to overcome these limitations by\nintegrating semantic-aware context retrieval, specialized agents, and\nstructured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of\nprotocol documentation (RFC Documents) to build embeddings in a vector database\nfor a retrieval-augmented generation (RAG) pipeline, enabling agents to\ngenerate more reliable and structured outputs, enhancing the fuzzer in mutating\nprotocol messages with enhanced state coverage and adherence to syntactic\nconstraints. The framework decomposes the fuzzing process into modular groups\nof agents that collaborate through chain-of-thought reasoning to dynamically\nadapt fuzzing strategies based on the retrieved contextual knowledge.\nExperimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate\nthat MultiFuzz significantly improves branch coverage and explores deeper\nprotocol states and transitions over state-of-the-art (SOTA) fuzzers such as\nNSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic\ncoordination, and language model reasoning, MultiFuzz establishes a new\nparadigm in autonomous protocol fuzzing, offering a scalable and extensible\nfoundation for future research in intelligent agentic-based fuzzing systems."}
{"id": "2508.14564", "pdf": "https://arxiv.org/pdf/2508.14564", "abs": "https://arxiv.org/abs/2508.14564", "authors": ["Luca Annese", "Sabrina Patania", "Silvia Serino", "Tom Foulsham", "Silvia Rossi", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "categories": ["cs.AI", "cs.CL", "cs.HC", "I.2.9; I.2.10; I.2.7; J.4"], "comment": "Accepted at ICSR25", "summary": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents."}
{"id": "2508.14209", "pdf": "https://arxiv.org/pdf/2508.14209", "abs": "https://arxiv.org/abs/2508.14209", "authors": ["Andrew J. Higgins", "Erik G. Boman", "Ichitaro Yamazaki"], "title": "A High Performance GPU CountSketch Implementation and Its Application to Multisketching and Least Squares Problems", "categories": ["math.NA", "cs.DC", "cs.NA", "cs.PF"], "comment": "8 pages", "summary": "Random sketching is a dimensionality reduction technique that approximately\npreserves norms and singular values up to some $O(1)$ distortion factor with\nhigh probability. The most popular sketches in literature are the Gaussian\nsketch and the subsampled randomized Hadamard transform, while the CountSketch\nhas lower complexity. Combining two sketches, known as multisketching, offers\nan inexpensive means of quickly reducing the dimension of a matrix by combining\na CountSketch and Gaussian sketch.\n  However, there has been little investigation into high performance\nCountSketch implementations. In this work, we develop an efficient GPU\nimplementation of the CountSketch, and demonstrate the performance of\nmultisketching using this technique. We also demonstrate the potential for\nusing this implementation within a multisketched least squares solver that is\nup to $77\\%$ faster than the normal equations with significantly better\nnumerical stability, at the cost of an $O(1)$ multiplicative factor introduced\ninto the relative residual norm."}
{"id": "2508.14526", "pdf": "https://arxiv.org/pdf/2508.14526", "abs": "https://arxiv.org/abs/2508.14526", "authors": ["Stefan Lenz", "David Schachtschneider", "Simon Jonas", "Liam Tirpitz", "Sandra Geisler", "Martin Henze"], "title": "CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production", "categories": ["cs.CR", "cs.NI"], "comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)", "summary": "While the digitization of industrial factories provides tremendous\nimprovements for the production of goods, it also renders such systems\nvulnerable to serious cyber-attacks. To research, test, and validate security\nmeasures protecting industrial networks against such cyber-attacks, the\nsecurity community relies on testbeds to simulate industrial systems, as\nutilizing live systems endangers costly components or even human life. However,\nexisting testbeds focus on individual parts of typically complex production\nlines in industrial factories. Consequently, the impact of cyber-attacks on\nindustrial networks as well as the effectiveness of countermeasures cannot be\nevaluated in an end-to-end manner. To address this issue and facilitate\nresearch on novel security mechanisms, we present CoFacS, the first COmplete\nFACtory Simulation that replicates an entire production line and affords the\nintegration of real-life industrial applications. To showcase that CoFacS\naccurately captures real-world behavior, we validate it against a physical\nmodel factory widely used in security research. We show that CoFacS has a\nmaximum deviation of 0.11% to the physical reference, which enables us to study\nthe impact of physical attacks or network-based cyber-attacks. Moreover, we\nhighlight how CoFacS enables security research through two cases studies\nsurrounding attack detection and the resilience of 5G-based industrial\ncommunication against jamming."}
{"id": "2508.14688", "pdf": "https://arxiv.org/pdf/2508.14688", "abs": "https://arxiv.org/abs/2508.14688", "authors": ["Veronica Ruozzi", "Sasan Matinfar", "Laura Schütz", "Benedikt Wiestler", "Alberto Redaelli", "Emiliano Votta", "Nassir Navab"], "title": "BioSonix: Can Physics-Based Sonification Perceptualize Tissue Deformations From Tool Interactions?", "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": "V. Ruozzi and S. Matinfar contributed equally to this work", "summary": "Perceptualizing tool interactions with deformable structures in surgical\nprocedures remains challenging, as unimodal visualization techniques often fail\nto capture the complexity of these interactions due to constraints such as\nocclusion and limited depth perception. This paper presents a novel approach to\naugment tool navigation in mixed reality environments by providing auditory\nrepresentations of tool-tissue dynamics, particularly for interactions with\nsoft tissue. BioSonix, a physics-informed design framework, utilizes tissue\ndisplacements in 3D space to compute excitation forces for a sound model\nencoding tissue properties such as stiffness and density. Biomechanical\nsimulations were employed to model particle displacements resulting from\ntool-tissue interactions, establishing a robust foundation for the method. An\noptimization approach was used to define configurations for capturing diverse\ninteraction scenarios with varying tool trajectories. Experiments were\nconducted to validate the accuracy of the sound-displacement mappings.\nAdditionally, two user studies were performed: the first involved two clinical\nprofessionals (a neuroradiologist and a cardiologist), who confirmed the\nmethod's impact and achieved high task accuracy; the second included 22\nbiomedical experts, who demonstrated high discrimination accuracy in tissue\ndifferentiation and targeting tasks. The results revealed a strong correlation\nbetween tool-tissue dynamics and their corresponding auditory profiles,\nhighlighting the potential of these sound representations to enhance the\nintuitive understanding of complex interactions."}
{"id": "2508.14318", "pdf": "https://arxiv.org/pdf/2508.14318", "abs": "https://arxiv.org/abs/2508.14318", "authors": ["Esha Choukse", "Brijesh Warrier", "Scot Heath", "Luz Belmont", "April Zhao", "Hassan Ali Khan", "Brian Harry", "Matthew Kappel", "Russell J. Hewett", "Kushal Datta", "Yu Pei", "Caroline Lichtenberger", "John Siegler", "David Lukofsky", "Zaid Kahn", "Gurpreet Sahota", "Andy Sullivan", "Charles Frederick", "Hien Thai", "Rebecca Naughton", "Daniel Jurnove", "Justin Harp", "Reid Carper", "Nithish Mahalingam", "Srini Varkala", "Alok Gautam Kumbhare", "Satyajit Desai", "Venkatesh Ramamurthy", "Praneeth Gottumukkala", "Girish Bhatia", "Kelsey Wildstone", "Laurentiu Olariu", "Mohammed Ayna", "Mike Kendrick", "Ricardo Bianchini", "Aaron Hurst", "Reza Zamani", "Xin Li", "Gene Oden", "Rory Carmichael", "Tom Li", "Apoorv Gupta", "Nilesh Dattani", "Lawrence Marwong", "Rob Nertney", "Jeff Liott", "Miro Enev", "Divya Ramakrishnan", "Ian Buck", "Jonah Alben"], "title": "Power Stabilization for AI Training Datacenters", "categories": ["cs.AR", "cs.AI", "cs.DC"], "comment": null, "summary": "Large Artificial Intelligence (AI) training workloads spanning several tens\nof thousands of GPUs present unique power management challenges. These arise\ndue to the high variability in power consumption during the training. Given the\nsynchronous nature of these jobs, during every iteration there is a\ncomputation-heavy phase, where each GPU works on the local data, and a\ncommunication-heavy phase where all the GPUs synchronize on the data. Because\ncompute-heavy phases require much more power than communication phases, large\npower swings occur. The amplitude of these power swings is ever increasing with\nthe increase in the size of training jobs. An even bigger challenge arises from\nthe frequency spectrum of these power swings which, if harmonized with critical\nfrequencies of utilities, can cause physical damage to the power grid\ninfrastructure. Therefore, to continue scaling AI training workloads safely, we\nneed to stabilize the power of such workloads. This paper introduces the\nchallenge with production data and explores innovative solutions across the\nstack: software, GPU hardware, and datacenter infrastructure. We present the\npros and cons of each of these approaches and finally present a multi-pronged\napproach to solving the challenge. The proposed solutions are rigorously tested\nusing a combination of real hardware and Microsoft's in-house cloud power\nsimulator, providing critical insights into the efficacy of these interventions\nunder real-world conditions."}
{"id": "2508.14575", "pdf": "https://arxiv.org/pdf/2508.14575", "abs": "https://arxiv.org/abs/2508.14575", "authors": ["Shuying Gan", "Xijun Wang", "Chao Xu", "Xiang Chen"], "title": "Minimizing Task-Oriented Age of Information for Remote Monitoring with Pre-Identification", "categories": ["cs.IT", "cs.NI", "math.IT"], "comment": "This work has been submitted to the lEEE for possible publication", "summary": "The emergence of new intelligent applications has fostered the development of\na task-oriented communication paradigm, where a comprehensive, universal, and\npractical metric is crucial for unleashing the potential of this paradigm. To\nthis end, we introduce an innovative metric, the Task-oriented Age of\nInformation (TAoI), to measure whether the content of information is relevant\nto the system task, thereby assisting the system in efficiently completing\ndesignated tasks. We apply TAoI to a wireless monitoring system tasked with\nidentifying targets and transmitting their images for subsequent analysis. To\nminimize TAoI and determine the optimal transmission policy, we formulate the\ndynamic transmission problem as a Semi-Markov Decision Process (SMDP) and\ntransform it into an equivalent Markov Decision Process (MDP). Our analysis\ndemonstrates that the optimal policy is threshold-based with respect to TAoI.\nBuilding on this, we propose a low-complexity relative value iteration\nalgorithm tailored to this threshold structure to derive the optimal\ntransmission policy. Additionally, we introduce a simpler single-threshold\npolicy, which, despite a slight performance degradation, offers faster\nconvergence. Comprehensive experiments and simulations validate the superior\nperformance of our optimal transmission policy compared to two established\nbaseline approaches."}
{"id": "2508.14339", "pdf": "https://arxiv.org/pdf/2508.14339", "abs": "https://arxiv.org/abs/2508.14339", "authors": ["Domantas Dilys", "Hamish Carr", "Steven Boeing"], "title": "Lagrangian Simulation Volume-Based Contour Tree Simplification", "categories": ["cs.CG", "cs.DC", "cs.DS"], "comment": "10 Pages. To be published at the IEEE Workshop on Topological Data\n  Analysis and Visualization (TopoInVis) in conjunction with IEEE VIS 2025,\n  Vienna, Austria", "summary": "Many scientific and engineering problems are modelled by simulating scalar\nfields defined either on space-filling meshes (Eulerian) or as particles\n(Lagrangian). For analysis and visualization, topological primitives such as\ncontour trees can be used, but these often need simplification to filter out\nsmall-scale features. For parcel-based convective cloud simulations,\nsimplification of the contour tree requires a volumetric measure rather than\npersistence. Unlike for cubic meshes, volume cannot be approximated by counting\nregular vertices. Typically, this is addressed by resampling irregular data\nonto a uniform grid. Unfortunately, the spatial proximity of parcels requires a\nhigh sampling frequency, resulting in a massive increase in data size for\nprocessing. We therefore extend volume-based contour tree simplification to\nparcel-in-cell simulations with a graph adaptor in Viskores (VTK-m), using\nDelaunay tetrahedralization of the parcel centroids as input. Instead of\nrelying on a volume approximation by counting regular vertices -- as was done\nfor cubic meshes -- we adapt the 2D area splines reported by Bajaj et al.\n10.1145/259081.259279, and Zhou et al. 10.1109/TVCG.2018.2796555. We implement\nthis in Viskores (formerly called VTK-m) as prefix-sum style hypersweeps for\nparallel efficiency and show how it can be generalized to compute any\nintegrable property. Finally, our results reveal that contour trees computed\ndirectly on the parcels are orders of magnitude faster than computing them on a\nresampled grid, while also arguably offering better quality segmentation,\navoiding interpolation artifacts."}
{"id": "2508.14830", "pdf": "https://arxiv.org/pdf/2508.14830", "abs": "https://arxiv.org/abs/2508.14830", "authors": ["Kushagra Agrawal", "Polat Goktas", "Anjan Bandopadhyay", "Debolina Ghosh", "Junali Jasmine Jena", "Mahendra Kumar Gourisaria"], "title": "MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems", "categories": ["cs.DC", "cs.GT", "cs.NE", "cs.NI"], "comment": null, "summary": "The rapid growth of Internet of Things (IoT) ecosystems has intensified the\nchallenge of efficiently allocating heterogeneous resources in highly dynamic,\ndistributed environments. Conventional centralized mechanisms and\nsingle-objective auction models, focusing solely on metrics such as cost\nminimization or revenue maximization, struggle to deliver balanced system\nperformance. This paper proposes the Multi-Objective Hierarchical Auction\nFramework (MOHAF), a distributed resource allocation mechanism that jointly\noptimizes cost, Quality of Service (QoS), energy efficiency, and fairness.\nMOHAF integrates hierarchical clustering to reduce computational complexity\nwith a greedy, submodular optimization strategy that guarantees a (1-1/e)\napproximation ratio. A dynamic pricing mechanism adapts in real time to\nresource utilization, enhancing market stability and allocation quality.\nExtensive experiments on the Google Cluster Data trace, comprising 3,553\nrequests and 888 resources, demonstrate MOHAF's superior allocation efficiency\n(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)\nauctions, while achieving perfect fairness (Jain's index = 1.000). Ablation\nstudies reveal the critical influence of cost and QoS components in sustaining\nbalanced multi-objective outcomes. With near-linear scalability, theoretical\nguarantees, and robust empirical performance, MOHAF offers a practical and\nadaptable solution for large-scale IoT deployments, effectively reconciling\nefficiency, equity, and sustainability in distributed resource coordination."}
{"id": "2508.14539", "pdf": "https://arxiv.org/pdf/2508.14539", "abs": "https://arxiv.org/abs/2508.14539", "authors": ["Tao Shen", "Zexi Li", "Didi Zhu", "Ziyu Zhao", "Chao Wu", "Fei Wu"], "title": "FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning (FL) is a machine learning paradigm that allows multiple\nclients to collaboratively train a shared model without exposing their private\ndata. Data heterogeneity is a fundamental challenge in FL, which can result in\npoor convergence and performance degradation. Client drift has been recognized\nas one of the factors contributing to this issue resulting from the multiple\nlocal updates in FedAvg. However, in cross-device FL, a different form of drift\narises due to the partial client participation, but it has not been studied\nwell. This drift, we referred as period drift, occurs as participating clients\nat each communication round may exhibit distinct data distribution that\ndeviates from that of all clients. It could be more harmful than client drift\nsince the optimization objective shifts with every round.\n  In this paper, we investigate the interaction between period drift and client\ndrift, finding that period drift can have a particularly detrimental effect on\ncross-device FL as the degree of data heterogeneity increases. To tackle these\nissues, we propose a predict-observe framework and present an instantiated\nmethod, FedEve, where these two types of drift can compensate each other to\nmitigate their overall impact. We provide theoretical evidence that our\napproach can reduce the variance of model updates. Extensive experiments\ndemonstrate that our method outperforms alternatives on non-iid data in\ncross-device settings."}
{"id": "2508.14565", "pdf": "https://arxiv.org/pdf/2508.14565", "abs": "https://arxiv.org/abs/2508.14565", "authors": ["Soumya Sarkar", "Shweta Jain"], "title": "Cooperative SGD with Dynamic Mixing Matrices", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted at 28th European Conference on Artificial Intelligence\n  (ECAI-2025) in main paper track", "summary": "One of the most common methods to train machine learning algorithms today is\nthe stochastic gradient descent (SGD). In a distributed setting, SGD-based\nalgorithms have been shown to converge theoretically under specific\ncircumstances. A substantial number of works in the distributed SGD setting\nassume a fixed topology for the edge devices. These papers also assume that the\ncontribution of nodes to the global model is uniform. However, experiments have\nshown that such assumptions are suboptimal and a non uniform aggregation\nstrategy coupled with a dynamically shifting topology and client selection can\nsignificantly improve the performance of such models. This paper details a\nunified framework that covers several Local-Update SGD-based distributed\nalgorithms with dynamic topologies and provides improved or matching\ntheoretical guarantees on convergence compared to existing work."}
{"id": "2508.14769", "pdf": "https://arxiv.org/pdf/2508.14769", "abs": "https://arxiv.org/abs/2508.14769", "authors": ["Ahmed Mujtaba", "Gleb Radchenko", "Radu Prodan", "Marc Masana"], "title": "Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data", "categories": ["cs.LG", "cs.DC"], "comment": "This paper was accepted at the International Conference on Federated\n  Learning Technologies and Applications, 2025. The final version is available\n  at IEEE Xplore", "summary": "Federated distillation has emerged as a promising collaborative machine\nlearning approach, offering enhanced privacy protection and reduced\ncommunication compared to traditional federated learning by exchanging model\noutputs (soft logits) rather than full model parameters. However, existing\nmethods employ complex selective knowledge-sharing strategies that require\nclients to identify in-distribution proxy data through computationally\nexpensive statistical density ratio estimators. Additionally, server-side\nfiltering of ambiguous knowledge introduces latency to the process. To address\nthese challenges, we propose a robust, resource-efficient EdgeFD method that\nreduces the complexity of the client-side density ratio estimation and removes\nthe need for server-side filtering. EdgeFD introduces an efficient KMeans-based\ndensity ratio estimator for effectively filtering both in-distribution and\nout-of-distribution proxy data on clients, significantly improving the quality\nof knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,\nincluding strong non-IID, weak non-IID, and IID data distributions on clients,\nwithout requiring a pre-trained teacher model on the server for knowledge\ndistillation. Experimental results demonstrate that EdgeFD outperforms\nstate-of-the-art methods, consistently achieving accuracy levels close to IID\nscenarios even under heterogeneous and challenging conditions. The\nsignificantly reduced computational overhead of the KMeans-based estimator is\nsuitable for deployment on resource-constrained edge devices, thereby enhancing\nthe scalability and real-world applicability of federated distillation. The\ncode is available online for reproducibility."}
