<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: 该研究探索了用大语言模型（LLMs）从业务流程描述生成智能合约代码的可行性，并提出了自动化评估框架。结果显示，LLMs在智能合约开发中的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的代码生成方法存在局限性，研究者希望通过LLMs改进这一过程，并验证其在实际应用中的表现。

Method: 研究采用了自动化评估框架，测试不同规模和类型的LLMs在生成代码时的表现，重点关注流程执行的关键特性，如流程控制、资源分配和数据条件。

Result: 研究发现LLMs生成的代码在智能合约开发中未能达到所需的完美可靠性。

Conclusion: 建议未来研究探索将LLMs负责任地集成到现有代码生成工具中，以提高输出的可靠性，并提出了一个基准框架作为开发此类集成的基础。

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [2] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL 是一个基于示例的自动化 ETL 管道架构，旨在减少人工干预，自动标准化和准备输入数据集。


<details>
  <summary>Details</summary>
Motivation: 现代 ETL 解决方案需要大量人工参与设计和实现上下文特定的转换，缺乏自动化能力。

Method: FlowETL 通过一个生态系统组件交互实现目标，包括规划引擎（使用输入-输出样本构建转换计划）和 ETL 工作器执行转换，同时提供监控和日志功能。

Result: 在 14 个不同领域、文件结构和大小的数据集上展示了良好的泛化能力。

Conclusion: FlowETL 是一种有效的自动化 ETL 解决方案，能够减少人工干预并提高效率。

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [3] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: 论文提出了一种结合AI和MDE的“vibe modeling”方法，以加速开发可靠复杂系统。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统的需求增长和复杂性提升，现有开发方法和工具（如MDE和基于LLM的编程）面临挑战，需要新方法结合两者优势。

Method: 提出“vibe modeling”概念，整合AI（如LLM）和模型驱动工程（MDE），以自然语言描述生成可靠代码。

Result: vibe modeling有望提升开发效率和质量，但同时也面临机遇和开放性问题。

Conclusion: vibe modeling为复杂系统开发提供新方向，需进一步解决其带来的挑战。

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [4] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: GitHub Marketplace每年增长41%，但许多新工具功能重复。研究聚焦于CI领域，发现65%的新Actions功能重复，且少数早期工具主导后续发展。


<details>
  <summary>Details</summary>
Motivation: 分析GitHub Marketplace中工具功能重复的现象，尤其是CI领域，以帮助开发者和管理者优化工具选择和维护。

Method: 通过链接6,983个CI Actions和3,869个提供者，并挖掘其版本历史，构建图模型跟踪功能发布时间、采用情况和重复工具聚类。

Result: 约65%的新CI Actions功能重复，且通常在6个月内出现；少数早期工具主导后续的衍生和扩展。

Conclusion: 研究为开发者提供了数据支持，帮助他们选择最佳发布时间和工具策略，同时公开数据集以促进长期研究和实践应用。

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [5] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge自动化生成IoT集成代码，通过分阶段策略和多级调试管道，显著减少人工干预，成功率和功能覆盖率高达93.87%和94.87%，用户反馈后可实现100%覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统IoT设备集成需大量人工编程，AutoBridge旨在自动化此过程以减少专家依赖和提升效率。

Method: 采用分治策略，分阶段生成设备控制逻辑和平台合规代码，并引入多级调试管道（虚拟测试和硬件验证）。

Result: 在34台设备上测试，平均成功率和功能覆盖率达93.87%和94.87%，用户反馈后可达100%。

Conclusion: AutoBridge显著优于专家编程和商用LLM，验证了自动化IoT集成的可行性和高效性。

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [6] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: 论文探讨了自主业务流程（ABPs）的优点与潜在问题，并提出了可解释的自主业务流程（XABPs）解决方案及其研究方法。


<details>
  <summary>Details</summary>
Motivation: ABPs虽能提升效率和降低成本，但也引发了信任、调试、责任、偏见和合规等问题，需要可解释性方案来应对。

Method: 提出XABPs框架，包括形式分类、可解释性结构设计，并识别关键BPM研究挑战。

Result: 为XABPs的系统化研究提供了理论框架和研究方向。

Conclusion: XABPs通过可解释性设计解决了ABPs的潜在问题，为未来研究奠定了基础。

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [7] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate通过竞争性多智能体辩论框架，解决现有方法在代码库中全局问题模式识别的不足，取得新SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的独立探索方法易陷入局部解，难以识别跨代码库的问题模式，因此提出SWE-Debate。

Method: 通过代码依赖图生成多故障传播路径，组织三轮辩论，整合不同推理视角，最终用MCTS生成补丁。

Result: 在SWE-bench基准测试中，SWE-Debate显著优于基线方法，创下新SOTA。

Conclusion: SWE-Debate通过多智能体辩论提升问题定位准确性，为开源智能体框架提供新思路。

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [8] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: IBM开发了一套自动化评估系统，用于评估COBOL转Java代码的质量，结合分析检测器和LLM评判技术，支持持续集成和大规模基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代码翻译模型的透明度和翻译质量评估复杂性等挑战。

Method: 结合分析检测器（analytic checkers）与LLM作为评判（LaaJ）技术，提供多维度、可扩展的评估。

Result: 系统支持持续集成、大规模基准测试，减少对人工审核的依赖，为开发者和管理者提供可操作的改进建议。

Conclusion: 该系统有助于推动高质量、现代化代码库的发展。

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [9] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp通过从先前的代理轨迹中提取可重复利用的知识，实现了跨问题的持续学习，显著提升了软件问题的解决率。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型代理在解决软件问题时缺乏记忆能力，无法复用或积累修复经验，导致效率低下。SWE-Exp旨在通过经验增强的方法解决这一问题。

Method: SWE-Exp引入了一个多方面的经验库，捕捉成功和失败的修复尝试，从高层次问题理解到具体的代码修改中提取可复用的知识。

Result: 实验结果表明，SWE-Exp在开源代理框架下的SWE-bench-Verified上达到了41.6%的Pass@1解决率，为当前最佳表现。

Conclusion: SWE-Exp为自动软件工程代理提供了一种系统积累和利用修复经验的新范式，从试错式探索转向了策略性的经验驱动问题解决。

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [10] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: 该论文提出了一种基于代理的集成推理方法Trae Agent，用于解决软件仓库级问题，通过模块化代理处理大规模集成空间和仓库级理解，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在大规模集成空间探索和仓库级理解方面存在局限性，限制了其整体效果。

Method: 提出Trae Agent，通过生成、剪枝和选择模块化代理，将目标建模为最优解搜索问题。

Result: 在SWE-bench基准测试中，Trae Agent平均性能提升10.22%，Pass@1得分为75.20%，排名第一。

Conclusion: Trae Agent通过代理方法有效解决了软件问题解决的挑战，并开源以支持研究社区。

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [11] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: Kieker框架最初为Java设计，现扩展支持Python，通过结合静态和动态分析提供全面的系统洞察。


<details>
  <summary>Details</summary>
Motivation: Python的流行使得对其应用程序的结构性分析变得重要，因此扩展Kieker框架以支持Python具有价值。

Method: 结合静态和动态分析，构建完整的系统视图。

Result: 实现了Python分析管道，能够提供对Python应用程序的全面洞察。

Conclusion: Kieker框架的Python支持为开发者提供了更多灵活性，满足了Python应用程序的可观测性需求。

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [12] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 该论文研究了代码审查（CR）中基于代码修改量的工作负荷，发现大多数合并请求需调整，且修正量不直接影响审查时间或参与者数量。通过机器学习模型预测CR工作量，成效显著。


<details>
  <summary>Details</summary>
Motivation: 探讨GitLab合并请求中代码审查的工作负荷，尤其是代码修改量的影响，填补现有研究的空白。

Method: 从四个GitLab项目中收集23,600多个合并请求数据，定义CR工作量为提交后的代码修改量。采用可解释的机器学习模型，结合多维指标（文本特征、代码复杂度等）预测工作量。

Result: 71%的合并请求需调整，28%涉及200多行代码修改；修正量与审查时间或参与者数量无关。模型预测性能优秀（AUC 0.84-0.88），复杂度、经验及文本特征是关键预测因子。

Conclusion: 机器学习可有效解释和预测代码审查工作量，项目历史特征对当前审查有显著影响。

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 论文提出了关于函数格序理论的理论和实践结果，专注于Galois连接的抽象，提出了新的B-bound抽象域和领域抽象方法，用于推断递归定义的函数的闭式边界，特别适用于程序分析和混合系统。


<details>
  <summary>Details</summary>
Motivation: 动机在于推断递归定义的函数（即算子的不动点或函数方程的解）的闭式边界，这在程序分析（如成本分析、循环加速、声明性语言分析）和由微分方程控制的混合系统中有广泛应用。

Method: 方法包括提出了一族新的基于约束的抽象域（B-bound域），用于抽象数值函数；引入了领域抽象方法，将任意值空间映射提升为函数空间中的Galois连接；并使用简单的算子语言构建传递函数。

Result: 结果表明，B-bound域能够推断高度非线性的数值不变量，而传统数值抽象域难以处理；约束空间的凸性简化了传递函数的设计，甚至在某些情况下实现了完全自动化。

Conclusion: 结论是通过B-bound域和领域抽象的联合应用，提出了一种高效的方法来抽象和分析复杂的数值函数，支持从符号函数到数值函数的抽象，并实现了方程的降维处理。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [14] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI 是一种语言无关的框架，通过源级转换实现跨语言无缝调用，解决了现有 FFI 在交互式环境中配置繁琐、缺乏递归和 OOP 支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有 FFI 解决方案在动态交互式笔记本环境中（如 Jupyter）配置复杂且不支持递归调用和 OOP，影响了多语言开发的效率。

Method: Kernel-FFI 通过源级转换自动重写跨语言调用，无需手动绑定，支持 OOP 和跨语言资源管理，并引入侧信道通信机制支持递归和异步调用。

Result: Kernel-FFI 实现了透明化的跨语言调用和对象操作，解决了现有 FFI 的局限性。

Conclusion: Kernel-FFI 为交互式笔记本环境提供了一种高效的跨语言开发解决方案，支持 OOP 和异步调用，并将开源发布。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: PRIME是一种伪随机轮询包喷洒方法，通过考虑网络拓扑来优化负载分配和性能，解决了现有AI/ML工作负载中的网络负载平衡问题。


<details>
  <summary>Details</summary>
Motivation: 生产数据中心的大规模分布式训练对网络基础设施提出了高要求，现有解决方案（如ECMP）在AI/ML工作负载中表现不佳，且包喷洒方案会导致缓冲区膨胀和尾延迟增加。

Method: PRIME采用伪随机轮询包喷洒方法，利用拥塞信号作为负载重新平衡的指标，考虑了拥塞严重性和衰减时间，以避免网络热点。

Result: PRIME在置换流量和网络退化场景中，分别实现了高达15%和27%的性能提升。

Conclusion: PRIME通过优化负载分配和性能，显著提升了网络在AI/ML工作负载中的表现。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [16] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出了一种名为InterfO-RAN的实时可编程解决方案，使用CNN处理I/Q样本，能在650微秒内以超过91%的准确率检测5G网络中的带内干扰。


<details>
  <summary>Details</summary>
Motivation: 5G及超5G超密集网络通过频谱共享和频率重用来提升吞吐量，但面临不可预测的带内上行干扰问题，严重影响信号质量，尤其是在小区边缘和毫米波系统中。

Method: 提出InterfO-RAN方案，利用CNN处理gNB物理层的I/Q样本，并使用GPU加速，成为首个O-RAN dApp。

Result: 在超过700万个NR UL时隙的真实数据中测试，InterfO-RAN能在650微秒内以超过91%的准确率检测干扰。

Conclusion: InterfO-RAN为密集部署的5G网络提供了高效的干扰检测解决方案，能够显著提升网络性能。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [17] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 论文研究了分组对随机接入网络排队延迟的影响，提出了优化分组策略以减少延迟的方法，并分析了网络参数的影响，同时探讨了在特定场景下的应用。


<details>
  <summary>Details</summary>
Motivation: 随着对低延迟服务的需求增长，现有研究忽视了分组对排队延迟的影响，尤其是以秒为单位的平均排队延迟。

Method: 通过数学建模和数值方法，建立了分组与平均排队延迟的关系，并探讨了最优分组策略；通过仿真研究了分组对延迟抖动的影响。

Result: 确定了最优的平均排队延迟及其对应的分组大小，分析了网络参数的影响，并将其应用于NTN场景。

Conclusion: 分组对排队延迟有显著影响，优化分组策略可以有效减少延迟，同时为连接自由和基于连接的方案提供了新的评估视角。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [18] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: FAST-LoRa 是一个新型仿真框架，旨在快速高效地评估 LoRaWAN 网络和选择传输参数，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工具虽能准确模拟真实场景，但计算开销和仿真时间较大，需要一种轻量级的近似工具。

Method: FAST-LoRa 通过依赖分析模型而非复杂的包级仿真，并采用高效矩阵运算实现网关接收，优化计算。

Result: FAST-LoRa 在关键网络指标估计上与传统仿真器精度相近，计算时间减少高达三个数量级。

Conclusion: FAST-LoRa 是适用于稳定流量模式和上行通信场景的轻量级准确近似工具。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [19] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 本文提出了一种双模通信框架，结合查询驱动（pull）和事件驱动（push）传输，通过自适应方法实现高效、及时的数据传输，能耗降低30%。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线设备在不同网络条件下数据交付的效率和能耗问题，同时支持关键事件的快速响应和查询请求的可靠处理。

Method: 设计了一种结合唤醒无线电机制和定制MAC协议的双模通信框架，支持不同通信类别的数据流，并通过系统级分析评估性能。

Result: 数值结果显示，系统能耗降低30%，同时保持了两种通信模式的成功率。

Conclusion: 所提出的双模通信框架在节能和可靠性方面优于传统方法，适用于多样化的网络需求。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>


### [20] [From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks](https://arxiv.org/abs/2507.23433)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文分析了信息版本年龄（VAoI）的稳态分布，提出多种调度策略下的闭式解，并在单跳和多跳网络中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注平均指标，缺乏对内容新鲜度与信息价值的全面分布分析，尤其是在多跳网络中。

Method: 研究多种调度策略（随机固定、均匀、阈值），推导VAoI稳态分布及平均值的闭式表达式，确定最优阈值。

Result: 得出了VAoI的最优阈值及闭式解，数值实验验证了理论结果。

Conclusion: 研究为通信网络设计提供了基于VAoI的高效调度策略参考。

Abstract: Timely and informative data dissemination in communication networks is
essential for enhancing system performance and energy efficiency, as it reduces
the transmission of outdated or redundant data. Timeliness metrics, such as Age
of Information (AoI), effectively quantify data freshness; however, these
metrics fail to account for the intrinsic informativeness of the content
itself. To address this limitation, content-based metrics have been proposed
that combine both timeliness and informativeness. Nevertheless, existing
studies have predominantly focused on evaluating average metric values, leaving
the complete distribution-particularly in multi-hop network scenarios-largely
unexplored. In this paper, we provide a comprehensive analysis of the
stationary distribution of the Version Age of Information (VAoI), a
content-based metric, under various scheduling policies, including randomized
stationary, uniform, and threshold-based policies, with transmission
constraints in single-hop and multi-hop networks. We derive closed-form
expressions for the stationary distribution and average VAoI under these
scheduling approaches. Furthermore, for threshold-based scheduling, we
analytically determine the optimal threshold value that minimizes VAoI and
derive the corresponding optimal VAoI in closed form. Numerical evaluations
verify our analytical findings, providing valuable insights into leveraging
VAoI in the design of efficient communication networks.

</details>


### [21] [Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching](https://arxiv.org/abs/2507.23556)
*Botao Zhu,Xianbin Wang*

Main category: cs.NI

TL;DR: 论文提出了一种超图辅助的信任任务-资源匹配框架（TTR-matching），用于解决复杂连接系统中任务和资源匹配的挑战，通过整合任务特定信任关系和物理属性，实现价值驱动的任务完成。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源和任务的物理属性多样化，复杂连接系统中任务与资源的匹配机制变得更具挑战性。

Method: 提出了一种基于超图的TTR-matching框架，包括任务特定信任物理资源超图和任务超图的定义，以及超图匹配算法的设计。

Result: 实验表明，TTR-matching框架在识别任务特定可信协作伙伴和最大化任务完成平均价值方面优于对比算法。

Conclusion: 该框架通过整合任务特定信任和物理属性，实现了高效的任务-资源匹配和价值驱动的任务完成。

Abstract: Due to the diverse physical attributes of computing resources and tasks,
developing effective mechanisms to facilitate task and resource matching in
complex connected systems for value-oriented task completion has become
increasingly challenging. To address the challenge, this paper proposes a
networked physical computing system that integrates the physical attributes of
computing resources and tasks as well as task-specific trust relationships
among devices to enable value-driven task completion. Specifically, we propose
a state-of-the-art hypergraph-aided trusted task-resource matching
(TTR-matching) framework to achieve the envisioned physical computing. First, a
task-specific trusted physical resource hypergraph is defined, which integrates
task-specific trust, the physical attributes of resources, and task types. This
enables accurate modeling of device collaboration dependencies under specific
task types. Next, a task hypergraph is generated to associate the task
initiator with the physical attributes of the corresponding tasks. Based on
these two hypergraphs, a hypergraph matching algorithm is designed to
facilitate task-specific trusted collaborator selection and accurate
task-resource matching for value-maximizing task completion. Extensive
experimental results demonstrate that the proposed TTR-matching framework
outperforms comparison algorithms in identifying task-specific trustworthy
collaborators and maximizing the average value of task completion.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [Hybrid CNN-Mamba Enhancement Network for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2507.23444)
*Xiang Li,Xianfu Cheng,Xiaoming Zhang,Zhoujun Li*

Main category: cs.MM

TL;DR: 提出了一种名为HCMEN的新框架，用于处理多模态情感分析中模态缺失的问题，通过结合CNN和Mamba架构实现局部和全局信息融合，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理模态缺失的多模态情感分析时面临对齐和融合的挑战，因此需要一种更有效的方法。

Method: 结合CNN和Mamba架构，引入跨模态增强机制和混合融合方法，实现局部和全局信息的综合处理。

Result: 在两个基准数据集上的实验表明，HCMEN在各种模态缺失情况下优于现有方法。

Conclusion: HCMEN框架在多模态情感分析中具有鲁棒性和优越性能，未来将公开代码。

Abstract: Multimodal Sentiment Analysis (MSA) with missing modalities has recently
attracted increasing attention. Although existing research mainly focuses on
designing complex model architectures to handle incomplete data, it still faces
significant challenges in effectively aligning and fusing multimodal
information. In this paper, we propose a novel framework called the Hybrid
CNN-Mamba Enhancement Network (HCMEN) for robust multimodal sentiment analysis
under missing modality conditions. HCMEN is designed around three key
components: (1) hierarchical unimodal modeling, (2) cross-modal enhancement and
alignment, and (3) multimodal mix-up fusion. First, HCMEN integrates the
strengths of Convolutional Neural Network (CNN) for capturing local details and
the Mamba architecture for modeling global contextual dependencies across
different modalities. Furthermore, grounded in the principle of Mutual
Information Maximization, we introduce a cross-modal enhancement mechanism that
generates proxy modalities from mixed token-level representations and learns
fine-grained token-level correspondences between modalities. The enhanced
unimodal features are then fused and passed through the CNN-Mamba backbone,
enabling local-to-global cross-modal interaction and comprehensive multimodal
integration. Extensive experiments on two benchmark MSA datasets demonstrate
that HCMEN consistently outperforms existing state-of-the-art methods,
achieving superior performance across various missing modality scenarios. The
code will be released publicly in the near future.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Explanations for Unrealizability of Infinite-State Safety Shields](https://arxiv.org/abs/2507.23603)
*Andoni Rodriguez,Irfansha Shaik,Davide Corsi,Roy Fox,Cesar Sanchez*

Main category: cs.LO

TL;DR: 该论文提出了一种通过时态公式展开生成无条件或有条件解释的方法，以解决屏蔽合成中由于规范不一致导致的不可实现性问题。


<details>
  <summary>Details</summary>
Motivation: 屏蔽合成在连续环境中扩展后更适用于现实场景，但规范不一致可能导致屏蔽不可实现，论文旨在解决这一问题。

Method: 使用时态公式展开技术，生成见证不可实现性的无条件或有条件解释。

Result: 展示了该方法的不同变体及其适用性。

Conclusion: 该方法为屏蔽合成中的不可实现性问题提供了有效的解释工具。

Abstract: Safe Reinforcement Learning focuses on developing optimal policies while
ensuring safety. A popular method to address such task is shielding, in which a
correct-by-construction safety component is synthesized from logical
specifications. Recently, shield synthesis has been extended to infinite-state
domains, such as continuous environments. This makes shielding more applicable
to realistic scenarios. However, often shields might be unrealizable because
the specification is inconsistent (e.g., contradictory). In order to address
this gap, we present a method to obtain simple unconditional and conditional
explanations that witness unrealizability, which goes by temporal formula
unrolling. In this paper, we show different variants of the technique and its
applicability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary](https://arxiv.org/abs/2507.23454)
*Marta Bieńkiewicz,Julia Ayache,Panayiotis Charalambous,Cristina Becchio,Marco Corragio,Bertram Taetz,Francesco De Lellis,Antonio Grotta,Anna Server,Daniel Rammer,Richard Kulpa,Franck Multon,Azucena Garcia-Palacios,Jessica Sutherland,Kathleen Bryson,Stéphane Donikian,Didier Stricker,Benoît Bardy*

Main category: cs.HC

TL;DR: 本文探讨了混合现实（MR）技术在真实复制人类体现和社会运动互动方面的不足，提出多模态数据流和多代理互动能力是提升MR社交体验的关键。


<details>
  <summary>Details</summary>
Motivation: 现有MR技术在人类体现和社会互动方面存在不足，未能实现真正的社交体验，作者希望通过多模态和多代理技术推动MR的变革。

Method: 通过提出一个综合术语表，涵盖虚拟角色、自主化、责任AI、设计伦理以及神经科学、体现和技术中的社会MR科学挑战。

Result: 推动以人为中心的MR技术创新，增强人类与虚拟自主代理之间的社交互动与协作，确保包容性、伦理设计和心理安全。

Conclusion: 提出MR技术需优先考虑人类中心创新，以促进更丰富的数字连接和社交互动。

Abstract: This article explores a critical gap in Mixed Reality (MR) technology: while
advances have been made, MR still struggles to authentically replicate human
embodiment and socio-motor interaction. For MR to enable truly meaningful
social experiences, it needs to incorporate multi-modal data streams and
multi-agent interaction capabilities. To address this challenge, we present a
comprehensive glossary covering key topics such as Virtual Characters and
Autonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges
of Social MR within Neuroscience, Embodiment, and Technology. Our aim is to
drive the transformative evolution of MR technologies that prioritize
human-centric innovation, fostering richer digital connections. We advocate for
MR systems that enhance social interaction and collaboration between humans and
virtual autonomous agents, ensuring inclusivity, ethical design and
psychological safety in the process.

</details>


### [25] [Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls](https://arxiv.org/abs/2507.22889)
*Tom Sheffer,Alon Miron,Yaniv Dover,Ariel Goldstein*

Main category: cs.HC

TL;DR: 研究了AI代理（LLMs）与人之间对话配置的合作效果，发现人之间的交互提升了准确性，而纯LLM交互反而降低了准确性，原因在于知识多样性不足。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理是否能通过对话产生类似人类的协同效应，优化AI在群体环境中的合作表现。

Method: 比较四种对话配置（LLM-LLM、LLM三人组、人类三人组、人-LLM混合组），分析对话前后的答案准确性变化及交互行为。

Result: 人类交互显著提升准确性，纯LLM交互准确性下降，知识多样性是协同改进的关键。

Conclusion: 建议AI开发中注重多样性而非单一模型性能，以提升群体合作效果。

Abstract: Conversations transform individual knowledge into collective insight,
allowing groups of humans and increasingly groups of artificial intelligence
(AI) agents to collaboratively solve complex problems. Whether interactions
between AI agents can replicate the synergy observed in human discussions
remains an open question. To investigate this, we systematically compared four
conversational configurations: pairs of large language models (LLM-LLM), trios
of LLMs, trios of humans, and mixed human-LLM pairs. After agents answered
questions individually, they engaged in open-ended discussions and then
reconsidered their initial answers. Interactions involving humans consistently
led to accuracy improvements after the conversations, benefiting both stronger
and weaker participants. By contrast, purely LLM-based pairs and trios
exhibited declines in accuracy, demonstrating limited conversational synergy.
Analysis of participants' confidence and answer-switching behavior revealed
that knowledge diversity is a critical factor enabling collaborative
improvement. Crucially, the lack of gains in LLM-LLM interactions did not stem
from a fundamental limitation of the models' ability to collaborate, but from
highly similar knowledge states that left little room for productive exchange.
Our findings argue for a paradigm shift in AI development: rather than
optimizing individual models solely for standalone performance, explicitly
cultivating diversity across agents, even at the cost of slightly lower
individual accuracy, may yield AI collaborators that are more effective in
group settings with humans or other AI systems.

</details>


### [26] [Evaluating LLMs for Visualization Generation and Understanding](https://arxiv.org/abs/2507.22890)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.HC

TL;DR: 该论文探讨了大型语言模型（LLMs）在信息可视化领域的应用潜力，展示了其生成可视化代码和理解常见图表的能力，但也指出了其在复杂任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在信息可视化任务中的表现，旨在促进LLMs和可视化系统的共同改进。

Method: 通过简单提示让LLMs生成可视化代码，并测试其对常见图表的理解能力。

Result: LLMs能够生成简单图表（如柱状图、饼图）的代码，并能回答关于图表的基本问题，但在复杂任务（如小提琴图生成或边界关系识别）中存在局限。

Conclusion: 研究为LLMs和可视化系统的优化提供了方向，未来需进一步解决复杂任务的挑战。

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
questions. Our study shows that LLMs could generate code for some simpler
visualizations such as bar and pie charts. Moreover, they could answer simple
questions about visualizations. However, LLMs also have several limitations.
For example, some of them had difficulty generating complex visualizations,
such as violin plot. LLMs also made errors in answering some questions about
visualizations, for example, identifying relationships between close boundaries
and determining lengths of shapes. We believe that our insights can be used to
improve both LLMs and Information Visualization systems.

</details>


### [27] [Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter](https://arxiv.org/abs/2507.22891)
*Jérôme Ferrari,Benoit Delinchant,Frédéric Wurtz,Olga Rouchouze*

Main category: cs.HC

TL;DR: 本文介绍了一种基于Linky电表数据的开源实时监控基础设施，用于支持法国集体自消费项目的能源管理，帮助参与者及时调整用电行为。


<details>
  <summary>Details</summary>
Motivation: 随着能源转型和能源价格上涨，法国集体自消费项目增多，但现有能源监控依赖历史数据，缺乏实时反馈。

Method: 开发了xKy设备，部署网关并搭建实时监控网站，应用于9户参与的集体自消费项目。

Result: 实现了实时能源流监控，参与者能及时决策和行动。

Conclusion: 该方案提升了集体自消费率，支持能源转型目标。

Abstract: As part of the energy transition and the rise in energy prices, the number of
collective self-consumption operations in France is steadily increasing.
However, energy flow monitoring currently relies on historical ''day+1'' data
provided by Linky meters, which does not offer real time feedback to help
participants adapt their energy consumption behaviors. This article introduces
a new open-source infrastructure for real-time monitoring based on Linky meter
data, enabling participants to make informed decisions and take timely actions.
It includes a description of the xKy device, applied to a collective
self-consumption operation involving nine participants, supported by the Energy
Transition Observatory (OTE). The project encompasses the implementation of
gateways in participants' homes and the development and operation of real-time
monitoring website, aimed at increasing participants' self-consumption rate.

</details>


### [28] [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)
*Ismail Hossain,Mridul Banik*

Main category: cs.HC

TL;DR: 提出一种结合EEG-BCI和LLM的混合框架，用于实时个性化的语言康复辅助系统。


<details>
  <summary>Details</summary>
Motivation: 传统AAC系统和语言学习平台缺乏实时适应性，尤其在神经系统疾病患者中表现不足。

Method: 利用EEG信号驱动LLM，实现语言模块导航、动态个性化任务和认知负荷监测。

Result: 系统能够支持严重言语或运动障碍用户，并提供个性化语言学习辅助。

Conclusion: 该混合框架为语言康复提供了一种创新的实时适应解决方案。

Abstract: Conventional augmentative and alternative communication (AAC) systems and
language-learning platforms often fail to adapt in real time to the user's
cognitive and linguistic needs, especially in neurological conditions such as
post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in
noninvasive electroencephalography (EEG)--based brain-computer interfaces
(BCIs) and transformer--based large language models (LLMs) offer complementary
strengths: BCIs capture users' neural intent with low fatigue, while LLMs
generate contextually tailored language content. We propose and evaluate a
novel hybrid framework that leverages real-time EEG signals to drive an
LLM-powered language rehabilitation assistant. This system aims to: (1) enable
users with severe speech or motor impairments to navigate language-learning
modules via mental commands; (2) dynamically personalize vocabulary,
sentence-construction exercises, and corrective feedback; and (3) monitor
neural markers of cognitive effort to adjust task difficulty on the fly.

</details>


### [29] [Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure](https://arxiv.org/abs/2507.22893)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: 提出“认知基础设施研究”（CIS）作为新领域，将AI视为重塑人类认知的“认知基础设施”，研究其如何通过隐形机制影响知识获取和社会行为。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互研究忽视AI如何潜在地重塑人类认知，需新的跨学科方法来分析这种隐形影响。

Method: 通过叙事场景（个体认知依赖、集体民主审议、社会治理）和“基础设施破坏方法”揭示AI预处理的影响。

Result: AI作为认知基础设施改变了人类认知、公共推理和社会认识论，需整合多学科方法研究其影响。

Conclusion: CIS填补了认知科学、数字社会学和计算方法的空白，为研究AI的隐形影响提供了新框架和方法。

Abstract: Contemporary human-AI interaction research overlooks how AI systems
fundamentally reshape human cognition pre-consciously, a critical blind spot
for understanding distributed cognition. This paper introduces "Cognitive
Infrastructure Studies" (CIS) as a new interdisciplinary domain to
reconceptualize AI as "cognitive infrastructures": foundational, often
invisible systems conditioning what is knowable and actionable in digital
societies. These semantic infrastructures transport meaning, operate through
anticipatory personalization, and exhibit adaptive invisibility, making their
influence difficult to detect. Critically, they automate "relevance judgment,"
shifting the "locus of epistemic agency" to non-human systems. Through
narrative scenarios spanning individual (cognitive dependency), collective
(democratic deliberation), and societal (governance) scales, we describe how
cognitive infrastructures reshape human cognition, public reasoning, and social
epistemologies. CIS aims to address how AI preprocessing reshapes distributed
cognition across individual, collective, and cultural scales, requiring
unprecedented integration of diverse disciplinary methods. The framework also
addresses critical gaps across disciplines: cognitive science lacks
population-scale preprocessing analysis capabilities, digital sociology cannot
access individual cognitive mechanisms, and computational approaches miss
cultural transmission dynamics. To achieve this goal CIS also provides
methodological innovations for studying invisible algorithmic influence:
"infrastructure breakdown methodologies", experimental approaches that reveal
cognitive dependencies by systematically withdrawing AI preprocessing after
periods of habituation.

</details>


### [30] [When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research](https://arxiv.org/abs/2507.22894)
*Monique Munarini*

Main category: cs.HC

TL;DR: 本文反思了设计和主导共同设计与参与式研讨会的常见挑战，为早期职业研究人员提供了实用策略。


<details>
  <summary>Details</summary>
Motivation: 探讨在缺乏机构支持或激励的情况下如何有效与非专家参与者互动，并重构失败为学习机会。

Method: 基于个人经验，详述从研讨会概念化到实施的完整过程，包括参与者的动态变化。

Result: 研讨会虽初期参与度低，但最终促成多元讨论并吸引参与者转为共同主导者。

Conclusion: 通过重构失败为学习点，为跨学科研究者提供了实践策略，强调了生活经验在研究中的核心地位。

Abstract: This reflective paper explores often-unspoken challenges of designing and
facilitating co-design and participatory workshops, offering practical
strategies for early career researchers (ECRs) navigating these methods.
Drawing from personal experience conducting a series of workshops titled: How
to Think About Equity in the AI Ecosystem. It follows the full arc of the
workshop experience, from conceptualization and activity planning to
participant recruitment and facilitation, offering a grounded account of what
happens when participation does not go as expected. The paper examines the
methodological challenges of engaging non-expert participants, particularly
when operating without institutional support, financial incentives, or
integration into larger events. Despite initial difficulties such as low
attendance, the workshop fostered rich discussions among a demographically
diverse group and ultimately led to one participant volunteering to
co-facilitate a subsequent session. This transition from participant to
co-facilitator exemplifies the redistribution of epistemic authority,
positioning lived experience as central to research and engagement practices.
By reframing perceived failure as a productive site of learning, the paper
offers practical strategies for ECRs working across disciplines who often
navigate unfamiliar methodological terrains, contributing to broader
conversations on the realities of doing interdisciplinary, participatory work
in practice.

</details>


### [31] [Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface](https://arxiv.org/abs/2507.22895)
*Ye Sun,Bowei Zhao,Dezhong Yao,Rui Zhang,Bohan Zhang,Xiaoyuan Li,Jing Wang,Mingxuan Qu,Gang Liu*

Main category: cs.HC

TL;DR: 该论文提出了一种基于非侵入式脑肌肉接口（BMuI）的新型运动意图提取框架，通过结合脑电信号（EEG）和肌电信号（EMG）提高意图识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于运动的脑机接口（如运动想象BCI）因标签不精确导致解码准确性不足，需要一种更可靠的方法来提取运动意图信号。

Method: 论文提出BMuI框架，模拟大脑到肌肉的神经通路，利用EMG作为高保真中继介质，通过离线和在线实验验证其可行性和有效性。

Result: BMuI方法在预测准确性上达到0.8314，在线实验中所有参与者成功控制Unity虚拟手臂。

Conclusion: BMuI框架能够有效解决现有BCI系统中意图信号提取不精确的问题，并提供了一种可靠的实时交互解决方案。

Abstract: Brain-computer interfaces (BCIs) enable real-time interaction between the
brain and external devices by decoding neural signals. However, existing
motor-based BCI paradigms, like motor imagery BCI, face challenges with
imprecise labeling in real-world use. This mismatch between EEG signals and
true behavioral intentions leads to pseudo-labels, undermining decoding
accuracy and system robustness. To overcome this bottleneck, this paper first
proposes a novel motor intention extraction framework based on a non-invasive
brain-muscle interface (BMuI)($\text{BCI} =
\frac{\text{Brain}}{\text{Computer}} \text{ Interface} =
\frac{\text{Brain}}{\not\text{Muscle}}\! \text{ (BMuI)} \times
\!\frac{\not\text{Muscle}}{\text{Computer}}\! \text{ Interface}$). This method
simulates the neural pathway from the brain to the muscles in order to capture
and enhance the weak motor intention signals originating in the brain. It then
uses EMG as a high-fidelity relay medium to achieve more accurate intention
recognition and transmission. To systematically validate the feasibility and
effectiveness of this approach, we conducted both offline experiments (to
repeatedly verify feasibility) and online experiments (to construct a real-time
interactive system and evaluate its performance). The results show that BMuI is
feasible, achieving a prediction accuracy of 0.8314; in the online experiment,
all participants are able to successfully control the Unity virtual arm.

</details>


### [32] [iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement](https://arxiv.org/abs/2507.22896)
*Kohou Wang,ZhaoXiang Liu,Lin Bai,Kun Fan,Xiang Liu,Huan Hu,Kai Wang,Shiguo Lian*

Main category: cs.HC

TL;DR: 本文提出了一种基于多模态大语言模型的交互式学习机器人系统，能够通过与用户的自然对话学习，并利用双模态检索模块避免重复错误，提高机器人适应性。


<details>
  <summary>Details</summary>
Motivation: 机器人部署后常遇到新场景，需持续改进性能。

Method: 采用多模态大语言模型，结合交互式学习和双模态检索模块，通过自然对话学习用户意图。

Result: 实验证明该方法在定量和定性上均有效提升性能。

Conclusion: 该系统的交互学习能力为机器人提供了更强的环境适应性和性能提升路径。

Abstract: It is crucial that robots' performance can be improved after deployment, as
they are inherently likely to encounter novel scenarios never seen before. This
paper presents an innovative solution: an interactive learning-based robot
system powered by a Multi-modal Large Language Model(MLLM). A key feature of
our system is its ability to learn from natural dialogues with non-expert
users. We also propose chain of question to clarify the exact intent of the
question before providing an answer and dual-modality retrieval modules to
leverage these interaction events to avoid repeating same mistakes, ensuring a
seamless user experience before model updates, which is in contrast to current
mainstream MLLM-based robotic systems. Our system marks a novel approach in
robotics by integrating interactive learning, paving the way for superior
adaptability and performance in diverse environments. We demonstrate the
effectiveness and improvement of our method through experiments, both
quantitively and qualitatively.

</details>


### [33] [RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems](https://arxiv.org/abs/2507.22897)
*Luyu Chen,Quanyu Dai,Zeyu Zhang,Xueyang Feng,Mingyu Zhang,Pengcheng Tang,Xu Chen,Yue Zhu,Zhenhua Dong*

Main category: cs.HC

TL;DR: 该论文提出了RecUserSim，一个基于大语言模型的用户模拟器，用于更真实、多样地模拟用户对话，并提供显式评分，以改进对话推荐系统的评估。


<details>
  <summary>Details</summary>
Motivation: 当前的对话推荐系统评估缺乏真实且多样化的用户模拟器，并且缺乏显式评分机制，因此需要一种更有效的方法。

Method: RecUserSim包括多个模块：用户画像模块、记忆模块、核心动作模块（基于有限理性理论）和细化模块，以生成更真实和个性化的对话。

Result: 实验表明，RecUserSim能够生成多样且可控的输出，即使基于较小的语言模型，也能产生高质量的对话，评分的跨模型一致性较高。

Conclusion: RecUserSim在模拟用户行为和提供显式评分方面表现出色，为对话推荐系统的评估提供了有效的工具。

Abstract: Conversational recommender systems (CRS) enhance user experience through
multi-turn interactions, yet evaluating CRS remains challenging. User
simulators can provide comprehensive evaluations through interactions with CRS,
but building realistic and diverse simulators is difficult. While recent work
leverages large language models (LLMs) to simulate user interactions, they
still fall short in emulating individual real users across diverse scenarios
and lack explicit rating mechanisms for quantitative evaluation. To address
these gaps, we propose RecUserSim, an LLM agent-based user simulator with
enhanced simulation realism and diversity while providing explicit scores.
RecUserSim features several key modules: a profile module for defining
realistic and diverse user personas, a memory module for tracking interaction
history and discovering unknown preferences, and a core action module inspired
by Bounded Rationality theory that enables nuanced decision-making while
generating more fine-grained actions and personalized responses. To further
enhance output control, a refinement module is designed to fine-tune final
responses. Experiments demonstrate that RecUserSim generates diverse,
controllable outputs and produces realistic, high-quality dialogues, even with
smaller base LLMs. The ratings generated by RecUserSim show high consistency
across different base LLMs, highlighting its effectiveness for CRS evaluation.

</details>


### [34] [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)
*Julian Acosta,Scott Adams,Julius Kernbach,Romain Hardy,Sung Eun Kim,Luyang Luo,Xiaoman Zhang,Shreya Johri,Mohammed Baharoon,Pranav Rajpurkar*

Main category: cs.HC

TL;DR: 开发了一款基于语音的AI系统，用于指导非专业人员完成中风评估，效果显著但仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 解决急救中中风识别不一致且准确率低的问题。

Method: 通过自然对话和智能手机视频捕获技术，由非医疗志愿者使用AI系统评估模拟中风患者。

Result: AI系统识别中风症状的准确率为84%，但存在误报，专家审查后诊断准确率100%。

Conclusion: 当前系统需人工监督，但未来有望通过语音AI模型提升评估准确性。

Abstract: We developed a voice-driven artificial intelligence (AI) system that guides
anyone - from paramedics to family members - through expert-level stroke
evaluations using natural conversation, while also enabling smartphone video
capture of key examination components for documentation and potential expert
review. This addresses a critical gap in emergency care: current stroke
recognition by first responders is inconsistent and often inaccurate, with
sensitivity for stroke detection as low as 58%, causing life-threatening delays
in treatment. Three non-medical volunteers used our AI system to assess ten
simulated stroke patients, including cases with likely large vessel occlusion
(LVO) strokes and stroke-like conditions, while we measured diagnostic
accuracy, completion times, user confidence, and expert physician review of the
AI-generated reports. The AI system correctly identified 84% of individual
stroke signs and detected 75% of likely LVOs, completing evaluations in just
over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use
(mean 4.67/5). The system successfully identified 86% of actual strokes but
also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert
physician reviewed the AI reports with videos, they identified the correct
diagnosis in 100% of cases, but felt confident enough to make preliminary
treatment decisions in only 40% of cases due to observed AI errors including
incorrect scoring and false information. While the current system's limitations
necessitate human oversight, ongoing rapid advancements in speech-to-speech AI
models suggest that future versions are poised to enable highly accurate
assessments. Achieving human-level voice interaction could transform emergency
medical care, putting expert-informed assessment capabilities in everyone's
hands.

</details>


### [35] [A visual analytics tool for taxonomy-based trajectory data exploration](https://arxiv.org/abs/2507.22899)
*Ivan A. Hanono Cozzetti,Ahmad Abdou*

Main category: cs.HC

TL;DR: 提出了一种结合数据可视化和统计计算的空间时空数据分析工具，通过多级方法和机器学习模型分类移动对象，并在北极狐和热带气旋的案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 空间时空数据由于其复杂性和异质性，分析难度大，因此需要一种有效的工具来简化和结构化分析过程。

Method: 工具结合数据可视化和统计计算，利用机器学习模型对移动对象进行分类，并通过多级方法分析行为模式。

Result: 案例研究成功分类并标记了北极狐的热带气旋的行为模式，揭示了不同类型行为的统计特征。

Conclusion: 该工具和方法证明了空间时空数据的可分析性，并提供了跨领域应用的蓝图。

Abstract: The analysis of spatio-temporal data presents significant challenges due to
the complexity and heterogeneity of movement patterns. This project proposes a
data analytics tool that combines data visualization and statistical
computation to facilitate spatio-temporal data analysis through a multi-level
approach. The tool categorizes moving objects into distinct taxonomies using
Machine Learning models, adding meaningful structure to the analysis. Two case
studies demonstrate the methodology's effectiveness. The first analyzed Arctic
fox trajectories, successfully identifying and labeling foxes with Geometric or
Kinematic-based behaviors, further categorized into Curvature and Acceleration
groups. Statistical indicators revealed that foxes with Acceleration-based
behavior showed constant, steady acceleration, while those with Curvature-based
behavior exhibited acceleration peaks and sudden deceleration. The second case
study examined tropical cyclone data, labeling trajectories with Speed,
Curvature, and hybrid Geometric-based behaviors through unique statistical
variables. Analysis of hybrid Geometric behavior (Curvature and Indentation
combined) identified specific angles with the highest impact on hurricane shape
and geometry. The proposed method and tool demonstrate that spatio-temporal
data, despite inherent complexity, can be analyzed and explained in detail,
providing a theoretical and practical blueprint applicable to multiple domains.

</details>


### [36] [Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants](https://arxiv.org/abs/2507.22900)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 研究探讨AI代码助手如何影响编程新手的考试体验，发现AI工具能提升信心但可能导致依赖和概念理解不足。


<details>
  <summary>Details</summary>
Motivation: 探索AI工具对编程初学者学习体验的影响，特别是在考试情境中。

Method: 通过两部分考试设计（一部分有AI支持，另一部分无）和20名学生的问卷调查（Likert量表和开放式问题）收集数据。

Result: AI工具被认为有助于代码理解和提升信心，但在无AI支持的任务中表现出知识迁移困难和概念理解不足。

Conclusion: 研究强调需要结合AI工具的教学策略，同时加强基础编程技能的培养。

Abstract: This exploratory study examines how AI code assistants shape novice
programmers' experiences during a two-part exam in an introductory programming
course. In the first part, students completed a programming task with access to
AI support; in the second, they extended their solutions without AI. We
collected Likert-scale and open-ended responses from 20 students to evaluate
their perceptions and challenges. Findings suggest that AI tools were perceived
as helpful for understanding code and increasing confidence, particularly
during initial development. However, students reported difficulties
transferring knowledge to unaided tasks, revealing possible overreliance and
gaps in conceptual understanding. These insights highlight the need for
pedagogical strategies that integrate AI meaningfully while reinforcing
foundational programming skills.

</details>


### [37] [Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images](https://arxiv.org/abs/2507.22901)
*Shingo Hattori,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 提出了一种加速优化的颜色对搜索方法，用于在LCD图像上嵌入不可见的颜色振动信息，以减少处理时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用串行循环操作搜索构成颜色振动的颜色对，由于搜索空间巨大，处理时间过长。

Method: 通过并行化搜索过程，并利用表示移动量的数组和条件提取操作，加速颜色对的搜索。

Result: 成功减少了颜色对搜索的处理时间，并在九种彩色图像上验证了可叠加的信息量，展示了颜色振动嵌入信息的适用性。

Conclusion: 所提出的并行优化方法显著提高了颜色对搜索的效率，为在公共显示屏上嵌入不可见信息提供了实用解决方案。

Abstract: Large, high-resolution displays are installed throughout the city as public
displays. By superimposing invisible information on the images of these
displays, large numbers of devices with cameras and sensors can communicate
with the displays without prior pairing. Several applications have been
proposed, such as operating robots or communicating information to users by
displaying 2D codes on images. However, the display of 2D codes has the problem
of compromising the appearance of displayed content.
  Abe et al. proposed a method of communicating with devices by superimposing
invisible information using color vibration on images displayed on
off-the-shelf liquid-crystal displays (LCD). Using this method, we can embed
the information for devices in images without interfering with the displayed
content. Abe et al. uses a simple serial loop operation to search for color
pairs comprising a color vibration, which requires a very long processing time
due to the huge search space.
  In this paper, we propose an accelerated and optimized search method for
color pairs that constitute the imperceptible color vibration for embedding
information on LCD images. To achieve fast color pair search, we parallelized
the search process, which is previously done individually, by using arrays
representing the amount of movement and an operation to extract elements from
the array that satisfy the conditions. In addition, we investigate the amount
of information that can be superimposed on nine color images using the
imperceptible color vibration and clarify the applicability of embedding
information into images using the color vibration.

</details>


### [38] [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)
*Hashim Hayat,Maksim Kudrautsau,Evgeniy Makarov,Vlad Melnichenko,Tim Tsykunou,Piotr Varaksin,Matt Pavelle,Adam Z. Oskowitz*

Main category: cs.HC

TL;DR: 研究评估了一种基于多智能体大语言模型的AI系统（Doctronic）在虚拟急诊中的表现，发现其诊断和治疗计划与人类医生高度一致，且在某些情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 全球预计到2030年将短缺1100万医疗从业者，且临床时间中50%用于行政负担。AI可能解决这些问题，但尚无端到端自主的AI系统在真实临床环境中被严格评估。

Method: 回顾性比较了500次虚拟急诊中Doctronic与认证临床医生的表现，通过盲法LLM裁决和专家评审评估诊断一致性、治疗计划一致性和安全性。

Result: Doctronic与医生的诊断一致率为81%，治疗计划一致率为99.2%。AI在36.1%的不一致病例中表现更优，优于人类的9.3%。

Conclusion: 多智能体AI系统在临床决策上表现与人类医生相当甚至更优，可能成为解决医疗劳动力短缺的方案。

Abstract: Background: Globally we face a projected shortage of 11 million healthcare
practitioners by 2030, and administrative burden consumes 50% of clinical time.
Artificial intelligence (AI) has the potential to help alleviate these
problems. However, no end-to-end autonomous large language model (LLM)-based AI
system has been rigorously evaluated in real-world clinical practice. In this
study, we evaluated whether a multi-agent LLM-based AI framework can function
autonomously as an AI doctor in a virtual urgent care setting. Methods: We
retrospectively compared the performance of the multi-agent AI system Doctronic
and board-certified clinicians across 500 consecutive urgent-care telehealth
encounters. The primary end points: diagnostic concordance, treatment plan
consistency, and safety metrics, were assessed by blinded LLM-based
adjudication and expert human review. Results: The top diagnosis of Doctronic
and clinician matched in 81% of cases, and the treatment plan aligned in 99.2%
of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not
supported by clinical findings). In an expert review of discordant cases, AI
performance was superior in 36.1%, and human performance was superior in 9.3%;
the diagnoses were equivalent in the remaining cases. Conclusions: In this
first large-scale validation of an autonomous AI doctor, we demonstrated strong
diagnostic and treatment plan concordance with human clinicians, with AI
performance matching and in some cases exceeding that of practicing clinicians.
These findings indicate that multi-agent AI systems achieve comparable clinical
decision-making to human providers and offer a potential solution to healthcare
workforce shortages.

</details>


### [39] [A blessing or a burden? Exploring worker perspectives of using a social robot in a church](https://arxiv.org/abs/2507.22903)
*Andrew Blair,Peggy Gregory,Mary Ellen Foster*

Main category: cs.HC

TL;DR: 论文探讨了社交机器人在非营利组织（如教会）中的应用，发现参与者对其使用有褒有贬，强调需考虑社会价值以及机器人角色。


<details>
  <summary>Details</summary>
Motivation: 研究社交机器人在非营利组织（如教会）中的潜在应用，关注社会利益而非经济利益。

Method: 与一家活跃的教会合作，对15名参与者进行访谈，采用反思性主题分析法。

Result: 参与者对机器人使用反应不一，认为教会在情感责任上需谨慎，但也认可其在信息提供和减少繁琐任务上的潜力。

Conclusion: 机器人引入需兼顾社会价值和无形利益，而不仅是财务因素。

Abstract: Recent technological advances have allowed robots to assist in the service
sector, and consequently accelerate job and sector transformation. Less
attention has been paid to the use of robots in real-world organisations where
social benefits, as opposed to profits, are the primary motivator. To explore
these opportunities, we have partnered with a working church and visitor
attraction. We conducted interviews with 15 participants from a range of
stakeholder groups within the church to understand worker perspectives of
introducing a social robot to the church and analysed the results using
reflexive thematic analysis. Findings indicate mixed responses to the use of a
robot, with participants highlighting the empathetic responsibility the church
has towards people and the potential for unintended consequences. However,
information provision and alleviation of menial or mundane tasks were
identified as potential use cases. This highlights the need to consider not
only the financial aspects of robot introduction, but also how social and
intangible values shape what roles a robot should take on within an
organisation.

</details>


### [40] [SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches](https://arxiv.org/abs/2507.22904)
*Ehsan Latif,Zirak Khan,Xiaoming Zhai*

Main category: cs.HC

TL;DR: SketchMind是一个基于多代理的认知框架，用于自动化评估和改进学生的科学草图。相比传统方法，它在准确性、解释性和教学适应性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的草图评估方法缺乏解释性、教学对齐性和跨认知层次的适应性，需要更有效的自动化解决方案。

Method: SketchMind采用模块化多代理框架，包括评分解析、草图感知、认知对齐和迭代反馈等模块。

Result: SketchMind在6项科学评估任务中表现优异，平均准确率达到77.1%，显著优于基准模型（如GPT-4o的55.6%）。

Conclusion: SketchMind展示了在AI驱动教育中的潜力，能够通过个性化反馈支持学生的概念成长。

Abstract: Scientific sketches (e.g., models) offer a powerful lens into students'
conceptual understanding, yet AI-powered automated assessment of such
free-form, visually diverse artifacts remains a critical challenge. Existing
solutions often treat sketch evaluation as either an image classification task
or monolithic vision-language models, which lack interpretability, pedagogical
alignment, and adaptability across cognitive levels. To address these
limitations, we present SketchMind, a cognitively grounded, multi-agent
framework for evaluating and improving student-drawn scientific sketches.
SketchMind comprises modular agents responsible for rubric parsing, sketch
perception, cognitive alignment, and iterative feedback with sketch
modification, enabling personalized and transparent evaluation. We evaluate
SketchMind on a curated dataset of 3,575 student-generated sketches across six
science assessment items with different highest order of Bloom's level that
require students to draw models to explain phenomena. Compared to baseline
GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG
integration achieves 77.1% average accuracy (+21.4% average absolute gain). We
also demonstrate that multi-agent orchestration with SRG enhances SketchMind
performance, for example, GPT-4.1 gains an average 8.9% increase in sketch
prediction accuracy, outperforming single-agent pipelines across all items.
Human evaluators rated the feedback and co-created sketches generated by
\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,
significantly higher than those of baseline models (e.g., 2.3 for GPT-4o).
Experts noted the system's potential to meaningfully support conceptual growth
through guided revision. Our code and (pending approval) dataset will be
released to support reproducibility and future research in AI-driven education.

</details>


### [41] [Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction](https://arxiv.org/abs/2507.22905)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.HC

TL;DR: 研究探讨了GPT-3.5、GPT-4和GPT-4o等大型语言模型在生成文化适应性触觉行为以传递情感方面的表现，发现文化匹配、交互角色和文化差异对行为解读和适宜性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在机器人系统中生成文化和社交适宜性触觉行为的潜力，填补相关研究空白。

Method: 生成12种情感在中、比和无特定文化背景下的触觉描述，通过90名参与者评估这些行为的情解读和适宜性。

Result: 文化匹配下成功解读6种情感，交互角色影响适宜性感知，文化不匹配降低解读准确性和适宜性。

Conclusion: LLMs能生成文化适应性触觉行为，但需考虑文化差异和交互角色，以提升适用性和用户接受度。

Abstract: As large language models (LLMs) become increasingly integrated into robotic
systems, their potential to generate socially and culturally appropriate
affective touch remains largely unexplored. This study investigates whether
LLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive
tactile behaviours to convey emotions in human-robot interaction. We produced
text based touch descriptions for 12 distinct emotions across three cultural
contexts (Chinese, Belgian, and unspecified), and examined their
interpretability in both robot-to-human and human-to-robot scenarios. A total
of 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)
evaluated these LLM-generated tactile behaviours for emotional decoding and
perceived appropriateness. Results reveal that: (1) under matched cultural
conditions, participants successfully decoded six out of twelve emotions-mainly
socially oriented emotions such as love and Ekman emotions such as anger,
however, self-focused emotions like pride and embarrassment were more difficult
to interpret; (2) tactile behaviours were perceived as more appropriate when
directed from human to robot than from robot to human, revealing an asymmetry
in social expectations based on interaction roles; (3) behaviours interpreted
as aggressive (e.g., anger), overly intimate (e.g., love), or emotionally
ambiguous (i.e., not clearly decodable) were significantly more likely to be
rated as inappropriate; and (4) cultural mismatches reduced decoding accuracy
and increased the likelihood of behaviours being judged as inappropriate.

</details>


### [42] [Automated Label Placement on Maps via Large Language Models](https://arxiv.org/abs/2507.22952)
*Harry Shomer,Jiejun Xu*

Main category: cs.HC

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的自动标签放置（ALP）新范式，通过任务数据化和上下文感知的空间标注，解决了当前手动标签放置的局限性，并发布了首个真实地图基准数据集MAPLE。


<details>
  <summary>Details</summary>
Motivation: 当前标签放置主要依赖手动操作，自动化系统难以整合制图规范或适应上下文，亟需一种可扩展的解决方案。

Method: 将标签放置任务转化为数据编辑问题，利用检索增强生成（RAG）结合领域指南，通过LLM生成理想标签坐标。

Result: 实验表明，LLM在结构化提示和领域检索引导下能实现精准的空间编辑，符合专家制图标准。

Conclusion: 该研究为AI辅助地图制作提供了可扩展框架，展示了基础模型在结构化数据编辑任务中的潜力。

Abstract: Label placement is a critical aspect of map design, serving as a form of
spatial annotation that directly impacts clarity and interpretability. Despite
its importance, label placement remains largely manual and difficult to scale,
as existing automated systems struggle to integrate cartographic conventions,
adapt to context, or interpret labeling instructions. In this work, we
introduce a new paradigm for automatic label placement (ALP) that formulates
the task as a data editing problem and leverages large language models (LLMs)
for context-aware spatial annotation. To support this direction, we curate
MAPLE, the first known benchmarking dataset for evaluating ALP on real-world
maps, encompassing diverse landmark types and label placement annotations from
open-source data. Our method retrieves labeling guidelines relevant to each
landmark type leveraging retrieval-augmented generation (RAG), integrates them
into prompts, and employs instruction-tuned LLMs to generate ideal label
coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall
performance and generalization across different types of landmarks. This
includes both zero-shot and instruction-tuned performance. Our results
demonstrate that LLMs, when guided by structured prompts and domain-specific
retrieval, can learn to perform accurate spatial edits, aligning the generated
outputs with expert cartographic standards. Overall, our work presents a
scalable framework for AI-assisted map finishing and demonstrates the potential
of foundation models in structured data editing tasks. The code and data can be
found at https://github.com/HarryShomer/MAPLE.

</details>


### [43] [ChatVis: Large Language Model Agent for Generating Scientific Visualizations](https://arxiv.org/abs/2507.23096)
*Tom Peterka,Tanwi Mallick,Orcun Yildiz,David Lenz,Cory Quammen,Berk Geveci*

Main category: cs.HC

TL;DR: ChatVis是一款辅助LLM生成ParaView科学可视化Python代码的工具，无需重新训练或微调LLM，显著提升了专业编程任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在高度专业化编程任务（如科学可视化）中的表现不足问题。

Method: 采用思维链提示简化、基于向量数据库的检索增强提示生成以及迭代反馈错误检查。

Result: 与未经辅助的顶级LLM相比，ChatVis在所有指标上均显著提升。

Conclusion: ChatVis有效提升了LLM在科学可视化任务中的表现，无需额外训练。

Abstract: Large language models (LLMs) are rapidly increasing in capability, but they
still struggle with highly specialized programming tasks such as scientific
visualization. We present an LLM assistant, ChatVis, that aids the LLM to
generate Python code for ParaView scientific visualization tasks, without the
need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought
prompt simplification, retrieval-augmented prompt generation using a vector
database of documentation and code examples, and error checking with iterative
prompt feedback to correct errors until a visualization is produced. An
integral part of our approach is a benchmark suite of canonical visualization
tasks, ParaView regression tests, and scientific use cases that includes
comprehensive evaluation metrics. We evaluate our visualization assistant by
comparing results with a variety of top-performing unassisted LLMs. We find
that all the metrics are significantly improved with ChatVis.

</details>


### [44] [Accessibility Scout: Personalized Accessibility Scans of Built Environments](https://arxiv.org/abs/2507.23190)
*William Huang,Xia Su,Jon E. Froehlich,Yang Zhang*

Main category: cs.HC

TL;DR: 论文介绍了基于大型语言模型（LLMs）的系统Accessibility Scout，通过照片识别建筑环境的无障碍问题，实现个性化与可扩展性结合的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的人工和自动评估方法无法满足残障人士对个性化无障碍环境的需求，LLMs的进展为解决这一问题提供了新思路。

Method: 开发了Accessibility Scout系统，利用LLMs从照片中识别无障碍问题，并通过人机协作实现个性化评估。

Result: 技术评估和用户研究表明，系统能生成超越传统ADA标准的个性化无障碍扫描报告。

Conclusion: 研究展示了LLMs在个性化无障碍评估中的潜力，为未来工作提供了方向。

Abstract: Assessing the accessibility of unfamiliar built environments is critical for
people with disabilities. However, manual assessments, performed by users or
their personal health professionals, are laborious and unscalable, while
automatic machine learning methods often neglect an individual user's unique
needs. Recent advances in Large Language Models (LLMs) enable novel approaches
to this problem, balancing personalization with scalability to enable more
adaptive and context-aware assessments of accessibility. We present
Accessibility Scout, an LLM-based accessibility scanning system that identifies
accessibility concerns from photos of built environments. With use,
Accessibility Scout becomes an increasingly capable "accessibility scout",
tailoring accessibility scans to an individual's mobility level, preferences,
and specific environmental interests through collaborative Human-AI
assessments. We present findings from three studies: a formative study with six
participants to inform the design of Accessibility Scout, a technical
evaluation of 500 images of built environments, and a user study with 10
participants of varying mobility. Results from our technical evaluation and
user study show that Accessibility Scout can generate personalized
accessibility scans that extend beyond traditional ADA considerations. Finally,
we conclude with a discussion on the implications of our work and future steps
for building more scalable and personalized accessibility assessments of the
physical world.

</details>


### [45] [Silent Impact: Tracking Tennis Shots from the Passive Arm](https://arxiv.org/abs/2507.23215)
*Junyong Park,Saelyne Yang,Sungho Jo*

Main category: cs.HC

TL;DR: 提出了一种新颖且用户友好的系统Silent Impact，通过被动手臂传感器分析网球击球动作，减少了对自然运动的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有网球分析设备需在球拍或主动手臂安装传感器，导致不适和干扰，因此需要更舒适的解决方案。

Method: 使用20名业余网球选手的被动手臂IMU传感器数据，训练神经网络检测和分类六种击球动作，并将模型集成到智能手表和手机应用原型中。

Result: 分类准确率88.2%，检测F1分数86.0%，用户研究中参与者感到身心负担更轻。

Conclusion: 被动手臂是一种有效且舒适的分析网球击球动作的替代方案，推动了用户友好的运动分析技术发展。

Abstract: Wearable technology has transformed sports analytics, offering new dimensions
in enhancing player experience. Yet, many solutions involve cumbersome setups
that inhibit natural motion. In tennis, existing products require sensors on
the racket or dominant arm, causing distractions and discomfort. We propose
Silent Impact, a novel and user-friendly system that analyzes tennis shots
using a sensor placed on the passive arm. Collecting Inertial Measurement Unit
sensor data from 20 recreational tennis players, we developed neural networks
that exclusively utilize passive arm data to detect and classify six shots,
achieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,
comparable to the dominant arm. These models were then incorporated into an
end-to-end prototype, which records passive arm motion through a smartwatch and
displays a summary of shots on a mobile app. User study (N=10) showed that
participants felt less burdened physically and mentally using Silent Impact on
the passive arm. Overall, our research establishes the passive arm as an
effective, comfortable alternative for tennis shot analysis, advancing
user-friendly sports analytics.

</details>


### [46] [Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System](https://arxiv.org/abs/2507.23298)
*Kazushi Kato,Koji Inoue,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.HC

TL;DR: 提出了一种实时预测点头时机和类型的模型，基于声音活动投影模型，结合多任务学习与预处理，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 非语言信息（如点头）在人机对话中至关重要，现有系统需要更自然的表达方式。

Method: 扩展声音活动投影模型，实现连续实时的点头预测，结合多任务学习和预处理。

Result: 多任务学习效果显著，降低处理率可保持实时性，主观评价优于传统方法。

Conclusion: 模型在实时点头预测中表现优异，可集成到对话系统中提升交互自然度。

Abstract: In human dialogue, nonverbal information such as nodding and facial
expressions is as crucial as verbal information, and spoken dialogue systems
are also expected to express such nonverbal behaviors. We focus on nodding,
which is critical in an attentive listening system, and propose a model that
predicts both its timing and type in real time. The proposed model builds on
the voice activity projection (VAP) model, which predicts voice activity from
both listener and speaker audio. We extend it to prediction of various types of
nodding in a continuous and real-time manner unlike conventional models. In
addition, the proposed model incorporates multi-task learning with verbal
backchannel prediction and pretraining on general dialogue data. In the timing
and type prediction task, the effectiveness of multi-task learning was
significantly demonstrated. We confirmed that reducing the processing rate
enables real-time operation without a substantial drop in accuracy, and
integrated the model into an avatar attentive listening system. Subjective
evaluations showed that it outperformed the conventional method, which always
does nodding in sync with verbal backchannel. The code and trained models are
available at https://github.com/MaAI-Kyoto/MaAI.

</details>


### [47] [Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models](https://arxiv.org/abs/2507.23470)
*Sebastian Gürtl,Gloria Schimetta,David Kerschbaumer,Michael Liut,Alexander Steinmaurer*

Main category: cs.HC

TL;DR: TL;DR: 论文介绍了DUET，一个基于LLM的工具，通过比较UML和ER图的文本表示提供结构化反馈，支持个性化学习。初步评估显示其在可访问性和扩展性方面有潜力，但也存在可靠性和误用问题。


<details>
  <summary>Details</summary>
Motivation: UML和ER图教学中存在抽象思维和上下文理解的挑战，传统教学方法难以提供大规模和个性化的反馈。DUET旨在通过LLM技术解决这些问题，支持自我学习和指导策略。

Method: DUET使用多阶段LLM管道，将参考图和学生的图转换为文本表示，基于差异生成结构化反馈。工具还提供教育分析功能，帮助改进教学策略。

Result: 通过六名参与者的半结构化访谈评估，DUET在可访问性、扩展性和学习支持方面表现良好，但也存在可靠性和潜在误用的局限性。

Conclusion: DUET展示了将LLM融入建模教育的前景，为未来课堂应用和实证评估奠定了基础。

Abstract: UML and ER diagrams are foundational in computer science education but come
with challenges for learners due to the need for abstract thinking, contextual
understanding, and mastery of both syntax and semantics. These complexities are
difficult to address through traditional teaching methods, which often struggle
to provide scalable, personalized feedback, especially in large classes. We
introduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,
which converts a reference diagram and a student-submitted diagram into a
textual representation and provides structured feedback based on the
differences. It uses a multi-stage LLM pipeline to compare diagrams and
generate reflective feedback. Furthermore, the tool enables analytical insights
for educators, aiming to foster self-directed learning and inform instructional
strategies. We evaluated DUET through semi-structured interviews with six
participants, including two educators and four teaching assistants. They
identified strengths such as accessibility, scalability, and learning support
alongside limitations, including reliability and potential misuse. Participants
also suggested potential improvements, such as bulk upload functionality and
interactive clarification features. DUET presents a promising direction for
integrating LLMs into modeling education and offers a foundation for future
classroom integration and empirical evaluation.

</details>


### [48] [Digital literacy interventions can boost humans in discerning deepfakes](https://arxiv.org/abs/2507.23492)
*Dominique Geissler,Claire Robertson,Stefan Feuerriegel*

Main category: cs.HC

TL;DR: 研究比较了五种数字素养干预措施，提升人们识别Deepfake的能力，结果显示这些方法可提升识别率13个百分点，同时保持对真实信息的信任。


<details>
  <summary>Details</summary>
Motivation: Deepfake可能破坏机构信任和选举结果，需要探索有效的数字素养提升方法。

Method: 测试了五种干预措施（文本引导、视觉演示、游戏化练习、隐式学习、生成解释），并通过1,200名美国参与者的实验验证其效果。

Result: 干预措施可将Deepfake识别率提升13个百分点，且不影响对真实信息的信任。

Conclusion: 该方法可扩展、适用于多样化人群，能有效提升Deepfake识别能力并保持对真实信息的信任。

Abstract: Deepfakes, i.e., images generated by artificial intelligence (AI), can erode
trust in institutions and compromise election outcomes, as people often
struggle to discern real images from deepfakes. Improving digital literacy can
help address these challenges, yet scalable and effective approaches remain
largely unexplored. Here, we compare the efficacy of five digital literacy
interventions to boost people's ability to discern deepfakes: (1) textual
guidance on common indicators of deepfakes; (2) visual demonstrations of these
indicators; (3) a gamified exercise for identifying deepfakes; (4) implicit
learning through repeated exposure and feedback; and (5) explanations of how
deepfakes are generated with the help of AI. We conducted an experiment with
N=1,200 participants from the United States to test the immediate and long-term
effectiveness of our interventions. Our results show that our interventions can
boost deepfake discernment by up to 13 percentage points while maintaining
trust in real images. Altogether, our approach is scalable, suitable for
diverse populations, and highly effective for boosting deepfake detection while
maintaining trust in truthful information.

</details>


### [49] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: 论文提出‘超文本摩擦’设计理念，旨在通过强调可追溯性、结构和摩擦，来在算法驱动的界面中恢复用户控制权。


<details>
  <summary>Details</summary>
Motivation: 当前算法驱动的界面（如推荐系统和生成式AI工具）牺牲了用户自主权，本文旨在通过重新引入经典超文本原则来解决这一问题。

Method: 通过对维基百科与Instagram Explore、Are.na与生成式AI图像工具的比较分析，研究不同系统如何构建用户体验、导航和创作。

Result: 超文本系统强调来源、关联思维和用户驱动的意义构建，而算法系统则模糊过程并削弱参与度。

Conclusion: 超文本价值可作为设计原则，帮助在算法主导的网络中恢复用户自主权。

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [50] [Noise-Coded Illumination for Forensic and Photometric Video Analysis](https://arxiv.org/abs/2507.23002)
*Peter F. Michael,Zekun Hao,Serge Belongie,Abe Davis*

Main category: cs.GR

TL;DR: 论文提出了一种通过编码光照明来嵌入时间水印的方法，以对抗视频伪造问题，利用信息不对称提高验证能力。


<details>
  <summary>Details</summary>
Motivation: 由于视频伪造技术的快速发展，真实视频与伪造视频的区分变得日益困难。作者希望通过编码光照明为视频添加可验证的水印，从而在验证过程中占据信息优势。

Method: 通过在场景照明中编码细微的噪声调制，嵌入时间水印。水印不编码具体信息，而是记录未篡改场景在编码照明下的图像。

Result: 即使攻击者知道该方法的存在，伪造带有编码水印的视频仍需解决更复杂的对抗性问题，且处于信息劣势。

Conclusion: 该方法为保护高价值场景（如公共事件和采访）的视频真实性提供了可行方案，特别是在可控制照明但无法控制拍摄相机的情况下。

Abstract: The proliferation of advanced tools for manipulating video has led to an arms
race, pitting those who wish to sow disinformation against those who want to
detect and expose it. Unfortunately, time favors the ill-intentioned in this
race, with fake videos growing increasingly difficult to distinguish from real
ones. At the root of this trend is a fundamental advantage held by those
manipulating media: equal access to a distribution of what we consider
authentic (i.e., "natural") video. In this paper, we show how coding very
subtle, noise-like modulations into the illumination of a scene can help combat
this advantage by creating an information asymmetry that favors verification.
Our approach effectively adds a temporal watermark to any video recorded under
coded illumination. However, rather than encoding a specific message, this
watermark encodes an image of the unmanipulated scene as it would appear lit
only by the coded illumination. We show that even when an adversary knows that
our technique is being used, creating a plausible coded fake video amounts to
solving a second, more difficult version of the original adversarial content
creation problem at an information disadvantage. This is a promising avenue for
protecting high-stakes settings like public events and interviews, where the
content on display is a likely target for manipulation, and while the
illumination can be controlled, the cameras capturing video cannot.

</details>


### [51] [XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding](https://arxiv.org/abs/2507.23777)
*Dian Chen,Yansong Qu,Xinyang Li,Ming Li,Shengchuan Zhang*

Main category: cs.GR

TL;DR: XSpecMesh是一种质量保持的加速方法，用于自回归网格生成模型，通过轻量级多头推测解码和验证重采样策略，实现1.7倍的加速且不牺牲生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前自回归模型在网格生成时推断延迟高，需优化加速推断过程。

Method: 采用轻量级多头推测解码、验证重采样策略和蒸馏训练解码头。

Result: 实验表明方法实现1.7倍加速且保持生成质量。

Conclusion: XSpecMesh高效加速自回归网格生成，代码将开源。

Abstract: Current auto-regressive models can generate high-quality, topologically
precise meshes; however, they necessitate thousands-or even tens of
thousands-of next-token predictions during inference, resulting in substantial
latency. We introduce XSpecMesh, a quality-preserving acceleration method for
auto-regressive mesh generation models. XSpecMesh employs a lightweight,
multi-head speculative decoding scheme to predict multiple tokens in parallel
within a single forward pass, thereby accelerating inference. We further
propose a verification and resampling strategy: the backbone model verifies
each predicted token and resamples any tokens that do not meet the quality
criteria. In addition, we propose a distillation strategy that trains the
lightweight decoding heads by distilling from the backbone model, encouraging
their prediction distributions to align and improving the success rate of
speculative predictions. Extensive experiments demonstrate that our method
achieves a 1.7x speedup without sacrificing generation quality. Our code will
be released.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [52] [WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems](https://arxiv.org/abs/2507.23419)
*James Rhodes,Lawrence Ong,Duy T. Ngo*

Main category: cs.ET

TL;DR: WiRM是一种两阶段的无线呼吸监测方法，通过相位净化和自适应多迹追踪提高呼吸频率估计准确性，并利用改进的估计结果分解和选择呼吸波形，显著提升了波形相关性。


<details>
  <summary>Details</summary>
Motivation: 现有呼吸监测方法多关注呼吸频率或胸廓运动，但缺乏全面的解决方案。

Method: WiRM采用共轭乘法净化相位，使用自适应多迹追踪（AMTC），并通过改进的呼吸频率估计分解和选择呼吸波形。

Result: WiRM减少了38%的呼吸频率均方根误差，波形相关性提升了178.3%，并在噪声环境中表现稳健。

Conclusion: WiRM在呼吸频率和波形监测方面均优于现有方法，且具备较强的噪声鲁棒性。

Abstract: Monitoring respiratory health with the use of channel state information (CSI)
has shown promising results. Many existing methods focus on monitoring only the
respiratory rate, while others focus on monitoring the motion of the chest as a
patient breathes, which is referred to as the respiratory waveform. This paper
presents WiRM, a two-staged approach to contactless respiration monitoring. In
the first stage, WiRM improves upon existing respiratory rate estimation
techniques by using conjugate multiplication for phase sanitisation and
adaptive multi-trace carving (AMTC) for tracing how the respiratory rate
changes over time. When compared against three state-of-the-art methods, WiRM
has achieved an average reduction of $38\%$ in respiratory rate root mean
squared error (RMSE). In the second stage, WiRM uses this improved respiratory
rate estimate to inform the decomposition and selection of the respiratory
waveform from the CSI data. Remarkably, WiRM delivers a $178.3\%$ improvement
in average absolute correlation with the ground truth respiratory waveform.
Within the literature, it is difficult to compare the robustness of existing
algorithms in noisy environments. In this paper, we develop a purpose-built
simulation toolkit to evaluate the robustness of respiration monitoring
solutions under various noise conditions, including thermal, multiplicative,
and phase noise. Our results show that WiRM demonstrates improved or comparable
resilience to these common noise sources.

</details>


### [53] [SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction](https://arxiv.org/abs/2507.23618)
*Xinyi Guo,Geguang Miao,Shinichi Nishizawa,Hiromitsu Awano,Shinji Kimura,Takashi Sato*

Main category: cs.ET

TL;DR: 提出了一种新型量子纠错解码器SOME，通过将解码任务转化为QUBO问题，显著降低了变量数量和计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统解码器（如MWPM和UF）存在高拓扑复杂度或长解码时间的问题，SOME旨在解决这些局限性。

Method: 将解码任务转化为QUBO问题（OHQ），通过对称独热编码和最小总权重优化构建置换矩阵。

Result: SOME将变量数量减少99.9倍，解码时间从毫秒级降至微秒级，且性能在高误码率下仍优于MWPM。

Conclusion: SOME是一种高效且实用的量子纠错解码器，适用于未来量子计算系统。

Abstract: Conventional quantum error correction (QEC) decoders such as Minimum-Weight
Perfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast
decoding, respectively, but both suffer from high topological complexity. In
contrast, Ising model-based decoders reduce topological complexity but demand
considerable decoding time. We propose the Symmetric One-Hot Matching Elector
(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic
Unconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO
(OHQ). Each variable in the QUBO represents whether a given pair of flipped
syndromes is matched, while the error probabilities between the pair are
encoded as interaction coefficients (weight). Constraints ensure that each
flipped syndrome is matched exactly once. Valid solutions of OHQ correspond to
self-inverse permutation matrices, characterized by symmetric one-hot encoding.
To solve the OHQ efficiently, SOME reformulates the decoding task as the
construction of permutation matrices that minimize the total weight. It
initializes each candidate matrix from one of the minimum-weight syndrome
pairs, then iteratively appends additional pairs in ascending order of weight,
and finally selects the permutation matrix with the lowest total energy. SOME
achieves up to a 99.9x reduction in variable count and reduces decoding times
from milliseconds to microseconds on a single-threaded commodity CPU. OHQ also
maintains performance up to a 10.5% physical error rate, surpassing the highest
known threshold of MWPM@.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with Precision Recovery and Cache-Aware Optimization](https://arxiv.org/abs/2507.23387)
*Weicheng Xue,Baisong Xu,Kai Yang,Yongxiang Liu,Dengdeng Fan,Pengxiang Xu,Yonghong Tian*

Main category: cs.DC

TL;DR: H2SGEMM是一种利用FP16计算单元模拟FP32 GEMM的高性能算法，通过分解数值和调优缩放策略，实现了高精度和性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前低精度矩阵引擎（如FP16）缺乏对全精度计算的支持，因此需要一种在高性能AI加速器上模拟FP32 GEMM的方法。

Method: 将FP32操作数分解为两个FP16值，并通过可调缩放策略补偿数值误差；采用缓存感知分块策略和双缓冲流水线。

Result: H2SGEMM在Ascend 910A NPU上实现了77%的理论FP32峰值性能，同时保持了高精度和数值稳定性。

Conclusion: 该方法不仅恢复了原生FP32 GEMM的精度，在某些条件下还表现出更高的数值稳定性。

Abstract: Low-precision matrix engines, such as FP16 cube, offer high throughput but
lack support for full-precision computation. In this work, we propose H2SGEMM,
a high-performance algorithm for emulating FP32 general matrix-matrix
multiplication (GEMM) using only FP16 computation units on a representative AI
accelerator. The method decomposes each FP32 operand into two FP16 values and
compensates for numerical errors through a tunable scaling strategy. A detailed
analysis of numerical errors, including underflow conditions and precision
loss, guides the selection of scaling parameters to preserve up to 22 bits of
mantissa accuracy. We further investigate the effect of computation order on
accuracy and demonstrate that a term-wise accumulation scheme improves
numerical stability over conventional FP32 GEMM in low-exponent regimes.
Finally, a cache-aware blocking strategy and double-buffered pipeline are
introduced to overlap memory transfers with computation, enabling H2SGEMM to
achieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend
910A NPU lacking native FP32 support. Extensive numerical experiments confirm
that our method not only recovers the accuracy of native FP32 GEMM but also
exhibits superior numerical stability under certain conditions, due to its
structured and error-aware computation order.

</details>


### [55] [Towards a Testbed for Scalable FaaS Platforms](https://arxiv.org/abs/2507.23431)
*Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 研究提出了一个可快速评估不同架构和技术对FaaS平台性能影响的测试框架。


<details>
  <summary>Details</summary>
Motivation: 了解云平台架构如何影响性能，特别是针对FaaS平台的扩展性。

Method: 开发一个研究导向的测试框架，用于评估不同架构和技术选择。

Result: 测试框架能够快速评估FaaS平台的性能和扩展性。

Conclusion: 该框架为研究FaaS平台的性能提供了有效的工具。

Abstract: Most cloud platforms have a Function-as-a-Service (FaaS) offering that
enables users to easily write highly scalable applications. To better
understand how the platform's architecture impacts its performance, we present
a research-focused testbed that can be adapted to quickly evaluate the impact
of different architectures and technologies on the characteristics of
scalability-focused FaaS platforms.

</details>


### [56] [Threshold-Driven Streaming Graph: Expansion and Rumor Spreading](https://arxiv.org/abs/2507.23533)
*Flora Angileri,Andrea Clementi,Emanuele Natale,Michele Salvi,Isabella Ziccardi*

Main category: cs.DC

TL;DR: 该论文研究了动态图模型下RAES算法的行为，证明了在节点流失的动态图中，每个快照Gₜ具有高度的扩展性，并推导出PUSH和PULL谣言传播协议的完成时间上界。


<details>
  <summary>Details</summary>
Motivation: 研究RAES算法在动态图中的表现，特别是节点流失的流式模型，填补了静态图假设的空白。

Method: 采用动态图模型（滑动窗口模型），分析RAES算法在节点流失情况下的扩展性。

Result: 证明动态图中每个快照Gₜ具有高概率的扩展性，并得出PUSH/PULL协议的完成时间上界为对数级别。

Conclusion: RAES算法在动态图中表现良好，为动态网络中的谣言传播提供了理论支持。

Abstract: A randomized distributed algorithm called RAES was introduced in [Becchetti
et al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex
expander graph $G = (V, E)$. The algorithm relies on a simple threshold-based
procedure. A key assumption in [Becchetti et al., SODA 2020] is that the input
graph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain
unchanged throughout the process - while the analysis of RAES in dynamic models
is left as a major open question.
  In this work, we investigate the behavior of RAES under a dynamic graph model
induced by a streaming node-churn process (also known as the sliding window
model), where, at each discrete round, a new node joins the graph and the
oldest node departs. This process yields a bounded-degree dynamic graph
$\mathcal{G} =\{ G_t = (V_t, E_t) : t \in \mathbb{N}\}$ that captures essential
characteristics of peer-to-peer networks -- specifically, node churn and
threshold on the number of connections each node can manage. We prove that
every snapshot $G_t$ in the dynamic graph sequence has good expansion
properties with high probability. Furthermore, we leverage this property to
establish a logarithmic upper bound on the completion time of the well-known
PUSH and PULL rumor spreading protocols over the dynamic graph $\mathcal{G}$.

</details>


### [57] [The ArborX library: version 2.0](https://arxiv.org/abs/2507.23700)
*Andrey Prokopenko,Daniel Arndt,Damien Lebrun-Grandié,Bruno Turcksin*

Main category: cs.DC

TL;DR: ArborX 2.0版本的概述，介绍了其新接口、搜索数据结构、回调功能及支持的算法扩展。


<details>
  <summary>Details</summary>
Motivation: 为了支持更广泛的用户问题，并提升库的功能性和性能。

Method: 引入了新接口、新的搜索数据结构（如暴力搜索、分布式搜索）、回调功能以及扩展的算法支持（如光线追踪、聚类）。

Result: ArborX 2.0提供了更高的灵活性和功能性，能够满足更多用户需求。

Conclusion: ArborX 2.0通过多项改进显著提升了其适用性和性能。

Abstract: This paper provides an overview of the 2.0 release of the ArborX library, a
performance portable geometric search library based on Kokkos. We describe the
major changes in ArborX 2.0 including a new interface for the library to
support a wider range of user problems, new search data structures (brute
force, distributed), support for user functions to be executed on the results
(callbacks), and an expanded set of the supported algorithms (ray tracing,
clustering).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads](https://arxiv.org/abs/2507.23084)
*Taiyi Wang,Eiko Yoneki*

Main category: cs.DB

TL;DR: AutoIndexer结合了工作负载压缩、查询优化和强化学习模型，显著降低了索引选择的复杂性，提高了性能。


<details>
  <summary>Details</summary>
Motivation: 大规模分析工作负载下，索引选择对数据库性能优化至关重要。现有RL方法难适应扩展工作负载。

Method: 通过工作负载压缩、查询优化和专用RL模型，AutoIndexer有效降低搜索复杂度。

Result: 相比基准，查询执行时间减少95%，优于现有RL方法20%，调优时间减半。

Conclusion: AutoIndexer适用于大规模多样化工作负载，实用性显著。

Abstract: Efficiently selecting indexes is fundamental to database performance
optimization, particularly for systems handling large-scale analytical
workloads. While deep reinforcement learning (DRL) has shown promise in
automating index selection through its ability to learn from experience, few
works address how these RL-based index advisors can adapt to scaling workloads
due to exponentially growing action spaces and heavy trial and error. To
address these challenges, we introduce AutoIndexer, a framework that combines
workload compression, query optimization, and specialized RL models to scale
index selection effectively. By operating on compressed workloads, AutoIndexer
substantially lowers search complexity without sacrificing much index quality.
Extensive evaluations show that it reduces end-to-end query execution time by
up to 95% versus non-indexed baselines. On average, it outperforms
state-of-the-art RL-based index advisors by approximately 20% in workload cost
savings while cutting tuning time by over 50%. These results affirm
AutoIndexer's practicality for large and diverse workloads.

</details>


### [59] [Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets](https://arxiv.org/abs/2507.23499)
*Piotr Sowinski,Kacper Grzymkowski,Anastasiya Danilenka*

Main category: cs.DB

TL;DR: Jelly-Patch 是一种高性能的压缩二进制序列化格式，用于 RDF 数据集的变化，显著提升了压缩率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在大型和低延迟的 RDF 应用中，高频率和大规模的更新可能导致性能瓶颈，需要高效的序列化和传输方法。

Method: 提出 Jelly-Patch 格式，并通过基准测试比较其与现有 RDF Patch 格式在压缩率和吞吐量上的表现。

Result: Jelly-Patch 在压缩率上提高了 3.5--8.9 倍，序列化和解析吞吐量分别最高提高了 2.5 倍和 4.6 倍。

Conclusion: Jelly-Patch 显著提升了 RDF 系统的性能，适用于大规模和低延迟应用。

Abstract: Recording data changes in RDF systems is a crucial capability, needed to
support auditing, incremental backups, database replication, and event-driven
workflows. In large-scale and low-latency RDF applications, the high volume and
frequency of updates can cause performance bottlenecks in the serialization and
transmission of changes. To alleviate this, we propose Jelly-Patch -- a
high-performance, compressed binary serialization format for changes in RDF
datasets. To evaluate its performance, we benchmark Jelly-Patch against
existing RDF Patch formats, using two datasets representing different use cases
(change data capture and IoT streams). Jelly-Patch is shown to achieve
3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in
serialization and parsing, respectively. These significant advancements in
throughput and compression are expected to improve the performance of
large-scale and low-latency RDF systems.

</details>


### [60] [DataLens: Enhancing Dataset Discovery via Network Topologies](https://arxiv.org/abs/2507.23515)
*Anaïs Ollagnier,Aline Menin*

Main category: cs.DB

TL;DR: DataLens是一个基于网络的平台，结合了面搜索和高级可视化技术，旨在提高资源发现的效率。


<details>
  <summary>Details</summary>
Motivation: 公开可用的文本资源迅速增长，但现有检索方法依赖关键词和元数据过滤，缺乏高级功能，难以揭示资源间的联系。

Method: DataLens采用面搜索和网络可视化技术，支持多视角数据探索和适应性网络结构。

Result: 用户研究显示，用户高度评价网络可视化工具，并提供了优化数据集搜索的见解。

Conclusion: DataLens通过可视化技术有效提升资源发现能力，未来可进一步优化以更好地支持用户需求。

Abstract: The rapid growth of publicly available textual resources, such as lexicons
and domain-specific corpora, presents challenges in efficiently identifying
relevant resources. While repositories are emerging, they often lack advanced
search and exploration features. Most search methods rely on keyword queries
and metadata filtering, which require prior knowledge and fail to reveal
connections between resources. To address this, we present DataLens, a
web-based platform that combines faceted search with advanced visualization
techniques to enhance resource discovery. DataLens offers network-based
visualizations, where the network structure can be adapted to suit the specific
analysis task. It also supports a chained views approach, enabling users to
explore data from multiple perspectives. A formative user study involving six
data practitioners revealed that users highly value visualization
tools-especially network-based exploration-and offered insights to help refine
our approach to better support dataset search.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [61] [Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance](https://arxiv.org/abs/2507.23088)
*Lalithkumar Seenivasan,Jiru Xu,Roger D. Soberanis Mukul,Hao Ding,Grayson Byrd,Yu-Chun Ku,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.RO

TL;DR: 提出了一种基于语言模型和分割模型的新型感知代理，用于实时手术中自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 现有AI解决方案在动态手术环境中缺乏灵活性，限制了自然的人机交互。

Method: 结合语音集成的大型语言模型、分割模型和记忆机制，实现对已知和未知元素的交互式分割。

Result: 在公开数据集上与手工提示策略性能相当，并能灵活分割新元素。

Conclusion: 该感知代理为动态手术环境中的AI实时辅助提供了更自然的交互方式。

Abstract: Emerging surgical data science and robotics solutions, especially those
designed to provide assistance in situ, require natural human-machine
interfaces to fully unlock their potential in providing adaptive and intuitive
aid. Contemporary AI-driven solutions remain inherently rigid, offering limited
flexibility and restricting natural human-machine interaction in dynamic
surgical environments. These solutions rely heavily on extensive task-specific
pre-training, fixed object categories, and explicit manual-prompting. This work
introduces a novel Perception Agent that leverages speech-integrated
prompt-engineered large language models (LLMs), segment anything model (SAM),
and any-point tracking foundation models to enable a more natural human-machine
interaction in real-time intraoperative surgical assistance. Incorporating a
memory repository and two novel mechanisms for segmenting unseen elements,
Perception Agent offers the flexibility to segment both known and unseen
elements in the surgical scene through intuitive interaction. Incorporating the
ability to memorize novel elements for use in future surgeries, this work takes
a marked step towards human-machine symbiosis in surgical procedures. Through
quantitative analysis on a public dataset, we show that the performance of our
agent is on par with considerably more labor-intensive manual-prompting
strategies. Qualitatively, we show the flexibility of our agent in segmenting
novel elements (instruments, phantom grafts, and gauze) in a custom-curated
dataset. By offering natural human-machine interaction and overcoming rigidity,
our Perception Agent potentially brings AI-based real-time assistance in
dynamic surgical environments closer to reality.

</details>


### [62] [User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals](https://arxiv.org/abs/2507.23544)
*Ryo Miyoshi,Yuki Okafuji,Takuya Iwamoto,Junya Nakanishi,Jun Baba*

Main category: cs.RO

TL;DR: 提出了一种基于多模态社交信号和Transformer模型的方法，用于评估人机交互中的用户体验（UX），其表现优于人工评估。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人需求的增长，准确评估用户体验（HRI中的UX）以实现行为自适应变得至关重要。现有方法多局限于单一层面（如情感或参与度），缺乏全面性。

Method: 通过构建UX数据集并开发基于Transformer的模型，利用面部表情和语音信号，结合多实例学习框架，捕捉短期和长期交互模式。

Result: 实验结果表明，该方法在UX评估中优于第三方人工评估。

Conclusion: 该方法通过多模态信号和时态动态捕捉，提供了更全面的UX表征，有望推动HRI自适应行为的实现。

Abstract: In recent years, the demand for social robots has grown, requiring them to
adapt their behaviors based on users' states. Accurately assessing user
experience (UX) in human-robot interaction (HRI) is crucial for achieving this
adaptability. UX is a multi-faceted measure encompassing aspects such as
sentiment and engagement, yet existing methods often focus on these
individually. This study proposes a UX estimation method for HRI by leveraging
multimodal social signals. We construct a UX dataset and develop a
Transformer-based model that utilizes facial expressions and voice for
estimation. Unlike conventional models that rely on momentary observations, our
approach captures both short- and long-term interaction patterns using a
multi-instance learning framework. This enables the model to capture temporal
dynamics in UX, providing a more holistic representation. Experimental results
demonstrate that our method outperforms third-party human evaluators in UX
estimation.

</details>


### [63] [Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation](https://arxiv.org/abs/2507.23592)
*Haiyun Zhang,Stefano Dalla Gasperina,Saad N. Yousaf,Toshimitsu Tsuboi,Tetsuya Narita,Ashish D. Deshpande*

Main category: cs.RO

TL;DR: 提出一种基于冗余关节传感和残差加权优化的手部外骨骼校准框架，显著减少关节和指尖跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 解决手部外骨骼因用户解剖差异和佩戴不一致导致的跟踪精度问题。

Method: 使用冗余关节传感和残差加权优化策略估计虚拟链接参数，并通过运动捕捉数据调优成本函数权重。

Result: 在七名受试者中，关节和指尖跟踪误差较未校准和均匀加权模型显著降低。

Conclusion: 该框架适用于闭环运动学和最小传感的多种外骨骼设计，为高保真远程操作和示范学习奠定基础。

Abstract: Hand exoskeletons are critical tools for dexterous teleoperation and
immersive manipulation interfaces, but achieving accurate hand tracking remains
a challenge due to user-specific anatomical variability and donning
inconsistencies. These issues lead to kinematic misalignments that degrade
tracking performance and limit applicability in precision tasks. We propose a
subject-specific calibration framework for exoskeleton-based hand tracking that
uses redundant joint sensing and a residual-weighted optimization strategy to
estimate virtual link parameters. Implemented on the Maestro exoskeleton, our
method improves joint angle and fingertip position estimation across users with
varying hand geometries. We introduce a data-driven approach to empirically
tune cost function weights using motion capture ground truth, enabling more
accurate and consistent calibration across participants. Quantitative results
from seven subjects show substantial reductions in joint and fingertip tracking
errors compared to uncalibrated and evenly weighted models. Qualitative
visualizations using a Unity-based virtual hand further confirm improvements in
motion fidelity. The proposed framework generalizes across exoskeleton designs
with closed-loop kinematics and minimal sensing, and lays the foundation for
high-fidelity teleoperation and learning-from-demonstration applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)
*Renato Vukovic,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Hsien-Chin Lin,Shutong Feng,Nurul Lubis,Milica Gasic*

Main category: cs.CL

TL;DR: 论文提出了TeQoDO方法，利用大语言模型（LLMs）的SQL编程能力和对话理论，无需监督即可构建任务导向对话（TOD）的本体。该方法在对话状态跟踪任务中表现优异，并能扩展至大规模本体构建。


<details>
  <summary>Details</summary>
Motivation: 传统的TOD系统依赖外部数据库和显式本体以确保可解释性和可控性，但构建这些本体需要人工标注或监督训练。TeQoDO旨在通过LLMs自主构建本体，提高可解释性和信任度。

Method: TeQoDO通过结合LLMs的SQL编程能力和对话理论，无需监督即可从零开始构建TOD本体。

Result: 实验显示，TeQoDO在性能上优于迁移学习方法，构建的本体在下游对话状态跟踪任务中具有竞争力。此外，TeQoDO还能扩展至大规模本体构建。

Conclusion: TeQoDO为LLMs的可解释性应用提供了一条新途径，展示了本体在大规模任务中的潜力。

Abstract: Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.

</details>


### [65] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 本文对五种主流大型语言模型（GPT、Claude、Perplexity、Gemini和DeepSeek）在金融自然语言处理任务中的表现进行了系统比较，发现GPT的表现最优，而Gemini和DeepSeek的输出变异性较大。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在金融领域应用广泛，但对其系统性的比较研究仍然不足。

Method: 通过人工标注、自动化语义度量（如ROUGE、余弦相似度、Jaccard）和模型行为诊断三种方法，评估模型在10-K文件上的表现。

Result: GPT生成的回答最连贯且语义对齐；Claude和Perplexity次之；Gemini和DeepSeek输出变异性大且一致性较低。

Conclusion: 模型表现受提示设计和源材料影响显著，需进一步研究以提高其在金融领域的适用性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [66] [Complexity-energy trade-off in programmable unitary interferometers](https://arxiv.org/abs/2507.22972)
*Nikita A. Nemkov,Stanislav S. Straupe*

Main category: physics.optics

TL;DR: 该论文指出当前多端口干涉仪在矩阵乘法实现中的编程复杂性问题，并认为这种复杂性是固有而非偶然的。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在集成光子学中实现高效的矩阵乘法，同时分析现有干涉仪架构的编程复杂性和能量效率限制。

Method: 论文分析了现有干涉仪架构（如MZI和分束器网格）的编程复杂性，并提出了关于高效编程算法的固有局限性的论点。

Result: 研究发现，高效编程算法通常会导致较低的有用输出能量，从而限制了干涉仪的准确性和能量效率。

Conclusion: 论文得出结论，当前干涉仪的编程复杂性是固有特性，且在提高效率的同时可能牺牲输出能量，这为未来研究提供了方向。

Abstract: Coherent multiport interferometers are a promising approach to realize matrix
multiplication in integrated photonics. However, most known architectures -
such as MZI and beamsplitter meshes, as well as more general interferometers -
suffer from complicated procedures for mapping the matrix elements of the
desired transformation to specific phaseshifts in the device. We point out that
the high programming complexity is intrinsic, rather than accidental. At the
same time, we argue that interferometers admitting efficient programming
algorithms in general yield a much lower useful output energy, which ultimately
limits their accuracy and energy efficiency.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [67] [Typing Tensor Calculus in 2-Categories (I)](https://arxiv.org/abs/1908.01212)
*Fatimah Rita Ahmadi*

Main category: math.CT

TL;DR: 该论文提出了一种基于范畴论的线性代数计算框架，通过矩阵和张量的范畴化表示，实现了无索引、类型化的高效算法开发，适用于函数式编程语言和并行计算。


<details>
  <summary>Details</summary>
Motivation: 动机在于为线性代数计算提供一个统一的理论框架，以支持高效算法的开发，并适应函数式编程语言和并行计算的需求。

Method: 方法是将矩阵视为矩阵范畴中的态射，并通过半加性2-范畴扩展以涵盖高阶张量。进一步推广到任意幺半半加性范畴和2-范畴，具体操作在2Vec范畴中演示。

Result: 结果是构建了一个无索引、类型化的框架，能够表示和处理四维以下的张量，并为高效计算提供了理论基础。

Conclusion: 结论是该框架成功地将线性代数计算范畴化，为算法设计和并行计算提供了新的理论基础和工具。

Abstract: To formalize calculations in linear algebra for the development of efficient
algorithms and a framework suitable for functional programming languages and
faster parallelized computations, we adopt an approach that treats elements of
linear algebra, such as matrices, as morphisms in the category of matrices,
$\mathbf{Mat_{k}}$. This framework is further extended by generalizing the
results to arbitrary monoidal semiadditive categories. To enrich this
perspective and accommodate higher-rank matrices (tensors), we define
semiadditive 2-categories, where matrices $T_{ij}$ are represented as
1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This
formalization provides an index-free, typed linear algebra framework that
includes matrices and tensors with up to four indices. Furthermore, we extend
the framework to monoidal semiadditive 2-categories and demonstrate detailed
operations and vectorization within the 2-category of 2Vec introduced by
Kapranov and Voevodsky.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [68] [Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation](https://arxiv.org/abs/2507.23398)
*Oliver Bause,Julia Werner,Paul Palomero Bernardo,Oliver Bringmann*

Main category: eess.IV

TL;DR: 针对边缘设备开发了一种高效的CNN模型，直接在Bayer图像上分类，省去了RGB转换步骤，显著降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的边缘设备，传统深度神经网络模型过大且能耗高，提出了一种直接在Bayer图像上分类的方案以减少能量消耗。

Method: 采用仅含63,000参数的CNN模型，结合Viterbi解码的时间序列分析，直接在Bayer图像上进行器官分类。

Result: 实现了93.06%的分类准确率，每张图像仅消耗5.31μJ能量，节省了89.9%的能耗。

Conclusion: 该方法显著提升了视频胶囊内窥镜的电池寿命，为资源受限设备提供了一种高效的AI分类方案。

Abstract: For many real-world applications involving low-power sensor edge devices deep
neural networks used for image classification might not be suitable. This is
due to their typically large model size and require- ment of operations often
exceeding the capabilities of such resource lim- ited devices. Furthermore,
camera sensors usually capture images with a Bayer color filter applied, which
are subsequently converted to RGB images that are commonly used for neural
network training. However, on resource-constrained devices, such conversions
demands their share of energy and optimally should be skipped if possible. This
work ad- dresses the need for hardware-suitable AI targeting sensor edge
devices by means of the Video Capsule Endoscopy, an important medical proce-
dure for the investigation of the small intestine, which is strongly limited by
its battery lifetime. Accurate organ classification is performed with a final
accuracy of 93.06% evaluated directly on Bayer images involv- ing a CNN with
only 63,000 parameters and time-series analysis in the form of Viterbi
decoding. Finally, the process of capturing images with a camera and raw image
processing is demonstrated with a customized PULPissimo System-on-Chip with a
RISC-V core and an ultra-low power hardware accelerator providing an
energy-efficient AI-based image clas- sification approach requiring just 5.31
{\mu}J per image. As a result, it is possible to save an average of 89.9% of
energy before entering the small intestine compared to classic video capsules.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: 论文介绍了KLLM框架，通过硬件-软件协同设计解决大语言模型（LLM）推理中的量化挑战，包括高效的K-Means量化计算和在线异常检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理对内存和计算资源需求极高，传统量化方法难以兼顾准确性和硬件效率。

Method: 提出KLLM框架，采用基于索引的计算方案降低量化计算开销，并设计新型在线异常检测引擎Orizuru。

Result: 实验显示KLLM在速度和能效上显著优于A100 GPU和Atom。

Conclusion: KLLM为LLM推理提供了一种高效、低延迟的解决方案。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [70] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: 本文提出了一种利用IEEE 754 NaN值传播特性检测黑箱函数稀疏性的方法，解决了传统有限差分法因梯度为零导致的假阴性问题，显著提升了梯度计算的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统有限差分法在稀疏性检测中因梯度为零产生假阴性，导致梯度计算错误且难以诊断。通过利用IEEE 754 NaN的传播特性，可以更准确地检测输入输出依赖关系。

Method: 采用NaN传播技术，通过系统性地将输入标记为NaN并观察输出变化来重建稀疏性模式，消除了假阴性问题。该方法兼容IEEE 754标准，无需修改黑箱代码。

Result: 在航空航天翼重模型中，该方法实现了1.52倍的加速，并检测到传统方法遗漏的多个依赖关系，显著提升了优化效率。

Conclusion: NaN传播技术提供了一种高效、跨语言的稀疏性检测方法，解决了传统方法的局限性，适用于复杂的工程优化问题。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [71] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: 该论文提出了一种基于量化SNN的低功耗强化学习算法，在SpiNNaker2芯片上实现了高效的实时控制任务，相比GPU可降低32%的能耗。


<details>
  <summary>Details</summary>
Motivation: 通过SNN和SpiNNaker2芯片的结合，解决传统计算平台的高能耗问题，实现低延迟、低功耗的强化学习算法。

Method: 使用Q-learning算法训练SNN，并量化至8位精度，部署在SpiNNaker2芯片上。与GPU基线比较延迟、功耗和能耗。

Result: SpiNNaker2在能耗上比GPU降低32倍，延迟表现相当，部分任务中表现更优。

Conclusion: SpiNNaker2在低功耗神经形态计算中具有潜力，适合实时控制和高效深度Q学习。

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


### [72] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍了一个用于序列建模的神经网络层API和库，支持逐层和逐步执行，具有状态管理和强一致性保障。


<details>
  <summary>Details</summary>
Motivation: 旨在简化序列模型的创建，使其既能逐层执行（如教师强制训练），又能逐步执行（如自回归采样）。

Method: 通过定义显式状态和逐步演化方法，确保与无状态逐层调用结果一致，提供组合式和声明式API。

Result: 实现了复杂模型的即时流式处理，减少了常见错误，支持多种深度学习库。

Conclusion: SequenceLayers库通过状态管理和API设计，简化了生产级序列模型的构建，同时确保正确性。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>


### [73] [Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions](https://arxiv.org/abs/2507.23335)
*Qilin Zhou,Haipeng Wang,Zhengyuan Wei,W. K. Chan*

Main category: cs.LG

TL;DR: Patch robustness certification是一种新兴的验证方法，用于防御对抗性补丁攻击并确保深度学习系统的可证明安全性。CostCert是一种新型、可扩展且精确的认证恢复技术，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有技术在适用于top-k预测时，通常进行标签间的两两比较，由于攻击者的控制（攻击预算），无法精确认证top k预测标签中的唯一真实标签。

Method: CostCert通过验证攻击预算是否不足以覆盖最小的总附加票数来排除真实标签，避免了组合爆炸问题。

Result: 实验表明，CostCert显著优于当前最佳技术PatchGuard，例如在补丁大小为96时仍然保持57.3%的认证准确率，而PatchGuard已降至零。

Conclusion: CostCert提供了一种高效且精确的认证方法，解决了现有技术的局限性，并在实验中表现出卓越的性能。

Abstract: Patch robustness certification is an emerging verification approach for
defending against adversarial patch attacks with provable guarantees for deep
learning systems. Certified recovery techniques guarantee the prediction of the
sole true label of a certified sample. However, existing techniques, if
applicable to top-k predictions, commonly conduct pairwise comparisons on those
votes between labels, failing to certify the sole true label within the top k
prediction labels precisely due to the inflation on the number of votes
controlled by the attacker (i.e., attack budget); yet enumerating all
combinations of vote allocation suffers from the combinatorial explosion
problem. We propose CostCert, a novel, scalable, and precise voting-based
certified recovery defender. CostCert verifies the true label of a sample
within the top k predictions without pairwise comparisons and combinatorial
explosion through a novel design: whether the attack budget on the sample is
infeasible to cover the smallest total additional votes on top of the votes
uncontrollable by the attacker to exclude the true labels from the top k
prediction labels. Experiments show that CostCert significantly outperforms the
current state-of-the-art defender PatchGuard, such as retaining up to 57.3% in
certified accuracy when the patch size is 96, whereas PatchGuard has already
dropped to zero.

</details>


### [74] [LLM-Assisted Cheating Detection in Korean Language via Keystrokes](https://arxiv.org/abs/2507.22956)
*Dong Hyun Roh,Rajesh Kumar,An Ngo*

Main category: cs.LG

TL;DR: 本文提出了一个基于击键的动态框架，用于检测韩语中基于LLM的作弊行为，填补了先前研究在语言覆盖、认知上下文和LLM参与粒度上的空白。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在检测LLM辅助作弊时的语言覆盖、认知上下文和LLM参与粒度方面的不足。

Method: 使用了一个包括69名参与者的数据集，参与者在三种条件下完成写作任务（真实写作、改写ChatGPT回答和转录ChatGPT回答），并提取了时序和节奏特征，评估了多种分类器。

Result: 时序特征在认知感知场景下表现良好，节奏特征在跨认知场景中更具泛化性；检测真实和转录反应比改写更容易，模型显著优于人类评估者。

Conclusion: 击键动态能够可靠地检测不同认知需求和写作策略下的LLM辅助写作，包括改写和转录。

Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted
cheating in Korean, addressing key gaps in prior research regarding language
coverage, cognitive context, and the granularity of LLM involvement. Our
proposed dataset includes 69 participants who completed writing tasks under
three conditions: Bona fide writing, paraphrasing ChatGPT responses, and
transcribing ChatGPT responses. Each task spans six cognitive processes defined
in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and
create). We extract interpretable temporal and rhythmic features and evaluate
multiple classifiers under both Cognition-Aware and Cognition-Unaware settings.
Temporal features perform well under Cognition-Aware evaluation scenarios,
while rhythmic features generalize better under cross-cognition scenarios.
Moreover, detecting bona fide and transcribed responses was easier than
paraphrased ones for both the proposed models and human evaluators, with the
models significantly outperforming the humans. Our findings affirm that
keystroke dynamics facilitate reliable detection of LLM-assisted writing across
varying cognitive demands and writing strategies, including paraphrasing and
transcribing LLM-generated responses.

</details>


### [75] [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](https://arxiv.org/abs/2507.23756)
*Diana Mortagua*

Main category: cs.LG

TL;DR: 论文提出了一种基于知识推荐系统的查询-标注者对策略，综合考虑标注者过去准确率、情绪和疲劳水平，以减少标注错误并提高模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 主动学习在减少标注数据需求的同时，仍面临标注错误问题，现有策略未充分考量影响标注者生产力的内部因素（如情绪、疲劳）。

Method: 采用知识推荐系统，结合标注者历史准确率、情绪和疲劳水平，为其排序并选择最佳标注者。

Result: 新策略显著减少标注错误和模型训练不确定性，同时在准确率和F1分数上有所提升。

Conclusion: 研究首次探讨了人类认知因素对主动学习的影响，为未来相关领域研究奠定了基础。

Abstract: This study centers on overcoming the challenge of selecting the best
annotators for each query in Active Learning (AL), with the objective of
minimizing misclassifications. AL recognizes the challenges related to cost and
time when acquiring labeled data, and decreases the number of labeled data
needed. Nevertheless, there is still the necessity to reduce annotation errors,
aiming to be as efficient as possible, to achieve the expected accuracy faster.
Most strategies for query-annotator pairs do not consider internal factors that
affect productivity, such as mood, attention, motivation, and fatigue levels.
This work addresses this gap in the existing literature, by not only
considering how the internal factors influence annotators (mood and fatigue
levels) but also presenting a new query-annotator pair strategy, using a
Knowledge-Based Recommendation System (RS). The RS ranks the available
annotators, allowing to choose one or more to label the queried instance using
their past accuracy values, and their mood and fatigue levels, as well as
information about the instance queried. This work bases itself on existing
literature on mood and fatigue influence on human performance, simulating
annotators in a realistic manner, and predicting their performance with the RS.
The results show that considering past accuracy values, as well as mood and
fatigue levels reduces the number of annotation errors made by the annotators,
and the uncertainty of the model through its training, when compared to not
using internal factors. Accuracy and F1-score values were also better in the
proposed approach, despite not being as substantial as the aforementioned. The
methodologies and findings presented in this study begin to explore the open
challenge of human cognitive factors affecting AL.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [76] [Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving](https://arxiv.org/abs/2507.23042)
*Santosh Patapati,Trisanth Srinivasan*

Main category: cs.CV

TL;DR: NovaDrive是一个单分支的视觉语言架构，通过处理前摄像头图像、高清地图、LiDAR深度和文本航点，结合轻量级交叉注意力块和平滑损失函数，实现了实时推理，显著提升了自动驾驶的性能和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要在毫秒内应对复杂交通情况，对道路几何和交通意图进行快速推理，NovaDrive旨在通过多模态融合简化架构并提升性能。

Method: 采用单分支架构，结合前摄像头图像、高清地图、LiDAR深度和文本航点，设计了两阶段交叉注意力块和平滑损失函数，并对预训练的视觉语言模型进行部分微调。

Result: 在nuScenes/Waymo子集上，NovaDrive将成功率提高到84%，路径效率（SPL）提升至0.66，碰撞频率从2.6%降至1.2%。

Conclusion: NovaDrive通过多模态融合和优化方法显著提升了自动驾驶的性能和安全性，并有望扩展到其他具身AI领域。

Abstract: Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.

</details>


### [77] [Phi-Ground Tech Report: Advancing Perception in GUI Grounding](https://arxiv.org/abs/2507.23779)
*Miaosen Zhang,Ziqiang Xu,Jialiang Zhu,Qi Dai,Kai Qiu,Yifan Yang,Chong Luo,Tianyi Chen,Justin Wagle,Tim Franklin,Baining Guo*

Main category: cs.CV

TL;DR: 论文研究了GUI grounding技术，开发了Phi-Ground模型家族，在多个基准测试中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理模型的发展使得计算机使用代理（CUAs）成为可能，但现有端到端grounding模型的准确率不足65%，影响了实际部署。

Method: 通过对从数据收集到模型训练的细节进行实证研究，开发了Phi-Ground模型家族。

Result: Phi-Ground在5个grounding基准测试中表现最佳，在端到端设置下分别在ScreenSpot-pro和UI-Vision上取得43.2和27.2的分数。

Conclusion: 研究中讨论的细节和成功经验不仅适用于grounding模型构建，也对其他感知任务有帮助。

Abstract: With the development of multimodal reasoning models, Computer Use Agents
(CUAs), akin to Jarvis from \textit{"Iron Man"}, are becoming a reality. GUI
grounding is a core component for CUAs to execute actual actions, similar to
mechanical control in robotics, and it directly leads to the success or failure
of the system. It determines actions such as clicking and typing, as well as
related parameters like the coordinates for clicks. Current end-to-end
grounding models still achieve less than 65\% accuracy on challenging
benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from
being ready for deployment. % , as a single misclick can result in unacceptable
consequences. In this work, we conduct an empirical study on the training of
grounding models, examining details from data collection to model training.
Ultimately, we developed the \textbf{Phi-Ground} model family, which achieves
state-of-the-art performance across all five grounding benchmarks for models
under $10B$ parameters in agent settings. In the end-to-end model setting, our
model still achieves SOTA results with scores of \textit{\textbf{43.2}} on
ScreenSpot-pro and \textit{\textbf{27.2}} on UI-Vision. We believe that the
various details discussed in this paper, along with our successes and failures,
not only clarify the construction of grounding models but also benefit other
perception tasks. Project homepage:
\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}

</details>


### [78] [Consistent Point Matching](https://arxiv.org/abs/2507.23609)
*Halid Ziya Yerebakan,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 通过将一致性启发式引入点匹配算法，提高了医学图像解剖位置匹配的鲁棒性，并在多模态数据集上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 针对医学图像解剖位置匹配的鲁棒性问题，提出改进方法以提高准确性和效率。

Method: 将一致性启发式整合到点匹配算法中，无需机器学习模型或训练数据。

Result: 在Deep Lesion Tracking数据集上实现优于现有技术的效果，且算法在标准CPU硬件上高效运行。

Conclusion: 该方法为医学图像导航提供了一种高精度且高效的新途径。

Abstract: This study demonstrates that incorporating a consistency heuristic into the
point-matching algorithm \cite{yerebakan2023hierarchical} improves robustness
in matching anatomical locations across pairs of medical images. We validated
our approach on diverse longitudinal internal and public datasets spanning CT
and MRI modalities. Notably, it surpasses state-of-the-art results on the Deep
Lesion Tracking dataset. Additionally, we show that the method effectively
addresses landmark localization. The algorithm operates efficiently on standard
CPU hardware and allows configurable trade-offs between speed and robustness.
The method enables high-precision navigation between medical images without
requiring a machine learning model or training data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 介绍了一种使用双代理架构的LLM代理，用于将自然语言查询转换为SQL语句，并连接到ERP系统。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询转换为ERP系统可执行SQL的可靠性问题。

Method: 提出了一种结合推理和批判阶段的双代理架构，利用开源LLM。

Result: 提高了查询生成的可靠性。

Conclusion: 双代理架构显著提升了LLM代理在工业ERP系统中的表现。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [80] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文探讨了数据准备（DRAI）原则在领导级科学数据集中的应用，提出了一个针对高性能计算环境的二维成熟度框架，以标准化跨领域支持可扩展的科学AI。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为科学领域的基础模型训练提供数据准备的标准框架，解决数据预处理和跨领域挑战。

Method: 通过分析四个代表性领域的工作流，提出了一个二维框架（数据准备级别和处理阶段），专注于高性能计算环境和生成模型。

Result: 结果表明该框架能有效描述科学数据准备状态，并指导基础设施开发，支持可扩展和可复现的AI训练。

Conclusion: 结论是该框架为跨领域科学数据提供了标准化的准备方法，有助于推动科学AI的规模化应用。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [81] [Rational complex Bezier curves](https://arxiv.org/abs/2507.23485)
*A. Canton,L. Fernandez-Jambrina,M. J. Vazquez-Gallo*

Main category: math.NA

TL;DR: 本文提出了有理复贝塞尔曲线的形式化方法，扩展了CAD范式，允许控制多边形和权重取复数值。


<details>
  <summary>Details</summary>
Motivation: 扩展CAD范式以引入复数值控制多边形和权重，利用复数投影变换实现更灵活的设计操作。

Method: 通过复数值扩展有理贝塞尔曲线，利用实数平面和复数平面的投影变换群，并引入多项式的判别式来降低曲线次数。

Result: 该方法可以简化某些曲线设计，例如通过判别式判断有理三次曲线是否为圆锥曲线。

Conclusion: 复数框架扩展了曲线设计的灵活性，能够实现更高效的几何变换和降阶操作。

Abstract: In this paper we develop the formalism of rational complex Bezier curves.
This framework is a simple extension of the CAD paradigm, since it describes
arc of curves in terms of control polygons and weights, which are extended to
complex values. One of the major advantages of this extension is that we may
make use of two different groups of projective transformations. Besides the
group of projective transformations of the real plane, we have the group of
complex projective transformations. This allows us to apply useful
transformations like the geometric inversion to curves in design. In addition
to this, the use of the complex formulation allows to lower the degree of the
curves in some cases. This can be checked using the resultant of two
polynomials and provides a simple formula for determining whether a rational
cubic curve is a conic or not. Examples of application of the formalism to
classical curves are included.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [82] [Scalable contribution bounding to achieve privacy](https://arxiv.org/abs/2507.23432)
*Vincent Cohen-Addad,Alessandro Epasto,Jason Lee,Morteza Zadimoghaddam*

Main category: cs.DS

TL;DR: 提出了一种分布式算法，通过超图建模和并行处理解决多所有者数据集中用户级差分隐私的贡献限制问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据集中单个记录可能属于多个所有者，实现用户级差分隐私需要限制每个用户的总贡献，但现有方法计算开销高且难以扩展。

Method: 将所有权结构建模为超图，用户为顶点，记录为超边；采用多轮并行处理，要求所有所有者一致同意才添加记录。

Result: 算法提供了一种高效、可扩展的方案，能在保证隐私的同时最大化数据集规模。

Conclusion: 该方法为大规模实际系统中实现用户级差分隐私提供了实用解决方案。

Abstract: In modern datasets, where single records can have multiple owners, enforcing
user-level differential privacy requires capping each user's total
contribution. This "contribution bounding" becomes a significant combinatorial
challenge. Existing sequential algorithms for this task are computationally
intensive and do not scale to the massive datasets prevalent today. To address
this scalability bottleneck, we propose a novel and efficient distributed
algorithm. Our approach models the complex ownership structure as a hypergraph,
where users are vertices and records are hyperedges. The algorithm proceeds in
rounds, allowing users to propose records in parallel. A record is added to the
final dataset only if all its owners unanimously agree, thereby ensuring that
no user's predefined contribution limit is violated. This method aims to
maximize the size of the resulting dataset for high utility while providing a
practical, scalable solution for implementing user-level privacy in large,
real-world systems.

</details>
