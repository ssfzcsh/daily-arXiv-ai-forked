<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [math.NT](#math.NT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该论文通过系统性映射研究，总结了如何将领域知识有效融入需求工程的现有方法、技术和工具，分析了主要类型、质量属性及挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 领域知识对需求工程的成功至关重要，但目前缺乏系统性的方法将其有效运用于实践中。论文旨在填补这一空白。

Method: 采用混合搜索策略的系统性映射研究，结合数据库搜索与迭代的前后向滚雪球法，筛选出75篇符合条件的论文进行分析。

Result: 总结了领域知识在需求工程中的主要应用类型、常见质量属性以及挑战，为研究者和实践者提供了现有方法与未解决问题的支持。

Conclusion: 研究为知识驱动的需求工程建立了概念和方法基础，并提出了未来开发可扩展、自动化解决方案的方向。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [2] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [3] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [4] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文介绍了五个基于真实场景的RAG系统应用，涵盖多个领域，并通过用户评估总结出十二个关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 弥补RAG系统在真实用例中缺乏实证研究的不足，提供实践经验和技术挑战的文档化总结。

Method: 开发五个领域特定的RAG应用，结合多语言OCR、语义检索和领域适应LLM，并通过100名用户的网络评估。

Result: 评估结果显示系统在易用性、相关性等六个维度表现良好，同时总结了十二个关键经验教训。

Conclusion: RAG系统在实际应用中面临技术、操作和伦理挑战，但通过系统化开发和评估可以提升其可靠性和可用性。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [5] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [6] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: 该论文提出了一种基于函数间多边关联分析的漏洞检测框架（IFMA-VD），通过构建代码行为超图并利用超边卷积提取多边关联特征，显著提升漏洞检测的准确性和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法大多关注独立函数，忽视了函数间的复杂多边关联，可能导致漏洞漏检。为此，论文旨在填补这一空白，提升漏洞检测的效果。

Method: 论文首先将函数解析为代码属性图以生成函数内特征，然后通过分割程序依赖图构建代码行为超图，将行为特征编码为超边，最后利用超图网络捕获多边关联知识以增强漏洞检测。

Result: 实验表明，IFMA-VD在三个广泛使用的漏洞数据集上相比基线方法在F-measure和Recall指标上有显著提升，并验证了多边关联特征对代码特征表征的增强作用。

Conclusion: IFMA-VD通过多边关联分析显著提升了漏洞检测性能，证明了其在真实数据集上的有效性。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [7] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [8] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 该研究系统评估了在自动程序修复（APR）任务中常见的CoT技术表现，并提出了新颖的框架$T^3$，结合了LLMs的强大推理能力和树搜索，显著提升了修复方案生成的精度。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）和思维链（CoT）技术提升自动程序修复的效率，但因APR领域对复杂逻辑和多步推理的要求，CoT技术的应用尚未充分开发。

Method: 提出$T^3$框架，整合LLMs的推理能力与树搜索技术，优化样本选择和修复策略。

Result: $T^3$显著提高了生成候选修复方案的精度，并为APR任务提供了有效的调试框架。

Conclusion: $T^3$框架为高效自动程序修复提供了新思路，展示了LLMs与树搜索结合在APR任务中的潜力。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks](https://arxiv.org/abs/2506.20762)
*Shisheng Hu,Jie Gao,Xue Qin,Conghao Zhou,Xinyu Huang,Mushu Li,Mingcheng He,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出了一种用于协作集成感知与通信网络的漂移自适应切片资源管理方案，通过数字孪生技术优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备和感知目标空间分布非平稳性导致的资源分配漂移问题。

Method: 建立两个网络切片分别提供感知和通信服务，利用数字孪生技术开发漂移自适应统计模型和仿真功能。

Result: 服务满意度提升18%，资源消耗降低13.1%。

Conclusion: 漂移自适应切片方案显著优化了网络资源管理效率。

Abstract: In this paper, we propose a novel drift-adaptive slicing-based resource
management scheme for cooperative integrated sensing and communication (ISAC)
networks. Particularly, we establish two network slices to provide sensing and
communication services, respectively. In the large-timescale planning for the
slices, we partition the sensing region of interest (RoI) of each mobile device
and reserve network resources accordingly, facilitating low-complexity
distance-based sensing target assignment in small timescales. To cope with the
non-stationary spatial distributions of mobile devices and sensing targets,
which can result in the drift in modeling the distributions and ineffective
planning decisions, we construct digital twins (DTs) of the slices. In each DT,
a drift-adaptive statistical model and an emulation function are developed for
the spatial distributions in the corresponding slice, which facilitates
closed-form decision-making and efficient validation of a planning decision,
respectively. Numerical results show that the proposed drift-adaptive
slicing-based resource management scheme can increase the service satisfaction
ratio by up to 18% and reduce resource consumption by up to 13.1% when compared
with benchmark schemes.

</details>


### [10] [Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees](https://arxiv.org/abs/2506.21406)
*Tommaso Bonato,Daniele De Sensi,Salvatore Di Girolamo,Abdulla Bataineh,David Hewson,Duncan Roweth,Torsten Hoefler*

Main category: cs.NI

TL;DR: 提出了一种新的自适应路由算法flowcut switching，确保网络数据包的有序传输，适用于各种网络条件。


<details>
  <summary>Details</summary>
Motivation: 网络延迟严重影响超算应用的性能，现有的自适应路由算法可能导致数据包乱序，影响TCP、QUIC和RoCE等协议的效率。

Method: 采用flowcut switching算法，不同于现有的基于突发流量的flowlet switching，该算法适用于非突发流量（如RDMA），确保数据包有序传输。

Result: flowcut switching在各种网络条件下都能保证数据包有序传输，提升网络性能。

Conclusion: flowcut switching是一种高效的自适应路由算法，解决了现有方案在数据包有序传输上的不足。

Abstract: Network latency severely impacts the performance of applications running on
supercomputers. Adaptive routing algorithms route packets over different
available paths to reduce latency and improve network utilization. However, if
a switch routes packets belonging to the same network flow on different paths,
they might arrive at the destination out-of-order due to differences in the
latency of these paths. For some transport protocols like TCP, QUIC, and RoCE,
out-of-order (OOO) packets might cause large performance drops or significantly
increase CPU utilization. In this work, we propose flowcut switching, a new
adaptive routing algorithm that provides high-performance in-order packet
delivery. Differently from existing solutions like flowlet switching, which are
based on the assumption of bursty traffic and that might still reorder packets,
flowcut switching guarantees in-order delivery under any network conditions,
and is effective also for non-bursty traffic, as it is often the case for RDMA.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [11] [E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs](https://arxiv.org/abs/2506.20944)
*Van-Hoang Phan,Long-Khanh Pham,Dang Vu,Anh-Duy Tran,Minh-Son Dao*

Main category: cs.MM

TL;DR: 提出一种基于检索的多模态事实验证系统，利用预训练模型验证信息真实性，有效应对移动无线网络中的虚假信息传播。


<details>
  <summary>Details</summary>
Motivation: 解决移动和无线网络中虚假信息快速传播带来的安全挑战。

Method: 引入无需训练的检索式多模态事实验证系统，结合预训练视觉语言模型和大语言模型，动态检索并交叉参考可信数据源。

Result: 在两个事实核查基准测试中达到了最先进水平，验证了其在虚假信息检测和对抗攻击方面的有效性。

Conclusion: 该方法在移动和无线通信环境中具有潜在的安全增强能力，且因其轻量化设计适用于边缘设备。

Abstract: The rapid spread of misinformation in mobile and wireless networks presents
critical security challenges. This study introduces a training-free,
retrieval-based multimodal fact verification system that leverages pretrained
vision-language models and large language models for credibility assessment. By
dynamically retrieving and cross-referencing trusted data sources, our approach
mitigates vulnerabilities of traditional training-based models, such as
adversarial attacks and data poisoning. Additionally, its lightweight design
enables seamless edge device integration without extensive on-device
processing. Experiments on two fact-checking benchmarks achieve SOTA results,
confirming its effectiveness in misinformation detection and its robustness
against various attack vectors, highlighting its potential to enhance security
in mobile and wireless communication environments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [Pebble Games and Algebraic Proof Systems](https://arxiv.org/abs/2506.21149)
*Lisa-Marie Jaser,Jacobo Toran*

Main category: cs.LO

TL;DR: 该论文研究了pebble games与代数证明系统之间的新联系，证明了可逆、黑和白-黑pebble游戏与三个代数证明系统（Nullstellensatz、Monomial Calculus和Polynomial Calculus）之间的并行性。


<details>
  <summary>Details</summary>
Motivation: 探索pebble games与代数证明系统之间的深层联系，以揭示二者在计算复杂度上的关系。

Method: 通过分析pebble游戏的空间和时间，与代数证明系统的度数和规模进行对比，建立数学模型和证明策略。

Result: 证明了pebble游戏空间与代数证明系统变量空间复杂度之间的精确匹配，并展示了不同系统之间的分离和权衡。

Conclusion: 这些结果不仅深化了对pebble games与代数证明系统之间关系的理解，还为复杂度分离和权衡提供了新的证据。

Abstract: Analyzing refutations of the well known 0pebbling formulas Peb$(G)$ we prove
some new strong connections between pebble games and algebraic proof system,
showing that there is a parallelism between the reversible, black and
black-white pebbling games on one side, and the three algebraic proof systems
Nullstellensatz, Monomial Calculus and Polynomial Calculus on the other side.
In particular we prove that for any DAG $G$ with a single sink, if there is a
Monomial Calculus refutation for Peb$(G)$ having simultaneously degree $s$ and
size $t$ then there is a black pebbling strategy on $G$ with space $s$ and time
$t+s$. Also if there is a black pebbling strategy for $G$ with space $s$ and
time $t$ it is possible to extract from it a MC refutation for Peb$(G)$ having
simultaneously degree $s$ and size $ts$. These results are analogous to those
proven in {deRezende et al.21} for the case of reversible pebbling and
Nullstellensatz. Using them we prove degree separations between NS, MC and PC,
as well as strong degree-size tradeoffs for MC.
  We also notice that for any directed acyclic graph $G$ the space needed in a
pebbling strategy on $G$, for the three versions of the game, reversible, black
and black-white, exactly matches the variable space complexity of a refutation
of the corresponding pebbling formula Peb$(G)$ in each of the algebraic proof
systems NS, MC and PC. Using known pebbling bounds on graphs, this connection
implies separations between the corresponding variable space measures.

</details>


### [13] [Deciding Robust Instances of an Escape Problem for Dynamical Systems in Euclidean Space](https://arxiv.org/abs/2506.21481)
*Eike Neumann*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of deciding whether a point escapes a closed subset of
$\mathbb{R}^d$ under the iteration of a continuous map $f \colon \mathbb{R}^d
\to \mathbb{R}^d$ in the bit-model of real computation. We give a sound partial
decision method for this problem which is complete in the sense that its
halting set contains the halting set of all sound partial decision methods for
the problem. Equivalently, our decision method terminates on all problem
instances whose answer is robust under all sufficiently small perturbations of
the function. We further show that the halting set of our algorithm is dense in
the set of all problem instances. While our algorithm applies to general
continuous functions, we demonstrate that it also yields complete decision
methods for much more rigid function families: affine linear systems and
quadratic complex polynomials. In the latter case, completeness is subject to
the density of hyperbolicity conjecture in complex dynamics. This in particular
yields an alternative proof of Hertling's (2004) conditional answer to a
question raised by Penrose (1989) regarding the computability of the Mandelbrot
set.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [14] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [15] ["TikTok, Do Your Thing": User Reactions to Social Surveillance in the Public Sphere](https://arxiv.org/abs/2506.20884)
*Meira Gilbert,Miranda Wei,Lindah Kotut*

Main category: cs.HC

TL;DR: 研究分析了TikTok上‘TikTok, Do Your Thing’这一趋势，用户通过众包信息识别陌生人，主要用于浪漫目的。研究发现19人成功被识别，支持评论（883条）远超反对（310条），探讨了人际监控的常态化及其社会影响。


<details>
  <summary>Details</summary>
Motivation: 探讨用户在公共场合通过TikTok进行陌生人识别活动的反应和态度，分析这种人际监控趋势的社会影响。

Method: 对60个TikTok视频和1901条用户评论进行定性分析。

Result: 19人被成功识别；支持评论多于反对，反映了用户对社区和算法参与的认可，反对意见则涉及隐私和性别问题。

Conclusion: 研究揭示了人际监控的常态化趋势，为理解用户对公共场合识别行为的感知提供了新视角。

Abstract: ''TikTok, Do Your Thing'' is a viral trend where users attempt to identify
strangers they see in public via information crowd-sourcing. The trend started
as early as 2021 and users typically engage with it for romantic purposes
(similar to a ''Missed Connections'' personal advertisement). This practice
includes acts of surveillance and identification in the public sphere, although
by peers rather than governments or corporations. To understand users'
reactions to this trend we conducted a qualitative analysis of 60 TikTok videos
and 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were
successfully identified. We also find that while there were comments expressing
disapproval (n=310), more than double the number expressed support (n=883).
Supportive comments demonstrated genuine interest and empathy, reflecting
evolving conceptions of community and algorithmic engagement. On the other
hand, disapproving comments highlighted concerns about inappropriate
relationships, stalking, consent, and gendered double standards. We discuss
these insights in relation to the normalization of interpersonal surveillance,
online stalking, and as an evolution of social surveillance to offer a new
perspective on user perceptions surrounding interpersonal surveillance and
identification in the public sphere.

</details>


### [16] [Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment](https://arxiv.org/abs/2506.20952)
*Kyosuke Ishibashi,Atsushi Saito,Zin Y. Tun,Lucas Ray,Megan C. Coram,Akihiro Sakurai,Allison M. Okamura,Ko Yamamoto*

Main category: cs.HC

TL;DR: 研究通过用户实验探讨了触觉反馈对VR环境中密集动态人群下行走行为的影响，发现触觉反馈改变了用户的避碰动作，增强了碰撞敏感性和视觉探索。


<details>
  <summary>Details</summary>
Motivation: 触觉反馈可提升VR的沉浸感，但其对密集动态人群行走行为的影响尚不清楚，研究旨在填补这一空白。

Method: 通过用户实验分析触觉反馈对行走轨迹、骨盆角度变化及视觉探索的影响。

Result: 触觉反馈增加了行走轨迹长度、骨盆角度变化，并提升了侧面和后方NPC接近时的视觉探索。

Conclusion: 触觉反馈增强了用户在VR环境中的碰撞敏感性和行为适应性。

Abstract: Human crowd simulation in virtual reality (VR) is a powerful tool with
potential applications including emergency evacuation training and assessment
of building layout. While haptic feedback in VR enhances immersive experience,
its effect on walking behavior in dense and dynamic pedestrian flows is
unknown. Through a user study, we investigated how haptic feedback changes user
walking motion in crowded pedestrian flows in VR. The results indicate that
haptic feedback changed users' collision avoidance movements, as measured by
increased walking trajectory length and change in pelvis angle. The
displacements of users' lateral position and pelvis angle were also increased
in the instantaneous response to a collision with a non-player character (NPC),
even when the NPC was inside the field of view. Haptic feedback also enhanced
users' awareness and visual exploration when an NPC approached from the side
and back. Furthermore, variation in walking speed was increased by the haptic
feedback. These results suggested that the haptic feedback enhanced users'
sensitivity to a collision in VR environment.

</details>


### [17] [Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact](https://arxiv.org/abs/2506.21195)
*Neha Raghuvanshi*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Have you wondered how cross-functional teams balance between maximizing value
that users derive and business growth leading to win-win situations? This case
study shows how User Experience Research (UXR) and Data Science teams used
mixed methods research to strategically influence Product Led Growth (PLG) for
a Password Manager used by million+ users, thus allowing our users, internal
teams, and business to win. The audience will take away practical
lessons/techniques related to leveraging mixed methods to: a. Maximize user
value while meeting business growth goals b. Influence cross-functional teams
c. Measure user and business impact This case study can be easily tied to the
UXR Point of view pyramid (POV) [2] that represents a methodological approach
to construct a POV and further dives into actioning POV to create measurable
user and business impact.

</details>


### [18] [Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices](https://arxiv.org/abs/2506.21201)
*Zihao You,Michael Crabb*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The consumption of subtitles via TVs, laptops and smartphones has the
potential to marginalize people based on their complex accessibility needs. The
current one-size-fits-all approach to this accessibility aid is no longer fit
for purpose and work is required to look at how it can be adapted to be
personalised for individual users based on individual context, content, and
consumption habits. People with Aphasia, for example, encounter significant
challenges in understanding subtitle texts.
  We see our work as a call to action for more inclusive practices, focusing on
how the thoughts and opinions of people with aphasia can be included in media
research. Our work investigates how to develop future media solutions for
people with aphasia to create a more inclusive media viewing environment. We
believe the key to this is appropriate prototyping tools and methods to allow
equitable inclusion in the system design process.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [19] [Generative Blocks World: Moving Things Around in Pictures](https://arxiv.org/abs/2506.20703)
*Vaibhav Vavilala,Seemandhar Jain,Rahul Vasanth,D. A. Forsyth,Anand Bhattad*

Main category: cs.GR

TL;DR: 提出一种基于生成图像的场景交互方法，通过几何抽象操作实现高效编辑。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种灵活且高效的方式来编辑生成图像中的场景，同时保持视觉逼真度和对象一致性。

Method: 将场景表示为3D凸多面体的组装体，支持不同数量的基元表示；通过流式生成方法基于深度和纹理提示生成图像。

Result: 实验表明，该方法在视觉逼真度、可编辑性和组合泛化能力上优于现有方法。

Conclusion: 该方法通过几何抽象和纹理提示实现了高效且逼真的场景编辑，为生成图像的交互操作提供了新思路。

Abstract: We describe Generative Blocks World to interact with the scene of a generated
image by manipulating simple geometric abstractions. Our method represents
scenes as assemblies of convex 3D primitives, and the same scene can be
represented by different numbers of primitives, allowing an editor to move
either whole structures or small details. Once the scene geometry has been
edited, the image is generated by a flow-based method which is conditioned on
depth and a texture hint. Our texture hint takes into account the modified 3D
primitives, exceeding texture-consistency provided by existing key-value
caching techniques. These texture hints (a) allow accurate object and camera
moves and (b) largely preserve the identity of objects depicted. Quantitative
and qualitative experiments demonstrate that our approach outperforms prior
works in visual fidelity, editability, and compositional generalization.

</details>


### [20] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 3DGH是一个无条件生成3D人头模型的生成模型，能够分离头发和面部的建模，通过新的数据表示和双生成器架构实现高效的合成与编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法中头发和面部的建模通常是耦合的，这限制了模型的表现力和编辑灵活性。3DGH通过分离头发和面部的建模，旨在提高生成的多样性和可编辑性。

Method: 使用基于模板的3D高斯散射表示头发和面部几何形状，引入可变形的头发几何形状捕捉不同发型的变化。采用基于3D GAN的双生成器架构，并结合交叉注意力机制建模头发和面部的相关性。

Result: 实验证明3DGH在无条件完整头部图像合成和可组合的3D发型编辑方面优于现有方法。

Conclusion: 3DGH通过分离头发和面部的建模，展示了在生成和编辑任务中的优越性能，为3D头部建模提供了新的思路。

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with
composable hair and face components. Unlike previous work that entangles the
modeling of hair and face, we propose to separate them using a novel data
representation with template-based 3D Gaussian Splatting, in which deformable
hair geometry is introduced to capture the geometric variations across
different hairstyles. Based on this data representation, we design a 3D
GAN-based architecture with dual generators and employ a cross-attention
mechanism to model the inherent correlation between hair and face. The model is
trained on synthetic renderings using carefully designed objectives to
stabilize training and facilitate hair-face separation. We conduct extensive
experiments to validate the design choice of 3DGH, and evaluate it both
qualitatively and quantitatively by comparing with several state-of-the-art 3D
GAN methods, demonstrating its effectiveness in unconditional full-head image
synthesis and composable 3D hairstyle editing. More details will be available
on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [21] [Data Visualization for Improving Financial Literacy: A Systematic Review](https://arxiv.org/abs/2506.20901)
*Meng Du,Robert Amor,Kwan-Liu Ma,Burkhard C. Wünsche*

Main category: cs.GR

TL;DR: 该论文通过系统性综述37篇研究，探讨数据可视化和视觉分析在金融教育中的应用，总结了五大关键领域，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 提升金融素养对个人财务决策至关重要，但许多人对金融概念感到困惑，数据可视化可以简化并增强学习效果。

Method: 分析37篇研究，分类为五个领域：可视化工具的演变、使用动机、金融主题与教学方法、工具类型及教学效果评估。

Result: 研究发现可视化工具能有效提升金融素养，并指出了当前研究的不足与未来机会。

Conclusion: 论文为教育者和专业人士提供了设计和使用可视化工具的实用建议，以促进金融素养的提升。

Abstract: Financial literacy empowers individuals to make informed and effective
financial decisions, improving their overall financial well-being and security.
However, for many people understanding financial concepts can be daunting and
only half of US adults are considered financially literate. Data visualization
simplifies these concepts, making them accessible and engaging for learners of
all ages. This systematic review analyzes 37 research papers exploring the use
of data visualization and visual analytics in financial education and literacy
enhancement. We classify these studies into five key areas: (1) the evolution
of visualization use across time and space, (2) motivations for using
visualization tools, (3) the financial topics addressed and instructional
approaches used, (4) the types of tools and technologies applied, and (5) how
the effectiveness of teaching interventions was evaluated. Furthermore, we
identify research gaps and highlight opportunities for advancing financial
literacy. Our findings offer practical insights for educators and professionals
to effectively utilize or design visual tools for financial literacy.

</details>


### [22] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理的时空不一致问题，通过几何感知条件和结构UV扩散策略，生成更平滑、一致的纹理。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解，导致不一致性。视频生成模型的成功启发我们解决纹理合成的时空不一致问题。

Method: 提出VideoTex框架，结合几何感知条件和结构UV扩散策略，利用3D网格结构和保留语义信息优化纹理生成。

Result: VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法，实现了高质量、时间稳定的纹理。

Conclusion: VideoTex为动态实时应用提供了视觉质量和时间一致性的纹理合成解决方案。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


### [23] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon
videos from a single child's drawing, while faithfully preserving its unique
artistic style. Unlike previous storytelling methods that primarily focus on
character consistency and basic motion, FairyGen explicitly disentangles
character modeling from stylized background generation and incorporates
cinematic shot design to support expressive and coherent storytelling. Given a
single character sketch, we first employ an MLLM to generate a structured
storyboard with shot-level descriptions that specify environment settings,
character actions, and camera perspectives. To ensure visual consistency, we
introduce a style propagation adapter that captures the character's visual
style and applies it to the background, faithfully retaining the character's
full visual identity while synthesizing style-consistent scenes. A shot design
module further enhances visual diversity and cinematic quality through frame
cropping and multi-view synthesis based on the storyboard. To animate the
story, we reconstruct a 3D proxy of the character to derive physically
plausible motion sequences, which are then used to fine-tune an MMDiT-based
image-to-video diffusion model. We further propose a two-stage motion
customization adapter: the first stage learns appearance features from
temporally unordered frames, disentangling identity from motion; the second
stage models temporal dynamics using a timestep-shift strategy with frozen
identity weights. Once trained, FairyGen directly renders diverse and coherent
video scenes aligned with the storyboard. Extensive experiments demonstrate
that our system produces animations that are stylistically faithful,
narratively structured natural motion, highlighting its potential for
personalized and engaging story animation. The code will be available at
https://github.com/GVCLab/FairyGen

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [Scalable GPU Performance Variability Analysis framework](https://arxiv.org/abs/2506.20674)
*Ankur Lahiry,Ayush Pokharel,Seth Ockerman,Amal Gueroudji,Line Pouchard,Tanzima Z. Islam*

Main category: cs.DC

TL;DR: 论文提出一种分布式数据分析框架，用于高效处理大规模GPU性能日志，解决了现有工具的内存和运行时间限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具处理GPU性能日志时内存需求高、运行时间长，无法满足HPC工作流的需求。

Method: 引入分布式框架，将数据集分区并并行处理，降低单节点内存压力。

Result: 成功应用于实际HPC和AI工作负载的Nsight Compute跟踪数据，诊断了性能变异性并揭示了内存传输延迟对GPU内核行为的影响。

Conclusion: 该方法显著提高了性能日志分析的效率，支持低延迟探索高维跟踪数据。

Abstract: Analyzing large-scale performance logs from GPU profilers often requires
terabytes of memory and hours of runtime, even for basic summaries. These
constraints prevent timely insight and hinder the integration of performance
analytics into automated workflows. Existing analysis tools typically process
data sequentially, making them ill-suited for HPC workflows with growing trace
complexity and volume. We introduce a distributed data analysis framework that
scales with dataset size and compute availability. Rather than treating the
dataset as a single entity, our system partitions it into independently
analyzable shards and processes them concurrently across MPI ranks. This design
reduces per-node memory pressure, avoids central bottlenecks, and enables
low-latency exploration of high-dimensional trace data. We apply the framework
to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate
its ability to diagnose performance variability, and uncover the impact of
memory transfer latency on GPU kernel behavior.

</details>


### [25] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [26] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: GPU内存带宽是低延迟大型语言模型推理的主要瓶颈。传统密集模型通过推测解码提升吞吐量，但在MoE模型中因权重激活增加导致效率下降。Cascade框架动态调整推测解码以优化MoE性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决MoE模型中推测解码因权重激活增加而导致的效率下降问题，研究提出Cascade框架以动态调整策略。

Method: Cascade通过轻量级指标“推测效用”动态选择是否启用推测解码，并定期调整K值以最大化吞吐量。

Result: 在五种流行MoE模型上，Cascade将减速限制在5%以下，吞吐量提升7-14%。

Conclusion: Cascade使推测解码在MoE模型中变得实用，显著优化了性能。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


### [27] [ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks](https://arxiv.org/abs/2506.20938)
*Joshua H. Davis,Daniel Nichols,Ishan Khillan,Abhinav Bhatele*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: GPGPU architectures have become significantly diverse in recent years, which
has led to an emergence of a variety of specialized programming models and
software stacks to support them. While portable execution models exist, they
still require significant developer effort to port to and optimize for
different hardware architectures. Recent advances in large language models
(LLMs) can help us reduce some of this programmer burden. In this paper, we
present a novel benchmark and testing framework, ParEval-Repo, which can be
used to evaluate the efficacy of LLM-based approaches in automatically
translating entire codebases across GPGPU execution models. ParEval-Repo
includes several scientific computing and AI mini-applications in a range of
programming models, and levels of repository complexity. We use ParEval-Repo to
evaluate a range of state-of-the-art open-source and commercial LLMs, with both
a non-agentic and a top-down agentic approach. We assess code generated by the
LLMs and approaches in terms of compilability, functional correctness,
categories of build errors, and the cost of translation in terms of the number
of inference tokens. Our results demonstrate that LLM translation of scientific
applications is feasible for small programs but difficulty with generating
functional build systems and cross-file dependencies pose challenges in scaling
to larger codebases.

</details>


### [28] [Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe](https://arxiv.org/abs/2506.20994)
*Måns I. Andersson,Martin Karp,Niclas Jansson,Stefano Markidis*

Main category: cs.DC

TL;DR: 本文介绍如何使用DaCe框架自动化生成高性能计算（HPC）内核，以应对多样化的硬件架构挑战，并以CFD中的Neko求解器为例展示了其便携性和性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算（HPC）加速器（如Nvidia和AMD GPU）的涌现，针对多样化硬件架构的高效编程成为重大挑战，手动开发特定架构代码会阻碍大规模科学应用的可持续性。

Method: 利用DaCe框架，通过其数据流多图（SDFG）表示自动生成高性能代码，并将其集成到Fortran-based的Neko求解器中。

Result: 在多平台上（Nvidia GH200、A100和AMD MI250X GPU）生成的代码表现出便携性和性能优势。

Conclusion: 自动代码生成可行，可为大规模科学应用提供可持续的便携解决方案。

Abstract: With the emergence of new high-performance computing (HPC) accelerators, such
as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures
has become a major challenge for HPC application developers. The increasing
hardware diversity in HPC systems often necessitates the development of
architecture-specific code, hindering the sustainability of large-scale
scientific applications. In this work, we leverage DaCe, a data-centric
parallel programming framework, to automate the generation of high-performance
kernels. DaCe enables automatic code generation for multicore processors and
various accelerators, reducing the burden on developers who would otherwise
need to rewrite code for each new architecture. Our study demonstrates DaCe's
capabilities by applying its automatic code generation to a critical
computational kernel used in Computational Fluid Dynamics (CFD). Specifically,
we focus on Neko, a Fortran-based solver that employs the spectral-element
method, which relies on small tensor operations. We detail the formulation of
this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)
representation and discuss how this approach facilitates high-performance code
generation. Additionally, we outline the workflow for seamlessly integrating
DaCe's generated code into the Neko solver. Our results highlight the
portability and performance of the generated code across multiple platforms,
including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive
performance results. By demonstrating the potential of automatic code
generation, we emphasise the feasibility of using portable solutions to ensure
the long-term sustainability of large-scale scientific applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [Condensed Representation of RDF and its Application on Graph Versioning](https://arxiv.org/abs/2506.21203)
*Jey Puget Gil,Emmanuel Coquery,John Samuel,Gilles Gesquiere*

Main category: cs.DB

TL;DR: 该研究提出了演化知识图的紧凑表示形式，以帮助管理和分析随时间变化的复杂现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了理解不同时间点实体间的关系并预测未来趋势，同时解决知识图管理中数据演化的组织问题。

Method: 通过提出并形式化演化图的紧凑表示方法，利用来自多源的异构数据进行建模。

Result: 该方法能够使不断演化的数据更易于访问和分析。

Conclusion: 结论是紧凑表示形式对于知识图管理系统中数据的有效组织和利用至关重要。

Abstract: The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [30] [Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G Networks](https://arxiv.org/abs/2506.21073)
*Ilias Papalamprou,Nikolaos Fotos,Nikolaos Chatzivasileiadis,Anna Angelogianni,Dimosthenis Masouros,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 论文提出了一种结合硬件和软件的混合解决方案，通过远程认证和量子后加密算法（PQC）增强FPGA的安全性，并利用区块链技术确保安全证据的存储。实验表明，该方法仅带来2%的性能开销。


<details>
  <summary>Details</summary>
Motivation: 随着5G及更高性能网络的普及，服务部署更接近用户，但FPGA常处于未受保护的环境，且量子计算的兴起威胁传统加密算法的安全性，需要更强大的安全基础设施。

Method: 采用远程认证和量子后加密算法（PQC）配置FPGA，并结合区块链存储安全证据。

Result: 在两种FPGA家族中评估该方法，相比非PQC方案仅带来2%的性能开销。

Conclusion: 提出的混合方案在保证安全性的同时，性能开销极低，适用于边缘计算环境。

Abstract: The advent of 5G and beyond has brought increased performance networks,
facilitating the deployment of services closer to the user. To meet performance
requirements such services require specialized hardware, such as Field
Programmable Gate Arrays (FPGAs). However, FPGAs are often deployed in
unprotected environments, leaving the user's applications vulnerable to
multiple attacks. With the rise of quantum computing, which threatens the
integrity of widely-used cryptographic algorithms, the need for a robust
security infrastructure is even more crucial. In this paper we introduce a
hybrid hardware-software solution utilizing remote attestation to securely
configure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms
for enhanced security. Additionally, to enable trustworthiness across the whole
edge computing continuum, our solution integrates a blockchain infrastructure,
ensuring the secure storage of any security evidence. We evaluate the proposed
secure configuration process under different PQC algorithms in two FPGA
families, showcasing only 2% overheard compared to the non PQC approach.

</details>


### [31] [Accelerating GNN Training through Locality-aware Dropout and Merge](https://arxiv.org/abs/2506.21414)
*Gongjian Sun,Mingyu Yan,Dengke Han,Runzhen Xue,Duo Wang,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: LiGNN是一种基于硬件的解决方案，通过改进数据局部性来加速GNN训练，显著减少了DRAM访问。


<details>
  <summary>Details</summary>
Motivation: GNN在图形学习中取得了显著成功，但其不规则的顶点连接导致了低效的邻居聚合和大量的不规则DRAM访问，从而降低了性能。

Method: LiGNN结合了位置感知的特征丢弃策略和内存访问合并技术，优化了数据局部性。

Result: 在0.5的丢弃率下，LiGNN比现有方法提速1.48~3.02倍，减少了34%~55%的DRAM访问，并降低了59%~82%的DRAM行激活，同时保持了模型准确率。

Conclusion: LiGNN通过硬件优化显著提升了GNN训练的性能和效率。

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in graph
learning and are widely adopted across various critical domains. However, the
irregular connectivity between vertices leads to inefficient neighbor
aggregation, resulting in substantial irregular and coarse-grained DRAM
accesses. This lack of data locality presents significant challenges for
execution platforms, ultimately degrading performance. While previous
accelerator designs have leveraged on-chip memory and data access scheduling
strategies to address this issue, they still inevitably access features at
irregular addresses from DRAM. In this work, we propose LiGNN, a hardware-based
solution that improves data locality by applying dropout and merge techniques
during neighbor aggregation to accelerate GNN training. Unlike conventional
algorithm-level dropout methods that primarily aim to improve accuracy while
overlooking hardware costs, LiGNN introduces a locality-aware feature dropout
mechanism. This approach selectively drops node features with data locality
awareness, effectively reducing irregular DRAM accesses without compromising
model accuracy. Moreover, by leveraging detailed knowledge of memory layout and
organization-including critical alignment constraints-LiGNN strategically
merges memory accesses during neighbor aggregation at the DRAM row level,
guided by GNN-level semantics. This optimization significantly improves data
locality with minimal additional cost. Under the commonly adopted 0.5 dropout
rate, LiGNN outperforms state-of-the-art methods, delivering a 1.48~3.02x
speedup, reducing DRAM accesses by 34%~55%, and lowering DRAM row activations
by 59%~82%, all while maintaining model accuracy.

</details>


### [32] [OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits](https://arxiv.org/abs/2506.21487)
*Maryam Ghane,Amir M. Hajisadeghi,Hamid R. Zarandi*

Main category: cs.AR

TL;DR: OptGM是一种优化的门合并方法，旨在减少数字电路中的负偏压温度不稳定性（NBTI），效果显著且面积开销小。


<details>
  <summary>Details</summary>
Motivation: NBTI会导致数字电路性能退化，OptGM通过优化门合并解决这一问题。

Method: 识别NBTI关键节点，合并驱动和被驱动门形成新复合门，消除关键节点。

Result: NBTI关键晶体管减少89.29%，延迟退化降低23.87%，晶体管总数减少6.47%，性能成本比提升12.8%。

Conclusion: OptGM高效解决NBTI问题，提升电路性能且成本低。

Abstract: This paper presents OptGM, an optimized gate merging method designed to
mitigate negative bias temperature instability (NBTI) in digital circuits.
First, the proposed approach effectively identifies NBTI-critical internal
nodes, defined as those with a signal probability exceeding a predefined
threshold. Next, based on the proposed optimized algorithm, the sensitizer gate
(which drives the critical node) and the sensitive gate (which is fed by it)
are merged into a new complex gate. This complex gate preserves the original
logic while eliminating NBTI-critical nodes. Finally, to evaluate the
effectiveness of OptGM, we assess it on several combinational and sequential
benchmark circuits. Simulation results demonstrate that, on average, the number
of NBTI-critical transistors (i.e., PMOS transistors connected to critical
nodes), NBTI-induced delay degradation, and the total transistor count are
reduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances
performance per cost (PPC) by 12.8% on average, with minimal area overhead.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [33] [Hierarchical Sub-action Tree for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.20947)
*Dejie Yang,Zhu Xu,Xinjie Gao,Yang Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continuous sign language recognition (CSLR) aims to transcribe untrimmed
videos into glosses, which are typically textual words. Recent studies indicate
that the lack of large datasets and precise annotations has become a bottleneck
for CSLR due to insufficient training data. To address this, some works have
developed cross-modal solutions to align visual and textual modalities.
However, they typically extract textual features from glosses without fully
utilizing their knowledge. In this paper, we propose the Hierarchical
Sub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge
with visual representation learning. By incorporating gloss-specific knowledge
from large language models, our approach leverages textual information more
effectively. Specifically, we construct an HST for textual information
representation, aligning visual and textual modalities step-by-step and
benefiting from the tree structure to reduce computational complexity.
Additionally, we impose a contrastive alignment enhancement to bridge the gap
between the two modalities. Experiments on four datasets (PHOENIX-2014,
PHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the
effectiveness of our HST-CSLR.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [34] [ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers](https://arxiv.org/abs/2506.21537)
*Nicholas S. DiBrita,Jason Han,Tirthak Patel*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Research in quantum machine learning has recently proliferated due to the
potential of quantum computing to accelerate machine learning. An area of
machine learning that has not yet been explored is neural ordinary differential
equation (neural ODE) based residual neural networks (ResNets), which aim to
improve the effectiveness of neural networks using the principles of ordinary
differential equations. In this work, we present our insights about why analog
Rydberg atom quantum computers are especially well-suited for ResNets. We also
introduce ResQ, a novel framework to optimize the dynamics of Rydberg atom
quantum computers to solve classification problems in machine learning using
analog quantum neural ODEs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [35] [Metadata Enrichment of Long Text Documents using Large Language Models](https://arxiv.org/abs/2506.20918)
*Manika Lamba,You Peng,Sophie Nikolov,Glen Layne-Worthey,J. Stephen Downie*

Main category: cs.DL

TL;DR: 通过结合人工与大语言模型增强HathiTrust图书馆英文文献元数据，提升数字图书馆检索与可访问性。


<details>
  <summary>Details</summary>
Motivation: 改善数字图书馆中元数据的完整性与多样性，以支持计算社会科学、数字人文和信息科学的研究。

Method: 结合人工努力与大语言模型对1920-2020年间的英文文献元数据进行语义增强。

Result: 引入更多元数据访问点，有效填补缺失数据，提升检索效果与可访问性。

Conclusion: 大语言模型在元数据增强中效果显著，特别适用于数据缺失严重的数字图书馆。

Abstract: In this project, we semantically enriched and enhanced the metadata of long
text documents, theses and dissertations, retrieved from the HathiTrust Digital
Library in English published from 1920 to 2020 through a combination of manual
efforts and large language models. This dataset provides a valuable resource
for advancing research in areas such as computational social science, digital
humanities, and information science. Our paper shows that enriching metadata
using LLMs is particularly beneficial for digital repositories by introducing
additional metadata access points that may not have originally been foreseen to
accommodate various content types. This approach is particularly effective for
repositories that have significant missing data in their existing metadata
fields, enhancing search results and improving the accessibility of the digital
repository.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [36] [Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions](https://arxiv.org/abs/2506.20677)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.DS

TL;DR: 本文提出了一种自适应混合排序范式，通过实时监控输入数据模式自动选择最高效的排序算法（计数排序、基数排序或快速排序）。


<details>
  <summary>Details</summary>
Motivation: 由于不同数据分布下排序算法表现不同，传统静态排序算法难以在所有场景下保持最优性能。

Method: 通过特征提取模块计算数据量、值范围和熵等参数，利用有限状态机和XGBoost分类器选择最优排序策略。

Result: 实验证明，该方法在执行时间、灵活性和效率上显著优于传统静态排序算法。

Conclusion: 该框架适用于大数据分析、边缘计算等广泛数据处理场景，具有高扩展性和实用性。

Abstract: Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.

</details>


### [37] [Practical and Accurate Local Edge Differentially Private Graph Algorithms](https://arxiv.org/abs/2506.20828)
*Pranay Mundra,Charalampos Papamanthou,Julian Shun,Quanquan C. Liu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [38] [Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment](https://arxiv.org/abs/2506.20965)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 论文结合奥地利资本理论与重复博弈论，分析区块链系统中不同制度条件下矿工的策略行为。


<details>
  <summary>Details</summary>
Motivation: 探讨协议规则可变性如何影响矿工的长期规划和合作均衡。

Method: 通过形式博弈论分析和奥地利经济学原理，研究可变协议对矿工激励的影响。

Result: 可变协议导致矿工从生产性投资转向政治寻租和影响力博弈，比特币固定规则集则提供稳定性和低时间偏好。

Conclusion: 协议不可变性对恢复战略一致性、企业家信心和可持续网络均衡至关重要。

Abstract: This paper integrates Austrian capital theory with repeated game theory to
examine strategic miner behaviour under different institutional conditions in
blockchain systems. It shows that when protocol rules are mutable, effective
time preference rises, undermining rational long-term planning and cooperative
equilibria. Using formal game-theoretic analysis and Austrian economic
principles, the paper demonstrates how mutable protocols shift miner incentives
from productive investment to political rent-seeking and influence games. The
original Bitcoin protocol is interpreted as an institutional anchor: a fixed
rule-set enabling calculability and low time preference. Drawing on the work of
Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability
is essential for restoring strategic coherence, entrepreneurial confidence, and
sustainable network equilibrium.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [39] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [40] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON是一种灵活的框架，用于微调媒体生成模型，支持多样化的奖励函数，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法如RLHF或DPO缺乏灵活性，无法适应各种奖励函数。DRAGON旨在提供更通用的优化框架。

Method: 通过选择编码器和参考样例构建奖励函数，利用对比集最大化奖励，支持跨模态任务。

Result: 实验表明，DRAGON在20种不同奖励函数上平均获胜率为81.45%，且能提升生成质量。

Conclusion: DRAGON为设计和优化奖励函数提供了新方法，显著提升了人类感知质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [42] [RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation](https://arxiv.org/abs/2506.20817)
*Ali Tourani,Fatemeh Nazary,Yashar Deldjoo*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [43] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 首个理论框架将SNN应用于SAR干涉相位解缠，填补了现有方法空白。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据激增（如NISAR任务将生成100PB数据），高效节能处理成关键。SNN有望节省30-100倍能耗。

Method: 开发了针对相位数据的脉冲编码方案，提出利用相位解缠空间传播特性的SNN架构，分析计算复杂性和收敛性。

Result: 框架显示SNN的时间动态可自然建模相位解缠的空间连续性约束。

Conclusion: 开创了神经形态计算与SAR干涉测量交叉新方向，提供更可持续的大规模InSAR处理方案。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [44] [Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher](https://arxiv.org/abs/2506.20834)
*Tomas Gallo Aquino,Victoria Liu,Habiba Azab,Raissa Mathura,Andrew J Watrous,Eleonora Bartoli,Benjamin Y Hayden,Paul Sajda,Sameer A Sheth,Nuttida Rungratsameetaweemana*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transfer learning enhances the training of novel sensory and decision models
by employing rich feature representations from large, pre-trained teacher
models. Cognitive neuroscience shows that the human brain creates
low-dimensional, abstract representations for efficient sensorimotor coding.
Importantly, the brain can learn these representations with significantly fewer
data points and less computational power than artificial models require. We
introduce Brain2Model Transfer Learning (B2M), a framework where neural
activity from human sensory and decision-making tasks acts as the teacher model
for training artificial neural networks. We propose two B2M strategies: (1)
Brain Contrastive Transfer, which aligns brain activity and network activations
through a contrastive objective; and (2) Brain Latent Transfer, which projects
latent dynamics from similar cognitive tasks onto student networks via
supervised regression of brain-derived features. We validate B2M in
memory-based decision-making with a recurrent neural network and scene
reconstruction for autonomous driving with a variational autoencoder. The
results show that student networks benefiting from brain-based transfer
converge faster and achieve higher predictive accuracy than networks trained in
isolation. Our findings indicate that the brain's representations are valuable
for artificial learners, paving the way for more efficient learning of complex
decision-making representations, which would be costly or slow through purely
artificial training.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [45] [Diophantine Equations over $\mathbb Z$: Universal Bounds and Parallel Formalization](https://arxiv.org/abs/2506.20909)
*Jonas Bayer,Marco David,Malte Hassler,Yuri Matiyasevich,Dierk Schleicher*

Main category: math.NT

TL;DR: 该论文探讨了Diophantine方程的复杂度边界，并通过形式化证明工具Isabelle验证了结果。研究提出了新的显式通用对(ν, δ)，并为数学实践中的新方法提供了见解。


<details>
  <summary>Details</summary>
Motivation: 研究动机是确定Diophantine方程的哪些子类在Hilbert第十问题下仍然不可判定，并通过形式化工具将证明过程与数学发现相结合。

Method: 通过定义通用对(ν, δ)来限制方程复杂度，并使用Isabelle进行形式化验证，将证明与发现过程结合。

Result: 提出了新的显式通用对(ν, δ)，并通过Isabelle验证了结果，展示了形式化工具在数学研究中的应用潜力。

Conclusion: 研究不仅推进了Diophantine方程的研究，还探索了形式化证明工具在21世纪数学实践中的新方法。

Abstract: This paper explores multiple closely related themes: bounding the complexity
of Diophantine equations over the integers and developing mathematical proofs
in parallel with formal theorem provers.
  Hilbert's Tenth Problem (H10) asks about the decidability of Diophantine
equations and has been answered negatively by Davis, Putnam, Robinson and
Matiyasevich. It is natural to ask for which subclasses of Diophantine
equations H10 remains undecidable. Such subclasses can be defined in terms of
universal pairs: bounds on the number of variables $\nu$ and degree $\delta$
such that all Diophantine equations can be rewritten in at most this
complexity. Our work develops explicit universal pairs $(\nu, \delta)$ for
integer unknowns, achieving new bounds that cannot be obtained by naive
translations from known results over $\mathbb N$.
  In parallel, we have conducted a formal verification of our results using the
proof assistant Isabelle. While formal proof verification has traditionally
been applied a posteriori to known results, this project integrates
formalization into the discovery and development process. In a final section,
we describe key insights gained from this unusual approach and its implications
for mathematical practice. Our work contributes both to the study of
Diophantine equations and to the broader question of how mathematics is
conducted in the 21st century.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations](https://arxiv.org/abs/2506.21134)
*Jacopo Bufalino,Jose Luis Martin-Navarro,Mario Di Francesco,Tuomas Aura*

Main category: cs.CR

TL;DR: 该论文研究了Kubernetes网络中横向移动相关的配置错误，分析了287个开源应用，发现了634个配置问题，远超现有工具的检测能力，并协助修复了30多个应用的问题。


<details>
  <summary>Details</summary>
Motivation: Kubernetes的网络配置对安全部署的影响尚未得到充分研究，论文填补了这一空白，特别关注了横向移动相关的安全风险。

Method: 论文对287个开源应用进行了全面分析，覆盖了六类组织（IT公司、公共机构和非营利组织等），重点检测网络配置错误。

Result: 共发现634个配置错误，远超现有工具的检测能力，并通过负责任披露协助修复了30多个应用的问题。

Conclusion: Kubernetes的网络配置错误对安全部署有重大影响，论文的研究结果有助于提升实际应用的安全性。

Abstract: Kubernetes has emerged as the de facto standard for container orchestration.
Unfortunately, its increasing popularity has also made it an attractive target
for malicious actors. Despite extensive research on securing Kubernetes, little
attention has been paid to the impact of network configuration on the security
of application deployments. This paper addresses this gap by conducting a
comprehensive analysis of network misconfigurations in a Kubernetes cluster
with specific reference to lateral movement. Accordingly, we carried out an
extensive evaluation of 287 open-source applications belonging to six different
organizations, ranging from IT companies and public entities to non-profits. As
a result, we identified 634 misconfigurations, well beyond what could be found
by solutions in the state of the art. We responsibly disclosed our findings to
the concerned organizations and engaged in a discussion to assess their
severity. As of now, misconfigurations affecting more than thirty applications
have been fixed with the mitigations we proposed.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [47] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>
