<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 24]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.HC](#cs.HC) [Total: 30]
- [cs.GR](#cs.GR) [Total: 11]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.SP](#eess.SP) [Total: 6]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 9]
- [eess.IV](#eess.IV) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGMAF是一种知识导向的多代理框架，专注于自动需求开发，填补了当前SE自动化系统中忽视需求任务的空白。


<details>
  <summary>Details</summary>
Motivation: 当前自动化系统过于注重代码开发，而需求任务的复杂性被忽视，KGMAF旨在解决这一问题。

Method: KGMAF由六个专业代理和一个构件池组成，详细描述了每个代理的功能、行为和知识，并提供了构件池的概念设计。

Result: 案例研究表明KGMAF在真实场景中具有潜力。

Conclusion: KGMAF将在LLM时代对自动需求开发的未来发展起关键作用，并提供了进一步研究和改进的机会。

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [2] [An LLM-assisted approach to designing software architectures using ADD](https://arxiv.org/abs/2506.22688)
*Humberto Cervantes,Rick Kazman,Yuanfang Cai*

Main category: cs.SE

TL;DR: 提出了一种基于大型语言模型（LLM）辅助的软件架构设计方法，结合属性驱动设计（ADD）方法，通过与人类架构师协作生成架构设计。验证结果显示该方法能生成与既定解决方案高度一致的设计，但也强调了人类监督和迭代优化的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统的软件架构设计依赖专家经验，复杂且迭代性强。本文旨在探索利用LLM辅助架构设计，以提高效率和协作性。

Method: 采用属性驱动设计（ADD）方法，结合LLM的生成能力，通过明确的ADD描述、架构师角色定义和结构化迭代计划，与人类架构师协作生成架构设计。

Result: 案例研究表明，LLM辅助生成的架构设计与传统方案高度一致，并能部分满足架构驱动要求，但也存在局限性。

Conclusion: LLM在软件架构设计中具有潜力，但仍需人类监督和迭代优化以确保设计质量。

Abstract: Designing effective software architectures is a complex, iterative process
that traditionally relies on expert judgment. This paper proposes an approach
for Large Language Model (LLM)-assisted software architecture design using the
Attribute-Driven Design (ADD) method. By providing an LLM with an explicit
description of ADD, an architect persona, and a structured iteration plan, our
method guides the LLM to collaboratively produce architecture artifacts with a
human architect. We validate the approach through case studies, comparing
generated designs against proven solutions and evaluating them with
professional architects. Results show that our LLM-assisted ADD process can
generate architectures closely aligned with established solutions and partially
satisfying architectural drivers, highlighting both the promise and current
limitations of using LLMs in architecture design. Our findings emphasize the
importance of human oversight and iterative refinement when leveraging LLMs in
this domain.

</details>


### [3] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Main category: cs.SE

TL;DR: P4OMP是一个基于检索增强的框架，使用大语言模型将串行C/C++代码转换为OpenMP并行代码，显著提高代码生成的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决无微调或编译器插桩情况下，OpenMP pragma正确性问题，提高LLM生成代码的可靠性。

Method: 利用检索增强生成（RAG）结合OpenMP教程的结构化知识，提升提示驱动的代码生成。

Result: 在108个真实C++程序测试中，P4OMP实现100%编译成功，而基线模型失败20例。

Conclusion: P4OMP提供了一种可靠、模块化的方法，显著提升LLM生成OpenMP代码的实用性。

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [4] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Main category: cs.SE

TL;DR: RAILS框架通过检索增强和迭代验证，显著提升了LLM在Java代码生成中的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 针对LLM在软件开发中生成不完整代码或错误导入的缺陷，提出一种检索增强的方法。

Method: 结合FAISS和OpenAI嵌入，从Java资源中检索语义上下文，并通过编译器反馈迭代优化建议。

Result: 在78个真实Java导入错误案例中，RAILS优于基线方法，能够保留意图并避免幻觉。

Conclusion: 未来将进一步集成符号过滤并扩展支持其他语言和IDE。

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [5] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Main category: cs.SE

TL;DR: 量化不仅能压缩大型语言模型（LLMs），还能提升其在代码生成任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究量化对LLMs鲁棒性的影响，填补现有研究空白。

Method: 通过对抗攻击和噪声扰动实验，评估四种LLM家族的量化模型。

Result: 量化后的LLMs在51.59%的对抗实验中表现更优，且能承受更高权重扰动。

Conclusion: 量化不仅降低计算需求，还能增强LLMs的可靠性，为高效部署提供新思路。

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [6] [Privacy-Preserving Methods for Bug Severity Prediction](https://arxiv.org/abs/2506.22752)
*Havvanur Dervişoğlu,Ruşen Halepmollası,Elif Eyvaz*

Main category: cs.SE

TL;DR: 论文研究了基于源代码指标和大型语言模型（LLMs）的方法级Bug严重性预测，比较了集中学习、联邦学习和合成数据生成三种训练模型的性能，结果表明联邦学习和合成数据能实现与集中学习相当的效果，为数据共享受限的工业场景提供了隐私保护解决方案。


<details>
  <summary>Details</summary>
Motivation: 工业应用中数据共享受限和标记数据不足是Bug严重性预测的主要挑战，研究旨在探索隐私保护方法（如联邦学习和合成数据生成）以解决这一问题。

Method: 使用源代码指标和大型语言模型（LLMs），在两种广泛使用的数据集上对比集中学习、联邦学习和合成数据生成模型的性能。

Result: 联邦学习和合成数据训练的模型在无需数据共享的情况下，达到了与集中学习模型相当的效果。

Conclusion: 联邦学习和合成数据生成等隐私保护方法能在数据共享受限的工业场景中有效实现Bug严重性预测。

Abstract: Bug severity prediction is a critical task in software engineering as it
enables more efficient resource allocation and prioritization in software
maintenance. While AI-based analyses and models significantly require access to
extensive datasets, industrial applications face challenges due to data-sharing
constraints and the limited availability of labeled data. In this study, we
investigate method-level bug severity prediction using source code metrics and
Large Language Models (LLMs) with two widely used datasets. We compare the
performance of models trained using centralized learning, federated learning,
and synthetic data generation. Our experimental results, obtained using two
widely recognized software defect datasets, indicate that models trained with
federated learning and synthetic data achieve comparable results to centrally
trained models without data sharing. Our finding highlights the potential of
privacy-preserving approaches such as federated learning and synthetic data
generation to enable effective bug severity prediction in industrial context
where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository:
https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.

</details>


### [7] [On the Feasibility of Deduplicating Compiler Bugs with Bisection](https://arxiv.org/abs/2506.23281)
*Xintong Zhou,Zhenyang Xu,Chengnian Sun*

Main category: cs.SE

TL;DR: 论文探讨了利用二分法进行编译器错误去重，提出了一种新方法BugLens，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随机测试在编译器验证中有效，但重复测试程序的调试挑战大，现有去重方法计算开销高且通用性有限。

Method: 研究二分法的可行性，并提出BugLens方法，结合二分法和触发优化的识别以减少假阴性。

Result: BugLens在四个真实数据集上平均节省26.98%和9.64%人力，优于Tamer和D3方法。

Conclusion: 二分法因其简单性和通用性，为编译器错误去重提供了实用解决方案。

Abstract: Random testing has proven to be an effective technique for compiler
validation. However, the debugging of bugs identified through random testing
presents a significant challenge due to the frequent occurrence of duplicate
test programs that expose identical compiler bugs. The process to identify
duplicates is a practical research problem known as bug deduplication. Prior
methodologies for compiler bug deduplication primarily rely on program analysis
to extract bug-related features for duplicate identification, which can result
in substantial computational overhead and limited generalizability. This paper
investigates the feasibility of employing bisection, a standard debugging
procedure largely overlooked in prior research on compiler bug deduplication,
for this purpose. Our study demonstrates that the utilization of bisection to
locate failure-inducing commits provides a valuable criterion for
deduplication, albeit one that requires supplementary techniques for more
accurate identification. Building on these results, we introduce BugLens, a
novel deduplication method that primarily uses bisection, enhanced by the
identification of bug-triggering optimizations to minimize false negatives.
Empirical evaluations conducted on four real-world datasets demonstrate that
BugLens significantly outperforms the state-of-the-art analysis-based
methodologies Tamer and D3 by saving an average of 26.98% and 9.64% human
effort to identify the same number of distinct bugs. Given the inherent
simplicity and generalizability of bisection, it presents a highly practical
solution for compiler bug deduplication in real-world applications.

</details>


### [8] [What Challenges Do Developers Face When Using Verification-Aware Programming Languages?](https://arxiv.org/abs/2506.23696)
*Francisco Oliveira,Alexandra Mendes,Carolina Carreira*

Main category: cs.SE

TL;DR: 研究通过话题建模和开发者调查分析了验证感知（VA）语言采用率低的原因，发现学习曲线陡峭和可用性问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 为提高软件可靠性，验证感知语言提供更强的正确性保证，但其采用率低，研究旨在探索阻碍其采用的原因。

Method: 通过话题建模分析开发者讨论，并开展开发者调查，了解VA语言的实际挑战。

Result: 发现主要障碍包括陡峭的学习曲线和可用性问题。

Conclusion: 建议简化工具接口、提供更好教育材料和改进开发环境集成，以提高VA语言的可用性和采用率。

Abstract: Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.

</details>


### [9] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Main category: cs.SE

TL;DR: 论文提出了一种基于CoT、ICL和LLM的新方法，从软件文档中提取隐私行为并生成用户故事形式的隐私需求，结果显示GPT-4o和Llama 3等模型的F1分数超过0.8。


<details>
  <summary>Details</summary>
Motivation: 现有方法常将隐私视为安全概念或事后考虑，可能导致隐私违规。本文旨在通过新技术提前提取隐私行为并生成需求，以改善隐私合规性。

Method: 采用链式思维提示（CoT）、上下文学习（ICL）和大语言模型（LLM），从软件文档中提取隐私行为并生成用户故事。

Result: GPT-4o和Llama 3等模型在提取隐私行为和生成用户故事方面的F1分数超过0.8，且参数调优可进一步提升性能。

Conclusion: 研究证明了LLM在生成隐私需求方面的潜力，为软件生命周期中隐私需求的提取和优化提供了新思路。

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [10] [Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation](https://arxiv.org/abs/2506.23034)
*Hao Yan,Swapneel Suhas Vaidya,Xiaokuan Zhang,Ziyu Yao*

Main category: cs.SE

TL;DR: 论文评估了大语言模型（LLMs）在代码生成中的安全性，探讨了如何通过引导模型生成安全代码以及修复漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成上表现强大，但缺乏对安全实践的关注，导致生成的代码可能包含漏洞。目前缺乏对LLMs生成安全代码和修复漏洞能力的深入分析。

Method: 通过量化分析，评估了LLMs生成不安全代码的倾向、在漏洞提示下生成安全代码的能力，以及在不同反馈下修复漏洞的效果。涵盖了多种模型和漏洞类型。

Result: 研究发现，尽管LLMs容易生成不安全代码，但高级模型在漏洞提示和详细反馈下能够避免或修复漏洞。

Conclusion: 论文为开发者提供了减少漏洞的具体建议，并表明LLMs在安全代码生成和漏洞修复上有潜力。

Abstract: Large Language Models (LLMs) have become powerful tools for automated code
generation. However, these models often overlook critical security practices,
which can result in the generation of insecure code that contains
vulnerabilities-weaknesses or flaws in the code that attackers can exploit to
compromise a system. However, there has been limited exploration of strategies
to guide LLMs in generating secure code and a lack of in-depth analysis of the
effectiveness of LLMs in repairing code containing vulnerabilities. In this
paper, we present a comprehensive evaluation of state-of-the-art LLMs by
examining their inherent tendencies to produce insecure code, their capability
to generate secure code when guided by self-generated vulnerability hints, and
their effectiveness in repairing vulnerabilities when provided with different
levels of feedback. Our study covers both proprietary and open-weight models
across various scales and leverages established benchmarks to assess a wide
range of vulnerability types. Through quantitative and qualitative analyses, we
reveal that although LLMs are prone to generating insecure code, advanced
models can benefit from vulnerability hints and fine-grained feedback to avoid
or fix vulnerabilities. We also provide actionable suggestions to developers to
reduce vulnerabilities when using LLMs for code generation.

</details>


### [11] [HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing](https://arxiv.org/abs/2506.23063)
*Guangfa Lyu,Zhenzhong Cao,Xiaofei Ren,Fengyu Wang*

Main category: cs.SE

TL;DR: HF-DGF是一个新的定向灰盒模糊测试框架，通过混合反馈机制（控制流距离、值流影响分数和切片覆盖率）提高效率，比现有工具快数倍。


<details>
  <summary>Details</summary>
Motivation: 当前定向灰盒模糊测试工具由于运行时反馈不足，无法高效到达目标位置并探索状态空间。

Method: HF-DGF采用了混合反馈机制（控制流距离、值流影响分数和切片覆盖率）和选择性插桩策略。

Result: HF-DGF在41个真实漏洞测试中表现优异，比现有工具显著更快且覆盖率更低，展示了更高的方向性和效率。

Conclusion: HF-DGF通过混合反馈和选择性插桩显著提升了定向灰盒模糊测试的效率。

Abstract: Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for
crash reproduction and patch testing, leveraging its capability to precisely
navigate toward target locations and exploit vulnerabilities. However, current
DGF tools are constrained by insufficient runtime feedback, limiting their
efficiency in reaching targets and exploring state spaces. This study presents
HF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is
guided by a hybrid feedback mechanism integrating control-flow distance,
value-flow influence score, and slice coverage. To enable precise control-flow
distance feedback, we propose a backward-stepping algorithm to calculate basic
block-level seed distances on a virtual inter-procedural control-flow graph
(ICFG). For effective state space exploration, we introduce value-flow
influence and a corresponding metric, the value-flow influence score.
Additionally, to mitigate runtime overhead from hybrid feedback, we adopt a
novel selective instrumentation strategy. Evaluations on 41 real-world
vulnerabilities show HF-DGF outperforms existing tools: it achieves crash
reproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75
times faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times
faster than Beacon on average. Notably, when all fuzzers triggered crashes,
HF-DGF exhibited the lowest code coverage, demonstrating superior
directionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and
Beacon in static analysis efficiency.

</details>


### [12] [Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search](https://arxiv.org/abs/2506.23100)
*Jiayi Zhang,Kai Huang,Jian Zhang,Yang Liu,Chunyang Chen*

Main category: cs.SE

TL;DR: ReinFix 是一个新颖的框架，通过结合静态分析工具和外部修复经验，提升 LLM 在自动程序修复中的表现，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 自动程序修复技术常忽略修复所需的关键上下文和历史经验，导致修复结果不理想。

Method: ReinFix 在推理阶段使用静态分析工具获取内部修复要素，在解决方案阶段借鉴历史修复经验，指导 LLM 生成更准确的补丁。

Result: 在 Defects4J V1.2 和 V2.0 基准测试中，ReinFix 分别修复了 146 和 38 个额外错误，表现优于现有技术。

Conclusion: ReinFix 通过结合上下文分析和历史经验，显著提升了 LLM 的修复能力，适用于无数据泄漏风险的环境。

Abstract: Automated Program Repair (APR) techniques aim to automatically fix buggy
programs. Among these, Large Language Model-based (LLM-based) approaches have
shown great promise. Recent advances demonstrate that directly leveraging LLMs
can achieve leading results. However, these techniques remain suboptimal in
generating contextually relevant and accurate patches, as they often overlook
repair ingredients crucial for practical program repair. In this paper, we
propose ReinFix, a novel framework that enables LLMs to autonomously search for
repair ingredients throughout both the reasoning and solution phases of bug
fixing. In the reasoning phase, ReinFix integrates static analysis tools to
retrieve internal ingredients, such as variable definitions, to assist the LLM
in root cause analysis when it encounters difficulty understanding the context.
During the solution phase, when the LLM lacks experience in fixing specific
bugs, ReinFix searches for external ingredients from historical bug fixes with
similar bug patterns, leveraging both the buggy code and its root cause to
guide the LLM in identifying appropriate repair actions, thereby increasing the
likelihood of generating correct patches. Evaluations on two popular benchmarks
(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over
SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the
baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than
the SOTA. Importantly, when evaluating on the recent benchmarks that are free
of data leakage risk, ReinFix also maintains the best performance.

</details>


### [13] [From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers](https://arxiv.org/abs/2506.23234)
*Peerachai Banyongrakkul,Mansooreh Zahedi,Patanamon Thongtanunam,Christoph Treude,Haoyu Gao*

Main category: cs.SE

TL;DR: 该论文研究了预训练模型（PTM）在下游开发中的重用挑战，通过分析840个GitHub问题报告，提出了七类主要挑战，并发现PTM相关问题解决时间更长。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在多个领域取得成功，但下游开发者在重用PTM时面临的问题尚未深入探索。

Method: 定性分析来自31个GitHub项目的840个PTM相关问题报告，构建全面的挑战分类。

Result: 识别了七类PTM重用挑战，发现PTM相关问题解决时间显著更长。

Conclusion: 研究结果对实践者具有指导意义，并指出了未来研究方向。

Abstract: Pre-trained models (PTMs) have gained widespread popularity and achieved
remarkable success across various fields, driven by their groundbreaking
performance and easy accessibility through hosting providers. However, the
challenges faced by downstream developers in reusing PTMs in software systems
are less explored. To bridge this knowledge gap, we qualitatively created and
analyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub
projects. We systematically developed a comprehensive taxonomy of PTM-related
challenges that developers face in downstream projects. Our study identifies
seven key categories of challenges that downstream developers face in reusing
PTMs, such as model usage, model performance, and output quality. We also
compared our findings with existing taxonomies. Additionally, we conducted a
resolution time analysis and, based on statistical tests, found that
PTM-related issues take significantly longer to be resolved than issues
unrelated to PTMs, with significant variation across challenge categories. We
discuss the implications of our findings for practitioners and possibilities
for future research.

</details>


### [14] [Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning](https://arxiv.org/abs/2506.23534)
*Siyu Chen,Jiongyi Yang,Xiang Chen,Menglin Zheng,Minnan Wei,Xiaolin Ju*

Main category: cs.SE

TL;DR: 该论文提出了一种统一方法，结合EDAT和MTL，显著提升了软件漏洞检测与分类的性能，尤其是对罕见漏洞类型的识别。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞对现代系统构成严重威胁，现有方法因数据稀缺和任务独立性而受限，亟需有效解决方案。

Method: 采用嵌入层驱动的对抗训练（EDAT）和多任务学习（MTL），增强模型鲁棒性并利用任务间相关性。

Result: 实验显示，该方法在漏洞类型预测（VTP）和行级漏洞检测（LVD）中均优于现有技术，尤其提升了罕见类型的识别效果。

Conclusion: 结合EDAT与MTL的统一方法显著改进任务性能，值得进一步研究。

Abstract: Context: Software vulnerabilities pose a significant threat to modern
software systems, as evidenced by the growing number of reported
vulnerabilities and cyberattacks. These escalating trends underscore the urgent
need for effective approaches that can automatically detect and understand
software vulnerabilities. Objective: However, the scarcity of labeled samples
and the class imbalance issue in vulnerability datasets present significant
challenges for both Vulnerability Type Prediction (VTP) and Line-level
Vulnerability Detection (LVD), especially for rare yet critical vulnerability
types. Moreover, most existing studies treat VTP and LVD as independent tasks,
overlooking their inherent correlation, which limits the potential to leverage
shared semantic patterns across tasks. Methods: To address these limitations,
we propose a unified approach that integrates Embedding-Layer Driven
Adversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT
enhances model robustness by introducing adversarial perturbations to
identifier embeddings, guided by semantic importance. Meanwhile, MTL improves
overall performance by leveraging shared representations and inter-task
correlations between VTP and LVD. Results: Extensive experiments demonstrate
that our proposed approach outperforms state-of-the-art baselines on both VTP
and LVD tasks. For VTP, it yields notable improvements in accuracy, precision,
recall, and F1-score, particularly in identifying rare vulnerability types.
Similarly, for LVD, our approach enhances line-level detection accuracy while
significantly reducing false positives. Conclusion: Our study demonstrates that
combining EDAT with MTL provides a unified solution that improves performance
on both tasks and warrants further investigation.

</details>


### [15] [Green Metrics Tool: Measuring for fun and profit](https://arxiv.org/abs/2506.23967)
*Geerd-Dietger Hoffmann,Verena Majuntke*

Main category: cs.SE

TL;DR: 论文讨论了如何通过测量和评估软件资源消耗来减少碳排放，介绍了一种名为Green Metrics Tool（GMT）的新框架，用于准确测量软件资源消耗，并讨论了其可视化、可比性和优化功能。


<details>
  <summary>Details</summary>
Motivation: 随着计算资源需求的增加，软件环境影响日益受到关注，需要优化资源消耗和减少碳排放。

Method: 引入Green Metrics Tool（GMT），一个基于容器化、可控制且可重复的生命周期框架，测量软件资源消耗。

Result: GMT提供了可视化、可比性和基于规则及LLM的优化功能，帮助开发者和研究人员减少软件环境影响。

Conclusion: GMT是一种有效的工具，能够帮助测量和优化软件资源消耗，从而减少碳排放。

Abstract: The environmental impact of software is gaining increasing attention as the
demand for computational resources continues to rise. In order to optimize
software resource consumption and reduce carbon emissions, measuring and
evaluating software is a first essential step. In this paper we discuss what
metrics are important for fact base decision making. We introduce the Green
Metrics Tool (GMT), a novel framework for accurately measuring the resource
consumption of software. The tool provides a containerized, controlled, and
reproducible life cycle-based approach, assessing the resource use of software
during key phases. Finally, we discuss GMT features like visualization,
comparability and rule- and LLM-based optimisations highlighting its potential
to guide developers and researchers in reducing the environmental impact of
their software.

</details>


### [16] [Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance](https://arxiv.org/abs/2506.23535)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 比较大型语言模型生成的C++代码是否符合MISRA C++标准


<details>
  <summary>Details</summary>
Motivation: 确保安全关键系统中的代码生成符合严格的编码标准

Method: 对多种大型语言模型生成的C++代码进行MISRA C++合规性分析

Result: 代码生成模型在安全关键系统中的适用性分析

Conclusion: 需进一步验证和改进模型以符合安全关键领域的要求

Abstract: Safety-critical systems are engineered systems whose failure or malfunction
could result in catastrophic consequences. The software development for
safety-critical systems necessitates rigorous engineering practices and
adherence to certification standards like DO-178C for avionics. DO-178C is a
guidance document which requires compliance to well-defined software coding
standards like MISRA C++ to enforce coding guidelines that prevent the use of
ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have
demonstrated significant capabilities in automatic code generation across a
wide range of programming languages, including C++. Despite their impressive
performance, code generated by LLMs in safety-critical domains must be
carefully analyzed for conformance to MISRA C++ coding standards. In this
paper, I have conducted a comparative analysis of the C++ code generated by
popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and
Microsoft Copilot for compliance with MISRA C++.

</details>


### [17] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Main category: cs.SE

TL;DR: QLPro是一个结合LLMs和静态分析工具的漏洞检测框架，在开源项目中表现优于CodeQL，发现更多漏洞包括未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 通过结合LLMs和静态分析工具，提高开源项目漏洞检测的全面性和准确性。

Method: 系统集成LLMs和静态分析工具，构建新数据集JavaTest进行测试。

Result: QLPro检测到41个漏洞（CodeQL仅检测到24个），并发现6个未知漏洞（包括2个0-day漏洞）。

Conclusion: QLPro显著提升了漏洞检测能力，尤其在发现未知漏洞方面表现突出。

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


### [18] [Towards a Science of Developer eXperience (DevX)](https://arxiv.org/abs/2506.23715)
*Benoit Combemale*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As software continues to permeate nearly every facet of modern life, the
complexity and ubiquity of digital services underscore the need for
sustainable, effective, and inclusive software development practices. Although
software engineering has made significant progress in technical challenges
since its inception, the human experience of those involved in software
creation, broadly defined as developers, remains underexplored. This column
advocates for the formal recognition of Developer eXperience (DevX) as a
distinct research field. We argue that DevX profoundly influences critical
development activities and overall productivity, especially as development
becomes increasingly collaborative and diverse in terms of application domains.
Building on existing efforts to measure and enhance DevX, we identify key
rationales, scientific enablers, and interdisciplinary intersections that
support this emerging discipline. We also outline the core scientific
challenges ahead, aiming to call for actions from the research community and to
promote more human-centered approaches to software engineering.

</details>


### [19] [A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications](https://arxiv.org/abs/2506.23749)
*Boyang Yang,Zijian Cai,Fengling Liu,Bach Le,Lingming Zhang,Tegawendé F. Bissyandé,Yang Liu,Haoye Tian*

Main category: cs.SE

TL;DR: LLMs在程序自动修复(APR)中的应用被分为四类范式，每种范式在任务对齐、部署速度、控制性和复杂度上有不同权衡。未来研究方向包括轻量级人类反馈和成本感知规划。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何改变APR领域，并分析不同范式的优缺点，以推动更可靠和高效的修复系统。

Method: 将63个LLM-based APR系统分类为四种范式，分析其增强上下文的方法及关键权衡。

Result: 发现每种范式在训练成本、部署速度、控制性和复杂性方面有显著差异。

Conclusion: 需要进一步研究轻量级反馈和成本优化，以解决语义验证和规模化修复的挑战。

Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We
categorize the recent 63 LLM-based APR systems published from January 2022 to
June 2025 into four paradigms, and show how retrieval- or analysis-augmented
contexts strengthen any of them. This taxonomy clarifies key trade-offs:
fine-tuning delivers strong task alignment at high training cost; prompting
enables rapid deployment but is limited by prompt design and context windows;
procedural pipelines offer reproducible control with moderate overhead; agentic
frameworks tackle multi-hunk or cross-file bugs at the price of increased
latency and complexity. Persistent challenges include verifying semantic
correctness beyond test suites, repairing repository-scale defects, and
lowering the costs of LLMs. We outline research directions that combine
lightweight human feedback, repository-aware retrieval, code analysis, and
cost-aware planning to advance reliable and efficient LLM-based APR.

</details>


### [20] [Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead](https://arxiv.org/abs/2506.23762)
*Hongzhou Rao,Yanjie Zhao,Xinyi Hou,Shenao Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文从软件工程角度系统分析了大型语言模型（LLM）开发周期中的挑战，并提出潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究未从软件工程视角系统探讨LLM开发中的挑战，需填补这一空白。

Method: 将LLM开发生命周期分为六个阶段，逐一分析研究现状、挑战及解决方向。

Result: 总结了每个阶段的关键挑战，并提出了未来的研究方向。

Conclusion: 从软件工程角度为LLM开发的未来进展提供了宝贵见解。

Abstract: The rapid advancement of large language models (LLMs) has redefined
artificial intelligence (AI), pushing the boundaries of AI research and
enabling unbounded possibilities for both academia and the industry. However,
LLM development faces increasingly complex challenges throughout its lifecycle,
yet no existing research systematically explores these challenges and solutions
from the perspective of software engineering (SE) approaches. To fill the gap,
we systematically analyze research status throughout the LLM development
lifecycle, divided into six phases: requirements engineering, dataset
construction, model development and enhancement, testing and evaluation,
deployment and operations, and maintenance and evolution. We then conclude by
identifying the key challenges for each phase and presenting potential research
directions to address these challenges. In general, we provide valuable
insights from an SE perspective to facilitate future advances in LLM
development.

</details>


### [21] [Requirements for Active Assistance of Natural Questions in Software Architecture](https://arxiv.org/abs/2506.23898)
*Diogo Lemos,Ademar Aguiar,Neil B. Harrison*

Main category: cs.SE

TL;DR: 论文探讨了自然问题对架构设计的重要性，提出了一个生命周期模型和支持环境，以更好地管理和利用这些问题，从而提高架构知识的保存和决策效率。


<details>
  <summary>Details</summary>
Motivation: 自然问题在架构设计中至关重要，但常被忽视，导致架构漂移和知识流失。研究旨在理解其生命周期并提出解决方案。

Method: 通过文献综述、需求研讨会和三次设计迭代，提出自然问题的生命周期模型，并结合专家调查验证需求。

Result: 提出的支持环境整合了知识管理和AI技术，能有效提升协作和决策效率，优于传统方法。

Conclusion: 研究为架构设计中的自然问题管理提供了可行方案，未来可进一步优化环境适应性和实用性。

Abstract: Natural questions are crucial to shaping key architectural decisions and
preserving architectural knowledge. They arise organically during the
architectural design process, often resulting from the existing architectural
experience of the designer and the distinctive characteristics of the system
being designed. However, natural questions are often mismanaged or ignored,
which can lead to architectural drift, knowledge loss, inefficient resource
use, or poor understandability of the system's architecture. We aim to better
understand the lifecycle of natural questions, its key requirements, challenges
and difficulties, and then to envision an assisted environment to properly
support it. The environment should be adaptable and responsive to real-world
constraints and uncertainties by seamlessly integrating knowledge management
tools and artificial intelligence techniques into software development
workflows. Based on existing literature, a requirements workshop, and three
design iterations, we proposed a lifecycle for natural questions and elicited
essential functional and non-functional requirements for such an environment.
At last, the results of a survey conducted with experts helped to analyze and
validate the elicited requirements and proposed features for the environment to
enhance collaboration, decision-making, and the preservation of architectural
knowledge more effectively than conventional methods.

</details>


### [22] [STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.23995)
*Mingfei Cheng,Renzhi Wang,Xiaofei Xie,Yuan Zhou,Lei Ma*

Main category: cs.SE

TL;DR: 摘要提出了一种名为STCLocker的测试技术，用于评估自动驾驶系统（ADS）在多车协同场景下避免死锁的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶系统测试技术主要集中在单车环境下，而多车协同场景下的死锁问题尚未充分研究，因此需要一种新的测试方法。

Method: STCLocker包括三个关键组件：死锁预言机（检测死锁）、冲突反馈和冲突感知场景生成（引导车辆进入竞争状态）。

Result: 实验结果表明，STCLocker在两种ADS（Roach和OpenCDA）上生成了比基线方法更多的死锁场景。

Conclusion: STCLocker为多车协同场景下的死锁测试提供了一种有效的解决方案。

Abstract: Autonomous Driving System (ADS) testing is essential to ensure the safety and
reliability of autonomous vehicles (AVs) before deployment. However, existing
techniques primarily focus on evaluating ADS functionalities in single-AV
settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes
crucial to assess their cooperative performance, particularly regarding
deadlocks, a fundamental coordination failure in which multiple AVs enter a
circular waiting state indefinitely, resulting in motion planning failures.
Despite its importance, the cooperative capability of ADSs to prevent deadlocks
remains insufficiently underexplored. To address this gap, we propose the first
dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,
STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs
controlled by the ADS under test are in a circular wait state. STCLocker
consists of three key components: Deadlock Oracle, Conflict Feedback, and
Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable
black-box mechanism for detecting deadlock cycles among multiple AVs within a
given scenario. Conflict Feedback and Conflict-aware Scenario Generation
collaborate to actively guide AVs into simultaneous competition over spatial
conflict resources (i.e., shared passing regions) and temporal competitive
behaviors (i.e., reaching the conflict region at the same time), thereby
increasing the effectiveness of generating conflict-prone deadlocks. We
evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,
a module-based ADS supporting cooperative communication. Experimental results
show that, on average, STCLocker generates more DLS than the best-performing
baseline.

</details>


### [23] [Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection](https://arxiv.org/abs/2506.24015)
*Ramtin Ehsani,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 提出了一种分层知识注入框架，通过逐步增加结构化上下文改进LLM的自动程序修复能力，显著提升修复率。


<details>
  <summary>Details</summary>
Motivation: 利用更广泛的项目级上下文（如依赖关系、文档和历史提交）来解决当前仅依赖局部代码信息时仍难以修复的bug。

Method: 采用三层知识注入框架：Bug知识层（错误函数和失败测试）、仓库知识层（结构依赖和相关文件）和项目知识层（文档与历史修复）。

Result: 在314个bug上测试，使用Llama 3.3达到79%的修复率，比先前工作提升23%；不同类型bug对不同层级上下文的需求不同。

Conclusion: 分层上下文注入显著提升修复效果，但复杂或孤立bug仍需交互式自适应系统。

Abstract: Prompting LLMs with bug-related context (e.g., error messages, stack traces)
improves automated program repair, but many bugs still remain unresolved. In
real-world projects, developers often rely on broader repository and
project-level context beyond the local code to resolve such bugs. In this
paper, we investigate how automatically extracting and providing such knowledge
can improve LLM-based program repair. We propose a layered knowledge injection
framework that incrementally augments LLMs with structured context. It starts
with the Bug Knowledge Layer, which includes information such as the buggy
function and failing tests; expands to the Repository Knowledge Layer, which
adds structural dependencies, related files, and commit history; and finally
injects the Project Knowledge Layer, which incorporates relevant details from
documentation and previously fixed bugs. We evaluate this framework on a
dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),
and analyze fix rates across six bug types. By progressively injecting
knowledge across layers, our approach achieves a fix rate of 79% (250/314)
using Llama 3.3, a significant improvement of 23% over previous work. All bug
types show improvement with the addition of repository-level context, while
only a subset benefit further from project-level knowledge, highlighting that
different bug types require different levels of contextual information for
effective repair. We also analyze the remaining unresolved bugs and find that
more complex and structurally isolated bugs, such as Program Anomaly and GUI
bugs, remain difficult even after injecting all available information. Our
results show that layered context injection improves program repair and suggest
the need for interactive and adaptive APR systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [24] [Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language](https://arxiv.org/abs/2506.23058)
*Nikolaj Hey Hinnerskov,Robert Schenck,Cosmin E. Oancea*

Main category: cs.PL

TL;DR: 提出了一种自动验证纯数据并行程序属性的新方法，通过索引函数转换和代数不等式推理。


<details>
  <summary>Details</summary>
Motivation: 解决非线性索引的纯数据并行程序的属性验证问题，为编译器优化提供实用保证。

Method: 使用索引函数表示数组，通过代数（不）等式推理，结合Fourier-Motzkin求解器验证属性。

Result: 在Futhark语言中实现，验证时间平均1秒，动态验证消除后GPU程序性能显著提升。

Conclusion: 该方法实用且灵活，能为编译器优化提供有效支持。

Abstract: This paper presents a novel approach to automatically verify properties of
pure data-parallel programs with non-linear indexing -- expressed as pre- and
post-conditions on functions. Programs consist of nests of second-order array
combinators (e.g., map, scan, and scatter) and loops. The key idea is to
represent arrays as index functions: programs are index function
transformations over which properties are propagated and inferred. Our
framework proves properties on index functions by distilling them into
algebraic (in)equalities and discharging them to a Fourier-Motzkin-based
solver. The framework is practical and accessible: properties are not
restricted to a decidable logic, but instead are carefully selected to express
practically useful guarantees that can be automatically reasoned about and
inferred. These guarantees extend beyond program correctness and can be
exploited by the entire compiler pipeline for optimization. We implement our
system in the pure data-parallel language Futhark and demonstrate its
practicality on seven applications, reporting an average verification time of 1
second. Two case studies show how eliminating dynamic verification in GPU
programs results in significant speedups.

</details>


### [25] [A Denotational Semantics for Quantum Loops](https://arxiv.org/abs/2506.23320)
*Nicola Assolini,Alessandra Di Pierro*

Main category: cs.PL

TL;DR: 本文提出了一种用于高级量子编程结构的指称语义学，重点研究量子控制分支和循环的概念意义。


<details>
  <summary>Details</summary>
Motivation: 为量子计算机编程时，需在不同抽象层次上处理问题，本文旨在通过指称语义学明确量子控制流（尤其是循环）的数学意义。

Method: 引入一个指称域，用于定义具有循环的量子控制流的数学意义，反映量子系统在程序执行中的相干演化。

Result: 成功构建了一种能够描述量子控制流程的数学框架，特别是明确了量子循环的语义。

Conclusion: 本文通过指称语义学为量子编程提供了一种新的理论工具，特别适用于复杂控制流的语义描述。

Abstract: Programming a quantum computer, i.e., implementing quantum algorithms on a
quantum processor-based copmputer architecture, is a task that can be addressed
(just as for classical computers) at different levels of abstraction. This
paper proposes a denotational semantics for high-level quantum programming
constructs, focusing on the conceptual meaning of quantum-controlled branching
and iteration. We introduce a denotational domain where a mathematical meaning
of a quantum control flow with loops can be defined, which reflects the
coherent evolution of the quantum system implementing the program.

</details>


### [26] [Compiling a Q# Subset to QASM 3.0 in TypeScript via a JSON Based IR](https://arxiv.org/abs/2506.23407)
*Marcus Edwards*

Main category: cs.PL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We implement a compile toolchain from Q# to QASM 3.0 including a
full-featured lexer and parser implementation, as well as a compiler that
supports a subset of Q# features. The lexer, parser and compiler are shown to
work with various input Q# programs and the implementation is compared against
existing Q# compile tools. Unlike the Microsoft implementation of the official
Q# compile toolchain, our implementation is written in TypeScript in order to
port functionality to web environments.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [27] [Data-Driven Power Modeling and Monitoring via Hardware Performance Counter Tracking](https://arxiv.org/abs/2506.23672)
*Sergio Mazzola,Gabriele Ara,Thomas Benz,Björn Forsberg,Tommaso Cucinotta,Luca Benini*

Main category: cs.PF

TL;DR: 论文提出了一种新型功耗建模方法，通过性能监控计数器（PMCs）与动态电压频率调整（DVFS）状态的线性相关性，实现高精度、低开销的全系统功耗评估。


<details>
  <summary>Details</summary>
Motivation: 在当前嵌入式计算时代，高能效设计至关重要，硬件异构性和并行性虽提升效率但增加了功耗评估的复杂性，亟需一种无需微架构细节的低开销方法。

Method: 基于PMCs与各硬件子系统的功耗线性相关性，构建简单模型并组合为完整系统模型，集成到Linux内核的Runmeter框架中。

Result: 平均功耗估算误差7.5%，能量估算误差1.3%，Runmeter运行时开销仅0.7%。

Conclusion: 该方法为动态硬件和软件调整提供了高精度、低开销的功耗测量，适用于DVFS和任务调度等场景。

Abstract: Energy-centric design is paramount in the current embedded computing era: use
cases require increasingly high performance at an affordable power budget,
often under real-time constraints. Hardware heterogeneity and parallelism help
address the efficiency challenge, but greatly complicate online power
consumption assessments, which are essential for dynamic hardware and software
stack adaptations. We introduce a novel power modeling methodology with
state-of-the-art accuracy, low overhead, and high responsiveness, whose
implementation does not rely on microarchitectural details. Our methodology
identifies the Performance Monitoring Counters (PMCs) with the highest linear
correlation to the power consumption of each hardware sub-system, for each
Dynamic Voltage and Frequency Scaling (DVFS) state. The individual, simple
models are composed into a complete model that effectively describes the power
consumption of the whole system, achieving high accuracy and low overhead. Our
evaluation reports an average estimation error of 7.5% for power consumption
and 1.3% for energy. We integrate these models in the Linux kernel with
Runmeter, an open-source, PMC-based monitoring framework. Runmeter manages PMC
sampling and processing, enabling the execution of our power models at runtime.
With a worst-case time overhead of only 0.7%, Runmeter provides responsive and
accurate power measurements directly in the kernel. This information can be
employed for actuation policies in workload-aware DVFS and power-aware,
closed-loop task scheduling.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [28] [Using SBPF to Accelerate Kernel Memory Access From Userspace](https://arxiv.org/abs/2506.22632)
*Boming Kong,Zhizhou Zhang,Jonathan Balkind*

Main category: cs.OS

TL;DR: 论文提出了一种通过共享内存和uBPF虚拟机改进用户空间与内核通信的新方法，避免了传统系统调用的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统的用户空间与内核通信方式（如系统调用）存在性能瓶颈，包括内存拷贝和微架构状态刷新，阻碍了软件性能提升。

Method: 通过建立用户空间与内核的共享内存区域，并配合uBPF虚拟机控制对内核内存的访问，实现非阻塞数据传输。

Result: 实验表明，该方法在多个用例中优于传统通信机制，显著提升了速度。

Conclusion: 共享内存结合uBPF虚拟机的方法有效提升了用户与内核通信的效率，同时保持了地址空间隔离的安全性。

Abstract: The cost of communication between the operating system kernel and user
applications has long blocked improvements in software performance.
Traditionally, operating systems encourage software developers to use the
system call interface to transfer (or initiate transfer of) data between user
applications and the kernel. This approach not only hurts performance at the
software level due to memory copies between user space address spaces and
kernel space address spaces, it also hurts system performance at the
microarchitectural level by flushing processor pipelines and other
microarchitectural state.
  In this paper, we propose a new communication interface between user
applications and the kernel by setting up a shared memory region between user
space applications and the kernel's address space. We acknowledge the danger in
breaking the golden law of user-kernel address space isolation, so we coupled a
uBPF VM (user-space BPF Virtual Machine) with shared memory to control access
to the kernel's memory from the user's application. In this case, user-space
programs can access the shared memory under the supervision of the uBPF VM (and
the kernel's blessing of its shared library) to gain non-blocking data transfer
to and from the kernel's memory space. We test our implementation in several
use cases and find this mechanism can bring speedups over traditional
user-kernel information passing mechanisms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [29] [Golden Ratio Assisted Localization for Wireless Sensor Network](https://arxiv.org/abs/2506.22464)
*Hitesh Mohapatra*

Main category: cs.NI

TL;DR: 提出了一种基于黄金比例的新型无线传感器网络定位算法（GRL），通过优化节点布署和通信范围，提高了定位精度并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 利用黄金比例（phi 1.618）的数学特性优化无线传感器网络的节点布署和通信范围，以在保证定位精度的同时减少能耗。

Method: 采用基于phi的锚节点布署和跳数敏感权重（使用phi指数），通过仿真（100 m * 100 m区域，100个节点，10个锚节点）验证效果。

Result: GRL的平均定位误差为2.35米（优于DV-Hop的3.87米和Centroid的4.95米），能耗降至1.12微焦耳/节点（优于DV-Hop的1.78微焦耳和Centroid的1.45微焦耳）。

Conclusion: GRL在能耗和定位精度上具有显著优势，适合能耗受限和大规模的无线传感器网络布署。

Abstract: This paper presents a novel localization algorithm for wireless sensor
networks (WSNs) called Golden Ratio Localization (GRL), which leverages the
mathematical properties of the golden ratio (phi 1.618) to optimize both node
placement and communication range. GRL introduces phi-based anchor node
deployment and hop-sensitive weighting using phi-exponents to improve
localization accuracy while minimizing energy consumption. Through extensive
simulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10
anchors, GRL achieved an average localization error of 2.35 meters,
outperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of
energy efficiency, GRL reduced localization energy consumption to 1.12 microJ
per node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid.
These results confirm that GRL provides a more balanced and efficient
localization approach, making it especially suitable for energy-constrained and
large-scale WSN deployments.

</details>


### [30] [Reliable Transmission of LTP Using Reinforcement Learning-Based Adaptive FEC](https://arxiv.org/abs/2506.22470)
*Liang Chen,Yu Song,Kanglian Zhao,Juan A. Fraire,Wenfeng Li*

Main category: cs.NI

TL;DR: 本文提出了一种基于强化学习的自适应前向纠错算法，用于优化星际网络中数据传输的效率。


<details>
  <summary>Details</summary>
Motivation: 现有静态和基于延迟反馈的动态编码方法难以应对高度变化且不可预测的深空信道条件，因此需要更智能的算法来优化数据传输。

Method: 该算法利用历史反馈和系统状态预测未来信道条件，并主动调整编码率以预防解码失败和减少冗余。

Result: 在模拟的地球-月球和地球-火星链路场景中，该算法显著降低了矩阵解码失败率（至少减少了2/3），表现出优越性能。

Conclusion: 提出的强化学习自适应FEC算法有效优化了星际网络中的数据传输，相较于现有方法具有明显优势。

Abstract: Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission
Protocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery
in challenging interplanetary networks. While previous studies have integrated
packet-level Forward Erasure Correction (FEC) into LTP to reduce retransmission
time costs, existing static and delay-feedback-based dynamic coding methods
struggle with highly variable and unpredictable deep space channel conditions.
This paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm
to address these limitations. The algorithm utilizes historical feedback and
system state to predict future channel conditions and proactively adjust the
code rate. This approach aims to anticipate channel quality degradation,
thereby preventing decoding failures and subsequent LTP retransmissions and
improving coding efficiency by minimizing redundancy during favorable channel
conditions. Performance evaluations conducted in simulated Earth-Moon and
Earth-Mars link scenarios demonstrate this algorithm's effectiveness in
optimizing data transmission for interplanetary networks. Compared to existing
methods, this approach demonstrates significant improvement, with matrix
decoding failures reduced by at least 2/3.

</details>


### [31] [RL-based Adaptive Task Offloading in Mobile-Edge Computing for Future IoT Networks](https://arxiv.org/abs/2506.22474)
*Ziad Qais Al Abbasi,Khaled M. Rabie,Senior Member,Xingwang Li,Senior Member,Wali Ullah Khan,Asma Abu Samah*

Main category: cs.NI

TL;DR: 该研究提出了一种基于强化学习的移动边缘计算卸载方案，用于超密集蜂窝网络，以优化资源分配和动态卸载决策，显著提高了能效、网络吞吐量和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）设备受计算和电源限制，需将任务发送至远距离云服务器，导致低延迟服务（如工业控制和自动驾驶）面临挑战。

Method: 采用强化学习技术设计了一种新的移动边缘计算卸载方案，结合非正交多址技术以提高资源利用率。

Result: 仿真结果表明，该方案在能效、网络吞吐量和用户满意度方面优于现有最先进的卸载算法。

Conclusion: 该研究为超密集蜂窝网络中的任务卸载提供了一种高效的解决方案，显著提升了网络性能和用户体验。

Abstract: The Internet of Things (IoT) has been increasingly used in our everyday lives
as well as in numerous industrial applications. However, due to limitations in
computing and power capabilities, IoT devices need to send their respective
tasks to cloud service stations that are usually located at far distances.
Having to transmit data far distances introduces challenges for services that
require low latency such as industrial control in factories and plants as well
as artificial intelligence assisted autonomous driving. To solve this issue,
mobile edge computing (MEC) is deployed at the networks edge to reduce
transmission time. In this regard, this study proposes a new offloading scheme
for MEC-assisted ultra dense cellular networks using reinforcement learning
(RL) techniques. The proposed scheme enables efficient resource allocation and
dynamic offloading decisions based on varying network conditions and user
demands. The RL algorithm learns from the networks historical data and adapts
the offloading decisions to optimize the networks overall performance.
Non-orthogonal multiple access is also adopted to improve resource utilization
among the IoT devices. Simulation results demonstrate that the proposed scheme
outperforms other stateof the art offloading algorithms in terms of energy
efficiency, network throughput, and user satisfaction.

</details>


### [32] [Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI](https://arxiv.org/abs/2506.22477)
*Huiwen Han*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces an innovative design for robotic operating platforms,
underpinned by a transformative Internet of Things (IoT) architecture,
seamlessly integrating cutting-edge technologies such as large language models
(LLMs), generative AI, edge computing, and 5G networks. The proposed platform
aims to elevate the intelligence and autonomy of IoT systems and robotics,
enabling them to make real-time decisions and adapt dynamically to changing
environments. Through a series of compelling case studies across industries
including smart manufacturing, healthcare, and service sectors, this paper
demonstrates the substantial potential of IoT-enabled robotics to optimize
operational workflows, enhance productivity, and deliver innovative, scalable
solutions. By emphasizing the roles of LLMs and generative AI, the research
highlights how these technologies drive the evolution of intelligent robotics
and IoT, shaping the future of industry-specific advancements. The findings not
only showcase the transformative power of these technologies but also offer a
forward-looking perspective on their broader societal and industrial
implications, positioning them as catalysts for next-generation automation and
technological convergence.

</details>


### [33] [Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits](https://arxiv.org/abs/2506.22480)
*Mariam Yahya,Aydin Sezgin,Setareh Maghsudi*

Main category: cs.NI

TL;DR: 论文提出了一种分布式自适应多代理最佳臂识别算法，用于解决多接入边缘计算中的服务部署问题，以减少用户延迟。


<details>
  <summary>Details</summary>
Motivation: 小型蜂窝网络中，用户对计算密集型服务的需求增加了云端访问的延迟，而边缘计算的有限容量使得服务部署决策复杂化，尤其是在服务需求和网络条件未知的情况下。

Method: 将服务需求建模为服务属性的线性函数，并将服务部署任务表述为线性多臂老虎机问题，提出了一种分布式自适应多代理最佳臂识别算法。

Result: 仿真结果表明，该算法能够在固定置信度下识别出最优服务，并通过SBS的协作显著缩短学习轮数。

Conclusion: 该算法有效解决了边缘计算中的服务部署问题，显著降低了用户延迟，并为边缘网络的动态资源分配提供了理论支持。

Abstract: As users in small cell networks increasingly rely on computation-intensive
services, cloud-based access often results in high latency. Multi-access edge
computing (MEC) mitigates this by bringing computational resources closer to
end users, with small base stations (SBSs) serving as edge servers to enable
low-latency service delivery. However, limited edge capacity makes it
challenging to decide which services to deploy locally versus in the cloud,
especially under unknown service demand and dynamic network conditions. To
tackle this problem, we model service demand as a linear function of service
attributes and formulate the service placement task as a linear bandit problem,
where SBSs act as agents and services as arms. The goal is to identify the
service that, when placed at the edge, offers the greatest reduction in total
user delay compared to cloud deployment. We propose a distributed and adaptive
multi-agent best-arm identification (BAI) algorithm under a fixed-confidence
setting, where SBSs collaborate to accelerate learning. Simulations show that
our algorithm identifies the optimal service with the desired confidence and
achieves near-optimal speedup, as the number of learning rounds decreases
proportionally with the number of SBSs. We also provide theoretical analysis of
the algorithm's sample complexity and communication overhead.

</details>


### [34] [Wireless Home Automation Using Social Networking Websites](https://arxiv.org/abs/2506.22482)
*Divya Alok Gupta,Dwith Chenna,B. Aditya Vighnesh Ramakanth*

Main category: cs.NI

TL;DR: 该论文提出了一种基于社交媒体认证的无线家庭自动化系统（WHAS），利用Twitter的安全认证跟踪用户活动并控制家用电器，突出了其应用和传统系统的优势。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的发展，无线家庭自动化系统（WHAS）面临安全、多设备统一控制和用户体验等挑战，需要创新解决方案。

Method: 通过社交媒体（如Twitter）的安全认证系统跟踪用户活动，实现对家用电器的控制。

Result: 系统能够有效结合社交媒体认证与控制功能，提升了安全性和用户体验。

Conclusion: 提出的WHAS系统在安全性和用户体验方面优于传统家庭自动化系统，具有广泛的应用潜力。

Abstract: With the advent of Internet of Things, Wireless Home Automation Systems WHAS
are gradually gaining popularity. These systems are faced with multiple
challenges such as security; controlling a variety of home appliances with a
single interface and user friendliness. In this paper we propose a system that
uses secure authentication systems of social networking websites such as
Twitter, tracks the end-users activities on the social network and then control
his or her domestic appliances. At the end, we highlight the applications of
the proposed WHAS and compare the advantages of our proposed system over
traditional home automation systems.

</details>


### [35] [Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications](https://arxiv.org/abs/2506.22991)
*Mehdi Bennis,Sumudu Samarakoon,Tamara Alshammari,Chathuranga Weeraddana,Zhoujun Tian,Chaouki Ben Issaid*

Main category: cs.NI

TL;DR: 本文讨论了无线网络作为关键社会基础设施的弹性，区分了弹性与可靠性和鲁棒性，并探讨了弹性的数学基础、技术方法及其应用。


<details>
  <summary>Details</summary>
Motivation: 随着自然和人为干扰的增加，无线网络必须具备弹性以应对意外情况和故障，确保其在不利条件下仍能恢复和适应。

Method: 文章首先澄清了弹性的定义，随后探讨了基于抽象、组合性和涌现的数学基础，并分析了多种弹性技术和方法。

Result: 提出了一套统一的弹性理论基础，并展示了多种应用案例，为下一代智能无线系统的设计奠定了基础。

Conclusion: 本文为无线通信系统的弹性建模和工程设计提供了统一框架，并指明了未来研究方向。

Abstract: Just like power, water, and transportation systems, wireless networks are a
crucial societal infrastructure. As natural and human-induced disruptions
continue to grow, wireless networks must be resilient. This requires them to
withstand and recover from unexpected adverse conditions, shocks, unmodeled
disturbances and cascading failures. Unlike robustness and reliability,
resilience is based on the understanding that disruptions will inevitably
happen. Resilience, as elasticity, focuses on the ability to bounce back to
favorable states, while resilience as plasticity involves agents and networks
that can flexibly expand their states and hypotheses through real-time
adaptation and reconfiguration. This situational awareness and active
preparedness, adapting world models and counterfactually reasoning about
potential system failures and the best responses, is a core aspect of
resilience. This article will first disambiguate resilience from reliability
and robustness, before delving into key mathematical foundations of resilience
grounded in abstraction, compositionality and emergence. Subsequently, we focus
our attention on a plethora of techniques and methodologies pertaining to the
unique characteristics of resilience, as well as their applications through a
comprehensive set of use cases. Ultimately, the goal of this paper is to
establish a unified foundation for understanding, modeling, and engineering
resilience in wireless communication systems, while laying a roadmap for the
next-generation of resilient-native and intelligent wireless systems.

</details>


### [36] [An Urban Multi-Operator QoE-Aware Dataset for Cellular Networks in Dense Environments](https://arxiv.org/abs/2506.22484)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Farah Yasmin binti Mohd Shaharuddin*

Main category: cs.NI

TL;DR: 该研究提供了一个包含30,925条标记记录的数据集，用于城市蜂窝网络的QoE和动态多模式优化。


<details>
  <summary>Details</summary>
Motivation: 解决城市蜂窝网络因高基础设施密度、用户移动性和多样化服务需求而面临的复杂性能挑战，填补现有数据集在QoE和移动模式方面的空白。

Method: 使用GNetTrack Pro在2平方公里的密集城市区域收集数据，涵盖三大运营商的关键信号参数和多模式移动场景。

Result: 生成一个适用于机器学习的结构化数据集，包含132个已验证的物理基站，支持网络规划与优化应用。

Conclusion: 该数据集为研究人员和从业者提供了一个可复现、可直接应用的资源，适用于QoE驱动的网络优化和移动管理。

Abstract: Urban cellular networks face complex performance challenges due to high
infrastructure density, varied user mobility, and diverse service demands.
While several datasets address network behaviour across different environments,
there is a lack of datasets that captures user centric Quality of Experience
(QoE), and diverse mobility patterns needed for efficient network planning and
optimization solutions, which are important for QoE driven optimizations and
mobility management. This study presents a curated dataset of 30,925 labelled
records, collected using GNetTrack Pro within a 2 km2 dense urban area,
spanning three major commercial network operators. The dataset captures key
signal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world
mobility modes including pedestrian routes, canopy walkways, shuttle buses, and
Bus Rapid Transit (BRT) routes. It also includes diverse network traffic
scenarios including (1) FTP upload and download, (2) video streaming, and (3)
HTTP browsing. A total of 132 physical cell sites were identified and validated
through OpenCellID and on-site field inspections, illustrating the high cell
density characteristic of 5G and emerging heterogeneous network deployment. The
dataset is particularly suited for machine learning applications, such as
handover optimization, signal quality prediction, and multi operator
performance evaluation. Released in a structured CSV format with accompanying
preprocessing and visualization scripts, this dataset offers a reproducible,
application ready resource for researchers and practitioners working on urban
cellular network planning and optimization.

</details>


### [37] [Towards an Optimized Multi-Cyclic Queuing and Forwarding in Time Sensitive Networking with Time Injection](https://arxiv.org/abs/2506.22671)
*Rubi Debnath,Mohammadreza Barzegaran,Sebastian Steinhorst*

Main category: cs.NI

TL;DR: Multi-CQF是一种改进的TSN整形机制，相比CQF能更好地适应多样化的时序需求。本文提出了一种遗传算法与模拟退火相结合的方法，优化Multi-CQF配置并引入时间注入（TI），调度能力提升了15%。


<details>
  <summary>Details</summary>
Motivation: 现有的Multi-CQF配置研究不足，导致实际应用受限。文中通过探索TI的影响和开发高效算法，旨在提升Multi-CQF的可调度性和实用性。

Method: 利用领域特定知识（DSK）缩小配置搜索空间，提出开源遗传算法（GA）和混合GA-模拟退火（GASA）方法，优化Multi-CQF网络配置和TI的引入。

Result: 实验表明，所提算法比基准模拟退火（SA）模型多调度15%的TT流，且GASA收敛速度更快，时间复杂度更低。

Conclusion: Multi-CQF通过TI和高效算法显著提升了调度能力，GASA在速度和效率上表现更优，为实际应用提供了有力支持。

Abstract: Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN)
shaping mechanism that provides bounded latency and deterministic Quality of
Service (QoS). However, CQF's use of a single cycle restricts its ability to
support TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and
Forwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses
multiple cycles on the same egress port, allowing it to accommodate TSN flows
with varied timing requirements more effectively than CQF. Despite its
potential, current Multi-CQF configuration studies are limited, leading to a
lack of comprehensive research, poor understanding of the mechanism, and
limited adoption of Multi-CQF in practical applications. Previous work has
shown the impact of Time Injection (TI), defined as the start time of
Time-Triggered (TT) flows at the source node, on CQF queue resource
utilization. However, the impact of TI has not yet been explored in the context
of Multi-CQF. This paper introduces a set of constraints and leverages Domain
Specific Knowledge (DSK) to reduce the search space for Multi-CQF
configuration. Building on this foundation, we develop an open-source Genetic
Algorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to
efficiently configure Multi-CQF networks and introduce TI in Multi-CQF to
enhance schedulability. Experimental results show that our proposed algorithms
significantly increase the number of scheduled TT flows compared to the
baseline Simulated Annealing (SA) model, improving scheduling by an average of
15%. Additionally, GASA achieves a 20% faster convergence rate and lower time
complexity, outperforming the SA model in speed, and efficiency.

</details>


### [38] [AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space](https://arxiv.org/abs/2506.22487)
*Amar Khelloufi,Huansheng Ning,Sahraoui Dhelim,Jianguo Ding*

Main category: cs.NI

TL;DR: 该论文综述了人工智能通用智能（AGI）与万物互联网（IoX）的整合，探讨了AGI如何解决CPST生态系统中的关键瓶颈问题，包括数据管理、协议优化和决策框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决互联系统（如CPST生态系统）在感知层、网络层和应用层中的瓶颈问题，通过整合AGI技术提供高效解决方案。

Method: 研究方法包括系统综述AGI在IoX中的应用，重点关注自适应传感器融合、边缘预处理、选择注意力机制、神经符号推理等技术。

Result: 研究表明，AGI技术为数据过载、协议异构性和身份爆炸等问题提供了新解决方案，并强调了跨层集成和伦理治理的重要性。

Conclusion: 结论指出，AGI增强的IoX是一个新兴研究领域，但仍需解决计算需求、可扩展性等挑战，以充分发挥其潜力。

Abstract: The integration of the Internet of Everything (IoX) and Artificial General
Intelligence (AGI) has given rise to a transformative paradigm aimed at
addressing critical bottlenecks across sensing, network, and application layers
in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide
a systematic and comprehensive review of AGI-enhanced IoX research, focusing on
three key components: sensing-layer data management, network-layer protocol
optimization, and application-layer decision-making frameworks. Specifically,
this survey explores how AGI can mitigate IoX bottlenecks challenges by
leveraging adaptive sensor fusion, edge preprocessing, and selective attention
mechanisms at the sensing layer, while resolving network-layer issues such as
protocol heterogeneity and dynamic spectrum management, neuro-symbolic
reasoning, active inference, and causal reasoning, Furthermore, the survey
examines AGI-enabled frameworks for managing identity and relationship
explosion. Key findings suggest that AGI-driven strategies, such as adaptive
sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions
to sensing-layer data overload, network-layer protocol heterogeneity, and
application-layer identity explosion. The survey underscores the importance of
cross-layer integration, quantum-enabled communication, and ethical governance
frameworks for future AGI-enabled IoX systems. Finally, the survey identifies
unresolved challenges, such as computational requirements, scalability, and
real-world validation, calling for further research to fully realize AGI's
potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is
emerging as a critical research field at the intersection of interconnected
systems and advanced AI.

</details>


### [39] [Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures](https://arxiv.org/abs/2506.22507)
*Yubo Peng,Luping Xiang,Kun Yang,Feibo Jiang,Kezhi Wang,Christos Masouros*

Main category: cs.NI

TL;DR: 文章讨论了6G网络中从单模态到多模态的集成感知与通信（ISAC）的范式转变，提出了三种架构（F-MAC、I-MAC、R-MAC）并展示了F-MAC在传感精度上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有单模态ISAC系统在6G应用需求下表现受限，促使转向多模态ISAC以提升环境特征表征和性能。

Method: 分析了多模态ISAC的关键挑战，引入大AI模型、语义通信等技术，并设计了F-MAC、I-MAC、R-MAC三种架构。

Result: F-MAC方案比传统单模态ISAC系统传感精度提高约80%。

Conclusion: 多模态ISAC是未来方向，但仍需解决开放性挑战。

Abstract: The evolution towards 6G networks requires the intelligent integration of
communication and sensing capabilities to support diverse and complex
applications, such as autonomous driving and immersive services. However,
existing integrated sensing and communication (ISAC) systems predominantly rely
on single-modal sensors as primary participants, which leads to a limited
representation of environmental features and significant performance
bottlenecks under the emerging requirements of 6G applications. This limitation
motivates a paradigm shift from single-modal to multimodal ISAC. In this
article, we first analyze the key challenges in realizing multimodal ISAC,
including the fusion of heterogeneous multimodal data, the high communication
overhead among distributed sensors, and the design of efficient and scalable
system architectures. We then introduce several enabling technologies, such as
large AI models, semantic communication, and multi-agent systems, that hold
promise for addressing these challenges. To operationalize these technologies,
we zoom into three architectural paradigms: fusion-based multimodal ISAC
(F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal
ISAC (R-MAC), each tailored to organize devices and modalities for efficient
collaboration in different scenarios. Thereafter, a case study is presented
based on the F-MAC scheme, demonstrating that the scheme achieves more
comprehensive sensing and improves sensing accuracy by approximately 80%
compared to conventional single-modal ISAC systems. Finally, we discuss several
open issues to be addressed in the future.

</details>


### [40] [Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks](https://arxiv.org/abs/2506.22745)
*Sijie He,Ziye Jia,Qiuming Zhu,Fuhui Zhou,Qihui Wu*

Main category: cs.NI

TL;DR: 本文提出了一种基于区块链的零信任架构和多智能体深度Q网络的自适应路由算法，以解决低空智能网络中路由稳定性和安全性的挑战，并显著降低了端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 低空智能网络（LAINs）因其扩展性和便携性在监视、灾难救援等领域至关重要，但由于分布式拓扑和高动态移动性，无人机（UAVs）易受安全威胁，导致路由性能下降。

Method: 提出区块链驱动的零信任架构管理无人机的加入和退出；将路由问题建模为分布式部分可观察马尔可夫决策过程，并设计了基于多智能体双深度Q网络的自适应路由算法。

Result: 仿真结果显示，所提机制的总端到端延迟比基准方法平均降低了22.38%。

Conclusion: 采用区块链和深度强化学习方法显著提高了低空智能网络的路由安全性和性能。

Abstract: Due to the scalability and portability, the low-altitude intelligent networks
(LAINs) are essential in various fields such as surveillance and disaster
rescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by
the distributed topology and high dynamic mobility, and vulnerable to security
threats, which may degrade the routing performance for data transmission.
Hence, how to ensure the routing stability and security of LAINs is a
challenge. In this paper, we focus on the routing process in LAINs with
multiple UAV clusters and propose the blockchain-enabled zero-trust
architecture to manage the joining and exiting of UAVs. Furthermore, we
formulate the routing problem to minimize the end-to-end (E2E) delay, which is
an integer linear programming and intractable to solve. Therefore, considering
the distribution of LAINs, we reformulate the routing problem into a
decentralized partially observable Markov decision process. With the proposed
soft hierarchical experience replay buffer, the multi-agent double deep
Q-network based adaptive routing algorithm is designed. Finally, simulations
are conducted and numerical results show that the total E2E delay of the
proposed mechanism decreases by 22.38\% than the benchmark on average.

</details>


### [41] [Offline Reinforcement Learning for Mobility Robustness Optimization](https://arxiv.org/abs/2506.22793)
*Pegah Alizadeh,Anastasios Giovanidis,Pradeepa Ramachandra,Vasileios Koutsoukis,Osama Arouk*

Main category: cs.NI

TL;DR: 利用离线强化学习优化MRO算法，决策变换器和保守Q学习方法表现优于基于规则的MRO，提升达7%。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过离线强化学习优化Cell Individual Offset调谐，避免进一步探索。

Method: 采用决策变换器和保守Q学习方法，使用与基于规则的MRO相同的输入特征。

Result: 在3500 MHz载频的NR网络中，离线强化学习方法比基于规则的MRO性能提升达7%。

Conclusion: 离线强化学习方法性能更优，且支持同一数据集上的多种目标函数训练，灵活性更高。

Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm
and study the possibility of learning the optimal Cell Individual Offset tuning
using offline Reinforcement Learning. Such methods make use of collected
offline datasets to learn the optimal policy, without further exploration. We
adapt and apply a sequence-based method called Decision Transformers as well as
a value-based method called Conservative Q-Learning to learn the optimal policy
for the same target reward as the vanilla rule-based MRO. The same input
features related to failures, ping-pongs, and other handover issues are used.
Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on
a traffic mix including diverse user service types and a specific tunable
cell-pair shows that offline-RL methods outperform rule-based MRO, offering up
to 7% improvement. Furthermore, offline-RL can be trained for diverse objective
functions using the same available dataset, thus offering operational
flexibility compared to rule-based methods.

</details>


### [42] [Reliable Image Transmission in CPS-based Pub/Sub](https://arxiv.org/abs/2506.22875)
*Everson Flores,Bruna Guterres,Thomaz Pereira Junior,Paula Barros,Alberto Cabral,Cristiana Lima Dora,Marcelo Malheiros,Marcelo Pias*

Main category: cs.NI

TL;DR: 论文研究了MQTT协议在高流量和间歇性连接场景下的实时图像传输可靠性，实验表明其在工业应用中具有高效和弹性数据处理能力。


<details>
  <summary>Details</summary>
Motivation: MQTT协议在物联网和工业系统中广泛应用，但在高流量和网络中断场景下的性能研究存在空白，限制了其关键应用。

Method: 通过控制实验验证MQTT基于发布/订阅模型的分布式系统在网络中断和高流量下的性能。

Result: MQTT系统在正常情况下可靠传输，恢复能力取决于故障点；适应性强，防止重复错误。

Conclusion: MQTT系统适合需要高效和弹性数据处理的工业应用。

Abstract: Developments in communication and automation have driven the expansion of
distributed networks, essential for IoT and CPS development in industrial
applications requiring reliable image processing and real-time adaptability.
Although broadly adopted, there is a literature gap regarding the performance
of MQTT protocol for image sharing and transmission under high-traffic
scenarios with intermittent connectivity, restricting its use in critical IoT
and CPS applications. In this context, the present work examines the
reliability of real-time image transmission in IoT and CPS industrial systems
that utilize the MQTT-based publish/subscribe communication model. It focuses
on scenarios with network interruptions and high data traffic, evaluating the
performance of a distributed system through a series of controlled testbed
validation experiments. Experimental validation demonstrated that while the
MQTT-based system sustains reliable transmission under normal conditions, its
recovery capability depends on the failure point, with complete restoration
occurring when disruptions affect the Orchestrator Node and partial recovery
when the Producer Node or Broker are affected. The study also confirmed that
the system prevents duplicate errors and adapts well to increasing network
demands, reinforcing its suitability for industrial applications that require
efficient and resilient data handling.

</details>


### [43] [Model-Based Diagnosis: Automating End-to-End Diagnosis of Network Failures](https://arxiv.org/abs/2506.23083)
*Changrong Wu,Yiyao Yu,Myungjin Lee,Jayanth Srinivasa,Ennan Zhai,George Varghese,Yuval Tamir*

Main category: cs.NI

TL;DR: 提出了一种基于模型的网络诊断新范式NetDx，通过自动化程序快速定位网络故障根源，显著提升诊断效率。


<details>
  <summary>Details</summary>
Motivation: 企业网络故障诊断与修复对业务影响重大，现有方法仅针对部分问题（如数据平面或控制平面故障），缺乏系统性解决方案。

Method: 基于数据包转发和路由模型，系统化推导自动化诊断程序，覆盖数据平面和分布式控制平面的硬件、固件及软件故障。

Result: NetDx在仿真网络中实现了100%的故障诊断准确率，并在实际云提供商案例中显著缩短诊断时间。

Conclusion: 模型化诊断方法高效且鲁棒，能够替代人工操作，显著提升网络故障诊断速度。

Abstract: Fast diagnosis and repair of enterprise network failures is critically
important since disruptions cause major business impacts. Prior works focused
on diagnosis primitives or procedures limited to a subset of the problem, such
as only data plane or only control plane faults. This paper proposes a new
paradigm, model-based network diagnosis, that provides a systematic way to
derive automated procedures for identifying the root cause of network failures,
based on reports of end-to-end user-level symptoms. The diagnosis procedures
are systematically derived from a model of packet forwarding and routing,
covering hardware, firmware, and software faults in both the data plane and
distributed control plane. These automated procedures replace and dramatically
accelerate diagnosis by an experienced human operator. Model-based diagnosis is
inspired by, leverages, and is complementary to recent work on network
verification. We have built NetDx, a proof-of-concept implementation of
model-based network diagnosis. We deployed NetDx on a new emulator of networks
consisting of P4 switches with distributed routing software. We validated the
robustness and coverage of NetDx with an automated fault injection campaign, in
which 100% of faults were diagnosed correctly. Furthermore, on a data set of 33
faults from a large cloud provider that are within the domain targeted by
NetDx, 30 are efficiently diagnosed in seconds instead of hours.

</details>


### [44] [Autonomous Vision-Aided UAV Positioning for Obstacle-Aware Wireless Connectivity](https://arxiv.org/abs/2506.23190)
*Kamran Shafafi,Manuel Ricardo,Rui Campos*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unmanned Aerial Vehicles (UAVs) offer a promising solution for enhancing
wireless connectivity and Quality of Service (QoS) in urban environments,
acting as aerial Wi-Fi access points or cellular base stations. Their
flexibility and rapid deployment capabilities make them suitable for addressing
infrastructure gaps and traffic surges. However, optimizing UAV positions to
maintain Line of Sight (LoS) links with ground User Equipment (UEs) remains
challenging in obstacle-dense urban scenarios. This paper proposes VTOPA, a
Vision-Aided Traffic- and Obstacle-Aware Positioning Algorithm that
autonomously extracts environmental information -- such as obstacles and UE
locations -- via computer vision and optimizes UAV positioning accordingly. The
algorithm prioritizes LoS connectivity and dynamically adapts to user traffic
demands in real time. Evaluated through simulations in ns-3, VTOPA achieves up
to a 50% increase in aggregate throughput and a 50% reduction in delay, without
compromising fairness, outperforming benchmark approaches in obstacle-rich
environments.

</details>


### [45] [On the Resilience of Underwater Semantic Wireless Communications](https://arxiv.org/abs/2506.23350)
*João Pedro Loureiro,Patrícia Delgado,Tomás Feliciano Ribeiro,Filipe B. Teixeira,Rui Campos*

Main category: cs.NI

TL;DR: 水下无线通信因传播限制面临挑战。语义通信通过传输语义特征而非原始数据，减少传输量。本文评估了SAGE框架的鲁棒性，其在声学链路上压缩和传输图像数据为文本描述，并在模拟器中测试。结果显示，SAGE能在不同错误条件下重建有意义的内容。


<details>
  <summary>Details</summary>
Motivation: 解决水下无线通信中传统技术的限制（如低带宽、高错误率、多路径干扰），通过语义通信减少数据量。

Method: 提出SAGE框架，结合语义处理和生成式人工智能（GenAI），将图像数据压缩为文本描述在声学链路上传输，并使用定制模拟器测试其鲁棒性。

Result: SAGE在不同错误条件下能成功重建有意义的图像内容，展现出高效的通信潜力。

Conclusion: SAGE框架在恶劣水下环境中表现鲁棒，有望成为高效水下无线通信的解决方案。

Abstract: Underwater wireless communications face significant challenges due to
propagation constraints, limiting the effectiveness of traditional radio and
optical technologies. Long-range acoustic communications support distances up
to a few kilometers, but suffer from low bandwidth, high error ratios, and
multipath interference. Semantic communications, which focus on transmitting
extracted semantic features rather than raw data, present a promising solution
by significantly reducing the volume of data transmitted over the wireless
link.
  This paper evaluates the resilience of SAGE, a semantic-oriented
communications framework that combines semantic processing with Generative
Artificial Intelligence (GenAI) to compress and transmit image data as textual
descriptions over acoustic links. To assess robustness, we use a
custom-tailored simulator that introduces character errors observed in
underwater acoustic channels. Evaluation results show that SAGE can
successfully reconstruct meaningful image content even under varying error
conditions, highlighting its potential for robust and efficient underwater
wireless communication in harsh environments.

</details>


### [46] [Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent Metasurfaces](https://arxiv.org/abs/2506.23488)
*Geng Sun,Mingzhe Fan,Lei Zhang,Hongyang Pan,Jiahui Li,Chuang Zhang,Linyao Li,Changyuan Zhao,Chau Yuen*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Wireless communication systems face significant challenges in meeting the
increasing demands for higher data rates and more reliable connectivity in
complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a
promising technology for realizing wave-domain signal processing, with mobile
SIMs offering superior communication performance compared to their fixed
counterparts. In this paper, we investigate a novel unmanned aerial vehicle
(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the
low-altitude economy (LAE) networks paradigm, where UAVs function as both base
stations that cache SIM-processed data and mobile platforms that flexibly
deploy SIMs to enhance uplink communications from ground users. To maximize
network capacity, we formulate a UAV-SIM-based joint optimization problem
(USBJOP) that comprehensively addresses three critical aspects: the association
between UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and
the phase shifts across multiple SIM layers. Due to the inherent non-convexity
and NP-hardness of USBJOP, we decompose it into three sub-optimization
problems, \textit{i.e.}, association between UAV-SIMs and users optimization
problem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase
shifts optimization problem (USPSOP), and solve them using an alternating
optimization strategy. Specifically, we transform AUUOP and ULOP into convex
forms solvable by the CVX tool, while addressing USPSOP through a generative
artificial intelligence (GAI)-based hybrid optimization algorithm. Simulations
demonstrate that our proposed approach significantly outperforms benchmark
schemes, achieving approximately 1.5 times higher network capacity compared to
suboptimal alternatives. Additionally, our proposed GAI method reduces the
algorithm runtime by 10\% while maintaining solution quality.

</details>


### [47] [Securing the Sky: Integrated Satellite-UAV Physical Layer Security for Low-Altitude Wireless Networks](https://arxiv.org/abs/2506.23493)
*Jiahui Li,Geng Sun,Xiaoyu Sun,Fang Mei,Jingjing Wang,Xiangwang Hou,Daxin Tian,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 本文提出了一种基于协作波束成形的物理层安全方案，用于低空无线网络（LAWNs），通过卫星与无人机（UAV）的协同工作提升安全性，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在6G网络中，低空无线网络（LAWNs）的广泛应用引发了传输安全问题，尤其是在低空环境中更高的视距概率增加了安全风险。

Method: 提出了一种协作波束成形技术，结合卫星和无人机的优势，设计物理层安全方案，并通过两个案例研究（安全中继系统和双向空中安全通信框架）进行验证。

Result: 仿真结果表明，该物理层安全方案能有效提升低空无线通信的安全性，并具有实际应用潜力。

Conclusion: 该研究为LAWNs的安全性提供了有效的解决方案，同时指出了当前挑战和未来研究方向。

Abstract: Low-altitude wireless networks (LAWNs) have garnered significant attention in
the forthcoming 6G networks. In LAWNs, satellites with wide coverage and
unmanned aerial vehicles (UAVs) with flexible mobility can complement each
other to form integrated satellite-UAV networks, providing ubiquitous and
high-speed connectivity for low-altitude operations. However, the higher
line-of-sight probability in low-altitude airspace increases transmission
security concerns. In this work, we present a collaborative beamforming-based
physical layer security scheme for LAWNs. We introduce the fundamental aspects
of integrated satellite-UAV networks, physical layer security, UAV swarms, and
collaborative beamforming for LAWN applications. Following this, we highlight
several opportunities for collaborative UAV swarm secure applications enabled
by satellite networks, including achieving physical layer security in scenarios
involving data dissemination, data relay, eavesdropper collusion, and imperfect
eavesdropper information. Next, we detail two case studies: a secure relay
system and a two-way aerial secure communication framework specifically
designed for LAWN environments. Simulation results demonstrate that these
physical layer security schemes are effective and beneficial for secure
low-altitude wireless communications. A short practicality analysis shows that
the proposed method is applicable to LAWN scenarios. Finally, we discuss
current challenges and future research directions for enhancing security in
LAWNs.

</details>


### [48] [The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking](https://arxiv.org/abs/2506.23628)
*Antonio Ojea*

Main category: cs.NI

TL;DR: Kubernetes Network Drivers (KNDs) 是一种模块化、声明式架构，旨在解决传统 Kubernetes 网络在处理 AI/ML 和 Telco 基础设施需求时的不足，通过集成 DRA、NRI 和 OCI Runtime 改进，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统 Kubernetes 网络在应对 AI/ML 和高性能 Telco 基础设施需求时的局限性。

Method: 引入 KNDs 架构，利用 DRA、NRI 改进和 OCI Runtime 规范变更，实现声明式网络资源管理。

Result: DraNet 实现展示了网络接口（如 RDMA 设备）的声明式附加，显著提升 AI/ML 工作负载性能。

Conclusion: KNDs 为云原生应用和未来 Telco 解决方案奠定基础，支持专用 KNDs 发展以简化运维。

Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of
AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes
Network Drivers (KNDs), a transformative, modular, and declarative architecture
designed to overcome current imperative provisioning and API limitations. KNDs
integrate network resource management into Kubernetes' core by utilizing
Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,
and upcoming OCI Runtime Specification changes. Our DraNet implementation
demonstrates declarative attachment of network interfaces, including Remote
Direct Memory Access (RDMA) devices, significantly boosting high-performance
AI/ML workloads. This capability enables sophisticated cloud-native
applications and lays crucial groundwork for future Telco solutions, fostering
a "galaxy" of specialized KNDs for enhanced application delivery and reduced
operational complexity.

</details>


### [49] [Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies](https://arxiv.org/abs/2506.23640)
*Ximeng Liu,Shizhen Zhao,Xinbing Wang*

Main category: cs.NI

TL;DR: Geminet是一种轻量级且可扩展的基于机器学习的流量工程框架，通过解耦神经网络与拓扑结构，以及优化边缘级双变量，显著提升了可扩展性和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的流量工程方案无法有效处理拓扑变化或存在计算和内存开销过大的问题，因此需要一种更实用的解决方案。

Method: Geminet采用迭代梯度下降调整过程来解耦神经网络与拓扑结构，并将优化从路径级路由权重转移到边缘级双变量，以减少内存消耗。

Result: Geminet的神经网络大小仅占现有方案的0.04%至7%，且在大规模拓扑下内存消耗低于10 GiB，收敛速度比现有方案快5.45倍。

Conclusion: Geminet展示出在大规模部署中的潜力，能够有效处理拓扑变化，同时显著降低资源消耗。

Abstract: Recently, researchers have explored ML-based Traffic Engineering (TE),
leveraging neural networks to solve TE problems traditionally addressed by
optimization. However, existing ML-based TE schemes remain impractical: they
either fail to handle topology changes or suffer from poor scalability due to
excessive computational and memory overhead. To overcome these limitations, we
propose Geminet, a lightweight and scalable ML-based TE framework that can
handle changing topologies. Geminet is built upon two key insights: (i) a
methodology that decouples neural networks from topology by learning an
iterative gradient-descent-based adjustment process, as the update rule of
gradient descent is topology-agnostic, relying only on a few gradient-related
quantities; (ii) shifting optimization from path-level routing weights to
edge-level dual variables, reducing memory consumption by leveraging the fact
that edges are far fewer than paths. Evaluations on WAN and data center
datasets show that Geminet significantly improves scalability. Its neural
network size is only 0.04% to 7% of existing schemes, while handling topology
variations as effectively as HARP, a state-of-the-art ML-based TE approach,
without performance degradation. When trained on large-scale topologies,
Geminet consumes under 10 GiB of memory, more than eight times less than the
80-plus GiB required by HARP, while achieving 5.45 times faster convergence
speed, demonstrating its potential for large-scale deployment.

</details>


### [50] [Campus5G: A Campus Scale Private 5G Open RAN Testbed](https://arxiv.org/abs/2506.23740)
*Andrew E. Ferguson,Ujjwal Pawar,Tianxin Wang,Mahesh K. Marina*

Main category: cs.NI

TL;DR: 论文介绍了Campus5G，这是首个校园范围内符合O-RAN标准的私有5G测试平台，详细描述了从规划到部署和性能测量的全过程，并总结了经验教训和研究机会。


<details>
  <summary>Details</summary>
Motivation: 移动网络正在经历解耦趋势，Open RAN成为行业方向，而私有5G网络因其环境、高度控制性和创新潜力，被视为Open RAN的早期采用者。

Method: 通过在爱丁堡大学中心校区部署Campus5G测试平台，详细记录了从规划、架构设计到部署和性能测试的全过程。

Result: 成功部署了首个校园范围内的O-RAN兼容私有5G测试平台，并对其性能进行了测量。

Conclusion: 总结了部署过程中的经验教训，并指出了一些从部署经验中涌现的研究机会。

Abstract: Mobile networks are embracing disaggregation, reflected by the industry trend
towards Open RAN. Private 5G networks are viewed as particularly suitable
contenders as early adopters of Open RAN, owing to their setting, high degree
of control, and opportunity for innovation they present. Motivated by this, we
have recently deployed Campus5G, the first of its kind campus-wide,
O-RAN-compliant private 5G testbed across the central campus of the University
of Edinburgh. We present in detail our process developing the testbed, from
planning, to architecting, to deployment, and measuring the testbed
performance. We then discuss the lessons learned from building the testbed, and
highlight some research opportunities that emerged from our deployment
experience.

</details>


### [51] [How Long Can I Transmit? A Mobility Aware mmWave-based UAV Communication Framework](https://arxiv.org/abs/2506.23755)
*Shawon Mitra,Subhojit Sarkar,Sasthi C. Ghosh*

Main category: cs.NI

TL;DR: 本文提出了一种分析移动地面用户与静态无人机之间的LoS预期持续时间的框架，并通过模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信中移动用户的LoS概率缺乏分析，无人机可以解决其传输衰减问题。

Method: 采用MPLP模型模拟城市环境，推导移动用户与无人机间LoS的预期持续时间。

Result: 提出的贪心算法优于现有的基于最近LoS距离的分配方案。

Conclusion: 考虑用户移动性的LoS分析方法对无人机通信效能提升有重要意义。

Abstract: One primary focus of next generation wireless communication networks is the
millimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz
frequency range. Despite their promise of high data rates, mmWaves suffer from
severe attenuation while passing through obstacles. Unmanned aerial vehicles
(UAVs) have been proposed to offset this limitation on account of their
additional degrees of freedom, which can be leveraged to provide line of sight
(LoS) transmission paths. While some prior works have proposed analytical
frameworks to compute the LoS probability for static ground users and a UAV,
the same is lacking for mobile users on the ground. In this paper, we consider
the popular Manhattan point line process (MPLP) to model an urban environment,
within which a ground user moves with a known velocity for a small time
interval along the roads. We derive an expression for the expected duration of
LoS between a static UAV in the air and a mobile ground user, and validate the
same through simulations. To demonstrate the efficacy of the proposed analysis,
we propose a simple user association algorithm that greedily assigns the UAVs
to users with the highest expected LoS time, and show that it outperforms the
existing benchmark schemes that assign the users to the nearest UAVs with LoS
without considering the user mobility.

</details>


### [52] [Learning Constraints Directly from Network Data](https://arxiv.org/abs/2506.23964)
*Hongyu Hè,Minhao Jin,Maria Apostolaki*

Main category: cs.NI

TL;DR: NetNomos 是一种从原始网络测量数据中学习命题逻辑约束的方法，用于改进合成数据、增强机器学习模型的鲁棒性以及提升网络测量的语义理解。通过基于格结构的搜索，NetNomos 降低了学习复杂度并能高效提取规则。


<details>
  <summary>Details</summary>
Motivation: 网络数据遵循多种规则，但这些规则的提取通常依赖手动或机器学习方法，导致规则不完整或不准确。NetNomos 旨在自动化提取规则，以提升数据质量和模型性能。

Method: NetNomos 将规则提取建模为约束问题，并利用基于约束特异性和简洁性的格结构搜索方法，将学习复杂度从超二次降低到对数级别。

Result: NetNomos 在多样化的网络数据集中能学习所有基准规则（包括仅占 0.01% 数据点的规则），并在三小时内完成。基线方法仅发现不到 25% 的规则且需数天运行。

Conclusion: NetNomos 可用于评估合成流量生成器、检测异常以及支持监测任务，展示了其在网络数据分析中的广泛实用性。

Abstract: Network data conforms to a wide range of rules that arise from protocols,
design principles, and deployment decisions (e.g., a packet's queuing delay
must be less than its end-to-end delay). Formalizing such rules as logic
constraints can (i) improve the quality of synthetic data, (ii) reduce the
brittleness of machine learning (ML) models, and (iii) improve semantic
understanding of network measurements. However, these benefits remain out of
reach if rule extraction is manual or solely reliant on ML, as both approaches
yield incomplete, unreliable, and/or inaccurate rules.
  This paper formulates rule extraction as a constraint modeling problem and
introduces NetNomos that learns propositional logic constraints directly from
raw network measurements. Constraint modeling in this domain is uniquely
challenging due to the scale of the data, the inherent learning complexity and
passive environment, and the lack of ground truth supervision. NetNomos
addresses these challenges via a lattice-based search structured by constraint
specificity and succinctness. Our approach reduces learning complexity from
superquadratic to logarithmic and enables efficient traversal in combinatorial
search space.
  Our evaluations on diverse network datasets show that NetNomos learns all
benchmark rules, including those associated with as little as 0.01% of data
points, in under three hours. In contrast, baseline methods discover less than
25% of the rules and require several days to run. Through three case studies,
we show that: NetNomos (i) finds rule violations in the outputs of all seven
synthetic traffic generators, hence can be used to assess and guide their
generation process; (ii) detects semantic differences in traffic, hence can be
used for anomaly detection; and (iii) automatically finds rules used for
telemetry imputation, hence can support monitoring through inference.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [53] [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/abs/2506.23484)
*Yuzhuo Chen,Zehua Ma,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.MM

TL;DR: 本文提出了一种名为TAG-WM的防篡改生成图像水印方法，解决了AIGC中的版权和真实性问题，提高了篡改鲁棒性和定位能力。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（AIGC）在高效创作的同时带来版权和真实性风险，当前无损视觉质量水印的篡改鲁棒性和被动检测方法不足，需要更强大的解决方案。

Method: 提出TAG-WM方法，包括双标记联合采样（DMJS）、水印潜在重构（WLR）、密集变区检测器（DVRD）和篡改感知解码（TAD）四大模块。

Result: 实验显示TAG-WM在保持无损生成质量和256位容量的同时，实现了最佳的篡改鲁棒性和定位能力。

Conclusion: TAG-WM为AIGC中的版权保护和真实性验证提供了高效解决方案，具有实际应用潜力。

Abstract: AI-generated content (AIGC) enables efficient visual creation but raises
copyright and authenticity risks. As a common technique for integrity
verification and source tracing, digital image watermarking is regarded as a
potential solution to above issues. Among these, watermarking methods capable
of preserving the generation quality are receiving increased attention.
However, the proliferation and high performance of generative image editing
applications have elevated the risks of malicious tampering, creating new
demands. 1) The tamper robustness of current lossless visual quality watermarks
remains constrained by the modification-sensitive diffusion inversion process,
necessitating enhanced robustness. 2) The improved tampering quality and rapid
iteration cycles render passive tampering detection methods inadequate, making
proactive tampering localization capability a desired feature for watermarks.
To address these requirements, this paper proposes a Tamper-Aware Generative
image WaterMarking method named TAG-WM. The proposed method comprises four key
modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright
and localization watermarks into the latent space while preserving generative
quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a
dense variation region detector (DVRD) leveraging diffusion inversion
sensitivity to identify tampered areas via statistical deviation analysis, and
the tamper-aware decoding (TAD) guided by localization results. The
experimental results indicate that TAG-WM achieves SOTA tampering robustness
and tampering localization capability with distortions while maintaining
lossless generation quality and a considerable capacity of 256 bits.

</details>


### [54] [Efficient and Accurate Image Provenance Analysis: A Scalable Pipeline for Large-scale Images](https://arxiv.org/abs/2506.23707)
*Jiewei Lai,Lan Zhang,Chen Tang,Pengcheng Sun*

Main category: cs.MM

TL;DR: 该论文提出了一种可扩展的端到端图像来源分析流水线，通过改进过滤效果和优化相似性计算，实现了高精度和线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有图像来源分析方法面临准确性低和难以扩展的问题，需要一种能够处理大规模修改图像并高效分析的解决方案。

Method: 利用修改关系追踪和局部特征匹配技术，结合压缩伪影捕捉，构建了一个高性能的流水线，并优化了图构建过程。

Result: 该流水线在准确性上提升了16.7-56.1%，并在处理1000万张图像时平均响应时间为3秒，显著优于现有方法。

Conclusion: 提出的流水线在图像来源分析中表现出高精度和强扩展性，适用于大规模实际应用。

Abstract: The rapid proliferation of modified images on social networks that are driven
by widely accessible editing tools demands robust forensic tools for digital
governance. Image provenance analysis, which filters various query image
variants and constructs a directed graph to trace their phylogeny history, has
emerged as a critical solution. However, existing methods face two fundamental
limitations: First, accuracy issues arise from overlooking heavily modified
images due to low similarity while failing to exclude unrelated images and
determine modification directions under diverse modification scenarios. Second,
scalability bottlenecks stem from pairwise image analysis incurs quadratic
complexity, hindering application in large-scale scenarios. This paper presents
a scalable end-to-end pipeline for image provenance analysis that achieves high
precision with linear complexity. This improves filtering effectiveness through
modification relationship tracing, which enables the comprehensive discovery of
image variants regardless of their visual similarity to the query. In addition,
the proposed pipeline integrates local features matching and compression
artifact capturing, enhancing robustness against diverse modifications and
enabling accurate analysis of images' relationships. This allows the generation
of a directed provenance graph that accurately characterizes the image's
phylogeny history. Furthermore, by optimizing similarity calculations and
eliminating redundant pairwise analysis during graph construction, the pipeline
achieves a linear time complexity, ensuring its scalability for large-scale
scenarios. Experiments demonstrate pipeline's superior performance, achieving a
16.7-56.1% accuracy improvement. Notably, it exhibits significant scalability
with an average 3.0-second response time on 10 million scale images, which is
far shorter than the SOTA approach's 12-minute duration.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [55] [On the Reachability Problem for Two-Dimensional Branching VASS](https://arxiv.org/abs/2506.22561)
*Clotilde Bizière,Thibault Hilaire,Jérôme Leroux,Grégoire Sutre*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vectors addition systems with states (VASS), or equivalently Petri nets, are
arguably one of the most studied formalisms for the modeling and analysis of
concurrent systems. A central decision problem for VASS is reachability:
whether there exists a run from an initial configuration to a final one. This
problem has been known to be decidable for over forty years, and its complexity
has recently been precisely characterized. Our work concerns the reachability
problem for BVASS, a branching generalization of VASS. In dimension one, the
exact complexity of this problem is known. In this paper, we prove that the
reachability problem for 2-dimensional BVASS is decidable. In fact, we even
show that the reachability set admits a computable semilinear presentation. The
decidability status of the reachability problem for BVASS remains open in
higher dimensions.

</details>


### [56] [From MBQI to Enumerative Instantiation and Back](https://arxiv.org/abs/2506.22584)
*Marek Dančo,Petra Hozzová,Mikoláš Janota*

Main category: cs.LO

TL;DR: 比较模型基于量化实例化（MBQI）和枚举实例化（EI）在SMT中的关系，提出结合两者的算法并初步实验。


<details>
  <summary>Details</summary>
Motivation: MBQI和EI在SMT中的应用各有优缺点：MBQI有语义优势但可能导致弱实例化，EI在语法层面追求完备性但可能误判。研究两者关系以提升实例化效果。

Method: 分析MBQI和EI的关系，提出结合两种技术的新算法，并进行初步实验验证。

Result: 初步实验表明结合MBQI和EI的算法效果良好，具体性能和数据未详述。

Conclusion: MBQI和EI的结合有望弥补各自不足，未来研究可进一步优化算法。

Abstract: This work investigates the relation between model-based quantifier
instantiation (MBQI) and enumerative instantiation (EI) in Satisfiability
Modulo Theories (SMT). MBQI operates at the semantic level and guarantees to
find a counterexample to a given a non-model. However, it may lead to weak
instantiations. In contrast, EI strives for completeness by systematically
enumerating terms at the syntactic level. However, such terms may not be
counter-examples. Here we investigate the relation between the two techniques
and report on our initial experiments of the proposed algorithm that combines
the two.

</details>


### [57] [Compositional Control-Driven Boolean Circuits](https://arxiv.org/abs/2506.22687)
*Damian Arellanes*

Main category: cs.LO

TL;DR: 提出了一种基于colimit的布尔电路组合方法，支持模块化和形式化推理，展示了新模型至少与经典模型一样强大。


<details>
  <summary>Details</summary>
Motivation: 布尔电路的组合性长期被忽视或非正式研究，本文旨在填补这一理论空白。

Method: 提出基于colimit的操作符，定义顺序、并行、分支和迭代电路的构造方法。

Result: 展示了新模型（控制驱动的布尔电路）至少与经典模型一样强大。

Conclusion: 通过形式化的组合性方法，为布尔电路提供了模块化和可扩展的理论基础。

Abstract: Boolean circuits abstract away from physical details to focus on the logical
structure and computational behaviour of digital components. Despite they have
been studied for many decades, compositionality has been widely ignored or
examined in an informal manner, which is a property for combining circuits
without delving into their internal structure, while supporting modularity and
formal reasoning. In this paper, we address this longstanding theoretical gap
by proposing colimit-based operators for compositional circuit construction. We
define separate operators for forming sequential, parallel, branchial and
iterative circuits. As composites encapsulate explicit control flow, a new
model of computation emerges which we refer to as (families of) control-driven
Boolean circuits. We show how this model is at least as powerful as its
classical counterpart. In other words, it is able to non-uniformly compute any
Boolean function on inputs of arbitrary length.

</details>


### [58] [Questions as cognitive filters](https://arxiv.org/abs/2506.22735)
*Willem Conradie,Krishna Manoorkar,Alessandra Palmigiano,Apostolos Tzimoulis,Nachoem Wijnberg*

Main category: cs.LO

TL;DR: 该论文提出了一种逻辑-代数框架，用于模拟多智能体环境中通过审议的决策过程，核心概念是询问议程（interrogative agendas），代表智能体对决策相关特征的认知立场。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个框架，以形式化多智能体审议中智能体对决策相关特征的认知立场及其交互。

Method: 方法包括形式化询问议程为等价关系，分析其对应的子格结构，并引入双排序的逻辑-代数结构来建模智能体与议程的交互。

Result: 结果显示该框架能够建模不同“获胜规则”下的相关议程子格，并探讨了框架内可定义与不可定义的交互条件。

Conclusion: 结论表明该框架有效支持了多智能体审议中决策的形式化建模，但某些交互条件无法在该框架内定义。

Abstract: In this paper, we develop a logico-algebraic framework for modeling
decision-making through deliberation in multi-agent settings. The central
concept in this framework is that of interrogative agendas, which represent the
cognitive stances of agents regarding which features should be considered
relevant in the final decision. We formalize an agent's interrogative agenda as
an equivalence relation that identifies outcomes differing only in aspects the
agent deems irrelevant. Moreover, we characterize the sublattices of the
resulting lattice that correspond to relevant interrogative agendas for
deliberation scenarios governed by different ``winning rules." We then
introduce a two-sorted logico-algebraic structure-comprising the lattice of
relevant interrogative agendas and the Boolean algebras of agent coalitions-to
model the interaction between agents and agendas during deliberation. Finally,
we discuss which interaction conditions can and cannot be defined within this
framework.

</details>


### [59] [Model-theoretic Forcing in Transition Algebra](https://arxiv.org/abs/2506.22828)
*Go Hashimoto,Daniel Găină*

Main category: cs.LO

TL;DR: 研究了过渡代数中的Löwenheim-Skolem和Omitting Types定理，发现向上Löwenheim-Skolem定理、紧致性及联合Robinson一致性因过渡的传递闭包表达性而失效，并提出了一种强制技术方法。


<details>
  <summary>Details</summary>
Motivation: 探讨在动态逻辑增强的多类一阶逻辑系统中，Löwenheim-Skolem和Omitting Types定理的适用性及其限制。

Method: 通过扩展经典的Keisler强制技术，使用有向签名图方法，在多类逻辑系统中开发强制技术，并扩展证明系统以适应构造器和有限过渡代数。

Result: 在非紧致的多类逻辑系统中，建立了向下Löwenheim-Skolem和Omitting Types定理，并证明了扩展系统在构造器和有限过渡代数模型中的完备性。

Conclusion: 过渡代数的表达性限制了某些经典定理的适用性，但通过新的强制技术和扩展证明系统，仍能在特定模型下实现完备性。

Abstract: We study L\"owenheim-Skolem and Omitting Types theorems in Transition
Algebra, a logical system obtained by enhancing many sorted first-order logic
with features from dynamic logic. The sentences we consider include
compositions, unions, and transitive closures of transition relations, which
are treated similarly to actions in dynamic logics to define necessity and
possibility operators. We show that Upward L\"owenheim-Skolem theorem, any form
of compactness, and joint Robinson consistency property fail due to the
expressivity of transitive closures of transitions. In this non-compact
many-sorted logical system, we develop a forcing technique method by
generalizing the classical method of forcing used by Keisler to prove Omitting
Types theorem. Instead of working within a single signature, we work with a
directed diagram of signatures, which allows us to establish Downward
L\"owenheim-Skolem and Omitting Types theorems despite the fact that models
interpret sorts as sets, possibly empty. Building on a complete system of proof
rules for Transition Algebra, we extend it with additional proof rules to
reason about constructor-based and/or finite transition algebras. We then
establish the completeness of this extended system for a fragment of Transition
Algebra obtained by restricting models to constructor-based and/or finite
transition algebras.

</details>


### [60] [One-Parametric Presburger Arithmetic has Quantifier Elimination](https://arxiv.org/abs/2506.23730)
*Alessio Mansutti,Mikhail R. Starchak*

Main category: cs.LO

TL;DR: 该论文提出了一种针对一参数Presburger算术的量词消去方法，解决了Bogart等人2017年提出的开放问题。方法结合了多项式和整数除法的扩展结构，并通过高效算法在NP时间内完成。


<details>
  <summary>Details</summary>
Motivation: 解决一参数Presburger算术中的量词消去问题，填补了Bogart等人在2017年提出的研究空白，并验证了Goodrick在2018年的猜想。

Method: 通过迭代消去存在量词块，结合多项式系数处理的基础方法和类似“基t除法”的技术，算法在NP时间内运行。

Result: 证明了该片段的可满足性问题属于NP类，且最小解的位数规模为多项式级。

Conclusion: 该研究为一参数Presburger算术提供了完整的量词消去框架，对非线性整数规划等领域有广泛应用。

Abstract: We give a quantifier elimination procedure for one-parametric Presburger
arithmetic, the extension of Presburger arithmetic with the function $x \mapsto
t \cdot x$, where $t$ is a fixed free variable ranging over the integers. This
resolves an open problem proposed in [Bogart et al., Discrete Analysis, 2017].
As conjectured in [Goodrick, Arch. Math. Logic, 2018], quantifier elimination
is obtained for the extended structure featuring all integer division functions
$x \mapsto \lfloor{\frac{x}{f(t)}}\rfloor$, one for each integer polynomial
$f$.
  Our algorithm works by iteratively eliminating blocks of existential
quantifiers. The elimination of a block builds on two sub-procedures, both
running in non-deterministic polynomial time. The first one is an adaptation of
a recently developed and efficient quantifier elimination procedure for
Presburger arithmetic, modified to handle formulae with coefficients over the
ring $\mathbb{Z}[t]$ of univariate polynomials. The second is reminiscent of
the so-called "base $t$ division method" used by Bogart et al. As a result, we
deduce that the satisfiability problem for the existential fragment of
one-parametric Presburger arithmetic (which encompasses a broad class of
non-linear integer programs) is in NP, and that the smallest solution to a
satisfiable formula in this fragment is of polynomial bit size.

</details>


### [61] [Querying Attack-Fault-Defense Trees: Property Specification in Smart Grid and Aerospace Case Studies](https://arxiv.org/abs/2506.23789)
*Reza Soltani,Stefano M. Nicoletti,Milan Lopuhaä-Zwakenberg,Mariëlle Stoelinga*

Main category: cs.LO

TL;DR: AFDL是一个基于逻辑的框架，用于分析攻击-故障-防御树中的安全、防御和交互问题，并提出了一种特定领域的查询语言LangAFDL。


<details>
  <summary>Details</summary>
Motivation: 为了在单一框架中整合安全、防御和防御措施的交互，并支持领域专家通过直观模板表达复杂分析目标。

Method: 提出了AFDL逻辑框架和LangAFDL查询语言，支持布尔和量化查询，以及最小割集分析。

Result: 通过两个实际案例展示了AFDL和LangAFDL的表达能力和实用性。

Conclusion: AFDL为关键任务系统的自动化安全-防御分析奠定了基础，为未来工具开发和设计工作流集成提供了方向。

Abstract: This paper introduces AFDL, a logic-based framework for reasoning about
safety, security, and defense interactions in Attack-Fault-Defense Trees, which
is a model that captures all safety, security, and defense domains in a single
framework. We showcase both AFDL and propose a structured domain specific query
language, LangAFDL, which enables domain experts to express complex analysis
goals through intuitive templates. LangAFDL supports both Boolean and
quantified queries as well as minimal cut set analysis, capturing the interplay
between safety, security, and defensive measures. We illustrate the
expressiveness and utility of the approach through representative queries over
two different real-world case studies: Gridshield and Ground Segment as a
Service. The formalization lays the automated safety-security groundwork for
analyses in mission-critical systems and paves the way for future tool
development and integration into design workflows.

</details>


### [62] [Protocol insecurity with finitely many sessions and XOR](https://arxiv.org/abs/2506.24072)
*R Ramanujam,Vaishnavi Sundararajan,S P Suresh*

Main category: cs.LO

TL;DR: 本文提出了一个关于XOR不安全问题的新证明，利用类型化术语和良好类型化证明的概念，扩展了[CKRT05]证明的适用范围。


<details>
  <summary>Details</summary>
Motivation: 旨在通过类型化术语和良好类型化证明，提供一个更通用的安全性证明，克服了[CKRT05]中对协议类的限制。

Method: 引入了一种新的协议定义，确保诚实代理的发送行为可以从同一会话中的先前接收行为中推导出来。

Result: 成功扩展了[CKRT05]证明的适用性，使其适用于更广泛的协议类。

Conclusion: 新方法不仅验证了XOR的不安全性，还提供了一种更自然的协议定义，为后续研究奠定了基础。

Abstract: We present a different proof of the insecurity problem for XOR, solved in by
Chevalier, Kuesters, Rusinowitch and Turuani (2005). Our proof uses the notion
of typed terms and well-typed proofs, and removes a restriction on the class of
protocols to which the [CKRT05] proof applies, by introducing a slightly
different (but very natural) notion of protocols, where honest agent sends are
derivable from previous receives in the same session.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [63] [Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics](https://arxiv.org/abs/2506.22520)
*Mustafa Demir,Jacob Miratsky,Jonathan Nguyen,Chun Kit Chan,Punya Mishra,Abhishek Singharoy*

Main category: cs.HC

TL;DR: 研究探讨AI导师作为队友如何通过激发好奇心提升学生参与度和学习效果，在分子动力学任务中表现出积极影响。


<details>
  <summary>Details</summary>
Motivation: 探索AI作为教育工具在激发学生好奇心和提升学习效果方面的潜力。

Method: 采用混合方法设计，通过Wizard-of-Oz范式动态调整AI行为，分析学生问题复杂度和团队表现。

Result: 高绩效团队表现出更强的任务完成度和认知投入，AI触发好奇心与学生高级问题相关。

Conclusion: AI兼具队友和教育者角色，能通过适应性反馈持续激发好奇心和学习参与度。

Abstract: This study examines the impact of an Artificial Intelligence tutor teammate
(AI) on student curiosity-driven engagement and learning effectiveness during
Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics
platform. It explores the role of the AI's curiosity-triggering and response
behaviors in stimulating and sustaining student curiosity, affecting the
frequency and complexity of student-initiated questions. The study further
assesses how AI interventions shape student engagement, foster discovery
curiosity, and enhance team performance within the IMD learning environment.
Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI
tutor teammate's behavior through a large language model. By employing a
mixed-methods exploratory design, a total of 11 high school students
participated in four IMD tasks that involved molecular visualization and
calculations, which increased in complexity over a 60-minute period. Team
performance was evaluated through real-time observation and recordings, whereas
team communication was measured by question complexity and AI's
curiosity-triggering and response behaviors. Cross Recurrence Quantification
Analysis (CRQA) metrics reflected structural alignment in coordination and were
linked to communication behaviors. High-performing teams exhibited superior
task completion, deeper understanding, and increased engagement. Advanced
questions were associated with AI curiosity-triggering, indicating heightened
engagement and cognitive complexity. CRQA metrics highlighted dynamic
synchronization in student-AI interactions, emphasizing structured yet adaptive
engagement to promote curiosity. These proof-of-concept findings suggest that
the AI's dual role as a teammate and educator indicates its capacity to provide
adaptive feedback, sustaining engagement and epistemic curiosity.

</details>


### [64] [Supra-threshold control of peripheral LOD](https://arxiv.org/abs/2506.22583)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: 论文探讨了超出感知阈值的详细程度（LOD）控制应不同于阈值控制，并提出任务依赖的可靠感知性是关键。


<details>
  <summary>Details</summary>
Motivation: 当前LOD控制基于阈值感知，但大多数操作超出阈值且感知方式不同，需要重新审视。

Method: 通过两个实验研究视觉外周区的超阈值LOD控制。

Result: LOD需支持任务依赖的感知性，细节对比度比大小更能预测感知性，且需根据偏心率和对比度调整。

Conclusion: 结果与基于阈值的LOD控制方案相反，建议重新评估LOD控制方法。

Abstract: Level of detail (LOD) is widely used to control visual feedback in
interactive applications. LOD control is typically based on perception at
threshold - the conditions in which a stimulus first becomes perceivable. Yet
most LOD manipulations are quite perceivable and occur well above threshold.
Moreover, research shows that supra-threshold perception differs drastically
from perception at threshold. In that case, should supra-threshold LOD control
also differ from LOD control at threshold?
  In two experiments, we examine supra-threshold LOD control in the visual
periphery and find that indeed, it should differ drastically from LOD control
at threshold. Specifically, we find that LOD must support a task-dependent
level of reliable perceptibility. Above that level, perceptibility of LOD
control manipulations should be minimized, and detail contrast is a better
predictor of perceptibility than detail size. Below that level, perceptibility
must be maximized, and LOD should be improved as eccentricity rises or contrast
drops. This directly contradicts prevailing threshold-based LOD control
schemes, and strongly suggests a reexamination of LOD control for foveal
display.

</details>


### [65] [A tangible user interface for assessing cognitive mapping ability](https://arxiv.org/abs/2506.22597)
*Ehud Sharlin,Benjamin Watson,Steve Sutphen,Lili Liu,Robert Lederer,John Frazer*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Wayfinding, the ability to recall the environment and navigate through it, is
an essential cognitive skill relied upon almost every day in a person's life. A
crucial component of wayfinding is the construction of cognitive maps, mental
representations of the environments through which a person travels. Age,
disease or injury can severely affect cognitive mapping, making assessment of
this basic survival skill particularly important to clinicians and therapists.
Cognitive mapping has also been the focus of decades of basic research by
cognitive psychologists. Both communities have evolved a number of techniques
for assessing cognitive mapping ability. We present the Cognitive Map Probe
(CMP), a new computerized tool for assessment of cognitive mapping ability that
increases consistency and promises improvements in flexibility, accessibility,
sensitivity and control. The CMP uses a tangible user interface that affords
spatial manipulation. We describe the design of the CMP, and find that it is
sensitive to factors known to affect cognitive mapping performance in extensive
experimental testing.

</details>


### [66] [Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China](https://arxiv.org/abs/2506.22674)
*Weiyin Xie,Chunxi Huang,Jiyao Wang,Dengbo He*

Main category: cs.HC

TL;DR: 研究探讨电动车（EV）比燃油车（FV）更易引发晕车（MS），通过调查发现EV晕车症状更严重，但频率较低，影响因素包括个体差异、车内活动和路况。


<details>
  <summary>Details</summary>
Motivation: 虽然电动车因其环保和低成本受到青睐，但用户普遍反映其更易引发晕车，但此前未量化研究EV与FV在晕车方面的差异。

Method: 通过问卷调查收集了中国大陆639名乘客过去一年在EV和FV中的晕车经历，分析其频率和严重程度及相关因素。

Result: FV晕车频率更高，但EV晕车症状更严重；晕车严重程度与个体差异、车内活动和路况相关，频率则与车辆拥有和乘坐频率有关。

Conclusion: 研究结果可为未来减少EV晕车的优化提供指导，并推动进一步实证研究。

Abstract: Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs),
given some unique characteristics of EVs, for example, the low air pollution
and maintenance cost. However, the increasing prevalence of EVs is accompanied
by widespread complaints regarding the high likelihood of motion sickness (MS)
induction, especially when compared to FVs, which has become one of the major
obstacles to the acceptance and popularity of EVs. Despite the prevalence of
such complaints online and among EV users, the association between vehicle type
(i.e., EV versus FV) and MS prevalence and severity has not been quantified.
Thus, this study aims to investigate the existence of EV-induced MS and explore
the potential factors leading to it. A survey study was conducted to collect
passengers' MS experience in EVs and FVs in the past one year. In total, 639
valid responses were collected from mainland China. The results show that FVs
were associated with a higher frequency of MS, while EVs were found to induce
more severe MS symptoms. Further, we found that passengers' MS severity was
associated with individual differences (i.e., age, gender, sleep habits,
susceptibility to motion-induced MS), in-vehicle activities (i.e., chatting
with others and watching in-vehicle displays), and road conditions (i.e.,
congestion and slope), while the MS frequency was associated with the vehicle
ownership and riding frequency. The results from this study can guide the
directions of future empirical studies that aim to quantify the inducers of MS
in EVs and FVs, as well as the optimization of EVs to reduce MS.

</details>


### [67] [Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions](https://arxiv.org/abs/2506.22926)
*Qixuan Liu,Shi Qiu,Yinqiao Wang,Xiwen Wu,Kenneth Siu Ho Chok,Chi-Wing Fu,Pheng-Ann Heng*

Main category: cs.HC

TL;DR: 本文介绍了一种基于XR的系统，用于医学数据可视化，结合多层多平面重建、3D网格模型及多模态交互，证明了其提升空间理解和减少认知负担的能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学数据可视化对非专业人士的挑战，提升空间理解和交互效率。

Method: 开发XR系统，整合多层多平面重建与3D网格模型的多模态交互框架，结合手势和基于LLM的语音命令。

Result: 用户研究和专家访谈显示系统显著提升任务完成时间、可用性和交互效果。

Conclusion: 该系统具有推动医学培训和临床实践的潜力，但仍需进一步优化。

Abstract: Volumetric medical imaging technologies produce detailed 3D representations
of anatomical structures. However, effective medical data visualization and
exploration pose significant challenges, especially for individuals with
limited medical expertise. We introduce a novel XR-based system with two key
innovations: (1) a coordinated visualization module integrating Multi-layered
Multi-planar Reconstruction with 3D mesh models and (2) a multimodal
interaction framework combining hand gestures with LLM-enabled voice commands.
We conduct preliminary evaluations, including a 15-participant user study and
expert interviews, to demonstrate the system's abilities to enhance spatial
understanding and reduce cognitive load. Experimental results show notable
improvements in task completion times, usability metrics, and interaction
effectiveness enhanced by LLM-driven voice control. While identifying areas for
future refinement, our findings highlight the potential of this immersive
visualization system to advance medical training and clinical practice. Our
demo application and supplemental materials are available for download at:
https://osf.io/bpjq5/.

</details>


### [68] [Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India](https://arxiv.org/abs/2506.22741)
*Akshay Nayak Kolgar,Yash Prakash,Sampath Jayarathna,Hae-Na Lee,Vikas Ashok*

Main category: cs.HC

TL;DR: 数字化就业环境的变化为视力障碍者（BVI）提供了新机遇，但许多印度BVI求职者仍面临失业问题，主要因行业需求未满足且缺乏建设性反馈。研究建议设计个性化干预系统以改善就业结果。


<details>
  <summary>Details</summary>
Motivation: 探讨印度BVI群体在数字产业中就业率低的原因，尽管他们接受了数字化培训和干预措施。

Method: 对20位BVI求职者进行半结构化访谈。

Result: 研究发现BVI求职者在满足行业需求方面存在困难，且缺乏有效反馈和干预工具。

Conclusion: 设计个性化干预系统可帮助BVI求职者更好地准备就业，改善就业结果。

Abstract: Significant changes in the digital employment landscape, driven by rapid
technological advancements and the COVID-19 pandemic, have introduced new
opportunities for blind and visually impaired (BVI) individuals in developing
countries like India. However, a significant portion of the BVI population in
India remains unemployed despite extensive accessibility advancements and job
search interventions. Therefore, we conducted semi-structured interviews with
20 BVI persons who were either pursuing or recently sought employment in the
digital industry. Our findings reveal that despite gaining digital literacy and
extensive training, BVI individuals struggle to meet industry requirements for
fulfilling job openings. While they engage in self-reflection to identify
shortcomings in their approach and skills, they lack constructive feedback from
peers and recruiters. Moreover, the numerous job intervention tools are limited
in their ability to meet the unique needs of BVI job seekers. Our results
therefore provide key insights that inform the design of future collaborative
intervention systems that offer personalized feedback for BVI individuals,
effectively guiding their self-reflection process and subsequent job search
behaviors, and potentially leading to improved employment outcomes.

</details>


### [69] [Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents](https://arxiv.org/abs/2506.22815)
*Haichang Li*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This position paper aims to rethink the role and design of memory in Large
Language Model (LLM)-based agent systems. We observe that while current memory
practices have begun to transcend the limitations of single interactions, they
remain conceptually grounded in "bound memory" in terms of design concept-where
memory is treated as local state attached to specific context or entities,
forming "memory silos" that impede cross-entity collaboration. To overcome this
architectural bottleneck, this paper proposes the timely design perspective of
"Memory as a Service" (MaaS). MaaS advocates decoupling memory from its
conventional role as an interaction byproduct and encapsulating it as a modular
service that can be independently callable, dynamically composable, and finely
governed. At its core, MaaS leverages the duality of memory-its inherently
private nature and its potential for public service-to grant memory controlled,
on-demand interoperability across entities. This paper introduces a
two-dimensional design space defined by entity structure and service type,
illustrating how MaaS aligns with current memory practices while naturally
extending them to cross-entity collaborative scenarios. Finally, we outline an
open research agenda spanning governance, security, and ethical ecosystems, and
call upon the broader research community to explore this shift toward
service-oriented memory for collaborative agents operating across entity
boundaries.

</details>


### [70] [Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation](https://arxiv.org/abs/2506.22841)
*George Bell,Alma Cantu*

Main category: cs.HC

TL;DR: 提出了一种新型遮挡管理方法“双目透明度”，通过调整双眼透明度来改善遮挡问题，用户研究表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统透明度调整方法会破坏深度关系并移除遮挡物的关键信息，需要新的遮挡管理方案。

Method: 提出“双目透明度”，通过对比双眼遮挡物的透明度，同时展示遮挡物与被遮挡物。

Result: 用户研究表明，该方法用户参与度高且优于传统方法，但需进一步确定最佳透明度值。

Conclusion: “双目透明度”在遮挡管理中表现出潜力，值得进一步研究其透明度的优化范围。

Abstract: Adjusting transparency is a common method of mitigating occlusion but is
often detrimental for understanding the relative depth relationships between
objects as well as removes potentially important information from the occluding
object. We propose using dichoptic opacity, a novel method for occlusion
management that contrasts the transparency of occluders presented to each eye.
This allows for better simultaneous understanding of both occluder and
occluded. A user study highlights the technique's potential, showing strong
user engagement and a clear preference for dichoptic opacity over traditional
presentations. While it does not determine optimal transparency values, it
reveals promising trends in both percentage and range that merit further
investigation.

</details>


### [71] [Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives](https://arxiv.org/abs/2506.22932)
*Zoe Anastasiadou,Andreas Lanitis*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The increase of the percentage of elderly population in modern societies
dictates the use of emerging technologies as a means of supporting elder
members of the society. Within this scope, Extended Reality (XR) technologies
pose as a promising technology for improving the daily lives of the elderly
population. This paper presents a literature review that describes the most
common characteristics of the physical and mental state of the elderly,
allowing readers, and specifically XR developers, to understand the main
difficulties faced by elderly users of extended reality applications so they
can develop accessible, user friendly and engaging applications for the target
audience. Furthermore, a review of existing extended reality applications that
target the elder population is presented, allowing readers to get acquainted
with existing design paradigms that can inspire future developments.

</details>


### [72] [GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework](https://arxiv.org/abs/2506.22937)
*Tianrun Qiu,Changxin Chen,Sizhe Cheng,Yiming Yang,Yixiao Guo,Zhicong Lu,Yuxin Ma*

Main category: cs.HC

TL;DR: GamerAstra是一个通用无障碍框架，通过多模态技术和多智能体设计，帮助盲人和低视力玩家更便捷地玩游戏。


<details>
  <summary>Details</summary>
Motivation: 解决盲人和低视力玩家因视觉元素不可访问、界面导航困难和交互输入限制而难以参与电子游戏的问题。

Method: 引入GamerAstra框架，结合大型语言模型和视觉语言模型等多模态技术，提供可定制的辅助功能和多输入模态的界面导航支持。

Result: 技术评估和用户研究表明，GamerAstra显著提升了盲人和低视力玩家的游戏可玩性和沉浸感。

Conclusion: GamerAstra展示了智能无障碍框架在游戏领域的潜力，为未来研究提供了方向。

Abstract: Blind and low-vision (BLV) players encounter critical challenges in engaging
with video games due to the inaccessibility of visual elements, difficulties in
navigating interfaces, and limitations in sending interaction input. Moreover,
the development of specialized accessibility features typically requires
substantial programming effort and is often implemented on a game-by-game
basis. To address these challenges, we introduce \textit{GamerAstra}, a
generalized accessibility framework that leverages a multi-agent design to
facilitate access to video games for BLV players. It integrates multi-modal
techniques including large language models and vision-language models, enabling
interaction with games lacking native accessibility support. The framework
further incorporates customizable assistance granularities to support varying
degrees of visual impairment and enhances interface navigation through multiple
input modalities. The evaluation through technical assessments and user studies
indicate that \textit{GamerAstra} effectively enhances playability and delivers
a more immersive gaming experience for BLV players. These findings also
underscore potential avenues for advancing intelligent accessibility frameworks
in the gaming domain.

</details>


### [73] [Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools](https://arxiv.org/abs/2506.22940)
*Varun Sangwan,Heidi Makitalo*

Main category: cs.HC

TL;DR: 探讨协作式AI系统如何通过透明化界面设计增强用户识别和评估社交媒体错误信息的能力，研究显示其效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在识别情绪化或缺乏上下文的信息时效果不足，需开发更有效的工具。

Method: 设计并评估了一种结合实时解释、多源聚合和辩论式交互的协作AI界面。

Result: 79%的参与者认为辩论模式更有效，多源视图评分为4.6/5。

Conclusion: 上下文丰富的对话式AI系统可提升媒介素养，未来工具应注重伦理设计与用户互动。

Abstract: This paper investigates how collaborative AI systems can enhance user agency
in identifying and evaluating misinformation on social media platforms.
Traditional methods, such as personal judgment or basic fact-checking, often
fall short when faced with emotionally charged or context-deficient content. To
address this, we designed and evaluated an interactive interface that
integrates collaborative AI features, including real-time explanations, source
aggregation, and debate-style interaction. These elements aim to support
critical thinking by providing contextual cues and argumentative reasoning in a
transparent, user-centered format. In a user study with 14 participants, 79%
found the debate mode more effective than standard chatbot interfaces, and the
multiple-source view received an average usefulness rating of 4.6 out of 5. Our
findings highlight the potential of context-rich, dialogic AI systems to
improve media literacy and foster trust in digital information environments. We
argue that future tools for misinformation mitigation should prioritize ethical
design, explainability, and interactive engagement to empower users in a
post-truth era.

</details>


### [74] [Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions](https://arxiv.org/abs/2506.22941)
*Kaixuan Wang,Jason T. Jacques,Chenxin Diao*

Main category: cs.HC

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）为药物使用者（PWUD）提供准确且实用的减害信息，同时解决现有线上渠道的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于现有线上渠道在适应性、可访问性和污名化问题上的不足，药物使用者的健康信息需求未得到充分满足。

Method: 通过定性研讨会与多方利益相关者（学者、减害实践者、在线社区管理员）合作，探索LLM的潜力与设计考量。

Result: LLMs可以解决部分信息障碍（如快速响应、多语言支持、减少污名化），但需解决伦理、上下文理解等挑战。

Conclusion: 通过协同设计，LLMs可作为减害生态中有用、安全且负责任的工具。

Abstract: Access to accurate and actionable harm reduction information can directly
impact the health outcomes of People Who Use Drugs (PWUD), yet existing online
channels often fail to meet their diverse and dynamic needs due to limitations
in adaptability, accessibility, and the pervasive impact of stigma. Large
Language Models (LLMs) present a novel opportunity to enhance information
provision, but their application in such a high-stakes domain is under-explored
and presents socio-technical challenges. This paper investigates how LLMs can
be responsibly designed to support the information needs of PWUD. Through a
qualitative workshop involving diverse stakeholder groups (academics, harm
reduction practitioners, and an online community moderator), we explored LLM
capabilities, identified potential use cases, and delineated core design
considerations. Our findings reveal that while LLMs can address some existing
information barriers (e.g., by offering responsive, multilingual, and
potentially less stigmatising interactions), their effectiveness is contingent
upon overcoming challenges related to ethical alignment with harm reduction
principles, nuanced contextual understanding, effective communication, and
clearly defined operational boundaries. We articulate design pathways
emphasising collaborative co-design with experts and PWUD to develop LLM
systems that are helpful, safe, and responsibly governed. This work contributes
empirically grounded insights and actionable design considerations for the
responsible development of LLMs as supportive tools within the harm reduction
ecosystem.

</details>


### [75] [Against 'softmaxing' culture](https://arxiv.org/abs/2506.22968)
*Daniel Mwesigwa*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI is flattening culture. Evaluations of "culture" are showing the myriad
ways in which large AI models are homogenizing language and culture, averaging
out rich linguistic differences into generic expressions. I call this
phenomenon "softmaxing culture," and it is one of the fundamental challenges
facing AI evaluations today. Efforts to improve and strengthen evaluations of
culture are central to the project of cultural alignment in large AI systems.
This position paper argues that machine learning (ML) and human-computer
interaction (HCI) approaches to evaluation are limited. I propose two key
shifts. First, instead of asking "what is culture?" at the start of system
evaluations, I propose beginning with the question: "when is culture?" Second,
while I acknowledge the philosophical claim that cultural universals exist, the
challenge is not simply to describe them, but to situate them in relation to
their particulars. Taken together, these conceptual shifts invite evaluation
approaches that move beyond technical requirements, toward perspectives more
responsive to the complexities of culture.

</details>


### [76] [Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks](https://arxiv.org/abs/2506.23016)
*Tomás Silva Santos Rocha,Anastasiia Mikhailova,Moreno I. Coco,José Santos-Victor*

Main category: cs.HC

TL;DR: 该研究利用眼动数据和深度学习模型VTNet，通过视觉记忆任务区分健康人和轻度认知障碍（MCI）。模型性能与类似研究相当，为MCI的自动化诊断工具开发提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着全球痴呆症患病率的预计增长，亟需可扩展的诊断工具。本研究旨在利用眼动数据和深度学习技术，开发一种区分健康人和MCI的方法。

Method: 研究使用44名参与者（24名MCI，20名健康人）的眼动数据，训练了基于VTNet的深度学习模型。模型整合了时间序列和空间数据，包括扫描路径、热图和图像内容，并测试了图像分辨率等参数的影响。

Result: 最佳模型（使用700×700像素热图）的灵敏度为68%，特异性为76%。尽管面临数据集小、任务时间短等挑战，其性能与类似研究相当。

Conclusion: 研究为MCI的自动化诊断工具开发提供了支持，未来需进一步优化模型并采用标准化的长期视觉记忆任务。

Abstract: The global prevalence of dementia is projected to double by 2050,
highlighting the urgent need for scalable diagnostic tools. This study utilizes
digital cognitive tasks with eye-tracking data correlated with memory processes
to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment
(MCI), a precursor to dementia. A deep learning model based on VTNet was
trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who
performed a visual memory task. The model utilizes both time series and spatial
data derived from eye-tracking. It was modified to incorporate scan paths, heat
maps, and image content. These modifications also enabled testing parameters
such as image resolution and task performance, analyzing their impact on model
performance. The best model, utilizing $700\times700px$ resolution heatmaps,
achieved 68% sensitivity and 76% specificity. Despite operating under more
challenging conditions (e.g., smaller dataset size, shorter task duration, or a
less standardized task), the model's performance is comparable to an
Alzheimer's study using similar methods (70% sensitivity and 73% specificity).
These findings contribute to the development of automated diagnostic tools for
MCI. Future work should focus on refining the model and using a standardized
long-term visual memory task.

</details>


### [77] [Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age](https://arxiv.org/abs/2506.23017)
*Noverah Khan,Hira Eiraj Daud,Suleman Shahid*

Main category: cs.HC

TL;DR: 研究探讨了技术中欺骗性设计元素对儿童的潜在影响，并通过游戏化应用提升了儿童的识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前关于黑暗模式对儿童影响的研究较少，而儿童在数字设备中的独立性增加，亟需早期教育。

Method: 开发了一款游戏化应用，用于教育儿童识别和应对黑暗模式。

Result: 教育显著提高了儿童对黑暗模式的意识，改变了他们在数字平台上的行为。

Conclusion: 早期教育对培养儿童识别欺骗性设计的能力至关重要，有助于塑造数字素养高的新一代。

Abstract: This paper addresses the critical issue of deceptive design elements
prevalent in technology, and their potential impact on children. Recent
research highlights the impact of dark patterns on adults and adolescents,
while studies involving children are scarce. In an era where children wield
greater independence with digital devices, their vulnerability to dark patterns
amplifies without early education. Our findings show a significant positive
impact of dark pattern education on children's awareness, revealing that
heightened awareness considerably alters children's navigation of social media,
video games, and streaming platforms. To this end, we developed a gamified
application aimed at instructing children on identifying and responding to
various dark patterns. Our evaluation results emphasize the critical role of
early education in empowering children to recognize and counter deceptive
design, thereby cultivating a digitally literate generation capable of making
informed choices in the complex landscape of digital technology.

</details>


### [78] [CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding](https://arxiv.org/abs/2506.23075)
*Yuchen Zhou,Jiamin Wu,Zichen Ren,Zhouheng Yao,Weiheng Lu,Kunyu Peng,Qihao Zheng,Chunfeng Song,Wanli Ouyang,Chao Gou*

Main category: cs.HC

TL;DR: CSBrain是一种跨尺度时空脑信号基础模型，通过改进EEG信号解码方法，解决了传统模型忽视神经活动多尺度特性导致的泛化问题。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码面临的核心挑战是神经活动的跨尺度时空结构特性。传统模型忽视了这一特性，导致表现不佳。

Method: CSBrain引入跨尺度时空标记化（CST）和结构化稀疏注意力（SSA），分别用于提取多尺度特征和捕捉跨窗口依赖关系。

Result: 在11个EEG任务和16个数据集上的实验表明，CSBrain优于任务专用模型和其他基础模型。

Conclusion: 跨尺度建模是EEG解码的关键归纳偏差，CSBrain为未来脑-AI研究提供了强大基础。

Abstract: Understanding and decoding brain activity from electroencephalography (EEG)
signals is a fundamental challenge in neuroscience and AI, with applications in
cognition, emotion recognition, diagnosis, and brain-computer interfaces. While
recent EEG foundation models advance generalized decoding via unified
architectures and large-scale pretraining, they adopt a scale-agnostic dense
modeling paradigm inherited from NLP and vision. This design neglects a core
property of neural activity: cross-scale spatiotemporal structure. EEG task
patterns span a wide range of temporal and spatial scales, from short bursts to
slow rhythms, and from localized cortical responses to distributed
interactions. Ignoring this diversity leads to suboptimal representations and
weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain
foundation model for generalized EEG decoding. CSBrain introduces: (i)
Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale
features from localized temporal windows and anatomical brain regions into
compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which
captures cross-window and cross-region dependencies, enhancing scale diversity
while removing spurious correlations. CST and SSA are alternately stacked to
progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks
across 16 datasets show that CSBrain consistently outperforms task-specific and
foundation model baselines. These results establish cross-scale modeling as a
key inductive bias and position CSBrain as a robust backbone for future
brain-AI research.

</details>


### [79] [A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences](https://arxiv.org/abs/2506.23116)
*Wei Xu*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: User experience (UX) practices have evolved in stages and are entering a
transformative phase (UX 3.0), driven by AI technologies and shifting user
needs. Human-centered AI (HCAI) experiences are emerging, necessitating new UX
approaches to support UX practices in the AI era. We propose a UX 3.0 paradigm
framework to respond and guide UX practices in developing HCAI systems.

</details>


### [80] [ImprovMate: Multimodal AI Assistant for Improv Actor Training](https://arxiv.org/abs/2506.23180)
*Riccardo Drago,Yotam Sechayk,Mustafa Doga Dogan,Andrea Sanna,Takeo Igarashi*

Main category: cs.HC

TL;DR: ImprovMate利用大型语言模型（LLMs）自动生成叙事提示和线索，帮助演员在即兴表演中专注于创造力。


<details>
  <summary>Details</summary>
Motivation: 即兴表演训练中需要维持叙事连贯性和管理认知负荷，传统方法依赖人工干预。

Method: 基于专业即兴演员的见解，设计练习（如突发故事结局和反应思维练习），结合参考表保持连贯性。

Result: 初步研究表明，演员接受AI技术，尤其是当其与传统方法相似且加入AI生成线索时。

Conclusion: ImprovMate为即兴训练提供了创新工具，平衡随机性和结构化指导。

Abstract: Improvisation training for actors presents unique challenges, particularly in
maintaining narrative coherence and managing cognitive load during
performances. Previous research on AI in improvisation performance often
predates advances in large language models (LLMs) and relies on human
intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate
the generation of narrative stimuli and cues, allowing actors to focus on
creativity without keeping track of plot or character continuity. Based on
insights from professional improvisers, ImprovMate incorporates exercises that
mimic live training, such as abrupt story resolution and reactive thinking
exercises, while maintaining coherence via reference tables. By balancing
randomness and structured guidance, ImprovMate provides a groundbreaking tool
for improv training. Our pilot study revealed that actors might embrace AI
techniques if the latter mirrors traditional practices, and appreciate the
fresh twist introduced by our approach with the AI-generated cues.

</details>


### [81] [Vibe coding: programming through conversation with artificial intelligence](https://arxiv.org/abs/2506.23253)
*Advait Sarkar,Ian Drosos*

Main category: cs.HC

TL;DR: 探讨了“氛围编程”，即开发者通过与AI互动生成代码的新兴编程范式，研究了其工作流程、调试方法及挑战。


<details>
  <summary>Details</summary>
Motivation: 研究AI辅助编程的演进，特别是开发者如何通过与大语言模型互动生成代码，以及这种范式对编程技能需求的影响。

Method: 通过分析一系列视频资料，结合框架分析方法，研究程序员的目标、工作流、提示技术、调试方法及遇到的挑战。

Result: 氛围编程表现为开发者交替使用AI生成代码、快速评估和手动编辑的循环；调试是AI辅助与手动结合的过程，且需要编程技能进行上下文管理。

Conclusion: 氛围编程并未消除对编程技能的需求，而是将其转化为上下文管理、快速评估和策略性决策，代表了一种新型AI辅助编程的演进。

Abstract: We examine "vibe coding": an emergent programming paradigm where developers
primarily write code by interacting with code-generating large language models
rather than writing code directly. We analysed a curated set of videos
depicting extended vibe coding sessions with rich think-aloud reflections.
Using framework analysis, we investigated programmers' goals, workflows,
prompting techniques, debugging approaches, and challenges encountered. We find
that vibe coding follows iterative goal satisfaction cycles where developers
alternate between prompting AI, evaluating generated code through rapid
scanning and application testing, and manual editing. Prompting strategies
blend vague, high-level directives with detailed technical specifications.
Debugging remains a hybrid process combining AI assistance with manual
practices. Critically, vibe coding does not eliminate the need for programming
expertise but rather redistributes it toward context management, rapid code
evaluation, and decisions about when to transition between AI-driven and manual
manipulation of code. Trust in AI tools during vibe coding is dynamic and
contextual, developed through iterative verification rather than blanket
acceptance. Vibe coding is an evolution of AI-assisted programming that
represents an early manifestation of "material disengagement", where
practitioners orchestrate code production and manipulation, mediated through
AI, while maintaining selective and strategic oversight.

</details>


### [82] [Accessible Data Access and Analysis by People who are Blind or Have Low Vision](https://arxiv.org/abs/2506.23443)
*Samuel Reinders,Munazza Zaib,Matthew Butler,Bongshin Lee,Ingrid Zukerman,Lizhen Qu,Kim Marriott*

Main category: cs.HC

TL;DR: 开发新型辅助技术，帮助盲人或低视力人群便捷探索和分析数据。


<details>
  <summary>Details</summary>
Motivation: 当前盲人或低视力人群在探索和分析数据时面临障碍，限制了其对政府、健康和个人数据的访问，并减少了就业机会。

Method: 通过协作设计和开发一种创新系统，重点关注可刷新触觉显示器和对话代理的使用。系统将结合触觉图形和语音与用户交互，并主动协助数据分析任务。

Result: 预计系统将填补重要的公平性差距，并在辅助技术、多模态界面、对话系统及自然语言理解和生成方面带来创新。

Conclusion: 该研究有望为盲人或低视力人群提供更广泛的数据访问途径，并推动相关技术领域的进步。

Abstract: Our work aims to develop new assistive technologies that enable blind or low
vision (BLV) people to explore and analyze data readily. At present, barriers
exist for BLV people to explore and analyze data, restricting access to
government, health and personal data, and limiting employment opportunities.
This work explores the co-design and development of an innovative system to
support data access, with a focus on the use of refreshable tactile displays
(RTDs) and conversational agents. The envisaged system will use a combination
of tactile graphics and speech to communicate with BLV users, and proactively
assist with data analysis tasks. As well as addressing significant equity gaps,
our work expects to produce innovations in assistive technology, multimodal
interfaces, dialogue systems, and natural language understanding and
generation.

</details>


### [83] [Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path](https://arxiv.org/abs/2506.23457)
*Yuya Ide,Hailong Liu,Takahiro Wada*

Main category: cs.HC

TL;DR: 研究调查了在自动驾驶个人移动车辆（APMV）中提供路径信息对乘客头部运动行为及晕动症的影响，发现路径提示能显著降低晕动症评分并延迟症状出现。


<details>
  <summary>Details</summary>
Motivation: APMV在共享空间中频繁避让可能导致快速转向调整，增加乘客晕动症风险，研究旨在探讨路径信息对此的缓解作用。

Method: 通过对比手动驾驶（MD）、无路径信息的自动驾驶（AD w/o path）和有路径信息的自动驾驶（AD w/ path）进行控制实验，分析16名乘客的数据。

Result: 提供路径信息显著降低晕动症评分，乘客在MD和AD w/ path条件下更倾向于主动调整头部运动方向与车辆转向同步。

Conclusion: 路径提示能有效缓解晕动症，但头部运动延迟与晕动症的生理机制尚需进一步研究。

Abstract: Autonomous personal mobility vehicles (APMVs) are small mobility devices
designed for individual automated transportation in shared spaces. In such
environments, frequent pedestrian avoidance maneuvers may cause rapid steering
adjustments and passive postural responses from passengers, thereby increasing
the risk of motion sickness. This study investigated the effects of providing
path information on 16 passengers' head movement behavior and motion sickness
while riding an APMV. Through a controlled experiment comparing manual driving
(MD), autonomous driving without path information (AD w/o path), and autonomous
driving with path information (AD w/ path), we found that providing path cues
significantly reduced MISC scores and delayed the onset of motion sickness
symptoms. In addition, participants were more likely to proactively align their
head movements with the direction of vehicle rotation in both MD and AD w/ path
conditions. Although a small correlation was observed between the delay in yaw
rotation of the passenger's head relative to the vehicle and the occurrence of
motion sickness, the underlying physiological mechanism remains to be
elucidated.

</details>


### [84] [Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs](https://arxiv.org/abs/2506.23458)
*Xiaoxiao Yang,Chan Feng,Jiancheng Chen*

Main category: cs.HC

TL;DR: 论文提出MuseCogNet框架，结合自监督和监督学习方法，提升便携式EEG设备在认知负荷检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 便携式EEG设备（如Muse头带）在移动性方面表现出色，但信号的非平稳性限制了数据质量和解码准确性，需要在便携性和性能之间找到平衡。

Method: 提出MuseCogNet框架，整合自监督和监督学习，通过平均池化的自监督重建损失捕获稳健的神经生理模式，并结合交叉熵损失优化任务特异性认知判别。

Result: MuseCogNet在公开Muse数据集上显著优于现有方法，为生态环境中的神经认知监测提供可行方案。

Conclusion: MuseCogNet通过联合学习框架有效解决了便携式EEG的性能问题，推动了便携脑机接口的实际应用。

Abstract: Portable and wearable consumer-grade electroencephalography (EEG) devices,
like Muse headbands, offer unprecedented mobility for daily brain-computer
interface (BCI) applications, including cognitive load detection. However, the
exacerbated non-stationarity in portable EEG signals constrains data fidelity
and decoding accuracy, creating a fundamental trade-off between portability and
performance. To mitigate such limitation, we propose MuseCogNet (Muse-based
Cognitive Network), a unified joint learning framework integrating
self-supervised and supervised training paradigms. In particular, we introduce
an EEG-grounded self-supervised reconstruction loss based on average pooling to
capture robust neurophysiological patterns, while cross-entropy loss refines
task-specific cognitive discriminants. This joint learning framework resembles
the bottom-up and top-down attention in humans, enabling MuseCogNet to
significantly outperform state-of-the-art methods on a publicly available Muse
dataset and establish an implementable pathway for neurocognitive monitoring in
ecological settings.

</details>


### [85] [Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research](https://arxiv.org/abs/2506.23545)
*Barbara Karpowicz,Maciej Grzeszczuk,Adam Kuzdraliński,Monika Kornacka,Aliaksandr Marozau,Wiktor Stawski,Pavlo Zinevych,Grzegorz Marcin Wójcik,Tomasz Kowalewski,Grzegorz Pochwatko,Wiesław Kopeć*

Main category: cs.HC

TL;DR: 该论文讨论了VR/AR/XR技术在多个领域中的应用，包括心理研究、太空探索和医学教育，强调了其在提升人类表现方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨沉浸式系统如何在高风险和高度监管环境中提升人类表现，特别是在心理研究、太空探索和医学教育等领域。

Method: 通过讨论和案例研究，分析了XR技术在心理测量、宇航员训练、医学培训和康复中的应用。

Result: XR技术能够提供受控且生态有效的环境，显著改善学习和治疗效果。

Conclusion: 沉浸式环境为多个领域提供了创新的解决方案，提升了训练、诊断和研究的效率与效果。

Abstract: Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are
increasingly recognized for their applications in training, diagnostics, and
psychological research, particularly in high-risk and highly regulated
environments. In this panel we discuss how immersive systems enhance human
performance across multiple domains, including clinical psychology, space
exploration, and medical education. In psychological research and training, XR
can offer a controlled yet ecologically valid setting for measuring cognitive
and affective processes. In space exploration, we discuss the development of
VR-based astronaut training and diagnostic systems, allowing astronauts to
perform real-time health assessments. In medical education and rehabilitation,
we cover procedural training and patient engagement. From virtual surgical
simulations to gamified rehabilitation exercises, immersive environments
enhance both learning outcomes and treatment adherence.

</details>


### [86] [Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2506.23678)
*Rock Yuren Pang,K. J. Kevin Feng,Shangbin Feng,Chu Li,Weijia Shi,Yulia Tsvetkov,Jeffrey Heer,Katharina Reinecke*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The output quality of large language models (LLMs) can be improved via
"reasoning": generating segments of chain-of-thought (CoT) content to further
condition the model prior to producing user-facing output. While these chains
contain valuable information, they are verbose and lack explicit organization,
making them tedious to review. Moreover, they lack opportunities for user
feedback, such as to remove unwanted considerations, add desired ones, or
clarify unclear assumptions. We introduce Interactive Reasoning, an interaction
design that visualizes chain-of-thought outputs as a hierarchy of topics and
enables user review and modification. We implement interactive reasoning in
Hippo, a prototype for AI-assisted decision making in the face of uncertain
trade-offs. In a user study with 16 participants, we find that interactive
reasoning in Hippo allows users to quickly identify and interrupt erroneous
generations, efficiently steer the model towards customized responses, and
better understand both model reasoning and model outputs. Our work contributes
to a new paradigm that incorporates user oversight into LLM reasoning
processes.

</details>


### [87] [If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons](https://arxiv.org/abs/2506.23694)
*Patrick Stadler,Christopher Lazik,Christopher Katins,Thomas Kosch*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The process of requirements analysis requires an understanding of the end
users of a system. Thus, expert stakeholders, such as User Experience (UX)
designers, usually create various descriptions containing information about the
users and their possible needs. In our paper, we investigate to what extent UX
novices are able to write such descriptions into user scenarios. We conducted a
user study with 60 participants consisting of 30 UX experts and 30 novices who
were asked to write a user scenario with or without the help of an
LLM-supported writing assistant. Our findings show that LLMs empower laypersons
to write reasonable user scenarios and provide first-hand insights for
requirements analysis that are comparable to UX experts in terms of structure
and clarity, while especially excelling at audience-orientation. We present our
qualitative and quantitative findings, including user scenario anatomies,
potential influences, and differences in the way participants approached the
task.

</details>


### [88] [The Impact of AI on Educational Assessment: A Framework for Constructive Alignment](https://arxiv.org/abs/2506.23815)
*Patrick Stokkink*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The influence of Artificial Intelligence (AI), and specifically Large
Language Models (LLM), on education is continuously increasing. These models
are frequently used by students, giving rise to the question whether current
forms of assessment are still a valid way to evaluate student performance and
comprehension. The theoretical framework developed in this paper is grounded in
Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning
objectives. We argue that AI influences learning objectives of different Bloom
levels in a different way, and assessment has to be adopted accordingly.
Furthermore, in line with Bloom's vision, formative and summative assessment
should be aligned on whether the use of AI is permitted or not.
  Although lecturers tend to agree that education and assessment need to be
adapted to the presence of AI, a strong bias exists on the extent to which
lecturers want to allow for AI in assessment. This bias is caused by a
lecturer's familiarity with AI and specifically whether they use it themselves.
To avoid this bias, we propose structured guidelines on a university or faculty
level, to foster alignment among the staff. Besides that, we argue that
teaching staff should be trained on the capabilities and limitations of AI
tools. In this way, they are better able to adapt their assessment methods.

</details>


### [89] [Email as the Interface to Generative AI Models: Seamless Administrative Automation](https://arxiv.org/abs/2506.23850)
*Andres Navarro,Carlos de Quinto,José Alberto Hernández*

Main category: cs.HC

TL;DR: 论文提出了一种新型架构框架，将大语言模型（LLMs）与电子邮件接口结合，自动化企业环境中的行政任务，尤其针对无障碍问题。


<details>
  <summary>Details</summary>
Motivation: 旨在通过熟悉的企业邮件界面，为非技术行政人员提供复杂表格填写和文档处理的自动化解决方案，弥合先进AI技术与实际可用性之间的差距。

Method: 系统利用光学字符识别（OCR）和智能自动化，将邮件正文作为自然语言提示，附件作为上下文信息，实现自动化处理。

Result: 评估表明，系统能在8秒内完成复杂行政表格填写，人工监督下可减少3-4倍的工作时间，最高效的LLM准确填写了29个字段中的16个，成本降低了64%。

Conclusion: 研究表明，基于电子邮件的LLM集成是一种可行且经济高效的自动化方法，支持广泛采用而无须专业知识或重大流程更改，提升了技术的包容性和效率。

Abstract: This paper introduces a novel architectural framework that integrates Large
Language Models (LLMs) with email interfaces to automate administrative tasks,
specifically targeting accessibility barriers in enterprise environments. The
system connects email communication channels with Optical Character Recognition
(OCR) and intelligent automation, enabling non-technical administrative staff
to delegate complex form-filling and document processing tasks using familiar
email interfaces. By treating the email body as a natural language prompt and
attachments as contextual information, the workflow bridges the gap between
advanced AI capabilities and practical usability. Empirical evaluation shows
that the system can complete complex administrative forms in under 8 seconds of
automated processing, with human supervision reducing total staff time by a
factor of three to four compared to manual workflows. The top-performing LLM
accurately filled 16 out of 29 form fields and reduced the total cost per
processed form by 64% relative to manual completion. These findings demonstrate
that email-based LLM integration is a viable and cost-effective approach for
democratizing advanced automation in organizational settings, supporting
widespread adoption without requiring specialized technical knowledge or major
workflow changes. This aligns with broader trends in leveraging LLMs to enhance
accessibility and automate complex tasks for non-technical users, making
technology more inclusive and efficient.

</details>


### [90] [Autonomy by Design: Preserving Human Autonomy in AI Decision-Support](https://arxiv.org/abs/2506.23952)
*Stefan Buijsman,Sarah Carter,Juan Pablo Bermúdez*

Main category: cs.HC

TL;DR: 本文研究了AI决策支持系统对领域特定自主性的影响，提出了保持自主性的框架。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何影响领域特定自主性，即专业领域内的自我治理能力。

Method: 分析先前研究和实证案例（医学、金融、教育领域），提出自主性保护的AI支持系统框架。

Result: 指出AI可能通过缺乏可靠的失败指标和无意识的价值侵蚀领域自主性。

Conclusion: 提出了社会技术设计模式以维护领域自主性，同时利用AI能力增强人类代理。

Abstract: AI systems increasingly support human decision-making across domains of
professional, skill-based, and personal activity. While previous work has
examined how AI might affect human autonomy globally, the effects of AI on
domain-specific autonomy -- the capacity for self-governed action within
defined realms of skill or expertise -- remain understudied. We analyze how AI
decision-support systems affect two key components of domain-specific autonomy:
skilled competence (the ability to make informed judgments within one's domain)
and authentic value-formation (the capacity to form genuine domain-relevant
values and preferences). By engaging with prior investigations and analyzing
empirical cases across medical, financial, and educational domains, we
demonstrate how the absence of reliable failure indicators and the potential
for unconscious value shifts can erode domain-specific autonomy both
immediately and over time. We then develop a constructive framework for
autonomy-preserving AI support systems. We propose specific socio-technical
design patterns -- including careful role specification, implementation of
defeater mechanisms, and support for reflective practice -- that can help
maintain domain-specific autonomy while leveraging AI capabilities. This
framework provides concrete guidance for developing AI systems that enhance
rather than diminish human agency within specialized domains of action.

</details>


### [91] [Access InContext: Futuring Accessible Prototyping Tools and Methods](https://arxiv.org/abs/2506.24057)
*Patricia Piedade,Peter A Hayton,Cynthia Bennett,Anna R L Carter,Clara Crivellaro,Alan Dix,Jess McGowan,Katta Spiel,Miriam Sturdee,Garreth W. Tigwell,Hugo Nicolau*

Main category: cs.HC

TL;DR: 探讨HCI研究中现有原型工具和方法对残障人士的包容性问题，提出改进方向，并在CHI 2025工作坊中促进讨论和实践。


<details>
  <summary>Details</summary>
Motivation: 当前HCI研究中的原型工具和方法存在实际可访问性障碍，阻碍了残障人士在设计过程中的参与。

Method: 通过讨论、重新利用现有资源和开发新工具，促进残障人士的参与。

Result: 为CHI 2025工作坊提供平台，推动可访问原型设计的实践和未来发展方向。

Conclusion: 改进原型工具和方法是构建更具包容性技术环境的关键。

Abstract: The popularity of accessibility research has grown recently, improving
digital inclusion for people with disabilities. However, researchers, including
those who have disabilities, have attempted to include people with disabilities
in all aspects of design, and they have identified a myriad of practical
accessibility barriers posed by tools and methods leveraged by human-computer
interaction (HCI) researchers during prototyping. To build a more inclusive
technological landscape, we must question the effectiveness of existing
prototyping tools and methods, repurpose/retrofit existing resources, and build
new tools and methods to support the participation of both researchers and
people with disabilities within the prototyping design process of novel
technologies. This full-day workshop at CHI 2025 will provide a platform for
HCI researchers, designers, and practitioners to discuss barriers and
opportunities for creating accessible prototyping and promote hands-on ideation
and fabrication exercises aimed at futuring accessible prototyping.

</details>


### [92] [Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities](https://arxiv.org/abs/2506.24104)
*Mariia Ershova,Graziano Blasilli*

Main category: cs.HC

TL;DR: 本文探讨了将服务设计方法整合到医疗数字孪生中的潜力，以提升其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前医疗数字孪生系统缺乏结构化服务设计方法，整合服务设计与可视化分析可提升其适用性。

Method: 通过分析整合服务设计与可视化分析的差距，并提出研究方向。

Result: 提出了填补整合差距的方法并指出了未来研究方向。

Conclusion: 服务设计与可视化分析的结合可为医疗数字孪生提供更实用的解决方案。

Abstract: Digital twins (DT) are increasingly used in healthcare to model patients,
processes, and physiological systems. While recent solutions leverage
visualization, visual analytics, and user interaction, these systems rarely
incorporate structured service design methodologies. Bridging service design
with visual analytics and visualization can be valuable for the healthcare DT
community. This paper aims to introduce the service design discipline to
visualization researchers by framing this integration gap and suggesting
research directions to enhance the real-world applicability of DT solutions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [93] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: VoteSplat结合3D高斯分布和霍夫投票技术，提升3D场景理解能力，降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯分布方法仅关注几何和外观建模，缺乏深度场景理解且计算成本高，VoteSplat目标是通过结合SAM和霍夫投票解决这些问题。

Method: 利用SAM进行实例分割生成2D投票图，并将空间偏移向量嵌入高斯基元以构建3D投票映射，同时通过深度约束优化定位。

Result: 实验证明VoteSplat在开放词汇3D实例定位、点云理解和交互式定位等任务中表现优异。

Conclusion: VoteSplat有效提升3D场景理解能力，同时简化训练流程。

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


### [94] [DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations](https://arxiv.org/abs/2506.22849)
*Michael A. Kern,Alain Galvan,David Oldcorn,Daniel Skinner,Rohan Mehalwal,Leo Reyes Lozano,Matthäus G. Chajdas*

Main category: cs.GR

TL;DR: 提出一种新的OBB构建技术，通过固定离散旋转集合实现高效编码和减少计算复杂度，显著提升光线追踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统OBB构建在薄长和旋转几何场景中计算开销大且内存需求高，因此需要更高效的OBB构建方法。

Method: 采用固定离散旋转集合的OBB变换，并结合k-DOPs扩展为多子节点层次结构，作为后处理步骤集成到现有构建流程中。

Result: 实验显示射线追踪性能平均提升18.5%（主射线）和32.4%（次级射线），最大增益达65%，构建时间增加12.6%。

Conclusion: 该方法在提升光线追踪性能的同时保持构建时间可控，适用于实时应用。

Abstract: Oriented bounding box (OBB) bounding volume hierarchies offer a more precise
fit than axis-aligned bounding box hierarchies in scenarios with thin elongated
and arbitrarily rotated geometry, enhancing intersection test performance in
ray tracing. However, determining optimally oriented bounding boxes can be
computationally expensive and have high memory requirements. Recent research
has shown that pre-built hierarchies can be efficiently converted to OBB
hierarchies on the GPU in a bottom-up pass, yielding significant ray tracing
traversal improvements. In this paper, we introduce a novel OBB construction
technique where all internal node children share a consistent OBB transform,
chosen from a fixed set of discrete quantized rotations. This allows for
efficient encoding and reduces the computational complexity of OBB
transformations. We further extend our approach to hierarchies with multiple
children per node by leveraging Discrete Orientation Polytopes (k-DOPs),
demonstrating improvements in traversal performance while limiting the build
time impact for real-time applications. Our method is applied as a
post-processing step, integrating seamlessly into existing hierarchy
construction pipelines. Despite a 12.6% increase in build time, our
experimental results demonstrate an average improvement of 18.5% in primary,
32.4% in secondary rays, and maximum gain of 65% in ray intersection
performance, highlighting its potential for advancing real-time applications.

</details>


### [95] [Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions](https://arxiv.org/abs/2506.22973)
*AmirHossein Naghi Razlighi,Elaheh Badali Golezani,Shohreh Kasaei*

Main category: cs.GR

TL;DR: 提出了一种基于可学习置信评分的新型有损压缩方法，用于优化3D高斯泼溅的存储和计算开销。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然能实现高质量实时渲染，但通常生成数百万个泼溅，导致存储和计算开销过大。

Method: 通过建模Beta分布的置信分数，并通过重构感知损失优化每个泼溅的置信度，从而修剪低置信度泼溅，同时保持视觉保真度。

Result: 实验证明该方法在压缩与保真度之间取得了良好平衡，且适用于任何高斯泼溅变体。

Conclusion: 该方法架构无关，且置信平均值可作为新的场景质量评估指标。

Abstract: 3D Gaussian Splatting enables high-quality real-time rendering but often
produces millions of splats, resulting in excessive storage and computational
overhead. We propose a novel lossy compression method based on learnable
confidence scores modeled as Beta distributions. Each splat's confidence is
optimized through reconstruction-aware losses, enabling pruning of
low-confidence splats while preserving visual fidelity. The proposed approach
is architecture-agnostic and can be applied to any Gaussian Splatting variant.
In addition, the average confidence values serve as a new metric to assess the
quality of the scene. Extensive experiments demonstrate favorable trade-offs
between compression and fidelity compared to prior work. Our code and data are
publicly available at
https://github.com/amirhossein-razlighi/Confident-Splatting

</details>


### [96] [The ultimate display: Where will all the pixels come from?](https://arxiv.org/abs/2506.23001)
*Benjamin Watson,David Luebke*

Main category: cs.GR

TL;DR: 研究探讨了通过减少像素计算来实现高速更新打印机分辨率墙显示的可能性。


<details>
  <summary>Details</summary>
Motivation: 传统渲染模式无法满足打印机分辨率墙显示的高速更新需求，需要找到更高效的渲染方法。

Method: 采用时间自适应采样技术，打破传统帧模式。

Result: 该方法可能支持每秒数百次更新的高分辨率显示。

Conclusion: 时间自适应采样是解决高分辨率高速显示问题的潜在方案。

Abstract: Could the answer be to compute fewer pixels? Renderers that break traditional
framed patterns and opt for temporally adaptive sampling might be the key to
printer-resolution wall displays that update hundreds of times per second.

</details>


### [97] [Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics](https://arxiv.org/abs/2506.23092)
*Arisa Cowe,Tyson Neuroth,Qi Wu,Martin Rieth,Jacqueline Chen,Myoungkyu Lee,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 提出了一种新的局部空间统计可视化方法，用于多物理场和多尺度数据，通过曲线波变换、局部区域划分和符号设计实现高效的可视化。


<details>
  <summary>Details</summary>
Motivation: 解决多物理场、多尺度数据中难以直观展示跨空间、尺度和场之间相关性的挑战。

Method: 利用曲线波变换分解尺度，通过水平集约束的Centroidal Voronoi Tessellation分区，并结合符号设计展示数据。

Result: 实现了交互式可视化系统，成功应用于湍流燃烧数据和无压缩湍流通道流数据。

Conclusion: 该方法帮助科学家更好地理解湍流中多场和多尺度间的相互作用。

Abstract: Many scientific and engineering problems involving multi-physics span a wide
range of scales. Understanding the interactions across these scales is
essential for fully comprehending such complex problems. However, visualizing
multivariate, multiscale data within an integrated view where correlations
across space, scales, and fields are easily perceived remains challenging. To
address this, we introduce a novel local spatial statistical visualization of
flow fields across multiple fields and turbulence scales. Our method leverages
the curvelet transform for scale decomposition of fields of interest, a
level-set-restricted centroidal Voronoi tessellation to partition the spatial
domain into local regions for statistical aggregation, and a set of glyph
designs that combines information across scales and fields into a single, or
reduced set of perceivable visual representations. Each glyph represents data
aggregated within a Voronoi region and is positioned at the Voronoi site for
direct visualization in a 3D view centered around flow features of interest. We
implement and integrate our method into an interactive visualization system
where the glyph-based technique operates in tandem with linked 3D spatial views
and 2D statistical views, supporting a holistic analysis. We demonstrate with
case studies visualizing turbulent combustion data--multi-scalar compressible
flows--and turbulent incompressible channel flow data. This new capability
enables scientists to better understand the interactions between multiple
fields and length scales in turbulent flows.

</details>


### [98] [Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization](https://arxiv.org/abs/2506.23364)
*Patrick Komon,Gerald Kimmersdorfer,Adam Celarek,Manuela Waldner*

Main category: cs.GR

TL;DR: 提出基于WebGPU的交互式数据驱动计算叠加层，用于原生和Web 3D地理地图应用，支持GPU多步骤计算工作流，适用于雪盖和雪崩模拟。


<details>
  <summary>Details</summary>
Motivation: 优化3D地理地图应用的性能和交互性，通过GPU计算实现快速数据可视化和模拟。

Method: 采用基于WebGPU的多步计算工作流，从多数据源生成数据驱动叠加层，支持交互式参数调整。

Result: 测试显示，大规模雪崩模拟可在毫秒至秒级完成，比Python实现快数个数量级。

Conclusion: 该方法显著提升3D地理应用的交互性和计算效率，适用于需快速模拟的领域。

Abstract: We present interactive data-driven compute overlays for native and web-based
3D geographic map applications based on WebGPU. Our data-driven overlays are
generated in a multi-step compute workflow from multiple data sources on the
GPU. We demonstrate their potential by showing results from snow cover and
avalanche simulations, where simulation parameters can be adjusted
interactively and results are visualized instantly. Benchmarks show that our
approach can compute large-scale avalanche simulations in milliseconds to
seconds, depending on the size of the terrain and the simulation parameters,
which is multiple orders of magnitude faster than a state-of-the-art Python
implementation.

</details>


### [99] [Escher Tile Deformation via Closed-Form Solution](https://arxiv.org/abs/2506.23388)
*Crane He Chen,Vladimir G. Kim*

Main category: cs.GR

TL;DR: 提出了一种实时变形方法，用于处理Escher瓷砖（可无缝拼接的有机形状），通过周期性位移场实现无间隙或重叠的变形。


<details>
  <summary>Details</summary>
Motivation: 为了实现Escher瓷砖的无缝变形，同时保留其对称性和纹理完整性，满足艺术创作和实际应用的需求。

Method: 采用解析解生成周期性位移场，支持17种壁纸群表示形式，并通过交互工具提供用户可控的自适应衰减参数。

Result: 方法成功实现了瓷砖边界和内部分同时变形，并在照片编辑和形状雕刻等应用中展示了有效性。

Conclusion: 该方法不仅为艺术创作提供了精细控制，还可广泛应用于制造和动画等领域。

Abstract: We present a real-time deformation method for Escher tiles -- interlocking
organic forms that seamlessly tessellate the plane following symmetry rules. We
formulate the problem as determining a periodic displacement field. The goal is
to deform Escher tiles without introducing gaps or overlaps. The resulting
displacement field is obtained in closed form by an analytical solution. Our
method processes tiles of 17 wallpaper groups across various representations
such as images and meshes. Rather than treating tiles as mere boundaries, we
consider them as textured shapes, ensuring that both the boundary and interior
deform simultaneously. To enable fine-grained artistic input, our interactive
tool features a user-controllable adaptive fall-off parameter, allowing precise
adjustment of locality and supporting deformations with meaningful semantic
control. We demonstrate the effectiveness of our method through various
examples, including photo editing and shape sculpting, showing its use in
applications such as fabrication and animation.

</details>


### [100] [Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles](https://arxiv.org/abs/2506.23406)
*Tim Gerrits*

Main category: cs.GR

TL;DR: 本文提出了一种推广方法，将不确定的退化解张量特征扩展到任意模式值的不确定模式表面，形成了一个统一框架来分析和显示张量场集合中的不确定模式拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 张量场分析中，模式表面（如中性表面和任意模式表面）对于全面理解张量场拓扑至关重要，但现有方法通常未能有效捕捉全局行为。

Method: 提出了一种将不确定退化解张量特征推广到任意模式值的不确定模式表面的方法，支持表面和线几何的统一框架。

Result: 在多个工程和材料科学的实际仿真数据集上验证了该方法的有效性。

Conclusion: 该方法为张量场集合中的不确定模式拓扑特征分析提供了统一的解决方案。

Abstract: The analysis of 3D symmetric second-order tensor fields often relies on
topological features such as degenerate tensor lines, neutral surfaces, and
their generalization to mode surfaces, which reveal important structural
insights into the data. However, uncertainty in such fields is typically
visualized using derived scalar attributes or tensor glyph representations,
which often fail to capture the global behavior. Recent advances have
introduced uncertain topological features for tensor field ensembles by
focusing on degenerate tensor locations. Yet, mode surfaces, including neutral
surfaces and arbitrary mode surfaces are essential to a comprehensive
understanding of tensor field topology. In this work, we present a
generalization of uncertain degenerate tensor features to uncertain mode
surfaces of arbitrary mode values, encompassing uncertain degenerate tensor
lines as a special case. Our approach supports both surface and line
geometries, forming a unified framework for analyzing uncertain mode-based
topological features in tensor field ensembles. We demonstrate the
effectiveness of our method on several real-world simulation datasets from
engineering and materials science.

</details>


### [101] [Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios](https://arxiv.org/abs/2506.23777)
*Haoyang Du,Kiran Chhatre,Christopher Peters,Brian Keegan,Rachel McDonnell,Cathy Ennis*

Main category: cs.GR

TL;DR: 研究探讨了虚拟现实中真实与合成的语音和手势对用户体验的影响，尤其是情感共鸣和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语音和手势在沉浸式环境中的整合效果，以及它们如何影响用户的情感和共情反应。

Method: 通过比较VR和2D显示下的真实与合成语音及手势，并结合不同情感情境（积极、中性、消极），评估用户感知。

Result: 结果显示VR能增强自然语音手势配对的感知，但对合成的配对效果不明显，突显了现有技术的不足。

Conclusion: 结论强调需要重新评估手势的适用性，并优化AI驱动的合成技术以适应沉浸式环境。

Abstract: The creation of virtual humans increasingly leverages automated synthesis of
speech and gestures, enabling expressive, adaptable agents that effectively
engage users. However, the independent development of voice and gesture
generation technologies, alongside the growing popularity of virtual reality
(VR), presents significant questions about the integration of these signals and
their ability to convey emotional detail in immersive environments. In this
paper, we evaluate the influence of real and synthetic gestures and speech,
alongside varying levels of immersion (VR vs. 2D displays) and emotional
contexts (positive, neutral, negative) on user perceptions. We investigate how
immersion affects the perceived match between gestures and speech and the
impact on key aspects of user experience, including emotional and empathetic
responses and the sense of co-presence. Our findings indicate that while VR
enhances the perception of natural gesture-voice pairings, it does not
similarly improve synthetic ones - amplifying the perceptual gap between them.
These results highlight the need to reassess gesture appropriateness and refine
AI-driven synthesis for immersive environments. See video:
https://youtu.be/WMfjIB1X-dc

</details>


### [102] [GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering](https://arxiv.org/abs/2506.23957)
*Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai*

Main category: cs.GR

TL;DR: 论文提出了一种基于3D的GaVS方法，通过局部重建与渲染范式解决视频稳定化中的问题，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频稳定化方法存在几何失真和过度裁剪等问题，影响用户体验。

Method: 提出GaVS方法，利用3D相机位姿预测高斯Splatting基元，并通过多视角动态光度和跨帧正则化实现时间一致性。

Result: 在定量和定性评估中，GaVS优于现有2D和2.5D方法，用户研究也验证了其优越性。

Conclusion: GaVS通过3D重构和渲染提供了更优的视频稳定化方案，显著提升了效果和用户体验。

Abstract: Video stabilization is pivotal for video processing, as it removes unwanted
shakiness while preserving the original user motion intent. Existing
approaches, depending on the domain they operate, suffer from several issues
(e.g. geometric distortions, excessive cropping, poor generalization) that
degrade the user experience. To address these issues, we introduce
\textbf{GaVS}, a novel 3D-grounded approach that reformulates video
stabilization as a temporally-consistent `local reconstruction and rendering'
paradigm. Given 3D camera poses, we augment a reconstruction model to predict
Gaussian Splatting primitives, and finetune it at test-time, with multi-view
dynamics-aware photometric supervision and cross-frame regularization, to
produce temporally-consistent local reconstructions. The model are then used to
render each stabilized frame. We utilize a scene extrapolation module to avoid
frame cropping. Our method is evaluated on a repurposed dataset, instilled with
3D-grounded information, covering samples with diverse camera motions and scene
dynamics. Quantitatively, our method is competitive with or superior to
state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics
and new geometry consistency. Qualitatively, our method produces noticeably
better results compared to alternatives, validated by the user study.

</details>


### [103] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出了一种动态调整引导比例的调度器，提升文本到图像生成的质量和文本对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有的Classifi er-Free Guidance (CFG)机制中，引导比例的选择对图像质量和文本对齐性至关重要，但固定比例可能导致性能不稳定。

Method: 引入一种基于条件噪声信号的动态引导比例调度器，通过学习调度策略优化CFG的表现。

Result: 实验结果表明该方法显著提升了图像质量和文本对齐性，且无需额外计算资源。

Conclusion: 该动态调度器为文本到图像生成提供了一种更优的解决方案。

Abstract: Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this work, we propose an annealing
guidance scheduler which dynamically adjusts the guidance scale over time based
on the conditional noisy signal. By learning a scheduling policy, our method
addresses the temperamental behavior of CFG. Empirical results demonstrate that
our guidance scheduler significantly enhances image quality and alignment with
the text prompt, advancing the performance of text-to-image generation.
Notably, our novel scheduler requires no additional activations or memory
consumption, and can seamlessly replace the common classifier-free guidance,
offering an improved trade-off between prompt alignment and quality.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [104] [Prediction of Protein Three-dimensional Structures via a Hardware-Executable Quantum Computing Framework](https://arxiv.org/abs/2506.22677)
*Yuqi Zhang,Yuxin Yang,William Martin,Kingsten Lin,Zixu Wang,Cheng-Chang Lu,Weiwen Jiang,Ruth Nussinov,Joseph Loscalzo,Qiang Guan,Feixiong Cheng*

Main category: cs.ET

TL;DR: 提出一种基于量子计算的框架，用于预测蛋白质活性位点结构，并在实际量子处理器上验证其优于AlphaFold3的性能。


<details>
  <summary>Details</summary>
Motivation: 解决短肽和柔性肽片段结构预测的挑战，传统方法在此类情况下表现不佳。

Method: 使用变分量子本征求解器（VQE）将结构预测任务转化为基态能量最小化问题，并通过两阶段架构优化量子噪声。

Result: 在PDBbind数据集上测试，量子方法在RMSD和对接效果上均优于AlphaFold3。

Conclusion: 首次展示了量子硬件在生物结构预测中的端到端应用，展现了工程可行性和实际优势。

Abstract: Accurate prediction of protein active site structures remains a central
challenge in structural biology, particularly for short and flexible peptide
fragments where conventional methods often fail. Here, we present a quantum
computing framework specifically developed for utility-level quantum processors
to address this problem. Starting from an amino acid sequence, we formulate the
structure prediction task as a ground-state energy minimization problem using
the Variational Quantum Eigensolver (VQE). Amino acid connectivity is encoded
on a tetrahedral lattice model, and structural constraints-including steric,
geometric, and chirality terms-are mapped into a problem-specific Hamiltonian
expressed as sparse Pauli operators. The optimization is executed via a
two-stage architecture separating energy estimation and measurement decoding,
allowing noise mitigation under realistic quantum device conditions. We
evaluate the framework on 23 randomly selected real protein fragments from the
PDBbind dataset, as well as 7 real fragments from proteins with therapeutic
potential, and run the experiments on the IBM-Cleveland Clinic quantum
processor. Structural predictions are benchmarked against AlphaFold3 (AF3)
using identical postprocessing and docking procedures. Our quantum method
outperformed AF3 in both RMSD (Root-Mean-Square Deviation) and docking
efficacy. This work demonstrates, for the first time, a complete end-to-end
pipeline for biologically relevant structure prediction on real quantum
hardware, highlighting its engineering feasibility and practical advantage over
existing classical and deep learning approaches.

</details>


### [105] [Stateful Logic In-Memory Using Gain-Cell eDRAM](https://arxiv.org/abs/2506.23185)
*Barak Hoffer,Shahar Kvatinsky*

Main category: cs.ET

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern data-intensive applications demand memory solutions that deliver
high-density, low-power, and integrated computational capabilities to reduce
data movement overhead. This paper presents the use of Gain-Cell embedded DRAM
(GC-eDRAM) - a compelling alternative to traditional SRAM and eDRAM - for
stateful, in-memory logic. We propose a circuit design that exploits GC-eDRAM's
dual-port architecture and nondestructive read operation to perform logic
functions directly within the GC-eDRAM memory array. Our simulation results
demonstrate a 5us retention time coupled with a 99.5% success rate for
computing the logic gates. By incorporating processing-in-memory (PIM)
functionality into GC-eDRAM, our approach enhances memory and compute
densities, lowers power consumption, and improves overall performance for
data-intensive applications.

</details>


### [106] [CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors upon GPGPU Platforms](https://arxiv.org/abs/2506.23405)
*Faaiq Waqar,Ming-Yen Lee,Seongwon Yoon,Seongkwang Lim,Shimeng Yu*

Main category: cs.ET

TL;DR: 该论文探讨了利用非晶氧化物半导体（AOS）晶体管与CMOS集成，替换GPGPU中的SRAM存储器，以提高性能和能效。


<details>
  <summary>Details</summary>
Motivation: SRAM的扩展性和漏电功耗限制了GPGPU的性能提升，需要替代技术。

Method: 研究AOS晶体管在电容性持久存储器拓扑中的集成，并分析其密度和能效权衡。

Result: 实验显示，AOS增益单元在体积更小的同时，性能功耗比提升5.2倍，IPC平均提高8%。

Conclusion: AOS集成技术为GPGPU提供了显著的性能和能效改进。

Abstract: In contemporary general-purpose graphics processing units (GPGPUs), the
continued increase in raw arithmetic throughput is constrained by the
capabilities of the register file (single-cycle) and last-level cache (high
bandwidth), which require the delivery of operands at a cadence demanded by
wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,
density, or bandwidth of these memories can unlock substantial performance
gains; however, the recent stagnation of SRAM bit-cell scaling leads to
inequivalent losses in compute density.
  To address the challenges posed by SRAM's scaling and leakage power
consumption, this paper explores the potential CMOS+X integration of amorphous
oxide semiconductor (AOS) transistors in capacitive, persistent memory
topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in
multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the
density and energy tradeoffs of back-end-of-line (BEOL) integrated memories
utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while
accounting for the macro-level limitations of integrating AOS candidate
structures proposed by the device community (an aspect often overlooked in
prior work). By exploiting the short lifetime of register operands, we propose
a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of
the footprint of SRAM with over 70% lower standby power, enabling enhancements
to compute capacity, such as larger warp sizes or processor counts. Benchmarks
run on a validated NVIDIA Ampere-class GPU model, using a modified version of
Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and
an average 8% higher geometric mean instruction per cycle (IPC) on various
compute- and memory-bound tasks.

</details>


### [107] [Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins](https://arxiv.org/abs/2506.23826)
*Lluís C. Coll,Martin W. Lauer-Schmaltz,Philip Cash,John P. Hansen,Anja Maier*

Main category: cs.ET

TL;DR: 本文提出了一种集成大型语言模型与动态更新的个人数据的新型人类数字孪生（HDT）系统架构，能够模拟个体的对话风格、记忆和行为。


<details>
  <summary>Details</summary>
Motivation: 通过结合对话AI的最新进展，旨在创建更真实、互动的数字个体，支持更自然的对话和动态更新的个人体验。

Method: 采用上下文感知记忆检索、神经可塑性启发的巩固机制和自适应学习机制，构建一个不断演变的数字人格。

Result: 系统不仅能根据对话对象复制个体的独特对话风格，还能通过动态捕获的个人体验和记忆丰富响应。

Conclusion: 这一研究标志着向真实虚拟个体迈出了重要一步，但也提出了隐私和伦理方面的挑战，需确保HDT的负责任发展。

Abstract: Human Digital Twins (HDTs) have traditionally been conceptualized as
data-driven models designed to support decision-making across various domains.
However, recent advancements in conversational AI open new possibilities for
HDTs to function as authentic, interactive digital counterparts of individuals.
This paper introduces a novel HDT system architecture that integrates large
language models with dynamically updated personal data, enabling it to mirror
an individual's conversational style, memories, and behaviors. To achieve this,
our approach implements context-aware memory retrieval, neural
plasticity-inspired consolidation, and adaptive learning mechanisms, creating a
more natural and evolving digital persona. The resulting system does not only
replicate an individual's unique conversational style depending on who they are
speaking with, but also enriches responses with dynamically captured personal
experiences, opinions, and memories. While this marks a significant step toward
developing authentic virtual counterparts, it also raises critical ethical
concerns regarding privacy, accountability, and the long-term implications of
persistent digital identities. This study contributes to the field of HDTs by
describing our novel system architecture, demonstrating its capabilities, and
discussing future directions and emerging challenges to ensure the responsible
and ethical development of HDTs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [108] [Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714)
*Jinliang Shi,Shigang Li,Youxuan Xu,Xueying Wang,Rongtian Fu,Zhi Ma,Tong Wu*

Main category: cs.DC

TL;DR: 该论文提出了一种名为Libra的系统方法，通过协同利用CUDA和Tensor核心来优化稀疏矩阵乘法的性能，实验结果显示其性能显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现代加速器中的Tensor核心和CUDA核心各有优势和局限性，单独使用其中一种资源会导致稀疏矩阵乘法性能不佳。

Method: 提出Libra，结合2D感知的工作负载分配策略和系统优化（如混合负载均衡、优化内核实现和GPU加速预处理），实现CUDA和Tensor核心的协同计算。

Result: 在H100和RTX 4090 GPU上的实验表明，Libra比现有技术平均快3.1倍（最高9.23倍），在图神经网络应用中快2.9倍（最高3.9倍）。

Conclusion: Libra通过充分利用GPU的异构计算资源，为稀疏算子加速提供了新的视角。

Abstract: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used
in deep learning and scientific computing. Modern accelerators are commonly
equipped with Tensor cores and CUDA cores to accelerate sparse operators. The
former brings superior computing power but only for structured matrix
multiplication, while the latter has relatively lower performance but with
higher programming flexibility. In this work, we discover that utilizing one
resource alone leads to inferior performance for sparse matrix multiplication,
due to their respective limitations. To this end, we propose Libra, a
systematic approach that enables synergistic computation between CUDA and
Tensor cores to achieve the best performance for sparse matrix multiplication.
Specifically, we propose a 2D-aware workload distribution strategy to find out
the sweet point of task mapping for different sparse operators, leveraging both
the high performance of Tensor cores and the low computational redundancy on
CUDA cores. In addition, Libra incorporates systematic optimizations for
heterogeneous computing, including hybrid load-balancing, finely optimized
kernel implementations, and GPU-accelerated preprocessing. Extensive
experimental results on H100 and RTX 4090 GPUs show that Libra outperforms the
state-of-the-art by on average 3.1x (up to 9.23x) over DTC-SpMM and 2.9x (up to
3.9x) for end-to-end GNN applications. Libra opens up a new perspective for
sparse operator acceleration by fully exploiting the heterogeneous computing
resources on GPUs.

</details>


### [109] [Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing](https://arxiv.org/abs/2506.22773)
*Yanran Wu,Inez Hua,Yi Ding*

Main category: cs.DC

TL;DR: SCARF是一个评估计算水足迹的框架，考虑时空水压差异，提出调整水影响指标，通过案例研究展示节水潜力。


<details>
  <summary>Details</summary>
Motivation: 计算可持续性中的水资源消耗日益重要，现有评估常忽略时空水压差异。

Method: SCARF框架结合水消耗量与时空水压，提出AWI指标。

Result: 案例研究表明优化时空选择可显著减少水影响。

Conclusion: SCARF为水可持续计算提供了新方向。

Abstract: Water consumption is an increasingly critical dimension of computing
sustainability, especially as AI workloads rapidly scale. However, current
water impact assessment often overlooks where and when water stress is more
severe. To fill in this gap, we present SCARF, the first general framework that
evaluates water impact of computing by factoring in both spatial and temporal
variations in water stress. SCARF calculates an Adjusted Water Impact (AWI)
metric that considers both consumption volume and local water stress over time.
Through three case studies on LLM serving, datacenters, and semiconductor
fabrication plants, we show the hidden opportunities for reducing water impact
by optimizing location and time choices, paving the way for water-sustainable
computing. The code is available at https://github.com/jojacola/SCARF.

</details>


### [110] [TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations](https://arxiv.org/abs/2506.22818)
*Stanislav Sedukhin,Yoichi Tomioka,Kazuya Matsumoto,Yuichi Okuyama*

Main category: cs.DC

TL;DR: 论文提出了一种名为TriADA的算法和架构，用于高效计算三线性（3D）离散正交变换（3D-DXTs），旨在解决高计算和内存需求问题，同时提高大规模并行计算的能效。


<details>
  <summary>Details</summary>
Motivation: 高维数据的计算和内存需求高，传统的并行处理方式能耗大，尤其是在稀疏数据场景下。TriADA旨在通过创新算法和架构优化解决这些问题。

Method: TriADA包括四个创新：(1) 低秩算法计算3D-DXTs；(2) 基于外积的GEMM核；(3) 分布式3D网络架构；(4) 弹性稀疏外积（ESOP）方法。

Result: TriADA能够在线性时间内完成三线性变换，具有高度并行性、可扩展性和能效性，适用于AI和HPC中最耗时的张量运算。

Conclusion: TriADA通过算法和架构的协同设计，显著提升了大规?并行计算中张量运算的效率和能效，适用于AI和HPC应用。

Abstract: Multilinear transformations are key in high-performance computing (HPC) and
artificial intelligence (AI) workloads, where data is represented as tensors.
However, their high computational and memory demands, which grow with
dimensionality, often slow down critical tasks. Moreover, scaling computation
by enlarging the number of parallel processing units substantially increases
energy consumption, limiting widespread adoption, especially for sparse data,
which is common in HPC and AI applications. This paper introduces the Trilinear
Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address
these challenges with the following innovations: (1) a massively parallel,
low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal
transformations (3D-DXTs), which is a special case of the more general 3-mode
matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM
kernel with decoupled streaming active memory, specially designed to accelerate
3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully
distributed 3D network of mesh interconnected processing elements or cells with
a coordinate-free, data-driven local processing activity, which is independent
of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids
unnecessary computing and communication operations with zero-valued operands,
thereby enhancing energy efficiency, computational accuracy, and stability.
TriADA is capable of performing a variety of trilinear transformations with
hypercubic arithmetic complexity in a linear number of time-steps. The
massively parallel, scalable, and energy-efficient architecture of TriADA is
ideal for accelerating multilinear tensor operations, which are the most
demanding parts of AI and HPC workloads.

</details>


### [111] [Performance Measurements in the AI-Centric Computing Continuum Systems](https://arxiv.org/abs/2506.22884)
*Praveen Kumar Donta,Qiyang Zhang,Schahram Dustdar*

Main category: cs.DC

TL;DR: 回顾分布式计算连续体（DCC）中的性能指标，探讨新兴需求如可持续性和能效。


<details>
  <summary>Details</summary>
Motivation: 计算范式向分布式架构转变，生成式AI和大语言模型增加了计算需求，需要更新性能指标以适应新需求。

Method: 回顾DCC和IoT环境中常用指标，讨论新兴性能维度如可持续性和系统可观测性。

Result: 提出选择合适指标的准则，以支持系统设计和用户需求的对齐。

Conclusion: 研究为未来DCC性能指标的优化和开发提供了方向。

Abstract: Over the Eight decades, computing paradigms have shifted from large,
centralized systems to compact, distributed architectures, leading to the rise
of the Distributed Computing Continuum (DCC). In this model, multiple layers
such as cloud, edge, Internet of Things (IoT), and mobile platforms work
together to support a wide range of applications. Recently, the emergence of
Generative AI and large language models has further intensified the demand for
computational resources across this continuum. Although traditional performance
metrics have provided a solid foundation, they need to be revisited and
expanded to keep pace with changing computational demands and application
requirements. Accurate performance measurements benefit both system designers
and users by supporting improvements in efficiency and promoting alignment with
system goals. In this context, we review commonly used metrics in DCC and IoT
environments. We also discuss emerging performance dimensions that address
evolving computing needs, such as sustainability, energy efficiency, and system
observability. We also outline criteria and considerations for selecting
appropriate metrics, aiming to inspire future research and development in this
critical area.

</details>


### [112] [FastSet: Parallel Claim Settlement](https://arxiv.org/abs/2506.23395)
*Xiaohong Chen,Grigore Rosu*

Main category: cs.DC

TL;DR: FastSet是一种基于角色的分布式协议，用于去中心化金融和结算，灵感来自区块链但放弃了强一致性要求。


<details>
  <summary>Details</summary>
Motivation: 解决区块链的强一致性限制，同时保留其大部分优点，如去中心化和安全性。

Method: 通过账户持有人的签名声明和验证网络的分布式验证来实现状态更新，无需验证器之间通信。

Result: 协议被证明是正确的，尽管其具有高度并行性，且保留了区块链的许多优势。

Conclusion: FastSet提供了一种无需强一致性的高效去中心化解决方案，适用于多种应用场景。

Abstract: FastSet is an actor-based distributed protocol for decentralized finance and
settlement, which is inspired from blockchains. Account holders cooperate by
making claims, which can include payments, holding and transferring assets,
accessing and updating shared data, medical records, digital identity, and
mathematical theorems, among many others. The claims are signed by their owners
and are broadcast to a decentralized network of validators, which validate and
settle them. Validators replicate the global state of the accounts and need not
communicate with each other. In sharp contrast to blockchains, strong
consistency is purposely given up as a requirement. Yet, many if not most of
the blockchain benefits are preserved. The protocol is proved to be correct,
despite its massively parallel nature.

</details>


### [113] [Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model](https://arxiv.org/abs/2506.23635)
*Mu-Chi Chen,Po-Hsuan Huang,Xiangrui Ke,Chia-Heng Tu,Chun Jason Xue,Shih-Hao Hung*

Main category: cs.DC

TL;DR: 论文提出了一种基于Mac Studio集群的低成本私有LLM方案，利用M2 Ultra芯片和MoE架构优化性能，比现有H100 GPU方案更高效。


<details>
  <summary>Details</summary>
Motivation: 解决私有LLM系统的高成本和扩展性问题，适用于个人或小群体服务。

Method: 使用Mac Studio集群和M2 Ultra芯片，通过并行执行MoE架构的专家模块减少推理时间，并优化内存管理。

Result: 集群比H100 GPU方案成本效率高1.15倍，同时开发了性能模型指导系统设计。

Conclusion: Mac Studio集群是高效且经济的私有LLM解决方案，性能模型为系统设计提供了重要参考。

Abstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)
with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and
Databricks' DBRX. This paper addresses the cost and scalability challenges
encountered when constructing private LLM systems for personal or small group
services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2
Ultra chips is established as a cost-efficient solution to host and accelerate
the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our
performance analysis reveal that parallel execution of the model's experts
across two to four machine nodes significantly reduces inference time. We find
that computation time for the experts is comparable to the communication time
for exchanging their outputs, emphasizing the importance of network latency
over bandwidth. We also observe significant management overhead due to Apple
software stack's memory management logic. Based on these findings, we develop
optimization schemes to eliminate the memory management overhead. As a result,
the Mac Studio cluster is 1.15 times more cost-efficient than the
state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we
construct a performance model to estimate system performance under varying
configurations, and the model provides valuable insights for designing private
LLM systems.

</details>


### [114] [Large-scale Neural Network Quantum States for ab initio Quantum Chemistry Simulations on Fugaku](https://arxiv.org/abs/2506.23809)
*Hongtao Xu,Zibo Wu,Mingzhen Li,Weile Jia*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Solving quantum many-body problems is one of the fundamental challenges in
quantum chemistry. While neural network quantum states (NQS) have emerged as a
promising computational tool, its training process incurs exponentially growing
computational demands, becoming prohibitively expensive for large-scale
molecular systems and creating fundamental scalability barriers for real-world
applications. To address above challenges, we present \ours, a high-performance
NQS training framework for \textit{ab initio} electronic structure
calculations. First, we propose a scalable sampling parallelism strategy with
multi-layers workload division and hybrid sampling scheme, which break the
scalability barriers for large-scale NQS training. Then, we introduce
multi-level parallelism local energy parallelism, enabling more efficient local
energy computation. Last, we employ cache-centric optimization for
transformer-based \textit{ansatz} and incorporate it with sampling parallelism
strategy, which further speedup up the NQS training and achieve stable memory
footprint at scale. Experiments demonstrate that \ours accelerate NQS training
with up to 8.41x speedup and attains a parallel efficiency up to 95.8\% when
scaling to 1,536 nodes.

</details>


### [115] [QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference](https://arxiv.org/abs/2506.23934)
*Xiangchen Li,Saeid Ghafouri,Bo Ji,Hans Vandierendonck,Deepu John,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As machine learning inferences increasingly move to edge devices, adapting to
diverse computational capabilities, hardware, and memory constraints becomes
more critical. Instead of relying on a pre-trained model fixed for all future
inference queries across diverse edge devices, we argue that planning an
inference pattern with a request-specific model tailored to the device's
computational capacity, accuracy requirements, and time constraints is more
cost-efficient and robust to diverse scenarios. To this end, we propose an
accuracy-aware and workload-balanced inference system that integrates joint
model quantization and inference partitioning. In this approach, the server
dynamically responds to inference queries by sending a quantized model and
adaptively sharing the inference workload with the device. Meanwhile, the
device's computational power, channel capacity, and accuracy requirements are
considered when deciding.
  Furthermore, we introduce a new optimization framework for the inference
system, incorporating joint model quantization and partitioning. Our approach
optimizes layer-wise quantization bit width and partition points to minimize
time consumption and cost while accounting for varying accuracy requirements of
tasks through an accuracy degradation metric in our optimization model. To our
knowledge, this work represents the first exploration of optimizing
quantization layer-wise bit-width in the inference serving system, by
introducing theoretical measurement of accuracy degradation. Simulation results
demonstrate a substantial reduction in overall time and power consumption, with
computation payloads decreasing by over 80% and accuracy degradation kept below
1%.

</details>


### [116] [Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC](https://arxiv.org/abs/2506.24045)
*Xinming Wei,Jiahao Zhang,Haoran Li,Jiayu Chen,Rui Qu,Maoliang Li,Xiang Chen,Guojie Luo*

Main category: cs.DC

TL;DR: Agent.xpu是一个高效的设备端代理LLM服务系统，通过异构执行图和在线调度优化，显著降低了反应任务的延迟并提高了主动任务的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有设备端LLM引擎无法有效处理反应任务（低延迟）和主动任务（高吞吐）的并发需求，尤其是在异构SoC上。

Method: Agent.xpu通过离线异构执行图构建、内核级抢占调度、空闲时间填充和带宽感知调度，优化任务分配和执行。

Result: 在Intel Core Ultra SoC上，Agent.xpu使反应任务延迟降低4.6倍，主动任务吞吐提升1.6-6.8倍。

Conclusion: Agent.xpu为异构SoC上的代理LLM工作负载提供了高效的服务解决方案，平衡了反应性和主动性的需求。

Abstract: The proliferation of agentic Large Language Models (LLMs) on personal devices
introduces a new class of workloads characterized by a dichotomy of objectives.
Reactive tasks, initiated by users, demand immediate, low-latency responses,
while proactive tasks operate invisibly and prioritize throughput. Existing
on-device LLM engines, designed for isolated inferences, fail to efficiently
manage these concurrent and conflicting requests on consumer-grade
heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces
Agent.xpu, an efficient serving system for agentic LLM workloads on
memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu
first constructs a heterogeneous execution graph, which fuses and chunks model
kernels for affinity-guided, elastic accelerator mapping with predictive kernel
annotation. At runtime, its online scheduler enables fine-grained, kernel-level
preemption to guarantee the responsiveness of reactive tasks. To maximize SoC
utilization, it adopts slack-aware kernel backfill to opportunistically append
proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware
dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves
4.6$\times$ lower latency for reactive tasks and sustains
1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to
state-of-the-art inference engines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [117] [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
*Wei Zhou,Ji Sun,Xuanhe Zhou,Guoliang Li,Luyang Liu,Hao Wu,Tianyuan Wang*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [118] [Oobleck: Low-Compromise Design for Fault Tolerant Accelerators](https://arxiv.org/abs/2506.22654)
*Guy Wilks,Brian Li,Jonathan Balkind*

Main category: cs.AR

TL;DR: 提出了一种名为Oobleck的新型加速器容错架构，通过模块化加速实现容错，同时减少芯片面积占用。Viscosity语言用于简化开发并强制模块化。数据中心模型表明该方法能减少故障导致的芯片更换，不影响吞吐量。三个案例研究验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 数据中心硬件更新周期延长，处理器复杂度增加导致故障风险上升，需要针对加速器数据路径的容错方案，传统方法面积占用高。

Method: 提出Oobleck架构，结合模块化加速和Viscosity语言（硬件-软件协同设计的基于角色的方法），减少面积需求。

Result: 数据中心模型显示减少故障导致的芯片购买，不影响吞吐量；三个案例（FFT、AES、DCT）验证速度和容错能力。单故障下加速应用仍保持1.7x-5.16x速度提升。

Conclusion: Oobleck架构和Viscosity语言能有效实现加速器容错，减少数据中心成本并维持性能，具有实际可行性。

Abstract: Data center hardware refresh cycles are lengthening. However, increasing
processor complexity is raising the potential for faults. To achieve longevity
in the face of increasingly fault-prone datapaths, fault tolerance is needed,
especially in on-chip accelerator datapaths. Previously researched methods for
adding fault tolerance to accelerator designs require high area, lowering chip
utilisation. We propose a novel architecture for accelerator fault tolerance,
Oobleck, which leverages modular acceleration to enable fault tolerance without
burdensome area requirements.
  In order to streamline the development and enforce modular conventions, we
introduce the Viscosity language, an actor based approach to hardware-software
co-design. Viscosity uses a single description of the accelerator's function
and produces both hardware and software descriptions.
  Our high-level models of data centers indicate that our approach can decrease
the number of failure-induced chip purchases inside data centers while not
affecting aggregate throughput, thus reducing data center costs. To show the
feasibility of our approach, we show three case-studies: FFT, AES, and DCT
accelerators. We additionally profile the performance under the key parameters
affecting latency. Under a single fault we can maintain speedups of between
1.7x-5.16x for accelerated applications over purely software implementations.
We show further benefits can be achieved by adding hot-spare FPGAs into the
chip.

</details>


### [119] [Approximate Logic Synthesis Using BLASYS](https://arxiv.org/abs/2506.22772)
*Jingxiao Ma,Soheil Hashemi,Sherief Reda*

Main category: cs.AR

TL;DR: BLASYS是一个开源工具，通过布尔矩阵分解（BMF）生成近似电路，实现设计精度与面积/功耗的权衡，平均节省48.14%面积，误差5%。


<details>
  <summary>Details</summary>
Motivation: 近似计算可以在设计精度与面积、功耗等设计指标之间实现权衡，BLASYS旨在提供一种可扩展的解决方案。

Method: 使用布尔矩阵分解（BMF）对电路真值表进行可控近似，并通过分区技术及设计空间探索优化子电路近似顺序。

Result: BLASYS在多个基准测试中平均节省48.14%面积，同时引入5%的相对误差。

Conclusion: BLASYS展示了一种有效的近似电路设计方法，能够实现精度与复杂度的优雅权衡。

Abstract: Approximate computing is an emerging paradigm where design accuracy can be
traded for improvements in design metrics such as design area and power
consumption. In this work, we overview our open-source tool, BLASYS, for
synthesis of approximate circuits using Boolean Matrix Factorization (BMF). In
our methodology the truth table of a given circuit is approximated using BMF to
a controllable approximation degree, and the results of the factorization are
used to synthesize the approximate circuit output. BLASYS scales up the
computations to large circuits through the use of partition techniques, where
an input circuit is partitioned into a number of interconnected subcircuits and
then a design-space exploration technique identifies the best order for
subcircuit approximations. BLASYS leads to a graceful trade-off between
accuracy and full circuit complexity as measured by design area. Using an
open-source design flow, we extensively evaluate our methodology on a number of
benchmarks, where we demonstrate that the proposed methodology can achieve on
average 48.14% in area savings, while introducing an average relative error of
5%.

</details>


### [120] [Sustainable operation of research infrastructure for novel computing](https://arxiv.org/abs/2506.23901)
*Yannik Stradmann,Joscha Ilmberger,Eric Müller,Johannes Schemmel*

Main category: cs.AR

TL;DR: 论文展示如何将神经形态计算系统BrainScaleS-2从实验室设备转变为可持续、公开可用的研究平台，并分享了十年运营的经验教训。


<details>
  <summary>Details</summary>
Motivation: 为新兴计算系统提供研究基础设施，促进其被广泛接受和使用。

Method: 将系统嵌入专门机构，结合传统集群和新型硬件，优化网络基础设施，采用19英寸标准化单元设计，运用现代CI/CD技术和自动化监控。

Result: 成功构建了一个稳定、易维护且可扩展的公开研究平台。

Conclusion: 通过长期运营，证明了神经形态计算系统作为公开研究平台的可行性，并总结了宝贵经验。

Abstract: Novel compute systems are an emerging research topic, aiming towards building
next-generation compute platforms. For these systems to thrive, they need to be
provided as research infrastructure to allow acceptance and usage by a large
community. By the example of the neuromorphic BrainScaleS-2 system, we showcase
the transformation from a laboratory setup to a sustainable, publicly available
platform. It is embedded into a purpose-built institute, tightly coupling a
conventional cluster with novel compute hardware. The network infrastructure is
optimized for robust operation, even in the case of unintended behavior of
individual devices. The systems themselves are packaged into 19-inch compatible
units to allow for easy maintenance and extension. We operate the platform
using modern CI/CD techniques and continuously assert its health using
automated system monitoring. Finally, we share our lessons learned during the
decade-long endeavor of operating analog neuromorphic systems as a publicly
available research platform.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [121] [Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan](https://arxiv.org/abs/2506.23851)
*Israel Fianyi,Soonja Yeom,Ju-Hyun Shin*

Main category: cs.CY

TL;DR: 云计算融入教育可为不同国家提供可扩展、经济高效的适应性学习系统，研究探讨了其在不同社会经济和基础设施背景下的应用。


<details>
  <summary>Details</summary>
Motivation: 探索云计算和适应性学习技术如何在不同国家（如澳大利亚、韩国、加纳和尼日利亚）中部署，以弥合数字和教育鸿沟。

Method: 研究分析了云计算在教育领域的具体应用，并识别推动因素和系统性挑战。

Result: 提出了如何根据全球不同背景定制云教育解决方案的见解。

Conclusion: 云教育可成为全球教育公平的有效工具，但需解决适应性挑战。

Abstract: The integration of cloud computing in education can revolutionise learning in
advanced (Australia & South Korea) and middle-income (Ghana & Nigeria)
countries, while offering scalable, cost-effective and equitable access to
adaptive learning systems. This paper explores how cloud computing and adaptive
learning technologies are deployed across different socio-economic and
infrastructure contexts. The study identifies enabling factors and systematic
challenges, providing insights into how cloud-based education can be tailored
to bridge the digital and educational divide globally.

</details>


### [122] [Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management](https://arxiv.org/abs/2506.23774)
*Ewelina Gajewska,Michal Wawer,Katarzyna Budzynska,Jarosław A. Chudziak*

Main category: cs.CY

TL;DR: 论文研究了利用大型语言模型（LLMs）辅助教师培训，通过多智能体系统模拟学校中的仇恨事件管理，提升教师的处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统教师培训存在成本高、时间受限和地理局限等问题，研究旨在探索LLMs在教育中的应用潜力。

Method: 构建了基于LLM的多智能体系统，结合检索增强提示和角色建模，模拟真实的仇恨事件场景。

Result: 系统帮助教师在安全环境中分析仇恨事件动态，提升理解和干预策略，试点评估显示效果显著。

Conclusion: LLM系统能有效辅助教师培训，提升教学中仇恨事件的应对能力。

Abstract: Computer-aided teacher training is a state-of-the-art method designed to
enhance teachers' professional skills effectively while minimising concerns
related to costs, time constraints, and geographical limitations. We
investigate the potential of large language models (LLMs) in teacher education,
using a case of teaching hate incidents management in schools. To this end, we
create a multi-agent LLM-based system that mimics realistic situations of hate,
using a combination of retrieval-augmented prompting and persona modelling. It
is designed to identify and analyse hate speech patterns, predict potential
escalation, and propose effective intervention strategies. By integrating
persona modelling with agentic LLMs, we create contextually diverse simulations
of hate incidents, mimicking real-life situations. The system allows teachers
to analyse and understand the dynamics of hate incidents in a safe and
controlled environment, providing valuable insights and practical knowledge to
manage such situations confidently in real life. Our pilot evaluation
demonstrates teachers' enhanced understanding of the nature of annotator
disagreements and the role of context in hate speech interpretation, leading to
the development of more informed and effective strategies for addressing hate
in classroom settings.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [123] [A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry](https://arxiv.org/abs/2506.22702)
*Zina Mohamed,Ammar B. Kouki,Sonia Aïssa*

Main category: eess.SY

TL;DR: 本文提出了一种基于相移值相关性的新型RIS设计（Connected-RIS），通过共享控制信号简化硬件结构并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 旨在简化无线通信中可重构智能表面（RIS）的硬件结构并降低能耗。

Method: 通过分析表面元素相移值的相关性，设计了Connected-RIS结构，其中相关元素共享控制信号。

Result: 该设计显著减少了负载阻抗和控制信号数量，功耗降低86-92%，控制信号减少83-98%，同时保持公平覆盖。

Conclusion: Connected-RIS在降低硬件成本和控制复杂度方面表现出色，同时满足通信性能需求。

Abstract: Aiming at simplifying the hardware structure and reducing the energy
consumption in wireless communication via reconfigurable intelligent surfaces
(RIS), this paper introduces a novel RIS design founded on the correlation
between the phase shift values of the surface elements. First, a correlation
analysis is conducted, considering the azimuth angle of a target device within
a coverage region spanning from $-80^{\circ}$ to $80^{\circ}$. The correlation
is demonstrated for different deployment cases, creating the basis for the new
RIS structure, termed Connected-RIS, where correlated elements are designed to
share the same control signal. The fundamental performance of the proposed
design is then analyzed in terms of control signals, power consumption, and
communication system performance, comparing it to two RIS structures with full
control: one with the same size as the proposed design, and the other employing
the minimum number of elements necessary to satisfy the fair coverage
criterion. The correlation-based RIS design enables three-dimensional passive
beamforming and significantly reduces the number of required load impedances
and control signals, thereby lowering the hardware cost and simplifying the
control circuitry. It also achieves substantial power savings as compared to
the baseline schemes, while maintaining sufficient gain for a fair radio
coverage. For instance, numerical simulations demonstrate that the proposed
design reduces the power consumption by almost 86-92\% and the control signals
by 83-98\% compared to operation with fully controlled RIS.

</details>


### [124] [Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity](https://arxiv.org/abs/2506.22855)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 本文提出了一种基于梯度追踪和动量加速的分布式优化算法，适用于局部非凸优化问题，并解决了信息共享网络中的非线性问题。


<details>
  <summary>Details</summary>
Motivation: 分布式优化通过并行和去中心化学习提升集中式机器学习方法，但现有算法在处理非线性网络和动态环境时仍有局限。

Method: 使用梯度追踪技术和动量加速（heavy-ball方法），结合扰动理论和特征谱分析，处理非线性和非凸问题，并采用权重平衡网络设计。

Result: 算法在存在非线性干扰和局部非凸代价函数的情况下仍能收敛，且适用于动态有向网络。

Conclusion: 该算法在分布式优化中表现出高效性和鲁棒性，尤其适用于动态和非线性环境。

Abstract: Distributed optimization advances centralized machine learning methods by
enabling parallel and decentralized learning processes over a network of
computing nodes. This work provides an accelerated consensus-based distributed
algorithm for locally non-convex optimization using the gradient-tracking
technique. The proposed algorithm (i) improves the convergence rate by adding
momentum towards the optimal state using the heavy-ball method, while (ii)
addressing general sector-bound nonlinearities over the information-sharing
network. The link nonlinearity includes any sign-preserving odd sector-bound
mapping, for example, log-scale data quantization or clipping in practical
applications. For admissible momentum and gradient-tracking parameters, using
perturbation theory and eigen-spectrum analysis, we prove convergence even in
the presence of sector-bound nonlinearity and for locally non-convex cost
functions. Further, in contrast to most existing weight-stochastic algorithms,
we adopt weight-balanced (WB) network design. This WB design and
perturbation-based analysis allow to handle dynamic directed network of agents
to address possible time-varying setups due to link failures or packet drops.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [125] [An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals](https://arxiv.org/abs/2506.22476)
*A. Subedi,S. De,L. Cavuoto,S. Schwaitzberg,M. Hackett,J. Norfleet*

Main category: eess.SP

TL;DR: 提出了一种基于fNIRS信号的Transformer基础模型，用于跨程序技能评估，具有高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估方法任务特定、预处理繁琐且缺乏鲁棒性，需通用、可解释且适应性强的模型。

Method: 使用自监督学习预训练fNIRS信号，结合轻量适配模块和通道注意力机制实现跨任务泛化。

Result: 模型在所有任务上分类准确率超过88%，尤其在新任务上仅需少量样本即可达到高AUC。

Conclusion: 该模型为高复杂度程序环境提供了高效、可解释的技能评估工具。

Abstract: Objective skill assessment in high-stakes procedural environments requires
models that not only decode underlying cognitive and motor processes but also
generalize across tasks, individuals, and experimental contexts. While prior
work has demonstrated the potential of functional near-infrared spectroscopy
(fNIRS) for evaluating cognitive-motor performance, existing approaches are
often task-specific, rely on extensive preprocessing, and lack robustness to
new procedures or conditions. Here, we introduce an interpretable
transformer-based foundation model trained on minimally processed fNIRS signals
for cross-procedural skill assessment. Pretrained using self-supervised
learning on data from laparoscopic surgical tasks and endotracheal intubation
(ETI), the model achieves greater than 88% classification accuracy on all
tasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It
generalizes to a novel emergency airway procedure--cricothyrotomy--using fewer
than 30 labeled samples and a lightweight (less than 2k parameter) adapter
module, attaining an AUC greater than 87%. Interpretability is achieved via a
novel channel attention mechanism--developed specifically for fNIRS--that
identifies functionally coherent prefrontal sub-networks validated through
ablation studies. Temporal attention patterns align with task-critical phases
and capture stress-induced changes in neural variability, offering insight into
dynamic cognitive states.

</details>


### [126] [Mutli-Level Autoencoder: Deep Learning Based Channel Coding and Modulation](https://arxiv.org/abs/2506.23511)
*Ahmad Abdel-Qader,Anas Chaaban,Mohamed S. Shehata*

Main category: eess.SP

TL;DR: 提出一种基于深度学习的卷积自编码器，用于信道编码和调制，能够适应不同信噪比，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 开发一种自适应方案，适用于多种信噪比，并改进现有AI编码器/解码器框架中仅测试少量代码的局限性。

Method: 采用多级编码和解码方法，将消息分块处理，每层编码/解码块独立处理B位，可测试所有可能的码字。

Result: 与经典极性码和TurboAE-MOD方案相比，表现出更高的可靠性，甚至在某些设置中优于现有方法。

Conclusion: 该架构灵活高效，能通过选择性移除编码/解码层来适应不同信噪比，适用于实际无线通信场景。

Abstract: In this paper, we design a deep learning-based convolutional autoencoder for
channel coding and modulation. The objective is to develop an adaptive scheme
capable of operating at various signal-to-noise ratios (SNR)s without the need
for re-training. Additionally, the proposed framework allows validation by
testing all possible codes in the codebook, as opposed to previous AI-based
encoder/decoder frameworks which relied on testing only a small subset of the
available codes. This limitation in earlier methods often led to unreliable
conclusions when generalized to larger codebooks. In contrast to previous
methods, our multi-level encoding and decoding approach splits the message into
blocks, where each encoder block processes a distinct group of $B$ bits. By
doing so, the proposed scheme can exhaustively test $2^{B}$ possible codewords
for each encoder/decoder level, constituting a layer of the overall scheme. The
proposed model was compared to classical polar codes and TurboAE-MOD schemes,
showing improved reliability with achieving comparable, or even superior
results in some settings. Notably, the architecture can adapt to different SNRs
by selectively removing one of the encoder/decoder layers without re-training,
thus demonstrating flexibility and efficiency in practical wireless
communication scenarios.

</details>


### [127] [Continual Learning for Wireless Channel Prediction](https://arxiv.org/abs/2506.22471)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,John M. Cioffi*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern 5G/6G deployments routinely face cross-configuration handovers--users
traversing cells with different antenna layouts, carrier frequencies, and
scattering statistics--which inflate channel-prediction NMSE by $37.5\%$ on
average when models are naively fine-tuned. The proposed improvement frames
this mismatch as a continual-learning problem and benchmarks three adaptation
families: replay with loss-aware reservoirs, synaptic-importance
regularization, and memory-free learning-without-forgetting. Across three
representative 3GPP urban micro scenarios, the best replay and regularization
schemes cut the high-SNR error floor by up to 2~dB ($\approx 35\%$), while even
the lightweight distillation recovers up to $30\%$ improvement over baseline
handover prediction schemes. These results show that targeted rehearsal and
parameter anchoring are essential for handover-robust CSI prediction and
suggest a clear migration path for embedding continual-learning hooks into
current channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can
be found at
https://github.com/ahmd-mohsin/continual-learning-channel-prediction.git.

</details>


### [128] [Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band](https://arxiv.org/abs/2506.22844)
*Navid Keshtiarast,Marina Petrova*

Main category: eess.SP

TL;DR: 论文研究了Wi-Fi 6E和5G NR-U在6 GHz频段密集住宅场景下的共存性能，通过模拟分析了吞吐量并探讨了关键参数的调整策略。


<details>
  <summary>Details</summary>
Motivation: 随着6 GHz频段开放用于非许可用途，需要研究Wi-Fi 6E和5G NR-U在密集高干扰环境下的公平共存策略以支持QoS敏感应用。

Method: 通过广泛的模拟实验，分析Wi-Fi 6E和5G NR-U在下行吞吐量及关键参数（如MAC帧聚合、能量检测阈值和MCOT）对共存的影响。

Result: 研究结果为设计公平共存策略提供了关键参数的优化建议，以提升高干扰环境下的网络性能。

Conclusion: 通过合理调整参数，可以实现Wi-Fi 6E和5G NR-U在6 GHz频段的公平高效共存。

Abstract: The ever-increasing demand for broadband and IoT wireless connectivity has
recently urged the regulators around the world to start opening the 6 GHz
spectrum for unlicensed use. These bands will, for example, permit the use of
additional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access
technologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To
support QoS-sensitive applications with both technologies, fair and efficient
coexistence approaches between the two RATs, as well as with incumbents already
operating in the 6 GHz band, are crucial. In this paper, we study through
extensive simulations the achievable mean downlink throughput of both Wi-Fi 6E
APs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario
under high-interference conditions. We also explore how different parameter
settings e.g., MAC frame aggregation, energy detection threshold and maximum
channel occupancy time (MCOT) affect the coexistence. Our findings give
important insights into how to tune the key parameters to design fair
coexistence policies.

</details>


### [129] [E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks](https://arxiv.org/abs/2506.23788)
*Naomi Stricker,David Blaser,Andres Gomez,Lothar Thiele*

Main category: eess.SP

TL;DR: 论文提出了一种名为E-WAN的协议，用于能量收集的广域低功耗网络，结合了多跳和单跳通信的优势，以动态适应网络状态和资源变化。


<details>
  <summary>Details</summary>
Motivation: 分布式嵌入式系统（如IoT、WSN和CPS）依赖无线通信，但传统的单跳和多跳通信方式各有缺点，且能量收集网络面临能量供应不稳定的挑战。

Method: E-WAN协议通过虚拟子网络的概念，动态切换多跳和单跳通信方式，仅基于易获取的网络状态信息自主调整。

Result: E-WAN在多种通信和能量收集场景下表现出高效性和适应性，并在真实室内环境中验证了其可行性。

Conclusion: E-WAN协议为能量收集网络提供了一种灵活且高效的解决方案，能够适应动态变化的网络需求和资源。

Abstract: The ever-increasing number of distributed embedded systems in the context of
the Internet of Things (IoT), Wireless Sensor Networks (WSN), and
Cyber-Physical Systems (CPS) rely on wireless communication to collect and
exchange data. Nodes can employ single-hop communication which, despite its
ease, may necessitate energy-intensive long-range communication to cover long
distances. Conversely, multi-hop communication allows for more energy-efficient
short-range communication since nodes can rely on other nodes to forward their
data. Yet, this approach requires relay nodes to be available and continuous
maintenance of a dynamically changing distributed state. At the same time,
energy harvesting has the potential to outperform traditional battery-based
systems by improving their lifetime, scalability with lower maintenance costs,
and environmental impact. However, the limited and temporally and spatially
variable harvested energy poses significant challenges for networking in energy
harvesting networks, particularly considering the energy demands and
characteristics of both multi-hop and single-hop communication. We propose
E-WAN, a protocol for energy harvesting wide-area low-power networks that
builds on the concept of \emph{virtual sub-networks} to enable
resource-efficient multi-hop communication when possible and reliable however
energy-intensive point-to-point communication otherwise. Nodes autonomously and
dynamically move between the two and adjust to changing network states and
resources based only on easily obtainable network state information. We
illustrate E-WAN's advantages both in terms of efficiency and adaptability in
various communication and harvesting scenarios. Furthermore, we demonstrate
E-WAN operating in a realistic setting by deploying an energy harvesting
network in a real-world indoor environment.

</details>


### [130] [Privacy-aware IoT Fall Detection Services For Aging in Place](https://arxiv.org/abs/2506.22462)
*Abdallah Lakhdari,Jiajie Li,Amani Abusafia,Athman Bouguettaya*

Main category: eess.SP

TL;DR: 本文提出一种基于物联网的跌倒检测服务框架（FDaaS），通过UWB雷达传感器和FD-GPT解决数据稀缺和隐私问题，准确率达90.72%。


<details>
  <summary>Details</summary>
Motivation: 随着老年人口增长至2050年的21亿，跌倒检测至关重要。传统方法存在数据稀缺和隐私问题。

Method: 设计了一个服务导向架构，结合UWB雷达和FD-GPT模型，并通过数据增强技术解决数据稀缺问题。

Result: 实验结果显示，该方法在区分跌倒事件和日常活动时达到了90.72%的准确率和89.33%的精确率。

Conclusion: FDaaS框架有效解决了跌倒检测中的数据稀缺和隐私问题，表现出高准确性和实用性。

Abstract: Fall detection is critical to support the growing elderly population,
projected to reach 2.1 billion by 2050. However, existing methods often face
data scarcity challenges or compromise privacy. We propose a novel IoT-based
Fall Detection as a Service (FDaaS) framework to assist the elderly in living
independently and safely by accurately detecting falls. We design a
service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors
as an IoT health-sensing service, ensuring privacy and minimal intrusion. We
address the challenges of data scarcity by utilizing a Fall Detection
Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.
We developed a protocol to collect a comprehensive dataset of the elderly daily
activities and fall events. This resulted in a real dataset that carefully
mimics the elderly's routine. We rigorously evaluate and compare various models
using this dataset. Experimental results show our approach achieves 90.72%
accuracy and 89.33% precision in distinguishing between fall events and regular
activities of daily living.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [131] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [132] [CoreMark: Toward Robust and Universal Text Watermarking Technique](https://arxiv.org/abs/2506.23066)
*Jiale Meng,Yiming Li,Zheming Lu,Zewei He,Hao Luo,Tianwei Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的文本水印嵌入范式CORE，并基于此设计了CoreMark框架，通过动态提取和调整CORE厚度嵌入数据，实现了优异的通用性和抗干扰性。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方案在同时实现鲁棒性、通用性和不可感知性方面仍面临挑战，因此需要一种新方法来解决这些问题。

Method: 提出CORE嵌入范式，并通过CoreMark框架动态提取CORE、选择鲁棒字符、调整厚度嵌入数据，同时设计嵌入强度调制器以适应小字体。

Result: 实验表明，CoreMark在多语言和字体中表现优异，显著提升了抗截图、打印扫描和打印相机攻击的能力。

Conclusion: CoreMark在鲁棒性、通用性和不可感知性方面均优于现有方法，为文本水印提供了新的解决方案。

Abstract: Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking framework named CoreMark. Specifically,
CoreMark first dynamically extracts COREs from characters. Then, the characters
with stronger robustness are selected according to the lengths of COREs. By
modifying the thickness of the CORE, the hidden data is embedded into the
selected characters without causing significant visual distortions. Moreover, a
general plug-and-play embedding strength modulator is proposed, which can
adaptively enhance the robustness for small font sizes by adjusting the
embedding strength according to the font size. Experimental evaluation
indicates that CoreMark demonstrates outstanding generalizability across
multiple languages and fonts. Compared to existing methods, CoreMark achieves
significant improvements in resisting screenshot, print-scan, and print camera
attacks, while maintaining satisfactory imperceptibility.

</details>


### [133] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: MEMFOF是一种高效的GPU内存光学流估计方法，通过多帧设计和优化架构，在保持高性能的同时显著降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率（如FullHD）输入下光学流估计的内存消耗问题，提供一种高效的替代方案。

Method: 结合RAFT-like架构的优化，使用减少的相关体积和高分辨率训练协议，实现多帧估计。

Result: 在多个基准测试中表现优异，内存消耗显著降低，性能优于现有方法。

Conclusion: MEMFOF在高分辨率光学流估计中实现了性能与内存效率的平衡，具有显著的实用价值。

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [134] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [135] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出了通过结合隐式解码器和改进的损失函数，解决神经细胞自动机(NCA)在高分辨率下训练和推理的限制，实现了高效、高质量的全高清实时输出。


<details>
  <summary>Details</summary>
Motivation: NCA在低分辨率网格中表现良好，但在高分辨率下因计算资源、信息传播限制和实时推理需求而受限。本文旨在突破这些限制。

Method: 通过引入共享隐式解码器，将NCA在粗网格上的结果解码为任意分辨率；同时设计了适用于高分辨率任务的损失函数。

Result: 所提方法显著提升了NCA在高分辨率下的质量、效率和性能，实现了实时全高清输出，并保持了自组织和涌现特性。

Conclusion: 结合隐式解码器的NCA方案在多任务和多场景中均表现出色，为解决高分辨率输出问题提供了有效途径。

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [136] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/abs/2506.22907)
*Yunzhe Shao,Xinyu Yi,Lu Yin,Shihui Guo,Junhai Yong,Feng Xu*

Main category: cs.CV

TL;DR: MagShield提出了一种新颖的“检测-校正”方法，用于解决稀疏惯性动作捕捉系统中的磁场干扰问题，显著提高了在磁场干扰环境下的动作捕捉精度。


<details>
  <summary>Details</summary>
Motivation: 现有的惯性测量单元（IMU）系统在磁场干扰环境下容易出现方向估计误差，限制了其在实际场景中的应用。

Method: MagShield采用“检测-校正”策略，通过多IMU联合分析检测磁场干扰，并利用人体运动先验校正方向误差。

Result: 实验表明，MagShield显著提高了在磁场干扰下的动作捕捉精度，且具有良好的兼容性。

Conclusion: MagShield为稀疏惯性动作捕捉系统提供了一种有效的磁场干扰解决方案，具有广泛的应用潜力。

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [137] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS是一个统一的神经表面重建框架，解决了多视角辐射不一致、无纹理区域关键点缺失以及Eikonal约束过度导致的结构退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景下难以同时实现几何保真和光度一致性，HiNeuS旨在通过统一管道解决这些问题。

Method: 采用差异可见性验证、平面共形正则化和物理基础的Eikonal松弛，实现外观与几何约束的协同优化。

Result: 在合成和真实数据集上达到了最先进性能，显著减少了Chamfer距离并提升了PSNR。

Conclusion: HiNeuS在恢复高光物体、城市布局和低纹理表面方面表现出色，并适用于逆向渲染任务。

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


### [138] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [139] [Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data](https://arxiv.org/abs/2506.24039)
*Shubhabrata Mukherjee,Jack Lang,Obeen Kwon,Iryna Zenyuk,Valerie Brogden,Adam Weber,Daniela Ushizima*

Main category: cs.CV

TL;DR: Zenesis是一个无需代码的交互式平台，通过轻量级多模态适应技术解决科学图像数据稀缺问题，显著提升了FIB-SEM数据的分析性能。


<details>
  <summary>Details</summary>
Motivation: 传统零样本和提示技术在处理稀缺的科学图像数据时表现不佳，需要一种低门槛的解决方案。

Method: 开发轻量级多模态适应技术，支持零样本操作，并结合人工干预和启发式时间增强。

Result: 在FIB-SEM数据上，Zenesis平均准确率达到0.947（非晶催化剂）和0.987（晶体），显著优于基线方法。

Conclusion: Zenesis是科学图像分析的强大工具，尤其在高质量标注数据稀缺的领域中表现突出。

Abstract: Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with human-in-the-loop refinement and
heuristic-based temporal enhancement options. We demonstrate the performance of
our approach through comprehensive comparison and validation on challenging
Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded
membranes. Zenesis significantly outperforms baseline methods, achieving an
average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a
Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an
IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results
mark a substantial improvement over traditional methods like Otsu thresholding
and even advanced models like Segment Anything Model (SAM) when used in
isolation. Our results demonstrate that Zenesis is a powerful tool for
scientific applications, particularly in fields where high-quality annotated
datasets are unavailable, accelerating accurate analysis of experimental
imaging.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [140] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 本文提出了一种任务无关的对比预训练方法（RDL），通过多级对比目标学习关系数据库的可迁移表示。


<details>
  <summary>Details</summary>
Motivation: 现有RDL模型依赖任务特定的监督学习，可扩展性和复用性受限。

Method: 提出三种对比目标（行级、链接级、上下文级），并采用模块化RDL架构和高效采样策略。

Result: 初步实验表明，基于预训练模型的微调优于从头训练，验证了方法的有效性。

Conclusion: 提出的方法为关系数据学习可迁移表示提供了新方向。

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [141] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: BEST-Route框架通过动态选择模型和生成多响应，在保持性能的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统LLM查询路由方法因生成单响应而过度依赖昂贵大型模型，导致成本高，而小型模型的多响应选择可提升质量。

Method: 提出BEST-Route框架，根据查询难度和质量阈值动态选择模型和生成响应数量。

Result: 实验显示，该方法降低成本60%且性能下降小于1%。

Conclusion: BEST-Route在成本与性能间实现高效平衡。

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [142] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: 论文提出了一种名为渐进精度更新（P²U）的方法，通过在带宽受限环境中传输低精度模型及其与高精度模型的差异更新，实现了精度、带宽和延迟的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限环境下，高效模型分布变得越来越重要。为了解决这一问题，论文提出了P²U方法。

Method: P²U传输低精度模型及与高精度模型的差异更新，而非原始高精度模型。支持多种压缩技术（如量化、稀疏化）。

Result: 实验表明，P²U在不同模型规模和数据集中均表现优异，且在带宽或启动时间优先时，激进量化（如4位）不会显著影响性能。

Conclusion: P²U是一种高效且实用的模型分布解决方案，适用于联邦学习、边缘计算和物联网部署等低资源环境。

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [143] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr,Yanen Li,Philip Tee*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLM）在逻辑推理和可解释性方面的局限性，并提出了一种结合逻辑推理模块的神经符号方法，通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM依赖概率推理，在需要严格逻辑推理和可解释性的领域表现受限，因此需要增强其能力。

Method: 采用神经符号方法，将LLM与基于Prolog谓词和可组合工具的逻辑推理模块结合，通过一阶逻辑和显式规则系统分解复杂任务。

Result: 在DABStep基准测试中，该框架提高了多步推理任务的精度和覆盖率，同时减少了幻觉和错误分解。

Conclusion: 结合逻辑推理模块的LLM能增强系统可靠性和可解释性，为复杂领域的可信AI提供了可扩展的路径。

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [144] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: 该论文研究了生成个性化中的语义折叠问题，提出了无需训练的方法来调整嵌入向量的方向和大小，显著改善了文本-图像对齐效果。


<details>
  <summary>Details</summary>
Motivation: 生成个性化中的视觉概念（$V^*$）会逐渐偏离原始文本含义，导致多概念输入提示的语义丰富性降低，输出图像简化。这一问题的根源是无约束的优化。

Method: 提出了一种简单有效的无需训练方法，在推理时调整预训练嵌入向量的大小和方向，以减轻语义折叠问题。

Result: 该方法适用于多种个性化方法，在多样化用例中显著提升了文本-图像对齐效果。

Conclusion: 该方法通过调整嵌入向量解决了语义折叠问题，为生成个性化提供了改进方向。

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [145] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Main category: cs.LG

TL;DR: 摘要提出了一种名为DistShap的并行算法，用于解决图神经网络(GNN)预测解释中计算成本高的问题，并通过多GPU实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 随着GNN的广泛应用，解释其预测的重要性日益凸显，但现有的方法在计算上非常昂贵。

Method: DistShap通过在多GPU环境下采样子图、并行执行GNN推断，并求解分布式最小二乘问题来计算边的重要性分数。

Result: DistShap在准确性上优于现有大多数GNN解释方法，并首次支持具有数百万特征的GNN模型，扩展到128个GPU。

Conclusion: DistShap为大规模GNN预测解释提供了一种高效且可扩展的解决方案。

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [146] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon,Bongjun Choi*

Main category: cs.LG

TL;DR: 联邦学习(FL)用于分布式场景，保护用户隐私的同时训练AI模型。本文提出基于参考模型的优化方法，解决模型性能不足和用户需求多样化的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护方面高效，但模型性能可能不足，且用户需求多样化难以满足。

Method: 提出基于参考模型的联邦学习方法，结合贝叶斯参数高效迁移学习，通过优化近端项避免每轮训练的灾难性遗忘。

Result: 该方法在提高模型性能的同时降低了计算成本。

Conclusion: 基于参考模型的联邦学习方法有效解决了性能优化和灾难性遗忘问题。

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [147] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng,Xiaofei Xie,Renzhi Wang,Yuan Zhou,Ming Hu*

Main category: cs.LG

TL;DR: ADReFT是一种新型的自适应决策修复方法，通过离线学习和在线修复提升自动驾驶系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有在线修复方法缺乏普适性和适应性，导致修复效果不佳，因此需要一种更有效的修复方法。

Method: ADReFT结合了Transformer模型，通过监督学习和强化学习训练，生成自适应修复动作。

Result: 评估结果显示，ADReFT在修复性能上表现更好。

Conclusion: ADReFT能够有效提升自动驾驶系统的安全性和修复效果。

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [148] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: RL-Net结合规则模型与神经网络，在保持高性能（93.03% F1）的同时提高了可解释性，是透明性与性能的实用折中方案。


<details>
  <summary>Details</summary>
Motivation: 解决规则模型在复杂数据上表现不足及深度学习缺乏可解释性的问题。

Method: 提出RL-Net，通过神经优化学习可解释的规则列表，并首次应用于雷达手势识别。

Result: RL-Net在高性能和规则简洁性间取得平衡，优于全透明和黑盒模型。

Conclusion: 

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [149] [ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge](https://arxiv.org/abs/2506.22790)
*Yixu Chen,Bowen Chen,Hai Wei,Alan C. Bovik,Baojun Li,Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Dounia Hammou,Fei Yin,Rafal Mantiuk,Amritha Premkumar,Prajit T Rajendran,Vignesh V Menon*

Main category: eess.IV

TL;DR: 该论文报告了2025年IEEE ICME关于HDR和SDR视频质量测量的挑战赛，旨在推动能同时处理HDR和SDR内容的视频质量评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频技术的发展，尤其是HDR和SDR内容的普及，现有VQA模型在动态范围和失真类型多样化时表现不佳，亟需更通用的评估方法。

Method: 通过挑战赛形式征集并评估VQA模型，分为全参考（FR）和无参考（NR）赛道，最终有五支团队提交七种模型。

Result: 四种方法超越VMAF基准，其中最优模型达到最新性能，为通用视频质量评估树立了新标杆。

Conclusion: 挑战赛成功推动了能同时处理HDR和SDR内容的VQA方法的发展，并展示了其技术潜力。

Abstract: This paper reports IEEE International Conference on Multimedia \& Expo (ICME)
2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.
With the rapid development of video technology, especially High Dynamic Range
(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and
generalizable Video Quality Assessment (VQA) methods has become increasingly
demanded. Existing VQA models often struggle to deliver consistent performance
across varying dynamic ranges, distortion types, and diverse content. This
challenge was established to benchmark and promote VQA approaches capable of
jointly handling HDR and SDR content. In the final evaluation phase, five teams
submitted seven models along with technical reports to the Full Reference (FR)
and No Reference (NR) tracks. Among them, four methods outperformed VMAF
baseline, while the top-performing model achieved state-of-the-art performance,
setting a new benchmark for generalizable video quality assessment.

</details>


### [150] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/abs/2506.23721)
*Gijs Luijten,Roberto Maria Scardigno,Lisle Faray de Paiva,Peter Hoyer,Jens Kleesiek,Domenico Buongiorno,Vitoantonio Bevilacqua,Jan Egger*

Main category: eess.IV

TL;DR: 论文提出了一种结合深度学习和增强现实的超声系统，用于实时自动肾脏体积测量，减轻医生负担并提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决超声影像学习曲线陡峭、医生需频繁切换注意力的问题，提升临床评估效率。

Method: 结合深度学习的语义分割和增强现实技术，通过HoloLens-2实现两种超声管道，评估实时可行性和准确性。

Result: 使用开源数据集和模型验证了系统的实时性和准确性，提供了开源代码支持临床应用。

Conclusion: 该技术能显著改善超声训练的效率和诊断准确性，特别适合即时诊疗场景。

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [151] [Universal Gluing and Contextual Choice: Categorical Logic and the Foundations of Analytic Approximation](https://arxiv.org/abs/2506.22693)
*Andreu Ballus Santacana*

Main category: math.FA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a new categorical and constructive foundation for analytic
approximation based on a Contextual Choice Principle (CCP), which enforces
locality and compatibility in the construction of mathematical objects. Central
to our approach is the Universal Embedding and Linear Approximation Theorem
(UELAT), which establishes that functions in broad spaces -- including C(K),
Sobolev spaces W^{k,p}(Omega), and distributions D'(Omega) -- can be explicitly
approximated by finite-rank linear projections, each with a constructive,
algorithmically verifiable certificate of accuracy.
  These constructions are governed categorically by a functorial adjunction
between local logical probes and analytic models, making analytic existence
both formally certifiable and programmatically extractable. As a key result, we
prove a uniform certificate stability theorem, ensuring that approximation
certificates persist under uniform convergence.
  The CCP avoids classical pathologies (e.g., non-measurable sets,
Banach--Tarski paradoxes) by eliminating non-constructive choice and replacing
it with a coherent, local-to-global semantic logic. Our framework strengthens
the foundations of constructive analysis while contributing tools relevant to
formal verification, type-theoretic proof systems, and computational
mathematics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [152] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: ContextCache是一种上下文感知的语义缓存系统，针对多轮对话设计，通过两阶段检索架构提升缓存命中准确性，显著降低延迟和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存系统仅基于单查询匹配，无法识别多轮对话上下文，导致在不同对话场景中出现错误的缓存命中。

Method: 采用两阶段检索架构：首先执行向量检索寻找潜在匹配，然后通过自注意力机制整合当前和历史对话表征进行精确匹配。

Result: ContextCache在真实对话中提高了精确率和召回率，缓存响应延迟比直接调用LLM低约10倍。

Conclusion: ContextCache显著提升了多轮对话中语义缓存的效率和准确性，适用于需要降低LLM计算成本的应用场景。

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [153] [Segmented Operations using Matrix Multiplications](https://arxiv.org/abs/2506.23906)
*Aleksandros Sobczyk,Giuseppe Sorrentino,Anastasios Zouzias*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Specialized computational units that perform small matrix multiplications as
primitive operations are typically present in modern accelerators. However,
these units are often underutilized for many fundamental operations besides
dense matrix multiplications. The analysis of algorithms for such architectures
is currently stagnated due to the lack of a rigorous theoretical model of
computation that captures their characteristics. In this work, we propose
MMV-RAM, a computational model tailored to matrix multiplication accelerators.
MMV-RAM judiciously extends the Vector-RAM model with an additional processing
unit that multiplies two matrices of sizes $n\times s$ and $s\times s$ in a
single parallel step, where $s$ is a model parameter. We provide a detailed
theoretical analysis of the model, and carefully balance the computational
power between the matrix and vector units, guided by the circuit complexity
lower bound that parity is not in AC[0].
  In MMV-RAM, we study algorithms for segmented scan and sum, two fundamental
parallel primitives. We propose a segmented scan algorithm that uses matrix
multiplications to perform speculative block-scan computations, which runs in
$O(\log_s(n))$ steps. In contrast, we show that any algorithm that uses only
the vector unit of MMV-RAM requires
$\Omega\left(\frac{\log_2(n)}{\log_2\log_2(n)}\right)$ steps. We further apply
these techniques to obtain similar theoretical speedups for element-wise vector
multiplication and matrix multiplication. Beyond the worst-case complexity
analysis, we propose algorithms for segmented operations that could lead to
highly efficient and pragmatic implementations. For example, we observe that
segmented sum is a combination of three elementary parallel primitives: scan,
compress, and vector differentiation. As a case study, we implement...

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [154] [Spatial QUBO: Convolutional Formulation of Large-Scale Binary Optimization with Dense Interactions](https://arxiv.org/abs/2506.24008)
*Hiroshi Yamashita,Hideyuki Suzuki*

Main category: cond-mat.dis-nn

TL;DR: 论文探讨了空间光子伊辛机（SPIM）在解决大规模组合优化问题中的潜力，并提出了空间QUBO（spQUBO）方法以提高其适用性与效率。通过卷积结构，spQUBO可在SPIM上高效实现，且适用于多种实际问题如布局和聚类问题。


<details>
  <summary>Details</summary>
Motivation: 随着SPIM在解决高秩交互问题时效率不足，研究旨在明确其内在表示能力，并提出无需多路复用的高效方法。

Method: 提出spQUBO方法，通过卷积结构降低问题维度，并证明其在SPIM上的高效实现能力。

Result: spQUBO不仅保留了卷积结构，还能高效应用于实际问题（如布局和聚类问题），并支持FFT加速计算。

Conclusion: 研究深化了对SPIM适用性问题类的理解，spQUBO的卷积结构为组合优化提供了高效解决方案。

Abstract: The spatial photonic Ising machine (SPIM) is a promising optical hardware
solver for large-scale combinatorial optimization problems with dense
interactions. As the SPIM can represent Ising problems with rank-one coupling
matrices, multiplexed versions have been proposed to enhance the applicability
to higher-rank interactions. However, the multiplexing cost reduces the
implementation efficiency, and even without multiplexing, the SPIM is known to
represent coupling matrices beyond rank-one. In this paper, to clarify the
intrinsic representation power of the original SPIM, we propose spatial QUBO
(spQUBO), a formulation of Ising problems with spatially convolutional
structures. We prove that any spQUBO reduces to a two-dimensional spQUBO, with
the convolutional structure preserved, and that any two-dimensional spQUBO can
be efficiently implemented on the SPIM without multiplexing. We further
demonstrate its practical applicability to distance-based combinatorial
optimization, such as placement problems and clustering problems. These results
advance our understanding of the class of optimization problems where SPIMs
exhibit superior efficiency and scalability. Furthermore, spQUBO's efficiency
is not limited to the SPIM architecture; we show that its convolutional
structure allows efficient computation using Fast Fourier Transforms (FFT).

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [155] [Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction](https://arxiv.org/abs/2506.23836)
*Alexander Tyurin*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider centralized distributed optimization in the classical federated
learning setup, where $n$ workers jointly find an $\varepsilon$-stationary
point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access
only to unbiased stochastic gradients with variance $\sigma^2$. Each worker
requires at most $h$ seconds to compute a stochastic gradient, and the
communication times from the server to the workers and from the workers to the
server are $\tau_{s}$ and $\tau_{w}$ seconds per coordinate, respectively. One
of the main motivations for distributed optimization is to achieve scalability
with respect to $n$. For instance, it is well known that the distributed
version of SGD has a variance-dependent runtime term $\frac{h \sigma^2 L
\Delta}{n \varepsilon^2},$ which improves with the number of workers $n,$ where
$\Delta = f(x^0) - f^*,$ and $x^0 \in R^d$ is the starting point. Similarly,
using unbiased sparsification compressors, it is possible to reduce both the
variance-dependent runtime term and the communication runtime term. However,
once we account for the communication from the server to the workers
$\tau_{s}$, we prove that it becomes infeasible to design a method using
unbiased random sparsification compressors that scales both the server-side
communication runtime term $\tau_{s} d \frac{L \Delta}{\varepsilon}$ and the
variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{\varepsilon^2},$
better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case,
where all workers access the same distribution. To establish this result, we
construct a new "worst-case" function and develop a new lower bound framework
that reduces the analysis to the concentration of a random sum, for which we
prove a concentration bound. These results reveal fundamental limitations in
scaling distributed optimization, even under the homogeneous assumption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [156] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: BayesL 是一种用于贝叶斯网络的逻辑框架，支持查询和验证，无需手动修改模型即可进行全面的假设场景评估。


<details>
  <summary>Details</summary>
Motivation: 为了简化贝叶斯网络的查询和验证过程，支持灵活的因果和证据推理。

Method: 开发了结构化语言 BayesL，支持创建查询和进行假设评估。

Result: BayesL 能够高效地执行复杂的推理任务，无需手动调整模型。

Conclusion: BayesL 提供了一种强大的工具，提升了贝叶斯网络的实用性和灵活性。

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [157] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 该论文探讨了人工智能基础模型的社会影响与治理，通过概念框架、实证分析和政策行动三个主题，为AI时代的社会治理提供科学基础。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽前景广阔，但因其理解的不足和潜在危害，需要研究AI技术与社会如何共同演化。

Method: 论文围绕三大主题展开：概念框架（能力、风险和供应链）、实证分析（模型评估和组织透明度）以及政策行动（基于证据的AI政策）。

Result: 为AI治理提供了科学基础和研究-政策接口，推动了更好的社会成果。

Conclusion: 论文通过多角度研究，为AI时代的治理奠定了科学基础，促进了技术与社会的协调发展。

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [158] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: 研究提出了一种基于AI的框架，处理难民儿童的心理健康数据，并比较了两种RAG模型在避免幻觉风险下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决国际难民危机中儿童的心理健康问题，提供可扩展的策略帮助相关机构和决策者。

Method: 比较了Zephyr-7B-beta和DeepSeek R1-7B两种RAG模型在处理难民健康数据时的表现。

Result: 两个模型均有效，但DeepSeek R1在答案相关性上表现更优（准确率0.91）。

Conclusion: AI方法结合心理学研究可为难民儿童心理健康提供有效支持，DeepSeek R1更适合此类任务。

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [159] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: 研究结合自然语言和拖放界面编程机器人任务的效果，发现大语言模型能生成更接近人类操作的动作序列，但小模型表现也令人满意。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合自然语言和拖放界面的优势，为机器人任务提供更直观且精确的编程方式。

Method: 构建基于大语言模型的流程，将自然语言输入转换为接近人类操作的动作序列，并与手工指定的动作序列进行比较。

Result: 大模型生成的动作序列更接近人类操作，但小模型表现也令人满意。

Conclusion: 结合自然语言和拖放界面的混合方法可行，且模型规模对性能有显著影响。

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [160] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: 本文探讨了人工智能（AI）在企业管理中的潜力，重点关注通过AI驱动的代理提升企业决策效率。作者提出了六大原则，强调从AI中心化转向用户中心化的AI范式。


<details>
  <summary>Details</summary>
Motivation: AI对个人、社会、健康、教育和职业等领域具有广泛影响。在企业管理中，决策是一个关键且频繁的任务，AI驱动的代理有望提升决策效率。

Method: 通过分析当前AI中心化用户范式的不足，作者提出了六大原则，旨在推动用户中心化的AI设计。

Result: 研究强调了市场机制的作用，以平台为基础，将AI设计和代理交付与企业用户需求对齐。

Conclusion: 向用户中心化AI的转变有助于更好地满足企业决策需求，提升AI在企业管理中的实用性。

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [161] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 论文提出了一种名为Coordination Transformers（CooT）的新型框架，通过快速适应未见过的伙伴来改善多智能体系统中的协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如自我对弈和基于群体的方法在未见过的伙伴上泛化能力差或需要大量训练。因此，提出了CooT来解决这些问题。

Method: CooT利用最近的交互历史，通过预测与观察到的伙伴行为一致的动作来快速适应新伙伴，无需显式监督或微调。

Result: 在Overcooked基准测试中，CooT显著优于基线方法。人类评估也确认了CooT是最有效的协作伙伴。

Conclusion: CooT在多智能体场景中表现出鲁棒性、灵活性和对上下文的敏感性，为解决协调问题提供了新思路。

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [162] [Lock Prediction for Zero-Downtime Database Encryption](https://arxiv.org/abs/2506.23985)
*Mohamed Sami Rakha,Adam Sorrenti,Greg Stager,Walid Rjaibi,Andriy Miranskyy*

Main category: cs.CR

TL;DR: 论文提出了一种基于深度学习的预测方法，以实现在线数据库加密，避免停机和高存储开销，并展示了其在锁序列预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据库加密技术需要停机和高存储开销，难以在高吞吐环境中实现在线加密，因此研究需要一种无干扰的在线加密解决方案。

Method: 利用深度学习模型（如Transformer和LSTM）预测数据库锁序列，使用TPC-C基准测试的数据集进行训练和评估。

Result: 深度学习模型在表级和页级锁预测中分别达到49%和66%的平均准确率，优于朴素基线。

Conclusion: 该研究为在线加密提供了可行路径，通过预测锁序列减少了加密对数据库性能的影响。

Abstract: Modern enterprise database systems face significant challenges in balancing
data security and performance. Ensuring robust encryption for sensitive
information is critical for systems' compliance with security standards.
Although holistic database encryption provides strong protection, existing
database systems often require a complete backup and restore cycle, resulting
in prolonged downtime and increased storage usage. This makes it difficult to
implement online encryption techniques in high-throughput environments without
disrupting critical operations.
  To address this challenge, we envision a solution that enables online
database encryption aligned with system activity, eliminating the need for
downtime, storage overhead, or full-database reprocessing. Central to this
vision is the ability to predict which parts of the database will be accessed
next, allowing encryption to be applied online. As a step towards this
solution, this study proposes a predictive approach that leverages deep
learning models to forecast database lock sequences, using IBM Db2 as the
database system under study. In this study, we collected a specialized dataset
from TPC-C benchmark workloads, leveraging lock event logs for model training
and evaluation. We applied deep learning architectures, such as Transformer and
LSTM, to evaluate models for various table-level and page-level lock
predictions. We benchmark the accuracy of the trained models versus a Naive
Baseline across different prediction horizons and timelines.
  The study experiments demonstrate that the proposed deep learning-based
models achieve up to 49% average accuracy for table-level and 66% for
page-level predictions, outperforming a Naive Baseline. By anticipating which
tables and pages will be locked next, the proposed approach is a step toward
online encryption, offering a practical path toward secure, low-overhead
database systems.

</details>


### [163] [Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?](https://arxiv.org/abs/2506.23682)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: 研究分析了开发者在将软件移植到CHERI架构时遇到的挑战，主要问题集中在警告/错误信息的展示和文档不足。


<details>
  <summary>Details</summary>
Motivation: CHERI架构通过硬件层面的改动提升了内存安全性，但其对C语言基本假设的调整可能对开发者造成困扰，因此研究开发者体验很有必要。

Method: 通过用户研究（usability study）观察开发者在CHERI上移植软件时的反应和困难。

Result: 开发者普遍对CHERI的警告/错误信息展示方式感到困惑，且认为文档多样性不足影响开发效率。

Conclusion: CHERI架构的改进需要更友好的开发工具支持和更全面的文档，以降低开发者学习曲线。

Abstract: A digital security-by-design computer architecture, like CHERI, lets you
program without fear of buffer overflows or other memory safety errors, but
CHERI also rewrites some of the assumptions about how C works and how
fundamental types (such as pointers) are implemented in hardware. We conducted
a usability study to examine how developers react to the changes required by
CHERI when porting software to run on it. We find that developers struggle with
CHERI's display of warnings and errors and a lack of diverse documentation.

</details>


### [164] [Detect \& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2506.23583)
*Marvin Xhemrishi,Alexandre Graell i Amat,Balázs Pejó*

Main category: cs.CR

TL;DR: 结合QI和FedGT的优势，实现了更强大的恶意行为检测和准确的贡献评估。


<details>
  <summary>Details</summary>
Motivation: 安全聚合联邦学习虽然保护了客户隐私，但也增加了恶意行为检测和贡献评估的难度。

Method: 结合QI和FedGT的方法，同时优化恶意行为检测和贡献评估。

Result: 实验表明，结合后的方法比单独使用QI或FedGT表现更优。

Conclusion: 通过整合现有方法的优点，可以同时提升恶意行为检测和贡献评估的效果。

Abstract: Federated learning with secure aggregation enables private and collaborative
learning from decentralised data without leaking sensitive client information.
However, secure aggregation also complicates the detection of malicious client
behaviour and the evaluation of individual client contributions to the
learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et
al.) were proposed for contribution evaluation (CE) and misbehaviour detection
(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance
on the random selection of clients in each training round, while FedGT lacks
the CE ability. In this work, we combine the strengths of QI and FedGT to
achieve both robust MD and accurate CE. Our experiments demonstrate superior
performance compared to using either method independently.

</details>


### [165] [Threadbox: Sandboxing for Modular Security](https://arxiv.org/abs/2506.23683)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: Threadbox提出了一种新的沙盒机制，支持模块化和独立的沙盒，适用于线程和特定函数的沙盒化。


<details>
  <summary>Details</summary>
Motivation: 现有沙盒机制在某些应用中难以应用，需要开发者重构代码。

Method: 研究现有沙盒机制的挑战，提出Threadbox，支持模块化和独立沙盒。

Result: 通过案例研究展示了Threadbox的适用性，并讨论了其限制。

Conclusion: Threadbox提供了一种更灵活的沙盒化方案，适用于特定场景。

Abstract: There are many sandboxing mechanisms provided by operating systems to limit
what resources applications can access, however, sometimes the use of these
mechanisms requires developers to refactor their code to fit the sandboxing
model. In this work, we investigate what makes existing sandboxing mechanisms
challenging to apply to certain types of applications, and propose Threadbox, a
sandboxing mechanism that enables having modular and independent sandboxes, and
can be applied to threads and sandbox specific functions. We present case
studies to illustrate the applicability of the idea and discuss its
limitations.

</details>


### [166] [An ontological lens on attack trees: Toward adequacy and interoperability](https://arxiv.org/abs/2506.23841)
*Ítalo Oliveira,Stefano M. Nicoletti,Gal Engelberg,Mattia Fumagalli,Dan Klein,Giancarlo Guizzardi*

Main category: cs.CR

TL;DR: 本文分析了攻击树（AT）在安全分析中的局限性，并提出基于本体论的分析方法来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 攻击树是一种流行的安全分析形式，但其缺乏本体论基础，可能导致建模和分析的不一致性。

Method: 通过基于统一基础本体（UFO）的通用价值和风险本体（COVER），对攻击树进行本体论分析。

Result: 研究发现攻击树存在四个主要问题：术语模糊、本体论缺陷、建模指导不足以及语义互操作性差。

Conclusion: 研究为解决攻击树的局限性提供了本体论基础，并为更广泛的风险管理建模方法铺平了道路。

Abstract: Attack Trees (AT) are a popular formalism for security analysis. They are
meant to display an attacker's goal decomposed into attack steps needed to
achieve it and compute certain security metrics (e.g., attack cost,
probability, and damage). ATs offer three important services: (a) conceptual
modeling capabilities for representing security risk management scenarios, (b)
a qualitative assessment to find root causes and minimal conditions of
successful attacks, and (c) quantitative analyses via security metrics
computation under formal semantics, such as minimal time and cost among all
attacks. Still, the AT language presents limitations due to its lack of
ontological foundations, thus compromising associated services. Via an
ontological analysis grounded in the Common Ontology of Value and Risk (COVER)
-- a reference core ontology based on the Unified Foundational Ontology (UFO)
-- we investigate the ontological adequacy of AT and reveal four significant
shortcomings: (1) ambiguous syntactical terms that can be interpreted in
various ways; (2) ontological deficit concerning crucial domain-specific
concepts; (3) lacking modeling guidance to construct ATs decomposing a goal;
(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.
We also discuss existing incremental solutions and how our analysis paves the
way for overcoming those issues through a broader approach to risk management
modeling.

</details>


### [167] [Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions](https://arxiv.org/abs/2506.23866)
*Jason Kayembe,Iness Ben Guirat,Jan Tobias Mühlberg*

Main category: cs.CR

TL;DR: 研究探讨了基于云的办公解决方案在隐私、安全与环境可持续性上的交集，量化了用户和网络侧的能源使用与碳排放。假设隐私优先的服务比依赖数据收集和广告的服务更节能，提出了一评估框架，并验证了假设。


<details>
  <summary>Details</summary>
Motivation: 探索隐私保护服务在能源效率上的优势，为环境可持续性与数字服务的设计提供参考。

Method: 提出框架量化能源使用与网络流量，分析架构与商业模式对环境的影响，应用于三款主流邮件服务及自托管解决方案。

Result: 自托管方案最节能，比Gmail节能33%；Proton Mail在商业服务中最优，比Outlook节能0.1 gCO2e/会话。

Conclusion: 隐私优先设计可显著降低能源消耗，为可持续云服务提供方向。

Abstract: In this paper, we explore the intersection of privacy, security, and
environmental sustainability in cloud-based office solutions, focusing on
quantifying user- and network-side energy use and associated carbon emissions.
We hypothesise that privacy-focused services are typically more
energy-efficient than those funded through data collection and advertising. To
evaluate this, we propose a framework that systematically measures
environmental costs based on energy usage and network data traffic during
well-defined, automated usage scenarios. To test our hypothesis, we first
analyse how underlying architectures and business models, such as monetisation
through personalised advertising, contribute to the environmental footprint of
these services. We then explore existing methodologies and tools for software
environmental impact assessment. We apply our framework to three mainstream
email services selected to reflect different privacy policies, from
ad-supported tracking-intensive models to privacy-focused designs: Microsoft
Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a
self-hosted email solution, evaluated with and without end-to-end encryption.
We show that the self-hosted solution, even with 14% of device energy and 15%
of emissions overheads from PGP encryption, remains the most energy-efficient,
saving up to 33% of emissions per session compared to Gmail. Among commercial
providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per
session compared to Outlook, whose emissions can be further reduced by 2%
through ad-blocking.

</details>


### [168] [All Proof of Work But No Proof of Play](https://arxiv.org/abs/2506.23435)
*Hayder Tirmazi*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Speedrunning is a competition that emerged from communities of early video
games such as Doom (1993). Speedrunners try to finish a game in minimal time.
Provably verifying the authenticity of submitted speedruns is an open problem.
Traditionally, best-effort speedrun verification is conducted by on-site human
observers, forensic audio analysis, or a rigorous mathematical analysis of the
game mechanics. Such methods are tedious, fallible, and, perhaps worst of all,
not cryptographic. Motivated by naivety and the Dunning-Kruger effect, we
attempt to build a system that cryptographically proves the authenticity of
speedruns. This paper describes our attempted solutions and ways to circumvent
them. Through a narration of our failures, we attempt to demonstrate the
difficulty of authenticating live and interactive human input in untrusted
environments, as well as the limits of signature schemes, game integrity, and
provable play.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [169] [Characterizing Small Circuit Classes from FAC^0 to FAC^1 via Discrete Ordinary Differential Equations](https://arxiv.org/abs/2506.23404)
*Melissa Antonelli,Arnaud Durand,Juha Kontinen*

Main category: cs.CC

TL;DR: 本文提出了一种统一框架，通过常微分方程（ODEs）研究小电路类和界限。通过限制线性性和特定增长率函数的推导，证明了多种函数类（从FAC^0到FAC^1）的对应关系，并建立了多个完备性结果。


<details>
  <summary>Details</summary>
Motivation: 探索电路计算中不同层次函数类的界限问题，通过ODEs提供新的研究工具，以加深对计数器作用的了解。

Method: 利用ODE-based递归模式和限制线性性及增长率函数推导，分析小电路类。

Result: 揭示了ODEs与电路计算之间的联系，首次实现了FACC[2]和FNC1类的ODE-based特征化。

Conclusion: 通过ODEs为电路层次结构中类的界限提供新工具，可能增强对计数器的理解。

Abstract: In this paper, we provide a uniform framework for investigating small circuit
classes and bounds through the lens of ordinary differential equations (ODEs).
Following an approach recently introduced to capture the class of
polynomial-time computable functions via ODE-based recursion schemas and later
applied to the context of functions computed by unbounded fan-in circuits of
constant depth (FAC^0), we study multiple relevant small circuit classes. In
particular, we show that natural restrictions on linearity and derivation along
functions with specific growth rate correspond to kinds of functions that can
be proved to be in various classes, ranging from FAC^0 to FAC^1. This reveals
an intriguing link between constraints over linear-length ODEs and circuit
computation, providing new tools to tackle the complex challenge of
establishing bounds for classes in the circuit hierarchies and possibly
enhancing our understanding of the role of counters in this setting.
Additionally, we establish several completeness results, in particular
obtaining the first ODE-based characterizations for the classes of functions
computable in constant depth with unbounded fan-in and Mod 2 gates (FACC[2])
and in logarithmic depth with bounded fan-in Boolean gates (FNC1).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [170] [NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance](https://arxiv.org/abs/2506.23397)
*Gaurav Sehgal,Semih Salihoglu*

Main category: cs.IR

TL;DR: NaviX是一个为图数据库管理系统（GDBMS）设计的原生向量索引，支持过滤向量搜索查询，并通过预过滤和自适应算法提高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代预测应用需要联合查询向量嵌入与结构化属性，现有DBMS缺乏统一的向量索引支持。

Method: 基于HNSW图结构，采用预过滤方法并设计自适应算法，根据局部选择性选择启发式策略。

Result: NaviX在实验中表现出优于现有基线方法的鲁棒性和效率。

Conclusion: NaviX为GDBMS提供了一种高效的向量索引方案，支持复杂的查询需求。

Abstract: There is an increasing demand for extending existing DBMSs with vector
indices so that they become unified systems capable of supporting modern
predictive applications, which require joint querying of vector embeddings
together with the structured properties and connections of objects. We present
NaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design
goals. First, we aim to implement a disk-based vector index that leverages the
core storage and query-processing capabilities of the underlying GDBMS. To this
end, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,
which itself is a graph-based structure. Second, we aim to support
predicate-agnostic filtered vector search queries, in which the k nearest
neighbors (kNNs) of a query vector vQ are searched only within an arbitrary
subset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a
prefiltering approach that evaluates QS first and passes the full description
of subset S to the kNN search operator. We study how to design a prefiltering
search algorithm that remains robust under varying selectivities and under
different correlations between subset S and query vector vQ. We propose an
adaptive algorithm that uses the local selectivity of each vector in the HNSW
graph to choose an appropriate heuristic at every iteration of the kNN search.
Finally, We demonstrate NaviX's robustness and efficiency through extensive
experiments against both existing prefiltering- and postfiltering-based
baselines.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [171] [Evaluating and Improving Large Language Models for Competitive Program Generation](https://arxiv.org/abs/2506.22954)
*Minnan Wei,Ziming Li,Xiang Chen,Menglin Zheng,Ziyan Qu,Cheng Yu,Siyu Chen,Xiaolin Ju*

Main category: cs.SI

TL;DR: 评估和改进大型语言模型在解决真实世界竞争编程问题中的能力，通过精心设计的基准测试和改进策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 竞争编程生成对大型语言模型提出了高要求，但现有研究存在数据泄露和算法多样性不足的问题，因此需要更全面的评估和改进方法。

Method: 收集117个问题，筛选80个构建基准测试，使用DeepSeek-R1模型通过在线评测平台评估，并为错误设计分类和多阶段改进策略。

Result: 基础提示下仅解决5个问题，改进后成功解决46个问题，错误分类包括通用和专用错误类型。

Conclusion: 提出的多阶段改进框架显著提升了模型在竞争编程中的表现，为未来研究提供了有效方法。

Abstract: Context: Due to the demand for strong algorithmic reasoning, complex logic
implementation, and strict adherence to input/output formats and resource
constraints, competitive programming generation by large language models (LLMs)
is considered the most challenging problem in current LLM-based code
generation. However, previous studies often evaluate LLMs using simple prompts
and benchmark datasets prone to data leakage. Moreover, prior work has limited
consideration of the diversity in algorithm types and difficulty levels.
Objective: In this study, we aim to evaluate and improve LLMs in solving
real-world competitive programming problems. Methods: We initially collect 117
problems from nine regional ICPC/CCPC contests held in 2024 and design four
filtering criteria to construct a curated benchmark consisting of 80 problems.
Leveraging DeepSeek-R1 as the LLM, we evaluate its competitive program
generation capabilities through the online judge (OJ) platforms, guided by a
carefully designed basic prompt. For incorrect submissions, we construct a
fine-grained error taxonomy and then propose a targeted improvement framework
by combining a multi-turn dialogue-based repair phase and an
information-augmented regeneration phase. Results: Experimental results show
that only 5 out of 80 problems are fully accepted when using basic prompts. For
the unsolved problems, we construct the error taxonomy, including general
errors (such as design, boundary, condition, data type, syntax, and
input/output errors) and specialized errors (such as those in mathematical
problems, greedy algorithms, and graph theories). After applying our proposed
improvement strategies, we substantially increased the number of correct
solutions, with 46 out of 80 problems successfully accepted.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [172] [Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment](https://arxiv.org/abs/2506.23739)
*Lisa Marie Otto,Michael Kaiser,Daniel Seebacher,Steffen Müller*

Main category: cs.RO

TL;DR: 该论文提出了一种结合车辆在环测试和运动实验室的测试环境，用于评估自动驾驶系统与弱势道路使用者的互动，验证了人位估计方法在真实与虚拟环境中的一致性。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶系统与弱势道路用户（如行人和骑行者）在城市场景中的安全互动，需要先进的测试方法。

Method: 通过车辆在环测试与运动实验室结合，利用虚幻引擎5生成虚拟场景，实时投影弱势道路用户的动作，并通过单目摄像头进行3D骨骼检测。

Result: 在稳定运动模式下，真实与虚拟环境中的人位估计结果高度一致，但在动态运动和遮挡情况下（尤其是复杂骑行姿势）仍存在显著偏差。

Conclusion: 该研究为优化基于人工智能的车辆感知测试方法提供了依据，并改进了自动驾驶车辆与弱势道路用户在虚拟环境中的互动模型。

Abstract: Ensuring safe and realistic interactions between automated driving systems
and vulnerable road users (VRUs) in urban environments requires advanced
testing methodologies. This paper presents a test environment that combines a
Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the
feasibility of cyber-physical (CP) testing of vehicle-pedestrian and
vehicle-cyclist interactions. Building upon previous work focused on pedestrian
localization, we further validate a human pose estimation (HPE) approach
through a comparative analysis of real-world (RW) and virtual representations
of VRUs. The study examines the perception of full-body motion using a
commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is
generated in Unreal Engine 5, where VRUs are animated in real time and
projected onto a screen to stimulate the camera. The proposed stimulation
technique ensures the correct perspective, enabling realistic vehicle
perception. To assess the accuracy and consistency of HPE across RW and CP
domains, we analyze the reliability of detections as well as variations in
movement trajectories and joint estimation stability. The validation includes
dynamic test scenarios where human avatars, both walking and cycling, are
monitored under controlled conditions. Our results show a strong alignment in
HPE between RW and CP test conditions for stable motion patterns, while notable
inaccuracies persist under dynamic movements and occlusions, particularly for
complex cyclist postures. These findings contribute to refining CP testing
approaches for evaluating next-generation AI-based vehicle perception and to
enhancing interaction models of automated vehicles and VRUs in CP environments.

</details>


### [173] [Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy](https://arxiv.org/abs/2506.24046)
*Olivia Richards,Keith L. Obstein,Nabil Simaan*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: New endoscopists require a large volume of expert-proctored colonoscopies to
attain minimal competency. Developing multi-fingered, synchronized control of a
colonoscope requires significant time and exposure to the device. Current
training methods inhibit this development by relying on tool hand-off for
expert demonstrations. There is a need for colonoscopy training tools that
enable in-hand expert guidance in real-time. We present a new concept of a
tandem training system that uses a telemanipulated preceptor colonoscope to
guide novice users as they perform a colonoscopy. This system is capable of
dual-control and can automatically toggle between expert and novice control of
a standard colonoscope's angulation control wheels. Preliminary results from a
user study with novice and expert users show the effectiveness of this device
as a skill acquisition tool. We believe that this device has the potential to
accelerate skill acquisition for colonoscopy and, in the future, enable
individualized instruction and responsive teaching through bidirectional
actuation.

</details>
