{"id": "2507.07325", "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "categories": ["cs.SE"], "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u5fb7\u8bed\u5f00\u53d1\u8005\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u5fb7\u8bed\u9886\u57df\u6570\u636e\u7f3a\u5931\u7684\u7a7a\u767d\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u5206\u6790\u5de5\u5177\u4e3b\u8981\u4f9d\u8d56\u82f1\u8bed\u6570\u636e\uff0c\u5fb7\u8bed\u9886\u57df\u7f3a\u4e4f\u76f8\u5173\u8d44\u6e90\uff0c\u5f71\u54cd\u4e86\u5fb7\u8bed\u5f00\u53d1\u8005\u793e\u533a\u7684\u60c5\u611f\u5206\u6790\u7814\u7a76\u3002", "method": "\u4ece\u5fb7\u56fd\u5f00\u53d1\u8005\u8bba\u575b\u63d0\u53d65,949\u6761\u8bed\u53e5\uff0c\u7531\u56db\u4f4d\u5fb7\u8bed\u5b66\u751f\u57fa\u4e8eShaver\u7b49\u4eba\u7684\u60c5\u611f\u6a21\u578b\u6807\u6ce8\u516d\u79cd\u57fa\u672c\u60c5\u611f\u3002", "result": "\u6807\u6ce8\u8fc7\u7a0b\u663e\u793a\u51fa\u9ad8\u8bc4\u5206\u8005\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff1b\u73b0\u6709\u5fb7\u8bed\u5de5\u5177\u65e0\u6cd5\u6ee1\u8db3\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u5fb7\u8bed\u6570\u636e\u96c6\u4e3a\u5fb7\u8bed\u5f00\u53d1\u8005\u793e\u533a\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u4f18\u5316\u6807\u6ce8\u548c\u5e94\u7528\u7684\u65b9\u5411\u3002"}}
{"id": "2507.07344", "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "categories": ["cs.SE"], "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u7528\u6237\u53cd\u9988\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u6027\u9700\u6c42\u5e76\u751f\u6210\u76f8\u5e94\u89e3\u91ca\uff0c\u8bc4\u4f30\u663e\u793aAI\u751f\u6210\u7684\u89e3\u91ca\u5728\u6e05\u6670\u5ea6\u548c\u98ce\u683c\u4e0a\u66f4\u53d7\u9752\u7750\uff0c\u4f46\u6b63\u786e\u6027\u4ecd\u9700\u4eba\u5de5\u9a8c\u8bc1\u3002", "motivation": "\u589e\u5f3a\u900f\u660e\u5ea6\u3001\u5efa\u7acb\u7528\u6237\u4fe1\u4efb\u53ca\u786e\u4fdd\u5408\u89c4\u6027\u9700\u8981\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4ece\u7528\u6237\u53cd\u9988\u5230\u7ed3\u6784\u5316\u9700\u6c42\u7684\u7cfb\u7edf\u5316\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u5de5\u5177\u652f\u6301\u7684\u65b9\u6cd5\uff0c\u4e0e\u5de5\u4e1a\u81ea\u52a8\u5316\u5236\u9020\u5546\u5408\u4f5c\u521b\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8bc4\u4f30AI\u751f\u6210\u9700\u6c42\u4e0e\u89e3\u91ca\u7684\u6548\u679c\u3002", "result": "AI\u751f\u6210\u7684\u89e3\u91ca\u5728\u6e05\u6670\u5ea6\u548c\u98ce\u683c\u4e0a\u4f18\u4e8e\u4eba\u5de5\uff0c\u4f46\u9700\u6c42\u7684\u76f8\u5173\u6027\u548c\u6b63\u786e\u6027\u4e0d\u8db3\uff0c\u9700\u4eba\u5de5\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u6027\u9700\u6c42\u81ea\u52a8\u5316\u5904\u7406\u7684\u8fdb\u5c55\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\uff0c\u5e76\u5f3a\u8c03\u4e86\u4eba\u5de5\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.07468", "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "categories": ["cs.SE"], "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "\u5de5\u4e1a4.0\u6280\u672f\u4e0e\u5de5\u7a0b\u5de5\u4f5c\u6d41\u7684\u878d\u5408\u4e3a\u5de5\u5382\u548c\u6d41\u7a0b\u5de5\u7a0b\u7684\u81ea\u52a8\u5316\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6301\uff0c\u5176\u4e2dAAS\u662f\u5b9e\u73b0\u6570\u5b57\u5b6a\u751f\u4e92\u64cd\u4f5c\u6027\u7684\u6838\u5fc3\u5de5\u5177\u3002\u672c\u6587\u7ed3\u5408BPMN\u63a2\u8ba8\u4e86AAS\u5728\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0fAAS\u57fa\u7840\u8bbe\u65bd\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u548c\u6269\u5c55\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u6548\u7387\u4e0e\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u5de5\u4e1a4.0\u6280\u672f\u7684\u5e94\u7528\u9700\u8981\u9ad8\u6548\u3001\u5b89\u5168\u7684\u5de5\u7a0b\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0cAAS\u4f5c\u4e3a\u4e00\u79cd\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u80fd\u591f\u4fc3\u8fdb\u8de8\u7ec4\u7ec7\u534f\u4f5c\u548c\u6570\u636e\u4ea4\u6362\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0fAAS copy-on-write\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408BPMN\u5b9e\u73b0\u7ed3\u6784\u5316\u4e0e\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u4f5c\u6d41\u7ba1\u7406\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u5206\u5e03\u5f0fAAS\u57fa\u7840\u8bbe\u65bd\u548c\u539f\u578b\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e86\u5de5\u7a0b\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "conclusion": "AAS\u4e0eBPMN\u7684\u7ed3\u5408\u4e3a\u5de5\u4e1a4.0\u6280\u672f\u652f\u6301\u4e0b\u7684\u5de5\u7a0b\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07548", "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "categories": ["cs.SE"], "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524d\u7684\u9700\u6c42\u6587\u6863\u5bf9\u76f4\u63a5\u7528\u4e8eLLM\u751f\u6210\u4ee3\u7801\u8fc7\u4e8e\u62bd\u8c61\uff0c\u9700\u8981\u5148\u624b\u52a8\u5206\u89e3\u4e3a\u7f16\u7a0b\u4efb\u52a1\u5e76\u8865\u5145\u8bbe\u8ba1\u51b3\u7b56\u3002", "motivation": "\u63a2\u8ba8\u5f00\u53d1\u8005\u5982\u4f55\u7ed3\u5408\u9700\u6c42\u548c\u8bbe\u8ba1\u5de5\u4ef6\u4f7f\u7528LLM\u751f\u6210\u4ee3\u7801\uff0c\u8bc4\u4f30LLM\u662f\u5426\u53ef\u80fd\u53d6\u4ee3\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u3002", "method": "\u8bbf\u8c0814\u5bb6\u516c\u53f8\u768418\u540d\u4ece\u4e1a\u8005\uff0c\u5206\u6790\u5176\u4f7f\u7528\u9700\u6c42\u548c\u8bbe\u8ba1\u5de5\u4ef6\u7684\u8fc7\u7a0b\u3002", "result": "\u9700\u6c42\u9700\u5206\u89e3\u5e76\u8865\u5145\u540e\u8f93\u5165LLM\uff0c\u4f20\u7edf\u9700\u6c42\u5de5\u7a0b\u5728LLM\u65f6\u4ee3\u4ecd\u4e0d\u53ef\u6216\u7f3a\u3002", "conclusion": "LLM\u65f6\u4ee3\u4ecd\u9700\u57fa\u7840\u9700\u6c42\u5de5\u7a0b\u5de5\u4f5c\uff0c\u79d1\u5b66\u65b9\u6cd5\u81ea\u52a8\u5316\u9700\u6c42\u4e2d\u5fc3\u4efb\u52a1\u9700\u7ed3\u5408\u8fd9\u4e00\u80cc\u666f\u3002"}}
{"id": "2507.07133", "pdf": "https://arxiv.org/pdf/2507.07133", "abs": "https://arxiv.org/abs/2507.07133", "authors": ["Mathieu Tuli", "Kaveh Kamali", "David B. Lindell"], "title": "Generative Panoramic Image Stitching", "categories": ["cs.GR", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the task of generative panoramic image stitching, which aims to\nsynthesize seamless panoramas that are faithful to the content of multiple\nreference images containing parallax effects and strong variations in lighting,\ncamera capture settings, or style. In this challenging setting, traditional\nimage stitching pipelines fail, producing outputs with ghosting and other\nartifacts. While recent generative models are capable of outpainting content\nconsistent with multiple reference images, they fail when tasked with\nsynthesizing large, coherent regions of a panorama. To address these\nlimitations, we propose a method that fine-tunes a diffusion-based inpainting\nmodel to preserve a scene's content and layout based on multiple reference\nimages. Once fine-tuned, the model outpaints a full panorama from a single\nreference image, producing a seamless and visually coherent result that\nfaithfully integrates content from all reference images. Our approach\nsignificantly outperforms baselines for this task in terms of image quality and\nthe consistency of image structure and scene layout when evaluated on captured\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5168\u666f\u56fe\u50cf\u62fc\u63a5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u65e0\u7f1d\u5168\u666f\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u62fc\u63a5\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u5dee\u3001\u5149\u7ebf\u53d8\u5316\u7b49\u95ee\u9898\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u4e5f\u65e0\u6cd5\u5408\u6210\u5927\u578b\u8fde\u8d2f\u7684\u5168\u666f\u56fe\u3002", "method": "\u5fae\u8c03\u57fa\u4e8e\u6269\u6563\u7684\u53bb\u566a\u6a21\u578b\uff0c\u4ee5\u4fdd\u7559\u573a\u666f\u5185\u5bb9\u548c\u5e03\u5c40\uff0c\u4ece\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u751f\u6210\u5168\u666f\u3002", "result": "\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u548c\u73b0\u6709\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5168\u666f\u5408\u6210\u3002"}}
{"id": "2507.07480", "pdf": "https://arxiv.org/pdf/2507.07480", "abs": "https://arxiv.org/abs/2507.07480", "authors": ["Tobias Kapp\u00e9"], "title": "On Propositional Program Equivalence (extended abstract)", "categories": ["cs.PL"], "comment": null, "summary": "General program equivalence is undecidable. However, if we abstract away the\nsemantics of statements, then this problem becomes not just decidable, but\npractically feasible. For instance, a program of the form \"if $b$ then $e$ else\n$f$\" should be equivalent to \"if not $b$ then $f$ else $e$\" - no matter what\n$b$, $e$ and $f$ are. This kind of equivalence is known as propositional\nequivalence. In this extended abstract, we discuss recent developments in\npropositional program equivalence from the perspective of (Guarded) Kleene\nAlgebra with Tests, or (G)KAT.", "AI": {"tldr": "\u7a0b\u5e8f\u7684\u4e00\u822c\u7b49\u4ef7\u6027\u4e0d\u53ef\u5224\u5b9a\uff0c\u4f46\u5ffd\u7565\u8bed\u53e5\u8bed\u4e49\u540e\u95ee\u9898\u53ef\u5224\u5b9a\u4e14\u5b9e\u7528\u3002\u547d\u9898\u7b49\u4ef7\u6027\u5728(G)KAT\u6846\u67b6\u4e0b\u7684\u65b0\u8fdb\u5c55\u3002", "motivation": "\u63a2\u8ba8\u5728\u62bd\u8c61\u8bed\u53e5\u8bed\u4e49\u540e\uff0c\u7a0b\u5e8f\u7b49\u4ef7\u6027\u95ee\u9898\u5982\u4f55\u53d8\u5f97\u53ef\u5224\u5b9a\u53ca\u5b9e\u7528\u3002", "method": "\u57fa\u4e8e(Guarded) Kleene Algebra with Tests ((G)KAT)\u7684\u7406\u8bba\u6846\u67b6\u3002", "result": "\u5c55\u793a\u547d\u9898\u7a0b\u5e8f\u7b49\u4ef7\u6027\u5728\u5b9e\u9645\u4e2d\u7684\u53ef\u884c\u6027\u53ca\u5176\u7406\u8bba\u652f\u6301\u3002", "conclusion": "\u5728(G)KAT\u6846\u67b6\u4e0b\uff0c\u547d\u9898\u7a0b\u5e8f\u7b49\u4ef7\u6027\u6210\u4e3a\u53ef\u5224\u5b9a\u4e14\u6709\u5b9e\u7528\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2507.07396", "pdf": "https://arxiv.org/pdf/2507.07396", "abs": "https://arxiv.org/abs/2507.07396", "authors": ["Zeyang Song", "Shimin Zhang", "Yuhong Chou", "Jibin Wu", "Haizhou Li"], "title": "IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech Processing", "categories": ["cs.MM", "cs.LG", "cs.SD", "eess.AS"], "comment": "Under review of TNNLS", "summary": "Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,\nrepresent a promising neuromorphic computing paradigm that offers\nenergy-efficient alternatives to traditional Artificial Neural Networks (ANNs).\nDespite proven effectiveness, SNN architectures have struggled to achieve\ncompetitive performance on large-scale speech processing task. Two key\nchallenges hinder progress: (1) the high computational overhead during training\ncaused by multi-timestep spike firing, and (2) the absence of large-scale SNN\narchitectures tailored to speech processing tasks. To overcome the issues, we\nintroduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking\nTransformer architecture specifically designed for large-scale speech\nprocessing. Central to our design is the Input-aware Multi-Level Spike (IMLS)\nmechanism, which simulate multi-timestep spike firing within a single timestep\nusing an adaptive, input-aware thresholding scheme. IML-Spikeformer further\nintegrates a Reparameterized Spiking Self-Attention (RepSSA) module with a\nHierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module\nenhances the precision of attention maps and enables modeling of multi-scale\ntemporal dependencies in speech signals. Experiments demonstrate that\nIML-Spikeformer achieves word error rates of 6.0\\% on AiShell-1 and 3.4\\% on\nLibrispeech-960, comparable to conventional ANN transformers while reducing\ntheoretical inference energy consumption by 4.64$\\times$ and 4.32$\\times$\nrespectively. IML-Spikeformer marks an advance of scalable SNN architectures\nfor large-scale speech processing in both task performance and energy\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIML-Spikeformer\u7684\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9488\u5bf9\u5927\u89c4\u6a21\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8f93\u5165\u611f\u77e5\u591a\u7ea7\u8109\u51b2\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u80fd\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u65f6\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u7f3a\u4e4f\u9488\u5bf9\u8bed\u97f3\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u4e86\u8f93\u5165\u611f\u77e5\u591a\u7ea7\u8109\u51b2\uff08IMLS\uff09\u673a\u5236\u548c\u5e26\u6709\u5c42\u6b21\u8870\u51cf\u63a9\u7801\u7684\u91cd\u65b0\u53c2\u6570\u5316\u8109\u51b2\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff08HD-RepSSA\uff09\uff0c\u4ee5\u6a21\u62df\u591a\u65f6\u95f4\u6b65\u8109\u51b2\u5e76\u5728\u5355\u65f6\u95f4\u6b65\u5185\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728AiShell-1\u548cLibrispeech-960\u6570\u636e\u96c6\u4e0a\u7684\u8bcd\u9519\u8bef\u7387\u5206\u522b\u4e3a6.0%\u548c3.4%\uff0c\u4e0eANN\u53d8\u538b\u5668\u76f8\u5f53\uff0c\u540c\u65f6\u7406\u8bba\u63a8\u7406\u80fd\u8017\u964d\u4f4e\u4e864.64\u500d\u548c4.32\u500d\u3002", "conclusion": "IML-Spikeformer\u5728\u5927\u89c4\u6a21\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u548c\u80fd\u6e90\u6548\u7387\u7684\u53cc\u91cd\u7a81\u7834\uff0c\u4e3aSNN\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07208", "pdf": "https://arxiv.org/pdf/2507.07208", "abs": "https://arxiv.org/abs/2507.07208", "authors": ["Matteo Spadetto"], "title": "A 2-categorical approach to the semantics of dependent type theory with computation axioms", "categories": ["math.LO", "cs.LO", "math.CT", "03F50, 03G30, 18C10, 03B38, 03B70, 18N45, 18D30, 55U35, 55U40"], "comment": "64 pages, comments welcome", "summary": "Axiomatic type theory is a dependent type theory without computation rules.\nThe term equality judgements that usually characterise these rules are replaced\nby computation axioms, i.e., additional term judgements that are typed by\nidentity types. This paper is devoted to providing an effective description of\nits semantics, from a higher categorical perspective: given the challenge of\nencoding intensional type formers into 1-dimensional categorical terms and\nproperties, a challenge that persists even for axiomatic type formers, we adopt\nRichard Garner's approach in the 2-dimensional study of dependent types. We\nprove that the type formers of axiomatic theories can be encoded into natural\n2-dimensional category theoretic data, obtaining a presentation of the\nsemantics of axiomatic type theory via 2-categorical models called display map\n2-categories. In the axiomatic case, the 2-categorical requirements identified\nby Garner for interpreting intensional type formers are relaxed. Therefore, we\nobtain a presentation of the semantics of the axiomatic theory that generalises\nGarner's one for the intensional case. Our main result states that the\ninterpretation of axiomatic theories within display map 2-categories is\nwell-defined and enjoys the soundness property. We use this fact to provide a\nsemantic proof that the computation rule of intensional identity types is not\nadmissible in axiomatic type theory. This is achieved via a revisitation of\nHofmann and Streicher's groupoid model that believes axiomatic identity types\nbut does not believe intensional ones.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2\u7ef4\u8303\u7574\u8bed\u4e49\u7684\u65b9\u6cd5\u6765\u63cf\u8ff0\u516c\u7406\u7c7b\u578b\u8bba\u7684\u8bed\u4e49\uff0c\u6269\u5c55\u4e86Garner\u5bf9\u5185\u6db5\u7c7b\u578b\u7684\u5904\u7406\uff0c\u8bc1\u660e\u5176\u89e3\u91ca\u662f\u5408\u7406\u7684\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5185\u6db5\u6052\u7b49\u7c7b\u578b\u7684\u8ba1\u7b97\u89c4\u5219\u5728\u516c\u7406\u7c7b\u578b\u8bba\u4e2d\u4e0d\u53ef\u6210\u7acb\u3002", "motivation": "\u516c\u7406\u7c7b\u578b\u8bba\u7f3a\u4e4f\u8ba1\u7b97\u89c4\u5219\uff0c\u73b0\u6709\u7684\u8303\u7574\u8bed\u4e49\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4ece2\u7ef4\u8303\u7574\u89c6\u89d2\u63d0\u4f9b\u5176\u8bed\u4e49\u7684\u6709\u6548\u63cf\u8ff0\u3002", "method": "\u91c7\u7528Richard Garner\u76842\u7ef4\u65b9\u6cd5\uff0c\u5c06\u516c\u7406\u7c7b\u578b\u8bba\u7684\u8bed\u4e49\u7f16\u7801\u4e3a\u81ea\u7136\u76842\u7ef4\u8303\u7574\u6570\u636e\uff0c\u5b9a\u4e49\u4e86\u663e\u793a\u6620\u5c042-\u8303\u7574\u6a21\u578b\uff0c\u5e76\u653e\u5bbd\u4e86\u5bf9\u5185\u6db5\u7c7b\u578b\u7684\u9650\u5236\u3002", "result": "\u8bc1\u660e\u4e86\u516c\u7406\u7c7b\u578b\u8bba\u57282\u7ef4\u8303\u7574\u6a21\u578b\u4e2d\u7684\u89e3\u91ca\u662f\u5408\u7406\u7684\uff0c\u4e14\u5177\u6709\u5b8c\u5907\u6027\u3002\u901a\u8fc7\u91cd\u6784Hofmann\u548cStreicher\u7684\u7fa4\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5185\u6db5\u6052\u7b49\u7c7b\u578b\u8ba1\u7b97\u89c4\u5219\u7684\u4e0d\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86Garner\u7684\u5de5\u4f5c\uff0c\u4e3a\u516c\u7406\u7c7b\u578b\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u72ec\u7279\u6027\u3002"}}
{"id": "2507.07238", "pdf": "https://arxiv.org/pdf/2507.07238", "abs": "https://arxiv.org/abs/2507.07238", "authors": ["Stephen Kasica", "Charles Berret", "Tamara Munzner"], "title": "Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science", "categories": ["cs.HC", "cs.CY", "A.0"], "comment": "18 pages, 3 figures, Published in proceedings of the 2023 CHI\n  Conference on Human Factors in Computing Systems", "summary": "The work involved in gathering, wrangling, cleaning, and otherwise preparing\ndata for analysis is often the most time consuming and tedious aspect of data\nwork. Although many studies describe data preparation within the context of\ndata science workflows, there has been little research on data preparation in\ndata journalism. We address this gap with a hybrid form of thematic analysis\nthat combines deductive codes derived from existing accounts of data science\nworkflows and inductive codes arising from an interview study with 36\nprofessional data journalists. We extend a previous model of data science work\nto incorporate detailed activities of data preparation. We synthesize 60 dirty\ndata issues from 16 taxonomies on dirty data and our interview data, and we\nprovide a novel taxonomy to characterize these dirty data issues as\ndiscrepancies between mental models. We also identify four challenges faced by\njournalists: diachronic, regional, fragmented, and disparate data sources.", "AI": {"tldr": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6570\u636e\u65b0\u95fb\u4e2d\u6570\u636e\u51c6\u5907\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6df7\u5408\u4e3b\u9898\u5206\u6790\u65b9\u6cd5\u6269\u5c55\u4e86\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u810f\u6570\u636e\u95ee\u9898\u5206\u7c7b\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6570\u636e\u51c6\u5907\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u6570\u636e\u65b0\u95fb\u9886\u57df\u7684\u6570\u636e\u51c6\u5907\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e3b\u9898\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u6f14\u7ece\u7f16\u7801\uff08\u6765\u81ea\u73b0\u6709\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff09\u548c\u5f52\u7eb3\u7f16\u7801\uff08\u6765\u81ea36\u540d\u6570\u636e\u65b0\u95fb\u8bb0\u8005\u7684\u8bbf\u8c08\uff09\u3002", "result": "\u6269\u5c55\u4e86\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6a21\u578b\uff0c\u603b\u7ed3\u4e8660\u79cd\u810f\u6570\u636e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5fc3\u7406\u6a21\u578b\u5dee\u5f02\u7684\u65b0\u5206\u7c7b\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u65b0\u95fb\u8bb0\u8005\u9762\u4e34\u7684\u56db\u5927\u6311\u6218\uff08\u5386\u65f6\u6027\u3001\u533a\u57df\u6027\u3001\u788e\u7247\u5316\u548c\u6570\u636e\u6e90\u5206\u6563\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.07118", "pdf": "https://arxiv.org/pdf/2507.07118", "abs": "https://arxiv.org/abs/2507.07118", "authors": ["Zelin Zhu", "Kai Yang", "Rui Zhang"], "title": "Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Wireless localization and sensing technologies are essential in modern\nwireless networks, supporting applications in smart cities, the Internet of\nThings (IoT), and autonomous systems. High-performance localization and sensing\nsystems are critical for both network efficiency and emerging intelligent\napplications. Integrating channel state information (CSI) with deep learning\nhas recently emerged as a promising solution. Recent works have leveraged the\nspatial diversity of multiple input multiple output (MIMO) systems and the\nfrequency granularity of orthogonal frequency division multiplexing (OFDM)\nwaveforms to improve spatial resolution. Nevertheless, the joint modeling of\nlocalization and sensing under the high-dimensional CSI characteristics of\nMIMO-OFDM systems remains insufficiently investigated. This work aims to\njointly model and optimize localization and sensing tasks to harness their\npotential synergy. We first formulate localization and sensing as a\nmixed-integer bilevel deep learning problem and then propose a novel stochastic\nproximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)\nalgorithm. SPG-MIBO is well-suited for high-dimensional and large-scale\ndatasets, leveraging mini-batch training at each step for computational and\nmemory efficiency. The algorithm is also supported by theoretical convergence\nguarantees. Extensive experiments on multiple datasets validate its\neffectiveness and highlight the performance gains from joint localization and\nsensing optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u7ebf\u5b9a\u4f4d\u548c\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\uff08SPG-MIBO\uff09\uff0c\u9002\u7528\u4e8eMIMO-OFDM\u7cfb\u7edf\u7684\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u65e0\u7ebf\u5b9a\u4f4d\u548c\u611f\u77e5\u6280\u672f\u5728\u73b0\u4ee3\u7f51\u7edc\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4CSI\u7279\u6027\u4e0b\u7684\u8054\u5408\u5efa\u6a21\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u7684\u6df7\u5408\u6574\u6570\u53cc\u5c42\u4f18\u5316\u7b97\u6cd5\uff08SPG-MIBO\uff09\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u5927\u89c4\u6a21\u6570\u636e\u3002", "result": "\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u4e86\u8054\u5408\u4f18\u5316\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SPG-MIBO\u5728\u8054\u5408\u4f18\u5316\u65e0\u7ebf\u5b9a\u4f4d\u548c\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u5907\u7406\u8bba\u548c\u5b9e\u9a8c\u652f\u6301\u3002"}}
{"id": "2507.07342", "pdf": "https://arxiv.org/pdf/2507.07342", "abs": "https://arxiv.org/abs/2507.07342", "authors": ["Dogan Kutay Pekcan", "Hongyi Liao", "Ender Ayanoglu"], "title": "Discrete Beamforming Optimization for RISs with a Limited Phase Range and Amplitude Attenuation", "categories": ["eess.SP", "cs.ET"], "comment": "13 pages, 17 figures, 2 tables", "summary": "This paper addresses the problem of maximizing the received power at a user\nequipment via reconfigurable intelligent surface (RIS) characterized by\nphase-dependent amplitude (PDA) and discrete phase shifts over a limited phase\nrange. Given complex RIS coefficients, that is, discrete phase shifts and PDAs,\nwe derive the necessary and sufficient conditions to achieve the optimal\nsolution. To this end, we propose an optimal search algorithm that is proven to\nconverge in linear time within at most NK steps, significantly outperforming\nthe exhaustive search approach that would otherwise be needed for RISs with\namplitude attenuation. Furthermore, we introduce a practical quantization\nframework for PDA-introduced RISs termed amplitude-introduced polar\nquantization (APQ), and extend it to a novel algorithm named extended\namplitude-introduced polar quantization (EAPQ) that works with geometric\nprojections. We derive closed-form expressions to assess how closely the\nperformance of the proposed RIS configuration can approximate the ideal case\nwith continuous phases and no attenuation. Our analysis reveals that increasing\nthe number of discrete phases beyond K = 4 yields only marginal gains,\nregardless of attenuation levels, provided the RIS has a sufficiently wide\nphase range R. Furthermore, we also show and quantify that when the phase range\nR is limited, the performance is sensitive to attenuation for larger R, and\nsensitive to R when there is less attenuation. Finally, the proposed optimal\nalgorithm provides a generic upper bound that could serve as a benchmark for\ndiscrete beamforming in RISs with amplitude constraints.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7RIS\u6700\u5927\u5316\u7528\u6237\u8bbe\u5907\u63a5\u6536\u529f\u7387\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6700\u4f18\u641c\u7d22\u7b97\u6cd5\u548c\u91cf\u5316\u6846\u67b6\uff0c\u5206\u6790\u4e86\u79bb\u6563\u76f8\u4f4d\u6570\u548c\u76f8\u4f4d\u8303\u56f4\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u901a\u8fc7RIS\u4f18\u5316\u63a5\u6536\u529f\u7387\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728PDA\u548c\u79bb\u6563\u76f8\u4f4d\u9650\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u627e\u5230\u6700\u4f18\u89e3\u7684\u6761\u4ef6\u548c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u6700\u4f18\u641c\u7d22\u7b97\u6cd5\u548cAPQ\u91cf\u5316\u6846\u67b6\uff0c\u5e76\u6269\u5c55\u4e3aEAPQ\u7b97\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u6295\u5f71\u548c\u5c01\u95ed\u5f0f\u8868\u8fbe\u5f0f\u5206\u6790\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u589e\u52a0\u79bb\u6563\u76f8\u4f4d\u6570\u8d85\u8fc7K=4\u65f6\u6536\u76ca\u751a\u5fae\uff0c\u6027\u80fd\u53d7\u76f8\u4f4d\u8303\u56f4R\u548c\u8870\u51cf\u6c34\u5e73\u7684\u590d\u6742\u5f71\u54cd\uff0c\u4e14\u6700\u4f18\u7b97\u6cd5\u53ef\u4f5c\u4e3a\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u4e3aRIS\u7684\u79bb\u6563\u6ce2\u675f\u6210\u5f62\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u4e0e\u76f8\u4f4d\u8303\u56f4\u53ca\u8870\u51cf\u7684\u5173\u7cfb\u3002"}}
{"id": "2507.07683", "pdf": "https://arxiv.org/pdf/2507.07683", "abs": "https://arxiv.org/abs/2507.07683", "authors": ["Jude Haris", "Jos\u00e9 Cano"], "title": "Accelerating Transposed Convolutions on FPGA-based Edge Devices", "categories": ["cs.AR", "cs.DC", "cs.LG"], "comment": "Accepted to 35th International Conference on Field-Programmable Logic\n  and Applications (FPL) 2025", "summary": "Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u52a0\u901f\u5668MM2IM\uff0c\u901a\u8fc7\u7ed3\u5408\u77e9\u9635\u4e58\u6cd5\u4e0ecol2IM\u4f18\u5316\u8f6c\u7f6e\u5377\u79ef\u5c42\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u8f93\u5165\u5bfc\u5411\u6620\u5c04\u65b9\u6cd5\u5728\u8f6c\u7f6e\u5377\u79ef\u5c42\u4e2d\u5b58\u5728\u8f93\u51fa\u6620\u5c04\u590d\u6742\u3001\u8ba1\u7b97\u91cd\u53e0\u548c\u65e0\u6548\u8ba1\u7b97\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002", "method": "\u63d0\u51fa\u540d\u4e3aMM2IM\u7684\u52a0\u901f\u5668\uff0c\u7ed3\u5408\u77e9\u9635\u4e58\u6cd5\u4e0ecol2IM\u65b9\u6cd5\uff0c\u9488\u5bf9\u8f6c\u7f6e\u5377\u79ef\u5c42\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7SECDA-TFLite\u8bbe\u8ba1\u5de5\u5177\u5305\u5b9e\u73b0\u548c\u8bc4\u4f30\u3002", "result": "\u5728261\u79cd\u8f6c\u7f6e\u5377\u79ef\u914d\u7f6e\u4e2d\uff0cMM2IM\u5e73\u5747\u52a0\u901f\u6bd4\u8fbe1.9\u500d\uff0c\u90e8\u5206\u751f\u6210\u6a21\u578b\u52a0\u901f\u6bd4\u9ad8\u8fbe4.2\u500d\uff0c\u4e14\u80fd\u6548\u6bd4\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u52a0\u901f\u5668\u3002", "conclusion": "MM2IM\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.07438", "pdf": "https://arxiv.org/pdf/2507.07438", "abs": "https://arxiv.org/abs/2507.07438", "authors": ["Yingze Li", "Xianglong Liu", "Dong Wang", "Zixuan Wang", "Hongzhi Wang", "Kaixing Zhang", "Yiming Guan"], "title": "Algorithmic Complexity Attacks on All Learned Cardinality Estimators: A Data-centric Approach", "categories": ["cs.DB"], "comment": null, "summary": "Learned cardinality estimators show promise in query cardinality prediction,\nyet they universally exhibit fragility to training data drifts, posing risks\nfor real-world deployment. This work is the first to theoretical investigate\nhow minimal data-level drifts can maximally degrade the accuracy of learned\nestimators. We propose data-centric algorithmic complexity attacks against\nlearned estimators in a black-box setting, proving that finding the optimal\nattack strategy is NP-Hard. To address this, we design a polynomial-time\napproximation algorithm with a $(1-\\kappa)$ approximation ratio. Extensive\nexperiments demonstrate our attack's effectiveness: on STATS-CEB and IMDB-JOB\nbenchmarks, modifying just 0.8\\% of training tuples increases the 90th\npercentile Qerror by three orders of magnitude and raises end-to-end processing\ntime by up to 20$\\times$. Our work not only reveals critical vulnerabilities in\ndeployed learned estimators but also provides the first unified worst-case\ntheoretical analysis of their fragility under data updates. Additionally, we\nidentify two countermeasures to mitigate such black-box attacks, offering\ninsights for developing robust learned database optimizers.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u5668\u5728\u6570\u636e\u6f02\u79fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u7ea7\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5b9e\u9a8c\u8bc1\u660e\u653b\u51fb\u6548\u679c\u663e\u8457\uff0c\u540c\u65f6\u4e5f\u63d0\u51fa\u4e86\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u5668\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6613\u53d7\u6570\u636e\u6f02\u79fb\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u8106\u5f31\u6027\u5e76\u4e3a\u5f00\u53d1\u7a33\u5065\u4f18\u5316\u5668\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u7ea7\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u8bc1\u660e\u6700\u4f18\u653b\u51fb\u7b56\u7565\u662fNP-Hard\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4fee\u65390.8%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u4f7f90\u5206\u4f4d\u6570Qerror\u589e\u52a0\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u7aef\u5230\u7aef\u5904\u7406\u65f6\u95f4\u6700\u591a\u589e\u52a020\u500d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u5668\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u5176\u6570\u636e\u66f4\u65b0\u8106\u5f31\u6027\u7684\u7edf\u4e00\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2507.07114", "pdf": "https://arxiv.org/pdf/2507.07114", "abs": "https://arxiv.org/abs/2507.07114", "authors": ["Erez Weintraub", "Ron Banner", "Ariel Orda"], "title": "Distributed Training under Packet Loss", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "State-of-the-art language and vision models are routinely trained across\nthousands of GPUs, often spanning multiple data-centers, yet today's\ndistributed frameworks still assume reliable connections (e.g., InfiniBand or\nRoCE). The resulting acknowledgment traffic and retransmissions inflate tail\nlatencies and limit scalability. Leveraging unreliable connections will reduce\nlatency but may sacrifice model accuracy and convergence once packets are\ndropped. A principled, end-to-end solution that preserves accuracy and\nconvergence guarantees under genuine packet loss has previously been missing.\nWe address this critical gap by introducing a novel distributed training\nframework capable of operating over unreliable connections, offering unbiased\ngradient aggregation and bounded parameter drift without modifying model code\nor optimizers. The key insight is a two-stage defense against missing messages:\n(i) Unbiased gradient aggregation: each worker reconstructs a consistent\ngradient estimate from whatever packets arrive, guaranteeing expectation-level\ncorrectness; and (ii) Bounded-drift parameter broadcasts: we prove the\ninter-worker model discrepancy remains O(1) even after arbitrarily many\niterations, preventing the unbounded divergence typical of asynchronous setups.\nAnalytical bounds are matched by experiments on the LLAMA2 7B model with 64\nGPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.\nThis work bridges the gap between communication-efficient datacenter protocols\nand the accuracy and generalization guarantees demanded by modern large-model\ntraining, enabling robust, high-throughput learning on commodity or wide-area\nnetworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u53ef\u9760\u7684\u7f51\u7edc\u8fde\u63a5\u4e0b\u8fd0\u884c\uff0c\u4fdd\u8bc1\u68af\u5ea6\u805a\u5408\u7684\u65e0\u504f\u6027\u548c\u53c2\u6570\u6f02\u79fb\u7684\u6709\u754c\u6027\uff0c\u4ece\u800c\u89e3\u51b3\u4f20\u7edf\u6846\u67b6\u5728\u4e22\u5305\u65f6\u6a21\u578b\u7cbe\u5ea6\u548c\u6536\u655b\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u6846\u67b6\u4f9d\u8d56\u53ef\u9760\u7f51\u7edc\u8fde\u63a5\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002\u4e0d\u53ef\u9760\u8fde\u63a5\u867d\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u4e22\u5305\u4f1a\u635f\u5bb3\u6a21\u578b\u7cbe\u5ea6\u548c\u6536\u655b\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9632\u5fa1\u673a\u5236\uff1a1\uff09\u65e0\u504f\u68af\u5ea6\u805a\u5408\uff0c\u901a\u8fc7\u91cd\u6784\u68af\u5ea6\u4f30\u8ba1\u4fdd\u8bc1\u671f\u671b\u6b63\u786e\u6027\uff1b2\uff09\u6709\u754c\u53c2\u6570\u5e7f\u64ad\uff0c\u786e\u4fdd\u6a21\u578b\u5dee\u5f02\u6709\u754c\u3002", "result": "\u5728LLAMA2 7B\u6a21\u578b\u548c64 GPU\u5b9e\u9a8c\u4e2d\uff0c\u5bb9\u5fcd10%\u4e22\u5305\u65f6\u56f0\u60d1\u5ea6\u53d8\u5316\u4e0d\u8d85\u8fc70.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u9ad8\u6548\u901a\u4fe1\u534f\u8bae\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5728\u666e\u901a\u6216\u5e7f\u57df\u7f51\u7edc\u4e0a\u5b9e\u73b0\u9c81\u68d2\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.07682", "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "categories": ["cs.SE"], "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684Prompt Engineering\uff08PE\uff09\u5728\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u4e2d\u7684\u5e94\u7528\u7efc\u8ff0\uff08PE4RE\uff09\uff0c\u5206\u6790\u4e8635\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5c40\u9650\u548c\u7814\u7a76\u7a7a\u767d\uff0c\u6700\u540e\u7ed9\u51fa\u4e86\u4e00\u4e2a\u9010\u6b65\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u53ef\u63a7\u6027\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u63d0\u793a\u6307\u5bfc\uff0c\u963b\u788d\u4e86\u5176\u53ef\u4fe1\u8d56\u7684\u5b9e\u73b0\u3002", "method": "\u57fa\u4e8eKitchenham\u548cPetersen\u7684\u4e8c\u7ea7\u7814\u7a76\u534f\u8bae\uff0c\u641c\u7d22\u4e866\u4e2a\u6570\u5b57\u56fe\u4e66\u9986\uff0c\u7b5b\u9009\u4e86867\u6761\u8bb0\u5f55\uff0c\u5206\u6790\u4e8635\u9879\u4e3b\u8981\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u5206\u7c7b\u6cd5\u3002", "result": "\u5206\u7c7b\u6cd5\u5c06\u6280\u672f\u5bfc\u5411\u7684\u6a21\u5f0f\u4e0e\u4efb\u52a1\u5bfc\u5411\u7684RE\u89d2\u8272\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5c40\u9650\u548c\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4ece\u4e34\u65f6PE\u539f\u578b\u5411\u53ef\u590d\u73b0\u3001\u9002\u5408\u5b9e\u8df5\u7684\u5de5\u4f5c\u6d41\u7a0b\u7684\u6f14\u53d8\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2507.07136", "pdf": "https://arxiv.org/pdf/2507.07136", "abs": "https://arxiv.org/abs/2507.07136", "authors": ["Wanhua Li", "Yujie Zhao", "Minghan Qin", "Yang Liu", "Yuanhao Cai", "Chuang Gan", "Hanspeter Pfister"], "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS", "categories": ["cs.GR"], "comment": "Project Page: https://langsplat-v2.github.io", "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 $\\times$ speedup and a 47\n$\\times$ boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.", "AI": {"tldr": "LangSplatV2\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u548c\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u8a00\u573a\u7684\u9ad8\u7ef4\u7279\u5f81\u5904\u7406\u901f\u5ea6\u548c\u67e5\u8be2\u6548\u7387\uff0c\u5b9e\u73b0\u4e86476.2 FPS\u7684\u9ad8\u7ef4\u7279\u5f81\u6e85\u5c04\u548c384.6 FPS\u76843D\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709LangSplat\u5728\u9ad8\u7ea7GPU\u4e0a\u4ecd\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u6027\u80fd\uff088.2 FPS\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u8bc6\u522b\u4e86\u91cd\u91cf\u7ea7\u89e3\u7801\u5668\u4e3a\u4e3b\u8981\u901f\u5ea6\u74f6\u9888\uff0c\u65e8\u5728\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u6280\u672f\u6d88\u9664\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u5047\u8bbe\u6bcf\u4e2a\u9ad8\u65af\u4f5c\u4e3a\u5168\u5c40\u5b57\u5178\u4e2d\u7684\u7a00\u758f\u7f16\u7801\uff0c\u5b66\u4e603D\u7a00\u758f\u7cfb\u6570\u573a\uff0c\u5b8c\u5168\u6d88\u9664\u91cd\u91cf\u7ea7\u89e3\u7801\u5668\u9700\u6c42\uff0c\u5e76\u7ed3\u5408CUDA\u4f18\u5316\u7684\u9ad8\u6548\u7a00\u758f\u7cfb\u6570\u6e85\u5c04\u65b9\u6cd5\u3002", "result": "LangSplatV2\u5b9e\u73b0\u4e8642\u500d\u901f\u5ea6\u63d0\u5347\u548c47\u500d\u67e5\u8be2\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u67e5\u8be2\u7cbe\u5ea6\u3002", "conclusion": "LangSplatV2\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u548c\u9ad8\u6548\u6e85\u5c04\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u8a00\u573a\u7684\u5904\u7406\u901f\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e2d\u7684\u8bed\u8a00\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07911", "pdf": "https://arxiv.org/pdf/2507.07911", "abs": "https://arxiv.org/abs/2507.07911", "authors": ["Yasmin Elsaddik Valdivieso", "Mohd Faisal", "Karim Alghoul", "Monireh", "Vahdati", "Kamran Gholizadeh Hamlabadi", "Fedwa Laamarti", "Hussein Al Osman", "Abdulmotaleb El Saddik"], "title": "The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality", "categories": ["cs.MM", "cs.HC"], "comment": "Accepted to IEEE Medical Measurements & Applications (MeMeA) 2025", "summary": "Immersive virtual reality (VR) is a promising tool for stress reduction and\nrelaxation, traditionally relying on visual and auditory stimuli. This study\nexamines the role of olfactory stimuli in enhancing these effects, using a\nrandomized within-subject design. Thirty participants aged 18-60 experienced VR\nscenarios simulating a calming seaside environment, with sessions lasting 45\nminutes, in two conditions: with and without a \"Beach\" essential oil scent\n(Yankee Candle) administered via diffuser. Stress and relaxation were assessed\nthrough self-reported surveys and physiological measures, specifically\nECG-based heart rate variability (HRV). Results showed no significant\ndifference in self-reported relaxation scores (p=0.371) between conditions, but\nHRV analysis revealed a significant stress reduction (p=0.002) with olfactory\ninput, with HF increasing 108% from the Math Stress Test to the scented\nrelaxation condition, compared to 44% without scent. Additionally, 71.4% of\nparticipants expressed willingness to use olfactory-enhanced VR for relaxation,\nsuggesting practical appeal. These findings indicate that olfactory stimuli may\nenhance relaxation subconsciously, underscoring the importance of multisensory\nintegration in VR. Future work could explore personalized scents and long-term\neffects to optimize VR- based interventions for emotional and physical\nwell-being.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u55c5\u89c9\u523a\u6fc0\uff08\u5982\u6d77\u6ee9\u7cbe\u6cb9\u6c14\u5473\uff09\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u867d\u7136\u672a\u663e\u8457\u63d0\u5347\u81ea\u6211\u62a5\u544a\u7684\u653e\u677e\u611f\uff0c\u4f46\u901a\u8fc7\u751f\u7406\u6307\u6807\uff08HRV\uff09\u663e\u8457\u964d\u4f4e\u4e86\u538b\u529b\uff0c\u4e14\u591a\u6570\u53c2\u4e0e\u8005\u8ba4\u53ef\u5176\u5b9e\u9645\u5438\u5f15\u529b\u3002", "motivation": "\u63a2\u8ba8\u55c5\u89c9\u523a\u6fc0\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u5bf9\u538b\u529b\u7f13\u89e3\u548c\u653e\u677e\u7684\u5f71\u54cd\uff0c\u4ee5\u5f25\u8865\u4f20\u7edfVR\u4ec5\u4f9d\u8d56\u89c6\u89c9\u548c\u542c\u89c9\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u968f\u673a\u7ec4\u5185\u8bbe\u8ba1\uff0c30\u540d\u53c2\u4e0e\u8005\u4f53\u9a8c\u5e26\u6709\u548c\u4e0d\u5e26\u6709\u6d77\u6ee9\u7cbe\u6cb9\u6c14\u5473\u7684VR\u573a\u666f\uff0c\u901a\u8fc7\u81ea\u6211\u62a5\u544a\u548c\u5fc3\u7535\u56fe\uff08HRV\uff09\u6d4b\u91cf\u6548\u679c\u3002", "result": "\u55c5\u89c9\u523a\u6fc0\u663e\u8457\u964d\u4f4e\u4e86\u751f\u7406\u538b\u529b\uff08HRV\u6307\u6807\u63d0\u9ad8108%\uff09\uff0c\u4f46\u672a\u663e\u8457\u5f71\u54cd\u81ea\u6211\u62a5\u544a\u7684\u653e\u677e\u611f\uff1b71.4%\u7684\u53c2\u4e0e\u8005\u613f\u610f\u4f7f\u7528\u55c5\u89c9\u589e\u5f3aVR\u3002", "conclusion": "\u55c5\u89c9\u523a\u6fc0\u53ef\u80fd\u901a\u8fc7\u6f5c\u610f\u8bc6\u589e\u5f3a\u653e\u677e\u6548\u679c\uff0c\u7a81\u663e\u591a\u611f\u5b98\u6574\u5408\u5728VR\u4e2d\u7684\u91cd\u8981\u6027\u3002\u672a\u6765\u53ef\u7814\u7a76\u4e2a\u6027\u5316\u6c14\u5473\u548c\u957f\u671f\u6548\u679c\u3002"}}
{"id": "2507.07217", "pdf": "https://arxiv.org/pdf/2507.07217", "abs": "https://arxiv.org/abs/2507.07217", "authors": ["Zili Wang", "Frank Montabon", "Kristin Yvonne Rozier"], "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains", "categories": ["cs.AI", "cs.LG", "cs.LO", "I.2.4; I.2.7; J.4"], "comment": null, "summary": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u68c0\u6d4b\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u6cd5\u6d3b\u52a8\uff0c\u6bd4\u8f83\u4e86\u624b\u52a8\u548c\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u65b0\u95fb\u6587\u7ae0\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u6cd5\u6d3b\u52a8\uff08\u5982\u5047\u5192\u96f6\u4ef6\u3001\u5f3a\u8feb\u52b3\u52a8\uff09\u6570\u636e\u7a00\u758f\u4e14\u6613\u88ab\u7be1\u6539\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u95ee\u9898\u6811\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u91cf\u5316\u65b0\u95fb\u6587\u7ae0\u7684\u76f8\u5173\u6027\u3002", "result": "\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u5bf9\u65b0\u95fb\u6587\u7ae0\u5206\u7c7b\u7684\u5dee\u5f02\uff0c\u4e3a\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u6cd5\u6d3b\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7ed3\u5408LLM\u53ef\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u5347\u4f9b\u5e94\u94fe\u975e\u6cd5\u6d3b\u52a8\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.07362", "pdf": "https://arxiv.org/pdf/2507.07362", "abs": "https://arxiv.org/abs/2507.07362", "authors": ["Xinyu Li", "Tongguang Li", "Lixiang Yan", "Yuheng Li", "Linxuan Zhao", "Mladen Rakovi\u0107", "Inge Molenaar", "Dragan Ga\u0161evi\u0107", "Yizhou Fan"], "title": "FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "SRL, defined as learners' ability to systematically plan, monitor, and\nregulate their learning activities, is crucial for sustained academic\nachievement and lifelong learning competencies. Emerging Artificial\nIntelligence (AI) developments profoundly influence SRL interactions by\npotentially either diminishing or strengthening learners' opportunities to\nexercise their own regulatory skills. Recent literature emphasizes a balanced\napproach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI\nprovides targeted, timely scaffolding while preserving the learners' role as\nactive decision-makers and reflective monitors of their learning process.\nNevertheless, existing digital tools frequently fall short, lacking\nadaptability, focusing narrowly on isolated SRL phases, and insufficiently\nsupport meaningful human-AI interactions. In response, this paper introduces\nthe enhanced \\flora Engine, which incorporates advanced Generative Artificial\nIntelligence (GenAI) features and state-of-the-art learning analytics,\nexplicitly grounded in SRL and HHAIRL theories. The \\flora Engine offers\ninstrumentation tools such as collaborative writing, multi-agents chatbot, and\ndetailed learning trace logging to support dynamic, adaptive scaffolding\ntailored to individual needs in real time. We further present a summary of\nseveral research studies that provide the validations for and illustrate how\nthese instrumentation tools can be utilized in real-world educational and\nexperimental contexts. These studies demonstrate the effectiveness of \\flora\nEngine in fostering SRL and HHAIRL, providing both theoretical insights and\npractical solutions for the future of AI-enhanced learning context.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u8005\u7684\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\uff0c\u63d0\u51fa\u6df7\u5408\u4eba\u673a\u8c03\u8282\u5b66\u4e60\uff08HHAIRL\uff09\u6846\u67b6\uff0c\u5e76\u4ecb\u7ecd\u4e86\u589e\u5f3a\u7248\\flora Engine\u5de5\u5177\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u5b66\u4e60\u5206\u6790\u6280\u672f\uff0c\u4ee5\u52a8\u6001\u652f\u6301SRL\u3002", "motivation": "\u73b0\u6709\u6570\u5b57\u5de5\u5177\u5728\u9002\u5e94\u6027\u3001\u652f\u6301SRL\u5168\u9636\u6bb5\u53ca\u4eba\u673a\u4e92\u52a8\u65b9\u9762\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u7248\\flora Engine\uff0c\u96c6\u6210\u751f\u6210\u5f0fAI\u548c\u5b66\u4e60\u5206\u6790\uff0c\u63d0\u4f9b\u534f\u4f5c\u5199\u4f5c\u3001\u591a\u4ee3\u7406\u804a\u5929\u673a\u5668\u4eba\u7b49\u5de5\u5177\u3002", "result": "\u7814\u7a76\u9a8c\u8bc1\u4e86\\flora Engine\u5728\u652f\u6301SRL\u548cHHAIRL\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\\flora Engine\u4e3aAI\u589e\u5f3a\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u652f\u6301\uff0c\u63a8\u52a8\u4e86SRL\u548cHHAIRL\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.07149", "pdf": "https://arxiv.org/pdf/2507.07149", "abs": "https://arxiv.org/abs/2507.07149", "authors": ["Renyuan Liu", "Yuyang Leng", "Kaiyan Liu", "Shaohan Hu", "Chun-Fu", "Chen", "Peijun Zhao", "Heechul Yun", "Shuochao Yao"], "title": "DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training", "categories": ["cs.NI", "cs.LG"], "comment": "Accepted to MobiSys 2025", "summary": "Recent advancements in on-device training for deep neural networks have\nunderscored the critical need for efficient activation compression to overcome\nthe memory constraints of mobile and edge devices. As activations dominate\nmemory usage during training and are essential for gradient computation,\ncompressing them without compromising accuracy remains a key research\nchallenge. While existing methods for dynamic activation quantization promise\ntheoretical memory savings, their practical deployment is impeded by\nsystem-level challenges such as computational overhead and memory\nfragmentation.\n  To address these challenges, we introduce DAF, a Dynamic Activation Framework\nthat enables scalable and efficient on-device training through system-level\noptimizations. DAF achieves both memory- and time-efficient dynamic\nquantization training by addressing key system bottlenecks. It develops hybrid\nreduction operations tailored to the memory hierarchies of mobile and edge\nSoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic\nquantization, and implements an importance-aware paging memory management\nscheme to reduce fragmentation and support dynamic memory adjustments.\n  These optimizations collectively enable DAF to achieve substantial memory\nsavings and speedup without compromising model training accuracy. Evaluations\non various deep learning models across embedded and mobile platforms\ndemonstrate up to a $22.9\\times$ reduction in memory usage and a $3.2\\times$\nspeedup, making DAF a scalable and practical solution for resource-constrained\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DAF\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u5b9e\u73b0\u52a8\u6001\u6fc0\u6d3b\u91cf\u5316\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u56e0\u6fc0\u6d3b\u5360\u7528\u5185\u5b58\u8fc7\u9ad8\u800c\u9762\u4e34\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "method": "\u5f15\u5165DAF\u6846\u67b6\uff0c\u5305\u62ec\u6df7\u5408\u5f52\u7ea6\u64cd\u4f5c\u3001CPU-GPU\u534f\u4f5c\u4f4d\u538b\u7f29\u548c\u91cd\u8981\u6027\u611f\u77e5\u5206\u9875\u5185\u5b58\u7ba1\u7406\u3002", "result": "\u5728\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0a\u5b9e\u73b022.9\u500d\u5185\u5b58\u8282\u7701\u548c3.2\u500d\u52a0\u901f\uff0c\u4e14\u4e0d\u5f71\u54cd\u8bad\u7ec3\u7cbe\u5ea6\u3002", "conclusion": "DAF\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07116", "pdf": "https://arxiv.org/pdf/2507.07116", "abs": "https://arxiv.org/abs/2507.07116", "authors": ["Juan Cano-Benito", "Andrea Cimmino", "Sven Hertling", "Heiko Paulheim", "Ra\u00fal Garc\u00eda-Castro"], "title": "Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces", "categories": ["cs.DC", "cs.AI", "cs.ET"], "comment": null, "summary": "Data spaces are emerging as decentralised infrastructures that enable\nsovereign, secure, and trustworthy data exchange among multiple participants.\nTo achieve semantic interoperability within these environments, the use of\nsemantic web technologies and knowledge graphs has been proposed. Although\ndistributed ledger technologies (DLT) fit as the underlying infrastructure for\ndata spaces, there remains a significant gap in terms of the efficient storage\nof semantic data on these platforms. This paper presents a systematic\nevaluation of semantic data storage across different types of DLT (public,\nprivate, and hybrid), using a real-world knowledge graph as an experimental\nbasis. The study compares performance, storage efficiency, resource\nconsumption, and the capabilities to update and query semantic data. The\nresults show that private DLTs are the most efficient for storing and managing\nsemantic content, while hybrid DLTs offer a balanced trade-off between public\nauditability and operational efficiency. This research leads to a discussion on\nthe selection of the most appropriate DLT infrastructure based on the data\nsovereignty requirements of decentralised data ecosystems.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e0d\u540c\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f\uff08DLT\uff09\u5728\u8bed\u4e49\u6570\u636e\u5b58\u50a8\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u79c1\u6709DLT\u6548\u7387\u6700\u9ad8\uff0c\u6df7\u5408DLT\u5728\u53ef\u5ba1\u8ba1\u6027\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u7a7a\u95f4\u4e2d\u8bed\u4e49\u6570\u636e\u5b58\u50a8\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5b89\u5168\u3001\u53ef\u4fe1\u7684\u6570\u636e\u4ea4\u6362\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u516c\u6709\u3001\u79c1\u6709\u548c\u6df7\u5408DLT\u5728\u5b58\u50a8\u3001\u66f4\u65b0\u548c\u67e5\u8be2\u8bed\u4e49\u6570\u636e\u65f6\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "result": "\u7ed3\u679c\u663e\u793a\u79c1\u6709DLT\u6700\u6709\u6548\uff0c\u6df7\u5408DLT\u5219\u5728\u53ef\u5ba1\u8ba1\u6027\u548c\u6548\u7387\u95f4\u63d0\u4f9b\u5e73\u8861\u3002", "conclusion": "\u7ed3\u8bba\u662f\u4e3a\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u751f\u6001\u7cfb\u7edf\u9009\u62e9DLT\u57fa\u7840\u8bbe\u65bd\u65f6\uff0c\u9700\u6839\u636e\u6570\u636e\u4e3b\u6743\u9700\u6c42\u6743\u8861\u6548\u7387\u548c\u53ef\u5ba1\u8ba1\u6027\u3002"}}
{"id": "2507.07223", "pdf": "https://arxiv.org/pdf/2507.07223", "abs": "https://arxiv.org/abs/2507.07223", "authors": ["Myoungsoo Jung"], "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure", "categories": ["cs.DC", "cs.AR", "B.4.3; C.0; C.2.1; C.2.2"], "comment": null, "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCXL\u7684\u6a21\u5757\u5316\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982LLMs\u548cRAG\uff09\u5728\u5185\u5b58\u3001\u901a\u4fe1\u5e26\u5bbd\u548c\u8d44\u6e90\u7075\u6d3b\u6027\u65b9\u9762\u7684\u9700\u6c42\u3002\u901a\u8fc7\u4f18\u5316XLink\u4e92\u8fde\u548c\u5206\u5c42\u5185\u5b58\u6a21\u578b\uff0c\u63d0\u5347\u4e86AI\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u6269\u5c55\u6027\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edfGPU-centric\u67b6\u6784\u56e0GPU\u95f4\u901a\u4fe1\u5f00\u9500\u589e\u52a0\u800c\u96be\u4ee5\u6269\u5c55\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eCXL\u7684\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408XLink\u4f18\u5316\u4e92\u8fde\u548c\u5206\u5c42\u5185\u5b58\u6a21\u578b\uff08\u672c\u5730\u4e0e\u6c60\u5316\u5185\u5b58\u7ed3\u5408\uff09\uff0c\u5e76\u8bc4\u4f30\u8f7b\u91cf\u7ea7CXL\u5b9e\u73b0\u3001HBM\u548c\u7845\u5149\u5b50\u6280\u672f\u3002", "result": "\u901a\u8fc7\u6df7\u5408CXL-over-XLink\u8bbe\u8ba1\u51cf\u5c11\u957f\u8ddd\u79bb\u6570\u636e\u4f20\u8f93\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3001\u541e\u5410\u91cf\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u65b0\u578b\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86AI\u57fa\u7840\u8bbe\u65bd\u7684\u6269\u5c55\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21AI\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07471", "pdf": "https://arxiv.org/pdf/2507.07471", "abs": "https://arxiv.org/abs/2507.07471", "authors": ["Johannes Wehrstein", "Timo Eckmann", "Roman Heinrich", "Carsten Binnig"], "title": "JOB-Complex: A Challenging Benchmark for Traditional & Learned Query Optimization", "categories": ["cs.DB"], "comment": "Accepted to AIDB@VLDB'25", "summary": "Query optimization is a fundamental task in database systems that is crucial\nto providing high performance. To evaluate learned and traditional optimizer's\nperformance, several benchmarks, such as the widely used JOB benchmark, are\nused. However, in this paper, we argue that existing benchmarks are inherently\nlimited, as they do not reflect many real-world properties of query\noptimization, thus overstating the performance of both traditional and learned\noptimizers. In fact, simple but realistic properties, such as joins over string\ncolumns or complex filter predicates, can drastically reduce the performance of\nexisting query optimizers. Thus, we introduce JOB-Complex, a new benchmark\ndesigned to challenge traditional and learned query optimizers by reflecting\nreal-world complexity. Overall, JOB-Complex contains 30 SQL queries and comes\ntogether with a plan-selection benchmark containing nearly 6000 execution\nplans, making it a valuable resource to evaluate the performance of query\noptimizers and cost models in real-world scenarios. In our evaluation, we show\nthat traditional and learned cost models struggle to achieve high performance\non JOB-Complex, providing a runtime of up to 11x slower compared to the optimal\nplans.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u73b0\u6709\u7684\u67e5\u8be2\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982JOB\u57fa\u51c6\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5JOB-Complex\uff0c\u4ee5\u6311\u6218\u4f20\u7edf\u548c\u5b66\u4e60\u7684\u67e5\u8be2\u4f18\u5316\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u4f53\u73b0\u771f\u5b9e\u4e16\u754c\u4e2d\u67e5\u8be2\u4f18\u5316\u7684\u590d\u6742\u7279\u6027\uff08\u5982\u5b57\u7b26\u4e32\u5217\u8fde\u63a5\u6216\u590d\u6742\u8fc7\u6ee4\u6761\u4ef6\uff09\uff0c\u5bfc\u81f4\u5bf9\u4f20\u7edf\u548c\u5b66\u4e60\u4f18\u5316\u5668\u6027\u80fd\u7684\u9ad8\u4f30\u3002", "method": "\u4f5c\u8005\u63d0\u51faJOB-Complex\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b30\u4e2aSQL\u67e5\u8be2\u548c\u8fd16000\u4e2a\u6267\u884c\u8ba1\u5212\uff0c\u65e8\u5728\u6a21\u62df\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f20\u7edf\u548c\u5b66\u4e60\u7684\u6210\u672c\u6a21\u578b\u5728JOB-Complex\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd0\u884c\u65f6\u95f4\u6bd4\u6700\u4f18\u8ba1\u5212\u616211\u500d\u3002", "conclusion": "JOB-Complex\u4e3a\u8bc4\u4f30\u67e5\u8be2\u4f18\u5316\u5668\u548c\u6210\u672c\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6807\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.07689", "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "categories": ["cs.SE"], "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u578b\u652f\u6301\u592a\u7a7a\u9886\u57df\u9700\u6c42\u7684\u534a\u81ea\u52a8\u5316\u751f\u6210\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7684AI\u65b9\u6cd5\u5904\u7406\u6587\u6863\u3001\u5206\u7c7b\u5185\u5bb9\u5e76\u5408\u6210\u9700\u6c42\u8349\u6848\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a\u53ef\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u548c\u63d0\u5347\u9700\u6c42\u8986\u76d6\u7387\u3002", "motivation": "\u5c0f\u89c4\u6a21\u592a\u7a7a\u7ec4\u7ec7\u548c\u65b0\u5174\u4f01\u4e1a\u5728\u4ece\u5927\u91cf\u975e\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u63d0\u53d6\u53ef\u64cd\u4f5c\u9700\u6c42\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u9700\u8981\u9ad8\u7cbe\u5ea6\u4e14\u7b26\u5408\u4e25\u683c\u6807\u51c6\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528RAG\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5bf9\u539f\u59cb\u592a\u7a7a\u4efb\u52a1\u6587\u6863\u8fdb\u884c\u9884\u5904\u7406\u3001\u5206\u7c7b\uff0c\u68c0\u7d22\u9886\u57df\u6807\u51c6\u76f8\u5173\u5185\u5bb9\u5e76\u5408\u6210\u9700\u6c42\u8349\u6848\u3002", "result": "\u521d\u6b65\u5e94\u7528\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3001\u63d0\u5347\u9700\u6c42\u8986\u76d6\u7387\uff0c\u5e76\u652f\u6301\u8f7b\u91cf\u7ea7\u5408\u89c4\u6027\u5bf9\u9f50\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u964d\u4f4e\u5c0f\u7ec4\u7ec7\u53c2\u4e0e\u5927\u89c4\u6a21\u5b89\u5168\u5173\u952e\u4efb\u52a1\u7684\u969c\u788d\u3002"}}
{"id": "2507.07387", "pdf": "https://arxiv.org/pdf/2507.07387", "abs": "https://arxiv.org/abs/2507.07387", "authors": ["Chengan He", "Jorge Alejandro Amador Herrera", "Zhixin Shu", "Xin Sun", "Yao Feng", "S\u00f6ren Pirk", "Dominik L. Michels", "Meng Zhang", "Tuanfeng Y. Wang", "Julie Dorsey", "Holly Rushmeier", "Yi Zhou"], "title": "Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "We introduce Digital Salon, a comprehensive hair authoring system that\nsupports real-time 3D hair generation, simulation, and rendering. Unlike\nexisting methods that focus on isolated parts of 3D hair modeling and involve a\nheavy computation process or network training, Digital Salon offers a holistic\nand interactive system that lowers the technical barriers of 3D hair modeling\nthrough natural language-based interaction. The system guides users through\nfour key stages: text-guided hair retrieval, real-time hair simulation,\ninteractive hair refinement, and hair-conditioned image generation. This\ncohesive workflow makes advanced hair design accessible to users of varying\nskill levels and dramatically streamlines the creative process in digital media\nwith an intuitive, versatile, and efficient solution for hair modeling. User\nstudies show that our system can outperform traditional hair modeling workflows\nfor rapid prototyping. Furthermore, we provide insights into the benefits of\nour system with future potential of deploying our system in real salon\nenvironments. More details can be found on our project page:\nhttps://digital-salon.github.io/.", "AI": {"tldr": "Digital Salon \u662f\u4e00\u4e2a\u5168\u9762\u76843D\u5934\u53d1\u521b\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u652f\u6301\u5b9e\u65f6\u751f\u6210\u3001\u6a21\u62df\u548c\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e13\u6ce8\u4e8e3D\u5934\u53d1\u5efa\u6a21\u7684\u5b64\u7acb\u90e8\u5206\uff0c\u4e14\u8ba1\u7b97\u91cf\u5927\u6216\u9700\u8981\u7f51\u7edc\u8bad\u7ec3\uff0cDigital Salon\u65e8\u5728\u63d0\u4f9b\u66f4\u6574\u4f53\u548c\u4ea4\u4e92\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u56db\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u6587\u672c\u5f15\u5bfc\u7684\u5934\u53d1\u68c0\u7d22\u3001\u5b9e\u65f6\u5934\u53d1\u6a21\u62df\u3001\u4ea4\u4e92\u5f0f\u5934\u53d1\u7ec6\u5316\u53ca\u5934\u53d1\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\uff0c\u672a\u6765\u53ef\u80fd\u5728\u771f\u5b9e\u6c99\u9f99\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "conclusion": "Digital Salon\u4e3a\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u76f4\u89c2\u7684\u5934\u53d1\u5efa\u6a21\u5de5\u5177\uff0c\u6781\u5927\u7b80\u5316\u4e86\u6570\u5b57\u5a92\u4f53\u4e2d\u7684\u521b\u610f\u6d41\u7a0b\u3002"}}
{"id": "2507.07938", "pdf": "https://arxiv.org/pdf/2507.07938", "abs": "https://arxiv.org/abs/2507.07938", "authors": ["Abolfazl Zarghani", "Amirhossein Ebrahimi", "Amir Malekesfandiari"], "title": "Multimodal Framework for Explainable Autonomous Driving: Integrating Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency", "categories": ["cs.MM"], "comment": null, "summary": "Autonomous vehicles (AVs) are poised to redefine transportation by enhancing\nroad safety, minimizing human error, and optimizing traffic efficiency. The\nsuccess of AVs depends on their ability to interpret complex, dynamic\nenvironments through diverse data sources, including video streams, sensor\nmeasurements, and contextual textual information. However, seamlessly\nintegrating these multimodal inputs and ensuring transparency in AI-driven\ndecisions remain formidable challenges. This study introduces a novel\nmultimodal framework that synergistically combines video, sensor, and textual\ndata to predict driving actions while generating human-readable explanations,\nfostering trust and regulatory compliance. By leveraging VideoMAE for\nspatiotemporal video analysis, a custom sensor fusion module for real-time data\nprocessing, and BERT for textual comprehension, our approach achieves robust\ndecision-making and interpretable outputs. Evaluated on the BDD-X (21113\nsamples) and nuScenes (1000 scenes) datasets, our model reduces training loss\nfrom 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy\nof 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming\nstate-of-the-art methods. Ablation studies confirm the critical role of each\nmodality, while qualitative analyses and human evaluations highlight the\nmodel's ability to produce contextually rich, user-friendly explanations. These\nadvancements underscore the transformative potential of multimodal integration\nand explainability in building safe, transparent, and trustworthy AV systems,\npaving the way for broader societal adoption of autonomous driving\ntechnologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u9891\u3001\u4f20\u611f\u5668\u548c\u6587\u672c\u6570\u636e\u7684\u65b0\u578b\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u884c\u4e3a\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\u7684\u51c6\u786e\u7406\u89e3\u548c\u51b3\u7b56\u900f\u660e\u5ea6\uff0c\u4f46\u591a\u6a21\u6001\u6570\u636e\u7684\u65e0\u7f1d\u6574\u5408\u548cAI\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u91cd\u5927\u6311\u6218\u3002", "method": "\u7ed3\u5408VideoMAE\u8fdb\u884c\u65f6\u7a7a\u89c6\u9891\u5206\u6790\uff0c\u5b9a\u5236\u4f20\u611f\u5668\u878d\u5408\u6a21\u5757\u5b9e\u65f6\u5904\u7406\u6570\u636e\uff0c\u4ee5\u53caBERT\u7406\u89e3\u6587\u672c\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\u3002", "result": "\u5728BDD-X\u548cnuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u8bad\u7ec3\u635f\u5931\u4ece5.7231\u964d\u81f30.0187\uff0c\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u7387\u8fbe92.5%\uff0c\u89e3\u91ca\u8d28\u91cfBLEU-4\u5f97\u52060.75\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u96c6\u6210\u548c\u53ef\u89e3\u91ca\u6027\u4e3a\u6784\u5efa\u5b89\u5168\u3001\u900f\u660e\u4e14\u53ef\u4fe1\u8d56\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a81\u7834\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.07576", "pdf": "https://arxiv.org/pdf/2507.07576", "abs": "https://arxiv.org/abs/2507.07576", "authors": ["Mohamed Siala", "Jordi Planes", "Joao Marques-Silva"], "title": "On Trustworthy Rule-Based Models and Explanations", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "A task of interest in machine learning (ML) is that of ascribing explanations\nto the predictions made by ML models. Furthermore, in domains deemed high risk,\nthe rigor of explanations is paramount. Indeed, incorrect explanations can and\nwill mislead human decision makers. As a result, and even if interpretability\nis acknowledged as an elusive concept, so-called interpretable models are\nemployed ubiquitously in high-risk uses of ML and data mining (DM). This is the\ncase for rule-based ML models, which encompass decision trees, diagrams, sets\nand lists. This paper relates explanations with well-known undesired facets of\nrule-based ML models, which include negative overlap and several forms of\nredundancy. The paper develops algorithms for the analysis of these undesired\nfacets of rule-based systems, and concludes that well-known and widely used\ntools for learning rule-based ML models will induce rule sets that exhibit one\nor more negative facets.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u89c4\u5219\u578b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u89e3\u91ca\u9884\u6d4b\u65f6\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u5982\u8d1f\u9762\u91cd\u53e0\u548c\u5197\u4f59\uff0c\u5e76\u5f00\u53d1\u4e86\u7b97\u6cd5\u6765\u8bc6\u522b\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9519\u8bef\u7684\u89e3\u91ca\u53ef\u80fd\u8bef\u5bfc\u51b3\u7b56\u8005\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u8bc4\u4f30\u89c4\u5219\u578b\u6a21\u578b\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u7b97\u6cd5\u5206\u6790\u89c4\u5219\u578b\u6a21\u578b\u4e2d\u7684\u8d1f\u9762\u91cd\u53e0\u548c\u5197\u4f59\u7b49\u4e0d\u53d7\u6b22\u8fce\u7684\u7279\u5f81\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c4\u5219\u578b\u5b66\u4e60\u5de5\u5177\u4f1a\u5bfc\u81f4\u89c4\u5219\u96c6\u51fa\u73b0\u8fd9\u4e9b\u8d1f\u9762\u7279\u5f81\u3002", "conclusion": "\u73b0\u6709\u89c4\u5219\u578b\u5b66\u4e60\u5de5\u5177\u53ef\u80fd\u5bfc\u81f4\u89e3\u91ca\u8d28\u91cf\u4e0d\u4f73\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.07550", "pdf": "https://arxiv.org/pdf/2507.07550", "abs": "https://arxiv.org/abs/2507.07550", "authors": ["Marianne Bossema", "Rob Saunders", "Aske Plaat", "Somaya Ben Allouch"], "title": "Pluri-perspectivism in Human-robot Co-creativity with Older Adults", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "This position paper explores pluriperspectivism as a core element of human\ncreative experience and its relevance to humanrobot cocreativity We propose a\nlayered fivedimensional model to guide the design of cocreative behaviors and\nthe analysis of interaction dynamics This model is based on literature and\nresults from an interview study we conducted with 10 visual artists and 8 arts\neducators examining how pluriperspectivism supports creative practice The\nfindings of this study provide insight in how robots could enhance human\ncreativity through adaptive contextsensitive behavior demonstrating the\npotential of pluriperspectivism This paper outlines future directions for\nintegrating pluriperspectivism with visionlanguage models VLMs to support\ncontext sensitivity in cocreative robots", "AI": {"tldr": "\u63a2\u8ba8\u591a\u89c6\u89d2\u4e3b\u4e49\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u540c\u521b\u4f5c\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u4e94\u7ef4\u6a21\u578b\u6765\u6307\u5bfc\u8bbe\u8ba1\u548c\u5206\u6790\u4ea4\u4e92\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u591a\u89c6\u89d2\u4e3b\u4e49\u5982\u4f55\u652f\u6301\u521b\u9020\u6027\u5b9e\u8df5\uff0c\u5e76\u63a2\u7d22\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u521b\u9020\u529b\u4e2d\u7684\u6f5c\u5728\u589e\u5f3a\u4f5c\u7528\u3002", "method": "\u57fa\u4e8e\u6587\u732e\u548c\u4e0e\u89c6\u89c9\u827a\u672f\u5bb6\u53ca\u827a\u672f\u6559\u80b2\u8005\u7684\u8bbf\u8c08\u7814\u7a76\uff0c\u63d0\u51fa\u4e94\u7ef4\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u89c6\u89d2\u4e3b\u4e49\u80fd\u901a\u8fc7\u81ea\u9002\u5e94\u3001\u60c5\u5883\u654f\u611f\u7684\u884c\u4e3a\u589e\u5f3a\u4eba\u7c7b\u521b\u9020\u529b\u3002", "conclusion": "\u672a\u6765\u53ef\u5c06\u591a\u89c6\u89d2\u4e3b\u4e49\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u534f\u540c\u521b\u4f5c\u673a\u5668\u4eba\u7684\u60c5\u5883\u654f\u611f\u6027\u3002"}}
{"id": "2507.07437", "pdf": "https://arxiv.org/pdf/2507.07437", "abs": "https://arxiv.org/abs/2507.07437", "authors": ["Jiasheng Wu", "Shaojie Su", "Wenjun Zhu", "Xiong Wang", "Jingjing Zhang", "Xingqiu He", "Yue Gao"], "title": "PHandover: Parallel Handover in Mobile Satellite Network", "categories": ["cs.NI"], "comment": "14 pages, 14 figures", "summary": "The construction of Low Earth Orbit (LEO) satellite constellations has\nrecently attracted tremendous attention from both academia and industry. The 5G\nand 6G standards have identified LEO satellite networks as a key component of\nfuture mobile networks. However, due to the high-speed movement of satellites,\nground terminals often experience frequent and high-latency handovers, which\nsignificantly deteriorate the performance of latency-sensitive applications. To\naddress this challenge, we propose a parallel handover mechanism for mobile\nsatellite networks that can considerably reduce handover latency. The main idea\nis to employ plan-based handovers instead of measurement-based handovers to\navoid interactions between the access and core networks, thereby eliminating\nthe significant time overhead associated with traditional handover procedures.\nSpecifically, we introduce a novel network function named the Satellite\nSynchronized Function (SSF), which is designed to be fully compliant with the\nstandard 5G core network. In addition, we propose a machine learning model for\nsignal strength prediction, coupled with an efficient handover scheduling\nalgorithm. We have conducted extensive experiments, and the results demonstrate\nthat our proposed handover scheme can reduce handover latency by 21\\times\ncompared to the standard NTN handover scheme and two other existing handover\napproaches, along with significant improvements in network stability and\nuser-level performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u5207\u6362\u673a\u5236\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u5212\u7684\u5207\u6362\u548c\u536b\u661f\u540c\u6b65\u529f\u80fd\uff08SSF\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86LEO\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u5207\u6362\u5ef6\u8fdf\uff0c\u5b9e\u9a8c\u8868\u660e\u5ef6\u8fdf\u51cf\u5c11\u4e8621\u500d\u3002", "motivation": "LEO\u536b\u661f\u7f51\u7edc\u56e0\u5176\u9ad8\u901f\u79fb\u52a8\u5bfc\u81f4\u9891\u7e41\u9ad8\u5ef6\u8fdf\u5207\u6362\uff0c\u5f71\u54cd\u65f6\u5ef6\u654f\u611f\u5e94\u7528\u6027\u80fd\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8ba1\u5212\u7684\u5207\u6362\u66ff\u4ee3\u57fa\u4e8e\u6d4b\u91cf\u7684\u5207\u6362\uff0c\u5f15\u5165SSF\u529f\u80fd\uff0c\u7ed3\u5408\u4fe1\u53f7\u5f3a\u5ea6\u9884\u6d4b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u9ad8\u6548\u5207\u6362\u8c03\u5ea6\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65b9\u6848\u8f83\u6807\u51c6NTN\u5207\u6362\u548c\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1121\u500d\u5ef6\u8fdf\uff0c\u7f51\u7edc\u7a33\u5b9a\u6027\u548c\u7528\u6237\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5e76\u884c\u5207\u6362\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LEO\u536b\u661f\u7f51\u7edc\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5177\u59075G\u6838\u5fc3\u7f51\u517c\u5bb9\u6027\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.07416", "pdf": "https://arxiv.org/pdf/2507.07416", "abs": "https://arxiv.org/abs/2507.07416", "authors": ["Jenifer Paulraj", "Brindha Raghuraman", "Nagarani Gopalakrishnan", "Yazan Otoum"], "title": "Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "comment": "7 pages, IEEE conference", "summary": "Critical infrastructure systems, including energy grids, healthcare\nfacilities, transportation networks, and water distribution systems, are\npivotal to societal stability and economic resilience. However, the increasing\ninterconnectivity of these systems exposes them to various cyber threats,\nincluding ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent\nThreats (APTs). This paper examines cybersecurity vulnerabilities in critical\ninfrastructure, highlighting the threat landscape, attack vectors, and the role\nof Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid\nAI-driven cybersecurity framework to enhance real-time vulnerability detection,\nthreat modelling, and automated remediation. This study also addresses the\ncomplexities of adversarial AI, regulatory compliance, and integration. Our\nfindings provide actionable insights to strengthen the security and resilience\nof critical infrastructure systems against emerging cyber threats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408AI\u9a71\u52a8\u7684\u7f51\u7edc\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u65f6\u6f0f\u6d1e\u68c0\u6d4b\u3001\u5a01\u80c1\u5efa\u6a21\u548c\u81ea\u52a8\u4fee\u590d\u80fd\u529b\u3002", "motivation": "\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u5bf9\u793e\u4f1a\u7a33\u5b9a\u548c\u7ecf\u6d4e\u97e7\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u65e5\u76ca\u4e92\u8054\u7684\u7279\u6027\u4f7f\u5176\u9762\u4e34\u591a\u79cd\u7f51\u7edc\u5a01\u80c1\uff0c\u5982\u52d2\u7d22\u8f6f\u4ef6\u3001\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u548c\u9ad8\u7ea7\u6301\u7eed\u6027\u5a01\u80c1\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u7f51\u7edc\u5b89\u5168\u7684\u8106\u5f31\u6027\uff0c\u5305\u62ec\u5a01\u80c1\u6001\u52bf\u3001\u653b\u51fb\u9014\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408AI\u9a71\u52a8\u7684\u7f51\u7edc\u5b89\u5168\u6846\u67b6\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4ee5\u589e\u5f3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u5bf9\u65b0\u5174\u7f51\u7edc\u5a01\u80c1\u7684\u5b89\u5168\u6027\u548c\u97e7\u6027\u3002", "conclusion": "AI\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\u5b89\u5168\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4e5f\u9700\u89e3\u51b3\u5bf9\u6297\u6027AI\u3001\u6cd5\u89c4\u9075\u4ece\u6027\u548c\u96c6\u6210\u590d\u6742\u6027\u7b49\u95ee\u9898\u3002"}}
{"id": "2507.07979", "pdf": "https://arxiv.org/pdf/2507.07979", "abs": "https://arxiv.org/abs/2507.07979", "authors": ["Benedikt T. Arnold", "Christoph Lange", "Christina Gillmann", "Stefan Decker"], "title": "A Service Architecture for Dataspaces", "categories": ["cs.DB"], "comment": "Preprint. Under review", "summary": "Dataspaces are designed to support sovereign, trusted and decentralized data\nexchange between participants forming an ecosystem. They are standardized by\ninitiatives such as the International Data Spaces Association or Gaia-X and\nhave gained adoption in several domains such as mobility, manufacturing,\ntourism or culture. In dataspaces, participants use connectors to communicate\npeer-to-peer. The Eclipse Dataspace Components (EDC) Connector is a broadly\nadopted, open-source implementation that adheres to the standards and is\nsupported by a large community. As dataspaces in general, it focuses on the\nexchange of data assets with associated usage policies and does not support\nservices. In practice, however, there is demand for dataspace-based services\nand conceptual arguments support their inclusion in dataspaces. In this paper,\nwe propose an abstraction layer for providing generic services within\ndataspaces. Adopters can use this layer to easily develop own services,\nseamlessly integrated with the existing dataspace technology. Besides, we\npresent an initial implementation of this service architecture for the EDC\nConnector and demonstrate its practical applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6570\u636e\u7a7a\u95f4\u4e2d\u63d0\u4f9b\u901a\u7528\u670d\u52a1\u7684\u62bd\u8c61\u5c42\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728EDC Connector\u4e2d\u7684\u5b9e\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u7a7a\u95f4\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u8d44\u4ea7\u4ea4\u6362\uff0c\u4f46\u5b9e\u9645\u4e0a\u5bf9\u57fa\u4e8e\u6570\u636e\u7a7a\u95f4\u7684\u670d\u52a1\u6709\u9700\u6c42\uff0c\u6982\u5ff5\u4e0a\u4e5f\u9700\u8981\u5c06\u5176\u7eb3\u5165\u6570\u636e\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u62bd\u8c61\u5c42\uff0c\u652f\u6301\u5728\u6570\u636e\u7a7a\u95f4\u4e2d\u8f7b\u677e\u5f00\u53d1\u4e0e\u73b0\u6709\u6280\u672f\u65e0\u7f1d\u96c6\u6210\u7684\u670d\u52a1\uff0c\u5e76\u4ee5EDC Connector\u4e3a\u4f8b\u8fdb\u884c\u5b9e\u73b0\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u670d\u52a1\u67b6\u6784\u7684\u521d\u6b65\u5b9e\u73b0\u53ca\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u62bd\u8c61\u5c42\u7684\u63d0\u51fa\u4e3a\u6570\u636e\u7a7a\u95f4\u670d\u52a1\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u4fbf\u5229\uff0c\u6269\u5c55\u4e86\u6570\u636e\u7a7a\u95f4\u7684\u529f\u80fd\u3002"}}
{"id": "2507.07117", "pdf": "https://arxiv.org/pdf/2507.07117", "abs": "https://arxiv.org/abs/2507.07117", "authors": ["Jit Gupta", "Andrew Li", "Tarun Banka", "Ariel Cohen", "T. Sridhar", "Raj Yavatkar"], "title": "Collective Communication Profiling of Modern-day Machine Learning Workloads", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": "Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA", "summary": "Machine Learning jobs, carried out on large number of distributed high\nperformance systems, involve periodic communication using operations like\nAllReduce, AllGather, and Broadcast. These operations may create high bandwidth\nand bursty traffic patterns, leading to network congestion and packet loss,\nthus impacting the performance of these jobs. Hence it is imperative to analyze\nthese patterns, which can be helpful in provisioning network resources\ndepending on the type of machine learning workloads. In this poster we carry\nout extensive analysis of the collective communication behavior seen in a wide\nvariety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we\ninstrument Nvidia Collective Communication Library logging functionality for\nricher context about the collectives and workloads. We adjust configuration\nparameters that influence collective communication behavior, such as\nparallelism, number of nodes, and model type. This overview presents and\ndiscusses some of the results on the collective communication behavior for the\nopen source DeepSeek V3 inferencing model, which includes operation type and\ncount, transfer sizes per operation, and request size distribution. Our\nanalysis shows that it makes sense to rethink current collective communication\nframeworks and network topologies so as to accommodate the effect of network\nanomalies on the mentioned workloads.", "AI": {"tldr": "\u5206\u6790\u4e86\u5206\u5e03\u5f0f\u9ad8\u6027\u80fd\u7cfb\u7edf\u4e2d\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u96c6\u4f53\u901a\u4fe1\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u7f51\u7edc\u8d44\u6e90\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u96c6\u4f53\u901a\u4fe1\u64cd\u4f5c\u53ef\u80fd\u5f15\u53d1\u9ad8\u5e26\u5bbd\u548c\u7a81\u53d1\u6027\u6d41\u91cf\uff0c\u5bfc\u81f4\u7f51\u7edc\u62e5\u585e\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u5206\u6790\u4ee5\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "method": "\u5229\u7528Nvidia Collective Communication Library\u8bb0\u5f55\u901a\u4fe1\u884c\u4e3a\uff0c\u8c03\u6574\u5e76\u884c\u5ea6\u3001\u8282\u70b9\u6570\u548c\u6a21\u578b\u7c7b\u578b\u7b49\u53c2\u6570\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86DeepSeek V3\u63a8\u7406\u6a21\u578b\u7684\u901a\u4fe1\u884c\u4e3a\u6570\u636e\uff0c\u5305\u62ec\u64cd\u4f5c\u7c7b\u578b\u3001\u4f20\u8f93\u5927\u5c0f\u7b49\uff0c\u8868\u660e\u9700\u6539\u8fdb\u901a\u4fe1\u6846\u67b6\u548c\u7f51\u7edc\u62d3\u6251\u3002", "conclusion": "\u73b0\u6709\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\u548c\u7f51\u7edc\u62d3\u6251\u9700\u8c03\u6574\u4ee5\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7f51\u7edc\u5f02\u5e38\u5f71\u54cd\u3002"}}
{"id": "2507.07448", "pdf": "https://arxiv.org/pdf/2507.07448", "abs": "https://arxiv.org/abs/2507.07448", "authors": ["Otso Kinanen", "Andr\u00e9s D. Mu\u00f1oz-Moller", "Vlad Stirbu", "Tommi Mikkonen"], "title": "Toolchain for Faster Iterations in Quantum Software Development", "categories": ["quant-ph", "cs.SE"], "comment": "arXiv admin note: text overlap with arXiv:2408.06756", "summary": "Quantum computing proposes a revolutionary paradigm that can radically\ntransform numerous scientific and industrial application domains. To realize\nthis promise, these new capabilities need software solutions that are able to\neffectively harness its power. However, developers may face significant\nchallenges when developing and executing quantum software due to the limited\navailability of quantum computer hardware, high computational demands of\nsimulating quantum computers on classical systems, and complicated technology\nstack to enable currently available accelerators into development environments.\nThese limitations make it difficult for the developer to create an efficient\nworkflow for quantum software development. In this paper, we investigate the\npotential of using remote computational capabilities in an efficient manner to\nimprove the workflow of quantum software developers, by lowering the barrier of\nmoving between local execution and computationally more efficient remote\nhardware and offering speedup in execution with simulator surroundings. The\ngoal is to allow the development of more complex circuits and to support an\niterative software development approach. In our experiment, with the solution\npresented in this paper, we have obtained up to 5 times faster circuit\nexecution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple\nplug-and-play kernel for the Jupyter notebook.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u8fdc\u7a0b\u8ba1\u7b97\u80fd\u529b\u4f18\u5316\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u964d\u4f4e\u672c\u5730\u6267\u884c\u4e0e\u8fdc\u7a0b\u786c\u4ef6\u4e4b\u95f4\u7684\u8f6c\u6362\u95e8\u69db\uff0c\u63d0\u5347\u6267\u884c\u901f\u5ea6\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5f00\u53d1\u8005\u5728\u5f00\u53d1\u91cf\u5b50\u8f6f\u4ef6\u65f6\u9762\u4e34\u786c\u4ef6\u9650\u5236\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u548c\u6280\u672f\u6808\u590d\u6742\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5229\u7528\u8fdc\u7a0b\u8ba1\u7b97\u80fd\u529b\uff0c\u4f18\u5316\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u548c\u8fed\u4ee3\u5f00\u53d1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65b9\u6848\u5b9e\u73b0\u4e86\u9ad8\u8fbe5\u500d\u7684\u7535\u8def\u6267\u884c\u52a0\u901f\uff0c\u652f\u630121\u81f329\u91cf\u5b50\u4f4d\u7684\u5f00\u53d1\u3002", "conclusion": "\u8fdc\u7a0b\u8ba1\u7b97\u80fd\u529b\u80fd\u663e\u8457\u63d0\u5347\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u5e94\u7528\u5f00\u53d1\u3002"}}
{"id": "2507.07440", "pdf": "https://arxiv.org/pdf/2507.07440", "abs": "https://arxiv.org/abs/2507.07440", "authors": ["Yue Li", "Gene Wei-Chin Lin", "Egor Larionov", "Aljaz Bozic", "Doug Roble", "Ladislav Kavan", "Stelian Coros", "Bernhard Thomaszewski", "Tuur Stuyck", "Hsiao-yu Chen"], "title": "Self-supervised Learning of Latent Space Dynamics", "categories": ["cs.GR"], "comment": null, "summary": "Modeling the dynamic behavior of deformable objects is crucial for creating\nrealistic digital worlds. While conventional simulations produce high-quality\nmotions, their computational costs are often prohibitive. Subspace simulation\ntechniques address this challenge by restricting deformations to a\nlower-dimensional space, improving performance while maintaining visually\ncompelling results. However, even subspace methods struggle to meet the\nstringent performance demands of portable devices such as virtual reality\nheadsets and mobile platforms. To overcome this limitation, we introduce a\nnovel subspace simulation framework powered by a neural latent-space\nintegrator. Our approach leverages self-supervised learning to enhance\ninference stability and generalization. By operating entirely within latent\nspace, our method eliminates the need for full-space computations, resulting in\na highly efficient method well-suited for deployment on portable devices. We\ndemonstrate the effectiveness of our approach on challenging examples involving\nrods, shells, and solids, showcasing its versatility and potential for\nwidespread adoption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6f5c\u5728\u7a7a\u95f4\u79ef\u5206\u5668\u7684\u65b0\u578b\u5b50\u7a7a\u95f4\u6a21\u62df\u6846\u67b6\uff0c\u4ee5\u9ad8\u6548\u6a21\u62df\u53d8\u5f62\u7269\u4f53\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u4fbf\u643a\u8bbe\u5907\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5b50\u7a7a\u95f4\u65b9\u6cd5\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4ecd\u96be\u4ee5\u6ee1\u8db3\u4fbf\u643a\u8bbe\u5907\u7684\u4e25\u683c\u6027\u80fd\u9700\u6c42\u3002", "method": "\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b8c\u5168\u5728\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u907f\u514d\u5168\u7a7a\u95f4\u8ba1\u7b97\u3002", "result": "\u5728\u6746\u3001\u58f3\u4f53\u548c\u56fa\u4f53\u7b49\u590d\u6742\u793a\u4f8b\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9002\u5408\u4fbf\u643a\u8bbe\u5907\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07108", "pdf": "https://arxiv.org/pdf/2507.07108", "abs": "https://arxiv.org/abs/2507.07108", "authors": ["Zhiwei Hu", "V\u00edctor Guti\u00e9rrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Accepted at KDD 2025", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u6a21\u578bMMoE\uff0c\u901a\u8fc7\u591a\u7ea7\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u52a8\u6001\u9009\u62e9\u6a21\u6001\u4fe1\u606f\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u63d0\u53ca\u6a21\u7cca\u6027\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u63d0\u53ca\u6a21\u7cca\u6027\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\uff0c\u963b\u788d\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "MMoE\u6a21\u578b\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u63d0\u53ca\u63cf\u8ff0\uff0c\u63d0\u53d6\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u52a8\u6001\u9009\u62e9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMMoE\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MMoE\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u548c\u589e\u5f3a\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.07942", "pdf": "https://arxiv.org/pdf/2507.07942", "abs": "https://arxiv.org/abs/2507.07942", "authors": ["Joshua Brakensiek", "Venkatesan Guruswami", "Bart M. P. Jansen", "Victor Lagerkvist", "Magnus Wahlstr\u00f6m"], "title": "The Richness of CSP Non-redundancy", "categories": ["cs.DM", "cs.CC", "cs.LO", "math.CO"], "comment": "82 pages, 5 figures", "summary": "In the field of constraint satisfaction problems (CSP), a clause is called\nredundant if its satisfaction is implied by satisfying all other clauses. An\ninstance of CSP$(P)$ is called non-redundant if it does not contain any\nredundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum\nnumber of clauses in a non-redundant instance of CSP$(P)$, as a function of the\nnumber of variables $n$. Recent progress has shown that non-redundancy is\ncrucially linked to many other important questions in computer science and\nmathematics including sparsification, kernelization, query complexity,\nuniversal algebra, and extremal combinatorics. Given that non-redundancy is a\nnexus for many of these important problems, the central goal of this paper is\nto more deeply understand non-redundancy.\n  Our first main result shows that for every rational number $r \\ge 1$, there\nexists a finite CSP predicate $P$ such that the non-redundancy of $P$ is\n$\\Theta(n^r)$. Our second main result explores the concept of conditional\nnon-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We\ncompletely classify the conditional non-redundancy of all binary predicates\n(i.e., constraints on two variables) by connecting these non-redundancy\nproblems to the structure of high-girth graphs in extremal combinatorics.\n  Inspired by these concrete results, we build off the work of Carbonnel [CP\n2022] to develop an algebraic theory of conditional non-redundancy. As an\napplication of this algebraic theory, we revisit the notion of Mal'tsev\nembeddings, which is the most general technique known to date for establishing\nthat a predicate has linear non-redundancy. For example, we provide the first\nexample of predicate with a Mal'tsev embedding that cannot be attributed to the\nstructure of an Abelian group, but rather to the structure of the quantum Pauli\ngroup.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u4e2d\u7684\u975e\u5197\u4f59\u6027\uff08NRD\uff09\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6709\u9650CSP\u8c13\u8bcd\u8868\u8fbe\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u975e\u5197\u4f59\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u6761\u4ef6\u975e\u5197\u4f59\u6027\u7684\u5206\u7c7b\u4e0e\u4ee3\u6570\u7406\u8bba\u3002", "motivation": "\u975e\u5197\u4f59\u6027\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6570\u5b66\u4e2d\u5177\u6709\u91cd\u8981\u5730\u4f4d\uff0c\u4e0e\u7a00\u758f\u5316\u3001\u6838\u5316\u3001\u67e5\u8be2\u590d\u6742\u5ea6\u7b49\u95ee\u9898\u7d27\u5bc6\u76f8\u5173\u3002\u8bba\u6587\u65e8\u5728\u6df1\u5165\u7406\u89e3\u975e\u5197\u4f59\u6027\u53ca\u5176\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6784\u9020\u6709\u9650CSP\u8c13\u8bcd\u5c55\u793a\u975e\u5197\u4f59\u6027\u7684\u591a\u9879\u5f0f\u590d\u6742\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6781\u503c\u7ec4\u5408\u5b66\u4e2d\u7684\u9ad8\u56f4\u957f\u56fe\u7406\u8bba\u5bf9\u4e8c\u5143\u8c13\u8bcd\u7684\u6761\u4ef6\u975e\u5197\u4f59\u6027\u8fdb\u884c\u5206\u7c7b\u3002\u8fdb\u4e00\u6b65\uff0c\u53d1\u5c55\u4e86\u6761\u4ef6\u975e\u5197\u4f59\u6027\u7684\u4ee3\u6570\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u4e86\u5b58\u5728\u6709\u9650CSP\u8c13\u8bcd\u7684\u975e\u5197\u4f59\u6027\u4e3a\u0398(n^r)\uff0c\u4e14\u5206\u7c7b\u4e86\u6240\u6709\u4e8c\u5143\u8c13\u8bcd\u7684\u6761\u4ef6\u975e\u5197\u4f59\u6027\u3002\u4ee3\u6570\u7406\u8bba\u7684\u5e94\u7528\u63ed\u793a\u4e86Mal'tsev\u5d4c\u5165\u7684\u65b0\u4f8b\u5b50\u3002", "conclusion": "\u975e\u5197\u4f59\u6027\u662f\u591a\u4e2a\u91cd\u8981\u95ee\u9898\u7684\u6838\u5fc3\uff0c\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u4e0e\u4ee3\u6570\u65b9\u6cd5\u6df1\u5316\u4e86\u5bf9\u5176\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u5e94\u7528\u4f8b\u5b50\u3002"}}
{"id": "2507.07551", "pdf": "https://arxiv.org/pdf/2507.07551", "abs": "https://arxiv.org/abs/2507.07551", "authors": ["Line Abele", "Gerrit Anders", "Tolgahan Ayd\u0131n", "J\u00fcrgen Buder", "Helen Fischer", "Dominik Kimmel", "Markus Huff"], "title": "ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing", "categories": ["cs.HC", "cs.AI", "cs.DL"], "comment": "56 pages, 7 figures", "summary": "The accelerating growth of photographic collections has outpaced manual\ncataloguing, motivating the use of vision language models (VLMs) to automate\nmetadata generation. This study examines whether Al-generated catalogue\ndescriptions can approximate human-written quality and how generative Al might\nintegrate into cataloguing workflows in archival and museum collections. A VLM\n(InternVL2) generated catalogue descriptions for photographic prints on\nlabelled cardboard mounts with archaeological content, evaluated by archive and\narchaeology experts and non-experts in a human-centered, experimental\nframework. Participants classified descriptions as AI-generated or\nexpert-written, rated quality, and reported willingness to use and trust in AI\ntools. Classification performance was above chance level, with both groups\nunderestimating their ability to detect Al-generated descriptions. OCR errors\nand hallucinations limited perceived quality, yet descriptions rated higher in\naccuracy and usefulness were harder to classify, suggesting that human review\nis necessary to ensure the accuracy and quality of catalogue descriptions\ngenerated by the out-of-the-box model, particularly in specialized domains like\narchaeological cataloguing. Experts showed lower willingness to adopt AI tools,\nemphasizing concerns on preservation responsibility over technical performance.\nThese findings advocate for a collaborative approach where AI supports draft\ngeneration but remains subordinate to human verification, ensuring alignment\nwith curatorial values (e.g., provenance, transparency). The successful\nintegration of this approach depends not only on technical advancements, such\nas domain-specific fine-tuning, but even more on establishing trust among\nprofessionals, which could both be fostered through a transparent and\nexplainable AI pipeline.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u751f\u6210\u7684\u76ee\u5f55\u63cf\u8ff0\u662f\u5426\u80fd\u5ab2\u7f8e\u4eba\u5de5\u8d28\u91cf\uff0c\u4ee5\u53caAI\u5982\u4f55\u878d\u5165\u6863\u6848\u548c\u535a\u7269\u9986\u7684\u7f16\u76ee\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6307\u51faAI\u9700\u4eba\u7c7b\u5ba1\u6838\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u6444\u5f71\u6536\u85cf\u5feb\u901f\u589e\u957f\uff0c\u624b\u52a8\u7f16\u76ee\u96be\u4ee5\u8ddf\u4e0a\uff0c\u56e0\u6b64\u63a2\u7d22\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u81ea\u52a8\u751f\u6210\u5143\u6570\u636e\u3002", "method": "\u5229\u7528VLM\uff08InternVL2\uff09\u4e3a\u8003\u53e4\u7167\u7247\u751f\u6210\u76ee\u5f55\u63cf\u8ff0\uff0c\u7531\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u8bc4\u4f30\u5206\u7c7b\u3001\u8d28\u91cf\u53ca\u4f7f\u7528\u610f\u613f\u3002", "result": "AI\u751f\u6210\u7684\u63cf\u8ff0\u8d28\u91cf\u53d7OCR\u9519\u8bef\u548c\u5e7b\u89c9\u9650\u5236\uff0c\u4f46\u51c6\u786e\u6027\u9ad8\u7684\u63cf\u8ff0\u66f4\u96be\u88ab\u5206\u7c7b\uff1b\u4e13\u5bb6\u5bf9AI\u5de5\u5177\u7684\u63a5\u53d7\u5ea6\u8f83\u4f4e\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u534f\u4f5c\u6a21\u5f0f\uff0cAI\u751f\u6210\u521d\u7a3f\u540e\u7531\u4eba\u7c7b\u5ba1\u6838\uff0c\u5efa\u7acb\u900f\u660e\u53ef\u89e3\u91ca\u7684\u6d41\u7a0b\u4ee5\u589e\u5f3a\u4e13\u4e1a\u4eba\u58eb\u4fe1\u4efb\u3002"}}
{"id": "2507.07481", "pdf": "https://arxiv.org/pdf/2507.07481", "abs": "https://arxiv.org/abs/2507.07481", "authors": ["Wen Zhang", "Aimin Wang", "Jiahui Li", "Geng Sun", "Jiacheng Wang", "Weijie Yuan", "Dusit Niyato"], "title": "Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "The integration of wireless power transfer (WPT) with Internet of Things\n(IoT) offers promising solutions for sensing applications, but faces\nsignificant challenges when deployed in hard-to-access areas such as\nhigh-temperature environments. In such extreme conditions, traditional fixed\nWPT infrastructure cannot be safely installed, and batteries rapidly degrade\ndue to hardware failures. In this paper, we propose an uncrewed aerial vehicle\n(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)\nnetworks deployed in these challenging environments. Specifically, we consider\na practical scenario where a UAV first transfers energy to BLS nodes via WPT,\nenabling these nodes to subsequently transmit their collected data to the UAV\nthrough orthogonal frequency-division multiple access (OFDMA). Then, we\nformulate a multi-objective optimization problem that aims to maximize the fair\ndata collection volume while minimizing the UAV energy consumption through\njoint optimization of transmit power allocation and flight trajectory planning.\nDue to the non-convex nature and dynamic characteristics of this problem,\nconventional optimization methods prove inadequate. To address these\nchallenges, we propose an enhanced soft actor-critic algorithm with\nparameter-free attention, prioritized experience replay, and value-based reward\ncentering (SAC-PPV), thereby improving the exploration efficiency and learning\nstability of the algorithm in complex WPT scenarios. Simulation results\ndemonstrate that the proposed approach consistently outperforms benchmark\nalgorithms under various network configurations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u65e0\u7535\u6c60\u4f20\u611f\u5668\u7f51\u7edc\u6570\u636e\u91c7\u96c6\u4e0e\u65e0\u7ebf\u4f9b\u7535\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u80fd\u91cf\u6d88\u8017\u4e0e\u6570\u636e\u91c7\u96c6\u516c\u5e73\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u6e29\u7b49\u6781\u7aef\u73af\u5883\u4e0b\u65e0\u7ebf\u4f9b\u7535\u4f20\u611f\u5668\u7f51\u7edc\u7684\u90e8\u7f72\u4e0e\u6570\u636e\u91c7\u96c6\u96be\u9898\u3002", "method": "\u7ed3\u5408\u65e0\u4eba\u673a\u4f9b\u7535\u4e0eOFDMA\u6570\u636e\u4f20\u8f93\uff0c\u5e76\u91c7\u7528\u6539\u8fdb\u7684SAC-PPV\u7b97\u6cd5\u4f18\u5316\u529f\u7387\u5206\u914d\u4e0e\u98de\u884c\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u79cd\u7f51\u7edc\u914d\u7f6e\u4e0b\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002", "conclusion": "\u65e0\u4eba\u673a\u8f85\u52a9\u6846\u67b6\u53ca\u6539\u8fdb\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6781\u7aef\u73af\u5883\u4e0b\u7684\u65e0\u7ebf\u4f9b\u7535\u4e0e\u6570\u636e\u91c7\u96c6\u95ee\u9898\u3002"}}
{"id": "2507.07597", "pdf": "https://arxiv.org/pdf/2507.07597", "abs": "https://arxiv.org/abs/2507.07597", "authors": ["Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "title": "Quantum Executor: A Unified Interface for Quantum Computing", "categories": ["quant-ph", "cs.ET", "cs.SE"], "comment": "11 pages, 1 figure", "summary": "As quantum computing evolves from theoretical promise to practical\ndeployment, the demand for robust, portable, and scalable tools for quantum\nsoftware experimentation is growing. This paper introduces Quantum Executor, a\nbackend-agnostic execution engine designed to orchestrate quantum experiments\nacross heterogeneous platforms. Quantum Executor provides a declarative and\nmodular interface that decouples experiment design from backend execution,\nenabling seamless interoperability and code reuse across diverse quantum and\nclassical resources. Key features include support for asynchronous and\ndistributed execution, customizable execution strategies and a unified API for\nmanaging quantum experiments. We illustrate its applicability through two\nlife-like usage scenarios such as automated benchmarking and hybrid validation,\ndiscussing its capacity to streamline quantum development. We conclude by\ndiscussing current limitations and outlining a roadmap for future enhancements.", "AI": {"tldr": "Quantum Executor\u662f\u4e00\u4e2a\u540e\u7aef\u65e0\u5173\u7684\u6267\u884c\u5f15\u64ce\uff0c\u652f\u6301\u8de8\u5f02\u6784\u5e73\u53f0\u8fdb\u884c\u91cf\u5b50\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u58f0\u660e\u5f0f\u548c\u6a21\u5757\u5316\u63a5\u53e3\uff0c\u89e3\u8026\u5b9e\u9a8c\u8bbe\u8ba1\u4e0e\u540e\u7aef\u6267\u884c\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u4ece\u7406\u8bba\u8d70\u5411\u5b9e\u8df5\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u3001\u53ef\u79fb\u690d\u4e14\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8f6f\u4ef6\u5b9e\u9a8c\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86Quantum Executor\uff0c\u652f\u6301\u5f02\u6b65\u548c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684API\u548c\u7ba1\u7406\u5de5\u5177\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u57fa\u51c6\u6d4b\u8bd5\u548c\u6df7\u5408\u9a8c\u8bc1\u7b49\u573a\u666f\u5c55\u793a\u4e86\u5176\u9002\u7528\u6027\uff0c\u80fd\u591f\u7b80\u5316\u91cf\u5b50\u5f00\u53d1\u6d41\u7a0b\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2507.07216", "pdf": "https://arxiv.org/pdf/2507.07216", "abs": "https://arxiv.org/abs/2507.07216", "authors": ["Yunyi Li", "Maria De-Arteaga", "Maytal Saar-Tsechansky"], "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.HC"], "comment": null, "summary": "Reliable data is a cornerstone of modern organizational systems. A notable\ndata integrity challenge stems from label bias, which refers to systematic\nerrors in a label, a covariate that is central to a quantitative analysis, such\nthat its quality differs across social groups. This type of bias has been\nconceptually and empirically explored and is widely recognized as a pressing\nissue across critical domains. However, effective methodologies for addressing\nit remain scarce. In this work, we propose Decoupled Confident Learning\n(DeCoLe), a principled machine learning based framework specifically designed\nto detect mislabeled instances in datasets affected by label bias, enabling\nbias aware mislabelling detection and facilitating data quality improvement. We\ntheoretically justify the effectiveness of DeCoLe and evaluate its performance\nin the impactful context of hate speech detection, a domain where label bias is\na well documented challenge. Empirical results demonstrate that DeCoLe excels\nat bias aware mislabeling detection, consistently outperforming alternative\napproaches for label error detection. Our work identifies and addresses the\nchallenge of bias aware mislabeling detection and offers guidance on how DeCoLe\ncan be integrated into organizational data management practices as a powerful\ntool to enhance data reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeCoLe\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u6807\u7b7e\u504f\u89c1\u5f15\u8d77\u7684\u6570\u636e\u6807\u8bb0\u9519\u8bef\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7b49\u9886\u57df\u3002", "motivation": "\u6807\u7b7e\u504f\u89c1\u662f\u6570\u636e\u5b8c\u6574\u6027\u7684\u91cd\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7a00\u7f3a\uff0c\u4e9f\u9700\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684DeCoLe\u6846\u67b6\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u8bc4\u4f30\u6027\u80fd\u3002", "result": "DeCoLe\u5728\u504f\u89c1\u611f\u77e5\u7684\u6807\u8bb0\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "DeCoLe\u4e3a\u6570\u636e\u8d28\u91cf\u7ba1\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u53ef\u63d0\u5347\u6570\u636e\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07120", "pdf": "https://arxiv.org/pdf/2507.07120", "abs": "https://arxiv.org/abs/2507.07120", "authors": ["Nidhi Bhatia", "Ankit More", "Ritika Borkar", "Tiyasa Mitra", "Ramon Matas", "Ritchie Zhao", "Maximilian Golub", "Dheevatsa Mudigere", "Brian Pharris", "Bita Darvish Rouhani"], "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.", "AI": {"tldr": "Helix Parallelism\u901a\u8fc7\u6df7\u5408\u6267\u884c\u7b56\u7565\u4f18\u5316LLM\u4e2d\u7684KV\u7f13\u5b58\u548cFFN\u8ba1\u7b97\uff0c\u51cf\u5c11\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u957fKV\u7f13\u5b58\u548cFFN\u6743\u91cd\u8bfb\u53d6\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u9ad8\u5b9e\u65f6\u89e3\u7801\u6548\u7387\u3002", "method": "\u7ed3\u5408KV\u5e76\u884c\u548cTP/EP\uff0c\u901a\u8fc7\u8f7b\u91cf\u901a\u4fe1\u548c\u6279\u5904\u7406\u91cd\u53e0\u4f18\u5316\u3002", "result": "\u5728\u56fa\u5b9a\u6279\u91cf\u548c\u5ef6\u8fdf\u9884\u7b97\u4e0b\uff0c\u5ef6\u8fdf\u964d\u4f4e1.5\u500d\uff0c\u6279\u91cf\u652f\u630132\u500d\u589e\u957f\u3002", "conclusion": "Helix\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u5b9e\u65f6\u63a8\u7406\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2507.07465", "pdf": "https://arxiv.org/pdf/2507.07465", "abs": "https://arxiv.org/abs/2507.07465", "authors": ["Wei Yao", "Shuzhao Xie", "Letian Li", "Weixiang Zhang", "Zhixin Lai", "Shiqi Dai", "Ke Zhang", "Zhi Wang"], "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver\nimpressive visual fidelity and rendering speed, however, the inherent trade-off\nbetween storage costs and the ability to characterize complex physical motions\nsignificantly limits the practical application of these methods. To tackle\nthese problems, we propose SD-GS, a compact and efficient dynamic Gaussian\nsplatting framework for complex dynamic scene reconstruction, featuring two key\ncontributions. First, we introduce a deformable anchor grid, a hierarchical and\nmemory-efficient scene representation where each anchor point derives multiple\n3D Gaussians in its local spatiotemporal region and serves as the geometric\nbackbone of the 3D scene. Second, to enhance modeling capability for complex\nmotions, we present a deformation-aware densification strategy that adaptively\ngrows anchors in under-reconstructed high-dynamic regions while reducing\nredundancy in static areas, achieving superior visual quality with fewer\nanchors. Experimental results demonstrate that, compared to state-of-the-art\nmethods, SD-GS achieves an average of 60\\% reduction in model size and an\naverage of 100\\% improvement in FPS, significantly enhancing computational\nefficiency while maintaining or even surpassing visual quality.", "AI": {"tldr": "SD-GS \u662f\u4e00\u79cd\u9ad8\u6548\u52a8\u6001\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u53d8\u5f62\u951a\u70b9\u7f51\u683c\u548c\u53d8\u5f62\u611f\u77e5\u7684\u5bc6\u96c6\u5316\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u5e76\u63d0\u5347\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d4D\u9ad8\u65af\u6846\u67b6\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u5b58\u50a8\u6210\u672c\u4e0e\u590d\u6742\u8fd0\u52a8\u8868\u5f81\u80fd\u529b\u7684\u6298\u4e2d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u53ef\u53d8\u5f62\u951a\u70b9\u7f51\u683c\u548c\u53d8\u5f62\u611f\u77e5\u7684\u5bc6\u96c6\u5316\u7b56\u7565\uff0c\u4f18\u5316\u573a\u666f\u8868\u793a\u548c\u8fd0\u52a8\u5efa\u6a21\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cSD-GS \u6a21\u578b\u5927\u5c0f\u5e73\u5747\u51cf\u5c1160%\uff0cFPS\u63d0\u5347100%\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u4e14\u89c6\u89c9\u8d28\u91cf\u66f4\u4f18\u3002", "conclusion": "SD-GS \u901a\u8fc7\u9ad8\u6548\u8868\u793a\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u5b58\u50a8\u548c\u6027\u80fd\u7684\u663e\u8457\u4f18\u5316\u3002"}}
{"id": "2507.07250", "pdf": "https://arxiv.org/pdf/2507.07250", "abs": "https://arxiv.org/abs/2507.07250", "authors": ["Jordi Serra-Ruiz", "David Meg\u00edas"], "title": "Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling", "categories": ["cs.CR", "cs.MM"], "comment": null, "summary": "A semi-fragile watermarking scheme for multiple band images is presented in\nthis article. We propose to embed a mark into remote sensing images applying a\ntree-structured vector quantization approach to the pixel signatures instead of\nprocessing each band separately. The signature of the multispectral or\nhyperspectral image is used to embed the mark in it order to detect any\nsignificant modification of the original image. The image is segmented into\nthree-dimensional blocks, and a tree-structured vector quantizer is built for\neach block. These trees are manipulated using an iterative algorithm until the\nresulting block satisfies a required criterion, which establishes the embedded\nmark. The method is shown to be able to preserve the mark under lossy\ncompression (above a given threshold) but, at the same time, it detects\npossibly forged blocks and their position in the whole image.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6ce2\u6bb5\u56fe\u50cf\u7684\u534a\u8106\u5f31\u6c34\u5370\u65b9\u6848\uff0c\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u77e2\u91cf\u91cf\u5316\u65b9\u6cd5\u5d4c\u5165\u6c34\u5370\uff0c\u901a\u8fc7\u8fed\u4ee3\u7b97\u6cd5\u68c0\u6d4b\u56fe\u50cf\u7be1\u6539\u3002", "motivation": "\u4e3a\u4e86\u9632\u6b62\u9065\u611f\u56fe\u50cf\u5728\u4f20\u8f93\u6216\u5b58\u50a8\u8fc7\u7a0b\u4e2d\u88ab\u7be1\u6539\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u62b5\u6297\u6709\u635f\u538b\u7f29\u53c8\u80fd\u68c0\u6d4b\u7be1\u6539\u7684\u6c34\u5370\u65b9\u6848\u3002", "method": "\u5c06\u591a\u5149\u8c31\u6216\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u50cf\u7d20\u7b7e\u540d\u901a\u8fc7\u6811\u72b6\u7ed3\u6784\u77e2\u91cf\u91cf\u5316\u5904\u7406\uff0c\u5206\u5272\u4e3a\u4e09\u7ef4\u5757\uff0c\u5e76\u8fed\u4ee3\u8c03\u6574\u4ee5\u6ee1\u8db3\u6c34\u5370\u5d4c\u5165\u6761\u4ef6\u3002", "result": "\u8be5\u65b9\u6848\u80fd\u591f\u5728\u6709\u635f\u538b\u7f29\uff08\u8d85\u8fc7\u9608\u503c\uff09\u65f6\u4fdd\u7559\u6c34\u5370\uff0c\u540c\u65f6\u68c0\u6d4b\u51fa\u7be1\u6539\u533a\u57df\u53ca\u5176\u4f4d\u7f6e\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u5bf9\u9065\u611f\u56fe\u50cf\u7684\u534a\u8106\u5f31\u6c34\u5370\u4fdd\u62a4\uff0c\u517c\u5177\u6297\u538b\u7f29\u548c\u7be1\u6539\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.07560", "pdf": "https://arxiv.org/pdf/2507.07560", "abs": "https://arxiv.org/abs/2507.07560", "authors": ["Nils Mandischer", "Larissa F\u00fcller", "Torsten Alles", "Frank Flemisch", "Lars Mikelsons"], "title": "Conjugated Capabilities: Interrelations of Elementary Human Capabilities and Their Implication on Human-Machine Task Allocation and Capability Testing Procedures", "categories": ["cs.HC", "cs.MA", "cs.RO"], "comment": "This work was accepted by the IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC), Vienna, Austria, 2025", "summary": "Human and automation capabilities are the foundation of every human-autonomy\ninteraction and interaction pattern. Therefore, machines need to understand the\ncapacity and performance of human doing, and adapt their own behavior,\naccordingly. In this work, we address the concept of conjugated capabilities,\ni.e. capabilities that are dependent or interrelated and between which effort\ncan be distributed. These may be used to overcome human limitations, by\nshifting effort from a deficient to a conjugated capability with performative\nresources. For example: A limited arm's reach may be compensated by tilting the\ntorso forward. We analyze the interrelation between elementary capabilities\nwithin the IMBA standard to uncover potential conjugation, and show evidence in\ndata of post-rehabilitation patients. From the conjugated capabilities, within\nthe example application of stationary manufacturing, we create a network of\ninterrelations. With this graph, a manifold of potential uses is enabled. We\nshowcase the graph's usage in optimizing IMBA test design to accelerate data\nrecordings, and discuss implications of conjugated capabilities on task\nallocation between the human and an autonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5171\u8f6d\u80fd\u529b\u201d\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u4eba\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u5206\u6790\u548c\u7f51\u7edc\u5efa\u6a21\u5c55\u793a\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8ba9\u4eba\u673a\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u5171\u8f6d\u80fd\u529b\u4f18\u5316\u4efb\u52a1\u5206\u914d\u548c\u884c\u4e3a\u9002\u5e94\uff0c\u4ee5\u5f25\u8865\u4eba\u7c7b\u7684\u80fd\u529b\u9650\u5236\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u57fa\u672c\u80fd\u529b\u7684\u76f8\u4e92\u5173\u7cfb\u3001\u5efa\u7acb\u5171\u8f6d\u80fd\u529b\u7f51\u7edc\u56fe\uff0c\u5e76\u901a\u8fc7IMBA\u6807\u51c6\u4e2d\u7684\u5eb7\u590d\u60a3\u8005\u6570\u636e\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5171\u8f6d\u80fd\u529b\u7f51\u7edc\u53ef\u7528\u4e8e\u4f18\u5316\u6d4b\u8bd5\u8bbe\u8ba1\u548c\u4efb\u52a1\u5206\u914d\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u6307\u51fa\uff0c\u5171\u8f6d\u80fd\u529b\u7684\u6982\u5ff5\u53ca\u5176\u7f51\u7edc\u6a21\u578b\u4e3a\u4eba\u673a\u4ea4\u4e92\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2507.07535", "pdf": "https://arxiv.org/pdf/2507.07535", "abs": "https://arxiv.org/abs/2507.07535", "authors": ["Jingzhao Xie", "Zhenglian Li", "Gang Sun", "Long Luo", "Hongfang Yu", "Dusit Niyato"], "title": "A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks", "categories": ["cs.NI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Computing Power Network (CPN) unifies wide-area computing resources through\ncoordinated network control, while cloud-native abstractions enable flexible\nresource orchestration and on-demand service provisioning atop the elastic\ninfrastructure CPN provides. However, current approaches fall short of fully\nintegrating computing resources via network-enabled coordination as envisioned\nby CPN. In particular, optimally mapping services to an underlying\ninfrastructure to maximize resource efficiency and service satisfaction remains\nchallenging. To overcome this challenge, we formally define the service mapping\nproblem in CPN, establish its theoretical intractability, and identify key\nchallenges in practical optimization. We propose Adaptive Bilevel Search (ABS),\na modular framework featuring (1) graph partitioning-based reformulation to\ncapture variable coupling, (2) a bilevel optimization architecture for\nefficient global exploration with local optimality guarantees, and (3)\nfragmentation-aware evaluation for global performance guidance. Implemented\nusing distributed particle swarm optimization, ABS is extensively evaluated\nacross diverse CPN scenarios, consistently outperforming existing approaches.\nNotably, in complex scenarios, ABS achieves up to 73.2% higher computing\nresource utilization and a 60.2% higher service acceptance ratio compared to\nthe best-performing baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aABS\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8ba1\u7b97\u80fd\u529b\u7f51\u7edc\uff08CPN\uff09\u4e2d\u7684\u670d\u52a1\u6620\u5c04\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u670d\u52a1\u63a5\u53d7\u7387\u3002", "motivation": "\u8ba1\u7b97\u80fd\u529b\u7f51\u7edc\uff08CPN\uff09\u65e8\u5728\u901a\u8fc7\u7f51\u7edc\u534f\u8c03\u7edf\u4e00\u5e7f\u57df\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6574\u5408\u8d44\u6e90\u5e76\u4f18\u5316\u670d\u52a1\u6620\u5c04\u3002", "method": "\u63d0\u51faABS\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u5206\u5272\u7684\u91cd\u6784\u3001\u53cc\u5c42\u4f18\u5316\u67b6\u6784\u548c\u788e\u7247\u611f\u77e5\u8bc4\u4f30\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u7c92\u5b50\u7fa4\u4f18\u5316\u5b9e\u73b0\u3002", "result": "\u5728\u590d\u6742\u573a\u666f\u4e2d\uff0cABS\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534773.2%\u7684\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u548c60.2%\u7684\u670d\u52a1\u63a5\u53d7\u7387\u3002", "conclusion": "ABS\u6210\u529f\u89e3\u51b3\u4e86CPN\u4e2d\u7684\u670d\u52a1\u6620\u5c04\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u6548\u7387\u548c\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2507.07130", "pdf": "https://arxiv.org/pdf/2507.07130", "abs": "https://arxiv.org/abs/2507.07130", "authors": ["Zihan Zhang", "Leon Wong", "Blesson Varghese"], "title": "Ampere: Communication-Efficient and High-Accuracy Split Federated Learning", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "A Federated Learning (FL) system collaboratively trains neural networks\nacross devices and a server but is limited by significant on-device computation\ncosts. Split Federated Learning (SFL) systems mitigate this by offloading a\nblock of layers of the network from the device to a server. However, in doing\nso, it introduces large communication overheads due to frequent exchanges of\nintermediate activations and gradients between devices and the server and\nreduces model accuracy for non-IID data. We propose Ampere, a novel\ncollaborative training system that simultaneously minimizes on-device\ncomputation and device-server communication while improving model accuracy.\nUnlike SFL, which uses a global loss by iterative end-to-end training, Ampere\ndevelops unidirectional inter-block training to sequentially train the device\nand server block with a local loss, eliminating the transfer of gradients. A\nlightweight auxiliary network generation method decouples training between the\ndevice and server, reducing frequent intermediate exchanges to a single\ntransfer, which significantly reduces the communication overhead. Ampere\nmitigates the impact of data heterogeneity by consolidating activations\ngenerated by the trained device block to train the server block, in contrast to\nSFL, which trains on device-specific, non-IID activations. Extensive\nexperiments on multiple CNNs and transformers show that, compared to\nstate-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up\nto 13.26% while reducing training time by up to 94.6%, (ii) reduces\ndevice-server communication overhead by up to 99.1% and on-device computation\nby up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for\nvarious non-IID degrees highlighting superior performance when faced with\nheterogeneous data.", "AI": {"tldr": "Ampere\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u51cf\u5c11\u8bbe\u5907\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5757\u8054\u90a6\u5b66\u4e60\uff08SFL\uff09\u7cfb\u7edf\u867d\u7136\u51cf\u8f7b\u4e86\u8bbe\u5907\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u5f15\u5165\u4e86\u5927\u91cf\u901a\u4fe1\u5f00\u9500\uff0c\u4e14\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u8868\u73b0\u4e0d\u4f73\u3002", "method": "Ampere\u91c7\u7528\u5355\u5411\u5757\u95f4\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u8f85\u52a9\u7f51\u7edc\u751f\u6210\uff0c\u51cf\u5c11\u68af\u5ea6\u4f20\u8f93\u548c\u4e2d\u95f4\u4ea4\u6362\uff0c\u540c\u65f6\u901a\u8fc7\u6574\u5408\u6fc0\u6d3b\u6570\u636e\u5e94\u5bf9\u6570\u636e\u5f02\u6784\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAmpere\u5728\u6a21\u578b\u7cbe\u5ea6\u3001\u8bad\u7ec3\u65f6\u95f4\u3001\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8eSFL\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "Ampere\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u7684\u5e73\u8861\uff0c\u5c24\u5176\u9002\u5408\u5904\u7406\u5f02\u6784\u6570\u636e\u3002"}}
{"id": "2507.07649", "pdf": "https://arxiv.org/pdf/2507.07649", "abs": "https://arxiv.org/abs/2507.07649", "authors": ["Domenik Eichhorn", "Nick Poser", "Maximilian Schweikart", "Ina Schaefer"], "title": "ProvideQ: A Quantum Optimization Toolbox", "categories": ["quant-ph", "cs.SE"], "comment": "This paper was submitted and accepted at the IEEE QCE 2025", "summary": "Hybrid solvers for combinatorial optimization problems combine the advantages\nof classical and quantum computing to overcome difficult computational\nchallenges. Although their theoretical performance seems promising, their\npractical applicability is challenging due to the lack of a technological stack\nthat can seamlessly integrate quantum solutions with existing classical\noptimization frameworks. We tackle this challenge by introducing the ProvideQ\ntoolbox, a software tool that enables users to easily adapt and configure\nhybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements\ndecomposition techniques, which splits problems into classical and quantum\nsubroutines. The ProvideQ toolbox enables the interactive creation of such\ndecompositions via a Meta-Solver configuration tool. It combines\nwell-established classical optimization techniques with quantum circuits that\nare seamlessly executable on multiple backends. This paper introduces the\ntechnical details of the ProvideQ toolbox, explains its architecture, and\ndemonstrates possible applications for several real-world use cases. Our proof\nof concept shows that Meta-Solver strategies already enable the application of\nquantum subroutines today, however, more sophisticated hardware is required to\nmake their performance competitive.", "AI": {"tldr": "ProvideQ\u5de5\u5177\u7bb1\u901a\u8fc7\u5143\u6c42\u89e3\u5668\u7b56\u7565\u5b9e\u73b0\u4e86\u7ecf\u5178\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u6280\u672f\u6808\u6574\u5408\u7684\u96be\u9898\uff0c\u4f46\u5176\u6027\u80fd\u8fd8\u9700\u66f4\u5148\u8fdb\u7684\u786c\u4ef6\u652f\u6301\u3002", "motivation": "\u6df7\u5408\u6c42\u89e3\u5668\u5728\u7406\u8bba\u4e0a\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u7f3a\u4e4f\u6574\u5408\u7ecf\u5178\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7684\u6280\u672f\u6808\u3002", "method": "\u63d0\u4f9bProvideQ\u5de5\u5177\u7bb1\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u5143\u6c42\u89e3\u5668\u914d\u7f6e\u5de5\u5177\u52a8\u6001\u5206\u89e3\u95ee\u9898\u4e3a\u7ecf\u5178\u4e0e\u91cf\u5b50\u5b50\u7a0b\u5e8f\u3002", "result": "\u6982\u5ff5\u9a8c\u8bc1\u5c55\u793a\u4e86\u91cf\u5b50\u5b50\u7a0b\u5e8f\u7684\u53ef\u884c\u6027\uff0c\u4f46\u6027\u80fd\u4ecd\u9700\u63d0\u5347\u786c\u4ef6\u652f\u6301\u3002", "conclusion": "ProvideQ\u5de5\u5177\u7bb1\u4e3a\u6df7\u5408\u6c42\u89e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4f46\u786c\u4ef6\u53d1\u5c55\u662f\u5176\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u3002"}}
{"id": "2507.07623", "pdf": "https://arxiv.org/pdf/2507.07623", "abs": "https://arxiv.org/abs/2507.07623", "authors": ["Hannah Dr\u00f6ge", "Janelle Pfeifer", "Saskia Rabich", "Markus Plack", "Reinhard Klein", "Matthias B. Hullin"], "title": "Capture Stage Environments: A Guide to Better Matting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Capture stages are high-end sources of state-of-the-art recordings for\ndownstream applications in movies, games, and other media. One crucial step in\nalmost all pipelines is the matting of images to isolate the captured\nperformances from the background. While common matting algorithms deliver\nremarkable performance in other applications like teleconferencing and mobile\nentertainment, we found that they struggle significantly with the peculiarities\nof capture stage content. The goal of our work is to share insights into those\nchallenges as a curated list of those characteristics along with a constructive\ndiscussion for proactive intervention and present a guideline to practitioners\nfor an improved workflow to mitigate unresolved challenges. To this end, we\nalso demonstrate an efficient pipeline to adapt state-of-the-art approaches to\nsuch custom setups without the need of extensive annotations, both offline and\nreal-time. For an objective evaluation, we propose a validation methodology\nbased on a leading diffusion model that highlights the benefits of our\napproach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6355\u83b7\u821e\u53f0\u5185\u5bb9\u4e2d\u56fe\u50cf\u62a0\u56fe\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\u7684\u6307\u5357\uff0c\u5e76\u5c55\u793a\u4e86\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u7684\u9ad8\u6548\u7ba1\u9053\u3002", "motivation": "\u73b0\u6709\u7684\u62a0\u56fe\u7b97\u6cd5\u5728\u6355\u83b7\u821e\u53f0\u5185\u5bb9\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u5206\u4eab\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7ba1\u9053\uff0c\u5229\u7528\u73b0\u6709\u7b97\u6cd5\u9002\u5e94\u6355\u83b7\u821e\u53f0\u7684\u7279\u6b8a\u9700\u6c42\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\uff0c\u540c\u65f6\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u9a8c\u8bc1\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6355\u83b7\u821e\u53f0\u5185\u5bb9\u7684\u62a0\u56fe\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6355\u83b7\u821e\u53f0\u5185\u5bb9\u7684\u62a0\u56fe\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u548c\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u79bb\u7ebf\u548c\u5b9e\u65f6\u573a\u666f\u3002"}}
{"id": "2507.07270", "pdf": "https://arxiv.org/pdf/2507.07270", "abs": "https://arxiv.org/abs/2507.07270", "authors": ["Sidong Zhang", "Shiv Shankar", "Trang Nguyen", "Andrea Fanelli", "Madalina Fiterau"], "title": "Audio-Visual Speech Separation via Bottleneck Iterative Network", "categories": ["cs.SD", "cs.MM", "eess.AS"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  Workshop on Machine Learning for Audio", "summary": "Integration of information from non-auditory cues can significantly improve\nthe performance of speech-separation models. Often such models use deep\nmodality-specific networks to obtain unimodal features, and risk being too\ncostly or lightweight but lacking capacity. In this work, we present an\niterative representation refinement approach called Bottleneck Iterative\nNetwork (BIN), a technique that repeatedly progresses through a lightweight\nfusion block, while bottlenecking fusion representations by fusion tokens. This\nhelps improve the capacity of the model, while avoiding major increase in model\nsize and balancing between the model performance and training cost. We test BIN\non challenging noisy audio-visual speech separation tasks, and show that our\napproach consistently outperforms state-of-the-art benchmark models with\nrespect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously\nachieving a reduction of more than 50% in training and GPU inference time\nacross nearly all settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86Bottleneck Iterative Network (BIN)\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u878d\u5408\u5757\u548c\u74f6\u9888\u878d\u5408\u8868\u793a\u63d0\u5347\u8bed\u97f3\u5206\u79bb\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u6574\u5408\u975e\u542c\u89c9\u7ebf\u7d22\u4fe1\u606f\u53ef\u63d0\u5347\u8bed\u97f3\u5206\u79bb\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6a21\u578b\u8fc7\u5927\u6216\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u8981\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "\u63d0\u51faBIN\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u878d\u5408\u5757\u548c\u74f6\u9888\u878d\u5408\u8868\u793a\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728NTCD-TIMIT\u548cLRS3+WHAM!\u6570\u636e\u96c6\u4e0a\uff0cBIN\u5728SI-SDRi\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "BIN\u5728\u6027\u80fd\u4e0e\u6210\u672c\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bed\u97f3\u5206\u79bb\u65b9\u6cd5\u3002"}}
{"id": "2507.07930", "pdf": "https://arxiv.org/pdf/2507.07930", "abs": "https://arxiv.org/abs/2507.07930", "authors": ["Nesrine Fourati", "Alisa Barkar", "Marion Drag\u00e9e", "Liv Danthon-Lefebvre", "Mathieu Chollet"], "title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Background: Public speaking is a vital professional skill, yet it remains a\nsource of significant anxiety for many individuals. Traditional training relies\nheavily on expert coaching, but recent advances in AI has led to novel types of\ncommercial automated public speaking feedback tools. However, most research has\nfocused on prototypes rather than commercial applications, and little is known\nabout how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and\ndesign of commercial AI-based public speaking training tools and to propose\nguidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus\ngroups with public speaking experts. Participants discussed their views on\ncurrent commercial tools, their potential integration into traditional\ncoaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in\nhandling repetitive, technical aspects of training, allowing coaches to focus\non higher-level skills. However they found key issues in current tools,\nemphasising the need for personalised, understandable, carefully selected\nfeedback and clear instructional design. Overall, they supported a hybrid model\ncombining traditional coaching with AI-supported exercises.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4ef7\u4e86\u4e13\u5bb6\u5bf9\u5546\u4e1aAI\u6f14\u8bb2\u8bad\u7ec3\u5de5\u5177\u7684\u770b\u6cd5\uff0c\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u63a2\u7d22\u4e13\u5bb6\u5bf9AI\u6f14\u8bb2\u8bad\u7ec3\u5de5\u5177\u7684\u770b\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5176\u8bbe\u8ba1\u548c\u6548\u679c\u3002", "method": "\u901a\u8fc716\u6b21\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c2\u6b21\u7126\u70b9\u5c0f\u7ec4\uff0c\u6536\u96c6\u4e13\u5bb6\u610f\u89c1\u3002", "result": "\u4e13\u5bb6\u8ba4\u540cAI\u5728\u6280\u672f\u8bad\u7ec3\u4e2d\u7684\u4ef7\u503c\uff0c\u4f46\u6307\u51fa\u9700\u4e2a\u6027\u5316\u53cd\u9988\u548c\u6e05\u6670\u8bbe\u8ba1\u3002", "conclusion": "\u652f\u6301\u4f20\u7edf\u6559\u7ec3\u4e0eAI\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u5f0f\u3002"}}
{"id": "2507.07677", "pdf": "https://arxiv.org/pdf/2507.07677", "abs": "https://arxiv.org/abs/2507.07677", "authors": ["Miguel Casasnovas", "Marc Carrascosa-Zamacois", "Boris Bellalta"], "title": "Can cloud-based VR streaming handle Wi-Fi OBSS contention?", "categories": ["cs.NI"], "comment": "preprint", "summary": "This paper experimentally analyzes the negative impact of contention caused\nby neighboring Wi-Fi networks operating on overlapping channels on Virtual\nReality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full\nchannel overlap within an 80 MHz channel. Our results show that (i) increasing\nthe number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies\ncontention and degrades VR streaming performance; (ii) OBSS activity on the\nsecondary-sided 40 MHz portion degrades performance more than activity on the\nprimary-sided 40 MHz portion; (iii) for the same aggregate load, full channel\noverlap with two 40 MHz OBSS contenders is less detrimental than partial\noverlap with a single high-load 40 MHz contender, but more disruptive than full\noverlap with two 80 MHz contenders; and (iv) full channel overlap with two 40\nMHz OBSS contenders has a smaller impact on VR streaming under symmetric\ntraffic loads than under asymmetric loads. Moreover, our results demonstrate\nthat our previously proposed Network-aware Step-wise adaptive bitrate algorithm\nfor VR streaming (NeSt-VR) effectively mitigates performance degradation in\nOBSS environments, enabling VR streaming under heavier OBSS traffic conditions.", "AI": {"tldr": "\u7814\u7a76\u4e86Wi-Fi\u7f51\u7edc\u4e2d\u91cd\u53e0\u4fe1\u9053\u5bf9VR\u6d41\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u53d1\u73b0OBSS\u6570\u91cf\u548c\u4f4d\u7f6e\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u5e76\u63d0\u51faNeSt-VR\u7b97\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22Wi-Fi\u7f51\u7edc\u4e2d\u91cd\u53e0\u4fe1\u9053\uff08\u5c24\u5176\u662f80 MHz\u4fe1\u9053\u5185\u7684\u90e8\u5206\u548c\u5b8c\u5168\u91cd\u53e0\uff09\u5bf9VR\u6d41\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540cOBSS\u914d\u7f6e\u4e0bVR\u6d41\u7684\u6027\u80fd\u53d8\u5316\uff0c\u5e76\u6d4b\u8bd5NeSt-VR\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "OBSS\u6570\u91cf\u548c\u4f4d\u7f6e\u5bf9VR\u6d41\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0cNeSt-VR\u80fd\u6709\u6548\u51cf\u8f7b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u91cd\u53e0\u4fe1\u9053\u5bf9VR\u6d41\u6709\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46NeSt-VR\u7b97\u6cd5\u80fd\u6539\u5584\u6027\u80fd\uff0c\u5c24\u5176\u5728OBSS\u5bc6\u96c6\u73af\u5883\u4e2d\u3002"}}
{"id": "2507.07144", "pdf": "https://arxiv.org/pdf/2507.07144", "abs": "https://arxiv.org/abs/2507.07144", "authors": ["Hongyi Xie", "Min Zhou", "Qiao Yu", "Jialiang Yu", "Zhenli Sheng", "Hong Xie", "Defu Lian"], "title": "M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure", "categories": ["cs.DC"], "comment": null, "summary": "As cloud services become increasingly integral to modern IT infrastructure,\nensuring hardware reliability is essential to sustain high-quality service.\nMemory failures pose a significant threat to overall system stability, making\naccurate failure prediction through the analysis of memory error logs (i.e.,\nCorrectable Errors) imperative. Existing memory failure prediction approaches\nhave notable limitations: rule-based expert models suffer from limited\ngeneralizability and low recall rates, while automated feature extraction\nmethods exhibit suboptimal performance. To address these limitations, we\npropose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction\nframework designed to enhance the reliability and availability of cloud\ninfrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level\nbinary matrix representations and introduces a Binary Spatial Feature Extractor\n(BSFE) to automatically extract high-order features at both DIMM-level and\nbit-level. Building upon the BSFE outputs, we develop a dual-path temporal\nmodeling architecture: 1) a time-patch module that aggregates multi-level\nfeatures within observation windows, and 2) a time-point module that employs\ninterpretable rule-generation trees trained on bit-level patterns. Experiments\non both benchmark datasets and real-world deployment show the superiority of\nM$^2$-MFP as it outperforms existing state-of-the-art methods by significant\nmargins. Code and data are available at this repository:\nhttps://github.com/hwcloud-RAS/M2-MFP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u5206\u5c42\u5185\u5b58\u6545\u969c\u9884\u6d4b\u6846\u67b6M2-MFP\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u5668\u548c\u53cc\u8def\u5f84\u65f6\u5e8f\u5efa\u6a21\u67b6\u6784\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5185\u5b58\u6545\u969c\u5bf9\u4e91\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u7a33\u5b9a\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5c06\u5185\u5b58\u9519\u8bef\u65e5\u5fd7\u8f6c\u6362\u4e3a\u591a\u7ea7\u4e8c\u8fdb\u5236\u77e9\u9635\u8868\u793a\uff0c\u4f7f\u7528\u4e8c\u8fdb\u5236\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u5668\u548c\u53cc\u8def\u5f84\u65f6\u5e8f\u5efa\u6a21\u67b6\u6784\uff08\u65f6\u95f4\u5757\u6a21\u5757\u548c\u65f6\u95f4\u70b9\u6a21\u5757\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "M2-MFP\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5185\u5b58\u6545\u969c\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u4e91\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2507.07733", "pdf": "https://arxiv.org/pdf/2507.07733", "abs": "https://arxiv.org/abs/2507.07733", "authors": ["Yongyang Zhou", "Fang-Lue Zhang", "Zichen Wang", "Lei Zhang"], "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in\nnovel view synthesis. However, rendering reflective objects remains a\nsignificant challenge, particularly in inverse rendering and relighting. We\nintroduce RTR-GS, a novel inverse rendering framework capable of robustly\nrendering objects with arbitrary reflectance properties, decomposing BRDF and\nlighting, and delivering credible relighting results. Given a collection of\nmulti-view images, our method effectively recovers geometric structure through\na hybrid rendering model that combines forward rendering for radiance transfer\nwith deferred rendering for reflections. This approach successfully separates\nhigh-frequency and low-frequency appearances, mitigating floating artifacts\ncaused by spherical harmonic overfitting when handling high-frequency details.\nWe further refine BRDF and lighting decomposition using an additional\nphysically-based deferred rendering branch. Experimental results show that our\nmethod enhances novel view synthesis, normal estimation, decomposition, and\nrelighting while maintaining efficient training inference process.", "AI": {"tldr": "3DGS\u5728\u53cd\u5c04\u7269\u4f53\u6e32\u67d3\u4e0a\u5b58\u5728\u6311\u6218\uff0cRTR-GS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6e32\u67d3\u6a21\u578b\u6709\u6548\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u591a\u79cd\u6e32\u67d3\u4efb\u52a1\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b33DGS\u5728\u6e32\u67d3\u53cd\u5c04\u7269\u4f53\u65f6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u9006\u6e32\u67d3\u548c\u91cd\u65b0\u5149\u7167\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86RTR-GS\u6846\u67b6\uff0c\u7ed3\u5408\u524d\u5411\u6e32\u67d3\u548c\u5ef6\u8fdf\u6e32\u67d3\uff0c\u5206\u79bb\u9ad8\u9891\u548c\u4f4e\u9891\u5916\u89c2\uff0c\u5e76\u7ec6\u5316BRDF\u548c\u5149\u7167\u5206\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89d2\u5408\u6210\u3001\u6cd5\u7ebf\u4f30\u8ba1\u3001\u5206\u89e3\u548c\u91cd\u65b0\u5149\u7167\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6548\u7387\u9ad8\u3002", "conclusion": "RTR-GS\u663e\u8457\u63d0\u5347\u4e86\u53cd\u5c04\u7269\u4f53\u6e32\u67d3\u7684\u591a\u4efb\u52a1\u8868\u73b0\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07633", "pdf": "https://arxiv.org/pdf/2507.07633", "abs": "https://arxiv.org/abs/2507.07633", "authors": ["Zhitao Wang", "Hengyu Man", "Wenrui Li", "Xingtao Wang", "Xiaopeng Fan", "Debin Zhao"], "title": "T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent advances in video generation techniques have given rise to an emerging\nparadigm of generative video coding, aiming to achieve semantically accurate\nreconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong\ngenerative priors. However, most existing methods are limited by domain\nspecificity (e.g., facial or human videos) or an excessive dependence on\nhigh-level text guidance, which often fails to capture motion details and\nresults in unrealistic reconstructions. To address these challenges, we propose\na Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC\nemploys a semantic-aware sparse motion sampling pipeline to effectively bridge\nlow-level motion tracking with high-level semantic understanding by extracting\npixel-wise motion as sparse trajectory points based on their semantic\nimportance, not only significantly reducing the bitrate but also preserving\ncritical temporal semantic information. In addition, by incorporating\ntrajectory-aligned loss constraints into diffusion processes, we introduce a\ntraining-free latent space guidance mechanism to ensure physically plausible\nmotion patterns without sacrificing the inherent capabilities of generative\nmodels. Experimental results demonstrate that our framework outperforms both\ntraditional codecs and state-of-the-art end-to-end video compression methods\nunder ULB conditions. Furthermore, additional experiments confirm that our\napproach achieves more precise motion control than existing text-guided\nmethods, paving the way for a novel direction of generative video coding guided\nby geometric motion modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u89c6\u9891\u7f16\u7801\u6846\u67b6\uff08T-GVC\uff09\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u7a00\u758f\u8fd0\u52a8\u91c7\u6837\u548c\u8f68\u8ff9\u5bf9\u9f50\u7684\u635f\u5931\u7ea6\u675f\uff0c\u5728\u8d85\u4f4e\u7801\u7387\uff08ULB\uff09\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u89c6\u9891\u7f16\u7801\u65b9\u6cd5\u53d7\u9650\u4e8e\u9886\u57df\u7279\u5f02\u6027\u6216\u5bf9\u9ad8\u7ea7\u6587\u672c\u5f15\u5bfc\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u96be\u4ee5\u6355\u6349\u8fd0\u52a8\u7ec6\u8282\u3002", "method": "T-GVC\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u7a00\u758f\u8fd0\u52a8\u91c7\u6837\u7ba1\u9053\uff0c\u7ed3\u5408\u8f68\u8ff9\u5bf9\u9f50\u7684\u635f\u5931\u7ea6\u675f\uff0c\u5728\u4e0d\u727a\u7272\u751f\u6210\u6a21\u578b\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7269\u7406\u5408\u7406\u7684\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-GVC\u5728ULB\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u548c\u7aef\u5230\u7aef\u538b\u7f29\u65b9\u6cd5\uff0c\u4e14\u8fd0\u52a8\u63a7\u5236\u66f4\u7cbe\u786e\u3002", "conclusion": "T-GVC\u4e3a\u57fa\u4e8e\u51e0\u4f55\u8fd0\u52a8\u5efa\u6a21\u7684\u751f\u6210\u5f0f\u89c6\u9891\u7f16\u7801\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.07841", "pdf": "https://arxiv.org/pdf/2507.07841", "abs": "https://arxiv.org/abs/2507.07841", "authors": ["Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Lu\u00eds Fraz\u00e3o", "Nuno Costa", "Ant\u00f3nio Pereira"], "title": "HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN", "categories": ["cs.NI", "cs.CY", "cs.SY", "eess.SY", "68M10, 68M12, 68W15", "C.2.1; C.2.2; C.2.3; C.2.6; H.5.5; K.4.1"], "comment": null, "summary": "Events such as catastrophes and disasters are, in most cases, unpredictable.\nConsequently, reusing existing infrastructures to develop alternative\ncommunication strategies after disasters is essential to minimise the impact of\nthese events on the population's ability to communicate and promptly receive\nalerts from authorities. In this context, the emergence of smart cities,\ncharacterised by dense and geographically distributed IoT networks, presents\nsignificant potential for such reuse. This work proposes HaLert, a resilient\narchitecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,\nwhose resources can be readily reallocated to support a emergency communication\nsystem to exchange messages (including text, location, image, audio, and video)\nbetween citizens, authorities, and between both parties. To facilitate remote\nmonitoring and configuration of the network, the architecture incorporates the\nSDN (Software-Defined Networking) paradigm, supported by a LoRa controlled\nflooding mesh network. A prototype was developed based on this architecture and\ntested in a real urban scenario comprising both indoor and outdoor\nenvironments. The results demonstrated that, despite the significant impact of\nobstacles, lack of line-of-sight, and terrain slopes on the latency (average\nlatency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and\n726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow\nnetwork, it remained stable and resilient, successfully providing all\nfunctionalities associated with the HaLert architecture. The tests conducted on\nthe LoRa network revealed a high average message success rate of 94.96%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWi-Fi HaLow IEEE 802.11s\u7f51\u72b6\u7f51\u7edc\u7684\u667a\u80fd\u57ce\u5e02\u5e94\u6025\u901a\u4fe1\u67b6\u6784HaLert\uff0c\u652f\u6301\u6587\u672c\u3001\u4f4d\u7f6e\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u901a\u4fe1\uff0c\u5e76\u7ed3\u5408SDN\u548cLoRa\u7f51\u7edc\u5b9e\u73b0\u8fdc\u7a0b\u76d1\u63a7\u548c\u914d\u7f6e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u707e\u96be\u4e8b\u4ef6\u4e2d\u57fa\u7840\u8bbe\u65bd\u53d7\u635f\u5bfc\u81f4\u901a\u4fe1\u4e2d\u65ad\u7684\u95ee\u9898\uff0c\u5229\u7528\u667a\u80fd\u57ce\u5e02\u4e2d\u5bc6\u96c6\u5206\u5e03\u7684IoT\u7f51\u7edc\u8d44\u6e90\uff0c\u5f00\u53d1\u4e00\u79cd\u53ef\u5feb\u901f\u91cd\u5206\u914d\u7684\u5e94\u6025\u901a\u4fe1\u7cfb\u7edf\u3002", "method": "\u63d0\u51faHaLert\u67b6\u6784\uff0c\u7ed3\u5408Wi-Fi HaLow\u7f51\u72b6\u7f51\u7edc\u548cSDN\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7LoRa\u63a7\u5236\u7684\u6cdb\u6d2a\u7f51\u72b6\u7f51\u7edc\u5b9e\u73b0\u8fdc\u7a0b\u76d1\u63a7\u548c\u914d\u7f6e\u3002", "result": "\u5728\u771f\u5b9e\u57ce\u5e02\u73af\u5883\u4e2d\u6d4b\u8bd5\u663e\u793a\uff0cWi-Fi HaLow\u7f51\u7edc\u5e73\u5747\u5ef6\u8fdf15-54.8 ms\uff0c\u4e0a\u4f20\u901f\u7387134-726 Kbps\uff0c\u4e0b\u8f7d\u901f\u7387117-682 Kbps\uff0cLoRa\u7f51\u7edc\u6d88\u606f\u6210\u529f\u7387\u8fbe94.96%\uff0c\u7cfb\u7edf\u7a33\u5b9a\u4e14\u529f\u80fd\u5b8c\u6574\u3002", "conclusion": "HaLert\u67b6\u6784\u5728\u969c\u788d\u7269\u3001\u975e\u89c6\u8ddd\u548c\u5730\u5f62\u5761\u5ea6\u7b49\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u97e7\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u5408\u4f5c\u4e3a\u667a\u80fd\u57ce\u5e02\u7684\u5e94\u6025\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07890", "pdf": "https://arxiv.org/pdf/2507.07890", "abs": "https://arxiv.org/abs/2507.07890", "authors": ["Radi Muhammad Reza", "Benjamin A Watson"], "title": "Hi-d maps: An interactive visualization technique for multi-dimensional categorical data", "categories": ["cs.GR"], "comment": null, "summary": "In this paper, we present Hi-D maps, a novel method for the visualization of\nmulti-dimensional categorical data. Our work addresses the scarcity of\ntechniques for visualizing a large number of data-dimensions in an effective\nand space-efficient manner. We have mapped the full data-space onto a 2D\nregular polygonal region. The polygon is cut hierarchically with lines parallel\nto a user-controlled, ordered sequence of sides, each representing a dimension.\nWe have used multiple visual cues such as orientation, thickness, color,\ncountable glyphs, and text to depict cross-dimensional information. We have\nadded interactivity and hierarchical browsing to facilitate flexible\nexploration of the display: small areas can be scrutinized for details. Thus,\nour method is also easily extendable to visualize hierarchical information. Our\nglyph animations add an engaging aesthetic during interaction. Like many\nvisualizations, Hi-D maps become less effective when a large number of\ndimensions stresses perceptual limits, but Hi-D maps may add clarity before\nthose limits are reached.", "AI": {"tldr": "Hi-D maps\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7ef4\u5206\u7c7b\u6570\u636e\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5207\u5272\u591a\u8fb9\u5f62\u5e76\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u7ebf\u7d22\uff0c\u6709\u6548\u5c55\u793a\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6280\u672f\u5728\u9ad8\u7ef4\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u6548\u679c\u4e0d\u4f73\u548c\u7a7a\u95f4\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5c06\u6570\u636e\u7a7a\u95f4\u6620\u5c04\u52302D\u591a\u8fb9\u5f62\u533a\u57df\uff0c\u5206\u5c42\u5207\u5272\u5e76\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u65b9\u5411\u3001\u989c\u8272\u3001\u6587\u672c\u7b49\uff09\u53ca\u4ea4\u4e92\u6d4f\u89c8\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u7ef4\u6570\u636e\u7684\u6e05\u6670\u5c55\u793a\uff0c\u5e76\u5728\u4ea4\u4e92\u4e2d\u63d0\u4f9b\u52a8\u6001\u6548\u679c\u3002", "conclusion": "Hi-D maps\u5728\u591a\u7ef4\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ef4\u5ea6\u8fc7\u591a\u65f6\u6548\u679c\u4f1a\u4e0b\u964d\u3002"}}
{"id": "2507.07327", "pdf": "https://arxiv.org/pdf/2507.07327", "abs": "https://arxiv.org/abs/2507.07327", "authors": ["Brian B. Vuong", "Josie Davidson", "Sangheui Cheon", "Kyujin Cho", "Allison M. Okamura"], "title": "Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task", "categories": ["cs.RO", "cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Previous work has shown that the addition of haptic feedback to the hands can\nimprove awareness of tool-tissue interactions and enhance performance of\nteleoperated tasks in robot-assisted minimally invasive surgery. However,\nhand-based haptic feedback occludes direct interaction with the manipulanda of\nsurgeon console in teleoperated surgical robots. We propose relocating haptic\nfeedback to the wrist using a wearable haptic device so that haptic feedback\nmechanisms do not need to be integrated into the manipulanda. However, it is\nunknown if such feedback will be effective, given that it is not co-located\nwith the finger movements used for manipulation. To test if relocated haptic\nfeedback improves force application during teleoperated tasks using da Vinci\nResearch Kit (dVRK) surgical robot, participants learned to palpate a phantom\ntissue to desired forces. A soft pneumatic wrist-worn haptic device with an\nanchoring system renders tool-tissue interaction forces to the wrist of the\nuser. Participants performed the palpation task with and without wrist-worn\nhaptic feedback and were evaluated for the accuracy of applied forces.\nParticipants demonstrated statistically significant lower force error when\nwrist-worn haptic feedback was provided. Participants also performed the\npalpation task with longer movement times when provided wrist-worn haptic\nfeedback, indicating that the haptic feedback may have caused participants to\noperate at a different point in the speed-accuracy tradeoff curve.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u5c06\u89e6\u89c9\u53cd\u9988\u4ece\u624b\u90e8\u79fb\u81f3\u624b\u8155\uff0c\u4ee5\u907f\u514d\u4e0e\u624b\u672f\u673a\u5668\u4eba\u64cd\u7eb5\u6746\u7684\u76f4\u63a5\u4ea4\u4e92\u51b2\u7a81\uff0c\u5e76\u9a8c\u8bc1\u4e86\u624b\u8155\u89e6\u89c9\u53cd\u9988\u5728\u63d0\u9ad8\u64cd\u4f5c\u529b\u51c6\u786e\u6027\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u89e6\u89c9\u53cd\u9988\u4e0e\u624b\u672f\u673a\u5668\u4eba\u64cd\u7eb5\u6746\u4ea4\u4e92\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u624b\u8155\u89e6\u89c9\u53cd\u9988\u5728\u624b\u672f\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u8f6f\u6c14\u52a8\u624b\u8155\u89e6\u89c9\u88c5\u7f6e\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u6709\u65e0\u89e6\u89c9\u53cd\u9988\u65f6\u53c2\u4e0e\u8005\u64cd\u4f5c\u8fbe\u82ac\u5947\u624b\u672f\u673a\u5668\u4eba\u6267\u884c\u7ec4\u7ec7\u89e6\u8bca\u4efb\u52a1\u7684\u529b\u51c6\u786e\u6027\u3002", "result": "\u63d0\u4f9b\u624b\u8155\u89e6\u89c9\u53cd\u9988\u65f6\uff0c\u53c2\u4e0e\u8005\u65bd\u52a0\u7684\u529b\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u4f46\u64cd\u4f5c\u65f6\u95f4\u5ef6\u957f\u3002", "conclusion": "\u624b\u8155\u89e6\u89c9\u53cd\u9988\u80fd\u6709\u6548\u63d0\u9ad8\u529b\u51c6\u786e\u6027\uff0c\u4f46\u9700\u6743\u8861\u64cd\u4f5c\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u5173\u7cfb\u3002"}}
{"id": "2507.07352", "pdf": "https://arxiv.org/pdf/2507.07352", "abs": "https://arxiv.org/abs/2507.07352", "authors": ["Lo\u00efc Pottier", "Konstantia Georgouli", "Timothy S. Carpenter", "Fikret Aydin", "Jeremy O. B. Tempkin", "Dwight V. Nissley", "Frederick H. Streitz", "Thomas R. W. Scogland", "Peer-Timo Bremer", "Felice C. Lightstone", "Helgi I. Ing\u00f3lfsson"], "title": "Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Computational models have become one of the prevalent methods to model\ncomplex phenomena. To accurately model complex interactions, such as detailed\nbiomolecular interactions, scientists often rely on multiscale models comprised\nof several internal models operating at difference scales, ranging from\nmicroscopic to macroscopic length and time scales. Bridging the gap between\ndifferent time and length scales has historically been challenging but the\nadvent of newer machine learning (ML) approaches has shown promise for tackling\nthat task. Multiscale models require massive amounts of computational power and\na powerful workflow management system. Orchestrating ML-driven multiscale\nstudies on parallel systems with thousands of nodes is challenging, the\nworkflow must schedule, allocate and control thousands of simulations operating\nat different scales. Here, we discuss the massively parallel Multiscale\nMachine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow\nmanagement infrastructure, that can orchestrate thousands of molecular dynamics\n(MD) simulations operating at different timescales, spanning from millisecond\nto nanosecond. More specifically, we introduce a novel version of MuMMI called\n\"mini-MuMMI\". Mini-MuMMI is a curated version of MuMMI designed to run on\nmodest HPC systems or even laptops whereas MuMMI requires larger HPC systems.\nWe demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions\nand discuss the different challenges behind the generalization of multiscale\nworkflows and how mini-MuMMI can be leveraged to target a broader range of\napplications outside of MD and RAS-RAF interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u591a\u5c3a\u5ea6\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u57fa\u7840\u8bbe\u65bd\uff08MuMMI\uff09\u7684\u8f7b\u91cf\u7248mini-MuMMI\uff0c\u80fd\u591f\u5728\u5c0f\u89c4\u6a21HPC\u7cfb\u7edf\u6216\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u8fd0\u884c\uff0c\u7528\u4e8e\u7ba1\u7406\u591a\u5c3a\u5ea6\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5de5\u4f5c\u6d41\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728RAS-RAF\u819c\u76f8\u4e92\u4f5c\u7528\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u591a\u5c3a\u5ea6\u6a21\u578b\u662f\u6a21\u62df\u590d\u6742\u73b0\u8c61\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u5728\u4e0d\u540c\u65f6\u95f4\u548c\u957f\u5ea6\u5c3a\u5ea6\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\u4e00\u76f4\u662f\u6311\u6218\u3002\u968f\u7740\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u8fd9\u4e00\u4efb\u52a1\u6709\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002MuMMI\u901a\u8fc7\u5e76\u884c\u7cfb\u7edf\u7ba1\u7406\u5927\u89c4\u6a21\u6a21\u62df\u5de5\u4f5c\u6d41\uff0c\u4f46\u5176\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e86mini-MuMMI\uff0c\u662fMuMMI\u7684\u8f7b\u91cf\u7248\u672c\uff0c\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21HPC\u7cfb\u7edf\u6216\u7b14\u8bb0\u672c\u7535\u8111\u3002\u901a\u8fc7\u4f18\u5316\u5de5\u4f5c\u6d41\u7ba1\u7406\u529f\u80fd\uff0cmini-MuMMI\u80fd\u591f\u9ad8\u6548\u5730\u534f\u8c03\u591a\u5c3a\u5ea6\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u540c\u65f6\u5728\u8d44\u6e90\u6709\u9650\u7684\u7cfb\u7edf\u4e2d\u8fd0\u884c\u3002", "result": "mini-MuMMI\u6210\u529f\u5e94\u7528\u4e8eRAS-RAF\u819c\u76f8\u4e92\u4f5c\u7528\u7684\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u5c3a\u5ea6\u6a21\u62df\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u5982\u4f55\u63a8\u5e7f\u591a\u5c3a\u5ea6\u5de5\u4f5c\u6d41\u5e76\u5c06\u5176\u6269\u5c55\u5230\u5206\u5b50\u52a8\u529b\u5b66\u4ee5\u5916\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u4e2d\u3002", "conclusion": "mini-MuMMI\u4e3a\u89e3\u51b3\u591a\u5c3a\u5ea6\u5efa\u6a21\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6269\u5c55\u4e86MuMMI\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u9886\u57df\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.07581", "pdf": "https://arxiv.org/pdf/2507.07581", "abs": "https://arxiv.org/abs/2507.07581", "authors": ["Michail Kalntis", "Fernando A. Kuipers", "George Iosifidis"], "title": "CHOMET: Conditional Handovers via Meta-Learning", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Handovers (HOs) are the cornerstone of modern cellular networks for enabling\nseamless connectivity to a vast and diverse number of mobile users. However, as\nmobile networks become more complex with more diverse users and smaller cells,\ntraditional HOs face significant challenges, such as prolonged delays and\nincreased failures. To mitigate these issues, 3GPP introduced conditional\nhandovers (CHOs), a new type of HO that enables the preparation (i.e., resource\nallocation) of multiple cells for a single user to increase the chance of HO\nsuccess and decrease the delays in the procedure. Despite its advantages, CHO\nintroduces new challenges that must be addressed, including efficient resource\nallocation and managing signaling/communication overhead from frequent cell\npreparations and releases. This paper presents a novel framework aligned with\nthe O-RAN paradigm that leverages meta-learning for CHO optimization, providing\nrobust dynamic regret guarantees and demonstrating at least 180% superior\nperformance than other 3GPP benchmarks in volatile signal conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eO-RAN\u8303\u5f0f\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u53163GPP\u63d0\u51fa\u7684\u6761\u4ef6\u5207\u6362\uff08CHO\uff09\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u4fe1\u53f7\u6ce2\u52a8\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u7f51\u7edc\u590d\u6742\u5316\u548c\u5c0f\u533a\u5c0f\u578b\u5316\uff0c\u4f20\u7edf\u5207\u6362\u6280\u672f\u9762\u4e34\u5ef6\u8fdf\u589e\u52a0\u548c\u5931\u8d25\u7387\u5347\u9ad8\u7684\u95ee\u9898\u30023GPP\u5f15\u5165\u6761\u4ef6\u5207\u6362\uff08CHO\uff09\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46CHO\u4ecd\u5b58\u5728\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u4fe1\u4ee4\u5f00\u9500\u7b49\u65b0\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eO-RAN\u8303\u5f0f\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316CHO\u7684\u8d44\u6e90\u5206\u914d\u548c\u4fe1\u4ee4\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86CHO\u7684\u6311\u6218\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4fe1\u53f7\u6ce2\u52a8\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u6bd43GPP\u57fa\u51c6\u65b9\u6848\u63d0\u5347\u4e86\u81f3\u5c11180%\uff0c\u5e76\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u52a8\u6001\u540e\u6094\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCHO\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u7f51\u7edc\u7684\u5207\u6362\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.07400", "pdf": "https://arxiv.org/pdf/2507.07400", "abs": "https://arxiv.org/abs/2507.07400", "authors": ["Zaifeng Pan", "Ajjkumar Patel", "Zhengding Hu", "Yipeng Shen", "Yue Guan", "Wan-Lu Li", "Lianhui Qin", "Yida Wang", "Yufei Ding"], "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows", "categories": ["cs.DC", "cs.MA"], "comment": null, "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.", "AI": {"tldr": "KVFlow\u662f\u4e00\u4e2a\u9488\u5bf9\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u9884\u6d4b\u548c\u9884\u52a0\u8f7d\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4f7f\u7528LRU\u7b56\u7565\u7ba1\u7406KV\u7f13\u5b58\uff0c\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u4f7f\u7528\u5bfc\u81f4\u9891\u7e41\u7f13\u5b58\u672a\u547d\u4e2d\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "KVFlow\u901a\u8fc7Agent Step Graph\u9884\u6d4b\u667a\u80fd\u4f53\u6fc0\u6d3b\u65f6\u95f4\uff0c\u5e76\u5f15\u5165\u91cd\u53e0KV\u9884\u53d6\u673a\u5236\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cKVFlow\u5728\u5355\u5de5\u4f5c\u6d41\u548c\u591a\u5e76\u53d1\u573a\u666f\u4e0b\u5206\u522b\u5b9e\u73b01.83\u500d\u548c2.19\u500d\u52a0\u901f\u3002", "conclusion": "KVFlow\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610", "abs": "https://arxiv.org/abs/2507.07610", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpatialViz-Bench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u7a7a\u95f4\u53ef\u89c6\u5316\u80fd\u529b\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8868\u660e\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u5c06\u7a7a\u95f4\u53ef\u89c6\u5316\u80fd\u529b\u5d4c\u5165\u5230\u66f4\u5e7f\u6cdb\u7684\u6570\u5b66\u548c\u903b\u8f91\u6d4b\u8bd5\u4e2d\uff0c\u4e14\u53ef\u80fd\u53d7\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e13\u95e8\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b12\u4e2a\u4efb\u52a1\u548c1180\u4e2a\u81ea\u52a8\u751f\u6210\u95ee\u9898\u7684SpatialViz-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u7a7a\u95f4\u53ef\u89c6\u5316\u80fd\u529b\u3002", "result": "\u8bc4\u4f3033\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u4e86\u6027\u80fd\u5dee\u5f02\u5927\u3001\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e0d\u7b26\u7684\u56f0\u96be\u611f\u77e5\u30012D\u52303D\u6027\u80fd\u9aa4\u964d\u7b49\u95ee\u9898\u3002", "conclusion": "SpatialViz-Bench\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7a7a\u95f4\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.07671", "pdf": "https://arxiv.org/pdf/2507.07671", "abs": "https://arxiv.org/abs/2507.07671", "authors": ["Jovan Prodanov", "Bla\u017e Bertalani\u010d", "Carolina Fortuna", "Shih-Kai Chou", "Matja\u017e Branko Juri\u010d", "Ramon Sanchez-Iborra", "Jernej Hribar"], "title": "Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems", "categories": ["cs.DC"], "comment": "Accepted at IEEE Cloud 2025", "summary": "Modern edge-cloud systems face challenges in efficiently scaling resources to\nhandle dynamic and unpredictable workloads. Traditional scaling approaches\ntypically rely on static thresholds and predefined rules, which are often\ninadequate for optimizing resource utilization and maintaining performance in\ndistributed and dynamic environments. This inefficiency hinders the\nadaptability and performance required in edge-cloud infrastructures, which can\nonly be achieved through the newly proposed in-place scaling. To address this\nproblem, we propose the Multi-Agent Reinforcement Learning-based In-place\nScaling Engine (MARLISE) that enables seamless, dynamic, reactive control with\nin-place resource scaling. We develop our solution using two Deep Reinforcement\nLearning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization\n(PPO). We analyze each version of the proposed MARLISE solution using dynamic\nworkloads, demonstrating their ability to ensure low response times of\nmicroservices and scalability. Our results show that MARLISE-based approaches\noutperform heuristic method in managing resource elasticity while maintaining\nmicroservice response times and achieving higher resource efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u8d44\u6e90\u52a8\u6001\u6269\u5c55\u5f15\u64ce\uff08MARLISE\uff09\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u9759\u6001\u9608\u503c\u65b9\u6cd5\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u4f18\u5316\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u7684\u8d44\u6e90\u5229\u7528\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u9608\u503c\u65b9\u6cd5\u5728\u52a8\u6001\u8d1f\u8f7d\u73af\u5883\u4e0b\u8d44\u6e90\u6269\u5c55\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08DQN\u548cPPO\uff09\u5f00\u53d1MARLISE\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u53cd\u5e94\u5f0f\u7684\u8d44\u6e90\u6269\u5c55\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMARLISE\u5728\u5904\u7406\u52a8\u6001\u8d1f\u8f7d\u65f6\uff0c\u80fd\u4fdd\u8bc1\u5fae\u670d\u52a1\u7684\u4f4e\u54cd\u5e94\u65f6\u95f4\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "MARLISE\u5728\u8d44\u6e90\u52a8\u6001\u6269\u5c55\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9002\u5408\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u7684\u9700\u6c42\u3002"}}
{"id": "2507.07661", "pdf": "https://arxiv.org/pdf/2507.07661", "abs": "https://arxiv.org/abs/2507.07661", "authors": ["Daria Trinitatova", "Dzmitry Tsetserukou"], "title": "FiDTouch: A 3D Wearable Haptic Display for the Finger Pad", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to the IEEE World Haptics Conference 2025 (IEEE WHC 2025), 7\n  pages, 8 figures, 3 tables", "summary": "The applications of fingertip haptic devices have spread to various fields\nfrom revolutionizing virtual reality and medical training simulations to\nfacilitating remote robotic operations, proposing great potential for enhancing\nuser experiences, improving training outcomes, and new forms of interaction. In\nthis work, we present FiDTouch, a 3D wearable haptic device that delivers\ncutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin\nstretch, and vibrotactile feedback. The application of a tiny inverted Delta\nrobot in the mechanism design allows providing accurate contact and fast\nchanging dynamic stimuli to the finger pad surface. The performance of the\ndeveloped display was evaluated in a two-stage user study of the perception of\nstatic spatial contact stimuli and skin stretch stimuli generated on the finger\npad. The proposed display, by providing users with precise touch and force\nstimuli, can enhance user immersion and efficiency in the fields of\nhuman-computer and human-robot interactions.", "AI": {"tldr": "FiDTouch\u662f\u4e00\u79cd3D\u53ef\u7a7f\u6234\u89e6\u89c9\u8bbe\u5907\uff0c\u901a\u8fc7\u5fae\u578b\u5012\u7acbDelta\u673a\u5668\u4eba\u63d0\u4f9b\u7cbe\u786e\u7684\u89e6\u89c9\u53cd\u9988\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u6307\u5c16\u89e6\u89c9\u8bbe\u5907\u5728\u865a\u62df\u73b0\u5b9e\u3001\u533b\u7597\u8bad\u7ec3\u548c\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u57f9\u8bad\u6548\u679c\u3002", "method": "\u91c7\u7528\u5fae\u578b\u5012\u7acbDelta\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u63a5\u89e6\u548c\u5feb\u901f\u53d8\u5316\u7684\u52a8\u6001\u523a\u6fc0\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u9759\u6001\u7a7a\u95f4\u63a5\u89e6\u548c\u76ae\u80a4\u62c9\u4f38\u523a\u6fc0\u7684\u611f\u77e5\u6548\u679c\u3002", "result": "FiDTouch\u80fd\u63d0\u4f9b\u7cbe\u51c6\u7684\u89e6\u89c9\u548c\u529b\u523a\u6fc0\uff0c\u589e\u5f3a\u4eba\u673a\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u6c89\u6d78\u611f\u548c\u6548\u7387\u3002", "conclusion": "FiDTouch\u5c55\u793a\u4e86\u5728\u9ad8\u7cbe\u5ea6\u89e6\u89c9\u53cd\u9988\u9886\u57df\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2507.07932", "pdf": "https://arxiv.org/pdf/2507.07932", "abs": "https://arxiv.org/abs/2507.07932", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Qiang Guan", "Hailong Jiang"], "title": "KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling", "categories": ["cs.DC"], "comment": "8 pages, 6 figures", "summary": "Autoscaling GPU inference workloads in Kubernetes remains challenging due to\nthe reactive and threshold-based nature of default mechanisms such as the\nHorizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty\ntraffic patterns and lack integration with GPU-level metrics. We present KIS-S,\na unified framework that combines KISim, a GPU-aware Kubernetes Inference\nSimulator, with KIScaler, a Proximal Policy Optimization (PPO)-based\nautoscaler. KIScaler learns latency-aware and resource-efficient scaling\npolicies entirely in simulation, and is directly deployed without retraining.\nExperiments across four traffic patterns show that KIScaler improves average\nreward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and\ngeneralizes without retraining. Our work bridges the gap between reactive\nautoscaling and intelligent orchestration for scalable GPU-accelerated\nenvironments.", "AI": {"tldr": "KIS-S\u6846\u67b6\u901a\u8fc7\u7ed3\u5408GPU\u611f\u77e5\u7684\u6a21\u62df\u5668\u548cPPO\u9a71\u52a8\u7684\u81ea\u52a8\u7f29\u653e\u5668\uff0c\u4f18\u5316Kubernetes\u4e2dGPU\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a8\u6001\u6269\u5c55\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3Kubernetes\u9ed8\u8ba4Horizontal Pod Autoscaler\uff08HPA\uff09\u5728\u9762\u5bf9\u52a8\u6001\u7a81\u53d1\u6d41\u91cf\u548c\u7f3a\u4e4fGPU\u6307\u6807\u65f6\u7684\u4e0d\u8db3\u3002", "method": "KIS-S\u7ed3\u5408KISim\uff08GPU\u611f\u77e5\u6a21\u62df\u5668\uff09\u548cKIScaler\uff08\u57fa\u4e8ePPO\u7684\u81ea\u52a8\u7f29\u653e\u5668\uff09\uff0c\u5728\u6a21\u62df\u4e2d\u5b66\u4e60\u5ef6\u8fdf\u611f\u77e5\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cKIScaler\u5e73\u5747\u5956\u52b1\u63d0\u9ad875.2%\uff0cP95\u5ef6\u8fdf\u964d\u4f4e6.7\u500d\uff0c\u4e14\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u901a\u7528\u6027\u5f3a\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u53cd\u5e94\u5f0f\u81ea\u52a8\u6269\u5c55\u4e0e\u667a\u80fd\u7f16\u6392\u5728GPU\u52a0\u901f\u73af\u5883\u4e2d\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.07881", "pdf": "https://arxiv.org/pdf/2507.07881", "abs": "https://arxiv.org/abs/2507.07881", "authors": ["Roberto Ulloa", "Juhi Kulshrestha", "Celina Kacperski"], "title": "Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "The rise of conversational AI (CAI), powered by large language models, is\ntransforming how individuals access and interact with digital information.\nHowever, these tools may inadvertently amplify existing digital inequalities.\nThis study investigates whether differences in formal education are associated\nwith CAI avoidance, leveraging behavioral data from an online experiment (N =\n1,636). Participants were randomly assigned to a control or an\ninformation-seeking task, either a traditional online search or a CAI\n(Perplexity AI). Task avoidance (operationalized as survey abandonment or\nproviding unrelated responses during task assignment) was significantly higher\nin the CAI group (51%) compared to the search (30.9%) and control (16.8%)\ngroups, with the highest CAI avoidance among participants with lower education\nlevels (~74.4%). Structural equation modeling based on the theoretical\nframework UTAUT2 and LASSO regressions reveal that education is strongly\nassociated with CAI avoidance, even after accounting for various cognitive and\naffective predictors of technology adoption. These findings underscore\neducation's central role in shaping AI adoption and the role of self-selection\nbiases in AI-related research, stressing the need for inclusive design to\nensure equitable access to emerging technologies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6559\u80b2\u6c34\u5e73\u8f83\u4f4e\u7684\u4eba\u7fa4\u66f4\u5bb9\u6613\u56de\u907f\u4f7f\u7528\u5bf9\u8bdd\u5f0fAI\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u5305\u5bb9\u6027\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u5bf9\u8bdd\u5f0fAI\u5de5\u5177\u662f\u5426\u4f1a\u56e0\u6559\u80b2\u6c34\u5e73\u5dee\u5f02\u5bfc\u81f4\u6570\u5b57\u4e0d\u5e73\u7b49\u52a0\u5267\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\uff08N=1,636\uff09\u548c\u884c\u4e3a\u6570\u636e\u5206\u6790\uff0c\u7ed3\u5408UTAUT2\u7406\u8bba\u548cLASSO\u56de\u5f52\u3002", "result": "\u5bf9\u8bdd\u5f0fAI\u7ec4\u7684\u4efb\u52a1\u56de\u907f\u7387\uff0851%\uff09\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u641c\u7d22\u7ec4\uff0830.9%\uff09\u548c\u5bf9\u7167\u7ec4\uff0816.8%\uff09\uff0c\u6559\u80b2\u6c34\u5e73\u8f83\u4f4e\u7684\u53c2\u4e0e\u8005\u56de\u907f\u7387\u6700\u9ad8\uff0874.4%\uff09\u3002", "conclusion": "\u6559\u80b2\u6c34\u5e73\u5bf9AI\u5de5\u5177\u7684\u91c7\u7eb3\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u901a\u8fc7\u5305\u5bb9\u6027\u8bbe\u8ba1\u51cf\u5c11\u4e0d\u5e73\u7b49\u3002"}}
{"id": "1602.03104", "pdf": "https://arxiv.org/pdf/1602.03104", "abs": "https://arxiv.org/abs/1602.03104", "authors": ["Ayan Dutta", "Prithviraj Dasgupta", "Carl Nelson"], "title": "A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation", "categories": ["cs.RO", "cs.DC", "cs.DS"], "comment": null, "summary": "We consider the problem of configuration formation in modular robot systems\nwhere a set of modules that are initially in different configurations and\nlocated at different locations are required to assume appropriate positions so\nthat they can get into a new, user-specified, target configuration. We propose\na novel algorithm based on graph isomorphism, where the modules select\nlocations or spots in the target configuration using a utility-based framework,\nwhile retaining their original configuration to the greatest extent possible,\nto reduce the time and energy required by the modules to assume the target\nconfiguration. We have shown analytically that our proposed algorithm is\ncomplete and guarantees a Pareto-optimal allocation. Experimental simulations\nof our algorithm with different number of modules in different initial\nconfigurations and located initially at different locations, show that the\nplanning time of our algorithm is nominal (order of msec. for 100 modules). We\nhave also compared our algorithm against a market-based allocation algorithm\nand shown that our proposed algorithm performs better in terms of time and\nnumber of messages exchanged.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u540c\u6784\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u914d\u7f6e\u5f62\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u6548\u7528\u6846\u67b6\u9009\u62e9\u76ee\u6807\u4f4d\u7f6e\uff0c\u51cf\u5c11\u65f6\u95f4\u548c\u80fd\u8017\uff0c\u5e76\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u5b8c\u5907\u6027\u548c\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u3002", "motivation": "\u89e3\u51b3\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u521d\u59cb\u914d\u7f6e\u4e0d\u540c\u7684\u6a21\u5757\u5982\u4f55\u9ad8\u6548\u79fb\u52a8\u5230\u7528\u6237\u6307\u5b9a\u76ee\u6807\u914d\u7f6e\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u56fe\u540c\u6784\u548c\u6548\u7528\u6846\u67b6\u7684\u7b97\u6cd5\uff0c\u6a21\u5757\u9009\u62e9\u76ee\u6807\u4f4d\u7f6e\u65f6\u5c3d\u53ef\u80fd\u4fdd\u7559\u539f\u59cb\u914d\u7f6e\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u5b8c\u5907\u6027\u548c\u5e15\u7d2f\u6258\u6700\u4f18\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u89c4\u5212\u65f6\u95f4\u77ed\uff08100\u6a21\u5757\u4ec5\u9700\u6beb\u79d2\u7ea7\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u5e02\u573a\u5206\u914d\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u65f6\u95f4\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u6548\u914d\u7f6e\u5f62\u6210\u3002"}}
{"id": "2507.07589", "pdf": "https://arxiv.org/pdf/2507.07589", "abs": "https://arxiv.org/abs/2507.07589", "authors": ["Arpana Sinhal", "Anay Sinhal", "Amit Sinhal"], "title": "Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Healthcare professionals, particularly nurses, face elevated occupational\nstress, a concern amplified during the COVID-19 pandemic. While wearable\nsensors offer promising avenues for real-time stress monitoring, existing\nstudies often lack comprehensive datasets and robust analytical frameworks.\nThis study addresses these gaps by introducing a multimodal dataset comprising\nphysiological signals, electrodermal activity, heart rate and skin temperature.\nA systematic literature review identified limitations in prior stress-detection\nmethodologies, particularly in handling class imbalance and optimizing model\ngeneralizability. To overcome these challenges, the dataset underwent\npreprocessing with the Synthetic Minority Over sampling Technique (SMOTE),\nensuring balanced representation of stress states. Advanced machine learning\nmodels including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were\nevaluated and combined into a Stacking Classifier to leverage their collective\npredictive strengths. By using a publicly accessible dataset and a reproducible\nanalytical pipeline, this work advances the development of deployable\nstress-monitoring systems, offering practical implications for safeguarding\nhealthcare workers' mental health. Future research directions include expanding\ndemographic diversity and exploring edge-computing implementations for low\nlatency stress alerts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89e3\u51b3\u4e86\u533b\u7597\u5de5\u4f5c\u8005\u538b\u529b\u76d1\u6d4b\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u53ef\u90e8\u7f72\u7684\u538b\u529b\u76d1\u6d4b\u7cfb\u7edf\u3002", "motivation": "COVID-19\u75ab\u60c5\u671f\u95f4\uff0c\u533b\u62a4\u4eba\u5458\u538b\u529b\u95ee\u9898\u7a81\u51fa\uff0c\u800c\u73b0\u6709\u538b\u529b\u76d1\u6d4b\u6280\u672f\u56e0\u6570\u636e\u96c6\u548c\u5206\u6790\u6846\u67b6\u4e0d\u8db3\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u7ed3\u5408SMOTE\u9884\u5904\u7406\u6280\u672f\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u3001XGBoost\u548cMLP\u6a21\u578b\uff0c\u5e76\u6574\u5408\u4e3a\u5806\u53e0\u5206\u7c7b\u5668\u3002", "result": "\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u548c\u53ef\u590d\u73b0\u5206\u6790\u6d41\u7a0b\uff0c\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u538b\u529b\u76d1\u6d4b\u7cfb\u7edf\u3002", "conclusion": "\u672a\u6765\u9700\u6269\u5927\u6837\u672c\u591a\u6837\u6027\u5e76\u63a2\u7d22\u8fb9\u7f18\u8ba1\u7b97\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u538b\u529b\u9884\u8b66\u3002"}}
{"id": "2507.07916", "pdf": "https://arxiv.org/pdf/2507.07916", "abs": "https://arxiv.org/abs/2507.07916", "authors": ["Federico Maria Cau", "Giuseppe Desolda", "Francesco Greco", "Lucio Davide Spano", "Luca Vigan\u00f2"], "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations", "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u5426\u751f\u6210\u6e05\u6670\u3001\u7b80\u6d01\u4e14\u53ef\u6269\u5c55\u7684\u7f51\u7edc\u9493\u9c7c\u8b66\u544a\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u4f18\u4e8e\u4eba\u5de5\u751f\u6210\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u9493\u9c7c\u8b66\u544a\u5bf9\u8bdd\u56e0\u89e3\u91ca\u4e0d\u6e05\u6670\u548c\u5185\u5bb9\u9759\u6001\u5bfc\u81f4\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u7528\u6237\u5b9e\u9a8c\uff08N=750\uff09\uff0c\u6bd4\u8f83\u4eba\u5de5\u751f\u6210\u4e0eLLM\uff08Claude 3.5 Sonnet\u548cLlama 3.3 70B\uff09\u751f\u6210\u7684\u4e24\u79cd\u89e3\u91ca\u98ce\u683c\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "LLM\u751f\u6210\u7684\u89e3\u91ca\u5728\u964d\u4f4e\u9493\u9c7c\u653b\u51fb\u6613\u611f\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cClaude\u6548\u679c\u5c24\u4e3a\u7a81\u51fa\uff1b\u7279\u5f81\u89e3\u91ca\u5bf9\u771f\u5b9e\u9493\u9c7c\u66f4\u6709\u6548\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u51cf\u5c11\u8bef\u62a5\u3002", "conclusion": "LLM\u53ef\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u9ad8\u6548\u7684\u9493\u9c7c\u8b66\u544a\u89e3\u91ca\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u7b26\u5408\u4ee5\u4eba\u4e3a\u672c\u7684\u4ef7\u503c\u89c2\u3002"}}
