<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 11]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [math.SG](#math.SG) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code](https://arxiv.org/abs/2508.08322)
*Muhammad Haseeb*

Main category: cs.SE

TL;DR: 该论文提出了一种结合多AI组件的上下文工程工作流，以提高语言模型在复杂多文件项目中的代码生成能力，显著提升了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在代码生成和软件工程任务中表现出潜力，但在处理复杂多文件项目时，仍受限于上下文不足和知识缺口的挑战。

Method: 采用了多组件协同的工作流，包括意图翻译（GPT-5）、语义文献检索（Elicit）、文档合成（NotebookLM）和多代理代码生成（Claude Code）。

Result: 实验表明，该方法在真实代码库中显著提升了单次成功率，且在上下文理解方面优于单代理基线方法。

Conclusion: 多代理系统通过目标明确的上下文注入和角色分解，实现了先进的性能，为生产环境中部署LLM代码助手提供了新思路。

Abstract: Large Language Models (LLMs) have shown promise in automating code generation
and software engineering tasks, yet they often struggle with complex,
multi-file projects due to context limitations and knowledge gaps. We propose a
novel context engineering workflow that combines multiple AI components: an
Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered
semantic literature retrieval for injecting domain knowledge, NotebookLM-based
document synthesis for contextual understanding, and a Claude Code multi-agent
system for code generation and validation. Our integrated approach leverages
intent clarification, retrieval-augmented generation, and specialized
sub-agents orchestrated via Claude's agent framework. We demonstrate that this
method significantly improves the accuracy and reliability of code assistants
in real-world repositories, yielding higher single-shot success rates and
better adherence to project context than baseline single-agent approaches.
Qualitative results on a large Next.js codebase show the multi-agent system
effectively plans, edits, and tests complex features with minimal human
intervention. We compare our system with recent frameworks like CodePlan,
MASAI, and HyperAgent, highlighting how targeted context injection and agent
role decomposition lead to state-of-the-art performance. Finally, we discuss
the implications for deploying LLM-based coding assistants in production, along
with lessons learned on context management and future research directions.

</details>


### [2] [Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming](https://arxiv.org/abs/2508.08332)
*Humza Ashraf,Syed Muhammad Danish,Aris Leivadeas,Yazan Otoum,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 研究比较了小型开源语言模型（SLMs）与大型商业模型（LLMs）在代码生成性能与能源效率上的差异，发现SLMs在能效上更优，但LLMs在正确性上更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在代码生成中能耗高且碳排放量大，研究旨在探讨小型开源模型是否能以更低的能源消耗达到相近的性能。

Method: 评估了150个LeetCode编程问题，涉及三种难度级别，比较了三种SLMs与两种LLMs在运行时间、内存使用、能耗和正确性上的表现。

Result: LLMs在所有难度级别上正确性最高，但SLMs在能效上有优势，52%的问题中SLMs能耗相同或更低。

Conclusion: SLMs在能效上优于LLMs，适用于对能源敏感的场景，但LLMs仍主导高正确性需求的任务。

Abstract: Large Language Models (LLMs) are widely used for code generation. However,
commercial models like ChatGPT require significant computing power, which leads
to high energy use and carbon emissions. This has raised concerns about their
environmental impact. In this study, we evaluate open-source Small Language
Models (SLMs) trained explicitly for code generation and compare their
performance and energy efficiency against large LLMs and efficient
human-written Python code. The goal is to investigate whether SLMs can match
the performance of LLMs on certain types of programming problems while
producing more energy-efficient code. We evaluate 150 coding problems from
LeetCode, evenly distributed across three difficulty levels: easy, medium, and
hard. Our comparison includes three small open-source models, StableCode-3B,
StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial
models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using
four key metrics: run-time, memory usage, energy consumption, and correctness.
We use human-written solutions as a baseline to assess the quality and
efficiency of the model-generated code. Results indicate that LLMs achieve the
highest correctness across all difficulty levels, but SLMs are often more
energy-efficient when their outputs are correct. In over 52% of the evaluated
problems, SLMs consumed the same or less energy than LLMs.

</details>


### [3] [Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization](https://arxiv.org/abs/2508.08342)
*Maximilian Jungwirth,Martin Gruber,Gordon Fraser*

Main category: cs.SE

TL;DR: 通过历史构建数据、PR元数据和上下文信息预测PR成功构建的概率，优化合并管道中PR的顺序，显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型软件仓库中合并管道的负载过高成为瓶颈，现有优化方法依赖特定构建系统，通用性较差。

Method: 利用历史数据预测PR构建成功率，动态优先处理可能通过的PR。

Result: 实验表明，基于预测的排序策略显著优于FIFO和非学习排序策略，且不受构建系统限制。

Conclusion: 该方法通用性强，易于集成到现有合并管道中，有效解决负载瓶颈问题。

Abstract: Integrating changes into large monolithic software repositories is a critical
step in modern software development that substantially impacts the speed of
feature delivery, the stability of the codebase, and the overall productivity
of development teams. To ensure the stability of the main branch, many
organizations use merge pipelines that test software versions before the
changes are permanently integrated. However, the load on merge pipelines is
often so high that they become bottlenecks, despite the use of parallelization.
Existing optimizations frequently rely on specific build systems, limiting
their generalizability and applicability. In this paper we propose to optimize
the order of PRs in merge pipelines using practical build predictions utilizing
only historical build data, PR metadata, and contextual information to estimate
the likelihood of successful builds in the merge pipeline. By dynamically
prioritizing likely passing PRs during peak hours, this approach maximizes
throughput when it matters most. Experiments conducted on a real-world,
large-scale project demonstrate that predictive ordering significantly
outperforms traditional first-in-first-out (FIFO), as well as
non-learning-based ordering strategies. Unlike alternative optimizations, this
approach is agnostic to the underlying build system and thus easily integrable
into existing automated merge pipelines.

</details>


### [4] [OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval](https://arxiv.org/abs/2508.08545)
*Youssef Esseddiq Ouatiti,Mohammed Sayagh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: OmniLLP框架通过语义相似性和开发者所有权凝聚性聚类源代码文件，提升LLM日志级别预测器的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的日志级别预测器依赖随机上下文示例，忽略了代码结构和多样化日志实践，导致预测效果有限。

Method: OmniLLP通过语义和开发者所有权聚类源代码文件，从中选择上下文学习示例，生成更一致的提示。

Result: 结果显示，OmniLLP显著提升了预测准确性（AUC提升8%），综合方法在评估项目中AUC达到0.88-0.96。

Conclusion: 结合代码语义和开发者所有权信号可显著提升LLM日志级别预测器的准确性，增强系统可维护性和可观测性。

Abstract: Developers insert logging statements in source code to capture relevant
runtime information essential for maintenance and debugging activities. Log
level choice is an integral, yet tricky part of the logging activity as it
controls log verbosity and therefore influences systems' observability and
performance. Recent advances in ML-based log level prediction have leveraged
large language models (LLMs) to propose log level predictors (LLPs) that
demonstrated promising performance improvements (AUC between 0.64 and 0.8).
Nevertheless, current LLM-based LLPs rely on randomly selected in-context
examples, overlooking the structure and the diverse logging practices within
modern software projects. In this paper, we propose OmniLLP, a novel LLP
enhancement framework that clusters source files based on (1) semantic
similarity reflecting the code's functional purpose, and (2) developer
ownership cohesion. By retrieving in-context learning examples exclusively from
these semantic and ownership aware clusters, we aim to provide more coherent
prompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.
Our results show that both semantic and ownership-aware clusterings
statistically significantly improve the accuracy (by up to 8\% AUC) of the
evaluated LLM-based LLPs compared to random predictors (i.e., leveraging
randomly selected in-context examples from the whole project). Additionally,
our approach that combines the semantic and ownership signal for in-context
prediction achieves an impressive 0.88 to 0.96 AUC across our evaluated
projects. Our findings highlight the value of integrating software
engineering-specific context, such as code semantic and developer ownership
signals into LLM-LLPs, offering developers a more accurate, contextually-aware
approach to logging and therefore, enhancing system maintainability and
observability.

</details>


### [5] [Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics](https://arxiv.org/abs/2508.08661)
*Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文首次全面分析了代码变更任务中的幻觉问题，发现在代码审查和提交信息生成中幻觉率分别为50%和20%，并探讨了多种检测方法。


<details>
  <summary>Details</summary>
Motivation: 研究代码变更任务中的幻觉现象，填补该领域的空白。

Method: 量化幻觉率并探索基于多种指标的自动检测方法。

Result: 代码审查和提交信息的幻觉率分别为50%和20%，结合多指标可显著提升检测性能。

Conclusion: 模型置信度和特征归因指标在幻觉检测中表现良好，有望用于实时检测。

Abstract: Language models have shown strong capabilities across a wide range of tasks
in software engineering, such as code generation, yet they suffer from
hallucinations. While hallucinations have been studied independently in natural
language and code generation, their occurrence in tasks involving code changes
which have a structurally complex and context-dependent format of code remains
largely unexplored. This paper presents the first comprehensive analysis of
hallucinations in two critical tasks involving code change to natural language
generation: commit message generation and code review comment generation. We
quantify the prevalence of hallucinations in recent language models and explore
a range of metric-based approaches to automatically detect them. Our findings
reveal that approximately 50\% of generated code reviews and 20\% of generated
commit messages contain hallucinations. Whilst commonly used metrics are weak
detectors on their own, combining multiple metrics substantially improves
performance. Notably, model confidence and feature attribution metrics
effectively contribute to hallucination detection, showing promise for
inference-time detection.\footnote{All code and data will be released upon
acceptance.

</details>


### [6] [Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset](https://arxiv.org/abs/2508.08868)
*Henning Femmer,Frank Houdek,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 该论文介绍了QuRE数据集，包含2111条工业需求，标注详细，旨在提升需求质量研究的透明度和可比性。


<details>
  <summary>Details</summary>
Motivation: 需求质量对软件和系统工程至关重要，但现有数据集通常不可访问、规模小或缺乏细节。

Method: 发布了QuRE数据集，并提供描述性统计，对比现有数据集和合成需求。

Result: QuRE数据集体现了语言多样性，并与现有数据集相似，但标注更详细且工业背景丰富。

Conclusion: QuRE支持建立一个通用的需求质量黄金标准，促进更严谨的实证研究。

Abstract: Requirements quality is central to successful software and systems
engineering. Empirical research on quality defects in natural language
requirements relies heavily on datasets, ideally as realistic and
representative as possible. However, such datasets are often inaccessible,
small, or lack sufficient detail. This paper introduces QuRE (Quality in
Requirements), a new dataset comprising 2,111 industrial requirements that have
been annotated through a real-world review process. Previously used for over
five years as part of an industrial contract, this dataset is now being
released to the research community. In this work, we furthermore provide
descriptive statistics on the dataset, including measures such as lexical
diversity and readability, and compare it to existing requirements datasets and
synthetically generated requirements. In contrast to synthetic datasets, QuRE
is linguistically similar to existing ones. However, this dataset comes with a
detailed context description, and its labels have been created and used
systematically and extensively in an industrial context over a period of close
to a decade. Our goal is to foster transparency, comparability, and empirical
rigor by supporting the development of a common gold standard for requirements
quality datasets. This, in turn, will enable more sound and collaborative
research efforts in the field.

</details>


### [7] [Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories](https://arxiv.org/abs/2508.08872)
*Dylan Callaghan,Alexandra van der Spuy,Bernd Fischer*

Main category: cs.SE

TL;DR: 论文分析了16个开源Java和Python项目中软件故障的时空特征，发现多数故障长期存在且分布均匀，与原始数据集假设的单故障版本不符。


<details>
  <summary>Details</summary>
Motivation: 降低软件维护成本需要理解故障特征，但现有数据集假设与实际情况存在差异。

Method: 使用Defects4J和BugsInPy数据集，对16个开源项目进行故障时空特征分析。

Result: 多数故障长期存在，版本中常有多故障共存，故障分布均匀，热点较少。

Conclusion: 研究揭示了故障的真实特征，为优化测试和评估提供了新视角。

Abstract: Fixing software faults contributes significantly to the cost of software
maintenance and evolution. Techniques for reducing these costs require datasets
of software faults, as well as an understanding of the faults, for optimal
testing and evaluation. In this paper, we present an empirical analysis of the
temporal and spatial characteristics of faults existing in 16 open-source Java
and Python projects, which form part of the Defects4J and BugsInPy datasets,
respectively. Our findings show that many faults in these software systems are
long-lived, leading to the majority of software versions having multiple
coexisting faults. This is in contrast to the assumptions of the original
datasets, where the majority of versions only identify a single fault. In
addition, we show that although the faults are found in only a small subset of
the systems, these faults are often evenly distributed amongst this subset,
leading to relatively few bug hotspots.

</details>


### [8] [Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments](https://arxiv.org/abs/2508.08952)
*Hyunwoo Kim,Jaeseong Lee,Sunpyo Hong,Changmin Han*

Main category: cs.SE

TL;DR: 论文提出了一种自动化场景生成框架，帮助汽车行业高效分配硬件资源给多个虚拟机，优化了资源整合和开发效率。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆（SDVs）的兴起，如何动态适应系统需求和多样化工作负载成为汽车行业的重要挑战。

Method: 提出自动化框架，结合运行时行为分析和理论模型与厂商启发式方法，生成优化的虚拟机配置。比较了两种QoS建模方法：领域引导参数建模和深度学习建模。

Result: 框架在资源受限环境下显著提升了集成效率和减少了开发时间。

Conclusion: 通过自动化工具优化资源分配，能够有效应对汽车行业中的虚拟化挑战。

Abstract: In the automotive industry, the rise of software-defined vehicles (SDVs) has
  driven a shift toward virtualization-based architectures that consolidate
  diverse automotive workloads on a shared hardware platform. To support this
  evolution, chipset vendors provide board support packages (BSPs), hypervisor
  setups, and resource allocation guidelines. However, adapting these static
  configurations to varying system requirements and workloads remain a
  significant challenge for Tier 1 integrators.
  This paper presents an automated scenario generation framework, which helps
  automotive vendors to allocate hardware resources efficiently across multiple
  VMs. By profiling runtime behavior and integrating both theoretical models
and
  vendor heuristics, the proposed tool generates optimized hypervisor
  configurations tailored to system constraints.
  We compare two main approaches for modeling target QoS based on profiled data
  and resource allocation: domain-guided parametric modeling and deep
  learning-based modeling. We further describe our optimization strategy using
  the selected QoS model to derive efficient resource allocations. Finally, we
  report on real-world deployments to demonstrate the effectiveness of our
  framework in improving integration efficiency and reducing development time
in
  resource-constrained environments.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving](https://arxiv.org/abs/2508.08343)
*Ferran Agullo,Joan Oliveras,Chen Wang,Alberto Gutierrez-Torre,Olivier Tardieu,Alaa Youssef,Jordi Torres,Josep Ll. Berral*

Main category: cs.PF

TL;DR: 该论文提出了一种AI驱动的分析管道，用于优化LLM适配器的分配，以提高性能和资源利用效率，并通过数字孪生技术验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM适配器的广泛应用，其性能和资源分配的优化成为关键问题，目前方法存在性能下降和资源利用不足的挑战。

Method: 开发了一种分析管道，结合数字孪生技术，动态确定单节点设置中的适配器最优分配，并扩展到多副本部署中。

Result: 数字孪生的吞吐量差异不超过5.5%，管道能准确预测最优分配并最小化延迟。

Conclusion: 该方案显著提升了LLM适配器服务的性能和资源效率，适用于多种部署场景。

Abstract: Serving LLM adapters has gained significant attention as an effective
approach to adapt general-purpose language models to diverse, task-specific use
cases. However, serving a wide range of adapters introduces several and
substantial overheads, leading to performance degradation and challenges in
optimal placement. To address these challenges, we present an analytical,
AI-driven pipeline that accurately determines the optimal allocation of
adapters in single-node setups. This allocation maximizes performance,
effectively using GPU resources, while preventing request starvation.
Crucially, the proposed allocation is given based on current workload patterns.
These insights in single-node setups can be leveraged in multi-replica
deployments for overall placement, load balancing and server configuration,
ultimately enhancing overall performance and improving resource efficiency. Our
approach builds on an in-depth analysis of LLM adapter serving, accounting for
overheads and performance variability, and includes the development of the
first Digital Twin capable of replicating online LLM-adapter serving systems
with matching key performance metrics. The experimental results demonstrate
that the Digital Twin achieves a SMAPE difference of no more than 5.5% in
throughput compared to real results, and the proposed pipeline accurately
predicts the optimal placement with minimal latency.

</details>


### [10] [Profiling Large Language Model Inference on Apple Silicon: A Quantization Perspective](https://arxiv.org/abs/2508.08531)
*Afsara Benazir,Felix Xiaozhu Lin*

Main category: cs.PF

TL;DR: 该论文首次系统研究了苹果芯片（Apple Silicon）在设备端大语言模型（LLM）推理中的性能表现，并与NVIDIA GPU进行了对比，揭示了其统一内存架构的优势和性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注GPU加速大规模训练或推理，缺乏对苹果芯片硬件效率的系统理解，尤其是其统一内存架构在设备端LLM推理中的潜力。

Method: 通过直接对比苹果芯片（M2 Ultra、M2 Max、M4 Pro）和NVIDIA GPU（RTX A6000）在延迟和吞吐量上的表现，并结合低层硬件指标（如ALU利用率、内存带宽等）分析性能差异。

Result: 研究发现苹果芯片的统一内存使其在超大规模语言模型推理中兼具成本效益和效率，同时揭示了量化压缩并非对所有硬件平台都带来加速效果的性能瓶颈。

Conclusion: 苹果芯片在设备端LLM推理中表现优异，尤其是在处理超大规模模型时，但需注意性能瓶颈问题；该研究填补了相关领域的空白。

Abstract: A systematic understanding of Apple Silicon is lacking in the current
landscape of hardware efficiency; research focus is largely centered on
accelerating GPUs for large-scale training or inference on CUDA devices. This
paper investigates Apple Silicon's unique memory architecture that offers a
unified memory integrating CPU and GPU memory and its implications for
on-device LLM inference.
  We decipher myths about whether Apple Silicon is efficient for on-device
inference compared to competitors such as NVIDIA GPUs by directly conducting
latency and throughput comparison benchmarks. We explain the performance gap
between them through profiling low level hardware metrics - ALU utilization,
memory bandwidth, buffer usage, cache residency etc. at runtime. We draw
several insights regarding performance bottlenecks such as dequantization
overhead, compute throughput and memory bandwidth. We debunk existing false
claims regarding large language model inference such as compressing models to
lower bit precision is a defacto promise for faster inference across all
hardware platforms. We find that the large unified memory enables Apple Silicon
to be both cost effective and efficient against NVIDIA GPUs for ultra large
language models.
  Our large scale evaluation on 5 hardware testbeds incorporating three Apple
M-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX
A6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from
8B to 405B parameters and 14 quantization schemes gives an understanding of how
Apple Silicon fits within the paradigm of on-device LLM inference. Our analysis
reveals multiple resource interdependencies and unexpected findings, while also
quantifying established insights. To the best of our knowledge, this study
makes the first attempt to present a thorough characterization and analysis of
Apple Silicon for on-device inference.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [11] [Towards Efficient and Practical GPU Multitasking in the Era of LLM](https://arxiv.org/abs/2508.08448)
*Jiarong Xing,Yifan Qiao,Simon Mo,Xingqi Cui,Gur-Eyal Sela,Yang Zhou,Joseph Gonzalez,Ion Stoica*

Main category: cs.OS

TL;DR: 本文讨论了GPU单任务模式的低效性，并提出向多任务模式的转变，类似于CPU的发展历程。文中提出了解决GPU资源管理和共享的潜在方案，并呼吁社区共同努力构建基于多任务的下一代GPU计算范式。


<details>
  <summary>Details</summary>
Motivation: 随着硬件能力的提升和工作负载的多样化，GPU单任务模式变得低效且不可持续。作者认为GPU必须转向多任务模式以满足现代AI工作负载的需求。

Method: 本文首先分析了GPU多任务的关键需求，回顾了已有研究的不足，并提出一个类似CPU操作系统的资源管理层，用于管理GPU资源的共享。作者还讨论了相关挑战和潜在解决方案。

Result: 通过分析，作者指出了现有方法的局限性，并提出了构建GPU多任务模式的潜在路径，为下一代GPU计算范式的发展奠定了基础。

Conclusion: GPU多任务是未来的发展方向，需要社区共同努力实现。本文为这一目标提供了理论框架和潜在解决方案的探讨。

Abstract: GPU singletasking is becoming increasingly inefficient and unsustainable as
hardware capabilities grow and workloads diversify. We are now at an inflection
point where GPUs must embrace multitasking, much like CPUs did decades ago, to
meet the demands of modern AI workloads. In this work, we highlight the key
requirements for GPU multitasking, examine prior efforts, and discuss why they
fall short. To advance toward efficient and practical GPU multitasking, we
envision a resource management layer, analogous to a CPU operating system, to
handle various aspects of GPU resource management and sharing. We outline the
challenges and potential solutions, and hope this paper inspires broader
community efforts to build the next-generation GPU compute paradigm grounded in
multitasking.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [12] [Experimental Validation of Provably Covert Communication Using Software-Defined Radio](https://arxiv.org/abs/2508.08380)
*Rohan Bali,Trevor E. Bailey,Michael S. Bullock,Boulat A. Bash*

Main category: cs.NI

TL;DR: 论文摘要讨论了隐蔽通信的信息理论极限及其硬件实现的挑战，并展示了基于软件定义无线电的隐蔽射频通信实验验证。


<details>
  <summary>Details</summary>
Motivation: 隐蔽通信（LPD/LPI）的理论研究已有十多年，但硬件实现面临挑战。实验验证极少，尤其在射频领域。

Method: 使用软件定义无线电（SDRs）进行隐蔽射频通信的实验验证。

Result: 成功验证了理论预测，并为隐蔽通信系统的实际实现提供了方向。

Conclusion: 实验不仅验证了理论预测，还为隐蔽通信的实际应用开辟了新途径，并提出了进一步研究的问题。

Abstract: The fundamental information-theoretic limits of covert, or low probability of
detection/intercept (LPD/LPI), communication have been extensively studied for
over a decade, resulting in the square root law (SRL): only $L\sqrt{n}$ covert
bits can be reliably transmitted over time-bandwidth product $n$, for constant
$L>0$. Transmitting more either results in detection or decoding errors. The
SRL imposes significant constraints on hardware realization of
mathematically-guaranteed covert communication. Indeed, they preclude using
standard link maintenance operations that are taken for granted in non-covert
communication. Thus, experimental validation of covert communication is
underexplored: to date, only two experimental studies of SRL-based covert
communication are available, both focusing on optical channels. Here, we report
a demonstration of provably-secure covert radio-frequency (RF) communication
using software-defined radios (SDRs). This validates theoretical predictions,
opens practical avenues for implementing covert communication systems, and
raises further research questions.

</details>


### [13] [LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework](https://arxiv.org/abs/2508.08535)
*Azin Sabzian,Mohammad Jalili Torkamani,Negin Mahmoudi,Kiana Kiashemshaki*

Main category: cs.NI

TL;DR: 本文综述了无线体域网（WBAN）的架构、路由策略和安全机制，提出了一种由大型语言模型驱动的自适应WBAN框架，以解决现有系统在适应性、能效和抗量子安全方面的不足。


<details>
  <summary>Details</summary>
Motivation: 无线体域网（WBAN）在慢性病管理和紧急响应等应用中表现出潜力，但如何整合6G通信、后量子密码学等技术以提升其性能仍是一个挑战。

Method: 通过综述现有WBAN技术，提出了一种基于大型语言模型的认知控制平面框架，实时协调路由、物理层选择、微能量收集和抗量子安全。

Result: 文章指出了当前启发式设计的局限性，并提出了面向6G医疗系统的研究议程。

Conclusion: 该框架旨在实现超可靠、安全且自优化的下一代移动健康应用WBAN。

Abstract: Wireless Body Area Networks (WBANs) enable continuous monitoring of
physiological signals for applications ranging from chronic disease management
to emergency response. Recent advances in 6G communications, post-quantum
cryptography, and energy harvesting have the potential to enhance WBAN
performance. However, integrating these technologies into a unified, adaptive
system remains a challenge. This paper surveys some of the most well-known
Wireless Body Area Network (WBAN) architectures, routing strategies, and
security mechanisms, identifying key gaps in adaptability, energy efficiency,
and quantum-resistant security. We propose a novel Large Language Model-driven
adaptive WBAN framework in which a Large Language Model acts as a cognitive
control plane, coordinating routing, physical layer selection, micro-energy
harvesting, and post-quantum security in real time. Our review highlights the
limitations of current heuristic-based designs and outlines a research agenda
for resource-constrained, 6G-ready medical systems. This approach aims to
enable ultra-reliable, secure, and self-optimizing WBANs for next-generation
mobile health applications.

</details>


### [14] [Ultra Ethernet's Design Principles and Architectural Innovations](https://arxiv.org/abs/2508.08906)
*Torsten Hoefler,Karen Schramm,Eric Spada,Keith Underwood,Cedell Alexander,Bob Alverson,Paul Bottorff,Adrian Caulfield,Mark Handley,Cathy Huang,Costin Raiciu,Abdul Kabbani,Eugene Opsasnick,Rong Pan,Adee Ran,Rip Sohan*

Main category: cs.NI

TL;DR: Ultra Ethernet (UE) 1.0是一种面向未来AI和HPC系统的高性能以太网标准，其核心创新是Ultra Ethernet Transport (UET)，旨在为超大规模系统提供高效、可靠的通信。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有高性能网络标准（如InfiniBand）在极端规模系统中的不足，同时利用以太网生态系统和计算效率的巨大提升。

Method: 通过设计硬件加速的Ultra Ethernet Transport (UET)协议，优化整个以太网堆栈，以实现更高效的数据传输。

Result: Ultra Ethernet (UE) 1.0定义了一种全新的高性能网络标准，为未来AI和HPC系统提供更快速、可靠的通信能力。

Conclusion: UE 1.0通过创新设计和充分利用以太网生态系统，开创了高性能网络的新时代。

Abstract: The recently released Ultra Ethernet (UE) 1.0 specification defines a
transformative High-Performance Ethernet standard for future Artificial
Intelligence (AI) and High-Performance Computing (HPC) systems. This paper,
written by the specification's authors, provides a high-level overview of UE's
design, offering crucial motivations and scientific context to understand its
innovations. While UE introduces advancements across the entire Ethernet stack,
its standout contribution is the novel Ultra Ethernet Transport (UET), a
potentially fully hardware-accelerated protocol engineered for reliable, fast,
and efficient communication in extreme-scale systems. Unlike InfiniBand, the
last major standardization effort in high-performance networking over two
decades ago, UE leverages the expansive Ethernet ecosystem and the 1,000x gains
in computational efficiency per moved bit to deliver a new era of
high-performance networking.

</details>


### [15] [Traffic Load-Aware Resource Management Strategy for Underwater Wireless Sensor Networks](https://arxiv.org/abs/2508.08555)
*Tong Zhang,Yu Gou,Jun Liu,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 该论文通过优化通信链路调度和传输参数，解决水下无线传感器网络（UWSNs）能源和通信资源有限的问题，提出了基于多智能体深度强化学习的TARM策略。


<details>
  <summary>Details</summary>
Motivation: 水下网络面临严苛的通信环境、能源限制和信号传输问题，亟需高效可靠的通信解决方案。

Method: 将问题建模为分散式部分可观察马尔可夫决策过程，提出基于深度多智能体强化学习的TARM策略，包括流量感知机制和解决方案空间优化算法。

Result: 仿真结果显示TARM在不同场景中表现出适应性，并能有效支持资源有限的水下网络通信。

Conclusion: TARM策略为解决水下网络的通信和资源限制问题提供了有效途径。

Abstract: Underwater Wireless Sensor Networks (UWSNs) represent a promising technology
that enables diverse underwater applications through acoustic communication.
However, it encounters significant challenges including harsh communication
environments, limited energy supply, and restricted signal transmission. This
paper aims to provide efficient and reliable communication in underwater
networks with limited energy and communication resources by optimizing the
scheduling of communication links and adjusting transmission parameters (e.g.,
transmit power and transmission rate). The efficient and reliable communication
multi-objective optimization problem (ERCMOP) is formulated as a decentralized
partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware
Resource Management (TARM) strategy based on deep multi-agent reinforcement
learning (MARL) is presented to address this problem. Specifically, a traffic
load-aware mechanism that leverages the overhear information from neighboring
nodes is designed to mitigate the disparity between partial observations and
global states. Moreover, by incorporating a solution space optimization
algorithm, the number of candidate solutions for the deep MARL-based
decision-making model can be effectively reduced, thereby optimizing the
computational complexity. Simulation results demonstrate the adaptability of
TARM in various scenarios with different transmission demands and collision
probabilities, while also validating the effectiveness of the proposed approach
in supporting efficient and reliable communication in underwater networks with
limited resources.

</details>


### [16] [QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach](https://arxiv.org/abs/2508.08627)
*Conghao Zhou,Lulu Sun,Xiucheng Wang,Peng Yang,Feng Lyu,Sihan Lu,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出了一种基于代理的边缘辅助移动增强现实通信服务方法，通过LLM代理和用户级QoE建模降低通信开销并提升体验质量。


<details>
  <summary>Details</summary>
Motivation: 解决MAR应用中网络控制器无法获取特定信息的问题，同时应对用户数据流量的动态特性。

Method: 利用LLM建立代理连接MAR与网络域，开发用户级QoE建模方法进行个性化资源管理。

Result: 仿真结果显示该方法在QoE建模精度和通信资源效率上优于传统方法。

Conclusion: 代理驱动的方法能有效提升MAR通信服务的质量和效率。

Abstract: Mobile augmented reality (MAR) is envisioned as a key immersive application
in 6G, enabling virtual content rendering aligned with the physical environment
through device pose estimation. In this paper, we propose a novel agent-driven
communication service provisioning approach for edge-assisted MAR, aiming to
reduce communication overhead between MAR devices and the edge server while
ensuring the quality of experience (QoE). First, to address the inaccessibility
of MAR application-specific information to the network controller, we establish
a digital agent powered by large language models (LLMs) on behalf of the MAR
service provider, bridging the data and function gap between the MAR service
and network domains. Second, to cope with the user-dependent and dynamic nature
of data traffic patterns for individual devices, we develop a user-level QoE
modeling method that captures the relationship between communication resource
demands and perceived user QoE, enabling personalized, agent-driven
communication resource management. Trace-driven simulation results demonstrate
that the proposed approach outperforms conventional LLM-based QoE-aware service
provisioning methods in both user-level QoE modeling accuracy and communication
resource efficiency.

</details>


### [17] [Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring](https://arxiv.org/abs/2508.09085)
*Zihan Fang,Zheng Lin,Senkang Hu,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.NI

TL;DR: 提出了一个名为DUAL-Health的框架，用于在动态和嘈杂环境中进行户外健康监测，通过量化模态不确定性和多模态融合提升检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 户外健康监测对人类健康和安全至关重要，但现有方法受限于噪声数据和多模态融合的困难，需要一种更高效的解决方案。

Method: 提出DUAL-Health框架：1)量化由输入噪声和波动噪声引起的模态不确定性；2)基于不确定性定制多模态融合权重；3)在共同语义空间中对齐模态分布以增强数据恢复能力。

Result: 实验证明DUAL-Health在检测精度和鲁棒性上优于现有方法。

Conclusion: DUAL-Health通过不确定性感知的多模态融合，显著提升了户外健康监测的性能，适用于动态嘈杂环境。

Abstract: Outdoor health monitoring is essential to detect early abnormal health status
for safeguarding human health and safety. Conventional outdoor monitoring
relies on static multimodal deep learning frameworks, which requires extensive
data training from scratch and fails to capture subtle health status changes.
Multimodal large language models (MLLMs) emerge as a promising alternative,
utilizing only small datasets to fine-tune pre-trained information-rich models
for enabling powerful health status monitoring. Unfortunately, MLLM-based
outdoor health monitoring also faces significant challenges: I) sensor data
contains input noise stemming from sensor data acquisition and fluctuation
noise caused by sudden changes in physiological signals due to dynamic outdoor
environments, thus degrading the training performance; ii) current transformer
based MLLMs struggle to achieve robust multimodal fusion, as they lack a design
for fusing the noisy modality; iii) modalities with varying noise levels hinder
accurate recovery of missing data from fluctuating distributions. To combat
these challenges, we propose an uncertainty-aware multimodal fusion framework,
named DUAL-Health, for outdoor health monitoring in dynamic and noisy
environments. First, to assess the impact of noise, we accurately quantify
modality uncertainty caused by input and fluctuation noise with current and
temporal features. Second, to empower efficient muitimodal fusion with
low-quality modalities,we customize the fusion weight for each modality based
on quantified and calibrated uncertainty. Third, to enhance data recovery from
fluctuating noisy modalities, we align modality distributions within a common
semantic space. Extensive experiments demonstrate that our DUAL-Health
outperforms state-of-the-art baselines in detection accuracy and robustness.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [Fact-Checking at Scale: Multimodal AI for Authenticity and Context Verification in Online Media](https://arxiv.org/abs/2508.08592)
*Van-Hoang Phan,Tung-Duong Le-Duc,Long-Khanh Pham,Anh-Thu Le,Quynh-Huong Dinh-Nguyen,Dang-Quan Vo,Hoang-Quoc Nguyen-Son,Anh-Duy Tran,Dang Vu,Minh-Son Dao*

Main category: cs.MM

TL;DR: 论文介绍了一种多媒体验证系统，用于检测虚假信息，结合视觉取证、文本分析和多模态推理，并在多种语言和场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体中虚假信息的泛滥，尤其是在危机时期，开发强大的多媒体验证工具变得至关重要。

Method: 采用统一的验证流程，结合视觉取证、文本分析和多模态推理，并提出了基于语义相似性、时间对齐和地理位置的混合方法来检测上下文不符的媒体。

Result: 系统在ACM Multimedia 2025 Grand Challenge基准测试中展现了高效性，适用于多样化的现实场景。

Conclusion: 该系统推动了多媒体验证技术的发展，为记者、事实核查者和研究者提供了实用工具，以应对数字时代的信息完整性挑战。

Abstract: The proliferation of multimedia content on social media platforms has
dramatically transformed how information is consumed and disseminated. While
this shift enables real-time coverage of global events, it also facilitates the
rapid spread of misinformation and disinformation, especially during crises
such as wars, natural disasters, or elections. The rise of synthetic media and
the reuse of authentic content in misleading contexts have intensified the need
for robust multimedia verification tools. In this paper, we present a
comprehensive system developed for the ACM Multimedia 2025 Grand Challenge on
Multimedia Verification. Our system assesses the authenticity and contextual
accuracy of multimedia content in multilingual settings and generates both
expert-oriented verification reports and accessible summaries for the general
public. We introduce a unified verification pipeline that integrates visual
forensics, textual analysis, and multimodal reasoning, and propose a hybrid
approach to detect out-of-context (OOC) media through semantic similarity,
temporal alignment, and geolocation cues. Extensive evaluations on the Grand
Challenge benchmark demonstrate the system's effectiveness across diverse
real-world scenarios. Our contributions advance the state of the art in
multimedia verification and offer practical tools for journalists,
fact-checkers, and researchers confronting information integrity challenges in
the digital age.

</details>


### [19] [DASC: Depth-of-Field Aware Scene Complexity Metric for 3D Visualization on Light Field Display](https://arxiv.org/abs/2508.08928)
*Kamran Akbar,Robert Bregovic,Federica Battisti*

Main category: cs.MM

TL;DR: 提出了一种深度感知场景复杂度（DASC）指标，用于评估光场显示中3D内容的质量，并通过主观研究预测用户偏好的模糊程度。


<details>
  <summary>Details</summary>
Motivation: 光场显示技术在3D沉浸式可视化中存在深度范围限制，导致在屏幕外区域出现伪影。现有解决方案（如景深渲染）可能会丢失细节，因此需要一种新的指标来优化内容质量。

Method: 提出DASC指标，结合几何和位置因素评估3D内容，并通过主观研究分析用户对不同模糊程度的偏好，最终建立预测模型。

Result: 通过多个场景的主观研究，开发了一个基于DASC指标的模型，能够预测用户偏好的模糊程度。

Conclusion: DASC指标和预测模型可帮助优化光场显示的3D内容渲染，提升用户体验。

Abstract: Light field display is one of the technologies providing 3D immersive
visualization. However, a light field display generates only a limited number
of light rays which results in finite angular and spatial resolutions.
Therefore, 3D content can be shown with high quality only within a narrow depth
range notated as Depth of Field (DoF) around the display screen. Outside this
range, due to the appearance of aliasing artifacts, the quality degrades
proportionally to the distance from the screen. One solution to mitigate the
artifacts is depth of field rendering which blurs the content in the distorted
regions, but can result in the removal of scene details. This research focuses
on proposing a DoF Aware Scene Complexity (DASC) metric that characterizes 3D
content based on geometrical and positional factors considering the light field
display's DoF. In this research, we also evaluate the observers' preference
across different level of blurriness caused by DoF rendering ranging from
sharp, aliased scenes to overly smoothed alias-free scenes. We have conducted
this study over multiple scenes that we created to account for different types
of content. Based on the outcome of subjective studies, we propose a model that
takes the value of DASC metric as input and predicts the preferred level of
blurring for the given scene as output.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [20] [Solving Set Constraints with Comprehensions and Bounded Quantifiers](https://arxiv.org/abs/2508.08496)
*Mudathir Mohamed,Nick Feng,Andrew Reynolds,Cesare Tinelli,Clark Barrett,Marsha Chechik*

Main category: cs.LO

TL;DR: 该论文探讨了在SMT中使用集合有界量词的技术，提出了一种优于现有方法的量化处理方案，并在特定应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决SMT求解器在处理量化公式时的困难，特别是在某些应用生成的公式中表现不佳的问题，研究者探索了集合有界量词的使用。

Method: 论文提出使用量词自由片段和有限关系理论的过滤操作符来实现集合有界量词，这是一种受限的理解形式。

Result: 实验显示，该方法在SLEEC工具生成的可满足问题上优于其他量化技术，并在不可满足基准测试中与LEGOS（专用于SLEEC的求解器）竞争激烈。

Conclusion: 研究还发现，限制过滤操作符的应用范围可以保证约束问题的可判定性，而无限应用则会导致不可判定性。

Abstract: Many real applications problems can be encoded easily as quantified formulas
in SMT. However, this simplicity comes at the cost of difficulty during solving
by SMT solvers. Different strategies and quantifier instantiation techniques
have been developed to tackle this. However, SMT solvers still struggle with
quantified formulas generated by some applications. In this paper, we discuss
the use of set-bounded quantifiers, quantifiers whose variable ranges over a
finite set. These quantifiers can be implemented using quantifier-free fragment
of the theory of finite relations with a filter operator, a form of restricted
comprehension, that constructs a subset from a finite set using a predicate. We
show that this approach outperforms other quantification techniques in
satisfiable problems generated by the SLEEC tool, and is very competitive on
unsatisfiable benchmarks compared to LEGOS, a specialized solver for SLEEC. We
also identify a decidable class of constraints with restricted applications of
the filter operator, while showing that unrestricted applications lead to
undecidability.

</details>


### [21] [Behavioural Theory of Reflective Algorithms II: Reflective Parallel Algorithms](https://arxiv.org/abs/2508.09053)
*Klaus-Dieter Schewe,Flavio Ferrarotti*

Main category: cs.LO

TL;DR: 本文提出了反射并行算法（RAs）的行为理论，包括定义RAs的公理、抽象机器模型，并证明该模型能捕获所有RAs。


<details>
  <summary>Details</summary>
Motivation: 研究具有自我修改行为的同步并行算法，为其建立理论基础。

Method: 定义反射并行算法的类、抽象机器模型（rASMs），并通过多集理解项保持有界探索。

Result: 证明所有RAs可由反射抽象状态机（rASMs）模型捕获。

Conclusion: rASMs成功扩展了ASMs，为自修改并行算法提供了理论模型。

Abstract: We develop a behavioural theory of reflective parallel algorithms (RAs), i.e.
synchronous parallel algorithms that can modify their own behaviour. The theory
comprises a set of postulates defining the class of RAs, an abstract machine
model, and the proof that all RAs are captured by this machine model. RAs are
sequential-time, parallel algorithms, where every state includes a
representation of the algorithm in that state, thus enabling linguistic
reflection. Bounded exploration is preserved using multiset comprehension terms
as values. The abstract machine model is defined by reflective Abstract State
Machines (rASMs), which extend ASMs using extended states that include an
updatable representation of the main ASM rule to be executed by the machine in
that state.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [22] [Empowering Children to Create AI-Enabled Augmented Reality Experiences](https://arxiv.org/abs/2508.08467)
*Lei Zhang,Shuyao Zhou,Amna Liaqat,Tinney Mak,Brian Berengard,Emily Qian,Andrés Monroy-Hernández*

Main category: cs.HC

TL;DR: Capybara是一个基于AR和AI的视觉编程环境，帮助儿童成为创作者而非消费者，通过文本生成3D角色并编程交互行为。


<details>
  <summary>Details</summary>
Motivation: 尽管AI和AR技术有潜力提升儿童学习体验，但目前它们多让儿童成为消费者而非创作者，Capybara旨在改变这一现状。

Method: Capybara结合文本生成3D模型、自动骨骼绑定、身体追踪和视觉AI识别，支持儿童编程虚拟角色与物理环境的交互。

Result: 用户研究表明，20名儿童能利用Capybara创建个性化的AR体验，无缝连接虚拟与物理世界。

Conclusion: Capybara成功赋能儿童利用AI创作AR内容，证明了其在增强儿童创造力和学习体验方面的潜力。

Abstract: Despite their potential to enhance children's learning experiences,
AI-enabled AR technologies are predominantly used in ways that position
children as consumers rather than creators. We introduce Capybara, an AR-based
and AI-powered visual programming environment that empowers children to create,
customize, and program 3D characters overlaid onto the physical world. Capybara
enables children to create virtual characters and accessories using text-to-3D
generative AI models, and to animate these characters through auto-rigging and
body tracking. In addition, our system employs vision-based AI models to
recognize physical objects, allowing children to program interactive behaviors
between virtual characters and their physical surroundings. We demonstrate the
expressiveness of Capybara through a set of novel AR experiences. We conducted
user studies with 20 children in the United States and Argentina. Our findings
suggest that Capybara can empower children to harness AI in authoring
personalized and engaging AR experiences that seamlessly bridge the virtual and
physical worlds.

</details>


### [23] [Designing for Disclosure in Data Visualizations](https://arxiv.org/abs/2508.08383)
*Krisha Mehta,Gordon Kindlmann,Alex Kale*

Main category: cs.HC

TL;DR: 论文提出了一种名为“披露战术”的分类法，通过分析学术文献中425个可视化技术实例，系统化地描述了可视化如何限制对底层数据的访问。该分类法帮助设计师权衡信息的揭示、扭曲或隐藏，并为可视化研究提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 由于可视化工具和框架对信息披露缺乏明确指导，研究者希望通过系统化分析可视化技术中的披露行为，填补这一空白。

Method: 通过对425个可视化技术实例进行内容分析，构建了一种基于数据表示变化的披露战术分类法。

Result: 提出了一个系统的披露战术分类法，展示了其在设计权衡中的应用价值。

Conclusion: 将信息披露作为可视化研究框架，为工具开发、普及教育、不确定性沟通、个性化设计和伦理设计提供了新思路。

Abstract: Visualizing data often entails data transformations that can reveal and hide
information, operations we dub disclosure tactics. Whether designers hide
information intentionally or as an implicit consequence of other design
choices, tools and frameworks for visualization offer little explicit guidance
on disclosure. To systematically characterize how visualizations can limit
access to an underlying dataset, we contribute a content analysis of 425
examples of visualization techniques sampled from academic papers in the
visualization literature, resulting in a taxonomy of disclosure tactics. Our
taxonomy organizes disclosure tactics based on how they change the data
representation underlying a chart, providing a systematic way to reason about
design trade-offs in terms of what information is revealed, distorted, or
hidden. We demonstrate the benefits of using our taxonomy by showing how it can
guide reasoning in design scenarios where disclosure is a first-order
consideration. Adopting disclosure as a framework for visualization research
offers new perspective on authoring tools, literacy, uncertainty communication,
personalization, and ethical design.

</details>


### [24] [AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and its Privacy Concerns](https://arxiv.org/abs/2508.08502)
*Marta Robledo-Moreno,Ruben Vera-Rodriguez,Ruben Tolosana,Javier Ortega-Garcia,Andres Huergo,Julian Fierrez*

Main category: cs.HC

TL;DR: AirSignatureDB是一个公开数据集，包含108名参与者的空中签名数据，支持真实攻击场景下的系统鲁棒性评估。研究还提出了一种从惯性传感器数据重建签名三维轨迹的方法，挑战了空中签名无痕的假设，并引发了对行为生物识别隐私边界的重新思考。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补空中签名数据集的空白，并探索其在实际应用中的鲁棒性和隐私问题。

Method: 使用108名参与者的真实签名和伪造签名数据，结合传统和深度学习方法进行验证，同时尝试从惯性传感器数据重建三维轨迹。

Result: 结果表明空中签名的三维轨迹可以重建，且数据集支持高效的鲁棒性评估。

Conclusion: 研究揭示了惯性传感器数据潜在的隐私风险，呼吁在设计空中签名系统时重新考虑隐私假设。

Abstract: Behavioral biometrics based on smartphone motion sensors are growing in
popularity for authentication purposes. In this study, AirSignatureDB is
presented: a new publicly accessible dataset of in-air signatures collected
from 108 participants under real-world conditions, using 83 different
smartphone models across four sessions. This dataset includes genuine samples
and skilled forgeries, enabling a comprehensive evaluation of system robustness
against realistic attack scenarios. Traditional and deep learning-based methods
for in-air signature verification are benchmarked, while analyzing the
influence of sensor modality and enrollment strategies. Beyond verification, a
first approach to reconstructing the three-dimensional trajectory of in-air
signatures from inertial sensor data alone is introduced. Using on-line
handwritten signatures as a reference, we demonstrate that the recovery of
accurate trajectories is feasible, challenging the long-held assumption that
in-air gestures are inherently traceless. Although this approach enables
forensic traceability, it also raises critical questions about the privacy
boundaries of behavioral biometrics. Our findings underscore the need for a
reevaluation of the privacy assumptions surrounding inertial sensor data, as
they can reveal user-specific information that had not previously been
considered in the design of in-air signature systems.

</details>


### [25] [Adaptique: Multi-objective and Context-aware Online Adaptation of Selection Techniques in Virtual Reality](https://arxiv.org/abs/2508.08505)
*Chao-Jung Lai,Mauricio Sousa,Tianyu Zhang,Ludwig Sidenmark,Tovi Grossman*

Main category: cs.HC

TL;DR: Adaptique是一个自适应模型，根据用户和环境信息选择最优的虚拟现实选择技术，优于单一技术。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实中目标小、远、遮挡或密集，现有技术复杂且局限，需一种自适应方案。

Method: 结合目标大小、距离、遮挡等上下文信息与速度、准确性、舒适度等目标，动态切换最佳选择技术。

Result: 用户研究表明Adaptique更受欢迎且性能优于单一技术。

Conclusion: Adaptique通过自适应技术选择提高了虚拟现实中的选择效率和用户体验。

Abstract: Selection is a fundamental task that is challenging in virtual reality due to
issues such as distant and small targets, occlusion, and target-dense
environments. Previous research has tackled these challenges through various
selection techniques, but complicates selection and can be seen as tedious
outside of their designed use case. We present Adaptique, an adaptive model
that infers and switches to the most optimal selection technique based on user
and environmental information. Adaptique considers contextual information such
as target size, distance, occlusion, and user posture combined with four
objectives: speed, accuracy, comfort, and familiarity which are based on
fundamental predictive models of human movement for technique selection. This
enables Adaptique to select simple techniques when they are sufficiently
efficient and more advanced techniques when necessary. We show that Adaptique
is more preferred and performant than single techniques in a user study, and
demonstrate Adaptique's versatility in an application.

</details>


### [26] [StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI](https://arxiv.org/abs/2508.08524)
*Jon E. Froehlich,Alexander Fiannaca,Nimer Jaber,Victor Tsara,Shaun Kane*

Main category: cs.HC

TL;DR: StreetViewAI 是首个为盲人设计的无障碍街景工具，结合了上下文感知的多模态AI、无障碍导航控制和对话语音，支持盲人虚拟探索目的地和远程路线规划。


<details>
  <summary>Details</summary>
Motivation: 现有的街景工具如Google Street View对盲人用户不友好，需要开发无障碍解决方案。

Method: 结合AI、无障碍导航和语音交互，通过与混合视觉能力的团队合作迭代设计，并对11名盲人用户进行评估。

Result: StreetViewAI 支持盲人进行目的地调查和远程路线规划，证明了其价值。

Conclusion: 提出了未来工作的关键指南，强调无障碍街景工具的重要性。

Abstract: Interactive streetscape mapping tools such as Google Street View (GSV) and
Meta Mapillary enable users to virtually navigate and experience real-world
environments via immersive 360{\deg} imagery but remain fundamentally
inaccessible to blind users. We introduce StreetViewAI, the first-ever
accessible street view tool, which combines context-aware, multimodal AI,
accessible navigation controls, and conversational speech. With StreetViewAI,
blind users can virtually examine destinations, engage in open-world
exploration, or virtually tour any of the over 220 billion images and 100+
countries where GSV is deployed. We iteratively designed StreetViewAI with a
mixed-visual ability team and performed an evaluation with eleven blind users.
Our findings demonstrate the value of an accessible street view in supporting
POI investigations and remote route planning. We close by enumerating key
guidelines for future work.

</details>


### [27] [Explore, Listen, Inspect: Supporting Multimodal Interaction with 3D Surface and Point Data Visualizations](https://arxiv.org/abs/2508.08554)
*Sanchita S. Kamath,Aziz N. Zeidieh,JooYoung Seo*

Main category: cs.HC

TL;DR: DIXTRAL 是一个为盲人和视力低下用户设计的基于浏览器的多模态交互系统，旨在通过听觉、视觉和键盘导航等反馈方式提升他们对3D数据的访问能力。


<details>
  <summary>Details</summary>
Motivation: 盲人和低视力用户在访问3D可视化数据时面临困难，现有解决方案在非视觉交互方面支持不足，尤其是在浏览器环境中。

Method: 通过与合作设计的BLV研究人员进行持续设计会话，测试原型并收集反馈，重点关注空间导航、听觉反馈和可用性。

Result: 研究发现，通过同步的多模态反馈（听觉、视觉和文本）以及键盘和游戏手柄导航，可以显著提升结构发现和空间定向能力。

Conclusion: DIXTRAL 为盲人和低视力用户提供了更好的3D数据访问方式，并为设计包容性3D可视化提供了实践指南。

Abstract: Blind and low-vision (BLV) users remain largely excluded from
three-dimensional (3D) surface and point data visualizations due to the
reliance on visual interaction. Existing approaches inadequately support
non-visual access, especially in browser-based environments. This study
introduces DIXTRAL, a hosted web-native system, co-designed with BLV
researchers to address these gaps through multimodal interaction. Conducted
with two blind and one sighted researcher, this study took place over sustained
design sessions. Data were gathered through iterative testing of the prototype,
collecting feedback on spatial navigation, sonification, and usability.
Co-design observations demonstrate that synchronized auditory, visual, and
textual feedback, combined with keyboard and gamepad navigation, enhances both
structure discovery and orientation. DIXTRAL aims to improve access to 3D
continuous scalar fields for BLV users and inform best practices for creating
inclusive 3D visualizations.

</details>


### [28] [CoSight: Exploring Viewer Contributions to Online Video Accessibility Through Descriptive Commenting](https://arxiv.org/abs/2508.08582)
*Ruolin wang,Xingyu Liu,Biao Wang,Wayne Zhang,Ziqian Liao,Ziwen Li,Amy Pavel,Xiang 'Anthony' Chen*

Main category: cs.HC

TL;DR: CoSight是一款Chrome扩展，通过轻量化的提示帮助普通观众在观看YouTube视频时生成描述性评论，从而提升视频对盲人和低视力人群的可访问性。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容快速增长，但传统的专业音频描述（AD）成本高且难以规模化。CoSight旨在通过普通观众的参与补充AD的不足。

Method: CoSight利用Fogg的行为模型，在YouTube中提供视觉提示、弹窗建议和评论参考，鼓励观众生成描述性评论。

Result: 48名视力正常的用户测试中，89%的评论包含视觉描述。盲人和低视力观众表示这些评论虽不如专业AD严谨，但提供了有用的视觉背景和情感细节。

Conclusion: CoSight通过普通观众的参与，为视频可访问性提供了一种可扩展的补充方案，虽然效果不及专业AD，但仍具有实用价值。

Abstract: The rapid growth of online video content has outpaced efforts to make visual
information accessible to blind and low vision (BLV) audiences. While
professional Audio Description (AD) remains the gold standard, it is costly and
difficult to scale across the vast volume of online media. In this work, we
explore a complementary approach to broaden participation in video
accessibility: engaging everyday video viewers at their watching and commenting
time. We introduce CoSight, a Chrome extension that augments YouTube with
lightweight, in-situ nudges to support descriptive commenting. Drawing from
Fogg's Behavior Model, CoSight provides visual indicators of accessibility
gaps, pop-up hints for what to describe, reminders to clarify vague comments,
and related captions and comments as references. In an exploratory study with
48 sighted users, CoSight helped integrate accessibility contribution into
natural viewing and commenting practices, resulting in 89% of comments
including grounded visual descriptions. Follow-up interviews with four BLV
viewers and four professional AD writers suggest that while such comments do
not match the rigor of professional AD, they can offer complementary value by
conveying visual context and emotional nuance for understanding the videos.

</details>


### [29] [Imposing AI: Deceptive design patterns against sustainability](https://arxiv.org/abs/2508.08672)
*Anaëlle Beignon,Thomas Thibault,Nolwenn Maudet*

Main category: cs.HC

TL;DR: 论文摘要讨论了生成式AI大规模部署对环境的影响，以及科技公司如何通过界面设计和叙事策略强制用户使用AI。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示科技公司如何通过设计策略强制用户使用AI，以及这种强制行为对环境的潜在负面影响。

Method: 研究方法包括分析科技公司的用户界面设计策略和叙事手段，并将其与已知的欺骗性模式类别进行比较。

Result: 研究结果识别了两种主要的设计策略：在界面中强制替换非AI功能，以及通过叙事手段降低用户对AI的抗拒。

Conclusion: 结论呼吁通过监管手段限制AI功能的强制推广，以减少对环境的负面影响。

Abstract: Generative AI is being massively deployed in digital services, at a scale
that will result in significant environmental harm. We document how tech
companies are transforming established user interfaces to impose AI use and
show how and to what extent these strategies fit within established deceptive
pattern categories. We identify two main design strategies that are implemented
to impose AI use in both personal and professional contexts: imposing AI
features in interfaces at the expense of existing non-AI features and promoting
narratives about AI that make it harder to resist using it. We discuss
opportunities for regulating the imposed adoption of AI features, which would
inevitably lead to negative environmental effects.

</details>


### [30] [Caption: Generating Informative Content Labels for Image Buttons Using Next-Screen Context](https://arxiv.org/abs/2508.08731)
*Mingyuan Zhong,Ajit Mallavarapu,Qing Nie*

Main category: cs.HC

TL;DR: Caption 是一款基于 LLM 的工具，用于为移动设备的视觉交互元素生成内容标签，通过结合交互前后的屏幕信息提高标签准确性。初步结果显示其优于人工标注和 LLM 基线。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备中视觉交互元素内容标签缺失或不准确的问题，提升屏幕阅读器的可用性。

Method: Caption 通过分析交互前后的屏幕信息（即原屏幕和目标屏幕的上下文），生成更具描述性的标签。

Result: 初步结果表明，Caption 生成的标签比人工标注和 LLM 基线更准确。

Conclusion: Caption 有望帮助开发者提供更易访问的建议，并支持屏幕阅读器用户即时修复问题。

Abstract: We present Caption, an LLM-powered content label generation tool for visual
interactive elements on mobile devices. Content labels are essential for screen
readers to provide announcements for image-based elements, but are often
missing or uninformative due to developer neglect. Automated captioning systems
attempt to address this, but are limited to on-screen context, often resulting
in inaccurate or unspecific labels. To generate more accurate and descriptive
labels, Caption collects next-screen context on interactive elements by
navigating to the destination screen that appears after an interaction and
incorporating information from both the origin and destination screens.
Preliminary results show Caption generates more accurate labels than both human
annotators and an LLM baseline. We expect Caption to empower developers by
providing actionable accessibility suggestions and directly support on-demand
repairs by screen reader users.

</details>


### [31] [From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation](https://arxiv.org/abs/2508.08737)
*Jonathan C. Roberts,Peter Butcher,Panagiotis D. Ritsos*

Main category: cs.HC

TL;DR: 论文探讨了基于场景的可视化教学策略，帮助学生学习数据洞察、表示和解释的复杂性。


<details>
  <summary>Details</summary>
Motivation: 教授数据可视化时，学生常难以理解数据管理和展示的挑战，需要通过具体案例来强调关键问题。

Method: 提出了一系列数据驱动的场景，通过真实案例帮助学生批判性评估数据的展示方式及其原因。

Result: 提供了一套可供教师使用的场景示例，并开始从方法中抽象出适用于其他教学的原则。

Conclusion: 该方法符合真实与场景学习原则，通过现实背景促进学生批判性思考数据。

Abstract: This paper explores the use of scenario-based visualisation examples as a
pedagogical strategy for teaching students the complexities of data insight,
representation, and interpretation. Teaching data visualisation often involves
explaining intricate issues related to data management and the challenges of
presenting data meaningfully. In this work, we present a series of data-driven
scenarios. These concise stories depict specific situations, and are created to
help the educators highlight key concerns in data communication, such as chart
selection, temporal versus categorical comparison, visual bias, and narrative
framing. By grounding these examples in real-world contexts, students are
encouraged to critically assess not only what the data shows, but how and why
it is shown that way. The paper presents a collection of example scenarios,
that educators can use for their own lessons; the work fits with a larger
project on looking at critical thinking in the classroom, and developing
appropriate tools. We also start to abstract principles, from our approach, so
that others can develop their own scenarios for their teaching. Our approach
aligns with principles of authentic and scenario-based learning, using
real-world contexts to foster critical engagement with data.

</details>


### [32] [Addressing the Heterogeneity of Visualization in an Introductory PhD Course in the Swedish Context](https://arxiv.org/abs/2508.08958)
*Kostiantyn Kucher,Niklas Rönnberg,Jonas Löwgren*

Main category: cs.HC

TL;DR: 摘要讨论了可视化领域的异构性对博士生理解学科整体结构的挑战，并介绍了一门帮助博士生建立学科整体认知的课程的设计和实施经验。


<details>
  <summary>Details</summary>
Motivation: 可视化领域的异构性使得新博士生难以理解学科的整体结构和建立跨机构的学术关系，需要通过课程设计来帮助他们解决这一问题。

Method: 设计并实施了一门面向博士生的可视化技术与方法论入门课程，包括与其他博士教育活动的互动和实地考察多个研究小组。

Result: 课程设计和实施的经验总结，可供其他研究者和教育工作者在建立或改进类似课程时参考。

Conclusion: 通过课程设计可以帮助博士生更好地理解可视化领域的整体结构，同时为其他教育者提供有价值的经验。

Abstract: Visualization is a heterogeneous field, and this aspect is often reflected by
the organizational structures at higher education institutions that academic
researchers in visualization and related fields including computer graphics,
human-computer interaction, and media design are typically affiliated with. It
may thus be a challenge for new PhD students to grasp the fragmented structure
of their new workplace, form collegial relations across the institution, and to
build a coherent picture of the discipline as a whole. We report an attempt to
address this challenge, in the form of an introductory course on the subject of
Visualization Technology and Methodology for PhD students at the Division for
Media and Information Technology, Link\"oping University, Sweden. We discuss
the course design, including interactions with other doctoral education
activities and field trips to multiple research groups and units within the
division (ranging from scientific visualization and computer graphics to media
design and visual communication). Lessons learned from the course preparation
work as well as the first instance of the course offered during autumn term
2023 can be helpful to researchers and educators aiming to establish or improve
similar doctoral courses.

</details>


### [33] [Envisioning Generative Artificial Intelligence in Cartography and Mapmaking](https://arxiv.org/abs/2508.09028)
*Yuhao Kang,Chenglong Wang*

Main category: cs.HC

TL;DR: 本文探讨了生成式人工智能（GenAI）在制图和地图学中的潜在应用，包括地图制作和使用中的多种设计决策，同时也指出了其局限性和伦理问题。


<details>
  <summary>Details</summary>
Motivation: GenAI凭借其世界知识、创造性和多模态集成能力，为地图学和制图领域提供了新的技术进步机会。

Method: 通过案例研究（如符号化、地图评估和地图阅读）分析GenAI如何优化制图过程。

Result: 尽管GenAI在制图中有巨大潜力，但其不适用于需要深厚制图知识或高精度的任务，且需关注伦理和社会问题。

Conclusion: 本文为GenAI与地图学的交叉研究奠定了基础，并提出了未来研究方向。

Abstract: Generative artificial intelligence (GenAI), including large language models,
diffusion-based image generation models, and GenAI agents, has provided new
opportunities for advancements in mapping and cartography. Due to their
characteristics including world knowledge and generalizability, artistic style
and creativity, and multimodal integration, we envision that GenAI may benefit
a variety of cartographic design decisions, from mapmaking (e.g.,
conceptualization, data preparation, map design, and map evaluation) to map use
(such as map reading, interpretation, and analysis). This paper discusses
several important topics regarding why and how GenAI benefits cartography with
case studies including symbolization, map evaluation, and map reading. Despite
its unprecedented potential, we identify key scenarios where GenAI may not be
suitable, such as tasks that require a deep understanding of cartographic
knowledge or prioritize precision and reliability. We also emphasize the need
to consider ethical and social implications, such as concerns related to
hallucination, reproducibility, bias, copyright, and explainability. This work
lays the foundation for further exploration and provides a roadmap for future
research at the intersection of GenAI and cartography.

</details>


### [34] [Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration](https://arxiv.org/abs/2508.09033)
*Tina Behzad,Nikolos Gurney,Ning Wang,David V. Pynadath*

Main category: cs.HC

TL;DR: 研究探讨AI解释能力对人类-AI团队表现的影响，发现AI对其优缺点的自我认知能提升任务表现和信任度。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过AI的自我认知和解释能力，优化人类与AI的合作效能。

Method: 训练决策树识别模型的错误，并通过用户研究评估不同解释对收入预测任务的影响。

Result: AI对其优缺点的解释能提升任务表现，并改善用户对AI的信任校准。

Conclusion: 信息传递方式对用户信任和AI辅助决策的依赖至关重要。

Abstract: The promise of human-AI teaming lies in humans and AI working together to
achieve performance levels neither could accomplish alone. Effective
communication between AI and humans is crucial for teamwork, enabling users to
efficiently benefit from AI assistance. This paper investigates how AI
communication impacts human-AI team performance. We examine AI explanations
that convey an awareness of its strengths and limitations. To achieve this, we
train a decision tree on the model's mistakes, allowing it to recognize and
explain where and why it might err. Through a user study on an income
prediction task, we assess the impact of varying levels of information and
explanations about AI predictions. Our results show that AI performance
insights enhance task performance, and conveying AI awareness of its strengths
and weaknesses improves trust calibration. These findings highlight the
importance of considering how information delivery influences user trust and
reliance in AI-assisted decision-making.

</details>


### [35] [Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks](https://arxiv.org/abs/2508.09043)
*Yanbing Chen,Jonathan Nelson,Bing Zhou,Ryan Zhenqi Zhou,Shan Ye,Haokun Liu,Zhining Gu,Armita Kar,Hoeyun Kwon,Pengyu Chen,Maoran Sun,Yuhao Kang*

Main category: cs.HC

TL;DR: 该研究分析了全球946名GIScience教师的就业模式，揭示了西方国家和部分学术机构在教师招聘中的主导地位，以及研究主题的演变趋势。


<details>
  <summary>Details</summary>
Motivation: 探索GIScience领域中教师招聘网络对知识传播和学术合作的影响，填补现有研究的空白。

Method: 通过收集志愿者贡献的数据，绘制博士学位授予机构与当前教师任职机构的联系图。

Result: 发现招聘集中在西方国家，研究主题逐渐转向空间数据分析、地图学与地理可视化、地理计算及环境科学等领域。

Conclusion: 研究结果揭示了招聘实践对全球知识传播的影响，有助于促进GIScience和地理学领域的学术公平。

Abstract: Academia is profoundly influenced by faculty hiring networks, which serve as
critical conduits for knowledge dissemination and the formation of
collaborative research initiatives. While extensive research in various
disciplines has revealed the institutional hierarchies inherent in these
networks, their impacts within GIScience remain underexplored. To fill this
gap, this study analyzes the placement patterns of 946 GIScience faculty
worldwide by mapping the connections between PhD-granting institutions and
current faculty affiliations. Our dataset, which is compiled from
volunteer-contributed information, is the most comprehensive collection
available in this field. While there may be some limitations in its
representativeness, its scope and depth provide a unique and valuable
perspective on the global placement patterns of GIScience faculty. Our analysis
reveals several influential programs in placing GIScience faculty, with hiring
concentrated in the western countries. We examined the diversity index to
assess the representation of regions and institutions within the global
GIScience faculty network. We observe significant internal retention at both
the continental and country levels, and a high level of non-self-hired ratio at
the institutional level. Over time, research themes have also evolved, with
growing research clusters emphasis on spatial data analytics, cartography and
geovisualization, geocomputation, and environmental sciences, etc. These
results illuminate the influence of hiring practices on global knowledge
dissemination and contribute to promoting academic equity within GIScience and
Geography.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [36] [Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors](https://arxiv.org/abs/2508.08384)
*Mutian Tong,Rundi Wu,Changxi Zheng*

Main category: cs.GR

TL;DR: 提出一种基于视频的室内空间-时间光照估计方法，利用2D扩散先验优化MLP表示的光场，并通过微调预训练扩散模型实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 解决单图像或视频中空间-时间变化的光照估计问题，因其高度不适定性而具挑战性。

Method: 通过微调预训练图像扩散模型预测多个位置的光照，联合修复多个铬球作为光探针，优化MLP表示的光场。

Result: 在室内光照估计任务中表现优于基线方法，展示了在真实视频中空间-时间一致的光照估计效果。

Conclusion: 方法能有效估计真实视频中的空间-时间变化光照，解决了以往研究中较少展示的难题。

Abstract: Indoor lighting estimation from a single image or video remains a challenge
due to its highly ill-posed nature, especially when the lighting condition of
the scene varies spatially and temporally. We propose a method that estimates
from an input video a continuous light field describing the spatiotemporally
varying lighting of the scene. We leverage 2D diffusion priors for optimizing
such light field represented as a MLP. To enable zero-shot generalization to
in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict
lighting at multiple locations by jointly inpainting multiple chrome balls as
light probes. We evaluate our method on indoor lighting estimation from a
single image or video and show superior performance over compared baselines.
Most importantly, we highlight results on spatiotemporally consistent lighting
estimation from in-the-wild videos, which is rarely demonstrated in previous
works.

</details>


### [37] [Improving Facial Rig Semantics for Tracking and Retargeting](https://arxiv.org/abs/2508.08429)
*Dalton Omens,Allise Thurman,Jihun Yu,Ronald Fedkiw*

Main category: cs.GR

TL;DR: 该论文提出了一种面部表情重定向方法，通过统一的框架和“Simon-Says”校准技术优化动画控制，解决了从真人到虚拟角色的语义匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决面部追踪和重定向中因不同语义框架导致的语义匹配困难，特别是从真人到虚拟角色的场景。

Method: 使用统一的3DMM等框架，结合体积变形和Simon-Says校准技术，再通过精细调优提升动画控制的语义准确性。

Result: 校准后的框架能生成期望的表情动画，但仍需通过精细调优解决动态追踪时控制不良的问题。

Conclusion: 通过统一框架和精细调优方法，论文实现了高效的面部表情重定向，尤其适用于虚拟环境中的应用。

Abstract: In this paper, we consider retargeting a tracked facial performance to either
another person or to a virtual character in a game or virtual reality (VR)
environment. We remove the difficulties associated with identifying and
retargeting the semantics of one rig framework to another by utilizing the same
framework (3DMM, FLAME, MetaHuman, etc.) for both subjects. Although this does
not constrain the choice of framework when retargeting from one person to
another, it does force the tracker to use the game/VR character rig when
retargeting to a game/VR character. We utilize volumetric morphing in order to
fit facial rigs to both performers and targets; in addition, a carefully chosen
set of Simon-Says expressions is used to calibrate each rig to the motion
signatures of the relevant performer or target. Although a uniform set of
Simon-Says expressions can likely be used for all person to person retargeting,
we argue that person to game/VR character retargeting benefits from Simon-Says
expressions that capture the distinct motion signature of the game/VR character
rig. The Simon-Says calibrated rigs tend to produce the desired expressions
when exercising animation controls (as expected). Unfortunately, these
well-calibrated rigs still lead to undesirable controls when tracking a
performance (a well-behaved function can have an arbitrarily ill-conditioned
inverse), even though they typically produce acceptable geometry
reconstructions. Thus, we propose a fine-tuning approach that modifies the rig
used by the tracker in order to promote the output of more semantically
meaningful animation controls, facilitating high efficacy retargeting. In order
to better address real-world scenarios, the fine-tuning relies on implicit
differentiation so that the tracker can be treated as a (potentially
non-differentiable) black box.

</details>


### [38] [Hybrid Long and Short Range Flows for Point Cloud Filtering](https://arxiv.org/abs/2508.08542)
*Dasith de Silva Edirimuni,Xuequan Lu,Ajmal Saeed Mian,Lei Wei,Gang Li,Scott Schaefer,Ying He*

Main category: cs.GR

TL;DR: 提出HybridPF方法，结合短程和长程轨迹进行点云去噪，通过并行模块提升效果和速度。


<details>
  <summary>Details</summary>
Motivation: 当前点云去噪方法存在点聚类或噪声残留问题，需改进。

Method: 设计ShortModule和LongModule并行模块，结合短程分数和长程流，动态图卷积解码器优化推理。

Result: HybridPF实现最佳去噪效果和更快推理速度。

Conclusion: HybridPF通过联合训练和多轨迹结合，显著提升点云去噪性能。

Abstract: Point cloud capture processes are error-prone and introduce noisy artifacts
that necessitate filtering/denoising. Recent filtering methods often suffer
from point clustering or noise retaining issues. In this paper, we propose
Hybrid Point Cloud Filtering ($\textbf{HybridPF}$) that considers both
short-range and long-range filtering trajectories when removing noise. It is
well established that short range scores, given by $\nabla_{x}\log p(x_t)$, may
provide the necessary displacements to move noisy points to the underlying
clean surface. By contrast, long range velocity flows approximate constant
displacements directed from a high noise variant patch $x_0$ towards the
corresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed as
intermediate states between the high noise variant and the clean patches. Our
intuition is that long range information from velocity flow models can guide
the short range scores to align more closely with the clean points. In turn,
score models generally provide a quicker convergence to the clean surface.
Specifically, we devise two parallel modules, the ShortModule and LongModule,
each consisting of an Encoder-Decoder pair to respectively account for
short-range scores and long-range flows. We find that short-range scores,
guided by long-range features, yield filtered point clouds with good point
distributions and convergence near the clean surface. We design a joint loss
function to simultaneously train the ShortModule and LongModule, in an
end-to-end manner. Finally, we identify a key weakness in current displacement
based methods, limitations on the decoder architecture, and propose a dynamic
graph convolutional decoder to improve the inference process. Comprehensive
experiments demonstrate that our HybridPF achieves state-of-the-art results
while enabling faster inference speed.

</details>


### [39] [Revisiting the City Tower Project: Geometric Principles and Structural Morphology in the Works of Louis I. Kahn and Anne Tyng](https://arxiv.org/abs/2508.08561)
*Aysan Mokhtarimousavi,Michael Kleiss,Mostafa Alani,Sida Dai*

Main category: cs.GR

TL;DR: 本文研究了Louis Kahn City Tower项目的计算与形态学，分析了其基于四面体和八面体的结构及其在模块化建筑设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 目的是探索City Tower的几何结构如何用于开发模块化和适应性建筑设计。

Method: 采用分析性形状语法重现原始结构，并基于其形态学生成新结构配置。

Result: 研究发现四面体和八面体可作为创建可扩展和模块化设计的基础几何体。

Conclusion: 该研究为建筑设计提供了新的几何框架，展示了其在未来应用中的潜力。

Abstract: This paper presents a study of computation and morphology of Louis Kahn City
Tower project. The City Tower is an unbuilt design by Louis I. Kahn and Anne
Tyng that integrates form and structure using 3D space triangular geometries.
Although never built, the City Tower geometrical framework anticipated later
developments in design of space-frame structures. Initially envisioned in the
1950s, the City Tower project is a skyscraper structure based on a tetrahedral
and octahedral space frame called Octet-Truss. The aim of this study is to
analyze the geometry of the City Tower structure and how it can be used to
develop modular and adaptable architectural forms. The study is based on an
analytical shape grammar that is used to recreate the original structure, and
later to generate new structural configurations based on the City Tower's
morphology. This study also investigates the potential applications of these
findings in architecture and reveals the possibilities of using tetrahedrons
and octahedrons as fundamental geometries for creating scalable and modular
designs and presents initial findings.

</details>


### [40] [Bio-Generative Design Morphology with Radiolaria: An application of a Nature-Based Generative Shape Grammar for Geometrical Design of Space Frames](https://arxiv.org/abs/2508.08572)
*Michael Kleiss,Seyedehaysan Mokhtarimousavi,Sida Dai,Mostafa Alani*

Main category: cs.GR

TL;DR: 该研究利用放射虫（Radiolaria）的几何结构，开发了一种生成形状语法，用于空间结构的3D设计。


<details>
  <summary>Details</summary>
Motivation: 放射虫因其复杂的几何结构成为建筑设计的灵感来源，研究旨在将其几何特性转化为实用的设计工具。

Method: 分析放射虫的几何结构，提取简化的四面体形态，并结合八面体生成空间配置，开发生成形状语法。

Result: 生成了多种3D空间结构框架设计，并探讨了其在空间框架结构中的潜在应用。

Conclusion: 放射虫的几何结构为空间结构设计提供了高效的生成工具，未来可进一步探索其实际应用。

Abstract: This paper presents a study on using Radiolaria as a basis for generation of
space-based geometry for structural design with shape grammars. Radiolaria has
been a source of inspiration for architectural design with its intricate
structural features and geometric patterns (Lim, 2012). We use the basis of the
Radiolaria geometry to create a generative shape grammar as a computational
system; then use the shape grammar to create spatial configurations for
potential applications in design of 3D space structural frames. This study
begins with the geometric analysis of Radiolaria and the dissection of its
structure and geometry into a simplified morphological source, in this case a
tetrahedral structure. Tetrahedrons are used in combination with octahedrons to
generate spatial configurations to generate 3D spatial structural frames. The
paper presents the Radiolaria spatial analysis, the shape grammar, the
collection of generated designs, and possible applications in space frame
structures.

</details>


### [41] [Exploring Palette based Color Guidance in Diffusion Models](https://arxiv.org/abs/2508.08754)
*Qianru Qiu,Jiafeng Mao,Xueting Wang*

Main category: cs.GR

TL;DR: 提出一种通过整合调色板作为额外引导机制来增强图像色彩方案控制的新方法，显著提升了基于扩散模型的文本到图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在控制整体图像色彩方案时存在不足，尤其是对背景和非显眼对象的颜色控制较差。

Method: 通过结合调色板和提示指令作为双重引导机制，研究不同调色板表示方法在扩散模型中的应用。构建专门的调色板-文本-图像数据集，并进行定量与定性分析。

Result: 调色板引导显著提升了模型生成符合期望色彩方案的图像能力，实现了更精细的色彩控制。

Conclusion: 该方法为文本到图像生成中的色彩控制提供了新的解决方案，具有重要的实际应用价值。

Abstract: With the advent of diffusion models, Text-to-Image (T2I) generation has seen
substantial advancements. Current T2I models allow users to specify object
colors using linguistic color names, and some methods aim to personalize
color-object association through prompt learning. However, existing models
struggle to provide comprehensive control over the color schemes of an entire
image, especially for background elements and less prominent objects not
explicitly mentioned in prompts. This paper proposes a novel approach to
enhance color scheme control by integrating color palettes as a separate
guidance mechanism alongside prompt instructions. We investigate the
effectiveness of palette guidance by exploring various palette representation
methods within a diffusion-based image colorization framework. To facilitate
this exploration, we construct specialized palette-text-image datasets and
conduct extensive quantitative and qualitative analyses. Our results
demonstrate that incorporating palette guidance significantly improves the
model's ability to generate images with desired color schemes, enabling a more
controlled and refined colorization process.

</details>


### [42] [Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination](https://arxiv.org/abs/2508.08826)
*Meng Gai,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种基于学习的屏幕空间间接光照预测方法，结合直接光照实现实时全局光照渲染，通过改进的注意力机制和单色设计解决长距离间接光照建模问题。


<details>
  <summary>Details</summary>
Motivation: 为了实现虚拟环境中用户的高动态范围（HDR）全局光照效果，同时解决神经网络在捕捉长距离间接光照及复杂场景时的挑战。

Method: 设计了一种新颖的网络架构，结合改进的注意力机制和单色设计，通过空间几何特征引导全局信息聚合。

Result: 实验证明该方法优于现有学习技术，能有效处理多色光照和环境光照，捕捉远距离间接光照和表面交互反射（如颜色渗透效果），并泛化到训练集外的场景。

Conclusion: 该方法通过创新的网络设计，成功实现了实时高质量的全局光照渲染，适用于复杂场景和光照条件。

Abstract: Real-time rendering with global illumination is crucial to afford the user
realistic experience in virtual environments. We present a learning-based
estimator to predict diffuse indirect illumination in screen space, which then
is combined with direct illumination to synthesize globally-illuminated high
dynamic range (HDR) results. Our approach tackles the challenges of capturing
long-range/long-distance indirect illumination when employing neural networks
and is generalized to handle complex lighting and scenarios.
  From the neural network thinking of the solver to the rendering equation, we
present a novel network architecture to predict indirect illumination. Our
network is equipped with a modified attention mechanism that aggregates global
information guided by spacial geometry features, as well as a monochromatic
design that encodes each color channel individually.
  We conducted extensive evaluations, and the experimental results demonstrate
our superiority over previous learning-based techniques. Our approach excels at
handling complex lighting such as varying-colored lighting and environment
lighting. It can successfully capture distant indirect illumination and
simulates the interreflections between textured surfaces well (i.e., color
bleeding effects); it can also effectively handle new scenes that are not
present in the training dataset.

</details>


### [43] [DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI](https://arxiv.org/abs/2508.08831)
*Bo-Hsun Chen,Nevindu M. Batagoda,Dan Negrut*

Main category: cs.GR

TL;DR: DiffPhysCam是一种可微分相机模拟器，支持机器人学和具身AI应用，通过梯度优化提升视觉感知管道的性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟相机在控制内参设置、光学伪影捕获和校准参数调优方面不足，限制了从模拟到现实的迁移。DiffPhysCam旨在解决这些问题。

Method: 采用多阶段管道，精细控制相机设置、建模关键光学效果（如散焦模糊），并支持真实数据校准，实现正向渲染和逆向渲染。

Result: DiffPhysCam提升了合成图像任务中的机器人感知性能，并通过逆向渲染创建了真实场景的数字孪生。

Conclusion: DiffPhysCam为解决模拟到现实的视觉感知问题提供了有效工具，展示了在自主地面车辆导航中的潜力。

Abstract: We introduce DiffPhysCam, a differentiable camera simulator designed to
support robotics and embodied AI applications by enabling gradient-based
optimization in visual perception pipelines. Generating synthetic images that
closely mimic those from real cameras is essential for training visual models
and enabling end-to-end visuomotor learning. Moreover, differentiable rendering
allows inverse reconstruction of real-world scenes as digital twins,
facilitating simulation-based robotics training. However, existing virtual
cameras offer limited control over intrinsic settings, poorly capture optical
artifacts, and lack tunable calibration parameters -- hindering sim-to-real
transfer. DiffPhysCam addresses these limitations through a multi-stage
pipeline that provides fine-grained control over camera settings, models key
optical effects such as defocus blur, and supports calibration with real-world
data. It enables both forward rendering for image synthesis and inverse
rendering for 3D scene reconstruction, including mesh and material texture
optimization. We show that DiffPhysCam enhances robotic perception performance
in synthetic image tasks. As an illustrative example, we create a digital twin
of a real-world scene using inverse rendering, simulate it in a multi-physics
environment, and demonstrate navigation of an autonomous ground vehicle using
images generated by DiffPhysCam.

</details>


### [44] [How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive Reasoning for Embodied Head Rotation](https://arxiv.org/abs/2508.08930)
*Juyeong Hwang,Seong-Eun Hon,JaeYoung Seon,Hyeongyeop Kang*

Main category: cs.GR

TL;DR: SCORE是一个符号认知推理框架，用于生成具有上下文感知的头部动作，不需要任务特定训练或人工调整启发式方法。


<details>
  <summary>Details</summary>
Motivation: 头部旋转是虚拟代理逼真行为的关键，但现有算法通常忽略认知动机，导致行为缺乏真实性。

Method: SCORE结合视觉语言模型（VLM）和大型语言模型（LLM），通过离线推理生成头部动作，并用轻量级FastVLM在线验证。

Result: SCORE能够预测头部动作的动机（如兴趣、安全等），并适用于新场景和多代理环境。

Conclusion: SCORE显著提升了虚拟代理行为的真实性和泛化能力。

Abstract: Natural head rotation is critical for believable embodied virtual agents, yet
this micro-level behavior remains largely underexplored. While head-rotation
prediction algorithms could, in principle, reproduce this behavior, they
typically focus on visually salient stimuli and overlook the cognitive motives
that guide head rotation. This yields agents that look at conspicuous objects
while overlooking obstacles or task-relevant cues, diminishing realism in a
virtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning
framework for Embodied Head Rotation, a data-agnostic framework that produces
context-aware head movements without task-specific training or hand-tuned
heuristics. A controlled VR study (N=20) identifies five motivational drivers
of human head movements: Interest, Information Seeking, Safety, Social Schema,
and Habit. SCORE encodes these drivers as symbolic predicates, perceives the
scene with a Vision-Language Model (VLM), and plans head poses with a Large
Language Model (LLM). The framework employs a hybrid workflow: the VLM-LLM
reasoning is executed offline, after which a lightweight FastVLM performs
online validation to suppress hallucinations while maintaining responsiveness
to scene dynamics. The result is an agent that predicts not only where to look
but also why, generalizing to unseen scenes and multi-agent crowds while
retaining behavioral plausibility.

</details>


### [45] [VertexRegen: Mesh Generation with Continuous Level of Detail](https://arxiv.org/abs/2508.09062)
*Xiang Zhang,Yawar Siddiqui,Armen Avetisyan,Chris Xie,Jakob Engel,Henry Howard-Jenkins*

Main category: cs.GR

TL;DR: VertexRegen是一种新颖的网格生成框架，支持连续细节级别的网格生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成网格时采用部分到整体的方式，导致中间步骤的网格不完整。VertexRegen通过逆转边折叠过程（顶点分割）实现了连续细节生成。

Method: 框架受渐进网格启发，通过学习生成模型实现顶点分割过程。

Result: 实验表明，VertexRegen生成的网格质量与最先进方法相当，并能在任意步骤停止以生成具有不同细节级别的有效网格。

Conclusion: VertexRegen提供了灵活且连续的网格生成能力，优于现有方法。

Abstract: We introduce VertexRegen, a novel mesh generation framework that enables
generation at a continuous level of detail. Existing autoregressive methods
generate meshes in a partial-to-complete manner and thus intermediate steps of
generation represent incomplete structures. VertexRegen takes inspiration from
progressive meshes and reformulates the process as the reversal of edge
collapse, i.e. vertex split, learned through a generative model. Experimental
results demonstrate that VertexRegen produces meshes of comparable quality to
state-of-the-art methods while uniquely offering anytime generation with the
flexibility to halt at any step to yield valid meshes with varying levels of
detail.

</details>


### [46] [Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer](https://arxiv.org/abs/2508.09131)
*Zixin Yin,Xili Dai,Ling-Hao Chen,Deyu Zhou,Jianan Wang,Duomin Wang,Gang Yu,Lionel M. Ni,Heung-Yeung Shum*

Main category: cs.GR

TL;DR: 本文提出了一种无需训练的文本引导颜色编辑方法ColorCtrl，利用多模态扩散变换器的注意力机制实现精确颜色控制和一致性编辑。


<details>
  <summary>Details</summary>
Motivation: 解决图像和视频中文本引导颜色编辑的难题，要求细粒度操控颜色属性并保持物理一致性。

Method: 通过目标性地操控注意力图和值标记分离结构和颜色，实现准确一致的颜色编辑。

Result: 实验表明ColorCtrl在编辑质量和一致性上优于现有方法，且在视频模型中表现更优。

Conclusion: ColorCtrl展现了广泛的适用性和卓越性能，在多种模型和场景中均表现出色。

Abstract: Text-guided color editing in images and videos is a fundamental yet unsolved
problem, requiring fine-grained manipulation of color attributes, including
albedo, light source color, and ambient lighting, while preserving physical
consistency in geometry, material properties, and light-matter interactions.
Existing training-free methods offer broad applicability across editing tasks
but struggle with precise color control and often introduce visual
inconsistency in both edited and non-edited regions. In this work, we present
ColorCtrl, a training-free color editing method that leverages the attention
mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By
disentangling structure and color through targeted manipulation of attention
maps and value tokens, our method enables accurate and consistent color
editing, along with word-level control of attribute intensity. Our method
modifies only the intended regions specified by the prompt, leaving unrelated
areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate
that ColorCtrl outperforms existing training-free approaches and achieves
state-of-the-art performances in both edit quality and consistency.
Furthermore, our method surpasses strong commercial models such as FLUX.1
Kontext Max and GPT-4o Image Generation in terms of consistency. When extended
to video models like CogVideoX, our approach exhibits greater advantages,
particularly in maintaining temporal coherence and editing stability. Finally,
our method also generalizes to instruction-based editing diffusion models such
as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [47] [Millisecond-scale Volatile Memory in HZO Ferroelectric Capacitors for Bio-inspired Temporal Computing](https://arxiv.org/abs/2508.08973)
*Luca Fehlings,Thomas Mikolajick,Beatriz Noheda,Erika Covi*

Main category: cs.ET

TL;DR: 本文研究了HfO2基铁电器件的去极化效应，提出了通过界面配置实现可变时间常数的硬件设计，为时间计算和脑启发硬件提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 研究HfO2基铁电器件的物理机制，以优化非易失性存储器和实时信号处理应用中的性能，特别是在实现可变时间常数方面。

Method: 通过电学表征和界面缺陷的配置，分析了铁电薄膜极性稳定性和保留时间的关系，并探讨了氧空位对内部偏置场的影响。

Result: 展示了基于HfO2的铁电电容器通过特定界面配置实现毫秒级保留时间的非易失性存储，以及时间常数可调的特性。

Conclusion: HfO2基铁电器件通过界面设计和内部偏置场的调控，为时间计算和脑启发硬件提供了可靠的解决方案。

Abstract: With the broad recent research on ferroelectric hafnium oxide for
non-volatile memory technology, depolarization effects in HfO2-based
ferroelectric devices gained a lot of interest. Understanding the physical
mechanisms regulating the retention of these devices provides an excellent
opportunity for device optimization both towards non-volatile memory
applications and towards real-time signal processing applications in which
controlled time constants are of paramount importance. Indeed, we argue that
ferroelectric devices, particularly HfO2-based, are an elegant solution to
realize possibly arbitrary time constants in a single scaled memory device,
which paves the way for temporal and brain-inspired computing in hardware. Here
we present a ferroelectric capacitor stack realizing volatile memory due to its
unique interface configuration. We provide electrical characterization of the
device to motivate its use for realizing time constants in hardware, followed
by an investigation of the electronic mechanisms and their possible relation to
the observed retention times to facilitate further modeling of the retention
process in HfO2-based ferroelectric capacitors. In the presented device,
internal electric fields stabilize one polarization of the ferroelectric film,
opening the possibility for unipolar operation with millisecond retention for
the unstable polarization state. We show a dependence of the retention on both
the polarization as well as the electrical stimuli, allowing us to exploit a
range of time scales in a single device. Further, the intentionally defective
interface in the presented material stack allows an insight into the interplay
between retention loss in HfO2-based ferroelectric devices and the internal
bias field, which we relate to the interface composition and the role of oxygen
vacancies as a possible source of the internal bias fields.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Profiling Concurrent Vision Inference Workloads on NVIDIA Jetson -- Extended](https://arxiv.org/abs/2508.08430)
*Abhinaba Chakraborty,Wouter Tavernier,Akis Kourtis,Mario Pickavet,Andreas Oikonomakis,Didier Colle*

Main category: cs.DC

TL;DR: 论文分析了边缘计算中GPU资源利用的瓶颈，发现关键低层资源利用率低，且CPU事件常成为限制性能的因素。


<details>
  <summary>Details</summary>
Motivation: 为应对实时数据处理需求，低功耗AI加速器（如GPU）被广泛用于边缘计算，但其资源利用率不足，尤其是GPU资源共享机制不明。

Method: 通过多层次指标分析（如GPU利用率、内存使用等）结合多种性能分析工具，研究NVIDIA Jetson设备在并发视觉推理任务中的资源行为。

Result: GPU利用率可达100%，但SM和Tensor核心利用率仅15%-30%，CPU事件（如线程调度）常成为瓶颈。

Conclusion: 提出了针对视觉推理任务的优化建议，揭示了边缘设备中GPU资源利用的关键限制因素。

Abstract: The proliferation of IoT devices and advancements in network technologies
have intensified the demand for real-time data processing at the network edge.
To address these demands, low-power AI accelerators, particularly GPUs, are
increasingly deployed for inference tasks, enabling efficient computation while
mitigating cloud-based systems' latency and bandwidth limitations. Despite
their growing deployment, GPUs remain underutilised even in computationally
intensive workloads. This underutilisation stems from the limited understanding
of GPU resource sharing, particularly in edge computing scenarios. In this
work, we conduct a detailed analysis of both high- and low-level metrics,
including GPU utilisation, memory usage, streaming multiprocessor (SM)
utilisation, and tensor core usage, to identify bottlenecks and guide
hardware-aware optimisations. By integrating traces from multiple profiling
tools, we provide a comprehensive view of resource behaviour on NVIDIA Jetson
edge devices under concurrent vision inference workloads. Our findings indicate
that while GPU utilisation can reach $100\%$ under specific optimisations,
critical low-level resources, such as SMs and tensor cores, often operate only
at $15\%$ to $30\%$ utilisation. Moreover, we observe that certain CPU-side
events, such as thread scheduling, context switching, etc., frequently emerge
as bottlenecks, further constraining overall GPU performance. We provide
several key observations for users of vision inference workloads on NVIDIA edge
devices.

</details>


### [49] [Benchmarking Federated Learning for Throughput Prediction in 5G Live Streaming Applications](https://arxiv.org/abs/2508.08479)
*Yuvraj Dutta,Soumyajit Chatterjee,Sandip Chakraborty,Basabdatta Palit*

Main category: cs.DC

TL;DR: 本文首次对联邦学习（FL）策略在5G边缘场景中的网络吞吐量预测进行了全面基准测试，评估了三种聚合算法和四种时序架构，发现FedBN在非IID数据下表现稳健，LSTM和Transformer在R2分数上优于CNN，且LSTM在准确性和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有吞吐量预测方法依赖于集中式训练和均匀数据，难以适应5G和6G网络中异构移动环境的非IID数据分布，因此需要探索更灵活的FL策略。

Method: 评估了FedAvg、FedProx和FedBN三种FL聚合算法，以及LSTM、CNN、CNN+LSTM和Transformer四种时序架构，使用五个真实数据集，分析客户端异质性、分组大小和历史窗口长度对性能的影响。

Result: FedBN在非IID条件下表现最佳；LSTM和Transformer在R2分数上比CNN高出80%；Transformer收敛更快但对历史窗口要求更高；LSTM在准确性、收敛速度和计算开销上表现均衡。应用于实时流媒体后，基于FedBN的模型在QoE上提升了11%以上。

Conclusion: 研究为下一代无线网络中可扩展、隐私保护和边缘感知的吞吐量预测系统提供了实用指导，推荐FedBN与LSTM或Transformer结合使用。

Abstract: Accurate and adaptive network throughput prediction is essential for
latency-sensitive and bandwidth-intensive applications in 5G and emerging 6G
networks. However, most existing methods rely on centralized training with
uniformly collected data, limiting their applicability in heterogeneous mobile
environments with non-IID data distributions. This paper presents the first
comprehensive benchmarking of federated learning (FL) strategies for throughput
prediction in realistic 5G edge scenarios. We evaluate three aggregation
algorithms - FedAvg, FedProx, and FedBN - across four time-series
architectures: LSTM, CNN, CNN+LSTM, and Transformer, using five diverse
real-world datasets. We systematically analyze the effects of client
heterogeneity, cohort size, and history window length on prediction
performance. Our results reveal key trade-offs among model complexities,
convergence rates, and generalization. It is found that FedBN consistently
delivers robust performance under non-IID conditions. On the other hand, LSTM
and Transformer models outperform CNN-based baselines by up to 80% in R2
scores. Moreover, although Transformers converge in half the rounds of LSTM,
they require longer history windows to achieve a high R2, indicating higher
context dependence. LSTM is, therefore, found to achieve a favorable balance
between accuracy, rounds, and temporal footprint. To validate the end-to-end
applicability of the framework, we have integrated our FL-based predictors into
a live adaptive streaming pipeline. It is seen that FedBN-based LSTM and
Transformer models improve mean QoE scores by 11.7% and 11.4%, respectively,
over FedAvg, while also reducing the variance. These findings offer actionable
insights for building scalable, privacy-preserving, and edge-aware throughput
prediction systems in next-generation wireless networks.

</details>


### [50] [A Reinforcement Learning-Driven Task Scheduling Algorithm for Multi-Tenant Distributed Systems](https://arxiv.org/abs/2508.08525)
*Xiaopei Zhang,Xingang Wang,Xin Wang*

Main category: cs.DC

TL;DR: 提出了基于强化学习的自适应调度方法，用于解决多租户分布式系统中的任务调度问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对多租户分布式系统中的动态资源变化、异构租户需求和公平性保证等核心挑战。

Method: 将调度过程建模为马尔可夫决策过程，使用PPO算法设计调度策略学习框架，通过多目标奖励机制优化任务延迟、资源利用和公平性。

Result: 实验验证了该方法在多个指标上优于现有方法，展现了强稳定性和泛化能力。

Conclusion: 该框架为复杂条件下的调度效率和资源管理提供了实用价值。

Abstract: This paper addresses key challenges in task scheduling for multi-tenant
distributed systems, including dynamic resource variation, heterogeneous tenant
demands, and fairness assurance. An adaptive scheduling method based on
reinforcement learning is proposed. By modeling the scheduling process as a
Markov decision process, the study defines the state space, action space, and
reward function. A scheduling policy learning framework is designed using
Proximal Policy Optimization (PPO) as the core algorithm. This enables dynamic
perception of complex system states and real-time decision-making. Under a
multi-objective reward mechanism, the scheduler jointly optimizes task latency,
resource utilization, and tenant fairness. The coordination between the policy
network and the value network continuously refines the scheduling strategy.
This enhances overall system performance. To validate the effectiveness of the
proposed method, a series of experiments were conducted in multi-scenario
environments built using a real-world public dataset. The experiments evaluated
task latency control, resource efficiency, policy stability, and fairness. The
results show that the proposed method outperforms existing scheduling
approaches across multiple evaluation metrics. It demonstrates strong stability
and generalization ability. The proposed scheduling framework provides
practical and engineering value in policy design, dynamic resource modeling,
and multi-tenant service assurance. It effectively improves scheduling
efficiency and resource management in distributed systems under complex
conditions.

</details>


### [51] [P/D-Device: Disaggregated Large Language Model between Cloud and Devices](https://arxiv.org/abs/2508.09035)
*Yibo Jin,Yixu Xu,Yue Chen,Chengbin Wang,Tao Wang,Jiaqi Huang,Rongfei Zhang,Yiming Dong,Yuting Yan,Ke Cheng,Yingjie Zhu,Shulan Wang,Qianqian Tang,Shuaishuai Meng,Guanxin Cheng,Ze Wang,Shuyan Miao,Ketao Wang,Wen Liu,Yifan Yang,Tong Zhang,Anran Wang,Chengzhou Lu,Tiantian Dong,Yongsheng Zhang,Zhe Wang,Hefei Guo,Hongjie Liu,Wei Lu,Zhengyong Zhang*

Main category: cs.DC

TL;DR: 论文提出了一种将大语言模型分割到云端和设备端的方法，以降低首词延迟（TTFT）并提高云端吞吐量。


<details>
  <summary>Details</summary>
Motivation: 由于解码阶段生成过多令牌占用云端资源，以及设备端资源有限导致预填充阶段延迟增加，论文旨在解决这些资源瓶颈问题。

Method: 通过将大语言模型分割到云端和设备端，云端仅协助设备端的预填充阶段；设备端在收到首词后立即响应，云端后续令牌通过速度控制器平滑输出。

Result: 实验结果表明，TTFT降低至少60%，最大TPOT约为几十毫秒，云端吞吐量提升高达15倍。

Conclusion: 提出的P/D-Device方案在减少延迟和提高吞吐量方面优于其他方案，并通过算法优化设置进一步提升了性能。

Abstract: Serving disaggregated large language models has been widely adopted in
industrial practice for enhanced performance. However, too many tokens
generated in decoding phase, i.e., occupying the resources for a long time,
essentially hamper the cloud from achieving a higher throughput. Meanwhile, due
to limited on-device resources, the time to first token (TTFT), i.e., the
latency of prefill phase, increases dramatically with the growth on prompt
length. In order to concur with such a bottleneck on resources, i.e., long
occupation in cloud and limited on-device computing capacity, we propose to
separate large language model between cloud and devices. That is, the cloud
helps a portion of the content for each device, only in its prefill phase.
Specifically, after receiving the first token from the cloud, decoupling with
its own prefill, the device responds to the user immediately for a lower TTFT.
Then, the following tokens from cloud are presented via a speed controller for
smoothed TPOT (the time per output token), until the device catches up with the
progress. On-device prefill is then amortized using received tokens while the
resource usage in cloud is controlled. Moreover, during cloud prefill, the
prompt can be refined, using those intermediate data already generated, to
further speed up on-device inference. We implement such a scheme P/D-Device,
and confirm its superiority over other alternatives. We further propose an
algorithm to decide the best settings. Real-trace experiments show that TTFT
decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud
throughput increases by up to 15x.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM Inference](https://arxiv.org/abs/2508.08256)
*Dongwei Wang,Zijie Liu,Song Wang,Yuxin Ren,Jianing Deng,Jingtong Hu,Tianlong Chen,Huanrui Yang*

Main category: cs.DB

TL;DR: 本文提出了一种细粒度且高效的KV缓存检索方法Fier，通过1位量化键估计每个令牌的重要性，显著减少了长上下文LLM推理的延迟。


<details>
  <summary>Details</summary>
Motivation: KV缓存的读取延迟随着上下文长度的增加而显著上升，影响了长上下文LLM推理的效率。现有方法由于令牌稀疏分布，导致页面级检索不准确。

Method: 提出Fier方法，使用1位量化键快速评估令牌重要性，实现细粒度且高效的KV缓存检索。

Result: 实验表明，Fier仅需11%的缓存预算即可达到全KV性能，解码延迟降低1.2至1.5倍。

Conclusion: Fier在长上下文任务中表现出色，显著提升了KV缓存的检索效率和推理速度。

Abstract: The Key-Value (KV) cache reading latency increases significantly with context
lengths, hindering the efficiency of long-context LLM inference. To address
this, previous works propose retaining a small fraction of KV cache based on
token importance. For example, KV eviction uses static heuristics to retain
tokens, while KV retrieval dynamically selects query-relevant tokens for more
adaptive cache management. However, we observe that important tokens are often
sparsely distributed across the long context. This sparsity makes existing
page-level KV retrieval inaccurate, as each page may include irrelevant tokens
and miss critical ones. In this work, we propose Fier, a
\underline{Fi}ne-Grained and \underline{E}fficient KV cache
\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the
importance of each token, resulting in efficient and precise retrieval.
Experiments show that Fier matches full KV performance using only 11\% of the
cache budget across various long-context tasks, reducing decoding latency by
1.2$\times$ to 1.5$\times$.

</details>


### [53] [Synthesize, Retrieve, and Propagate: A Unified Predictive Modeling Framework for Relational Databases](https://arxiv.org/abs/2508.08327)
*Ning Li,Kounianhua Du,Han Zhang,Quan Gan,Minjie Wang,David Wipf,Weinan Zhang*

Main category: cs.DB

TL;DR: SRP提出了一种统一的关系数据库预测建模框架，通过捕获单依赖和复合依赖，增强表格数据预测的能力。


<details>
  <summary>Details</summary>
Motivation: 关系数据库（RDBs）的固有结构限制了其在深度学习中的应用潜力。现有研究主要利用主键-外键关系处理单依赖，而忽略了表间隐含的复合关系。

Method: SRP框架通过单依赖合成特征、检索相关信息捕获复合依赖，并在构建的图中传播消息以学习相邻模式。

Result: 在五个真实数据集上的实验表明SRP的有效性，并展示了其在工业场景中的潜在应用价值。

Conclusion: SRP通过新的检索机制充分探索了关系数据库中的单依赖和复合依赖，为表格数据预测提供了更广泛的感受野。

Abstract: Relational databases (RDBs) have become the industry standard for storing
massive and heterogeneous data. However, despite the widespread use of RDBs
across various fields, the inherent structure of relational databases hinders
their ability to benefit from flourishing deep learning methods. Previous
research has primarily focused on exploiting the unary dependency among
multiple tables in a relational database using the primary key - foreign key
relationships, either joining multiple tables into a single table or
constructing a graph among them, which leaves the implicit composite relations
among different tables and a substantial potential of improvement for
predictive modeling unexplored. In this paper, we propose SRP, a unified
predictive modeling framework that synthesizes features using the unary
dependency, retrieves related information to capture the composite dependency,
and propagates messages across a constructed graph to learn adjacent patterns
for prediction on relation databases. By introducing a new retrieval mechanism
into RDB, SRP is designed to fully capture both the unary and the composite
dependencies within a relational database, thereby enhancing the receptive
field of tabular data prediction. In addition, we conduct a comprehensive
analysis on the components of SRP, offering a nuanced understanding of model
behaviors and practical guidelines for future applications. Extensive
experiments on five real-world datasets demonstrate the effectiveness of SRP
and its potential applicability in industrial scenarios. The code is released
at https://github.com/NingLi670/SRP.

</details>


### [54] [Vector-Centric Machine Learning Systems: A Cross-Stack Approach](https://arxiv.org/abs/2508.08469)
*Wenqi Jiang*

Main category: cs.DB

TL;DR: 该论文探讨了机器学习系统效率提升的跨栈方法，涵盖算法、系统和硬件层面的优化，重点关注了RAG、向量搜索和推荐系统的效率改进。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统复杂度的增加和摩尔定律的终结，传统方法难以提升系统效率，需要跨栈优化来应对硬件和算法的快速演进。

Method: 采用跨栈方法，分别从算法（如PipeRAG）、系统（如RAGO）和硬件（如Chameleon）层面优化RAG效率；同时对向量搜索（如FANNS、Falcon）和推荐系统（如MicroRec、FleetRec）进行优化。

Result: 提出了多种针对不同层面的优化方案，显著提升了RAG、向量搜索和推荐系统的服务效率。

Conclusion: 跨栈方法能够有效解决现代ML系统效率问题，为异构硬件环境下的优化提供了新思路。

Abstract: Today, two major trends are shaping the evolution of ML systems. First,
modern AI systems are becoming increasingly complex, often integrating
components beyond the model itself. A notable example is Retrieval-Augmented
Generation (RAG), which incorporates not only multiple models but also vector
databases, leading to heterogeneity in both system components and underlying
hardware. Second, with the end of Moore's Law, achieving high system efficiency
is no longer feasible without accounting for the rapid evolution of the
hardware landscape.
  Building on the observations above, this thesis adopts a cross-stack approach
to improving ML system efficiency, presenting solutions that span algorithms,
systems, and hardware. First, it introduces several pioneering works about RAG
serving efficiency across the computing stack. PipeRAG focuses on
algorithm-level improvements, RAGO introduces system-level optimizations, and
Chameleon explores heterogeneous accelerator systems for RAG. Second, this
thesis investigates algorithm-hardware co-design for vector search.
Specifically, FANNS and Falcon optimize quantization-based and graph-based
vector search, the two most popular paradigms of retrieval algorithms. Third,
this thesis addresses the serving efficiency of recommender systems, another
example of vector-centric ML systems, where the memory-intensive lookup
operations on embedding vector tables often represent a major performance
bottleneck. MicroRec and FleetRec propose solutions at the hardware and system
levels, respectively, optimizing both data movement and computation to enhance
the efficiency of large-scale recommender models.

</details>


### [55] [Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.08744)
*Zhonggen Li,Xiangyu Ke,Yifan Zhu,Bocheng Yu,Baihua Zheng,Yunjun Gao*

Main category: cs.DB

TL;DR: Tagore是一个基于GPU加速的图索引库，通过高效算法和并行处理大幅提升高维近似最近邻搜索的速度。


<details>
  <summary>Details</summary>
Motivation: 传统图索引方法在高维向量空间中的构建开销大，且随着数据量增长和动态调整需求增加，效率问题日益突出。

Method: 提出GNN-Descent算法优化k-NN图初始化，设计通用计算过程CFS支持多种剪枝策略，并开发异步GPU-CPU-磁盘索引框架处理大规模数据。

Result: 在7个真实数据集上，Tagore实现了1.32x-112.79x的加速，同时保持索引质量。

Conclusion: Tagore通过GPU加速和并行化设计，显著降低了图索引的开销，适用于大规模动态数据场景。

Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces
has a wide range of real-world applications. Numerous methods have been
proposed to handle ANNS efficiently, while graph-based indexes have gained
prominence due to their high accuracy and efficiency. However, the indexing
overhead of graph-based indexes remains substantial. With exponential growth in
data volume and increasing demands for dynamic index adjustments, this overhead
continues to escalate, posing a critical challenge. In this paper, we introduce
Tagore, a fast library accelerated by GPUs for graph indexing, which has
powerful capabilities of constructing refinement-based graph indexes such as
NSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for
efficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up
the similarity comparison by a two-phase descent procedure and enables highly
parallelized neighbor updates. Next, aiming to support various k-NN graph
pruning strategies, we formulate a universal computing procedure termed CFS and
devise two generalized GPU kernels for parallel processing complex dependencies
in neighbor relationships. For large-scale datasets exceeding GPU memory
capacity, we propose an asynchronous GPU-CPU-disk indexing framework with a
cluster-aware caching mechanism to minimize the I/O pressure on the disk.
Extensive experiments on 7 real-world datasets exhibit that Tagore achieves
1.32x-112.79x speedup while maintaining the index quality.

</details>


### [56] [A Framework for FAIR and CLEAR Ecological Data and Knowledge: Semantic Units for Synthesis and Causal Modelling](https://arxiv.org/abs/2508.08959)
*Lars Vogt,Birgitta König-Ries,Tim Alamenciak,Joshua I. Brian,Carlos Alberto Arnillas,Lotte Korell,Robert Frühstückl,Tina Heger*

Main category: cs.DB

TL;DR: 提出了一种名为“语义单元框架”的新方法，用于解决生态数据和知识整合中的互操作性和因果理解问题，符合FAIR和CLEAR原则。


<details>
  <summary>Details</summary>
Motivation: 生态研究需要整合异构数据和知识，但由于数据类型、术语和文档的差异，互操作性和因果理解变得困难。

Method: 通过模块化、逻辑感知的语义单元（声明单元和复合单元）建模数据和知识，使用RDF、OWL和知识图实现，并序列化为FAIR数字对象。

Result: 该框架能够构建生态因果网络，支持因果推理、混杂因素检测和效应识别，并与其他模型对齐。

Conclusion: 语义单元框架为生态知识合成、跨领域整合和AI研究提供了基础。

Abstract: Ecological research increasingly relies on integrating heterogeneous datasets
and knowledge to explain and predict complex phenomena. Yet, differences in
data types, terminology, and documentation often hinder interoperability,
reuse, and causal understanding. We present the Semantic Units Framework, a
novel, domain-agnostic semantic modelling approach applied here to ecological
data and knowledge in compliance with the FAIR (Findable, Accessible,
Interoperable, Reusable) and CLEAR (Cognitively interoperable, semantically
Linked, contextually Explorable, easily Accessible, human-Readable and
-interpretable) Principles. The framework models data and knowledge as modular,
logic-aware semantic units: single propositions (statement units) or coherent
groups of propositions (compound units). Statement units can model
measurements, observations, or universal relationships, including causal ones,
and link to methods and evidence. Compound units group related statement units
into reusable, semantically coherent knowledge objects. Implemented using RDF,
OWL, and knowledge graphs, semantic units can be serialized as FAIR Digital
Objects with persistent identifiers, provenance, and semantic interoperability.
We show how universal statement units build ecological causal networks, which
can be composed into causal maps and perspective-specific subnetworks. These
support causal reasoning, confounder detection (back-door), effect
identification with unobserved confounders (front-door), application of
do-calculus, and alignment with Bayesian networks, structural equation models,
and structural causal models. By linking fine-grained empirical data to
high-level causal reasoning, the Semantic Units Framework provides a foundation
for ecological knowledge synthesis, evidence annotation, cross-domain
integration, reproducible workflows, and AI-ready ecological research.

</details>


### [57] [E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency](https://arxiv.org/abs/2508.09023)
*Dongjie Xu,Yue Cui,Weijie Shi,Qingzhi Ma,Hanghui Guo,Jiaming Li,Yao Zhao,Ruiyuan Zhang,Shimin Di,Jia Zhu,Kai Zheng,Jiajie Xu*

Main category: cs.DB

TL;DR: E3-Rewrite 是一个基于LLM的SQL查询重写框架，通过结合执行计划和强化学习，生成高效且语义等价的查询重写，显著提升了重写成功率和查询执行效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的SQL查询重写方法在面对复杂查询或新查询模式时表现不佳，且难以覆盖所有高效重写策略。LLM虽然能捕捉复杂策略，但直接应用易生成次优或不正确的重写。

Method: E3-Rewrite 结合上下文构造模块（利用执行计划和示例引导重写）和强化学习框架（通过奖励函数和多阶段课程学习优化重写质量）。

Result: 实验显示，E3-Rewrite 平均减少25.6%的查询执行时间，并提升24.4%的成功重写率，尤其擅长处理传统方法无法应对的复杂查询。

Conclusion: E3-Rewrite 成功解决了传统方法的局限性，通过LLM与执行感知的结合，实现了高效、等价且可执行的SQL查询重写。

Abstract: SQL query rewriting aims to reformulate a query into a more efficient form
while preserving equivalence. Most existing methods rely on predefined rewrite
rules. However, such rule-based approaches face fundamental limitations: (1)
fixed rule sets generalize poorly to novel query patterns and struggle with
complex queries; (2) a wide range of effective rewriting strategies cannot be
fully captured by declarative rules. To overcome these issues, we propose using
large language models (LLMs) to generate rewrites. LLMs can capture complex
strategies, such as evaluation reordering and CTE rewriting. Despite this
potential, directly applying LLMs often results in suboptimal or non-equivalent
rewrites due to a lack of execution awareness and semantic grounding. To
address these challenges, We present E3-Rewrite, an LLM-based SQL rewriting
framework that produces executable, equivalent, and efficient queries. It
integrates two core components: a context construction module and a
reinforcement learning framework. First, the context module leverages execution
plans and retrieved demonstrations to build bottleneck-aware prompts that guide
inference-time rewriting. Second, we design a reward function targeting
executability, equivalence, and efficiency, evaluated via syntax checks,
equivalence verification, and cost estimation. Third, to ensure stable
multi-objective learning, we adopt a staged curriculum that first emphasizes
executability and equivalence, then gradually incorporates efficiency.
Extensive experiments show that E3-Rewrite achieves up to a 25.6\% reduction in
query execution time compared to state-of-the-art methods across multiple SQL
benchmarks. Moreover, it delivers up to 24.4\% more successful rewrites,
expanding coverage to complex queries that previous systems failed to handle.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [58] [XDMA: A Distributed, Extensible DMA Architecture for Layout-Flexible Data Movements in Heterogeneous Multi-Accelerator SoCs](https://arxiv.org/abs/2508.08396)
*Fanchen Kong,Yunhao Deng,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: XDMA是一个分布式、可扩展的DMA架构，通过硬件地址生成器和灵活的数据传输插件，解决了异构加速器中数据布局灵活性和链路利用率低的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载依赖异构加速器，但数据在高带宽和布局灵活性之间的移动成为挑战，现有DMA引擎仅适用于连续内存访问，导致控制开销大和链路利用率低。

Method: XDMA引入了三项关键创新：硬件地址生成器替代软件、分布式架构分离配置与数据传输、灵活的传输插件，实现了布局灵活的高效数据传输。

Result: XDMA在合成工作负载中链路利用率提升151.2x/8.2x，实际应用中比现有DMA快2.3x，面积开销<2%，功耗为系统的17%。

Conclusion: XDMA证明，协同优化内存访问、布局转换和互联协议是释放异构多加速器SoC性能的关键。

Abstract: As modern AI workloads increasingly rely on heterogeneous accelerators,
ensuring high-bandwidth and layout-flexible data movements between accelerator
memories has become a pressing challenge. Direct Memory Access (DMA) engines
promise high bandwidth utilization for data movements but are typically optimal
only for contiguous memory access, thus requiring additional software loops for
data layout transformations. This, in turn, leads to excessive control overhead
and underutilized on-chip interconnects. To overcome this inefficiency, we
present XDMA, a distributed and extensible DMA architecture that enables
layout-flexible data movements with high link utilization. We introduce three
key innovations: (1) a data streaming engine as XDMA Frontend, replacing
software address generators with hardware ones; (2) a distributed DMA
architecture that maximizes link utilization and separates configuration from
data transfer; (3) flexible plugins for XDMA enabling on-the-fly data
manipulation during data transfers. XDMA demonstrates up to 151.2x/8.2x higher
link utilization than software-based implementations in synthetic workloads and
achieves 2.3x average speedup over accelerators with SoTA DMA in real-world
applications. Our design incurs <2% area overhead over SoTA DMA solutions while
consuming 17% of system power. XDMA proves that co-optimizing memory access,
layout transformation, and interconnect protocols is key to unlocking
heterogeneous multi-accelerator SoC performance.

</details>


### [59] [Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories](https://arxiv.org/abs/2508.08457)
*Ming-Yen Lee,Faaiq Waqar,Hanchen Yang,Muhammed Ahosan Ul Karim,Harsono Simka,Shimeng Yu*

Main category: cs.AR

TL;DR: 提出了一种结合3D嵌入式存储器的预取调度架构，显著提升长上下文LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文LLM推理中KV缓存传输导致的HBM带宽瓶颈问题。

Method: 采用M3D BEOL兼容嵌入式存储器，结合预取和调度优化。

Result: 在Llama3.1-8B上实现8.06倍解码加速和1.83倍延迟降低。

Conclusion: 通过设计优化，显著缓解HBM限制，提升长上下文LLM推理效率。

Abstract: Long-context Large Language Model (LLM) inference faces increasing compute
bottlenecks as attention calculations scale with context length, primarily due
to the growing KV-cache transfer overhead that saturates High Bandwidth Memory
(HBM). While prefetching techniques mitigate cache misses by fetching KV data
in advance, their spatial and temporal benefits present new opportunities to
exploit. This work proposes a packing-prefetch scheduling architecture with
monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with
ultra-large on-chip capacity to accelerate long-context LLM inference. Our
optimizations demonstrate 8.06x decode speedup and 1.83x overall latency
reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL
memories over the serial execution. Evaluations of multi-request workloads on
TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM
bandwidth reduction compared to packing-only methods on Llama3.1-8B and
Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL
memories, our approach alleviates HBM constraints and enables efficient
long-context LLM inference.

</details>


### [60] [JSPIM: A Skew-Aware PIM Accelerator for High-Performance Databases Join and Select Operations](https://arxiv.org/abs/2508.08503)
*Sabiha Tajdari,Anastasia Ailamaki,Sandhya Dwarkadas*

Main category: cs.AR

TL;DR: 论文提出JSPIM，一种通过算法-硬件协同设计优化的PIM模块，显著加速哈希连接查询，解决内存带宽和延迟瓶颈。


<details>
  <summary>Details</summary>
Motivation: 数据库应用受限于内存带宽和延迟，而处理内存（PIM）设计中未解决CPU导向算法的并行性不足和数据倾斜问题。

Method: JSPIM采用子阵列级并行搜索引擎和优化的哈希表设计，实现O(1)查找开销，同时结合子阵列和rank级处理解决数据倾斜。

Result: JSPIM在连接查询上比DuckDB快400至1000倍，与DuckDB结合时整体吞吐量提升2.5倍，数据开销仅7%。

Conclusion: JSPIM通过算法与硬件协同设计，高效解决内存瓶颈和查询性能问题，为PIM架构提供了实用方案。

Abstract: Database applications are increasingly bottlenecked by memory bandwidth and
latency due to the memory wall and the limited scalability of DRAM. Join
queries, central to analytical workloads, require intensive memory access and
are particularly vulnerable to inefficiencies in data movement. While
Processing-in-Memory (PIM) offers a promising solution, existing designs
typically reuse CPU-oriented join algorithms, limiting parallelism and
incurring costly inter-chip communication. Additionally, data skew, a main
challenge in CPU-based joins, remains unresolved in current PIM architectures.
  We introduce JSPIM, a PIM module that accelerates hash join and, by
extension, corresponding select queries through algorithm-hardware co-design.
JSPIM deploys parallel search engines within each subarray and redesigns hash
tables to achieve O(1) lookups, fully exploiting PIM's fine-grained
parallelism. To mitigate skew, our design integrates subarray-level parallelism
with rank-level processing, eliminating redundant off-chip transfers.
Evaluations show JSPIM delivers 400x to 1000x speedup on join queries versus
DuckDB. When paired with DuckDB for the full SSB benchmark, JSPIM achieves an
overall 2.5x throughput improvement (individual query gains of 1.1x to 28x), at
just a 7% data overhead and 2.1% per-rank PIM-enabled chip area increase.

</details>


### [61] [OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads](https://arxiv.org/abs/2508.08822)
*Shady Agwa,Yihan Pan,Georgios Papandroulidakis,Themis Prodromakis*

Main category: cs.AR

TL;DR: OISMA是一种新型内存计算架构，利用准随机计算域的简单性实现高效矩阵乘法，显著提升了精度和能源效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型的计算复杂性增加，内存计算架构虽避免了冯·诺伊曼瓶颈，但数字/模拟架构存在性能限制。OISMA旨在解决这些问题。

Method: OISMA将内存读取操作转换为原位随机乘法操作，通过积累外围实现矩阵乘法。实验评估了从4x4到512x512矩阵的精度。

Result: 基准测试显示，平均相对Frobenius误差显著降低（从9.42%降至1.81%）。在180nm技术下，能源效率达0.891TOPS/W，面积效率为3.98GOPS/mm²。

Conclusion: OISMA在精度和效率上优于传统内存计算架构，未来技术节点下仍有显著提升空间。

Abstract: Artificial Intelligence models are currently driven by a significant
up-scaling of their complexity, with massive matrix multiplication workloads
representing the major computational bottleneck. In-memory computing
architectures are proposed to avoid the Von Neumann bottleneck. However, both
digital/binary-based and analogue in-memory computing architectures suffer from
various limitations, which significantly degrade the performance and energy
efficiency gains. This work proposes OISMA, a novel in-memory computing
architecture that utilizes the computational simplicity of a quasi-stochastic
computing domain (Bent-Pyramid system), while keeping the same efficiency,
scalability, and productivity of digital memories. OISMA converts normal memory
read operations into in-situ stochastic multiplication operations with a
negligible cost. An accumulation periphery then accumulates the output
multiplication bitstreams, achieving the matrix multiplication functionality.
Extensive matrix multiplication benchmarking was conducted to analyze the
accuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4
to 512x512. The accuracy results show a significant decrease in the average
relative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared
to 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB
capacity was implemented using a commercial 180nm technology node and in-house
RRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for
energy and area efficiency, respectively, occupying an effective computing area
of 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a
significant improvement of two orders of magnitude in energy efficiency and one
order of magnitude in area efficiency, compared to dense matrix multiplication
in-memory computing architectures.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [62] [Evaluating Imputation Techniques for Short-Term Gaps in Heart Rate Data](https://arxiv.org/abs/2508.08268)
*Vaibhav Gupta,Maria Maleshkova*

Main category: stat.AP

TL;DR: 该论文研究了可穿戴设备中心率数据的缺失值填补方法，评估了四种统计填补技术的性能，并提出了一种综合评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴设备数据中缺失值的问题，填补现有评估方法的不足，尤其是针对生理信号的统计结构。

Method: 评估了线性插值、K近邻（KNN）、分段三次Hermite插值多项式（PCHIP）和B样条四种填补方法，结合预测准确性指标和统计距离度量。

Result: 分析揭示了现有填补方法的局限性及缺乏对生理信号填补质量的鲁棒评估框架。

Conclusion: 提出了一个综合评估指标的基础框架，以改进填补性能的评估。

Abstract: Recent advances in wearable technology have enabled the continuous monitoring
of vital physiological signals, essential for predictive modeling and early
detection of extreme physiological events. Among these physiological signals,
heart rate (HR) plays a central role, as it is widely used in monitoring and
managing cardiovascular conditions and detecting extreme physiological events
such as hypoglycemia. However, data from wearable devices often suffer from
missing values. To address this issue, recent studies have employed various
imputation techniques. Traditionally, the effectiveness of these methods has
been evaluated using predictive accuracy metrics such as RMSE, MAPE, and MAE,
which assess numerical proximity to the original data. While informative, these
metrics fail to capture the complex statistical structure inherent in
physiological signals. This study bridges this gap by presenting a
comprehensive evaluation of four statistical imputation methods, linear
interpolation, K Nearest Neighbors (KNN), Piecewise Cubic Hermite Interpolating
Polynomial (PCHIP), and B splines, for short term HR data gaps. We assess their
performance using both predictive accuracy metrics and statistical distance
measures, including the Cohen Distance Test (CDT) and Jensen Shannon Distance
(JS Distance), applied to HR data from the D1NAMO dataset and the BIG IDEAs Lab
Glycemic Variability and Wearable Device dataset. The analysis reveals
limitations in existing imputation approaches and the absence of a robust
framework for evaluating imputation quality in physiological signals. Finally,
this study proposes a foundational framework to develop a composite evaluation
metric to assess imputation performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [Diminution: On Reducing the Size of Grounding ASP Programs](https://arxiv.org/abs/2508.08633)
*HuanYu Yang,Fengming Zhu,YangFan Wu,Jianmin Ji*

Main category: cs.AI

TL;DR: 论文提出了一种称为"diminution"的概念，通过选择Herbrand宇宙的子集来减少ASP中的地程序规模，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为解决ASP中由于大规模Herbrand宇宙导致的地程序过大问题，需要一种更正式且通用的策略来优化地程序性能。

Method: 引入diminution概念，定义其为Herbrand宇宙的子集，并提出一种编码方法，利用现有ASP求解器评估子集的有效性。

Result: 实验结果表明，该方法平均减少70%的地程序时间，地程序文件大小降低85%。

Conclusion: diminution是一种有效且通用的方法，能够显著缓解ASP中的地程序瓶颈问题。

Abstract: Answer Set Programming (ASP) is often hindered by the grounding bottleneck:
large Herbrand universes generate ground programs so large that solving becomes
difficult. Many methods employ ad-hoc heuristics to improve grounding
performance, motivating the need for a more formal and generalizable strategy.
We introduce the notion of diminution, defined as a selected subset of the
Herbrand universe used to generate a reduced ground program before solving. We
give a formal definition of diminution, analyze its key properties, and study
the complexity of identifying it. We use a specific encoding that enables
off-the-shelf ASP solver to evaluate candidate subsets. Our approach integrates
seamlessly with existing grounders via domain predicates. In extensive
experiments on five benchmarks, applying diminutions selected by our strategy
yields significant performance improvements, reducing grounding time by up to
70% on average and decreasing the size of grounding files by up to 85%. These
results demonstrate that leveraging diminutions constitutes a robust and
general-purpose approach for alleviating the grounding bottleneck in ASP.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [64] [Financial and symbolic incentives promote 'green' charging choices](https://arxiv.org/abs/2508.08282)
*Celina Kacperski,Florian Kutzner*

Main category: physics.soc-ph

TL;DR: 研究探讨了通过金融和象征性激励措施促进电动汽车用户选择更环保的充电行为，发现两者均有效，但无显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何通过激励措施平衡电动汽车充电行为的灵活性需求与时间成本，以减少温室气体排放。

Method: 在实验室实验中模拟真实时间成本，测试金融和象征性激励对充电选择的影响。

Result: 发现两种激励措施均能促进更环保的充电行为，但两者效果无显著统计差异。

Conclusion: 结论是金融和象征性激励均可有效引导用户行为，但需进一步研究以优化激励策略。

Abstract: Electromobility can contribute to a reduction in greenhouse gas emissions if
usage behavior is aligned with the increasing availability of renewable energy.
To achieve this, smart navigation systems can be used to inform drivers of
optimal charging times and locations. Yet, required flexibility may impart time
penalties. We investigate the impact of financial and symbolic incentive
schemes to counteract these additional costs. In a laboratory experiment with
real-life time costs, we find that monetary and symbolic incentives are both
effective in changing behavior towards 'greener' charging choices, while we
find no significant statistical difference between them.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [65] [How Conversational Structure and Style Shape Online Community Experiences](https://arxiv.org/abs/2508.08596)
*Galen Weld,Carl Pearson,Bradley Spahn,Tim Althoff,Amy X. Zhang,Sanjay Kairam*

Main category: cs.SI

TL;DR: 研究探讨了在线社区的对话结构和语言风格如何预测虚拟社区感（SOVC），并通过大规模Reddit用户调查提出了可推广的模型。


<details>
  <summary>Details</summary>
Motivation: 理解在线社交互动与虚拟社区感之间的关系，填补研究空白。

Method: 使用2,826名Reddit用户的数据，开发分层模型，量化互动模式和语言风格对SOVC的影响。

Result: 发现特定互动模式（如互惠回复链、亲社会语言）与更强的社区感相关，并识别了SOVC的三个主要维度。

Conclusion: 研究为在线社区互动与SOVC的关系提供了定量证据，并为增强社区归属感提出了实际建议。

Abstract: Sense of Community (SOC) is vital to individual and collective well-being.
Although social interactions have moved increasingly online, still little is
known about the specific relationships between the nature of these interactions
and Sense of Virtual Community (SOVC). This study addresses this gap by
exploring how conversational structure and linguistic style predict SOVC in
online communities, using a large-scale survey of 2,826 Reddit users across 281
varied subreddits. We develop a hierarchical model to predict self-reported
SOVC based on automatically quantifiable and highly generalizable features that
are agnostic to community topic and that describe both individual users and
entire communities. We identify specific interaction patterns (e.g., reciprocal
reply chains, use of prosocial language) associated with stronger communities
and identify three primary dimensions of SOVC within Reddit -- Membership &
Belonging, Cooperation & Shared Values, and Connection & Influence. This study
provides the first quantitative evidence linking patterns of social interaction
to SOVC and highlights actionable strategies for fostering stronger community
attachment, using an approach that can generalize readily across community
topics, languages, and platforms. These insights offer theoretical implications
for the study of online communities and practical suggestions for the design of
features to help more individuals experience the positive benefits of online
community participation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI](https://arxiv.org/abs/2508.08270)
*Dong Xue,Ziyao Shao,Zhaoyang Duan,Fangzhou Liu,Bing Li,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 论文介绍了Doctor Sun，一种专为医学设计的 multimodal 模型，旨在解决现有模型在处理复杂医学概念和文本-图像关系上的不足，并开源了相关数据集和资源。


<details>
  <summary>Details</summary>
Motivation: 现有的医学 multimodal 模型难以理解复杂医学概念，且文本与图像的关系处理不佳，因此需要开发更高效的模型。

Method: Doctor Sun 结合预训练的视觉编码器和医学 LLM，通过两阶段训练（特征对齐和指令调整）进行优化。

Result: 提出了 Doctor Sun 模型，并开源了 SunMed-VL 数据集及相关资源，支持生物医学 multimodal 研究的发展。

Conclusion: Doctor Sun 为医学 multimodal 研究提供了新工具，并通过开源推动了该领域的进步。

Abstract: Large multimodal models (LMMs) have demonstrated significant potential in
providing innovative solutions for various biomedical tasks, including
pathology analysis, radiology report generation, and biomedical assistance.
However, the existing multimodal biomedical AI is typically based on foundation
LLMs, thus hindering the understanding of intricate medical concepts with
limited medical training data. Moreover, recent LLaVA-induced medical LMMs
struggle to effectively capture the intricate relationship between the texts
and the images. Therefore, we introduce Doctor Sun, a large multimodal
generative model specialized in medicine, developed to encode, integrate, and
interpret diverse biomedical data modalities such as text and images. In
particular, Doctor Sun integrates a pre-trained vision encoder with a medical
LLM and conducts two-stage training on various medical datasets, focusing on
feature alignment and instruction tuning. Moreover, we release SunMed-VL, a
wide-range bilingual medical multimodal dataset, along with all associated
models, code, and resources, to freely support the advancement of biomedical
multimodal research.

</details>


### [67] [Multi-grained spatial-temporal feature complementarity for accurate online cellular traffic prediction](https://arxiv.org/abs/2508.08281)
*Ningning Fu,Shengheng Liu,Weiliang Xie,Yongming Huang*

Main category: cs.LG

TL;DR: 论文提出了一种基于多粒度时空特征互补（MGSTC）的在线蜂窝流量预测方法，旨在解决现有研究中忽视的蜂窝流量特性和概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 电信数据的分析有助于动态理解和优化网络资源，但目前依赖人工干预，现有方法忽略了蜂窝流量的突发性和概念漂移问题。

Method: MGSTC通过粗粒度时间注意力和细粒度空间注意力捕捉多粒度时空特征互补，并结合在线学习策略实时检测和适应概念漂移。

Result: 在四个真实数据集上的实验表明，MGSTC在连续预测任务中优于11种现有方法。

Conclusion: MGSTC方法显著提升了蜂窝流量预测的精度和适应性，适用于实际连续预测场景。

Abstract: Knowledge discovered from telecom data can facilitate proactive understanding
of network dynamics and user behaviors, which in turn empowers service
providers to optimize cellular traffic scheduling and resource allocation.
Nevertheless, the telecom industry still heavily relies on manual expert
intervention. Existing studies have been focused on exhaustively explore the
spatial-temporal correlations. However, they often overlook the underlying
characteristics of cellular traffic, which are shaped by the sporadic and
bursty nature of telecom services. Additionally, concept drift creates
substantial obstacles to maintaining satisfactory accuracy in continuous
cellular forecasting tasks. To resolve these problems, we put forward an online
cellular traffic prediction method grounded in Multi-Grained Spatial-Temporal
feature Complementarity (MGSTC). The proposed method is devised to achieve
high-precision predictions in practical continuous forecasting scenarios.
Concretely, MGSTC segments historical data into chunks and employs the
coarse-grained temporal attention to offer a trend reference for the prediction
horizon. Subsequently, fine-grained spatial attention is utilized to capture
detailed correlations among network elements, which enables localized
refinement of the established trend. The complementarity of these multi-grained
spatial-temporal features facilitates the efficient transmission of valuable
information. To accommodate continuous forecasting needs, we implement an
online learning strategy that can detect concept drift in real-time and
promptly switch to the appropriate parameter update stage. Experiments carried
out on four real-world datasets demonstrate that MGSTC outperforms eleven
state-of-the-art baselines consistently.

</details>


### [68] [FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm](https://arxiv.org/abs/2508.09056)
*Shreya Ghosh,Abu Shafin Mohammad Mahdee Jameel,Aly El Gamal*

Main category: cs.LG

TL;DR: FetFIDS是一种基于特征嵌入而非位置嵌入的转换器深度学习系统，用于提升入侵检测性能，特别适合边缘学习和联邦学习环境，优于当前最佳系统。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过改进深度学习模型（特别是转换器）的特征嵌入方法，提升入侵检测系统的性能，并适应联邦学习环境。

Method: 采用特征嵌入替代位置嵌入，设计了一个适合联邦学习的转换器模型FetFIDS，通过多轮通信优化本地性能。

Result: FetFIDS在联邦环境中表现出色，优于现有入侵检测系统，并适合联邦学习场景。

Conclusion: FetFIDS通过特征嵌入和联邦学习显著提升了入侵检测性能，适用于边缘部署。

Abstract: Intrusion Detection Systems (IDS) have an increasingly important role in
preventing exploitation of network vulnerabilities by malicious actors. Recent
deep learning based developments have resulted in significant improvements in
the performance of IDS systems. In this paper, we present FetFIDS, where we
explore the employment of feature embedding instead of positional embedding to
improve intrusion detection performance of a transformer based deep learning
system. Our model is developed with the aim of deployments in edge learning
scenarios, where federated learning over multiple communication rounds can
ensure both privacy and localized performance improvements. FetFIDS outperforms
multiple state-of-the-art intrusion detection systems in a federated
environment and demonstrates a high degree of suitability to federated
learning. The code for this work can be found at
https://github.com/ghosh64/fetfids.

</details>


### [69] [SHEFL: Resource-Aware Aggregation and Sparsification in Heterogeneous Ensemble Federated Learning](https://arxiv.org/abs/2508.08552)
*Keumseo Ryum,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: SHEFL是一种面向计算能力各异客户端的联邦学习框架，通过动态调整资源分配和新型聚合方案解决系统异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在通信效率和预测多样性方面存在不足，尤其是在系统异质性场景下。SHEFL旨在解决这些问题。

Method: 通过分配不同数量的全局模型给客户端，并引入新型聚合方案，动态调整资源比例以减少计算负担和数据偏差。

Result: 实验表明，SHEFL在公平性和整体性能上显著优于现有方法。

Conclusion: SHEFL有效解决了计算异质性，提升了联邦学习的公平性和性能。

Abstract: Federated learning enables distributed training with private data of clients,
but its convergence is hindered by data and system heterogeneity in realistic
communication scenarios. Most existing system heterogeneous FL schemes utilize
global pruning or ensemble distillation, yet they often overlook typical
constraints required for communication efficiency. Meanwhile, deep ensembles
can aggregate predictions from individually trained models to improve
performance, but current ensemble-based FL methods fall short in fully
capturing the diversity of model predictions. In this work, we propose SHEFL, a
global ensemble-based federated learning framework suited for clients with
diverse computational capacities. We allocate different numbers of global
models to clients based on their available resources. We further introduce a
novel aggregation scheme that accounts for bias between clients with different
computational capabilities. To reduce the computational burden of training deep
ensembles and mitigate data bias, we dynamically adjust the resource ratio
across clients - aggressively reducing the influence of underpowered clients in
constrained scenarios, while increasing their weight in the opposite case.
Extensive experiments demonstrate that our method effectively addresses
computational heterogeneity, significantly improving both fairness and overall
performance compared to existing approaches.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [70] [Frequency-Assisted Adaptive Sharpening Scheme Considering Bitrate and Quality Tradeoff](https://arxiv.org/abs/2508.08854)
*Yingxue Pang,Shijie Zhao,Haiqiang Wang,Gen Zhan,Junlin Li,Li Zhang*

Main category: eess.IV

TL;DR: 该论文提出了一种频率辅助的锐化级别预测模型（FreqSP），通过结合CNN特征和高频分量，以优化视频质量与带宽成本的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的锐化技术虽然能提升视频质量，但过度锐化会导致视频比特率增加和服务质量下降，因此需要找到锐化级别与比特率之间的最佳平衡点。

Method: 论文首先为每个视频标注与最优比特率和质量权衡相对应的锐化级别作为真实标签，然后利用CNN特征和高频分量预测最优锐化级别。

Result: 大量实验表明，FreqSP模型能有效提高视频质量并控制带宽成本。

Conclusion: FreqSP模型为视频锐化问题提供了一种高效解决方案，显著优化了质量与带宽的权衡。

Abstract: Sharpening is a widely adopted technique to improve video quality, which can
effectively emphasize textures and alleviate blurring. However, increasing the
sharpening level comes with a higher video bitrate, resulting in degraded
Quality of Service (QoS). Furthermore, the video quality does not necessarily
improve with increasing sharpening levels, leading to issues such as
over-sharpening. Clearly, it is essential to figure out how to boost video
quality with a proper sharpening level while also controlling bandwidth costs
effectively. This paper thus proposes a novel Frequency-assisted Sharpening
level Prediction model (FreqSP). We first label each video with the sharpening
level correlating to the optimal bitrate and quality tradeoff as ground truth.
Then taking uncompressed source videos as inputs, the proposed FreqSP leverages
intricate CNN features and high-frequency components to estimate the optimal
sharpening level. Extensive experiments demonstrate the effectiveness of our
method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [71] [EU Digital Regulation and Guatemala: AI, 5G, and Cybersecurity](https://arxiv.org/abs/2508.08315)
*Victor Lopez Juarez*

Main category: cs.CY

TL;DR: 论文探讨了欧盟在AI、5G和网络安全领域的规则如何作为跨国治理工具影响危地马拉的政策。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟法规对危地马拉等第三国的具体影响，揭示跨国治理的机制。

Method: 分析欧盟的AI法案、5G行动计划、安全工具箱及网络安全制度，并追踪其跨国渠道。

Result: 危地马拉面临中小企业合规成本、采购限制、环境权衡等挑战。

Conclusion: 提出五项防护措施，包括数字宪政主义和绿色IT义务，以应对跨国治理的多样性。

Abstract: The paper examines how EU rules in AI, 5G, and cybersecurity operate as
transnational governance and shape policy in Guatemala. It outlines the AI
Act's risk approach, the 5G Action Plan and Security Toolbox, and the
cybersecurity regime built on ENISA, NIS2, the Cybersecurity Act, and the Cyber
Resilience Act. It traces extraterritorial channels such as the Brussels
effect, private standards, supply chain clauses, and data transfer controls.
Guatemala specific impacts include SME compliance costs, procurement limits,
environmental trade-offs in rollout, rights risks, and capacity gaps. The paper
maps current national measures and proposes five guardrails: digital
constitutionalism, green IT duties, third country impact assessment, standards
co-design, and recognition of regulatory diversity.

</details>


### [72] [Resisting AI Solutionism through Workplace Collective Action](https://arxiv.org/abs/2508.08313)
*Kevin Zheng,Linda Huber,Aaron Stark,Nathan Kim,Francesca Lameiro,Wells Lucas Santo,Shreya Chowdhary,Eugene Kim,Justine Zhang*

Main category: cs.CY

TL;DR: 密歇根大学的一群工人和学生自2024年秋季起联合发起“AI抵抗”项目，通过跨部门联盟组织公开研讨会，讨论AI使用的技术决定论不可避免性，并分享了集体抵抗的策略与挑战。


<details>
  <summary>Details</summary>
Motivation: 面对日益严峻的财政紧缩和AI可能替代劳动的威胁，工人和学生希望通过集体行动抵抗AI在校园中的技术决定论倾向。

Method: 组织跨部门联盟，举办公开研讨会，并与其他校园组织合作，建立一个持续的集体行动空间。

Result: 分享了抵抗策略和集体行动中的挑战，为其他大学的技术人员、学生和员工提供抵抗AI技术决定论的具体灵感。

Conclusion: 该研究旨在为校园内的AI抵抗行动提供实践经验和启发。

Abstract: In the face of increasing austerity and threats of AI-enabled labor
replacement at the University of Michigan, a group of workers and students have
coalesced around the project of "AI resistance" since Fall 2024. Forming a
cross-departmental coalition including librarians, faculty, staff, graduate
workers, and undergraduate students, we have hosted a public workshop
questioning the techno-deterministic inevitability of AI use at the University
and are working with other campus organizations to maintain an ongoing
organizing space. This workshop submission incorporates our reflections thus
far on the strategies we've employed, the challenges to collective resistance,
and our role as workers in resisting AI within the University. Our aim for this
work is to provide concrete inspiration for technologists, students, and staff
looking to resist AI techno-solutionism within their own universities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [73] [Neutone SDK: An Open Source Framework for Neural Audio Processing](https://arxiv.org/abs/2508.09126)
*Christopher Mitcheltree,Bogdan Teleaga,Andrew Fyfe,Naotake Masuda,Matthias Schäfer,Alfie Bradic,Nao Tokui*

Main category: cs.SD

TL;DR: Neutone SDK是一个开源框架，简化了基于PyTorch的神经音频模型在实时和离线应用中的部署，解决了DAW集成中的常见挑战。


<details>
  <summary>Details</summary>
Motivation: 当前将深度学习模型集成到数字音频工作站（DAW）中面临实时性和插件开发的复杂性，Neutone SDK旨在解决这些问题。

Method: 通过统一接口封装缓冲区大小、采样率转换和延迟补偿等常见问题，支持Python开发，并提供了详细的SDK实现技术概览。

Result: SDK展示了在音频效果模拟、音色转换和样本生成等多方面的应用潜力，并已被研究人员和艺术家广泛采用。

Conclusion: Neutone SDK为神经音频模型的DAW集成提供了高效且易用的解决方案，推动了音频处理技术的发展。

Abstract: Neural audio processing has unlocked novel methods of sound transformation
and synthesis, yet integrating deep learning models into digital audio
workstations (DAWs) remains challenging due to real-time / neural network
inference constraints and the complexities of plugin development. In this
paper, we introduce the Neutone SDK: an open source framework that streamlines
the deployment of PyTorch-based neural audio models for both real-time and
offline applications. By encapsulating common challenges such as variable
buffer sizes, sample rate conversion, delay compensation, and control parameter
handling within a unified, model-agnostic interface, our framework enables
seamless interoperability between neural models and host plugins while allowing
users to work entirely in Python. We provide a technical overview of the
interfaces needed to accomplish this, as well as the corresponding SDK
implementations. We also demonstrate the SDK's versatility across applications
such as audio effect emulation, timbre transfer, and sample generation, as well
as its adoption by researchers, educators, companies, and artists alike. The
Neutone SDK is available at https://github.com/Neutone/neutone_sdk

</details>


### [74] [SonicRadiation: A Hybrid Numerical Solution for Sound Radiation without Ghost Cells](https://arxiv.org/abs/2508.08775)
*Xutong Jin,Guoping Wang,Sheng Li*

Main category: cs.SD

TL;DR: 提出一种无需依赖幽灵细胞的新方法SonicRadiation，通过结合FDTD和TDBEM，有效解决复杂边界下的声辐射模拟问题。


<details>
  <summary>Details</summary>
Motivation: 解决复杂物体边界下声辐射模拟的挑战，提升物理音效合成的准确性和效率。

Method: 提出SonicRadiation混合数值方法，结合FDTD和TDBEM，通过边界网格同步策略实现无缝集成。

Result: 实验显示，新方法在复杂场景下显著优于现有方法，兼顾近场精度和远场效率。

Conclusion: SonicRadiation有效解决了复杂边界的模拟难题，验证了其在高精度和高效性上的优势。

Abstract: Interactive synthesis of physical sound effects is crucial in digital media
production. Sound radiation simulation, a key component of physically based
sound synthesis, has posed challenges in the context of complex object
boundaries. Previous methods, such as ghost cell-based finite-difference
time-domain (FDTD) wave solver, have struggled to address these challenges,
leading to large errors and failures in complex boundaries because of the
limitation of ghost cells. We present SonicRadiation, a hybrid numerical
solution capable of handling complex and dynamic object boundaries in sound
radiation simulation without relying on ghost cells. We derive a consistent
formulation to connect the physical quantities on grid cells in FDTD with the
boundary elements in the time-domain boundary element method (TDBEM). Hereby,
we propose a boundary grid synchronization strategy to seamlessly integrate
TDBEM with FDTD while maintaining high numerical accuracy. Our method holds
both advantages from the accuracy of TDBEM for the near-field and the
efficiency of FDTD for the far-field. Experimental results demonstrate the
superiority of our method in sound radiation simulation over previous
approaches in terms of accuracy and efficiency, particularly in complex scenes,
further validating its effectiveness.

</details>


### [75] [Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems](https://arxiv.org/abs/2508.08805)
*Liam Pram,Fabio Morreale*

Main category: cs.SD

TL;DR: 该论文探讨了生成式AI音乐创作系统的市场宣传与实际用户接受的差异，揭示了其中的意识形态问题，重点关注‘民主化’的虚假性。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI音乐系统在市场宣传中的‘民主化’口号与实际功能和用户接受之间的不一致，揭示背后的意识形态驱动。

Method: 结合自民族志和数字民族志方法，分析四种公开的生成式AI音乐系统（AIVA、Stable Audio、Suno和Udio）的开发者和用户言谈。

Result: 发现开发者和用户共享的意识形态是个人主义、全球主义、技术自由主义和伦理逃避，这种‘总体意识形态’模糊了个人责任，并重塑了音乐实践的本质以适应生成技术。

Conclusion: 生成式AI音乐系统的‘民主化’宣传更多是市场策略而非真实原则，其背后的意识形态对音乐实践的本质产生了深远影响。

Abstract: AI systems for music generation are increasingly common and easy to use,
granting people without any musical background the ability to create music.
Because of this, generative-AI has been marketed and celebrated as a means of
democratizing music making. However, inclusivity often functions as marketable
rhetoric rather than a genuine guiding principle in these industry settings. In
this paper, we look at four generative-AI music making systems available to the
public as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they
are rhetoricized by their developers, and received by users. Our aim is to
investigate ideologies that are driving the early-stage development and
adoption of generative-AI in music making, with a particular focus on
democratization. A combination of autoethnography and digital ethnography is
used to examine patterns and incongruities in rhetoric when positioned against
product functionality. The results are then collated to develop a nuanced,
contextual discussion. The shared ideology we map between producers and
consumers is individualist, globalist, techno-liberal, and ethically evasive.
It is a 'total ideology' which obfuscates individual responsibility, and
through which the nature of music and musical practice is transfigured to suit
generative outcomes.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [76] [Two for One, One for All: Deterministic LDC-based Robust Computation in Congested Clique](https://arxiv.org/abs/2508.08740)
*Keren Censor-Hillel,Orr Fischer,Ran Gelles,Pedro Soto*

Main category: cs.DS

TL;DR: 设计一个确定性编译器，使Congested Clique模型中的计算对恒定比例α < 1的恶意崩溃故障具有鲁棒性，同时保持低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在网络节点可能崩溃的情况下，确保计算能够继续进行，避免崩溃节点对整体计算的影响。

Method: 利用局部可解码代码（LDC）在网络中编码信息，并通过确定性选择查询集和自适应加倍策略来应对节点崩溃。

Result: 提出的编译器能够在有崩溃故障的模型中，高效完成电路计算和算法编译，时间复杂性为T^2 n^{o(1)}轮次。

Conclusion: 通过结合LDC和自适应策略，成功实现了在崩溃故障下的高效确定性计算。

Abstract: We design a deterministic compiler that makes any computation in the
Congested Clique model robust to a constant fraction $\alpha<1$ of adversarial
crash faults. In particular, we show how a network of $n$ nodes can compute any
circuit of depth $d$, width $\omega$, and gate total fan $\Delta$, in
$d\cdot\lceil\frac{\omega}{n^2}+\frac{\Delta}{n}\rceil\cdot
2^{O(\sqrt{\log{n}}\log\log{n})}$ rounds in such a faulty model. As a
corollary, any $T$-round Congested Clique algorithm can be compiled into an
algorithm that completes in $T^2 n^{o(1)}$ rounds in this model.
  Our compiler obtains resilience to node crashes by coding information across
the network, where we leverage locally-decodable codes (LDCs) to maintain a low
complexity overhead, as these allow recovering the information needed at each
computational step by querying only small parts of the codeword.
  The main technical contribution is that because erasures occur in known
locations, which correspond to crashed nodes, we can derandomize classical LDC
constructions by deterministically selecting query sets that avoid sufficiently
many erasures. Moreover, when decoding multiple codewords in parallel, our
derandomization load-balances the queries per-node, thereby preventing
congestion and maintaining a low round complexity.
  Deterministic decoding of LDCs presents a new challenge: the adversary can
target precisely the (few) nodes that are queried for decoding a certain
codeword. We overcome this issue via an adaptive doubling strategy: if a
decoding attempt for a codeword fails, the node doubles the number of its
decoding attempts. Similarly, when the adversary crashes the decoding node
itself, we replace it dynamically with two other non-crashed nodes. By
carefully combining these two doubling processes, we overcome the challenges
posed by the combination of a deterministic LDC with a worst case pattern of
crashes.

</details>


<div id='math.SG'></div>

# math.SG [[Back]](#toc)

### [77] [Symplectification of Circular Arcs and Arc Splines](https://arxiv.org/abs/2508.07726)
*Stefan Gössner*

Main category: math.SG

TL;DR: 本文研究了圆弧及其在分段圆弧曲线中的应用，利用端点参数化和辛几何视角提供了新的向量关系。


<details>
  <summary>Details</summary>
Motivation: 探索圆弧在几何构建中的高效表示及其优化潜力。

Method: 采用端点参数化和辛几何方法，分析圆弧及其拼接条件。

Result: 圆弧样条可视为单参数曲线族，并能针对不同标准优化参数。

Conclusion: 圆弧参数化和辛几何为曲线族优化提供了新视角。

Abstract: In this article, circular arcs are considered both individually and as
elements of a piecewise circular curve. The endpoint parameterization proves to
be quite advantageous here. The perspective of symplectic geometry provides new
vectorial relationships for the circular arc. Curves are considered whose
neighboring circular elements each have a common end point or, in addition, a
common tangent. These arc splines prove to be a one-parameter curve family,
whereby this parameter can be optimized with regard to various criteria.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [78] [Extremely Scalable Distributed Computation of Contour Trees via Pre-Simplification](https://arxiv.org/abs/2508.08433)
*Mingzhe Li,Hamish Carr,Oliver Rübel,Bei Wang,Gunther H. Weber*

Main category: cs.CG

TL;DR: 提出一种预简化策略，显著减少分布式层次轮廓树分析任务的内存开销，提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式层次轮廓树在大规模科学数据分析中内存占用过高的问题。

Method: 采用预简化策略优化内存使用。

Result: 在15分钟内构建了包含超过5000亿节点的最大轮廓树。

Conclusion: 预简化策略有效提升了轮廓树在大规模数据集中的可扩展性。

Abstract: Contour trees offer an abstract representation of the level set topology in
scalar fields and are widely used in topological data analysis and
visualization. However, applying contour trees to large-scale scientific
datasets remains challenging due to scalability limitations. Recent
developments in distributed hierarchical contour trees have addressed these
challenges by enabling scalable computation across distributed systems.
Building on these structures, advanced analytical tasks -- such as volumetric
branch decomposition and contour extraction -- have been introduced to
facilitate large-scale scientific analysis. Despite these advancements, such
analytical tasks substantially increase memory usage, which hampers
scalability. In this paper, we propose a pre-simplification strategy to
significantly reduce the memory overhead associated with analytical tasks on
distributed hierarchical contour trees. We demonstrate enhanced scalability
through strong scaling experiments, constructing the largest known contour tree
-- comprising over half a trillion nodes with complex topology -- in under 15
minutes on a dataset containing 550 billion elements.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [79] [Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization](https://arxiv.org/abs/2508.08667)
*Ke Liu,Xuanhan Wang,Qilong Zhang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出了一种新的深度图像水印方法HiWL，通过两阶段优化同时满足不可见性、鲁棒性和广泛适用性，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像水印方法难以同时满足不可见性、鲁棒性和广泛适用性，因此提出了HiWL来解决这一问题。

Method: HiWL采用两阶段优化：第一阶段通过分布对齐学习建立共同潜在空间，确保水印不可见和鲁棒性；第二阶段通过广义水印表示学习分离水印和图像内容。

Result: 实验结果显示，HiWL在水印提取准确率上比现有方法高7.6%，且处理速度极快（8秒处理10万张图像）。

Conclusion: HiWL是一种高效的深度图像水印解决方案，能够同时满足不可见性、鲁棒性和广泛适用性，适用于版权保护。

Abstract: Deep image watermarking, which refers to enable imperceptible watermark
embedding and reliable extraction in cover images, has shown to be effective
for copyright protection of image assets. However, existing methods face
limitations in simultaneously satisfying three essential criteria for
generalizable watermarking: 1) invisibility (imperceptible hide of watermarks),
2) robustness (reliable watermark recovery under diverse conditions), and 3)
broad applicability (low latency in watermarking process). To address these
limitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stage
optimization that enable a watermarking model to simultaneously achieve three
criteria. In the first stage, distribution alignment learning is designed to
establish a common latent space with two constraints: 1) visual consistency
between watermarked and non-watermarked images, and 2) information invariance
across watermark latent representations. In this way, multi-modal inputs
including watermark message (binary codes) and cover images (RGB pixels) can be
well represented, ensuring the invisibility of watermarks and robustness in
watermarking process thereby. The second stage employs generalized watermark
representation learning to establish a disentanglement policy for separating
watermarks from image content in RGB space. In particular, it strongly
penalizes substantial fluctuations in separated RGB watermarks corresponding to
identical messages. Consequently, HiWL effectively learns generalizable
latent-space watermark representations while maintaining broad applicability.
Extensive experiments demonstrate the effectiveness of proposed method. In
particular, it achieves 7.6\% higher accuracy in watermark extraction than
existing methods, while maintaining extremely low latency (100K images
processed in 8s).

</details>


### [80] [QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection](https://arxiv.org/abs/2508.08590)
*Yuxiao Wang,Wolin Liang,Yu Lei,Weiying Xue,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为QueryCraft的HOI检测框架，通过语义先验和特征学习改进DETR方法，结合ACTOR和PDQD技术提升检测性能。


<details>
  <summary>Details</summary>
Motivation: DETR方法在HOI检测中因查询初始化缺乏明确语义而导致性能受限，需要改进。

Method: 提出QueryCraft框架，引入ACTOR（跨模态编码器）提取动作相关特征，并结合PDQD（感知蒸馏查询解码器）优化查询初始化。

Result: 在HICO-Det和V-COCO基准测试中取得了最先进的性能。

Conclusion: QueryCraft通过语义引导和特征蒸馏，显著提升了HOI检测的准确性和泛化能力。

Abstract: Human-Object Interaction (HOI) detection aims to localize human-object pairs
and recognize their interactions in images. Although DETR-based methods have
recently emerged as the mainstream framework for HOI detection, they still
suffer from a key limitation: Randomly initialized queries lack explicit
semantics, leading to suboptimal detection performance. To address this
challenge, we propose QueryCraft, a novel plug-and-play HOI detection framework
that incorporates semantic priors and guided feature learning through
transformer-based query initialization. Central to our approach is
\textbf{ACTOR} (\textbf{A}ction-aware \textbf{C}ross-modal
\textbf{T}ransf\textbf{OR}mer), a cross-modal Transformer encoder that jointly
attends to visual regions and textual prompts to extract action-relevant
features. Rather than merely aligning modalities, ACTOR leverages
language-guided attention to infer interaction semantics and produce
semantically meaningful query representations. To further enhance object-level
query quality, we introduce a \textbf{P}erceptual \textbf{D}istilled
\textbf{Q}uery \textbf{D}ecoder (\textbf{PDQD}), which distills object category
awareness from a pre-trained detector to serve as object query initiation. This
dual-branch query initialization enables the model to generate more
interpretable and effective queries for HOI detection. Extensive experiments on
HICO-Det and V-COCO benchmarks demonstrate that our method achieves
state-of-the-art performance and strong generalization. Code will be released
upon publication.

</details>


### [81] [ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation](https://arxiv.org/abs/2508.08987)
*Ding Xia,Naoto Inoue,Qianru Qiu,Kotaro Kikuchi*

Main category: cs.CV

TL;DR: 论文探索了基于大型语言模型（LLM）的颜色推荐方法，开发了ColorGPT管道，在颜色推荐和调色板生成任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 颜色在矢量图形设计中至关重要，但传统方法因数据有限和复杂性难以有效推荐颜色。研究旨在验证LLM能否成为更好的颜色推荐工具。

Method: 通过测试多种颜色表示和提示工程技术，开发了ColorGPT管道，用于颜色调色板补全和基于文本描述的完整调色板生成。

Result: 实验表明，ColorGPT在颜色推荐准确性和分布上优于现有方法，同时在颜色多样性和相似性上也有所提升。

Conclusion: LLM可作为高效的颜色推荐工具，ColorGPT在颜色设计中展现了显著优势。

Abstract: Colors play a crucial role in the design of vector graphic documents by
enhancing visual appeal, facilitating communication, improving usability, and
ensuring accessibility. In this context, color recommendation involves
suggesting appropriate colors to complete or refine a design when one or more
colors are missing or require alteration. Traditional methods often struggled
with these challenges due to the complex nature of color design and the limited
data availability. In this study, we explored the use of pretrained Large
Language Models (LLMs) and their commonsense reasoning capabilities for color
recommendation, raising the question: Can pretrained LLMs serve as superior
designers for color recommendation tasks? To investigate this, we developed a
robust, rigorously validated pipeline, ColorGPT, that was built by
systematically testing multiple color representations and applying effective
prompt engineering techniques. Our approach primarily targeted color palette
completion by recommending colors based on a set of given colors and
accompanying context. Moreover, our method can be extended to full palette
generation, producing an entire color palette corresponding to a provided
textual description. Experimental results demonstrated that our LLM-based
pipeline outperformed existing methods in terms of color suggestion accuracy
and the distribution of colors in the color palette completion task. For the
full palette generation task, our approach also yielded improvements in color
diversity and similarity compared to current techniques.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [82] [CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.08709)
*Lukas Krupp,Maximilian Schöffel,Elias Biehl,Norbert Wehn*

Main category: cs.RO

TL;DR: CRADLE是一个基于LLM的多智能体系统框架，用于RTL设计的对话式设计空间探索，显著减少资源使用。


<details>
  <summary>Details</summary>
Motivation: 解决现有RTL设计方法过于刚性，缺乏用户引导和自动化优化的问题。

Method: 采用生成器-批评家智能体系统，结合LLM技术进行内部自验证、修正和优化。

Result: 在RTLLM基准测试中，LUT和FF资源使用平均减少48%和40%。

Conclusion: CRADLE提供了一种灵活高效的RTL设计探索方法，显著优化资源利用率。

Abstract: This paper presents CRADLE, a conversational framework for design space
exploration of RTL designs using LLM-based multi-agent systems. Unlike existing
rigid approaches, CRADLE enables user-guided flows with internal
self-verification, correction, and optimization. We demonstrate the framework
with a generator-critic agent system targeting FPGA resource minimization using
state-of-the-art LLMs. Experimental results on the RTLLM benchmark show that
CRADLE achieves significant reductions in resource usage with averages of 48%
and 40% in LUTs and FFs across all benchmark designs.

</details>


### [83] [AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality](https://arxiv.org/abs/2508.08507)
*Shaun Macdonald,Salma ElSayed,Mark McGill*

Main category: cs.RO

TL;DR: AZRA是一个通过AR增强拟动物机器人情感交互的框架，无需物理改造即可扩展其情感能力，包括情绪显示和互动方式，并采用情感计算模型以模拟动态情感反应。


<details>
  <summary>Details</summary>
Motivation: 拟动物机器人在情感交互上通常过于简单且短暂，限制了其在家庭中的应用潜力，因此需要更动态和细腻的交互方式。

Method: 提出AZRA框架，利用AR技术为拟动物机器人添加情感显示（如表情、灯光、声音、思维气泡）和交互方式（如语音、触摸、接近、注视），并采用情感计算模型模拟动态情感反应。

Result: AZRA成功扩展了拟动物机器人的情感交互能力，并可用于快速原型设计和增强现有机器人功能。

Conclusion: AZRA为拟动物机器人的未来发展提供了新的可能性，推动了更丰富的交互体验。

Abstract: Zoomorphic robots could serve as accessible and practical alternatives for
users unable or unwilling to keep pets. However, their affective interactions
are often simplistic and short-lived, limiting their potential for domestic
adoption. In order to facilitate more dynamic and nuanced affective
interactions and relationships between users and zoomorphic robots we present
AZRA, a novel augmented reality (AR) framework that extends the affective
capabilities of these robots without physical modifications. To demonstrate
AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays
(face, light, sound, thought bubbles) and interaction modalities (voice, touch,
proximity, gaze). Additionally, AZRA features a computational model of emotion
to calculate the robot's emotional responses, daily moods, evolving personality
and needs. We highlight how AZRA can be used for rapid participatory
prototyping and enhancing existing robots, then discuss implications on future
zoomorphic robot development.

</details>


### [84] [Robot can reduce superior's dominance in group discussions with human social hierarchy](https://arxiv.org/abs/2508.08767)
*Kazuki Komura,Kumi Ozaki,Seiji Yamada*

Main category: cs.RO

TL;DR: 研究探讨机器人代理是否能通过社交层级关系减少上级主导并平等化讨论参与。实验显示机器人行为可能影响发言时间，但未能显著证明其效果。


<details>
  <summary>Details</summary>
Motivation: 解决社交层级讨论中上级主导问题，通过机器人干预平等化参与。

Method: 30名医学生参与层级实验，机器人根据层级鼓励发言，与无干预和均等干预对比。

Result: 机器人行为可能影响发言时间，但无显著差异；上级满意度未下降。

Conclusion: 机器人可能通过控制反馈行为抑制上级主导并平等化讨论参与。

Abstract: This study investigated whether robotic agents that deal with social
hierarchical relationships can reduce the dominance of superiors and equalize
participation among participants in discussions with hierarchical structures.
Thirty doctors and students having hierarchical relationship were gathered as
participants, and an intervention experiment was conducted using a robot that
can encourage participants to speak depending on social hierarchy. These were
compared with strategies that intervened equally for all participants without
considering hierarchy and with a no-action. The robots performed follow
actions, showing backchanneling to speech, and encourage actions, prompting
speech from members with less speaking time, on the basis of the hierarchical
relationships among group members to equalize participation. The experimental
results revealed that the robot's actions could potentially influence the
speaking time among members, but it could not be conclusively stated that there
were significant differences between the robot's action conditions. However,
the results suggested that it might be possible to influence speaking time
without decreasing the satisfaction of superiors. This indicates that in
discussion scenarios where experienced superiors are likely to dominate,
controlling the robot's backchanneling behavior could potentially suppress
dominance and equalize participation among group members.

</details>


### [85] [Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality](https://arxiv.org/abs/2508.08999)
*Chao Wang,Michael Gienger,Fan Zhang*

Main category: cs.RO

TL;DR: 提出了一个通过混合现实捕捉人类专家演示，实时生成多样化机器人情感表达的框架。


<details>
  <summary>Details</summary>
Motivation: 研究机器人表达行为对传达情感状态的重要性。

Method: 利用专家在混合现实中操作虚拟机器人，捕捉其面部表情、头部动作和上半身手势，通过流匹配生成模型实时生成机器人行为。

Result: 初步测试验证了该框架在生成自主表达方面的有效性。

Conclusion: 该框架能够实时生成多样化的机器人情感表达，增强了人机交互的情感传达效果。

Abstract: Expressive behaviors in robots are critical for effectively conveying their
emotional states during interactions with humans. In this work, we present a
framework that autonomously generates realistic and diverse robotic emotional
expressions based on expert human demonstrations captured in Mixed Reality
(MR). Our system enables experts to teleoperate a virtual robot from a
first-person perspective, capturing their facial expressions, head movements,
and upper-body gestures, and mapping these behaviors onto corresponding robotic
components including eyes, ears, neck, and arms. Leveraging a
flow-matching-based generative process, our model learns to produce coherent
and varied behaviors in real-time in response to moving objects, conditioned
explicitly on given emotional states. A preliminary test validated the
effectiveness of our approach for generating autonomous expressions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Putnam-AXIOM: A Functional and Static Benchmark](https://arxiv.org/abs/2508.08292)
*Aryan Gulati,Brando Miranda,Eric Chen,Emily Xia,Kai Fronsdal,Bruno Dumont,Elyas Obbad,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 提出了Putnam-AXIOM基准测试，通过大学数学竞赛题及其变体评估大语言模型的数学推理能力，减少数据污染影响。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准测试已接近饱和，且受训练集污染影响，需要动态、抗污染的新评估方法。

Method: 从Putnam数学竞赛中选取522道题，并生成100道变体，使用动态变体协议生成无限新题。引入TFA指标评估推理过程。

Result: 最强模型在原始题上得分41.9%，但在变体上下降19.6%（相对46.8%），其他模型也表现下降趋势。

Conclusion: Putnam-AXIOM提供了抗污染的动态基准测试框架，强调模型推理能力的真实评估。

Abstract: Current mathematical reasoning benchmarks for large language models (LLMs)
are approaching saturation, with some achieving > 90% accuracy, and are
increasingly compromised by training-set contamination. We introduce
Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn
from the prestigious William Lowell Putnam Mathematical Competition, and
Putnam-AXIOM Variation, an unseen companion set of 100 functional variants
generated by programmatically perturbing variables and constants. The variation
protocol produces an unlimited stream of equally difficult, unseen instances --
yielding a contamination-resilient test bed. On the Original set, OpenAI's
o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy
drops by 19.6% (46.8% relative decrease) on the paired Variations. The
remaining eighteen models show the same downward trend, ten of them with
non-overlapping 95% confidence intervals. These gaps suggest memorization and
highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy
with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores
reasoning traces and automates natural language proof evaluations. Putnam-AXIOM
therefore provides a rigorous, contamination-resilient evaluation framework for
assessing advanced mathematical reasoning of LLMs. Data and evaluation code are
publicly available at https://github.com/brando90/putnam-axiom.

</details>


### [87] [AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators](https://arxiv.org/abs/2508.09101)
*Jason Chou,Ao Liu,Yuchi Deng,Zhiying Zeng,Tao Zhang,Haotian Zhu,Jianwei Cai,Yue Mao,Chenchen Zhang,Lingyun Tan,Ziyan Xu,Bohui Zhai,Hengyi Liu,Speed Zhu,Wiggin Zhou,Fengzong Lian*

Main category: cs.CL

TL;DR: AutoCodeGen提出了一种自动化生成多语言代码生成数据集的方法，解决了现有基准测试的局限性，并发布了AutoCodeBench系列，用于评估LLMs在复杂多语言任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试依赖人工标注且局限于Python，难以扩展和覆盖多语言及高难度任务。

Method: 通过LLM生成测试输入，利用多语言沙箱获取输出，结合逆序问题生成和多步过滤确保数据质量。

Result: AutoCodeBench包含3920个问题，覆盖20种语言；评估显示高级LLMs在复杂多语言任务中表现不佳。

Conclusion: AutoCodeBench系列为多语言代码生成评估提供了宝贵资源，推动社区关注更具挑战性的实际场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains, with code generation emerging as a key area of focus. While
numerous benchmarks have been proposed to evaluate their code generation
abilities, these benchmarks face several critical limitations. First, they
often rely on manual annotations, which are time-consuming and difficult to
scale across different programming languages and problem complexities. Second,
most existing benchmarks focus primarily on Python, while the few multilingual
benchmarks suffer from limited difficulty and uneven language distribution. To
address these challenges, we propose AutoCodeGen, an automated method for
generating high-difficulty multilingual code generation datasets without manual
annotations. AutoCodeGen ensures the correctness and completeness of test cases
by generating test inputs with LLMs and obtaining test outputs through a
multilingual sandbox, while achieving high data quality through reverse-order
problem generation and multiple filtering steps. Using this novel method, we
introduce AutoCodeBench, a large-scale code generation benchmark comprising
3,920 problems evenly distributed across 20 programming languages. It is
specifically designed to evaluate LLMs on challenging, diverse, and practical
multilingual tasks. We evaluate over 30 leading open-source and proprietary
LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The
results show that even the most advanced LLMs struggle with the complexity,
diversity, and multilingual nature of these tasks. Besides, we introduce
AutoCodeBench-Complete, specifically designed for base models to assess their
few-shot code generation capabilities. We hope the AutoCodeBench series will
serve as a valuable resource and inspire the community to focus on more
challenging and practical multilingual code generation scenarios.

</details>


### [88] [A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models](https://arxiv.org/abs/2508.08712)
*Lingzhe Zhang,Liancheng Fang,Chiming Duan,Minghua He,Leyi Pan,Pei Xiao,Shiyu Huang,Yunpeng Zhai,Xuming Hu,Philip S. Yu,Aiwei Liu*

Main category: cs.CL

TL;DR: 本文综述了平行文本生成方法，探讨了其如何突破传统自回归方法的限制，并分类分析了不同技术的速度、质量和效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要依赖自回归生成，其逐词生成的特性导致速度受限。平行文本生成技术旨在解决这一问题，但对具体技术和性能提升缺乏系统分析。

Method: 文章系统调查了平行文本生成方法，将其分为基于自回归和非自回归的范式，并详细分析了各类技术的核心特点。

Result: 通过分类和分析，总结了不同技术在速度、质量和效率上的理论权衡，并探讨了与其他加速策略的结合潜力。

Conclusion: 研究成果为平行文本生成领域提供了全面视角，指出了未来研究的挑战和方向。

Abstract: As text generation has become a core capability of modern Large Language
Models (LLMs), it underpins a wide range of downstream applications. However,
most existing LLMs rely on autoregressive (AR) generation, producing one token
at a time based on previously generated context-resulting in limited generation
speed due to the inherently sequential nature of the process. To address this
challenge, an increasing number of researchers have begun exploring parallel
text generation-a broad class of techniques aimed at breaking the
token-by-token generation bottleneck and improving inference efficiency.
Despite growing interest, there remains a lack of comprehensive analysis on
what specific techniques constitute parallel text generation and how they
improve inference performance. To bridge this gap, we present a systematic
survey of parallel text generation methods. We categorize existing approaches
into AR-based and Non-AR-based paradigms, and provide a detailed examination of
the core techniques within each category. Following this taxonomy, we assess
their theoretical trade-offs in terms of speed, quality, and efficiency, and
examine their potential for combination and comparison with alternative
acceleration strategies. Finally, based on our findings, we highlight recent
advancements, identify open challenges, and outline promising directions for
future research in parallel text generation.

</details>


### [89] [Heartificial Intelligence: Exploring Empathy in Language Models](https://arxiv.org/abs/2508.08271)
*Victoria Williams,Benjamin Rosman*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在认知共情上优于人类，但在情感共情上表现较差，显示出作为虚拟伴侣的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在认知和情感共情方面的表现，以评估其作为虚拟伴侣的能力。

Method: 使用标准化心理学测试对比小型和大型语言模型与人类的表现。

Result: 大型语言模型在认知共情任务中优于人类，但在情感共情上显著低于人类。

Conclusion: 语言模型在认知共情上的优势使其适合提供客观的虚拟陪伴，但情感共情能力需进一步提升。

Abstract: Large language models have become increasingly common, used by millions of
people worldwide in both professional and personal contexts. As these models
continue to advance, they are frequently serving as virtual assistants and
companions. In human interactions, effective communication typically involves
two types of empathy: cognitive empathy (understanding others' thoughts and
emotions) and affective empathy (emotionally sharing others' feelings). In this
study, we investigated both cognitive and affective empathy across several
small (SLMs) and large (LLMs) language models using standardized psychological
tests. Our results revealed that LLMs consistently outperformed humans -
including psychology students - on cognitive empathy tasks. However, despite
their cognitive strengths, both small and large language models showed
significantly lower affective empathy compared to human participants. These
findings highlight rapid advancements in language models' ability to simulate
cognitive empathy, suggesting strong potential for providing effective virtual
companionship and personalized emotional support. Additionally, their high
cognitive yet lower affective empathy allows objective and consistent emotional
support without running the risk of emotional fatigue or bias.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [90] [Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference](https://arxiv.org/abs/2508.08438)
*Kexin Chu,Zecheng Lin,Dawei Xiang,Zixu Shen,Jianchang Su,Cheng Chu,Yiwei Yang,Wenhui Zhang,Wenfei Wu,Wei Zhang*

Main category: cs.CR

TL;DR: SafeKV是一种隐私感知的KV缓存管理框架，通过选择性共享非敏感条目和隔离敏感内容，显著减少了时序侧信道攻击，同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 全局KV缓存共享虽然加速了大语言模型推理，但也引发了新的时序侧信道攻击风险，现有防御方法（如用户隔离）会显著降低性能。

Method: SafeKV结合多层级检测管道、统一基数树索引和基于熵的访问监控，实现敏感与非敏感内容的动态隔离。

Result: SafeKV减少了94%-97%的时序侧信道攻击，在TTFT和吞吐量上分别提升40.58%和2.66倍，显著优于传统隔离方法。

Conclusion: SafeKV通过精细的隐私控制和高缓存效率，在提供运行时隐私保障的同时，恢复了全局共享的性能优势。

Abstract: Global KV-cache sharing has emerged as a key optimization for accelerating
large language model (LLM) inference. However, it exposes a new class of timing
side-channel attacks, enabling adversaries to infer sensitive user inputs via
shared cache entries. Existing defenses, such as per-user isolation, eliminate
leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),
making them impractical for high-throughput deployment. To address this gap, we
introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware
KV-cache management framework that selectively shares non-sensitive entries
while confining sensitive content to private caches. SafeKV comprises three
components: (i) a hybrid, multi-tier detection pipeline that integrates
rule-based pattern matching, a general-purpose privacy detector, and
context-aware validation; (ii) a unified radix-tree index that manages public
and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and
(iii) entropy-based access monitoring to detect and mitigate residual
information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of
timing-based side-channel attacks. Compared to per-user isolation method,
SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across
diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from
50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with
high cache reuse efficiency, SafeKV reclaims the performance advantages of
global sharing while providing robust runtime privacy guarantees for LLM
inference.

</details>


### [91] [Approximate DBSCAN under Differential Privacy](https://arxiv.org/abs/2508.08749)
*Yuan Qiu,Ke Yi*

Main category: cs.CR

TL;DR: 本文重新审视了差分隐私（DP）下的DBSCAN问题，提出了一种基于span的新定义，并设计了一个线性时间算法，证明了其理论下界。


<details>
  <summary>Details</summary>
Motivation: 现有DP-DBSCAN算法在发布输入点的聚类标签时表现不佳，缺乏实用性。

Method: 提出基于span的新定义，设计了一个线性时间DP-DBSCAN算法，并在理论上证明了其近似比的下界。

Result: 算法在任何常数维度下都能保证三明治质量，并通过实验验证了其实用性。

Conclusion: 新方法在可视化与分类任务中表现更优，且具有高效的理论和实践性能。

Abstract: This paper revisits the DBSCAN problem under differential privacy (DP).
Existing DP-DBSCAN algorithms aim at publishing the cluster labels of the input
points. However, we show that both empirically and theoretically, this approach
cannot offer any utility in the published results. We therefore propose an
alternative definition of DP-DBSCAN based on the notion of spans. We argue that
publishing the spans actually better serves the purposes of visualization and
classification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm
achieving the sandwich quality guarantee in any constant dimensions, as well as
matching lower bounds on the approximation ratio. A key building block in our
algorithm is a linear-time algorithm for constructing a histogram under
pure-DP, which is of independent interest. Finally, we conducted experiments on
both synthetic and real-world datasets to verify the practical performance of
our DP-DBSCAN algorithm.

</details>


### [92] [Developing a Transferable Federated Network Intrusion Detection System](https://arxiv.org/abs/2508.09060)
*Abu Shafin Mohammad Mahdee Jameel,Shreya Ghosh,Aly El Gamal*

Main category: cs.CR

TL;DR: 本文开发了一种基于深度学习的分布式入侵检测系统，通过算法最大化已知攻击的知识迁移关系，提出了一种CNN模型及两种算法，实现了优异的迁移性和本地检测性能。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型对未知攻击的防御能力，利用已知攻击的知识提升检测效果。

Method: 提出了一种CNN模型，并开发了两种算法：两步数据预处理和块基智能聚合算法（BBSA）。

Result: 系统在保持高本地检测率的同时，实现了卓越的迁移性能，且方法具有泛化能力。

Conclusion: 所提方法在跨数据集和不同模型骨干的情况下展现出迁移潜力，代码已开源。

Abstract: Intrusion Detection Systems (IDS) are a vital part of a network-connected
device. In this paper, we develop a deep learning based intrusion detection
system that is deployed in a distributed setup across devices connected to a
network. Our aim is to better equip deep learning models against unknown
attacks using knowledge from known attacks. To this end, we develop algorithms
to maximize the number of transferability relationships. We propose a
Convolutional Neural Network (CNN) model, along with two algorithms that
maximize the number of relationships observed. One is a two step data
pre-processing stage, and the other is a Block-Based Smart Aggregation (BBSA)
algorithm. The proposed system succeeds in achieving superior transferability
performance while maintaining impressive local detection rates. We also show
that our method is generalizable, exhibiting transferability potential across
datasets and even with different backbones. The code for this work can be found
at https://github.com/ghosh64/tabfidsv2.

</details>


### [93] [Redactable Blockchains: An Overview](https://arxiv.org/abs/2508.08898)
*Federico Calandra,Marco Bernardo,Andrea Esposito,Francesco Fabris*

Main category: cs.CR

TL;DR: 摘要探讨了可编辑区块链的必要性及其实现方法，重点关注了隐私和合规需求，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 区块链的不可篡改性虽然保证了数据安全，但在需要合规或修正错误时却带来挑战。可编辑区块链旨在解决这一问题。

Method: 通过变色龙哈希函数和其他加密方案实现可控的、可审计的数据修改。

Result: 报告总结了可编辑区块链的加密方法及其在医疗、金融等领域的应用潜力。

Conclusion: 可编辑区块链有望构建合规、可信的数字基础设施，但仍需解决可逆计算等挑战。

Abstract: Blockchains are widely recognized for their immutability, which provides
robust guarantees of data integrity and transparency. However, this same
feature poses significant challenges in real-world situations that require
regulatory compliance, correction of erroneous data, or removal of sensitive
information. Redactable blockchains address the limitations of traditional ones
by enabling controlled, auditable modifications to blockchain data, primarily
through cryptographic mechanisms such as chameleon hash functions and
alternative redaction schemes. This report examines the motivations for
introducing redactability, surveys the cryptographic primitives that enable
secure edits, and analyzes competing approaches and their shortcomings. Special
attention is paid to the practical deployment of redactable blockchains in
private settings, with discussions of use cases in healthcare, finance,
Internet of drones, and federated learning. Finally, the report outlines
further challenges, also in connection with reversible computing, and the
future potential of redactable blockchains in building law-compliant,
trustworthy, and scalable digital infrastructures.

</details>
