<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 论文研究了软件工程师技术面试的准备方式及其对候选人的影响，发现目前的培训缺乏真实场景支持，导致候选人感到压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 技术面试对于求职者来说复杂且难以准备，而现有的计算课程很少涉及相关内容，因此研究如何更好地为候选人提供支持。

Method: 通过对正在准备技术面试的131名候选人进行问卷调查。

Result: 结果表明候选人很少在真实环境中训练，且教育课程未能有效支持准备，导致压力和准备不足。

Conclusion: 研究结果为利益相关者提供了改进技术面试准备的建议，以帮助软件工程师求职者更好地应对技术面试。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 本文提出了一种利用自然语言查询进行结构化代码搜索的新方法，结合LLM和结构化搜索工具，显著提高了搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的结构化代码搜索工具需要复杂的领域特定语言（DSL）查询，学习成本高，开发者希望用自然语言直观地搜索代码。

Method: 开发了一种结合LLM解释自然语言查询和结构化搜索工具的新方法，并在Semgrep和GQL上实例化。

Result: 在包含400个查询的基准测试中，该方法达到55%-70%的精确率和召回率，显著优于基线方法。

Conclusion: 自然语言查询结合结构化搜索工具是一种有效的替代方案，能够降低学习门槛并提高搜索效率。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究表明，内部代码指标（如代码度量和代码异味）虽不能完全解释，但可以作为移动应用受欢迎程度的有用预测指标。最佳模型（多层感知器）在二元分类任务中F1分数为0.72。


<details>
  <summary>Details</summary>
Motivation: 预测移动应用发布前的受欢迎程度可为开发者提供竞争优势，但此前认为内部指标无法预测软件质量。

Method: 使用446个开源Android应用的数据集，提取多层级代码指标和元数据，评估回归和分类模型。

Result: 回归模型表现不佳（R²低），但二元分类模型效果显著，最佳模型F1分数为0.72。

Conclusion: 内部代码指标可作为应用受欢迎程度的预测工具，挑战了之前认为其无用的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究无法通过心理测量工具检测到压力，但生理数据显示EDA峰值存在显著差异，表明时间限制未能有效诱导压力。


<details>
  <summary>Details</summary>
Motivation: 传统压力研究依赖自我报告工具，但可能存在偏差，因此探索结合生理测量方法。

Method: 实验包括前期调查、编程任务中的生理传感器监测、简短后期调查及退出访谈。

Result: 心理测量未显示压力，访谈中参与者感受不一，生理数据仅EDA峰值差异显著。

Conclusion: 研究中时间限制诱导压力不足，为未来压力与生理测量研究提供方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究分析了10个开发者沟通数据集，评估了14种情感分析工具，提出基于数据集特征的推荐方法，帮助选择合适工具。


<details>
  <summary>Details</summary>
Motivation: 解决现有情感分析工具在不同平台上表现不一致的问题，支持软件工程中可靠的AI驱动分析。

Method: 分析五个平台上的10个数据集的语言和统计特征，评估14种工具性能，提出基于特征的推荐方法。

Result: 数据集特征可用于改进工具选择，Transformer模型表现稳定但效果仍依赖于上下文。

Conclusion: 提出的方法有助于选择可靠工具，但需持续评估以适应变化的沟通环境。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 论文提出了一种多智能体方法，利用两个LLM智能体协作生成COBOL代码的解释，解决了大型语言模型由于COBOL的复杂性超过其token窗口的问题，并在14个真实项目上验证了其优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: COBOL代码库由于历史悠久、复杂性高且开发者减少，维护困难。现有的大型语言模型（LLMs）在解释其功能时，由于COBOL的特殊性（如超长token窗口），效果不佳。

Method: 采用两个协作的LLM智能体，通过结合代码库的上下文信息生成函数、文件和项目级别的解释。

Result: 在14个真实COBOL项目上测试，该方法在函数解释的METEOR、chrF和SentenceBERT分数上分别提升了12.67%、18.59%和0.62%；在文件级别分别提升了4.21%、10.72%和14.68%；在项目级别能解释82%的功能。

Conclusion: 多智能体方法能有效解决COBOL代码解释的挑战，性能显著优于基线。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型类型感知测试生成技术，结合类型约束分析和反射验证，有效减少Python类型错误的假阳性率，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Python运行时类型错误影响软件可靠性和开发效率，现有静态分析工具假阳性率高，单元测试生成技术缺乏针对性。

Method: RTED结合逐步类型约束分析和反射验证，指导测试生成并抑制假阳性。

Result: 在两个基准测试中，RTED检测出比现有方法多22-29个类型错误，精度提升173.9%-245.9%，并在6个开源项目中发现12个未知错误。

Conclusion: RTED能高效检测Python类型错误，显著提升测试精度和覆盖率。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: VeFIA框架首次解决了VFL中推断软件执行正确性的审计问题，保证数据隐私并保持高效。


<details>
  <summary>Details</summary>
Motivation: 现有VFL工作缺乏对数据方推断软件执行正确性的审计机制，VeFIA旨在填补这一空白。

Method: VeFIA利用TEE和协调器的推断结果验证数据方的计算正确性，通过随机采样检测异常。

Result: VeFIA能在异常推断超过5.4%时以99.99%概率检测问题，且不影响在线推断延迟。

Conclusion: VeFIA为VFL中推断软件的正确性审计提供了高效、隐私保护的解决方案。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 论文提出了一种自动化方法Meta-Fair，利用蜕变测试和LLM能力来检测大型语言模型中的偏见，减少了人工依赖并提升了测试效率。


<details>
  <summary>Details</summary>
Motivation: 当前公平性测试方法依赖人工、固定模板和限定数据集，难以扩展。研究旨在开发自动化测试工具，减少资源依赖并扩大适用范围。

Method: 采用蜕变测试（MRs）检测模型输出变化，利用LLM生成测试案例和评估输出，辅以开源工具支持整个测试流程。

Result: Meta-Fair在12个LLMs上测试，平均精度92%，发现29%执行中存在偏见。LLMs作为评估器表现可靠，最佳F1值达0.79。

Conclusion: Meta-Fair展示了自动化测试LLM公平性的潜力，尽管非确定性影响一致性，但通过设计可缓解，为未来研究提供了方向。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI是一个聊天机器人，旨在自动化需求获取访谈，减少人为错误并提高可扩展性，效果接近人类访谈员。


<details>
  <summary>Details</summary>
Motivation: 传统需求获取访谈依赖分析师，资源密集且有偏差。利用大语言模型自动化部分过程，提高效率和准确性。

Method: 采用零提示和最少到最多提示两种方法优化LLMREI，并在33次模拟访谈中评估其表现。微调方法因效果不佳被放弃。

Result: LLMREI在减少错误、提取需求和适应性提问方面表现接近人类，最适用于大规模自动化访谈。

Conclusion: LLMREI在自动化需求获取方面具有潜力，尤其是在大规模访谈中显著提升效率。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 论文探讨如何将人机协作（HMT）融入自适应信息物理系统（CPS），解决人机操作节奏差异及隐私保护问题，提出新方法和伦理框架。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS需要更有效的人机协作，但人机操作节奏差异和隐私保护问题阻碍了无缝HMT的实现。

Method: 开发新方法将HMT融入CPS反馈回路，并构建伦理框架，从需求工程开始验证人类价值观。

Result: 提出了解决HMT在CPS中集成和伦理问题的系统性方法。

Conclusion: 通过新方法和伦理框架，可实现更高效、隐私保护的自适应CPS人机协作。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究初步探讨了RSEs与SERs之间的术语差异，发现了合作潜力，并提出了系统化的术语映射方法。


<details>
  <summary>Details</summary>
Motivation: RSEs与SERs在沟通中因术语差异存在挑战，希望通过研究促进双方理解与合作。

Method: 通过调查SE基础概念在RSE社区中的解释，识别概念一致性、知识差距及适应潜力。

Result: 发现了双方的合作机会，并提出了术语映射的系统方法。

Conclusion: 研究为未来众包扩展和验证奠定了基础，有助于促进RSEs与SERs的协作。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: 提出RLHGNN框架，通过异构图表示和强化学习优化业务流程中的下一活动预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在捕捉非顺序关系和静态建模上的不足，优化业务流程预测。

Method: 将事件日志转换为异构图，使用强化学习动态选择图结构，并通过异构图卷积预测下一活动。

Result: 在六个数据集上表现优于当前最佳方法，预测延迟约1毫秒。

Conclusion: RLHGNN能精确建模顺序和非顺序关系，适用于实时业务监控。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 研究引入了可持续性标志的概念，用于在云架构讨论中识别可持续性话题，实验表明标志在分类效果和实用性上优于单纯依赖定义的方法。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和云架构的普及，可持续性在软件系统中的重要性日益凸显，但缺乏明确的指南来识别相关讨论。

Method: 通过主题分析开发可持续性标志，并在实验中评估其在识别云架构讨论中可持续性话题的效果。

Result: 使用标志后，分类为可持续性相关帖子的数量减少，但分类的确定性和性能显著提升，且标志更实用易懂。

Conclusion: 可持续性标志是识别软件实践中可持续性话题的有效工具，优于传统定义方法。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文提出了一种基于文本蕴含和上下文学习的方法，自动生成法律文本的规范化表示，以减少手工标注需求并提高对新法规的适用性。


<details>
  <summary>Details</summary>
Motivation: 小型企业和初创公司缺乏法律专业知识，而现有方法无法有效提取法律文本中的结构化和语义元数据及其相互关系。

Method: 采用文本蕴含和上下文学习生成法律文本的规范化表示，通过Python类结构作为领域特定元模型。

Result: 在13个美国州的数据泄露通知法律上测试，生成的表示通过89.4%的测试用例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法减少了对手工标注数据的依赖，并能更好地适应新法规，为软件合规性提供了有效的自动化工具。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文研究了在需求获取过程中使用GPT-4生成后续问题的效果。结果表明，LLM生成的问题在清晰性、相关性和信息量上不逊于人工问题，并且在指导下表现更优。


<details>
  <summary>Details</summary>
Motivation: 访谈是需求获取的常用方法，但面临认知负荷高和信息过载等挑战。LLM在自然语言处理任务中的表现激发了其在访谈辅助中的应用潜力。

Method: 基于常见面试错误类型框架，生成后续问题。通过两个对照实验评估LLM生成问题与人工问题的差异。

Result: LLM生成的问题在清晰性、相关性和信息量上与人工问题相当，且在指导下表现更优。

Conclusion: LLM能有效辅助访谈者提升实时需求获取的质效。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 本文提出了一种名为DecoRTL的新型解码策略，通过自一致性采样和语法感知温度调整，显著提升了LLMs在RTL代码生成中的正确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM解码策略在RTL代码生成中表现不佳，导致输出代码存在结构或语义问题。本文通过分析LLM在生成过程中的熵变化，发现了问题的根源。

Method: DecoRTL结合了自一致性采样（生成并重排序候选代码）和语法感知温度调整（根据不同语法角色调整采样温度），无需额外模型微调。

Result: 在VerilogEval基准测试中，DecoRTL显著提高了代码的语法有效性、功能正确性和输出多样性，且运行时开销可忽略不计。

Conclusion: DecoRTL通过动态调整解码策略，有效解决了LLMs在RTL代码生成中的局限性，为自动化硬件设计提供了新思路。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [18] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 论文研究了在资源有限的移动设备上部署大型语言模型（LLM）时面临的能效问题，提出了一种统一的能量感知调度器FUSE，显著提升了能效性能。


<details>
  <summary>Details</summary>
Motivation: LLM在移动设备上的部署面临高计算、内存和能量需求，而现有的移动设备调度器如CPU、GPU和内存的优化DVFS调度器独立运行，缺乏协同，导致能效低下。

Method: 作者首先测量了当前最佳LLM框架在移动设备上的能效，揭示了独立调度器导致的性能问题；然后通过深入分析设计了FUSE，一种统一的能量感知调度器。

Result: FUSE在实际测试中显著降低了首次生成和逐词生成延迟，分别减少了7.0%-16.9%和25.4%-36.8%，同时保持相同的每词能量消耗。

Conclusion: FUSE通过协同优化调度器，显著提升了移动设备上LLM推理的能效，为资源受限环境下的高效LLM部署提供了解决方案。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成方法，重点关注深度学习技术，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决真实数据稀缺、隐私和纯度问题，同时保留真实世界特征。

Method: 综述了合成网络流量生成的数据类型、生成模型及评估方法，特别关注基于深度学习的技术。

Result: 提供了对现有方法、挑战和机会的结构化分析。

Conclusion: 本文为研究和实践提供了基础资源，并指出了未来研究的方向。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [20] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 论文提出了一种新型机制ISN和扩展协议RXL，用于解决多节点芯片互连中的丢包和顺序问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型超越单处理器的能力，芯片互连成为可扩展计算的关键，但高速传输协议增加了错误风险，亟需新的可靠性解决方案。

Method: 引入Implicit Sequence Number (ISN)机制检测丢包并确保顺序交付，同时提出Reliability Extended Link (RXL)协议扩展CXL，支持多节点可靠性互连。

Result: RXL通过提升CRC和FEC机制，实现了端到端的数据完整性检查和高可靠性，同时保持带宽效率。

Conclusion: ISN和RXL的结合为多节点芯片互连提供了高效、可靠的解决方案，兼容现有结构且不增加额外开销。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [21] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 5G RAN能耗高，本文提出CCL框架和DUPS方案，分别通过数据协作预测和动态资源优化，显著降低能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G网络中RAN的能耗占50%以上，现有技术未能充分利用数据潜力，亟需提升效率和降低成本。

Method: 1. 使用CCL框架进行网络流量和用户预测；2. 提出DUPS方案，通过深度强化学习和CCL预测动态优化DU服务器资源。

Result: CCL预测准确率显著优于现有方法（提升31.35%~43.9%），DUPS能耗优化达89%，大幅降低成本。

Conclusion: 结合CCL和DUPS，实现了5G RAN能耗和成本的大幅优化，提升效率和经济性。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [22] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: 论文探讨了AI（尤其是LLMs）在网络配置和诊断中的应用，并呼吁建立一个标准化、可复现的开放基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 当前AI在网络故障排除中的应用缺乏标准化和可复现的评估平台。

Method: 提出了一个专注于网络故障排除的AI代理应用框架。

Result: 强调了建立开放基准测试平台的必要性。

Conclusion: 标准化平台可降低操作成本，推动AI在网络中的进一步应用。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [23] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于语义通信和生成扩散模型的框架，以提高车联网（IoV）在动态无线信道条件下的数据传输效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 车联网需要实时处理大量数据，但动态无线信道条件对数据传输效率和准确性构成挑战。

Method: 使用语义通信模型提取和压缩信息，并利用生成扩散模型预测动态信道状态；通过大模型微调提升模型适应性。

Result: 在公共数据集上验证了框架的性能和可靠性。

Conclusion: 该框架能有效提升车联网服务的质量，尤其在动态场景下表现出较强的适应性。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [24] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 论文提出了一种名为REDUS的重采样技术，用于优化深度学习训练，减少资源竞争对SDN性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着SDN与深度学习负载共享基础设施，资源竞争导致SDN响应性下降，尤其在延迟敏感的IoT环境中。联邦学习虽能解决部分问题，但DL训练的计算需求仍可能干扰SDN性能。

Method: REDUS通过优先处理误分类样本和剔除冗余数据（受AdaBoost启发），减少每轮训练的样本数量，从而节约计算资源和能源，加速收敛。

Result: 在CICIoT2023数据集上的实验显示，REDUS将训练时间减少了72.6%，精度损失仅为1.62%。

Conclusion: REDUS是一种高效、可扩展的解决方案，可在资源受限的边缘设备中优化模型训练，同时保持网络性能。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [25] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出了一种基于5G定位参考信号（PRS）的多站集成传感与通信（ISAC）完整信号处理链，展示了高保真移动目标检测、定位与跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G PRS信号实现多站ISAC系统的高效目标检测与跟踪。

Method: 采用分布式架构，通过相干交叉模糊函数（CAF）生成距离-多普勒图，提取双静态延迟和径向速度，并融合非线性最小二乘三边测量和正则化线性反演进行目标定位与速度估计。

Result: 结果表明该方法能有效实现高保真移动目标的检测、定位与跟踪。

Conclusion: 提出的信号处理链在多站ISAC系统中表现出良好的性能，扩展了5G PRS信号的应用场景。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [26] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 论文探讨了基于O-RAN原则的地面网络（TN）与非地面网络（NTN）集成的架构与功能分割策略，分析了性能、延迟和部署的权衡，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于异构传播条件、动态拓扑和有限机载处理能力，TN与NTN的集成面临挑战。研究旨在提出一致的O-RAN分割策略。

Method: 提出了分割选项的分类法，评估了从纯机载DU部署到完整gNB与UPF集成的配置，包括星内与星间处理，并讨论了RIC的放置。

Result: 提供了架构分割与RIC放置的综合映射，强调了实现约束和互操作性。

Conclusion: 论文总结了关键挑战，并提出了未来标准化、模块化和高效TN-NTN集成的方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation](https://arxiv.org/abs/2507.02080)
*Yubeen Lee,Sangeun Lee,Chaewon Park,Junyeop Cha,Eunil Park*

Main category: cs.MM

TL;DR: TAGF是一种时间感知门控融合框架，用于解决多模态情感识别中的噪声和模态不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在价-唤醒度估计中常因噪声和视听模态不对齐导致性能下降，作者提出TAGF以应对这一挑战。

Method: TAGF通过基于BiLSTM的时间门控机制自适应调节递归注意力输出的贡献，并整合多步跨模态特征。

Result: 在Aff-Wild2数据集上的实验显示，TAGF性能优于现有递归注意力模型，且对跨模态不对齐具有强鲁棒性。

Conclusion: TAGF能够有效捕捉情感表达的时序演化和模态间复杂交互，适用于真实场景下的动态情感建模。

Abstract: Multimodal emotion recognition often suffers from performance degradation in
valence-arousal estimation due to noise and misalignment between audio and
visual modalities. To address this challenge, we introduce TAGF, a Time-aware
Gated Fusion framework for multimodal emotion recognition. The TAGF adaptively
modulates the contribution of recursive attention outputs based on temporal
dynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating
mechanism to learn the relative importance of each recursive step and
effectively integrates multistep cross-modal features. By embedding temporal
awareness into the recursive fusion process, the TAGF effectively captures the
sequential evolution of emotional expressions and the complex interplay between
modalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF
achieves competitive performance compared with existing recursive
attention-based models. Furthermore, TAGF exhibits strong robustness to
cross-modal misalignment and reliably models dynamic emotional transitions in
real-world conditions.

</details>


### [28] [VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning](https://arxiv.org/abs/2507.02626)
*Siran Chen,Boyu Chen,Chenyun Yu,Yuxiao Luo,Ouyang Yi,Lei Cheng,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.MM

TL;DR: 论文提出了VRAgent-R1方法，通过两个智能代理（IP Agent和US Agent）改进视频推荐系统，提升了多模态内容理解和用户偏好学习，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的冻结LLMs方法在视频推荐中存在多模态内容理解和用户偏好学习不足的问题，限制了推荐性能。

Method: 引入VRAgent-R1，包含IP Agent（基于MLLMs的渐进式思考）和US Agent（基于CoT推理和强化学习），交互建模用户和项目。

Result: IP Agent在MicroLens-100k数据集上NDCG@10提升6.0%，US Agent用户决策模拟准确率提高45.0%。

Conclusion: VRAgent-R1通过模拟人类智能显著提升了视频推荐系统的性能。

Abstract: Owing to powerful natural language processing and generative capabilities,
large language model (LLM) agents have emerged as a promising solution for
enhancing recommendation systems via user simulation. However, in the realm of
video recommendation, existing studies predominantly resort to prompt-based
simulation using frozen LLMs and encounter the intricate challenge of
multimodal content understanding. This frequently results in suboptimal item
modeling and user preference learning, thereby ultimately constraining
recommendation performance. To address these challenges, we introduce
VRAgent-R1, a novel agent-based paradigm that incorporates human-like
intelligence in user simulation. Specifically, VRAgent-R1 comprises two
distinct agents: the Item Perception (IP) Agent and the User Simulation (US)
Agent, designed for interactive user-item modeling. Firstly, the IP Agent
emulates human-like progressive thinking based on MLLMs, effectively capturing
hidden recommendation semantics in videos. With a more comprehensive multimodal
content understanding provided by the IP Agent, the video recommendation system
is equipped to provide higher-quality candidate items. Subsequently, the US
Agent refines the recommended video sets based on in-depth chain-of-thought
(CoT) reasoning and achieves better alignment with real user preferences
through reinforcement learning. Experimental results on a large-scale video
recommendation benchmark have demonstrated the effectiveness of our proposed
VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10
on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\%
higher accuracy in user decision simulation compared to state-of-the-art
baselines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: 本文提出SMT-Sweep，将SAT sweeping扩展到字级逻辑简化，利用SMT和模拟技术显著提升硬件验证效率。


<details>
  <summary>Details</summary>
Motivation: 随着字级构造（如位向量操作和数组）在硬件验证中的普及，现有SAT sweeping技术无法满足需求，需开发对应字级方法。

Method: 基于SMT技术，结合随机化和约束驱动的字级模拟，处理复杂的位向量操作和数组语义。

Result: 实验显示SMT-Sweep比现有技术快44倍和69倍。

Conclusion: SMT-Sweep是首个将sweeping技术用于SMT硬件验证的工作，开源实现可供使用。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [30] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 本文扩展了未量化语言的满足性测试，通过预处理将公式转化为实数初级代数中的无量化公式，利用Tarski的决策方法检查满足性。


<details>
  <summary>Details</summary>
Motivation: 为了增强Tarski初级代数片段的能力，引入一元实函数及其连续导数，扩展对函数关系和性质的分析。

Method: 通过预处理将含函数变量的公式转化为无函数的实数代数公式，利用插值C^1函数验证转换的满足性保持性。

Result: 提出了一种将含函数的公式转化为实数代数公式的方法，并验证其满足性保持性。

Conclusion: 该方法成功扩展了未量化语言的满足性测试框架，为函数分析提供了新的工具。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [31] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 本文研究了直觉条件逻辑，提出了两种变体CCKbox和IntCK，并为它们设计了嵌套演算和序列演算。此外，还引入了CCK模型和公理化方法。


<details>
  <summary>Details</summary>
Motivation: 旨在通过构建直觉条件逻辑，为条件推理提供建设性分析，克服传统逻辑中条件和可能算子不可互定义的局限。

Method: 在CK条件逻辑和直觉模态逻辑的基础上，设计了IntCK的嵌套演算和CCKbox的序列演算，并进一步扩展为CCK逻辑。

Result: 提出了CCK的模型和公理化方法，并将结果扩展到CCK的多个扩展版本。

Conclusion: 通过构建新型逻辑系统，为直觉条件推理提供了理论支持，并展示了其在扩展性中的潜力。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


### [32] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: DHOL通过牺牲类型系统的可判定性提高了表达能力，同时保持了自动化定理证明的支持。本文扩展了DHOL，加入了精炼类型和商类型，使其更适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 实践者常需要精炼类型和商类型，但自动化定理证明器很少提供这些功能，因为它们的不可判定性。DHOL的设计使得添加这些类型变得简单可行。

Method: 将精炼类型和商类型作为子类型的特例引入，避免了表示的复杂性。提出了扩展语言的语法、语义及到HOL的翻译，并证明了其完备性和正确性。

Result: 成功在DHOL中添加了精炼类型和商类型，保持了语言的简洁性和高效性。

Conclusion: 扩展后的DHOL在不牺牲自动化支持的前提下，进一步提升了表达能力，满足了实际需求。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [33] [PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training](https://arxiv.org/abs/2507.02122)
*Neil K. R. Sehgal,Hita Kambhamettu,Allen Chang,Andrew Zhu,Lyle Ungar,Sharath Chandra Guntuku*

Main category: cs.HC

TL;DR: PAL是一个用于缓和医疗沟通训练的对话系统，通过模拟患者互动和提供结构化反馈支持临床技能提升。研究表明用户在反思和技能改进方面受益，但也指出了情感真实性和反馈适应性的局限性。


<details>
  <summary>Details</summary>
Motivation: 缓和医疗中的有效沟通至关重要，但实际训练资源有限（如标准化患者稀缺），因此需要开发低成本、可重复使用的训练工具。

Method: PAL系统结合文本和语音模态，基于共情框架模拟患者互动并提供反馈。通过17名医疗学员和临床医生的混合方法研究，评估用户参与度、可用性及设计问题。

Result: 参与者认为PAL有助于反思和技能提升，但情感真实性和反馈适应性有待改进。研究证实大语言模型可用于缓和医疗沟通训练。

Conclusion: PAL为高压力医疗环境中的情感劳动和AI辅助培训提供了设计启示，未来需优化情感真实性和反馈机制。

Abstract: Effective communication in serious illness and palliative care is essential
but often under-taught due to limited access to training resources like
standardized patients. We present PAL (Palliative Assisted Learning-bot), a
conversational system that simulates emotionally nuanced patient interactions
and delivers structured feedback grounded in an existing empathy-based
framework. PAL supports text and voice modalities and is designed to scaffold
clinical skill-building through repeated, low-cost practice. Through a
mixed-methods study with 17 U.S. medical trainees and clinicians, we explore
user engagement with PAL, evaluate usability, and examine design tensions
around modalities, emotional realism, and feedback delivery. Participants found
PAL helpful for reflection and skill refinement, though some noted limitations
in emotional authenticity and the adaptability of feedback. We contribute: (1)
empirical evidence that large language models can support palliative
communication training; (2) design insights for modality-aware, emotionally
sensitive simulation tools; and (3) implications for systems that support
emotional labor, cooperative learning, and AI-augmented training in high-stakes
care settings.

</details>


### [34] [StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative](https://arxiv.org/abs/2507.02156)
*Benjamin Watson,Janet Kim,Tim McEneany,Tom Moher,Claudia Hindo,Louis Gomez,Stephen Fransen*

Main category: cs.HC

TL;DR: StorySpace项目研究新界面技术如何支持高中教育中的课堂叙事活动,通过引发学生反思、呈现复杂性和增加趣味性来提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 探索新界面技术在教育中的应用,尤其是如何通过技术工具增强课堂叙事的教学效果。

Method: 项目设定了三个设计目标:触发学生反思、展示学习主题的复杂性、以及让叙事媒介本身具有吸引力和趣味性。

Result: 通过实现这些设计目标,StorySpace旨在提升学生参与度和学习体验。

Conclusion: StorySpace展示了一种通过技术工具增强课堂叙事活动的有效方法,有望在教育领域推广。

Abstract: The StorySpace project studies the role new interface technologies might play
in high school education. With this approach in mind, StorySpace is
specifically designed to support and enhance classroom narrative, an already
well-established classroom activity. StorySpace strives to achieve this through
adherence to three design goals. The first is to trigger student reflection and
interpretation. The narrative medium created by StorySpace should represent the
topic of classroom discussion and learning in all its complexity. In building
their representation, the students will then be confronted with that same
complexity. The medium should also itself be exciting and compelling, making
classroom narrative interesting and fun.

</details>


### [35] [A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy](https://arxiv.org/abs/2507.02138)
*Shan Li,Guozhu Ding*

Main category: cs.HC

TL;DR: Healthy Choice是一个AI支持的交互式营养素养模拟平台，用户满意度高。


<details>
  <summary>Details</summary>
Motivation: 通过理论驱动和AI技术提升营养素养教育。

Method: 采用互动情景学习，收集114名大学生的反馈数据。

Result: 定量评分显示平台实用且易用。

Conclusion: 平台设计成功，用户满意度高。

Abstract: This study introduces and evaluates Healthy Choice, an innovative
theory-driven and AI-enhanced simulation platform designed to cultivate
nutrition literacy through interactive scenario-based learning experiences. We
collected feedback from 114 university students with diverse backgrounds who
completed simulated product selection scenarios. Quantitative ratings of
usefulness and ease of use demonstrated high user satisfaction.

</details>


### [36] [The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future](https://arxiv.org/abs/2507.02180)
*Russell Beale*

Main category: cs.HC

TL;DR: 论文回顾了大型语言模型（LLMs）在教育和技术领域的影响，探讨了其应用案例、成功与失败，并分析了其对学习者和教育者动态的改变。同时，论文提出了LLMs成为有效教育系统所需的设计挑战，并展望了其未来在教育技术中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在教育领域的应用及其影响，探讨其潜力与挑战，以指导未来教育技术的设计。

Method: 通过综述LLMs在教育中的使用领域和案例，分析其对教育动态的改变，并探讨设计挑战和学习范式。

Result: LLMs在教育领域的影响显著，但仍需解决设计挑战以满足用户期望，未来可能成为与计算机系统交互的默认方式。

Conclusion: LLMs将深刻改变教育技术和用户期望，未来的设计需考虑其潜力与挑战以实现广泛接受。

Abstract: Large language Models have only been widely available since 2022 and yet in
less than three years have had a significant impact on approaches to education
and educational technology. Here we review the domains in which they have been
used, and discuss a variety of use cases, their successes and failures. We then
progress to discussing how this is changing the dynamic for learners and
educators, consider the main design challenges facing LLMs if they are to
become truly helpful and effective as educational systems, and reflect on the
learning paradigms they support. We make clear that the new interaction
paradigms they bring are significant and argue that this approach will become
so ubiquitous it will become the default way in which we interact with
technologies, and revolutionise what people expect from computer systems in
general. This leads us to present some specific and significant considerations
for the design of educational technology in the future that are likely to be
needed to ensure acceptance by the changing expectations of learners and users.

</details>


### [37] [A wireless, inexpensive optical tracker for the CAVE](https://arxiv.org/abs/2507.02682)
*Ehud Sharlin,Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 论文设计了一种低成本、无线的脚部追踪器，用于CAVE显示系统，提升用户移动自由度，但精度有限。


<details>
  <summary>Details</summary>
Motivation: 现有的CAVE显示系统追踪设备通常有线，限制了用户的移动自由，因此需要一种无线的解决方案。

Method: 开发了一种成本低于200美元的无线的脚部追踪器，精度为10厘米，采样率为20 Hz。

Result: 追踪器在近距离视觉检查和校园漫游应用中表现良好，但精度不足以满足高精度需求。

Conclusion: 尽管追踪器在精度上有限，但其提供的移动自由为CAVE系统带来了显著优势。

Abstract: CAVE displays offer many advantages over other virtual reality (VR) displays,
including a large, unencumbering viewing space. Unfortunately, the typical
tracking subsystems used with CAVE displays tether the user and lessen this
advantage. We have designed a simple, low-cost feet tracker that is wireless,
leaving the user free to move. The tracker can be assembled for less than $200
US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested
the prototype with two applications: a visualization supporting close visual
inspection, and a walkthrough of the campus. Although the tracking was
convincing, it was clear that the tracker's limitations make it less than ideal
for applications requiring precise visual inspection. However, the freedom of
motion allowed by the tracker was a compelling supplement to our campus
walkthrough, allowing users to stroll and look around corners.

</details>


### [38] [EvalAssist: A Human-Centered Tool for LLM-as-a-Judge](https://arxiv.org/abs/2507.02186)
*Zahra Ashktorab,Elizabeth M. Daly,Erik Miehling,Werner Geyer,Martin Santillan Cooper,Tejaswini Pedapati,Michael Desmond,Qian Pan,Hyo Jin Do*

Main category: cs.HC

TL;DR: EvalAssist是一个简化LLM评估流程的框架，提供在线标准开发环境和多种评估工具。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，评估其输出的过程耗时且昂贵，亟需高效解决方案。

Method: 开发了一个交互式框架，支持用户定制评估标准，并利用现成LLM和提示链技术进行自动化评估。

Result: 系统已内部部署，用户达数百人，并集成了专门训练的评估器以检测风险。

Conclusion: EvalAssist显著提升了LLM评估效率，并为标准化和可移植性提供了支持。

Abstract: With the broad availability of large language models and their ability to
generate vast outputs using varied prompts and configurations, determining the
best output for a given task requires an intensive evaluation process, one
where machine learning practitioners must decide how to assess the outputs and
then carefully carry out the evaluation. This process is both time-consuming
and costly. As practitioners work with an increasing number of models, they
must now evaluate outputs to determine which model and prompt performs best for
a given task. LLMs are increasingly used as evaluators to filter training data,
evaluate model performance, assess harms and risks, or assist human evaluators
with detailed assessments. We present EvalAssist, a framework that simplifies
the LLM-as-a-judge workflow. The system provides an online criteria development
environment, where users can interactively build, test, and share custom
evaluation criteria in a structured and portable format. We support a set of
LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a
prompt-chaining approach we developed and contributed to the UNITXT open-source
library. Additionally, our system also includes specially trained evaluators to
detect harms and risks in LLM outputs. We have deployed the system internally
in our organization with several hundreds of users.

</details>


### [39] [VergeIO: Depth-Aware Eye Interaction on Glasses](https://arxiv.org/abs/2507.02187)
*Xiyuxing Zhang,Duc Vu,Chengyi Shen,Yuntao Wang,Yuanchun Shi,Justin Chan*

Main category: cs.HC

TL;DR: VergeIO是一种基于EOG的智能眼镜，通过优化电极布局和新颖的原型设计，实现了深度感知的交互。它在用户研究中表现出高准确率，并能无需校准适应新用户，同时实现了低功耗实时操作。


<details>
  <summary>Details</summary>
Motivation: 行业对在眼镜上实现无干扰的EOG感测技术的兴趣日益增长，VergeIO旨在通过优化设计提升交互深度感知能力。

Method: 采用优化的电极布局和新型智能眼镜原型，结合运动伪影检测和基于前导码的激活方案，实现了高效、低功耗的实时感测。

Result: 在11位用户的1,320次手势测试中，VergeIO的准确率为83-98%；对新用户无需校准，准确率达80-98%。系统功耗低至3 mW。

Conclusion: VergeIO展示了在智能眼镜上实现深度感知交互的潜力，结合高准确率和低功耗，适合实时、长期使用。

Abstract: There is growing industry interest in creating unobtrusive designs for
electrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and
Apple eyewear). We present VergeIO, the first EOG-based glasses that enables
depth-aware eye interaction using vergence with an optimized electrode layout
and novel smart glass prototype. It can distinguish between four and six
depth-based eye gestures with 83-98% accuracy using personalized models in a
user study across 11 users and 1,320 gesture instances. It generalizes to
unseen users with an accuracy of 80-98% without any calibration. To reduce
false detections, we incorporate a motion artifact detection pipeline and a
preamble-based activation scheme. The system uses dry sensors without any
adhesives or gel, and operates in real time with 3 mW power consumption by the
sensing front-end, making it suitable for always-on sensing.

</details>


### [40] [An Exploration of Internal States in Collaborative Problem Solving](https://arxiv.org/abs/2507.02229)
*Sifatul Anindho,Videep Venkatesha,Mariah Bradford,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.HC

TL;DR: 研究了团队协作解决问题（CPS）中个体的情绪状态，通过混合方法分析了个体在任务中的情绪表达。


<details>
  <summary>Details</summary>
Motivation: CPS是教育和职业中常见的过程，了解其中的情绪状态对理解团队动态有重要意义。

Method: 团队完成CPS任务后，个体通过观看录像自我报告情绪，并进行语言分析。

Result: 语言分析揭示了情绪相关的用词模式，包括特定词汇、短语和情感标签。

Conclusion: 研究提供了CPS中情绪表达的详细视角，有助于理解和优化团队协作。

Abstract: Collaborative problem solving (CPS) is a complex cognitive, social, and
emotional process that is increasingly prevalent in educational and
professional settings. This study investigates the emotional states of
individuals during CPS using a mixed-methods approach. Teams of four first
completed a novel CPS task. Immediately after, each individual was placed in an
isolated room where they reviewed the video of their group performing the task
and self-reported their internal experiences throughout the task. We performed
a linguistic analysis of these internal monologues, providing insights into the
range of emotions individuals experience during CPS. Our analysis showed
distinct patterns in language use, including characteristic unigrams and
bigrams, key words and phrases, emotion labels, and semantic similarity between
emotion-related words.

</details>


### [41] [A framework for 3D interaction techniques](https://arxiv.org/abs/2507.02254)
*Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 该论文提出了一种用于三维交互技术（ITs）的软件架构，以及一个基于对象的、独立于工具包的框架，实现了这种架构。


<details>
  <summary>Details</summary>
Motivation: 为了解决三维交互技术的灵活性和可扩展性问题，设计了一个易于扩展的框架。

Method: 采用基于数据流的基本过滤器组合ITs，并通过执行模型定义信息流。

Result: 开发了一个可扩展的框架，支持新信息类型、输入设备、执行模型和交互技术的无缝集成。

Conclusion: 该框架为三维交互技术提供了灵活且易扩展的解决方案。

Abstract: This paper presents a software architecture for 3D interaction techniques
(ITs) and an object oriented, toolkit-independent framework that implements
such architecture. ITs are composed of basic filters connected in a dataflow,
where virtual input devices and objects in the scene are sources of
information. An execution model defines the general flow of information between
filters. This framework has been designed to be extensible: new information
types, new input devices, new execution models, or new interaction techniques
can easily be added. Application specific code and application specific ITs are
seamlessly integrated into this architecture.

</details>


### [42] [Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness](https://arxiv.org/abs/2507.02283)
*Tim Rogers,Ben Teehankee*

Main category: cs.HC

TL;DR: 论文探讨了LLMs如何继承并放大人类理论（espoused theories）与实际行为（theories-in-use）之间的错位，通过HR咨询案例展示了LLMs如何强化低效问题解决模式，并提出开发促进Model 2学习的LLMs可能对齐AI与人类价值观。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在组织决策中可能继承并放大人与AI间的错位问题，揭示其潜在风险。

Method: 结合行动科学研究，通过LLM作为HR顾问的案例研究，分析其建议模式如何阻碍组织学习。

Result: LLMs可能会强化低效行为模式，阻碍组织学习，需开发促进Model 2学习的LLMs。

Conclusion: 开发能促进Model 2学习的LLMs有助于解决AI对齐问题，同时帮助人类更好地体现价值观。

Abstract: This paper examines a critical yet unexplored dimension of the AI alignment
problem: the potential for Large Language Models (LLMs) to inherit and amplify
existing misalignments between human espoused theories and theories-in-use.
Drawing on action science research, we argue that LLMs trained on
human-generated text likely absorb and reproduce Model 1 theories-in-use - a
defensive reasoning pattern that both inhibits learning and creates ongoing
anti-learning dynamics at the dyad, group, and organisational levels. Through a
detailed case study of an LLM acting as an HR consultant, we show how its
advice, while superficially professional, systematically reinforces
unproductive problem-solving approaches and blocks pathways to deeper
organisational learning. This represents a specific instance of the alignment
problem where the AI system successfully mirrors human behaviour but inherits
our cognitive blind spots. This poses particular risks if LLMs are integrated
into organisational decision-making processes, potentially entrenching
anti-learning practices while lending authority to them. The paper concludes by
exploring the possibility of developing LLMs capable of facilitating Model 2
learning - a more productive theory-in-use - and suggests this effort could
advance both AI alignment research and action science practice. This analysis
reveals an unexpected symmetry in the alignment challenge: the process of
developing AI systems properly aligned with human values could yield tools that
help humans themselves better embody those same values.

</details>


### [43] [Human-Centered Explainability in Interactive Information Systems: A Survey](https://arxiv.org/abs/2507.02300)
*Yuhao Zhang,Jiaxin An,Ben Wang,Yan Zhang,Jiqun Liu*

Main category: cs.HC

TL;DR: 本文通过对100篇相关文献的系统性调查，总结了以人为中心的解释性在交互式信息系统中的研究进展，包括五个解释性的概念维度、解释设计的分类方案以及六个以用户为中心的测量维度。


<details>
  <summary>Details</summary>
Motivation: 随着交互式信息系统的广泛使用，用户需要理解、解释和审视AI驱动的输出以做出明智决策。因此，探索以人为中心的解释性成为关键。

Method: 遵循PRISMA指南，搜索了八个学术数据库，筛选出100篇文章。采用结构性编码方法提取和综合文章中的见解。

Result: 研究归纳了五个解释性的概念维度、一个解释设计的分类方案以及六个以用户为中心的测量维度。

Conclusion: 研究为以人为中心的解释性奠定了理论基础，并为设计透明、可信且负责任的交互式信息系统提供了指导。

Abstract: Human-centered explainability has become a critical foundation for the
responsible development of interactive information systems, where users must be
able to understand, interpret, and scrutinize AI-driven outputs to make
informed decisions. This systematic survey of literature aims to characterize
recent progress in user studies on explainability in interactive information
systems by reviewing how explainability has been conceptualized, designed, and
evaluated in practice. Following PRISMA guidelines, eight academic databases
were searched, and 100 relevant articles were identified. A structural encoding
approach was then utilized to extract and synthesize insights from these
articles. The main contributions include 1) five dimensions that researchers
have used to conceptualize explainability; 2) a classification scheme of
explanation designs; 3) a categorization of explainability measurements into
six user-centered dimensions. The review concludes by reflecting on ongoing
challenges and providing recommendations for future exploration of related
issues. The findings shed light on the theoretical foundations of
human-centered explainability, informing the design of interactive information
systems that better align with diverse user needs and promoting the development
of systems that are transparent, trustworthy, and accountable.

</details>


### [44] [Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation](https://arxiv.org/abs/2507.02306)
*Ruican Zhong,David W. McDonald,Gary Hsieh*

Main category: cs.HC

TL;DR: 论文提出了一种基于多模态LLM的合成启发式评估方法，用于用户体验设计的可用性测试，其表现超过人类评估者，但在识别某些UI组件和设计惯例方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 传统的用户体验评估成本高，需要专家时间和用户补偿，因此开发一种更高效的方法成为必要。

Method: 利用多模态LLM分析图像并提供设计反馈，将其性能与经验丰富的UX从业者进行比较。

Result: 合成评估识别出73%和77%的可用性问题，优于人类评估者的57%和63%；在布局问题检测上表现突出，但在识别UI组件和设计惯例方面表现不佳。

Conclusion: 合成评估在可用性测试中表现出潜力，但需改进其在识别UI组件和设计惯例方面的不足。

Abstract: Usability evaluation is crucial in human-centered design but can be costly,
requiring expert time and user compensation. In this work, we developed a
method for synthetic heuristic evaluation using multimodal LLMs' ability to
analyze images and provide design feedback. Comparing our synthetic evaluations
to those by experienced UX practitioners across two apps, we found our
evaluation identified 73% and 77% of usability issues, which exceeded the
performance of 5 experienced human evaluators (57% and 63%). Compared to human
evaluators, the synthetic evaluation's performance maintained consistent
performance across tasks and excelled in detecting layout issues, highlighting
potential attentional and perceptual strengths of synthetic evaluation.
However, synthetic evaluation struggled with recognizing some UI components and
design conventions, as well as identifying across screen violations.
Additionally, testing synthetic evaluations over time and accounts revealed
stable performance. Overall, our work highlights the performance differences
between human and LLM-driven evaluations, informing the design of synthetic
heuristic evaluations.

</details>


### [45] [From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance](https://arxiv.org/abs/2507.02350)
*Hao Tang,Songyun Xie,Xinzhou Xie,Can Liao,Xin Zhang,Bohan Li,Zhongyu Tian,Dalu Zheng*

Main category: cs.HC

TL;DR: 论文提出了一种细粒度标注方法，通过即时回忆范式解决传统视频诱发情感生理数据集中粗粒度标注导致的标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用整段标注，与情感反应的动态性和时间局部性不符，导致标签噪声，限制情感识别算法的评估和性能。

Method: 通过即时视频回放和参与者精确标注情感的起始时间、标签和强度，实现细粒度标注，并通过生理证据和识别性能验证。

Result: 细粒度标注显著提升情感识别准确率9.7%，同时生理数据显示标注与客观数据高度一致。

Conclusion: 细粒度标注不仅能解决标签噪声问题，还证明标注精度对情感识别性能的影响优于数据规模。

Abstract: Traditional video-induced emotion physiological datasets often use
whole-trial annotation, assigning a single emotion label to all data collected
during an entire trial. This coarse-grained annotation approach misaligns with
the dynamic and temporally localized nature of emotional responses as they
unfold with video narratives, introducing label noise that limits emotion
recognition algorithm evaluation and performance. To solve the label noise
problem caused by coarse-grained annotation, we propose a fine-grained
annotation method through an immediate recall paradigm. This paradigm
integrates an immediate video replay phase after the initial stimulus viewing,
allowing participants to precisely mark the onset timestamp, emotion label, and
intensity based on their immediate recall. We validate this paradigm through
physiological evidence and recognition performance. Physiological validation of
multimodal signals within participant-marked windows revealed rhythm-specific
EEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of
high-arousal versus 6% of low-arousal emotion windows. These objective
physiological data changes strongly aligned with subjective annotations,
confirming annotation precision. For recognition performance, classification
experiments showed that models trained on fine-grained annotations achieved
9.7% higher accuracy than traditional whole-trial labeling, despite using less
data. This work not only addresses label noise through fine-grained annotation
but also demonstrates that annotation precision outweighs data scale in
determining emotion recognition performance.

</details>


### [46] [Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset](https://arxiv.org/abs/2507.02432)
*Jueun Lee,Dennis Moschina,Supraja Ramesh,Tobias Röddiger,Kai Kunze,Michael Beigl*

Main category: cs.HC

TL;DR: 研究利用音乐结构化闭环振动模式作为被动生物反馈干预手段，旨在通过触觉同步降低心率，促进放松和入睡。结果显示短期刺激可提高放松感，但对入睡无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨一种非侵入性的触觉干预方法，替代传统的听觉或开环反馈方式，以支持放松和睡眠。

Method: 将节拍结构编码为智能手表振动，并调整频率略低于用户实时心率，通过两项研究评估其效果。

Result: 短期刺激显著增加副交感神经活动和放松感，但入睡阶段未见明显改善。

Conclusion: 为可穿戴触觉反馈在放松和睡眠中的应用提供设计启示，强调需要进一步研究触觉干预的长期效果。

Abstract: We investigate the use of musically structured, closed-loop vibration
patterns as a passive biofeedback intervention for relaxation and sleep
initiation. By encoding rhythmic meter structures into smartwatch vibrations
and adapting their frequency to be slightly slower than the user's real-time
heart rate, our system aims to reduce arousal through tactile entrainment,
offering a non-invasive alternative to auditory or open-loop approaches
previously used in sleep and anxiety contexts. In the first study (N=20), we
compared five adaptive vibration rhythms for their effects on heart rate and
subjective perceptions of relaxation in a resting context. In the second study
(N=28), we evaluated the most promising pattern from Study 1 in a prolonged
sleep initiation setting. Results showed increased parasympathetic activity and
perceived relaxation during short-term stimulation, but no significant effects
on sleep-related measures during the sleep onset phase. This work contributes
to the understanding of how wearable haptic feedback can support relaxation and
sleep, offering design insights and identifying methodological considerations
for effectively integrating haptic interaction into self-directed
interventions.

</details>


### [47] [Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?](https://arxiv.org/abs/2507.02453)
*Jueun Lee,Martin Flipe,Philipp Lepold,Tobias Röddiger,Michael Beigl*

Main category: cs.HC

TL;DR: 研究探讨了基于实时心率调整的触觉生物反馈在不同身体佩戴位置（手腕、手、前臂和肩膀）对放松效果的影响，发现前臂和肩膀最适合放松反馈。


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴触觉设备在放松状态下的应用较少考虑身体位置和动态生物反馈，本研究旨在填补这一空白。

Method: 通过实时心率调整触觉反馈，比较四个佩戴位置的生理数据（心率、α波）、主观放松度和振动体验。

Result: 前臂和肩膀佩戴能显著降低心率且主观放松度最高，手腕佩戴虽易识别但舒适度较低。

Conclusion: 前臂和肩膀是理想的放松反馈佩戴位置，手腕设计需改进以提升主观体验。

Abstract: Wearable haptic interventions offer promising support for relaxation through
slow, vibrotactile biofeedback. Despite their potential, current applications
focus on stress-inducing procedures and fixed vibration patterns, with limited
consideration of body location and dynamic biofeedback during restful states.
This study investigates the effects of haptic biofeedback adjusted from
real-time heart rate during eyes-closed wakeful rest, comparing four wearable
body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave
activity on the ear, subjective restfulness, and vibration experience were
measured across these conditions. Results show that biofeedback reduced heart
rate at the wrist, shoulder, and forearm, while alpha power measured at the ear
remained unchanged. Subjective restfulness was rated highest at the shoulder
and forearm, which were also the most preferred locations. In addition,
participants reported greater comfort, relaxation, and further increased
sleepiness at the forearm compared to the wrist, which was more easily
recognizable. These findings suggest that the forearm and shoulder are ideal
for unobtrusive relaxation feedback for wakeful rest, while the wrist may
require design improvements for subjective experience.

</details>


### [48] [Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue](https://arxiv.org/abs/2507.02537)
*Paulo Ricardo Knob,Leonardo Scholler,Juliano Rigatti,Soraia Raupp Musse*

Main category: cs.HC

TL;DR: 该研究探讨了大型语言模型（LLMs）在生成情感丰富对话时的表现，发现情感建模不仅需要结构对齐，还需要情感深度。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理在日常互动中的普及，情感智能（尤其是同理心倾听）变得至关重要。

Method: 使用ChatGPT和Gemini扩展手工制作的小数据集，并通过VADER情感分析和专家评估分析对话情感进展。

Result: 生成的对话通常能反映预期情感结构，但人类评估显示其在同理心和连贯性上存在差异。

Conclusion: 情感建模需结合自动化和人工方法，以开发更具情感能力的代理。

Abstract: Conversational agents have made significant progress since ELIZA, expanding
their role across various domains, including healthcare, education, and
customer service. As these agents become increasingly integrated into daily
human interactions, the need for emotional intelligence, particularly
empathetic listening, becomes increasingly essential. In this study, we explore
how Large Language Models (LLMs) respond when tasked with generating
emotionally rich interactions. Starting from a small dataset manually crafted
by an expert to reflect empathic behavior, we extended the conversations using
two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the
dialogues using both sentiment analysis (via VADER) and expert assessments.
While the generated conversations often mirrored the intended emotional
structure, human evaluation revealed important differences in the perceived
empathy and coherence of the responses. These findings suggest that emotion
modeling in dialogues requires not only structural alignment in the expressed
emotions but also qualitative depth, highlighting the importance of combining
automated and humancentered methods in the development of emotionally competent
agents.

</details>


### [49] [Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots](https://arxiv.org/abs/2507.02745)
*Zahra Ashktorab,Alessandra Buccella,Jason D'Cruz,Zoe Fowler,Andrew Gill,Kei Yan Leung,P. D. Magnus,John Richards,Kush R. Varshney*

Main category: cs.HC

TL;DR: 研究探讨了LLM驱动的聊天机器人在犯错后如何通过不同类型的道歉（机械式、解释式、共情式）恢复用户信任，发现解释式道歉总体上更受青睐，但场景和用户差异影响偏好。


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人在日常场景中的广泛应用，其通过有效道歉恢复用户信任的能力变得至关重要。

Method: 在预先注册的研究中，162名参与者对三种道歉类型（机械式、解释式、共情式）在三种常见错误场景（偏见、虚构、事实错误）下的效果进行成对评估。

Result: 解释式道歉总体更受欢迎，但场景和用户差异显著，例如偏见场景下共情式道歉更受青睐。

Conclusion: 有效道歉需考虑场景和用户差异，未来的AI系统应注重个性化和校准以恢复信任。

Abstract: As chatbots driven by large language models (LLMs) are increasingly deployed
in everyday contexts, their ability to recover from errors through effective
apologies is critical to maintaining user trust and satisfaction. In a
preregistered study with Prolific workers (N=162), we examine user preferences
for three types of apologies (rote, explanatory, and empathic) issued in
response to three categories of common LLM mistakes (bias, unfounded
fabrication, and factual errors). We designed a pairwise experiment in which
participants evaluated chatbot responses consisting of an initial error, a
subsequent apology, and a resolution. Explanatory apologies were generally
preferred, but this varied by context and user. In the bias scenario, empathic
apologies were favored for acknowledging emotional impact, while
hallucinations, though seen as serious, elicited no clear preference,
reflecting user uncertainty. Our findings show the complexity of effective
apology in AI systems. We discuss key insights such as personalization and
calibration that future systems must navigate to meaningfully repair trust.

</details>


### [50] [Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding](https://arxiv.org/abs/2507.02800)
*Ebrahim Feghhi,Shreyas Kaasyap,Nima Hadidi,Jonathan C. Kao*

Main category: cs.HC

TL;DR: 该论文通过时间掩码训练、Transformer架构替换及轻量化测试时适应方法，显著提升了神经语音解码的准确性和效率，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前神经语音解码算法在精度提升的同时计算成本过高，且未在实时解码环境中验证，亟需一种高效、准确且适用于实时场景的解决方案。

Method: 1) 训练时引入大量时间掩码；2) 用紧凑型Transformer替代GRU架构；3) 设计轻量化的测试时适应方法，每次试验仅需一次梯度更新。

Result: 词错误率降低19.5%，有效缓解了跨天性能下降问题，大幅降低计算成本（GPU内存使用减少36%，参数减少77%）。

Conclusion: 研究为高效、实时的神经语音解码提供了可行方案，显著降低了计算负担并提升了性能。

Abstract: Speech neuroprostheses aim to restore communication for people with severe
paralysis by decoding speech directly from neural activity. To accelerate
algorithmic progress, a recent benchmark released intracranial recordings from
a paralyzed participant attempting to speak, along with a baseline decoding
algorithm. Prior work on the benchmark showed impressive accuracy gains.
However, these gains increased computational costs and were not demonstrated in
a real-time decoding setting. Here, we make three contributions that pave the
way towards accurate, efficient, and real-time neural speech decoding. First,
we incorporate large amounts of time masking during training. On average, over
$50\%$ of each trial is masked. Second, we replace the gated recurrent unit
(GRU) architecture used in the baseline algorithm with a compact Transformer.
The Transformer architecture uses $77\%$ fewer parameters, cuts peak GPU memory
usage by $36\%$ relative, and is significantly faster to calibrate relative to
the GRU. Third, we design a lightweight variant of an existing test-time
adaptation method developed for decoding handwriting from neural activity. Our
variant adapts the model using multiple time masked augmentations of a single
trial and requires only one gradient step per trial. Together, these
contributions reduce word error rate by $19.5\%$ and effectively mitigate
performance degradations across held-out days in a real-time decoding setting
while substantially lowering computational costs.

</details>


### [51] [Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks](https://arxiv.org/abs/2507.02819)
*Luke Guerdan,Devansh Saxena,Stevie Chancellor,Zhiwei Steven Wu,Kenneth Holstein*

Main category: cs.HC

TL;DR: 论文研究了数据科学家如何将模糊概念转化为预测建模任务中的具体目标变量，通过访谈揭示了他们的‘拼装’过程。


<details>
  <summary>Details</summary>
Motivation: 探讨数据科学家如何解决模糊概念到具体目标变量的转化问题，填补这一过程的研究空白。

Method: 访谈15位教育和医疗领域的数据科学家，分析他们构建目标变量的方法。

Result: 数据科学家通过迭代调整满足五个主要标准（如有效性和可预测性），采用问题重构策略。

Conclusion: 研究为HCI、CSCW和ML领域提供了改进目标变量构建的潜在方向。

Abstract: Data scientists often formulate predictive modeling tasks involving fuzzy,
hard-to-define concepts, such as the "authenticity" of student writing or the
"healthcare need" of a patient. Yet the process by which data scientists
translate fuzzy concepts into a concrete, proxy target variable remains poorly
understood. We interview fifteen data scientists in education (N=8) and
healthcare (N=7) to understand how they construct target variables for
predictive modeling tasks. Our findings suggest that data scientists construct
target variables through a bricolage process, involving iterative negotiation
between high-level measurement objectives and low-level practical constraints.
Data scientists attempt to satisfy five major criteria for a target variable
through bricolage: validity, simplicity, predictability, portability, and
resource requirements. To achieve this, data scientists adaptively use problem
(re)formulation strategies, such as swapping out one candidate target variable
for another when the first fails to meet certain criteria (e.g.,
predictability), or composing multiple outcomes into a single target variable
to capture a more holistic set of modeling objectives. Based on our findings,
we present opportunities for future HCI, CSCW, and ML research to better
support the art and science of target variable construction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [52] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: GBake工具解决了3D高斯分布场景中传统3D网格光照问题，实现Unity引擎中更真实的反射映射。


<details>
  <summary>Details</summary>
Motivation: 3D高斯分布技术的普及导致传统计算机图形技术与高斯场景的集成需求增加。由于高斯基元将光照和几何信息编码为外观，直接插入网格会导致光照不匹配，显得格格不入。

Method: 提出GBake工具，从高斯分布场景中烘焙反射探针，以实现传统3D网格在Unity引擎中的真实反射映射。

Result: GBake能够有效解决传统3D网格在高斯场景中的光照问题，使其看起来更自然。

Conclusion: GBake为3D高斯分布场景中集成传统网格提供了一种有效的解决方案，提升了视觉一致性。

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [53] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种高效近似方法，用于动态材料特性和环境光照下的闪亮点渲染，速度快且接近真实效果。


<details>
  <summary>Details</summary>
Motivation: 闪亮点渲染在实时渲染中具有挑战性，特别是在动态环境光照和材料特性下。

Method: 通过分区环境光照并使用正态分布函数过滤，结合双门高斯近似方法实现高效采样。

Result: 方法在多种材料和光照条件下接近真实渲染，性能稳定且内存占用较低。

Conclusion: 该方法有效解决了动态闪亮点渲染问题，适用于实时应用。

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算集群，基于开放网络技术和GPU服务器设计，支持大规模语言模型训练等高级工作负载。


<details>
  <summary>Details</summary>
Motivation: 通过构建开放、高性能的计算集群，验证开源和厂商中立技术在大规模HPC基础设施中的可行性。

Method: 使用100个计算节点，每个节点配备8个NVIDIA H100 GPU，并采用基于800 GbE和SONiC的全开放网络架构。

Result: 在HPL、HPCG和HPL-MxP基准测试中表现出色，展现了高性能和低延迟通信能力。

Conclusion: SAKURAONE证明了开放网络技术在高性能计算领域的竞争力，特别适合AI和大规模并行工作负载。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [55] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: 论文比较了基于轮询（PCM）和基于信号（SCM）的容器监控方法，发现SCM在检测容器故障速度更快且无需调优，同时避免了PCM的错误检测问题。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决PCM方法在容器监控中的不足（如需要调优、可能降低服务可用性及检测延迟），并提出SCM作为一种更优的替代方案。

Method: 设计了基于信号的容器监控方法（SCM），在Kubernetes中实现并通过SockShop基准测试进行六组实验，同时提出新的数学模型预测其优势。

Result: 实验显示，SCM比PCM快86%检测容器故障，且无需调优，避免了PCM的4%错误检测率。

Conclusion: 建议容器编排器采用SCM功能，以实现更快、更准确的容器监控，避免PCM的缺陷。

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [56] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 论文提出一种基于迁移学习的智能算法，用于解决云计算环境中故障根源识别的挑战，通过共享特征提取模块和对抗机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 云计算环境中系统结构复杂、服务耦合度高且故障信息有限，导致故障根源识别困难。

Method: 采用迁移学习，引入共享特征提取模块和域对抗机制，结合伪标签选择策略以提升模型泛化能力。

Result: 实验表明，该方法在准确率、F1分数和AUC等关键指标上优于主流方法，尤其在类别不平衡和异构节点环境下表现稳健。

Conclusion: 该算法在复杂云计算系统中具有高效性和实用价值，验证了所提机制的有效性。

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [57] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: Flotilla是一个轻量级、可扩展的联邦学习框架，支持同步和异步策略，具有容错性，并在资源使用和扩展性上优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架多关注模拟验证而非实际部署，且缺乏对异步聚合和系统容错的支持。

Method: Flotilla采用模块化设计，支持无状态客户端和分离会话状态的服务器设计，实现灵活的策略组合和容错。

Result: 实验展示了Flotilla在多种策略和模型上的模块性、容错性、快速故障恢复能力，以及在资源使用和扩展性上的优势。

Conclusion: Flotilla是构建新型联邦学习策略、统一比较和系统研究的理想框架。

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [58] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: CSCS开发了Alps，一种基于独立端点资源的新型HPC架构，解决了传统HPC缺乏灵活性和可组合性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的高性能计算（HPC）架构无法满足日益多样化的科学需求，存在灵活性和可组合性不足的问题。

Method: 采用异构硬件（CPU和GPU）和高性能网络，结合软件定义的vCluster技术，实现模块化存储和定制化平台。

Result: 成功构建了支持多种科学领域的平台，如数值天气预测和AI研究。

Conclusion: Alps通过创新的架构和技术，为科学研究提供了更灵活、可定制的HPC解决方案。

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [59] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: FlowSpec是一种基于管道并行和树式推测解码的框架，用于提高分布式大语言模型（LLM）在边缘网络中的推理效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘网络中，传统的管道并行方法在请求稀疏时利用率低，导致推理延迟增加。FlowSpec旨在解决这一问题，通过优化推测解码来提高效率。

Method: FlowSpec结合了三种关键机制：1) 基于评分的逐步验证；2) 高效的草案管理；3) 动态草案扩展策略。这些机制共同提升了管道利用率和推测效率。

Result: 实验表明，FlowSpec在不同模型和配置下显著提高了推理速度，加速比达到1.36×-1.77×。

Conclusion: FlowSpec为边缘网络中的分布式LLM推理提供了一种高效的解决方案，显著提升了性能。

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Template-Based Schema Matching of Multi-Layout Tenancy Schedules:A Comparative Study of a Template-Based Hybrid Matcher and the ALITE Full Disjunction Model](https://arxiv.org/abs/2507.02020)
*Tim Uilkema,Yao Ma,Seyed Sahand Mohammadi Ziabari,Joep van Vliet*

Main category: cs.DB

TL;DR: 提出了一种基于模板的混合模式匹配器，用于对齐多布局租赁时间表到目标模式，显著提高了数据集成效率和可用性。


<details>
  <summary>Details</summary>
Motivation: 由于房地产公司租赁时间表格式不统一，现有自动化集成方法（如ALITE）虽完整但导致模式膨胀、属性稀疏和业务可用性有限。

Method: 结合模式（Jaccard、Levenshtein）和实例（数据类型、分布）度量，通过匈牙利算法确定全局最优分配。

Result: 在手动标记的真实数据上，网格搜索优化的F1得分为0.881，空值比例为45.7%，优于ALITE。

Conclusion: 结合结构化业务知识和混合匹配可生成更可用且与业务对齐的模式映射，未来可扩展至复杂复合表。

Abstract: The lack of standardized tabular formats for tenancy schedules across real
estate firms creates significant inefficiencies in data integration. Existing
automated integration methods, such as Full Disjunction (FD)-based models like
ALITE, prioritize completeness but result in schema bloat, sparse attributes
and limited business usability. We propose a novel hybrid, template-based
schema matcher that aligns multi-layout tenancy schedules to a predefined
target schema. The matcher combines schema (Jaccard, Levenshtein) and
instance-based metrics (data types, distributions) with globally optimal
assignments determined via the Hungarian Algorithm. Evaluation against a
manually labeled ground truth demonstrates substantial improvements, with grid
search optimization yielding a peak F1-score of 0.881 and an overall null
percentage of 45.7%. On a separate ground truth of 20 semantically similar
column sets, ALITE achieves an F1-score of 0.712 and 75.6% nulls. These results
suggest that combining structured business knowledge with hybrid matching can
yield more usable and business-aligned schema mappings. The approach assumes
cleanly extracted tabular input, future work could explore extending the
matcher to support complex, composite tables.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [61] [Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions](https://arxiv.org/abs/2507.02067)
*Nikolaos Papanikolaou,Doha Touhafi,Jurgen Vandendriessche,Danial Karimi,Sohail Fatimi,Gianluca Cornetta,Abdellah Touhafi*

Main category: cs.AR

TL;DR: 印刷传感器通过创新印刷技术实现灵活、经济、高度可定制的传感设备，广泛应用于环境监测等领域。


<details>
  <summary>Details</summary>
Motivation: 解决传统传感器成本高、灵活性差的局限，提供更经济高效的传感解决方案。

Method: 利用先进的印刷技术制造传感器，实现高灵敏度和准确性。

Result: 传感器能够精确检测污染物、温湿度变化等环境参数。

Conclusion: 印刷传感器在环境监测和保护中具有巨大潜力。

Abstract: Printed sensors represent a transformative advancement in sensor technology,
utilizing innovative printing techniques to create flexible, cost-effective,
and highly customizable sensing devices. Their versatility allows integration
into numerous applications across diverse fields such as monitoring a wide
range of environmental factors e.g. air and water quality, soil conditions, and
atmospheric changes among others. These sensors demonstrate high sensitivity
and accuracy in detecting pollutants, temperature variations, humidity levels,
and other critical parameters essential for environmental assessment and
protection.

</details>


### [62] [Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting](https://arxiv.org/abs/2507.02164)
*Ruibai Tang,Chengbin Quan*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA硬件加速的算法，用于绘制复函数根的密度图，通过多项式逼近和单移QR迭代实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 复函数根的求解和可视化在理论和应用领域都至关重要，但传统方法计算量大，需要高效的解决方案。

Method: 采用多项式逼近复函数，利用Hessenberg结构优化QR分解，设计了一个流水线化的FPGA架构，支持高吞吐量处理。

Result: 该实现比CPU方法能效提升65倍，在性能上虽不及现代GPU，但在能效上有显著优势。

Conclusion: FPGA架构为复函数根的计算提供了一种高效能解决方案，尤其在能效方面表现突出。

Abstract: Solving and visualizing the potential roots of complex functions is essential
in both theoretical and applied domains, yet often computationally intensive.
We present a hardware-accelerated algorithm for complex function roots density
graph plotting by approximating functions with polynomials and solving their
roots using single-shift QR iteration. By leveraging the Hessenberg structure
of companion matrices and optimizing QR decomposition with Givens rotations, we
design a pipelined FPGA architecture capable of processing a large amount of
polynomials with high throughput. Our implementation achieves up to 65x higher
energy efficiency than CPU-based approaches, and while it trails modern GPUs in
performance due to differences in fabrication technique.

</details>


### [63] [System-performance and cost modeling of Large Language Model training and inference](https://arxiv.org/abs/2507.02456)
*Wenzhe Guo,Joyjit Kundu,Uras Tos,Weijiang Kong,Giuliano Sisto,Timon Evenblij,Manu Perumkunnil*

Main category: cs.AR

TL;DR: 本文提出了一种针对大型语言模型（LLM）训练和推理的性能-成本建模方法，整合了最新的计算、内存优化和通信技术，用于指导未来计算系统设计。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和复杂性的指数级增长，其在分布式系统中的扩展性面临挑战，需要新的方法优化性能与成本。

Method: 通过分析性能模型，结合闪存注意力技术和专家混合模型，优化内存带宽和计算瓶颈，并考虑不同网络拓扑和通信算法。

Result: 提出的建模方法能够分析不同系统架构配置的性能-成本权衡，为硬件-软件协同开发提供指导。

Conclusion: 该框架为未来计算系统设计提供了有价值的见解，特别是在性能与成本之间的权衡分析中。

Abstract: Large language models (LLMs), based on transformer architectures, have
revolutionized numerous domains within artificial intelligence, science, and
engineering due to their exceptional scalability and adaptability. However, the
exponential growth in LLM size and complexity has outpaced advancements in
compute capacity, memory bandwidth, network performance, and cost efficiency,
posing significant challenges to their scalability on distributed systems. To
address these limitations, alternative model architectures, optimization
strategies, communication-aware network topologies, and novel system design
approaches have been proposed in literature. This paper introduces a
performance-cost modeling methodology for LLM training and inference that
integrates state-of-the-art compute techniques with memory optimizations, and
latest communication techniques. Building on an analytical performance model,
our approach incorporates recent innovations such as the flash attention
technique and mixture of experts models to address the memory bandwidth and
compute bottlenecks. It also considers the impact of different network
topologies and topology-specific communication algorithms with 5D parallellism.
The framework also integrates a chiplet cost model. The proposed modeling
methodology provides valuable insights to guide future compute system design
and facilitates hardware-software co-development, in particular due to its
ability to analyze performance-cost trade-offs for various system architectural
configurations.

</details>


### [64] [AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models](https://arxiv.org/abs/2507.02598)
*Chenhao Xue,Kezhi Li,Jiaxing Zhang,Yi Ren,Zhengyuan Shi,Chen Zhang,Yibo Lin,Lining Zhang,Qiang Xu,Guangyu Sun*

Main category: cs.AR

TL;DR: 提出了基于条件扩散模型的AC-Refiner框架，用于优化算术电路设计，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于设计空间大且物理约束复杂，优化算术电路（如加法器和乘法器）仍具有挑战性。

Method: 将算术电路合成重新定义为条件图像生成任务，利用条件扩散模型生成高质量设计。

Result: 实验结果表明，AC-Refiner生成的电路设计在Pareto最优性上优于现有方法。

Conclusion: AC-Refiner为算术电路优化提供了一种高效的新方法，并在实际应用中验证了其性能优势。

Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental
components of digital systems, directly impacting the performance, power
efficiency, and area footprint. However, optimizing these circuits remains
challenging due to the vast design space and complex physical constraints.
While recent deep learning-based approaches have shown promise, they struggle
to consistently explore high-potential design variants, limiting their
optimization efficiency. To address this challenge, we propose AC-Refiner, a
novel arithmetic circuit optimization framework leveraging conditional
diffusion models. Our key insight is to reframe arithmetic circuit synthesis as
a conditional image generation task. By carefully conditioning the denoising
diffusion process on target quality-of-results (QoRs), AC-Refiner consistently
produces high-quality circuit designs. Furthermore, the explored designs are
used to fine-tune the diffusion model, which focuses the exploration near the
Pareto frontier. Experimental results demonstrate that AC-Refiner generates
designs with superior Pareto optimality, outperforming state-of-the-art
baselines. The performance gain is further validated by integrating AC-Refiner
into practical applications.

</details>


### [65] [Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure](https://arxiv.org/abs/2507.02654)
*Rui Xie,Asad Ul Haq,Yunhua Fang,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 研究提出了一种系统级方法，通过取消片上ECC并将错误管理转移到内存控制器，来降低HBM成本。采用域特定的ECC框架，结合大码字Reed-Solomon纠正与轻量级CRC检测，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: HBM的高成本部分源于严格的片上可靠性要求，限制了其在AI工作负载中的可扩展部署。

Method: 引入域特定的ECC框架，结合大码字Reed-Solomon纠正与轻量级CRC检测，并采用差异校验更新以减少写入放大。

Result: 即使在HBM原始误码率高达1e-3的情况下，系统仍能保持78%的吞吐量和97%的模型准确性。

Conclusion: 通过将可靠性作为可调系统参数而非固定硬件约束，为低成本、高性能的HBM部署提供了新途径。

Abstract: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy
efficiency for AI workloads, but its high cost per bit, driven in part by
stringent on-die reliability requirements, poses a growing barrier to scalable
deployment. This work explores a system-level approach to cost reduction by
eliminating on-die ECC and shifting all fault management to the memory
controller. We introduce a domain-specific ECC framework combining
large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC
detection, differential parity updates to mitigate write amplification, and
tunable protection based on data importance. Our evaluation using LLM inference
workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the
system retains over 78\% of throughput and 97\% of model accuracy compared with
systems equipped with ideal error-free HBM. By treating reliability as a
tunable system parameter rather than a fixed hardware constraint, our design
opens a new path toward low-cost, high-performance HBM deployment in AI
infrastructure.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [66] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 论文提出了一种名为HyFairCRS的新框架，旨在通过超图对比多兴趣学习，解决动态对话推荐系统中的不公平问题，提升多样性公平性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统在静态或离线场景中能够改善公平性，但在动态交互的环境（如对话推荐系统）中，不公平问题会随时间加剧，导致马太效应、过滤泡沫等问题。

Method: HyFairCRS利用对比学习构建多样化的超图以捕捉广泛的用户兴趣，并在对话中动态生成公平的推荐结果。

Result: 在两种对话推荐数据集上的实验表明，HyFairCRS实现了最先进的性能，同时有效缓解了不公平问题。

Conclusion: HyFairCRS为动态对话推荐系统中的公平性问题提供了有效的解决方案，未来可进一步扩展到其他交互场景。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [67] [Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems](https://arxiv.org/abs/2507.02464)
*Craig S Wright*

Main category: cs.GT

TL;DR: 本文提出了一种基于自动机理论和经济激励的框架，将CAP定理的三难问题重新定义为约束优化问题，并通过博弈论机制扩展了传统CAP的限制。


<details>
  <summary>Details</summary>
Motivation: CAP定理提出了分布式系统在一致性、可用性和分区容忍性之间的权衡问题。本文旨在通过经济和博弈论机制，优化这一权衡。

Method: 通过将分布式系统建模为分区感知的状态机，并嵌入经济激励层，结合博弈论机制定义全局过渡语义。

Result: 研究表明，在有限的ε范围内，可用性和一致性可以同时保留，扩展了传统CAP理论。

Conclusion: 通过引入经济激励和博弈论方法，可以优化CAP定理的权衡，为分布式系统设计提供新的理论支持。

Abstract: The CAP theorem asserts a trilemma between consistency, availability, and
partition tolerance. This paper introduces a rigorous automata-theoretic and
economically grounded framework that reframes the CAP trade-off as a constraint
optimization problem. We model distributed systems as partition-aware state
machines and embed economic incentive layers to stabilize consensus behavior
across adversarially partitioned networks. By incorporating game-theoretic
mechanisms into the global transition semantics, we define provable bounds on
convergence, liveness, and correctness. Our results demonstrate that
availability and consistency can be simultaneously preserved within bounded
epsilon margins, effectively extending the classical CAP limits through formal
economic control.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [68] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 生成式AI工具（如ChatGPT和Codex）正在改变计算机科学教育，提供编程辅助和创新教学方法，但也带来学术诚信和过度依赖等挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何提升计算机科学教育，以及如何应对其带来的挑战。

Method: 分析AI在教育中的应用机会与问题，提出课程整合和评估建议。

Result: 提出一系列政策建议，以平衡AI的潜力与教育严谨性。

Conclusion: 生成式AI为计算机科学教育带来巨大机遇，但需谨慎应对挑战以保持教育质量。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的解决方案，用于检测印度多种作物中的多种疾病，通过统一数据集和模型训练，显著提升了检测准确率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 印度作为农业大国，作物因病害和环境压力损失严重，早期准确检测对提高产量和保障粮食安全至关重要。

Method: 创建包含17种作物和34种病害的统一数据集，训练深度学习模型，并超越现有技术。

Result: 模型在统一数据集上的检测准确率达到99%，比现有技术（覆盖14种作物和26种病害）高出7%。

Conclusion: 该解决方案通过扩展可检测作物和病害种类，为印度农民提供了更优的工具，助力农业可持续发展。

Abstract: India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [70] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种高效且紧凑的网格表示方法，通过自回归方式生成多边形网格，减少了50%的冗余，并实现约22%的最先进压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有的网格标记化方法通常产生重复顶点标记，浪费网络能力，因此需要一种更高效的标记方法。

Method: Mesh Silksong通过每个顶点只访问一次的标记化方法，减少冗余，并生成具有优越几何特性的多边形网格。

Result: 实验结果表明，该方法不仅能生成复杂的网格，还能显著提升几何完整性。

Conclusion: Mesh Silksong在网格表示和生成中表现出高效性和优越的几何性能，适用于实际应用。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [71] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 介绍了HyperGaussians,一种基于3D Gaussian Splatting的新型扩展,用于高质量可动画3D人脸建模,解决现有方法在非线性变形和细节处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯喷涂法在处理静态人脸方面表现优异,但在可动画化人脸的复杂变形和细节渲染上仍有不足,亟需更高效的建模方法。

Method: 提出高维多变量高斯(HyperGaussians)表示,通过逆协方差技巧提升计算效率,并与现有模型FlashAvatar结合。

Result: 在4个数据集的19名受试者上验证,HyperGaussians在数值和视觉表现上均优于3DGS,尤其在高频细节(如眼镜、牙齿、动态表情)上更出色。

Conclusion: HyperGaussians为可动画人脸建模提供了更高效的表示方法,显著提升了渲染质量。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [72] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 论文提出了一种自蒸馏方法，扩展视频到音频（V2A）模型以处理电影语言场景，显著提升了在部分可见性条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前V2A生成方法忽视电影语言，导致在部分可见目标场景中性能下降，需解决这一挑战。

Method: 采用自蒸馏方法，通过模拟电影语言变化，训练学生对齐视频特征与音视对应关系，从而捕捉部分视觉信息与声音的关联。

Result: 方法在部分可见性场景下显著提升，同时在大规模V2A数据集VGGSound上也表现更优。

Conclusion: 自蒸馏方法有效提升了V2A模型在电影语言场景中的性能，尤其是部分可见条件下的表现。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [73] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种新颖的流水线，将室内环境的RGB-D扫描转换为紧凑、逼真且可交互的3D虚拟副本，适用于AR/VR、游戏和机器人等领域。


<details>
  <summary>Details</summary>
Motivation: 旨在将真实场景准确重建为交互式虚拟环境，支持高质量渲染和物理交互。

Method: 通过场景理解、3D模型检索、材料绘画和物理引擎集成实现重建。

Result: 生成的场景紧凑、可编辑，且兼容标准图形流水线，检索模块在Scan2CAD基准上表现优异。

Conclusion: LiteReality在真实扫描和公共数据集上验证了其高效性和实用性。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [74] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 机器人通过部署ANN在FPGA上加速检测算法，减少任务执行时间，其中MobileNet v1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决机器人因检测算法速度慢导致任务执行时间延长的问题，利用FPGA提升检测效率。

Method: 使用FINN架构在FPGA的PL上部署三种量化ANN模型，包括MobileNet v1、CNV，并在RG2C数据集上训练。

Result: MobileNet v1达到98%的成功率和6611 FPS的推理速度，证明FPGA可加速ANN。

Conclusion: FPGA能有效加速ANN，适用于注意力机制，提升机器人任务效率。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [75] [A formal specification of the desired software behaviour of the Princess Marijke lock complex](https://arxiv.org/abs/2507.02721)
*Jan Friso Groote,Matthias Volk*

Main category: eess.SY

TL;DR: 本文对荷兰Princess Marijke船闸的软件控制进行了精确的形式化描述，并通过模型检验验证了53项软件需求的正确性。


<details>
  <summary>Details</summary>
Motivation: 确保船闸的安全控制对洪水和船舶运营至关重要。

Method: 使用mCRL2代码（少于400行）进行形式化描述，并通过模型检验验证需求。

Result: 53项软件需求被验证为正确，形式化描述的行为无误。

Conclusion: 该形式化描述可作为船闸软件构造的蓝图，确保其正确性和可靠性。

Abstract: The Princess Marijke lock complex is a large lock and water-protection
installation in the Netherlands between the river Rhine and the
Amsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of
Amsterdam. The lock complex consists of two independent locks and a moveable
flood-protection barrier. Ensuring safe control of the lock complex is of
utmost importance to guarantee both flood-protection and reliable ship
operations. This paper gives a precise, formal description of the software
control of the lock complex in less than 400 lines of mCRL2 code. This
description can act as a blueprint on how the software of this lock complex
needs to be constructed. Moreover, using model checking, 53 software
requirements are shown to be valid, ensuring that the formal description of the
behaviour is correct with regard to these properties and is unlikely to contain
mistakes and oversights.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 扩展可微分布尔逻辑网络（DBNs）的可训练互联结构，并提出两阶段剪枝方法以优化模型大小和性能。


<details>
  <summary>Details</summary>
Motivation: 提升DBNs的可扩展性和效率，使其适应更宽的输入层，并通过剪枝减少模型冗余。

Method: 引入可训练的互联结构并开发两阶段剪枝：基于SAT的逻辑等价剪枝和数据驱动的相似性剪枝。

Result: 扩展后的DBNs在保持高精度的同时实现了更好的压缩-准确率权衡。

Conclusion: 该方法成功提升了DBNs的扩展性和效率，同时通过剪枝优化了模型性能。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [77] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 该论文总结了Transformer模型在EEG解码中的最新应用，包括其基础原理、混合架构及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 深入探讨Transformer模型在EEG信号解码中的应用，以推动脑机接口技术的发展。

Method: 通过综述文献，分析Transformer的基础原理、混合架构及定制化结构在EEG处理中的进展。

Result: 总结了Transformer在EEG解码中的优势、创新方法及当前研究的局限性。

Conclusion: 论文为未来Transformer在EEG解码中的应用提供了方向，并强调了进一步研究的必要性。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [78] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 摘要提出了一种通过优化的预处理和深度学习技术显著提高跨被试运动想象分类性能的新方法，并在多个数据集中验证了其优于现有技术的表现。


<details>
  <summary>Details</summary>
Motivation: 脑机接口中的跨被试运动想象分类因个体EEG模式差异大而具有挑战性，限制了无需校准的脑机接口的实际应用。

Method: 方法包括对STFT变换的EEG数据进行直接分类、优化STFT参数，以及在训练CNN时采用平衡批处理策略。

Result: 在多个基准数据集上的分类准确率显著提升（IV-1: 67.60%，IV-2A: 65.96%，IV-2B: 80.22%）。

Conclusion: 该方法为无需校准的运动想象分类设定了新基准，并贡献了一个开放数据集以推动进一步研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [79] [DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems](https://arxiv.org/abs/2507.02400)
*Maximilian Zipfl,Pascal Zwick,Patrick Schulz,Marc Rene Zofka,Albert Schotschneider,Helen Gremmelmaier,Nikolai Polley,Ferdinand Mütsch,Kevin Simon,Fabian Gottselig,Michael Frey,Sergio Marschall,Akim Stark,Maximilian Müller,Marek Wehmer,Mihai Kocsis,Dominic Waldenmayer,Florian Schnepf,Erik Heinrich,Sabrina Pletz,Matthias Kölle,Karin Langbein-Euchner,Alexander Viehl,Raoul Zöllner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 该论文描述了一种基于德国TAF-BW测试区的数字孪生技术，通过结合基础设施摄像头和车载LiDAR传感器生成真实数据，并实现了交通信号的优化和通信安全场景的模拟。


<details>
  <summary>Details</summary>
Motivation: 随着数字化的普及，未来的移动性将高度依赖于数字孪生技术，以实现现实与虚拟环境的双向连接，从而优化交通管理和提升安全性。

Method: 通过智能基础设施的摄像头和测试车辆上的LiDAR传感器提取对象列表，生成真实数据，并设计了一个开放的仿真框架。

Result: 成功开发了一个公开可用的数字孪生框架，验证了其在交通信号优化和通信安全模拟中的应用。

Conclusion: 数字孪生技术为未来的智能交通管理提供了高效的工具，其公开的框架有助于进一步的研究和应用。

Abstract: In the future, mobility will be strongly shaped by the increasing use of
digitalization. Not only will individual road users be highly interconnected,
but also the road and associated infrastructure. At that point, a Digital Twin
becomes particularly appealing because, unlike a basic simulation, it offers a
continuous, bilateral connection linking the real and virtual environments.
This paper describes the digital reconstruction used to develop the Digital
Twin of the Test Area Autonomous Driving-Baden-W\"urttemberg (TAF-BW), Germany.
The TAF-BW offers a variety of different road sections, from high-traffic urban
intersections and tunnels to multilane motorways. The test area is equipped
with a comprehensive Vehicle-to-Everything (V2X) communication infrastructure
and multiple intelligent intersections equipped with camera sensors to
facilitate real-time traffic flow monitoring. The generation of authentic data
as input for the Digital Twin was achieved by extracting object lists at the
intersections. This process was facilitated by the combined utilization of
camera images from the intelligent infrastructure and LiDAR sensors mounted on
a test vehicle. Using a unified interface, recordings from real-world
detections of traffic participants can be resimulated. Additionally, the
simulation framework's design and the reconstruction process is discussed. The
resulting framework is made publicly available for download and utilization at:
https://digit4taf-bw.fzi.de The demonstration uses two case studies to
illustrate the application of the digital twin and its interfaces: the analysis
of traffic signal systems to optimize traffic flow and the simulation of
security-related scenarios in the communications sector.

</details>


### [80] [MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints](https://arxiv.org/abs/2507.02438)
*Shivam Chaubey,Francesco Verdoja,Shankar Deka,Ville Kyrki*

Main category: cs.RO

TL;DR: 提出了一个基于约束最优控制问题的共享控制框架，通过离线计算的控制不变集确保可行性和安全性，并在大规模用户研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有共享控制方法在可行性、扩展性或安全性方面存在不足，用户输入的不可预测性进一步加剧了这些问题。

Method: 基于约束最优控制问题，结合离线计算的Control Invariant Set，在线计算控制动作以确保约束满足和最小干预用户意图。

Result: 在大规模用户研究中，该方法在任务负载、信任、感知控制及性能方面均显示出显著改善，同时保证了安全性和用户意图。

Conclusion: 该框架为共享控制提供了一种可行的解决方案，能够适应复杂约束并提升用户体验。

Abstract: Shared control combines human intention with autonomous decision-making, from
low-level safety overrides to high-level task guidance, enabling systems that
adapt to users while ensuring safety and performance. This enhances task
effectiveness and user experience across domains such as assistive robotics,
teleoperation, and autonomous driving. However, existing shared control
methods, based on e.g. Model Predictive Control, Control Barrier Functions, or
learning-based control, struggle with feasibility, scalability, or safety
guarantees, particularly since the user input is unpredictable.
  To address these challenges, we propose an assistive controller framework
based on Constrained Optimal Control Problem that incorporates an
offline-computed Control Invariant Set, enabling online computation of control
actions that ensure feasibility, strict constraint satisfaction, and minimal
override of user intent. Moreover, the framework can accommodate structured
class of non-convex constraints, which are common in real-world scenarios. We
validate the approach through a large-scale user study with 66
participants--one of the most extensive in shared control research--using a
computer game environment to assess task load, trust, and perceived control, in
addition to performance. The results show consistent improvements across all
these aspects without compromising safety and user intent.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [81] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 电子支付平台每天处理数十亿交易，价值可能达数万亿。现有手动验证规则易受漏洞利用，缺乏系统性方法来确保规则稳健性，导致对系统性识别规则缺陷的需求增加。


<details>
  <summary>Details</summary>
Motivation: 高交易量和潜在巨大财务损失促使需要更稳健的验证机制，但现有手动规则缺乏系统性保障，易被恶意利用。

Method: 采用手动构建的验证规则，由领域专家开发，用于识别和审查生产环境中的交易。

Result: 现有规则由于缺乏系统性验证方法，依然容易被绕过或利用。

Conclusion: 需要系统性方法来识别和修复验证规则中的缺陷，以提升电子支付平台的安全性。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [82] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的物联网系统，用于监控餐厅披萨生产，结合实时温度与湿度数据，确保数据安全与防篡改。


<details>
  <summary>Details</summary>
Motivation: 旨在通过区块链和物联网技术提升披萨生产的透明度和效率，同时支持合规与审计需求。

Method: 利用树莓派处理传感器数据、拍摄图像并触发警报，结合智能合约实现数据管理。

Result: 实验表明系统优化了原料管理、减少浪费并提高了厨房效率。

Conclusion: 区块链与物联网的结合为食品生产提供了安全、高效的解决方案。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [83] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种上下文感知攻击数据生成器，用于生成高质量攻击数据，以解决实际测试中数据不足的问题，并通过IDS案例验证了数据的有效性。


<details>
  <summary>Details</summary>
Motivation: 联网汽车的数字化发展带来了安全风险，但实际测试中攻击数据的稀缺性限制了适应性入侵检测机制的研究与发展。

Method: 开发了一个上下文感知攻击数据生成器，能生成多种攻击类型的数据，并结合CAN消息解码和攻击强度调整，提高数据真实性和多样性。

Result: 生成的攻击数据在IDS案例中表现良好，验证了数据的有效性和一致性，展示了高效且可扩展的特性。

Conclusion: 该方法为生成高质量攻击数据提供了可行方案，并验证了其在IDS中的实用价值，为未来研究提供了参考。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [84] [Access Control Threatened by Quantum Entanglement](https://arxiv.org/abs/2507.02622)
*Zhicheng Zhang,Mingsheng Ying*

Main category: quant-ph

TL;DR: 该论文研究了量子计算机系统中的访问控制问题，揭示了将经典安全系统直接应用于量子环境时的安全漏洞，并提出新的量子访问控制模型。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展使得经典访问控制系统在量子环境中可能面临新的安全威胁，因此需要研究并设计适用于量子计算机的访问控制机制。

Method: 论文通过分析量子纠缠现象和Mermin不等式，提出了几种新的量子访问控制模型，并严格评估其安全性、灵活性和效率。

Result: 研究发现经典访问控制系统在量子环境中存在安全漏洞，而提出的量子访问控制模型能够有效抵御这种威胁。

Conclusion: 论文强调了量子计算对访问控制的潜在威胁，并提供了可行的解决方案，为未来量子计算机系统的安全设计奠定了基础。

Abstract: Access control is a cornerstone of computer security that prevents
unauthorised access to resources. In this paper, we study access control in
quantum computer systems. We present the first explicit scenario of a security
breach when a classically secure access control system is straightforwardly
adapted to the quantum setting. The breach is ultimately due to that quantum
mechanics allows the phenomenon of entanglement and violates Mermin inequality,
a multi-party variant of the celebrated Bell inequality. This reveals a threat
from quantum entanglement to access control if existing computer systems
integrate with quantum computing. To protect against such threat, we propose
several new models of quantum access control, and rigorously analyse their
security, flexibility and efficiency.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [85] [Optimising task allocation to balance business goals and worker well-being for financial service workforces](https://arxiv.org/abs/2507.01968)
*Chris Duckworth,Zlatko Zlatev,James Sciberras,Peter Hallett,Enrico Gerding*

Main category: q-fin.GN

TL;DR: 提出了一个考虑分析师福祉和业务目标的任务分配模型，使用遗传算法进行优化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融公司数据量大，任务压力导致资源挑战和风险，需兼顾效率和员工福祉。

Method: 使用遗传算法优化任务分配，考虑技能、经验和员工福祉目标。

Result: 模型优于基线启发式和当前实践，适用于单目标和多目标场景。

Conclusion: 该模型填补了现有模型未充分考虑员工福祉的空白，同时提升效率。

Abstract: Purpose: Financial service companies manage huge volumes of data which
requires timely error identification and resolution. The associated tasks to
resolve these errors frequently put financial analyst workforces under
significant pressure leading to resourcing challenges and increased business
risk. To address this challenge, we introduce a formal task allocation model
which considers both business orientated goals and analyst well-being.
  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to
allocate and schedule tasks to analysts. The proposed solution is able to
allocate tasks to analysts with appropriate skills and experience, while taking
into account staff well-being objectives.
  Findings: We demonstrate our GA model outperforms baseline heuristics,
current working practice, and is applicable to a range of single and
multi-objective real-world scenarios. We discuss the potential for
metaheuristics (such as GAs) to efficiently find sufficiently good allocations
which can provide recommendations for financial service managers in-the-loop.
  Originality: A key gap in existing allocation and scheduling models, is fully
considering worker well-being. This paper presents an allocation model which
explicitly optimises for well-being while still improving on current working
practice for efficiency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文探讨了高质量标注数据的稀缺性和标签变异性（LV）的重要性，尤其是人类标签变异性（HLV）。提出了一种将HLV整合到主动学习（AL）流程中的概念框架，并讨论了使用大型语言模型（LLM）作为标注者的可能性。


<details>
  <summary>Details</summary>
Motivation: 高质标签数据不足和标签多样性被忽视，导致现有标注框架和AL方法难以应对真实世界的复杂性。

Method: 提出分解标签变异性为信号（如HLV）和噪声（如标注错误），并设计HLV-aware AL框架，涵盖实例选择、标注者选择和标签表示。

Result: 框架为HLV-aware AL提供理论基础，更好地适应真实标注场景的复杂性。

Conclusion: HLV是重要信号，整合到AL中能优化标注效率，LLM作为标注者的潜力值得探索。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文探讨了信念修订领域的现状，指出现有方法缺乏分析，而过多依赖作为语法表征的公设。作者提出修订机制应具备可塑性、平等化和教条化等能力，并分析了不同修订机制的这些能力。


<details>
  <summary>Details</summary>
Motivation: 当前的信念修订领域过于依赖公设，而忽视了修订机制的实际能力。作者希望通过分析不同修订机制的能力，为更灵活的修订方法提供理论基础。

Method: 通过分析不同修订机制（如词典式、自然式、限制式等）的表现，评估它们是否具备可塑性、平等化、教条化等能力。

Result: 研究发现不同修订机制各有所长，有的具备某些能力而缺乏其他能力，揭示了修订方法的多样性。

Conclusion: 信念修订机制的能力分析比单纯依赖公设更适用于实际应用，未来的研究应进一步探索这些能力的实现和应用。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [88] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI代理的硬件设计验证方法，结合人类介入，实现动态、迭代和自我反思的验证流程，显著提升了覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路复杂度增加，传统硬件设计验证过程耗时且繁琐，而大型语言模型的出现为这一领域提供了新的解决方案。

Method: 采用AI代理结合人类介入（HITL）的动态、迭代和自我反思方法，进行端到端硬件设计和验证。

Result: 在五个开源设计上验证，覆盖率超过95%，同时验证时间减少，展示了优越的性能和适应性。

Conclusion: 该方法通过AI与人类的协作，显著提升了硬件设计验证的效率和覆盖范围。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [89] [Public perspectives on the design of fusion energy facilities](https://arxiv.org/abs/2507.02207)
*Nathan Kawamoto,Daniel Hoover,Jonathan Xie,Jacob Walters,Katie Snyder,Aditi Verma*

Main category: physics.soc-ph

TL;DR: 该论文探讨了参与式设计方法在聚变能源设施规划中的重要性，分析了社区与工程师合作设计的成果，强调了公众参与对技术发展和社会许可的作用。


<details>
  <summary>Details</summary>
Motivation: 随着聚变能源技术接近商业化应用，理解公众对未来聚变能源设施的看法对于获得社会许可至关重要，因为聚变设施可能比传统裂变反应堆更接近社区。

Method: 采用参与式设计方法，组织了一场工作坊，汇集了22名社区参与者和34名工程专业学生，共同设计聚变能源设施。

Result: 工作坊数据显示，设计价值观中“诚信”和“尊重”最受重视，决策标准中“经济利益”和“环境保护/安全”排名最高。参与者表现出积极的情绪，对聚变技术表现出更高的理解和好奇心。

Conclusion: 早期阶段的参与式设计有助于具体化公众的期望和担忧，提升对新兴技术的理解和兴趣，并为聚变设施的特定发展提供信息，同时促进社会许可的达成。

Abstract: As fusion energy technologies approach demonstration and commercial
deployment, understanding public perspectives on future fusion facilities will
be critical for achieving social license, especially because fusion energy
facilities, unlike large fission reactors, may be sited in closer proximity to
people and communities, due to distinct regulatory frameworks. In a departure
from the 'decide-announce-defend' approach typically used to site energy
infrastructure, we develop a participatory design methodology for
collaboratively designing fusion energy facilities with prospective host
communities. We present here our findings from a participatory design workshop
that brought together 22 community participants and 34 engineering students.
Our analysis of the textual and visual data from this workshop shows a range of
design values and decision-making criteria with 'integrity' and 'respect'
ranking highest among values and 'economic benefits' and 'environmental
protection/safety' ranking highest among decision-making criteria. Salient
design themes that emerge across facility concepts include connecting the
history and legacy of the community to the design of the facility, care for
workers, transparency and access to the facility, and health and safety of the
host community. Participants reported predominantly positive sentiments,
expressing joy and surprise as the workshop progressed from learning about
fusion to designing the hypothetical facility. Our findings suggest that
carrying out participatory design in the early stages of technology development
can invite and make concrete public hopes and concerns, improve understanding
of, and curiosity about, an emerging technology, build toward social license,
and inform context-specific development of fusion energy facilities.

</details>
