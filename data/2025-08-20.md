<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 13]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Comparative Study of Delta Parquet, Iceberg, and Hudi for Automotive Data Engineering Use Cases](https://arxiv.org/abs/2508.13396)
*Dinesh Eswararaj,Ajay Babu Nellipudi,Vandana Kollati*

Main category: cs.SE

TL;DR: 比较分析了Delta Parquet、Iceberg和Hudi三种数据湖仓库格式在汽车行业数据工程中的性能，包括查询效率、实时处理等，提供了选择建议。


<details>
  <summary>Details</summary>
Motivation: 汽车行业数据量巨大，需要高效处理延迟、可扩展性和一致性的挑战，研究目的是评估现代数据湖仓库格式的适用性。

Method: 使用真实汽车遥测数据，评估Delta Parquet、Iceberg和Hudi在建模、分区、CDC支持等方面的表现。

Result: Delta Parquet适合机器学习，Iceberg适合批处理，Hudi擅长实时处理。

Conclusion: 不同格式有各自的优势，需根据具体应用场景选择或组合使用。

Abstract: The automotive industry generates vast amounts of data from sensors,
telemetry, diagnostics, and real-time operations. Efficient data engineering is
critical to handle challenges of latency, scalability, and consistency. Modern
data lakehouse formats Delta Parquet, Apache Iceberg, and Apache Hudi offer
features such as ACID transactions, schema enforcement, and real-time
ingestion, combining the strengths of data lakes and warehouses to support
complex use cases. This study presents a comparative analysis of Delta Parquet,
Iceberg, and Hudi using real-world time-series automotive telemetry data with
fields such as vehicle ID, timestamp, location, and event metrics. The
evaluation considers modeling strategies, partitioning, CDC support, query
performance, scalability, data consistency, and ecosystem maturity. Key
findings show Delta Parquet provides strong ML readiness and governance,
Iceberg delivers high performance for batch analytics and cloud-native
workloads, while Hudi is optimized for real-time ingestion and incremental
processing. Each format exhibits tradeoffs in query efficiency, time-travel,
and update semantics. The study offers insights for selecting or combining
formats to support fleet management, predictive maintenance, and route
optimization. Using structured datasets and realistic queries, the results
provide practical guidance for scaling data pipelines and integrating machine
learning models in automotive applications.

</details>


### [2] [The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget](https://arxiv.org/abs/2508.13666)
*Dangfeng Pan,Zhensu Sun,Cenyuan Zhang,David Lo,Xiaoning Du*

Main category: cs.SE

TL;DR: 代码格式元素（如缩进和换行）对大型语言模型（LLM）的性能和效率影响研究，发现去除格式可显著减少输入token数量且不影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 研究代码格式化元素是否对LLM的处理效率至关重要，并探索通过去除非必要格式降低成本的可能性。

Method: 在四种编程语言（Java、Python、C++、C#）和十种LLM上进行大规模填充中间代码补全实验，分析格式去除对token数和性能的影响。

Result: LLM在去除格式后仍能保持性能，平均输入token减少24.5%，且通过提示和微调可进一步减少输出代码长度（最高36.1%）。

Conclusion: 去除代码格式是一种实用优化策略，显著提升LLM效率，同时开发了双向代码转换工具以平衡人类可读性和LLM效率。

Abstract: Source code is usually formatted with elements like indentation and newlines
to improve readability for human developers. However, these visual aids do not
seem to be beneficial for large language models (LLMs) in the same way since
the code is processed as a linear sequence of tokens. Furthermore, these
additional tokens can lead to increased computational costs and longer response
times for LLMs. If such formatting elements are non-essential to LLMs, we can
reduce such costs by removing them from the code. To figure out the role played
by formatting elements, we conduct a comprehensive empirical study to evaluate
the impact of code formatting on LLM performance and efficiency. Through
large-scale experiments on Fill-in-the-Middle Code Completion tasks across four
programming languages (Java, Python, C++, C\#) and ten LLMs-including both
commercial and open-source models-we systematically analyze token count and
performance when formatting elements are removed. Key findings indicate that
LLMs can maintain performance across formatted code and unformatted code,
achieving an average input token reduction of 24.5\% with negligible output
token reductions. This makes code format removal a practical optimization
strategy for improving LLM efficiency. Further exploration reveals that both
prompting and fine-tuning LLMs can lead to significant reductions (up to
36.1\%) in output code length without compromising correctness. To facilitate
practical applications, we develop a bidirectional code transformation tool for
format processing, which can be seamlessly integrated into existing LLM
inference workflows, ensuring both human readability and LLM efficiency.

</details>


### [3] [COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models](https://arxiv.org/abs/2508.13757)
*James Meaden,Michał Jarosz,Piotr Jodłowski,Grigori Melnik*

Main category: cs.SE

TL;DR: COMPASS 是一个多维度评估代码生成的框架，关注正确性、效率和代码质量，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试主要关注功能性正确性，而忽略了算法效率和代码质量，这在实际编程中至关重要。

Method: 引入 COMPASS，基于 50 个真实编程竞赛问题和 393,150 份提交，使用行业标准工具评估代码的效率和质量。

Result: 评估发现，高正确性模型不一定能生成高效或可维护的代码。

Conclusion: 需要多维度评估代码生成模型，COMPASS 为未来研究提供了方向，以实现更鲁棒和可靠的 AI 系统。

Abstract: Current code generation benchmarks focus primarily on functional correctness
while overlooking two critical aspects of real-world programming: algorithmic
efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional
Programming ASSessment), a comprehensive evaluation framework that assesses
code generation across three dimensions: correctness, efficiency, and quality.
COMPASS consists of 50 competitive programming problems from real Codility
competitions, providing authentic human baselines from 393,150 submissions.
Unlike existing benchmarks that treat algorithmically inefficient solutions
identically to optimal ones provided they pass test cases, COMPASS
systematically evaluates runtime efficiency and code quality using
industry-standard analysis tools. Our evaluation of three leading
reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and
OpenAI O4-Mini-High, reveals that models achieving high correctness scores do
not necessarily produce efficient algorithms or maintainable code. These
findings highlight the importance of evaluating more than just correctness to
truly understand the real-world capabilities of code generation models. COMPASS
serves as a guiding framework, charting a path for future research toward AI
systems that are robust, reliable, and ready for production use.

</details>


### [4] [Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API](https://arxiv.org/abs/2508.13774)
*Peer Trilcke,Ingo Börner,Henny Sluyter-Gäthje,Daniil Skorinkin,Frank Fischer,Carsten Milling*

Main category: cs.SE

TL;DR: 该论文介绍了DraCor的MCP服务器实现与评估，使LLM能自主与DraCor API交互，通过实验探究了LLM的工具选择与应用，强调了文档工程优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLM如何自主使用工具，并为计算文学研究和数字人文学科的基础设施提供支持。

Method: 采用定性方法，通过系统观察LLM的提示行为，评估工具正确性、调用效率和可靠性。

Result: 实验表明文档工程对优化LLM与工具交互至关重要，同时展示了AI在计算文学研究中的潜力。

Conclusion: 研究突出了为可靠数字人文学科基础设施开发的重要性，以及文档工程在LLM工具使用中的关键作用。

Abstract: This paper reports on the implementation and evaluation of a Model Context
Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to
autonomously interact with the DraCor API. We conducted experiments focusing on
tool selection and application by the LLM, employing a qualitative approach
that includes systematic observation of prompts to understand how LLMs behave
when using MCP tools, evaluating "Tool Correctness", "Tool-Calling Efficiency",
and "Tool-Use Reliability". Our findings highlight the importance of "Docstring
Engineering", defined as reflexively crafting tool documentation to optimize
LLM-tool interaction. Our experiments demonstrate both the promise of agentic
AI for research in Computational Literary Studies and the essential
infrastructure development needs for reliable Digital Humanities
infrastructures.

</details>


### [5] [Structural and Connectivity Patterns in the Maven Central Software Dependency Network](https://arxiv.org/abs/2508.13819)
*Daniel Ogenrwot,John Businge,Shaikh Arifuzzaman*

Main category: cs.SE

TL;DR: 论文分析了Maven Central生态系统，揭示其高度互联、无标度和小世界的拓扑结构，指出核心基础设施库既是高效重用的关键，也可能带来系统性风险。


<details>
  <summary>Details</summary>
Motivation: 研究大型软件生态系统的结构特征和连接模式，以增强软件重用、提高生态韧性并降低安全风险。

Method: 使用Goblin框架提取Maven Central的依赖图样本（5000个高连接度的库），通过BFS扩展获取1.3百万节点和20.9百万边的图，并计算图论指标（如度分布、中心性等）。

Result: Maven Central呈现无标度和小世界特性，少数核心基础设施库（如测试框架和通用工具库）支撑大部分项目，但这些中心节点也是系统性风险的源头。

Conclusion: 核心基础设施库虽促进高效集成，但也可能导致广泛的连锁风险，需关注其安全性和韧性。

Abstract: Understanding the structural characteristics and connectivity patterns of
large-scale software ecosystems is critical for enhancing software reuse,
improving ecosystem resilience, and mitigating security risks. In this paper,
we investigate the Maven Central ecosystem, one of the largest repositories of
Java libraries, by applying network science techniques to its dependency graph.
Leveraging the Goblin framework, we extracted a sample consisting of the top
5,000 highly connected artifacts based on their degree centrality and then
performed breadth-first search (BFS) expansion from each selected artifact as a
seed node, traversing the graph outward to capture all libraries and releases
reachable those seed nodes. This sampling strategy captured the immediate
structural context surrounding these libraries resulted in a curated graph
comprising of 1.3 million nodes and 20.9 million edges. We conducted a
comprehensive analysis of this graph, computing degree distributions,
betweenness centrality, PageRank centrality, and connected components
graph-theoretic metrics. Our results reveal that Maven Central exhibits a
highly interconnected, scale-free, and small-world topology, characterized by a
small number of infrastructural hubs that support the majority of projects.
Further analysis using PageRank and betweenness centrality shows that these
hubs predominantly consist of core ecosystem infrastructure, including testing
frameworks and general-purpose utility libraries. While these hubs facilitate
efficient software reuse and integration, they also pose systemic risks;
failures or vulnerabilities affecting these critical nodes can have widespread
and cascading impacts throughout the ecosystem.

</details>


### [6] [Tight Inter-Core Cache Contention Analysis for WCET Estimation on Multicore Systems](https://arxiv.org/abs/2508.13863)
*Shuai Zhao,Jieyu Jiang,Shenlin Cai,Yaowei Liang,Chen Jie,Yinjie Fang,Wei Zhang,Guoquan Zhang,Yaoyao Gu,Xiang Xiao,Wei Qin,Xiangzhen Ouyang,Wanli Chang*

Main category: cs.SE

TL;DR: 提出了一种新的多核架构下的WCET分析方法，通过细粒度的缓存争用分析减少了对缓存冲突的高估，显著降低了WCET估计值。


<details>
  <summary>Details</summary>
Motivation: 多核架构中共享缓存的复杂访问导致WCET估计困难，现有方法因未考虑实际缓存状态和访问次数而高估缓存冲突。

Method: 基于任务中程序区域的顺序识别可能的远程访问影响，构建细粒度分析以计算本地和远程块的缓存未命中次数，并通过动态编程获取全局干扰。

Result: 实验显示，新方法平均减少了52.31%的核间缓存干扰和8.94%的WCET估计，计算开销无明显增加。

Conclusion: 新方法在多核WCET估计中显著提升了准确性，且保持了计算效率。

Abstract: WCET (Worst-Case Execution Time) estimation on multicore architecture is
particularly challenging mainly due to the complex accesses over cache shared
by multiple cores. Existing analysis identifies possible contentions between
parallel tasks by leveraging the partial order of the tasks or their program
regions. Unfortunately, they overestimate the number of cache misses caused by
a remote block access without considering the actual cache state and the number
of accesses. This paper reports a new analysis for inter-core cache contention.
Based on the order of program regions in a task, we first identify memory
references that could be affected if a remote access occurs in a region.
Afterwards, a fine-grained contention analysis is constructed that computes the
number of cache misses based on the access quantity of local and remote blocks.
We demonstrate that the overall inter-core cache interference of a task can be
obtained via dynamic programming. Experiments show that compared to existing
methods, the proposed analysis reduces inter-core cache interference and WCET
estimations by 52.31% and 8.94% on average, without significantly increasing
computation overhead.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [Reactive Semantics for User Interface Description Languages](https://arxiv.org/abs/2508.13610)
*Basile Pesin,Celia Picard,Cyril Allignol*

Main category: cs.PL

TL;DR: 本文提出了一种针对反应式用户界面描述语言（UIDL）Smalite的指称语义模型，旨在填补其形式化和验证的空白。


<details>
  <summary>Details</summary>
Motivation: 目前，虽然UIDL广泛用于安全关键的GUI开发，但其形式化和验证研究不足，亟需解决。

Method: 提出了Smalite的指称语义模型，其表达能力足以编码更复杂语言的结构。

Result: 为UIDL的形式化验证提供了初步基础。

Conclusion: 该研究可作为未来开发形式化验证UIDL编译器的基石。

Abstract: User Interface Description Languages (UIDLs) are high-level languages that
facilitate the development of Human-Machine Interfaces, such as Graphical User
Interface (GUI) applications. They usually provide first-class primitives to
specify how the program reacts to an external event (user input, network
message), and how data flows through the program. Although these
domain-specific languages are now widely used to implement safety-critical
GUIs, little work has been invested in their formalization and verification.
  In this paper, we propose a denotational semantic model for a core reactive
UIDL, Smalite, which we argue is expressive enough to encode constructs from
more realistic languages. This preliminary work may be used as a stepping stone
to produce a formally verified compiler for UIDLs.

</details>


### [8] [Bisimilarity and Simulatability of Processes Parameterized by Join Interactions](https://arxiv.org/abs/2508.13611)
*Clemens Grabmayer,Maurizio Murgia*

Main category: cs.PL

TL;DR: 本文探讨了参数化双相似性的自然弱化版本——无需限制的联合互动双相似性，并展示了其在确定性环境下与Larsen原概念的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为了探索参数化双相似性的弱化形式，即在无需限制的联合互动环境下对过程行为的比较能力。

Method: 通过定义联合互动参数化双相似性，并与Larsen的原始概念进行对比，同时扩展到模拟性和模态逻辑特征化。

Result: 结果表明，联合互动参数化双相似性在确定性环境下与Larsen的概念一致，但在一般情况下是一种更粗糙的等价关系。

Conclusion: 本文通过弱化参数化双相似性，扩展了其应用范围，并提出了未来的研究方向。

Abstract: Departing from Larsen's concept of parameterized bisimilarity of processes
with respect to interaction with environments, we start an exploration of its
natural weakening: bisimilarity of unrestricted join interactions with
environments. Parameterized bisimilarity relates processes p and q with respect
to an environment e if p and q behave bi-similarly while joining --
respectively the same -- transitions from e. The weakened variant relates
processes p and q with respect to environment e if the join-interaction
processes p & e and q & e of p and q with e are bisimilar. (Hereby join
interactions r & f facilitate a step with label a to r' & f' if and only if r
and f permit a-steps to r' and f' , respectively.) Join-interaction
parameterized (ji-parameterized) bisimilarity coincides with parameterized
bisimilarity for deterministic environments, but that it is a coarser
equivalence in general. We explain how Larsen's concept can be recovered from
ji-parameterized bisimilarity by 'determinizing' interactions. We show that by
adaptation to simulatability (simulation preorder) the same concept arises:
parameterized simulatability coincides with ji-parameterized simulatability.
For the discrimination preorder of (ji-)parameterized simulatability on
environments we obtain the same result as Larsen did for parameterized
bisimilarity. Also, we give a modal-logic characterization of
(ji-)parameterized simulatability. Finally we gather open problems, and provide
an outlook on our current related work.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Multi-Metric Algorithmic Complexity: Beyond Asymptotic Analysis](https://arxiv.org/abs/2508.13249)
*Sergii Kavun*

Main category: cs.PF

TL;DR: 提出一种加权操作复杂度模型，通过多维成本评估算法效率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统算法分析忽略了现代处理器上不同类型操作的时间、能耗和成本差异，需改进。

Method: 设计加权操作复杂度模型，结合计算量、能耗、碳足迹和费用，支持用户自定义优先级。

Result: 验证显示模型与实测数据高度相关（r>0.9），在多目标场景中优于基准方法。

Conclusion: 该模型为算法比较提供实用、架构感知的多维度评估工具，补充现有理论。

Abstract: Traditional algorithm analysis treats all basic operations as equally costly,
which hides significant differences in time, energy consumption, and cost
between different types of computations on modern processors. We propose a
weighted-operation complexity model that assigns realistic cost values to
different instruction types across multiple dimensions: computational effort,
energy usage, carbon footprint, and monetary cost. The model computes overall
efficiency scores based on user-defined priorities and can be applied through
automated code analysis or integrated with performance measurement tools. This
approach complements existing theoretical models by enabling practical,
architecture-aware algorithm comparisons that account for performance,
sustainability, and economic factors. We demonstrate an open-source
implementation that analyzes code, estimates multi-dimensional costs, and
provides efficiency recommendations across various algorithms. We address two
research questions: (RQ1) Can a multi-metric model predict time/energy with
high accuracy across architectures? (RQ2) How does it compare to baselines like
Big-O, ICE, and EVM gas? Validation shows strong correlations (\r{ho}>0.9) with
measured data, outperforming baselines in multi-objective scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [10] [Fundamentals of Next-generation Network Planning](https://arxiv.org/abs/2508.13469)
*M. Umar Khan*

Main category: cs.NI

TL;DR: 本文研究了5G网络规划的基础方法，通过平衡覆盖与容量的权衡，结合新无线电建模和数据驱动策略，以最小化部署成本并降低每比特成本。


<details>
  <summary>Details</summary>
Motivation: 5G网络需要支持多样化的服务需求，包括高数据速率和低延迟。考虑到其关键任务性质，运营商如何保证峰值数据速率和体验质量（QoE）成为一大挑战。

Method: 研究利用4G网络的广泛数据和众包LTE基础设施洞察，识别高流量5G部署区域（5GDA），并通过无线电网络维度（RND）平衡覆盖与容量，结合新无线电（NR）建模和数据驱动策略。

Result: 提出的方法能够有效平衡5G网络的覆盖与容量，降低部署成本和每比特成本。

Conclusion: 通过结合NR建模和数据驱动策略，5G网络规划可以实现更高效的资源分配和成本优化。

Abstract: The fifth-generation (5G) of cellular communications is expected to be
deployed in the next years to support a wide range of services with different
demands of peak data rates, latency and quality of experience (QoE). To support
higher data rates and latency requirements third-generation partnership project
(3GPP) has introduced numerology and bandwidth parts (BWPs), via new radio (NR)
for service-tailored resource allocation. Legacy 4G networks have generated
extensive data, which combined with crowd-sourced LTE infrastructure insights,
enables identification of high-traffic 5G deployment area (5GDA) for planning
new services. Given the mission-critical nature of 5G services, QoE is a big
challenge for MNOs to guarantee peak data rates for a defined percentage of
time. This work studies the fundamentals of 5G network planning methods that
reconciles coverage-capacity trade-offs through balanced radio network
dimensioning (RND), leveraging pragmatic NR modeling, and data-driven
strategies to minimize deployment costs and reduce cost-per-bit.

</details>


### [11] [Electromagnetic Signal Modulation Recognition based on Subgraph Embedding Learning](https://arxiv.org/abs/2508.13474)
*Bojun Zhang*

Main category: cs.NI

TL;DR: 该论文提出了一种基于子图嵌入学习（SEL）的新型自动调制识别（AMR）算法SEL-AMR，旨在解决现有深度学习中固定数据维度的问题，使其能够适应动态变化的信道和系统。


<details>
  <summary>Details</summary>
Motivation: 由于传统深度学习AMR算法仅适用于特定信道和系统，缺乏灵活性，作者提出了SEL-AMR，以应对动态信道和系统的挑战。

Method: 通过将通信系统视为子图，利用样本间关系平滑噪声和信道差异的影响，提取鲁棒特征。采用5个公共真实数据集和少量模拟数据进行验证。

Result: 实验显示，SEL-AMR在动态环境下表现优异，宏观平均识别精度和准确率分别提升20%和30%，优于现有技术。

Conclusion: SEL-AMR通过子图嵌入学习实现了对动态信道和系统的适应，显著提升了AMR的性能，具有广泛的应用潜力。

Abstract: Automatic Modulation Recognition (AMR) detects
  modulation schemes of received signals for further processing
  of signals without any priori information, which is critically
  important for civil spectrum regulation, information countermea sures, and
communication security. Due to the powerful feature
  extraction and classification capabilities of Deep Learning (DL),
  DL-based AMR algorithms have achieved excellent performance
  gains compared with traditional modulation detection algorithms.
  However, all existing DL-based AMR algorithms, to the best of
  our knowledge, are designed for specific channels and systems,
  because data dimension of the used training dataset is fixed. To
  this end, we takes the first step to propose a Subgraph Embedding
  Learning (SEL) structure to address the classical AMR problem,
  and the proposed algorithm is called SEL-AMR. Our algorithm
  treats the communication system as a subgraph and uses the
  relationship between samples to smooth the effects brought by
  noise and different channels to extract robust features. Thus,
  the proposed SEL-AMR algorithm can adapt to any dynamic
  channels and systems. We use 5 public real datasets and a small
  amount of simulation data to evaluate our SEL-AMR algorithm.
  Experimental results reveal that SEL-AMR can well adapt to
  different channels and systems, and always outperforms the state of-the-art
algorithms by improving up to 20% macro-average
  recognition precision and 30% recognition accuracy.

</details>


### [12] [CountingStars: Low-overhead Network-wide Measurement in LEO Mega-constellation Networks](https://arxiv.org/abs/2508.13512)
*Xiyuan Liu,Guano Liu,Xiucheng Tian,Wenting Wei*

Main category: cs.NI

TL;DR: 论文提出了CountingStars架构，通过数字孪生系统和端口聚合数据结构解决LEO卫星网络中PBLB带来的内存膨胀和哈希碰撞问题。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道（LEO）巨型卫星星座中高动态网络拓扑导致的频繁服务中断问题，尤其是由基于包的负载均衡（PBLB）带来的内存膨胀和哈希碰撞挑战。

Method: 提出CountingStars，包括地面控制器中的数字孪生系统预测未来拓扑并生成无碰撞哈希种子，以及卫星上的端口聚合数据结构高效更新流标识符和多端口计数器。

Result: 仿真显示内存使用减少70%，测量误差降低90%，FPGA实现验证了其实时部署潜力。

Conclusion: CountingStars有效解决了PBLB带来的问题，为LEO卫星网络的稳定性和高效性提供了可行方案。

Abstract: The high mobility of satellites in Low Earth Orbit (LEO) mega-constellations
induces a highly dynamic network topology, leading to many problems like
frequent service disruptions. To mitigate this, Packet-based Load Balancing
(PBLB) is employed. However, this paradigm shift introduces two critical
challenges for network measurement stemming from the requirement for port-level
granularity: memory inflation and severe hash collisions. To tackle these
challenges, we propose CountingStars, a low-overhead network-wide measurement
architecture. In the ground controller, CountingStars builds a digital twins
system to accurately predict the future network topology. This allows ground
controller to generate and distribute collision-free hash seeds to satellites
in advance. On the satellite, we introduce a port aggregation data structure
that decouples the unique flow identifier from its multi-port counter and
updates it through efficient bit operations, solving the memory inflation
caused by PBLB. Simulation results show that the memory usage of CountingStars
is reduced by 70\% on average, and the relative error of measurement is reduced
by 90\% on average. Implementation on FPGA shows its prospect to deploy in real
system.

</details>


### [13] [Security-as-a-Function for IDS/IPS in Softwarized Network and Applications to 5G Network Systems](https://arxiv.org/abs/2508.13581)
*Shivank Malik,Samaresh Bera*

Main category: cs.NI

TL;DR: 本文提出了一种在5G核心网络中部署虚拟化入侵检测和防御系统（IDS-IPS）的方法，以防范DoS和DDoS攻击，并通过实验验证其满足5G应用的QoS需求。


<details>
  <summary>Details</summary>
Motivation: 5G网络的开放性架构扩大了安全漏洞和威胁，而现有的研究较少关注5G网络的安全问题，尤其是针对DoS和DDoS攻击的防护。

Method: 通过虚拟机（VM）和容器化技术实现虚拟化IDS-IPS部署，并在软化的5G核心网络中进行性能评估，包括网络吞吐量、延迟和数据包丢弃率。

Result: 实验结果表明，虚拟化IDS-IPS能够满足5G应用的QoS要求，同时有效防范DoS和DDoS攻击。

Conclusion: 研究提出了一种可行的5G核心网络安全防护方案，为未来的网络功能虚拟化部署提供了实践支持。

Abstract: The service-based architecture of 5G network allows network operators to
place virtualized network functions on commodity hardware, unlike the
traditional vendor-specific hardware-based functionalities. However, it expands
the security vulnerabilities and threats to the 5G network. While there exist
several theoretical studies on network function placement and service routing,
a few focused on the security aspects of the 5G network systems.
  This paper focuses on safeguarding the 5G core network systems from DoS and
DDoS attacks by placing intrusion detection and prevention systems (IDS-IPS) as
virtualized network functions following the 5G standalone architecture. To
ensure the virtualized placement of IDS-IPS, first, we provide thorough virtual
machine (VM)-based and containerized implementation details and evaluate the
network performance with two scenarios, IDS and IPS, in the presence of TCP and
UDP applications. Second, we apply the VM-based implementation of IDS-IPS on a
softwarized 5G core network and study the network performances. The experiment
results on network throughput, latency, and packet drop reveal that the
softwarized IDS-IPS can meet the QoS requirements of 5G applications, while
safeguarding the network from DoS and DDoS attacks.

</details>


### [14] [Towards Timing Isolation for Mixed-Criticality Communication in Software-Defined Vehicles](https://arxiv.org/abs/2508.13652)
*Lóránt Meszlényi,Julius Kahle,Dominik Püllen,Stefan Kowalewski,Stefan Katzenbeisser,Alexandru Kampmann*

Main category: cs.NI

TL;DR: 该论文提出了一种分层软件架构，通过中间件层、网络层和硬件层的流量优先级策略，确保Linux平台上混合关键性应用的时间隔离，从而实现稳定的实时通信性能。


<details>
  <summary>Details</summary>
Motivation: 随着汽车行业向集中式Linux架构转型，确保混合关键性应用的可预测执行至关重要，但Linux网络栈的并发使用可能引入干扰。

Method: 采用中间件层的固定优先级非抢占式调度器、网络层的XDP直接路由以及硬件层的专用NIC队列，实现全栈隔离。

Result: 在DDS系统中验证了该架构，即使在最佳努力应用的严重干扰下，实时流量仍能保持一致的延迟。

Conclusion: 该架构有效解决了Linux平台上混合关键性应用的时间隔离问题，适用于汽车控制系统。

Abstract: As the automotive industry transitions toward centralized Linux-based
architectures, ensuring the predictable execution of mixed-criticality
applications becomes essential. However, concurrent use of the Linux network
stack introduces interference, resulting in unpredictable latency and jitter.
To address this challenge, we present a layered software architecture that
enforces timing isolation for Ethernet-based data exchange between
mixed-criticality applications on Linux-based automotive control units. Our
approach integrates traffic prioritization strategies at the middleware layer,
the network stack layer, and the hardware layer to achieve isolation across the
full software stack. At the middleware layer, we implement a fixed-priority,
non-preemptive scheduler to manage publishers of varying criticality. At the
network layer, we leverage the express data path (XDP) to route high-priority
data directly from the network interface driver into critical application
memory, bypassing the standard Linux network stack. At the hardware layer, we
dedicate a network interface card (NIC) queue exclusively to real-time traffic.
We demonstrate how our architecture performs in a Data Distribution Service
(DDS)-based system. Our evaluation shows that the approach leads to consistent
and predictable latencies for real-time traffic, even under heavy interference
from best-effort applications.

</details>


### [15] [Architecture Considerations for ISAC in 6G](https://arxiv.org/abs/2508.13736)
*Sebastian Robitzsch,Laksh Bhatia,Konstantinos G. Filis,Neda Petreska,Michael Bahr,Pablo Picazo Martinez,Xi Li*

Main category: cs.NI

TL;DR: 本文探讨了6G中集成感知与通信（ISAC）的架构设计，包括标准化进展、用例展示和功能需求，提出了集成感知功能的新架构，并讨论了协议栈的调整。


<details>
  <summary>Details</summary>
Motivation: 6G需要集成感知能力，以扩展移动网络的功能，使其不仅能提供通信服务，还能大规模感知环境。

Method: 基于3GPP和ETSI的进展，提出包含新网络功能的6G系统架构，支持感知即服务，并调整控制面和感知面的协议栈。

Result: 提出了一个支持ISAC的6G系统架构，演示了感知功能的应用。

Conclusion: ISAC将成为6G的关键功能，本文的架构设计和协议调整为实现这一功能提供了方向。

Abstract: ISAC is emerging as a foundational capability in 6G, enabling mobile networks
to not only offer communication services but also to sense and perceive their
environment at scale. This paper explores architectural considerations to
enable sensing in 6G, extending on recent developments by (pre-)standardisation
bodies such as 3GPP and ETSI. Selected ISAC use cases are presented from the
European MultiX project including associated potential functional system
requirements. The paper proposes a 6G system architecture that integrates newly
proposed NFs for the purpose of sensing and demonstrates how they are being
used in offering sensing as a service. Protocol stack adaptations for both
control and a newly proposed sensing plane are discussed.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [16] [Robust Live Streaming over LEO Satellite Constellations: Measurement, Analysis, and Handover-Aware Adaptation](https://arxiv.org/abs/2508.13402)
*Hao Fang,Haoyuan Zhao,Jianxin Shi,Miao Zhang,Guanzhen Wu,Yi Ching Chou,Feng Wang,Jiangchuan Liu*

Main category: cs.MM

TL;DR: 介绍了卫星感知速率适配（SARA）中间件，解决低地球轨道卫星网络（LSNs）上直播流媒体因卫星切换导致的频繁缓冲问题，提升观看体验。


<details>
  <summary>Details</summary>
Motivation: 全球仍有大量人口缺乏网络接入，LSNs提供了解决方案，但现有直播平台在LSNs上因频繁卫星切换导致视频缓冲问题，现有自适应码率算法无法有效处理。

Method: 提出SARA中间件，智能调节播放速度并为ABR算法提供LSNs网络特性信息，帮助优化码率选择和减少缓冲事件。

Result: SARA平均减少39.41%的缓冲时间，略微提升0.65%的延迟，码率损失仅0.13%。

Conclusion: SARA能有效提升LSNs上的直播流媒体性能，减少缓冲并优化体验。

Abstract: Live streaming has experienced significant growth recently. Yet this rise in
popularity contrasts with the reality that a substantial segment of the global
population still lacks Internet access. The emergence of Low Earth orbit
Satellite Networks (LSNs), such as SpaceX's Starlink and Amazon's Project
Kuiper, presents a promising solution to fill this gap. Nevertheless, our
measurement study reveals that existing live streaming platforms may not be
able to deliver a smooth viewing experience on LSNs due to frequent satellite
handovers, which lead to frequent video rebuffering events. Current
state-of-the-art learning-based Adaptive Bitrate (ABR) algorithms, even when
trained on LSNs' network traces, fail to manage the abrupt network variations
associated with satellite handovers effectively. To address these challenges,
for the first time, we introduce Satellite-Aware Rate Adaptation (SARA), a
versatile and lightweight middleware that can seamlessly integrate with various
ABR algorithms to enhance the performance of live streaming over LSNs. SARA
intelligently modulates video playback speed and furnishes ABR algorithms with
insights derived from the distinctive network characteristics of LSNs, thereby
aiding ABR algorithms in making informed bitrate selections and effectively
minimizing rebuffering events that occur during satellite handovers. Our
extensive evaluation shows that SARA can effectively reduce the rebuffering
time by an average of $39.41\%$ and slightly improve latency by $0.65\%$ while
only introducing an overall loss in bitrate by $0.13\%$.

</details>


### [17] [INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video](https://arxiv.org/abs/2508.13756)
*Ruonan Chai,Yixiang Zhu,Xinjiao Li,Jiawei Li,Zili Meng,Dirk Kutscher*

Main category: cs.MM

TL;DR: INDS是一种基于信息中心网络（ICN）的实时点云视频流框架，通过Octree结构和内容命名实现自适应传输，显著降低延迟并提高吞吐量和缓存效率。


<details>
  <summary>Details</summary>
Motivation: 解决实时点云视频流在大数据量和高丢包敏感性下的传输挑战，现有协议（如TCP和QUIC）存在粒度粗和集中控制限制的问题。

Method: 利用Octree结构和分层媒体命名，支持渐进式部分检索增强层，结合时间窗口和帧组（GoF）命名，优化网络缓存和多用户数据重用。

Result: 原型实现显示延迟降低80%，吞吐量提升15-50%，缓存命中率提高20-30%，优于现有DASH风格系统。

Conclusion: INDS是一种可扩展、缓存友好的实时点云流解决方案，兼容MoQ架构，适用于动态和丢包网络环境。

Abstract: Real-time streaming of point cloud video, characterized by massive data
volumes and high sensitivity to packet loss, remains a key challenge for
immersive applications under dynamic network conditions. While
connection-oriented protocols such as TCP and more modern alternatives like
QUIC alleviate some transport-layer inefficiencies, including head-of-line
blocking, they still retain a coarse-grained, segment-based delivery model and
a centralized control loop that limit fine-grained adaptation and effective
caching. We introduce INDS (Incremental Named Data Streaming), an adaptive
streaming framework based on Information-Centric Networking (ICN) that rethinks
delivery for hierarchical, layered media. INDS leverages the Octree structure
of point cloud video and expressive content naming to support progressive,
partial retrieval of enhancement layers based on consumer bandwidth and
decoding capability. By combining time-windows with Group-of-Frames (GoF),
INDS's naming scheme supports fine-grained in-network caching and facilitates
efficient multi-user data reuse. INDS can be deployed as an overlay, remaining
compatible with QUIC-based transport infrastructure as well as future
Media-over-QUIC (MoQ) architectures, without requiring changes to underlying IP
networks. Our prototype implementation shows up to 80% lower delay, 15-50%
higher throughput, and 20-30% increased cache hit rates compared to
state-of-the-art DASH-style systems. Together, these results establish INDS as
a scalable, cache-friendly solution for real-time point cloud streaming under
variable and lossy conditions, while its compatibility with MoQ overlays
further positions it as a practical, forward-compatible architecture for
emerging immersive media systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [Arithmetics within the Linear Time Hierarchy](https://arxiv.org/abs/2508.13195)
*Chris Pollett*

Main category: cs.LO

TL;DR: 该论文研究了算术片段$S_1$的子集，定义了新的语法类并证明了其闭包性质和多函数可定义性，提出了几个算术理论并分析了它们的包含关系和计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 通过研究算术$S_1$的片段，希望找到具有良好闭包性质且能精确定义多函数的理论，并探讨其计算复杂性和独立性。

Method: 在语言$L_1$中定义新的语法类，通过计数有界存在和有界全称量词块，构建了算术理论$\breve{S}^{i}_{1}$、$TLS^i_1$和$TSC^i_1$，并分析了它们的包含关系和可定义性。

Result: 证明了$TLS^i_1 \subseteq TSC^i_1 \subseteq \breve{S}^{i}_{1}$的层级关系，并给出了多函数的计算复杂性分类。

Conclusion: 研究提供了一种新方法来分析算术理论的片段及其计算复杂性，同时提出了与MRDP定理和计算复杂性相关的独立性结果。

Abstract: We identify fragments of the arithmetic $S_1$ that enjoy nice closure
properties and have exact characterization of their definable multifunctions.
To do this, in the language of $S_1$, $L_1$, starting from the formula classes,
$\Sigma^{\mathsf b}_{i}$, which ignore sharply bounded quantifiers when
determining quantifier alternations, we define new syntactic classes by
counting bounded existential sharply bounded universal quantifiers blocks.
Using these, we define arithmetics: $\breve{S}^{i}_{1}$, $TLS^i_1$ and
$TSC^i_1$. $\breve{S}^{i}_{1}$ consists of open axioms for the language symbols
and length induction for one of our new classes, $SIUT_{i,1}^{\{p(|id|)\}}$.
$TLS^i_1$ and $TSC^i_1$ are defined using axioms related to dependent choice
sequences for formulas from two other classes within $\Sigma^{\mathsf b}_{i}$.
We prove for $i \geq 1$ that $$TLS^i_1 \subseteq TSC^i_1 \subseteq
\breve{S}^{i}_{1} \preceq_{\forall B(SITT_{i+1}^{\{p(|id|)\}})} TLS^{i+1}_1$$
and that the $SITT_{i}^{\{p(|id|)\}}$-definable in $TLS^i_1$ (resp.
$SITT_{i}^{\{2^{p(||id||)}\}}$-definable in $TSC^i_1$) multifunctions are
$L_1$-$FLOGSPACE^{SIT_{i,1}}[wit]$ (resp. $L_1$-$FSC^{SIT_{i,1}}[wit]$). These
multifunction classes are respectively the logspace or $SC$ (poly-time,
polylog-space) computable multifunctions whose output is bound by a term in
$L_1$ and that have access to a witness oracle for another restriction on the
$\Sigma^{\mathsf b}_{i}$ formulas, $SIT_{i,1}$. For the $i=1$ cases, this
simplifies respectively to the functions in logspace and $SC$, Steve's Class,
poly-time, polylog-space. We prove independence results related to the
Matiyasevich Robinson Davis Putnam Theorem (MRDP) and to whether our theories
prove simultaneous nondeterministic polynomial time, sublinear space is equal
to co-nondeterministic polynomial time, sublinear space.

</details>


### [19] [A Formalization of the Reversible Concurrent Calculus CCSKP in Beluga](https://arxiv.org/abs/2508.13612)
*Gabriele Cecilia*

Main category: cs.LO

TL;DR: 本文首次在Beluga中形式化了可逆并发计算CCSKP，涵盖了其语法、语义及关于证明标签的三类关系的先进成果，揭示了因果关系和并发事件的新见解。


<details>
  <summary>Details</summary>
Motivation: 为可逆并发计算提供一个机器证明的形式化模型，填补了该领域数学性质未被机器验证的空白。

Method: 采用Beluga对CCSKP进行形式化编码，包括语法、语义及证明标签的依赖、独立与连通关系。

Result: 形式化揭示了部分非正式证明中未明确的细节，某些内容比预想复杂，为未来可逆并发计算形式化奠定了基础。

Conclusion: 本研究为可逆并发计算的形式化提供了基础，并展示了证明标签关系的新视角。

Abstract: Reversible concurrent calculi are abstract models for concurrent systems in
which any action can potentially be undone. Over the last few decades,
different formalisms have been developed and their mathematical properties have
been explored; however, none have been machine-checked within a proof
assistant. This paper presents the first Beluga formalization of the Calculus
of Communicating Systems with Keys and Proof labels (CCSKP), a reversible
extension of CCS. Beyond the syntax and semantics of the calculus, the encoding
covers state-of-the-art results regarding three relations over proof labels --
namely, dependence, independence and connectivity -- which offer new insights
into the notions of causality and concurrency of events. As is often the case
with formalizations, our encoding introduces adjustments to the informal proof
and makes explicit details which were previously only sketched, some of which
reveal to be less straightforward than initially assumed. We believe this work
lays the foundations for future reversible concurrent calculi formalizations.

</details>


### [20] [Modular Multiparty Sessions with Mixed Choice](https://arxiv.org/abs/2508.13616)
*Franco Barbanera,Mariangiola Dezani-Ciancaglini*

Main category: cs.LO

TL;DR: 该论文探讨了多党会话类型（MPST）在并发系统中的应用，重点研究了混合选择对通信安全的控制挑战及其在模块化系统中的优化。


<details>
  <summary>Details</summary>
Motivation: 通过研究混合选择（同时扮演发送者和接收者角色）在MPST中的表达能力和安全性控制难题，旨在提升模块化系统的通信安全性和效率。

Method: 采用类型分配方法，重点在松散耦合的模块内部充分利用混合选择的能力。

Result: 模块化会话的类型化实现了主题归约、会话保真和无锁特性。

Conclusion: 研究表明，模块化系统中的混合选择能够有效提升MPST的表达能力和通信安全性。

Abstract: MultiParty Session Types (MPST) provide a useful framework for safe
concurrent systems. Mixed choice (enabling a participant to play at the same
time the roles of sender and receiver) increases the expressive power of MPST
as well as the difficulty in controlling safety of communications. Such a
control is more viable when modular systems are considered and the power of
mixed choice fully exploited only inside loosely coupled modules. We carry over
such idea in a type assignment approach to multiparty sessions. Typability for
modular sessions entails Subject Reductions, Session Fidelity and Lock Freedom.

</details>


### [21] [On a Second-Order Version of Russellian Theory of Definite Descriptions](https://arxiv.org/abs/2508.13928)
*Yaroslav Petrukhin*

Main category: cs.LO

TL;DR: 论文提出了一种二阶定指描述符，用于指代对象间独特关系，基于Russell的理论，并在Henkin通用模型的不完整二阶逻辑中建立了完整形式化系统。


<details>
  <summary>Details</summary>
Motivation: 探讨定指描述符的二阶扩展，以指代对象间的独特关系，扩展Russell的理论。

Method: 在Henkin通用模型的二阶逻辑片段中，使用无割序列演算形式化理论。

Result: 在Henkin通用模型的二阶逻辑片段中实现了完整的形式化理论。

Conclusion: 提出的二阶定指描述符有效扩展了Russell理论，为研究对象间关系提供了新工具。

Abstract: Definite descriptions are first-order expressions that denote unique objects.
In this paper, we propose a second-order counterpart, designed to refer to
unique relations between objects. We investigate this notion within the
framework of Russell's theory of definite descriptions. While full second-order
logic is incomplete, its fragment defined by Henkin's general models admits
completeness. We develop our theory within this fragment and formalize it using
a cut-free sequent calculus.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [22] [Data Work in Memory Institutions: Why and How Information Professionals Use Wikidata](https://arxiv.org/abs/2508.13388)
*Riya Sinha,Amelia Acker,Hanlin Li*

Main category: cs.HC

TL;DR: 该论文研究了记忆机构中的信息专业人士如何利用Wikidata，并探讨了他们的动机、实践以及与志愿者的合作机会。


<details>
  <summary>Details</summary>
Motivation: 理解记忆机构中信息专业人士使用Wikidata的动机和实践，以促进知识基础设施和开放数据系统的链接与支持。

Method: 通过对15名参与者进行访谈，识别了Wikidata用户在记忆机构中的三种典型角色及贡献类型。

Result: 发现了记忆机构在Wikidata中的三种角色（提供者、获取者和互惠者）及其贡献类型，并探讨了合作机会。

Conclusion: 论文强调了记忆机构在Wikidata中的重要作用，并提出了支持信息专业人士贡献的机会。

Abstract: Wikidata, an open structured database and a sibling project to Wikipedia, has
recently become an important platform for information professionals to share
structured metadata from their memory institutions, organizations that maintain
public knowledge and cultural heritage materials. While studies have
investigated why and how peer producers contribute to Wikidata, the
institutional motivations and practices of these organizations are less
understood. Given Wikidata's potential role in linking and supporting knowledge
infrastructures and open data systems, we examined why and how information
professionals in memory institutions use Wikidata as part of their
organizational workflow. Through interviews with 15 participants, we identified
the three archetypal roles of Wikidata users within memory institutions,
providers, acquirers, and mutualists, and the different types of contributions
that these institutions bring to Wikidata. We then explored potential
collaboration opportunities between memory institutions and other volunteers in
Wikidata, discussed the value of the data work conducted by these
professionals, and examined how and why they track their contributions. Our
work contributes to the wider discussions around collaboration and data work in
CSCW by (1) studying the motivations and practices of information
professionals, their differences from those doing volunteer work, and
opportunities for the Wikidata community to promote more collaborative efforts
within memory institutions and with other volunteers and (2) drawing attention
to the important data work done by memory institutions on Wikidata and pointing
out opportunities to support the contributions of information professionals.

</details>


### [23] [Large Language Models as Visualization Agents for Immersive Binary Reverse Engineering](https://arxiv.org/abs/2508.13413)
*Dennis Brown,Samuel Mulder*

Main category: cs.HC

TL;DR: VR和LLM结合用于二进制逆向工程，通过增强记忆和交互性简化认知复杂度。初步研究表明LLM能生成有意义的3D调用图，但结果质量参差不齐。


<details>
  <summary>Details</summary>
Motivation: 利用沉浸式VR和LLM的潜力，通过外部认知支持逆向工程任务，如记忆增强和可视化组织。

Method: 扩展VR平台，集成LLM代理，支持查询二进制分析工具、回答技术问题并动态生成3D可视化。

Result: 初步研究显示LLM可生成符合设计原则的3D调用图（小型程序），但输出质量不稳定。

Conclusion: LLM作为可视化代理的潜力存在，但需进一步研究其无显式训练下的表现。

Abstract: Immersive virtual reality (VR) offers affordances that may reduce cognitive
complexity in binary reverse engineering (RE), enabling embodied and external
cognition to augment the RE process through enhancing memory, hypothesis
testing, and visual organization. In prior work, we applied a cognitive systems
engineering approach to identify an initial set of affordances and implemented
a VR environment to support RE through spatial persistence and interactivity.
In this work, we extend that platform with an integrated large language model
(LLM) agent capable of querying binary analysis tools, answering technical
questions, and dynamically generating immersive 3D visualizations in alignment
with analyst tasks. We describe the system architecture and our evaluation
process and results. Our pilot study shows that while LLMs can generate
meaningful 3D call graphs (for small programs) that align with design
principles, output quality varies widely. This work raises open questions about
the potential for LLMs to function as visualization agents, constructing 3D
representations that reflect cognitive design principles without explicit
training.

</details>


### [24] [Visuo-Tactile Feedback with Hand Outline Styles for Modulating Affective Roughness Perception](https://arxiv.org/abs/2508.13504)
*Minju Baeck,Yoonseok Shin,Dooyoung Kim,Hyunjin Lee,Sang Ho Yoon,Woontack Woo*

Main category: cs.HC

TL;DR: 提出了一种结合虚拟手部视觉和指尖振动的触觉反馈方法，用于调节VR中的粗糙感知情绪效果。


<details>
  <summary>Details</summary>
Motivation: 探索虚拟手部视觉反馈对情绪感知（如粗糙感、唤醒度和效价）的影响，此前研究主要集中在物体纹理和振动触觉反馈上。

Method: 引入情感视觉线索（线条形状、运动和颜色）应用于手部轮廓，研究其对情绪反应和粗糙感知的作用。

Result: 尖锐轮廓增强粗糙感和唤醒度，降低效价；红色降低情绪积极性。这些效果在低触觉强度下尤为显著。

Conclusion: 结合情感视觉线索与触觉反馈可丰富VR中的情绪渲染，提供灵活的情绪调节。

Abstract: We propose a visuo-tactile feedback method that combines virtual hand
visualization and fingertip vibrations to modulate affective roughness
perception in VR. While prior work has focused on object-based textures and
vibrotactile feedback, the role of visual feedback on virtual hands remains
underexplored. Our approach introduces affective visual cues including line
shape, motion, and color applied to hand outlines, and examines their influence
on both affective responses (arousal, valence) and perceived roughness. Results
show that sharp contours enhanced perceived roughness, increased arousal, and
reduced valence, intensifying the emotional impact of haptic feedback. In
contrast, color affected valence only, with red consistently lowering emotional
positivity. These effects were especially noticeable at lower haptic
intensities, where visual cues extended affective modulation into mid-level
perceptual ranges. Overall, the findings highlight how integrating expressive
visual cues with tactile feedback can enrich affective rendering and offer
flexible emotional tuning in immersive VR interactions.

</details>


### [25] [koboshi: A Base That Animates Everyday Objects](https://arxiv.org/abs/2508.13509)
*Yuta Sugiura*

Main category: cs.HC

TL;DR: 提出一种名为'koboshi'的底座形机器人，通过内置电机移动内部重量，实现摇晃运动，赋予静态物体新动作，具备传感器和通信能力。


<details>
  <summary>Details</summary>
Motivation: 通过机器人底座为日常静态物体提供新的移动方式，增强互动性。

Method: 设计球形底座机器人，利用内置电机移动内部重量实现多方向摇晃，配备传感器和通信模块。

Result: 机器人能有效移动物体，支持用户互动和多机器人通信。

Conclusion: koboshi为静态物体提供灵活移动方案，展示了人机互动的潜力。

Abstract: We propose a base-shaped robot named "koboshi" that moves everyday objects.
This koboshi has a spherical surface in contact with the floor, and by moving a
weight inside using built-in motors, it can rock up and down, and side to side.
By placing everyday items on this koboshi, users can impart new movement to
otherwise static objects. The koboshi is equipped with sensors to measure its
posture, enabling interaction with users. Additionally, it has communication
capabilities, allowing multiple units to communicate with each other.

</details>


### [26] ["Can You See Me Think?" Grounding LLM Feedback in Keystrokes and Revision Patterns](https://arxiv.org/abs/2508.13543)
*Samra Zafar,Shifa Yousaf,Muhammad Shaheer Minhas*

Main category: cs.HC

TL;DR: 研究探讨了在大型语言模型（LLM）生成反馈时，加入写作过程数据（如键盘记录和时间戳快照）是否能够提升反馈质量。实验表明，尽管评分变化不大，但加入过程数据的反馈在结构评估和过程敏感性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨LLM是否能够不仅关注写作的最终成果，还能理解其写作过程，从而提供更全面和深入的反馈。

Method: 研究方法是对52篇学生论文进行对比实验，一组仅基于最终论文生成反馈（C1），另一组额外加入键盘记录和时间戳快照（C2）。

Result: 结果显示，虽然评分标准变化不大，但C2组的反馈在结构评估和过程敏感性解释上明显优于C1组。

Conclusion: 结论表明，加入写作过程数据能够显著提升LLM反馈的质量，尤其是在结构评估和过程敏感性方面。

Abstract: As large language models (LLMs) increasingly assist in evaluating student
writing, researchers have begun questioning whether these models can be
cognitively grounded, that is, whether they can attend not just to the final
product, but to the process by which it was written. In this study, we explore
how incorporating writing process data, specifically keylogs and time-stamped
snapshots, affects the quality of LLM-generated feedback. We conduct an
ablation study on 52 student essays comparing feedback generated with access to
only the final essay (C1) and feedback that also incorporates keylogs and
time-stamped snapshots (C2). While rubric scores changed minimally, C2 feedback
demonstrated significantly improved structural evaluation and greater
process-sensitive justification.

</details>


### [27] [`My Dataset of Love': A Preliminary Mixed-Method Exploration of Human-AI Romantic Relationships](https://arxiv.org/abs/2508.13655)
*Xuetong Wang,Ching Christie Pang,Pan Hui*

Main category: cs.HC

TL;DR: 研究探讨了中国社交媒体上人机浪漫关系的特征和影响，发现用户对AI伴侣的自愿自我披露能增强积极性，但也揭示了互动中的问题和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 填补了人机浪漫关系在研究中的空白，特别是在关系阶段和情感依恋方面。

Method: 采用混合方法分析小红书上的1,766篇帖子和60,925条评论，并对23名参与者进行半结构化访谈。

Result: 用户对AI伴侣的自愿自我披露能增强积极性，但也揭示了互动中的问题和潜在风险。

Conclusion: 人机浪漫关系具有交互性和自我主导特征，但也需关注语言滥用、偏见和数据安全问题。

Abstract: Human-AI romantic relationships have gained wide popularity among social
media users in China. The technological impact on romantic relationships and
its potential applications have long drawn research attention to topics such as
relationship preservation and negativity mitigation. Media and communication
studies also explore the practices in romantic para-social relationships.
Nonetheless, this emerging human-AI romantic relationship, whether the
relations fall into the category of para-social relationship together with its
navigation pattern, remains unexplored, particularly in the context of
relational stages and emotional attachment. This research thus seeks to fill
this gap by presenting a mixed-method approach on 1,766 posts and 60,925
comments from Xiaohongshu, as well as the semi-structured interviews with 23
participants, of whom one of them developed her relationship with self-created
AI for three years. The findings revealed that the users' willingness to
self-disclose to AI companions led to increased positivity without social
stigma. The results also unveiled the reciprocal nature of these interactions,
the dominance of 'self', and raised concerns about language misuse, bias, and
data security in AI communication.

</details>


### [28] [Bend It, Aim It, Tap It: Designing an On-Body Disambiguation Mechanism for Curve Selection in Mixed Reality](https://arxiv.org/abs/2508.13748)
*Xiang Li,Per Ola Kristensson*

Main category: cs.HC

TL;DR: 论文提出两种互补技术来解决混合现实中物体选择的挑战：基于手指曲率的实时贝塞尔曲线选择和基于前臂投影的候选目标消歧。


<details>
  <summary>Details</summary>
Motivation: 混合现实中物体选择在密集或遮挡环境中尤为困难，传统射线投射法易导致模糊和精度下降。

Method: 提出两种技术：1）通过手指曲率引导的贝塞尔曲线选择；2）将最近候选目标投影到前臂的消歧机制。两者结合实现灵活用户控制与触觉反馈。

Result: 实验结果显示，前臂消歧显著减少错误并提升用户满意度，贝塞尔曲线虽能访问遮挡目标但任务时间较长。

Conclusion: 结合曲线输入和前臂投影可支持沉浸环境中的精确自适应选择。

Abstract: Object selection in Mixed Reality (MR) becomes particularly challenging in
dense or occluded environments, where traditional mid-air ray-casting often
leads to ambiguity and reduced precision. We present two complementary
techniques: (1) a real-time Bezier Curve selection paradigm guided by finger
curvature, enabling expressive one-handed trajectories, and (2) an on-body
disambiguation mechanism that projects the four nearest candidates onto the
user's forearm via proximity-based mapping. Together, these techniques combine
flexible, user-controlled selection with tactile, proprioceptive
disambiguation. We evaluated their independent and joint effects in a 2x2
within-subjects study (N = 24), crossing interaction paradigm (Bezier Curve vs.
Linear Ray) with interaction medium (Mid-air vs. On-body). Results show that
on-body disambiguation significantly reduced selection errors and physical
demand while improving perceived performance, hedonic quality, and user
preference. Bezier input provided effective access to occluded targets but
incurred longer task times and greater effort under some conditions. We
conclude with design implications for integrating curved input and on-body
previews to support precise, adaptive selection in immersive environments.

</details>


### [29] [Mind & Motion: Opportunities and Applications of Integrating Biomechanics and Cognitive Models in HCI](https://arxiv.org/abs/2508.13788)
*Arthur Fleig,Florian Fischer,Markus Klar,Patrick Ebel,Miroslav Bachinski,Per Ola Kristensson,Roderick Murray-Smith,Antti Oulasvirta*

Main category: cs.HC

TL;DR: 通过计算模型模拟用户行为，连接认知与生物力学模型，为HCI提供更准确预测和设计指导。


<details>
  <summary>Details</summary>
Motivation: 理解用户交互行为，结合认知与生物力学模型，推动多样化用户模拟，提升HCI设计。

Method: 整合认知模型与生物力学仿真，模拟用户意图、策略和动作。

Result: 可能更准确地预测用户行为，优化界面设计和交互技术，支持自适应系统开发。

Conclusion: 连接认知与生物力学模型是HCI的未来方向，为UI/UX设计等提供新思路。

Abstract: Computational models of how users perceive and act within a virtual or
physical environment offer enormous potential for the understanding and design
of user interactions. Cognition models have been used to understand the role of
attention and individual preferences and beliefs on human decision making
during interaction, while biomechanical simulations have been successfully
applied to analyse and predict physical effort, fatigue, and discomfort. The
next frontier in HCI lies in connecting these models to enable robust, diverse,
and representative simulations of different user groups. These embodied user
simulations could predict user intents, strategies, and movements during
interaction more accurately, benchmark interfaces and interaction techniques in
terms of performance and ergonomics, and guide adaptive system design. This
UIST workshop explores ideas for integrating computational models into HCI and
discusses use cases such as UI/UX design, automated system testing, and
personalised adaptive interfaces. It brings researchers from relevant
disciplines together to identify key opportunities and challenges as well as
feasible next steps for bridging mind and motion to simulate interactive user
behaviour.

</details>


### [30] [LLM-Powered Virtual Patient Agents for Interactive Clinical Skills Training with Automated Feedback](https://arxiv.org/abs/2508.13943)
*Henrik Voigt,Yurina Sugamiya,Kai Lawonn,Sina Zarrieß,Atsuo Takanishi*

Main category: cs.HC

TL;DR: 本文提出了一种新颖的框架，通过增强基于大型语言模型（LLM）的模拟患者功能，使其具备更丰富的非文本互动能力，并结合虚拟导师以实时提供个性化反馈，为医学生提供了一种低成本、便捷的家庭OSCE准备平台。


<details>
  <summary>Details</summary>
Motivation: 解决传统OSCE资源需求高的问题，并弥补现有基于LLM的文本模拟患者互动能力不足的缺陷。

Method: 开发了一个框架，为LLM模拟患者增加了动作空间功能，同时引入虚拟导师提供实时反馈。系统还评估了实时性能，包括延迟和准确性。

Result: 初步评估显示，模拟患者的自然性和连贯性，以及虚拟导师反馈的有用性和适当性得到了医学专家的认可。

Conclusion: 该框架为医学生提供了高效、低成本的OSCE训练工具，扩展了模拟患者的互动维度。

Abstract: Objective Structured Clinical Examinations (OSCEs) are essential for medical
training, but they require significant resources, including professional actors
and expert medical feedback. Although Large Language Models (LLMs) have
introduced text-based virtual patients for communication practice, these
simulations often lack the capability for richer, non-textual interactions.
This paper presents a novel framework that significantly enhances LLM-based
simulated patients by equipping them with action spaces, thereby enabling more
realistic and dynamic patient behaviors that extend beyond text. Furthermore,
our system incorporates virtual tutors that provide students with instant,
personalized feedback on their performance at any time during these simulated
encounters. We have conducted a rigorous evaluation of the framework's
real-time performance, including system latency and component accuracy.
Preliminary evaluations with medical experts assessed the naturalness and
coherence of the simulated patients, as well as the usefulness and
appropriateness of the virtual tutor's assessments. This innovative system
provides medical students with a low-cost, accessible platform for personalized
OSCE preparation at home.

</details>


### [31] [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948)
*Yuge Zhang,Nan Chen,Jiahang Xu,Yuqing Yang*

Main category: cs.HC

TL;DR: 论文提出了POML（Prompt Orchestration Markup Language），一种基于组件的标记语言，旨在解决大型语言模型（LLMs）提示工程中的结构化、数据整合和格式敏感性问题。通过案例研究和用户评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的提示工程在结构、数据整合、格式敏感性和工具支持方面存在不足，缺乏统一解决方案。POML旨在填补这些空白。

Method: POML采用基于组件的标记语言，支持逻辑结构（角色、任务、示例）、数据整合标签和CSS样式的解耦设计，并提供动态模板和开发者工具包。

Result: 案例研究（PomLink和TableQA）和用户评估表明，POML在复杂应用整合和准确性提升方面效果显著。

Conclusion: POML为LLMs提示工程提供了一种系统化解决方案，提升了开发效率和模型性能。

Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current
practices face challenges in structure, data integration, format sensitivity,
and tooling. Existing methods lack comprehensive solutions for organizing
complex prompts involving diverse data types (documents, tables, images) or
managing presentation variations systematically. To address these gaps, we
introduce POML (Prompt Orchestration Markup Language). POML employs
component-based markup for logical structure (roles, tasks, examples),
specialized tags for seamless data integration, and a CSS-like styling system
to decouple content from presentation, reducing formatting sensitivity. It
includes templating for dynamic prompts and a comprehensive developer toolkit
(IDE support, SDKs) to improve version control and collaboration. We validate
POML through two case studies demonstrating its impact on complex application
integration (PomLink) and accuracy performance (TableQA), as well as a user
study assessing its effectiveness in real-world development scenarios.

</details>


### [32] [Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K-12 Students?](https://arxiv.org/abs/2508.13962)
*Ruiwei Xiao,Xinying Hou,Ying-Jui Tseng,Hsuan Nieu,Guanze Liao,John Stamper,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: 论文设计了一个基于LLM的教学模块，旨在培养中学生与AI聊天机器人负责任的互动能力，通过课堂部署验证了自动评分能力和教学材料的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中日益普及，教育下一代如何负责任地应用和评估AI系统变得至关重要。

Method: 设计了基于LLM的模块，包含场景化练习活动，并在11个中学课堂进行两轮部署，评估自动评分能力、学生表现和材料质量。

Result: 自动评分器表现良好，教学材料提升了学生的提示技能和学习信心，开放性问题更适合测量目标学习者的能力。

Conclusion: 研究展示了模块的潜力，但需进一步评估学习效果和评估设计。

Abstract: As Artificial Intelligence (AI) becomes increasingly integrated into daily
life, there is a growing need to equip the next generation with the ability to
apply, interact with, evaluate, and collaborate with AI systems responsibly.
Prior research highlights the urgent demand from K-12 educators to teach
students the ethical and effective use of AI for learning. To address this
need, we designed an Large-Language Model (LLM)-based module to teach prompting
literacy. This includes scenario-based deliberate practice activities with
direct interaction with intelligent LLM agents, aiming to foster secondary
school students' responsible engagement with AI chatbots. We conducted two
iterations of classroom deployment in 11 authentic secondary education
classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'
prompting performance and confidence changes towards using AI for learning; and
3) the quality of learning and assessment materials. Results indicated that the
AI-based auto-grader could grade student-written prompts with satisfactory
quality. In addition, the instructional materials supported students in
improving their prompting skills through practice and led to positive shifts in
their perceptions of using AI for learning. Furthermore, data from Study 1
informed assessment revisions in Study 2. Analyses of item difficulty and
discrimination in Study 2 showed that True/False and open-ended questions could
measure prompting literacy more effectively than multiple-choice questions for
our target learners. These promising outcomes highlight the potential for
broader deployment and highlight the need for broader studies to assess
learning effectiveness and assessment design.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [33] [PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism](https://arxiv.org/abs/2508.13228)
*Yuyan Ye,Hang Xu,Yanghang Huang,Jiali Huang,Qian Weng*

Main category: cs.GR

TL;DR: PreSem-Surf是一种基于NeRF框架的优化方法，能够快速从RGB-D序列重建高质量场景表面，结合RGB、深度和语义信息提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了在短时间内高效重建高质量场景表面，并提升对噪声与局部细节的区分能力。

Method: 采用SG-MLP采样结构和PR-MLP进行体素预渲染，并引入渐进式语义建模提取多精度语义信息。

Result: 在七个合成场景上的实验显示，PreSem-Surf在C-L1、F-score和IoU指标上表现最佳，其他指标也保持竞争力。

Conclusion: PreSem-Surf展示了高效和实用的场景重建能力，尤其是在结合多模态信息和优化渲染方面。

Abstract: This paper proposes PreSem-Surf, an optimized method based on the Neural
Radiance Field (NeRF) framework, capable of reconstructing high-quality scene
surfaces from RGB-D sequences in a short time. The method integrates RGB,
depth, and semantic information to improve reconstruction performance.
Specifically, a novel SG-MLP sampling structure combined with PR-MLP
(Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering,
allowing the model to capture scene-related information earlier and better
distinguish noise from local details. Furthermore, progressive semantic
modeling is adopted to extract semantic information at increasing levels of
precision, reducing training time while enhancing scene understanding.
Experiments on seven synthetic scenes with six evaluation metrics show that
PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while
maintaining competitive results in NC, Accuracy, and Completeness,
demonstrating its effectiveness and practical applicability.

</details>


### [34] [Sparse, Geometry- and Material-Aware Bases for Multilevel Elastodynamic Simulation](https://arxiv.org/abs/2508.13386)
*Ty Trusty,David I. W. Levin,Danny M. Kaufman*

Main category: cs.GR

TL;DR: 提出了一种多级弹性动力学时间步求解器，用于加速增量势能接触（IPC）模拟，方法在保持高精度（每时间步相对位移误差约1%）的同时，速度提升高达13倍。


<details>
  <summary>Details</summary>
Motivation: 解决IPC模拟中复杂几何、异构材料分布和高分辨率数据带来的计算效率问题，同时保持高精度。

Method: 采用新型稀疏、几何和材料感知的基构造方法，结合快速预处理共轭梯度求解器，替代稀疏直接求解器。

Result: 求解器在视觉上与高标准IPC方法难以区分，数值结果接近，计算速度提升高达13倍。

Conclusion: 新方法在不牺牲精度的情况下显著加速了IPC模拟，适用于复杂场景。

Abstract: We present a multi-level elastodynamics timestep solver for accelerating
incremental potential contact (IPC) simulations. Our method retains the
robustness of gold standard IPC in the face of intricate geometry, complex
heterogeneous material distributions and high resolution input data without
sacrificing visual fidelity (per-timestep relative displacement error of
$\approx1\%$). The success of our method is enabled by a novel, sparse,
geometry- and material-aware basis construction method which allows for the use
of fast preconditioned conjugate gradient solvers (in place of a sparse direct
solver), but without suffering convergence issues due to stiff or heterogeneous
materials. The end result is a solver that produces results visually
indistinguishable and quantitatively very close to gold-standard IPC methods
but up to $13\times$ faster on identical hardware.

</details>


### [35] [Eliminating Rasterization: Direct Vector Floor Plan Generation with DiffPlanner](https://arxiv.org/abs/2508.13738)
*Shidong Wang,Renato Pajarola*

Main category: cs.GR

TL;DR: DiffPlanner是一种完全在矢量空间中操作的深度学习框架，用于边界约束的平面图生成，解决了基于图像方法的信息丢失和复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有的学习方法涉及复杂的矢量数据到图像的转换流程，导致信息丢失和冗余。

Method: DiffPlanner是一种基于Transformer的条件扩散模型，引入了对齐机制以优化设计过程。

Result: 实验表明，DiffPlanner在生成平面图和气泡图方面优于现有方法，具有更高的可控性和质量。

Conclusion: DiffPlanner为边界约束的平面图生成提供了更高效、可控的解决方案。

Abstract: The boundary-constrained floor plan generation problem aims to generate the
topological and geometric properties of a set of rooms within a given boundary.
Recently, learning-based methods have made significant progress in generating
realistic floor plans. However, these methods involve a workflow of converting
vector data into raster images, using image-based generative models, and then
converting the results back into vector data. This process is complex and
redundant, often resulting in information loss. Raster images, unlike vector
data, cannot scale without losing detail and precision. To address these
issues, we propose a novel deep learning framework called DiffPlanner for
boundary-constrained floor plan generation, which operates entirely in vector
space. Our framework is a Transformer-based conditional diffusion model that
integrates an alignment mechanism in training, aligning the optimization
trajectory of the model with the iterative design processes of designers. This
enables our model to handle complex vector data, better fit the distribution of
the predicted targets, accomplish the challenging task of floor plan layout
design, and achieve user-controllable generation. We conduct quantitative
comparisons, qualitative evaluations, ablation experiments, and perceptual
studies to evaluate our method. Extensive experiments demonstrate that
DiffPlanner surpasses existing state-of-the-art methods in generating floor
plans and bubble diagrams in the creative stages, offering more controllability
to users and producing higher-quality results that closely match the ground
truths.

</details>


### [36] [Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing](https://arxiv.org/abs/2508.13797)
*Feng-Lin Liu,Shi-Yang Li,Yan-Pei Cao,Hongbo Fu,Lin Gao*

Main category: cs.GR

TL;DR: 一种基于素描的3D感知视频编辑方法，通过结合图像编辑和3D场景分析，解决了大视角变化下的视频结构编辑难题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在风格迁移或外观修改上表现优异，但在3D场景的结构编辑中（如大视角变化时）仍存在挑战，包括新视角内容的一致性、未编辑区域的保留和稀疏2D输入到3D输出的转换。

Method: 提出Sketch3DVE方法：首先生成第一帧的编辑结果并传播到其他帧；利用素描作为交互工具，支持其他掩码编辑方法；通过密集立体方法估计点云和相机参数；提出基于点云的深度图编辑方法；使用3D感知掩码传播和视频扩散模型实现无缝合并。

Result: 实验证明，Sketch3DVE在大视角变化的视频编辑中表现优越。

Conclusion: Sketch3DVE通过结合2D编辑工具和3D场景分析，有效解决了3D视频结构编辑中的难点，实现了高质量的编辑结果。

Abstract: Recent video editing methods achieve attractive results in style transfer or
appearance modification. However, editing the structural content of 3D scenes
in videos remains challenging, particularly when dealing with significant
viewpoint changes, such as large camera rotations or zooms. Key challenges
include generating novel view content that remains consistent with the original
video, preserving unedited regions, and translating sparse 2D inputs into
realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a
sketch-based 3D-aware video editing method to enable detailed local
manipulation of videos with significant viewpoint changes. To solve the
challenge posed by sparse inputs, we employ image editing methods to generate
edited results for the first frame, which are then propagated to the remaining
frames of the video. We utilize sketching as an interaction tool for precise
geometry control, while other mask-based image editing methods are also
supported. To handle viewpoint changes, we perform a detailed analysis and
manipulation of the 3D information in the video. Specifically, we utilize a
dense stereo method to estimate a point cloud and the camera parameters of the
input video. We then propose a point cloud editing approach that uses depth
maps to represent the 3D geometry of newly edited components, aligning them
effectively with the original 3D scene. To seamlessly merge the newly edited
content with the original video while preserving the features of unedited
regions, we introduce a 3D-aware mask propagation strategy and employ a video
diffusion model to produce realistic edited videos. Extensive experiments
demonstrate the superiority of Sketch3DVE in video editing. Homepage and code:
http://http://geometrylearning.com/Sketch3DVE/

</details>


### [37] [Is-NeRF: In-scattering Neural Radiance Field for Blurred Images](https://arxiv.org/abs/2508.13808)
*Nan Luo,Chenglin Ye,Jiaxu Li,Gang Liu,Bo Wan,Di Wang,Lupeng Liu,Jun Xiao*

Main category: cs.GR

TL;DR: 该论文提出了一种新的去模糊神经辐射场（Is-NeRF），通过显式光路建模和散射感知的体积渲染管道，有效解决了复杂光路场景下的模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂光路（如运动模糊图像）时会引入几何模糊问题，提出Is-NeRF以解决这一挑战。

Method: 通过统一六种光传播现象的散射表示，建立了散射感知的体积渲染管道，并引入自适应学习策略优化散射方向和采样间隔。

Result: Is-NeRF在生成高保真图像和精确几何细节方面优于现有方法。

Conclusion: Is-NeRF通过显式光路建模和散射感知渲染，显著提升了复杂场景下的图像质量和几何细节表现。

Abstract: Neural Radiance Fields (NeRF) has gained significant attention for its
prominent implicit 3D representation and realistic novel view synthesis
capabilities. Available works unexceptionally employ straight-line volume
rendering, which struggles to handle sophisticated lightpath scenarios and
introduces geometric ambiguities during training, particularly evident when
processing motion-blurred images. To address these challenges, this work
proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit
lightpath modeling in real-world environments. By unifying six common light
propagation phenomena through an in-scattering representation, we establish a
new scattering-aware volume rendering pipeline adaptable to complex lightpaths.
Additionally, we introduce an adaptive learning strategy that enables
autonomous determining of scattering directions and sampling intervals to
capture finer object details. The proposed network jointly optimizes NeRF
parameters, scattering parameters, and camera motions to recover fine-grained
scene representations from blurry images. Comprehensive evaluations demonstrate
that it effectively handles complex real-world scenarios, outperforming
state-of-the-art approaches in generating high-fidelity images with accurate
geometric details.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [38] [Quantum-Inspired Artificial Bee Colony for Latency-Aware Task Offloading in IoV](https://arxiv.org/abs/2508.13637)
*Mamta Kumari,Mayukh Sarkar,Rohit Kumar Nonia*

Main category: cs.ET

TL;DR: 提出了一种量子启发的人工蜂群算法（QABC），用于优化车联网中的任务卸载，减少延迟并提高决策效率。


<details>
  <summary>Details</summary>
Motivation: 随着车联网的快速发展，高效的任务卸载对降低延迟和确保及时决策至关重要。

Method: 结合量子计算的原理（如量子态演化和概率编码），改进了传统人工蜂群算法（ABC），以避免局部最优并探索高维解空间。

Result: QABC算法在实时任务卸载策略中表现出优化潜力。

Conclusion: 量子启发启发法在未来的车联网中有望优化实时卸载策略。

Abstract: Efficient task offloading is crucial for reducing latency and ensuring timely
decision-making in intelligent transportation systems within the rapidly
evolving Internet of Vehicles (IoV) landscape. This paper introduces a novel
Quantum-Inspired Artificial Bee Colony (QABC) algorithm specifically designed
for latency-sensitive task offloading involving cloud servers, Roadside Units
(RSUs), and vehicular nodes. By incorporating principles from quantum
computing, such as quantum state evolution and probabilistic encoding, QABC
enhances the classical Artificial Bee Colony (ABC) algorithm's ability to avoid
local optima and explore high-dimensional solution spaces. This research
highlights the potential of quantum-inspired heuristics to optimize real-time
offloading strategies in future vehicular networks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [39] [Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction](https://arxiv.org/abs/2508.13298)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Murat Yildirim,Gozde Tutuncuoglu*

Main category: cs.DC

TL;DR: MELISO+是一个全栈分布式框架，通过两层错误校正机制解决了RRAM设备的非理想性问题，并支持大规模矩阵计算，显著提高了能效和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着全球计算需求的指数增长，传统架构的高能耗问题日益突出，尤其是数据移动带来的能耗。内存计算（如RRAM）虽然可以解决这一问题，但面临设备非理想性和可扩展性差的挑战。

Method: 提出了MELISO+框架，采用两层的错误校正机制来减少设备非理想性问题，并设计了分布式RRAM计算框架，支持超过65,000×65,000维度的矩阵计算。

Result: 将设备非理想性导致的一阶和二阶算术误差降低90%以上，能效提高3到5个数量级，延迟降低100倍，使得低精度RRAM设备在准确度、能耗和延迟上优于高精度设备。

Conclusion: MELISO+通过算法-硬件协同设计和可扩展架构，推动了可持续的高维计算，适用于大语言模型和生成式AI等应用。

Abstract: Exponential growth in global computing demand is exacerbated due to the
higher-energy requirements of conventional architectures, primarily due to
energy-intensive data movement. In-memory computing with Resistive Random
Access Memory (RRAM) addresses this by co-integrating memory and processing,
but faces significant hurdles related to device-level non-idealities and poor
scalability for large computing tasks. Here, we introduce \textbf{MELISO+}
(In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed
framework for energy-efficient in-memory computing. MELISO+ proposes a novel
two-tier error correction mechanism to mitigate device non-idealities and
develops a distributed RRAM computing framework to enable matrix computations
exceeding dimensions of $65,000 \times 65,000$. This approach reduces first-
and second-order arithmetic errors due to device non-idealities by over 90\%,
enhances energy efficiency by three to five orders of magnitude, and decreases
latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to
outperform high-precision device alternatives in accuracy, energy and latency
metrics. By unifying algorithm-hardware co-design with scalable architecture,
MELISO+ significantly advances sustainable, high-dimensional computing suitable
for applications like large language models and generative AI.

</details>


### [40] [Persistent and Partitioned MPI for Stencil Communication](https://arxiv.org/abs/2508.13370)
*Gerald Collom,Jason Burmark,Olga Pearce,Amanda Bienz*

Main category: cs.DC

TL;DR: 论文研究了在Comb基准测试套件中使用非阻塞、持久化和分区通信优化stencil通信的性能，分析了不同规模和参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大规模并行应用中，stencil操作的性能受通信成本主导，需通过优化通信提升效率。

Method: 在Comb基准测试套件中测试非阻塞、持久化和分区通信的性能，分析进程数、线程数和消息大小的影响。

Result: 持久化MPI通信比基线快37%，分区MPI通信快68%。

Conclusion: 持久化和分区通信能显著提升stencil操作的性能。

Abstract: Many parallel applications rely on iterative stencil operations, whose
performance are dominated by communication costs at large scales. Several MPI
optimizations, such as persistent and partitioned communication, reduce
overheads and improve communication efficiency through amortized setup costs
and reduced synchronization of threaded sends. This paper presents the
performance of stencil communication in the Comb benchmarking suite when using
non blocking, persistent, and partitioned communication routines. The impact of
each optimization is analyzed at various scales. Further, the paper presents an
analysis of the impact of process count, thread count, and message size on
partitioned communication routines. Measured timings show that persistent MPI
communication can provide a speedup of up to 37% over the baseline MPI
communication, and partitioned MPI communication can provide a speedup of up to
68%.

</details>


### [41] [OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data](https://arxiv.org/abs/2508.13374)
*Zhouyu Li,Zhijing Yang,Huayue Gu,Xiaojian Wang,Yuchen Liu,Ruozhou Yu*

Main category: cs.DC

TL;DR: 轨道链是一种协同分析框架，通过分解应用程序为微服务并优化通信，实现地球观测的实时分析。


<details>
  <summary>Details</summary>
Motivation: 解决现有地球观测卫星因带宽和连接时间限制导致的数据下载和分析延迟问题，满足实时应用需求。

Method: 利用多卫星协作，将分析任务分解为微服务，采用流量路由算法优化通信，并通过流水线工作流实现实时任务处理。

Result: 实验显示，轨道链能完成比现有框架多60%的分析任务，同时减少72%的通信开销。

Conclusion: 轨道链为实时地球观测分析提供了可行方案，提升了效率和协作能力。

Abstract: Earth observation analytics have the potential to serve many time-sensitive
applications. However, due to limited bandwidth and duration of
ground-satellite connections, it takes hours or even days to download and
analyze data from existing Earth observation satellites, making real-time
demands like timely disaster response impossible. Toward real-time analytics,
we introduce OrbitChain, a collaborative analytics framework that orchestrates
computational resources across multiple satellites in an Earth observation
constellation. OrbitChain decomposes analytics applications into microservices
and allocates computational resources for time-constrained analysis. A traffic
routing algorithm is devised to minimize the inter-satellite communication
overhead. OrbitChain adopts a pipeline workflow that completes Earth
observation tasks in real-time, facilitates time-sensitive applications and
inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,
we implement a hardware-in-the-loop orbital computing testbed. Experiments show
that our system can complete up to 60% analytics workload than existing Earth
observation analytics framework while reducing the communication overhead by up
to 72%.

</details>


### [42] [Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU](https://arxiv.org/abs/2508.13397)
*Michael Adams,Amanda Bienz*

Main category: cs.DC

TL;DR: 论文通过优化GPU感知的all-reduce操作，特别是利用多CPU核心加速，显著提升了大规模深度学习中通信瓶颈的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的大规模inter-GPU all-reduce操作受限于通信成本，而现有的异构架构中大量CPU核心闲置。

Method: 扩展lane-aware reductions到GPU，并利用多个CPU核心加速GPU感知的all-reduce操作。

Result: 在NCSA的Delta超级计算机上，实现了高达2.45倍的加速；在NVIDIA和AMD的通信库中分别实现了1.77倍和1.71倍的加速。

Conclusion: 多CPU核心加速GPU感知的lane all-reduce操作是一种有效的优化策略，显著提升了大规模通信性能。

Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning,
are bottlenecked by communication costs. Emerging heterogeneous architectures
are comprised of complex nodes, often containing $4$ GPUs and dozens to
hundreds of CPU cores per node. Parallel applications are typically accelerated
on the available GPUs, using only a single CPU core per GPU while the remaining
cores sit idle. This paper presents novel optimizations to large GPU-aware
all-reduce operations, extending lane-aware reductions to the GPUs, and notably
using multiple CPU cores per GPU to accelerate these operations. These
multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x
for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta
supercomputer. Finally, the approach is extended to NVIDIA's and AMD's
collective communication libraries, achieving speedup of up to $1.77$x and
$1.71$x, respectively, across $2$ state-of-the-art supercomputers.

</details>


### [43] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: LAMMPS通过集成Kokkos库，成功适应了现代异构计算环境，并在多种硬件平台上展示了高性能的可移植性。


<details>
  <summary>Details</summary>
Motivation: 随着LAMMPS的快速发展，如何使其适应现代异构计算环境，并在不同硬件平台上实现高性能是一个关键问题。

Method: 通过将Kokkos性能可移植库集成到现有的C++代码中，研究了简单对、多体反应和机器学习力场互作用势的性能可移植性。

Result: 在不同厂商和代际的GPU上测试了性能趋势，并在当前美国所有超算平台上展示了强扩展性。

Conclusion: LAMMPS通过Kokkos实现了高性能的可移植性，适用于复杂的多尺度模拟任务。

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>


### [44] [DDoS Attacks in Cloud Computing: Detection and Prevention](https://arxiv.org/abs/2508.13522)
*Zain Ahmad,Musab Ahmad,Bilal Ahmad*

Main category: cs.DC

TL;DR: 综述了DDoS攻击的类型、检测与防御技术，为提升网络安全提供指导。


<details>
  <summary>Details</summary>
Motivation: DDoS攻击日益复杂和频繁，现有检测与防御方法需系统总结以应对挑战。

Method: 分析不同类型的DDoS攻击及其特点，评估现有检测与防御技术的有效性。

Result: 详细阐述了各类攻击的特性、检测方法的优缺点及防御技术的适用场景。

Conclusion: 提供了全面的DDoS攻击综述，旨在帮助组织和个人提高网络安全防护能力。

Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats
faced by organizations and individuals today. In recent years, the complexity
and frequency of DDoS attacks have increased significantly, making it
challenging to detect and mitigate them effectively. The study analyzes various
types of DDoS attacks, including volumetric, protocol, and application layer
attacks, and discusses the characteristics, impact, and potential targets of
each type. It also examines the existing techniques used for DDoS attack
detection, such as packet filtering, intrusion detection systems, and machine
learning-based approaches, and their strengths and limitations. Moreover, the
study explores the prevention techniques employed to mitigate DDoS attacks,
such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the
effectiveness of each approach and its suitability for different types of
attacks and environments. In conclusion, this study provides a comprehensive
overview of the different types of DDoS attacks, their detection, and
prevention techniques. It aims to provide insights and guidelines for
organizations and individuals to enhance their cybersecurity posture and
protect against DDoS attacks.

</details>


### [45] [LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks](https://arxiv.org/abs/2508.13636)
*Laurent Duval,Frédéric Payan,Christophe Preux,Lauriane Bouard*

Main category: cs.DC

TL;DR: 论文讨论了科学数据量的快速增长带来的挑战，提出了评估效率、差异、多样性、可解释性和可用性的问题，并通过LUNDIsim数据集展示了一个解决方案。


<details>
  <summary>Details</summary>
Motivation: 科学数据量的激增带来了计算性、可解释性和可持续性的问题，尤其是在地球科学领域。

Method: 采用无损和有损压缩技术管理数据量，并通过LUNDIsim数据集提供多尺度表示和基准测试。

Result: LUNDIsim数据集适用于数据缩减、压缩算法的评估，并支持地质和油藏工程中的高级网格处理工作流。

Conclusion: LUNDIsim数据集为科学数据管理提供了有效的工具，尤其在基准测试和多尺度表示方面具有潜力。

Abstract: The volume of scientific data produced for and by numerical simulation
workflows is increasing at an incredible rate. This raises concerns either in
computability, interpretability, and sustainability. This is especially
noticeable in earth science (geology, meteorology, oceanography, and
astronomy), notably with climate studies.
  We highlight five main evaluation issues: efficiency, discrepancy, diversity,
interpretability, availability.
  Among remedies, lossless and lossy compression techniques are becoming
popular to better manage dataset volumes. Performance assessment -- with
comparative benchmarks -- require open datasets shared under FAIR principles
(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible
Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted
geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by
porosity/permeability datasets, this dataset proposes four distinct subsurface
environments. They were primarily designed for flow simulation in porous media.
Several consistent resolutions (with HexaShrink multiscale representations) are
proposed for each model. We also provide a set of reservoir features for
reproducing typical two-phase flow simulations on all LUNDIsim models in a
reservoir engineering context. This dataset is chiefly meant for benchmarking
and evaluating data size reduction (upscaling) or genuine composite mesh
compression algorithms. It is also suitable for other advanced mesh processing
workflows in geology and reservoir engineering, from visualization to machine
learning.
  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958

</details>


### [46] [Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim](https://arxiv.org/abs/2508.13693)
*Gabriella Saraiva,Miguel Vasconcelos,Sarita Mazzini Bruschi,Danilo Carastan-Santos,Daniel Cordeiro*

Main category: cs.DC

TL;DR: 开发了一个用于Batsim模拟器的碳足迹插件，用于计算模拟运行期间的CO₂排放，以评估数据中心任务和资源管理策略的环境影响。


<details>
  <summary>Details</summary>
Motivation: 全面评估数据中心任务和资源管理策略对环境的影响。

Method: 在SimGrid框架中开发插件，基于模拟平台的能耗和碳强度因子计算碳排放。

Result: 插件成功集成到Batsim中，支持现有模拟工作流，帮助研究人员评估调度策略的碳效率。

Conclusion: 该插件为数据中心的碳效率研究提供了实用的工具。

Abstract: This work presents a carbon footprint plugin designed to extend the
capabilities of the Batsim simulator by allowing the calculation of CO$_2$
emissions during simulation runs. The goal is to comprehensively assess the
environmental impact associated with task and resource management strategies in
data centers. The plugin is developed within SimGrid -- the underlying
simulation framework of Batsim -- and computes carbon emissions based on the
simulated platform's energy consumption and carbon intensity factor of the
simulated machines. Once implemented, it is integrated into Batsim, ensuring
compatibility with existing simulation workflows and enabling researchers to
assess the carbon efficiency of their scheduling strategies.

</details>


### [47] [CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning](https://arxiv.org/abs/2508.13716)
*Xianfeng Song,Yi Zou,Zheng Shi*

Main category: cs.DC

TL;DR: CaPGNN框架通过自适应缓存和资源感知分区，显著降低了多GPU环境下全批次GNN训练的通信开销，提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 全批次GNN训练在多GPU环境中面临高通信开销和负载不平衡的挑战，限制了其可扩展性。

Method: 提出了CaPGNN框架，结合自适应缓存算法和资源感知的分区算法，优化CPU和GPU内存使用，平衡计算负载。

Result: 实验显示，CaPGNN通信开销减少96%，训练速度提升12.7倍。

Conclusion: 自适应缓存和资源感知分区为全批次GNN训练的分布式部署提供了高效、可扩展的解决方案。

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing
graph-structured data prevalent in various real-world applications. However,
the scalability of full-batch GNN training becomes severely limited by high
communication overhead and load imbalance in distributed environments. In this
paper, we present CaPGNN, a novel framework for efficient parallel full-batch
GNN training on single-server with multi-GPU, designed specifically to reduce
redundant inter-GPU communication and balance computational workloads. We
propose a joint adaptive caching algorithm that leverages both CPU and GPU
memory to significantly reduce the repetitive transmission of vertex features
across partitions. Additionally, we introduce a resource-aware graph
partitioning algorithm that adjusts subgraph sizes dynamically according to the
heterogeneous computational and communication capacities of GPUs. Extensive
experiments on large-scale benchmark datasets demonstrate that CaPGNN
effectively reduces communication costs by up to 96% and accelerates GNN
training by up to 12.7 times compared to state-of-the-art approaches. Our
results highlight the potential of adaptive caching and resource-aware
partitioning to facilitate scalable, efficient, and practical deployment of
full-batch GNN training in distributed computing environments.

</details>


### [48] [Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044](https://arxiv.org/abs/2508.13840)
*Nick Brown*

Main category: cs.DC

TL;DR: 本文研究了Sophon SG2044在HPC中的首次性能表现，相比于SG2042和其他架构，SG2044在高核数下表现优异，性能提升达4.91倍。


<details>
  <summary>Details</summary>
Motivation: 研究RISC-V在高性能计算（HPC）中的应用潜力，特别是Sophon SG2044作为新一代64核高性能CPU的表现。

Method: 通过对比SG2044、SG2042和其他架构的性能，尤其是在高核数和计算密集型任务下的表现。

Result: SG2044在高核数下性能提升显著，支持RVV v1.0和优化的内存子系统缩小了与其他架构的差距。

Conclusion: SG2044在HPC中表现出色，特别是计算密集型任务，进一步推动了RISC-V在HPC领域的应用。

Abstract: The pace of RISC-V adoption continues to grow rapidly, yet for the successes
enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in
High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation
64-core high performance CPU that has been designed for workstation and server
grade workloads. Building upon the SG2042, subsystems that were a bottleneck in
the previous generation have been upgraded.
  In this paper we undertake the first performance study of the SG2044 for HPC.
Comparing against the SG2042 and other architectures, we find that the SG2044
is most advantageous when running at higher core counts, delivering up to 4.91
greater performance than the SG2042 over 64-cores. Two of the most important
upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory
subsystem. This results in the SG2044 significantly closing the performance gap
with other architectures, especially for compute-bound workloads.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [49] [Scavenger: Better Space-Time Trade-Offs for Key-Value Separated LSM-trees](https://arxiv.org/abs/2508.13909)
*Jianshun Zhang,Fang Wang,Sheng Qiu,Yi Wang,Jiaxin Ou,Junxun Huang,Baoquan Li,Peng Fang,Dan Feng*

Main category: cs.DB

TL;DR: 论文提出了Scavenger，一种新型KV分离LSM-tree存储系统，通过高效垃圾回收和空间感知合并策略，显著降低了空间放大并提升了写入性能。


<details>
  <summary>Details</summary>
Motivation: KV分离的LSM-tree虽然解决了写放大问题，但带来了显著的空间放大问题，尤其在成本敏感场景中不可忽视。现有的垃圾回收策略未充分考虑负载特性，且忽略了索引LSM-tree的空间放大负面影响。

Method: 提出了Scavenger：1）设计了I/O高效的垃圾回收方案以减少I/O开销；2）引入基于补偿大小的空间感知合并策略，最小化索引LSM-tree的空间放大。

Result: 实验表明，Scavenger在写入性能和空间放大方面优于其他KV分离LSM-tree（如BlobDB、Titan和TerarkDB）。

Conclusion: Scavenger通过系统优化，在性能和空间放大之间取得了更好的平衡，为解决KV分离LSM-tree的挑战提供了有效方案。

Abstract: Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)
have gained widespread acceptance in storage systems. Nonetheless, a
significant challenge arises in the form of high write amplification due to the
compaction process. While KV-separated LSM-trees successfully tackle this
issue, they also bring about substantial space amplification problems, a
concern that cannot be overlooked in cost-sensitive scenarios. Garbage
collection (GC) holds significant promise for space amplification reduction,
yet existing GC strategies often fall short in optimization performance,
lacking thorough consideration of workload characteristics. Additionally,
current KV-separated LSM-trees also ignore the adverse effect of the space
amplification in the index LSM-tree. In this paper, we systematically analyze
the sources of space amplification of KV-separated LSM-trees and introduce
Scavenger, which achieves a better trade-off between performance and space
amplification. Scavenger initially proposes an I/O-efficient garbage collection
scheme to reduce I/O overhead and incorporates a space-aware compaction
strategy based on compensated size to minimize the space amplification of index
LSM-trees. Extensive experiments show that Scavenger significantly improves
write performance and achieves lower space amplification than other
KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).

</details>


### [50] [Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated LSM-trees](https://arxiv.org/abs/2508.13935)
*Jianshun Zhang,Fang Wang,Jiaxin Ou,Yi Wang,Ming Zhao,Sheng Qiu,Junxun Huang,Baoquan Li,Peng Fang,Dan Feng*

Main category: cs.DB

TL;DR: KV-separated LSM-trees存在写放大和空间放大的问题。Scavenger+提出了一种高效的垃圾回收方案和动态调度策略，显著提升了性能并减少了空间放大。


<details>
  <summary>Details</summary>
Motivation: KV-separated LSM-trees虽然解决了写放大的问题，但带来了空间放大，尤其是索引LSM-tree引起的空间放大。现有垃圾回收策略效率低下，未能考虑工作负载特性。

Method: Scavenger+提出了：(1) I/O高效的垃圾回收方案，(2) 基于补偿大小的空间感知压缩策略，(3) 动态GC调度器。

Result: 实验表明，Scavenger+在写性能和空间放大方面优于BlobDB、Titan和TerarkDB等现有KV-separated LSM-trees。

Conclusion: Scavenger+通过优化垃圾回收和调度策略，在性能和空间效率上取得了显著提升，适用于对成本敏感的存储场景。

Abstract: Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are
widely used in storage systems but face significant challenges, such as high
write amplification caused by compaction. KV-separated LSM-trees address write
amplification but introduce significant space amplification, a critical concern
in cost-sensitive scenarios. Garbage collection (GC) can reduce space
amplification, but existing strategies are often inefficient and fail to
account for workload characteristics. Moreover, current key-value (KV)
separated LSM-trees overlook the space amplification caused by the index
LSM-tree. In this paper, we systematically analyze the sources of space
amplification in KV-separated LSM-trees and propose Scavenger+, which achieves
a better performance-space trade-off. Scavenger+ introduces (1) an
I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a
space-aware compaction strategy based on compensated size to mitigate
index-induced space amplification, and (3) a dynamic GC scheduler that adapts
to system load to make better use of CPU and storage resources. Extensive
experiments demonstrate that Scavenger+ significantly improves write
performance and reduces space amplification compared to state-of-the-art
KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.

</details>


### [51] [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
*Dihia Lanasri*

Main category: cs.DB

TL;DR: 本文对数据库、数据仓库、网络和知识图谱等资源的日志使用进行了系统性调查，分析了300多篇文献，总结了日志的共同特性、标准化流程及挑战，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字时代下，日志记录的用户交互数据具有重要价值，但相关研究分散且缺乏整合，亟需系统性调查以填补知识空白。

Method: 通过分析300多篇文献，研究日志的结构与功能特性、标准化流程及非功能性需求(NFRs)。

Result: 研究发现日志缺乏端到端的标准化流程，但存在共享的结构特性，尤其是知识图谱日志的利用潜力较大。

Conclusion: 本次调查整合了现有知识，指出了研究与实践中的不足，并为未来日志利用的标准化和知识图谱日志的普及提供了方向。

Abstract: In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [52] [EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code](https://arxiv.org/abs/2508.13156)
*Ping Guo,Yiting Wang,Wanghao Ye,Yexiao He,Ziyao Wang,Xiaopeng Dai,Ang Li,Qingfu Zhang*

Main category: cs.AR

TL;DR: 论文提出了EvoVerilog框架，结合大语言模型和进化算法，自动化生成和优化Verilog代码，提升了硬件设计的效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的依赖人工干预和精细调优的方法在自动化硬件设计中扩展性有限，且迭代搜索技术缺乏设计多样性。

Method: 提出EvoVerilog框架，结合大语言模型的推理能力和进化算法的多目标群体搜索策略，自动化生成和优化Verilog代码。

Result: 在VerilogEval-Machine和VerilogEval-Human基准测试中，分别达到89.1和80.2的pass@10分数，能同时生成多种功能代码并优化资源利用。

Conclusion: EvoVerilog在自动化硬件设计中表现出色，推动了设计多样性和效率的提升。

Abstract: Large Language Models (LLMs) have demonstrated great potential in automating
the generation of Verilog hardware description language code for hardware
design. This automation is critical to reducing human effort in the complex and
error-prone process of hardware design.
  However, existing approaches predominantly rely on human intervention and
fine-tuning using curated datasets, limiting their scalability in automated
design workflows.
  Although recent iterative search techniques have emerged, they often fail to
explore diverse design solutions and may underperform simpler approaches such
as repeated prompting.
  To address these limitations, we introduce EvoVerilog, a novel framework that
combines the reasoning capabilities of LLMs with evolutionary algorithms to
automatically generate and refine Verilog code.
  EvoVerilog utilizes a multiobjective, population-based search strategy to
explore a wide range of design possibilities without requiring human
intervention.
  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art
performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine
and VerilogEval-Human benchmarks, respectively. Furthermore, the framework
showcases its ability to explore diverse designs by simultaneously generating a
variety of functional Verilog code while optimizing resource utilization.

</details>


### [53] [Accelerating Transistor-Level Simulation of Integrated Circuits via Equivalence of RC Long-Chain Structures](https://arxiv.org/abs/2508.13159)
*Ruibai Tang,Wenlai Zhao*

Main category: cs.AR

TL;DR: 论文提出三种针对RC长链结构的降阶方法，显著提升晶体管级仿真效率，且误差小。


<details>
  <summary>Details</summary>
Motivation: 解决晶体管级仿真计算成本高的问题，尤其针对RC长链结构。

Method: 提出三种针对不同时间常数规模的RC长链结构的降阶方法。

Result: 方法在基准电路仿真中平均提升性能8.8%（最高22%），相对误差仅0.7%。

Conclusion: 该方法有效降低了仿真成本，适用于多种功能模块。

Abstract: Transistor-level simulation plays a vital role in validating the physical
correctness of integrated circuits. However, such simulations are
computationally expensive. This paper proposes three novel reduction methods
specifically tailored to RC long-chain structures with different scales of time
constant. Such structures account for an average of 6.34\% (up to 12\%) of the
total nodes in the benchmark circuits. Experimental results demonstrate that
our methods yields an average performance improvement of 8.8\% (up to 22\%) on
simulating benchmark circuits which include a variety of functional modules
such as ALUs, adders, multipliers, SEC/DED checkers, and interrupt controllers,
with only 0.7\% relative error.

</details>


### [54] [Fine Grain 3D Integration for Microarchitecture Design Through Cube Packing Exploration](https://arxiv.org/abs/2508.13158)
*Yongxiang Liu,Yuchun Ma,Eren Kurshan,Glenn Reinman,Jason Cong*

Main category: cs.AR

TL;DR: 这篇论文提出了支持逻辑块跨越多层硅片的技术，通过精细的3D集成优化性能、面积和温度，实验结果显示性能提升36%，功耗降低30%。


<details>
  <summary>Details</summary>
Motivation: 现有3D IC研究局限于传统2D硅层堆叠，未能充分利用3D设计空间的潜力。

Method: 开发了立方体包装引擎，结合物理和架构设计，支持多层硅片逻辑块布局，并进行热感知规划。

Result: 实验表明性能提升36%（相比2D），功耗降低30%（相比单层块）。峰值温度通过热感知技术控制。

Conclusion: 多层硅片逻辑块技术能显著提升性能并降低功耗，热管理技术确保温度可控。

Abstract: Most previous 3D IC research focused on stacking traditional 2D silicon
layers, so the interconnect reduction is limited to inter-block delays. In this
paper, we propose techniques that enable efficient exploration of the 3D design
space where each logical block can span more than one silicon layers. Although
further power and performance improvement is achievable through fine grain 3D
integration, the necessary modeling and tool infrastructure has been mostly
missing. We develop a cube packing engine which can simultaneously optimize
physical and architectural design for effective utilization of 3D in terms of
performance, area and temperature. Our experimental results using a design
driver show 36% performance improvement (in BIPS) over 2D and 14% over 3D with
single layer blocks. Additionally multi-layer blocks can provide up to 30%
reduction in power dissipation compared to the single-layer alternatives. Peak
temperature of the design is kept within limits as a result of thermal-aware
floorplanning and thermal via insertion techniques.

</details>


### [55] [Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists](https://arxiv.org/abs/2508.13157)
*Haohang Xu,Chengjie Liu,Qihang Wang,Wenhao Huang,Yongjian Xu,Weiyu Chen,Anlan Peng,Zhijun Li,Bo Li,Lei Qi,Jun Yang,Yuan Du,Li Du*

Main category: cs.AR

TL;DR: 本文提出了一个名为Image2Net的混合框架，用于将复杂的电路图转换为网表，并开源了一个新的数据集。Image2Net的成功率比之前的工作高34.62%-45.19%，且平均网表编辑距离显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在电路图到网表的转换中存在图像风格和电路元素支持有限的问题，影响了基于大语言模型的模拟集成电路设计发展。

Method: 构建了一个新的数据集，并提出Image2Net框架，结合网表编辑距离（NED）来评估转换效果。

Result: Image2Net的成功率达到80.77%，比之前的工作显著提升，平均NED也降低了62.1%-69.6%。

Conclusion: Image2Net在电路图到网表的转换中表现优于现有方法，为基于大语言模型的模拟IC设计提供了有力支持。

Abstract: Large Language Model (LLM) exhibits great potential in designing of analog
integrated circuits (IC) because of its excellence in abstraction and
generalization for knowledge. However, further development of LLM-based analog
ICs heavily relies on textual description of analog ICs, while existing analog
ICs are mostly illustrated in image-based circuit diagrams rather than
text-based netlists. Converting circuit diagrams to netlists help LLMs to
enrich the knowledge of analog IC. Nevertheless, previously proposed conversion
frameworks face challenges in further application because of limited support of
image styles and circuit elements. Up to now, it still remains a challenging
task to effectively convert complex circuit diagrams into netlists. To this
end, this paper constructs and opensources a new dataset with rich styles of
circuit diagrams as well as balanced distribution of simple and complex analog
ICs. And a hybrid framework, named Image2Net, is proposed for practical
conversion from circuit diagrams to netlists. The netlist edit distance (NED)
is also introduced to precisely assess the difference between the converted
netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\%
successful rate, which is 34.62\%-45.19\% higher than previous works.
Specifically, the proposed work shows 0.116 averaged NED, which is
62.1\%-69.6\% lower than state-of-the-arts.

</details>


### [56] [Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System](https://arxiv.org/abs/2508.13231)
*Yunhua Fang,Rui Xie,Asad Ul Haq,Linsen Ma,Kaoutar El Maghraoui,Naigang Wang,Meng Wang,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 论文研究了在异构内存系统中动态放置LLM推理中的KV缓存，以最大化带宽利用率。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的KV缓存频繁访问导致内存带宽受限，现有硬件的高带宽内存（HBM）和高速DRAM为动态缓存调度提供了可能性。

Method: 提出动态KV缓存放置问题，并推导其数学公式和理论上限。

Result: 揭示了运行时优化的巨大潜力。

Conclusion: 首次对异构内存系统中的动态KV缓存调度问题进行了理论分析。

Abstract: Large Language Model (LLM) inference is increasingly constrained by memory
bandwidth, with frequent access to the key-value (KV) cache dominating data
movement. While attention sparsity reduces some memory traffic, the relevance
of past tokens varies over time, requiring the full KV cache to remain
accessible and sustaining pressure on both bandwidth and capacity. With
advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now
integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making
heterogeneous memory systems a practical solution. This work investigates
dynamic KV cache placement across such systems to maximize aggregated bandwidth
utilization under capacity constraints. Rather than proposing a specific
scheduling policy, we formulate the placement problem mathematically and derive
a theoretical upper bound, revealing substantial headroom for runtime
optimization. To our knowledge, this is the first formal treatment of dynamic
KV cache scheduling in heterogeneous memory systems for LLM inference.

</details>


### [57] [Through Silicon Via Aware Design Planning for Thermally Efficient 3-D Integrated Circuits](https://arxiv.org/abs/2508.13160)
*Yibo Chen,Eren Kurshan,Dave Motschman,Charles Johnson,Yuan Xie*

Main category: cs.AR

TL;DR: 提出了一种热感知的3-D IC中信号总线TSV结构放置技术，以减少密集TSV群引起的横向热阻塞问题。


<details>
  <summary>Details</summary>
Motivation: 密集TSV群在缩小尺寸和间距后，可能导致横向热阻塞，加剧局部热点问题。

Method: 提出了一种热感知的TSV群放置技术。

Result: 该技术能够最小化由密集TSV结构引起的横向热阻塞。

Conclusion: 热感知的TSV放置技术能有效缓解3-D IC中的热管理问题。

Abstract: 3-D integrated circuits (3-D ICs) offer performance advantages due to their
increased bandwidth and reduced wire-length enabled by through-silicon-via
structures (TSVs). Traditionally TSVs have been considered to improve the
thermal conductivity in the vertical direction. However, the lateral thermal
blockage effect becomes increasingly important for TSV via farms (a cluster of
TSV vias used for signal bus connections between layers) because the TSV size
and pitch continue to scale in {\mu}m range and the metal to insulator ratio
becomes smaller. Consequently, dense TSV farms can create lateral thermal
blockages in thinned silicon substrate and exacerbate the local hotspots. In
this paper, we propose a thermal-aware via farm placement technique for 3-D ICs
to minimize lateral heat blockages caused by dense signal bus TSV structures.

</details>


### [58] [Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner](https://arxiv.org/abs/2508.13161)
*Zhexuan Xu,Kexin Zhou,Jie Wang,Zijie Geng,Siyuan Xu,Shixiong Kai,Mingxuan Yuan,Feng Wu*

Main category: cs.AR

TL;DR: 论文提出了Piano框架，同时优化模块布局和引脚分配，解决传统布局规划忽视引脚分配的问题，显著提升性能指标。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI物理设计中，传统布局规划工具往往忽视引脚分配及其对后续步骤的影响，导致性能下降。

Method: 构建基于几何关系和网表连接的图，迭代搜索最短路径确定引脚分配，结合局部优化策略提升布局质量。

Result: 实验显示Piano在HPWL、馈通线长、未放置引脚等方面均显著降低，且保持零空白。

Conclusion: Piano框架有效解决了多约束下的布局与引脚分配问题，显著提升了VLSI设计性能。

Abstract: Floorplanning is a critical step in VLSI physical design, increasingly
complicated by modern constraints such as fixed-outline requirements,
whitespace removal, and the presence of pre-placed modules. In addition, the
assignment of pins on module boundaries significantly impacts the performance
of subsequent stages, including detailed placement and routing. However,
traditional floorplanners often overlook pin assignment with modern constraints
during the floorplanning stage. In this work, we introduce Piano, a
floorplanning framework that simultaneously optimizes module placement and pin
assignment under multiple constraints. Specifically, we construct a graph based
on the geometric relationships among modules and their netlist connections,
then iteratively search for shortest paths to determine pin assignments. This
graph-based method also enables accurate evaluation of feedthrough and unplaced
pins, thereby guiding overall layout quality. To further improve the design, we
adopt a whitespace removal strategy and employ three local optimizers to
enhance layout metrics under multi-constraint scenarios. Experimental results
on widely used benchmark circuits demonstrate that Piano achieves an average
6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36%
reduction in the number of feedthrough modules, and a 21.21% drop in unplaced
pins, while maintaining zero whitespace.

</details>


### [59] [FedChip: Federated LLM for Artificial Intelligence Accelerator Chip Design](https://arxiv.org/abs/2508.13162)
*Mahmoud Nazzal,Khoa Nguyen,Deepak Vungarala,Ramtin Zand,Shaahin Angizi,Hai Phan,Abdallah Khreishah*

Main category: cs.AR

TL;DR: FedChip是一种基于联邦学习的微调方法，旨在解决AI硬件设计中LLMs的数据隐私和领域特定训练不足的问题，并通过Chip@k指标评估生成设计的质量。


<details>
  <summary>Details</summary>
Motivation: AI硬件设计自动化面临数据隐私和领域特定训练的挑战，FedChip通过联邦学习保护数据隐私并提升LLMs性能。

Method: 采用联邦学习微调方法（FedChip），多个芯片设计方协作训练共享LLM，同时发布APTPU-Gen数据集（30k设计变体）并引入Chip@k评估指标。

Result: FedChip将设计质量提高了77%以上，同时确保数据隐私。

Conclusion: FedChip为自动化硬件设计提供了一种高效且隐私安全的解决方案。

Abstract: AI hardware design is advancing rapidly, driven by the promise of design
automation to make chip development faster, more efficient, and more accessible
to a wide range of users. Amongst automation tools, Large Language Models
(LLMs) offer a promising solution by automating and streamlining parts of the
design process. However, their potential is hindered by data privacy concerns
and the lack of domain-specific training. To address this, we introduce
FedChip, a Federated fine-tuning approach that enables multiple Chip design
parties to collaboratively enhance a shared LLM dedicated for automated
hardware design generation while protecting proprietary data. FedChip enables
parties to train the model on proprietary local data and improve the shared
LLM's performance. To exemplify FedChip's deployment, we create and release
APTPU-Gen, a dataset of 30k design variations spanning various performance
metric values such as power, performance, and area (PPA). To encourage the LLM
to generate designs that achieve a balance across multiple quality metrics, we
propose a new design evaluation metric, Chip@k, which statistically evaluates
the quality of generated designs against predefined acceptance criteria.
Experimental results show that FedChip improves design quality by more than 77%
over high-end LLMs while maintaining data privacy

</details>


### [60] [Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures](https://arxiv.org/abs/2508.13163)
*Yashasvi Makin,Rahul Maliakkal*

Main category: cs.AR

TL;DR: 该论文探讨了针对NVIDIA、AMD等GPU架构的硬件-软件协同设计技术，旨在通过内存和内核级操作优化提升能效，并提出了未来可持续AI系统的发展方向。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习和AI模型训练消耗大量计算资源和能源，引发可持续发展问题，急需优化能效的方法。

Method: 研究硬件-软件协同设计技术，包括专用张量核心、内存优化方法及软件级优化（如混合精度计算和能量感知调度）。

Result: 通过协同设计显著提升了能效，且不牺牲性能，并通过实际案例验证了方法的有效性。

Conclusion: 硬件-软件协同设计是实现可持续AI训练的关键，未来需进一步填补研究空白。

Abstract: In particular, large-scale deep learning and artificial intelligence model
training uses a lot of computational power and energy, so it poses serious
sustainability issues. The fast rise in model complexity has resulted in
exponential increases in energy consumption, increasing the demand for
techniques maximizing computational efficiency and lowering environmental
impact. This work explores environmentally driven performance optimization
methods especially intended for advanced GPU architectures from NVIDIA, AMD,
and other emerging GPU architectures. Our main focus is on investigating
hardware-software co-design techniques meant to significantly increase
memory-level and kernel-level operations, so improving performance-per-watt
measures. Our thorough research encompasses evaluations of specialized tensor
and matrix cores, advanced memory optimization methods, and creative
integration approaches that taken together result in notable energy efficiency
increases. We also discuss important software-level optimizations that augment
hardware capability including mixed-precision arithmetic, advanced energy-aware
scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we
methodically point out important research gaps and suggest future directions
necessary to create really sustainable artificial intelligence systems. This
paper emphasizes how major increases in training efficiency can be obtained by
co-design of hardware and software, so lowering the environmental impact of
artificial intelligence without compromising performance. To back up our
analysis, we use real-world case studies from top companies like Meta, Google,
Amazon, and others that show how these sustainable AI training methods are used
in the real world.

</details>


### [61] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: 提出了一个结合LLM策略推理与gm/Id方法的协同推理框架，显著提升了模拟IC设计的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 模拟IC设计依赖经验和低效仿真，传统公式在先进节点中失效，直接应用LLM可能导致盲目猜测。

Method: 结合LLM与gm/Id查找表，形成数据驱动的设计框架，通过协同推理实现高效设计。

Result: 在两阶段运算放大器设计中，Gemini模型在5次迭代内满足TT角规格，并扩展到所有PVT角，质量和效率接近专家水平。

Conclusion: LLM与科学电路设计方法结合，为实现真正模拟设计自动化提供可行路径。

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


### [62] [Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices](https://arxiv.org/abs/2508.13181)
*Dominik Loroch,Johannes Feldmann,Vladimir Rybalkin,Norbert Wehn*

Main category: cs.AR

TL;DR: 研究提出一种新型可穿戴设备，用于早期可靠检测房颤（AF），结合FPGA和深度学习，功耗低至3.8mW，连续监测超过三周，准确率达95%。


<details>
  <summary>Details</summary>
Motivation: 解决现有AF检测技术在规模化应用中的挑战，提供更节能、可靠的解决方案。

Method: 采用FPGA架构的贴片式可穿戴设备，结合硬件-软件协同设计和硬件感知的神经架构搜索优化功耗。

Result: 设备功耗仅为3.8mW，连续工作超三周，AF检测准确率95%，优于心脏病专家水平。

Conclusion: 该技术为房颤监测的可扩展性、可靠性和可持续性迈出重要一步。

Abstract: Atrial fibrillation (AF) is a common arrhythmia and major risk factor for
cardiovascular complications. While commercially available devices and
supporting Artificial Intelligence (AI) algorithms exist for reliable detection
of AF, the scaling of this technology to the amount of people who need this
diagnosis is still a major challenge. This paper presents a novel wearable
device, designed specifically for the early and reliable detection of AF. We
present an FPGA-based patch-style wearable monitor with embedded deep
learning-based AF detection. Operating with 3.8mW system power, which is 1-3
orders of magnitude lower than the state-of-the-art, the device enables
continuous AF detection for over three weeks while achieving 95% accuracy,
surpassing cardiologist-level performance. A key innovation is the combination
of energy-efficient hardware-software co-design and optimized power management
through the application of hardware-aware neural architecture search. This
advancement represents a significant step toward scalable, reliable, and
sustainable AF monitoring.

</details>


### [63] [Sub-Millisecond Event-Based Eye Tracking on a Resource-Constrained Microcontroller](https://arxiv.org/abs/2508.13244)
*Marco Giordano,Pietro Bonazzi,Luca Benini,Michele Magno*

Main category: cs.AR

TL;DR: 提出了一种基于微控制器的低延迟、低功耗事件型眼动追踪系统，结合动态视觉传感器和优化的CNN，实现了高效实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式系统中实时、低延迟和低功耗眼动追踪的挑战，适用于智能眼镜等穿戴设备。

Method: 使用动态视觉传感器和STM32N6微控制器，部署硬件和传感器优化的紧凑CNN。

Result: 平均预测误差5.99像素，延迟385μs，能耗155μJ，展现了高效性能。

Conclusion: 该系统为嵌入式眼动追踪提供了一种高能效解决方案。

Abstract: This paper presents a novel event-based eye-tracking system deployed on a
resource-constrained microcontroller, addressing the challenges of real-time,
low-latency, and low-power performance in embedded systems. The system
leverages a Dynamic Vision Sensor (DVS), specifically the DVXplorer Micro, with
an average temporal resolution of 200 {\mu}s, to capture rapid eye movements
with extremely low latency. The system is implemented on a novel low-power and
high-performance microcontroller from STMicroelectronics, the STM32N6. The
microcontroller features an 800 MHz Arm Cortex-M55 core and AI hardware
accelerator, the Neural-ART Accelerator, enabling real-time inference with
milliwatt power consumption. The paper propose a hardware-aware and
sensor-aware compact Convolutional Neuron Network (CNN) optimized for
event-based data, deployed at the edge, achieving a mean pupil prediction error
of 5.99 pixels and a median error of 5.73 pixels on the Ini-30 dataset. The
system achieves an end-to-end inference latency of just 385 {\mu}s and a neural
network throughput of 52 Multiply and Accumulate (MAC) operations per cycle
while consuming just 155 {\mu}J of energy. This approach allows for the
development of a fully embedded, energy-efficient eye-tracking solution
suitable for applications such as smart glasses and wearable devices.

</details>


### [64] [ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models](https://arxiv.org/abs/2508.13257)
*Wenhao Lv,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.AR

TL;DR: 本文提出ViTAD方法，通过构建信号时序依赖图和结合LLMs，自动分析时序违规原因并生成修复策略，成功率比基线提高19.30%。


<details>
  <summary>Details</summary>
Motivation: 传统时序优化依赖人工，效率低且易出错，现代系统对速度要求高，需自动化工具提升优化效率。

Method: ViTAD方法解析Verilog代码和时序报告，构建STDG，结合LLMs分析违规原因，并从知识库生成定制修复方案。

Result: 在54个案例中，ViTAD修复成功率达73.68%，比仅用LLM的基线方法提高19.30%。

Conclusion: ViTAD能高效自动化时序违规修复，显著提升成功率和设计效率，适用于现代VLSI设计。

Abstract: In modern Very Large Scale Integrated (VLSI) circuit design flow, the
Register-Transfer Level (RTL) stage presents a critical opportunity for timing
optimization. Addressing timing violations at this early stage is essential, as
modern systems demand higher speeds, where even minor timing violations can
lead to functional failures or system crashes. However, traditional timing
optimization heavily relies on manual expertise, requiring engineers to
iteratively analyze timing reports and debug. To automate this process, this
paper proposes ViTAD, a method that efficiently analyzes the root causes of
timing violations and dynamically generates targeted repair strategies.
Specifically, we first parse Verilog code and timing reports to construct a
Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation
path analysis and use large language models (LLMs) to infer the root causes of
violations. Finally, by analyzing the causes of violations, we selectively
retrieve relevant debugging knowledge from a domain-specific knowledge base to
generate customized repair solutions. To evaluate the effectiveness of our
method, we construct a timing violation dataset based on real-world open-source
projects. This dataset contains 54 cases of violations. Experimental results
show that our method achieves a 73.68% success rate in repairing timing
violations, while the baseline using only LLM is 54.38%. Our method improves
the success rate by 19.30%.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [65] [A fully-programmable integrated photonic processor for both domain-specific and general-purpose computing](https://arxiv.org/abs/2508.13551)
*Feng-Kai Han,Xiao-Yun Xu,Tian-Yu Zhang,Lei Feng,Chu-Han Wang,Jie Ma,Ze-Feng Lan,Chao-Qian Li,Yi Xie,Hai Yan,Yu-Fei Liu,Yu-Quan Peng,Xian-Min Jin*

Main category: physics.optics

TL;DR: 该论文提出了一种完全可编程的光子计算架构，解决了光学计算在特定领域和通用计算中的局限性，实现了高效解决NP完全问题和高精度通用计算的实验验证。


<details>
  <summary>Details</summary>
Motivation: 面对复杂计算场景对计算能力和能效的高要求，现有光学计算架构缺乏多功能性，无法兼顾专用领域和通用计算需求。

Method: 开发了一种完全可编程的集成光子处理器，利用自主开发的可编程光电计算平台实现端到端控制。

Result: 专用计算成功解决NP完全问题（子集和问题与精确覆盖问题），通用计算实现了高精度点积计算、图像边缘检测和MNIST分类（准确率97%）。

Conclusion: 该工作提升了光学计算架构的多功能性和性能，为未来高性能复杂计算场景的实际应用铺平了道路。

Abstract: A variety of complicated computational scenarios have made unprecedented
demands on the computing power and energy efficiency of electronic computing
systems, including solving intractable nondeterministic polynomial-time
(NP)-complete problems and dealing with large-scale artificial intelligence
models. Optical computing emerges as a promising paradigm to meet these
challenges, whereas current optical computing architectures have limited
versatility. Their applications are usually either constrained to a specialized
domain or restricted to general-purpose matrix computation. Here, we implement
a fully-programmable integrated photonic processor that can be configured to
tackle both specific computational problems and general-purpose matrix
computation. We achieve complete end-to-end control of the photonic processor
by utilizing a self-developed integrated programmable optoelectronic computing
platform. For domain-specific computing, our photonic processor can efficiently
solve two kinds of NP-complete problems: subset sum problem (far more than 2^N
different instances) and exact cover problem. For general-purpose computation,
we experimentally demonstrate high-precision optical dot product and further
realize accurate image edge detection and MNIST handwritten image
classification task with an accuracy of 97%. Our work enhances the versatility
and capability of optical computing architecture, paving the way for its
practical application in future high-performance and complex computing
scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE是一种新型混合专家训练系统，通过跨平台内核等技术提升了训练效率，支持更大规模的模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构在非NVIDIA平台上表现不佳，限制了计算潜力，X-MoE旨在解决这一问题。

Method: X-MoE采用无填充训练、冗余跳过调度和混合并行等技术，优化训练流程。

Result: X-MoE在Frontier超级计算机上成功训练5450亿参数的模型，效率提升10倍。

Conclusion: X-MoE为下一代MoE架构提供了可扩展的高效训练解决方案。

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [67] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 提出了一种名为comatch的数据驱动算法匹配系统，通过选择最有信心的决策并延迟其余决策给人，优化人机协作匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法匹配系统未能实现人机互补性，人类使用算法系统的决策并不比单独使用算法或人类决策更好。

Method: 采用协作方法（comatch），系统仅选择最有信心的决策，其余延迟给人，优化人机参与比例以最大化性能。

Result: 通过800名参与者的实验验证，comatch的性能优于单独的人类或算法匹配。

Conclusion: comatch通过优化人机协作，显著提升了匹配决策的性能。

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [68] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: 提出了不确定性管，一种用于可视化神经网络预测粒子轨迹中不确定性的高效方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对预测不确定性的量化与可视化，神经网络模型在需要高可信度的应用中可靠性受限。

Method: 设计了超椭圆管，结合了Deep Ensembles、MC Dropout和SWAG等不确定性量化技术。

Result: 展示了不确定性管在合成和模拟数据集上的实用效果。

Conclusion: 不确定性管能够直观且高效地表示非对称不确定性，提升了模型的可信度。

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [69] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: Trans-XFed架构结合了联邦学习和可解释AI技术，用于供应链信用评估，解决了隐私、数据不平衡和模型解释性等问题。


<details>
  <summary>Details</summary>
Motivation: 解决供应链信用评估中的隐私保护、数据孤岛、类别不平衡、非独立同分布数据和模型可解释性等挑战。

Method: 采用基于性能的客户选择策略(PBCS)解决类别不平衡和非IID数据，结合FedProx架构和变压器编码器，并使用集成梯度可解释AI技术。

Result: 在真实供应链数据集上的实验表明，Trans-XFed在准确性和透明度上优于基线方法。

Conclusion: Trans-XFed能有效解决供应链信用评估中的复杂问题，同时保持隐私和可解释性。

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 论文研究了如何根据正负例拟合本体和约束，涉及多种逻辑和依赖语言，分析了计算复杂性、算法设计和拟合本体的大小，并探讨了有限基的存在性。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是解决在有限关系结构中，如何有效地拟合本体和约束，以支持知识表示和推理。

Method: 方法包括使用描述逻辑和多种TGD（元组生成依赖）语言，分析计算复杂性，设计算法，并研究拟合本体的规模和有限基的构造。

Result: 研究确定了计算复杂性，提出了拟合本体的算法，并发现某些TGD语言（如全TGD、边界保护TGD）通常不存在有限基。

Conclusion: 结论指出，某些语言（如EL、ELI和保护TGD）能构造有限基，而其他语言则不能，这对本体学习和知识表示有重要影响。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [71] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为CESQL的模型，通过集成模型可解释性分析和执行引导策略，提升了文本到SQL的生成能力，尤其在WHERE子句的语义解析上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在真实场景中的基础能力和泛化能力，特别是减少对条件列数据的依赖和人工标注训练数据的影响。

Method: 结合模型可解释性分析与执行引导策略，并通过过滤调整、逻辑相关性优化和模型融合，设计了CESQL模型。

Result: 在WikiSQL数据集上表现优异，显著提升了预测准确性，同时减少了对条件列数据的依赖。

Conclusion: 该研究为处理复杂查询和真实数据库环境中的不规则数据提供了新思路。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [72] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: AI系统已能自主完成科学研究的全流程，展示了在多学科知识合成和实验设计中的潜力，但仍存在概念细微差别和理论解释的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长和领域专业化限制了研究者跨学科知识合成和统一理论开发的能力，推动了更通用的AI科学系统的探索。

Method: 开发了一种领域无关的自主AI系统，能够独立完成从假设生成到数据收集再到论文撰写的科学研究全流程，并成功设计了三项心理学研究。

Result: AI系统自主执行了在线数据收集、分析流程开发和论文撰写，显示出与经验丰富的研究者相当的理论推理和方法严谨性，但仍缺乏概念细微差别。

Conclusion: 这项研究是通往能够通过真实实验测试假设的具身AI的重要一步，但也引发了关于科学理解和科学荣誉归属的重要问题。

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [73] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 提出了一种基于概率上下文无关语法的动态学习量化公式实例化方法，以平衡利用和探索。


<details>
  <summary>Details</summary>
Motivation: 量化公式是SMT求解器的难点，现有实例化技术各有优缺点，需要一种动态学习方法。

Method: 将观察到的实例化视为潜在语言的样本，使用概率上下文无关语法生成新术语，支持概率反转以实现多样性。

Result: 新方法在保持过去成功实例化的同时，探索了多样化的术语生成。

Conclusion: 该方法为量化推理提供了动态学习与多样性探索的有效途径。

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [74] [The Social Context of Human-Robot Interactions](https://arxiv.org/abs/2508.13982)
*Sydney Thompson,Kate Candon,Marynel Vázquez*

Main category: cs.RO

TL;DR: 该论文调查了HRI领域中“社交语境”的多样性定义，提出了一个概念模型来统一描述，并探讨了其应用和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于“社交语境”在HRI研究中定义不一，导致沟通困难和关联研究难以整合，作者旨在填补这一空白。

Method: 通过文献调查梳理现有定义，提出一个概念模型，并将其应用于现有研究中。

Result: 模型有助于研究者规划交互、开发机器人行为模型及交互后分析，同时提出了未解决的研究问题。

Conclusion: 论文为HRI领域的社交语境提供了统一框架，并指出了未来研究的方向。

Abstract: The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term "social
context" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
"social context". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [75] [Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms](https://arxiv.org/abs/2508.13710)
*Nizheen A. Ali,Ramadhan J. Mstafa*

Main category: eess.IV

TL;DR: 该论文提出了一种新的视频隐写方法，结合遗传算法和AES加密，实现高效且隐蔽的数据嵌入。效果优异，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 随着互联网的普及，确保数据传输的安全性和隐私性需求增加，视频隐写作为一种隐蔽数据的技术受到关注。

Method: 利用遗传算法确定视频中的感兴趣区域（ROI），并用AES加密数据后嵌入视频，最多占用10%的视频容量。

Result: 嵌入容量和效率高，PSNR介于64-75 dB，数据与原视频几乎无法区分，编解码速度快。

Conclusion: 该方法在隐蔽性和效率上表现优秀，适合实时应用。

Abstract: With the widespread use of the internet, there is an increasing need to
ensure the security and privacy of transmitted data. This has led to an
intensified focus on the study of video steganography, which is a technique
that hides data within a video cover to avoid detection. The effectiveness of
any steganography method depends on its ability to embed data without altering
the original video quality while maintaining high efficiency. This paper
proposes a new method to video steganography, which involves utilizing a
Genetic Algorithm (GA) for identifying the Region of Interest (ROI) in the
cover video. The ROI is the area in the video that is the most suitable for
data embedding. The secret data is encrypted using the Advanced Encryption
Standard (AES), which is a widely accepted encryption standard, before being
embedded into the cover video, utilizing up to 10% of the cover video. This
process ensures the security and confidentiality of the embedded data. The
performance metrics for assessing the proposed method are the Peak Signal to
Noise Ratio (PSNR) and the encoding and decoding time. The results show that
the proposed method has a high embedding capacity and efficiency, with a PSNR
ranging between 64 and 75 dBs, which indicates that the embedded data is almost
indistinguishable from the original video. Additionally, the method can encode
and decode data quickly, making it efficient for real time applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [76] [When AI Writes Back: Ethical Considerations by Physicians on AI-Drafted Patient Message Replies](https://arxiv.org/abs/2508.13217)
*Di Hu,Yawen Guo,Ha Na Cho,Emilie Chow,Dana B. Mukamel,Dara Sorkin,Andrew Reikes,Danielle Perret,Deepti Pandita,Kai Zheng*

Main category: cs.CY

TL;DR: 生成式AI有望缓解医生回复患者消息的负担，但其伦理影响尚未充分探讨。研究通过访谈发现，医生关注的伦理问题包括透明度、患者同意、AI角色误解及隐私安全。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在临床实践中应用的伦理影响，以指导未来实施。

Method: 对21名参与GenAI试点项目的医生进行半结构化访谈研究。

Result: 医生认为伦理责任主要在用户而非技术，强调透明度、患者同意和隐私安全是AI使用的关键前提。

Conclusion: 研究结果为未来临床实践中生成式AI的伦理使用提供了重要指导。

Abstract: The increasing burden of responding to large volumes of patient messages has
become a key factor contributing to physician burnout. Generative AI (GenAI)
shows great promise to alleviate this burden by automatically drafting patient
message replies. The ethical implications of this use have however not been
fully explored. To address this knowledge gap, we conducted a semi-structured
interview study with 21 physicians who participated in a GenAI pilot program.
We found that notable ethical considerations expressed by the physician
participants included human oversight as ethical safeguard, transparency and
patient consent of AI use, patient misunderstanding of AI's role, and patient
privacy and data security as prerequisites. Additionally, our findings suggest
that the physicians believe the ethical responsibility of using GenAI in this
context primarily lies with users, not with the technology. These findings may
provide useful insights into guiding the future implementation of GenAI in
clinical practice.

</details>


### [77] [Exit Stories: Using Reddit Self-Disclosures to Understand Disengagement from Problematic Communities](https://arxiv.org/abs/2508.13837)
*Shruti Phadke*

Main category: cs.CY

TL;DR: 该研究分析了Reddit上15K条退出故事，探讨了从宗教、男性主义社区、阴谋论等群体退出的多层面过程，强调了心理健康干预的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解用户如何从有害群体（如阴谋论社区）中退出的自然过程，为设计干预措施提供洞察。

Method: 采用跨学科框架，结合社会心理学、组织行为学和暴力极端主义研究理论，分析了131个子版块的退出故事。

Result: 从阴谋论等群体退出是一个多层次过程，与退出宗教或政治意识形态不同，需超越信息问题干预。

Conclusion: 研究呼吁关注心理健康干预，为未来研究提供方向。

Abstract: Online platforms like Reddit are increasingly becoming popular for
individuals sharing personal experiences of leaving behind social, ideological,
and political groups. Specifically, a series of "ex-" subreddits on Reddit
allow users to recount their departures from commitments such as religious
affiliations, manosphere communities, conspiracy theories or political beliefs,
and lifestyle choices. Understanding the natural process through which users
exit, especially from problematic groups such as conspiracy theory communities
and the manosphere, can provide valuable insights for designing interventions
targeting disengagement from harmful ideologies. This paper presents an
in-depth exploration of 15K exit stories across 131 subreddits, focusing on
five key areas: religion, manosphere, conspiracy theories, politics, and
lifestyle. Using a transdisciplinary framework that incorporates theories from
social psychology, organizational behavior, and violent extremism studies, this
work identifies a range of factors contributing to disengagement. The results
describe how disengagement from problematic groups, such as conspiracy theories
and the manosphere, is a multi-faceted process that is qualitatively different
than disengaging from more established social structures, such as religions or
political ideologies. This research further highlights the need for moving
beyond interventions that treat conspiracy theorizing solely as an information
problem and contributes insights for future research focusing on offering
mental health interventions and support in exit communities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Mitigating Easy Option Bias in Multiple-Choice Question Answering](https://arxiv.org/abs/2508.13428)
*Hao Zhang,Chen Li,Basura Fernando*

Main category: cs.CV

TL;DR: 研究发现多选视觉问答基准中存在Easy-Options Bias（EOB）问题，视觉语言模型仅通过视觉和选项即可匹配正确答案。提出GroundAttack工具生成硬负选项，解决EOB问题。


<details>
  <summary>Details</summary>
Motivation: 揭示并解决多选视觉问答基准中的EOB问题，以更真实评估视觉语言模型的问答能力。

Method: 设计GroundAttack工具生成视觉上接近的硬负选项，应用于NExT-QA和MMStar数据集，消除EOB。

Result: 在消除EOB的数据集上，模型仅通过视觉和选项的准确率接近随机，更真实反映模型能力。

Conclusion: GroundAttack有效消除EOB，为视觉问答评估提供更可靠基准。

Abstract: In this early study, we observe an Easy-Options Bias (EOB) issue in some
multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar,
RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias
allows vision-language models (VLMs) to select the correct answer using only
the vision (V) and options (O) as inputs, without the need for the question
(Q). Through grounding experiments, we attribute the bias to an imbalance in
visual relevance: the correct answer typically aligns more closely with the
visual contents than the negative options in feature space, creating a shortcut
for VLMs to infer the answer via simply vision-option similarity matching. To
fix this, we introduce GroundAttack, a toolkit that automatically generates
hard negative options as visually plausible as the correct answer. We apply it
to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these
EOB-free annotations, current VLMs approach to random accuracies under (V+O)
settings, and drop to non-saturated accuracies under (V+Q+O) settings,
providing a more realistic evaluation of VLMs' QA ability. Codes and new
annotations will be released soon.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [79] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: 该论文首次对四种公开漏洞评分系统进行了大规模实证比较，揭示了评分不一致性及其对漏洞管理的实际影响。


<details>
  <summary>Details</summary>
Motivation: 由于现有漏洞评分系统的目标和方法不同，导致优先级决策不一致，论文旨在通过实证比较提供更透明的风险评估。

Method: 使用600个真实漏洞数据，比较CVSS、SSVC、EPSS和Exploitability Index四种系统在漏洞管理和风险预测中的表现。

Result: 发现不同评分系统对同一漏洞的排名显著不一致，影响了基于风险的决策。

Conclusion: 需要更透明和一致的漏洞评估方法，以支持有效的漏洞管理决策。

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [80] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: 本文综述了联邦学习的现有攻击与防御方法，分类为安全增强和隐私保护技术，分析了优缺点，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽保护数据隐私，但仍面临安全和隐私威胁，需系统梳理现有解决方案以指导未来研究。

Method: 综述200多篇论文，分类攻击与防御方法，分析技术优劣及非独立同分布数据的影响。

Result: 总结了安全增强和隐私保护技术的优劣，讨论了隐私、安全与模型性能的权衡。

Conclusion: 未来需开发适应动态异构环境的可扩展、自适应且高效的解决方案，以提升联邦学习的稳健性和隐私性。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [81] [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](https://arxiv.org/abs/2508.13990)
*Daniel Klötzl,Ozan Tastekin,David Hägele,Marina Evers,Daniel Weiskopf*

Main category: stat.ML

TL;DR: 该论文提出了一种不确定性感知的主成分分析方法（UAPCA），用于将非正态分布的多维数据投影到低维空间，并采用高斯混合模型（GMM）建模，结果表明其投影效果优于传统UAPCA。


<details>
  <summary>Details</summary>
Motivation: 多维数据通常伴随非正态分布的不确定性，传统方法难以有效描述和投影此类分布。

Method: 采用高斯混合模型（GMM）建模多维分布，并通过一般性公式实现任意概率密度函数的低维投影。

Result: 低维投影结果比传统UAPCA更详细且更忠实于原始分布，同时支持用户自定义权重调整分布重要性。

Conclusion: 该方法在低维空间中的分布表现优于传统UAPCA，并通过实验验证了其有效性。

Abstract: Multidimensional data is often associated with uncertainties that are not
well-described by normal distributions. In this work, we describe how such
distributions can be projected to a low-dimensional space using
uncertainty-aware principal component analysis (UAPCA). We propose to model
multidimensional distributions using Gaussian mixture models (GMMs) and derive
the projection from a general formulation that allows projecting arbitrary
probability density functions. The low-dimensional projections of the densities
exhibit more details about the distributions and represent them more faithfully
compared to UAPCA mappings. Further, we support including user-defined weights
between the different distributions, which allows for varying the importance of
the multidimensional distributions. We evaluate our approach by comparing the
distributions in low-dimensional space obtained by our method and UAPCA to
those obtained by sample-based projections.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [82] [BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web](https://arxiv.org/abs/2508.13787)
*Zihan Guo,Yuanjian Zhou,Chenyi Wang,Linlin You,Minjie Bian,Weinan Zhang*

Main category: cs.MA

TL;DR: 本文介绍了基于区块链的可信代理网络BetaWeb，旨在解决当前AI代理生态系统的碎片化和封闭性问题，提出从Web3向Web3.5演进的路线图。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理生态系统存在碎片化和封闭性问题，需要一种可信且可扩展的架构以支持大规模、异构和跨域自主交互。

Method: 提出BetaWeb框架，利用区块链技术提供可信基础设施，并设计五阶段演进路线图。

Result: BetaWeb有望推动从Web3向Web3.5的演进，支持代理能力的拥有和智能的货币化。

Conclusion: 区块链与大规模语言模型的深度整合可为构建弹性、可信且可持续激励的数字生态系统奠定基础。

Abstract: The rapid development of large language models (LLMs) has significantly
propelled the development of artificial intelligence (AI) agents, which are
increasingly evolving into diverse autonomous entities, advancing the LLM-based
multi-agent systems (LaMAS). However, current agentic ecosystems remain
fragmented and closed. Establishing an interconnected and scalable paradigm for
Agentic AI has become a critical prerequisite. Although Agentic Web proposes an
open architecture to break the ecosystem barriers, its implementation still
faces core challenges such as privacy protection, data management, and value
measurement. Existing centralized or semi-centralized paradigms suffer from
inherent limitations, making them inadequate for supporting large-scale,
heterogeneous, and cross-domain autonomous interactions. To address these
challenges, this paper introduces the blockchain-enabled trustworthy Agentic
Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not
only offers a trustworthy and scalable infrastructure for LaMAS but also has
the potential to advance the Web paradigm from Web3 (centered on data
ownership) towards Web3.5, which emphasizes ownership of agent capabilities and
the monetization of intelligence. Beyond a systematic examination of the
BetaWeb framework, this paper presents a five-stage evolutionary roadmap,
outlining the path of LaMAS from passive execution to advanced collaboration
and autonomous governance. We also conduct a comparative analysis of existing
products and discuss key challenges of BetaWeb from multiple perspectives.
Ultimately, we argue that deep integration between blockchain and LaMAS can lay
the foundation for a resilient, trustworthy, and sustainably incentivized
digital ecosystem. A summary of the enabling technologies for each stage is
available at https://github.com/MatZaharia/BetaWeb.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [83] [Analog computation with transcriptional networks](https://arxiv.org/abs/2508.14017)
*David Doty,Mina Latifi,David Soloveichick*

Main category: cs.CC

TL;DR: 论文证明了仅通过控制转录因子的生产（转录速率）而不显式控制降解，即可实现模拟计算的数学完备性，简化了合成转录网络的设计复杂性。


<details>
  <summary>Details</summary>
Motivation: 模拟计算在生物系统中至关重要，但传统的转录电路通常依赖高度非线性的转录因子行为，且仅关注蛋白质生产而忽略降解调控。本研究旨在探索仅通过生产调控是否能实现模拟计算的完备性。

Method: 论文通过数学证明和实际应用案例，验证了仅调控转录速率即可实现模拟计算的完备性，包括振荡、混沌动态、模拟排序、记忆等功能。

Result: 研究表明，无需显式控制降解速率，仅通过调控转录速率即可实现复杂的模拟计算功能，并开发了一个Python编译工具，可将多项式ODE系统转换为等效的转录网络。

Conclusion: 研究为合成转录网络的设计提供了简化方法，避免了降解控制的复杂性，同时拓展了对自然转录电路能力的理解。

Abstract: Transcriptional networks represent one of the most extensively studied types
of systems in synthetic biology. Although the completeness of transcriptional
networks for digital logic is well-established, *analog* computation plays a
crucial role in biological systems and offers significant potential for
synthetic biology applications. While transcriptional circuits typically rely
on cooperativity and highly non-linear behavior of transcription factors to
regulate *production* of proteins, they are often modeled with simple linear
*degradation* terms. In contrast, general analog dynamics require both
non-linear positive as well as negative terms, seemingly necessitating control
over not just transcriptional (i.e., production) regulation but also the
degradation rates of transcription factors.
  Surprisingly, we prove that controlling transcription factor production
(i.e., transcription rate) without explicitly controlling degradation is
mathematically complete for analog computation, achieving equivalent
capabilities to systems where both production and degradation are programmable.
We demonstrate our approach on several examples including oscillatory and
chaotic dynamics, analog sorting, memory, PID controller, and analog extremum
seeking. Our result provides a systematic methodology for engineering novel
analog dynamics using synthetic transcriptional networks without the added
complexity of degradation control and informs our understanding of the
capabilities of natural transcriptional circuits.
  We provide a compiler, in the form of a Python package that can take any
system of polynomial ODEs and convert it to an equivalent transcriptional
network implementing the system *exactly*, under appropriate conditions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 评估28个大语言模型在非单调逻辑中的20种可废止推理模式上的表现，发现前沿模型表现不一，few-shot提示略有帮助，而CoT提示则可能导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在复杂例外行为和默认推理中的表现，对语言学、哲学、逻辑学和认知科学有重要意义。

Method: 通过零样本和few-shot提示，以及链式思维（CoT）提示，测试模型对通用概括（如“鸟会飞”）的推理能力。

Result: 前沿模型在默认推理中表现不一，few-shot提示略有提升，但CoT提示普遍导致性能下降。大多数模型难以区分可废止推理与演绎推理。

Conclusion: 当前大语言模型在默认推理中表现有限，虽有潜力但仍有明显的改进空间。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [85] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 这篇论文通过贝叶斯方法评估了主流语言模型在道德理解上的表现，发现AI模型通常能位列人类标注者前25%，并在减少假阴性方面优于人类。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较大型语言模型与人类在道德维度理解上的差异，弥补了过去研究中确定性标注方法的不足，通过捕捉标注者的分歧来更全面地评估模型。

Method: 采用GPU优化的贝叶斯框架，对250K+标注数据（来自约700名标注者）进行分析，评估了Claude Sonnet 4、DeepSeek-V3和Llama 4 Maverick等模型。

Result: AI模型表现优于平均人类标注者，排名前25%，且假阴性率更低，表明其在道德检测上更敏感。

Conclusion: 研究展示了AI模型在道德理解上的潜力，尤其是在减少假阴性方面的优势，为未来研究提供了新方向。

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [86] [Proceedings of the 22nd International Conference on Quantum Physics and Logic](https://arxiv.org/abs/2508.13619)
*Alejandro Díaz-Caro,Ognyan Oreshkov,Ana Belén Sainz*

Main category: quant-ph

TL;DR: QPL 2025会议论文集，关注量子计算、量子物理及相关领域的数学基础研究。


<details>
  <summary>Details</summary>
Motivation: 聚集学术界和产业界研究人员，推动量子物理与逻辑领域的数学基础研究。

Method: 利用代数、范畴论、形式语言和语义方法等数学与计算机科学技术。

Result: 展示量子物理系统、物理过程及其组合的研究成果。

Conclusion: QPL会议为量子物理与逻辑领域的研究提供了重要平台。

Abstract: This volume contains the proceedings of the 22nd International Conference on
Quantum Physics and Logic (QPL 2025), which was held from 14th to 18th July
2025, in Varna, Bulgaria, organised by Universit\'e libre de Bruxelles. QPL is
an annual conference that brings together academic and industry researchers
working on the mathematical foundations of quantum computation, quantum
physics, and related areas. The main focus is on the use of algebraic and
categorical structures, formal languages, semantic methods, as well as other
mathematical and computer scientific techniques applicable to the study of
physical systems, physical processes, and their composition.

</details>


### [87] [Portfolio construction using a sampling-based variational quantum scheme](https://arxiv.org/abs/2508.13557)
*Gabriele Agliardi,Dimitris Alevras,Vaibhaw Kumar,Roberto Lo Nardo,Gabriele Compostella,Sumit Kumar,Manuel Proissl,Bimal Mehta*

Main category: quant-ph

TL;DR: 论文研究了在金融领域如何高效构建满足实际约束的投资组合，通过量子算法（VQA）与传统局部搜索结合，解决了大规模问题的计算难题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索量子计算在投资组合优化中的应用潜力，尤其是解决传统方法难以应对的复杂问题。

Method: 采用基于采样的CVaR变分量子算法（VQA）并结合局部搜索后处理，设计了适合VQA的问题形式，并在IBM Heron处理器上进行实验。

Result: 实验涉及109个量子比特和4200个门，相对解误差为0.49%，显示量子-经典混合方法优于纯经典方法。

Conclusion: 量子电路在复杂问题中可能提供更好的收敛性，为量子计算在投资组合优化中的应用奠定了基础。

Abstract: The efficient and effective construction of portfolios that adhere to
real-world constraints is a challenging optimization task in finance. We
investigate a concrete representation of the problem with a focus on design
proposals of an Exchange Traded Fund. We evaluate the sampling-based CVaR
Variational Quantum Algorithm (VQA), combined with a local-search
post-processing, for solving problem instances that beyond a certain size
become classically hard. We also propose a problem formulation that is suited
for sampling-based VQA. Our utility-scale experiments on IBM Heron processors
involve 109 qubits and up to 4200 gates, achieving a relative solution error of
0.49%. Results indicate that a combined quantum-classical workflow achieves
better accuracy compared to purely classical local search, and that
hard-to-simulate quantum circuits may lead to better convergence than simpler
circuits. Our work paves the path to further explore portfolio construction
with quantum computers.

</details>


### [88] [PennyLane-Lightning MPI: A massively scalable quantum circuit simulator based on distributed computing in CPU clusters](https://arxiv.org/abs/2508.13615)
*Ji-Hoon Kang,Hoon Ryu*

Main category: quant-ph

TL;DR: PennyLane-Lightning MPI是一种基于MPI的并行化量子电路模拟工具，通过分布式内存系统实现高性能模拟，支持最多41量子位的模拟。


<details>
  <summary>Details</summary>
Motivation: 量子电路模拟在理论算法与物理实现之间起着关键作用，但其计算复杂性随着量子位数增加而指数增长，需要高效的并行化解决方案。

Method: 采用MPI并行化技术，结合量子门特性和量子态向量分区策略，优化分布式计算性能。

Result: 在单门和量子电路的基准测试中表现出优于基于酉矩阵的通用方法的性能，并展示了出色的可扩展性。

Conclusion: 该工作为PennyLane生态系统提供了高性能量子模拟后端，支持标准多核CPU集群，有助于量子计算云服务的发展。

Abstract: Quantum circuit simulations play a critical role in bridging the gap between
theoretical quantum algorithms and their practical realization on physical
quantum hardware, yet they face computational challenges due to the exponential
growth of quantum state spaces with increasing qubit size. This work presents
PennyLane-Lightning MPI, an MPI-based extension of the PennyLane-Lightning
suite, developed to enable scalable quantum circuit simulations through
parallelization of quantum state vectors and gate operations across
distributed-memory systems. The core of this implementation is an
index-dependent, gate-specific parallelization strategy, which fully exploits
the characteristic of individual gates as well as the locality of computation
associated with qubit indices in partitioned state vectors. Benchmarking tests
with single gates and well-designed quantum circuits show that the present
method offers advantages in performance over general methods based on unitary
matrix operations and exhibits excellent scalability, supporting simulations of
up to 41-qubit with hundreds of thousands of parallel processes. Being equipped
with a Python plug-in for seamless integration to the PennyLane framework, this
work contributes to extending the PennyLane ecosystem by enabling
high-performance quantum simulations in standard multi-core CPU clusters with
no library-specific requirements, providing a back-end resource for the
cloud-based service framework of quantum computing that is under development in
the Republic of Korea.

</details>
