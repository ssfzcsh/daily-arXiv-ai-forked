{"id": "2507.11671", "pdf": "https://arxiv.org/pdf/2507.11671", "abs": "https://arxiv.org/abs/2507.11671", "authors": ["Mst Shamima Aktar", "Peng Liang", "Muhammad Waseem", "Amjed Tahir", "Mojtaba Shahin", "Muhammad Azeem Akbar", "Arif Ali Khan", "Aakash Ahmad", "Musengamana Jean de Dieu", "Ruiyin Li"], "title": "Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems", "categories": ["cs.SE"], "comment": "49 pages, 10 images, 16 tables, Manuscript submitted to a journal\n  (2025)", "summary": "Quantum software represents disruptive technologies in terms of\nquantum-specific software systems, services, and applications - leverage the\nprinciples of quantum mechanics via programmable quantum bits (Qubits) that\nmanipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.\nQuantum software architecture enables quantum software developers to abstract\naway implementation-specific details (i.e., mapping of Qubits and QuGates to\nhigh-level architectural components and connectors). Architectural patterns and\nstrategies can provide reusable knowledge and best practices to engineer\nquantum software systems effectively and efficiently. However, quantum software\npractitioners face significant challenges in selecting and implementing\nappropriate patterns and strategies due to the complexity of quantum software\nsystems and the lack of guidelines. To address these challenges, this study\nproposes decision models for selecting patterns and strategies in six critical\ndesign areas in quantum software systems: Communication, Decomposition, Data\nProcessing, Fault Tolerance, Integration and Optimization, and Algorithm\nImplementation. These decision models are constructed based on data collected\nfrom both a mining study (i.e., GitHub and Stack Exchange) and a Systematic\nLiterature Review, which were used to identify relevant patterns and strategies\nwith their involved Quality Attributes (QAs). We then conducted semi-structured\ninterviews with 16 quantum software practitioners to evaluate the familiarity,\nunderstandability, completeness, and usefulness of the proposed decision\nmodels. The results show that the proposed decision models can aid\npractitioners in selecting suitable patterns and strategies to address the\nchallenges related to the architecture design of quantum software systems. The\ndataset is available at [6], allowing the community to reproduce and build upon\nour findings.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u91cf\u5b50\u8f6f\u4ef6\u7cfb\u7edf\u7684\u51b3\u7b56\u6a21\u578b\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u9009\u62e9\u5408\u9002\u7684\u8bbe\u8ba1\u6a21\u5f0f\u548c\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u67b6\u6784\u8bbe\u8ba1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u8005\u7f3a\u4e4f\u9009\u62e9\u548c\u5b9e\u65bd\u5408\u9002\u6a21\u5f0f\u548c\u7b56\u7565\u7684\u6307\u5357\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u6587\u732e\u7efc\u8ff0\u6536\u96c6\u6a21\u5f0f\u548c\u7b56\u7565\uff0c\u6784\u5efa\u51b3\u7b56\u6a21\u578b\uff0c\u5e76\u8bbf\u8c0816\u4f4d\u4ece\u4e1a\u8005\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u51b3\u7b56\u6a21\u578b\u5728\u719f\u6089\u5ea6\u3001\u6613\u7406\u89e3\u6027\u3001\u5b8c\u6574\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u51b3\u7b56\u6a21\u578b\u80fd\u6709\u6548\u8f85\u52a9\u91cf\u5b50\u8f6f\u4ef6\u7cfb\u7edf\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u6570\u636e\u516c\u5f00\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2507.11687", "pdf": "https://arxiv.org/pdf/2507.11687", "abs": "https://arxiv.org/abs/2507.11687", "authors": ["Atharva Naik", "Lawanya Baghel", "Dhakshin Govindarajan", "Darsh Agrawal", "Daniel Fried", "Carolyn Rose"], "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.", "AI": {"tldr": "MetaLint\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u548c\u5408\u6210\u6570\u636e\u6765\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u80fd\u529b\uff0c\u80fd\u591f\u9002\u5e94\u65b0\u7684\u4ee3\u7801\u6a21\u5f0f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u4e0a\u56e0\u9759\u6001\u8bad\u7ec3\u6570\u636e\u9650\u5236\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u52a8\u6001\u6700\u4f73\u5b9e\u8df5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MetaLint\u91c7\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u5229\u7528\u5408\u6210linter\u751f\u6210\u7684\u6570\u636e\uff0c\u652f\u6301\u4ece\u6613\u5230\u96be\u7684\u6cdb\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u4ee3\u7801\u6a21\u5f0f\u3002", "result": "MetaLint\u5728\u672a\u89c1\u7684PEP\u4e60\u60ef\u7528\u6cd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0cF\u5206\u6570\u8fbe70.37%\uff0c\u5b9a\u4f4d\u80fd\u529b\u8fbe26.73%\uff0c\u4e0e\u66f4\u5927\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "MetaLint\u5c55\u793a\u4e86\u5176\u5728\u672a\u6765\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u3002"}}
{"id": "2507.11689", "pdf": "https://arxiv.org/pdf/2507.11689", "abs": "https://arxiv.org/abs/2507.11689", "authors": ["Sergio Di Meglio", "Valeria Pontillo", "Luigi Libero Lucio Starace"], "title": "REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps", "categories": ["cs.SE"], "comment": "Manuscript accepted for the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA)", "summary": "In Computer Science Bachelor's programs, software quality is often\nunderemphasized due to limited time and a focus on foundational skills, leaving\nmany students unprepared for industry expectations. To better understand the\ntypical quality of student code and inform both education and hiring practices,\nwe analyze 40 full-stack web applications developed in a third-year Web\nTechnologies course. Using an automated static analysis pipeline, we assess\nadherence to REST API design rules. Results reveal frequent violations of\nfoundational conventions, such as missing hyphens in endpoint paths (98%),\nincorrect pluralization (88%), and misuse of HTTP methods (83%). These findings\nhighlight the need for more focused instruction on API design and support the\nadoption of automated tools to improve code quality in student projects.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b66\u751f\u5728Web\u5f00\u53d1\u8bfe\u7a0b\u4e2d\u5f00\u53d1\u7684REST API\u5e38\u8fdd\u53cd\u57fa\u672c\u8bbe\u8ba1\u89c4\u5219\uff0c\u5efa\u8bae\u52a0\u5f3aAPI\u8bbe\u8ba1\u6559\u5b66\u5e76\u4f7f\u7528\u81ea\u52a8\u5316\u5de5\u5177\u6539\u8fdb\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u8bfe\u7a0b\u4e2d\u8f6f\u4ef6\u8d28\u91cf\u5e38\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u6bd5\u4e1a\u751f\u96be\u4ee5\u6ee1\u8db3\u884c\u4e1a\u9700\u6c42\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5b66\u751f\u4ee3\u7801\u8d28\u91cf\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u5bf940\u4e2a\u672c\u79d1\u4e09\u5e74\u7ea7\u5b66\u751f\u5f00\u53d1\u7684\u5168\u6808Web\u5e94\u7528\u4e2d\u7684REST API\u8bbe\u8ba1\u89c4\u5219\u9075\u5b88\u60c5\u51b5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u5b66\u751f\u4ee3\u7801\u4e2d\u666e\u904d\u5b58\u5728\u8fdd\u53cdREST API\u8bbe\u8ba1\u89c4\u5219\u7684\u884c\u4e3a\uff0c\u5982\u7aef\u70b9\u8def\u5f84\u7f3a\u5931\u8fde\u5b57\u7b26\uff0898%\uff09\u3001\u590d\u6570\u5f62\u5f0f\u9519\u8bef\uff0888%\uff09\u548cHTTP\u65b9\u6cd5\u8bef\u7528\uff0883%\uff09\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u52a0\u5f3aAPI\u8bbe\u8ba1\u6559\u5b66\uff0c\u5e76\u652f\u6301\u5f15\u5165\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u9ad8\u5b66\u751f\u9879\u76ee\u7684\u4ee3\u7801\u8d28\u91cf\u3002"}}
{"id": "2507.11898", "pdf": "https://arxiv.org/pdf/2507.11898", "abs": "https://arxiv.org/abs/2507.11898", "authors": ["Rathin Singha", "Harry Qian", "Srinath Saikrishnan", "Tracy Zhao", "Ryan Beckett", "Siva Kesava Reddy Kakarla", "George Varghese"], "title": "Extremal Testing for Network Software using LLMs", "categories": ["cs.SE", "cs.NI"], "comment": null, "summary": "Physicists often manually consider extreme cases when testing a theory. In\nthis paper, we show how to automate extremal testing of network software using\nLLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS\nname length limits); then ask the LLM to generate tests that violate the\nconstraints. We demonstrate how easy this process is by generating extremal\ntests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.\nWe show how this methodology extends to centralized network software such as\nshortest path algorithms, and how LLMs can generate filtering code to reject\nextremal input. We propose using agentic AI to further automate extremal\ntesting. LLM-generated extremal testing goes beyond an old technique in\nsoftware testing called Boundary Value Analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u6781\u503c\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f51\u7edc\u8f6f\u4ef6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u751f\u6210\u8fdd\u53cd\u8f93\u5165\u7ea6\u675f\u7684\u6d4b\u8bd5\u7528\u4f8b\u53d1\u73b0\u65b0\u6f0f\u6d1e\u3002", "motivation": "\u7269\u7406\u5b66\u5bb6\u5e38\u624b\u52a8\u6d4b\u8bd5\u6781\u7aef\u60c5\u51b5\u4ee5\u9a8c\u8bc1\u7406\u8bba\uff0c\u4f46\u7f51\u7edc\u8f6f\u4ef6\u7684\u6781\u503c\u6d4b\u8bd5\u4ecd\u9700\u81ea\u52a8\u5316\u3002", "method": "\u5206\u4e24\u6b65\uff1a1) \u7528LLM\u751f\u6210\u8f93\u5165\u7ea6\u675f\uff08\u5982DNS\u540d\u79f0\u957f\u5ea6\u9650\u5236\uff09\uff1b2) \u7528LLM\u751f\u6210\u8fdd\u53cd\u7ea6\u675f\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728HTTP\u3001BGP\u548cDNS\u5b9e\u73b0\u4e2d\u53d1\u73b0\u65b0\u6f0f\u6d1e\uff0c\u4e14\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u96c6\u4e2d\u5f0f\u7f51\u7edc\u8f6f\u4ef6\uff08\u5982\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\uff09\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6781\u503c\u6d4b\u8bd5\u8d85\u8d8a\u4e86\u4f20\u7edf\u8fb9\u754c\u503c\u5206\u6790\uff0c\u5efa\u8bae\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u8fdb\u4e00\u6b65\u81ea\u52a8\u5316\u3002"}}
{"id": "2507.11794", "pdf": "https://arxiv.org/pdf/2507.11794", "abs": "https://arxiv.org/abs/2507.11794", "authors": ["Nak-Jun Sung", "Jun Ma", "TaeHeon Kim", "Yoo-joo Choi", "Min-Hyung Choi", "Min Hong"], "title": "Real-Time Cloth Simulation Using WebGPU: Evaluating Limits of High-Resolution", "categories": ["cs.GR"], "comment": null, "summary": "This study explores the capabilities of WebGPU, an emerging web graphics\nparadigm, for real-time cloth simulation. Traditional WebGL-based methods have\nbeen in handling complex physical simulations due to their emphasis on graphics\nrendering rather than general-purpose GPU (GPGPU) operations. WebGPU, designed\nto provide modern 3D graphics and computational capabilities, offers\nsignificant improvements through parallel processing and support for\ncomputational shaders. In this work, we implemented a cloth simulation system\nusing the Mass-Spring Method within the WebGPU framework, integrating collision\ndetection and response handling with the 3D surface model. First, comparative\nperformance evaluations demonstrate that WebGPU substantially outperforms\nWebGL, particularly in high-resolution simulations, maintaining 60 frames per\nsecond (fps) even with up to 640K nodes. The second experiment aimed to\ndetermine the real-time limitations of WebGPU and confirmed that WebGPU can\nhandle real-time collisions between 4K and 100k cloth node models and a 100K\ntriangle surface model in real-time. These experiments also highlight the\nimportance of balancing real-time performance with realistic rendering when\nhandling collisions between cloth models and complex 3D objects. Our source\ncode is available at https://github.com/nakjun/Cloth-Simulation-WebGPU", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86WebGPU\u5728\u5b9e\u65f6\u5e03\u6599\u6a21\u62df\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u76f8\u8f83\u4e8eWebGL\uff0cWebGPU\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u548c\u8ba1\u7b97\u7740\u8272\u5668\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u6a21\u62df\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edfWebGL\u65b9\u6cd5\u5728\u590d\u6742\u7269\u7406\u6a21\u62df\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u800cWebGPU\u63d0\u4f9b\u4e86\u66f4\u73b0\u4ee3\u5316\u7684\u56fe\u5f62\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u9002\u5408\u5b9e\u73b0\u9ad8\u6548\u5e03\u6599\u6a21\u62df\u3002", "method": "\u7814\u7a76\u91c7\u7528Mass-Spring\u65b9\u6cd5\uff0c\u7ed3\u5408\u78b0\u649e\u68c0\u6d4b\u548c\u54cd\u5e94\u5904\u7406\uff0c\u5728WebGPU\u6846\u67b6\u4e0b\u5b9e\u73b0\u5e03\u6599\u6a21\u62df\u7cfb\u7edf\uff0c\u5e76\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "WebGPU\u5728\u9ad8\u5206\u8fa8\u7387\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u4fdd\u630160fps\u7684\u5e27\u7387\uff0c\u5e76\u5904\u74064K\u81f3100k\u8282\u70b9\u6a21\u578b\u7684\u5b9e\u65f6\u78b0\u649e\u3002", "conclusion": "WebGPU\u5728\u5b9e\u65f6\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5e03\u6599\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11676", "pdf": "https://arxiv.org/pdf/2507.11676", "abs": "https://arxiv.org/abs/2507.11676", "authors": ["Chris Heunen", "Louis Lemonnier", "Christopher McNally", "Alex Rice"], "title": "Quantum circuits are just a phase", "categories": ["cs.PL", "cs.LO", "quant-ph"], "comment": "43 pages, 5 figures", "summary": "Quantum programs today are written at a low level of abstraction - quantum\ncircuits akin to assembly languages - and even advanced quantum programming\nlanguages essentially function as circuit description languages. This state of\naffairs impedes scalability, clarity, and support for higher-level reasoning.\nMore abstract and expressive quantum programming constructs are needed.\n  To this end, we introduce a novel yet simple quantum programming language for\ngenerating unitaries from \"just a phase\"; we combine a (global) phase operation\nthat captures phase shifts with a quantum analogue of the \"if let\" construct\nthat captures subspace selection via pattern matching. This minimal language\nlifts the focus from quantum gates to eigendecomposition, conjugation, and\ncontrolled unitaries; common building blocks in quantum algorithm design.\n  We demonstrate several aspects of the expressive power of our language in\nseveral ways. Firstly, we establish that our representation is universal by\nderiving a universal quantum gate set. Secondly, we show that important quantum\nalgorithms can be expressed naturally and concisely, including Grover's search\nalgorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal\nProcessing, and the Quantum Eigenvalue Transformation. Furthermore, we give\nclean denotational semantics grounded in categorical quantum mechanics.\nFinally, we implement a prototype compiler that efficiently translates terms of\nour language to quantum circuits, and prove that it is sound with respect to\nthese semantics. Collectively, these contributions show that this construct\noffers a principled and practical step toward more abstract and structured\nquantum programming.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u7b80\u5355\u7684\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\uff0c\u901a\u8fc7\u63d0\u5347\u7f16\u7a0b\u62bd\u8c61\u7ea7\u522b\u6765\u89e3\u51b3\u73b0\u6709\u91cf\u5b50\u7f16\u7a0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6e05\u6670\u5ea6\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u8fc7\u4e8e\u5e95\u5c42\uff0c\u7f3a\u4e4f\u9ad8\u7ea7\u62bd\u8c61\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u5c42\u6b21\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u64cd\u4f5c\u548c\u91cf\u5b50\"if let\"\u6784\u9020\u7684\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\uff0c\u4e13\u6ce8\u4e8e\u7279\u5f81\u5206\u89e3\u3001\u5171\u8f6d\u548c\u53d7\u63a7\u5e7a\u6b63\u7b49\u91cf\u5b50\u7b97\u6cd5\u8bbe\u8ba1\u7684\u57fa\u7840\u6784\u5efa\u5757\u3002", "result": "\u901a\u8fc7\u5c55\u793a\u8be5\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u901a\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728Grover\u641c\u7d22\u7b97\u6cd5\u3001\u54c8\u5bc6\u987f\u6a21\u62df\u7b49\u91cd\u8981\u91cf\u5b50\u7b97\u6cd5\u4e2d\u7684\u81ea\u7136\u548c\u7b80\u6d01\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u539f\u578b\u7684\u7f16\u8bd1\u5668\u3002", "conclusion": "\u8be5\u8bed\u8a00\u4e3a\u91cf\u5b50\u7f16\u7a0b\u63d0\u4f9b\u4e86\u62bd\u8c61\u5316\u548c\u7ed3\u6784\u5316\u7684\u539f\u5219\u6027\u8fdb\u6b65\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.11644", "pdf": "https://arxiv.org/pdf/2507.11644", "abs": "https://arxiv.org/abs/2507.11644", "authors": ["Olivier Gasquet"], "title": "Comment on Decidability of Quasi-Dense Modal Logics by Lyon and Ostropolski-Nalewaja", "categories": ["cs.LO", "03B45", "F.4.1"], "comment": "Comment on arXiv:2405.10094", "summary": "In \\cite{Lyon24} the question of the decidability of quasi-dense modal logics\nis answered, and an upper bound in EXPSPACE is given. Unfortunately, authors'\nintricate proof contains a major flaw that cannot be fixed, leaving the\nquestion wide open. Once identified, this error roughly amounts to assuming\nthat the union of two consistent sets is consistent, which is of course wrong.", "AI": {"tldr": "\u8bba\u6587\u300aLyon24\u300b\u8bd5\u56fe\u89e3\u51b3\u51c6\u7a20\u5bc6\u6a21\u6001\u903b\u8f91\u7684\u53ef\u5224\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u7ed9\u51fa\u4e86EXPSPACE\u4e0a\u9650\uff0c\u4f46\u8bc1\u660e\u4e2d\u5b58\u5728\u65e0\u6cd5\u4fee\u590d\u7684\u91cd\u5927\u9519\u8bef\u3002", "motivation": "\u7814\u7a76\u51c6\u7a20\u5bc6\u6a21\u6001\u903b\u8f91\u7684\u53ef\u5224\u5b9a\u6027\u95ee\u9898\u53ca\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u4f5c\u8005\u5c1d\u8bd5\u901a\u8fc7\u590d\u6742\u7684\u8bc1\u660e\u65b9\u6cd5\u7ed9\u51fa\u7ed3\u8bba\u548c\u4e0a\u9650\u3002", "result": "\u8bc1\u660e\u4e2d\u5b58\u5728\u91cd\u5927\u9519\u8bef\uff0c\u5bfc\u81f4\u7ed3\u8bba\u65e0\u6cd5\u6210\u7acb\u3002", "conclusion": "\u51c6\u7a20\u5bc6\u6a21\u6001\u903b\u8f91\u7684\u53ef\u5224\u5b9a\u6027\u95ee\u9898\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.11678", "pdf": "https://arxiv.org/pdf/2507.11678", "abs": "https://arxiv.org/abs/2507.11678", "authors": ["Sulyab Thottungal Valapu", "John Heidemann"], "title": "Towards a Non-Binary View of IPv6 Adoption", "categories": ["cs.NI", "C.2.6"], "comment": null, "summary": "Twelve years have passed since World IPv6 Launch Day, but what is the current\nstate of IPv6 deployment? Prior work has examined IPv6 status as a binary: can\nyou use IPv6, or not? As deployment increases we must consider a more nuanced,\nnon-binary perspective on IPv6: how much and often can a user or a service use\nIPv6? We consider this question as a client, server, and cloud provider.\nConsidering the client's perspective, we observe user traffic. We see that the\nfraction of IPv6 traffic a user sends varies greatly, both across users and\nday-by-day, with a standard deviation of over 15%. We show this variation\noccurs for two main reasons. First, IPv6 traffic is primarily human-generated,\nthus showing diurnal patterns. Second, some services are IPv6-forward and\nothers IPv6-laggards, so as users do different things their fraction of IPv6\nvaries. We look at server-side IPv6 adoption in two ways. First, we expand\nanalysis of web services to examine how many are only partially IPv6 enabled\ndue to their reliance on IPv4-only resources. Our findings reveal that only\n12.5% of top 100k websites qualify as fully IPv6-ready. Finally, we examine\ncloud support for IPv6. Although all clouds and CDNs support IPv6, we find that\ntenant deployment rates vary significantly across providers. We find that ease\nof enabling IPv6 in the cloud is correlated with tenant IPv6 adoption rates,\nand recommend best practices for cloud providers to improve IPv6 adoption. Our\nresults suggest IPv6 deployment is growing, but many services lag, presenting a\npotential for improvement.", "AI": {"tldr": "\u6587\u7ae0\u5206\u6790\u4e86IPv6\u7684\u5f53\u524d\u90e8\u7f72\u72b6\u6001\uff0c\u63d0\u51fa\u4ece\u5ba2\u6237\u7aef\u3001\u670d\u52a1\u5668\u548c\u4e91\u63d0\u4f9b\u5546\u7684\u89d2\u5ea6\u6765\u8bc4\u4f30IPv6\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u5e76\u63ed\u793a\u4e86IPv6\u90e8\u7f72\u4e2d\u7684\u4e0d\u5747\u5300\u6027\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u968f\u7740IPv6\u90e8\u7f72\u7684\u589e\u52a0\uff0c\u9700\u8981\u4ece\u66f4\u7ec6\u81f4\u7684\u89d2\u5ea6\uff08\u975e\u4e8c\u5143\u89c6\u89d2\uff09\u8bc4\u4f30IPv6\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u4e86\u89e3\u5176\u5728\u5ba2\u6237\u7aef\u3001\u670d\u52a1\u5668\u548c\u4e91\u63d0\u4f9b\u5546\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u7528\u6237\u6d41\u91cf\u3001\u5206\u6790\u7f51\u7ad9\u670d\u52a1\u4ee5\u53ca\u7814\u7a76\u4e91\u63d0\u4f9b\u5546\u7684\u652f\u6301\u60c5\u51b5\uff0c\u8bc4\u4f30IPv6\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u53d1\u73b0IPv6\u6d41\u91cf\u5b58\u5728\u8f83\u5927\u6ce2\u52a8\uff0c\u4ec512.5%\u7684\u9876\u7ea7\u7f51\u7ad9\u5b8c\u5168\u652f\u6301IPv6\uff1b\u4e91\u63d0\u4f9b\u5546\u7684\u652f\u6301\u7a0b\u5ea6\u4e0e\u79df\u6237\u91c7\u7528\u7387\u76f8\u5173\u3002", "conclusion": "IPv6\u90e8\u7f72\u5728\u589e\u957f\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u670d\u52a1\u5546\u7684\u652f\u6301\u548c\u4e91\u63d0\u4f9b\u5546\u7684\u6613\u7528\u6027\u65b9\u9762\u3002"}}
{"id": "2507.11831", "pdf": "https://arxiv.org/pdf/2507.11831", "abs": "https://arxiv.org/abs/2507.11831", "authors": ["Fernando Koch", "Jessica Nahulan", "Jeremy Fox", "Martin Keen"], "title": "Generative Intelligence Systems in the Flow of Group Emotions", "categories": ["cs.HC", "cs.ET"], "comment": "8 pages, 10 figures", "summary": "Emotional cues frequently arise and shape group dynamics in interactive\nsettings where multiple humans and artificial agents communicate through shared\ndigital channels. While artificial agents lack intrinsic emotional states, they\ncan simulate affective behavior using synthetic modalities such as text or\nspeech. This work introduces a model for orchestrating emotion contagion,\nenabling agents to detect emotional signals, infer group mood patterns, and\ngenerate targeted emotional responses. The system captures human emotional\nexchanges and uses this insight to produce adaptive, generative responses that\ninfluence group affect in real time. The model supports applications in\ncollaborative, educational, and social environments by shifting affective\ncomputing from individual-level reactions to coordinated, group-level emotion\nmodulation. We present the system architecture and provide experimental results\nthat illustrate its effectiveness in sensing and steering group mood dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u60c5\u7eea\u4f20\u67d3\u6a21\u578b\uff0c\u4f7f\u4eba\u5de5\u4ee3\u7406\u80fd\u591f\u68c0\u6d4b\u60c5\u7eea\u4fe1\u53f7\u3001\u63a8\u65ad\u7fa4\u4f53\u60c5\u7eea\u6a21\u5f0f\u5e76\u751f\u6210\u9488\u5bf9\u6027\u60c5\u611f\u54cd\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u7684\u7fa4\u4f53\u60c5\u7eea\u8c03\u8282\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u6570\u5b57\u4ea4\u4e92\u73af\u5883\u4e2d\uff0c\u4eba\u5de5\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u6a21\u62df\u60c5\u611f\u884c\u4e3a\u6765\u5f71\u54cd\u7fa4\u4f53\u60c5\u7eea\u52a8\u6001\uff0c\u4ece\u800c\u589e\u5f3a\u534f\u4f5c\u3001\u6559\u80b2\u548c\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u60c5\u611f\u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u4e00\u4e2a\u60c5\u7eea\u4f20\u67d3\u6a21\u578b\uff0c\u901a\u8fc7\u6355\u6349\u4eba\u7c7b\u60c5\u611f\u4ea4\u6362\u3001\u63a8\u65ad\u7fa4\u4f53\u60c5\u7eea\u6a21\u5f0f\uff0c\u5e76\u751f\u6210\u81ea\u9002\u5e94\u7684\u60c5\u611f\u54cd\u5e94\u6765\u5b9e\u65f6\u8c03\u8282\u7fa4\u4f53\u60c5\u7eea\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u611f\u77e5\u548c\u5f15\u5bfc\u7fa4\u4f53\u60c5\u7eea\u52a8\u6001\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u534f\u4f5c\u548c\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8be5\u6a21\u578b\u5c06\u60c5\u611f\u8ba1\u7b97\u4ece\u4e2a\u4f53\u5c42\u9762\u6269\u5c55\u5230\u7fa4\u4f53\u5c42\u9762\uff0c\u4e3a\u534f\u4f5c\u548c\u6559\u80b2\u7b49\u9886\u57df\u7684\u60c5\u7eea\u8c03\u8282\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.11599", "pdf": "https://arxiv.org/pdf/2507.11599", "abs": "https://arxiv.org/abs/2507.11599", "authors": ["Harish Vijayakumar"], "title": "Neuroaesthetics and the Science of Visual Experience", "categories": ["cs.HC"], "comment": "7 pages", "summary": "Neuroaesthetics is an interdisciplinary field that brings together\nneuroscience, psychology, and the arts to explore how the human brain perceives\nand responds to visual beauty. This paper examines the neural mechanisms behind\naesthetic experiences, aiming to explain why certain designs or artworks feel\nemotionally or cognitively \"right.\" By analyzing the interaction between\nperception, emotion, and cognition, neuroaesthetics reveals how beauty is\nconstructed in the brain and how this understanding can inform fields such as\ngraphic and interface design. This paper offers a clear and accessible overview\nof core neuroaesthetic principles, making the subject approachable to a wide\naudience. The findings suggest that impactful design is more than surface-level\nappeal: well-crafted visual experiences can engage, support, and connect people\nin meaningful ways.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7f8e\u5b66\u5982\u4f55\u901a\u8fc7\u795e\u7ecf\u673a\u5236\u89e3\u91ca\u5ba1\u7f8e\u4f53\u9a8c\uff0c\u63ed\u793a\u7f8e\u5728\u5927\u8111\u4e2d\u7684\u6784\u5efa\u65b9\u5f0f\u53ca\u5176\u5bf9\u8bbe\u8ba1\u7684\u542f\u793a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5927\u8111\u5982\u4f55\u611f\u77e5\u548c\u54cd\u5e94\u89c6\u89c9\u7f8e\u5b66\uff0c\u4ee5\u53ca\u8fd9\u5bf9\u8bbe\u8ba1\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u611f\u77e5\u3001\u60c5\u611f\u548c\u8ba4\u77e5\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u4e0e\u5fc3\u7406\u5b66\u7684\u7406\u8bba\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u8868\u660e\uff0c\u6709\u6548\u7684\u8bbe\u8ba1\u4e0d\u4ec5\u5173\u4e4e\u8868\u9762\u5438\u5f15\u529b\uff0c\u8fd8\u80fd\u901a\u8fc7\u6df1\u523b\u7684\u89c6\u89c9\u4f53\u9a8c\u8fde\u63a5\u4eba\u4eec\u3002", "conclusion": "\u795e\u7ecf\u7f8e\u5b66\u4e3a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u79d1\u5b66\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u89c6\u89c9\u4f53\u9a8c\u4e0e\u4eba\u7c7b\u60c5\u611f\u548c\u8ba4\u77e5\u7684\u6df1\u5ea6\u8054\u7cfb\u3002"}}
{"id": "2507.11907", "pdf": "https://arxiv.org/pdf/2507.11907", "abs": "https://arxiv.org/abs/2507.11907", "authors": ["Zhaoheng Li", "Silu Huang", "Wei Ding", "Yongjoo Park", "Jianjun Chen"], "title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes", "categories": ["cs.DB", "cs.IR"], "comment": null, "summary": "Many real-world tasks such as recommending videos with the kids tag can be\nreduced to finding most similar vectors associated with hard predicates. This\ntask, filtered vector search, is challenging as prior state-of-the-art\ngraph-based (unfiltered) similarity search techniques quickly degenerate when\nhard constraints are considered. That is, effective graph-based filtered\nsimilarity search relies on sufficient connectivity for reaching the most\nsimilar items within just a few hops. To consider predicates, recent works\npropose modifying graph traversal to visit only the items that may satisfy\npredicates. However, they fail to offer the just-a-few-hops property for a wide\nrange of predicates: they must restrict predicates significantly or lose\nefficiency if only a small fraction of items satisfy predicates.\n  We propose an opposite approach: instead of constraining traversal, we build\nmany indexes each serving different predicate forms. For effective\nconstruction, we devise a three-dimensional analytical model capturing\nrelationships among index size, search time, and recall, with which we follow a\nworkload-aware approach to pack as many useful indexes as possible into a\ncollection. At query time, the analytical model is employed yet again to\ndiscern the one that offers the fastest search at a given recall. We show\nsuperior performance and support on datasets with varying selectivities and\nforms: our approach achieves up to 8.06x speedup while having as low as 1%\nbuild time versus other indexes, with less than 2.15x memory of a standard HNSW\ngraph and modest knowledge of past workloads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fc7\u6ee4\u5411\u91cf\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u4e2a\u7d22\u5f15\u6765\u652f\u6301\u4e0d\u540c\u8c13\u8bcd\u5f62\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684\u8fc7\u6ee4\u76f8\u4f3c\u6027\u641c\u7d22\u5728\u5904\u7406\u786c\u7ea6\u675f\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u8c13\u8bcd\u5f62\u5f0f\u7684\u5e7f\u6cdb\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u6784\u5efa\u591a\u4e2a\u7d22\u5f15\u6765\u652f\u6301\u4e0d\u540c\u8c13\u8bcd\u5f62\u5f0f\uff0c\u5229\u7528\u4e09\u7ef4\u5206\u6790\u6a21\u578b\u4f18\u5316\u7d22\u5f15\u6784\u5efa\u548c\u67e5\u8be2\u9009\u62e9\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u63d0\u901f8.06\u500d\uff0c\u6784\u5efa\u65f6\u95f4\u4ec5\u4e3a1%\uff0c\u5185\u5b58\u5360\u7528\u4ec5\u4e3a\u6807\u51c6HNSW\u56fe\u76842.15\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u6ee4\u5411\u91cf\u641c\u7d22\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u9009\u62e9\u6027\u548c\u5f62\u5f0f\u7684\u8c13\u8bcd\u9700\u6c42\u3002"}}
{"id": "2507.11545", "pdf": "https://arxiv.org/pdf/2507.11545", "abs": "https://arxiv.org/abs/2507.11545", "authors": ["Rhea Pritham Marpu", "Kevin J McNamara", "Preeti Gupta"], "title": "The AI Shadow War: SaaS vs. Edge Computing Architectures", "categories": ["cs.DC", "cs.ET", "cs.NE"], "comment": null, "summary": "The very DNA of AI architecture presents conflicting paths: centralized\ncloud-based models (Software-as-a-Service) versus decentralized edge AI (local\nprocessing on consumer devices). This paper analyzes the competitive\nbattleground across computational capability, energy efficiency, and data\nprivacy. Recent breakthroughs show edge AI challenging cloud systems on\nperformance, leveraging innovations like test-time training and\nmixture-of-experts architectures. Crucially, edge AI boasts a 10,000x\nefficiency advantage: modern ARM processors consume merely 100 microwatts\nforinference versus 1 watt for equivalent cloud processing. Beyond efficiency,\nedge AI secures data sovereignty by keeping processing local, dismantling\nsingle points of failure in centralized architectures. This democratizes access\nthroughaffordable hardware, enables offline functionality, and reduces\nenvironmental impact by eliminating data transmission costs. The edge AI market\nprojects explosive growth from $9 billion in 2025 to $49.6 billion by 2030\n(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical\napplications including personalized education, healthcare monitoring,\nautonomous transport, and smart infrastructure rely on edge AI's ultra-low\nlatency (5-10ms versus 100-500ms for cloud). The convergence of architectural\ninnovation with fundamental physics confirms edge AI's distributed approach\naligns with efficient information processing, signaling the inevitable\nemergence of hybrid edge-cloud ecosystems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u96c6\u4e2d\u5f0f\u4e91AI\u4e0e\u5206\u6563\u5f0f\u8fb9\u7f18AI\u5728\u8ba1\u7b97\u80fd\u529b\u3001\u80fd\u6548\u548c\u6570\u636e\u9690\u79c1\u65b9\u9762\u7684\u7ade\u4e89\uff0c\u6307\u51fa\u8fb9\u7f18AI\u51ed\u501f\u9ad8\u6548\u80fd\u548c\u9690\u79c1\u4f18\u52bf\u6b63\u5728\u6311\u6218\u4e91AI\u3002", "motivation": "\u7814\u7a76\u96c6\u4e2d\u5f0f\u4e91AI\u4e0e\u5206\u6563\u5f0f\u8fb9\u7f18AI\u7684\u7ade\u4e89\uff0c\u5206\u6790\u8fb9\u7f18AI\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u9690\u79c1\u65b9\u9762\u7684\u7a81\u7834\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8ba1\u7b97\u80fd\u529b\u3001\u80fd\u6548\u548c\u6570\u636e\u9690\u79c1\u7b49\u5173\u952e\u6307\u6807\uff0c\u5bf9\u6bd4\u8fb9\u7f18AI\u548c\u4e91AI\u7684\u6280\u672f\u5dee\u5f02\u548c\u5e02\u573a\u8d8b\u52bf\u3002", "result": "\u8fb9\u7f18AI\u5728\u80fd\u6548\u4e0a\u6bd4\u4e91AI\u9ad810,000\u500d\uff0c\u9690\u79c1\u4fdd\u62a4\u66f4\u4f18\uff0c\u5e02\u573a\u9884\u8ba1\u4ece2025\u5e74\u768490\u4ebf\u7f8e\u5143\u589e\u957f\u52302030\u5e74\u7684496\u4ebf\u7f8e\u5143\u3002", "conclusion": "\u8fb9\u7f18AI\u7684\u6280\u672f\u4f18\u52bf\u548c\u5e02\u573a\u9700\u6c42\u5c06\u63a8\u52a8\u6df7\u5408\u8fb9\u7f18-\u4e91\u751f\u6001\u7cfb\u7edf\u7684\u5174\u8d77\u3002"}}
{"id": "2507.11709", "pdf": "https://arxiv.org/pdf/2507.11709", "abs": "https://arxiv.org/abs/2507.11709", "authors": ["Junius Pun", "Xilai Dai", "Grace Zgheib", "Mahesh A. Iyer", "Andrew Boutros", "Vaughn Betz", "Mohamed S. Abdelfattah"], "title": "Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage", "categories": ["cs.AR"], "comment": "accepted at FPL 2025", "summary": "Flexibility and customization are key strengths of Field-Programmable Gate\nArrays (FPGAs) when compared to other computing devices. For instance, FPGAs\ncan efficiently implement arbitrary-precision arithmetic operations, and can\nperform aggressive synthesis optimizations to eliminate ineffectual operations.\nMotivated by sparsity and mixed-precision in deep neural networks (DNNs), we\ninvestigate how to optimize the current logic block architecture to increase\nits arithmetic density. We find that modern FPGA logic block architectures\nprevent the independent use of adder chains, and instead only allow adder chain\ninputs to be fed by look-up table (LUT) outputs. This only allows one of the\ntwo primitives -- either adders or LUTs -- to be used independently in one\nlogic element and prevents their concurrent use, hampering area optimizations.\nIn this work, we propose the Double Duty logic block architecture to enable the\nconcurrent use of the adders and LUTs within a logic element. Without adding\nexpensive logic cluster inputs, we use 4 of the existing inputs to bypass the\nLUTs and connect directly to the adder chain inputs. We accurately model our\nchanges at both the circuit and CAD levels using open-source FPGA development\ntools. Our experimental evaluation on a Stratix-10-like architecture\ndemonstrates area reductions of 21.6% on adder-intensive circuits from the\nKratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR\nbenchmarks respectively. These area improvements come without an impact to\ncritical path delay, demonstrating that higher density is feasible on modern\nFPGA architectures by adding more flexibility in how the adder chain is used.\nAveraged across all circuits from our three evaluated benchmark set, our Double\nDuty FPGA architecture improves area-delay product by 9.7%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDouble Duty\u7684\u65b0\u578bFPGA\u903b\u8f91\u5757\u67b6\u6784\uff0c\u901a\u8fc7\u5141\u8bb8\u903b\u8f91\u5143\u7d20\u4e2d\u52a0\u6cd5\u5668\u548c\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u5e76\u53d1\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b97\u672f\u5bc6\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e8621.6%\u7684\u9762\u79ef\u7f29\u51cf\u3002", "motivation": "\u53d7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u4e2d\u7a00\u758f\u6027\u548c\u6df7\u5408\u7cbe\u5ea6\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u53d1\u73b0\u73b0\u4ee3FPGA\u903b\u8f91\u5757\u67b6\u6784\u9650\u5236\u4e86\u52a0\u6cd5\u5668\u548cLUT\u7684\u72ec\u7acb\u4f7f\u7528\uff0c\u963b\u788d\u4e86\u9762\u79ef\u4f18\u5316\u3002", "method": "\u4f5c\u8005\u63d0\u51faDouble Duty\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6709\u8f93\u5165\u7ed5\u8fc7LUT\u76f4\u63a5\u8fde\u63a5\u5230\u52a0\u6cd5\u5668\u94fe\uff0c\u5b9e\u73b0\u4e86\u52a0\u6cd5\u5668\u548cLUT\u7684\u5e76\u53d1\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728Stratix-10\u7c7b\u4f3c\u67b6\u6784\u4e0a\uff0cDouble Duty\u67b6\u6784\u5728Kratos\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8621.6%\u7684\u9762\u79ef\u7f29\u51cf\uff0cKoios\u548cVTR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u7f29\u51cf\u4e869.3%\u548c8.2%\uff0c\u4e14\u4e0d\u5f71\u54cd\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u3002", "conclusion": "Double Duty\u67b6\u6784\u901a\u8fc7\u589e\u52a0\u52a0\u6cd5\u5668\u94fe\u4f7f\u7528\u7684\u7075\u6d3b\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u73b0\u4ee3FPGA\u67b6\u6784\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u5bc6\u5ea6\u662f\u53ef\u884c\u7684\uff0c\u5e73\u5747\u9762\u79ef-\u5ef6\u8fdf\u4e58\u79ef\u63d0\u9ad8\u4e869.7%\u3002"}}
{"id": "2507.11903", "pdf": "https://arxiv.org/pdf/2507.11903", "abs": "https://arxiv.org/abs/2507.11903", "authors": ["Daocheng Lin", "Yifan Wang", "Yutong Yang", "Xingyu Lan"], "title": "Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "When designed deliberately, data visualizations can become powerful\npersuasive tools, influencing viewers' opinions, values, and actions. While\nresearchers have begun studying this issue (e.g., to evaluate the effects of\npersuasive visualization), we argue that a fundamental mechanism of persuasion\nresides in rhetorical construction, a perspective inadequately addressed in\ncurrent visualization research. To fill this gap, we present a focused analysis\nof octopus maps, a visual genre that has maintained persuasive power across\ncenturies and achieved significant social impact. Employing rhetorical schema\ntheory, we collected and analyzed 90 octopus maps spanning from the 19th\ncentury to contemporary times. We closely examined how octopus maps implement\ntheir persuasive intents and constructed a design space that reveals how visual\nmetaphors are strategically constructed and what common rhetorical strategies\nare applied to components such as maps, octopus imagery, and text. Through the\nabove analysis, we also uncover a set of interesting findings. For instance,\ncontrary to the common perception that octopus maps are primarily a historical\nphenomenon, our research shows that they remain a lively design convention in\ntoday's digital age. Additionally, while most octopus maps stem from Western\ndiscourse that views the octopus as an evil symbol, some designs offer\nalternative interpretations, highlighting the dynamic nature of rhetoric across\ndifferent sociocultural settings. Lastly, drawing from the lessons provided by\noctopus maps, we discuss the associated ethical concerns of persuasive\nvisualization.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6570\u636e\u53ef\u89c6\u5316\u4f5c\u4e3a\u8bf4\u670d\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u4fee\u8f9e\u6784\u9020\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4ee5\u7ae0\u9c7c\u56fe\u4e3a\u4f8b\u5206\u6790\u4e86\u5176\u8de8\u4e16\u7eaa\u7684\u4fee\u8f9e\u7b56\u7565\u548c\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u53ef\u89c6\u5316\u4f5c\u4e3a\u8bf4\u670d\u5de5\u5177\u7684\u4fee\u8f9e\u673a\u5236\u5173\u6ce8\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5206\u6790\u7ae0\u9c7c\u56fe\u7684\u4fee\u8f9e\u7b56\u7565\uff0c\u63ed\u793a\u5176\u8bf4\u670d\u529b\u7684\u6765\u6e90\u3002", "method": "\u91c7\u7528\u4fee\u8f9e\u56fe\u5f0f\u7406\u8bba\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4e8690\u4e2a\u4ece19\u4e16\u7eaa\u81f3\u4eca\u7684\u7ae0\u9c7c\u56fe\uff0c\u7814\u7a76\u4e86\u5176\u89c6\u89c9\u9690\u55bb\u548c\u4fee\u8f9e\u7b56\u7565\u7684\u6784\u5efa\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u7ae0\u9c7c\u56fe\u4f5c\u4e3a\u4e00\u79cd\u8bbe\u8ba1\u60ef\u4f8b\u81f3\u4eca\u4ecd\u6d3b\u8dc3\uff0c\u4e14\u5176\u4fee\u8f9e\u7b56\u7565\u5728\u4e0d\u540c\u793e\u4f1a\u6587\u5316\u80cc\u666f\u4e0b\u5448\u73b0\u591a\u6837\u6027\u3002", "conclusion": "\u7ae0\u9c7c\u56fe\u5206\u6790\u63ed\u793a\u4e86\u8bf4\u670d\u6027\u53ef\u89c6\u5316\u7684\u4fee\u8f9e\u673a\u5236\uff0c\u5e76\u5f15\u53d1\u4e86\u5bf9\u76f8\u5173\u4f26\u7406\u95ee\u9898\u7684\u8ba8\u8bba\u3002"}}
{"id": "2507.11976", "pdf": "https://arxiv.org/pdf/2507.11976", "abs": "https://arxiv.org/abs/2507.11976", "authors": ["Jana-Rebecca Rehse", "Michael Grohs", "Finn Klessascheck", "Lisa-Marie Klein", "Tatiana von Landesberger", "Luise Pufahl"], "title": "A Task Taxonomy for Conformance Checking", "categories": ["cs.SE"], "comment": "Preprint submitted to Information Systems", "summary": "Conformance checking is a sub-discipline of process mining, which compares\nobserved process traces with a process model to analyze whether the process\nexecution conforms with or deviates from the process design. Organizations can\nleverage this analysis, for example to check whether their processes comply\nwith internal or external regulations or to identify potential improvements.\nGaining these insights requires suitable visualizations, which make complex\nresults accessible and actionable. So far, however, the development of\nconformance checking visualizations has largely been left to tool vendors. As a\nresult, current tools offer a wide variety of visual representations for\nconformance checking, but the analytical purposes they serve often remain\nunclear. However, without a systematic understanding of these purposes, it is\ndifficult to evaluate the visualizations' usefulness. Such an evaluation hence\nrequires a deeper understanding of conformance checking as an analysis domain.\nTo this end, we propose a task taxonomy, which categorizes the tasks that can\noccur when conducting conformance checking analyses. This taxonomy supports\nresearchers in determining the purpose of visualizations, specifying relevant\nconformance checking tasks in terms of their goal, means, constraint type, data\ncharacteristics, data target, and data cardinality. Combining concepts from\nprocess mining and visual analytics, we address researchers from both\ndisciplines to enable and support closer collaborations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5316\u4e00\u81f4\u6027\u68c0\u67e5\u4e2d\u7684\u53ef\u89c6\u5316\u76ee\u7684\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u548c\u6539\u8fdb\u53ef\u89c6\u5316\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e00\u81f4\u6027\u68c0\u67e5\u7684\u53ef\u89c6\u5316\u5de5\u5177\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u7c7b\uff0c\u5bfc\u81f4\u5176\u5206\u6790\u76ee\u7684\u4e0d\u660e\u786e\uff0c\u96be\u4ee5\u8bc4\u4f30\u5176\u6709\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u3001\u624b\u6bb5\u3001\u7ea6\u675f\u7c7b\u578b\u3001\u6570\u636e\u7279\u5f81\u3001\u6570\u636e\u76ee\u6807\u548c\u6570\u636e\u57fa\u6570\u7b49\u7ef4\u5ea6\u5bf9\u4e00\u81f4\u6027\u68c0\u67e5\u4efb\u52a1\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u8fc7\u7a0b\u6316\u6398\u548c\u53ef\u89c6\u5206\u6790\u7684\u6982\u5ff5\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6e05\u6670\u7684\u6846\u67b6\uff0c\u4ee5\u7406\u89e3\u548c\u8bc4\u4f30\u4e00\u81f4\u6027\u68c0\u67e5\u7684\u53ef\u89c6\u5316\u5de5\u5177\u3002", "conclusion": "\u4efb\u52a1\u5206\u7c7b\u6cd5\u6709\u52a9\u4e8e\u660e\u786e\u4e00\u81f4\u6027\u68c0\u67e5\u53ef\u89c6\u5316\u7684\u76ee\u7684\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u63d0\u5347\u5de5\u5177\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2507.11857", "pdf": "https://arxiv.org/pdf/2507.11857", "abs": "https://arxiv.org/abs/2507.11857", "authors": ["Benjamin Watson", "Alinda Friedman", "Aaron McGaffey"], "title": "Measuring and predicting visual fidelity", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "This paper is a study of techniques for measuring and predicting visual\nfidelity. As visual stimuli we use polygonal models, and vary their fidelity\nwith two different model simplification algorithms. We also group the stimuli\ninto two object types: animals and man made artifacts. We examine three\ndifferent experimental techniques for measuring these fidelity changes: naming\ntimes, ratings, and preferences. All the measures were sensitive to the type of\nsimplification and level of simplification. However, the measures differed from\none another in their response to object type. We also examine several automatic\ntechniques for predicting these experimental measures, including techniques\nbased on images and on the models themselves. Automatic measures of fidelity\nwere successful at predicting experimental ratings, less successful at\npredicting preferences, and largely failures at predicting naming times. We\nconclude with suggestions for use and improvement of the experimental and\nautomatic measures of visual fidelity.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6d4b\u91cf\u548c\u9884\u6d4b\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u6280\u672f\uff0c\u4f7f\u7528\u591a\u8fb9\u5f62\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u523a\u6fc0\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u7b80\u5316\u7b97\u6cd5\u6539\u53d8\u5176\u4fdd\u771f\u5ea6\u3002\u5b9e\u9a8c\u6d4b\u91cf\u4e86\u547d\u540d\u65f6\u95f4\u3001\u8bc4\u5206\u548c\u504f\u597d\u4e09\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u7b80\u5316\u7c7b\u578b\u548c\u6c34\u5e73\u7684\u654f\u611f\u6027\u4e0d\u540c\u3002\u81ea\u52a8\u9884\u6d4b\u6280\u672f\u5728\u8bc4\u5206\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u504f\u597d\u548c\u547d\u540d\u65f6\u95f4\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u5b9e\u9a8c\u6280\u672f\u548c\u81ea\u52a8\u65b9\u6cd5\u5728\u6d4b\u91cf\u548c\u9884\u6d4b\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u591a\u8fb9\u5f62\u6a21\u578b\u548c\u4e24\u79cd\u7b80\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u547d\u540d\u65f6\u95f4\u3001\u8bc4\u5206\u548c\u504f\u597d\u4e09\u79cd\u5b9e\u9a8c\u65b9\u6cd5\u6d4b\u91cf\u4fdd\u771f\u5ea6\u53d8\u5316\uff0c\u5e76\u7528\u57fa\u4e8e\u56fe\u50cf\u548c\u6a21\u578b\u7684\u81ea\u52a8\u6280\u672f\u9884\u6d4b\u8fd9\u4e9b\u6d4b\u91cf\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u6d4b\u91cf\u5bf9\u7b80\u5316\u7c7b\u578b\u548c\u6c34\u5e73\u654f\u611f\uff0c\u4f46\u4e0d\u540c\u5bf9\u8c61\u7c7b\u578b\u53cd\u5e94\u4e0d\u540c\u3002\u81ea\u52a8\u6280\u672f\u5728\u9884\u6d4b\u8bc4\u5206\u4e0a\u6210\u529f\uff0c\u504f\u597d\u4e0a\u6548\u679c\u4e00\u822c\uff0c\u547d\u540d\u65f6\u95f4\u4e0a\u5931\u8d25\u3002", "conclusion": "\u603b\u7ed3\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5b9e\u9a8c\u548c\u81ea\u52a8\u6d4b\u91cf\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u6cd5\u7684\u5efa\u8bae\u3002"}}
{"id": "2507.11731", "pdf": "https://arxiv.org/pdf/2507.11731", "abs": "https://arxiv.org/abs/2507.11731", "authors": ["Neng-Fa Zhou", "Cristian Grozea", "H\u00e5kan Kjellerstrand", "Ois\u00edn Mac Fheara\u00ed"], "title": "Picat Through the Lens of Advent of Code", "categories": ["cs.PL", "68N15", "D.3.2"], "comment": "14 pages", "summary": "Picat is a logic-based, multi-paradigm programming language that integrates\nfeatures from logic, functional, constraint, and imperative programming\nparadigms. This paper presents solutions to several problems from the 2024\nAdvent of Code (AoC). While AoC problems are not designed for any specific\nprogramming language, certain problem types, such as reverse engineering and\npath-finding, are particularly well-suited to Picat due to its built-in\nconstraint solving, pattern matching, backtracking, and dynamic programming\nwith tabling. This paper demonstrates that Picat's features, especially its\nSAT-based constraint solving and tabling, enable concise, declarative, and\nhighly efficient implementations of problems that would require significantly\nmore effort in imperative languages.", "AI": {"tldr": "Picat\u662f\u4e00\u79cd\u591a\u8303\u5f0f\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00\uff0c\u672c\u6587\u7528\u5176\u89e3\u51b3\u4e862024\u5e74Advent of Code\u7684\u591a\u4e2a\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7ea6\u675f\u6c42\u89e3\u548c\u52a8\u6001\u7f16\u7a0b\u7b49\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u4e0e\u7b80\u6d01\u3002", "motivation": "Picat\u8bed\u8a00\u7ed3\u5408\u4e86\u903b\u8f91\u3001\u51fd\u6570\u5f0f\u3001\u7ea6\u675f\u548c\u547d\u4ee4\u5f0f\u7f16\u7a0b\u7684\u7279\u70b9\uff0c\u9002\u5408\u89e3\u51b3AoC\u4e2d\u7684\u67d0\u4e9b\u7279\u5b9a\u95ee\u9898\u7c7b\u578b\uff0c\u5982\u9006\u5411\u5de5\u7a0b\u548c\u8def\u5f84\u67e5\u627e\u3002", "method": "\u5229\u7528Picat\u5185\u7f6e\u7684\u7ea6\u675f\u6c42\u89e3\u3001\u6a21\u5f0f\u5339\u914d\u3001\u56de\u6eaf\u548c\u52a8\u6001\u7f16\u7a0b\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u5bf9AoC\u95ee\u9898\u7684\u7b80\u6d01\u9ad8\u6548\u89e3\u6cd5\u3002", "result": "Picat\u7684SAT\u7ea6\u675f\u6c42\u89e3\u548c\u52a8\u6001\u7f16\u7a0b\u529f\u80fd\u663e\u8457\u7b80\u5316\u4e86\u95ee\u9898\u5b9e\u73b0\uff0c\u4e14\u6548\u7387\u4f18\u4e8e\u547d\u4ee4\u5f0f\u8bed\u8a00\u3002", "conclusion": "Picat\u5728\u591a\u8303\u5f0f\u7f16\u7a0b\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u9ad8\u7ea7\u903b\u8f91\u548c\u7ea6\u675f\u6c42\u89e3\u7684\u95ee\u9898\u3002"}}
{"id": "2507.11655", "pdf": "https://arxiv.org/pdf/2507.11655", "abs": "https://arxiv.org/abs/2507.11655", "authors": ["Mohimenul Kabir", "Supratik Chakraborty", "Kuldeep S Meel"], "title": "Counting Answer Sets of Disjunctive Answer Set Programs", "categories": ["cs.LO", "cs.AI"], "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Answer Set Programming (ASP) provides a powerful declarative paradigm for\nknowledge representation and reasoning. Recently, counting answer sets has\nemerged as an important computational problem with applications in\nprobabilistic reasoning, network reliability analysis, and other domains. This\nhas motivated significant research into designing efficient ASP counters. While\nsubstantial progress has been made for normal logic programs, the development\nof practical counters for disjunctive logic programs remains challenging.\n  We present SharpASP-SR, a novel framework for counting answer sets of\ndisjunctive logic programs based on subtractive reduction to projected\npropositional model counting. Our approach introduces an alternative\ncharacterization of answer sets that enables efficient reduction while ensuring\nthat intermediate representations remain of polynomial size. This allows\nSharpASP-SR to leverage recent advances in projected model counting technology.\nThrough extensive experimental evaluation on diverse benchmarks, we demonstrate\nthat SharpASP-SR significantly outperforms existing counters on instances with\nlarge answer set counts. Building on these results, we develop a hybrid\ncounting approach that combines enumeration techniques with SharpASP-SR to\nachieve state-of-the-art performance across the full spectrum of disjunctive\nprograms.", "AI": {"tldr": "SharpASP-SR \u662f\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u547d\u9898\u6a21\u578b\u8ba1\u6570\u7684\u51cf\u7ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\u7684\u7b54\u6848\u96c6\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u8ba1\u7b97\u7b54\u6848\u96c6\u6570\u91cf\u5728\u6982\u7387\u63a8\u7406\u3001\u7f51\u7edc\u53ef\u9760\u6027\u5206\u6790\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\u7684\u8ba1\u6570\u5668\u8bbe\u8ba1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "SharpASP-SR \u901a\u8fc7\u51cf\u7ea6\u65b9\u6cd5\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6295\u5f71\u547d\u9898\u6a21\u578b\u8ba1\u6570\uff0c\u5e76\u5f15\u5165\u65b0\u8868\u5f81\u4ee5\u786e\u4fdd\u9ad8\u6548\u4e14\u591a\u9879\u5f0f\u89c4\u6a21\u7684\u4e2d\u95f4\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e SharpASP-SR \u5728\u7b54\u6848\u96c6\u6570\u91cf\u5927\u7684\u5b9e\u4f8b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8ba1\u6570\u5668\uff0c\u7ed3\u5408\u679a\u4e3e\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "SharpASP-SR \u662f\u6790\u53d6\u903b\u8f91\u7a0b\u5e8f\u8ba1\u6570\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u6df7\u5408\u65b9\u6cd5\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2507.11798", "pdf": "https://arxiv.org/pdf/2507.11798", "abs": "https://arxiv.org/abs/2507.11798", "authors": ["Szilveszter N\u00e1das", "Lars Ernstr\u00f6m", "David Lindero", "Jonathan Lynam"], "title": "On QoE-Aware Traffic Management for Real-time, Interactive Video with Time-variant Spatial Complexity", "categories": ["cs.NI"], "comment": null, "summary": "We analyzed spatial complexity, defined as the relationship between the\nrequired bitrate and a corresponding picture Quality of Experience (QoE)\nmetric, for realistic, long, real-time, interactive video clips. Apart from\nvariation across different content types, e.g., game genres, we discovered\ntime-variability within a clip from second to second, and explored the\nramifications for traffic management. We introduced utility as an elegant way\nto manage resource sharing preferences. Our analysis of resource sharing\nmethods shows that frequent QoE-aware reallocation has significant performance\nadvantages compared to static rate allocation, even in case the latter is based\non rich information about long-term average spatial complexity. We have also\nshown that utility-based resource allocation has clear advantages over methods\ntargeting equal QoE allocation, it increases the average QoE, while it still\ncontrols the worst case QoE.", "AI": {"tldr": "\u5206\u6790\u4e86\u5b9e\u65f6\u4ea4\u4e92\u89c6\u9891\u7684\u7a7a\u95f4\u590d\u6742\u6027\u4e0eQoE\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6548\u7528\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u4f18\u4e8e\u9759\u6001\u548c\u5747\u7b49QoE\u5206\u914d\u3002", "motivation": "\u7814\u7a76\u5b9e\u65f6\u4ea4\u4e92\u89c6\u9891\u4e2d\u7a7a\u95f4\u590d\u6742\u6027\u4e0eQoE\u7684\u5173\u7cfb\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u5206\u6790\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u7684\u65f6\u95f4\u53d8\u5f02\u6027\uff0c\u5f15\u5165\u6548\u7528\u6a21\u578b\u8fdb\u884c\u8d44\u6e90\u5206\u914d\u3002", "result": "\u52a8\u6001QoE\u611f\u77e5\u8d44\u6e90\u5206\u914d\u663e\u8457\u4f18\u4e8e\u9759\u6001\u5206\u914d\uff0c\u6548\u7528\u6a21\u578b\u63d0\u9ad8\u4e86\u5e73\u5747QoE\u5e76\u63a7\u5236\u6700\u5dee\u60c5\u51b5\u3002", "conclusion": "\u6548\u7528\u4e3a\u57fa\u7840\u7684\u8d44\u6e90\u5206\u914d\u5728\u5b9e\u65f6\u89c6\u9891\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u3002"}}
{"id": "2507.12373", "pdf": "https://arxiv.org/pdf/2507.12373", "abs": "https://arxiv.org/abs/2507.12373", "authors": ["Dariush Pourkeramati", "Gareth Wadge", "Rachel Hassall", "Charlotte Mitchell", "Anish Khadka", "Shiwang Jaiswal", "Andrew Duncan", "Rossella Arcucci"], "title": "Emerging Paradigms in the Energy Sector: Forecasting and System Control Optimisation", "categories": ["cs.ET", "eess.SP"], "comment": null, "summary": "The energy sector is experiencing rapid transformation due to increasing\nrenewable energy integration, decentralisation of power systems, and a\nheightened focus on efficiency and sustainability. With energy demand becoming\nincreasingly dynamic and generation sources more variable, advanced forecasting\nand optimisation strategies are crucial for maintaining grid stability,\ncost-effectiveness, and environmental sustainability. This paper explores\nemerging paradigms in energy forecasting and management, emphasizing four\ncritical domains: Energy Demand Forecasting integrated with Weather Data,\nBuilding Energy Optimisation, Heat Network Optimisation, and Energy Management\nSystem (EMS) Optimisation within a System of Systems (SoS) framework.\nLeveraging machine learning techniques and Model Predictive Control (MPC), the\nstudy demonstrates substantial enhancements in energy efficiency across scales\n-- from individual buildings to complex interconnected energy networks.\nWeather-informed demand forecasting significantly improves grid resilience and\nresource allocation strategies. Smart building optimisation integrates\npredictive analytics to substantially reduce energy consumption without\ncompromising occupant comfort. Optimising CHP-based heat networks achieves cost\nand carbon savings while adhering to operational and asset constraints. At the\nsystems level, sophisticated EMS optimisation ensures coordinated control of\ndistributed resources, storage solutions, and demand-side flexibility. Through\nreal-world case studies we highlight the potential of AI-driven automation and\nintegrated control solutions in facilitating a resilient, efficient, and\nsustainable energy future.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u80fd\u6e90\u9886\u57df\u7684\u524d\u6cbf\u9884\u6d4b\u4e0e\u7ba1\u7406\u65b9\u6cd5\uff0c\u91cd\u70b9\u5305\u62ec\u80fd\u6e90\u9700\u6c42\u4e0e\u5929\u6c14\u6570\u636e\u7ed3\u5408\u7684\u9884\u6d4b\u3001\u5efa\u7b51\u80fd\u6e90\u4f18\u5316\u3001\u70ed\u7f51\u4f18\u5316\u53ca\u7cfb\u7edf\u6846\u67b6\u4e0b\u7684\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u4f18\u5316\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6280\u672f\u63d0\u5347\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u3001\u7535\u529b\u7cfb\u7edf\u5206\u6563\u5316\u4ee5\u53ca\u5bf9\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u7684\u5173\u6ce8\u589e\u52a0\uff0c\u80fd\u6e90\u90e8\u95e8\u6b63\u7ecf\u5386\u5feb\u901f\u53d8\u9769\uff0c\u9700\u8981\u5148\u8fdb\u7684\u9884\u6d4b\u548c\u4f18\u5316\u7b56\u7565\u4ee5\u7ef4\u6301\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u7814\u7a76\u6db5\u76d6\u4e86\u80fd\u6e90\u9700\u6c42\u9884\u6d4b\u3001\u5efa\u7b51\u80fd\u6e90\u4f18\u5316\u3001\u70ed\u7f51\u4f18\u5316\u53ca\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\uff0c\u7814\u7a76\u8868\u660eAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u548c\u96c6\u6210\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u80fd\u663e\u8457\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u5e76\u589e\u5f3a\u7535\u7f51\u97e7\u6027\u3002", "conclusion": "AI\u548c\u96c6\u6210\u63a7\u5236\u6280\u672f\u5728\u63a8\u52a8\u80fd\u6e90\u7cfb\u7edf\u5411\u9ad8\u6548\u3001\u97e7\u6027\u53ca\u53ef\u6301\u7eed\u65b9\u5411\u8f6c\u578b\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.11628", "pdf": "https://arxiv.org/pdf/2507.11628", "abs": "https://arxiv.org/abs/2507.11628", "authors": ["Jiangnan Xu", "Haeseul Cha", "Gosu Choi", "Gyu-cheol Lee", "Yeo-Jin Yoon", "Zucheul Lee", "Konstantinos Papangelis", "Dae Hyun Kim", "Juho Kim"], "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling", "categories": ["cs.HC"], "comment": null, "summary": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences.", "AI": {"tldr": "DiaryPlay\u662f\u4e00\u4e2aAI\u8f85\u52a9\u7684\u4ea4\u4e92\u5f0f\u5c0f\u6545\u4e8b\u521b\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u751f\u6210\u6838\u5fc3\u6545\u4e8b\u5143\u7d20\uff0c\u5e76\u5229\u7528LLM\u6280\u672f\u5b9e\u73b0\u5206\u652f\u53d9\u4e8b\u8bbe\u8ba1\uff0c\u7b80\u5316\u4e86\u65e5\u5e38\u8bb2\u6545\u4e8b\u7684\u521b\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u4ea4\u4e92\u5f0f\u5c0f\u6545\u4e8b\uff08\u4ea4\u4e92\u5f0f\u5c0f\u63d2\u56fe\uff09\u662f\u4e00\u79cd\u6d41\u884c\u7684\u6c89\u6d78\u5f0f\u53d9\u4e8b\u65b9\u5f0f\uff0c\u4f46\u7531\u4e8e\u5236\u4f5c\u590d\u6742\uff0c\u5c1a\u672a\u88ab\u65e5\u5e38\u8bb2\u6545\u4e8b\u8005\u5e7f\u6cdb\u4f7f\u7528\u3002DiaryPlay\u65e8\u5728\u901a\u8fc7AI\u8f85\u52a9\u964d\u4f4e\u521b\u4f5c\u95e8\u69db\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u4f7f\u7528\u3002", "method": "DiaryPlay\u63a5\u53d7\u81ea\u7136\u8bed\u8a00\u6545\u4e8b\u8f93\u5165\uff0c\u63d0\u53d6\u6838\u5fc3\u5143\u7d20\uff08\u73af\u5883\u3001\u89d2\u8272\u3001\u4e8b\u4ef6\uff09\uff0c\u5e76\u5229\u7528LLM\u6280\u672f\u5c06\u5355\u7ebf\u6545\u4e8b\u81ea\u52a8\u8f6c\u5316\u4e3a\u5206\u652f\u53d9\u4e8b\u7ed3\u6784\uff0c\u7b80\u5316\u591a\u5206\u652f\u8bbe\u8ba1\u3002", "result": "\u6280\u672f\u8bc4\u4f30\uff0816\u4eba\uff09\u663e\u793aDiaryPlay\u751f\u6210\u7684\u89d2\u8272\u6d3b\u52a8\u5728\u53ef\u4fe1\u5ea6\u4e0a\u4e0e\u4eba\u5de5\u521b\u4f5c\u76f8\u5f53\uff1b\u7528\u6237\u7814\u7a76\uff0816\u4eba\uff09\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301\u521b\u4f5c\u3001\u4fdd\u6301\u4f5c\u8005\u610f\u56fe\u5e76\u63d0\u4f9b\u5438\u5f15\u4eba\u7684\u89c2\u770b\u4f53\u9a8c\u3002", "conclusion": "DiaryPlay\u901a\u8fc7AI\u8f85\u52a9\u7b80\u5316\u4e86\u4ea4\u4e92\u5f0f\u5c0f\u6545\u4e8b\u7684\u521b\u4f5c\uff0c\u5e73\u8861\u4e86\u521b\u4f5c\u590d\u6742\u6027\u4e0e\u53d9\u4e8b\u7075\u6d3b\u6027\uff0c\u9002\u5408\u65e5\u5e38\u8bb2\u6545\u4e8b\u573a\u666f\u4f7f\u7528\u3002"}}
{"id": "2507.11897", "pdf": "https://arxiv.org/pdf/2507.11897", "abs": "https://arxiv.org/abs/2507.11897", "authors": ["Tyler Hou", "Shadaj Laddad", "Joseph M. Hellerstein"], "title": "Towards Relational Contextual Equality Saturation", "categories": ["cs.PL", "cs.DB"], "comment": "Appeared at EGRAPHS 2024", "summary": "Equality saturation is a powerful technique for program optimization.\nContextual equality saturation extends this to support rewrite rules that are\nconditioned on where a term appears in an expression. Existing work has brought\ncontextual reasoning to egg; in this paper, we share our ongoing work to extend\nthis to relational equality saturation in egglog. We summarize the existing\napproaches to contextual equality saturation, outline its main applications,\nand identify key challenges in combining this approach with relational models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5c06\u4e0a\u4e0b\u6587\u76f8\u7b49\u6027\u9971\u548c\u6269\u5c55\u5230egglog\u4e2d\u7684\u5173\u7cfb\u76f8\u7b49\u6027\u9971\u548c\u7684\u6b63\u5728\u8fdb\u884c\u7684\u5de5\u4f5c\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5df2\u5c06\u4e0a\u4e0b\u6587\u63a8\u7406\u5f15\u5165egg\uff0c\u672c\u6587\u65e8\u5728\u5c06\u5176\u6269\u5c55\u5230\u5173\u7cfb\u76f8\u7b49\u6027\u9971\u548c\uff0c\u63a2\u7d22\u5176\u5728\u5173\u7cfb\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4e0e\u6311\u6218\u3002", "method": "\u603b\u7ed3\u73b0\u6709\u4e0a\u4e0b\u6587\u76f8\u7b49\u6027\u9971\u548c\u65b9\u6cd5\uff0c\u63d0\u51fa\u5c06\u5176\u4e0e\u5173\u7cfb\u6a21\u578b\u7ed3\u5408\u7684\u6280\u672f\u8def\u7ebf\u3002", "result": "\u5c1a\u672a\u660e\u786e\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u5df2\u8bc6\u522b\u51fa\u7ed3\u5408\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u5173\u7cfb\u6a21\u578b\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u662f\u6b63\u5728\u8fdb\u884c\u7684\u5de5\u4f5c\uff0c\u65e8\u5728\u63a8\u52a8\u4e0a\u4e0b\u6587\u76f8\u7b49\u6027\u9971\u548c\u5728\u5173\u7cfb\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.11560", "pdf": "https://arxiv.org/pdf/2507.11560", "abs": "https://arxiv.org/abs/2507.11560", "authors": ["Xin Wang", "Xiao Huan Li", "Xun Wang"], "title": "A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing", "categories": ["cs.DC", "cs.AI"], "comment": "6 pages, 4 figures, accepted by ICCC 2025", "summary": "The integration of the Industrial Internet of Things (IIoT) with Artificial\nIntelligence-Generated Content (AIGC) offers new opportunities for smart\nmanufacturing, but it also introduces challenges related to\ncomputation-intensive tasks and low-latency demands. Traditional generative\nmodels based on cloud computing are difficult to meet the real-time\nrequirements of AIGC tasks in IIoT environments, and edge computing can\neffectively reduce latency through task offloading. However, the dynamic nature\nof AIGC tasks, model switching delays, and resource constraints impose higher\ndemands on edge computing environments. To address these challenges, this paper\nproposes an AIGC task offloading framework tailored for IIoT edge computing\nenvironments, considering the latency and energy consumption caused by AIGC\nmodel switching for the first time. IIoT devices acted as multi-agent\ncollaboratively offload their dynamic AIGC tasks to the most appropriate edge\nservers deployed with different generative models. A model aware AIGC task\noffloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG-MATO) is devised to minimize the latency and energy. Experimental\nresults show that MADDPG-MATO outperforms baseline algorithms, achieving an\naverage reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%\nincrease in task completion rate across four sets of experiments with model\nnumbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is\nrobust and efficient in dynamic, high-load IIoT environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5de5\u4e1a\u7269\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684AIGC\u4efb\u52a1\u5378\u8f7d\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u4f18\u5316\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u4e0eAIGC\u7684\u7ed3\u5408\u5728\u667a\u80fd\u5236\u9020\u4e2d\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\uff0c\u4f46\u4e5f\u9762\u4e34\u8ba1\u7b97\u5bc6\u96c6\u548c\u4f4e\u5ef6\u8fdf\u7684\u6311\u6218\uff0c\u4f20\u7edf\u4e91\u8ba1\u7b97\u6a21\u578b\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eMADDPG-MATO\u7b97\u6cd5\u7684AIGC\u4efb\u52a1\u5378\u8f7d\u6846\u67b6\uff0c\u9996\u6b21\u8003\u8651\u4e86\u6a21\u578b\u5207\u6362\u5e26\u6765\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u8bbe\u5907\u534f\u4f5c\u5378\u8f7d\u4efb\u52a1\u81f3\u5408\u9002\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMADDPG-MATO\u76f8\u6bd4\u57fa\u51c6\u7b97\u6cd5\u5e73\u5747\u964d\u4f4e\u5ef6\u8fdf6.98%\u3001\u80fd\u80177.12%\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u53473.72%\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u52a8\u6001\u9ad8\u8d1f\u8f7d\u7684\u5de5\u4e1a\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.12028", "pdf": "https://arxiv.org/pdf/2507.12028", "abs": "https://arxiv.org/abs/2507.12028", "authors": ["Soheil Mahdizadeh", "Elyas Oustad", "Mohsen Ansari"], "title": "MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments", "categories": ["cs.AR", "cs.DC", "cs.NI"], "comment": null, "summary": "Task offloading in three-layer fog computing environments presents a critical\nchallenge due to user equipment (UE) mobility, which frequently triggers costly\nservice migrations and degrades overall system performance. This paper\naddresses this problem by proposing MOFCO, a novel Mobility- and\nMigration-aware Task Offloading algorithm for Fog Computing environments. The\nproposed method formulates task offloading and resource allocation as a\nMixed-Integer Nonlinear Programming (MINLP) problem and employs a\nheuristic-aided evolutionary game theory approach to solve it efficiently. To\nevaluate MOFCO, we simulate mobile users using SUMO, providing realistic\nmobility patterns. Experimental results show that MOFCO reduces system cost,\ndefined as a combination of latency and energy consumption, by an average of\n19% and up to 43% in certain scenarios compared to state-of-the-art methods.", "AI": {"tldr": "MOFCO\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u8fdb\u5316\u535a\u5f08\u8bba\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e09\u5c42\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u4efb\u52a1\u5378\u8f7d\u7684\u6311\u6218\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7cfb\u7edf\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u8bbe\u5907\u79fb\u52a8\u6027\u5bfc\u81f4\u7684\u670d\u52a1\u8fc1\u79fb\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5c06\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u5efa\u6a21\u4e3aMINLP\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u542f\u53d1\u5f0f\u8f85\u52a9\u7684\u8fdb\u5316\u535a\u5f08\u8bba\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMOFCO\u5e73\u5747\u964d\u4f4e\u7cfb\u7edf\u6210\u672c19%\uff0c\u67d0\u4e9b\u573a\u666f\u4e0b\u6700\u9ad8\u53ef\u8fbe43%\u3002", "conclusion": "MOFCO\u5728\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u6709\u6548\u4f18\u5316\u4e86\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.11939", "pdf": "https://arxiv.org/pdf/2507.11939", "abs": "https://arxiv.org/abs/2507.11939", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "PolyChartQA\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\uff0c\u6db5\u76d610\u79cd\u8bed\u8a00\u768422,606\u4e2a\u56fe\u8868\u548c26,151\u5bf9\u95ee\u7b54\u5bf9\uff0c\u65e8\u5728\u63a8\u52a8\u5168\u7403\u5305\u5bb9\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u9650\u5236\u4e86\u5176\u5168\u7403\u53ef\u8bbf\u95ee\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7ba1\u9053\u5206\u79bb\u56fe\u8868\u6570\u636e\u548c\u6e32\u67d3\u4ee3\u7801\uff0c\u901a\u8fc7\u7ffb\u8bd1\u6570\u636e\u548c\u91cd\u7528\u4ee3\u7801\u7075\u6d3b\u751f\u6210\u591a\u8bed\u8a00\u56fe\u8868\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u548c\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\uff0c\u5c24\u5176\u662f\u975e\u62c9\u4e01\u6587\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5728\u6027\u80fd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "PolyChartQA\u4e3a\u7cfb\u7edf\u6027\u8bc4\u4f30\u591a\u8bed\u8a00\u56fe\u8868\u7406\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u5168\u7403\u5305\u5bb9\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.12084", "pdf": "https://arxiv.org/pdf/2507.12084", "abs": "https://arxiv.org/abs/2507.12084", "authors": ["Keke Gai", "Haochen Liang", "Jing Yu", "Liehuang Zhu", "Dusit Niyato"], "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u53cd\u9988\u667a\u80fd\u5408\u7ea6\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff08LLAMA\uff09\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u3001\u8fdb\u5316\u53d8\u5f02\u7b56\u7565\u548c\u6df7\u5408\u6d4b\u8bd5\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6f0f\u6d1e\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u6a21\u7cca\u6d4b\u8bd5\u4e2d\uff0c\u53d8\u5f02\u8c03\u5ea6\u5bf9\u6d4b\u8bd5\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u79cd\u5b50\u8c03\u5ea6\u548c\u751f\u6210\uff0c\u53d8\u5f02\u8c03\u5ea6\u7684\u7814\u7a76\u8f83\u5c11\u3002LLAMA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "LLAMA\u91c7\u7528\u5206\u5c42\u63d0\u793a\u7b56\u7565\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u521d\u59cb\u79cd\u5b50\uff0c\u7ed3\u5408\u591a\u53cd\u9988\u4f18\u5316\u673a\u5236\u52a8\u6001\u8c03\u6574\u79cd\u5b50\u751f\u6210\u3001\u9009\u62e9\u548c\u53d8\u5f02\u8c03\u5ea6\uff0c\u5e76\u901a\u8fc7\u8fdb\u5316\u6a21\u7cca\u6d4b\u8bd5\u5f15\u64ce\u7ed3\u5408\u7b26\u53f7\u6267\u884c\u63d0\u5347\u6d4b\u8bd5\u6df1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLAMA\u5728\u8986\u76d6\u7387\u548c\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8691%\u7684\u6307\u4ee4\u8986\u76d6\u7387\u548c90%\u7684\u5206\u652f\u8986\u76d6\u7387\uff0c\u68c0\u6d4b\u51fa148\u4e2a\u5df2\u77e5\u6f0f\u6d1e\u4e2d\u7684132\u4e2a\u3002", "conclusion": "LLAMA\u5c55\u793a\u4e86\u5176\u5728\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u6d4b\u8bd5\u4e2d\u7684\u9ad8\u6548\u6027\u3001\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u667a\u80fd\u5408\u7ea6\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2507.11949", "pdf": "https://arxiv.org/pdf/2507.11949", "abs": "https://arxiv.org/abs/2507.11949", "authors": ["Shuyang Xu", "Zhiyang Dou", "Mingyi Shi", "Liang Pan", "Leo Ho", "Jingbo Wang", "Yuan Liu", "Cheng Lin", "Yuexin Ma", "Wenping Wang", "Taku Komura"], "title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u97f3\u9891\u9a71\u52a8\u7684\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u9996\u4e2a\u5168\u9762\u7684\u7a7a\u95f4\u97f3\u9891\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u52a8\u4f5c\u3002", "motivation": "\u76ee\u524d\u865a\u62df\u4eba\u7c7b\u5bf9\u591a\u6837\u5316\u542c\u89c9\u523a\u6fc0\u7684\u52a8\u6001\u548c\u771f\u5b9e\u54cd\u5e94\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u7814\u7a76\u7684\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u901a\u5e38\u5ffd\u7565\u7a7a\u95f4\u97f3\u9891\u5bf9\u4eba\u7c7b\u52a8\u4f5c\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86MOSPA\u6846\u67b6\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u5229\u7528\u6709\u6548\u7684\u878d\u5408\u673a\u5236\u6355\u6349\u97f3\u9891\u4e0e\u52a8\u4f5c\u7684\u5173\u7cfb\u3002", "result": "\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u591a\u6837\u5316\u7684\u771f\u5b9e\u52a8\u4f5c\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86\u7a7a\u95f4\u97f3\u9891\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\u7684\u7a7a\u767d\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2507.11827", "pdf": "https://arxiv.org/pdf/2507.11827", "abs": "https://arxiv.org/abs/2507.11827", "authors": ["Shaurya Gomber", "Debangshu Banerjee", "Gagandeep Singh"], "title": "Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers", "categories": ["cs.PL"], "comment": "42 pages, 8 figures", "summary": "Numerical abstract interpretation is a widely used framework for the static\nanalysis of numerical programs. However, existing numerical abstract\ninterpreters rely on hand-crafted, instruction-specific transformers tailored\nto each domain, with no general algorithm for handling common operations across\ndomains. This limits extensibility, prevents precise compositional reasoning\nover instruction sequences, and forces all downstream tasks to use the same\nfixed transformer regardless of their precision, efficiency, or task-specific\nrequirements. To address these limitations, we propose a universal transformer\nsynthesis algorithm that constructs a parametric family of sound abstract\ntransformers for any given polyhedral numerical domain and a concrete operator\nfrom the class of Quadratic-Bounded Guarded Operators (QGO), which includes\nboth individual instructions and structured sequences. Each instantiation in\nthis family is sound by construction, enabling downstream analyses to adapt the\ntransformer to their particular needs. The space of transformers is\ndifferentiable but complex. To efficiently explore this space of transformers,\nwe introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided\nsearch strategy that steers the search process based on downstream analysis\nobjectives and runtime constraints. We implement these ideas in the USTAD\nframework and evaluate their effectiveness across three numerical abstract\ndomains: Zones, Octagons, and Polyhedra. Our results demonstrate that the\nuniversal synthesis algorithm successfully constructs sound families of\ntransformers across domains, and that USTAD achieves significant, tunable\nprecision gains over baselines by leveraging compositional reasoning and\nefficient gradient-guided traversal of the transformer space.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u62bd\u8c61\u8f6c\u6362\u5668\u5408\u6210\u7b97\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u503c\u62bd\u8c61\u89e3\u91ca\u4e2d\u624b\u5de5\u5b9a\u5236\u8f6c\u6362\u5668\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u591a\u57df\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u503c\u62bd\u8c61\u5206\u6790\u5668\u4f9d\u8d56\u624b\u5de5\u5b9a\u5236\u7684\u8f6c\u6362\u5668\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9650\u5236\u4e86\u6269\u5c55\u6027\u548c\u7cbe\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u901a\u7528\u8f6c\u6362\u5668\u5408\u6210\u7b97\u6cd5\uff0c\u652f\u6301\u591a\u57df\u64cd\u4f5c\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u5f15\u5bfc\u641c\u7d22\uff08AGG\uff09\u4f18\u5316\u8f6c\u6362\u5668\u9009\u62e9\u3002", "result": "USTAD\u6846\u67b6\u5728Zones\u3001Octagons\u548cPolyhedra\u7b49\u57df\u4e2d\u6210\u529f\u751f\u6210\u7cbe\u786e\u4e14\u53ef\u8c03\u7684\u8f6c\u6362\u5668\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u901a\u7528\u8f6c\u6362\u5668\u5408\u6210\u7b97\u6cd5\u548cAGG\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6570\u503c\u62bd\u8c61\u5206\u6790\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\u3002"}}
{"id": "2507.11704", "pdf": "https://arxiv.org/pdf/2507.11704", "abs": "https://arxiv.org/abs/2507.11704", "authors": ["Jorge Fandinno", "Christoph Glinzer", "Zachary Hansen", "Jan Heuer", "Yuliya Lierler", "Vladimir Lifschitz", "Torsten Schaub", "Tobias Stolzmann"], "title": "Anthem 2.0: Automated Reasoning for Answer Set Programming", "categories": ["cs.LO", "I.2.3"], "comment": "Accepted to Theory and Practice of Logic Programming (ICLP 2025)", "summary": "Anthem 2.0 is a tool to aid in the verification of logic programs written in\nan expressive fragment of Clingo's input language named mini-gringo, which\nincludes arithmetic operations and simple choice rules but not aggregates. It\ncan translate logic programs into formula representations in the logic of\nhere-and-there, and analyze properties of logic programs such as tightness.\nMost importantly, Anthem 2.0 can support program verification by invoking\nfirst-order theorem provers to confirm that a program adheres to a first-order\nspecification, or to establish strong and external equivalence of programs.\nThis paper serves as an overview of the system's capabilities. We demonstrate\nhow to use Anthem 2.0 effectively and interpret its results.", "AI": {"tldr": "Anthem 2.0\u662f\u4e00\u4e2a\u7528\u4e8e\u9a8c\u8bc1\u57fa\u4e8emini-gringo\u8bed\u8a00\u7f16\u5199\u7684\u903b\u8f91\u7a0b\u5e8f\u7684\u5de5\u5177\uff0c\u652f\u6301\u5c06\u903b\u8f91\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u903b\u8f91\u516c\u5f0f\u5e76\u901a\u8fc7\u4e00\u9636\u5b9a\u7406\u8bc1\u660e\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u903b\u8f91\u7a0b\u5e8f\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u4e00\u9636\u89c4\u8303\u5e76\u652f\u6301\u7a0b\u5e8f\u95f4\u7684\u7b49\u4ef7\u6027\u8bc1\u660e\u3002", "method": "\u5c06\u903b\u8f91\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u903b\u8f91\u516c\u5f0f\uff0c\u5e76\u5229\u7528\u4e00\u9636\u5b9a\u7406\u8bc1\u660e\u5668\u8fdb\u884c\u5206\u6790\u548c\u9a8c\u8bc1\u3002", "result": "Anthem 2.0\u80fd\u591f\u9a8c\u8bc1\u7a0b\u5e8f\u7684\u7d27\u81f4\u6027\u3001\u662f\u5426\u7b26\u5408\u89c4\u8303\u4ee5\u53ca\u7a0b\u5e8f\u95f4\u7684\u7b49\u4ef7\u6027\u3002", "conclusion": "Anthem 2.0\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u903b\u8f91\u7a0b\u5e8f\u7684\u591a\u79cd\u5206\u6790\u548c\u9a8c\u8bc1\u9700\u6c42\u3002"}}
{"id": "2507.11935", "pdf": "https://arxiv.org/pdf/2507.11935", "abs": "https://arxiv.org/abs/2507.11935", "authors": ["Jikang Deng", "Fizza Hassan", "Hui Zhou", "Saad Al-Ahmadi", "Mohamed-Slim Alouini", "Daniel B. Da Costa"], "title": "Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview", "categories": ["cs.NI", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "As the path toward 6G networks is being charted, the emerging applications\nhave motivated evolutions of network architectures to realize the efficient,\nreliable, and flexible wireless networks. Among the potential architectures,\nthe non-terrestrial network (NTN) and open radio access network (ORAN) have\nreceived increasing interest from both academia and industry. Although the\ndeployment of NTNs ensures coverage, enhances spectral efficiency, and improves\nthe resilience of wireless networks. The high altitude and mobility of NTN\npresent new challenges in the development and operations (DevOps) lifecycle,\nhindering intelligent and scalable network management due to the lack of native\nartificial intelligence (AI) capability. With the advantages of ORAN in\ndisaggregation, openness, virtualization, and intelligence, several works\npropose integrating ORAN principles into the NTN, focusing mainly on ORAN\ndeployment options based on transparent and regenerative systems. However, a\nholistic view of how to effectively combine ORAN and NTN throughout the DevOps\nlifecycle is still missing, especially regarding how intelligent ORAN addresses\nthe scalability challenges in NTN. Motivated by this, in this paper, we first\nprovide the background knowledge about ORAN and NTN, outline the\nstate-of-the-art research on ORAN for NTNs, and present the DevOps challenges\nthat motivate the adoption of ORAN solutions. We then propose the ORAN-based\nNTN framework, discussing its features and architectures in detail. These\ninclude the discussion about flexible fronthaul split, RAN intelligent\ncontrollers (RICs) enhancement for distributed learning, scalable deployment\narchitecture, and multi-domain service management. Finally, the future research\ndirections, including combinations of the ORAN-based NTN framework and other\nenabling technologies and schemes, as well as the candidate use cases, are\nhighlighted.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u5f00\u653e\u5f0f\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08ORAN\uff09\u4e0e\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u540e\u8005\u5728\u9ad8\u7a7a\u548c\u79fb\u52a8\u6027\u5e26\u6765\u7684\u8fd0\u7ef4\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u8be6\u7ec6\u67b6\u6784\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\uff0cNTN\u548cORAN\u7684\u7ed3\u5408\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u667a\u80fd\u5316\u548c\u53ef\u6269\u5c55\u6027\u7ba1\u7406\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u5f00\u53d1\u548c\u8fd0\u7ef4\uff08DevOps\uff09\u751f\u547d\u5468\u671f\u4e2d\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eORAN\u7684NTN\u6846\u67b6\uff0c\u5305\u62ec\u7075\u6d3b\u7684fronthaul\u62c6\u5206\u3001\u589e\u5f3a\u7684RAN\u667a\u80fd\u63a7\u5236\u5668\u3001\u53ef\u6269\u5c55\u7684\u90e8\u7f72\u67b6\u6784\u548c\u591a\u57df\u670d\u52a1\u7ba1\u7406\u3002", "result": "\u63d0\u51fa\u4e86\u8be6\u7ec6\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86ORAN\u5982\u4f55\u589e\u5f3aNTN\u7684\u667a\u80fd\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ORAN\u4e0eNTN\u7ed3\u5408\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0e\u5176\u4ed6\u6280\u672f\u7684\u878d\u5408\u53ca\u6f5c\u5728\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.11677", "pdf": "https://arxiv.org/pdf/2507.11677", "abs": "https://arxiv.org/abs/2507.11677", "authors": ["Mashrur Rashik", "Jean-Daniel Fekete", "Narges Mahyar"], "title": "CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations", "categories": ["cs.HC"], "comment": "To appear in the IEEE Visualization and Visual Analytics (VIS)\n  Conference, Short Paper, 2025", "summary": "Communicating climate change remains challenging, as climate reports, though\nrich in data and visualizations, often feel too abstract or technical for the\npublic. Although personalization can enhance communication, most tools still\nlack the narrative and visualization tailoring needed to connect with\nindividual experiences. We present CLAImate, an AI-enabled prototype that\npersonalizes conversation narratives and localizes visualizations based on\nusers' climate knowledge and geographic location. We evaluated CLAImate through\ninternal verification of factual correctness, a formative study with experts,\nand a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70%\nFACTSCORE. Visualization experts appreciated its clarity and personalization,\nand seven out of ten UK participants reported better understanding and local\nrelevance of climate risks with CLAImate. We also discuss design challenges in\npersonalization, accuracy, and scalability, and outline future directions for\nintegrating visualizations in personalized conversational interfaces.", "AI": {"tldr": "CLAImate\u662f\u4e00\u6b3e\u57fa\u4e8eAI\u7684\u539f\u578b\u5de5\u5177\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u5bf9\u8bdd\u548c\u672c\u5730\u5316\u53ef\u89c6\u5316\u6539\u8fdb\u6c14\u5019\u53d8\u5316\u7684\u4f20\u64ad\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u6c14\u5019\u62a5\u544a\u8fc7\u4e8e\u62bd\u8c61\u548c\u6280\u672f\u5316\uff0c\u96be\u4ee5\u4e0e\u516c\u4f17\u4ea7\u751f\u5171\u9e23\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e2a\u6027\u5316\u7684\u4f20\u64ad\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86CLAImate\uff0c\u901a\u8fc7AI\u751f\u6210\u4e2a\u6027\u5316\u53d9\u4e8b\u548c\u672c\u5730\u5316\u53ef\u89c6\u5316\uff0c\u5e76\u8fdb\u884c\u4e86\u5185\u90e8\u9a8c\u8bc1\u3001\u4e13\u5bb6\u8bc4\u4f30\u548c\u8bd5\u70b9\u6d4b\u8bd5\u3002", "result": "CLAImate\u5728SNLI\u51c6\u786e\u7387\u548cFACTSCORE\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e13\u5bb6\u8ba4\u53ef\u5176\u6e05\u6670\u6027\uff0c\u591a\u6570\u7528\u6237\u53cd\u9988\u63d0\u5347\u4e86\u7406\u89e3\u548c\u672c\u5730\u76f8\u5173\u6027\u3002", "conclusion": "CLAImate\u5c55\u793a\u4e86\u5728\u6c14\u5019\u53d8\u5316\u4f20\u64ad\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u8fd8\u9700\u89e3\u51b3\u4e2a\u6027\u5316\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u7b49\u8bbe\u8ba1\u6311\u6218\u3002"}}
{"id": "2507.11563", "pdf": "https://arxiv.org/pdf/2507.11563", "abs": "https://arxiv.org/abs/2507.11563", "authors": ["Giulio Attenni", "Novella Bartolini"], "title": "Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers", "categories": ["cs.DC"], "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "summary": "This paper presents a theoretical discussion for environmentally-conscious\njob deployment and migration in cloud environments, aiming to minimize the\nenvironmental impact of resource provisioning while incorporating\nsustainability requirements. As the demand for sustainable cloud services\ngrows, it is crucial for cloud customers to select data center operators based\non sustainability metrics and to accurately report the ecological footprint of\ntheir services. To this end, we analyze sustainability reports and define\ncomprehensive environmental impact profiles for data centers, incorporating key\nsustainability indicators. We formalize the problem as an optimization model,\nbalancing multiple environmental factors while respecting user preferences. A\nsimulative case study demonstrates the {potential} of our approach compared to\nbaseline strategies that optimize for single sustainability factors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7406\u8bba\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u4e91\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u90e8\u7f72\u548c\u8fc1\u79fb\u6765\u51cf\u5c11\u5176\u73af\u5883\u5f71\u54cd\uff0c\u540c\u65f6\u878d\u5165\u53ef\u6301\u7eed\u6027\u8981\u6c42\u3002", "motivation": "\u968f\u7740\u53ef\u6301\u7eed\u4e91\u670d\u52a1\u9700\u6c42\u7684\u589e\u957f\uff0c\u9009\u62e9\u53ef\u6301\u7eed\u6027\u6570\u636e\u4e2d\u5fc3\u7684\u5ba2\u6237\u9700\u8981\u66f4\u51c6\u786e\u7684\u670d\u52a1\u751f\u6001\u8db3\u8ff9\u62a5\u544a\uff0c\u9a71\u52a8\u4e86\u76f8\u5173\u5de5\u4f5c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u53ef\u6301\u7eed\u6027\u62a5\u544a\u5b9a\u4e49\u6570\u636e\u4e2d\u5fc3\u7684\u73af\u5883\u5f71\u54cd\u6982\u51b5\uff0c\u5efa\u7acb\u4f18\u5316\u6a21\u578b\uff0c\u5e73\u8861\u591a\u79cd\u73af\u5883\u56e0\u7d20\u4e0e\u7528\u6237\u504f\u597d\u3002", "result": "\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f18\u5316\u5355\u4e00\u53ef\u6301\u7eed\u6027\u56e0\u7d20\u7684\u57fa\u7ebf\u7b56\u7565\uff0c\u5c55\u73b0\u4e86\u5176\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u6a21\u578b\u548c\u6a21\u62df\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5728\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6301\u7eed\u8d44\u6e90\u8c03\u5ea6\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u52bf\u3002"}}
{"id": "2507.12418", "pdf": "https://arxiv.org/pdf/2507.12418", "abs": "https://arxiv.org/abs/2507.12418", "authors": ["George Alexakis", "Dimitrios Schoinianakis", "Giorgos Dimitrakopoulos"], "title": "High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic", "categories": ["cs.AR"], "comment": "28th Euromicro Conference Series on Digital System Design (DSD 2025)", "summary": "The Number Theoretic Transform (NTT) is a fundamental operation in\nprivacy-preserving technologies, particularly within fully homomorphic\nencryption (FHE). The efficiency of NTT computation directly impacts the\noverall performance of FHE, making hardware acceleration a critical technology\nthat will enable realistic FHE applications. Custom accelerators, in FPGAs or\nASICs, offer significant performance advantages due to their ability to exploit\nmassive parallelism and specialized optimizations. However, the operation of\nNTT over large moduli requires large word-length modulo arithmetic that limits\nachievable clock frequencies in hardware and increases hardware area costs. To\novercome such deficits, digit-serial arithmetic has been explored for modular\nmultiplication and addition independently. The goal of this work is to leverage\ndigit-serial modulo arithmetic combined with appropriate redundant data\nrepresentation to design modular pipelined NTT accelerators that operate\nuniformly on arbitrary small digits, without the need for intermediate\n(de)serialization. The proposed architecture enables high clock frequencies\nthrough regular pipelining while maintaining parallelism. Experimental results\ndemonstrate that the proposed approach outperforms state-of-the-art\nimplementations and reduces hardware complexity under equal performance and\ninput-output bandwidth constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u4e32\u884c\u6a21\u7b97\u672f\u7684NTT\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u786c\u4ef6\u67b6\u6784\u63d0\u9ad8\u65f6\u949f\u9891\u7387\uff0c\u51cf\u5c11\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "NTT\u5728\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5176\u6548\u7387\u76f4\u63a5\u5f71\u54cdFHE\u7684\u6027\u80fd\u3002\u4f20\u7edf\u786c\u4ef6\u52a0\u901f\u56e0\u5927\u6a21\u6570\u8fd0\u7b97\u9650\u5236\u65f6\u949f\u9891\u7387\u548c\u786c\u4ef6\u9762\u79ef\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u6570\u5b57\u4e32\u884c\u6a21\u7b97\u672f\u548c\u5197\u4f59\u6570\u636e\u8868\u793a\uff0c\u8bbe\u8ba1\u65e0\u9700\u4e2d\u95f4\uff08\u53cd\uff09\u5e8f\u5217\u5316\u7684\u6a21\u5757\u5316\u6d41\u6c34\u7ebfNTT\u52a0\u901f\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u67b6\u6784\u5728\u9ad8\u65f6\u949f\u9891\u7387\u4e0b\u4fdd\u6301\u5e76\u884c\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\u5e76\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u3002", "conclusion": "\u6570\u5b57\u4e32\u884c\u6a21\u7b97\u672f\u4e0e\u5197\u4f59\u8868\u793a\u7684\u7ed3\u5408\u6709\u6548\u4f18\u5316\u4e86NTT\u786c\u4ef6\u52a0\u901f\uff0c\u4e3aFHE\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12042", "pdf": "https://arxiv.org/pdf/2507.12042", "abs": "https://arxiv.org/abs/2507.12042", "authors": ["Kazuki Shimada", "Archontis Politis", "Iran R. Roman", "Parthasaarathy Sudarsanam", "David Diaz-Guerra", "Ruchi Pandey", "Kengo Uchida", "Yuichiro Koyama", "Naoya Takahashi", "Takashi Shibuya", "Shusuke Takahashi", "Tuomas Virtanen", "Yuki Mitsufuji"], "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "comment": "5 pages, 2 figures", "summary": "This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.", "AI": {"tldr": "DCASE2025 Task 3 \u6311\u6218\u8d5b\u805a\u7126\u4e8e\u7acb\u4f53\u58f0\u6570\u636e\u7684\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\uff08SELD\uff09\uff0c\u5f15\u5165\u65b0\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\uff0c\u5305\u62ec\u5c4f\u5e55\u5185\u5916\u4e8b\u4ef6\u5206\u7c7b\uff0c\u5e76\u8c03\u6574\u4e86\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5c06SELD\u4efb\u52a1\u4ece\u591a\u901a\u9053\u97f3\u9891\u6269\u5c55\u5230\u7acb\u4f53\u58f0\u6570\u636e\uff0c\u4ee5\u9002\u5e94\u66f4\u5e38\u89c1\u7684\u6709\u9650\u89c6\u573a\u573a\u666f\uff0c\u5e76\u89e3\u51b3\u7acb\u4f53\u58f0\u6570\u636e\u4e2d\u56fa\u6709\u7684\u89d2\u5ea6\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u58f0\u97f3\u9891\u548c\u89c6\u9891\u5e27\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bbe\u8ba1\u57fa\u7ebf\u7cfb\u7edf\uff0c\u652f\u6301\u5c4f\u5e55\u5185\u5916\u4e8b\u4ef6\u5206\u7c7b\uff0c\u5e76\u4fee\u6539\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u57fa\u7ebf\u7cfb\u7edf\u5728\u7acb\u4f53\u58f0\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7acb\u4f53\u58f0SELD\u662f\u53ef\u884c\u7684\u7814\u7a76\u65b9\u5411\uff0cDCASE2025 Task 3\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u6570\u636e\u96c6\u548c\u4efb\u52a1\u6846\u67b6\u3002"}}
{"id": "2507.12104", "pdf": "https://arxiv.org/pdf/2507.12104", "abs": "https://arxiv.org/abs/2507.12104", "authors": ["Francisco Javier Cavero", "Juan C. Alonso", "Antonio Ruiz-Cort\u00e9s"], "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs", "categories": ["cs.SE", "cs.AI"], "comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing\n  for AI Applications), held in conjunction with the 22nd International\n  Conference on Service-Oriented Computing (ICSOC 2024)", "summary": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u5b9a\u4ef7\uff08iPricing\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316SaaS\u5b9a\u4ef7\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u7684\u4f4e\u6548\u548c\u9519\u8bef\u95ee\u9898\u3002", "motivation": "SaaS\u5e02\u573a\u7684\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u5b9a\u4ef7\u7ba1\u7406\u590d\u6742\u5316\uff0c\u76ee\u524d\u7f3a\u4e4f\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u4e9f\u9700\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528LLM\u9a71\u52a8\u7684AI4Pricing2Yaml\u5de5\u5177\uff0c\u7ed3\u5408\u7f51\u9875\u722c\u53d6\u548c\u667a\u80fd\u63d0\u53d6\u6280\u672f\uff0c\u5c06\u9759\u6001HTML\u5b9a\u4ef7\u8f6c\u6362\u4e3a\u52a8\u6001\u53ef\u8bfb\u7684iPricing\u6a21\u578b\u3002", "result": "\u572830\u4e2a\u5546\u4e1aSaaS\u548c150\u591a\u4e2a\u667a\u80fd\u5b9a\u4ef7\u7684\u9a8c\u8bc1\u4e2d\uff0c\u7cfb\u7edf\u6709\u6548\u63d0\u53d6\u4e86\u5173\u952e\u8981\u7d20\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5e7b\u89c9\u3001\u590d\u6742\u7ed3\u6784\u548c\u52a8\u6001\u5185\u5bb9\u95ee\u9898\u3002", "conclusion": "\u667a\u80fd\u5b9a\u4ef7\u81ea\u52a8\u5316\u6f5c\u529b\u5de8\u5927\uff0c\u672a\u6765\u7814\u7a76\u5c06\u63d0\u5347\u63d0\u53d6\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.11971", "pdf": "https://arxiv.org/pdf/2507.11971", "abs": "https://arxiv.org/abs/2507.11971", "authors": ["Tielong Wang", "Yuxuan Xiong", "Jinfan Liu", "Zhifan Zhang", "Ye Chen", "Yue Shi", "Bingbing Ni"], "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76843D\u5206\u5c42\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u67093D\u8868\u793a\uff08\u5982\u7f51\u683c\u3001\u4f53\u7d20\u3001\u70b9\u4e91\u548cNeRF\uff09\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u6570\u636e\u590d\u6742\u5ea6\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u67093D\u8868\u793a\u65b9\u6cd5\uff08\u5982\u7f51\u683c\u3001NeRF\u7b49\uff09\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u8868\u5f81\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u5206\u5c42\u4ee3\u7406\u8282\u70b9\u7ed3\u6784\uff0c\u901a\u8fc7\u6811\u72b6\u7ec4\u7ec7\u7684\u7a00\u758f\u8282\u70b9\u548c\u8f7b\u91cf\u7ea7MLP\u7f16\u7801\u5c40\u90e8\u5f62\u72b6\u4e0e\u7eb9\u7406\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u795e\u7ecf\u63d2\u503c\u548c\u8f7b\u91cf\u89e3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u91cd\u5efa\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u8868\u8fbe\u80fd\u529b\u3001\u9ad8\u4fdd\u771f\u6e32\u67d3\u8d28\u91cf\u548c\u5353\u8d8a\u7684\u7f16\u8f91\u6027\u80fd\u3002", "conclusion": "3D\u5206\u5c42\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\u4e3a3D\u6570\u636e\u7684\u901a\u7528\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.11961", "pdf": "https://arxiv.org/pdf/2507.11961", "abs": "https://arxiv.org/abs/2507.11961", "authors": ["Pascal Kettmann", "Jesse Heyninck", "Hannes Strass"], "title": "Approximation Fixpoint Theory as a Unifying Framework for Fuzzy Logic Programming Semantics (Extended Version)", "categories": ["cs.LO"], "comment": null, "summary": "Fuzzy logic programming is an established approach for reasoning under\nuncertainty. Several semantics from classical, two-valued logic programming\nhave been generalized to the case of fuzzy logic programs. In this paper, we\nshow that two of the most prominent classical semantics, namely the stable\nmodel and the well-founded semantics, can be reconstructed within the general\nframework of approximation fixpoint theory (AFT). This not only widens the\nscope of AFT from two- to many-valued logics, but allows a wide range of\nexisting AFT results to be applied to fuzzy logic programming. As first\nexamples of such applications, we clarify the formal relationship between\nexisting semantics, generalize the notion of stratification from classical to\nfuzzy logic programs, and devise \"more precise\" variants of the semantics.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u5c06\u7ecf\u5178\u7684\u7a33\u5b9a\u6a21\u578b\u548c\u826f\u57fa\u8bed\u4e49\u91cd\u6784\u4e3a\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\uff08AFT\uff09\u6846\u67b6\u5185\u7684\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\uff0c\u6269\u5c55\u4e86AFT\u7684\u5e94\u7528\u8303\u56f4\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bed\u4e49\u53d8\u4f53\u3002", "motivation": "\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\u662f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u5c06\u5176\u4e0e\u7ecf\u5178\u7684\u903b\u8f91\u7f16\u7a0b\u8bed\u4e49\u7ed3\u5408\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7AFT\u6846\u67b6\u7edf\u4e00\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\u7684\u8bed\u4e49\u3002", "method": "\u8bba\u6587\u5c06\u7ecf\u5178\u7684\u7a33\u5b9a\u6a21\u578b\u548c\u826f\u57fa\u8bed\u4e49\u91cd\u6784\u4e3aAFT\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u591a\u503c\u903b\u8f91\uff0c\u540c\u65f6\u5e94\u7528AFT\u7684\u73b0\u6709\u6210\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5305\u62ec\u6f84\u6e05\u73b0\u6709\u8bed\u4e49\u5173\u7cfb\u3001\u5c06\u5206\u5c42\u6982\u5ff5\u4ece\u7ecf\u5178\u903b\u8f91\u63a8\u5e7f\u5230\u6a21\u7cca\u903b\u8f91\uff0c\u5e76\u63d0\u51fa\u66f4\u7cbe\u786e\u7684\u8bed\u4e49\u53d8\u4f53\u3002", "conclusion": "AFT\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u6a21\u7cca\u903b\u8f91\u7f16\u7a0b\uff0c\u4e0d\u4ec5\u6269\u5c55\u4e86\u5176\u9002\u7528\u8303\u56f4\uff0c\u8fd8\u4e3a\u8bed\u4e49\u5206\u6790\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.12265", "pdf": "https://arxiv.org/pdf/2507.12265", "abs": "https://arxiv.org/abs/2507.12265", "authors": ["Zihan Zhu", "Dongchao Wu", "Zhanbang Zhang", "Jian Yang"], "title": "FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks", "categories": ["cs.NI", "cs.DS"], "comment": null, "summary": "Ever since Clos topologies were used in datacenter networks (DCNs), a\npractical centralized scheduling algorithm that supports dynamic scheduling has\nbeen absent. The introduction of optical switches in DCNs as a future-proof\nsolution exacerbates this problem due to several properties of optical\nswitches, such as the fact that they are generally bufferless and therefore\nrely on centralized scheduling, and that they have long switching times and\ntherefore require the number of rearrangements to be minimized.\n  In this paper, we propose a centralized scheduling algorithm that achieves\ntheoretical maximum throughput even in one-rate bidirectional Clos networks,\nwhile producing schemes with near-minimal numbers of rearrangements. It is the\nonly algorithm that directly supports bidirectional Clos networks and has a\ntime efficiency high enough to support dynamic scheduling to date. For static\nminimal rewiring, its running time ranges from a fraction to a few hundredths\nof other algorithms, and the number of rearrangements has also been steadily\nimproved, allowing for more frequent adjustments and less impact on ongoing\ncommunications. In addition, the algorithm is very flexible and can support\nvarious functional requirements in real-world environments. We achieve this\nresult through the replacement chain concept and bitset optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5149\u4ea4\u6362\u673a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\uff0c\u652f\u6301\u53cc\u5411Clos\u62d3\u6251\uff0c\u5b9e\u73b0\u7406\u8bba\u6700\u5927\u541e\u5410\u91cf\u548c\u6700\u5c0f\u91cd\u6392\u6b21\u6570\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u4e2d\u5149\u4ea4\u6362\u673a\u56e0\u65e0\u7f13\u51b2\u548c\u957f\u5207\u6362\u65f6\u95f4\u5bfc\u81f4\u7684\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u66ff\u6362\u94fe\u6982\u5ff5\u548c\u4f4d\u96c6\u4f18\u5316\u6280\u672f\u8bbe\u8ba1\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u65f6\u95f4\u6548\u7387\u9ad8\uff0c\u91cd\u6392\u6b21\u6570\u5c11\uff0c\u652f\u6301\u52a8\u6001\u8c03\u5ea6\u548c\u591a\u79cd\u5b9e\u9645\u9700\u6c42\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u76ee\u524d\u552f\u4e00\u652f\u6301\u53cc\u5411Clos\u7f51\u7edc\u7684\u9ad8\u6548\u52a8\u6001\u8c03\u5ea6\u65b9\u6848\u3002"}}
{"id": "2507.11797", "pdf": "https://arxiv.org/pdf/2507.11797", "abs": "https://arxiv.org/abs/2507.11797", "authors": ["Diana Romero", "Yasra Chandio", "Fatima Anwar", "Salma Elmalaki"], "title": "GIST: Group Interaction Sensing Toolkit for Mixed Reality", "categories": ["cs.HC", "cs.ET"], "comment": "11 pages, 6 figures", "summary": "Understanding how teams coordinate, share work, and negotiate roles in\nimmersive environments is critical for designing effective mixed-reality (MR)\napplications that support real-time collaboration. However, existing methods\neither rely on external cameras and offline annotation or focus narrowly on\nsingle modalities, limiting their validity and applicability. To address this,\nwe present a novel group interaction sensing toolkit (GIST), a deployable\nsystem that passively captures multi-modal interaction data, such as speech,\ngaze, and spatial proximity from commodity MR headset's sensors and\nautomatically derives both overall static interaction networks and dynamic\nmoment-by-moment behavior patterns. We evaluate GIST with a human subject study\nwith 48 participants across 12 four-person groups performing an open-ended\nimage-sorting task in MR. Our analysis shows strong alignment between the\nidentified behavior modes and shifts in interaction network structure,\nconfirming that momentary changes in speech, gaze, and proximity data are\nobservable through the sensor data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u611f\u77e5\u5de5\u5177\u5305GIST\uff0c\u7528\u4e8e\u6355\u6349MR\u73af\u5883\u4e2d\u7684\u56e2\u961f\u534f\u4f5c\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u56e2\u961f\u5728\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u884c\u4e3a\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u6027\u8f83\u5927\u3002", "method": "\u5f00\u53d1\u4e86GIST\u5de5\u5177\u5305\uff0c\u5229\u7528MR\u5934\u663e\u4f20\u611f\u5668\u6355\u6349\u591a\u6a21\u6001\u4ea4\u4e92\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u52a8\u6001\u884c\u4e3a\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGIST\u80fd\u6709\u6548\u8bc6\u522b\u884c\u4e3a\u6a21\u5f0f\u548c\u4ea4\u4e92\u7f51\u7edc\u7684\u53d8\u5316\u3002", "conclusion": "GIST\u4e3a\u7814\u7a76MR\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2507.11683", "pdf": "https://arxiv.org/pdf/2507.11683", "abs": "https://arxiv.org/abs/2507.11683", "authors": ["Seth Ockerman", "Amal Gueroudji", "Tanwi Mallick", "Yixuan He", "Line Pouchard", "Robert Ross", "Shivaram Venkataraman"], "title": "PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "To be published in the 2025 International Conference for High\n  Performance Computing, Networking, Storage, and Analysis", "summary": "Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for\nmodeling spatial and temporal data dependencies. However, their applications\nhave been limited primarily to small-scale datasets because of memory\nconstraints. While distributed training offers a solution, current frameworks\nlack support for spatiotemporal models and overlook the properties of\nspatiotemporal data. Informed by a scaling study on a large-scale workload, we\npresent PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch\nGeometric Temporal that integrates distributed data parallel training and two\nnovel strategies: index-batching and distributed-index-batching. Our index\ntechniques exploit spatiotemporal structure to construct snapshots dynamically\nat runtime, significantly reducing memory overhead, while\ndistributed-index-batching extends this approach by enabling scalable\nprocessing across multiple GPUs. Our techniques enable the first-ever training\nof an ST-GNN on the entire PeMS dataset without graph partitioning, reducing\npeak memory usage by up to 89\\% and achieving up to a 13.1x speedup over\nstandard DDP with 128 GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e86PGT-I\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u548c\u521b\u65b0\u7684\u7d22\u5f15\u6279\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86ST-GNN\u7684\u5185\u5b58\u5f00\u9500\u5e76\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3ST-GNN\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5f53\u524d\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\u7f3a\u4e4f\u5bf9\u65f6\u7a7a\u6a21\u578b\u7684\u652f\u6301\u3002", "method": "\u7ed3\u5408\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u8bad\u7ec3\uff0c\u63d0\u51fa\u7d22\u5f15\u6279\u5904\u7406\u548c\u5206\u5e03\u5f0f\u7d22\u5f15\u6279\u5904\u7406\u6280\u672f\uff0c\u52a8\u6001\u6784\u5efa\u65f6\u7a7a\u5feb\u7167\u4ee5\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728PeMS\u6570\u636e\u96c6\u4e0a\u9996\u6b21\u5b9e\u73b0\u4e86\u5168\u56fe\u8bad\u7ec3\uff0c\u5cf0\u503c\u5185\u5b58\u964d\u4f4e89%\uff0c128 GPU\u4e0b\u901f\u5ea6\u63d0\u534713.1\u500d\u3002", "conclusion": "PGT-I\u4e3a\u5927\u89c4\u6a21\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12442", "pdf": "https://arxiv.org/pdf/2507.12442", "abs": "https://arxiv.org/abs/2507.12442", "authors": ["Saptarshi Mitra", "Rachid Karami", "Haocheng Xu", "Sitao Huang", "Hyoukjun Kwon"], "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "12 pages, 7 figures", "summary": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86Transformer\u548cSSM\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0SSM\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edfTransformer\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u7814\u7a76SSM\u548c\u6df7\u5408\u6a21\u578b\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5168\u9762\u6bd4\u8f83Transformer\u3001SSM\u548c\u6df7\u5408\u6a21\u578b\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5ef6\u8fdf\u548c\u5e8f\u5217\u957f\u5ea6\u7b49\u65b9\u9762\u3002", "result": "SSM\u5728\u957f\u5e8f\u5217\uff08220K tokens\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4Transformer\u5feb4\u500d\uff0c\u4e14\u786c\u4ef6\u4f18\u5316\u7684SSM\u5185\u6838\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "SSM\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u66f4\u5177\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u7814\u7a76\u6846\u67b6\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2507.12060", "pdf": "https://arxiv.org/pdf/2507.12060", "abs": "https://arxiv.org/abs/2507.12060", "authors": ["Kun-Hsiang Lin", "Yu-Wen Tseng", "Kang-Yang Huang", "Jhih-Ciang Wu", "Wen-Huang Cheng"], "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by MM'25", "summary": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.", "AI": {"tldr": "InstructFLIP\u662f\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u8c03\u6574\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u589e\u5f3a\u8de8\u57df\u4eba\u8138\u53cd\u6b3a\u8bc8\uff08FAS\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4eba\u8138\u53cd\u6b3a\u8bc8\uff08FAS\uff09\u4e2d\u653b\u51fb\u7c7b\u578b\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u548c\u8de8\u57df\u8bad\u7ec3\u5197\u4f59\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u96c6\u6210VLMs\u548c\u6539\u8fdb\u5143\u57df\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u51faInstructFLIP\u6846\u67b6\uff0c\u5206\u79bb\u6307\u4ee4\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eInstructFLIP\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8de8\u57df\u8bad\u7ec3\u5197\u4f59\u3002", "conclusion": "InstructFLIP\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5143\u57df\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86FAS\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.12118", "pdf": "https://arxiv.org/pdf/2507.12118", "abs": "https://arxiv.org/abs/2507.12118", "authors": ["Noe Zerme\u00f1o", "Cristina Zuheros", "Lucas Daniel Del Rosso Calache", "Francisco Herrera", "Rosana Montes"], "title": "An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment", "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "In recent years, attention has increasingly focused on enhancing user\nsatisfaction with user interfaces, spanning both mobile applications and\nwebsites. One fundamental aspect of human-machine interaction is the concept of\nweb usability. In order to assess web usability, the A/B testing technique\nenables the comparison of data between two designs. Expanding the scope of\ntests to include the designs being evaluated, in conjunction with the\ninvolvement of both real and fictional users, presents a challenge for which\nfew online tools offer support. We propose a methodology for web usability\nevaluation based on user-centered approaches such as design thinking and\nlinguistic decision-making, named Linguistic Decision-Making for Web Usability\nEvaluation. This engages people in role-playing scenarios and conducts a number\nof usability tests, including the widely recognized System Usability Scale. We\nincorporate the methodology into a decision support system based on A/B\ntesting. We use real users in a case study to assess three Moodle platforms at\nthe University of Guadalajara, Mexico.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u4e2d\u5fc3\u65b9\u6cd5\u7684\u7f51\u9875\u53ef\u7528\u6027\u8bc4\u4f30\u65b9\u6cd5\u8bba\uff0c\u7ed3\u5408\u8bbe\u8ba1\u601d\u7ef4\u548c\u8bed\u8a00\u51b3\u7b56\uff0c\u5e76\u5728A/B\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u3002", "motivation": "\u63d0\u9ad8\u7528\u6237\u5bf9\u754c\u9762\uff08\u5982\u79fb\u52a8\u5e94\u7528\u548c\u7f51\u7ad9\uff09\u7684\u6ee1\u610f\u5ea6\u662f\u7814\u7a76\u7684\u4e3b\u8981\u52a8\u673a\uff0c\u5c24\u5176\u662f\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u7f51\u9875\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\"\u8bed\u8a00\u51b3\u7b56\"\u65b9\u6cd5\u8bba\uff0c\u7ed3\u5408\u89d2\u8272\u626e\u6f14\u548c\u591a\u79cd\u53ef\u7528\u6027\u6d4b\u8bd5\uff08\u5982\u7cfb\u7edf\u53ef\u7528\u6027\u91cf\u8868\uff09\uff0c\u5e76\u901a\u8fc7A/B\u6d4b\u8bd5\u652f\u6301\u7cfb\u7edf\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\uff08\u58a8\u897f\u54e5\u74dc\u8fbe\u62c9\u54c8\u62c9\u5927\u5b66\u7684Moodle\u5e73\u53f0\uff09\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7f51\u9875\u53ef\u7528\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.12156", "pdf": "https://arxiv.org/pdf/2507.12156", "abs": "https://arxiv.org/abs/2507.12156", "authors": ["Chen Li", "Shanshan Dong", "Sheng Qiu", "Jianmin Han", "Zan Gao", "Kemeng Huang", "Taku Komura"], "title": "SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models", "categories": ["cs.GR"], "comment": null, "summary": "Reconstructing dynamic fluids from sparse views is a long-standing and\nchallenging problem, due to the severe lack of 3D information from insufficient\nview coverage. While several pioneering approaches have attempted to address\nthis issue using differentiable rendering or novel view synthesis, they are\noften limited by time-consuming optimization and refinement processes under\nill-posed conditions. To tackle above challenges, we propose SmokeSVD, an\nefficient and effective framework to progressively generate and reconstruct\ndynamic smoke from a single video by integrating both the powerful generative\ncapabilities from diffusion models and physically guided consistency\noptimization towards realistic appearance and dynamic evolution. Specifically,\nwe first propose a physically guided side-view synthesizer based on diffusion\nmodels, which explicitly incorporates divergence and gradient guidance of\nvelocity fields to generate visually realistic and spatio-temporally consistent\nside-view images frame by frame, significantly alleviating the ill-posedness of\nsingle-view reconstruction without imposing additional constraints.\nSubsequently, we determine a rough estimation of density field from the pair of\nfront-view input and side-view synthetic image, and further refine 2D blurry\nnovel-view images and 3D coarse-grained density field through an iterative\nprocess that progressively renders and enhances the images from increasing\nnovel viewing angles, generating high-quality multi-view image sequences.\nFinally, we reconstruct and estimate the fine-grained density field, velocity\nfield, and smoke source via differentiable advection by leveraging the\nNavier-Stokes equations. Extensive quantitative and qualitative experiments\nshow that our approach achieves high-quality reconstruction and outperforms\nprevious state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6SmokeSVD\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u7269\u7406\u5f15\u5bfc\u4f18\u5316\u4ece\u5355\u89c6\u9891\u9ad8\u6548\u91cd\u5efa\u52a8\u6001\u70df\u96fe\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u6d41\u4f53\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f18\u5316\u8fc7\u7a0b\u8017\u65f6\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548c\u7269\u7406\u5f15\u5bfc\u4f18\u5316\uff0c\u5206\u6b65\u751f\u6210\u4fa7\u89c6\u56fe\u3001\u4f18\u5316\u5bc6\u5ea6\u573a\uff0c\u5e76\u5229\u7528\u53ef\u5fae\u5206\u5e73\u6d41\u91cd\u5efa\u70df\u96fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u70df\u96fe\u91cd\u5efa\u3002", "conclusion": "SmokeSVD\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u6d41\u4f53\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.11873", "pdf": "https://arxiv.org/pdf/2507.11873", "abs": "https://arxiv.org/abs/2507.11873", "authors": ["Breandan Considine"], "title": "Syntax Repair as Language Intersection", "categories": ["cs.FL", "cs.PL"], "comment": null, "summary": "We introduce a new technique for repairing syntax errors in arbitrary\ncontext-free languages. This technique models syntax repair as a language\nintersection problem by defining a finite language that provably generates\nevery syntactically valid repair within a given edit distance. Leveraging a\ntheoretical connection between the Bar-Hillel construction from formal language\ntheory and CFL reachability from program analysis, we show that repairability\nin a finite number of typographic edits is polylogarithmic parallel time\ndecidable and provide an enumeration algorithm based on the Brzozowski\nderivative. Finally, we evaluate this algorithm and its implementation,\ndemonstrating state-of-the-art results on a Python syntax repair benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u590d\u4efb\u610f\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8bed\u6cd5\u9519\u8bef\u7684\u65b0\u6280\u672f\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u8bed\u8a00\u4ea4\u96c6\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u7ed9\u5b9a\u7f16\u8f91\u8ddd\u79bb\u5185\u751f\u6210\u6240\u6709\u6709\u6548\u4fee\u590d\u7684\u53ef\u80fd\uff0c\u5e76\u5728Python\u8bed\u6cd5\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8bed\u6cd5\u9519\u8bef\u7684\u4fee\u590d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4efb\u610f\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u4e2d\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u751f\u6210\u6240\u6709\u53ef\u80fd\u4fee\u590d\u65b9\u6848\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u8bed\u6cd5\u4fee\u590d\u95ee\u9898\u5efa\u6a21\u4e3a\u8bed\u8a00\u4ea4\u96c6\u95ee\u9898\uff0c\u5229\u7528Bar-Hillel\u6784\u9020\u548cCFL\u53ef\u8fbe\u6027\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eBrzozowski\u5bfc\u6570\u7684\u679a\u4e3e\u7b97\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6b21\u6570\u7684\u7f16\u8f91\u64cd\u4f5c\u5185\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6570\u7ea7\u5e76\u884c\u65f6\u95f4\u7684\u53ef\u5224\u5b9a\u6027\uff0c\u5e76\u5728Python\u8bed\u6cd5\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u6280\u672f\u4e3a\u8bed\u6cd5\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4fee\u590d\u6548\u7387\u3002"}}
{"id": "2507.12286", "pdf": "https://arxiv.org/pdf/2507.12286", "abs": "https://arxiv.org/abs/2507.12286", "authors": ["Anouk Oudshoorn", "Magdalena Ortiz", "Mantas Simkus"], "title": "SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques", "categories": ["cs.LO"], "comment": "36 pages, 6 figures, submitted to the journal of Artificial\n  Intelligence (AIJ)", "summary": "SHACL and OWL are two prominent W3C standards for managing RDF data. These\nlanguages share many features, but they have one fundamental difference: OWL,\ndesigned for inferring facts from incomplete data, makes the open-world\nassumption, whereas SHACL is a constraint language that treats the data as\ncomplete and must be validated under the closed-world assumption. The\ncombination of both formalisms is very appealing and has been called for, but\ntheir semantic gap is a major challenge, semantically and computationally. In\nthis paper, we advocate a semantics for SHACL validation in the presence of\nontologies based on core universal models. We provide a technique for\nconstructing these models for ontologies in the rich data-tractable description\nlogic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to\ndevelop a rewriting technique that reduces SHACL validation in the presence of\nontologies to standard validation. Finally, we study the complexity of SHACL\nvalidation in the presence of ontologies, and show that even very simple\nontologies make the problem EXPTIME-complete, and PTIME-complete in data\ncomplexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5fc3\u901a\u7528\u6a21\u578b\u7684SHACL\u9a8c\u8bc1\u8bed\u4e49\uff0c\u7528\u4e8e\u5904\u7406\u672c\u4f53\u8bba\u7684\u5b58\u5728\u3002\u901a\u8fc7\u6784\u5efaHorn-ALCHIQ\u903b\u8f91\u7684\u6570\u636e\u53ef\u5904\u7406\u6a21\u578b\uff0c\u91c7\u7528\u91cd\u5199\u6280\u672f\u5c06SHACL\u9a8c\u8bc1\u7b80\u5316\u4e3a\u6807\u51c6\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "SHACL\u548cOWL\u5728\u8bed\u4e49\u548c\u8ba1\u7b97\u4e0a\u7684\u5dee\u8ddd\u4f7f\u5176\u7ed3\u5408\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3SHACL\u5728\u672c\u4f53\u5b58\u5728\u4e0b\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u586b\u8865\u8fd9\u4e00\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u6784\u5efaHorn-ALCHIQ\u903b\u8f91\u7684\u6838\u5fc3\u901a\u7528\u6a21\u578b\uff0c\u5229\u7528\u6709\u9650\u8868\u793a\u6cd5\u5f00\u53d1\u91cd\u5199\u6280\u672f\uff0c\u5c06\u672c\u4f53\u4e0b\u7684SHACL\u9a8c\u8bc1\u8f6c\u5316\u4e3a\u6807\u51c6\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u7b80\u5355\u7684\u672c\u4f53\u8bba\u4f1a\u4f7fSHACL\u9a8c\u8bc1\u95ee\u9898\u6210\u4e3aEXPTIME\u5b8c\u5168\u95ee\u9898\uff0c\u6570\u636e\u590d\u6742\u5ea6\u4e0b\u4e3aPTIME\u5b8c\u5168\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86SHACL\u4e0e\u672c\u4f53\u7ed3\u5408\u65f6\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2507.12443", "pdf": "https://arxiv.org/pdf/2507.12443", "abs": "https://arxiv.org/abs/2507.12443", "authors": ["Rajdeep Mondal", "Nikolaj Bjorner", "Todd Millstein", "Alan Tang", "George Varghese"], "title": "LLM-Based Config Synthesis requires Disambiguation", "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.PL"], "comment": null, "summary": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528LLMs\u8fdb\u884c\u7a0b\u5e8f\u5408\u6210\u65f6\u7528\u6237\u610f\u56fe\u4e0d\u660e\u786e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edfClarify\uff0c\u901a\u8fc7\u65b0\u6a21\u5757Disambiguator\u5e2e\u52a9\u89e3\u51b3\u610f\u56fe\u6a21\u7cca\u6027\u3002", "motivation": "LLMs\u5728\u5408\u6210\u7f51\u7edc\u914d\u7f6e\uff08\u5982\u8def\u7531\u6620\u5c04\u548cACLs\uff09\u65f6\uff0c\u7ecf\u5e38\u56e0\u4e3a\u7528\u6237\u610f\u56fe\u6a21\u7cca\u800c\u5bfc\u81f4\u52a8\u4f5c\u4f18\u5148\u7ea7\u65e0\u6cd5\u63a8\u65ad\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742ACLs\u4e2d\u5b58\u5728\u5927\u91cf\u91cd\u53e0\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u539f\u578b\u7cfb\u7edfClarify\uff0c\u7ed3\u5408LLMs\u548c\u65b0\u6a21\u5757Disambiguator\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u660e\u786e\u610f\u56fe\uff0c\u5e76\u9a8c\u8bc1\u5408\u6210\u540e\u7684\u8def\u7531\u7b56\u7565\u3002", "result": "\u5728\u4e00\u4e2a\u5c0f\u89c4\u6a21\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0cClarify\u6210\u529f\u5728\u6d88\u6b67\u540e\u589e\u91cf\u5408\u6210\u4e86\u8def\u7531\u7b56\u7565\u3002", "conclusion": "Clarify\u7684\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u7f51\u7edc\u914d\u7f6e\u5408\u6210\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9886\u57df\uff0c\u5f53LLMs\u80fd\u6b63\u786e\u5408\u6210\u66f4\u65b0\u610f\u56fe\u4f46\u6574\u5408\u65f6\u53ef\u80fd\u4ea7\u751f\u6a21\u7cca\u6027\u65f6\u3002"}}
{"id": "2507.12384", "pdf": "https://arxiv.org/pdf/2507.12384", "abs": "https://arxiv.org/abs/2507.12384", "authors": ["Bo Wen", "Guoyun Gao", "Zhicheng Xu", "Ruibin Mao", "Xiaojuan Qi", "X. Sharon Hu", "Xunzhao Yin", "Can Li"], "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e$MoS_2$\u95ea\u5b58\u6a21\u62df\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668\uff08CAM\uff09\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6811\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u4fe1\u8d56\u6027\u7684\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002\u73b0\u6709\u7684\u6811\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6613\u53d7\u8bbe\u5907\u53d8\u5316\u548c\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528$MoS_2$\u95ea\u5b58\u6a21\u62dfCAM\uff0c\u5229\u7528\u5176\u56fa\u6709\u7684\u8f6f\u8fb9\u754c\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8f6f\u6811\u6a21\u578b\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8bbe\u5907\u53d8\u5316\u548c\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728WDBC\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5728MNIST\u6570\u636e\u96c6\u4e0a\u4ec5\u635f\u59310.6%\u7684\u51c6\u786e\u7387\uff08\u4f20\u7edf\u65b9\u6cd5\u635f\u593145.3%\uff09\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u63d0\u5347AI\u7684\u53ef\u4fe1\u8d56\u6027\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u4e13\u7528\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11841", "pdf": "https://arxiv.org/pdf/2507.11841", "abs": "https://arxiv.org/abs/2507.11841", "authors": ["Xingyu Lan", "Yutong Yang", "Yifan Wang"], "title": "\"Mapping What I Feel\": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Affective visualization design is an emerging research direction focused on\ncommunicating and influencing emotion through visualization. However, as\nrevealed by previous research, this area is highly interdisciplinary and\ninvolves theories and practices from diverse fields and disciplines, thus\nawaiting analysis from more fine-grained angles. To address this need, this\nwork focuses on a pioneering and relatively mature sub-area, affective\ngeovisualization design, to further the research in this direction and provide\nmore domain-specific insights. Through an analysis of a curated corpus of\naffective geovisualization designs using the Person-Process-Place (PPP) model\nfrom geographic theory, we derived a design taxonomy that characterizes a\nvariety of methods for eliciting and enhancing emotions through geographic\nvisualization. We also identified four underlying high-level design paradigms\nof affective geovisualization design (e.g., computational, anthropomorphic)\nthat guide distinct approaches to linking geographic information with human\nexperience. By extending existing affective visualization design frameworks\nwith geographic specificity, we provide additional design examples,\ndomain-specific analyses, and insights to guide future research and practices\nin this underexplored yet highly innovative domain.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u901a\u8fc7PPP\u6a21\u578b\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5206\u7c7b\u6cd5\u548c\u56db\u4e2a\u9ad8\u5c42\u8bbe\u8ba1\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002", "motivation": "\u60c5\u611f\u53ef\u89c6\u5316\u8bbe\u8ba1\u6d89\u53ca\u591a\u5b66\u79d1\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u6790\u3002\u672c\u6587\u805a\u7126\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\uff0c\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u5e76\u63d0\u4f9b\u9886\u57df\u7279\u5b9a\u89c1\u89e3\u3002", "method": "\u5229\u7528\u5730\u7406\u7406\u8bba\u4e2d\u7684PPP\u6a21\u578b\u5206\u6790\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\u6848\u4f8b\uff0c\u5f52\u7eb3\u51fa\u8bbe\u8ba1\u5206\u7c7b\u6cd5\u548c\u56db\u4e2a\u9ad8\u5c42\u8bbe\u8ba1\u8303\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u5206\u7c7b\u6cd5\u548c\u56db\u4e2a\u8bbe\u8ba1\u8303\u5f0f\uff08\u5982\u8ba1\u7b97\u578b\u3001\u62df\u4eba\u578b\uff09\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u6846\u67b6\u5e76\u63d0\u4f9b\u4e86\u5177\u4f53\u8bbe\u8ba1\u793a\u4f8b\u548c\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u60c5\u611f\u5730\u7406\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u8fd9\u4e00\u521b\u65b0\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11830", "pdf": "https://arxiv.org/pdf/2507.11830", "abs": "https://arxiv.org/abs/2507.11830", "authors": ["Samyam Rajbhandari", "Mert Hidayetoglu", "Aurick Qiao", "Ye Wang", "Juncheng Yang", "Jeff Rasley", "Michael Wyatt", "Yuxiong He"], "title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Inference is now the dominant AI workload, yet existing systems force\ntrade-offs between latency, throughput, and cost. Arctic Inference, an\nopen-source vLLM plugin from Snowflake AI Research, introduces Shift\nParallelism, a dynamic parallelism strategy that adapts to real-world traffic\nwhile integrating speculative decoding, SwiftKV compute reduction, and\noptimized embedding inference. It achieves up to 3.4 times faster request\ncompletion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for\nembeddings, outperforming both latency- and throughput-optimized deployments.\nAlready powering Snowflake Cortex AI, Arctic Inference delivers\nstate-of-the-art, cost-effective inference for enterprise AI and is now\navailable to the community.", "AI": {"tldr": "Arctic Inference\u662f\u4e00\u4e2a\u5f00\u6e90vLLM\u63d2\u4ef6\uff0c\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u7b56\u7565Shift Parallelism\uff0c\u4f18\u5316\u63a8\u7406\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684AI\u63a8\u7406\u7cfb\u7edf\u5728\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6210\u672c\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u91c7\u7528Shift Parallelism\u52a8\u6001\u5e76\u884c\u7b56\u7565\uff0c\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u3001SwiftKV\u8ba1\u7b97\u51cf\u5c11\u548c\u4f18\u5316\u7684\u5d4c\u5165\u63a8\u7406\u3002", "result": "\u5b9e\u73b0\u4e86\u8bf7\u6c42\u5b8c\u6210\u901f\u5ea6\u63d0\u53473.4\u500d\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u53471.75\u500d\uff0cGPU\u5d4c\u5165\u63a8\u7406\u901f\u5ea6\u8fbe1.6M tokens/\u79d2\u3002", "conclusion": "Arctic Inference\u4e3a\u4f01\u4e1a\u548c\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6210\u672c\u4f18\u5316\u7684AI\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12308", "pdf": "https://arxiv.org/pdf/2507.12308", "abs": "https://arxiv.org/abs/2507.12308", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08\u5982VHDL\uff09\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cChain-of-Descriptions (CoDes)\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u901a\u7528\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u603b\u7ed3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86CoDes\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u63cf\u8ff0\u6b65\u9aa4\u5e76\u7ed3\u5408\u539f\u59cb\u8f93\u5165\u63d0\u793a\uff0c\u63d0\u5347LLMs\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoDes\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "CoDes\u4e3a\u672a\u6765\u63d0\u5347LLMs\u5728VHDL\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2507.12284", "pdf": "https://arxiv.org/pdf/2507.12284", "abs": "https://arxiv.org/abs/2507.12284", "authors": ["Artem Chervyakov", "Alexander Kharitonov", "Pavel Zadorozhny", "Adamenko Pavel", "Rodion Levichev", "Dmitrii Vorobev", "Dmitrii Salikhov", "Aidar Valeev", "Alena Pestova", "Maria Dziuba", "Ilseyar Alimova", "Artem Zavgorodnev", "Aleksandr Medvedev", "Stanislav Moiseev", "Elena Bruches", "Daniil Grebenkin", "Roman Derunets", "Vikulov Vladimir", "Anton Emelyanov", "Dmitrii Babaev", "Vladimir V. Ivanov", "Valentin Malykh", "Alena Fenogenova"], "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.", "AI": {"tldr": "MERA Code\u662f\u65b0\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u6700\u65b0\u4ee3\u7801\u751f\u6210LLM\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u586b\u8865\u73b0\u6709\u4e13\u6ce8\u4e8e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u8d28\u91cf\u548c\u5b9e\u9645\u751f\u4ea7\u6027\u80fd\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51faMERA Code\u57fa\u51c6\uff0c\u5305\u542b11\u4e2a\u4efb\u52a1\u30018\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u63d0\u4f9b\u5f00\u6e90\u6e90\u7801\u3001\u8bc4\u5206\u7cfb\u7edf\u548c\u5e73\u53f0\u3002", "result": "\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u524d\u6cbfAPI\u6a21\u578b\uff0c\u63ed\u793a\u5176\u5728\u975e\u82f1\u8bed\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MERA Code\u53d1\u5e03\u4ee5\u6307\u5bfc\u7814\u7a76\u3001\u9884\u6d4b\u6a21\u578b\u53d1\u5c55\u8d8b\u52bf\u5e76\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2507.12168", "pdf": "https://arxiv.org/pdf/2507.12168", "abs": "https://arxiv.org/abs/2507.12168", "authors": ["Lu Yu", "Zhong Ren", "Youyi Zheng", "Xiang Chen", "Kun Zhou"], "title": "Shape Adaptation for 3D Hairstyle Retargeting", "categories": ["cs.GR"], "comment": null, "summary": "It is demanding to author an existing hairstyle for novel characters in games\nand VR applications. However, it is a non-trivial task for artists due to the\ncomplicated hair geometries and spatial interactions to preserve. In this\npaper, we present an automatic shape adaptation method to retarget 3D\nhairstyles. We formulate the adaptation process as a constrained optimization\nproblem, where all the shape properties and spatial relationships are converted\ninto individual objectives and constraints. To make such an optimization on\nhigh-resolution hairstyles tractable, we adopt a multi-scale strategy to\ncompute the target positions of the hair strands in a coarse-to-fine manner.\nThe global solving for the inter-strands coupling is restricted to the coarse\nlevel, and the solving for fine details is made local and parallel. In\naddition, we present a novel hairline edit tool to allow for user customization\nduring retargeting. We achieve it by solving physics-based deformations of an\nembedded membrane to redistribute the hair roots with minimal distortion. We\ndemonstrate the efficacy of our method through quantitative and qualitative\nexperiments on various hairstyles and characters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5f62\u72b6\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6e38\u620f\u548cVR\u5e94\u7528\u4e2d\u4e3a\u76ee\u6807\u89d2\u8272\u91cd\u65b0\u8c03\u65743D\u53d1\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7b56\u7565\u548c\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u81ea\u5b9a\u4e49\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u65b0\u89d2\u8272\u4e0a\u5b9e\u73b0\u590d\u6742\u7684\u53d1\u578b\u91cd\u5b9a\u5411\uff0c\u827a\u672f\u5bb6\u9762\u4e34\u9ad8\u51e0\u4f55\u590d\u6742\u6027\u548c\u7a7a\u95f4\u4ea4\u4e92\u7684\u6311\u6218\u3002", "method": "\u5c06\u53d1\u578b\u9002\u5e94\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u7b56\u7565\uff08\u4ece\u7c97\u5230\u7ec6\uff09\u8ba1\u7b97\u53d1\u4e1d\u4f4d\u7f6e\uff0c\u5e76\u7ed3\u5408\u7269\u7406\u53d8\u5f62\u5de5\u5177\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u53d1\u9645\u7ebf\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u53d1\u578b\u548c\u89d2\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u80fd\u591f\u6ee1\u8db3\u6e38\u620f\u548cVR\u5e94\u7528\u4e2d\u53d1\u578b\u91cd\u5b9a\u5411\u7684\u9700\u6c42\u3002"}}
{"id": "2507.12367", "pdf": "https://arxiv.org/pdf/2507.12367", "abs": "https://arxiv.org/abs/2507.12367", "authors": ["Diganta Misra", "Nizar Islah", "Victor May", "Brice Rauby", "Zihan Wang", "Justine Gehring", "Antonio Orvieto", "Muawiz Chaudhary", "Eilif B. Muller", "Irina Rish", "Samira Ebrahimi Kahou", "Massimo Caccia"], "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": "Version 2 of the dataset from: arXiv:2411.05830", "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.", "AI": {"tldr": "GitChameleon\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u7279\u5b9a\u5e93\u7248\u672c\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5e93\u5feb\u901f\u66f4\u65b0\u5bfc\u81f4\u4ee3\u7801\u751f\u6210\u9762\u4e34\u7248\u672c\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u9700\u8981\u6267\u884c\u6027\u8bc4\u4f30\u3002", "method": "\u6784\u5efa328\u4e2aPython\u4ee3\u7801\u8865\u5168\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u7ed1\u5b9a\u7279\u5b9a\u5e93\u7248\u672c\u5e76\u63d0\u4f9b\u5355\u5143\u6d4b\u8bd5\u3002", "result": "\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u7684\u57fa\u7ebf\u6210\u529f\u7387\u4ec5\u4e3a48-51%\uff0c\u8868\u660e\u4efb\u52a1\u590d\u6742\u6027\u9ad8\u3002", "conclusion": "GitChameleon\u4e3a\u52a8\u6001\u5e93\u73af\u5883\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5e2e\u52a9\u5f00\u53d1\u66f4\u53ef\u9760\u7684AI\u65b9\u6cd5\u3002"}}
{"id": "2507.12445", "pdf": "https://arxiv.org/pdf/2507.12445", "abs": "https://arxiv.org/abs/2507.12445", "authors": ["Soheil Mahdizadeh", "Amir Mahdi Rasouli", "Mohammad Pourashory", "Sadra Galavani", "Mohsen Ansari"], "title": "CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments", "categories": ["cs.NI", "cs.AR", "cs.DC"], "comment": null, "summary": "Reducing latency in the Internet of Things (IoT) is a critical concern. While\ncloud computing facilitates communication, it falls short of meeting real-time\nrequirements reliably. Edge and fog computing have emerged as viable solutions\nby positioning computing nodes closer to end users, offering lower latency and\nincreased processing power. An edge-fog framework comprises various components,\nincluding edge and fog nodes, whose strategic placement is crucial as it\ndirectly impacts latency and system cost. This paper presents an effective and\ntunable node placement strategy based on a genetic algorithm to address the\noptimization problem of deploying edge and fog nodes. The main objective is to\nminimize latency and cost through optimal node placement. Simulation results\ndemonstrate that the proposed framework achieves up to 2.77% latency and 31.15%\ncost reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u8fb9\u7f18\u548c\u96fe\u8282\u70b9\u4f18\u5316\u90e8\u7f72\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4e91\u8ba1\u7b97\u5728\u7269\u8054\u7f51\u4e2d\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u7684\u95ee\u9898\uff0c\u5229\u7528\u8fb9\u7f18\u548c\u96fe\u8ba1\u7b97\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u8fb9\u7f18\u548c\u96fe\u8282\u70b9\u7684\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u5ef6\u8fdf\u964d\u4f4e2.77%\uff0c\u6210\u672c\u51cf\u5c1131.15%\u3002", "conclusion": "\u9057\u4f20\u7b97\u6cd5\u5728\u4f18\u5316\u8282\u70b9\u90e8\u7f72\u4e0a\u6709\u6548\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002"}}
{"id": "2507.11848", "pdf": "https://arxiv.org/pdf/2507.11848", "abs": "https://arxiv.org/abs/2507.11848", "authors": ["Changjian Chen", "Pengcheng Wang", "Fei Lyu", "Zhuo Tang", "Li Yang", "Long Wang", "Yong Cai", "Feng Yu", "Kenli Li"], "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection", "categories": ["cs.HC", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Hybrid rice breeding crossbreeds different rice lines and cultivates the\nresulting hybrids in fields to select those with desirable agronomic traits,\nsuch as higher yields. Recently, genomic selection has emerged as an efficient\nway for hybrid rice breeding. It predicts the traits of hybrids based on their\ngenes, which helps exclude many undesired hybrids, largely reducing the\nworkload of field cultivation. However, due to the limited accuracy of genomic\nprediction models, breeders still need to combine their experience with the\nmodels to identify regulatory genes that control traits and select hybrids,\nwhich remains a time-consuming process. To ease this process, in this paper, we\nproposed a visual analysis method to facilitate interactive hybrid rice\nbreeding. Regulatory gene identification and hybrid selection naturally\nensemble a dual-analysis task. Therefore, we developed a parametric dual\nprojection method with theoretical guarantees to facilitate interactive dual\nanalysis. Based on this dual projection method, we further developed a gene\nvisualization and a hybrid visualization to verify the identified regulatory\ngenes and hybrids. The effectiveness of our method is demonstrated through the\nquantitative evaluation of the parametric dual projection method, identified\nregulatory genes and desired hybrids in the case study, and positive feedback\nfrom breeders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u6742\u4ea4\u6c34\u7a3b\u80b2\u79cd\u7684\u89c6\u89c9\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u53cc\u6295\u5f71\u65b9\u6cd5\u63d0\u9ad8\u80b2\u79cd\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u57fa\u56e0\u7ec4\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u6709\u9650\uff0c\u80b2\u79cd\u4ecd\u9700\u7ed3\u5408\u7ecf\u9a8c\uff0c\u8fc7\u7a0b\u8017\u65f6\uff1b\u9700\u7b80\u5316\u6742\u4ea4\u6c34\u7a3b\u80b2\u79cd\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u53c2\u6570\u5316\u53cc\u6295\u5f71\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u53cc\u5206\u6790\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u56e0\u548c\u6742\u4ea4\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u65b9\u6cd5\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u80b2\u79cd\u8005\u7684\u79ef\u6781\u53cd\u9988\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6742\u4ea4\u6c34\u7a3b\u80b2\u79cd\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u80b2\u79cd\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.11899", "pdf": "https://arxiv.org/pdf/2507.11899", "abs": "https://arxiv.org/abs/2507.11899", "authors": ["Saeid Aghasoleymani Najafabadi"], "title": "Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst", "categories": ["cs.DC"], "comment": null, "summary": "Load balancing plays a pivotal role in cloud computing, ensuring that\nresources are optimally allocated to maintain high service quality and\noperational efficiency. As workloads in cloud environments become increasingly\ndynamic and unpredictable, load balancing strategies are evolving from\ntraditional static methods to more adaptive and intelligent approaches. In this\nstudy, the Cloud Analyst simulation tool was used to evaluate the performance\nof different load balancing algorithms under various scenarios, including both\ncentralized and distributed resource setups. The results highlight that while\nthe Round Robin algorithm yields slightly better processing times within a\nsingle data center, Equally Spread and Throttled techniques perform\ncompetitively, especially when network latency is considered. More importantly,\nwhen resources are distributed across multiple data centers, response times are\nsignificantly reduced, emphasizing the value of proximity and efficient load\ndistribution. In these distributed environments, Equally Spread and Throttled\nalgorithms not only maintain quick response times but also contribute to lower\noperational costs. These findings demonstrate the necessity of strategic\nresource placement and proactive infrastructure planning to balance performance\nand cost. Adopting intelligent, dynamic load balancing and resource management\npractices can help organizations meet evolving cloud demands, optimize costs,\nand maintain a competitive advantage. Continuous evaluation and integration of\nemerging technologies are crucial for sustaining effective and scalable cloud\noperations.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7Cloud Analyst\u5de5\u5177\u8bc4\u4f30\u4e86\u4e0d\u540c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u5728\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u4e91\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5206\u5e03\u5f0f\u8d44\u6e90\u5e03\u5c40\u80fd\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\uff0c\u667a\u80fd\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u5bf9\u4f18\u5316\u6027\u80fd\u548c\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u4e91\u73af\u5883\u4e2d\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a8\u6001\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u4fc3\u4f7f\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u4ece\u9759\u6001\u65b9\u6cd5\u8f6c\u5411\u66f4\u81ea\u9002\u5e94\u548c\u667a\u80fd\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u6301\u670d\u52a1\u8d28\u91cf\u548c\u8fd0\u8425\u6548\u7387\u3002", "method": "\u4f7f\u7528Cloud Analyst\u6a21\u62df\u5de5\u5177\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\uff08\u5982Round Robin\u3001Equally Spread\u548cThrottled\uff09\u5728\u4e0d\u540c\u8d44\u6e90\u8bbe\u7f6e\uff08\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\uff09\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5355\u4e00\u6570\u636e\u4e2d\u5fc3\uff0cRound Robin\u7b97\u6cd5\u5904\u7406\u65f6\u95f4\u7565\u4f18\uff1b\u800c\u5728\u5206\u5e03\u5f0f\u6570\u636e\u4e2d\u5fc3\u4e2d\uff0cEqually Spread\u548cThrottled\u7b97\u6cd5\u5728\u54cd\u5e94\u65f6\u95f4\u548c\u8fd0\u8425\u6210\u672c\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u548c\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\u662f\u4f18\u5316\u4e91\u6027\u80fd\u548c\u6210\u672c\u7684\u5173\u952e\uff0c\u9700\u6301\u7eed\u8bc4\u4f30\u548c\u6574\u5408\u65b0\u5174\u6280\u672f\u4ee5\u652f\u6301\u53ef\u6269\u5c55\u7684\u4e91\u8fd0\u8425\u3002"}}
{"id": "2507.11649", "pdf": "https://arxiv.org/pdf/2507.11649", "abs": "https://arxiv.org/abs/2507.11649", "authors": ["Daniel Commey", "Benjamin Appiah", "Griffith S. Klogo", "Garth V. Crosby"], "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "categories": ["cs.LG", "cs.DC", "cs.NI"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u8054\u90a6\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u534f\u8bae\uff0c\u901a\u8fc7\u751f\u6210\u4f4e\u4e8e\u9608\u503c\u7684\u635f\u5931\u8bc1\u660e\u6765\u907f\u514d\u654f\u611f\u4fe1\u606f\u6cc4\u9732\uff0c\u9002\u7528\u4e8eCNN\u548cMLP\u6a21\u578b\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8bc4\u4f30\u9636\u6bb5\u53ef\u80fd\u901a\u8fc7\u5171\u4eab\u6027\u80fd\u6307\u6807\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u9a8c\u8bc1\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKPs\uff09\uff0c\u5ba2\u6237\u7aef\u751f\u6210\u672c\u5730\u635f\u5931\u4f4e\u4e8e\u9608\u503c\u7684\u8bc1\u660e\uff0c\u907f\u514d\u4e86\u539f\u59cb\u635f\u5931\u503c\u7684\u76f4\u63a5\u5171\u4eab\uff0c\u5e76\u901a\u8fc7\u81ea\u5305\u542b\u6a21\u5757\u5b9e\u73b0\u4e86\u8054\u90a6\u5b66\u4e60\u4eff\u771f\u548cZKP\u7535\u8def\u8bbe\u8ba1\u3002", "result": "\u5728MNIST\u548cHAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc4\u4f30\u4e86\u8ba1\u7b97\u5f00\u9500\u3001\u901a\u4fe1\u6210\u672c\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u8bae\u901a\u8fc7ZKPs\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u9a8c\u8bc1\u7684\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2507.11929", "pdf": "https://arxiv.org/pdf/2507.11929", "abs": "https://arxiv.org/abs/2507.11929", "authors": ["Minchen Yu", "Yinghao Ren", "Jiamu Zhao", "Jiaqi Li"], "title": "Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics", "categories": ["cs.DC"], "comment": null, "summary": "Serverless computing has attracted a broad range of applications due to its\nease of use and resource elasticity. However, developing serverless\napplications often poses a dilemma -- relying on general-purpose serverless\nplatforms can fall short of delivering satisfactory performance for complex\nworkloads, whereas building application-specific serverless systems undermines\nthe simplicity and generality. In this paper, we propose an extensible design\nprinciple for serverless computing. We argue that a platform should enable\ndevelopers to extend system behaviors for domain-specialized optimizations\nwhile retaining a shared, easy-to-use serverless environment. We take data\nanalytics as a representative serverless use case and realize this design\nprinciple in Proteus. Proteus introduces a novel abstraction of decision\nworkflows, allowing developers to customize control-plane behaviors for\nimproved application performance. Preliminary results show that Proteus's\nprototype effectively optimizes analytical query execution and supports\nfine-grained resource sharing across diverse applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7Proteus\u5e73\u53f0\u5b9e\u73b0\uff0c\u652f\u6301\u5f00\u53d1\u8005\u6839\u636e\u9700\u6c42\u5b9a\u5236\u63a7\u5236\u884c\u4e3a\uff0c\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u6027\u80fd\u4e0d\u8db3\u4e0e\u5b9a\u5236\u5316\u7cfb\u7edf\u590d\u6742\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u8bbe\u8ba1\u53ef\u6269\u5c55\u539f\u5219\uff0c\u901a\u8fc7\u51b3\u7b56\u5de5\u4f5c\u6d41\u62bd\u8c61\u5b9e\u73b0\u63a7\u5236\u884c\u4e3a\u7684\u5b9a\u5236\u5316\u3002", "result": "Proteus\u539f\u578b\u4f18\u5316\u4e86\u5206\u6790\u67e5\u8be2\u6267\u884c\uff0c\u652f\u6301\u8de8\u5e94\u7528\u7ec6\u7c92\u5ea6\u8d44\u6e90\u5171\u4eab\u3002", "conclusion": "\u53ef\u6269\u5c55\u8bbe\u8ba1\u539f\u5219\u80fd\u5e73\u8861\u6027\u80fd\u4e0e\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2507.12415", "pdf": "https://arxiv.org/pdf/2507.12415", "abs": "https://arxiv.org/abs/2507.12415", "authors": ["Xinyi He", "Qian Liu", "Mingzhe Du", "Lin Yan", "Zhijie Fan", "Yiming Huang", "Zejian Yuan", "Zejun Ma"], "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?", "categories": ["cs.SE"], "comment": null, "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u771f\u5b9e\u4ed3\u5e93\u73af\u5883\u4e0b\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5SWE-Perf\uff0c\u5305\u542b140\u4e2a\u5b9e\u4f8b\uff0c\u63ed\u793a\u4e86\u73b0\u6709LLMs\u4e0e\u4e13\u5bb6\u7ea7\u6027\u80fd\u4f18\u5316\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u4ee3\u7801\u751f\u6210\u548c\u4fee\u590d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e9f\u9700\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSWE-Perf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e140\u4e2aGitHub\u6027\u80fd\u4f18\u5316\u62c9\u53d6\u8bf7\u6c42\u7684\u5b9e\u4f8b\uff0c\u5305\u542b\u4ee3\u7801\u5e93\u3001\u76ee\u6807\u51fd\u6570\u3001\u6027\u80fd\u6d4b\u8bd5\u3001\u4e13\u5bb6\u8865\u4e01\u548c\u53ef\u6267\u884c\u73af\u5883\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709LLMs\uff08\u5982Agentless\u548cOpenHands\uff09\u4e0e\u4e13\u5bb6\u7ea7\u4f18\u5316\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "SWE-Perf\u586b\u8865\u4e86LLMs\u5728\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2507.11911", "pdf": "https://arxiv.org/pdf/2507.11911", "abs": "https://arxiv.org/abs/2507.11911", "authors": ["Xiaoqing Chen", "Siyang Li", "Dongrui Wu"], "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding", "categories": ["cs.HC", "cs.IR", "cs.LG"], "comment": null, "summary": "Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u6821\u51c6\u7684AFPM\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u5bf9\u9f50\u548c\u5e27-\u7247\u7f16\u7801\u89e3\u51b3\u4e86\u8de8\u6570\u636e\u96c6EEG\u89e3\u7801\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u8111\u673a\u63a5\u53e3\u4e2dEEG\u4fe1\u53f7\u56e0\u901a\u9053\u5e03\u5c40\u4e0d\u4e00\u81f4\u3001\u4fe1\u53f7\u5206\u5e03\u975e\u7a33\u6001\u548c\u751f\u7406\u5148\u9a8c\u6574\u5408\u4e0d\u8db3\u5bfc\u81f4\u7684\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u56f0\u96be\u3002", "method": "AFPM\u6846\u67b6\u5305\u542b\u7a7a\u95f4\u5bf9\u9f50\uff08\u9009\u62e9\u76f8\u5173\u901a\u9053\u5e76\u7edf\u4e00\u5e03\u5c40\uff09\u548c\u5e27-\u7247\u7f16\u7801\uff08\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3a\u7edf\u4e00\u65f6\u7a7a\u7247\uff09\u3002", "result": "AFPM\u5728\u8fd0\u52a8\u60f3\u8c61\u548c\u4e8b\u4ef6\u76f8\u5173\u7535\u4f4d\u4efb\u52a1\u4e0a\u5206\u522b\u5b9e\u73b0\u4e864.40%\u548c3.58%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AFPM\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u6821\u51c6\u7684\u8de8\u6570\u636e\u96c6EEG\u89e3\u7801\uff0c\u63d0\u5347\u4e86\u8111\u673a\u63a5\u53e3\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u6027\u3002"}}
{"id": "2507.11978", "pdf": "https://arxiv.org/pdf/2507.11978", "abs": "https://arxiv.org/abs/2507.11978", "authors": ["Jiacheng Huang", "Zimin Li", "Yinghui Li", "Haojie Wang"], "title": "NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning", "categories": ["cs.DC"], "comment": null, "summary": "The emergence of deep learning domain-specific languages (DSLs) has\nsubstantially reduced the obstacles in developing high-performance,\ncross-platform compute kernels. However, current DSLs, such as Triton, still\ndemand that developers possess expertise in parallel programming and expose\nthem to many low-level details. This requirement complicates the development\nprocess and adds to the difficulty of maintaining compute kernels.\nConsequently, developing a new programming model that supports serial\nprogramming for deep learning workloads is crucial.\n  This paper introduces NineToothed, a domain-specific language that offers\nserial semantics for machine learning programming. Through the automatic\ntransformation of serial code into parallel code, NineToothed significantly\nstreamlines the development process while causing minimal performance\ndegradation. NineToothed encompasses (1) a language with tensor-oriented\nmetaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the\nexpression of tiled computations without the need to manage low-level details\nand (2) a code generator for generating high-performance parallel code. Our\nevaluation results indicate that NineToothed can greatly simplify compute\nkernel development while maintaining performance comparable to that of Triton.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aNineToothed\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u4e32\u884c\u8bed\u4e49\u7b80\u5316\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08\u5982Triton\uff09\u9700\u8981\u5f00\u53d1\u8005\u5177\u5907\u5e76\u884c\u7f16\u7a0b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u66b4\u9732\u8fc7\u591a\u5e95\u5c42\u7ec6\u8282\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u548c\u7ef4\u62a4\u7684\u96be\u5ea6\u3002", "method": "NineToothed\u63d0\u4f9b\u4e86\u4e00\u79cd\u652f\u6301\u4e32\u884c\u7f16\u7a0b\u7684\u8bed\u8a00\uff0c\u81ea\u52a8\u5c06\u4e32\u884c\u4ee3\u7801\u8f6c\u6362\u4e3a\u5e76\u884c\u4ee3\u7801\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u4ee3\u7801\u751f\u6210\u5668\u4ee5\u751f\u6210\u9ad8\u6027\u80fd\u5e76\u884c\u4ee3\u7801\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cNineToothed\u80fd\u663e\u8457\u7b80\u5316\u8ba1\u7b97\u5185\u6838\u5f00\u53d1\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eTriton\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "NineToothed\u901a\u8fc7\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\u5e76\u51cf\u5c11\u6027\u80fd\u635f\u5931\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6613\u7528\u7684\u7f16\u7a0b\u6a21\u578b\u3002"}}
{"id": "2507.11948", "pdf": "https://arxiv.org/pdf/2507.11948", "abs": "https://arxiv.org/abs/2507.11948", "authors": ["Carlo Baronio", "Pietro Marsella", "Ben Pan", "Simon Guo", "Silas Alberti"], "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "comment": null, "summary": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5Kevin\uff0c\u7528\u4e8eCUDA\u5185\u6838\u7684\u751f\u6210\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u7684\u6b63\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "GPU\u5185\u6838\u7f16\u5199\u5bf9AI\u7cfb\u7edf\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8fed\u4ee3\u6027\u5f3a\u4e14\u5177\u6709\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\uff08\u5982\u6b63\u786e\u6027\u548c\u52a0\u901f\uff09\uff0c\u9002\u5408\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u8f6eRL\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u957f\u8f68\u8ff9\u5b66\u4e60\u548c\u591a\u8f6e\u5956\u52b1\u5206\u914d\u7b49\u6311\u6218\u3002", "result": "Kevin\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5176\u751f\u6210\u7684\u5185\u6838\u6b63\u786e\u6027\u4ece56%\u63d0\u5347\u81f382%\uff0c\u5e73\u5747\u52a0\u901f\u4ece0.53x\u63d0\u5347\u81f31.10x\uff0c\u8d85\u8d8a\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e32\u884c\u7ec6\u5316\u6bd4\u5e76\u884c\u91c7\u6837\u66f4\u6709\u6548\uff0c\u4e14\u968f\u7740\u7ec6\u5316\u8f6e\u6b21\u589e\u52a0\uff0cKevin\u7684\u6539\u8fdb\u901f\u5ea6\u66f4\u9ad8\u3002"}}
{"id": "2507.11960", "pdf": "https://arxiv.org/pdf/2507.11960", "abs": "https://arxiv.org/abs/2507.11960", "authors": ["Hyein Hong", "Sangbong Yoo", "SeokHwan Choi", "Jisue Kim", "Seongbum Seo", "Haneol Cho", "Chansoo Kim", "Yun Jang"], "title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Approaches to enhancing data quality (DQ) are classified into two main\ncategories: data- and process-driven. However, prior research has predominantly\nutilized batch data preprocessing within the data-driven framework, which often\nproves insufficient for optimizing machine learning (ML) model performance and\nfrequently leads to distortions in data characteristics. Existing studies have\nprimarily focused on data preprocessing rather than genuine data quality\nimprovement (DQI). In this paper, we introduce d-DQIVAR, a novel visual\nanalytics system designed to facilitate DQI strategies aimed at improving ML\nmodel performance. Our system integrates visual analytics techniques that\nleverage both data-driven and process-driven approaches. Data-driven techniques\ntackle DQ issues such as imputation, outlier detection, deletion, format\nstandardization, removal of duplicate records, and feature selection.\nProcess-driven strategies encompass evaluating DQ and DQI procedures by\nconsidering DQ dimensions and ML model performance and applying the\nKolmogorov-Smirnov test. We illustrate how our system empowers users to harness\nexpert and domain knowledge effectively within a practical workflow through\ncase studies, evaluations, and user studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ad-DQIVAR\u7684\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u548c\u6d41\u7a0b\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u4ece\u800c\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u9884\u5904\u7406\u800c\u975e\u771f\u6b63\u7684\u6570\u636e\u8d28\u91cf\u6539\u8fdb\uff08DQI\uff09\uff0c\u4e14\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u65f6\u6548\u679c\u6709\u9650\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e86\u6570\u636e\u9a71\u52a8\uff08\u5982\u63d2\u8865\u3001\u5f02\u5e38\u68c0\u6d4b\u7b49\uff09\u548c\u6d41\u7a0b\u9a71\u52a8\uff08\u5982\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u548c\u6a21\u578b\u6027\u80fd\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u5206\u6790\u6280\u672f\u8fdb\u884c\u6574\u5408\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7528\u6237\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u5982\u4f55\u6709\u6548\u5229\u7528\u4e13\u5bb6\u548c\u9886\u57df\u77e5\u8bc6\u5728\u5b9e\u9645\u5de5\u4f5c\u6d41\u4e2d\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "d-DQIVAR\u7cfb\u7edf\u662f\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u4e00\u79cd\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7ed3\u5408\u591a\u9886\u57df\u77e5\u8bc6\u7684\u573a\u666f\u3002"}}
{"id": "2507.12032", "pdf": "https://arxiv.org/pdf/2507.12032", "abs": "https://arxiv.org/abs/2507.12032", "authors": ["Brian-Frederik Jahnke", "Ren\u00e9 Brinkhege", "Jan Peter Meyer", "Daniel Tebernum", "Falk Howar"], "title": "ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum", "categories": ["cs.DC"], "comment": null, "summary": "Achieving sustainable, explainable, and maintainable automation for resource\noptimization is a core challenge across the edge-cloud continuum. Persistent\noverprovisioning and operational complexity often stem from heterogeneous\nplatforms and layered abstractions, while systems lacking explainability and\nmaintainability become fragile, impede safe recovery, and accumulate technical\ndebt. Existing solutions are frequently reactive, limited to single abstraction\nlayers, or require intrusive platform changes, leaving efficiency and\nmaintainability gains unrealized.\n  This paper addresses safe, transparent, and low-effort resource optimization\nin dynamic, multi-tenant edge-cloud systems, without disrupting operator\nworkflows or increasing technical debt. We introduce ARRC, a recommender system\nrooted in software engineering design principles, which delivers explainable,\ncross-layer resource recommendations directly into operator workflows (such as\ntickets and GitOps pull requests). ARRC encapsulates optimization logic in\nspecialized, auditable agents coordinated via a shared interface, supporting\nmaintainability and extensibility through transparency and the ability to\ninspect both recommendations and their rationale.\n  Empirical evaluation in a multi-region industrial deployment shows that ARRC\nreduces operator workload by over 50%, improves compute utilization by up to\n7.7x, and maintains error rates below 5%, with most benefits achieved through\nincremental, operator-approved changes. This demonstrates that explainable,\nrecommendation-based architectures can achieve sustainable efficiency and\nmaintainability improvements at production scale.\n  ARRC provides an empirically evaluated framework for integrating explainable,\nworkflow-driven automation into resource management, intended to advance best\npractices for robust, maintainable, and transparent edge-cloud continuum\nplatforms.", "AI": {"tldr": "ARRC\u662f\u4e00\u79cd\u57fa\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u8d44\u6e90\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u8de8\u5c42\u5efa\u8bae\uff0c\u663e\u8457\u964d\u4f4e\u64cd\u4f5c\u5458\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u8d44\u6e90\u4f18\u5316\u7684\u53ef\u6301\u7eed\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u6311\u6218\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165ARRC\u63a8\u8350\u7cfb\u7edf\uff0c\u5c01\u88c5\u4f18\u5316\u903b\u8f91\u4e8e\u53ef\u5ba1\u8ba1\u7684\u4ee3\u7406\u4e2d\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u63a5\u53e3\u534f\u8c03\uff0c\u76f4\u63a5\u96c6\u6210\u5230\u64cd\u4f5c\u5458\u5de5\u4f5c\u6d41\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cARRC\u51cf\u5c1150%\u4ee5\u4e0a\u64cd\u4f5c\u5458\u5de5\u4f5c\u91cf\uff0c\u63d0\u5347\u8ba1\u7b97\u5229\u7528\u73877.7\u500d\uff0c\u9519\u8bef\u7387\u4f4e\u4e8e5%\u3002", "conclusion": "ARRC\u901a\u8fc7\u900f\u660e\u63a8\u8350\u67b6\u6784\u5b9e\u73b0\u53ef\u6301\u7eed\u6548\u7387\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u63a8\u52a8\u8fb9\u7f18-\u4e91\u5e73\u53f0\u7684\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2507.12003", "pdf": "https://arxiv.org/pdf/2507.12003", "abs": "https://arxiv.org/abs/2507.12003", "authors": ["Cara Ellen Appel"], "title": "Expanding ML-Documentation Standards For Better Security", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering Workshop (REW 2025)", "summary": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86ML\u5b89\u5168\u548c\u6587\u6863\u8bb0\u5f55\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u5b9e\u8df5\u4e2d\u5b89\u5168\u610f\u8bc6\u548c\u6807\u51c6\u5316\u4e0d\u8db3\uff0c\u63d0\u51fa\u6539\u8fdb\u6587\u6863\u6807\u51c6\u4ee5\u5305\u62ec\u5b89\u5168\u90e8\u5206\u7684\u5efa\u8bae\u3002", "motivation": "\u76ee\u524dML\u5b9e\u8df5\u4e2d\u5bf9\u5b89\u5168\u6027\u7684\u610f\u8bc6\u8f83\u4f4e\uff0c\u6587\u6863\u8bb0\u5f55\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0d\u9ad8\u3002\u9700\u8981\u901a\u8fc7\u6539\u8fdb\u6587\u6863\u6807\u51c6\u6765\u5f25\u8865ML\u5b89\u5168\u7684\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u5bf9\u73b0\u6709\u6587\u732e\u7684\u5e7f\u6cdb\u56de\u987e\uff0c\u63d0\u51fa\u5728\u73b0\u6709ML\u6587\u6863\u6807\u51c6\uff08\u5982Model Cards\u548cDatasheets\uff09\u4e2d\u6269\u5c55\u5b89\u5168\u76f8\u5173\u5185\u5bb9\u3002", "result": "\u73b0\u6709\u6807\u51c6\u5728\u5b9e\u8df5\u4e2d\u7684\u91c7\u7528\u7387\u4f4e\uff0cIT\u5b89\u5168\u5e38\u88ab\u5ffd\u7565\uff0c\u6539\u8fdb\u7684\u6587\u6863\u6807\u51c6\u6709\u52a9\u4e8e\u63d0\u5347ML\u5b89\u5168\u6027\u3002", "conclusion": "\u5efa\u8bae\u5728ML\u6587\u6863\u4e2d\u589e\u52a0\u4e13\u95e8\u7684\u5b89\u5168\u90e8\u5206\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u7684\u5b89\u5168\u548c\u6587\u6863\u8d28\u91cf\u95ee\u9898\u3002"}}
{"id": "2507.11984", "pdf": "https://arxiv.org/pdf/2507.11984", "abs": "https://arxiv.org/abs/2507.11984", "authors": ["Hyeon Jeon", "Jeongin Park", "Soohyun Lee", "Dae Hyun Kim", "Sungbok Shin", "Jinwook Seo"], "title": "Dataset-Adaptive Dimensionality Reduction", "categories": ["cs.HC", "cs.LG"], "comment": "IEEE VIS 2025 & IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "summary": "Selecting the appropriate dimensionality reduction (DR) technique and\ndetermining its optimal hyperparameter settings that maximize the accuracy of\nthe output projections typically involves extensive trial and error, often\nresulting in unnecessary computational overhead. To address this challenge, we\npropose a dataset-adaptive approach to DR optimization guided by structural\ncomplexity metrics. These metrics quantify the intrinsic complexity of a\ndataset, predicting whether higher-dimensional spaces are necessary to\nrepresent it accurately. Since complex datasets are often inaccurately\nrepresented in two-dimensional projections, leveraging these metrics enables us\nto predict the maximum achievable accuracy of DR techniques for a given\ndataset, eliminating redundant trials in optimizing DR. We introduce the design\nand theoretical foundations of these structural complexity metrics. We\nquantitatively verify that our metrics effectively approximate the ground truth\ncomplexity of datasets and confirm their suitability for guiding\ndataset-adaptive DR workflow. Finally, we empirically show that our\ndataset-adaptive workflow significantly enhances the efficiency of DR\noptimization without compromising accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u590d\u6742\u6027\u6307\u6807\u7684\u964d\u7ef4\uff08DR\uff09\u4f18\u5316\u65b9\u6cd5\uff0c\u51cf\u5c11\u8bd5\u9519\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u964d\u7ef4\u6280\u672f\u7684\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bd5\u9519\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7ed3\u6784\u590d\u6742\u6027\u6307\u6807\u91cf\u5316\u6570\u636e\u96c6\u56fa\u6709\u590d\u6742\u6027\uff0c\u9884\u6d4b\u964d\u7ef4\u6280\u672f\u7684\u6700\u5927\u53ef\u80fd\u7cbe\u5ea6\uff0c\u51cf\u5c11\u5197\u4f59\u4f18\u5316\u3002", "result": "\u9a8c\u8bc1\u4e86\u6307\u6807\u80fd\u51c6\u786e\u8fd1\u4f3c\u6570\u636e\u96c6\u7684\u771f\u5b9e\u590d\u6742\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u964d\u7ef4\u4f18\u5316\u6548\u7387\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u81ea\u9002\u5e94\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u590d\u6742\u6027\u6307\u6807\uff0c\u6709\u6548\u6307\u5bfc\u964d\u7ef4\u4f18\u5316\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2507.12038", "pdf": "https://arxiv.org/pdf/2507.12038", "abs": "https://arxiv.org/abs/2507.12038", "authors": ["Alkida Balliu", "Thomas Boudier", "Francesco d'Amore", "Dennis Olivetti", "Gustav Schmid", "Jukka Suomela"], "title": "Distributed Algorithms for Potential Problems", "categories": ["cs.DC"], "comment": "28 pages, 4 figures", "summary": "In this work we present a fast distributed algorithm for local potential\nproblems: these are graph problems where the task is to find a locally optimal\nsolution where no node can unilaterally improve the utility in its local\nneighborhood by changing its own label. A simple example of such a problem is\nthe task of finding a locally optimal cut, i.e., a cut where for each node at\nleast half of its incident edges are cut edges. The distributed round\ncomplexity of locally optimal cut has been wide open; the problem is known to\nrequire $\\Omega(\\log n)$ rounds in the deterministic LOCAL model and\n$\\Omega(\\log \\log n)$ rounds in the randomized LOCAL model, but the only known\nupper bound is the trivial brute-force solution of $O(n)$ rounds. Locally\noptimal cut in bounded-degree graphs is perhaps the simplest example of a\nlocally checkable labeling problem for which there is still such a large gap\nbetween current upper and lower bounds. We show that in bounded-degree graphs,\nall local potential problems, including locally optimal cut, can be solved in\n$\\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.\nIn particular, the deterministic round complexity of the locally optimal cut\nproblem is now settled to $\\log^{\\Theta(1)} n$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5feb\u901f\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5c40\u90e8\u52bf\u95ee\u9898\uff08\u5982\u5c40\u90e8\u6700\u4f18\u5272\uff09\uff0c\u5728\u6709\u9650\u5ea6\u56fe\u4e2d\u53ef\u5728\u5bf9\u6570\u591a\u9879\u5f0f\u8f6e\u6b21\u5185\u5b8c\u6210\u3002", "motivation": "\u5c40\u90e8\u52bf\u95ee\u9898\u7684\u5206\u5e03\u5f0f\u8f6e\u590d\u6742\u5ea6\u957f\u671f\u5b58\u5728\u4e0a\u4e0b\u754c\u5dee\u8ddd\u8f83\u5927\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5c40\u90e8\u6700\u4f18\u5272\u95ee\u9898\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u6709\u9650\u5ea6\u56fe\uff0c\u80fd\u5728\u786e\u5b9a\u6027\u6216\u968f\u673a\u6027LOCAL\u6a21\u578b\u4e2d\u5b9e\u73b0\u5bf9\u6570\u591a\u9879\u5f0f\u8f6e\u6b21\u590d\u6742\u5ea6\u3002", "result": "\u8bc1\u660e\u6240\u6709\u5c40\u90e8\u52bf\u95ee\u9898\uff08\u5305\u62ec\u5c40\u90e8\u6700\u4f18\u5272\uff09\u5728\u6709\u9650\u5ea6\u56fe\u4e2d\u5747\u53ef\u5728\u5bf9\u6570\u591a\u9879\u5f0f\u8f6e\u6b21\u5185\u89e3\u51b3\uff0c\u586b\u8865\u4e86\u7406\u8bba\u590d\u6742\u5ea6\u5dee\u8ddd\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5c40\u90e8\u6700\u4f18\u5272\u95ee\u9898\u7684\u786e\u5b9a\u8f6e\u590d\u6742\u5ea6\u4e3a\u5bf9\u6570\u591a\u9879\u5f0f\u7ea7\u522b\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u5b58\u5728\u7684\u7406\u8bba\u95ee\u9898\u3002"}}
{"id": "2507.11999", "pdf": "https://arxiv.org/pdf/2507.11999", "abs": "https://arxiv.org/abs/2507.11999", "authors": ["Xiaolin Wen", "Qishuang Fu", "Shuangyue Han", "Yichen Guo", "Joseph K. Liu", "Yong Wang"], "title": "Envisage: Towards Expressive Visual Graph Querying", "categories": ["cs.HC"], "comment": null, "summary": "Graph querying is the process of retrieving information from graph data using\nspecialized languages (e.g., Cypher), often requiring programming expertise.\nVisual Graph Querying (VGQ) streamlines this process by enabling users to\nconstruct and execute queries via an interactive interface without resorting to\ncomplex coding. However, current VGQ tools only allow users to construct simple\nand specific query graphs, limiting users' ability to interactively express\ntheir query intent, especially for underspecified query intent. To address\nthese limitations, we propose Envisage, an interactive visual graph querying\nsystem to enhance the expressiveness of VGQ in complex query scenarios by\nsupporting intuitive graph structure construction and flexible parameterized\nrule specification. Specifically, Envisage comprises four stages: Query\nExpression allows users to interactively construct graph queries through\nintuitive operations; Query Verification enables the validation of constructed\nqueries via rule verification and query instantiation; Progressive Query\nExecution can progressively execute queries to ensure meaningful querying\nresults; and Result Analysis facilitates result exploration and interpretation.\nTo evaluate Envisage, we conducted two case studies and in-depth user\ninterviews with 14 graph analysts. The results demonstrate its effectiveness\nand usability in constructing, verifying, and executing complex graph queries.", "AI": {"tldr": "Envisage\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u89c6\u89c9\u56fe\u67e5\u8be2\u7cfb\u7edf\uff0c\u901a\u8fc7\u652f\u6301\u76f4\u89c2\u7684\u56fe\u7ed3\u6784\u6784\u5efa\u548c\u7075\u6d3b\u7684\u53c2\u6570\u5316\u89c4\u5219\u89c4\u8303\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u56fe\u67e5\u8be2\u5de5\u5177\u4ec5\u652f\u6301\u6784\u5efa\u7b80\u5355\u548c\u7279\u5b9a\u7684\u67e5\u8be2\u56fe\uff0c\u9650\u5236\u4e86\u7528\u6237\u4ea4\u4e92\u5f0f\u8868\u8fbe\u67e5\u8be2\u610f\u56fe\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u672a\u660e\u786e\u6307\u5b9a\u7684\u67e5\u8be2\u610f\u56fe\u3002", "method": "Envisage\u5305\u62ec\u56db\u4e2a\u9636\u6bb5\uff1a\u67e5\u8be2\u8868\u8fbe\u3001\u67e5\u8be2\u9a8c\u8bc1\u3001\u6e10\u8fdb\u5f0f\u67e5\u8be2\u6267\u884c\u548c\u7ed3\u679c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c14\u540d\u56fe\u5206\u6790\u5e08\u7684\u6df1\u5165\u7528\u6237\u8bbf\u8c08\uff0c\u8bc1\u660e\u4e86Envisage\u5728\u6784\u5efa\u3001\u9a8c\u8bc1\u548c\u6267\u884c\u590d\u6742\u56fe\u67e5\u8be2\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "Envisage\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u56fe\u67e5\u8be2\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.12106", "pdf": "https://arxiv.org/pdf/2507.12106", "abs": "https://arxiv.org/abs/2507.12106", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "categories": ["cs.DC", "cs.CY"], "comment": "18 pages, 6 Figures", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life.", "AI": {"tldr": "\u901a\u8fc7\u7269\u8054\u7f51\u548c\u6570\u636e\u5206\u6790\u6280\u672f\uff0c\u667a\u80fd\u7eff\u8272\u57ce\u5e02\u9879\u76ee\u4f18\u5316\u4e86\u57ce\u5e02\u7eff\u5730\u7ba1\u7406\uff0c\u63d0\u5347\u4e86\u5c45\u6c11\u751f\u6d3b\u8d28\u91cf\u3002", "motivation": "\u57ce\u5e02\u7eff\u5730\u5bf9\u5c45\u6c11\u5065\u5eb7\u548c\u798f\u7949\u81f3\u5173\u91cd\u8981\uff0c\u667a\u80fd\u7ba1\u7406\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u670d\u52a1\u8d28\u91cf\u3002", "method": "\u96c6\u6210\u7269\u8054\u7f51\u7cfb\u7edf\u3001\u6570\u636e\u9a71\u52a8\u5e73\u53f0\u548c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u5b9e\u65f6\u76d1\u6d4b\u6811\u6728\u548c\u7eff\u5730\u5065\u5eb7\u72b6\u51b5\u3002", "result": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u4f18\u5316\u704c\u6e89\uff0c\u63d0\u4f9b\u5b9a\u5236\u8b66\u62a5\uff0c\u63d0\u5347\u7ba1\u7406\u6548\u7387\u3002", "conclusion": "\u6570\u5b57\u5316\u548c\u521b\u65b0\u6280\u672f\u53ef\u652f\u6301\u53ef\u6301\u7eed\u57ce\u5e02\u6cbb\u7406\uff0c\u589e\u5f3a\u73af\u5883\u97e7\u6027\u3002"}}
{"id": "2507.12204", "pdf": "https://arxiv.org/pdf/2507.12204", "abs": "https://arxiv.org/abs/2507.12204", "authors": ["Pengyu Zhu", "Janghee Cho"], "title": "Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection", "categories": ["cs.HC"], "comment": null, "summary": "Adolescents' mobile technology use is often regulated through rigid control\nmechanisms that fail to account for their autonomy and natural usage patterns.\nDrawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this\nposition paper proposes Tao-Technology, a self-organizing, adaptive regulatory\nframework. Integrating insights from Reflective Informatics and Information\nEcologies, we explore how mobile technology can dynamically adjust to context\nwhile fostering self-reflection and meaning-making. This approach shifts from\nexternal restrictions to dynamic co-adaptative regulation, ensuring technology\ngovernance remains flexible yet structured, supporting adolescents in\ncultivating a balanced and intentional relationship with digital technology.", "AI": {"tldr": "\u57fa\u4e8e\u9053\u5bb6\u54f2\u5b66\uff08\u65e0\u4e3a\u3001\u9634\u9633\u3001\u81ea\u7136\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u7ec4\u7ec7\u7684\u9002\u5e94\u6027\u6280\u672f\u76d1\u7ba1\u6846\u67b6\u2014\u2014Tao-Technology\uff0c\u65e8\u5728\u52a8\u6001\u8c03\u6574\u79fb\u52a8\u6280\u672f\u4f7f\u7528\uff0c\u57f9\u517b\u9752\u5c11\u5e74\u4e0e\u6570\u5b57\u6280\u672f\u7684\u5e73\u8861\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u76d1\u7ba1\u673a\u5236\u8fc7\u4e8e\u50f5\u5316\uff0c\u5ffd\u89c6\u4e86\u9752\u5c11\u5e74\u7684\u81ea\u4e3b\u6027\u548c\u81ea\u7136\u4f7f\u7528\u6a21\u5f0f\uff0c\u9700\u5f15\u5165\u66f4\u7075\u6d3b\u3001\u52a8\u6001\u7684\u76d1\u7ba1\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u53cd\u601d\u6027\u4fe1\u606f\u5b66\u4e0e\u4fe1\u606f\u751f\u6001\u5b66\uff0c\u63d0\u51faTao-Technology\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5171\u9002\u5e94\u8c03\u8282\u53d6\u4ee3\u5916\u90e8\u9650\u5236\u3002", "result": "\u6280\u672f\u76d1\u7ba1\u66f4\u5177\u7075\u6d3b\u6027\u548c\u7ed3\u6784\u6027\uff0c\u652f\u6301\u9752\u5c11\u5e74\u4e0e\u6570\u5b57\u6280\u672f\u5efa\u7acb\u5e73\u8861\u3001\u6709\u610f\u4e49\u7684\u5173\u7cfb\u3002", "conclusion": "Tao-Technology\u6846\u67b6\u4e3a\u9752\u5c11\u5e74\u6280\u672f\u4f7f\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u3001\u81ea\u9002\u5e94\u7684\u76d1\u7ba1\u6a21\u5f0f\uff0c\u5951\u5408\u9053\u5bb6\u54f2\u5b66\u7684\u81ea\u7136\u548c\u8c10\u7406\u5ff5\u3002"}}
{"id": "2507.12205", "pdf": "https://arxiv.org/pdf/2507.12205", "abs": "https://arxiv.org/abs/2507.12205", "authors": ["Junqing Lin", "Jingwei Sun", "Mingge Lu", "Guangzhong Sun"], "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage", "categories": ["cs.DC"], "comment": "11 pages", "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR.", "AI": {"tldr": "EC-SpMV\u662f\u4e00\u79cd\u9488\u5bf9\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4f18\u5316\u7684GPU\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5757\u63d0\u53d6\u7b97\u6cd5\u548c\u65b0\u578b\u538b\u7f29\u7a00\u758f\u683c\u5f0f\uff08EC-CSR\uff09\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u3002", "motivation": "\u7a00\u758f\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff08SpMV\uff09\u5df2\u6210\u4e3a\u672c\u5730\u90e8\u7f72\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7a00\u758fLLM\u7684\u7ed3\u6784\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u5757\u63d0\u53d6\u7b97\u6cd5\u548c\u591a\u7c92\u5ea6\u5757\u7ed3\u6784\u6355\u6349\uff0c\u4ee5\u53ca\u57fa\u4e8eDelta\u7d22\u5f15\u7684\u538b\u7f29\u7a00\u758f\u683c\u5f0f\uff08EC-CSR\uff09\u3002", "result": "\u5728LLaMA\u548cOPT\u6a21\u578b\u7684\u7a00\u758f\u6743\u91cd\u77e9\u9635\u4e0a\uff0cEC-SpMV\u6bd4\u73b0\u6709SpMV\u5e93\u5feb6.44\u500d\uff0c\u5b58\u50a8\u5f00\u9500\u6bd4CSR\u51cf\u5c1155.4%\u3002", "conclusion": "EC-SpMV\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758fLLM\u63a8\u7406\u4e2d\u7684SpMV\u6027\u80fd\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u5b58\u50a8\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2507.12212", "pdf": "https://arxiv.org/pdf/2507.12212", "abs": "https://arxiv.org/abs/2507.12212", "authors": ["Garyoung Kim", "Huisung Kwon", "Seoju Yun", "Yu-Won Youn"], "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness", "categories": ["cs.HC", "cs.AI"], "comment": "7 pages, 3 figures", "summary": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u56db\u79cd\u751f\u6210\u5f0fAI\u6a21\u578b\u5982\u4f55\u7406\u89e3\u548c\u8868\u8fbe\u201c\u4e11\u964b\u201d\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u5d4c\u5165\u7684\u6587\u5316\u504f\u89c1\u3002\u7814\u7a76\u53d1\u73b0\uff0cAI\u6a21\u578b\u503e\u5411\u4e8e\u5c06\u4e11\u964b\u4e0e\u8001\u5e74\u767d\u4eba\u7537\u6027\u5f62\u8c61\u8054\u7cfb\u8d77\u6765\uff0c\u53cd\u6620\u4e86\u6df1\u5c42\u7684\u793e\u4f1a\u504f\u89c1\u548c\u77db\u76fe\u6027\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6279\u5224\u6027\u5730\u5206\u6790\u751f\u6210\u5f0fAI\u5982\u4f55\u7406\u89e3\u548c\u8868\u8fbe\u201c\u4e11\u964b\u201d\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u8868\u8fbe\u4e2d\u5d4c\u5165\u7684\u6587\u5316\u504f\u89c1\uff0c\u4ee5\u63a8\u52a8\u66f4\u516c\u6b63\u7684AI\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d613\u4e2a\u4e0e\u201c\u4e11\u964b\u201d\u76f8\u5173\u7684\u5f62\u5bb9\u8bcd\uff0c\u5e76\u5728\u56db\u79cdAI\u6a21\u578b\u548c\u4e09\u79cd\u63d0\u793a\u4e0b\u751f\u6210624\u5f20\u56fe\u50cf\u3002\u5bf9\u56fe\u50cf\u4e2d\u7684\u4eba\u53e3\u548c\u793e\u4f1a\u7ecf\u6d4e\u5c5e\u6027\u8fdb\u884c\u72ec\u7acb\u7f16\u7801\u548c\u4e3b\u9898\u5206\u6790\u3002", "result": "AI\u6a21\u578b\u5c06\u201c\u4e11\u964b\u201d\u4e0e\u8001\u5e74\u767d\u4eba\u7537\u6027\u5f62\u8c61\u8054\u7cfb\u8d77\u6765\uff0c\u663e\u793a\u51fa\u77db\u76fe\u6027\u504f\u89c1\u3002\u5c3d\u7ba1\u8bd5\u56fe\u907f\u514d\u5bf9\u8fb9\u7f18\u7fa4\u4f53\u7684\u523b\u677f\u63cf\u7ed8\uff0c\u4f46\u8d1f\u9762\u5c5e\u6027\u4ecd\u88ab\u8fc7\u5ea6\u6295\u5c04\u5230\u4e3b\u6d41\u7fa4\u4f53\u3002\u7269\u7406\u6807\u8bb0\uff08\u5982\u4e0d\u5bf9\u79f0\u548c\u8870\u8001\uff09\u4ecd\u662f\u6838\u5fc3\u89c6\u89c9\u4e3b\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u52aa\u529b\u5b9e\u73b0\u66f4\u5e73\u7b49\u7684\u8868\u5f81\uff0c\u751f\u6210\u5f0fAI\u4ecd\u5ef6\u7eed\u56fa\u6709\u548c\u77db\u76fe\u6027\u504f\u89c1\uff0c\u7a81\u663e\u4e86\u5f00\u53d1\u751f\u6001\u8bad\u7ec3\u8303\u5f0f\u548c\u5305\u5bb9\u6027AI\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.12296", "pdf": "https://arxiv.org/pdf/2507.12296", "abs": "https://arxiv.org/abs/2507.12296", "authors": ["Bevan Koopman", "Guido Zuccon"], "title": "Humans are more gullible than LLMs in believing common psychological myths", "categories": ["cs.HC"], "comment": null, "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u76f8\u4fe1\u5fc3\u7406\u8bef\u533a\uff0c\u5e76\u63d0\u51fa\u51cf\u8f7b\u8fd9\u79cd\u503e\u5411\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0LLMs\u7684\u8bef\u533a\u4fe1\u5ff5\u7387\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3LLMs\u662f\u5426\u6a21\u4eff\u4eba\u7c7b\u7684\u5fc3\u7406\u8bef\u533a\u4fe1\u5ff5\uff0c\u5e76\u63a2\u7d22\u51cf\u5c11\u8fd9\u79cd\u4fe1\u5ff5\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fc3\u8fdbLLM\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u752850\u4e2a\u5e38\u89c1\u7684\u5fc3\u7406\u8bef\u533a\uff0c\u5728\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u8bc4\u4f30LLMs\u7684\u8bef\u533a\u4fe1\u5ff5\uff0c\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5f15\u5bfc\u6027\u63d0\u793a\u3002", "result": "LLMs\u7684\u8bef\u533a\u4fe1\u5ff5\u7387\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u4f46\u7528\u6237\u63d0\u793a\u53ef\u80fd\u5f71\u54cd\u7ed3\u679c\uff1b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6709\u6548\u51cf\u5c11\u8bef\u533a\u4fe1\u5ff5\u5e76\u63ed\u793aLLMs\u6f5c\u5728\u7684\u7ea0\u504f\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u5bf9\u673a\u5668\u5fc3\u7406\u5b66\u9886\u57df\u6709\u8d21\u732e\uff0c\u5f3a\u8c03\u4e86\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\u5728LLM\u7cfb\u7edf\u8bc4\u4f30\u548c\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11916", "pdf": "https://arxiv.org/pdf/2507.11916", "abs": "https://arxiv.org/abs/2507.11916", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "categories": ["cs.AI", "cs.DC"], "comment": null, "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u7b97\u6cd5\uff0c\u540c\u65f6\u786e\u4fdd\u6700\u4f18\u6027\u3002", "motivation": "GPU\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u641c\u7d22\u7b97\u6cd5\u63d0\u4f9b\u4e86\u5e76\u884c\u5904\u7406\u7684\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u5f88\u5c11\u6709\u7b97\u6cd5\u4e13\u95e8\u4e3aGPU\u8bbe\u8ba1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u53d7\u9650\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08CB-DFS\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408CPU\u548cGPU\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\uff0c\u6269\u5c55\u4e86ID A*\u548cBTS\u7b97\u6cd5\u3002", "result": "\u57283x3\u9b54\u65b9\u548c4x4\u6ed1\u5757\u62fc\u56fe\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPU\u8ba1\u7b97\u53ef\u4ee5\u9ad8\u6548\u5730\u6279\u91cf\u5904\u7406DFS\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408CPU\u548cGPU\u7684\u5e76\u884c\u80fd\u529b\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u5728DFS\u4e2d\u9ad8\u6548\u6279\u5904\u7406GPU\u64cd\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5206\u6790\u4e86\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2507.12298", "pdf": "https://arxiv.org/pdf/2507.12298", "abs": "https://arxiv.org/abs/2507.12298", "authors": ["Rui Sheng", "Xingbo Wang", "Jiachen Wang", "Xiaofu Jin", "Zhonghua Sheng", "Zhenxing Xu", "Suraj Rajendran", "Huamin Qu", "Fei Wang"], "title": "TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials", "categories": ["cs.HC"], "comment": null, "summary": "Eligibility criteria play a critical role in clinical trials by determining\nthe target patient population, which significantly influences the outcomes of\nmedical interventions. However, current approaches for designing eligibility\ncriteria have limitations to support interactive exploration of the large space\nof eligibility criteria. They also ignore incorporating detailed\ncharacteristics from the original electronic health record (EHR) data for\ncriteria refinement. To address these limitations, we proposed TrialCompass, a\nvisual analytics system integrating a novel workflow, which can empower\nclinicians to iteratively explore the vast space of eligibility criteria\nthrough knowledge-driven and outcome-driven approaches. TrialCompass supports\nhistory-tracking to help clinicians trace the evolution of their adjustments\nand decisions when exploring various forms of data (i.e., eligibility criteria,\noutcome metrics, and detailed characteristics of original EHR data) through\nthese two approaches. This feature can help clinicians comprehend the impact of\neligibility criteria on outcome metrics and patient characteristics, which\nfacilitates systematic refinement of eligibility criteria. Using a real-world\ndataset, we demonstrated the effectiveness of TrialCompass in providing\ninsights into designing eligibility criteria for septic shock and\nsepsis-associated acute kidney injury. We also discussed the research prospects\nof applying visual analytics to clinical trials.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TrialCompass\uff0c\u4e00\u4e2a\u652f\u6301\u4ea4\u4e92\u5f0f\u63a2\u7d22\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u6807\u51c6\u7684\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u7ed3\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u7ed3\u679c\u9a71\u52a8\u65b9\u6cd5\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u7cfb\u7edf\u4f18\u5316\u6807\u51c6\u3002", "motivation": "\u5f53\u524d\u8bbe\u8ba1\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u6807\u51c6\u7684\u65b9\u6cd5\u5728\u652f\u6301\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u533b\u7597\u5e72\u9884\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86TrialCompass\u7cfb\u7edf\uff0c\u7ed3\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u7ed3\u679c\u9a71\u52a8\u65b9\u6cd5\uff0c\u652f\u6301\u4e34\u5e8a\u533b\u751f\u901a\u8fc7\u53ef\u89c6\u5316\u5de5\u5177\u8fed\u4ee3\u63a2\u7d22\u548c\u4f18\u5316\u8d44\u683c\u6807\u51c6\uff0c\u5e76\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u8be6\u7ec6\u6570\u636e\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u8bc1\u660e\u4e86TrialCompass\u5728\u8bbe\u8ba1\u548c\u4f18\u5316\u8113\u6bd2\u75c7\u4f11\u514b\u548c\u8113\u6bd2\u75c7\u76f8\u5173\u6025\u6027\u80be\u635f\u4f24\u7684\u8d44\u683c\u6807\u51c6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TrialCompass\u4e3a\u4e34\u5e8a\u8bd5\u9a8c\u7684\u8d44\u683c\u6807\u51c6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u53ef\u89c6\u5316\u5206\u6790\u5728\u4e34\u5e8a\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11941", "pdf": "https://arxiv.org/pdf/2507.11941", "abs": "https://arxiv.org/abs/2507.11941", "authors": ["Amos You"], "title": "BlockBPE: Parallel BPE Tokenization", "categories": ["cs.CL", "cs.DC"], "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE\u662f\u4e00\u79cd\u5e76\u884cGPU\u5b9e\u73b0\u7684\u5b57\u8282\u5bf9\u7f16\u7801\uff08BPE\uff09\uff0c\u9488\u5bf9\u9ad8\u541e\u5410\u91cf\u6279\u91cf\u63a8\u7406\u4f18\u5316\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb2-2.5\u500d\u3002", "motivation": "\u73b0\u6709CPU\u7ed1\u5b9a\u7684BPE\u5b9e\u73b0\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981GPU\u4f18\u5316\u3002", "method": "\u53bb\u9664Regex\u9884\u5206\u8bcd\uff0c\u91c7\u7528\u5e76\u884c\u5316\u7684\u7ebf\u7a0b\u5757\u5185\u5408\u5e76\uff0c\u590d\u6742\u5ea6\u964d\u81f3\u8fd1\u4f3c\u7ebf\u6027\u3002", "result": "\u5728\u9ad8\u6279\u91cf\u63a8\u7406\u4e2d\uff0cBlockBPE\u7684\u541e\u5410\u91cf\u662ftiktoken\u76842\u500d\uff0cHuggingFace Tokenizers\u76842.5\u500d\u3002", "conclusion": "BlockBPE\u5728\u727a\u7272\u5c11\u91cf\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86GPU\u4e0a\u7684\u5206\u8bcd\u6548\u7387\u3002"}}
{"id": "2507.12334", "pdf": "https://arxiv.org/pdf/2507.12334", "abs": "https://arxiv.org/abs/2507.12334", "authors": ["Chase Stokes", "Anjana Arunkumar", "Marti A. Hearst", "Lace Padilla"], "title": "An Analysis of Text Functions in Information Visualization", "categories": ["cs.HC", "H.5.0"], "comment": "11 pages, 3 figures, IEEE VIS Conference", "summary": "Text is an integral but understudied component of visualization design.\nAlthough recent studies have examined how text elements (e.g., titles and\nannotations) influence comprehension, preferences, and predictions, many\nquestions remain about textual design and use in practice. This paper\nintroduces a framework for understanding text functions in information\nvisualizations, building on and filling gaps in prior classifications and\ntaxonomies. Through an analysis of 120 real-world visualizations and 804 text\nelements, we identified ten distinct text functions, ranging from identifying\ndata mappings to presenting valenced subtext. We further identify patterns in\ntext usage and conduct a factor analysis, revealing four overarching\ntext-informed design strategies: Attribution and Variables, Annotation-Centric\nDesign, Visual Embellishments, and Narrative Framing. In addition to these\nfactors, we explore features of title rhetoric and text multifunctionality,\nwhile also uncovering previously unexamined text functions, such as text\nreplacing visual elements. Our findings highlight the flexibility of text,\ndemonstrating how different text elements in a given design can combine to\ncommunicate, synthesize, and frame visual information. This framework adds\nimportant nuance and detail to existing frameworks that analyze the diverse\nroles of text in visualization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u89e3\u4fe1\u606f\u53ef\u89c6\u5316\u4e2d\u6587\u672c\u529f\u80fd\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u5148\u524d\u5206\u7c7b\u7684\u7a7a\u767d\u3002\u901a\u8fc7\u5206\u6790120\u4e2a\u5b9e\u9645\u53ef\u89c6\u5316\u6848\u4f8b\u548c804\u4e2a\u6587\u672c\u5143\u7d20\uff0c\u8bc6\u522b\u4e8610\u79cd\u6587\u672c\u529f\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u56db\u79cd\u6587\u672c\u8bbe\u8ba1\u7b56\u7565\u3002\u7814\u7a76\u53d1\u73b0\u5f3a\u8c03\u4e86\u6587\u672c\u5728\u53ef\u89c6\u5316\u4e2d\u7684\u591a\u6837\u5316\u4f5c\u7528\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u8ba8\u4e86\u6587\u672c\u5143\u7d20\uff08\u5982\u6807\u9898\u548c\u6ce8\u91ca\uff09\u5bf9\u7406\u89e3\u3001\u504f\u597d\u548c\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u4f46\u5173\u4e8e\u6587\u672c\u8bbe\u8ba1\u548c\u5b9e\u9645\u4f7f\u7528\u7684\u8bb8\u591a\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6587\u672c\u529f\u80fd\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790120\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u53ef\u89c6\u5316\u548c804\u4e2a\u6587\u672c\u5143\u7d20\uff0c\u8bc6\u522b\u4e8610\u79cd\u6587\u672c\u529f\u80fd\uff0c\u5e76\u8fdb\u884c\u56e0\u5b50\u5206\u6790\uff0c\u63ed\u793a\u4e86\u56db\u79cd\u4e3b\u8981\u7684\u6587\u672c\u8bbe\u8ba1\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e8610\u79cd\u4e0d\u540c\u7684\u6587\u672c\u529f\u80fd\uff0c\u4ee5\u53ca\u56db\u79cd\u6587\u672c\u8bbe\u8ba1\u7b56\u7565\uff1a\u5f52\u56e0\u4e0e\u53d8\u91cf\u3001\u6ce8\u91ca\u4e3a\u6838\u5fc3\u7684\u8bbe\u8ba1\u3001\u89c6\u89c9\u88c5\u9970\u548c\u53d9\u8ff0\u6846\u67b6\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u6807\u9898\u4fee\u8f9e\u548c\u6587\u672c\u591a\u529f\u80fd\u6027\u7b49\u7279\u5f81\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u6709\u6587\u672c\u5728\u53ef\u89c6\u5316\u4e2d\u591a\u6837\u5316\u89d2\u8272\u7684\u5206\u6790\u589e\u6dfb\u4e86\u91cd\u8981\u7684\u7ec6\u8282\uff0c\u5c55\u793a\u4e86\u6587\u672c\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u5143\u7d20\u7684\u7ec4\u5408\u6765\u4f20\u8fbe\u3001\u7efc\u5408\u548c\u6784\u5efa\u89c6\u89c9\u4fe1\u606f\u3002"}}
{"id": "2507.12337", "pdf": "https://arxiv.org/pdf/2507.12337", "abs": "https://arxiv.org/abs/2507.12337", "authors": ["Xiao Pang", "Yan Huang", "Chang Liu", "JiYuan Liu", "MingYou Liu"], "title": "MExplore: an entity-based visual analytics approach for medical expertise acquisition", "categories": ["cs.HC"], "comment": null, "summary": "Acquiring medical expertise is a critical component of medical education and\nprofessional development. While existing studies focus primarily on\nconstructing medical knowledge bases or developing learning tools based on the\nstructured, private healthcare data, they often lack methods for extracting\nexpertise from unstructured medical texts. These texts constitute a significant\nportion of medical literature and offer greater flexibility and detail compared\nto structured data formats. Furthermore, many studies fail to provide explicit\nanalytical and learning pathways in this context.\n  This paper introduces MExplore, an interactive visual analytics system\ndesigned to support the acquisition of medical expertise. To address the\nchallenges of the inconsistencies and confidentiality concerns inherent in\nunstructured medical texts, we propose a workflow that employs a fine-tuned\nBERT-based model to extract medical entities (MEs) from them. We then present a\nnovel multilevel visual analysis framework that integrates multiple coordinated\nvisualizations, enabling a progressive and interactive exploration of medical\nknowledge.\n  To assess the effectiveness of MExplore, we conducted three case studies, a\nuser study, and interviews with domain experts. The results indicate that the\nsystem significantly enhances the medical expertise acquisition process,\nproviding an effective interactive approach for acquiring and retaining\nknowledge from medical texts.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMExplore\u7684\u4ea4\u4e92\u5f0f\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u533b\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u53ef\u89c6\u5316\u6846\u67b6\u652f\u6301\u533b\u5b66\u77e5\u8bc6\u7684\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u6559\u80b2\u5de5\u5177\u591a\u57fa\u4e8e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7f3a\u4e4f\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u6587\u672c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u8be6\u7ec6\u7684\u4fe1\u606f\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u5fae\u8c03\u7684BERT\u6a21\u578b\u63d0\u53d6\u533b\u5b66\u5b9e\u4f53\uff0c\u5e76\u7ed3\u5408\u591a\u7ea7\u53ef\u89c6\u5316\u6846\u67b6\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u3001\u7528\u6237\u7814\u7a76\u548c\u4e13\u5bb6\u8bbf\u8c08\uff0c\u8bc1\u660eMExplore\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u7684\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "MExplore\u4e3a\u4ece\u975e\u7ed3\u6784\u5316\u533b\u5b66\u6587\u672c\u4e2d\u83b7\u53d6\u77e5\u8bc6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2507.12377", "pdf": "https://arxiv.org/pdf/2507.12377", "abs": "https://arxiv.org/abs/2507.12377", "authors": ["Ke Er Amy Zhang", "Jodie Jenkinson", "Laura Garrison"], "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight", "categories": ["cs.HC"], "comment": "11 pages, 5 figures, accepted to IEEE VIS 2025 Conference", "summary": "We conduct a deconstructive reading of a qualitative interview study with 17\nvisual data journalists from newsrooms across the globe. We borrow a\ndeconstruction approach from literary critique to explore the instability of\nmeaning in language and reveal implicit beliefs in words and ideas. Through our\nanalysis we surface two sets of opposing implicit beliefs in visual data\njournalism: objectivity/subjectivity and humanism/mechanism. We contextualize\nthese beliefs through a genealogical analysis, which brings deconstruction\ntheory into practice by providing a historic backdrop for these opposing\nperspectives. Our analysis shows that these beliefs held within visual data\njournalism are not self-enclosed but rather a product of external societal\nforces and paradigm shifts over time. Through this work, we demonstrate how\nthinking with critical theories such as deconstruction and genealogy can\nreframe \"success\" in visual data storytelling and diversify visualization\nresearch outcomes. These efforts push the ways in which we as researchers\nproduce domain knowledge to examine the sociotechnical issues of today's values\ntowards datafication and data visualization.", "AI": {"tldr": "\u6458\u8981\u901a\u8fc7\u89e3\u6784\u65b9\u6cd5\u5206\u6790\u89c6\u89c9\u6570\u636e\u65b0\u95fb\u4e2d\u7684\u9690\u542b\u4fe1\u5ff5\uff0c\u63a2\u8ba8\u8bed\u8a00\u610f\u4e49\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u793e\u4f1a\u529b\u91cf\u5bf9\u65b0\u95fb\u4e1a\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u89c6\u89c9\u6570\u636e\u65b0\u95fb\u4e2d\u9690\u542b\u7684\u5bf9\u7acb\u4fe1\u5ff5\uff08\u5982\u5ba2\u89c2\u6027/\u4e3b\u89c2\u6027\uff09\uff0c\u5e76\u63a2\u8ba8\u8fd9\u4e9b\u4fe1\u5ff5\u5982\u4f55\u53d7\u5230\u793e\u4f1a\u548c\u5386\u53f2\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6587\u5b66\u6279\u8bc4\u4e2d\u7684\u89e3\u6784\u65b9\u6cd5\u548c\u8c31\u7cfb\u5b66\u5206\u6790\uff0c\u5bf917\u540d\u5168\u7403\u89c6\u89c9\u6570\u636e\u8bb0\u8005\u7684\u8bbf\u8c08\u8fdb\u884c\u7814\u7a76\uff0c\u63ed\u793a\u8bed\u8a00\u548c\u89c2\u5ff5\u7684\u9690\u542b\u610f\u4e49\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u6570\u636e\u65b0\u95fb\u4e2d\u7684\u4fe1\u5ff5\u5e76\u975e\u5b64\u7acb\uff0c\u800c\u662f\u5916\u90e8\u793e\u4f1a\u529b\u91cf\u548c\u8303\u5f0f\u8f6c\u53d8\u7684\u4ea7\u7269\uff0c\u8fd9\u4e9b\u4fe1\u5ff5\u5f71\u54cd\u4e86\u6570\u636e\u53d9\u4e8b\u7684\u201c\u6210\u529f\u201d\u6807\u51c6\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u7ed3\u5408\u89e3\u6784\u548c\u8c31\u7cfb\u5b66\u7406\u8bba\u53ef\u4ee5\u91cd\u65b0\u5b9a\u4e49\u6570\u636e\u53d9\u4e8b\u7684\u6210\u529f\uff0c\u5e76\u4e30\u5bcc\u53ef\u89c6\u5316\u7814\u7a76\u7684\u591a\u6837\u6027\uff0c\u63a8\u52a8\u5bf9\u5f53\u4eca\u6570\u636e\u5316\u95ee\u9898\u7684\u793e\u4f1a\u6280\u672f\u63a2\u8ba8\u3002"}}
{"id": "2507.11597", "pdf": "https://arxiv.org/pdf/2507.11597", "abs": "https://arxiv.org/abs/2507.11597", "authors": ["Richard Timpone", "Yongwei Yang"], "title": "AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Paper prepared for the 2025 European Survey Research Association\n  Conference; 30 pages, 5 tables and 4 figures", "summary": "AI is transforming research. It is being leveraged to construct surveys,\nsynthesize data, conduct analysis, and write summaries of the results. While\nthe promise is to create efficiencies and increase quality, the reality is not\nalways as clear cut. Leveraging our framework of Truth, Beauty, and Justice\n(TBJ) which we use to evaluate AI, machine learning and computational models\nfor effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024),\nwe consider the potential and limitation of analytic, generative, and agentic\nAI to augment data scientists or take on tasks traditionally done by human\nanalysts and researchers. While AI can be leveraged to assist analysts in their\ntasks, we raise some warnings about push-button automation. Just as earlier\neras of survey analysis created some issues when the increased ease of using\nstatistical software allowed researchers to conduct analyses they did not fully\nunderstand, the new AI tools may create similar but larger risks. We emphasize\na human-machine collaboration perspective (Daugherty and Wilson 2018)\nthroughout the data science workflow and particularly call out the vital role\nthat data scientists play under VUCA decision areas. We conclude by encouraging\nthe advance of AI tools to complement data scientists but advocate for\ncontinued training and understanding of methods to ensure the substantive value\nof research is fully achieved by applying, interpreting, and acting upon\nresults most effectively and ethically.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u7814\u7a76\u4e2d\u5e26\u6765\u7684\u6548\u7387\u548c\u8d28\u91cf\u63d0\u5347\u6f5c\u529b\uff0c\u4f46\u4e5f\u8b66\u544a\u4e86\u81ea\u52a8\u5316\u53ef\u80fd\u5e26\u6765\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u4eba\u673a\u534f\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bc4\u4f30AI\u5728\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\uff0c\u786e\u4fdd\u7814\u7a76\u7684\u771f\u5b9e\u6027\u548c\u4f26\u7406\u4ef7\u503c\u3002", "method": "\u57fa\u4e8eTruth, Beauty, Justice (TBJ)\u6846\u67b6\u8bc4\u4f30AI\u5de5\u5177\uff0c\u63d0\u51fa\u4eba\u673a\u534f\u4f5c\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "AI\u53ef\u4ee5\u8f85\u52a9\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u4f46\u81ea\u52a8\u5316\u5de5\u5177\u53ef\u80fd\u5e26\u6765\u98ce\u9669\uff0c\u9700\u52a0\u5f3a\u65b9\u6cd5\u7406\u89e3\u548c\u57f9\u8bad\u3002", "conclusion": "\u63d0\u5021AI\u4f5c\u4e3a\u8865\u5145\u5de5\u5177\uff0c\u540c\u65f6\u5f3a\u8c03\u6570\u636e\u79d1\u5b66\u5bb6\u5728VUCA\u51b3\u7b56\u4e2d\u7684\u6838\u5fc3\u89d2\u8272\u548c\u65b9\u6cd5\u57f9\u8bad\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11821", "pdf": "https://arxiv.org/pdf/2507.11821", "abs": "https://arxiv.org/abs/2507.11821", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": "Submitted to a computer science conference", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "AI": {"tldr": "MNIST-Gen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684MNIST\u98ce\u683c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u6807\u51c6\u6570\u636e\u96c6\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6807\u51c6\u6570\u636e\u96c6\uff08\u5982MNIST\uff09\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u4e0d\u9002\u7528\uff0c\u4e14\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8017\u65f6\u4e14\u53d7\u6cd5\u5f8b\u9650\u5236\u3002", "method": "\u7ed3\u5408CLIP\u8bed\u4e49\u7406\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u7c7b\u53cd\u9988\uff0c\u652f\u6301\u5206\u5c42\u8bed\u4e49\u5206\u7c7b\u548c\u591a\u6a21\u5f0f\u5904\u7406\u3002", "result": "\u751f\u6210Tree-MNIST\u548cFood-MNIST\u6570\u636e\u96c6\uff0c\u5b9e\u73b085%\u81ea\u52a8\u5206\u7c7b\u51c6\u786e\u7387\u548c80%\u65f6\u95f4\u8282\u7701\u3002", "conclusion": "MNIST-Gen\u4e3a\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u5177\u6709\u6e05\u6670\u6027\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.11892", "pdf": "https://arxiv.org/pdf/2507.11892", "abs": "https://arxiv.org/abs/2507.11892", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GRACE\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u8bed\u4e49\u6587\u672c\u7ec6\u5316\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u672a\u5145\u5206\u5229\u7528\u6587\u672c\u60c5\u611f\u7ebf\u7d22\u548c\u65e0\u6548\u8fc7\u6ee4\u65e0\u5173\u9762\u90e8\u52a8\u6001\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u4e2d\u7684\u7ec6\u5fae\u60c5\u611f\u7ebf\u7d22\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u8fc7\u6ee4\u65e0\u5173\u9762\u90e8\u52a8\u6001\u7684\u673a\u5236\u3002", "method": "GRACE\u7ed3\u5408\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u8bed\u4e49\u6587\u672c\u7ec6\u5316\uff08CATE\u6a21\u5757\uff09\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u901a\u8fc7\u8fd0\u52a8\u5dee\u5f02\u52a0\u6743\u548c\u71b5\u6b63\u5219\u5316\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u7cbe\u51c6\u5b9a\u4f4d\u60c5\u611f\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u6a21\u7cca\u6216\u4e0d\u5e73\u8861\u60c5\u611f\u7c7b\u522b\u573a\u666f\u4e0b\uff0cUAR\u548cWAR\u5747\u8fbe\u5230SOTA\u3002", "conclusion": "GRACE\u901a\u8fc7\u591a\u6a21\u6001\u7ec6\u5316\u548c\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.11906", "pdf": "https://arxiv.org/pdf/2507.11906", "abs": "https://arxiv.org/abs/2507.11906", "authors": ["Tadahiro Taniguchi", "Masatoshi Nagano", "Haruumi Omoto", "Yoshiki Hayashi"], "title": "CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models", "categories": ["cs.MA", "cs.HC"], "comment": null, "summary": "Collective human activities like using an Ouija board (or Kokkuri-san) often\nproduce emergent, coherent linguistic outputs unintended by any single\nparticipant. While psychological explanations such as the ideomotor effect\nexist, a computational understanding of how decentralized, implicit linguistic\nknowledge fuses through shared physical interaction remains elusive. We\nintroduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this\nphenomenon as collective Langevin dynamics sampling from implicitly fused\nlanguage models. Each participant is represented as an agent associated with an\nenergy landscape derived from an internal language model reflecting linguistic\npriors, and agents exert stochastic forces based on local energy gradients. We\ntheoretically prove that the collective motion of the shared pointer\n(planchette) corresponds to Langevin MCMC sampling from the sum of individual\nenergy landscapes, representing fused collective knowledge. Simulations\nvalidate that CoCre-Sam dynamics effectively fuse different models and generate\nmeaningful character sequences, while ablation studies confirm the essential\nroles of collective interaction and stochasticity. Altogether, CoCre-Sam\nprovides a novel computational mechanism linking individual implicit knowledge,\nembodied collective action, and emergent linguistic phenomena, grounding these\ncomplex interactions in the principles of probabilistic sampling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoCre-Sam\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u591a\u4eba\u901a\u8fc7\u7269\u7406\u4e92\u52a8\uff08\u5982\u4f7f\u7528Ouija\u677f\uff09\u4ea7\u751f\u7684\u81ea\u53d1\u8bed\u8a00\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u96c6\u4f53\u6d3b\u52a8\u4e2d\uff08\u5982Ouija\u677f\u4f7f\u7528\uff09\u5982\u4f55\u901a\u8fc7\u5206\u6563\u3001\u9690\u6027\u7684\u8bed\u8a00\u77e5\u8bc6\u878d\u5408\u4ea7\u751f\u6709\u610f\u4e49\u7684\u8bed\u8a00\u8f93\u51fa\u3002", "method": "\u91c7\u7528Langevin\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u5c06\u6bcf\u4e2a\u53c2\u4e0e\u8005\u89c6\u4e3a\u5177\u6709\u5185\u90e8\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u96c6\u4f53\u4e92\u52a8\u8fdb\u884c\u968f\u673a\u91c7\u6837\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u96c6\u4f53\u6307\u9488\u8fd0\u52a8\u5bf9\u5e94Langevin MCMC\u91c7\u6837\uff0c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoCre-Sam\u4e3a\u4e2a\u4f53\u9690\u6027\u77e5\u8bc6\u4e0e\u96c6\u4f53\u884c\u52a8\u7684\u5173\u8054\u63d0\u4f9b\u4e86\u65b0\u673a\u5236\uff0c\u57fa\u4e8e\u6982\u7387\u91c7\u6837\u539f\u7406\u89e3\u91ca\u590d\u6742\u4e92\u52a8\u3002"}}
{"id": "2507.12009", "pdf": "https://arxiv.org/pdf/2507.12009", "abs": "https://arxiv.org/abs/2507.12009", "authors": ["Florian David", "Michael Chan", "Elenor Morgenroth", "Patrik Vuilleumier", "Dimitri Van De Ville"], "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted in International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC) 2025", "summary": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u795e\u7ecf\u7f16\u7801-\u89e3\u7801\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7fMRI\u6570\u636e\u7f16\u7801\u548c\u89e3\u7801\u5927\u8111\u5bf9\u81ea\u7136\u523a\u6fc0\u7684\u53cd\u5e94\uff0c\u5e76\u901a\u8fc7\u663e\u8457\u6027\u56fe\u5206\u6790\u4e86\u89c6\u89c9\u89e3\u7801\u7684\u5173\u952e\u8111\u533a\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7406\u89e3\u5927\u8111\u5728\u5904\u7406\u81ea\u7136\u7535\u5f71\u523a\u6fc0\u65f6\u7684\u89c6\u89c9\u5904\u7406\u673a\u5236\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u5377\u79ef\u5c42\u7684\u7f16\u7801-\u89e3\u7801\u67b6\u6784\uff0c\u5229\u7528\u8fde\u7eed\u7535\u5f71\u5e27\u7684\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u586b\u8865\u4e86\u7535\u5f71\u523a\u6fc0\u4e0efMRI\u91c7\u96c6\u4e4b\u95f4\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u5dee\u8ddd\u3002", "result": "\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u89c6\u89c9\u76ae\u5c42\u53ca\u5176\u5468\u8fb9\u533a\u57df\u7684\u4f53\u7d20\u6d3b\u52a8\uff0c\u5e76\u80fd\u591f\u4ece\u795e\u7ecf\u6d3b\u52a8\u4e2d\u91cd\u5efa\u76f8\u5e94\u7684\u89c6\u89c9\u8f93\u5165\uff1b\u8d21\u732e\u6700\u5927\u7684\u8111\u533a\u4e3a\u6795\u53f6\u4e2d\u90e8\u3001\u68ad\u72b6\u56de\u548c\u8ddd\u72b6\u6c9f\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4f5c\u4e3a\u7406\u89e3\u89c6\u89c9\u5904\u7406\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u91cd\u5efa\u8fb9\u7f18\u3001\u9762\u90e8\u548c\u5bf9\u6bd4\u5ea6\u7b49\u57fa\u672c\u89c6\u89c9\u7279\u5f81\u65b9\u9762\u3002"}}
{"id": "2507.12108", "pdf": "https://arxiv.org/pdf/2507.12108", "abs": "https://arxiv.org/abs/2507.12108", "authors": ["Lorenzo Mannocci", "Stefano Cresci", "Matteo Magnani", "Anna Monreale", "Maurizio Tesconi"], "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u68c0\u6d4b\u5728\u7ebf\u534f\u540c\u884c\u4e3a\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u66f4\u5168\u9762\u5730\u7406\u89e3\u534f\u540c\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u591a\u6a21\u6001\u534f\u540c\u590d\u6742\u6027\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u534f\u540c\u884c\u4e3a\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5bf9\u6bd4\u4e86\u5f31\u6574\u5408\u4e0e\u5f3a\u6574\u5408\u591a\u6a21\u6001\u6a21\u578b\uff0c\u8bc4\u4f30\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u8d21\u732e\u53ca\u591a\u6a21\u6001\u5b9e\u73b0\u65b9\u5f0f\u5bf9\u68c0\u6d4b\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5e76\u975e\u6240\u6709\u6a21\u6001\u90fd\u80fd\u63d0\u4f9b\u72ec\u7279\u89c1\u89e3\uff0c\u4f46\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u66f4\u5168\u9762\u5730\u5206\u6790\u534f\u540c\u52a8\u6001\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u5347\u4e86\u5728\u7ebf\u534f\u540c\u884c\u4e3a\u7684\u68c0\u6d4b\u548c\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u7ef4\u62a4\u6570\u5b57\u5e73\u53f0\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.12356", "pdf": "https://arxiv.org/pdf/2507.12356", "abs": "https://arxiv.org/abs/2507.12356", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6027\u522b\u504f\u89c1\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bed\u97f3\u611f\u77e5\u4e2d\u663e\u8457\u5b58\u5728\uff0c\u7537\u6027\u8bed\u97f3\u66f4\u6613\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u4e14\u8fd9\u79cd\u504f\u89c1\u5728\u4e2d\u6587\u8bed\u97f3\u4e2d\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u63a2\u8ba8\u6027\u522b\u504f\u89c1\u5982\u4f55\u5f71\u54cdAD\u8bed\u97f3\u7684\u611f\u77e5\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u8bed\u8a00\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc716\u540d\u4e2d\u6587\u542c\u4f17\u8bc4\u4f30\u4e2d\u6587\u548c\u5e0c\u814a\u8bed\u8bed\u97f3\uff0c\u5e76\u8fdb\u884c\u58f0\u5b66\u5206\u6790\uff08\u5982\u95ea\u70c1\u503c\u548c\u8bed\u97f3\u90e8\u5206\uff09\u3002", "result": "\u7537\u6027\u8bed\u97f3\u66f4\u5e38\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u95ea\u70c1\u503c\u4e0eAD\u611f\u77e5\u663e\u8457\u76f8\u5173\uff0c\u800c\u8bed\u97f3\u90e8\u5206\u4e0eAD\u8bc6\u522b\u5448\u8d1f\u76f8\u5173\u3002\u8bed\u8a00\u5bf9AD\u611f\u77e5\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5f00\u53d1AD\u68c0\u6d4b\u6a21\u578b\u65f6\u9700\u8003\u8651\u8fd9\u4e00\u504f\u89c1\uff0c\u5e76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e0d\u540c\u8bed\u8a00\u80cc\u666f\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2507.12370", "pdf": "https://arxiv.org/pdf/2507.12370", "abs": "https://arxiv.org/abs/2507.12370", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cdLLM\u67b6\u6784\uff08\u5982Llama3-8B\u3001Gemma2-9B\u548cMistral-7B\uff09\u6765\u589e\u5f3a\u5bf9\u7528\u6237\u8bf7\u6c42\u4e2d\u6b67\u4e49\u7684\u68c0\u6d4b\u548c\u89e3\u51b3\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fa9\u8bba\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662fMistral-7B\u4e3b\u5bfc\u7684\u8fa9\u8bba\u8fbe\u5230\u4e8676.7%\u7684\u6210\u529f\u7387\u3002", "motivation": "LLM\u5728\u5904\u7406\u7528\u6237\u8bf7\u6c42\u65f6\u5b58\u5728\u6b67\u4e49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u68c0\u6d4b\u548c\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cdLLM\u67b6\u6784\uff08Llama3-8B\u3001Gemma2-9B\u548cMistral-7B\uff09\u548c\u4e00\u4e2a\u5305\u542b\u591a\u6837\u6b67\u4e49\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u8fa9\u8bba\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86Llama3-8B\u548cMistral-7B\u7684\u6027\u80fd\uff0cMistral-7B\u4e3b\u5bfc\u7684\u8fa9\u8bba\u6210\u529f\u7387\u8fbe\u523076.7%\uff0c\u5c24\u5176\u5728\u590d\u6742\u6b67\u4e49\u548c\u9ad8\u6548\u5171\u8bc6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\u662f\u4e00\u79cd\u6709\u6548\u7684LLM\u80fd\u529b\u589e\u5f3a\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u548c\u81ea\u9002\u5e94\u6027\u7684\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
