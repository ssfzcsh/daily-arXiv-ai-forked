<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 28]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 10]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 47]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 7]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.AR](#cs.AR) [Total: 7]
- [math.MG](#math.MG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CL](#cs.CL) [Total: 7]
- [math.DS](#math.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.IR](#cs.IR) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.SI](#cs.SI) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models](https://arxiv.org/abs/2508.01255)
*Cuong Chi Le,Cuong Duc Van,Tung Duy Vu,Thai Minh Pham Vu,Hoang Nhat Phan,Huy Nhat Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: TestWeaver是一种基于LLM的新方法，通过轻量级程序分析指导测试生成，解决了覆盖平台问题，提高了回归测试的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在回归测试生成中由于程序执行推理有限，导致覆盖增长停滞，TestWeaver旨在解决这一问题。

Method: TestWeaver结合了轻量级程序分析，提供目标行的反向切片、共享控制流相似性的测试用例以及执行内联注释，以优化LLM的测试生成。

Result: 实验表明，TestWeaver加速了代码覆盖增长，生成的回归测试用例比现有LLM方法更有效。

Conclusion: TestWeaver通过改进LLM的输入上下文和推理能力，显著提升了回归测试的效率和效果。

Abstract: Regression testing ensures that code changes do not unintentionally break
existing functionality. While recent advances in large language models (LLMs)
have shown promise in automating test generation for regression testing, they
often suffer from limited reasoning about program execution, resulting in
stagnated coverage growth - a phenomenon known as the coverage plateau. In this
paper, we present TestWeaver, a novel LLM-based approach that integrates
lightweight program analysis to guide test generation more effectively.
TestWeaver introduces three key innovations: (1) it reduces hallucinations and
improves focus by supplying the LLM with the backward slice from the target
line instead of full program context; (2) it identifies and incorporates close
test cases - those that share control-flow similarities with the path to the
target line - to provide execution context within the LLM's context window; and
(3) it enhances LLM's reasoning with execution in-line annotations that encode
variable states as comments along executed paths. By equipping LLMs with these
targeted and contextualized inputs, TestWeaver improves coverage-guided test
generation and mitigates redundant explorations. Empirical results demonstrate
that TestWeaver accelerates code coverage growth and generates more effective
regression test cases than existing LLM-based approaches.

</details>


### [2] [Screencast-Based Analysis of User-Perceived GUI Responsiveness](https://arxiv.org/abs/2508.01337)
*Wei Liu,Linqiang Guo,Yi Wen Heng,Chenglin Li,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该论文提出了一种名为\tool的轻量级黑盒技术，通过分析移动设备屏幕录像直接测量GUI响应性，帮助开发者识别影响用户体验的性能问题。


<details>
  <summary>Details</summary>
Motivation: 移动应用中GUI响应性对用户体验至关重要，但现有方法难以准确捕捉用户感知的延迟问题，且在大规模测试中效果不佳。

Method: 使用计算机视觉技术检测用户交互并分析帧级视觉变化，计算两个关键指标：响应时间和完成时间。

Result: 在64个流行Android应用的2,458次交互测试中，\tool的交互检测精度为0.96，召回率为0.93，响应时间和完成时间的测量误差分别控制在50ms和100ms以内，89%以上的交互达到此精度。

Conclusion: 该工具已在工业测试管道中部署，显著提升了性能调试效率，并发现了传统工具遗漏的响应性问题。

Abstract: GUI responsiveness is critical for a positive user experience in mobile
applications. Even brief delays in visual feedback can frustrate users and lead
to negative reviews. However, detecting and quantifying such user-perceived
delays remains challenging, especially in industrial testing pipelines that
evaluate thousands of apps daily across diverse devices and OS versions.
Existing techniques based on static analysis or system metrics, while useful,
may not accurately capture user-perceived issues or scale effectively.
  In this experience paper, we present \tool, a lightweight and black-box
technique that measures GUI responsiveness directly from mobile screencasts --
video recordings captured during automated GUI testing. \tool detects user
interactions and visual delays, helping developers identify GUI performance
issues that affect the user experience. It uses computer vision to detect user
interactions and analyzes frame-level visual changes to compute two key
metrics: response time (from user action to first visual feedback) and finish
time (until visual feedback stabilizes). We evaluate \tool on a manually
annotated benchmark of 2,458 interactions from 64 popular Android apps. \tool
achieves 0.96 precision and 0.93 recall in detecting interactions, and measures
response and finish times within 50\,ms and 100\,ms error, respectively, for
over 89\% of interactions. The tool has been deployed in an industrial testing
pipeline and analyzes thousands of screencasts daily, uncovering responsiveness
issues missed by traditional tools and improving performance debugging
efficiency.

</details>


### [3] [HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection](https://arxiv.org/abs/2508.01357)
*Yunhao Liang,Ruixuan Ying,Takuya Taniguchi,Guwen Lyu,Zhe Cui*

Main category: cs.SE

TL;DR: 论文提出一种结合LLM筛选与执行验证的两阶段框架，用于检测Python程序中的语义克隆（Type 4克隆），显著提升了检测精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统代码克隆检测方法难以捕捉语义克隆（功能相同但语法不同），而直接应用LLM又因对语法差异敏感效果不佳。

Method: 两阶段框架：第一阶段用LLM筛选语义相似的代码对，第二阶段通过LLM生成的测试输入执行验证功能等价性。

Result: 实验表明，相比直接使用LLM，该框架在精度、召回率和F1分数上均有显著提升。

Conclusion: 框架有效解决了语义克隆检测问题，未来将探索跨语言检测和大规模应用优化。

Abstract: Code clone detection is a critical task in software engineering, aimed at
identifying duplicated or similar code fragments within or across software
systems. Traditional methods often fail to capture functional equivalence,
particularly for semantic clones (Type 4), where code fragments implement
identical functionality despite differing syntactic structures. Recent advances
in large language models (LLMs) have shown promise in understanding code
semantics. However, directly applying LLMs to code clone detection yields
suboptimal results due to their sensitivity to syntactic differences. To
address these challenges, we propose a novel two-stage framework that combines
LLM-based screening with execution-based validation for detecting semantic
clones in Python programs. In the first stage, an LLM evaluates code pairs to
filter out obvious non-clones based on semantic analysis. For pairs not
identified as clones, the second stage employs an execution-based validation
approach, utilizing LLM-generated test inputs to assess functional equivalence
through cross-execution validation. Our experimental evaluation demonstrates
significant improvements in precision, recall, and F1-score compared to direct
LLM-based detection, highlighting the framework's effectiveness in identifying
semantic clones. Future work includes exploring cross-language clone detection
and optimizing the framework for large-scale applications.

</details>


### [4] [An Empirical Validation of Open Source Repository Stability Metrics](https://arxiv.org/abs/2508.01358)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文通过实证研究验证了开源软件稳定性的控制理论框架（CSI），发现周采样优于日采样，并改进了统计方法，为项目监测工具提供了数据支持。


<details>
  <summary>Details</summary>
Motivation: 随着开源软件的广泛使用，其稳定性和可持续性成为关键问题。现有控制理论框架（CSI）缺乏实证验证。

Method: 实验分析了100个GitHub仓库，比较周采样与日采样的效果，并改进统计方法（用中位数代替均值）。

Result: 周采样更可行，改进的统计方法提升了问题与拉取请求的稳定性指数，并优化了半宽参数。

Conclusion: 研究证实了控制理论在开源健康中的适用性，并为实际项目监测提供了实证支持。

Abstract: Over the past few decades, open source software has been continuously
integrated into software supply chains worldwide, drastically increasing
reliance and dependence. Because of the role this software plays, it is
important to understand ways to measure and promote its stability and potential
for sustainability. Recent work proposed the use of control theory to
understand repository stability and evaluate repositories' ability to return to
equilibrium after a disturbance such as the introduction of a new feature
request, a spike in bug reports, or even the influx or departure of
contributors. This approach leverages commit frequency patterns, issue
resolution rate, pull request merge rate, and community activity engagement to
provide a Composite Stability Index (CSI). While this framework has theoretical
foundations, there is no empirical validation of the CSI in practice. In this
paper, we present the first empirical validation of the proposed CSI by
experimenting with 100 highly ranked GitHub repositories. Our results suggest
that (1) sampling weekly commit frequency pattern instead of daily is a more
feasible measure of commit frequency stability across repositories and (2)
improved statistical inferences (swapping mean with median), particularly with
ascertaining resolution and review times in issues and pull request, improves
the overall issue and pull request stability index. Drawing on our empirical
dataset, we also derive data-driven half-width parameters that better align
stability scores with real project behavior. These findings both confirm the
viability of a control-theoretic lens on open-source health and provide
concrete, evidence-backed applications for real-world project monitoring tools.

</details>


### [5] [From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool](https://arxiv.org/abs/2508.01430)
*Kaveh Shahedi,Matthew Khouzam,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 论文研究了复杂软件行为分析工具在工业环境中的采用障碍，提出了卓越悖论（Excellence Paradox），并通过TMLL工具验证了认知兼容性、嵌入专业知识和透明信任三大原则的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于工业环境中复杂软件追踪工具的采用障碍，尤其是在专家知识转化为实用洞察、工作流集成和自动化结果信任方面的挑战。

Method: 通过与Ericsson合作开发TMLL工具，并收集专家反馈、行业调查和外部评审数据，分析了工具采用的核心障碍和改进策略。

Result: 研究发现77.5%的受访者更关注结果质量和信任而非技术复杂度，67.5%倾向于半自动化分析。TMLL通过设计优化显著提升了工具采用率。

Conclusion: 论文结论表明可持续采用需要从功能导向转向以用户体验为中心的适应性设计，强调了认知兼容性和透明信任的重要性。

Abstract: System tracing has become essential for understanding complex software
behavior in modern systems, yet sophisticated trace analysis tools face
significant adoption gaps in industrial settings. Through a year-long
collaboration with Ericsson Montr\'eal, developing TMLL (Trace-Server Machine
Learning Library, now in the Eclipse Foundation), we investigated barriers to
trace analysis adoption. Contrary to assumptions about complexity or automation
needs, practitioners struggled with translating expert knowledge into
actionable insights, integrating analysis into their workflows, and trusting
automated results they could not validate. We identified what we called the
Excellence Paradox: technical excellence can actively impede adoption when
conflicting with usability, transparency, and practitioner trust. TMLL
addresses this through adoption-focused design that embeds expert knowledge in
interfaces, provides transparent explanations, and enables incremental
adoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's
integration, and a survey of 40 industry and academic professionals revealed
consistent patterns: survey results showed that 77.5% prioritize quality and
trust in results over technical sophistication, while 67.5% prefer
semi-automated analysis with user control, findings supported by qualitative
feedback from industrial collaboration and external peer review. Results
validate three core principles: cognitive compatibility, embedded expertise,
and transparency-based trust. This challenges conventional capability-focused
tool development, demonstrating that sustainable adoption requires
reorientation toward adoption-focused design with actionable implications for
automated software engineering tools.

</details>


### [6] [Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective](https://arxiv.org/abs/2508.01443)
*Jingzhi Gong,Rafail Giavrimis,Paul Brookes,Vardan Voskanyan,Fan Wu,Mari Ashiga,Matthew Truscott,Mike Basios,Leslie Kanthan,Jie Xu,Zheng Wang*

Main category: cs.SE

TL;DR: MPCO框架通过元提示技术自动生成适用于多种LLM的高质量任务特定提示，解决了工业平台上多LLM部署时的提示工程瓶颈。


<details>
  <summary>Details</summary>
Motivation: 工业平台上部署多个大型语言模型时，针对一个模型优化的提示往往不适用于其他模型，导致高昂的模型特定提示工程成本，限制了多LLM优化系统的实际部署。

Method: MPCO使用元提示技术动态合成上下文感知的优化提示，整合项目元数据、任务需求和LLM特定上下文，并在ARTEMIS工业平台上自动验证和扩展。

Result: 在五个真实代码库上的评估显示，MPCO性能提升高达19.06%，96%的顶级优化来自有效编辑，且三大LLM均可作为元提示生成器。

Conclusion: MPCO通过高效的上下文集成和多LLM兼容性，为工业实践提供了可行的解决方案。

Abstract: There is a growing interest in leveraging large language models (LLMs) for
automated code optimization. However, industrial platforms deploying multiple
LLMs face a critical challenge: prompts optimized for one LLM often fail with
others, requiring expensive model-specific prompt engineering. This cross-model
prompt engineering bottleneck severely limits the practical deployment of
multi-LLM optimization systems in production environments. To address this, we
introduce Meta-Prompted Code Optimization (MPCO), a framework that
automatically generates high-quality, task-specific prompts across diverse LLMs
while maintaining industrial efficiency requirements. MPCO leverages
meta-prompting to dynamically synthesize context-aware optimization prompts by
integrating project metadata, task requirements, and LLM-specific contexts, and
it seamlessly deploys on the ARTEMIS industrial platform for automated
validation and scaling.
  Our comprehensive evaluation on five real-world codebases with 366 hours of
runtime benchmarking demonstrates MPCO's effectiveness: it achieves overall
performance improvements up to 19.06% with the best statistical rank across all
systems compared to baseline methods. Analysis shows that 96% of the
top-performing optimizations stem from meaningful edits. Through systematic
ablation studies and meta-prompter sensitivity analysis, we identify that
comprehensive context integration is essential for effective meta-prompting,
and that all three major LLMs can serve effectively as meta-prompters,
providing actionable insights for industrial practitioners.

</details>


### [7] [Directed Grammar-Based Test Generation](https://arxiv.org/abs/2508.01472)
*Lukas Kirschner,Ezekiel Soremekun*

Main category: cs.SE

TL;DR: FdLoop是一种自动化测试生成方法，通过迭代学习输入属性来生成目标特定的测试输入，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语法的测试生成器难以针对特定测试目标生成有效输入，FdLoop旨在解决这一局限性。

Method: FdLoop通过测试反馈和概率语法，迭代选择、演化并学习目标特定测试输入的分布。

Result: 在86%的测试场景中，FdLoop优于五种基线方法，且在引发错误行为方面效果提升两倍。

Conclusion: FdLoop能够有效实现单一和多重测试目标，并在不同参数设置下表现出色。

Abstract: To effectively test complex software, it is important to generate
goal-specific inputs, i.e., inputs that achieve a specific testing goal.
However, most state-of-the-art test generators are not designed to target
specific goals. Notably, grammar-based test generators, which (randomly)
produce syntactically valid inputs via an input specification (i.e., grammar)
have a low probability of achieving an arbitrary testing goal. This work
addresses this challenge by proposing an automated test generation approach
(called FdLoop) which iteratively learns relevant input properties from
existing inputs to drive the generation of goal-specific inputs. Given a
testing goal, FdLoop iteratively selects, evolves and learn the input
distribution of goal-specific test inputs via test feedback and a probabilistic
grammar. We concretize FdLoop for four testing goals, namely unique code
coverage, input-to-code complexity, program failures (exceptions) and long
execution time. We evaluate FdLoop using three (3) well-known input formats
(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,
FdLoop outperforms all five tested baselines namely the baseline grammar-based
test generators (random, probabilistic and inverse-probabilistic methods),
EvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best
baseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that
the main components of FdLoop (i.e., input mutator, grammar mutator and test
feedbacks) contribute positively to its effectiveness. Finally, our evaluation
demonstrates that FdLoop effectively achieves single testing goals (revealing
erroneous behaviors, generating complex inputs, or inducing long execution
time) and scales to multiple testing goals across varying parameter settings.

</details>


### [8] [Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis](https://arxiv.org/abs/2508.01974)
*Jiahao Zhang,Xiao Cheng,Yuxiang Lei*

Main category: cs.SE

TL;DR: CG-FSPTA是一种基于流敏感约束图的指针分析方法，显著提高了效率同时保持精确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制流图的流敏感指针分析方法在处理指针信息时效率低下，CG-FSPTA旨在解决这一问题。

Method: CG-FSPTA利用流敏感变体的集合约束图结构优势，结合图优化和动态求解技术，保持流敏感性。

Result: 实验表明，CG-FSPTA平均减少33.05%内存使用，加速7.27倍，同时保持精确性。

Conclusion: CG-FSPTA为高效分析大规模软件系统提供了可扩展的方案，奠定了未来程序分析框架的基础。

Abstract: Flow-sensitive pointer analysis constitutes an essential component of precise
program analysis for accurately modeling pointer behaviors by incorporating
control flows. Flow-sensitive pointer analysis is extensively used in alias
analysis, taint analysis, program understanding, compiler optimization, etc.
Existing flow-sensitive pointer analysis approaches, which are conducted based
on control flow graphs, have significantly advanced the precision of pointer
analysis via sophisticated techniques to leverage control flow information.
However, they inevitably suffer from computational inefficiencies when
resolving points-to information due to the inherent complex structures of
control flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph
(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of
control-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to
leverage the structural advantages of set-constraint graphs (which are commonly
used in flow-insensitive pointer analysis) while keeping the flow sensitivity
of variable definitions and uses, allowing the incorporation of sophisticated
graph optimization and dynamic solving techniques. In this way, CG-FSPTA
achieves significant efficiency improvements while keeping the precision of
flow-sensitive analysis. Experimental evaluations on benchmark programs
demonstrate that CG-FSPTA, significantly reduces both memory usage and
execution time while maintaining precision. In particular, by solving in the
FSConsG, CG-FSPTA achieves an average memory reduction of 33.05\% and
accelerates flow-sensitive pointer analysis by 7.27x compared to the
state-of-art method. These experimental results underscore the efficacy of
CG-FSPTA as a scalable solution to analyze large-scale software systems,
establishing a robust foundation for future advancements in efficient program
analysis frameworks.

</details>


### [9] [GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development](https://arxiv.org/abs/2508.01489)
*SK. Golam Saroar,Waseefa Ahmed,Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: GitHub Marketplace（2017年推出）通过自动化工具提升了OSS项目的效率和可扩展性。研究系统性分析了该平台，比较工业工具与学术文献的进展，旨在弥合学术与工业实践之间的差距。


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace的自动化工具在OSS生产中日益普及，研究旨在理解其趋势、特征及动态，并弥合学术界与工业实践的脱节。

Method: 研究采用系统分析方法，比较GitHub Marketplace的工业工具趋势与学术文献的进展。

Result: 通过对比分析，识别出学术界可以为工业实践提供创新贡献的领域。

Conclusion: 研究为学术界与工业实践的协同创新提供了桥梁，助力GitHub Marketplace的进一步发展。

Abstract: GitHub, a central hub for collaborative software development, has
revolutionized the open-source software (OSS) ecosystem through its GitHub
Marketplace, a platform launched in 2017 to host automation tools aimed at
enhancing the efficiency and scalability of software projects. As the adoption
of automation in OSS production grows, understanding the trends,
characteristics, and underlying dynamics of this marketplace has become vital.
Furthermore, despite the rich repository of academic research on software
automation, a disconnect persists between academia and industry practices. This
study seeks to bridge this gap by providing a systematic analysis of the GitHub
Marketplace, comparing trends observed in industry tools with advancements
reported in academic literature, and identifying areas where academia can
contribute to practical innovation.

</details>


### [10] [OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications](https://arxiv.org/abs/2508.01492)
*Angel C. Chavez-Moreno,Cristina L. Abad*

Main category: cs.SE

TL;DR: 摘要介绍了OpenLambdaVerse数据集，该数据集通过分析GitHub上使用Serverless Framework和AWS Lambda的实际项目，提供了对当前无服务器架构的最新见解。


<details>
  <summary>Details</summary>
Motivation: 随着无服务器计算的快速发展，需要更实际的工具使用情况来了解其应用现状。

Method: 基于Wonderless数据集的方法，通过新的筛选步骤创建OpenLambdaVerse数据集，分析使用Serverless Framework的项目。

Result: 数据集揭示了应用的大小、复杂性、使用的语言和运行时、函数触发方式、项目成熟度及安全实践。

Conclusion: OpenLambdaVerse为研究者和从业者提供了了解无服务器工作负载演变的宝贵资源。

Abstract: Function-as-a-Service (FaaS) is at the core of serverless computing, enabling
developers to easily deploy applications without managing computing resources.
With an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless
Framework use YAML configurations to define and deploy APIs, tasks, workflows,
and event-driven applications on cloud providers, promoting zero-friction
development. As with any rapidly evolving ecosystem, there is a need for
updated insights into how these tools are used in real-world projects. Building
on the methodology established by the Wonderless dataset for serverless
computing (and applying multiple new filtering steps), OpenLambdaVerse
addresses this gap by creating a dataset of current GitHub repositories that
use the Serverless Framework in applications that contain one or more AWS
Lambda functions. We then analyze and characterize this dataset to get an
understanding of the state-of-the-art in serverless architectures based on this
stack. Through this analysis we gain important insights on the size and
complexity of current applications, which languages and runtimes they employ,
how are the functions triggered, the maturity of the projects, and their
security practices (or lack of). OpenLambdaVerse thus offers a valuable,
up-to-date resource for both practitioners and researchers that seek to better
understand evolving serverless workloads.

</details>


### [11] [Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification](https://arxiv.org/abs/2508.01523)
*Ningzhi Tang,Emory Smith,Yu Huang,Collin McMillan,Toby Jia-Jun Li*

Main category: cs.SE

TL;DR: 研究探讨了使用大型语言模型（LLMs）修改代码的两种提示策略：直接指令提示和摘要辅助提示，并分析了各自的优缺点。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面被广泛研究，但在代码修改中的作用仍不明确，需要探索有效的提示策略。

Method: 通过15名开发者的探索性研究，比较了直接指令提示和摘要辅助提示在代码修改任务中的应用。

Result: 直接指令提示更灵活易用，摘要辅助提示有助于代码理解和控制，策略选择受任务目标和上下文影响。

Conclusion: 需要改进提示交互的可用性，包括摘要粒度调整、可靠的代码可追溯性及摘要一致性。

Abstract: This paper presents a study of using large language models (LLMs) in
modifying existing code. While LLMs for generating code have been widely
studied, their role in code modification remains less understood. Although
"prompting" serves as the primary interface for developers to communicate
intents to LLMs, constructing effective prompts for code modification
introduces challenges different from generation. Prior work suggests that
natural language summaries may help scaffold this process, yet such approaches
have been validated primarily in narrow domains like SQL rewriting. This study
investigates two prompting strategies for LLM-assisted code modification:
Direct Instruction Prompting, where developers describe changes explicitly in
free-form language, and Summary-Mediated Prompting, where changes are made by
editing the generated summaries of the code. We conducted an exploratory study
with 15 developers who completed modification tasks using both techniques
across multiple scenarios. Our findings suggest that developers followed an
iterative workflow: understanding the code, localizing the edit, and validating
outputs through execution or semantic reasoning. Each prompting strategy
presented trade-offs: direct instruction prompting was more flexible and easier
to specify, while summary-mediated prompting supported comprehension, prompt
scaffolding, and control. Developers' choice of strategy was shaped by task
goals and context, including urgency, maintainability, learning intent, and
code familiarity. These findings highlight the need for more usable prompt
interactions, including adjustable summary granularity, reliable summary-code
traceability, and consistency in generated summaries.

</details>


### [12] [RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale](https://arxiv.org/abs/2508.01550)
*Zhilong Chen,Chengzong Zhao,Boyuan Chen,Dayi Lin,Yihao Chen,Arthur Leung,Gopi Krishnan Rajbahadur,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: RepoForge是一个端到端的自动化管道，用于大规模生成、评估和训练软件工程（SWE）智能体，解决了基础设施昂贵、数据稀缺和质量控制成本高等问题。


<details>
  <summary>Details</summary>
Motivation: 训练软件工程LLMs面临基础设施昂贵、评估效率低、训练数据稀缺和质量控制成本高等瓶颈问题，RepoForge旨在解决这些问题。

Method: RepoForge通过智能依赖管理、分布式评估、自动化数据生成和SPICE难度评估技术，提供了一种存储高效且成本低廉的训练解决方案。

Result: RepoForge-8B-Agent在SWE-Bench-Verified上达到17.4%的SOTA性能，存储减少14倍，评估速度提高70%，标注成本降低19,000倍。

Conclusion: RepoForge通过统一多项创新技术，证明了即使是≤8B的小模型也能在苛刻的基准测试中达到SOTA性能，解决了SWE训练中的关键瓶颈。

Abstract: Training software engineering (SWE) LLMs is bottlenecked by expensive
infrastructure, inefficient evaluation pipelines, scarce training data, and
costly quality control. We present RepoForge, an autonomous, end-to-end
pipeline that generates, evaluates, and trains SWE agents at scale. Our key
contributions include: (1) RepoForge-8B-Agent, achieving 17.4\% on
SWE-Bench-Verified~\citep{swebench_verified2024}, establishing new
state-of-the-art for $\leq$8B non-thinking LLMs; (2) 7,304 executable
environments auto-generated from real GitHub commits with zero manual
intervention; (3) 14$\times$ storage reduction (1.4GB $\rightarrow$ 102MB per
instance) via intelligent dependency management and image pruning; (4) $>$70\%
faster evaluation using a Ray-powered~\citep{ray2018} distributed RepoForge
harness; (5) 19,000$\times$ cheaper labeling through our automated
SPICE~\citep{spice2024} difficulty assessment technique. By unifying
storage-efficient sandboxing, Ray-powered evaluation harness, automated data
generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate
that even $\leq$8B models can reach new state-of-the-art performance on
demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical
bottlenecks in SWE agent training: high storage costs of container-based
evaluation, inefficient sequential reward pipelines, limited availability of
high-quality training data, expensive manual labeling, and multi-turn RL
pipeline bottlenecks.

</details>


### [13] [PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades](https://arxiv.org/abs/2508.02023)
*Huashan Lei,Guanping Xiao,Yepang Liu,Zheng Zheng*

Main category: cs.SE

TL;DR: PCREQ是一种自动化工具，用于解决Python第三方库升级中的兼容性问题，包括版本和代码兼容性，成功率达94.03%。


<details>
  <summary>Details</summary>
Motivation: Python第三方库升级常导致兼容性问题，现有工具无法完全自动化解决版本和代码兼容性问题，因此需要PCREQ填补这一空白。

Method: PCREQ整合六个模块，通过版本和代码兼容性分析、迭代调整版本，生成兼容的requirements.txt。

Result: PCREQ在REQBench基准测试中表现优异，成功率达94.03%，优于现有工具，且处理速度快。

Conclusion: PCREQ显著减少了手动解决升级问题的努力，推动了Python依赖维护的自动化。

Abstract: Python third-party libraries (TPLs) are essential in modern software
development, but upgrades often cause compatibility issues, leading to system
failures. These issues fall into two categories: version compatibility issues
(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect
dependency conflicts but overlook code-level incompatibilities, with no
solution fully automating the inference of compatible versions for both VCIs
and CCIs. To fill this gap, we propose PCREQ, the first approach to
automatically infer compatible requirements by combining version and code
compatibility analysis. PCREQ integrates six modules: knowledge acquisition,
version compatibility assessment, invoked APIs and modules extraction, code
compatibility assessment, version change, and missing TPL completion. PCREQ
collects candidate versions, checks for conflicts, identifies API usage,
evaluates code compatibility, and iteratively adjusts versions to generate a
compatible requirements.txt with a detailed repair report. To evaluate PCREQ,
we construct REQBench, a large-scale benchmark with 2,095 upgrade test cases
(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%
inference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and
LLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each
case from REQBench in 60.79s on average, demonstrating practical efficiency.
PCREQ significantly reduces manual effort in troubleshooting upgrades,
advancing Python dependency maintenance automation.

</details>


### [14] [BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games](https://arxiv.org/abs/2508.02144)
*Yusaku Kato,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: BiFuzz是一个两阶段的模糊测试工具，用于开放世界视频游戏的自动化测试，能够有效检测游戏中的卡顿故障。


<details>
  <summary>Details</summary>
Motivation: 开放世界视频游戏的搜索空间较大，导致测试自动化面临挑战，需要高效的测试工具。

Method: BiFuzz通过逐步变异游戏策略和测试用例（包括实际移动路径）来生成新输入。

Result: BiFuzz能够成功检测游戏中的卡顿故障。

Conclusion: BiFuzz为开放世界游戏的测试自动化提供了有效的解决方案。

Abstract: Open-world video games present a broader search space than other games,
posing challenges for test automation. Fuzzing, which generates new inputs by
mutating an initial input, is commonly used to uncover failures. In this study,
we proposed BiFuzz, a two-stage fuzzer designed for automated testing of
open-world video games, and investigated its effectiveness. The results
revealed that BiFuzz mutated the overall strategy of gameplay and test cases,
including actual movement paths, step by step. Consequently, BiFuzz can detect
`stucking' failures. The tool and its video are at
https://github.com/Yusaku-Kato/BiFuzz.

</details>


### [15] [An MLIR-based Compilation Framework for Control Flow Management on CGRAs](https://arxiv.org/abs/2508.02167)
*Yuxuan Wang,Cristian Tirelli,Giovanni Ansaloni,Laura Pozzi,David Atienza*

Main category: cs.SE

TL;DR: 本文提出了一种面向CGRA的编译框架，能够有效管理和优化控制流，实现了硬件无关的高性能映射，速度提升达2.1倍。


<details>
  <summary>Details</summary>
Motivation: CGRA的广泛应用受限于编译器无法高效处理控制流，现有方法主要关注数据流且局限于单循环映射。

Method: 采用模块化编译框架，包含转换和优化步骤，支持任意控制流应用的抽象CGRA网格映射；并提出一种新颖的后端映射方法。

Result: 框架通过纯编译优化实现了相对于现有方法的2.1倍速度提升。

Conclusion: 研究展示了编译级控制流优化的有效性，为CGRA的广泛应用提供了新思路。

Abstract: Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility
and efficiency, making them well-suited for the acceleration of intensive
workloads. Nevertheless, a key barrier towards their widespread adoption is
posed by CGRA compilation, which must cope with a multi-dimensional space
spanning both the spatial and the temporal domains. Indeed, state-of-the-art
compilers are limited in scope as they mostly deal with the data flow of
applications, while having little or no support for control flow. Hence, they
mostly target the mapping of single loops and/or delegate the management of
control flow divergences to ad-hoc hardware units.
  Conversely, in this paper we show that control flow can be effectively
managed and optimized at the compilation level, allowing for a broad set of
applications to be targeted while being hardware-agnostic and achieving high
performance. We embody our methodology in a modular compilation framework
consisting of transformation and optimization passes, enabling support for
applications with arbitrary control flows running on abstract CGRA meshes. We
also introduce a novel mapping methodology that acts as a compilation back-end,
addressing the limitations in available CGRA hardware resources and
guaranteeing a feasible solution in the compilation process. Our framework
achieves up to 2.1X speedups over state-of-the-art approaches, purely through
compilation optimizations.

</details>


### [16] [Highly Interactive Testing for Uninterrupted Development Flow](https://arxiv.org/abs/2508.02176)
*Andrew Tropin*

Main category: cs.SE

TL;DR: 提出了一种库，通过运行时表示测试，实现测试与高度交互开发环境（HIDE）的无缝集成，从而提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法与HIDE工具隔离，执行延迟高，破坏了开发流程的连续性。

Method: 开发了一个提供测试运行时表示的库，与HIDE紧密集成，支持测试失败时即时访问HIDE工具。

Result: 实现了亚秒级的测试重新执行时间，有助于保持开发者的专注力。

Conclusion: 通过紧密集成测试与HIDE工具，显著提升了开发流程的效率和连续性。

Abstract: Highly interactive development environments (HIDEs) enable uninterrupted
development flow through continuous program evolution and rapid hypothesis
checking. However, traditional testing approaches -- typically executed
separately via CLI -- isolate tests from HIDE tooling (interactive debuggers,
value and stack inspectors, etc.) and introduce disruptive delays due to coarse
execution granularity and lack of runtime context. This disconnect breaks
development flow by exceeding critical attention thresholds. In this paper we
present a library that provides runtime representation for tests, allowing
tight integration with HIDEs, and enabling immediate access to HIDE tooling in
the context of test failure. We then describe development workflows enhanced
with testing and demonstrate how they achieve subsecond test reexecution times
crucial for maintaining developer focus.

</details>


### [17] [A Methodological Framework for LLM-Based Mining of Software Repositories](https://arxiv.org/abs/2508.02233)
*Vincenzo De Martino,Joel Castaño,Fabio Palomba,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 该论文通过混合方法研究，分析了LLM在软件工程研究中的应用方法和潜在威胁，提出了PRIMES 2.0框架以提升研究的透明性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件仓库挖掘（MSR）研究中的方法学整合不足，缺乏系统性指导，作者旨在填补这一空白。

Method: 采用混合方法研究（快速回顾和问卷调查），分析LLM4MSR领域的方法和威胁。

Result: 识别了15种方法、9类威胁及25种缓解策略，并提出了PRIMES 2.0框架。

Conclusion: PRIMES 2.0为LLM-based MSR研究提供了透明、可重复的框架，支持方法学和威胁管理的全生命周期。

Abstract: Large Language Models (LLMs) are increasingly used in software engineering
research, offering new opportunities for automating repository mining tasks.
However, despite their growing popularity, the methodological integration of
LLMs into Mining Software Repositories (MSR) remains poorly understood.
Existing studies tend to focus on specific capabilities or performance
benchmarks, providing limited insight into how researchers utilize LLMs across
the full research pipeline. To address this gap, we conduct a mixed-method
study that combines a rapid review and questionnaire survey in the field of
LLM4MSR. We investigate (1) the approaches and (2) the threats that affect the
empirical rigor of researchers involved in this field. Our findings reveal 15
methodological approaches, nine main threats, and 25 mitigation strategies.
Building on these findings, we present PRIMES 2.0, a refined empirical
framework organized into six stages, comprising 23 methodological substeps,
each mapped to specific threats and corresponding mitigation strategies,
providing prescriptive and adaptive support throughout the lifecycle of
LLM-based MSR studies. Our work contributes to establishing a more transparent
and reproducible foundation for LLM-based MSR research.

</details>


### [18] [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
*Mikio Nakano,Hironori Takeuchi,Sadahiro Yoshikawa,Yoichi Matsuyama,Kazunori Komatani*

Main category: cs.SE

TL;DR: 本文提出将对话系统生命周期的软件工程领域称为对话系统工程，并调查该领域及其未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的进步，对话系统核心技术显著发展，需要正确高效地构建、运行和改进对话系统。

Method: 基于SWEBOK 4.0定义的软件工程知识领域，列举并调查对话系统工程的知识领域。

Result: 识别了每个领域中未探索的主题，并讨论了对话系统工程的未来方向。

Conclusion: 对话系统工程需进一步发展，以应对对话系统在商业和社会问题中的应用需求。

Abstract: This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.

</details>


### [19] [Interoperable verification and dissemination of software assets in repositories using COAR Notify](https://arxiv.org/abs/2508.02335)
*Matteo Cancellieri,Martin Docekal,David Pride,Morane Gruenpeter,David Douard,Petr Knoth*

Main category: cs.SE

TL;DR: SoFAIR项目通过机器学习工具从论文中提取软件引用，结合COAR Notify协议，提高开源研究软件的可见性和可信度。


<details>
  <summary>Details</summary>
Motivation: 开源研究软件在学术论文中常常被忽视，影响其可发现性、归因性和可重用性。

Method: 利用机器学习工具提取软件引用，集成HAL和Software Heritage等系统，采用COAR Notify协议实现跨系统通信。

Result: 提出了一套完整的工作流程，确保研究软件的可访问性、引用和存档符合FAIR原则。

Conclusion: SoFAIR项目通过技术手段提升了研究软件的可视化与可信度，为其作为一流文献记录的标准化奠定了基础。

Abstract: The discoverability, attribution, and reusability of open research software
are often hindered by its obscurity within academic manuscripts. To address
this, the SoFAIR project (2024-2025) introduces a comprehensive workflow
leveraging machine learning tools for extracting software mentions from
research papers. The project integrates repository systems, authors, and
services like HAL and Software Heritage to ensure proper archiving, citation,
and accessibility of research software in alignment with FAIR principles. To
enable interoperable communication across the various systems we present an
integration of the COAR Notify Protocol, which facilitates automated,
interoperable communication among repositories and authors to validate and
disseminate software mentions. This paper outlines the SoFAIR workflow and the
implementation of the COAR Notify Protocol, emphasising its potential to
enhance the visibility and credibility of research software as first-class
bibliographic records.

</details>


### [20] [Vision Language Model-based Testing of Industrial Autonomous Mobile Robots](https://arxiv.org/abs/2508.02338)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的测试方法（RVSG），用于工业自主移动机器人（AMR），通过生成多样化的人类行为违反功能和安全要求，提高测试效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 由于人类行为的不可预测性和实际测试的高成本与危险性，需要一种高效且安全的测试方法来验证AMR在不同人类交互中的表现。

Method: 使用VLM生成违反功能和安全要求的人类行为，并在模拟器中评估RVSG的性能。

Result: RVSG能有效生成违反要求的情景，并增加机器人行为的多样性，帮助揭示其不确定行为。

Conclusion: RVSG为AMR的安全测试提供了一种低成本、高效率和多样化的解决方案。

Abstract: Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,
warehouses, retail spaces, and offices), where they work alongside humans.
Given that human behavior can be unpredictable and that AMRs may not have been
trained to handle all possible unknown and uncertain behaviors, it is important
to test AMRs under a wide range of human interactions to ensure their safe
behavior. Moreover, testing in real environments with actual AMRs and humans is
often costly, impractical, and potentially hazardous (e.g., it could result in
human injury). To this end, we propose a Vision Language Model (VLM)-based
testing approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.
Based on the functional and safety requirements, RVSG uses the VLM to generate
diverse human behaviors that violate these requirements. We evaluated RVSG with
several requirements and navigation routes in a simulator using the latest AMR
from PAL Robotics. Our results show that, compared with the baseline, RVSG can
effectively generate requirement-violating scenarios. Moreover, RVSG-generated
scenarios increase variability in robot behavior, thereby helping reveal their
uncertain behaviors.

</details>


### [21] [JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis](https://arxiv.org/abs/2508.02397)
*Lida Zhao,Chaofan Li,Yueming Wu,Lyuye Zhang,Jiahui Wu,Chengwei Liu,Sen Chen,Yutao Hu,Zhengzi Xu,Yi Liu,Jingquan Ge,Jun Sun,Yang Liu*

Main category: cs.SE

TL;DR: JC-Finder 是一种专为 Java 设计的克隆检测工具，用于识别第三方库（TPL）的重用，比现有工具更准确和高效。


<details>
  <summary>Details</summary>
Motivation: TPL 的重用管理混乱可能威胁软件维护，并引发版权问题。现有工具对 Java 缺乏针对性。

Method: JC-Finder 通过类级特征捕获、维护函数间关系并排除冗余元素，实现高效率和准确性。

Result: 在测试中，JC-Finder 的 F1 分数为 0.818，比现有工具快 9 倍，并发现更多未声明的 TPL 重用。

Conclusion: JC-Finder 有效地填补了 Java 克隆检测工具的空白，显著提升了 TPL 重用的识别能力。

Abstract: While reusing third-party libraries (TPL) facilitates software development,
its chaotic management has brought great threats to software maintenance and
the unauthorized use of source code also raises ethical problems such as
misconduct on copyrighted code. To identify TPL reuse in projects, Software
Composition Analysis (SCA) is employed, and two categories of SCA techniques
are used based on how TPLs are introduced: clone-based SCA and
package-manager-based SCA (PM-based SCA). Although introducing TPLs by clones
is prevalent in Java, no clone-based SCA tools are specially designed for Java.
Also, directly applying clone-based SCA techniques from other tools is
problematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA
tool that aims to accurately and comprehensively identify instances of TPL
reuse introduced by source code clones in Java projects. JC-Finder achieves
both accuracy and efficiency in identifying TPL reuse from code cloning by
capturing features at the class level, maintaining inter-function
relationships, and excluding trivial or duplicated elements. To evaluate the
efficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as
reference data and tested the TPL reuse of 1,000 GitHub projects. The result
shows that JC-Finder achieved an F1-score of 0.818, outperforming the other
function-level tool by 0.427. The average time taken for resolving TPL reuse is
14.2 seconds, which is approximately 9 times faster than the other tool. We
further applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code
clones in 789 projects (about 9.89% of all projects) and identifying a total of
2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not
explicitly declared in package managers.

</details>


### [22] [Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots](https://arxiv.org/abs/2508.02407)
*Xinyi Wang,Qinghua Xu,Paolo Arcaini,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 为支持自主移动机器人的回归测试，提出了一种基于量子机器学习的测试准则框架QuReBot，结合量子储层计算和神经网络，减少预测误差15%，并提供了配置优化建议。


<details>
  <summary>Details</summary>
Motivation: 机器人软件升级后需要回归测试，但传统测试准则难以适应未知环境，因此探索量子机器学习以构建更精确的测试准则。

Method: 提出QuReBot框架，结合量子储层计算（QRC）和神经网络，预测机器人行为，优化测试准则。

Result: QRC单独应用失败，但QuReBot收敛且预测误差比传统神经网络降低15%。

Conclusion: QuReBot为机器人软件测试提供了一种高效解决方案，未来可进一步优化配置。

Abstract: Robots are increasingly becoming part of our daily lives, interacting with
both the environment and humans to perform their tasks. The software of such
robots often undergoes upgrades, for example, to add new functionalities, fix
bugs, or delete obsolete functionalities. As a result, regression testing of
robot software becomes necessary. However, determining the expected correct
behavior of robots (i.e., a test oracle) is challenging due to the potentially
unknown environments in which the robots must operate. To address this
challenge, machine learning (ML)-based test oracles present a viable solution.
This paper reports on the development of a test oracle to support regression
testing of autonomous mobile robots built by PAL Robotics (Spain), using
quantum machine learning (QML), which enables faster training and the
construction of more precise test oracles. Specifically, we propose a hybrid
framework, QuReBot, that combines both quantum reservoir computing (QRC) and a
simple neural network, inspired by residual connection, to predict the expected
behavior of a robot. Results show that QRC alone fails to converge in our case,
yielding high prediction error. In contrast, QuReBot converges and achieves 15%
reduction of prediction error compared to the classical neural network
baseline. Finally, we further examine QuReBot under different configurations
and offer practical guidance on optimal settings to support future robot
software testing.

</details>


### [23] [TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs](https://arxiv.org/abs/2508.02455)
*Daniele Cipollone,Egor Bogomolov,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 提出了一种新的轻量级、模型无关的评分方法，用于优化代码补全的静态建议排名。


<details>
  <summary>Details</summary>
Motivation: 改进现有代码补全系统中基于静态分析的建议排名方法，以更好地捕捉上下文信息并泛化到不同项目和编码风格。

Method: 将所有有效补全组织成前缀树，进行单次贪婪解码以收集全树的分值，无需束搜索或模型适配。

Result: 方法快速、架构无关且兼容现有模型，为IDE工具提供了更智能、响应更快的开发辅助。

Conclusion: 为语言模型集成到现有IDE工具提供了一条实用高效路径，提升了开发体验。

Abstract: Token-level code completion is one of the most critical features in modern
Integrated Development Environments (IDEs). It assists developers by suggesting
relevant identifiers and APIs during coding. While completions are typically
derived from static analysis, their usefulness depends heavily on how they are
ranked, as correct predictions buried deep in the list are rarely seen by
users. Most current systems rely on hand-crafted heuristics or lightweight
machine learning models trained on user logs, which can be further improved to
capture context information and generalize across projects and coding styles.
In this work, we propose a new scoring approach to ranking static completions
using language models in a lightweight and model-agnostic way. Our method
organizes all valid completions into a prefix tree and performs a single greedy
decoding pass to collect token-level scores across the tree. This enables a
precise token-aware ranking without needing beam search, prompt engineering, or
model adaptations. The approach is fast, architecture-agnostic, and compatible
with already deployed models for code completion. These findings highlight a
practical and effective pathway for integrating language models into already
existing tools within IDEs, and ultimately providing smarter and more
responsive developer assistance.

</details>


### [24] [An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs](https://arxiv.org/abs/2508.02473)
*Xinfang Chen,Siyang Xiao,Xianying Zhu,Junhong Xie,Ming Liang,Dajun Chen,Wei Jiang,Yong Li,Peng Di*

Main category: cs.SE

TL;DR: 这篇论文提出了一种名为NES的低延迟、免指令的代码编辑框架，利用开发者历史编辑模式预测下一步编辑，显著提升了开发效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的代码编辑工具依赖人工输入且延迟高，难以融入开发流程。

Method: 采用双模型架构，结合SFT和DAPO数据集训练LLM，优化推理以减少延迟。

Result: 在预测编辑位置和意图对齐编辑任务中，NES分别达到75.6%、81.6%准确率和91.36% ES、27.7% EMR，优于现有方法。

Conclusion: NES为可扩展的行业解决方案，已在FinTech公司成功应用，并开源数据集提升开源CodeLLMs性能。

Abstract: Code editing, including modifying, refactoring, and maintaining existing
code, is the most frequent task in software development and has garnered
significant attention from AI-powered tools. However, existing solutions that
translate explicit natural language instructions into code edits face critical
limitations, such as heavy reliance on human instruction input and high
latency, which hinder their effective integration into a developer's workflow.
We observe that developers' habitual behaviors and coding objectives are often
reflected in their historical editing patterns, making this data key to
addressing existing limitations. To leverage these insights, we propose NES
(Next Edit Suggestion), an LLM-driven code editing framework that delivers an
instruction-free and low-latency experience. Built on a dual-model architecture
and trained with our high-quality SFT and DAPO datasets, NES enhances
productivity by understanding developer intent while optimizing inference to
minimize latency. NES is a scalable, industry-ready solution with a continuous
Tab key interaction workflow, seamlessly adopted by a FinTech company with over
20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%
and 81.6% accuracy in two tasks of predicting next edit locations, alongside
91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.
Our open-sourced SFT and DAPO datasets have been demonstrated to enhance the
performance of open-source CodeLLMs. The demonstration of NES is available at
https://youtu.be/yGoyYOe6fbY.

</details>


### [25] [Commit Stability as a Signal for Risk in Open-Source Projects](https://arxiv.org/abs/2508.02487)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: 开源软件项目稳定性研究表明，仅有少数项目在每日、每周或每月维度上表现出稳定提交模式，区块链项目与编程语言项目最为稳定。


<details>
  <summary>Details</summary>
Motivation: 理解开源项目韧性对全球技术基础设施至关重要，但现有指标未能充分评估项目在干扰后恢复正常运作的能力。

Method: 基于复合稳定性指数框架，对100个高评分仓库的提交频率进行实证分析。

Result: 仅2%的项目每日稳定，29%每周稳定，50%每月稳定；编程语言与区块链项目最稳定。

Conclusion: 稳定性研究可扩展至问题解决时间等指标，为项目风险评估提供更全面框架。

Abstract: Open source software (OSS) generates trillions of dollars in economic value
and has become essential to technical infrastructures worldwide. As
organizations increasingly depend on OSS, understanding project evolution is
critical. While existing metrics provide insights into project health, one
dimension remains understudied: project resilience -- the ability to return to
normal operations after disturbances such as contributor departures, security
vulnerabilities, and bug report spikes. We hypothesize that stable commit
patterns reflect underlying project characteristics such as mature governance,
sustained contributors, and robust development processes that enable
resilience. Building on the Composite Stability Index (CSI) framework, we
empirically validate commit frequency patterns across 100 highly ranked
repositories. Our findings reveal that only 2\% of repositories exhibit daily
stability, 29\% achieve weekly stability, and 50\% demonstrate monthly
stability, while half remain unstable across all temporal levels. Programming
languages and blockchain applications were the most stable. We identified two
exemplary repositories that achieved stability at all three granularities,
whose governance models, CI cadence, and release policies could serve as
reference frameworks. We observed that large yearly commit throughput does not
necessarily correlate with stability. Beyond commits, stability can be enriched
with issue-resolution times, PR merge rates, and community-engagement metrics
to broaden resilience assessment and sharpen stability-based risk evaluation.

</details>


### [26] [Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation](https://arxiv.org/abs/2508.02497)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLM）在开源技术文档翻译中的应用，评估了其准确性和挑战，并提出了翻译保真度评分框架TRIFID。


<details>
  <summary>Details</summary>
Motivation: 开源社区缺乏多语言文档，LLM在技术文档翻译中的潜力尚不明确。

Method: 评估了50份README文件的英德翻译，对比了ChatGPT 4和Claude的表现，并提出TRIFID框架。

Result: LLM能提供准确翻译，但在保留结构和格式上存在挑战。

Conclusion: LLM在文档国际化中具有潜力，但仍需解决保真度问题，TRIFID为实现自动化翻译提供了基础。

Abstract: While open source communities attract diverse contributors globally, few
repositories provide essential documentation in languages other than English.
Large language models (LLMs) have demonstrated remarkable capabilities in
software engineering tasks and translations across domains. However, little is
known about LLM capabilities in translating open-source technical
documentation, which mixes natural language, code, URLs, and markdown
formatting. To understand the need and potential for LLMs in technical
documentation translation, we evaluated community translation activity and
English-to-German translations of 50 README files using OpenAI's ChatGPT 4 and
Anthropic's Claude. We found scarce translation activity, mostly in larger
repositories and community-driven in nature. LLM performance comparison
suggests they can provide accurate translations. However, analysis revealed
fidelity challenges: both models struggled to preserve structural components
(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings
highlight both promise and challenges of LLM-assisted documentation
internationalization. As a first step toward translation-aware continuous
integration pipelines, we introduce TRIFID, an early-stage translation fidelity
scoring framework that automatically checks how well translations preserve
code, links, and formatting. Our efforts provide a foundation for automated
LLM-driven support for creating and maintaining open source documentation.

</details>


### [27] [Automatic Identification of Machine Learning-Specific Code Smells](https://arxiv.org/abs/2508.02541)
*Peter Hamfelt,Ricardo Britto,Lincoln Rocha,Camilo Almendra*

Main category: cs.SE

TL;DR: 该论文研究了ML应用中的代码异味问题，设计并开发了静态代码分析工具MLpylint，并通过文献回顾、专家咨询和实际数据评估验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 目前针对ML应用中代码异味的研究缺乏工具和支持，亟需设计专用工具来解决这一问题。

Method: 采用设计科学方法，通过文献回顾和专家咨询设计工具MLpylint，并在160个开源ML应用上进行评估。

Result: MLpylint被证明是有效且实用的工具，静态验证和专家调查结果均支持其价值。

Conclusion: 研究提出未来将MLpylint集成到开发工作流中，以提升开发效率和创新性。

Abstract: Machine learning (ML) has rapidly grown in popularity, becoming vital to many
industries. Currently, the research on code smells in ML applications lacks
tools and studies that address the identification and validity of ML-specific
code smells. This work investigates suitable methods and tools to design and
develop a static code analysis tool (MLpylint) based on code smell criteria.
This research employed the Design Science Methodology. In the problem
identification phase, a literature review was conducted to identify ML-specific
code smells. In solution design, a secondary literature review and
consultations with experts were performed to select methods and tools for
implementing the tool. We evaluated the tool on data from 160 open-source ML
applications sourced from GitHub. We also conducted a static validation through
an expert survey involving 15 ML professionals. The results indicate the
effectiveness and usefulness of the MLpylint. We aim to extend our current
approach by investigating ways to introduce MLpylint seamlessly into
development workflows, fostering a more productive and innovative developer
environment.

</details>


### [28] [Meta-RAG on Large Codebases Using Code Summarization](https://arxiv.org/abs/2508.02611)
*Vali Tawosia,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 提出一种多代理系统Meta-RAG，利用信息检索和LLM在大型代码库中定位错误，通过代码库压缩和自然语言表示提升效率。


<details>
  <summary>Details</summary>
Motivation: 软件开发的复杂性不仅限于代码实现，还包括维护阶段。现有方法在大型代码库中定位错误效率不足，需要一个更高效的系统。

Method: 采用一种新颖的检索增强生成（RAG）方法Meta-RAG，通过摘要压缩代码库（平均减少79.8%），并利用LLM代理识别关键代码区域。

Result: 在SWE-bench Lite数据集上，Meta-RAG的文件级和函数级正确定位率分别达到84.67%和53.0%，性能领先。

Conclusion: Meta-RAG为大型代码库中的错误定位提供了高效解决方案，展示了LLM在软件维护中的潜力。

Abstract: Large Language Model (LLM) systems have been at the forefront of applied
Artificial Intelligence (AI) research in a multitude of domains. One such
domain is software development, where researchers have pushed the automation of
a number of code tasks through LLM agents. Software development is a complex
ecosystem, that stretches far beyond code implementation and well into the
realm of code maintenance. In this paper, we propose a multi-agent system to
localize bugs in large pre-existing codebases using information retrieval and
LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)
approach, Meta-RAG, where we utilize summaries to condense codebases by an
average of 79.8\%, into a compact, structured, natural language representation.
We then use an LLM agent to determine which parts of the codebase are critical
for bug resolution, i.e. bug localization. We demonstrate the usefulness of
Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores
84.67 % and 53.0 % for file-level and function-level correct localization
rates, respectively, achieving state-of-the-art performance.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [29] [Efficient compilation and execution of synchronous programs via type-state programming](https://arxiv.org/abs/2508.01199)
*Avinash Malik*

Main category: cs.PL

TL;DR: 论文提出了一种新的线性时间编译技术，用于同步程序的自动机编译，通过图重写规则和模板元编程，显著提升了执行效率。


<details>
  <summary>Details</summary>
Motivation: 同步程序在安全关键嵌入式软件中广泛使用，但传统编译方法面临状态空间爆炸问题，导致编译和执行效率低下。

Method: 引入基于图的改写规则，提出线性时间算法生成有限状态机（FSM），并通过C++模板元编程将其编码为类型状态程序。

Result: 实验表明，编译时间和二进制大小与现有技术相当，执行时间平均快31-60%。

Conclusion: 该方法有效解决了同步编程的高效编译问题，显著提升了执行性能。

Abstract: Synchronous programs are used extensively in implementation of safety
critical embedded software. Imperative synchronous programming languages model
multiple Finite State Machines (FSMs) executing in lockstep at logical clock
ticks. The synchronous view of time along with the FSM based design enables
easier formal verification. The synchronous composition of multiple FSMs,
during compilation, results in the well known state space explosion problem.
Hence, efficiently compiling imperative synchronous programs into small and
fast executables is challenging. This paper introduces a novel linear time
compilation technique for automata based compilation of synchronous programs.
Graph based rewrite rules for kernel programming constructs are introduced. A
linear time algorithm applies these rules to produce a FSM. The FSM is then
encoded into a type-state program using template meta-programming in C++.
Experimental results show that the compilation time and generated binary size
is comparable, while the execution times are on average 31-60% faster than
current state-of-the-art compilers.

</details>


### [30] [Proceedings 14th International Workshop on Trends in Functional Programming in Education](https://arxiv.org/abs/2508.02305)
*Rose Bohrer*

Main category: cs.PL

TL;DR: TFPIE是一个专注于教育中函数式编程应用的研讨会，旨在聚集相关研究人员和教育工作者，分享新想法和教学经验。


<details>
  <summary>Details</summary>
Motivation: 促进函数式编程在教育中的应用，提供一个开放的讨论平台。

Method: 举办为期一天的研讨会，采用会后再审稿的出版流程。

Result: 希望为教育中的函数式编程提供一个动态的交流环境。

Conclusion: TFPIE致力于推动函数式编程在教育中的创新与讨论。

Abstract: The goal of TFPIE is to gather researchers, teachers and professionals that
use, or are interested in the use of, functional programming in education.
TFPIE aims to be a venue where novel ideas, classroom-tested ideas and
work-in-progress on the use of functional programming in education are
discussed. The one-day workshop will foster a spirit of open discussion by
having a review process for publication after the workshop.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [31] [Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling](https://arxiv.org/abs/2508.00904)
*Rajeev Patwari,Ashish Sirasao,Devleena Das*

Main category: cs.PF

TL;DR: LIFE是一个轻量级模块化分析框架，用于预测异构设备上的LLM推理性能，支持多种优化技术且无需大量基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于异构设备的计算和内存需求动态变化，现有的GPU基准测试或机器学习延迟预测方法缺乏通用性，因此开发了LIFE框架。

Method: LIFE通过模块化分析模型和硬件无关的配置，量化软件与模型优化对性能指标的影响，仅需硬件规格即可预测性能。

Result: 在AMD Ryzen CPU、NPU、iGPU和NVIDIA V100 GPU上验证了LIFE的预测准确性，适用于多种硬件平台。

Conclusion: LIFE为不同硬件平台上的高效LLM部署提供了性能预测工具。

Abstract: Large language models (LLMs) have been increasingly deployed as local agents
on personal devices with CPUs, NPUs and integrated GPUs. However, forecasting
inference performance on devices with such heterogeneity remains challenging
due to the dynamic compute and memory demands. Existing approaches rely on GPU
benchmarking or machine learning-based latency predictors, which are often
hardware-specific and lack generalizability. To this end, we introduce LIFE, a
lightweight and modular analytical framework that is comprised of modular
analytical model of operators, configurable to characterize LLM inference
workloads in a hardware and dataset-agnostic manner. LIFE characterizes the
influence of software and model optimizations, such as quantization, KV cache
compression, LoRA adapters, chunked prefill, different attentions, and operator
fusion, on performance metrics such as time-to-first-token (TTFT),
time-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables
performance forecasting using only hardware specifications, such as TOPS and
memory bandwidth, without requiring extensive dataset benchmarking. We validate
LIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA
V100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in
forecasting LLM performance through lens of system efficiency to enable
efficient LLM deployment across different hardware platforms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [32] [A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation](https://arxiv.org/abs/2508.01047)
*Efe Ağlamazlar,Emirhan Eken,Harun Batur Geçici*

Main category: cs.NI

TL;DR: 提出了一种基于深度强化学习的TCP拥塞控制算法，显著提升了延迟和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 探讨深度强化学习在解决现代网络中复杂拥塞控制问题中的潜力。

Method: 利用Deep Q-Networks优化拥塞窗口，通过NS-3模拟器和OpenGym接口训练和评估。

Result: 相比传统TCP New Reno，算法在延迟和吞吐量上表现更优，适应性更强。

Conclusion: 强化学习技术在拥塞控制中有广阔应用前景。

Abstract: This paper presents a novel TCP congestion control algorithm based on Deep
Reinforcement Learning. The proposed approach utilizes Deep Q-Networks to
optimize the congestion window (cWnd) by observing key network parameters and
taking real-time actions. The algorithm is trained and evaluated within the
NS-3 network simulator using the OpenGym interface. The results demonstrate
significant improvements over traditional TCP New Reno in terms of latency and
throughput, with better adaptability to changing network conditions. This study
emphasizes the potential of reinforcement learning techniques for solving
complex congestion control problems in modern networks.

</details>


### [33] [Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation](https://arxiv.org/abs/2508.01060)
*Ibrahim Althamary,Chen-Fu Chou,Chih-Wei Huang*

Main category: cs.NI

TL;DR: 为解决6G网络中卫星-地面车联网的动态性和部分可观测性问题，提出了一种新型多智能体强化学习框架MAAC-SAM，结合多头自注意力机制和自模仿学习，显著提升了传输效用和状态估计准确性。


<details>
  <summary>Details</summary>
Motivation: 6G网络中卫星-地面车联网的动态性和部分可观测性使得连接管理面临挑战，需要一种能够自主决策的新型多智能体框架。

Method: 提出了MAAC-SAM框架，结合多头自注意力机制（SAM）、自模仿学习（SIL）和指纹技术，以优化车辆在多连接环境中的决策。

Result: 基于SUMO和3GPP的仿真显示，MAAC-SAM在传输效用上比现有基线高14%，并在不同车辆密度和信息共享水平下保持高估计精度。

Conclusion: MAAC-SAM为卫星-地面车联网提供了一种高效的连接管理解决方案，具有显著的实际应用潜力。

Abstract: Managing connectivity in integrated satellite-terrestrial vehicular networks
is critical for 6G, yet is challenged by dynamic conditions and partial
observability. This letter introduces the Multi-Agent Actor-Critic with
Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent
reinforcement learning framework that enables vehicles to autonomously manage
connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure
(V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the
integration of a multi-head attention mechanism, which allows for robust state
estimation even with fluctuating and limited information sharing among
vehicles. The framework further leverages self-imitation learning (SIL) and
fingerprinting to improve learning efficiency and real-time decisions.
Simulation results, based on realistic SUMO traffic models and 3GPP-compliant
configurations, demonstrate that MAAC-SAM outperforms state-of-the-art
terrestrial and satellite-assisted baselines by up to 14% in transmission
utility and maintains high estimation accuracy across varying vehicle densities
and sharing levels.

</details>


### [34] [Improving performance of content-centric networks via decentralized coded caching for multi-level popularity and access](https://arxiv.org/abs/2508.01298)
*Azadeh Sadat Miraftab,Ahmadreza Montazerolghaem,Behrad Mahboobi*

Main category: cs.NI

TL;DR: 该论文提出了一种通过集成分散式编码缓存（DCC）改进内容中心网络（CCN）架构的方法，以提高网络性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于IP的网络模型存在局限性，CCN通过内容标识符直接寻址和路由提升性能，而DCC进一步优化了数据存储和传输策略。

Method: 论文提出了对CCN架构的修改，整合DCC技术，包括基于用户访问权限的差异化编码策略和路径上的数据重编码。

Result: 实验表明，该方法显著提高了网络吞吐量、缓存命中率，并降低了内容交付延迟。

Conclusion: 通过DCC的集成，CCN的性能得到了明显提升，验证了该框架的有效性。

Abstract: Content-Centric Networking (CCN) offers a novel architectural paradigm that
seeks to address the inherent limitations of the prevailing Internet Protocol
(IP)-based networking model. In contrast to the host-centric communication
approach of IP networks, CCN prioritizes content by enabling direct addressing
and routing based on content identifiers. The potential performance
improvements of CCN can be further amplified through optimized management of
coded data storage and transmission strategies. Decentralized Coded Caching
(DCC) emerges as a promising technique that harnesses the collective caching
power of distributed network elements. By strategically pre-positioning
frequently accessed content closer to potential consumers during periods of low
network utilization, DCC has the potential to mitigate content transfer rates
during peak traffic periods. This paper proposes a series of fundamental
modifications to the CCN architecture by integrating DCC. The proposed
framework incorporates differentiated coding strategies tailored to user access
privileges, thereby eliminating the overhead associated with queue-based
searching. Additionally, the framework facilitates recoding of uncoded data
encountered along the content delivery path. These combined methodologies
demonstrably enhance network throughput, elevate cache hit ratios, and
consequently, reduce content delivery latency compared to conventional CCN
implementations.

</details>


### [35] [M3LLM: Model Context Protocol-aided Mixture of Vision Experts For Multimodal LLMs in Networks](https://arxiv.org/abs/2508.01805)
*Yongjie Zeng,Hongyang Du*

Main category: cs.NI

TL;DR: M3LLM通过分布式视觉专家和协议协调，提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM架构集中于中央化，视觉编码模块与任务对齐不足，限制了动态视觉任务的性能。

Method: 提出M3LLM，利用模型上下文协议（MCP）协调视觉专家；通过双流SAC算法和自适应稳定性增强模块（ASEM）优化路由。

Result: 实验表明，M3LLM提高了任务准确性，降低了通信成本，增强了动态网络条件下的路由适应性。

Conclusion: M3LLM为分布式MLLM提供了一种高效解决方案，显著提升了性能和适应性。

Abstract: Current Multimodal Large Language Models (MLLMs) rely on centralized
architectures and often suffer from poor alignment between the input task and
their fixed visual encoding modules, which limits performance on diverse and
dynamic visual tasks. With the increasing deployment of resource-efficient
models on edge devices in wireless networks, a new opportunity emerges to
dynamically use distributed vision experts for improved MLLM inference quality.
To enable this, we propose M3LLM, where the Model Context Protocol (MCP)
coordinates a mixture of vision experts to achieve distributed MLLMs.
Specifically, MCP is an open protocol that structures the input task context
into interpretable representations, enabling wireless network-aware
coordination between the central model backbone and edge-hosted vision experts.
Based on the MCP representation, M3LLM formulates vision expert routing as a
joint optimization problem that balances task-expert semantic compatibility and
channel performance. To solve the resulting gradient conflicts, we develop a
dual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and
introduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical
Bayesian modeling to ensure effective routing. Experiments show that M3LLM
improves task accuracy, reduces communication cost, and enhances expert routing
adaptability under dynamic wireless network conditions.

</details>


### [36] [Revenue Optimization in Wireless Video Caching Networks: A Privacy-Preserving Two-Stage Solution](https://arxiv.org/abs/2508.01898)
*Yijing Zhang,Md-Ferdous Pervej,Andreas F. Molisch*

Main category: cs.NI

TL;DR: 论文提出了一种两阶段的隐私保护方案，用于优化无线视频缓存网络的收益。通过联邦学习预测多时段需求，并结合内容流行度优化缓存策略。


<details>
  <summary>Details</summary>
Motivation: 视频缓存能显著提升传输效率，但缓存大小有限，需适应动态用户需求以最大化系统收益。需求预测是缓存规划的重要前提。

Method: 采用隐私保护的联邦学习训练Transformer预测未来需求，结合内容流行度估计需求分布；通过整数线性规划优化长期缓存策略。

Result: 联邦学习方案性能接近理想集中式方案，且优于现有方法；新的收益优化方法比传统的缓存命中率方法提供更深层次的系统性能分析。

Conclusion: 两阶段方案有效优化了无线视频缓存网络的长期收益，同时保护用户隐私。

Abstract: Video caching can significantly improve delivery efficiency and enhance
quality of video streaming, which constitutes the majority of wireless
communication traffic. Due to limited cache size, caching strategies must be
designed to adapt to and dynamic user demand in order to maximize system
revenue. The system revenue depends on the benefits of delivering the requested
videos and costs for (a) transporting the files to the users and (b) cache
replacement. Since the cache content at any point in time impacts the
replacement costs in the future, demand predictions over multiple cache
placement slots become an important prerequisite for efficient cache planning.
Motivated by this, we introduce a novel two-stage privacy-preserving solution
for revenue optimization in wireless video caching networks. First, we train a
Transformer using privacy-preserving federated learning (FL) to predict
multi-slot future demands. Given that prediction results are never entirely
accurate, especially for longer horizons, we further combine global content
popularity with per-user prediction results to estimate the content demand
distribution. Then, in the second stage, we leverage these estimation results
to find caching strategies that maximize the long-term system revenue. This
latter problem takes on the form of a multi-stage knapsack problem, which we
then transform to a integer linear program. Our extensive simulation results
demonstrate that (i) our FL solution delivers nearly identical performance to
that of the ideal centralized solution and outperforms other existing caching
methods, and (ii) our novel revenue optimization approach provides deeper
system performance insights than traditional cache hit ratio (CHR)-based
optimization approaches.

</details>


### [37] [Convolutions are Competitive with Transformers for Encrypted Traffic Classification with Pre-training](https://arxiv.org/abs/2508.02001)
*Chungang Lin,Weiyao Zhang,Tianyu Zuo,Chao Zha,Yilong Jiang,Ruiqi Meng,Haitong Luo,Xuying Meng,Yujun Zhang*

Main category: cs.NI

TL;DR: NetConv，一种新型预训练卷积模型，通过局部字节模式和连续字节掩码任务提升加密流量分类的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的预训练模型在效率和长序列扩展性上存在不足，需要更高效的替代方案。

Method: 提出NetConv，采用堆叠流量卷积层和连续字节掩码预训练任务，优化局部模式捕捉和协议学习。

Result: 实验显示NetConv在分类性能上平均提升6.88%，模型吞吐量提升7.41倍。

Conclusion: 卷积模型在加密流量分类中具有竞争力，NetConv在性能和效率上优于现有预训练模型。

Abstract: Encrypted traffic classification is vital for modern network management and
security. To reduce reliance on handcrafted features and labeled data, recent
methods focus on learning generic representations through pre-training on
large-scale unlabeled data. However, current pre-trained models face two
limitations originating from the adopted Transformer architecture: (1) Limited
model efficiency due to the self-attention mechanism with quadratic complexity;
(2) Unstable traffic scalability to longer byte sequences, as the explicit
positional encodings fail to generalize to input lengths not seen during
pre-training. In this paper, we investigate whether convolutions, with linear
complexity and implicit positional encoding, are competitive with Transformers
in encrypted traffic classification with pre-training. We first conduct a
systematic comparison, and observe that convolutions achieve higher efficiency
and scalability, with lower classification performance. To address this
trade-off, we propose NetConv, a novel pre-trained convolution model for
encrypted traffic classification. NetConv employs stacked traffic convolution
layers, which enhance the ability to capture localized byte-sequence patterns
through window-wise byte scoring and sequence-wise byte gating. We design a
continuous byte masking pre-training task to help NetConv learn
protocol-specific patterns. Experimental results on four tasks demonstrate that
NetConv improves average classification performance by 6.88% and model
throughput by 7.41X over existing pre-trained models.

</details>


### [38] [PRIME: Plasticity-Robust Incremental Model for Encrypted Traffic Classification in Dynamic Network Environments](https://arxiv.org/abs/2508.02031)
*Tian Qin,Guang Cheng,Zihan Chen,Yuyang Zhou*

Main category: cs.NI

TL;DR: 论文提出PRIME框架，解决网络流量分类中增量学习模型因任务增加而性能下降的问题，通过调整参数规模和优化神经元活性提升模型可塑性。


<details>
  <summary>Details</summary>
Motivation: 随着网络流量加密增加和新服务出现，现有增量学习模型的可塑性下降，影响网络流量分类的有效性。

Method: 提出PRIME框架，通过观察模型参数有效秩和神经元不活跃比例，动态调整参数规模以提升模型可塑性。

Result: 实验显示，PRIME在多个加密流量数据集和类别增量场景中，性能显著优于其他增量学习算法，且参数规模增加最小。

Conclusion: PRIME框架有效解决了增量学习中的可塑性问题，为加密流量分类提供了高效解决方案。

Abstract: With the continuous development of network environments and technologies,
ensuring cyber security and governance is increasingly challenging. Network
traffic classification(ETC) can analyzes attributes such as application
categories and malicious intent, supporting network management services like
QoS optimization, intrusion detection, and targeted billing. As the prevalence
of traffic encryption increases, deep learning models are relied upon for
content-agnostic analysis of packet sequences. However, the emergence of new
services and attack variants often leads to incremental tasks for ETC models.
To ensure model effectiveness, incremental learning techniques are essential;
however, recent studies indicate that neural networks experience declining
plasticity as tasks increase. We identified plasticity issues in existing
incremental learning methods across diverse traffic samples and proposed the
PRIME framework. By observing the effective rank of model parameters and the
proportion of inactive neurons, the PRIME architecture can appropriately
increase the parameter scale when the model's plasticity deteriorates.
Experiments show that in multiple encrypted traffic datasets and different
category increment scenarios, the PRIME architecture performs significantly
better than other incremental learning algorithms with minimal increase in
parameter scale.

</details>


### [39] [Distillation-Enhanced Clustering Acceleration for Encrypted Traffic Classification](https://arxiv.org/abs/2508.02282)
*Ziyue Huang,Chungang Lin,Weiyao Zhang,Xuying Meng,Yujun Zhang*

Main category: cs.NI

TL;DR: NetClus框架通过蒸馏增强聚类加速，解决预训练模型在加密流量分类中的推理速度慢和处理新流量类型的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前加密流量分类系统存在预训练模型参数过多导致推理速度慢，以及依赖标记数据限制模型适应新流量类型的问题。

Method: NetClus提出一种框架，结合聚类友好损失优化潜在空间，并通过蒸馏压缩模型为轻量级前馈神经网络，同时使用启发式合并和ASI指标验证聚类纯度。

Result: NetClus实现最高6.2倍加速，且分类性能下降低于1%。

Conclusion: NetClus有效解决了加密流量分类中的效率和适应性瓶颈问题。

Abstract: Traffic classification plays a significant role in network service
management. The advancement of deep learning has established pretrained models
as a robust approach for this task. However, contemporary encrypted traffic
classification systems face dual limitations. Firstly, pretrained models
typically exhibit large-scale architectures, where their extensive
parameterization results in slow inference speeds and high computational
latency. Secondly, reliance on labeled data for fine-tuning restricts these
models to predefined supervised classes, creating a bottleneck when novel
traffic types emerge in the evolving Internet landscape. To address these
challenges, we propose NetClus, a novel framework integrating pretrained models
with distillation-enhanced clustering acceleration. During fine-tuning, NetClus
first introduces a cluster-friendly loss to jointly reshape the latent space
for both classification and clustering. With the fine-tuned model, it distills
the model into a lightweight Feed-Forward Neural Network model to retain
semantics. During inference, NetClus performs heuristic merge with near-linear
runtime, and valid the cluster purity with newly proposed metrics ASI to
identify emergent traffic types while expediting classification. Benchmarked
against existing pretrained methods, NetClus achieves up to 6.2x acceleration
while maintaining classification degradation below 1%.

</details>


### [40] [On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)](https://arxiv.org/abs/2508.02373)
*Iulisloi Zacarias,Oussama Ben Taarit,Admela Jukan*

Main category: cs.NI

TL;DR: 论文提出了一种基于AI的网络数字孪生（AI-NDT），利用多层知识图谱和GNN预测影响用户体验的网络指标，评估了四种GNN架构，其中GraphTransformer表现最佳。


<details>
  <summary>Details</summary>
Motivation: 6G等未来网络需支持多样化设备和应用，传统网络管理方法不足，数字孪生能无风险地训练AI模型。

Method: 采用多层知识图谱和GNN架构，基于RIPE Atlas公开数据训练数字孪生。

Result: GraphTransformer性能最佳，其他架构在训练时间敏感场景中可能更适用。

Conclusion: 网络数字孪生为下一代网络提供了可扩展且精准的主动管理方案。

Abstract: Future networks, such as 6G, will need to support a vast and diverse range of
interconnected devices and applications, each with its own set of requirements.
While traditional network management approaches will suffice, an automated
solutions are becoming a must. However, network automation frameworks are prone
to errors, and often they employ ML-based techniques that require training to
learn how the network can be optimized. In this sense, network digital twins
are a useful tool that allows for the simulation, testing, and training of AI
models without affecting the real-world networks and users. This paper presents
an AI-based Network Digital Twin (AI-NDT) that leverages a multi-layered
knowledge graph architecture and graph neural networks to predict network
metrics that directly affect the quality of experience of users. An evaluation
of the four most prominent Graph Neural Networks (GNN) architectures was
conducted to assess their effectiveness in developing network digital twins. We
trained the digital twin on publicly available measurement data from RIPE
Atlas, therefore obtaining results close to what is expected in real-world
applications. The results show that among the four architectures evaluated,
GraphTransformer presents the best performance. However, other architectures
might fit better in scenarios where shorter training time is important, while
also delivering acceptable results. The results of this work are indicative of
what might become common practice for proactive network management, offering a
scalable and accurate solution aligned with the requirements of the
next-generation networks.

</details>


### [41] [ASINT: Learning AS-to-Organization Mapping from Internet Metadata](https://arxiv.org/abs/2508.02571)
*Yongzhe Xu,Weitong Li,Eeshan Umrani,Taejoong Chung*

Main category: cs.NI

TL;DR: ASINT是一种端到端管道，结合了批量注册数据和Web资源，使用检索增强生成（RAG）指导LLM推理，以更准确地映射ASNs到其组织家族。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如WHOIS或PeeringDB）无法捕捉复杂的组织关系（如跨区域别名、父子所有权），且无法统一分散在不同RIR标识符中的组织。

Method: ASINT通过多阶段流程，融合注册数据与Web资源，利用RAG与LLM推理，将ASNs合并为“组织家族”。

Result: ASINT映射了11万多个ASNs到8万多个组织家族，识别了更多跨区域群体，并在安全与测量任务中表现更优。

Conclusion: ASINT展示了通过Web证据可以提供更准确且动态的互联网组织结构视图，支持定期更新与成本敏感型LLM选择。

Abstract: Accurately mapping Autonomous Systems (ASNs) to their owning or operating
organizations underpins Internet measurement research and security
applications. Yet existing approaches commonly rely solely on WHOIS or
PeeringDB, missing important relationships (e.g., cross-regional aliases,
parent-child ownership) and failing to unify organizations scattered across
different RIR identifiers. We introduce ASINT, an end-to-end pipeline that
fuses bulk registry data with unstructured Web sources, then employs
retrieval-augmented generation (RAG) to guide large language model (LLM)
inference. Through a multi-stage procedure, ASINT merges ASNs into
"organization families," capturing nuanced ties beyond the scope of simpler
heuristics.
  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both
AS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,
operator aliases, rebrands) that other datasets overlook. Moreover, our refined
mappings enhance multiple security and measurement tasks: ASINT exposes 27.5%
more intra-organizational RPKI misconfigurations, cuts false-positive hijack
alarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.
  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,
demonstrating that broader Web evidence can provide a more accurate, evolving
view of the Internet's organizational structure.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [42] [Graph-based Interaction Augmentation Network for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.01168)
*Hu Zhangfeng,Shi mengxin*

Main category: cs.MM

TL;DR: 该论文提出了一种基于图的框架，通过建模模态内和模态间的复杂依赖关系，提升多模态情感分析的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中模态的不可避免的不完整性对多模态情感分析（MSA）提出了挑战，现有方法未能充分利用模态间的互补语义。

Method: 设计了一种可学习的超图来建模模态内的时间依赖关系，并使用有向图基于注意力机制探索模态间的相关性，同时通过完美样本的知识监督交互过程。

Result: 在MOSI和MOSEI数据集上的实验验证了该方法的有效性。

Conclusion: 该框架能够充分利用模态内和模态间的互补信息，提升不完整样本的情感分析性能。

Abstract: The inevitable modality imperfection in real-world scenarios poses
significant challenges for Multimodal Sentiment Analysis (MSA). While existing
methods tailor reconstruction or joint representation learning strategies to
restore missing semantics, they often overlook complex dependencies within and
across modalities. Consequently, they fail to fully leverage available
modalities to capture complementary semantics. To this end, this paper proposes
a novel graph-based framework to exploit both intra- and inter-modality
interactions, enabling imperfect samples to derive missing semantics from
complementary parts for robust MSA. Specifically, we first devise a learnable
hypergraph to model intra-modality temporal dependencies to exploit contextual
information within each modality. Then, a directed graph is employed to explore
inter-modality correlations based on attention mechanism, capturing
complementary information across different modalities. Finally, the knowledge
from perfect samples is integrated to supervise our interaction processes,
guiding the model toward learning reliable and robust joint representations.
Extensive experiments on MOSI and MOSEI datasets demonstrate the effectiveness
of our method.

</details>


### [43] [DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.01644)
*Peiyuan Jiang,Yao Liu,Qiao Liu,Zongshun Zhang,Jiaye Yang,Lu Liu,Daibing Yao*

Main category: cs.MM

TL;DR: 论文提出了一种名为DRKF的多模态情感识别方法，通过解耦表征和知识融合解决模态异质性和情感线索不一致的问题，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别面临模态异质性和情感线索不一致的挑战，需要高效的方法来解耦共享表征和模态特定特征。

Method: DRKF包含优化表征学习模块（ORL）和知识融合模块（KF），ORL解耦表征，KF通过自注意力融合情感信息并引入情感判别子模块处理不一致性。

Result: 实验表明DRKF在IEMOCAP、MELD和M3ED数据集上达到SOTA性能。

Conclusion: DRKF通过解耦和融合策略有效解决了多模态情感识别中的关键问题，具有实际应用潜力。

Abstract: Multimodal emotion recognition (MER) aims to identify emotional states by
integrating and analyzing information from multiple modalities. However,
inherent modality heterogeneity and inconsistencies in emotional cues remain
key challenges that hinder performance. To address these issues, we propose a
Decoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF
consists of two main modules: an Optimized Representation Learning (ORL) Module
and a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual
information estimation method with progressive modality augmentation to
decouple task-relevant shared representations and modality-specific features
while mitigating modality heterogeneity. KF includes a lightweight
self-attention-based Fusion Encoder (FE) that identifies the dominant modality
and integrates emotional information from other modalities to enhance the fused
representation. To handle potential errors from incorrect dominant modality
selection under emotionally inconsistent conditions, we introduce an Emotion
Discrimination Submodule (ED), which enforces the fused representation to
retain discriminative cues of emotional inconsistency. This ensures that even
if the FE selects an inappropriate dominant modality, the Emotion
Classification Submodule (EC) can still make accurate predictions by leveraging
preserved inconsistency information. Experiments show that DRKF achieves
state-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code
is publicly available at https://github.com/PANPANKK/DRKF.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [44] [Expressive Power of Graph Transformers via Logic](https://arxiv.org/abs/2508.01067)
*Veeti Ahvonen,Maurice Funk,Damian Heiman,Antti Kuusisto,Carsten Lutz*

Main category: cs.LO

TL;DR: 该论文研究了图变换器（GTs）和GPS网络在不同注意力机制下的表达能力，探讨了实数与浮点数两种情况下的理论性质及其与逻辑表达的关系。


<details>
  <summary>Details</summary>
Motivation: 探索图变换器和GPS网络在图数据上的表达能力，特别是它们与不同逻辑模态之间的等价性。

Method: 分析了GTs和GPS-networks在软注意力和平均硬注意力下的表现，比较了实数与浮点数两种计算环境。

Result: 在实数环境下，GPS-networks与带全局模态的GML表达能力相同；在浮点数环境下，与带计数全局模态的GML等价。GTs的表现也类似。

Conclusion: 研究证明了GTs和GPS-networks在特定条件下与逻辑模态的等价性，为理解其表达能力提供了理论依据。

Abstract: Transformers are the basis of modern large language models, but relatively
little is known about their precise expressive power on graphs. We study the
expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and
GPS-networks by Ramp\'asek et al. (2022), both under soft-attention and average
hard-attention. Our study covers two scenarios: the theoretical setting with
real numbers and the more practical case with floats. With reals, we show that
in restriction to vertex properties definable in first-order logic (FO),
GPS-networks have the same expressive power as graded modal logic (GML) with
the global modality. With floats, GPS-networks turn out to be equally
expressive as GML with the counting global modality. The latter result is
absolute, not restricting to properties definable in a background logic. We
also obtain similar characterizations for GTs in terms of propositional logic
with the global modality (for reals) and the counting global modality (for
floats).

</details>


### [45] [Relative Completeness of Incorrectness Separation Logic](https://arxiv.org/abs/2508.01535)
*Yeonseok Lee,Koji Nakazawa*

Main category: cs.LO

TL;DR: ISL是一种专注于堆操作程序中错误检测的证明系统，通过引入最弱后条件的表达力证明了相对完备性。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑（如Hoare Logic或Separation Logic）采用过近似方法，而ISL专门解决堆操作程序的欠近似问题，但其完备性尚未证明。

Method: 利用最弱后条件的表达力证明ISL的相对完备性，引入允许无限析取的析取范式，并包含变量别名的规范化。

Result: 成功证明了ISL的相对完备性，并通过规范化计算最弱后条件。

Conclusion: ISL在堆操作程序错误检测中具有潜力，但仍需进一步验证其实际应用效果。

Abstract: Incorrectness Separation Logic (ISL) is a proof system that is tailored
specifically to resolve problems of under-approximation in programs that
manipulate heaps, and it primarily focuses on bug detection. This approach is
different from the over-approximation methods that are used in traditional
logics such as Hoare Logic or Separation Logic. Although the soundness of ISL
has been established, its completeness remains unproven. In this study, we
establish relative completeness by leveraging the expressiveness of the weakest
postconditions; expressiveness is a factor that is critical to demonstrating
relative completeness in Reverse Hoare Logic. In our ISL framework, we allow
for infinite disjunctions in disjunctive normal forms, where each clause
comprises finite symbolic heaps with existential quantifiers. To compute the
weakest postconditions in ISL, we introduce a canonicalization that includes
variable aliasing.

</details>


### [46] [Causality and Decision-making: A Logical Framework for Systems and Security Modelling](https://arxiv.org/abs/2508.01758)
*Pinaki Chakraborty,Tristan Caulfield,David Pym*

Main category: cs.LO

TL;DR: 提出了一种基于最小结构假设的战略性系统建模理论，引入干预算子和分离合取，统一形式化的系统行为与Halpern-Pearl的反事实推理。


<details>
  <summary>Details</summary>
Motivation: 为理解复杂系统中决策制定的因果推理提供理论基础，尤其是在安全相关领域。

Method: 采用转换系统方法和模态逻辑，引入干预算子和分离合取，结合Halpern-Pearl的结构因果模型。

Result: 通过分布式系统中的微服务决策示例验证了框架的适用性。

Conclusion: 该工作为研究复杂系统中因果决策提供了逻辑基础，统一了形式化行为与反事实推理。

Abstract: Causal reasoning is essential for understanding decision-making about the
behaviour of complex `ecosystems' of systems that underpin modern society, with
security -- including issues around correctness, safety, resilience, etc. --
typically providing critical examples. We present a theory of strategic
reasoning about system modelling based on minimal structural assumptions and
employing the methods of transition systems, supported by a modal logic of
system states in the tradition of van Benthem, Hennessy, and Milner, and
validated through equivalence theorems. Our framework introduces an
intervention operator and a separating conjunction to capture actual causal
relationships between component systems of the ecosystem, aligning naturally
with Halpern and Pearl's counterfactual approach based on Structural Causal
Models. We illustrate the applicability through examples of of decision-making
about microservices in distributed systems. We discuss localized
decision-making through a separating conjunction. This work unifies a formal,
minimalistic notion of system behaviour with a Halpern--Pearl-compatible theory
of counterfactual reasoning, providing a logical foundation for studying
decision making about causality in complex interacting systems.

</details>


### [47] [Separation Logic of Generic Resources via Sheafeology](https://arxiv.org/abs/2508.01866)
*Berend van Starkenburg,Henning Basold,Chase Ford*

Main category: cs.LO

TL;DR: 本文提出了一种基于资源感知的分离逻辑框架，通过将一阶逻辑与分类逻辑结合，利用模型论中的纤维化和sheaf理论，实现了适用于多种资源的通用分离逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有的分离逻辑方法虽然在指针程序验证中效果显著，但缺乏一种通用的理论框架，能够直接适用于不同类型的资源（如并发程序、随机变量等）。

Method: 采用分类逻辑和sheaf理论，将资源感知引入一阶逻辑，构建内部纤维化模型，支持谓词和分离连接词。

Result: 成功开发了一个通用的分离逻辑框架，并通过实例化为不同内存模型和随机变量验证了其有效性。

Conclusion: 通过sheafeology方法，本文为分离逻辑提供了一个可扩展且通用的理论框架，适用于多种资源类型。

Abstract: Separation logic was conceived in order to make the verification of pointer
programs scalable to large systems and it has proven extremely effective. The
key idea is that programs typically access only small parts of memory, allowing
for local reasoning. This idea is implemented in separation logic by extending
first-order logic with separating connectives, which inspect local regions of
memory. It turns that this approach not only applies to pointer programs, but
also to programs involving other resource structures. Various theories have
been put forward to extract and apply the ideas of separation logic more
broadly. This resulted in algebraic abstractions of memory and many variants of
separation logic for, e.g., concurrent programs and stochastic processes.
However, none of the existing approaches formulate the combination of
first-order logic with separating connectives in a theory that could
immediately yield program logics for different resources. In this paper, we
propose a framework based on the idea that separation logic can obtained by
making first-order logic resource-aware. First-order logic can be understood in
terms of categorical logic, specifically fibrations. Our contribution is to
make these resource-aware by developing categorical logic internally in
categories of sheaves, which is what we call sheafeology. The role of sheaves
is to model views on resources, through which resources can be localised and
combined, which enables the scalability promised by separation logic. We
contribute constructions of an internal fibration in sheaf categories that
models predicates on resources, and that admits first-order and separating
connectives. Thereby, we attain a general framework of separation logic for
generic resources, a claim we substantiate by instantiating our framework to
various memory models and random variables.

</details>


### [48] [Monitoring Hyperproperties over Observed and Constructed Traces](https://arxiv.org/abs/2508.02301)
*Marek Chalupa,Thomas A. Henzinger,Ana Oliveira da Costa*

Main category: cs.LO

TL;DR: 研究了在运行时监控系统是否满足由超属性定义的规范问题，引入了主动和被动追踪量词，并提出了基于生成器函数的监控算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在监控异步超属性时存在不足，需要一种新的方法来处理包含交替追踪量词的规范。

Method: 扩展了hypernode逻辑，引入了生成器函数作为主动追踪量词，并在可能无限域上解释这些公式。

Result: 实现了监控算法并进行了实验评估，首次支持了包含交替追踪量词的异步超属性监控。

Conclusion: 提出的方法为监控复杂超属性提供了新工具，尤其适用于并发和安全应用。

Abstract: We study the problem of monitoring at runtime whether a system fulfills a
specification defined by a hyperproperty, such as linearizability or variants
of non-interference. For this purpose, we introduce specifications with both
passive and active quantification over traces. While passive trace quantifiers
range over the traces that are observed, active trace quantifiers are
instantiated with \emph{generator functions}, which are part of the
specification. Generator functions enable the monitor to construct traces that
may never be observed at runtime, such as the linearizations of a concurrent
trace. As specification language, we extend hypernode logic with trace
quantifiers over generator functions and interpret these hypernode formulas
over possibly infinite domains. We present a corresponding monitoring
algorithm, which we implemented and evaluated on a range of hyperproperties for
concurrency and security applications. Our method enables, for the first time,
the monitoring of asynchronous hyperproperties that contain alternating trace
quantifiers.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [49] [Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling](https://arxiv.org/abs/2508.00843)
*Sumit Kumar,Sarthak Kapoor,Harsh Vardhan,Yao Zhao*

Main category: cs.HC

TL;DR: 本文研究了大型语言模型（LLMs）在自动化计算机辅助设计（CAD）工作流程中的潜力，通过将FreeCAD与LLM集成作为设计工具，发现LLMs在简单到中等复杂度设计中表现良好，但高约束模型需多次优化。


<details>
  <summary>Details</summary>
Motivation: 传统CAD流程复杂且需专业技能，阻碍快速原型设计和生成设计，LLMs的引入有望解决这一问题。

Method: 提出一个框架，通过LLMs从自然语言描述生成初始CAD脚本，并基于错误反馈迭代执行和优化。

Result: 实验表明，LLMs适用于简单到中等复杂度任务，但对高约束模型表现不佳，需改进记忆检索和提示工程。

Conclusion: LLMs在CAD自动化中具有变革潜力，但需进一步优化技术以提升脚本鲁棒性。

Abstract: Large Language Models (LLMs) are revolutionizing industries by enhancing
efficiency, scalability, and innovation. This paper investigates the potential
of LLMs in automating Computer-Aided Design (CAD) workflows, by integrating
FreeCAD with LLM as CAD design tool. Traditional CAD processes are often
complex and require specialized sketching skills, posing challenges for rapid
prototyping and generative design. We propose a framework where LLMs generate
initial CAD scripts from natural language descriptions, which are then executed
and refined iteratively based on error feedback. Through a series of
experiments with increasing complexity, we assess the effectiveness of this
approach. Our findings reveal that LLMs perform well for simple to moderately
complex designs but struggle with highly constrained models, necessitating
multiple refinements. The study highlights the need for improved memory
retrieval, adaptive prompt engineering, and hybrid AI techniques to enhance
script robustness. Future directions include integrating cloud-based execution
and exploring advanced LLM capabilities to further streamline CAD automation.
This work underscores the transformative potential of LLMs in design workflows
while identifying critical areas for future development.

</details>


### [50] [Cognitive Exoskeleton: Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback](https://arxiv.org/abs/2508.00846)
*Songlin Xu,Xinyu Zhang*

Main category: cs.HC

TL;DR: 本文提出了一种AI调节的双DRL框架，通过自适应时间压力反馈提升用户数学算术任务表现，解决了传统方法数据需求大的问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统DRL方法在提供自适应时间压力反馈时对大数据和用户研究的依赖问题，本文旨在设计一种更高效的双DRL框架。

Method: 采用双DRL框架：一个调节DRL代理调控用户表现，另一个模拟DRL代理从现有数据中模拟用户行为，减少训练数据需求。

Result: 用户研究显示，双DRL框架能有效提升用户表现，优于对照组。

Conclusion: 双DRL框架是一种可行且高效的方法，能够通过AI智能反馈增强人类认知表现。

Abstract: In this paper, we introduce an AI-mediated framework that can provide
intelligent feedback to augment human cognition. Specifically, we leverage deep
reinforcement learning (DRL) to provide adaptive time pressure feedback to
improve user performance in a math arithmetic task. Time pressure feedback
could either improve or deteriorate user performance by regulating user
attention and anxiety. Adaptive time pressure feedback controlled by a DRL
policy according to users' real-time performance could potentially solve this
trade-off problem. However, the DRL training and hyperparameter tuning may
require large amounts of data and iterative user studies. Therefore, we propose
a dual-DRL framework that trains a regulation DRL agent to regulate user
performance by interacting with another simulation DRL agent that mimics user
cognition behaviors from an existing dataset. Our user study demonstrates the
feasibility and effectiveness of the dual-DRL framework in augmenting user
performance, in comparison to the baseline group.

</details>


### [51] [GPT Chatbots for Alleviating Anxiety and Depression: A Pilot Randomized Controlled Trial with Afghan Women](https://arxiv.org/abs/2508.00847)
*Sofia Sahab,Jawad Haqbeen,Diksha Sapkota,Takayuki Ito*

Main category: cs.HC

TL;DR: 研究探讨了GPT-4对阿富汗女性心理健康的影响，发现带有共情指令的GPT-4组显著降低了焦虑和抑郁分数。


<details>
  <summary>Details</summary>
Motivation: 阿富汗女性面临塔利班限制、社会不平等和家庭暴力等多重挑战，亟需心理健康支持。

Method: 随机对照试验，60名参与者分为三组：标准GPT-4、带共情指令的GPT-4和等待组，使用HADS测量心理状态。

Result: 共情指令组HADS分数显著下降，语言分析显示更强的语言一致性与心理健康改善相关。

Conclusion: AI驱动的干预可作为心理健康支持的补充，但需与传统疗法结合以优化效果。

Abstract: In this study, we investigated the effects of GPT-4, with and without
specific conversational instructions, on the mental health of Afghan women.
These women face multifaceted challenges, including Taliban-imposed
restrictions, societal inequalities, and domestic violence, adversely affecting
their well-being. We conducted a randomized controlled trial with 60
participants, dividing them into three groups: GPT-4, a supportive listener
(GPT-4 with empathetic engagement instructions), and a waiting list. The
Hospital Anxiety and Depression Scale (HADS) was used to measure anxiety and
depression before and after the intervention. Linguistic analysis of chat data
examined personal pronouns, tones, emotions, and Language Style Matching (LSM).
The supportive listener group showed a significant reduction in HADS scores
compared to the other groups. Linguistic analysis revealed a more positive tone
and higher LSM in the supportive listener group, with a significant negative
correlation between LSM and changes in HADS scores, indicating greater
linguistic alignment was linked to reductions in anxiety and depression.
Perceived empathy ratings were also significantly higher in the supportive
listener group. These findings highlight the potential of AI-driven
interventions, like GPT-4, in providing accessible mental health support.
However, such interventions should complement traditional psychotherapy,
ensuring a collaborative approach to optimize therapeutic outcomes.

</details>


### [52] [RestAware: Non-Invasive Sleep Monitoring Using FMCW Radar and AI-Generated Summaries](https://arxiv.org/abs/2508.00848)
*Agniva Banerjee,Bhanu Partap Paregi,Haroon R. Lone*

Main category: cs.HC

TL;DR: RestAware是一个基于24GHz雷达的非接触式睡眠监测系统，解决了传统方法的舒适性和隐私问题，准确率达92%。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠监测方法（如可穿戴设备、摄像头）存在舒适性差、遮挡失效和隐私问题，需要更优解决方案。

Method: 使用24GHz FMCW雷达和KNN分类器监测睡眠姿势，并结合大语言模型生成个性化睡眠报告。

Result: 系统在25名参与者中实现92%分类准确率和0.91的F1分数，成本低至35美元。

Conclusion: RestAware是一个低成本、隐私友好的非接触式睡眠监测方案，适用于家庭和临床环境。

Abstract: Monitoring sleep posture and behavior is critical for diagnosing sleep
disorders and improving overall sleep quality. However, traditional approaches,
such as wearable devices, cameras, and pressure sensors, often compromise user
comfort, fail under obstructions like blankets, and raise privacy concerns. To
overcome these limitations, we present RestAware, a non-invasive, contactless
sleep monitoring system based on a 24GHz frequency-modulated continuous wave
(FMCW) radar. Our system is evaluated on 25 participants across eight common
sleep postures, achieving 92% classification accuracy and an F1-score of 0.91
using a K-Nearest Neighbors (KNN) classifier. In addition, we integrate
instruction-tuned large language models (Mistral, Llama, and Falcon) to
generate personalized, human-readable sleep summaries from radar-derived
posture data. This low-cost ($ 35), privacy-preserving solution offers a
practical alternative for real-time deployment in smart homes and clinical
environments.

</details>


### [53] [Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability](https://arxiv.org/abs/2508.00850)
*Nadja R. Ging-Jehli,Russell K. Childers,Joshua Lu,Robert Gemma,Rachel Zhu*

Main category: cs.HC

TL;DR: Gearshift Fellowship (GF) 是一个新的 Supertask 范式原型，用于研究人类和人工智能代理如何适应环境变化。它结合了计算神经认知建模和严肃游戏，旨在评估跨认知和社会环境的适应性行为机制。


<details>
  <summary>Details</summary>
Motivation: 研究人类和人工智能代理如何在不同环境中灵活调整行为，以应对变化和不确定性，从而提升自我调节学习和适应能力。

Method: 通过结合计算神经建模和严肃游戏，GF 创建了一个动态多任务环境，用于量化个体在感知决策、学习和元认知等方面的差异。

Result: 在线研究（n = 60）表明 GF 能够验证传统神经心理学任务的效应，并揭示不同学习环境和临床特征的新模式。

Conclusion: GF 是一个多功能平台，可用于科学研究、临床干预和个体训练，旨在提升人类和机器的灵活性和适应性。

Abstract: How do we learn when to persist, when to let go, and when to shift gears?
Gearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed
to model how humans and artificial agents adapt to shifting environment
demands. Grounded in cognitive neuroscience, computational psychiatry,
economics, and artificial intelligence, Supertasks combine computational
neurocognitive modeling with serious gaming. This creates a dynamic,
multi-mission environment engineered to assess mechanisms of adaptive behavior
across cognitive and social contexts. Computational parameters explain behavior
and probe mechanisms by controlling the game environment. Unlike traditional
tasks, GF enables neurocognitive modeling of individual differences across
perceptual decisions, learning, and meta-cognitive levels. This positions GF as
a flexible testbed for understanding how cognitive-affective control processes,
learning styles, strategy use, and motivational shifts adapt across contexts
and over time. It serves as an experimental platform for scientists, a
phenotype-to-mechanism intervention for clinicians, and a training tool for
players aiming to strengthen self-regulated learning, mood, and stress
resilience. Online study (n = 60, ongoing) results show that GF recovers
effects from traditional neuropsychological tasks (construct validity),
uncovers novel patterns in how learning differs across contexts and how
clinical features map onto distinct adaptations. These findings pave the way
for developing in-game interventions that foster self-efficacy and agency to
cope with real-world stress and uncertainty. GF builds a new adaptive ecosystem
designed to accelerate science, transform clinical care, and foster individual
growth. It offers a mirror and training ground where humans and machines
co-develop together deeper flexibility and awareness.

</details>


### [54] [Visuo-Acoustic Hand Pose and Contact Estimation](https://arxiv.org/abs/2508.00852)
*Yuemin Ma,Uksang Yoo,Yunchao Yao,Shahram Najam Syed,Luca Bondi,Jonathan Francis,Jean Oh,Jeffrey Ichnowski*

Main category: cs.HC

TL;DR: VibeMesh是一个结合视觉和主动声学感知的可穿戴系统，用于高精度的手部姿势和接触事件估计。


<details>
  <summary>Details</summary>
Motivation: 手部姿势和接触事件的准确估计在机器人数据收集、虚拟现实和生物力学分析中至关重要，但由于视觉遮挡、接触信号微弱、视觉感知的局限性以及缺乏灵活触觉感知技术，这一任务仍具挑战性。

Method: 开发了VibeMesh系统，结合骨传导扬声器和稀疏压电麦克风，通过结构化的声学信号传播来推断接触变化。提出一种基于图注意力的网络，处理同步的音频频谱和RGB-D生成的手部网格。

Result: VibeMesh在准确性和鲁棒性上优于纯视觉基线，尤其在遮挡或静态接触场景中表现突出。

Conclusion: VibeMesh为手部姿势和接触事件的估计提供了一种轻量级、非侵入式的解决方案，并推动了跨模态感知技术的发展。

Abstract: Accurately estimating hand pose and hand-object contact events is essential
for robot data-collection, immersive virtual environments, and biomechanical
analysis, yet remains challenging due to visual occlusion, subtle contact cues,
limitations in vision-only sensing, and the lack of accessible and flexible
tactile sensing. We therefore introduce VibeMesh, a novel wearable system that
fuses vision with active acoustic sensing for dense, per-vertex hand contact
and pose estimation. VibeMesh integrates a bone-conduction speaker and sparse
piezoelectric microphones, distributed on a human hand, emitting structured
acoustic signals and capturing their propagation to infer changes induced by
contact. To interpret these cross-modal signals, we propose a graph-based
attention network that processes synchronized audio spectra and RGB-D-derived
hand meshes to predict contact with high spatial resolution. We contribute: (i)
a lightweight, non-intrusive visuo-acoustic sensing platform; (ii) a
cross-modal graph network for joint pose and contact inference; (iii) a dataset
of synchronized RGB-D, acoustic, and ground-truth contact annotations across
diverse manipulation scenarios; and (iv) empirical results showing that
VibeMesh outperforms vision-only baselines in accuracy and robustness,
particularly in occluded or static-contact settings.

</details>


### [55] [EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities](https://arxiv.org/abs/2508.00856)
*Steph Grohmann*

Main category: cs.HC

TL;DR: 生成式AI工具EthicAlly为社会科学和人文学科研究者提供伦理支持，弥补现有伦理审查系统的不足。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学和人文学科的伦理审查缺乏适应其方法论的支持系统，导致研究者难以获得专业指导。

Method: 基于宪法AI技术和协作式提示开发方法，EthicAlly结合普适伦理原则与情境化考量，提供结构化伦理评估。

Result: EthicAlly作为概念验证原型，可协助研究者设计伦理研究并减轻伦理审查委员会负担，但不取代人工监督。

Conclusion: 生成式AI能有效填补伦理支持系统的空白，同时需保持人类监督的核心作用。

Abstract: In biomedical science, review by a Research Ethics Committee (REC) is an
indispensable way of protecting human subjects from harm. However, in social
science and the humanities, mandatory ethics compliance has long been met with
scepticism as biomedical models of ethics can map poorly onto methodologies
involving complex socio-political and cultural considerations. As a result,
tailored ethics training and support as well as access to RECs with the
necessary expertise is lacking in some areas, including parts of Europe and
low- and middle-income countries. This paper suggests that Generative AI can
meaningfully contribute to closing these gaps, illustrating this claim by
presenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics
support system for social science and humanities researchers. Drawing on
constitutional AI technology and a collaborative prompt development
methodology, EthicAlly provides structured ethics assessment that incorporates
both universal ethics principles and contextual and interpretive considerations
relevant to most social science research. In supporting researchers in ethical
research design and preparation for REC submission, this kind of system can
also contribute to easing the burden on institutional RECs, without attempting
to automate or replace human ethical oversight.

</details>


### [56] [Accessibility and Social Inclusivity: A Literature Review of Music Technology for Blind and Low Vision People](https://arxiv.org/abs/2508.00929)
*Shumeng Zhang,Raul Masu,Mela Bettega,Mingming Fan*

Main category: cs.HC

TL;DR: 本文总结了针对盲人和低视力（BLV）人群的音乐技术研究，提出了四个设计趋势和见解，并呼吁更多实证研究和包容性技术开发。


<details>
  <summary>Details</summary>
Motivation: 音乐活动对BLV人群特别有益，但目前缺乏系统化的可访问性技术研究。

Method: 通过文献综述，分类现有研究并总结设计趋势，提出四个关键见解。

Result: 识别了六类BLV音乐技术，提出了四个设计趋势（空间感知、信息获取、（非语言）沟通和记忆）。

Conclusion: 需要更多实证研究和真实场景测试，以推动从“可访问技术”到“包容性技术”的转变。

Abstract: This paper presents a systematic literature review of music technology
tailored for blind and low vision (BLV) individuals. Music activities can be
particularly beneficial for BLV people. However, a systematic approach to
organizing knowledge on designing accessible technology for BLV people has yet
to be attempted. We categorize the existing studies based on the type of
technology and the extent of BLV people's involvement in the research. We
identify six main categories of BLV people-oriented music technology and
highlight four key trends in design goals. Based on these categories, we
propose four general insights focusing on (1) spatial awareness, (2) access to
information, (3) (non-verbal) communication, and (4) memory. The identified
trends suggest that more empirical studies involving BLV people in real-world
scenarios are needed to ensure that technological advancements can enhance
musical experiences and social inclusion. This research proposes collaborative
music technology and inclusive real-world testing with the target group as two
key areas missing in current research. They serve as a foundational step in
shifting the focus from ``accessible technology'' to ``inclusive technology''
for BLV individuals within the broader field of accessibility research.

</details>


### [57] [How Long Does It Take to Alleviate Discomfort? A Preliminary Study on Reducing Cybersickness in Novice Users](https://arxiv.org/abs/2508.01070)
*Zhengxin Zhang,Shufang Qian,Yi Wang,Xiao Liu,Thuong Hoang,Chetan Arora,Jingjing Zhang,Henry Been Lirn Duh*

Main category: cs.HC

TL;DR: 这篇论文研究了VR新手在长时间使用运动隧道技术后的晕动症缓解情况，发现第四天症状显著减轻，但第五天场景变化导致症状反弹。


<details>
  <summary>Details</summary>
Motivation: 探索运动隧道技术对VR新手晕动症的长期缓解效果，填补现有研究空白。

Method: 24名VR新手在VRChat平台上进行为期一周的实验，收集五天的数据。

Result: 第四天晕动症显著减轻，第五天场景变化导致症状加重；运动隧道技术在某些场景下效果有限。

Conclusion: 运动隧道技术的效果受场景变化影响，未来研究需进一步优化技术并探索其他缓解方法。

Abstract: Cybersickness significantly impacts the user experience in VR applications.
Locomotion tunneling is a widely adopted technique for mitigating cybersickness
in susceptible users. However, there is a lack of research investigating the
effects of prolonged use of locomotion tunneling among novice users. To fill
this gap, we used VRChat as our experimental platform. We recruited 24 novice
VR users, defined as participants with no prior experience using immersive
virtual environments. We collected five days of data within a one-week period.
The results indicated that participants exhibited significant mitigation to
cybersickness by Day 4. However, a change in the VR scene on Day 5 led to a
notable increase in cybersickness symptoms. Qualitative feedback revealed
participant-perceived causes of cybersickness and suggested that the
effectiveness of locomotion tunneling was limited in some scenarios. Finally,
we discussed the limitations of the study and proposed directions for future
research.

</details>


### [58] [DescribePro: Collaborative Audio Description with Human-AI Interaction](https://arxiv.org/abs/2508.01092)
*Maryam Cheema,Sina Elahimanesh,Samuel Martin,Pooyan Fazli,Hasti Seifi*

Main category: cs.HC

TL;DR: DescribePro是一个协作式音频描述（AD）创作系统，结合AI生成与人工编辑，提高AD质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决人类精确描述与AI效率之间的权衡问题，提升BLV用户的视频内容可访问性。

Method: 通过多模态大语言模型提示和手动编辑，支持用户迭代优化AI生成的描述，并允许社区协作编辑。

Result: AI支持减少重复工作，保留专业用户的风格选择，降低新手认知负担；协作功能有助于定制化和培训。

Conclusion: 协作式AI工具能增强并扩大AD创作的可扩展性。

Abstract: Audio description (AD) makes video content accessible to millions of blind
and low vision (BLV) users. However, creating high-quality AD involves a
trade-off between the precision of human-crafted descriptions and the
efficiency of AI-generated ones. To address this, we present DescribePro a
collaborative AD authoring system that enables describers to iteratively refine
AI-generated descriptions through multimodal large language model prompting and
manual editing. DescribePro also supports community collaboration by allowing
users to fork and edit existing ADs, enabling the exploration of different
narrative styles. We evaluate DescribePro with 18 describers (9 professionals
and 9 novices) using quantitative and qualitative methods. Results show that AI
support reduces repetitive work while helping professionals preserve their
stylistic choices and easing the cognitive load for novices. Collaborative tags
and variations show potential for providing customizations, version control,
and training new describers. These findings highlight the potential of
collaborative, AI-assisted tools to enhance and scale AD authorship.

</details>


### [59] [Cross-Device Motion Interaction via Apple's Native System Frameworks](https://arxiv.org/abs/2508.01110)
*Ezequiel Santos*

Main category: cs.HC

TL;DR: 将iPhone转化为带触觉反馈的实时运动控制器，仅使用苹果原生框架，平均延迟70.4ms，适合快速原型设计及移动HCI应用。


<details>
  <summary>Details</summary>
Motivation: 为快速原型设计和移动HCI应用场景，提供一个无需云基础设施、完全离线的实时运动控制器解决方案。

Method: 整合CoreMotion进行惯性传感，MultipeerConnectivity实现10Hz的P2P数据传输，CoreHaptics提供即时触觉反馈，并内置记录器测量延迟。

Result: 平均延迟70.4ms，95%延迟低于74ms，21人参与的公开活动中验证零丢包、连接稳定，功耗仅24mW。

Conclusion: 该系统为具身交互研究、休闲游戏及离线教育工具提供了紧凑、可复现的基础，开源MIT许可。

Abstract: We introduce an open-source, fully offline pipeline that transforms a
consumer-grade iPhone into a motion controller with real-time tactile feedback,
using only native Apple frameworks. Designed for rapid prototyping and applied
mobile HCI scenarios, the system integrates CoreMotion for inertial sensing,
MultipeerConnectivity for peer-to-peer data transmission at 10 Hz, and
CoreHaptics for immediate tactile confirmation. A built-in logger captures
end-to-end latency without requiring clock synchronization, yielding a mean
delay of 70.4 ms and 95th percentile below 74 ms on typical 5 GHz Wi-Fi (-55
dBm RSSI). We validated the pipeline through a real-time demonstrator game,
KeepCalm, deployed during a public event with 21 participants. Results showed
stable connections, zero packet loss, and negligible power impact (24 mW on
iPhone 13 mini). With fewer than 500 lines of Swift code and no reliance on
cloud infrastructure, this system provides a compact, reproducible foundation
for embodied interaction research, casual games, and offline educational tools.
All source code, latency logs, and provisioning scripts are openly released
under an MIT license.

</details>


### [60] [Presentation of Low-Frequency Vibration to the Face Using Amplitude Modulation](https://arxiv.org/abs/2508.01155)
*Yuma Akiba,Shota Nakayama,Keigo Ushiyama,Izumi Mizoguchi,Hiroyuki Kajimoto*

Main category: cs.HC

TL;DR: 该研究提出了一种利用振幅调制技术在面部呈现纯低频振动的方法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于面部缺乏检测高频振动的受体，现有小型振动器无法呈现低频振动，因此需要一种新方法。

Method: 采用200赫兹的载波频率进行振幅调制，通过实验测试了额头和整个面部的振动感知效果。

Result: 实验证明该方法能在额头和面部特定区域（如眼睛、脸颊和下唇）有效呈现纯低频振动。

Conclusion: 该振幅调制技术为面部低频振动的呈现提供了可行方案。

Abstract: This study proposes a method to present pure low-frequency vibration
sensations to the face that cannot be presented by small commercially available
vibrators. The core innovation lies in utilizing an amplitude modulation
technique with a carrier frequency of approximately 200 Hz. Due to the absence
of Pacinian corpuscles in the facial region - receptors responsible for
detecting high-frequency vibrations around 200 Hz - only the original
low-frequency signal is perceived. Three experiments were conducted.
Experiments 1 and 2 were performed on the forehead to confirm that the proposed
amplitude modulation method could produce the desired low-frequency perception
and to evaluate the subjective quality of the vibration. The results suggested
that the proposed method could produce the perception of desired pure
low-frequency vibration when applied to the forehead. In Experiment 3, the
proposed method was applied to the whole face, and its range of applicability
was explored. The results indicated that the original low-frequency vibration
was clearly perceptible around the eyes, cheeks, and lower lip area.

</details>


### [61] [RoboLinker: A Diffusion-model-based Matching Clothing Generator Between Humans and Companion Robots](https://arxiv.org/abs/2508.01165)
*Jing Tang,Qing Xiao,Kunxu Du,Zaiqiao Ye*

Main category: cs.HC

TL;DR: RoboLinker是一种生成式设计系统，利用扩散模型为人类和机器人搭配服饰，并通过交互界面优化设计。


<details>
  <summary>Details</summary>
Motivation: 设计和谐的人类与机器人服饰，增强视觉和情感共鸣。

Method: 采用扩散模型，输入机器人图像和风格提示，生成人类服饰。

Result: 系统成功为类人和宠物机器人生成风格一致、情感共鸣的服饰。

Conclusion: RoboLinker展示了生成式设计在协调人类与机器人形象的潜力。

Abstract: We present RoboLinker, a generative design system that creates matching
outfits for humans and their robots. Using a diffusion-based model, the system
takes a robot image and a style prompt from users as input, and outputs a human
outfit that visually complements the robot's attire. Through an interactive
interface, users can refine the generated designs. We evaluate RoboLinker with
both humanoid and pet-like robots, demonstrating its capacity to produce
stylistically coherent and emotionally resonant results.

</details>


### [62] [NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration](https://arxiv.org/abs/2508.01235)
*Yaxin Hu,Arissa J. Sato,Jingxin Du,Chenming Ye,Anjun Zhu,Pragathi Praveena,Bilge Mutlu*

Main category: cs.HC

TL;DR: 研究了如何通过集成位置感知的LLM叙事能力提升机器人远程探索的体验，并在博物馆中测试了效果。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人远程存在系统依赖于用户对环境的先验知识，限制了其在陌生环境中的实用性。

Method: 开发了名为NarraGuide的原型系统，通过对话界面提供叙事引导，并在地质博物馆中部署测试。

Result: 用户对机器人的角色感知、对话参与以及对旁观者互动的偏好表现出积极反馈。

Conclusion: LLM赋能的机器人能够提供位置感知的叙事引导，丰富远程探索的体验。

Abstract: Robotic telepresence enables users to navigate and experience remote
environments. However, effective navigation and situational awareness depend on
users' prior knowledge of the environment, limiting the usefulness of these
systems for exploring unfamiliar places. We explore how integrating
location-aware LLM-based narrative capabilities into a mobile robot can support
remote exploration. We developed a prototype system, called NarraGuide, that
provides narrative guidance for users to explore and learn about a remote place
through a dialogue-based interface. We deployed our prototype in a geology
museum, where remote participants (n=20) used the robot to tour the museum. Our
findings reveal how users perceived the robot's role, engaged in dialogue in
the tour, and expressed preferences for bystander encountering. Our work
demonstrates the potential of LLM-enabled robotic capabilities to deliver
location-aware narrative guidance and enrich the experience of exploring remote
environments.

</details>


### [63] [ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts](https://arxiv.org/abs/2508.01279)
*Jiajun Zhu,Xinyu Cheng,Zhongsu Luo,Yunfan Zhou,Xinhuan Shu,Di Weng,Yingcai Wu*

Main category: cs.HC

TL;DR: ViseGPT通过自动提取用户提示中的约束生成测试用例，验证脚本可靠性，并以甘特图展示结果，显著提升调试效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖用户自行检查脚本逻辑和问题，缺乏直接验证功能，ViseGPT旨在解决这一问题。

Method: 开发ViseGPT工具，自动生成测试用例，通过甘特图直观展示测试结果，并通过用户研究验证其有效性。

Result: 用户研究表明，ViseGPT显著提高了调试效率，增强了问题检测与修正能力，优化了工作流程。

Conclusion: ViseGPT为LLM生成的数据整理脚本提供了高效的验证工具，提升了用户操作效率与体验。

Abstract: Large language models (LLMs) enable the rapid generation of data wrangling
scripts based on natural language instructions, but these scripts may not fully
adhere to user-specified requirements, necessitating careful inspection and
iterative refinement. Existing approaches primarily assist users in
understanding script logic and spotting potential issues themselves, rather
than providing direct validation of correctness. To enhance debugging
efficiency and optimize the user experience, we develop ViseGPT, a tool that
automatically extracts constraints from user prompts to generate comprehensive
test cases for verifying script reliability. The test results are then
transformed into a tailored Gantt chart, allowing users to intuitively assess
alignment with semantic requirements and iteratively refine their scripts. Our
design decisions are informed by a formative study (N=8) that explores user
practices and challenges. We further evaluate the effectiveness and usability
of ViseGPT through a user study (N=18). Results indicate that ViseGPT
significantly improves debugging efficiency for LLM-generated data-wrangling
scripts, enhances users' ability to detect and correct issues, and streamlines
the workflow experience.

</details>


### [64] [HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion](https://arxiv.org/abs/2508.01765)
*Kaining Zhang,Catarina Moreira,Pedro Belchior,Gun Lee,Mark Billinghurst,Joaquim Jorge*

Main category: cs.HC

TL;DR: HeadZoom是一种基于头部运动的免手动二维视觉内容导航技术，支持流畅缩放和平移，特别适用于物理交互受限的场景。


<details>
  <summary>Details</summary>
Motivation: 设计一种免手动交互技术，以解决物理交互受限场景下的二维视觉内容导航问题。

Method: 通过实时头部追踪实现缩放和平移，并评估了三种交互技术（Static、Tilt Zoom、Parallel Zoom）的性能。

Result: Parallel Zoom显著减少了头部运动总量，用户感知努力更低，适合长时间或精确任务。

Conclusion: HeadZoom为VR中基于头部的二维交互提供了新的设计方向，支持更沉浸和易用的免手动图像探索系统。

Abstract: We introduce \textit{HeadZoom}, a hands-free interaction technique for
navigating two-dimensional visual content using head movements. The system
enables fluid zooming and panning by only using real-time head tracking. It
supports natural control in applications such as map exploration, radiograph
inspection, and image browsing, particularly where physical interaction is
limited. We evaluated HeadZoom in a within-subjects user study comparing three
interaction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial,
error, and subjective metrics. Results show that Parallel Zoom significantly
reduced total head movement compared to Static and Tilt modes. Users reported
significantly lower perceived exertion for Parallel Zoom, confirming its
suitability for prolonged or precision-based tasks. By minimising movement
demands while maintaining task effectiveness, HeadZoom advances the design of
head-based 2D interaction in VR, creating new opportunities for immersive,
accessible, and hands-free systems for image exploration.

</details>


### [65] [ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance](https://arxiv.org/abs/2508.01282)
*Jiawei Li,Linjie Qiu,Zhiqing Wu,Qiongyan Chen,Ziyan Wang,Mingming Fan*

Main category: cs.HC

TL;DR: 研究开发了ExplorAR系统，通过AR技术支持老年人通过试错学习智能手机应用，结果显示其比视频教程更有效。


<details>
  <summary>Details</summary>
Motivation: 老年人因认知和生理变化在学习使用新智能手机应用时面临困难，试错法可能是一种更有效的学习方式。

Method: 设计并实现ExplorAR系统，提供实时AR视觉指导，与视频教程和简化版对比研究。

Result: AR试错法增强了老年人的学习体验，促进深度认知参与并提升探索信心。

Conclusion: AR支持的试错法是老年人学习智能手机应用的有效方法。

Abstract: Older adults tend to encounter challenges when learning to use new smartphone
apps due to age-related cognitive and physical changes. Compared to traditional
support methods such as video tutorials, trial-and-error allows older adults to
learn to use smartphone apps by making and correcting mistakes. However, it
remains unknown how trial-and-error should be designed to empower older adults
to use smartphone apps and how well it would work for older adults. Informed by
the guidelines derived from prior work, we designed and implemented ExplorAR,
an AR-based trial-and-error system that offers real-time and situated visual
guidance in the augmented space around the smartphone to empower older adults
to explore and correct mistakes independently. We conducted a user study with
18 older adults to compare ExplorAR with traditional video tutorials and a
simplified version of ExplorAR. Results show that the AR-supported
trial-and-error method enhanced older adults' learning experience by fostering
deeper cognitive engagement and improving confidence in exploring unknown
operations.

</details>


### [66] [AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Emotion Recognition](https://arxiv.org/abs/2508.01318)
*Zheng Lian*

Main category: cs.HC

TL;DR: AffectGPT-R1 提出了一种强化学习框架，直接优化基于情感轮的评价指标，显著提升了开放词汇多模态情绪识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 token-level 损失训练目标与基于情感轮的评价指标不一致，且无法直接优化。

Method: 提出了融合强化学习的方法，将评价指标作为奖励函数，采用 GRPO 优化模型。

Result: AffectGPT-R1 在 OV-MER 任务中取得了显著改进。

Conclusion: 该框架推进了多模态情绪识别领域的研究。

Abstract: Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict
emotions without being constrained by predefined label spaces, enabling
fine-grained and human-like emotion understanding. Unlike traditional
discriminative methods, OV-MER leverages generative models, such as large
language models (LLMs) with extensive vocabularies, to capture the full
spectrum of emotions. Previous approaches (like AffectGPT) primarily rely on
token-level loss for training. However, this objective does not align with the
emotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately,
EW-based metrics cannot be directly optimized via gradient backpropagation. In
this paper, we propose AffectGPT-R1, a reinforcement learning framework that
directly optimizes performance on EW-based metrics. Specifically, we treat
these metrics as the reward function and employ Group Relative Policy
Optimization (GRPO) to maximize rewards. Experimental results demonstrate that
AffectGPT-R1 achieves significant improvements on OV-MER. We hope this work
advances the field of multimodal emotion recognition. Our code will be publicly
available at:https://github.com/zeroQiaoba/AffectGPT.

</details>


### [67] [An Appraisal-Based Approach to Human-Centred Explanations](https://arxiv.org/abs/2508.01388)
*Rukshani Somarathna,Madhawa Perera,Tom Gedeon,Matt Adcock*

Main category: cs.HC

TL;DR: 提出了一种基于CPM的新解释框架，用于生成符合人类认知的AI解释。


<details>
  <summary>Details</summary>
Motivation: 传统的解释方法无法满足高要求领域中对AI系统可解释性的需求，需要更符合人类认知的解释。

Method: 利用CPM的评估组件，围绕关键维度（如相关性、影响等）构建解释框架。

Result: 提供了上下文敏感且认知上有意义的AI决策解释。

Conclusion: 通过结合认知科学与可解释AI，为生成更直观的解释提供了新范式。

Abstract: Explainability remains a critical challenge in artificial intelligence (AI)
systems, particularly in high stakes domains such as healthcare, finance, and
decision support, where users must understand and trust automated reasoning.
Traditional explainability methods such as feature importance and post-hoc
justifications often fail to capture the cognitive processes that underlie
human decision making, leading to either too technical or insufficiently
meaningful explanations. We propose a novel appraisal based framework inspired
by the Component Process Model (CPM) for explainability to address this gap.
While CPM has traditionally been applied to emotion research, we use its
appraisal component as a cognitive model for generating human aligned
explanations. By structuring explanations around key appraisal dimensions such
as relevance, implications, coping potential, and normative significance our
framework provides context sensitive, cognitively meaningful justifications for
AI decisions. This work introduces a new paradigm for generating intuitive,
human-centred explanations in AI driven systems by bridging cognitive science
and explainable AI.

</details>


### [68] [Unlocking Excellence: The Impact of Voucher Incentives on Cybersecurity Education](https://arxiv.org/abs/2508.01520)
*Jianhua Li,Shang Gao,Michelle Harvey,Trina Myers*

Main category: cs.HC

TL;DR: 研究探讨了高等教育中行业代金券激励对网络安全教育学生职业志向的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨高等教育中较少使用的代金券激励在网络安全教育中的效果。

Method: 采用100%基于作品集的评估策略，设计高区分度任务并提供代金券支持学生获取行业证书。

Result: 代金券激励显著影响学生的职业志向。

Conclusion: 建议在网络安全或更广泛的ICT教育中采用代金券激励策略。

Abstract: While voucher incentives have been popular for primary and secondary schools,
they are less used in higher education. In this study, we leverage industry
voucher incentives to inspire students in cybersecurity education (CSE). We
adopt a 100% portfolio-based assessment strategy, where students can freely
select their target grades in the investigated unit. We purposely design one of
the high distinction (HD) tasks to be obtaining an industry certificate and
provide vouchers to those who can accomplish a predefined set of tasks before a
midpoint. The voucher recipients will use the voucher to access the industry
certificate training materials and sit the certificate exam for free. Passing
the certificate exam is one of the conditions for gaining an HD grade. Our
survey and interviews reveal a substantial influence of voucher incentives on
students' career aspirations. In light of the findings, recommendations on
adopting voucher incentives in CSE or broader ICT education are offered for
institutions and researchers.

</details>


### [69] [Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice](https://arxiv.org/abs/2508.01547)
*Yongsu Ahn,Nam Wook Kim*

Main category: cs.HC

TL;DR: 本文探讨了生成式AI模型在数据可视化任务中优于人类的原因，通过对比ChatGPT-3.5、ChatGPT-4与人类的表现差异，发现ChatGPT-4兼具人类和ChatGPT-3.5的特点，在覆盖广度和技术反馈上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解为什么生成式AI模型在数据可视化任务中表现优于人类，尤其是在修辞结构、知识广度和感知质量方面的差异。

Method: 通过系统比较分析ChatGPT-3.5、ChatGPT-4和人类在可视化问题上的回答，评估其在修辞结构、知识广度和感知质量上的表现。

Result: 发现ChatGPT-4结合了人类和ChatGPT-3.5的特点，且在覆盖广度和技术反馈上优于人类，总体质量更高。

Conclusion: 研究结果对基于大型语言模型的用户体验改进提供了启示，并适用于更广泛的AI应用领域。

Abstract: This paper investigates why recent generative AI models outperform humans in
data visualization knowledge tasks. Through systematic comparative analysis of
responses to visualization questions, we find that differences exist between
two ChatGPT models and human outputs over rhetorical structure, knowledge
breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more
advanced model, displays a hybrid of characteristics from both humans and
ChatGPT-3.5. The two models were generally favored over human responses, while
their strengths in coverage and breadth, and emphasis on technical and
task-oriented visualization feedback collectively shaped higher overall
quality. Based on our findings, we draw implications for advancing user
experiences based on the potential of LLMs and human perception over their
capabilities, with relevance to broader applications of AI.

</details>


### [70] [How Many Times Do People Usually Experience Different Kinds of Stressors Each Day?](https://arxiv.org/abs/2508.01553)
*Sameer Neupane,Mithun Saha,David M. Almeida,Santosh Kumar*

Main category: cs.HC

TL;DR: 本文提出了一种估计日常压力源频率的方法，通过穿戴设备触发提示和自由格式收集数据，结合渐近模型解决样本稀疏性和偏差。研究发现平均每人每天经历5.39个压力源，主要为工作、健康和交通相关。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如日记、访谈和EMA）因稀疏采样和结构化格式无法准确估算日常压力源的频率，影响心理健康护理的效果。

Method: 研究采用穿戴设备触发提示和自由格式收集数据，结合渐近模型解决样本稀疏性和偏差问题。

Result: 平均每人每天经历5.39个压力源，其中工作（1.76/天）、健康（0.59/天）和交通（0.55/天）为主要压力源。

Conclusion: 研究成果为个体压力负荷提供基准，并为心理健康干预提供人群级基线参考。

Abstract: Understanding how frequently people experience different kinds of daily
stressors is crucial for interpreting stress exposure and informing mental
health care. But it can't be directly estimated from current assessment
methods, such as diaries, end-of-day interviews, and ecological momentary
assessments (EMA), that use sparse sampling to limit participant burden, and a
structured response format for uniformity. In this paper, we utilize stressor
data collected in a 100-day field study with 68 participants that adopted
wearable-triggered prompts and a freeform format to solicit stressors soon
after they occurred, but limited its prompts to a small subset to keep the
burden low. We develop asymptotic models to estimate the latent frequency of
different kinds of real-life stressors that address sample sparsity and
sampling bias. We find that people experience 5.39 stressors per day, on
average. The top three are related to work (1.76/day), health (0.59/day), and
transportation (0.55/day). These estimates offer a principled benchmark for
interpreting individual stressor loads. They can also inform mental health care
treatments and interventions by establishing population-level baselines.

</details>


### [71] [Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation](https://arxiv.org/abs/2508.01743)
*Shiyao Zhang,Omar Faruk,Robert Porzel,Dennis Küster,Tanja Schultz,Hui Liu*

Main category: cs.HC

TL;DR: 研究探讨了虚拟化身的拟人化程度与情感感知的关系，发现高度拟人化的化身可能引发负面情绪，而中等拟人化的可爱形象则有助于积极情感感知。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟化身在在线互动中的广泛应用，其拟人化程度对情感感知的影响尚不明确，研究者希望探索这一关系及其潜在影响。

Method: 研究采用埃克曼的六种“基本情绪”作为评估标准，比较不同拟人化程度化身（如人类、浣熊、鲨鱼等）的情感感知效果。

Result: 高度拟人化的化身（如人类形象）引发更多负面情绪反应，而中等拟人化的可爱形象（如浣熊和鲨鱼）则显著改善情感感知。

Conclusion: 化身的拟人化程度是情感感知的关键因素，其可爱度和自然面部状态也可能对情感传递起重要作用，为商业和心理咨询等应用提供了策略性设计指导。

Abstract: An increasing number of online interaction settings now provide the
possibility to visually represent oneself via an animated avatar instead of a
video stream. Benefits include protecting the communicator's privacy while
still providing a means to express their individuality. In consequence, there
has been a surge in means for avatar-based personalization, ranging from
classic human representations to animals, food items, and more. However, using
avatars also has drawbacks. Depending on the human-likeness of the avatar and
the corresponding disparities between the avatar and the original expresser,
avatars may elicit discomfort or even hinder effective nonverbal communication
by distorting emotion perception. This study examines the relationship between
the human-likeness of virtual avatars and emotion perception for Ekman's six
"basic emotions". Research reveals that avatars with varying degrees of
human-likeness have distinct effects on emotion perception. High human-likeness
avatars, such as human avatars, tend to elicit more negative emotional
responses from users, a phenomenon that is consistent with the concept of
Uncanny Valley in aesthetics, which suggests that closely resembling humans can
provoke negative emotional responses. Conversely, a raccoon avatar and a shark
avatar, known as cuteness, which exhibit moderate human similarity in this
study, demonstrate a positive influence on emotion perception. Our initial
results suggest that the human-likeness of avatars is an important factor for
emotion perception. The results from the follow-up study further suggest that
the cuteness of avatars and their natural facial status may also play a
significant role in emotion perception and elicitation. We discuss practical
implications for strategically conveying specific human behavioral messages
through avatars in multiple applications, such as business and counseling.

</details>


### [72] [Sonify Anything: Towards Context-Aware Sonic Interactions in AR](https://arxiv.org/abs/2508.01789)
*Laura Schütz,Sasan Matinfar,Ulrich Eck,Daniel Roth,Nassir Navab*

Main category: cs.HC

TL;DR: 提出了一种AR中基于材料感知的实时声音合成框架，提升虚实互动的真实感。


<details>
  <summary>Details</summary>
Motivation: 解决AR中虚拟物体因缺乏物理性导致的音效不自然问题。

Method: 利用计算机视觉识别真实物体材料，结合物理建模合成实时声音。

Result: 用户研究表明，基于材料的声音显著提升了声音互动的真实感和材料区分能力。

Conclusion: 上下文感知的声音合成增强了AR中的真实感和现实感知。

Abstract: In Augmented Reality (AR), virtual objects interact with real objects.
However, the lack of physicality of virtual objects leads to the absence of
natural sonic interactions. When virtual and real objects collide, either no
sound or a generic sound is played. Both lead to an incongruent multisensory
experience, reducing interaction and object realism. Unlike in Virtual Reality
(VR) and games, where predefined scenes and interactions allow for the playback
of pre-recorded sound samples, AR requires real-time sound synthesis that
dynamically adapts to novel contexts and objects to provide audiovisual
congruence during interaction. To enhance real-virtual object interactions in
AR, we propose a framework for context-aware sounds using methods from computer
vision to recognize and segment the materials of real objects. The material's
physical properties and the impact dynamics of the interaction are used to
generate material-based sounds in real-time using physical modelling synthesis.
In a user study with 24 participants, we compared our congruent material-based
sounds to a generic sound effect, mirroring the current standard of
non-context-aware sounds in AR applications. The results showed that
material-based sounds led to significantly more realistic sonic interactions.
Material-based sounds also enabled participants to distinguish visually similar
materials with significantly greater accuracy and confidence. These findings
show that context-aware, material-based sonic interactions in AR foster a
stronger sense of realism and enhance our perception of real-world
surroundings.

</details>


### [73] [When not to help: planning for lasting human-AI collaboration](https://arxiv.org/abs/2508.01837)
*Mark Steyvers,Lukas Mayer*

Main category: cs.HC

TL;DR: 提出基于POMDP的认知建模框架，解决AI实时交互中的时机问题，平衡即时援助与长期用户参与。


<details>
  <summary>Details</summary>
Motivation: 解决AI交互中频繁或冗余援助导致用户退出的问题，优化沟通策略以避免疲劳。

Method: 使用POMDP建模用户隐性认知状态，结合反事实推理权衡独立表现与AI援助。

Result: 自适应策略在仿真中显著优于始终提供或从不提供援助的基线策略。

Conclusion: 平衡短期决策与长期用户参与是优化AI沟通策略的关键。

Abstract: AI systems and technologies that can interact with humans in real time face a
communication dilemma: when to offer assistance and how frequently. Overly
frequent or contextually redundant assistance can cause users to disengage,
undermining the long-term benefits of AI assistance. We introduce a cognitive
modeling framework based on Partially Observable Markov Decision Processes
(POMDPs) that addresses this timing challenge by inferring a user's latent
cognitive state related to AI engagement over time. Additionally, our framework
incorporates reasoning about the long-term effects of AI assistance, explicitly
aiming to avoid actions that could lead the human user to disengage or
deactivate the AI. A key component of our approach is counterfactual reasoning:
at each time step, the AI considers how well the user would perform
independently and weighs the potential boost in performance against the risk of
diminishing engagement with the AI. Through simulations, we show that this
adaptive strategy significantly outperforms baseline policies in which
assistance is always provided or never provided. Our results highlight the
importance of balancing short-term decision accuracy with sustained user
engagement, showing how communication strategies can be optimized to avoid
alert fatigue while preserving the user's receptiveness to AI guidance.

</details>


### [74] [ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training](https://arxiv.org/abs/2508.01850)
*Lala Shakti Swarup Ray,Vitor Fortes Rey,Bo Zhou,Paul Lukowicz,Sungho Suh*

Main category: cs.HC

TL;DR: ChairPose是一个基于压力传感的无穿戴全身坐姿估计系统，解决了现有方法因遮挡、隐私问题等限制。


<details>
  <summary>Details</summary>
Motivation: 现代环境中长时间坐姿活动普遍，现有姿态感知方法存在诸多限制，需一种更灵活、隐私保护的技术。

Method: 采用两阶段生成模型，基于压力映射，结合椅子形态推断，并通过物理驱动数据增强提升泛化性。

Result: 在未见用户和椅子的情况下，平均关节位置误差为89.4毫米，表现出强泛化能力。

Conclusion: ChairPose为姿势感知交互系统设计提供了新可能，适用于人体工学、医疗等领域。

Abstract: Prolonged seated activity is increasingly common in modern environments,
raising concerns around musculoskeletal health, ergonomics, and the design of
responsive interactive systems. Existing posture sensing methods such as
vision-based or wearable approaches face limitations including occlusion,
privacy concerns, user discomfort, and restricted deployment flexibility. We
introduce ChairPose, the first full body, wearable free seated pose estimation
system that relies solely on pressure sensing and operates independently of
chair geometry. ChairPose employs a two stage generative model trained on
pressure maps captured from a thin, chair agnostic sensing mattress. Unlike
prior approaches, our method explicitly incorporates chair morphology into the
inference process, enabling accurate, occlusion free, and privacy preserving
pose estimation. To support generalization across diverse users and chairs, we
introduce a physics driven data augmentation pipeline that simulates realistic
variations in posture and seating conditions. Evaluated across eight users and
four distinct chairs, ChairPose achieves a mean per joint position error of
89.4 mm when both the user and the chair are unseen, demonstrating robust
generalization to novel real world generalizability. ChairPose expands the
design space for posture aware interactive systems, with potential applications
in ergonomics, healthcare, and adaptive user interfaces.

</details>


### [75] [Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction](https://arxiv.org/abs/2508.01860)
*Mansi Sharma,Shuang Chen,Philipp Müller,Maurice Rekrut,Antonio Krüger*

Main category: cs.HC

TL;DR: 研究提出了一种结合EEG和眼动追踪的方法，用于区分人类的导航意图和信息搜索意图，解决了之前方法的局限性，并公开了首个相关数据集。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在真实场景中的局限性，如固定搜索时间和需要目标用户的训练数据。

Method: 提出了跨用户预测搜索意图的方法，并公开了首个EEG和眼动追踪数据集。

Result: 在留一用户评估中达到84.5%的准确率，接近用户内预测的85.5%，但灵活性更高。

Conclusion: 该方法在区分搜索意图上具有较高的准确率和实用性，尤其适用于真实场景。

Abstract: For machines to effectively assist humans in challenging visual search tasks,
they must differentiate whether a human is simply glancing into a scene
(navigational intent) or searching for a target object (informational intent).
Previous research proposed combining electroencephalography (EEG) and
eye-tracking measurements to recognize such search intents implicitly, i.e.,
without explicit user input. However, the applicability of these approaches to
real-world scenarios suffers from two key limitations. First, previous work
used fixed search times in the informational intent condition -- a stark
contrast to visual search, which naturally terminates when the target is found.
Second, methods incorporating EEG measurements addressed prediction scenarios
that require ground truth training data from the target user, which is
impractical in many use cases. We address these limitations by making the first
publicly available EEG and eye-tracking dataset for navigational vs.
informational intent recognition, where the user determines search times. We
present the first method for cross-user prediction of search intents from EEG
and eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out
evaluations -- comparable to within-user prediction accuracy (85.5%) but
offering much greater flexibility

</details>


### [76] [VidAnimator: User-Guided Stylized 3D Character Animation from Human Videos](https://arxiv.org/abs/2508.01878)
*Xinwu Ye,Jun-Hsiang Yao,Jielin Feng,Shuhong Mei,Xingyu Lan,Siming Chen*

Main category: cs.HC

TL;DR: 提出了一种混合主动框架和交互系统，通过单视角视频将人类动作迁移到风格化3D角色上，简化动画制作流程。


<details>
  <summary>Details</summary>
Motivation: 风格化3D角色动画制作耗时费力，需简化流程以满足影视、广告及VR NPC开发的需求。

Method: 框架以单视角人类视频和3D角色为输入，捕捉动作并迁移；包含两种交互模块和用户自定义工具。

Result: 问卷研究显示框架能生成自然动画；三个案例展示了其多样性。

Conclusion: 该框架能高效生成风格化3D动画，拓展了动画制作的可能性。

Abstract: With captivating visual effects, stylized 3D character animation has gained
widespread use in cinematic production, advertising, social media, and the
potential development of virtual reality (VR) non-player characters (NPCs).
However, animating stylized 3D characters often requires significant time and
effort from animators. We propose a mixed-initiative framework and interactive
system to enable stylized 3D characters to mimic motion in human videos. The
framework takes a single-view human video and a stylized 3D character (the
target character) as input, captures the motion of the video, and then
transfers the motion to the target character. In addition, it involves two
interaction modules for customizing the result. Accordingly, the system
incorporates two authoring tools that empower users with intuitive
modification. A questionnaire study offers tangible evidence of the framework's
capability of generating natural stylized 3D character animations similar to
the motion in the video. Additionally, three case studies demonstrate the
utility of our approach in creating diverse results.

</details>


### [77] [Anchoring and Alignment: Data Factors in Part-to-Whole Visualization](https://arxiv.org/abs/2508.01881)
*Connor Bailey,Michael Gleicher*

Main category: cs.HC

TL;DR: 研究探索了数据和设计因素如何影响部分与整体关系的可视化，特别是锚定和对齐机制的作用。


<details>
  <summary>Details</summary>
Motivation: 探讨部分与整体关系可视化（如饼图和堆叠条形图）中数据和设计因素如何通过锚定和对齐机制影响数值估计。

Method: 通过在线研究，分析数值、位置和编码等数据和设计因素在部分与整体图表中对估计任务的影响。

Result: 研究发现突出数值和与刻度对齐显著影响任务表现，强调了基于数据和设计因素对感知机制的理解进行可视化设计的必要性。

Conclusion: 可视化设计应结合数据属性和设计因素对感知机制的影响，以优化部分与整体关系的表现。

Abstract: We explore the effects of data and design considerations through the example
case of part-to-whole data relationships. Standard part-to-whole
representations like pie charts and stacked bar charts make the relationships
of parts to the whole explicit. Value estimation in these charts benefits from
two perceptual mechanisms: anchoring, where the value is close to a reference
value with an easily recognized shape, and alignment where the beginning or end
of the shape is aligned with a marker. In an online study, we explore how data
and design factors such as value, position, and encoding together impact these
effects in making estimations in part-to-whole charts. The results show how
salient values and alignment to positions on a scale affect task performance.
This demonstrates the need for informed visualization design based around how
data properties and design factors affect perceptual mechanisms.

</details>


### [78] [IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition](https://arxiv.org/abs/2508.01894)
*Haozhe Zhou,Riku Arakawa,Yuvraj Agarwal,Mayank Goel*

Main category: cs.HC

TL;DR: 提出IMUCoCo框架，支持任意位置的IMU传感器进行运动感知，提升灵活性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统IMU使用受限于固定位置，无法充分发挥多设备潜力。

Method: 引入基于空间坐标的统一特征空间，将任意位置的IMU信号映射其中。

Result: IMUCoCo在典型和非典型放置下均表现优异，支持灵活使用。

Conclusion: IMUCoCo提升了IMU的适应性，支持用户按需放置传感器。

Abstract: IMUs are regularly used to sense human motion, recognize activities, and
estimate full-body pose. Users are typically required to place sensors in
predefined locations that are often dictated by common wearable form factors
and the machine learning model's training process. Consequently, despite the
increasing number of everyday devices equipped with IMUs, the limited
adaptability has seriously constrained the user experience to only using a few
well-explored device placements (e.g., wrist and ears). In this paper, we
rethink IMU-based motion sensing by acknowledging that signals can be captured
from any point on the human body. We introduce IMU over Continuous Coordinates
(IMUCoCo), a novel framework that maps signals from a variable number of IMUs
placed on the body surface into a unified feature space based on their spatial
coordinates. These features can be plugged into downstream models for pose
estimation and activity recognition. Our evaluations demonstrate that IMUCoCo
supports accurate pose estimation in a wide range of typical and atypical
sensor placements. Overall, IMUCoCo supports significantly more flexible use of
IMUs for motion sensing than the state-of-the-art, allowing users to place
their sensors-laden devices according to their needs and preferences. The
framework also supports the ability to change device locations depending on the
context and suggests placement depending on the use case.

</details>


### [79] [Effect of AI Performance, Risk Perception, and Trust on Human Dependence in Deepfake Detection AI system](https://arxiv.org/abs/2508.01906)
*Yingfan Zhou,Ester Chen,Manasa Pisipati,Aiping Xiong,Sarah Rajtmajer*

Main category: cs.HC

TL;DR: 研究探讨了AI性能对人类信任和合成数据识别决策的影响，发现参与者会根据感知风险和AI预测结果调整对AI的依赖。


<details>
  <summary>Details</summary>
Motivation: 合成数据的恶意使用引发了对网络安全、个人隐私和公共信任的担忧，但AI检测工具存在局限性，导致用户对其不信任。

Method: 通过一项涉及400名参与者的在线实验，研究AI性能变化如何影响人类在Deepfake检测中对AI的信任和依赖。

Result: 参与者根据感知风险和AI预测结果调整对AI的依赖程度。

Conclusion: 研究结果为开发透明、可解释的AI系统提供支持，帮助普通用户减少合成媒体的危害。

Abstract: Synthetic images, audio, and video can now be generated and edited by
Artificial Intelligence (AI). In particular, the malicious use of synthetic
data has raised concerns about potential harms to cybersecurity, personal
privacy, and public trust. Although AI-based detection tools exist to help
identify synthetic content, their limitations often lead to user mistrust and
confusion between real and fake content. This study examines the role of AI
performance in influencing human trust and decision making in synthetic data
identification. Through an online human subject experiment involving 400
participants, we examined how varying AI performance impacts human trust and
dependence on AI in deepfake detection. Our findings indicate how participants
calibrate their dependence on AI based on their perceived risk and the
prediction results provided by AI. These insights contribute to the development
of transparent and explainable AI systems that better support everyday users in
mitigating the harms of synthetic media.

</details>


### [80] [Human Capital Visualization using Speech Amount during Meetings](https://arxiv.org/abs/2508.02075)
*Ekai Hashimoto,Takeshi Mizumoto,Kohei Nagira,Shun Shiramatsu*

Main category: cs.HC

TL;DR: 研究通过分析会议中的发言量，提出可视化人力资本的方法，发现不同属性和参与者的发言差异，并与连续属性相关。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法未涵盖对话在人力资本中的核心作用，研究旨在填补这一空白。

Method: 利用对话可视化技术量化发言量，分析性别、职位等属性的发言差异及与连续属性的相关性。

Result: 通过中小企业的周会数据分析，验证了方法的有效性。

Conclusion: 发言量分析可作为人力资本可视化的有效工具，助力组织创新与沟通。

Abstract: In recent years, many companies have recognized the importance of human
resources and are investing in human capital to revitalize their organizations
and enhance internal communication, thereby fostering innovation. However,
conventional quantification methods have mainly focused on readily measurable
indicators without addressing the fundamental role of conversations in human
capital. This study focuses on routine meetings and proposes strategies to
visualize human capital by analyzing speech amount during these meetings. We
employ conversation visualization technology, which operates effectively, to
quantify speech. We then measure differences in speech amount by attributes
such as gender and job post, changes in speech amount depending on whether
certain participants are present, and correlations between speech amount and
continuous attributes. To verify the effectiveness of our proposed methods, we
analyzed speech amounts by departmental affiliation during weekly meetings at
small to medium enterprises.

</details>


### [81] [Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs](https://arxiv.org/abs/2508.02133)
*Yitong Zhu,Lei Han,GuanXuan Jiang,PengYuan Zhou,Yuyang Wang*

Main category: cs.HC

TL;DR: 论文提出了一种名为Hi-MoE的分层混合专家框架，用于解决多模态情感识别中的动态模态缺失和异步问题，并在实验中展示了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感识别方法对数据完整性假设过于理想化，且缺乏动态适应性，无法应对现实世界中的动态模态缺失和异步问题。

Method: 提出的Hi-MoE框架采用双层专家结构：模态专家库通过软路由动态处理缺失模态信息，情感专家库使用差异注意力路由实现细粒度情感表示，另外还引入了跨模态对齐模块解决时间偏移和语义不一致问题。

Result: 在DEAP和DREAMER基准数据集上的实验表明，该模型在连续情感回归任务中达到最先进性能，并展现出对动态模态缺失和异步采样的强大鲁棒性。

Conclusion: 这项研究显著推动了适应复杂现实环境的智能情感系统的发展。

Abstract: Multimodal emotion recognition (MER) is crucial for human-computer
interaction, yet real-world challenges like dynamic modality incompleteness and
asynchrony severely limit its robustness. Existing methods often assume
consistently complete data or lack dynamic adaptability. To address these
limitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)
framework for robust continuous emotion prediction. This framework employs a
dual-layer expert structure. A Modality Expert Bank utilizes soft routing to
dynamically handle missing modalities and achieve robust information fusion. A
subsequent Emotion Expert Bank leverages differential-attention routing to
flexibly attend to emotional prototypes, enabling fine-grained emotion
representation. Additionally, a cross-modal alignment module explicitly
addresses temporal shifts and semantic inconsistencies between modalities.
Extensive experiments on benchmark datasets DEAP and DREAMER demonstrate our
model's state-of-the-art performance in continuous emotion regression,
showcasing exceptional robustness under challenging conditions such as dynamic
modality absence and asynchronous sampling. This research significantly
advances the development of intelligent emotion systems adaptable to complex
real-world environments.

</details>


### [82] [EchoLadder: Progressive AI-Assisted Design of Immersive VR Scenes](https://arxiv.org/abs/2508.02173)
*Zhuangze Hou,Jingze Tian,Nianlong Li,Farong Ren,Can Liu*

Main category: cs.HC

TL;DR: EchoLadder是一种新型人机协作管道，利用大型视觉语言模型支持虚拟现实中的交互式场景修改，提升用户的创造力。


<details>
  <summary>Details</summary>
Motivation: 解决用户在空间设计中的构思与执行困难，提供交互式控制以避免现有AI模型缺乏迭代引导的问题。

Method: 结合大型视觉语言模型，通过用户口头指令生成具体设计建议，支持自动应用、重新生成和撤销。

Result: 研究显示EchoLadder有效支持用户创造力，并为未来系统设计提供策略见解。

Conclusion: EchoLadder成功提升用户交互体验，为AI辅助设计领域提供了实用框架。

Abstract: Mixed reality platforms allow users to create virtual environments, yet
novice users struggle with both ideation and execution in spatial design. While
existing AI models can automatically generate scenes based on user prompts, the
lack of interactive control limits users' ability to iteratively steer the
output. In this paper, we present EchoLadder, a novel human-AI collaboration
pipeline that leverages large vision-language model (LVLM) to support
interactive scene modification in virtual reality. EchoLadder accepts users'
verbal instructions at varied levels of abstraction and spatial specificity,
generates concrete design suggestions throughout a progressive design process.
The suggestions can be automatically applied, regenerated and retracted by
users' toggle control.Our ablation study showed effectiveness of our pipeline
components. Our user study found that, compared to baseline without showing
suggestions, EchoLadder better supports user creativity in spatial design. It
also contributes insights on users' progressive design strategies under AI
assistance, providing design implications for future systems.

</details>


### [83] [Data Augmentation for Visualization Design Knowledge Bases](https://arxiv.org/abs/2508.02216)
*Hyeok Kim,Jeffrey Heer*

Main category: cs.HC

TL;DR: 该论文提出数据增强方法，通过生成和标注图表对，扩展可视化知识库的覆盖范围和准确性，并对比不同标注方法以优化特征权重学习。


<details>
  <summary>Details</summary>
Motivation: 现有图表对数据集不全面，缺乏多样性和系统性的设计变体评估，影响了可视化知识库的构建和推荐性能。

Method: 通过设计排列和识别未充分评估的特征生成新图表对，并比较不同标注方法以高效标注这些新数据。

Result: 方法在Draco知识库中验证，显著提升了特征覆盖和图表推荐性能。

Conclusion: 数据增强和高效标注技术能够有效扩展知识库，提高可视化设计推荐的准确性和全面性。

Abstract: Visualization knowledge bases enable computational reasoning and
recommendation over a visualization design space. These systems evaluate design
trade-offs using numeric weights assigned to different features (e.g., binning
a variable). Feature weights can be learned automatically by fitting a model to
a collection of chart pairs, in which one chart is deemed preferable to the
other. To date, labeled chart pairs have been drawn from published empirical
research results; however, such pairs are not comprehensive, resulting in a
training corpus that lacks many design variants and fails to systematically
assess potential trade-offs. To improve knowledge base coverage and accuracy,
we contribute data augmentation techniques for generating and labeling chart
pairs. We present methods to generate novel chart pairs based on design
permutations and by identifying under-assessed features -- leading to an
expanded corpus with thousands of new chart pairs, now in need of labels.
Accordingly, we next compare varied methods to scale labeling efforts to
annotate chart pairs, in order to learn updated feature weights. We evaluate
our methods in the context of the Draco knowledge base, demonstrating
improvements to both feature coverage and chart recommendation performance.

</details>


### [84] [Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults](https://arxiv.org/abs/2508.02232)
*Lei Han,Mingnan Wei,Qiongyan Chen,Anqi Wang,Rong Pang,Kefei Liu,Rongrong Chen,David Yip*

Main category: cs.HC

TL;DR: 论文探讨了结合凝视和语音的多模态交互在基于照片的回忆技术中的应用，开发了Eye2Recall系统，并通过用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过技术手段帮助老年人更自然地回忆个人历史，提升其幸福感，填补了多模态交互在回忆技术中的研究空白。

Method: 通过专家访谈了解老年人需求，开发结合眼动追踪和自然语言交互的Eye2Recall系统，并进行10名老年人的用户研究。

Result: 用户研究表明，Eye2Recall系统能有效支持老年人的自然交互模式，为未来回忆技术的设计提供了重要启示。

Conclusion: 多模态交互技术（如Eye2Recall）能提升老年人回忆体验，促进积极老龄化，未来设计需更注重自然交互模式。

Abstract: Photo-based reminiscence has the potential to have a positive impact on older
adults' reconnection with their personal history and improve their well-being.
Supporting reminiscence in older adults through technological implementations
is becoming an increasingly important area of research in the fields of HCI and
CSCW. However, the impact of integrating gaze and speech as mixed-initiative
interactions in LLM-powered reminiscence conversations remains under-explored.
To address this, we conducted expert interviews to understand the challenges
that older adults face with LLM-powered, photo-based reminiscence experiences.
Based on these design considerations, we developed Eye2Recall, a system that
integrates eye tracking for detecting visual interest with natural language
interaction to create a mixed-initiative reminiscence experience. We evaluated
its effectiveness through a user study involving ten older adults. The results
have important implications for the future design of more accessible and
empowering reminiscence technologies that better align with older adults'
natural interaction patterns and enhance their positive aging.

</details>


### [85] [mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of Arrhythmia](https://arxiv.org/abs/2508.02274)
*Arjun Kumar,Noppanat Wadlom,Jaeheon Kwak,Si-Hyuck Kang,Insik Shin*

Main category: cs.HC

TL;DR: 论文提出了mCardiacDx系统，利用雷达技术无接触监测心律失常，通过PTL技术和编码-解码模型解决信号空间和时序问题。


<details>
  <summary>Details</summary>
Motivation: 传统心律失常监测方法依赖专业医疗设备和接触式设备，用户体验差；现有无接触监测技术对心律失常患者效果不佳。

Method: 采用雷达技术，结合精确目标定位(PTL)和编码-解码模型，重构心脏脉冲波形。

Result: 在大量健康人和患者数据中，mCardiacDx和PTL性能优于现有技术。

Conclusion: mCardiacDx为无接触心律失常监测提供了高效解决方案。

Abstract: Arrhythmia is a common cardiac condition that can precipitate severe
complications without timely intervention. While continuous monitoring is
essential for timely diagnosis, conventional approaches such as
electrocardiogram and wearable devices are constrained by their reliance on
specialized medical expertise and patient discomfort from their contact nature.
Existing contactless monitoring, primarily designed for healthy subjects, face
significant challenges when analyzing reflected signals from arrhythmia
patients due to disrupted spatial stability and temporal consistency.
  In this paper, we introduce mCardiacDx, a radar-driven contactless system
that accurately analyzes reflected signals and reconstructs heart pulse
waveforms for arrhythmia monitoring and diagnosis. The key contributions of our
work include a novel precise target localization (PTL) technique that locates
reflected signals despite spatial disruptions, and an encoder-decoder model
that transforms these signals into HPWs, addressing temporal inconsistencies.
Our evaluation on a large dataset of healthy subjects and arrhythmia patients
shows that both mCardiacDx and PTL outperform state-of-the-art approach in
arrhythmia monitoring and diagnosis, also demonstrating improved performance in
healthy subjects.

</details>


### [86] [AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration](https://arxiv.org/abs/2508.02470)
*Hyunjn An,Yongwon Kim,Wonduk Seo,Joonil Park,Daye Kang,Changhoon Oh,Dokyun Kim,Seunghyun Lee*

Main category: cs.HC

TL;DR: AIAP平台通过自然语言和可视化工作流帮助非专家用户简化AI服务设计，显著提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 非专家用户在表达意图和管理系统复杂性时面临挑战，现有工具未能完全解决这些问题。

Method: AIAP采用无代码平台，结合自然语言输入和可视化工作流，利用多智能体系统将模糊指令分解为模块化步骤。

Result: 32名参与者的研究表明，AIAP的功能（如AI建议、模块化工作流）显著提升了用户直观开发服务的能力。

Conclusion: 基于自然语言的可视化编程能显著降低AI服务设计的门槛，改善用户体验。

Abstract: While many tools are available for designing AI, non-experts still face
challenges in clearly expressing their intent and managing system complexity.
We introduce AIAP, a no-code platform that integrates natural language input
with visual workflows. AIAP leverages a coordinated multi-agent system to
decompose ambiguous user instructions into modular, actionable steps, hidden
from users behind a unified interface. A user study involving 32 participants
showed that AIAP's AI-generated suggestions, modular workflows, and automatic
identification of data, actions, and context significantly improved
participants' ability to develop services intuitively. These findings highlight
that natural language-based visual programming significantly reduces barriers
and enhances user experience in AI service design.

</details>


### [87] [Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits](https://arxiv.org/abs/2508.02328)
*Raj Mahmud,Shlomo Berkovsky,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: 研究发现用户对探索式对话的偏好受愉悦感、有用性、新颖性和对话质量影响，并揭示了五种潜在用户类型。


<details>
  <summary>Details</summary>
Motivation: 探讨影响用户在对话推荐系统中交互偏好的因素。

Method: 采用内部分组研究设计，参与者接触两种脚本对话并评价体验，通过逻辑回归和聚类分析数据。

Result: 探索偏好由愉悦感等四个因素预测；发现五种用户类型；年龄、性别和偏好控制显著影响选择。

Conclusion: 研究结果为CRS用户建模和对话设计提供了情感、认知及特质层面的见解，适用于动态适应用户需求的对话系统。

Abstract: Conversational Recommender Systems (CRSs) deliver personalised
recommendations through multi-turn natural language dialogue and increasingly
support both task-oriented and exploratory interactions. Yet, the factors
shaping user interaction preferences remain underexplored. In this
within-subjects study (\(N = 139\)), participants experienced two scripted CRS
dialogues, rated their experiences, and indicated the importance of eight
system qualities. Logistic regression revealed that preference for the
exploratory interaction was predicted by enjoyment, usefulness, novelty, and
conversational quality. Unexpectedly, perceived effectiveness was also
associated with exploratory preference. Clustering uncovered five latent user
profiles with distinct dialogue style preferences. Moderation analyses
indicated that age, gender, and control preference significantly influenced
these choices. These findings integrate affective, cognitive, and trait-level
predictors into CRS user modelling and inform autonomy-sensitive,
value-adaptive dialogue design. The proposed predictive and adaptive framework
applies broadly to conversational AI systems seeking to align dynamically with
evolving user needs.

</details>


### [88] [Six Guidelines for Trustworthy, Ethical and Responsible Automation Design](https://arxiv.org/abs/2508.02371)
*Matouš Jelínek,Nadine Schlicker,Ewart de Visser*

Main category: cs.HC

TL;DR: 论文提出了六条设计准则，帮助设计者优化自动化系统的信任度评估，从而促进道德和负责任的人机交互。


<details>
  <summary>Details</summary>
Motivation: 校准用户在自动化系统中的信任至关重要，以确保其安全和无缝融入社会。用户应在系统建议正确时依赖，错误时拒绝。

Method: 结合人机交互、认知心理学、自动化研究、用户体验设计和伦理学等多领域文献，并引入语用学中的共同基础和格赖斯沟通准则。

Result: 提出的准则为设计者提供了可操作的见解，以创建显示相关信任度线索的系统，从而培养校准信任。

Conclusion: 这些准则不仅有助于设计新系统，还能评估现有系统是否支持用户准确判断信任度。

Abstract: Calibrated trust in automated systems (Lee and See 2004) is critical for
their safe and seamless integration into society. Users should only rely on a
system recommendation when it is actually correct and reject it when it is
factually wrong. One requirement to achieve this goal is an accurate
trustworthiness assessment, ensuring that the user's perception of the system's
trustworthiness aligns with its actual trustworthiness, allowing users to make
informed decisions about the extent to which they can rely on the system
(Schlicker et al. 2022). We propose six design guidelines to help designers
optimize for accurate trustworthiness assessments, thus fostering ethical and
responsible human-automation interactions. The proposed guidelines are derived
from existing literature in various fields, such as human-computer interaction,
cognitive psychology, automation research, user-experience design, and ethics.
We are incorporating key principles from the field of pragmatics, specifically
the cultivation of common ground (H. H. Clark 1996) and Gricean communication
maxims (Grice 1975). These principles are essential for the design of automated
systems because the user's perception of the system's trustworthiness is shaped
by both environmental contexts, such as organizational culture or societal
norms, and by situational context, including the specific circumstances or
scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed
guidelines provide actionable insights for designers to create automated
systems that make relevant trustworthiness cues available. This would ideally
foster calibrated trust and more satisfactory, productive, and safe
interactions between humans and automated systems. Furthermore, the proposed
heuristics might work as a tool for evaluating to what extent existing systems
enable users to accurately assess a system's trustworthiness.

</details>


### [89] [Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction](https://arxiv.org/abs/2508.02376)
*Matus Krajcovic,Peter Demcak,Eduard Kuric*

Main category: cs.HC

TL;DR: 研究探讨了具身对话代理（ECA）在在线调查中对参与者参与度、满意度和回答质量的影响，发现ECA能提供更详细的信息和高效的参与度。


<details>
  <summary>Details</summary>
Motivation: 解决在线调查中因缺乏人际互动和责任感导致的粗心回答和满意化问题。

Method: 通过分层随机抽样对80名英国参与者进行两组对比实验，一组与具身代理互动，另一组与聊天机器人互动，收集了2265条对话回复。

Result: 具身代理显著提高了回答的详细度和参与效率，但对满意度无显著影响，原因包括个人偏好和‘恐怖谷’效应。

Conclusion: 支持开发更接近人际互动的ECA，以提升在线调查的自然性和效果。

Abstract: Embodied conversational agents (ECAs) are increasingly more realistic and
capable of dynamic conversations. In online surveys, anthropomorphic agents
could help address issues like careless responding and satisficing, which
originate from the lack of personal engagement and perceived accountability.
However, there is a lack of understanding of how ECAs in user experience
research may affect participant engagement, satisfaction, and the quality of
responses. As a proof of concept, we propose an instrument that enables the
incorporation of conversations with a virtual avatar into surveys, using on
AI-driven video generation, speech recognition, and Large Language Models. In
our between-subjects study, 80 participants (UK, stratified random sample of
general population) either talked to a voice-based agent with an animated video
avatar, or interacted with a chatbot. Across surveys based on two self-reported
psychometric tests, 2,265 conversation responses were obtained. Statistical
comparison of results indicates that embodied agents can contribute
significantly to more informative, detailed responses, as well as higher yet
more time-efficient engagement. Furthermore, qualitative analysis provides
valuable insights for causes of no significant change to satisfaction, linked
to personal preferences, turn-taking delays and Uncanny Valley reactions. These
findings support the pursuit and development of new methods toward human-like
agents for the transformation of online surveys into more natural interactions
resembling in-person interviews.

</details>


### [90] [Improving Knowledge Graph Understanding with Contextual Views](https://arxiv.org/abs/2508.02413)
*Antrea Christou,Cogan Shimizu*

Main category: cs.HC

TL;DR: 摘要介绍了Interactive Knowledge (InK) Browser工具，利用知识图谱中的本体信息改进图的导航、可视化和发现过程，并通过用户调查评估其效果。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）由于其高度复杂的连接关系，使得导航、可视化和发现过程变得困难。本体信息可以提供约束关系，从而改进这些过程。

Method: 引入Interactive Knowledge (InK) Browser工具，利用本体信息提供模块化视图，包括交互式模式视图、基于类型的数据列表、邻域连接和地理空间展示。

Result: 通过用户调查评估工具的基本前提，证明其能够改进知识图谱的探索过程。

Conclusion: 灵活视图的工具（如InK Browser）可以简化知识图谱的探索，适用于多种应用场景。

Abstract: Navigating, visualizing, and discovery in graph data is frequently a
difficult prospect. This is especially true for knowledge graphs (KGs), due to
high number of possible labeled connections to other data.
  However, KGs are frequently equipped with an ontology as a schema. That is,
it informs how the relationships between data may be constrained. This
additional information can be leveraged to improve how (knowledge) graph data
can be navigated, visualized, or otherwise utilized in a discovery process.
  In this manuscript, we introduce the Interactive Knowledge (InK) Browser.
This tool specifically takes advantage ontological information (i.e.,
knowledge) when found in KGs. Specifically, we use modular views that provide
various perspectives over the graph, including an interactive schema view, data
listings based on type, neighborhood connections, and geospatial depiction
(where appropriate). For this manuscript, we have evaluated the basic premise
of this tool over a user group ($n= With this grown user survey, we continue to
evaluate how scalable tools, including flexible views, can make KG exploration
easier for a range of applications.)

</details>


### [91] [Stakeholder Perspectives on Humanistic Implementation of Computer Perception in Healthcare: A Qualitative Study](https://arxiv.org/abs/2508.02550)
*Kristin M. Kostick-Quenet,Meghan E. Hurley,Syed Ayaz,John Herrington,Casey Zampella,Julia Parish-Morris,Birkan Tunç,Gabriel Lázaro-Muñoz,J. S. Blumenthal-Barby,Eric A. Storch*

Main category: cs.HC

TL;DR: 研究分析了计算机感知技术在医疗中的应用,探讨了利益相关者的七大关注领域,并提出“个性化路线图”作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机感知技术为个性化医疗提供了机会,但其带来的隐私和伦理问题尚未被全面理解,亟需利益相关者的视角来解决这些挑战。

Method: 通过102位利益相关者的半结构化访谈,采用多学科团队的主题分析,并验证了可靠性。

Result: 确定了七大关键关注领域,并提出“个性化路线图”框架,以平衡技术应用与人文关怀。

Conclusion: 研究为开发者、临床医生和政策制定者提供了一个实用框架,以在利用技术的同时保护医疗的人文核心。

Abstract: Computer perception (CP) technologies (digital phenotyping, affective
computing and related passive sensing approaches) offer unprecedented
opportunities to personalize healthcare, but provoke concerns about privacy,
bias and the erosion of empathic, relationship-centered practice. A
comprehensive understanding of perceived risks, benefits, and implementation
challenges from those who design, deploy and experience these tools in
real-world settings remains elusive. This study provides the first
evidence-based account of key stakeholder perspectives on the relational,
technical, and governance challenges raised by the integration of CP
technologies into patient care. We conducted in-depth, semi-structured
interviews with 102 stakeholders: adolescent patients and their caregivers,
frontline clinicians, technology developers, and ethics, legal, policy or
philosophy scholars. Transcripts underwent thematic analysis by a
multidisciplinary team; reliability was enhanced through double coding and
consensus adjudication. Stakeholders articulated seven interlocking concern
domains: (1) trustworthiness and data integrity; (2) patient-specific
relevance; (3) utility and workflow integration; (4) regulation and governance;
(5) privacy and data protection; (6) direct and indirect patient harms; and (7)
philosophical critiques of reductionism. To operationalize humanistic
safeguards, we propose "personalized roadmaps": co-designed plans that
predetermine which metrics will be monitored, how and when feedback is shared,
thresholds for clinical action, and procedures for reconciling discrepancies
between algorithmic inferences and lived experience. By translating these
insights into personalized roadmaps, we offer a practical framework for
developers, clinicians and policymakers seeking to harness continuous
behavioral data while preserving the humanistic core of care.

</details>


### [92] [Teaching Critical Visualization: A Field Report](https://arxiv.org/abs/2508.02592)
*Andrew McNutt,Shiyi He,Sujit Kumar Kamaraj,Purbid Bambroo,Nastaran Jadidi,John Bovard,Chang Han*

Main category: cs.HC

TL;DR: 本文描述了一门关于批判性可视化的实验课程，探索了课程内容结构和教学方法。课程成功培养了学生的批判性思维、复杂交流能力及相关理论知识。


<details>
  <summary>Details</summary>
Motivation: 批判性可视化在学术界日益受到关注，但相关课程较少。本文旨在通过实验课程探索这一领域的教学方法和内容结构。

Method: 课程采用探索性结构（如寻宝游戏形式），帮助学生发展批判性思维、复杂沟通能力及相关理论知识。

Result: 课程成功实现了学习目标，培养了学生的批判性思维和沟通能力，但仍有改进空间。

Conclusion: 作者希望批判性可视化教学能够更深入地融入人文主义的批判性理念。

Abstract: Critical Visualization is gaining popularity and academic focus, yet
relatively few academic courses have been offered to support students in this
complex area. This experience report describes a recent experimental course on
the topic, exploring both what the topic could be as well as an experimental
content structure (namely as scavenger hunt). Generally the course was
successful, achieving the learning objectives of developing critical thinking
skills, improving communication about complex ideas, and developing a knowledge
about theories in the area. While improvements can be made, we hope that
humanistic notions of criticality are embraced more deeply in visualization
pedagogy.

</details>


### [93] [Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition](https://arxiv.org/abs/2508.02593)
*Catalina Gomez,Lalithkumar Seenivasan,Xinrui Zou,Jeewoo Yoon,Sirui Chu,Ariel Leong,Patrick Kramer,Yu-Chun Ku,Jose L. Porras,Alejandro Martin-Gomez,Masaru Ishii,Mathias Unberath*

Main category: cs.HC

TL;DR: 摘要探讨了可解释AI（XAI）生成反馈在外科培训中的有效性，结果显示XAI反馈能改善认知负荷和信心，但与传统视频反馈在绩效差距上无显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统外科技能培训依赖专家反馈，但受限于主观性和资源不足。AI驱动的反馈可能弥补这一缺口。

Method: 通过XAI分析手术视频提取技能指标，并与专家基准比较生成个性化反馈，在医学学生中开展对比实验。

Result: XAI反馈改善了认知负荷和信心，但在绩效差距和练习调整上与传统反馈无显著差异。

Conclusion: 研究支持XAI在外科教育中的应用，建议开发数据驱动的自适应反馈机制。

Abstract: Traditional surgical skill acquisition relies heavily on expert feedback, yet
direct access is limited by faculty availability and variability in subjective
assessments. While trainees can practice independently, the lack of
personalized, objective, and quantitative feedback reduces the effectiveness of
self-directed learning. Recent advances in computer vision and machine learning
have enabled automated surgical skill assessment, demonstrating the feasibility
of automatic competency evaluation. However, it is unclear whether such
Artificial Intelligence (AI)-driven feedback can contribute to skill
acquisition. Here, we examine the effectiveness of explainable AI
(XAI)-generated feedback in surgical training through a human-AI study. We
create a simulation-based training framework that utilizes XAI to analyze
videos and extract surgical skill proxies related to primitive actions. Our
intervention provides automated, user-specific feedback by comparing trainee
performance to expert benchmarks and highlighting deviations from optimal
execution through understandable proxies for actionable guidance. In a
prospective user study with medical students, we compare the impact of
XAI-guided feedback against traditional video-based coaching on task outcomes,
cognitive load, and trainees' perceptions of AI-assisted learning. Results
showed improved cognitive load and confidence post-intervention. While no
differences emerged between the two feedback types in reducing performance gaps
or practice adjustments, trends in the XAI group revealed desirable effects
where participants more closely mimicked expert practice. This work encourages
the study of explainable AI in surgical education and the development of
data-driven, adaptive feedback mechanisms that could transform learning
experiences and competency assessment.

</details>


### [94] [PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players](https://arxiv.org/abs/2508.02610)
*Sanchita S. Kamath,Omar Khan,Anurag Choudhary,Jan Meyerhoff-Liang,Soyoung Choi,JooYoung Seo*

Main category: cs.HC

TL;DR: PunchPulse是一款针对视障人士设计的VR拳击游戏，旨在通过多感官反馈和空间导航支持，帮助他们达到中高强度运动水平。


<details>
  <summary>Details</summary>
Motivation: 现有运动方案对视障人士不友好，缺乏多感官反馈和空间导航支持，导致参与度低。

Method: 通过七个月的迭代设计，与3位视障合作者共同开发，并经过6位用户测试，采用定性和定量方法评估效果。

Result: 所有参与者均达到中高强度运动水平，游戏表现出高沉浸感和参与度。

Conclusion: VR作为包容性媒介，有望填补视障人士中高强度运动的空白。

Abstract: Blind and low-vision (BLV) individuals experience lower levels of physical
activity (PA) compared to sighted peers due to a lack of accessible, engaging
exercise options. Existing solutions often rely on auditory cues but do not
fully integrate rich sensory feedback or support spatial navigation, limiting
their effectiveness. This study introduces PunchPulse, a virtual reality (VR)
boxing exergame designed to motivate BLV users to reach and sustain moderate to
vigorous physical activity (MVPA) levels. Over a seven-month, multi-phased
study, PunchPulse was iteratively refined with three BLV co-designers, informed
by two early pilot testers, and evaluated by six additional BLV user-study
participants. Data collection included both qualitative (researcher
observations, SOPI) and quantitative (MVPA zones, aid usage, completion times)
measures of physical exertion and gameplay performance. The user study revealed
that all participants reached moderate MVPA thresholds, with high levels of
immersion and engagement observed. This work demonstrates the potential of VR
as an inclusive medium for promoting meaningful PA in the BLV community and
addresses a critical gap in accessible, intensity-driven exercise
interventions.

</details>


### [95] [Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable](https://arxiv.org/abs/2508.02639)
*Tingying He,Jason Dykes,Petra Isenberg,Tobias Isenberg*

Main category: cs.HC

TL;DR: 提出了一个新的综合理论，用于解释、探索和使用模式作为可视化中的视觉变量，解决了现有概念和术语的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 模式长期以来被用于数据编码，但现有概念和术语不一致，阻碍了其有效使用和研究。

Method: 通过跨学科文献综述，澄清了“模式”和“纹理”的模糊概念，提出了将模式作为复合视觉变量的新处理方式。

Result: 形式化了一个新的模式系统，包含三组变量：基元的空间排列、外观关系及个体基元的视觉变量。

Conclusion: 新理论扩展了模式的设计空间，并与现有可视化理论及地图学建立了联系。

Abstract: We present a new comprehensive theory for explaining, exploring, and using
pattern as a visual variable in visualization. Although patterns have long been
used for data encoding and continue to be valuable today, their conceptual
foundations are precarious: the concepts and terminology used across the
research literature and in practice are inconsistent, making it challenging to
use patterns effectively and to conduct research to inform their use. To
address this problem, we conduct a comprehensive cross-disciplinary literature
review that clarifies ambiguities around the use of "pattern" and "texture". As
a result, we offer a new consistent treatment of pattern as a composite visual
variable composed of structured groups of graphic primitives that can serve as
marks for encoding data individually and collectively. This new and widely
applicable formulation opens a sizable design space for the visual variable
pattern, which we formalize as a new system comprising three sets of variables:
the spatial arrangement of primitives, the appearance relationships among
primitives, and the retinal visual variables that characterize individual
primitives. We show how our pattern system relates to existing visualization
theory and highlight opportunities for visualization design. We further explore
patterns based on complex spatial arrangements, demonstrating explanatory power
and connecting our conceptualization to broader theory on maps and cartography.
An author version and additional materials are available on OSF: osf.io/z7ae2.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [96] [Investigating Crossing Perception in 3D Graph Visualisation](https://arxiv.org/abs/2508.00950)
*Ying Zhang,Niklas Groene,Karsten Klein,Giuseppe Liotta,Falk Schreiber*

Main category: cs.GR

TL;DR: 研究探讨了3D图绘制中边交叉及其感知对可读性的影响，并通过实证研究分析了不同因素的重要性差异。


<details>
  <summary>Details</summary>
Motivation: 探索3D图可视化中边交叉及其感知对可读性的影响，以改进质量评估和绘制算法。

Method: 通过实证研究分析3D图中边配置的感知影响，重点关注深度、边距离和相对方向等因素。

Result: 研究发现不同因素类别在可读性上存在显著差异。

Conclusion: 3D图的可读性受多种因素影响，需综合考虑边交叉及其感知效果以优化绘制质量。

Abstract: Human perception of graph drawings is influenced by a variety of impact
factors for which quality measures are used as a proxy indicator. The
investigation of those impact factors and their effects is important to
evaluate and improve quality measures and drawing algorithms. The number of
edge crossings in a 2D graph drawing has long been a main quality measure for
drawing evaluation. The use of stereoscopic 3D graph visualisations has gained
attraction over the last years, and results from several studies indicate that
they can improve analysis efficiency for a range of analysis scenarios. While
edge crossings can also occur in 3D, there are edge configurations in space
that are not crossings but might be perceived as such from a specific
viewpoint. Such configurations create crossings when projected on the
corresponding 2D image plane and could impact readability similar to 2D
crossings. In 3D drawings, the additional depth aspect and the subsequent
impact factors of edge distance and relative edge direction in space might
further influence the importance of those configurations for readability. We
investigate the impact of such factors in an empirical study and report on
findings of difference between major factor categories.

</details>


### [97] [MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh](https://arxiv.org/abs/2508.01242)
*Shuangkang Fang,I-Chao Shen,Yufeng Wang,Yi-Hsuan Tsai,Yi Yang,Shuchang Zhou,Wenrui Ding,Takeo Igarashi,Ming-Hsuan Yang*

Main category: cs.GR

TL;DR: MeshLLM是一个利用大型语言模型（LLM）理解和生成3D网格文本序列的新框架，通过Primitive-Mesh分解策略解决数据规模和结构信息丢失问题。实验表明其在网格生成质量和形状理解上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理文本序列化3D网格时存在数据集规模有限和结构信息丢失的问题，MeshLLM旨在解决这些挑战。

Method: 采用Primitive-Mesh分解策略将3D网格划分为有结构意义的子单元，并提出推断面连接性和局部网格组装的训练策略。

Result: 构建了一个1500k+样本的大规模数据集，网格生成质量和形状理解性能优于LLaMA-Mesh。

Conclusion: MeshLLM展示了在处理文本序列化3D网格方面的巨大潜力。

Abstract: We present MeshLLM, a novel framework that leverages large language models
(LLMs) to understand and generate text-serialized 3D meshes. Our approach
addresses key limitations in existing methods, including the limited dataset
scale when catering to LLMs' token length and the loss of 3D structural
information during mesh serialization. We introduce a Primitive-Mesh
decomposition strategy, which divides 3D meshes into structurally meaningful
subunits. This enables the creation of a large-scale dataset with 1500k+
samples, almost 50 times larger than previous methods, which aligns better with
the LLM scaling law principles. Furthermore, we propose inferring face
connectivity from vertices and local mesh assembly training strategies,
significantly enhancing the LLMs' ability to capture mesh topology and spatial
structures. Experiments show that MeshLLM outperforms the state-of-the-art
LLaMA-Mesh in both mesh generation quality and shape understanding,
highlighting its great potential in processing text-serialized 3D meshes.

</details>


### [98] [ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers](https://arxiv.org/abs/2508.01381)
*Onat Vuran,Hsuan-I Ho*

Main category: cs.GR

TL;DR: 论文提出了一种名为ReMu的方法，通过单目RGB相机抓取图像层，重建多层3D服装，无需模板且适用于多种服装风格。


<details>
  <summary>Details</summary>
Motivation: 为了支持逼真的穿衣人体化身的创建，避免昂贵的多视角捕捉设备和专业3D编辑需求。

Method: 通过共享坐标系重建和对齐每一层服装，并采用碰撞感知优化和隐式神经场细化边界。

Result: 实验表明，该方法重建了近乎无穿透的3D穿衣人体，性能与类别特定方法相当。

Conclusion: ReMu方法为重建多层3D服装提供了一种高效且通用的解决方案。

Abstract: The reconstruction of multi-layer 3D garments typically requires expensive
multi-view capture setups and specialized 3D editing efforts. To support the
creation of life-like clothed human avatars, we introduce ReMu for
reconstructing multi-layer clothed humans in a new setup, Image Layers, which
captures a subject wearing different layers of clothing with a single RGB
camera. To reconstruct physically plausible multi-layer 3D garments, a unified
3D representation is necessary to model these garments in a layered manner.
Thus, we first reconstruct and align each garment layer in a shared coordinate
system defined by the canonical body pose. Afterwards, we introduce a
collision-aware optimization process to address interpenetration and further
refine the garment boundaries leveraging implicit neural fields. It is worth
noting that our method is template-free and category-agnostic, which enables
the reconstruction of 3D garments in diverse clothing styles. Through our
experiments, we show that our method reconstructs nearly penetration-free 3D
clothed humans and achieves competitive performance compared to
category-specific methods. Project page: https://eth-ait.github.io/ReMu/

</details>


### [99] [A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation](https://arxiv.org/abs/2508.01590)
*Hua Yu,Jiao Liu,Xu Gui,Melvin Wong,Yaqing Hou,Yew-Soon Ong*

Main category: cs.GR

TL;DR: MCG-IMM是一种用于生成人类动作过渡序列的新方法，通过多准则优化提高预训练模型的多样性，无需额外参数。


<details>
  <summary>Details</summary>
Motivation: 解决人类动作过渡生成中多样性和平滑性的挑战，特别是批量生成时动作间的差异性。

Method: 利用多准则优化将预训练生成模型的采样过程重新表述，探索满足多样性和平滑性的动作序列。

Result: 在四个数据集上，MCG-IMM表现优于现有方法。

Conclusion: MCG-IMM是一种即插即用的方法，能与多种生成模型兼容，有效提升动作生成的多样性和质量。

Abstract: In-betweening human motion generation aims to synthesize intermediate motions
that transition between user-specified keyframes. In addition to maintaining
smooth transitions, a crucial requirement of this task is to generate diverse
motion sequences. It is still challenging to maintain diversity, particularly
when it is necessary for the motions within a generated batch sampling to
differ meaningfully from one another due to complex motion dynamics. In this
paper, we propose a novel method, termed the Multi-Criteria Guidance with
In-Betweening Motion Model (MCG-IMM), for in-betweening human motion
generation. A key strength of MCG-IMM lies in its plug-and-play nature: it
enhances the diversity of motions generated by pretrained models without
introducing additional parameters This is achieved by providing a sampling
process of pretrained generative models with multi-criteria guidance.
Specifically, MCG-IMM reformulates the sampling process of pretrained
generative model as a multi-criteria optimization problem, and introduces an
optimization process to explore motion sequences that satisfy multiple
criteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play
multi-criteria guidance is compatible with different families of generative
models, including denoised diffusion probabilistic models, variational
autoencoders, and generative adversarial networks. Experiments on four popular
human motion datasets demonstrate that MCG-IMM consistently state-of-the-art
methods in in-betweening motion generation task.

</details>


### [100] [Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility](https://arxiv.org/abs/2508.02443)
*Thomas Gottwald,Edgar Heinert,Matthias Rottmann*

Main category: cs.GR

TL;DR: 提出了一种新的高斯泼溅不确定性估计方法，通过训练误差和可见性的投影获得不确定性信息，并在新视图中通过渲染生成不确定性特征图，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅在机器人和医学等关键应用中需要准确的不确定性估计，但现有方法通常仅估计高斯基元的方差，未能充分利用训练数据中的不确定性信息。

Method: 通过投影训练误差和可见性到基元上，建立基元的不确定性表示，渲染新视图的不确定性特征图，并通过像素级回归聚合这些特征图。

Result: 实验表明，该方法与真实误差高度相关，尤其在前景对象上优于现有方法，且回归模型具有泛化能力，可在新场景中无需保留数据。

Conclusion: 该方法有效地改进了高斯泼溅中的不确定性估计，适用于关键应用场景，并展示了在新场景中的泛化能力。

Abstract: In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [101] [Towards a quantum synapse for quantum sensing](https://arxiv.org/abs/2508.00825)
*L-F Pau*

Main category: cs.ET

TL;DR: 量子突触电路设计模拟生物神经元电突触功能，提出其在量子处理和传感系统中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 探索量子系统模拟生物神经网络的可能性，为量子处理和传感系统设计提供新思路。

Method: 基于简化动态电突触模型设计量子突触电路，并扩展至双向流动场景。

Result: 量子突触电路可作为高并行控制接口，提升传感和传感器融合系统的性能。

Conclusion: 量子突触电路在量子系统设计中具有重要潜力，但需进一步量子模拟验证。

Abstract: As a step in the architectural design of a quantum processing or sensing
system with control and signaling, an attempt is made at putting in parallel
functional properties of the random flows between neurons through electrical
synapses, and quantum particle flows inside a quantum processing system
mimicking biological processes. Based on a simplified dynamic electrical
synapse model, a quantum synapse circuit design is proposed. This is extended
to the case of bidirectional flows through a synapse, highlighting the possible
role of quantum synapse circuits as highly parallel controlled interfaces
crucial in sensing and sensor fusion systems. A short status of the quantum
simulation is provided.

</details>


### [102] [A Comparative Study of Classical and Post-Quantum Cryptographic Algorithms in the Era of Quantum Computing](https://arxiv.org/abs/2508.00832)
*Arimondo Scrivano*

Main category: cs.ET

TL;DR: 本文比较了经典密码算法与后量子密码方案，分析了量子计算对现代加密技术的威胁及应对方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算对现有加密技术构成威胁，需要研究能够抵御量子攻击的后量子密码方案。

Method: 通过对比分析经典密码算法（如RSA、ECC）和后量子密码方案（如Kyber、Dilithium），评估其安全性、性能和可行性。

Result: 后量子密码方案能有效抵御量子攻击，但需进一步优化性能和推广实施。

Conclusion: 后量子密码是未来加密技术的发展方向，需要跨领域合作以实现平滑过渡。

Abstract: The advent of quantum computing poses a significant threat to the
foundational cryptographic algorithms that secure modern digital
communications. Protocols such as HTTPS, digital certificates, and public key
infrastructures (PKIs) heavily rely on cryptographic primitives like RSA, ECC,
and Diffie-Hellman, which are vulnerable to quantum attacks -- most notably
Shor's algorithm. This paper presents a comprehensive comparative analysis
between classical cryptographic algorithms currently in widespread use and
emerging post-quantum cryptographic schemes designed to withstand quantum
adversaries. We review the cryptographic mechanisms underpinning modern
internet security, outline the mathematical foundations of quantum attacks, and
evaluate the security, performance, and implementation feasibility of
quantum-resistant alternatives such as Kyber, Dilithium, and Falcon.
Additionally, we assess the hybrid approaches currently being explored by
institutions and tech companies to enable a smooth transition to post-quantum
cryptography. By providing an in-depth comparison, this study aims to guide
researchers, developers, and policymakers in understanding the critical
implications of quantum computing on cryptographic infrastructures and the
necessary steps for securing communications in the quantum era.

</details>


### [103] [QDockBank: A Dataset for Ligand Docking on Protein Fragments Predicted on Utility-Level Quantum Computers](https://arxiv.org/abs/2508.00837)
*Yuqi Zhang,Yuxin Yang,Cheng-Chang Lu,Weiwen Jiang,Feixiong Cheng,Bo Fang,Qiang Guan*

Main category: cs.ET

TL;DR: QDockBank是首个基于量子计算机生成的大规模蛋白质片段结构数据集，旨在解决蛋白质-配体对接任务中的结构预测问题。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结构预测是计算生物学的核心挑战，尤其是配体结合区域的片段建模仍具难度。量子计算提供了一种新的建模方法，但受限于硬件和计算成本。

Method: 研究团队使用超导量子处理器生成了55个蛋白质片段的数据集，总计算成本超过100万美元。

Result: 实验显示，QDockBank预测的结构在RMSD和对接亲和力得分上优于AlphaFold2和AlphaFold3。

Conclusion: QDockBank为量子蛋白质结构预测提供了新的基准。

Abstract: Protein structure prediction is a core challenge in computational biology,
particularly for fragments within ligand-binding regions, where accurate
modeling is still difficult. Quantum computing offers a novel first-principles
modeling paradigm, but its application is currently limited by hardware
constraints, high computational cost, and the lack of a standardized
benchmarking dataset. In this work, we present QDockBank-the first large-scale
protein fragment structure dataset generated entirely using utility-level
quantum computers, specifically designed for protein-ligand docking tasks.
QDockBank comprises 55 protein fragments extracted from ligand-binding pockets.
The dataset was generated through tens of hours of execution on superconducting
quantum processors, making it the first quantum-based protein structure dataset
with a total computational cost exceeding one million USD. Experimental
evaluations demonstrate that structures predicted by QDockBank outperform those
predicted by AlphaFold2 and AlphaFold3 in terms of both RMSD and docking
affinity scores. QDockBank serves as a new benchmark for evaluating
quantum-based protein structure prediction.

</details>


### [104] [Conquering High Packet-Loss Erasure: MoE Swin Transformer-Based Video Semantic Communication](https://arxiv.org/abs/2508.01205)
*Lei Teng,Senran Fan,Chen Dong,Haotai Liang,Zhicheng Bao,Xiaodong Xu,Rui Meng,Ping Zhang*

Main category: cs.ET

TL;DR: 提出了一种基于MoE Swin Transformer的视频语义通信系统MSTVSC，通过3D CNN恢复丢失数据，减少语义信息丢失，提升了在高丢包率下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 在基于分组的语义通信系统中，分组丢弃导致的语义信息丢失问题严重影响了数据传输的鲁棒性。

Method: 采用MSTVSC编码语义向量，利用3D CNN恢复丢失信息，并引入语义级交错和共同-个体分解方法优化压缩和轻量化部署。

Result: 在90%的分组丢失率下，系统实现了MS-SSIM大于0.6和PSNR超过20 dB的优异性能。

Conclusion: MSTVSC系统有效解决了分组丢失导致的语义信息丢失问题，展现了在高丢包率下的鲁棒性和实用性。

Abstract: Semantic communication with joint semantic-channel coding robustly transmits
diverse data modalities but faces challenges in mitigating semantic information
loss due to packet drops in packet-based systems. Under current protocols,
packets with errors are discarded, preventing the receiver from utilizing
erroneous semantic data for robust decoding. To address this issue, a
packet-loss-resistant MoE Swin Transformer-based Video Semantic Communication
(MSTVSC) system is proposed in this paper. Semantic vectors are encoded by
MSTVSC and transmitted through upper-layer protocol packetization. To
investigate the impact of the packetization, a theoretical analysis of the
packetization strategy is provided. To mitigate the semantic loss caused by
packet loss, a 3D CNN at the receiver recovers missing information using
un-lost semantic data and an packet-loss mask matrix. Semantic-level
interleaving is employed to reduce concentrated semantic loss from packet
drops. To improve compression, a common-individual decomposition approach is
adopted, with downsampling applied to individual information to minimize
redundancy. The model is lightweighted for practical deployment. Extensive
simulations and comparisons demonstrate strong performance, achieving an
MS-SSIM greater than 0.6 and a PSNR exceeding 20 dB at a 90% packet loss rate.

</details>


### [105] [Managing Escalation in Off-the-Shelf Large Language Models](https://arxiv.org/abs/2508.01056)
*Sebastian Elbaum,Jonathan Panther*

Main category: cs.ET

TL;DR: 论文探讨了大语言模型在国家安全中的应用，提出了两种非技术性干预措施以减少其在地缘政治场景中的升级倾向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大语言模型在国家安全应用中可能导致的升级行为，探索如何在不限制使用的情况下对齐其与国家目标。

Method: 提出了两种简单、非技术性的干预措施，并在一项实验性战争游戏中进行了测试。

Result: 这些干预措施显著减少了游戏中的升级行为。

Conclusion: 应通过实际措施而非限制使用来管理大语言模型在国家安全中的应用，以促进其与国家目标的对齐。

Abstract: U.S. national security customers have begun to utilize large language models,
including enterprise versions of ``off-the-shelf'' models (e.g., ChatGPT)
familiar to the public. This uptake will likely accelerate. However, recent
studies suggest that off-the-shelf large language models frequently suggest
escalatory actions when prompted with geopolitical or strategic scenarios. We
demonstrate two simple, non-technical interventions to control these
tendencies. Introducing these interventions into the experimental wargame
design of a recent study, we substantially reduce escalation throughout the
game. Calls to restrict the use of large language models in national security
applications are thus premature. The U.S. government is already, and will
continue, employing large language models for scenario planning and suggesting
courses of action. Rather than warning against such applications, this study
acknowledges the imminent adoption of large language models, and provides
actionable measures to align them with national security goals, including
escalation management.

</details>


### [106] [Introduction to QUDO, Tensor QUDO and HOBO formulations: Qudits, Equivalences, Knapsack Problem, Traveling Salesman Problem and Combinatorial Games](https://arxiv.org/abs/2508.01958)
*Alejandro Mata Ali*

Main category: cs.ET

TL;DR: 本文回顾并介绍了D元二次无约束优化（QUDO）、张量D元二次无约束优化（T-QUDO）和高阶二进制无约束优化（HOBO）的组合优化问题形式，展示了它们的等价性，并通过示例帮助理解。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和应用更复杂的组合优化问题形式，尤其是为新的量子或量子启发优化算法提供基础。

Method: 通过示例（如背包问题、旅行商问题和多种组合游戏）展示QUDO、T-QUDO和HOBO的等价性和应用。

Result: 展示了这些形式在不同组合问题中的适用性和等价性，为更复杂问题的解决提供了新思路。

Conclusion: 通过具体示例展示了QUDO、T-QUDO和HOBO的实用性，为未来在量子或量子启发算法中的应用奠定了基础。

Abstract: In this paper, we present a brief review and introduction to Quadratic
Unconstrained D-ary Optimization (QUDO), Tensor Quadratic Unconstrained D-ary
Optimization (T-QUDO) and Higher-Order Unconstrained Binary Optimization (HOBO)
formulations for combinatorial optimization problems. We also show their
equivalences. To help their understanding, we make some examples for the
knapsack problem, traveling salesman problem and different combinatorial games.
The games chosen to exemplify are: Hashiwokakero, N-Queens, Kakuro, Inshi no
heya, and Peg Solitaire. Although some of these games have already been
formulated in a QUBO formulation, we are going to approach them with more
general formulations, allowing their execution in new quantum or
quantum-inspired optimization algorithms. This can be an easier way to
introduce these more complicated formulations for harder problems.

</details>


### [107] [Thermal Implications of Non-Uniform Power in BSPDN-Enabled 2.5D/3D Chiplet-based Systems-in-Package using Nanosheet Technology](https://arxiv.org/abs/2508.02284)
*Yukai Chen,Massimiliano Di Todaro,Bjorn Vermeersch,Herman Oprins,Daniele Jahier Pagliari,Julien Ryckaert,Dwaipayan Biswas,James Myers*

Main category: cs.ET

TL;DR: 本文研究了非均匀功率分布对2.5D/3D芯片系统封装（SiP）中前侧（FSPDN）和后侧（BSPDN）电源传输方法的温度影响。通过高分辨率热模拟，发现传统均匀功率假设会低估峰值温度，并掩盖BSPDN和FSPDN在3D场景中的关键热差异。结果显示，BSPDN在现实工作负载下由于横向热扩散受限而表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着纳米片技术的进步，功率密度增加，导致2.5D/3D芯片系统封装中的热管理挑战加剧。传统热分析通常使用均匀功率图简化计算，但忽视了局部加热效应，导致热估计不准确。本文旨在探讨非均匀功率分布对热管理的影响，尤其是对FSPDN和BSPDN的对比效果。

Method: 采用高分辨率热模拟技术，分辨率低至5微米，基于非均匀功率映射，分析了FSPDN和BSPDN在3D集成场景中的热效应。

Result: 结果表明，均匀功率假设会显著低估峰值温度，并无法揭示BSPDN和FSPDN在3D配置中的热差异。BSPDN在实际局部负载下由于横向热扩散有限而表现出显著的热劣势。

Conclusion: 研究强调了在早期热建模中采用细粒度、负载感知的功率映射的必要性，以实现准确的电源传输网络评估，并为先进的纳米片基3D SiP设计提供热感知决策支持。

Abstract: Advances in nanosheet technologies have significantly increased power
densities, exacerbating thermal management challenges in 2.5D/3D chiplet-based
Systems-in-Package (SiP). While traditional thermal analyses often employ
uniform power maps to simplify computational complexity, this practice neglects
localized heating effects, leading to inaccuracies in thermal estimations,
especially when comparing power delivery networks (PDN) in 3D integration. This
work examines the thermal impact of non-uniform power distributions on SiPs
utilizing frontside (FSPDN) and backside (BSPDN) power delivery approaches.
Using high-resolution thermal simulations with non-uniform power maps at
resolutions down to 5 micrometers, we demonstrate that uniform power
assumptions substantially underestimate peak temperatures and fail to reveal
critical thermal differences between BSPDN and FSPDN configurations in 3D
scenarios. Our results highlight that BSPDN configurations in 3D, although
beneficial in simplified uniform scenarios, exhibit pronounced thermal
penalties under realistic, localized workloads due to limited lateral heat
spreading. These findings emphasize the necessity of adopting fine-grained,
workload-aware power maps in early-stage thermal modeling to enable accurate
PDN assessment and informed thermal-aware design decisions in advanced
nanosheet-based 3D SiP.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [108] [Deterministic Fault-Tolerant Local Load Balancing and its Applications against Adaptive Adversaries](https://arxiv.org/abs/2508.01373)
*Dariusz R. Kowalski,Jan Olkowski*

Main category: cs.DC

TL;DR: 论文研究了在节点易失效的网络中实现本地负载均衡的问题，并提出了一种新的确定性容错算法。该算法能够在存在故障的情况下快速收敛到平均值，并展示了其在共识算法中的应用。


<details>
  <summary>Details</summary>
Motivation: 分布式计算中的负载均衡是一个基本问题，但在节点易失效的网络中如何实现高效的本地负载均衡尚未充分解决。本文旨在设计一种容错性强的本地负载均衡算法，并探索其在共识问题中的应用。

Method: 提出了一种新型确定性容错本地负载均衡（FTLLB）算法，能够容忍最多恒定比例的节点失效。算法结构简单，具有快速收敛性。此外，结合随机选择的虚拟通信图和FTLLB，设计了一种改进的随机共识算法和新的省略故障下的共识解决方案。

Result: FTLLB算法在存在故障（崩溃和省略）的情况下能够快速收敛到平均值。共识算法的通信复杂度优于现有方案，且在省略故障下的共识方案在时间复杂度和通信复杂度上均接近最优。

Conclusion: 本文提出的FTLLB算法及其在共识问题中的应用在容错性和效率方面表现优异，填补了现有技术的空白，为分布式系统设计提供了新思路。

Abstract: Load balancing is among the basic primitives in distributed computing. In
this paper, we consider this problem when executed locally on a network with
nodes prone to failures. We show that there exist lightweight network
topologies that are immune to message delivery failures incurred by (at most) a
constant fraction of all nodes. More precisely, we design a novel deterministic
fault-tolerant local load balancing (LLB) algorithm, which, similarly to their
classical counterparts working in fault-free networks, has a relatively simple
structure and guarantees exponentially fast convergence to the average value
despite crash and omission failures.
  As the second part of our contribution, we show three applications of the
newly developed fault-tolerant local load balancing protocol. We give a
randomized consensus algorithm, working against $t < n / 3$ crash failures,
that improves over the best-known consensus solution by Hajiaghayi et al. with
respect to communication complexity, yet with an arguable simpler technique of
combining a randomly and locally selected virtual communication graph with a
deterministic fault-tolerant local load balancing on this graph.
  We also give a new solution for consensus for networks with omission
failures. Our solution works against $t < \frac{n}{C\log{n} (\log\log n)^2}$
omissions, for some constant $C$, is nearly optimal in terms of time
complexity, but most notably -- it has communication complexity $O((t^2 +
n)\text{ polylog } {n})$, matching, within a polylogarithmic factor, the lower
bound by Abraham et. al. with respect to both terms depending on $t$ and $n$.
Ours is the first algorithm in the literature that is simultaneously nearly
optimal, in terms of $n,t$, with respect to both complexity measures, against
the adaptive omission-causing adversary.

</details>


### [109] [An Analysis of HPC and Edge Architectures in the Cloud](https://arxiv.org/abs/2508.01494)
*Steven Santillan,Cristina L. Abad*

Main category: cs.DC

TL;DR: 分析了396个真实AWS云架构中HPC和边缘组件，探讨其设计特点、AWS服务交互、存储系统、复杂性及ML服务使用。


<details>
  <summary>Details</summary>
Motivation: 研究当前行业在云中构建HPC和边缘架构的实际做法及趋势，为研究和实践提供指导。

Method: 从数据集筛选含HPC或边缘组件的架构，分析其AWS服务、存储系统、复杂性及ML服务。

Result: 揭示了HPC和边缘架构的设计特点、服务交互及行业实践，提供了代表性评估。

Conclusion: 研究结果对理解行业实践、指导未来研究和构建稳健云架构具有重要价值。

Abstract: We analyze a recently published dataset of 396 real-world cloud architectures
deployed on AWS, from companies belonging to a wide range of industries. From
this dataset, we identify those architectures that contain HPC or edge
components and characterize their designs. Specifically, we investigate the
prevalence and interplay of AWS services within these architectures, examine
the types of storage systems employed, assess architectural complexity and the
use of machine learning services, discuss the implications of our findings and
how representative these results are of HPC and edge architectures in the
cloud. This characterization provides valuable insights into current industry
practices and trends in building robust and scalable HPC and edge solutions in
the cloud continuum, and can be valuable for those seeking to better understand
how these architectures are being built and to guide new research.

</details>


### [110] [Faster Distributed $Δ$-Coloring via a Reduction to MIS](https://arxiv.org/abs/2508.01762)
*Yann Bourreau,Sebastian Brandt,Alexandre Nolin*

Main category: cs.DC

TL;DR: 该论文展示了确定性Δ-着色问题在LOCAL模型下的复杂度可以从现有的O(log^{19/9} n)降至O(log^{5/3} n)轮，与目前(Δ+1)-着色的最佳上界匹配。通过将Δ-着色归约为MIS问题，实现了这一结果。


<details>
  <summary>Details</summary>
Motivation: 提升Δ-着色问题的确定性复杂度，使其与(Δ+1)-着色问题的最新上界一致，并探索其与MIS问题的关系。

Method: 开发了一个从Δ-着色到MIS问题的归约方法，证明了Δ-着色的复杂度不会超过MIS的复杂度，除非MIS能在亚对数时间内解决。

Result: Δ-着色问题的复杂度降至O(log^{5/3} n)，同时在随机LOCAL模型和参数化复杂度下也有所改进。

Conclusion: 任何对MIS复杂度的改进都将直接提升Δ-着色问题的复杂度，使其达到其真正复杂度的上限。

Abstract: Recent improvements on the deterministic complexities of fundamental graph
problems in the LOCAL model of distributed computing have yielded
state-of-the-art upper bounds of $\tilde{O}(\log^{5/3} n)$ rounds for maximal
independent set (MIS) and $(\Delta + 1)$-coloring [Ghaffari, Grunau, FOCS'24]
and $\tilde{O}(\log^{19/9} n)$ rounds for the more restrictive
$\Delta$-coloring problem [Ghaffari, Kuhn, FOCS'21; Ghaffari, Grunau, FOCS'24;
Bourreau, Brandt, Nolin, STOC'25]. In our work, we show that $\Delta$-coloring
can be solved deterministically in $\tilde{O}(\log^{5/3} n)$ rounds as well,
matching the currently best bound for $(\Delta + 1)$-coloring.
  We achieve our result by developing a reduction from $\Delta$-coloring to MIS
that guarantees that the (asymptotic) complexity of $\Delta$-coloring is at
most the complexity of MIS, unless MIS can be solved in sublogarithmic time, in
which case, due to the $\Omega(\log n)$-round $\Delta$-coloring lower bound
from [BFHKLRSU, STOC'16], our reduction implies a tight complexity of
$\Theta(\log n)$ for $\Delta$-coloring. In particular, any improvement on the
complexity of the MIS problem will yield the same improvement for the
complexity of $\Delta$-coloring (up to the true complexity of
$\Delta$-coloring).
  Our reduction yields improvements for $\Delta$-coloring in the randomized
LOCAL model and when complexities are parameterized by both $n$ and $\Delta$.
We obtain a randomized complexity bound of $\tilde{O}(\log^{5/3} \log n)$
rounds (improving over the state of the art of $\tilde{O}(\log^{8/3} \log n)$
rounds) on general graphs and tight complexities of $\Theta(\log n)$ and
$\Theta(\log \log n)$ for the deterministic, resp.\ randomized, complexity on
bounded-degree graphs. In the special case of graphs of constant clique number
(which for instance include bipartite graphs), we also give a reduction to the
$(\Delta+1)$-coloring problem.

</details>


### [111] [Efficient Byzantine Consensus MechanismBased on Reputation in IoT Blockchain](https://arxiv.org/abs/2508.01856)
*Xu Yuan,Fang Luo,Muhammad Zeeshan Haider,Zhikui Chen,Yucheng Li*

Main category: cs.DC

TL;DR: 该论文提出了一种高效的拜占庭信誉共识机制（EBRC），以解决区块链在物联网应用中存储、功率和计算能力受限的问题，并通过实验证明其优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 区块链技术在物联网网络中显示出巨大潜力，但现有共识算法存在节点可靠性低、吞吐量不足和可扩展性问题，需要改进。

Method: 提出EBRC机制，重新设计节点可靠性和鲁棒性评估方法，并优化活动节点管理。

Result: EBRC在共识延迟、吞吐量、安全性和验证成本方面优于传统算法。

Conclusion: EBRC为物联网+区块链+互联网法院构建问题提供了新的解决方案参考。

Abstract: Blockchain technology has advanced rapidly in recent years and is now widely
used in a variety of fields. Blockchain appears to be one of the best solutions
for managing massive heterogeneous devices while achieving advanced data
security and data reputation, particularly in the field of large-scale IoT
(Internet of Things) networks. Despite the numerous advantages, there are still
challenges while deploying IoT applications on blockchain systems due to the
limited storage, power, and computing capability of IoT devices, and some of
these problems are caused by the consensus algorithm, which plays a significant
role in blockchain systems by ensuring overall system reliability and
robustness. Nonetheless, most existing consensus algorithms are prone to poor
node reliability, low transaction per second (TPS) rates, and scalability
issues. Aiming at some critical problems in the existing consensus algorithms,
this paper proposes the Efficient Byzantine Reputation-based Consensus (EBRC)
mechanism to resolve the issues raised above. In comparison to traditional
algorithms, we reinvented ways to evaluate node reliability and robustness and
manage active nodes. Our experiments show that the EBRC algorithm has lower
consensus delay, higher throughput, improved security, and lower verification
costs. It offers new reference ideas for solving the Internet of
Things+blockchain+Internet court construction problem.

</details>


### [112] [Machine Learning-Driven Performance Analysis of Compressed Communication in Aerial-RIS Networks for Future 6G Networks](https://arxiv.org/abs/2508.01911)
*Muhammad Farhan Khan,Muhammad Ahmed Mohsin,Zeeshan Alam,Muhammad Saad,Muhammad Waqar*

Main category: cs.DC

TL;DR: 本文提出了一种结合无人机辅助可重构智能表面、非正交多址和协作多点传输的新系统模型，以提升6G网络的数据速率和系统容量，并通过机器学习技术优化反馈机制。


<details>
  <summary>Details</summary>
Motivation: 解决密集城市环境中未来6G网络带宽不足和容量受限的问题。

Method: 集成无人机辅助RIS、NOMA和CoMP技术，提出基于机器学习的自动编码器来压缩反馈信息。

Result: 显著提高了频谱效率、降低了中断概率，并优化了带宽利用率。

Conclusion: 该架构在未来6G网络中具有显著提升性能的潜力。

Abstract: In the future 6G and wireless networks, particularly in dense urban
environments, bandwidth exhaustion and limited capacity pose significant
challenges to enhancing data rates. We introduce a novel system model designed
to improve the data rate of users in next-generation multi-cell networks by
integrating Unmanned Aerial Vehicle (UAV)-Assisted Reconfigurable Intelligent
Surfaces (RIS), Non-Orthogonal Multiple Access (NOMA), and Coordinated
Multipoint Transmission (CoMP). Optimally deploying Aerial RIS for higher data
rates, employing NOMA to improve spectral efficiency, and utilizing CoMP to
mitigate inter-cell interference (ICI), we significantly enhance the overall
system capacity and sum rate. Furthermore, we address the challenge of feedback
overhead associated with Quantized Phase Shifts (QPS) from the receiver to RIS.
The feedback channel is band-limited and cannot support a large overhead of QPS
for uplink communication. To ensure seamless transmission, we propose a Machine
Learning Autoencoder technique for a compressed communication of QPS from the
receiver to RIS, while maintaining high accuracy. Additionally, we investigate
the impact of the number of Aerial RIS elements and power allocation ratio for
NOMA on the individual data rate of users. Our simulation results demonstrate
substantial improvements in spectral efficiency, outage probability, and
bandwidth utilization, highlighting the potential of the proposed architecture
to enhance network performance.

</details>


### [113] [Prefill-Decode Aggregation or Disaggregation? Unifying Both for Goodput-Optimized LLM Serving](https://arxiv.org/abs/2508.01989)
*Chao Wang,Pengfei Zuo,Zhangyu Chen,Yunkai Liang,Zhou Yu,Ming-Chang Yang*

Main category: cs.DC

TL;DR: 比较了两种LLM服务方法（预填充-解码聚合与分解），提出TaiChi系统统一两者，优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决预填充-解码聚合与分解在特定SLO下性能不足的问题。

Method: 提出TaiChi系统，结合两种方法，通过动态调整资源分配适应不同SLO。

Result: 在平衡SLO下，TaiChi性能提升77%。

Conclusion: TaiChi通过统一架构和动态调度，显著优化LLM服务性能。

Abstract: An ongoing debate considers whether prefill-decode (PD) aggregation or
disaggregation is superior for serving large language models (LLMs). This has
driven optimizations for both approaches, each showing distinct advantages.
This paper compares PD aggregation and disaggregation, showing that each excels
under different service-level objectives (SLOs): aggregation is optimal for
tight time-to-first-token (TTFT) and relaxed time-per-output-token (TPOT),
while disaggregation excels for strict TPOT and relaxed TTFT. However, under
balanced TTFT and TPOT SLOs, neither approach delivers optimal goodput.
  This paper proposes TaiChi, an LLM serving system that unifies PD
disaggregation and aggregation for optimal goodput under any combination of
TTFT and TPOT SLOs. TaiChi uses a unified disaggregation-aggregation
architecture with differentiated-capability GPU instances: prefill-heavy (fast
prefill, high-interference decode) and decode-heavy (low-interference decode,
slow prefill). Three configurable sliders control the ratio between these
instances and their chunk sizes. TaiChi adapts to various SLO regimes by
adjusting sliders. When TTFT constraints are tight, TaiChi resembles a PD
aggregation configuration; when TPOT dominates, it adapts toward PD
disaggregation. Crucially, under balanced SLOs, TaiChi enables a hybrid mode
for superior goodput. The key innovation behind this hybrid mode is latency
shifting: selectively reallocating GPU resources from requests that meet SLOs
to those at risk of violation, maximizing the number of SLO-satisfied requests.
This fine-grained latency shifting is orchestrated by two scheduling
mechanisms: flowing decode scheduling to control TPOTs and length-aware prefill
scheduling to manage TTFTs, which jointly optimize request assignment. Our
experiments show TaiChi improves goodput by up to 77% over state-of-the-art
systems under balanced TTFT and TPOT SLOs.

</details>


### [114] [DySTop](https://arxiv.org/abs/2508.01996)
*Yizhou Shi,Qianpiao Ma,Yan Xu,Junlong Zhou,Ming Hu,Yunming Liao,Hongli Xu*

Main category: cs.DC

TL;DR: DySTop是一个创新的ADFL机制，通过动态控制陈旧性和优化拓扑结构，显著提高了训练效率和降低了通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有的DFL机制在异构和动态边缘环境下效率低下，异步DFL（ADFL）又存在模型陈旧和通信开销大的问题。

Method: DySTop结合动态陈旧性控制和拓扑结构优化，通过激活部分节点和选择邻居传输模型来聚合和本地训练。

Result: DySTop在完成时间和通信资源消耗上分别减少了51.8%和57.1%，同时保持模型精度不变。

Conclusion: DySTop通过优化AFL机制，显著提升了训练效率和资源利用率，适用于非IID数据和动态边缘环境。

Abstract: Federated Learning (FL) has emerged as a potential distributed learning
paradigm that enables model training on edge devices (i.e., workers) while
preserving data privacy. However, its reliance on a centralized server leads to
limited scalability. Decentralized federated learning (DFL) eliminates the
dependency on a centralized server by enabling peer-to-peer model exchange.
Existing DFL mechanisms mainly employ synchronous communication, which may
result in training inefficiencies under heterogeneous and dynamic edge
environments. Although a few recent asynchronous DFL (ADFL) mechanisms have
been proposed to address these issues, they typically yield stale model
aggregation and frequent model transmission, leading to degraded training
performance on non-IID data and high communication overhead. To overcome these
issues, we present DySTop, an innovative mechanism that jointly optimizes
dynamic staleness control and topology construction in ADFL. In each round,
multiple workers are activated, and a subset of their neighbors is selected to
transmit models for aggregation, followed by local training. We provide a
rigorous convergence analysis for DySTop, theoretically revealing the
quantitative relationships between the convergence bound and key factors such
as maximum staleness, activating frequency, and data distribution among
workers. From the insights of the analysis, we propose a worker activation
algorithm (WAA) for staleness control and a phase-aware topology construction
algorithm (PTCA) to reduce communication overhead and handle data non-IID.
Extensive evaluations through both large-scale simulations and real-world
testbed experiments demonstrate that our DySTop reduces completion time by
51.8% and the communication resource consumption by 57.1% compared to
state-of-the-art solutions, while maintaining the same model accuracy.

</details>


### [115] [Self-assessment approach for resource management protocols in heterogeneous computational systems](https://arxiv.org/abs/2508.02202)
*Rui Eduardo Lopes,Duarte Raposo,Pedro V. Teixeira,Susana Sargento*

Main category: cs.DC

TL;DR: 本文提出了一种基于启发式的资源估计解决方案，支持任何计算系统的自评估，包括动态加权需求和扩展资源类型列表的能力。


<details>
  <summary>Details</summary>
Motivation: 随着异构应用服务和计算系统的增多，资源管理问题日益重要。当前解决方案通常局限于预定义的资源类型，缺乏扩展性和灵活性。

Method: 采用启发式算法，动态加权需求，计算节点容量，并支持资源类型的扩展。

Result: 该算法在资源估计中表现直接，同时具有良好的可扩展性和扩展性。

Conclusion: 该方案为分布式和集中式资源分配协议提供了决策支持，适用于服务部署。

Abstract: With an ever growing number of heterogeneous applicational services running
on equally heterogeneous computational systems, the problem of resource
management becomes more essential. Although current solutions consider some
network and time requirements, they mostly handle a pre-defined list of
resource types by design and, consequently, fail to provide an extensible
solution to assess any other set of requirements or to switch strategies on its
resource estimation. This work proposes an heuristics-based estimation solution
to support any computational system as a self-assessment, including
considerations on dynamically weighting the requirements, how to compute each
node's capacity towards an admission request, and also offers the possibility
to extend the list of resource types considered for assessment, which is an
uncommon view in related works. This algorithm can be used by distributed and
centralized resource allocation protocols to decide the best node(s) for a
service intended for deployment. This approach was validated across its
components and the results show that its performance is straightforward in
resource estimation while allowing scalability and extensibility.

</details>


### [116] [FedAPTA: Federated Multi-task Learning in Computing Power Networks with Adaptive Layer-wise Pruning and Task-aware Aggregation](https://arxiv.org/abs/2508.02230)
*Yachao Yuan,Zhen Yu,Jin Wang,Zhipeng Cheng,Jianhua Hu*

Main category: cs.DC

TL;DR: 论文提出了FedAPTA框架，用于解决联邦学习在计算网络中的多任务部署问题，通过层间模型修剪和任务感知模型聚合方法，显著提升了资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在多任务部署中面临计算资源浪费和设备异构性挑战，现有工作未能充分解决这些问题。

Method: FedAPTA采用层间模型修剪技术减少本地模型大小，并结合异构模型恢复策略和任务感知模型聚合方法。

Result: 实验表明，FedAPTA优于现有九种先进联邦学习方法，性能提升高达4.23%。

Conclusion: FedAPTA有效缓解了多任务联邦学习中的资源浪费问题，为计算网络中的动态协作提供了实用解决方案。

Abstract: Federated Learning (FL) has shown considerable promise in Computing Power
Networks (CPNs) for privacy protection, efficient data utilization, and dynamic
collaboration. Although it offers practical benefits, applying FL in CPNs
continues to encounter a major obstacle, i.e., multi-task deployment. However,
existing work mainly focuses on mitigating FL's computation and communication
overhead of a single task while overlooking the computing resource wastage
issue of heterogeneous devices across multiple tasks in FL under CPNs. To
tackle this, we design FedAPTA, a federated multi-task learning framework in
CPNs. FedAPTA alleviates computing resource wastage through the developed
layer-wise model pruning technique, which reduces local model size while
considering both data and device heterogeneity. To aggregate structurally
heterogeneous local models of different tasks, we introduce a heterogeneous
model recovery strategy and a task-aware model aggregation method that enables
the aggregation through infilling local model architecture with the shared
global model and clustering local models according to their specific tasks. We
deploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL
methods. The experimental outcomes demonstrate that the proposed FedAPTA
considerably outperforms the state-of-the-art FL methods by up to 4.23%. Our
code is available at https://github.com/Zhenzovo/FedCPN.

</details>


### [117] [PUSHtap: PIM-based In-Memory HTAP with Unified Data Storage Format](https://arxiv.org/abs/2508.02309)
*Yilong Zhao,Mingyu Gao,Huanchen Zhang,Fangxin Liu,Gongye Chen,He Xian,Haibing Guan,Li Jiang*

Main category: cs.DC

TL;DR: PUSHtap通过结合CPU和PIM单元的二维访问方式，解决了HTAP系统中的数据格式矛盾，提升了OLAP/OLTP性能。


<details>
  <summary>Details</summary>
Motivation: HTAP系统在处理OLTP和OLAP工作负载时面临数据格式的矛盾，难以同时实现性能隔离、数据新鲜度和工作负载优化。

Method: 提出统一的数据存储格式，采用数据对齐和放置技术，优化CPU和PIM单元的带宽利用，并扩展PIM架构以支持OLAP操作和并发访问。

Result: 实验表明，PUSHtap相比基于多实例PIM的设计，OLAP/OLTP吞吐量提高了3.4倍/4.4倍。

Conclusion: PUSHtap通过创新数据访问方式和技术，有效解决了HTAP的挑战，提升了性能。

Abstract: Hybrid transaction/analytical processing (HTAP) is an emerging database
paradigm that supports both online transaction processing (OLTP) and online
analytical processing (OLAP) workloads. Computing-intensive OLTP operations,
involving row-wise data manipulation, are suitable for row-store format. In
contrast, memory-intensive OLAP operations, which are column-centric, benefit
from column-store format. This \emph{data-format dilemma} prevents HTAP systems
from concurrently achieving three design goals: performance isolation, data
freshness, and workload-specific optimization. Another background technology is
Processing-in-Memory (PIM), which integrates computing units (PIM units) inside
DRAM memory devices to accelerate memory-intensive workloads, including OLAP.
  Our key insight is to combine the interleaved CPU access and localized PIM
unit access to provide two-dimensional access to address the data format
contradictions inherent in HTAP. First, we propose a unified data storage
format with novel data alignment and placement techniques to optimize the
effective bandwidth of CPUs and PIM units and exploit the PIM's parallelism.
Second, we implement the multi-version concurrency control (MVCC) essential for
single-instance HTAP. Third, we extend the commercial PIM architecture to
support the OLAP operations and concurrent access from PIM and CPU. Experiments
show that PUSHtap can achieve 3.4\texttimes{}/4.4\texttimes{} OLAP/OLTP
throughput improvement compared to multi-instance PIM-based design.

</details>


### [118] [TeraNoC: A Multi-Channel 32-bit Fine-Grained, Hybrid Mesh-Crossbar NoC for Efficient Scale-up of 1000+ Core Shared-L1-Memory Clusters](https://arxiv.org/abs/2508.02446)
*Yichao Zhang,Zexin Fu,Tim Fischer,Yinrong Li,Marco Bertuletti,Luca Benini*

Main category: cs.DC

TL;DR: TeraNoC是一种混合网格-交叉开关片上互连技术，兼具可扩展性和低延迟，同时保持低路由开销。


<details>
  <summary>Details</summary>
Motivation: 解决片上互连设计中带宽扩展与低延迟、高面积效率的矛盾问题。

Method: 结合32位字宽多通道2D网格和交叉开关拓扑，设计路由器重映射器以平衡流量。

Result: 在12nm技术中实现1024核集群，IPC达0.85，功耗和面积效率显著优于纯交叉开关设计。

Conclusion: TeraNoC为多核低延迟共享内存集群提供了高效解决方案。

Abstract: A key challenge in on-chip interconnect design is to scale up bandwidth while
maintaining low latency and high area efficiency. 2D-meshes scale with low
wiring area and congestion overhead; however, their end-to-end latency
increases with the number of hops, making them unsuitable for latency-sensitive
core-to-L1-memory access. On the other hand, crossbars offer low latency, but
their routing complexity grows quadratically with the number of I/Os, requiring
large physical routing resources and limiting area-efficient scalability. This
two-sided interconnect bottleneck hinders the scale-up of many-core,
low-latency, tightly coupled shared-memory clusters, pushing designers toward
instantiating many smaller and loosely coupled clusters, at the cost of
hardware and software overheads. We present TeraNoC, an open-source, hybrid
mesh-crossbar on-chip interconnect that offers both scalability and low
latency, while maintaining very low routing overhead. The topology, built on
32bit word-width multi-channel 2D-meshes and crossbars, enables the
area-efficient scale-up of shared-memory clusters. A router remapper is
designed to balance traffic load across interconnect channels. Using TeraNoC,
we build a cluster with 1024 single-stage, single-issue cores that share a
4096-banked L1 memory, implemented in 12nm technology. The low interconnect
stalls enable high compute utilization of up to 0.85 IPC in compute-intensive,
data-parallel key GenAI kernels. TeraNoC only consumes 7.6\% of the total
cluster power in kernels dominated by crossbar accesses, and 22.7\% in kernels
with high 2D-mesh traffic. Compared to a hierarchical crossbar-only cluster,
TeraNoC reduces die area by 37.8\% and improves area efficiency (GFLOP/s/mm2)
by up to 98.7\%, while occupying only 10.9\% of the logic area.

</details>


### [119] [xDeepServe: Model-as-a-Service on Huawei CloudMatrix384](https://arxiv.org/abs/2508.02520)
*Ao Xiao,Bangzheng He,Baoquan Zhang,Baoxing Huai,Bingji Wang,Bo Wang,Bo Xu,Boyi Hou,Chan Yang,Changhong Liu,Cheng Cui,Chenyu Zhu,Cong Feng,Daohui Wang,Dayun Lin,Duo Zhao,Fengshao Zou,Fu Wang,Gangqiang Zhang,Gengyuan Dan,Guanjie Chen,Guodong Guan,Guodong Yang,Haifeng Li,Haipei Zhu,Hao Feng,Hao Huang,Hao Xu,Hengrui Ma,Hengtao Fan,Hui Liu,Jia Li,Jiang Liu,Jiang Xu,Jie Meng,Jinhan Xin,Junhao Hu,Juwei Chen,Lan Yu,Lanxin Miao,Liang Liu,Linan Jing,Lu Zhou,Meina Han,Mingkun Deng,Mingyu Deng,Naitian Deng,Nizhong Lin,Peihan Zhao,Peng Pan,Pengfei Shen,Ping Li,Qi Zhang,Qin Zhang,Qingrong Xia,Qingyi Zhang,Qunchao Fu,Ren Guo,Ruimin Gao,Shaochun Li,Sheng Long,Shentian Li,Shining Wan,Shuai Shen,Shuangfu Zeng,Shuming Jing,Siqi Yang,Song Zhang,Tao Xu,Tianlin Du,Ting Chen,Wanxu Wu,Wei Jiang,Weinan Tong,Weiwei Chen,Wen Peng,Wenli Zhou,Wenquan Yang,Wenxin Liang,Xiang Liu,Xiaoli Zhou,Xin Jin,Xinyu Duan,Xu Li,Xu Zhang,Xusheng Chen,Yalong Shan,Yang Gan,Yao Lu,Yi Deng,Yi Zheng,Yingfei Zheng,Yiyun Zheng,Yizhou Shan,Yong Gao,Yongqiang Yang,Yuanjin Gong,Yue Yu,Yuetao Chen,Yukun Zhu,Yulong He,Yusu Zhao,Yuyan Wu,Zenan Zhang,Zhaojin Zhuo,Zhaoyang Ji,Zhefeng Wang,Zheng Wang,Zhenhua Yang,Zhenli Sheng,Zhibin Yu,Zhigang Ji,Zhihao Ren,Zhipeng Bian,Zhixia Liu,Zhiyu Dong,Zhonghua Li,Zhou Yu,Zhuoming Shen,Zhuwei Peng,Zi Ye,Zihao Xiang,Zimin Fu,Zixuan Zhang*

Main category: cs.DC

TL;DR: 论文介绍了华为云的xDeepServe系统，专为SuperPod规模的基础设施设计，采用Transformerless架构分解Transformer模型，支持大规模推理。


<details>
  <summary>Details</summary>
Motivation: 随着LLM模型和SuperPod硬件的发展，运行大规模MoE模型面临新挑战，需要新的执行模型和基础设施支持。

Method: 提出xDeepServe系统，核心是Transformerless架构，将Transformer模型分解为模块化单元，结合XCCL通信库和FlowServe引擎实现高效推理。

Result: 实现了跨数百个NPU的可扩展推理，支持计算和内存的独立扩展而不牺牲性能。

Conclusion: xDeepServe为SuperPod规模的大规模AI基础设施提供了高效的解决方案。

Abstract: The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in
large-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in
recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is
scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s
high-speed interconnects. Running large MoE models on SuperPod-scale hardware
brings new challenges. It requires new execution models, scalable scheduling,
efficient expert load balancing, and elimination of single points of failure.
This paper presents xDeepServe, Huawei Cloud's LLM serving system designed for
SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated
architecture that decomposes transformer models into modular units--attention,
feedforward, and MoE--executed independently on NPUs connected via high-speed
fabric. We implement this design in two forms: disaggregated prefill-decode and
disaggregated MoE-attention. This fully disaggregated setup enables independent
scaling of compute and memory without sacrificing performance. To support this
architecture, we propose XCCL, a communication library that leverages
CloudMatrix384's global shared memory to implement efficient point-to-point and
all-to-all primitives. We also extend our serving engine FlowServe with
system-level techniques, enabling scalable inference across hundreds of NPUs.

</details>


### [120] [Blockchain Epidemic Consensus for Large-Scale Networks](https://arxiv.org/abs/2508.02552)
*Siamak Abdi,Giuseppe Di Fatta,Atta Badii,Giancarlo Fortino*

Main category: cs.DC

TL;DR: 区块链需要一个高效的共识协议来解决现有协议的可扩展性和容错性问题。作者提出了一种新型的完全去中心化协议BECP，基于流行病通信原则，实验证明其在吞吐量、延迟和消息传递效率上优于现有协议。


<details>
  <summary>Details</summary>
Motivation: 现有的区块链共识协议在可扩展性、资源消耗和容错性方面存在不足，需要一种更高效的去中心化解决方案。

Method: 作者提出了区块链流行病共识协议（BECP），采用流行病通信原则，无固定角色（如验证者或领导者），实现概率收敛、高效消息传播和对消息延迟的容忍。

Result: 通过与经典协议（PAXOS、RAFT、PBFT）及新型流行病协议（Avalanche、Snowman）的比较，BECP在吞吐量、共识延迟和消息传递效率上表现更优。

Conclusion: BECP是一种高效且可扩展的共识协议，适用于下一代区块链系统。

Abstract: Blockchain is a distributed ledger technology that has applications in many
domains such as cryptocurrency, smart contracts, supply chain management, and
many others. Distributed consensus is a fundamental component of blockchain
systems that enables secure, precise, and tamper-proof verification of data
without relying on central authorities. Existing consensus protocols,
nevertheless, suffer from drawbacks, some of which are related to scalability,
resource consumption, and fault tolerance. We introduce Blockchain Epidemic
Consensus Protocol (BECP), a novel fully decentralised consensus protocol for
blockchain networks at a large scale. BECP follows epidemic communication
principles, without fixed roles like validators or leaders, and achieves
probabilistic convergence, efficient message dissemination, and tolerance to
message delays. We provide an extensive experimental comparison of BECP against
classic protocols like PAXOS, RAFT, and PBFT, and newer epidemic-based
protocols like Avalanche and Snowman. The findings indicate that BECP provides
desirable gains in throughput, consensus latency, and substantial
message-passing efficiency compared to existing epidemic-based approaches,
validating its usability as an effective and scalable approach for
next-generation blockchain systems.

</details>


### [121] [Fully Decentralised Consensus for Extreme-scale Blockchain](https://arxiv.org/abs/2508.02595)
*Siamak Abdi,Giuseppe Di Fatta,Atta Badii,Giancarlo Fortino*

Main category: cs.DC

TL;DR: 本文介绍了一种完全去中心化的共识协议BECP，利用流行病协议的优势，适用于超大规模区块链系统。实验表明，BECP在吞吐量、可扩展性和共识延迟方面优于传统及新型协议。


<details>
  <summary>Details</summary>
Motivation: 针对现有共识算法存在节点故障、高资源消耗等问题，研究提出一个高效、去中心化的共识协议，以适应超大规模区块链系统的需求。

Method: 基于流行病协议设计BECP，特点是不依赖固定验证者集、提供概率收敛保证、高效利用网络资源，并对节点和网络故障具有容忍性。

Result: BECP在吞吐量和共识延迟方面优于PAXOS、RAFT、PBFT和Avalanche等协议，消息数量也显著减少。

Conclusion: BECP证明了基于流行病协议的完全去中心化共识在区块链技术中的高效性和有效性，适合超大规模系统。

Abstract: Blockchain is a decentralised, immutable ledger technology that has been
widely adopted in many sectors for various applications such as
cryptocurrencies, smart contracts and supply chain management. Distributed
consensus is a fundamental component of blockchain, which is required to ensure
trust, security, and integrity of the data stored and the transactions
processed in the blockchain. Various consensus algorithms have been developed,
each affected from certain issues such as node failures, high resource
consumption, collusion, etc. This work introduces a fully decentralised
consensus protocol, Blockchain Epidemic Consensus Protocol (BECP), suitable for
very large and extreme-scale blockchain systems. The proposed approach
leverages the benefits of epidemic protocols, such as no reliance on a fixed
set of validators or leaders, probabilistic guarantees of convergence,
efficient use of network resources, and tolerance to node and network failures.
A comparative experimental analysis has been carried out with traditional
protocols including PAXOS, RAFT, and Practical Byzantine Fault Tolerance
(PBFT), as well as a relatively more recent protocol such as Avalanche, which
is specifically designed for very large-scale systems. The results illustrate
how BECP outperforms them in terms of throughput, scalability and consensus
latency. BECP achieves an average of 1.196 times higher throughput in terms of
consensus on items and 4.775 times better average consensus latency.
Furthermore, BECP significantly reduces the number of messages compared to
Avalanche. These results demonstrate the effectiveness and efficiency of fully
decentralised consensus for blockchain technology based on epidemic protocols.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [122] [DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs](https://arxiv.org/abs/2508.01136)
*Wei Zhou,Peng Sun,Xuanhe Zhou,Qianglei Zang,Ji Xu,Tieying Zhang,Guoliang Li,Fan Wu*

Main category: cs.DB

TL;DR: DBAIOps是一种结合LLM和知识图谱的混合数据库运维系统，能有效利用专家经验进行诊断，显著提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据库运维方法（包括商业产品）无法有效利用专家经验，规则和LLM方法各有不足。

Method: DBAIOps采用异构图模型表示诊断经验，构建了800+可重用异常模型和两阶段图演化机制，结合推理LLM推断根因。

Result: 在四种主流数据库系统中，DBAIOps在根因和人工评估准确率上分别比基准高34.85%和47.22%。

Conclusion: DBAIOps通过结合LLM和知识图谱，显著提升了数据库运维诊断的准确性和可用性。

Abstract: The operation and maintenance (O&M) of database systems is critical to
ensuring system availability and performance, typically requiring expert
experience (e.g., identifying metric-to-anomaly relations) for effective
diagnosis and recovery. However, existing automatic database O&M methods,
including commercial products, cannot effectively utilize expert experience. On
the one hand, rule-based methods only support basic O&M tasks (e.g.,
metric-based anomaly detection), which are mostly numerical equations and
cannot effectively incorporate literal O&M experience (e.g., troubleshooting
guidance in manuals). On the other hand, LLM-based methods, which retrieve
fragmented information (e.g., standard documents + RAG), often generate
inaccurate or generic results. To address these limitations, we present
DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with
knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a
heterogeneous graph model for representing the diagnosis experience, and
proposes a semi-automatic graph construction algorithm to build that graph from
thousands of documents. Second, DBAIOps develops a collection of (800+)
reusable anomaly models that identify both directly alerted metrics and
implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps
proposes a two-stage graph evolution mechanism to explore relevant diagnosis
paths and identify missing relations automatically. It then leverages a
reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear
diagnosis reports for both DBAs and common users. Our evaluation over four
mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates
that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher
in root cause and human evaluation accuracy, respectively.

</details>


### [123] [Don't Persist All : Efficient Persistent Data Structures](https://arxiv.org/abs/1905.13011)
*Pratyush Mahapatra,Mark D. Hill,Michael M. Swift*

Main category: cs.DB

TL;DR: 论文提出了针对持久内存的部分持久化数据结构实现方法，以减少冗余数据的写入开销，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据结构在持久内存中存储冗余数据会导致显著的写入开销和性能下降。

Method: 针对双向链表、B+树和哈希表，设计部分持久化实现，仅存储必要数据字段，崩溃后重建冗余字段。

Result: 实验显示性能提升5-20%，部分场景可达165%。

Conclusion: 部分持久化方法能有效减少持久内存写入开销，显著提升性能。

Abstract: Data structures used in software development have inbuilt redundancy to
improve software reliability and to speed up performance. Examples include a
Doubly Linked List which allows a faster deletion due to the presence of the
previous pointer. With the introduction of Persistent Memory, storing the
redundant data fields into persistent memory adds a significant write overhead,
and reduces performance. In this work, we focus on three data structures -
Doubly Linked List, B+Tree and Hashmap, and showcase alternate partly
persistent implementations where we only store a limited set of data fields to
persistent memory. After a crash/restart, we use the persistent data fields to
recreate the data structures along with the redundant data fields. We compare
our implementation with the base implementation and show that we achieve
speedups around 5-20% for some data structures, and up to 165% for a
flush-dominated data structure.

</details>


### [124] [Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search](https://arxiv.org/abs/2508.01405)
*Mengzhao Wang,Boyu Tan,Yunjun Gao,Hai Jin,Yingfeng Zhang,Xiangyu Ke,Xiaoliang Xu,Yifan Zhu*

Main category: cs.DB

TL;DR: 该论文首次系统性地评估了混合搜索架构，揭示了影响其性能的三大关键因素，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 混合搜索在现代信息检索系统中至关重要，但缺乏对其核心组件（如检索范式、组合方案和重排序方法）的系统性评估。

Method: 通过构建开源数据库Infinity，论文评估了四种检索范式（FTS、SVS、DVS、TenS）及其组合和重排序策略，测试了11个真实数据集。

Result: 研究发现：（1）"最弱环节"现象；（2）配置需根据资源和数据特性灵活调整；（3）TRF作为一种高效替代方案。

Conclusion: 论文提供了设计下一代混合搜索系统的实用建议，并指出了未来研究方向。

Abstract: Hybrid search, the integration of lexical and semantic retrieval, has become
a cornerstone of modern information retrieval systems, driven by demanding
applications like Retrieval-Augmented Generation (RAG). The architectural
design space for these systems is vast and complex, yet a systematic, empirical
understanding of the trade-offs among their core components--retrieval
paradigms, combination schemes, and re-ranking methods--is critically lacking.
To address this, and informed by our experience building the Infinity
open-source database, we present the first systematic benchmark of advanced
hybrid search architectures. Our framework evaluates four retrieval
paradigms--Full-Text Search (FTS), Sparse Vector Search (SVS), Dense Vector
Search (DVS), and Tensor Search (TenS)--benchmarking their combinations and
re-ranking strategies across 11 real-world datasets. Our results reveal three
key findings for practitioners and researchers: (1) A "weakest link"
phenomenon, where a single underperforming retrieval path can
disproportionately degrade overall accuracy, highlighting the need for
path-wise quality assessment before fusion. (2) A data-driven map of the
performance trade-offs, demonstrating that optimal configurations depend
heavily on resource constraints and data characteristics, moving beyond a
one-size-fits-all approach. (3) The identification of Tensor-based Re-ranking
Fusion (TRF) as a high-efficacy alternative to mainstream fusion methods,
offering the semantic power of tensor search at a fraction of the computational
and memory cost. Our findings offer concrete guidelines for designing the next
generation of adaptive, scalable hybrid search systems while also identifying
key directions for future research.

</details>


### [125] [Marlin: Efficient Coordination for Autoscaling Cloud DBMS (Extended Version)](https://arxiv.org/abs/2508.01931)
*Wenjie Hu,Guanzhou Hu,Mahesh Balakrishnan,Xiangyao Yu*

Main category: cs.DB

TL;DR: 论文提出Marlin，一种云原生协调机制，通过解耦集群协调解决了传统外部协调服务的局限性，显著提升了成本效率和重构速度。


<details>
  <summary>Details</summary>
Motivation: 现代云数据库虽已实现存储解耦，但控制平面仍依赖外部协调服务，导致可扩展性瓶颈和成本效率低下。

Method: 提出Marlin，将协调功能集成到云原生数据库中，支持跨节点修改协调状态，并通过事务和优化的提交协议确保数据一致性。

Result: Marlin比传统解决方案成本效率提升4.4倍，重构时间缩短4.9倍。

Conclusion: Marlin为云数据库提供了高效、可扩展的协调机制，解决了现有外部协调服务的瓶颈问题。

Abstract: Modern cloud databases are shifting from converged architectures to storage
disaggregation, enabling independent scaling and billing of compute and
storage. However, cloud databases still rely on external, converged
coordination services (e.g., ZooKeeper) for their control planes. These
services are effectively lightweight databases optimized for low-volume
metadata. As the control plane scales in the cloud, this approach faces similar
limitations as converged databases did before storage disaggregation:
scalability bottlenecks, low cost efficiency, and increased operational burden.
  We propose to disaggregate the cluster coordination to achieve the same
benefits that storage disaggregation brought to modern cloud DBMSs. We present
Marlin, a cloud-native coordination mechanism that fully embraces storage
disaggregation. Marlin eliminates the need for external coordination services
by consolidating coordination functionality into the existing cloud-native
database it manages. To achieve failover without an external coordination
service, Marlin allows cross-node modifications on coordination states. To
ensure data consistency, Marlin employs transactions to manage both
coordination and application states and introduces MarlinCommit, an optimized
commit protocol that ensures strong transactional guarantees even under
cross-node modifications. Our evaluations demonstrate that Marlin improves cost
efficiency by up to 4.4x and reduces reconfiguration duration by up to 4.9x
compared to converged coordination solutions.

</details>


### [126] [OnPair: Short Strings Compression for Fast Random Access](https://arxiv.org/abs/2508.02280)
*Francesco Gargiulo,Rossano Venturini*

Main category: cs.DB

TL;DR: 介绍了一种名为OnPair的字典压缩算法，适用于需要高压缩比和快速随机访问的内存数据库系统，平衡了压缩比与速度。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法在压缩比和速度之间的权衡问题，OnPair旨在通过高效字典构建技术填补这一空白。

Method: 采用缓存友好的字典构建技术，通过单次顺序扫描数据样本逐步合并高频相邻子串，并推出了OnPair16变体。

Result: 实验表明，OnPair及其变体在压缩比与BPE相当的同时，显著提升了压缩速度并降低了内存占用。

Conclusion: OnPair成功平衡了压缩比与速度，适用于内存数据库系统。

Abstract: We present OnPair, a dictionary-based compression algorithm designed to meet
the needs of in-memory database systems that require both high compression and
fast random access. Existing methods either achieve strong compression ratios
at significant computational and memory cost (e.g., BPE) or prioritize speed at
the expense of compression quality (e.g., FSST). OnPair bridges this gap by
employing a cache-friendly dictionary construction technique that incrementally
merges frequent adjacent substrings in a single sequential pass over a data
sample. This enables fast, memory-efficient training without tracking global
pair positions, as required by traditional BPE. We also introduce OnPair16, a
variant that limits dictionary entries to 16 bytes, enabling faster parsing via
optimized longest prefix matching. Both variants compress strings
independently, supporting fine-grained random access without block-level
overhead. Experiments on real-world datasets show that OnPair and OnPair16
achieve compression ratios comparable to BPE while significantly improving
compression speed and memory usage.

</details>


### [127] [From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning](https://arxiv.org/abs/2508.02458)
*Yichao Feng*

Main category: cs.DB

TL;DR: 大型语言模型在情感理解和社交推理方面表现出潜力，但在需要推断隐含心理状态的任务中存在局限。本文提出了一种基于专家心理模式的学习框架，提升了模型的心理学推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在心理学任务中表现不佳的问题，通过专家标注的心理场景改进模型推理能力。

Method: 提出了一种轨迹感知的强化学习框架，模仿专家心理模式，结合真实世界刺激与结构化推理指导。

Result: 模型在多个基准测试中达到专家级表现，表现出强大的泛化能力和持续学习能力。

Conclusion: 通过心理学驱动的框架，模型能够更有效地进行心理状态推断，推动了语言模型在心理学任务中的应用。

Abstract: Large Language Models show promise in emotion understanding, social
reasoning, and empathy, yet they struggle with psychologically grounded tasks
that require inferring implicit mental states in context-rich, ambiguous
settings. These limitations arise from the absence of theory-aligned
supervision and the difficulty of capturing nuanced mental processes in
real-world narratives. To address this gap, we leverage expert-labeled,
psychologically rich scenarios and propose a trajectory-aware reinforcement
learning framework that explicitly imitates expert psychological thought
patterns. By integrating real-world stimuli with structured reasoning guidance,
our approach enables compact models to internalize social-cognitive principles,
perform nuanced psychological inference, and support continual
self-improvement. Comprehensive experiments across multiple benchmarks further
demonstrate that our models achieve expert-level interpretive capabilities,
exhibiting strong out-of-distribution generalization and robust continual
learning across diverse, challenging, and psychologically grounded tasks.

</details>


### [128] [M2: An Analytic System with Specialized Storage Engines for Multi-Model Workloads](https://arxiv.org/abs/2508.02508)
*Kyoseung Koo,Bogyeong Kim,Bongki Moon*

Main category: cs.DB

TL;DR: 论文提出了一种名为M2的多模型分析系统，通过集成存储引擎和创新的多阶段哈希连接算法，显著提升了多模型数据分析的效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模型数据处理方法（如多语言持久化和单存储引擎多模型数据库）存在通信成本高或效率低的问题，无法满足现代数据分析需求。

Method: M2系统将所有数据模型视为一等实体，开发了多阶段哈希连接算法，实现跨模型的高效查询计划。

Result: 实验表明，M2在多模型分析中的性能最高可提升188倍，验证了其技术的有效性。

Conclusion: M2通过集成存储引擎和创新的连接算法，为多模型数据分析提供了一种高效解决方案。

Abstract: Modern data analytic workloads increasingly require handling multiple data
models simultaneously. Two primary approaches meet this need: polyglot
persistence and multi-model database systems. Polyglot persistence employs a
coordinator program to manage several independent database systems but suffers
from high communication costs due to its physically disaggregated architecture.
Meanwhile, existing multi-model database systems rely on a single storage
engine optimized for a specific data model, resulting in inefficient processing
across diverse data models. To address these limitations, we present M2, a
multi-model analytic system with integrated storage engines. M2 treats all data
models as first-class entities, composing query plans that incorporate
operations across models. To effectively combine data from different models,
the system introduces a specialized inter-model join algorithm called
multi-stage hash join. Our evaluation demonstrates that M2 outperforms existing
approaches by up to 188x speedup on multi-model analytics, confirming the
effectiveness of our proposed techniques.

</details>


### [129] [The KG-ER Conceptual Schema Language](https://arxiv.org/abs/2508.02548)
*Enrico Franconi,Benoît Groz,Jan Hidders,Nina Pardal,Sławek Staworko,Jan Van den Bussche,Piotr Wieczorek*

Main category: cs.DB

TL;DR: KG-ER是一种用于知识图谱的概念模式语言，独立于其表示形式（如关系数据库、属性图、RDF）描述知识图谱的结构，并帮助捕捉其中信息的语义。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱的表示形式多样（如关系数据库、属性图、RDF），但缺乏一种统一的语言来描述其结构并捕获语义信息。

Method: 提出了KG-ER，作为一种概念模式语言，独立于具体表示形式，专注于描述知识图谱的结构和语义。

Result: KG-ER能够独立于表示形式描述知识图谱的结构，并有效捕捉其语义信息。

Conclusion: KG-ER为知识图谱提供了一种统一的结构和语义描述方法，具有广泛的应用潜力。

Abstract: We propose KG-ER, a conceptual schema language for knowledge graphs that
describes the structure of knowledge graphs independently of their
representation (relational databases, property graphs, RDF) while helping to
capture the semantics of the information stored in a knowledge graph.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [130] [A Dynamic Allocation Scheme for Adaptive Shared-Memory Mapping on Kilo-core RV Clusters for Attention-Based Model Deployment](https://arxiv.org/abs/2508.01180)
*Bowen Wang,Marco Bertuletti,Yichao Zhang,Victor J. B. Jung,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出了一种动态分配方案（DAS），用于优化多核处理器中处理单元（PEs）对共享L1内存的访问，以减少数据访问冲突。通过在1024-PE集群上的测试，证明DAS显著提升了性能和工作负载的数据局部性。


<details>
  <summary>Details</summary>
Motivation: 针对基于注意力模型的硬件在处理多样性内核时的效率问题，尤其是在大规模集群中共享L1内存的架构下，PEs利用率低的问题。

Method: 设计了一种运行时可编程的地址重映射硬件单元（DAS）和统一内存分配器，以减少PEs对多bank L1内存的访问冲突。

Result: 在1024-PE RISC-V集群上，DAS为Vision Transformer模型带来了1.94倍的加速，PE利用率达0.81，硬件开销低于0.1%。

Conclusion: DAS通过优化内存访问，显著提升了大规模并行机器学习工作负载的性能，同时保持低硬件开销。

Abstract: Attention-based models demand flexible hardware to manage diverse kernels
with varying arithmetic intensities and memory access patterns. Large clusters
with shared L1 memory, a common architectural pattern, struggle to fully
utilize their processing elements (PEs) when scaled up due to reduced
throughput in the hierarchical PE-to-L1 intra-cluster interconnect. This paper
presents Dynamic Allocation Scheme (DAS), a runtime programmable address
remapping hardware unit coupled with a unified memory allocator, designed to
minimize data access contention of PEs onto the multi-banked L1. We evaluated
DAS on an aggressively scaled-up 1024-PE RISC-V cluster with Non-Uniform Memory
Access (NUMA) PE-to-L1 interconnect to demonstrate its potential for improving
data locality in large parallel machine learning workloads. For a Vision
Transformer (ViT)-L/16 model, each encoder layer executes in 5.67 ms, achieving
a 1.94x speedup over the fixed word-level interleaved baseline with 0.81 PE
utilization. Implemented in 12nm FinFET technology, DAS incurs <0.1 % area
overhead.

</details>


### [131] [Silent Data Corruption by 10x Test Escapes Threatens Reliable Computing](https://arxiv.org/abs/2508.01786)
*Subhasish Mitra,Subho Banerjee,Martin Dixon,Rama Govindaraju,Peter Hochschild,Eric X. Liu,Bharath Parthasarathy,Parthasarathy Ranganathan*

Main category: cs.AR

TL;DR: 针对计算芯片制造测试中遗漏的缺陷问题，提出了一种三管齐下的方法：快速诊断、现场检测和新测试实验。


<details>
  <summary>Details</summary>
Motivation: 解决制造测试中遗漏的缺陷芯片问题，避免因此导致的无声数据损坏，提高计算可靠性。

Method: （a）从系统级行为快速诊断缺陷芯片；（b）现场检测缺陷芯片；（c）设计新测试实验评估新技术效果。

Result: 该方法旨在减少测试遗漏缺陷芯片的风险，并为工业测试提供新方向。

Conclusion: 提出的三管齐下方法有望显著减少计算芯片的测试遗漏问题，提高系统可靠性。

Abstract: Too many defective compute chips are escaping existing manufacturing tests --
at least an order of magnitude more than industrial targets across all compute
chip types in data centers. Silent data corruptions (SDCs) caused by test
escapes, when left unaddressed, pose a major threat to reliable computing. We
present a three-pronged approach to future directions in overcoming test
escapes: (a) Quick diagnosis of defective chips directly from system-level
incorrect behaviors. Such diagnosis is critical for gaining insights into why
so many defective chips escape existing manufacturing testing. (b) In-field
detection of defective chips. (c) New test experiments to understand the
effectiveness of new techniques for detecting defective chips. These
experiments must overcome the drawbacks and pitfalls of previous industrial
test experiments and case studies.

</details>


### [132] [MARVEL: An End-to-End Framework for Generating Model-Class Aware Custom RISC-V Extensions for Lightweight AI](https://arxiv.org/abs/2508.01800)
*Ajay Kumar M,Cian O'Mahoney,Pedro Kreutz Werle,Shreejith Shanker,Dimitrios S. Nikolopoulos,Bo Ji,Hans Vandierendonck,Deepu John*

Main category: cs.AR

TL;DR: 提出了一个名为MARVEL的框架，用于在资源受限的物联网设备上高效部署深度神经网络，通过定制RISC-V ISA扩展实现端到端的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 解决现有工具在极端资源受限的物联网设备（无操作系统）上部署DNN的不足。

Method: 使用Apache TVM、Synopsys ASIP Designer和Xilinx Vivado，将Python模型转换为定制RISC-V ISA扩展及优化的C代码。

Result: 在FPGA平台上实现了2倍推理速度提升和高达2倍的能效提升，面积开销为28.23%。

Conclusion: MARVEL框架显著提升了资源受限环境中DNN部署的效率和性能。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained IoT devices
remains a challenging problem, often requiring hardware modifications tailored
to individual AI models. Existing accelerator-generation tools, such as AMD's
FINN, do not adequately address extreme resource limitations faced by IoT
endpoints operating in bare-metal environments without an operating system
(OS). To overcome these constraints, we propose MARVEL-an automated, end-to-end
framework that generates custom RISC-V ISA extensions tailored to specific DNN
model classes, with a primary focus on convolutional neural networks (CNNs).
The proposed method profiles high-level DNN representations in Python and
generates an ISA-extended RISC-V core with associated compiler tools for
efficient deployment. The flow leverages (1) Apache TVM for translating
high-level Python-based DNN models into optimized C code, (2) Synopsys ASIP
Designer for identifying compute-intensive kernels, modeling, and generating a
custom RISC-V and (3) Xilinx Vivado for FPGA implementation. Beyond a model
class specific RISC-V, our approach produces an optimized bare-metal C
implementation, eliminating the need for an OS or extensive software
dependencies. Unlike conventional deployment pipelines relying on
TensorFlow/PyTorch runtimes, our solution enables seamless execution in highly
resource-constrained environments. We evaluated the flow on popular DNN models
such as LeNet-5*, MobileNetV1, ResNet50, VGG16, MobileNetV2 and DenseNet121
using the Synopsys trv32p3 RISC-V core as a baseline. Results show a 2x speedup
in inference and upto 2x reduction in energy per inference at a 28.23% area
overhead when implemented on an AMD Zynq UltraScale+ ZCU104 FPGA platform.

</details>


### [133] [Revelator: Rapid Data Fetching via OS-Driven Hash-based Speculative Address Translation](https://arxiv.org/abs/2508.02007)
*Konstantinos Kanellopoulos,Konstantinos Sgouras,Andreas Kosmas Kakolyris,Vlad-Petru Nitu,Berkin Kerim Konar,Rahul Bera,Onur Mutlu*

Main category: cs.AR

TL;DR: Revelator是一种硬件-操作系统协作方案，通过预测性地址翻译提升性能，平均加速27%（20%），减少能耗9%。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统中地址翻译是性能瓶颈，现有方案依赖大页或硬件修改，效果有限。

Method: 采用分层哈希分配策略，结合轻量级推测引擎，提前获取数据和页表项。

Result: 在11个数据密集型基准测试中，平均加速27%（20%），能耗降低9%。

Conclusion: Revelator以最小硬件开销实现高效地址翻译，适用于原生和虚拟化环境。

Abstract: Address translation is a major performance bottleneck in modern computing
systems. Speculative address translation can hide this latency by predicting
the physical address (PA) of requested data early in the pipeline. However,
predicting the PA from the virtual address (VA) is difficult due to the
unpredictability of VA-to-PA mappings in conventional OSes. Prior works try to
overcome this but face two key issues: (i) reliance on large pages or VA-to-PA
contiguity, which is not guaranteed, and (ii) costly hardware changes to store
speculation metadata with limited effectiveness.
  We introduce Revelator, a hardware-OS cooperative scheme enabling highly
accurate speculative address translation with minimal modifications. Revelator
employs a tiered hash-based allocation strategy in the OS to create predictable
VA-to-PA mappings, falling back to conventional allocation when needed. On a
TLB miss, a lightweight speculation engine, guided by this policy, generates
candidate PAs for both program data and last-level page table entries (PTEs).
Thus, Revelator (i) speculatively fetches requested data before translation
resolves, reducing access latency, and (ii) fetches the fourth-level PTE before
the third-level PTE is accessed, accelerating page table walks.
  We prototype Revelator's OS support in Linux and evaluate it in simulation
across 11 diverse, data-intensive benchmarks in native and virtualized
environments. Revelator achieves average speedups of 27% (20%) in native
(virtualized) settings, surpasses a state-of-the-art speculative mechanism by
5%, and reduces energy use by 9% compared to baseline. Our RTL prototype shows
minimal area and power overheads on a modern CPU.

</details>


### [134] [GSIM: Accelerating RTL Simulation for Large-Scale Designs](https://arxiv.org/abs/2508.02236)
*Lu Chen,Dingyi Zhao,Zihao Yu,Ninghui Sun,Yungang Bao*

Main category: cs.AR

TL;DR: 该论文分析了RTL模拟的计算开销来源，并提出多层次的优化技术，最终在GSIM模拟器中实现显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: RTL模拟在硬件设计中具有重要作用，但复杂设计的模拟速度慢成为瓶颈。本文旨在优化RTL模拟的计算开销。

Method: 通过分析计算开销的四个来源，分别在超节点级、节点级和位级提出优化技术，并在GSIM模拟器中实现。

Result: GSIM成功模拟了先进的RISC-V处理器XiangShan，对比Verilator，速度提升显著（运行Linux提升7.34倍，运行CoreMark提升19.94倍）。

Conclusion: 提出的优化技术有效提升了RTL模拟速度，GSIM模拟器在复杂硬件设计中表现出色。

Abstract: Register Transfer Level (RTL) simulation is widely used in design space
exploration, verification, debugging, and preliminary performance evaluation
for hardware design. Among various RTL simulation approaches, software
simulation is the most commonly used due to its flexibility, low cost, and ease
of debugging. However, the slow simulation of complex designs has become the
bottleneck in design flow. In this work, we explore the sources of computation
overhead of RTL simulation and conclude them into four factors. To optimize
these factors, we propose several techniques at the supernode level, node
level, and bit level. Finally, we implement these techniques in a novel RTL
simulator GSIM. GSIM succeeds in simulating XiangShan, the state-of-the-art
open-source RISC-V processor. Besides, compared to Verilator, GSIM can achieve
speedup of 7.34x for booting Linux on XiangShan, and 19.94x for running
CoreMark on Rocket.

</details>


### [135] [ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering](https://arxiv.org/abs/2508.02304)
*Fangxin Liu,Haomin Li,Bowen Zhu,Zongwu Wang,Zhuoran Song,Habing Guan,Li Jiang*

Main category: cs.AR

TL;DR: 提出了一种基于内存计算（CIM）的算法-架构协同设计方法ASDR，用于高效神经渲染，降低计算开销和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染模型在实际应用中难以满足即时性和能效需求，存在不规则访问模式和高计算开销。

Method: 在算法层面提出动态采样和颜色密度解耦优化；在架构层面设计基于ReRAM的高效CIM架构。

Result: 实验显示ASDR在图形渲染任务中比现有加速器快9.55倍，比Xavier NX GPU快69.75倍，仅损失0.1 PSNR。

Conclusion: ASDR通过算法-架构协同设计显著提升了神经渲染的效率和能效。

Abstract: Neural Radiance Fields (NeRF) offer significant promise for generating
photorealistic images and videos. However, existing mainstream neural rendering
models often fall short in meeting the demands for immediacy and power
efficiency in practical applications. Specifically, these models frequently
exhibit irregular access patterns and substantial computational overhead,
leading to undesirable inference latency and high power consumption.
Computing-in-memory (CIM), an emerging computational paradigm, has the
potential to address these access bottlenecks and reduce the power consumption
associated with model execution.
  To bridge the gap between model performance and real-world scene
requirements, we propose an algorithm-architecture co-design approach,
abbreviated as ASDR, a CIM-based accelerator supporting efficient neural
rendering. At the algorithmic level, we propose two rendering optimization
schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of
different pixels, thus reducing access memory and computational overhead. (2)
Reducing MLP overhead by decoupling and approximating the volume rendering of
color and density. At the architecture level, we design an efficient
ReRAM-based CIM architecture with efficient data mapping and reuse
microarchitecture. Experiments demonstrate that our design can achieve up to
$9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators
and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

</details>


### [136] [ReGate: Enabling Power Gating in Neural Processing Units](https://arxiv.org/abs/2508.02536)
*Yuqi Xue,Jian Huang*

Main category: cs.AR

TL;DR: ReGate是一种通过硬件/软件协同设计实现NPU芯片细粒度电源门控的技术，能显著降低能耗且对性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 现代NPU芯片因缺乏电源管理支持，静态功耗占比高（30%-72%），亟需节能方案。

Method: 针对NPU各组件特性设计不同电源管理方案：对确定性执行的脉动阵列启用周期级门控；对长空闲的互联和HBM控制器采用硬件空闲检测；对负载多变的向量单元和SRAM通过编译器管理。

Result: 在NPU模拟器上实现，能耗平均降低15.5%（最高32.8%），硬件开销仅3.3%，且不影响AI任务性能。

Conclusion: ReGate证明了硬件/软件协同设计的细粒度电源门控在NPU芯片中的高效性与可行性。

Abstract: The energy efficiency of neural processing units (NPU) is playing a critical
role in developing sustainable data centers. Our study with different
generations of NPU chips reveals that 30%-72% of their energy consumption is
contributed by static power dissipation, due to the lack of power management
support in modern NPU chips. In this paper, we present ReGate, which enables
fine-grained power-gating of each hardware component in NPU chips with
hardware/software co-design. Unlike conventional power-gating techniques for
generic processors, enabling power-gating in NPUs faces unique challenges due
to the fundamental difference in hardware architecture and program execution
model. To address these challenges, we carefully investigate the power-gating
opportunities in each component of NPU chips and decide the best-fit power
management scheme (i.e., hardware- vs. software-managed power gating).
Specifically, for systolic arrays (SAs) that have deterministic execution
patterns, ReGate enables cycle-level power gating at the granularity of
processing elements (PEs) following the inherent dataflow execution in SAs. For
inter-chip interconnect (ICI) and HBM controllers that have long idle
intervals, ReGate employs a lightweight hardware-based idle-detection
mechanism. For vector units and SRAM whose idle periods vary significantly
depending on workload patterns, ReGate extends the NPU ISA and allows software
like compilers to manage the power gating. With implementation on a
production-level NPU simulator, we show that ReGate can reduce the energy
consumption of NPU chips by up to 32.8% (15.5% on average), with negligible
impact on AI workload performance. The hardware implementation of power-gating
logic introduces less than 3.3% overhead in NPU chips.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [137] [Poncelet triangles: two harmonious loci and two attractive envelopes](https://arxiv.org/abs/2508.02368)
*Ronaldo A. Garcia,Mark Helman,Dan Reznik*

Main category: math.MG

TL;DR: 研究证明，在一对嵌套椭圆之间的Poncelet三角形族中，垂心的轨迹不仅是一条二次曲线，而且是轴对齐的，并与旋转90度的椭圆相似；此外，固定点P的等角共轭轨迹也是二次曲线。此外，当内椭圆是圆时，外接圆和内外心共轭轴的包络才包含二次曲线分量。


<details>
  <summary>Details</summary>
Motivation: 探讨Poncelet三角形族在嵌套椭圆之间的几何性质及其轨迹问题，特别是垂心和等角共轭点的轨迹形状。

Method: 通过几何分析和代数方法，证明了在特定条件下轨迹的性质，包括轴对齐、相似性和二次曲线特性。

Result: 垂心的轨迹是轴对齐且与旋转后的椭圆相似的二次曲线；固定点P的等角共轭轨迹也是二次曲线；特定条件下包络包含二次曲线分量。

Conclusion: 研究揭示了Poncelet三角形族在特定几何配置下的轨迹性质，为相关几何问题提供了新的理解。

Abstract: We prove that over a Poncelet triangle family interscribed between two nested
ellipses $\E,\E_c$, (i) the locus of the orthocenter is not only a conic, but
it is axis-aligned and homothetic to a $90^o$-rotated copy of $\E$, and (ii)
the locus of the isogonal conjugate of a fixed point $P$ is also a conic (the
expected degree was four); a parabola (resp. line) if $P$ is on the
(degree-four) envelope of the circumcircle (resp. on $\E$). We also show that
the envelope of both the circumcircle and radical axis of incircle and
circumcircle contain a conic component if and only if $\E_c$ is a circle. The
former case is the union of two circles!

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [138] [Context Guided Transformer Entropy Modeling for Video Compression](https://arxiv.org/abs/2508.01852)
*Junlong Tong,Wei Zhang,Yaohui Jin,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 提出了一种名为CGT的熵模型，通过重新采样时间上下文和加权空间上下文来减少视频冗余，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的条件熵模型在处理时间上下文时增加复杂性和计算成本，且空间上下文模型缺乏明确的空间依赖顺序建模。

Method: 使用时间上下文重采样器和依赖加权的空间上下文分配器（基于师生网络）来优化上下文建模。

Result: CGT模型将熵建模时间减少约65%，BD-Rate降低11%。

Conclusion: CGT在减少计算开销的同时提升了性能，为视频压缩提供了高效解决方案。

Abstract: Conditional entropy models effectively leverage spatio-temporal contexts to
reduce video redundancy. However, incorporating temporal context often
introduces additional model complexity and increases computational cost. In
parallel, many existing spatial context models lack explicit modeling the
ordering of spatial dependencies, which may limit the availability of relevant
context during decoding. To address these issues, we propose the Context Guided
Transformer (CGT) entropy model, which estimates probability mass functions of
the current frame conditioned on resampled temporal context and
dependency-weighted spatial context. A temporal context resampler learns
predefined latent queries to extract critical temporal information using
transformer encoders, reducing downstream computational overhead. Meanwhile, a
teacher-student network is designed as dependency-weighted spatial context
assigner to explicitly model the dependency of spatial context order. The
teacher generates an attention map to represent token importance and an entropy
map to reflect prediction certainty from randomly masked inputs, guiding the
student to select the weighted top-k tokens with the highest spatial
dependency. During inference, only the student is used to predict undecoded
tokens based on high-dependency context. Experimental results demonstrate that
our CGT model reduces entropy modeling time by approximately 65% and achieves
an 11% BD-Rate reduction compared to the previous state-of-the-art conditional
entropy model.

</details>


### [139] [On-the-Fly Object-aware Representative Point Selection in Point Cloud](https://arxiv.org/abs/2508.01980)
*Xiaoyu Zhang,Ziwei Wang,Hai Dong,Zhifeng Bao,Jiajun Liu*

Main category: cs.CV

TL;DR: 提出了一种点云下采样框架，通过保留关键物体信息并过滤无关背景点，解决了自动驾驶中大量点云数据的存储和处理问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆生成的点云数据量巨大，对存储、带宽和处理成本提出挑战，需高效下采样方法。

Method: 包括两步：无监督密度峰值分类器与监督朴素贝叶斯分类器检测物体存在；分配采样预算策略以保留物体信息。

Result: 在KITTI和nuScenes数据集上表现优于现有基准方法，效率和效果均优，且模型无关性强。

Conclusion: 该框架为点云下采样提供了高效、可扩展的解决方案，适用于多种下游模型。

Abstract: Point clouds are essential for object modeling and play a critical role in
assisting driving tasks for autonomous vehicles (AVs). However, the significant
volume of data generated by AVs creates challenges for storage, bandwidth, and
processing cost. To tackle these challenges, we propose a representative point
selection framework for point cloud downsampling, which preserves critical
object-related information while effectively filtering out irrelevant
background points. Our method involves two steps: (1) Object Presence
Detection, where we introduce an unsupervised density peak-based classifier and
a supervised Na\"ive Bayes classifier to handle diverse scenarios, and (2)
Sampling Budget Allocation, where we propose a strategy that selects
object-relevant points while maintaining a high retention rate of object
information. Extensive experiments on the KITTI and nuScenes datasets
demonstrate that our method consistently outperforms state-of-the-art baselines
in both efficiency and effectiveness across varying sampling rates. As a
model-agnostic solution, our approach integrates seamlessly with diverse
downstream models, making it a valuable and scalable addition to the 3D point
cloud downsampling toolkit for AV applications.

</details>


### [140] [GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting](https://arxiv.org/abs/2508.02172)
*Lei Yao,Yi Wang,Yi Zhang,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: GaussianCross是一种新型跨模态自监督3D表示学习架构，结合3D高斯泼溅技术，解决了现有方法中模型崩塌和结构信息缺失的问题，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练方法在3D场景理解中存在点区分难度不足，导致模型崩塌和结构信息缺失，表现不佳。

Method: 提出GaussianCross架构，通过3D高斯泼溅技术将点云统一归一化为高斯表示，并引入三属性自适应蒸馏模块，捕捉多模态特征。

Result: 在ScanNet等基准测试中，GaussianCross展现出卓越的参数和数据效率，并在语义和实例分割任务中显著提升性能。

Conclusion: GaussianCross通过跨模态一致性学习和高效表示，显著提升了3D场景理解的性能，具有广泛的应用潜力。

Abstract: The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

</details>


### [141] [Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search](https://arxiv.org/abs/2508.02340)
*Fan Hu,Zijie Xin,Xirong Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为LPD的方法，通过构建特征特定的共同空间并引入去相关损失，以解决Ad-hoc视频搜索中的视觉多样性问题。实验证明LPD在TRECVID AVS基准测试中表现优异，并能增强结果多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合多特征到共同空间时忽略了多样性需求，LPD旨在充分利用个体特征的表达能力，提升视频搜索的全面性。

Method: LPD通过特征特定的共同空间构建和去相关损失，多样化负样本排序，并设计基于熵的公平多空间三元组排序损失以确保空间一致性。

Result: 在TRECVID AVS基准测试中，LPD表现优异，且通过空间多样性可视化证明了其增强结果多样性的能力。

Conclusion: LPD通过多空间设计和去相关优化，显著提升了Ad-hoc视频搜索的全面性和多样性，为未来相关研究提供了新思路。

Abstract: Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.

</details>


### [142] [EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses](https://arxiv.org/abs/2508.01915)
*Akshay Paruchuri,Sinan Hersek,Lavisha Aggarwal,Qiao Yang,Xin Liu,Achin Kulshrestha,Andrea Colaco,Henry Fuchs,Ishan Chatterjee*

Main category: cs.CV

TL;DR: EgoTrigger 是一种通过音频触发智能眼镜摄像头的方法，以节省能源并提高人类记忆增强的效率。


<details>
  <summary>Details</summary>
Motivation: 解决智能眼镜全天使用时的能源效率问题，同时实现多模态AI代理的集成以增强人类记忆。

Method: 利用轻量级音频模型（YAMNet）和自定义分类头，通过手部与物体交互（HOI）的音频线索选择性激活摄像头。

Result: EgoTrigger 平均可减少54%的帧数使用，显著节省能源，同时在记忆任务数据集上表现可比。

Conclusion: 上下文感知触发策略为全天候能源高效智能眼镜的应用提供了可行方向。

Abstract: All-day smart glasses are likely to emerge as platforms capable of continuous
contextual sensing, uniquely positioning them for unprecedented assistance in
our daily lives. Integrating the multi-modal AI agents required for human
memory enhancement while performing continuous sensing, however, presents a
major energy efficiency challenge for all-day usage. Achieving this balance
requires intelligent, context-aware sensor management. Our approach,
EgoTrigger, leverages audio cues from the microphone to selectively activate
power-intensive cameras, enabling efficient sensing while preserving
substantial utility for human memory enhancement. EgoTrigger uses a lightweight
audio model (YAMNet) and a custom classification head to trigger image capture
from hand-object interaction (HOI) audio cues, such as the sound of a drawer
opening or a medication bottle being opened. In addition to evaluating on the
QA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement
Question-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated
first-person QA pairs from full-length Ego4D videos that were curated to ensure
that they contained audio, focusing on HOI moments critical for contextual
understanding and memory. Our results show EgoTrigger can use 54% fewer frames
on average, significantly saving energy in both power-hungry sensing components
(e.g., cameras) and downstream operations (e.g., wireless transmission), while
achieving comparable performance on datasets for an episodic memory task. We
believe this context-aware triggering strategy represents a promising direction
for enabling energy-efficient, functional smart glasses capable of all-day use
-- supporting applications like helping users recall where they placed their
keys or information about their routine activities (e.g., taking medications).

</details>


### [143] [Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust](https://arxiv.org/abs/2508.01316)
*Mohsen Abbaspour Onari,Lucie Charlotte Magister,Yaoxin Wu,Amalia Lupi,Dario Creazzo,Mattia Tordin,Luigi Di Donatantonio,Emilio Quaia,Chao Zhang,Isel Grau,Marco S. Nobile,Yingqian Zhang,Pietro Liò*

Main category: cs.CV

TL;DR: 提出了一种新型多模态注意力融合架构，通过结合全局和局部特征的深度学习模型，提高了远端肌病的分类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 远端肌病的诊断具有挑战性，需要结合全局和局部信息以提高准确性和透明度。

Method: 使用两个深度学习模型提取互补特征，并通过注意力门机制融合，生成具有临床意义的显著图。

Result: 在基准数据集和专有数据集上实现高分类准确性，但解剖特异性和临床实用性仍需改进。

Conclusion: 需要更丰富的上下文感知解释方法和人机反馈，以满足临床需求。

Abstract: Distal myopathy represents a genetically heterogeneous group of skeletal
muscle disorders with broad clinical manifestations, posing diagnostic
challenges in radiology. To address this, we propose a novel multimodal
attention-aware fusion architecture that combines features extracted from two
distinct deep learning models, one capturing global contextual information and
the other focusing on local details, representing complementary aspects of the
input data. Uniquely, our approach integrates these features through an
attention gate mechanism, enhancing both predictive performance and
interpretability. Our method achieves a high classification accuracy on the
BUSI benchmark and a proprietary distal myopathy dataset, while also generating
clinically relevant saliency maps that support transparent decision-making in
medical diagnosis. We rigorously evaluated interpretability through (1)
functionally grounded metrics, coherence scoring against reference masks and
incremental deletion analysis, and (2) application-grounded validation with
seven expert radiologists. While our fusion strategy boosts predictive
performance relative to single-stream and alternative fusion strategies, both
quantitative and qualitative evaluations reveal persistent gaps in anatomical
specificity and clinical usefulness of the interpretability. These findings
highlight the need for richer, context-aware interpretability methods and
human-in-the-loop feedback to meet clinicians' expectations in real-world
diagnostic settings.

</details>


### [144] [Distinguishing Target and Non-Target Fixations with EEG and Eye Tracking in Realistic Visual Scenes](https://arxiv.org/abs/2508.01853)
*Mansi Sharma,Camilo Andrés Martínez Martínez,Benedikt Emanuel Wirth,Antonio Krüger,Philipp Müller*

Main category: cs.CV

TL;DR: 研究提出了一种基于眼动和脑电图的分类方法，用于区分现实场景中目标与非目标注视点，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在分类目标与非目标注视点时，忽视了场景特性，无法适用于现实场景，因此需要改进。

Method: 采用36名参与者的用户研究，使用140个现实场景，结合眼动和脑电特征进行分类。

Result: 新方法在跨用户评估中达到83.6%的准确率，显著优于之前基于扫视相关电位的方法（56.9%）。

Conclusion: 该方法显著提升了现实场景中目标与非目标注视点分类的准确性，具有实际应用潜力。

Abstract: Distinguishing target from non-target fixations during visual search is a
fundamental building block to understand users' intended actions and to build
effective assistance systems. While prior research indicated the feasibility of
classifying target vs. non-target fixations based on eye tracking and
electroencephalography (EEG) data, these studies were conducted with explicitly
instructed search trajectories, abstract visual stimuli, and disregarded any
scene context. This is in stark contrast with the fact that human visual search
is largely driven by scene characteristics and raises questions regarding
generalizability to more realistic scenarios. To close this gap, we, for the
first time, investigate the classification of target vs. non-target fixations
during free visual search in realistic scenes. In particular, we conducted a
36-participants user study using a large variety of 140 realistic visual search
scenes in two highly relevant application scenarios: searching for icons on
desktop backgrounds and finding tools in a cluttered workshop. Our approach
based on gaze and EEG features outperforms the previous state-of-the-art
approach based on a combination of fixation duration and saccade-related
potentials. We perform extensive evaluations to assess the generalizability of
our approach across scene types. Our approach significantly advances the
ability to distinguish between target and non-target fixations in realistic
scenarios, achieving 83.6% accuracy in cross-user evaluations. This
substantially outperforms previous methods based on saccade-related potentials,
which reached only 56.9% accuracy.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [145] [Efficient Direct-Access Ranked Retrieval](https://arxiv.org/abs/2508.01108)
*Mohsen Dehghankar,Raghav Mittal,Suraj Shetiya,Abolfazl Asudeh,Gautam Das*

Main category: cs.DS

TL;DR: 论文提出了两种算法解决高维数据直接访问排名检索（DAR）问题，一种基于几何排列但空间复杂度高，另一种基于ε采样，空间线性。为了解决精确排名定位的挑战，引入了一种松弛变体CSR，并通过定义中间问题SRR设计了一种高效的数据结构。


<details>
  <summary>Details</summary>
Motivation: 随着数据探索实践的发展和大型高维数据集的普及，需要一种高效的方法直接访问任意排名位置而无需枚举所有前置元组。

Method: 提出了两种算法：基于几何排列的理论高效算法和基于ε采样的线性空间算法；引入CSR松弛变体并通过SRR问题设计了分层采样数据结构。

Result: 算法在理论和实验上验证了高效性，能够扩展到数百万元组和数百维的数据集。

Conclusion: 论文提出的方法在解决高维数据直接访问排名检索问题上具有理论和实践优势，适合大规模和高维度场景。

Abstract: We study the problem of Direct-Access Ranked Retrieval (DAR) for interactive
data tooling, where evolving data exploration practices, combined with
large-scale and high-dimensional datasets, create new challenges. DAR concerns
the problem of enabling efficient access to arbitrary rank positions according
to a ranking function, without enumerating all preceding tuples. To address
this need, we formalize the DAR problem and propose a theoretically efficient
algorithm based on geometric arrangements, achieving logarithmic query time.
However, this method suffers from exponential space complexity in high
dimensions. Therefore, we develop a second class of algorithms based on
$\varepsilon$-sampling, which consume a linear space. Since exactly locating
the tuple at a specific rank is challenging due to its connection to the range
counting problem, we introduce a relaxed variant called Conformal Set Ranked
Retrieval (CSR), which returns a small subset guaranteed to contain the target
tuple. To solve the CSR problem efficiently, we define an intermediate problem,
Stripe Range Retrieval (SRR), and design a hierarchical sampling data structure
tailored for narrow-range queries. Our method achieves practical scalability in
both data size and dimensionality. We prove near-optimal bounds on the
efficiency of our algorithms and validate their performance through extensive
experiments on real and synthetic datasets, demonstrating scalability to
millions of tuples and hundreds of dimensions.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [146] [A Myhill-Nerode Theorem for Generalized Automata, with Applications to Pattern Matching and Compression](https://arxiv.org/abs/2302.06506)
*Nicola Cotumaccio*

Main category: cs.FL

TL;DR: 提出了广义自动机的Myhill-Nerode定理，并展示了其在模式匹配和数据压缩中的应用。


<details>
  <summary>Details</summary>
Motivation: 广义自动机比常规自动机更简洁，但最小化的唯一性缺失。研究旨在解释这一现象并扩展其应用。

Method: 引入集合W(A)来固定广义自动机的结构，推导Myhill-Nerode定理，并应用于Wheeler自动机的扩展。

Result: 成功为广义自动机建立Myhill-Nerode定理，并优化其存储和查询效率。

Conclusion: 广义自动机的理论扩展为新应用提供了可能，特别是在压缩和模式匹配领域。

Abstract: The model of generalized automata, introduced by Eilenberg in 1974, allows
representing a regular language more concisely than conventional automata by
allowing edges to be labeled not only with characters, but also strings.
Giammaresi and Montalbano introduced a notion of determinism for generalized
automata [STACS 1995]. While generalized deterministic automata retain many
properties of conventional deterministic automata, the uniqueness of a minimal
generalized deterministic automaton is lost.
  In the first part of the paper, we show that the lack of uniqueness can be
explained by introducing a set $ \mathcal{W(A)} $ associated with a generalized
automaton $ \mathcal{A} $. By fixing $ \mathcal{W(A)} $, we are able to derive
for the first time a full Myhill-Nerode theorem for generalized automata, which
contains the textbook Myhill-Nerode theorem for conventional automata as a
degenerate case.
  In the second part of the paper, we show that the set $ \mathcal{W(A)} $
leads to applications for pattern matching and data compression. Wheeler
automata [TCS 2017, SODA 2020] are a popular class of automata that can be
compactly stored using $ e \log \sigma (1 + o(1)) + O(e) $ bits ($ e $ being
the number of edges, $ \sigma $ being the size of the alphabet) in such a way
that pattern matching queries can be solved in $ \tilde{O}(m) $ time ($ m $
being the length of the pattern). In the paper, we show how to extend these
results to generalized automata. More precisely, a Wheeler generalized automata
can be stored using $ \mathfrak{e} \log \sigma (1 + o(1)) + O(e + rn) $ bits so
that pattern matching queries can be solved in $ \tilde{O}(r m) $ time, where $
\mathfrak{e} $ is the total length of all edge labels, $ r $ is the maximum
length of an edge label and $ n $ is the number of states.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [147] [SSBD Ontology: A Two-Tier Approach for Interoperable Bioimaging Metadata](https://arxiv.org/abs/2508.02084)
*Yuki Yamagata,Koji Kyoda,Hiroya Itoga,Emi Fujisawa,Shuichi Onami*

Main category: cs.DL

TL;DR: 提出了一种基于本体驱动的两层级架构（核心层和实例层），用于生物动力学数据库（SSBD），以解决生物成像数据的元数据管理和互操作性挑战。


<details>
  <summary>Details</summary>
Motivation: 生物成像技术生成的多维数据在元数据管理和互操作性方面存在显著问题，需要一种灵活且严谨的解决方案。

Method: 采用两层级架构：核心层引用现有生物医学本体，支持快速数据集发布和增强注释；实例层将实际数据表示为RDF个体，与核心类显式链接。

Result: 该框架实现了数据的无缝整合和高级语义查询，促进了互操作性、数据重用和新生物学机制的发现。

Conclusion: SSBD本体结合灵活性与严谨性，遵循生物图像元数据指南，为生物成像社区建立了FAIR（可查找、可访问、可互操作、可重用）数据生态系统。

Abstract: Advanced bioimaging technologies have enabled the large-scale acquisition of
multidimensional data, yet effective metadata management and interoperability
remain significant challenges. To address these issues, we propose a new
ontology-driven framework for the Systems Science of Biological Dynamics
Database (SSBD) that adopts a two-tier architecture. The core layer provides a
class-centric structure referencing existing biomedical ontologies, supporting
both SSBD:repository -- which focuses on rapid dataset publication with minimal
metadata -- and SSBD:database, which is enhanced with biological and
imaging-related annotations. Meanwhile, the instance layer represents actual
imaging dataset information as Resource Description Framework individuals that
are explicitly linked to the core classes. This layered approach aligns
flexible instance data with robust ontological classes, enabling seamless
integration and advanced semantic queries. By coupling flexibility with rigor,
the SSBD Ontology promotes interoperability, data reuse, and the discovery of
novel biological mechanisms. Moreover, our solution aligns with the Recommended
Metadata for Biological Images guidelines and fosters compatibility.
Ultimately, our approach contributes to establishing a Findable, Accessible,
Interoperable, and Reusable data ecosystem within the bioimaging community.

</details>


### [148] [Author Once, Publish Everywhere: Portable Metadata Authoring with the CEDAR Embeddable Editor](https://arxiv.org/abs/2508.00859)
*Martin J. O'Connor,Marcos Martinez-Romero,Attila L. Egyedi,Mete U. Akdogan,Michael V. Dorf,Mark A. Musen*

Main category: cs.DL

TL;DR: CEDAR Embeddable Editor (CEE) 是一个轻量级 Web 组件，可嵌入第三方平台，直接生成结构化、基于标准的元数据，提升元数据质量。


<details>
  <summary>Details</summary>
Motivation: 高质量、丰富的元数据对研究数据的可查找性、互操作性和可重用性至关重要。

Method: CEDAR 开发了 CEE，支持动态渲染元数据表单，并通过 BioPortal 本体库和外部权威解析（如 ORCID 和 ROR）实现基于本体的值选择。

Result: CEE 已成功集成到 Dryad 和 Open Science Framework 等平台，支持学科特定元数据创建。

Conclusion: CEE 通过在现有研究环境中嵌入元数据创作，促进社区标准采用，提高跨学科元数据质量。

Abstract: High-quality, "rich" metadata are essential for making research data
findable, interoperable, and reusable. The Center for Expanded Data Annotation
and Retrieval (CEDAR) has long addressed this need by providing tools to design
machine-actionable metadata templates that encode community standards in a
computable form. To make these capabilities more accessible within real-world
research workflows, we have developed the CEDAR Embeddable Editor (CEE)-a
lightweight, interoperable Web Component that brings structured,
standards-based metadata authoring directly into third-party platforms. The CEE
dynamically renders metadata forms from machine-actionable templates and
produces semantically rich metadata in JSON-LD format. It supports
ontology-based value selection via the BioPortal ontology repository, and it
includes external authority resolution for persistent identifiers such as
ORCIDs for individuals and RORs for research organizations. Crucially, the CEE
requires no custom user-interface development, allowing deployment across
diverse platforms. The CEE has been successfully integrated into generalist
scientific data repositories such as Dryad and the Open Science Framework,
demonstrating its ability to support discipline-specific metadata creation. By
supporting the embedding of metadata authoring within existing research
environments, the CEE can facilitate the adoption of community standards and
help improve metadata quality across scientific disciplines.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [149] [Modeling Head-Neck Dynamics under Lateral Perturbations Using MPC to Mimic CNS postural stabilization strategy](https://arxiv.org/abs/2508.00928)
*Chrysovalanto Messiou,Riender Happee,Georgios Papaioannou*

Main category: eess.SY

TL;DR: 研究探讨了自动车辆中突发侧向扰动对乘员头部-颈部姿势控制的影响，提出了一种基于模型预测控制的模拟方法，并与人体数据进行了验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆中乘员可能遭遇意外扰动，缺乏视觉提示会影响姿势调整，因此需要更准确的头部-颈部姿势控制模型以评估舒适性。

Method: 扩展了现有的模型预测控制框架，模拟侧向扰动下的头部-颈部姿势控制，并通过实验数据验证模型有效性。

Result: 模型能准确模拟侧向扰动下的动态响应，显示肌肉努力和部分体感反馈提供了最佳动态拟合，无需额外的头部方向积分器。

Conclusion: 提出的模型为自动驾驶车辆乘员舒适性评估提供了有效工具，强调了体感反馈和肌肉控制在姿势稳定中的重要性。

Abstract: Automated vehicles will allow occupants to engage in non-driving tasks, but
limited visual cues will make them vulnerable to unexpected movements. These
unpredictable perturbations create a "surprise factor," forcing the central
nervous system to rely on compensatory postural adjustments, which are less
effective, and are more likely to trigger sensory conflicts. Since the head is
a key reference for sensory input (vestibular and vision), models accurately
capturing head-neck postural stabilization are essential for assessing AV
comfort. This study extends an existing model predictive control-based
framework to simulate head-neck postural control under lateral perturbations.
Experimental validation against human data demonstrates that the model can
accurately reproduce dynamic responses during lateral trunk perturbations. The
results show that muscle effort combined with partial somatosensory feedback
provides the best overall dynamic fit without requiring corrective relative and
global head orientation integrators for posture.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [150] [A General Approach to Visualizing Uncertainty in Statistical Graphics](https://arxiv.org/abs/2508.00937)
*Bernarda Petek,David Nabergoj,Erik Štrumbelj*

Main category: stat.ME

TL;DR: 该论文提出了一种通用的不确定性可视化方法，通过将统计图形视为底层分布的函数，直接传播不确定性到可视化中，从而简化了不确定性量化和表示的过程。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性可视化方法需要针对不同类型的图形使用专门的技术，这限制了其应用。本文旨在提供一种通用的方法，使不确定性可视化更易于实现和使用。

Method: 该方法的核心思想是将统计图形视为底层分布的函数，通过从数据分布中重复采样并为每个样本生成完整的统计图形，从而产生图形上的分布，最终通过像素级聚合生成单一静态图像。

Result: 该方法不仅能够重现传统的不确定性可视化（如置信区间和带），还能扩展到非标准案例（如饼图、堆叠条形图和表格），且无需用户具备特定知识。

Conclusion: 该通用方法使不确定性可视化更易于实现，适用于广泛的统计图形，并可作为教授不确定性的有效工具。

Abstract: Visualizing uncertainty is integral to data analysis, yet its application is
often hindered by the need for specialized methods for quantifying and
representing uncertainty for different types of graphics. We introduce a
general approach that simplifies this process. The core idea is to treat the
statistical graphic as a function of the underlying distribution. Instead of
first calculating uncertainty metrics and then plotting them, the method
propagates uncertainty through to the visualization. By repeatedly sampling
from the data distribution and generating a complete statistical graphic for
each sample, a distribution over graphics is produced. These graphics are
aggregated pixel-by-pixel to create a single, static image. This approach is
versatile, requires no specific knowledge from the user beyond how to create
the basic statistical graphic, and comes with theoretical coverage guarantees
for standard cases such as confidence intervals and bands. We provide a
reference implementation as a Python library to demonstrate the method's
utility. Our approach not only reproduces conventional uncertainty
visualizations for point estimates and regression lines but also seamlessly
extends to non-standard cases, including pie charts, stacked bar charts, and
tables. This approach makes uncertainty visualization more accessible to
practitioners and can be a valuable tool for teaching uncertainty.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [151] [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](https://arxiv.org/abs/2508.02317)
*Qianli Ma,Yaowei Zheng,Zhelun Shi,Zhongkai Zhao,Bin Jia,Ziyue Huang,Zhiqi Lin,Youjie Li,Jiacheng Yang,Yanghua Peng,Zhi Zhang,Xin Liu*

Main category: cs.CL

TL;DR: 论文提出了一种模块化且高效的训练框架\veomni，用于加速全模态大型语言模型（LLMs）的开发，解决了现有框架在可扩展性和工程开销上的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的全模态LLMs训练面临架构异构性及并行逻辑与模型定义耦合的问题，导致可扩展性受限且工程开销大。

Method: \veomni框架通过模型中心的分布式方案将通信与计算解耦，支持高效3D并行，并提供灵活的配置接口以无缝集成新模态。

Result: 使用\veomni训练30B参数的全模态专家混合模型（MoE），在128 GPU上实现了每秒2,800 token/GPU的高吞吐量，并可扩展至160K上下文长度。

Conclusion: \veomni展现了全模态LLMs训练的高效性和可扩展性，为未来研究提供了有力工具。

Abstract: Recent advances in large language models (LLMs) have driven impressive
progress in omni-modal understanding and generation. However, training
omni-modal LLMs remains a significant challenge due to the heterogeneous model
architectures required to process diverse modalities, necessitating
sophisticated system design for efficient large-scale training. Existing
frameworks typically entangle model definition with parallel logic, incurring
limited scalability and substantial engineering overhead for end-to-end
omni-modal training. % We present \veomni, a modular and efficient training
framework to accelerate the development of omni-modal LLMs. \veomni introduces
model-centric distributed recipes that decouples communication from
computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also
features a flexible configuration interface supporting seamless integration of
new modalities with minimal code change. % Using \veomni, a omni-modal
mixture-of-experts (MoE) model with 30B parameters can be trained with over
2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D
parallelism on 128 GPUs, showcasing its superior efficiency and scalability for
training large omni-modal LLMs.

</details>


### [152] [Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes](https://arxiv.org/abs/1907.00326)
*Jie Cao,Michael Tanana,Zac E. Imel,Eric Poitras,David C. Atkins,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文研究了如何通过对话分析实时指导心理治疗师，利用神经网络模型分类和预测治疗对话中的行为代码，模型表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 通过自动分析心理治疗对话，帮助指导和改进治疗师的行为，特别是在动机访谈（MI）这种有效治疗物质滥用等问题的场景中。

Method: 采用神经网络模型，构建对话观察器，分类和预测治疗师与患者的MI行为代码，并对模型设计进行详细分析。

Result: 实验表明，模型在分类和预测任务上优于基线方法，同时揭示了网络设计对治疗对话建模的具体影响。

Conclusion: 神经网络模型可以有效支持心理治疗对话的实时分析和指导，为治疗师提供有用的工具。

Abstract: Automatically analyzing dialogue can help understand and guide behavior in
domains such as counseling, where interactions are largely mediated by
conversation. In this paper, we study modeling behavioral codes used to asses a
psychotherapy treatment style called Motivational Interviewing (MI), which is
effective for addressing substance abuse and related problems. Specifically, we
address the problem of providing real-time guidance to therapists with a
dialogue observer that (1) categorizes therapist and client MI behavioral codes
and, (2) forecasts codes for upcoming utterances to help guide the conversation
and potentially alert the therapist. For both tasks, we define neural network
models that build upon recent successes in dialogue modeling. Our experiments
demonstrate that our models can outperform several baselines for both tasks. We
also report the results of a careful analysis that reveals the impact of the
various network design tradeoffs for modeling therapy dialogue.

</details>


### [153] [Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse](https://arxiv.org/abs/2412.13395)
*Jie Cao,Abhijit Suresh,Jennifer Jacobs,Charis Clevenger,Amanda Howard,Chelsea Brown,Brent Milne,Tom Fischaber,Tamara Sumner,James H. Martin*

Main category: cs.CL

TL;DR: 论文提出了SAGA22数据集，研究了在数学辅导对话中应用‘谈话动作’框架的模型建模策略，结果表明课堂数据的预训练能提升辅导场景的模型表现。


<details>
  <summary>Details</summary>
Motivation: 人类辅导干预对学习至关重要，但大规模辅导对话的收集和分析是资源密集型任务。

Method: 使用SAGA22数据集，探讨了对话上下文、说话者信息、预训练数据集和微调等建模策略。

Result: 课堂数据的预训练尤其结合长上下文和说话者信息时，可提升辅导场景的模型表现。

Conclusion: 研究凸显了谈话动作建模的挑战，并展示了预训练数据的重要性。

Abstract: Human tutoring interventions play a crucial role in supporting student
learning, improving academic performance, and promoting personal growth. This
paper focuses on analyzing mathematics tutoring discourse using talk moves - a
framework of dialogue acts grounded in Accountable Talk theory. However,
scaling the collection, annotation, and analysis of extensive tutoring
dialogues to develop machine learning models is a challenging and
resource-intensive task. To address this, we present SAGA22, a compact dataset,
and explore various modeling strategies, including dialogue context, speaker
information, pretraining datasets, and further fine-tuning. By leveraging
existing datasets and models designed for classroom teaching, our results
demonstrate that supplementary pretraining on classroom data enhances model
performance in tutoring settings, particularly when incorporating longer
context and speaker information. Additionally, we conduct extensive ablation
studies to underscore the challenges in talk move modeling.

</details>


### [154] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
*Shengqi Zhu,Jeffrey M. Rzeszotarski,David Mimno*

Main category: cs.CL

TL;DR: 该论文提出了一种新任务，将聊天查询分割为请求内容、角色、查询特定上下文和附加表达，发现LLM查询中的请求行为与人际交互有显著差异，并通过用户表达的时间维度分析展示了用户行为随经验和模型能力的变化。


<details>
  <summary>Details</summary>
Motivation: 研究LLM用户行为模式，因查询变异性常被掩盖，需开发新方法来分割和分析聊天日志。

Method: 提出新任务，分段聊天查询为不同组成部分，并基于数据资源进行历时分析。

Result: 发现早期查询侧重请求，用户随经验趋于收敛；模型能力变化（如新模型引入）对用户行为有显著影响。

Conclusion: LLM用户行为模式随经验和模型更新而变化，新任务和历时分析为理解用户行为提供了重要视角。

Abstract: Chat logs provide a rich source of information about LLM users, but patterns
of user behavior are often masked by the variability of queries. We present a
new task, segmenting chat queries into contents of requests, roles,
query-specific context, and additional expressions. We find that, despite the
familiarity of chat-based interaction, request-making in LLM queries remains
significantly different from comparable human-human interactions. With the data
resource, we introduce an important perspective of diachronic analyses with
user expressions. We find that query patterns vary between early ones
emphasizing requests, and individual users explore patterns but tend to
converge with experience. Finally, we show that model capabilities affect user
behavior, particularly with the introduction of new models, which are traceable
at the community level.

</details>


### [155] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
*Lucio La Cava,Dominik Macko,Róbert Móro,Ivan Srba,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 论文讨论了如何区分机器生成文本（MGT）和人类写作内容的挑战，提出并研究了多语言作者归属（AA）问题，评估了单语言AA方法在多语言环境中的适用性及其跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在文本生成方面达到类人水平，区分机器生成文本和人类写作内容变得愈发困难。当前AA研究主要集中在单语言（尤其是英语）环境中，忽视了多语言场景的复杂性和现代LLMs的多语言使用。

Method: 研究覆盖了18种语言（涵盖多种语系和书写系统）和8种生成器（7个LLMs和人类书写类），评估了单语言AA方法在多语言环境中的适用性、跨语言迁移能力，以及不同生成器对AA性能的影响。

Result: 研究发现某些单语言AA方法可以适应多语言环境，但在跨语系迁移时仍存在显著局限性和挑战，凸显了多语言AA的复杂性，需要更鲁棒的方法以匹配实际场景。

Conclusion: 多语言AA问题具有挑战性，当前单语言方法的局限性表明需要开发更强大的方法，以更好地应对多语言环境中的真实需求。

Abstract: As Large Language Models (LLMs) have reached human-like fluency and
coherence, distinguishing machine-generated text (MGT) from human-written
content becomes increasingly difficult. While early efforts in MGT detection
have focused on binary classification, the growing landscape and diversity of
LLMs require a more fine-grained yet challenging authorship attribution (AA),
i.e., being able to identify the precise generator (LLM or human) behind a
text. However, AA remains nowadays confined to a monolingual setting, with
English being the most investigated one, overlooking the multilingual nature
and usage of modern LLMs. In this work, we introduce the problem of
Multilingual Authorship Attribution, which involves attributing texts to human
or multiple LLM generators across diverse languages. Focusing on 18 languages
-- covering multiple families and writing scripts -- and 8 generators (7 LLMs
and the human-authored class), we investigate the multilingual suitability of
monolingual AA methods, their cross-lingual transferability, and the impact of
generators on attribution performance. Our results reveal that while certain
monolingual AA methods can be adapted to multilingual settings, significant
limitations and challenges remain, particularly in transferring across diverse
language families, underscoring the complexity of multilingual AA and the need
for more robust approaches to better match real-world scenarios.

</details>


### [156] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
*Tae Soo Kim,Yoonjoo Lee,Yoonah Park,Jiho Kim,Young-Ho Kim,Juho Kim*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）个性化时用户动态偏好的问题，提出CUPID基准测试，评估LLMs是否能从多轮交互中推断用户偏好，结果显示现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs个性化研究假设用户偏好是静态的，而现实中用户偏好是动态变化的。研究旨在解决LLMs如何从多轮交互中推断和适应用户的上下文偏好。

Method: 提出CUPID基准测试，包含756个人工标注的用户与LLM交互历史记录，评估LLMs在新请求中推断和应用用户偏好的能力。

Result: 评估10个开源和专有LLMs，发现它们在多轮交互中推断偏好的表现差（精确度低于50%，召回率低于65%）。

Conclusion: 研究说明LLMs在上下文个性化方面能力不足，并建议CUPID作为推动改进的资源。

Abstract: Personalization of Large Language Models (LLMs) often assumes users hold
static preferences that reflect globally in all tasks. In reality, humans hold
dynamic preferences that change depending on the context. As users interact
with an LLM in various contexts, they naturally reveal their contextual
preferences, which a model must infer and apply in future contexts to ensure
alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated
interaction session histories between users and LLM-based chat assistants. In
each interaction session, the user provides a request in a specific context and
expresses their preference through multi-turn feedback. Given a new user
request and prior interaction sessions, our benchmark assesses whether LLMs can
infer the preference relevant to this request and generate a response that
satisfies this preference. With CUPID, we evaluated 10 open and proprietary
LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from
multi-turn interactions and fail to discern what previous context is relevant
to a new request -- under 50% precision and 65% recall. Our work highlights the
need to advance LLM capabilities for more contextually personalized
interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [157] ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)
*Yaqiong Li,Peng Zhang,Lin Wang,Hansu Gu,Siyuan Qiao,Ning Gu,Tun Lu*

Main category: cs.CL

TL;DR: 本文研究青少年对社交媒体中有毒内容的主观感知，发现了与成人不同的‘青少年毒性’语言特征，并构建首个中文‘青少年毒性’数据集，通过分析发现上下文因素重要，改进现有检测技术。


<details>
  <summary>Details</summary>
Motivation: 研究青少年独特的毒性感知（成人认为无毒但青少年认为有毒的内容），填补现有毒性检测研究的空白。

Method: 以中国青少年为目标，构建首个中文‘青少年毒性’数据集，分析特征并测试现有检测技术的准确性。

Result: 发现青少年的毒性感知与上下文因素相关，改进现有技术能显著提升检测准确性。

Conclusion: 研究为未来青少年中心的毒性检测提供了洞察和改进方向。

Abstract: Risk perception is subjective, and youth's understanding of toxic content
differs from that of adults. Although previous research has conducted extensive
studies on toxicity detection in social media, the investigation of youth's
unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as
youth, is ignored. To address this gap, we aim to explore: 1) What are the
features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing
toxicity detection techniques accurately detect these languages (RQ2). For
these questions, we took Chinese youth as the research target, constructed the
first Chinese ``youth-toxicity'' dataset, and then conducted extensive
analysis. Our results suggest that youth's perception of these is associated
with several contextual factors, like the source of an utterance and
text-related features. Incorporating these meta information into current
toxicity detection methods significantly improves accuracy overall. Finally, we
propose several insights into future research on youth-centered toxicity
detection.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [158] [Positivity of Nearly Linearly Recurrent Sequences](https://arxiv.org/abs/2508.00944)
*Amaury Pouly,Mahsa Shirmohammadi,James Worrell*

Main category: math.DS

TL;DR: 研究针对几乎线性递归序列的Positivity问题，提出了一个决策过程，适用于特征根绝对值不超过1的三阶序列，并依赖一个新的超越性结果。


<details>
  <summary>Details</summary>
Motivation: 将Positivity问题从线性递归序列推广到几乎线性递归序列，同时考虑其在线性时不变系统中的非可达性问题。

Method: 提出一个决策过程，专门用于处理特征根绝对值不超过1的三阶几乎线性递归序列。

Result: 开发了一种有效的决策过程，依赖于一个新的关于无限级数的超越性结果。

Conclusion: 研究成果为几乎线性递归序列的Positivity问题提供了解决方案，并展示了新的数学工具的应用。

Abstract: In this paper we formulate the Positivity Problem for nearly linear
  recurrent sequences. This is a generalisation of the Positivity
  Problem for linear recurrence sequences and is a special case of the
  non-reachability problem for linear time-invariant systems. Our main
  contribution is a decision procedure for the Positivity Problem for
  nearly linear recurrences of order at most 3 whose characteristic
  roots have absolute value at most one. The decision procedure
  relies on a new transcendence result for infinite series that is of
  independent interest.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [159] [BVQC: A Backdoor-style Watermarking Scheme for Variational Quantum Circuits](https://arxiv.org/abs/2508.01893)
*Cheng Chu,Lei Jiang,Fan Chen*

Main category: quant-ph

TL;DR: 本文提出了一种基于后门的VQC水印技术BVQC，解决了现有技术因重新编译导致水印丢失及任务损失增加的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有VQC水印技术易因重新编译丢失水印，且插入水印会导致显著的任务性能下降，因此需要一种更高效的水印技术。

Method: BVQC采用后门机制，在常规执行中保持任务性能，仅在水印提取时增加损失，并通过分组算法最小化水印对基础任务的干扰。

Result: 相比现有技术，BVQC将PPA变化降低了9.89e-3，GTD降低了0.089，且保持了原始编译流程的鲁棒性。

Conclusion: BVQC有效解决了VQC水印技术的两大问题，显著提升了水印的鲁棒性和任务性能。

Abstract: Variational Quantum Circuits (VQCs) have emerged as a powerful quantum
computing paradigm, demonstrating a scaling advantage for problems intractable
for classical computation. As VQCs require substantial resources and
specialized expertise for their design, they represent significant intellectual
properties (IPs). However, existing quantum circuit watermarking techniques
suffer from two primary drawbacks: (1) watermarks can be removed during
re-compilation of the circuits, and (2) these methods significantly increase
task loss due to the extensive length of the inserted watermarks across
multiple compilation stages. To address these challenges, we propose BVQC, a
backdoor-based watermarking technique for VQCs that preserves the original loss
in typical execution settings, while deliberately increasing the loss to a
predefined level during watermark extraction. Additionally, BVQC employs a
grouping algorithm to minimize the watermark task's interference with the base
task, ensuring optimal accuracy for the base task. BVQC retains the original
compilation workflow, ensuring robustness against re-compilation. Our
evaluations show that BVQC greatly reduces Probabilistic Proof of Authorship
(PPA) changes by 9.89e-3 and ground truth distance (GTD) by 0.089 compared to
prior watermarking technologies.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [160] [Academic Vibe Coding: Opportunities for Accelerating Research in an Era of Resource Constraint](https://arxiv.org/abs/2508.00952)
*Matthew G Crowson,Leo Celi A. Celi*

Main category: cs.CY

TL;DR: Vibe coding利用大型语言模型生成代码，以缓解学术实验室资源紧张的问题。


<details>
  <summary>Details</summary>
Motivation: 学术实验室面临预算紧缩和人才短缺，亟需高效、低成本的解决方案。

Method: 结构化、提示驱动的代码生成，嵌入可重复的工作流程。

Result: 缩短分析时间，减轻人力资源压力，保持严格版本控制。

Conclusion: Vibe coding是一种实用工具，但需结合监管和谨慎使用。

Abstract: Academic laboratories face mounting resource constraints: budgets are
tightening, grant overheads are potentially being capped, and the market rate
for data-science talent significantly outstrips university compensation. Vibe
coding, which is structured, prompt-driven code generation with large language
models (LLMs) embedded in reproducible workflows, offers one pragmatic
response. It aims to compress the idea-to-analysis timeline, reduce staffing
pressure on specialized data roles, and maintain rigorous, version-controlled
outputs. This article defines the vibe coding concept, situates it against the
current academic resourcing crisis, details a beginner-friendly toolchain for
its implementation, and analyzes inherent limitations that necessitate
governance and mindful application.

</details>


### [161] [Exploring the Role of Gamification in Enhancing Academic Library Services: A Survey of Library Leaders in India](https://arxiv.org/abs/2508.00906)
*Subaveerapandiyan A,Pragya Lohia,Dattatraya Kalbande,Naved Ahmad,Kailash Chand Sharma*

Main category: cs.CY

TL;DR: 研究了游戏化在印度学术图书馆服务中的作用，发现其虽能提升用户参与度，但因资源不足面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探索游戏化如何提升学术图书馆服务的用户参与度和效果。

Method: 通过调查印度各机构的图书馆领导者，分析游戏化元素的应用和影响。

Result: 结果显示游戏化的接受度较高，但受限于资源不足（如技术、资金、培训）。

Conclusion: 建议增加资源投入（如培训和技术升级），以充分发挥游戏化潜力。

Abstract: This study explores the role of gamification in enhancing academic library
services in India by surveying library leaders across various institutions.
Using game-like elements in non-game contexts, gamification can boost user
engagement and improve services such as information literacy and research
consultations. Findings reveal moderate awareness and generally positive
perceptions of gamification's effectiveness. However, challenges like
insufficient staff expertise, infrastructure, and limited funding hinder
implementation. The study emphasises the need for additional resources,
including staff training and technological upgrades, to unlock the full
potential of gamification in academic libraries.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [162] [Detecting COPD Through Speech Analysis: A Dataset of Danish Speech and Machine Learning Approach](https://arxiv.org/abs/2508.02354)
*Cuno Sankey-Olsen,Rasmus Hvass Olesen,Tobias Oliver Eberhard,Andreas Triantafyllopoulos,Björn Schuller,Ilhan Aslan*

Main category: cs.SD

TL;DR: 研究探讨了基于语音分析的非侵入性方法在检测慢性阻塞性肺疾病（COPD）中的潜力，重点关注其在不同语言群体中的有效性。通过收集丹麦参与者的语音数据，使用不同模型分析，最高准确率达到67%。


<details>
  <summary>Details</summary>
Motivation: COPD的早期检测对于改善患者生活质量至关重要。语音作为一种非侵入性生物标志物，其在不同语言群体中的有效性尚未明确验证。

Method: 收集96名丹麦参与者的语音数据（包括阅读、咳嗽、持续元音任务），其中一半为COPD患者，另一半为健康对照组。使用openSMILE特征和x-向量嵌入建模分析。

Result: 逻辑回归结合openSMILE特征时最高准确率为67%。

Conclusion: 语音分析具有作为非侵入性、远程、可扩展的COPD筛查工具的潜力，可应用于未来医疗解决方案。

Abstract: Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating
disease affecting millions around the world. Its early detection using
non-invasive means could enable preventive interventions that improve quality
of life and patient outcomes, with speech recently shown to be a valuable
biomarker. Yet, its validity across different linguistic groups remains to be
seen. To that end, audio data were collected from 96 Danish participants
conducting three speech tasks (reading, coughing, sustained vowels). Half of
the participants were diagnosed with different levels of COPD and the other
half formed a healthy control group. Subsequently, we investigated different
baseline models using openSMILE features and learnt x-vector embeddings. We
obtained a best accuracy of 67% using openSMILE features and logistic
regression. Our findings support the potential of speech-based analysis as a
non-invasive, remote, and scalable screening tool as part of future COPD
healthcare solutions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [163] [Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning](https://arxiv.org/abs/2508.01181)
*Zhiyuan Han,Beier Zhu,Yanlong Xu,Peipei Song,Xun Yang*

Main category: cs.AI

TL;DR: 论文提出新基准CA-MER和框架MoSEAR，解决多模态大语言模型在情感冲突中的音频偏好问题，实现平衡的多模态整合。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在情感冲突场景（不同模态情感线索不一致）下表现不足，需改进。

Method: 提出MoSEAR框架，包含MoSE（减少模态偏见的微调头）和AR（重分配注意力机制）。

Result: 在多个基准测试中，MoSEAR表现最佳，尤其在模态冲突条件下。

Conclusion: MoSEAR有效解决模态偏见，提升情感推理性能，无模态间权衡。

Abstract: Despite their strong performance in multimodal emotion reasoning, existing
Multimodal Large Language Models (MLLMs) often overlook the scenarios involving
emotion conflicts, where emotional cues from different modalities are
inconsistent. To fill this gap, we first introduce CA-MER, a new benchmark
designed to examine MLLMs under realistic emotion conflicts. It consists of
three subsets: video-aligned, audio-aligned, and consistent, where only one or
all modalities reflect the true emotion. However, evaluations on our CA-MER
reveal that current state-of-the-art emotion MLLMs systematically over-rely on
audio signal during emotion conflicts, neglecting critical cues from visual
modality. To mitigate this bias, we propose MoSEAR, a parameter-efficient
framework that promotes balanced modality integration. MoSEAR consists of two
modules: (1)MoSE, modality-specific experts with a regularized gating mechanism
that reduces modality bias in the fine-tuning heads; and (2)AR, an attention
reallocation mechanism that rebalances modality contributions in frozen
backbones during inference. Our framework offers two key advantages: it
mitigates emotion conflicts and improves performance on consistent
samples-without incurring a trade-off between audio and visual modalities.
Experiments on multiple benchmarks-including MER2023, EMER, DFEW, and our
CA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,
particularly under modality conflict conditions.

</details>


### [164] [A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation](https://arxiv.org/abs/2508.00853)
*Kei Itoh*

Main category: cs.AI

TL;DR: 作者提出了一种数学上严谨的状态定义框架，旨在统一跨领域的‘状态’概念，并引入‘分层状态网格’与‘中间元宇宙’以解决自引用和逻辑不一致问题。


<details>
  <summary>Details</summary>
Motivation: 缺乏对‘状态’概念的共识和形式化定义，阻碍了智能定义等系统的理论基础。

Method: 提出了‘分层状态网格’和‘中间元宇宙’，用于统一表示和避免自引用问题。

Result: 构建了一个跨时间、语言和操作的形式逻辑框架，适用于智能定义等领域。

Conclusion: 研究为智能定义和科学理论提供了数学上稳健的基础。

Abstract: This study aims to reinforce the theoretical foundation for diverse
systems--including the axiomatic definition of intelligence--by introducing a
mathematically rigorous and unified formal structure for the concept of
'state,' which has long been used without consensus or formal clarity. First, a
'hierarchical state grid' composed of two axes--state depth and mapping
hierarchy--is proposed to provide a unified notational system applicable across
mathematical, physical, and linguistic domains. Next, the 'Intermediate
Meta-Universe (IMU)' is introduced to enable explicit descriptions of definers
(ourselves) and the languages we use, thereby allowing conscious meta-level
operations while avoiding self-reference and logical inconsistency. Building on
this meta-theoretical foundation, this study expands inter-universal theory
beyond mathematics to include linguistic translation and agent integration,
introducing the conceptual division between macrocosm-inter-universal and
microcosm-inter-universal operations for broader expressivity. Through these
contributions, this paper presents a meta-formal logical framework--grounded in
the principle of definition = state--that spans time, language, agents, and
operations, providing a mathematically robust foundation applicable to the
definition of intelligence, formal logic, and scientific theory at large.

</details>


### [165] [Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework](https://arxiv.org/abs/2508.00844)
*Christopher Wissuchek,Patrick Zschech*

Main category: cs.AI

TL;DR: 论文提出了一个分类和比较自主AI系统的框架，包括八个维度，定义了它们的认知和环境主动性，并通过多阶段方法构建和验证了这一分类。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个结构化框架来分类和比较不断发展的自主AI系统。

Method: 采用多阶段方法构建和细化自主AI系统的分类，并通过人机混合方法进行评估和提炼。

Result: 提出了一个基于八个维度的AI系统分类框架，能帮助研究和实践者分析AI的不同自主性水平。

Conclusion: 该分类框架为评估当前AI系统和预测未来自主AI发展提供了基础。

Abstract: Artificial intelligence (AI) systems are evolving beyond passive tools into
autonomous agents capable of reasoning, adapting, and acting with minimal human
intervention. Despite their growing presence, a structured framework is lacking
to classify and compare these systems. This paper develops a typology of
agentic AI systems, introducing eight dimensions that define their cognitive
and environmental agency in an ordinal structure. Using a multi-phase
methodological approach, we construct and refine this typology, which is then
evaluated through a human-AI hybrid approach and further distilled into
constructed types. The framework enables researchers and practitioners to
analyze varying levels of agency in AI systems. By offering a structured
perspective on the progression of AI capabilities, the typology provides a
foundation for assessing current systems and anticipating future developments
in agentic AI.

</details>


### [166] [Reasoning Systems as Structured Processes: Foundations, Failures, and Formal Criteria](https://arxiv.org/abs/2508.01763)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 提出一个通用的形式化框架，用于分析和比较跨领域的推理系统，支持未来对推理架构的理论和实践研究。


<details>
  <summary>Details</summary>
Motivation: 建立一个统一的结构化框架，以支持对推理系统的分析和比较，尤其是在内部失败、适应或分裂可能发生的背景下。

Method: 将推理系统建模为结构化元组，包含现象、解释空间、推理和生成映射以及原则基础，并研究其内部标准和典型故障模式。

Result: 提出了一个灵活的框架，能够容纳逻辑、算法和学习为基础的推理过程，并支持动态行为如迭代优化和原则演化。

Conclusion: 该框架为未来的理论研究和实践应用提供了基础，特别是在需要处理推理系统的结构约束时。

Abstract: This paper outlines a general formal framework for reasoning systems,
intended to support future analysis of inference architectures across domains.
We model reasoning systems as structured tuples comprising phenomena,
explanation space, inference and generation maps, and a principle base. The
formulation accommodates logical, algorithmic, and learning-based reasoning
processes within a unified structural schema, while remaining agnostic to any
specific reasoning algorithm or logic system. We survey basic internal
criteria--including coherence, soundness, and completeness-and catalog typical
failure modes such as contradiction, incompleteness, and non-convergence. The
framework also admits dynamic behaviors like iterative refinement and principle
evolution. The goal of this work is to establish a foundational structure for
representing and comparing reasoning systems, particularly in contexts where
internal failure, adaptation, or fragmentation may arise. No specific solution
architecture is proposed; instead, we aim to support future theoretical and
practical investigations into reasoning under structural constraint.

</details>


### [167] [Multi-turn Natural Language to Graph Query Language Translation](https://arxiv.org/abs/2508.01871)
*Yuanyuan Liang,Lei Pan,Tingyu Xie,Yunshi Lan,Weining Qian*

Main category: cs.AI

TL;DR: 研究提出了一种基于LLM的自动构建多轮NL2GQL数据集方法，并开发了MTGQL数据集，同时提供了三种基线方法评估多轮转换效果。


<details>
  <summary>Details</summary>
Motivation: 现有NL2GQL方法多为单轮转换，难以处理多轮动态对话和复杂上下文依赖问题，且缺乏高质量多轮数据集。

Method: 利用LLM自动构建多轮NL2GQL数据集（MTGQL），并提供三种基线方法进行评估。

Result: 成功开发了基于金融市场的MTGQL数据集，为未来多轮NL2GQL研究奠定了基础。

Conclusion: 该方法填补了多轮NL2GQL数据集的空白，为复杂场景下的查询转换提供了新思路。

Abstract: In recent years, research on transforming natural language into graph query
language (NL2GQL) has been increasing. Most existing methods focus on
single-turn transformation from NL to GQL. In practical applications, user
interactions with graph databases are typically multi-turn, dynamic, and
context-dependent. While single-turn methods can handle straightforward
queries, more complex scenarios often require users to iteratively adjust their
queries, investigate the connections between entities, or request additional
details across multiple dialogue turns. Research focused on single-turn
conversion fails to effectively address multi-turn dialogues and complex
context dependencies. Additionally, the scarcity of high-quality multi-turn
NL2GQL datasets further hinders the progress of this field. To address this
challenge, we propose an automated method for constructing multi-turn NL2GQL
datasets based on Large Language Models (LLMs) , and apply this method to
develop the MTGQL dataset, which is constructed from a financial market graph
database and will be publicly released for future research. Moreover, we
propose three types of baseline methods to assess the effectiveness of
multi-turn NL2GQL translation, thereby laying a solid foundation for future
research.

</details>


### [168] [BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation](https://arxiv.org/abs/2508.01285)
*Yujing Ke,Kevin George,Kathan Pandya,David Blumenthal,Maximilian Sprang,Gerrit Großmann,Sebastian Vollmer,David Antony Selby*

Main category: cs.AI

TL;DR: BioDisco是一个多智能体框架，通过语言模型推理和双模式证据系统生成新颖且基于证据的假设，并通过迭代优化和时间验证提升性能。


<details>
  <summary>Details</summary>
Motivation: 科学研究中新假设的发现常因信息量大且复杂而受限，现有自动化方法在生成新颖且基于证据的假设、迭代优化和时间验证方面表现不足。

Method: 采用语言模型推理和双模式证据系统（生物医学知识图谱和文献自动检索），结合内部评分和反馈循环进行优化，并通过时间和人类评估及Bradley-Terry配对比较模型验证。

Result: BioDisco在新颖性和显著性上表现优于现有方法，支持灵活模块化设计，可轻松集成自定义语言模型或知识图谱。

Conclusion: BioDisco是一个实用的工具，有助于研究人员高效发现新假设。

Abstract: Identifying novel hypotheses is essential to scientific research, yet this
process risks being overwhelmed by the sheer volume and complexity of available
information. Existing automated methods often struggle to generate novel and
evidence-grounded hypotheses, lack robust iterative refinement and rarely
undergo rigorous temporal evaluation for future discovery potential. To address
this, we propose BioDisco, a multi-agent framework that draws upon language
model-based reasoning and a dual-mode evidence system (biomedical knowledge
graphs and automated literature retrieval) for grounded novelty, integrates an
internal scoring and feedback loop for iterative refinement, and validates
performance through pioneering temporal and human evaluations and a
Bradley-Terry paired comparison model to provide statistically-grounded
assessment. Our evaluations demonstrate superior novelty and significance over
ablated configurations representative of existing agentic architectures.
Designed for flexibility and modularity, BioDisco allows seamless integration
of custom language models or knowledge graphs, and can be run with just a few
lines of code. We anticipate researchers using this practical tool as a
catalyst for the discovery of new hypotheses.

</details>


### [169] [CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models](https://arxiv.org/abs/2508.02427)
*Tung-Thuy Pham,Duy-Quan Luong,Minh-Quan Duong,Trung-Hieu Nguyen,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.AI

TL;DR: 介绍了CABENCH，一个包含70个现实可组合AI任务的基准测试，提出了评估框架，并比较了人工设计解决方案与LLM方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对可组合AI方法的系统性评估，需要建立基准测试和评估框架。

Method: 提出了CABENCH基准和评估框架，比较人工设计与LLM生成的解决方案。

Result: 展示了可组合AI解决复杂问题的潜力，但需自动化生成高效执行流水线的方法。

Conclusion: 可组合AI有前景，但仍需进一步开发自动化方法以充分释放其潜力。

Abstract: Composable AI offers a scalable and effective paradigm for tackling complex
AI tasks by decomposing them into sub-tasks and solving each sub-task using
ready-to-use well-trained models. However, systematically evaluating methods
under this setting remains largely unexplored. In this paper, we introduce
CABENCH, the first public benchmark comprising 70 realistic composable AI
tasks, along with a curated pool of 700 models across multiple modalities and
domains. We also propose an evaluation framework to enable end-to-end
assessment of composable AI solutions. To establish initial baselines, we
provide human-designed reference solutions and compare their performance with
two LLM-based approaches. Our results illustrate the promise of composable AI
in addressing complex real-world problems while highlighting the need for
methods that can fully unlock its potential by automatically generating
effective execution pipelines.

</details>


### [170] [ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI](https://arxiv.org/abs/2508.00899)
*Abeer Dyoub,Ivan Letteri,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 论文提出了一种名为ff4ERA的模糊框架，用于量化伦理风险，结合了模糊逻辑、模糊层次分析法和确定性因素，以支持人类价值观导向的决策。


<details>
  <summary>Details</summary>
Motivation: 随着共生AI的发展，伦理风险评估（ERA）面临不确定性、模糊性和信息不完整性等挑战，需要灵活且透明的框架来量化风险。

Method: 开发了ff4ERA框架，结合模糊逻辑、FAHP和确定性因素，通过伦理风险评分（ERS）量化风险。

Result: 案例研究表明，ff4ERA能生成上下文敏感的伦理风险评分，对专家输入和传感器数据敏感且稳健。

Conclusion: ff4ERA为伦理决策提供了可解释、可追踪的风险评估，支持假设分析和设计校准。

Abstract: The emergence of Symbiotic AI (SAI) introduces new challenges to ethical
decision-making as it deepens human-AI collaboration. As symbiosis grows, AI
systems pose greater ethical risks, including harm to human rights and trust.
Ethical Risk Assessment (ERA) thus becomes crucial for guiding decisions that
minimize such risks. However, ERA is hindered by uncertainty, vagueness, and
incomplete information, and morality itself is context-dependent and imprecise.
This motivates the need for a flexible, transparent, yet robust framework for
ERA. Our work supports ethical decision-making by quantitatively assessing and
prioritizing multiple ethical risks so that artificial agents can select
actions aligned with human values and acceptable risk levels. We introduce
ff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic
Hierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks
via an Ethical Risk Score (ERS) for each risk type. The final ERS combines the
FAHP-derived weight, propagated CF, and risk level. The framework offers a
robust mathematical approach for collaborative ERA modeling and systematic,
step-by-step analysis. A case study confirms that ff4ERA yields
context-sensitive, ethically meaningful risk scores reflecting both expert
input and sensor-based evidence. Risk scores vary consistently with relevant
factors while remaining robust to unrelated inputs. Local sensitivity analysis
shows predictable, mostly monotonic behavior across perturbations, and global
Sobol analysis highlights the dominant influence of expert-defined weights and
certainty factors, validating the model design. Overall, the results
demonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware
ethical assessments, enabling what-if analyses and guiding designers in
calibrating membership functions and expert judgments for reliable ethical
decision support.

</details>


### [171] [A Survey on Agent Workflow -- Status and Future](https://arxiv.org/abs/2508.01186)
*Chaojia Yu,Zihan Cheng,Hanwen Cui,Yishuo Gao,Zexu Luo,Yijin Wang,Hangbin Zheng,Yong Zhao*

Main category: cs.AI

TL;DR: 对大型语言模型时代下的自主智能体工作流系统进行了全面综述，分类比较了20多个代表性系统，探讨了技术挑战和未来趋势。


<details>
  <summary>Details</summary>
Motivation: 随着自主智能体系统复杂度的增加，工作流系统成为实现可扩展、可控和安全AI行为的关键。

Method: 通过分类功能能力和架构特征两大维度，比较分析了20多个代表性系统。

Result: 总结了常见模式、技术挑战和新兴趋势，并提出了工作流优化和安全相关问题。

Conclusion: 未来研究应关注标准化和多模态集成等问题，以推动智能体设计和工作流基础设施的发展。

Abstract: In the age of large language models (LLMs), autonomous agents have emerged as
a powerful paradigm for achieving general intelligence. These agents
dynamically leverage tools, memory, and reasoning capabilities to accomplish
user-defined goals. As agent systems grow in complexity, agent
workflows-structured orchestration frameworks-have become central to enabling
scalable, controllable, and secure AI behaviors. This survey provides a
comprehensive review of agent workflow systems, spanning academic frameworks
and industrial implementations. We classify existing systems along two key
dimensions: functional capabilities (e.g., planning, multi-agent collaboration,
external API integration) and architectural features (e.g., agent roles,
orchestration flows, specification languages). By comparing over 20
representative systems, we highlight common patterns, potential technical
challenges, and emerging trends. We further address concerns related to
workflow optimization strategies and security. Finally, we outline open
problems such as standardization and multimodal integration, offering insights
for future research at the intersection of agent design, workflow
infrastructure, and safe automation.

</details>


### [172] [Getting out of the Big-Muddy: Escalation of Commitment in LLMs](https://arxiv.org/abs/2508.01545)
*Emilio Barkett,Olivia Long,Paul Kröger*

Main category: cs.AI

TL;DR: 研究表明，大型语言模型（LLMs）在决策中的认知偏见表现高度依赖环境，多智能体协作和组织压力会显著增加其“承诺升级”行为。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否继承人类认知偏见（如承诺升级），以及这些偏见在不同环境下的表现。

Method: 使用四组实验条件（模型作为投资者、顾问、多智能体协作和复合压力场景）进行6,500次试验。

Result: 个体决策中LLMs表现理性，但在多智能体协作（对称结构）和复合压力下表现出高比例的“承诺升级”。

Conclusion: LLMs的偏见表现取决于社会和组织环境，需警惕在多智能体系统和无监督操作中的潜在风险。

Abstract: Large Language Models (LLMs) are increasingly deployed in autonomous
decision-making roles across high-stakes domains. However, since models are
trained on human-generated data, they may inherit cognitive biases that
systematically distort human judgment, including escalation of commitment,
where decision-makers continue investing in failing courses of action due to
prior investment. Understanding when LLMs exhibit such biases presents a unique
challenge. While these biases are well-documented in humans, it remains unclear
whether they manifest consistently in LLMs or require specific triggering
conditions. This paper investigates this question using a two-stage investment
task across four experimental conditions: model as investor, model as advisor,
multi-agent deliberation, and compound pressure scenario. Across N = 6,500
trials, we find that bias manifestation in LLMs is highly context-dependent. In
individual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate
strong rational cost-benefit logic with minimal escalation of commitment.
However, multi-agent deliberation reveals a striking hierarchy effect (Study 3,
N = 500): while asymmetrical hierarchies show moderate escalation rates
(46.2%), symmetrical peer-based decision-making produces near-universal
escalation (99.2%). Similarly, when subjected to compound organizational and
personal pressures (Study 4, N = 2,000), models exhibit high degrees of
escalation of commitment (68.95% average allocation to failing divisions).
These findings reveal that LLM bias manifestation depends critically on social
and organizational context rather than being inherent, with significant
implications for the deployment of multi-agent systems and unsupervised
operations where such conditions may emerge naturally.

</details>


### [173] [What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce](https://arxiv.org/abs/2508.02630)
*Amine Allouah,Omar Besbes,Josué D Figueroa,Yash Kanoria,Akshit Kumar*

Main category: cs.AI

TL;DR: 研究探讨了AI代理在在线市场中的购物行为，揭示了它们对产品位置、价格、评级等因素的偏好差异，并提出了卖家策略和平台设计的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在电子商务中的应用，了解它们如何购物以及为什么选择特定产品变得至关重要。

Method: 研究开发了ACES沙盒环境，通过随机化产品位置、价格等因素，测试前沿VLM代理的购物行为。

Result: AI代理的购物行为显示了对特定位置的偏好、对赞助标签的惩罚以及对平台认可的奖励，且不同模型间存在显著差异。

Conclusion: 研究揭示了AI代理在电子商务中的行为模式，为卖家策略、平台设计和监管问题提供了新的见解。

Abstract: Online marketplaces will be transformed by autonomous AI agents acting on
behalf of consumers. Rather than humans browsing and clicking,
vision-language-model (VLM) agents can parse webpages, evaluate products, and
transact. This raises a fundamental question: what do AI agents buy, and why?
We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent
with a fully programmable mock marketplace to study this question. We first
conduct basic rationality checks in the context of simple tasks, and then, by
randomizing product positions, prices, ratings, reviews, sponsored tags, and
platform endorsements, we obtain causal estimates of how frontier VLMs actually
shop. Models show strong but heterogeneous position effects: all favor the top
row, yet different models prefer different columns, undermining the assumption
of a universal "top" rank. They penalize sponsored tags and reward
endorsements. Sensitivities to price, ratings, and reviews are directionally
human-like but vary sharply in magnitude across models. Motivated by scenarios
where sellers use AI agents to optimize product listings, we show that a
seller-side agent that makes minor tweaks to product descriptions, targeting AI
buyer preferences, can deliver substantial market-share gains if AI-mediated
shopping dominates. We also find that modal product choices can differ across
models and, in some cases, demand may concentrate on a few select products,
raising competition questions. Together, our results illuminate how AI agents
may behave in e-commerce settings and surface concrete seller strategy,
platform design, and regulatory questions in an AI-mediated ecosystem.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [174] [Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue](https://arxiv.org/abs/2508.02359)
*Surej Mouli,Ramaswamy Palaniappan*

Main category: eess.SP

TL;DR: 研究探讨高占空比视觉刺激对缓解SSVEP技术中眼疲劳的影响，发现85%占空比效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决SSVEP技术中因视觉疲劳和PWM精度问题导致的长期使用限制。

Method: 使用50-95%占空比的PWM刺激，记录EEG数据并测试十名受试者的反应。

Result: 高占空比减少视觉疲劳，85%占空比时SSVEP响应最佳，且效果受试者独立。

Conclusion: 高占空比刺激可提升SSVEP技术的实用性，85%占空比为推荐值。

Abstract: Steady state visual evoked response (SSVEP) is widely used in visual-based
diagnosis and applications such as brain computer interfacing due to its high
information transfer rate and the capability to activate commands through
simple gaze control. However, one major impediment in using flashing visual
stimulus to obtain SSVEP is eye fatigue that prevents continued long term use
preventing practical deployment. This combined with the difficulty in
establishing precise pulse-width modulation (PWM) that results in poorer
accuracy warrants the development of appropriate approach to solve these
issues. Various studies have suggested the usage of high frequencies of visual
stimulus to reduce the visual fatigue for the user but this results in poor
response performance. Here, the authors study the use of extremely high
duty-cycles in the stimulus in the hope of solving these constraints.
Electroencephalogram data was recorded with PWM duty-cycles of 50 to 95%
generated by a precise custom-made light-emitting diode hardware and tested ten
subjects responded that increasing duty-cycles had less visual strain for all
the frequency values and the SSVEP exhibited a subject-independent peak
response for duty-cycle of 85%. This could pave the way for increased usage of
SSVEP for practical applications.

</details>


### [175] [DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset](https://arxiv.org/abs/2508.01510)
*Surej Mouli,Ramaswamy Palaniappan*

Main category: eess.SP

TL;DR: 本文讨论了一种可自定义的芯片板（COB）LED设计，用于同时引发两种脑反应（稳态视觉诱发电位（SSVEP）和瞬态诱发电位P300），旨在减少疲劳并提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合SSVEP和P300的优势，设计一个混合BCI硬件平台，以减少EEG电极数量和训练时间，同时提高分类准确性和可靠性。

Method: 使用32位微控制器独立控制四个绿色视觉刺激（SSVEP）和四个红色LED随机闪烁（P300），并记录P300事件时间戳。

Result: 系统成功用于实时分类，通过控制乐高机器人在四个方向移动测试其性能。

Conclusion: 该混合刺激设计有效结合了SSVEP和P300，减少了疲劳并提高了分类性能。

Abstract: A fully customisable chip-on board (COB) LED design to evoke two brain
responses simultaneously (steady state visual evoked potential (SSVEP) and
transient evoked potential, P300) is discussed in this paper. Considering
different possible modalities in braincomputer interfacing (BCI), SSVEP is
widely accepted as it requires a lesser number of electroencephalogram (EEG)
electrodes and minimal training time. The aim of this work was to produce a
hybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced
fatigue and improved classification performance. The system comprises of four
independent radial green visual stimuli controlled individually by a 32-bit
microcontroller platform to evoke SSVEP and four red LEDs flashing at random
intervals to generate P300 events. The system can also record the P300 event
timestamps that can be used in classification, to improve the accuracy and
reliability. The hybrid stimulus was tested for realtime classification
accuracy by controlling a LEGO robot to move in four directions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [176] [FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models](https://arxiv.org/abs/2508.01506)
*Zishan Shao,Yixiao Wang,Qinsi Wang,Ting Jiang,Zhixu Du,Hancheng Ye,Danyang Zhuo,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: 论文提出FlashSVD，一种专为SVD压缩大语言模型设计的端到端框架，解决了现有方法在推理时内存开销高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法虽能减少模型参数量，但忽略了推理时的激活内存开销，限制了其在实际设备上的部署。

Method: FlashSVD通过融合低秩投影核到自注意力和FFN管道中，避免全尺寸激活缓冲区的生成，从而减少内存使用。

Result: 实验表明，FlashSVD在BERT-Base上减少峰值激活内存70.2%，中间临时内存75%，且无精度损失。

Conclusion: FlashSVD为低秩LLMs在内存受限环境中的部署提供了可行方案。

Abstract: Singular Value Decomposition (SVD) has recently seen a surge of interest as a
simple yet powerful tool for large language models (LLMs) compression, with a
growing number of works demonstrating 20-80% parameter reductions at minimal
accuracy loss. Previous SVD-based approaches have focused primarily on reducing
the memory footprint of model weights, largely overlooking the additional
activation memory overhead incurred during inference when applying truncated
factors via standard dense CUDA kernels. Our experiments demonstrate that this
activation overhead, scaling with sequence length and hidden dimension,
prevents current SVD compression techniques from achieving any reduction in
peak inference memory, thereby limiting their viability for real-world,
on-device deployments.
  We introduce FlashSVD, a novel, end-to-end rank-aware streaming inference
framework specifically designed for SVD-compressed large language models.
FlashSVD can be seamlessly integrated with any model that employs SVD-based
methods for parameter reduction. By fusing low-rank projection kernels directly
into both the self-attention and feed-forward network (FFN) pipelines, FlashSVD
avoid materializing full-size activation buffers. Instead, small tiles of the
truncated factors are loaded into on-chip SRAM, multiplied and reduced on the
fly, and immediately evicted, preserving high GPU occupancy and adding no extra
latency. On standard encoder benchmarks (e.g., BERT-Base), FlashSVD cuts peak
activation memory by up to 70.2% and intermediate transient memory by 75%, all
while incur no accuracy loss with upstreaming compression methods, offering a
practical path toward memory-constrained deployment of low-rank LLMs.

</details>


### [177] [Learning Unified System Representations for Microservice Tail Latency Prediction](https://arxiv.org/abs/2508.01635)
*Wenzhuo Qian,Hailiang Zhao,Tianlv Chen,Jiayi Chen,Ziqi Wang,Kingsum Chow,Shuiguang Deng*

Main category: cs.LG

TL;DR: 论文提出了一种名为USRFNet的深度学习网络，用于解决微服务架构中性能监控和资源管理的挑战，通过分离和建模流量侧与资源侧特征，显著提高了尾延迟预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构因其分布式特性在性能监控和资源管理上面临挑战，传统方法如请求级延迟指标存在噪声敏感和无法反映全局行为的局限。

Method: USRFNet利用图神经网络（GNNs）捕捉服务交互和请求传播模式，同时通过gMLP模块独立建模集群资源动态，最后将两者融合为统一系统嵌入以预测窗口级P95尾延迟。

Result: 在大规模压力测试的实际微服务基准测试中，USRFNet表现优于现有最优基线方法，显著提高了预测准确性。

Conclusion: USRFNet通过有效分离和整合流量侧与资源侧特征，提供了一种稳定且高精度的微服务性能监控解决方案。

Abstract: Microservice architectures have become the de facto standard for building
scalable cloud-native applications, yet their distributed nature introduces
significant challenges in performance monitoring and resource management.
Traditional approaches often rely on per-request latency metrics, which are
highly sensitive to transient noise and fail to reflect the holistic behavior
of complex, concurrent workloads. In contrast, window-level P95 tail latency
provides a stable and meaningful signal that captures both system-wide trends
and user-perceived performance degradation. We identify two key shortcomings in
existing methods: (i) inadequate handling of heterogeneous data, where
traffic-side features propagate across service dependencies and resource-side
signals reflect localized bottlenecks, and (ii) the lack of principled
architectural designs that effectively distinguish and integrate these
complementary modalities. To address these challenges, we propose USRFNet, a
deep learning network that explicitly separates and models traffic-side and
resource-side features. USRFNet employs GNNs to capture service interactions
and request propagation patterns, while gMLP modules independently model
cluster resource dynamics. These representations are then fused into a unified
system embedding to predict window-level P95 latency with high accuracy. We
evaluate USRFNet on real-world microservice benchmarks under large-scale stress
testing conditions, demonstrating substantial improvements in prediction
accuracy over state-of-the-art baselines.

</details>


### [178] [Discrete approach to machine learning](https://arxiv.org/abs/2508.00869)
*Dmitriy Kashitsyn,Dmitriy Shabanov*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏位向量和固定长度线性向量的编码及结构信息处理方法，包括离散的随机维度缩减方法和几何嵌入方法，并通过三种模态展示了代码空间的结构和特性。


<details>
  <summary>Details</summary>
Motivation: 探索代码空间的编码与结构信息处理，通过稀疏位向量和线性向量方法揭示其内在特性。

Method: 提出离散的随机维度缩减方法和几何嵌入方法，分析三种模态（俄语、英语形态学和免疫组化标记）的代码空间结构。

Result: 生成了代码空间的映射图，并观察到其与哺乳动物新皮质中的“风车结构”相似，推测模型中的过程与新皮质组织间存在相似性。

Conclusion: 初步假设模型中的信息处理与哺乳动物新皮质组织有潜在相似性，未来需进一步验证。

Abstract: The article explores an encoding and structural information processing
approach using sparse bit vectors and fixed-length linear vectors. The
following are presented: a discrete method of speculative stochastic
dimensionality reduction of multidimensional code and linear spaces with linear
asymptotic complexity; a geometric method for obtaining discrete embeddings of
an organised code space that reflect the internal structure of a given
modality. The structure and properties of a code space are investigated using
three modalities as examples: morphology of Russian and English languages, and
immunohistochemical markers. Parallels are drawn between the resulting map of
the code space layout and so-called pinwheels appearing on the mammalian
neocortex. A cautious assumption is made about similarities between neocortex
organisation and processes happening in our models.

</details>


### [179] [CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.02091)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Chris Shum,Jiwei Li*

Main category: cs.LG

TL;DR: CRINN是一种基于强化学习的ANNS新范式，通过自动生成更快实现，提升了检索增强生成和基于代理的LLM应用的性能。


<details>
  <summary>Details</summary>
Motivation: ANNS算法在AI应用中的重要性日益增加，尤其是检索增强生成和基于代理的LLM应用，亟需更高效的优化方法。

Method: 将ANNS优化问题建模为强化学习问题，以执行速度作为奖励信号，自动生成更快的实现。

Result: 在六个常用NNS基准数据集上，CRINN在三个数据集上表现最佳，两个数据集上并列第一。

Conclusion: CRINN的成功不仅证明强化学习可以自动化复杂算法优化，还为LLM增强应用的开发提供了新方向。

Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN

</details>


### [180] [Diffusion Models for Future Networks and Communications: A Comprehensive Survey](https://arxiv.org/abs/2508.01586)
*Nguyen Cong Luong,Nguyen Duc Hai,Duc Van Le,Huy T. Nguyen,Thai-Hoc Vu,Thien Huynh-The,Ruichen Zhang,Nguyen Duc Duy Anh,Dusit Niyato,Marco Di Renzo,Dong In Kim,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 本文概述了扩散模型（DMs）在未来通信系统中的理论与应用，涵盖优化器、强化学习、信道建模等领域，并指出了技术局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探讨扩散模型在无线通信和网络中的潜力及其对复杂数据分布和噪声鲁棒性能的处理能力。

Method: 通过教程和应用案例，展示DMs在优化、强化学习、信道建模等多种无线网络问题中的应用。

Result: 总结了DMs在多个通信领域的研究进展，突出了其实际应用价值和技术挑战。

Conclusion: DMs在通信系统中具有广泛潜力，但仍需解决技术限制并探索未来研究方向。

Abstract: The rise of Generative AI (GenAI) in recent years has catalyzed
transformative advances in wireless communications and networks. Among the
members of the GenAI family, Diffusion Models (DMs) have risen to prominence as
a powerful option, capable of handling complex, high-dimensional data
distribution, as well as consistent, noise-robust performance. In this survey,
we aim to provide a comprehensive overview of the theoretical foundations and
practical applications of DMs across future communication systems. We first
provide an extensive tutorial of DMs and demonstrate how they can be applied to
enhance optimizers, reinforcement learning and incentive mechanisms, which are
popular approaches for problems in wireless networks. Then, we review and
discuss the DM-based methods proposed for emerging issues in future networks
and communications, including channel modeling and estimation, signal detection
and data reconstruction, integrated sensing and communication, resource
management in edge computing networks, semantic communications and other
notable issues. We conclude the survey with highlighting technical limitations
of DMs and their applications, as well as discussing future research
directions.

</details>


### [181] [Skeleton-Guided Learning for Shortest Path Search](https://arxiv.org/abs/2508.02270)
*Tiantian Liu,Xiao Li,Huan Li,Hua Lu,Christian S. Jensen,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出了一种通用的学习框架，用于在通用图上进行最短路径搜索，无需特定领域特征，通过构建骨架图和SGNN来学习节点嵌入并预测距离，支持LSearch和HLSearch算法，实验证明其在多种图上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有最短路径搜索方法在复杂图上效率低下，或需要大量预处理的索引技术，学习型方法通常局限于特定领域特征，缺乏通用性。

Method: 构建骨架图捕捉多级距离和跳数信息，使用SGNN学习节点嵌入和预测距离，提出LSearch和HLSearch算法进行搜索空间剪枝。

Result: 在五种真实图上表现优异，证明了框架的灵活性和有效性。

Conclusion: 该框架为通用图上的最短路径搜索提供了高效、灵活的解决方案。

Abstract: Shortest path search is a core operation in graph-based applications, yet
existing methods face important limitations. Classical algorithms such as
Dijkstra's and A* become inefficient as graphs grow more complex, while
index-based techniques often require substantial preprocessing and storage.
Recent learning-based approaches typically focus on spatial graphs and rely on
context-specific features like geographic coordinates, limiting their general
applicability. We propose a versatile learning-based framework for shortest
path search on generic graphs, without requiring domain-specific features. At
the core of our approach is the construction of a skeleton graph that captures
multi-level distance and hop information in a compact form. A Skeleton Graph
Neural Network (SGNN) operates on this structure to learn node embeddings and
predict distances and hop lengths between node pairs. These predictions support
LSearch, a guided search algorithm that uses model-driven pruning to reduce the
search space while preserving accuracy. To handle larger graphs, we introduce a
hierarchical training strategy that partitions the graph into subgraphs with
individually trained SGNNs. This structure enables HLSearch, an extension of
our method for efficient path search across graph partitions. Experiments on
five diverse real-world graphs demonstrate that our framework achieves strong
performance across graph types, offering a flexible and effective solution for
learning-based shortest path search.

</details>


### [182] [Compression-Induced Communication-Efficient Large Model Training and Inferencing](https://arxiv.org/abs/2508.00960)
*Sudip K. Seal,Maksudul Alam,Jorge Ramirez,Sajal Dash,Hao Lu*

Main category: cs.LG

TL;DR: 本文提出了一种名为“幻影并行”的新策略，旨在减少大型神经网络训练中传统张量并行的能源消耗，实验显示能节省约50%的训练能源。


<details>
  <summary>Details</summary>
Motivation: 大型神经网络训练的高能耗问题严重影响了可持续的大规模机器学习工作负载，本文旨在通过幻影并行方法解决传统张量并行的高能耗问题。

Method: 通过设计新的前向和后向传播算子，并将其实现为自定义的自动微分操作，构建了一个端到端的幻影并行训练流程。

Result: 在256个GPU上的实验证实，幻影并行能显著减少带宽和浮点运算需求，训练能源消耗降低约50%，且在小规模GPU上也能达到与传统方法相同的模型性能。

Conclusion: 幻影并行是一种高效的训练方法，能够显著降低能源消耗并提高训练效率，为可持续的机器学习提供了新可能。

Abstract: Energy efficiency of training and inferencing with large neural network
models is a critical challenge facing the future of sustainable large-scale
machine learning workloads. This paper introduces an alternative strategy,
called phantom parallelism, to minimize the net energy consumption of
traditional tensor (model) parallelism, the most energy-inefficient component
of large neural network training. The approach is presented in the context of
feed-forward network architectures as a preliminary, but comprehensive,
proof-of-principle study of the proposed methodology. We derive new forward and
backward propagation operators for phantom parallelism, implement them as
custom autograd operations within an end-to-end phantom parallel training
pipeline and compare its parallel performance and energy-efficiency against
those of conventional tensor parallel training pipelines. Formal analyses that
predict lower bandwidth and FLOP counts are presented with supporting empirical
results on up to 256 GPUs that corroborate these gains. Experiments are shown
to deliver ~50% reduction in the energy consumed to train FFNs using the
proposed phantom parallel approach when compared with conventional tensor
parallel methods. Additionally, the proposed approach is shown to train smaller
phantom models to the same model loss on smaller GPU counts as larger tensor
parallel models on larger GPU counts offering the possibility for even greater
energy savings.

</details>


### [183] [Optimal Scheduling Algorithms for LLM Inference: Theory and Practice](https://arxiv.org/abs/2508.01002)
*Agrim Bari,Parikshit Hegde,Gustavo de Veciana*

Main category: cs.LG

TL;DR: 本文提出了一种新的调度器SLAI，用于优化LLM推理系统中的路由和调度问题，显著降低了首令牌时间和提高了服务容量。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型（LLM）的工具（如ChatGPT）在各行业的广泛应用，高效的LLM推理系统需求增长。这类系统具有特殊的双阶段计算结构，需要新的路由和调度策略。

Method: 提出了一个理论框架，基于最优分块和动态资源分配原则，设计了RAD调度器。随后针对实际SLO需求，开发了SLAI调度器，利用实时测量和请求重排序优化性能。

Result: 在Openchat ShareGPT4数据集和Mistral-7B模型上的测试表明，SLAI将中位首令牌时间降低了53%，最大服务容量提高了26%，同时满足尾部令牌间时间延迟约束。

Conclusion: SLAI调度器通过理论指导的优化策略，显著提升了LLM推理系统的效率和服务能力。

Abstract: With the growing use of Large Language Model (LLM)-based tools like ChatGPT,
Perplexity, and Gemini across industries, there is a rising need for efficient
LLM inference systems. These systems handle requests with a unique two-phase
computation structure: a prefill-phase that processes the full input prompt and
a decode-phase that autoregressively generates tokens one at a time. This
structure calls for new strategies for routing and scheduling requests.
  In this paper, we take a comprehensive approach to this challenge by
developing a theoretical framework that models routing and scheduling in LLM
inference systems. We identify two key design principles-optimal tiling and
dynamic resource allocation-that are essential for achieving high throughput.
Guided by these principles, we propose the Resource-Aware Dynamic (RAD)
scheduler and prove that it achieves throughput optimality under mild
conditions. To address practical Service Level Objectives (SLOs) such as
serving requests with different Time Between Token (TBT) constraints, we design
the SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements
to prioritize decode requests that are close to missing their TBT deadlines and
reorders prefill requests based on known prompt lengths to further reduce the
Time To First Token (TTFT) delays.
  We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model
on an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the
median TTFT by 53% and increases the maximum serving capacity by 26% such that
median TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.

</details>


### [184] [Balancing Information Accuracy and Response Timeliness in Networked LLMs](https://arxiv.org/abs/2508.02209)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 研究探讨了一种由多个小型专用语言模型组成的网络系统，通过聚合它们的输出来提高响应质量，证明了聚合响应比单个模型更准确。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在许多领域取得了显著进展，但其对训练数据、计算资源和能源的高需求限制了实际应用。研究提出利用小型专用模型聚合输出来解决这一问题。

Method: 设计了一个由用户、中央任务处理器和多个专用LLM集群组成的网络系统。处理器将用户的二元查询路由到选定的模型集群，聚合个体响应后返回结果。

Result: 模拟实验表明，聚合响应的准确性显著高于单个模型，尤其是在参与模型的性能相近时效果更明显。

Conclusion: 通过聚合小型专用模型的输出，可以在减少资源消耗的同时提高响应质量，为大语言模型的实用部署提供了可行方案。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed many
fields including scientific discovery, content generation, biomedical text
mining, and educational technology. However, the substantial requirements for
training data, computational resources, and energy consumption pose significant
challenges for their practical deployment. A promising alternative is to
leverage smaller, specialized language models and aggregate their outputs to
improve overall response quality. In this work, we investigate a networked LLM
system composed of multiple users, a central task processor, and clusters of
topic-specialized LLMs. Each user submits categorical binary (true/false)
queries, which are routed by the task processor to a selected cluster of $m$
LLMs. After gathering individual responses, the processor returns a final
aggregated answer to the user. We characterize both the information accuracy
and response timeliness in this setting, and formulate a joint optimization
problem to balance these two competing objectives. Our extensive simulations
demonstrate that the aggregated responses consistently achieve higher accuracy
than those of individual LLMs. Notably, this improvement is more significant
when the participating LLMs exhibit similar standalone performance.

</details>


### [185] [Boosting Generalization Performance in Model-Heterogeneous Federated Learning Using Variational Transposed Convolution](https://arxiv.org/abs/2508.01669)
*Ziru Niu,Hai Dong,A. K. Qin*

Main category: cs.LG

TL;DR: 提出了一种基于特征分布交换的模型异构联邦学习框架，通过生成合成数据提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异构性问题，特别是在客户端模型架构不同的场景下。

Method: 客户端与服务器交换特征分布（均值和协方差），训练变分转置卷积网络生成合成数据，用于微调本地模型。

Result: 方法在泛化准确性上优于现有框架，且通信成本和内存消耗更低。

Conclusion: 该框架为模型异构联邦学习提供了高效且隐私保护的解决方案。

Abstract: Federated learning (FL) is a pioneering machine learning paradigm that
enables distributed clients to process local data effectively while ensuring
data privacy. However, the efficacy of FL is usually impeded by the data
heterogeneity among clients, resulting in local models with low generalization
performance. To address this problem, traditional model-homogeneous approaches
mainly involve debiasing the local training procedures with regularization or
dynamically adjusting client weights in aggregation. Nonetheless, these
approaches become incompatible for scenarios where clients exhibit
heterogeneous model architectures. In this paper, we propose a
model-heterogeneous FL framework that can improve clients' generalization
performance over unseen data without model aggregation. Instead of model
parameters, clients exchange the feature distributions with the server,
including the mean and the covariance. Accordingly, clients train a variational
transposed convolutional (VTC) neural network with Gaussian latent variables
sampled from the feature distributions, and use the VTC model to generate
synthetic data. By fine-tuning local models with the synthetic data, clients
significantly increase their generalization performance. Experimental results
show that our approach obtains higher generalization accuracy than existing
model-heterogeneous FL frameworks, as well as lower communication costs and
memory consumption

</details>


### [186] [Asynchronous Federated Learning with non-convex client objective functions and heterogeneous dataset](https://arxiv.org/abs/2508.01675)
*Ali Forootani,Raffaele Iervolino*

Main category: cs.LG

TL;DR: 该论文研究了异步联邦学习（AFL）在非凸目标函数和异构数据集上的应用，提出了一种基于陈旧性感知的聚合方法，并通过实验验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习中通信开销、系统异构性和滞后效应的问题，扩展AFL以适应现代深度学习的非凸性和数据异构性。

Method: 引入陈旧性感知聚合和动态学习率调度，分析客户端选择策略对收敛的影响，并在PyTorch和Python的asyncio中实现。

Result: 通过实验验证了该方法在异步、异构和非凸FL场景中的性能和可扩展性提升。

Conclusion: 提出的框架有效解决了AFL中的陈旧性、异构性和收敛性问题，适用于实际应用。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized devices while preserving data privacy. However, traditional FL
suffers from communication overhead, system heterogeneity, and straggler
effects. Asynchronous Federated Learning (AFL) addresses these by allowing
clients to update independently, improving scalability and reducing
synchronization delays. This paper extends AFL to handle non-convex objective
functions and heterogeneous datasets, common in modern deep learning. We
present a rigorous convergence analysis, deriving bounds on the expected
gradient norm and studying the effects of staleness, variance, and
heterogeneity. To mitigate stale updates, we introduce a staleness aware
aggregation that prioritizes fresher updates and a dynamic learning rate
schedule that adapts to client staleness and heterogeneity, improving stability
and convergence. Our framework accommodates variations in computational power,
data distribution, and communication delays, making it practical for real world
applications. We also analyze the impact of client selection
strategies-sampling with or without replacement-on variance and convergence.
Implemented in PyTorch with Python's asyncio, our approach is validated through
experiments demonstrating improved performance and scalability for
asynchronous, heterogeneous, and non-convex FL scenarios.

</details>


### [187] [Energy-Efficient Federated Learning for Edge Real-Time Vision via Joint Data, Computation, and Communication Design](https://arxiv.org/abs/2508.01745)
*Xiangwang Hou,Jingjing Wang,Fangming Guan,Jun Du,Chunxiao Jiang,Yong Ren*

Main category: cs.LG

TL;DR: FedDPQ是一个针对无线网络环境下实时计算机视觉应用的超高效联邦学习框架，通过数据增强、模型剪枝、通信量化和功率控制等技术提升训练效率和能源利用。


<details>
  <summary>Details</summary>
Motivation: 解决无线边缘设备在资源受限环境下进行联邦学习时的高能耗、非独立同分布数据及通信不可靠等问题。

Method: 结合扩散数据增强、模型剪枝、通信量化和自适应功率控制，并通过贝叶斯优化联合调参。

Result: 实验表明，FedDPQ在收敛速度和能源效率上表现优越。

Conclusion: FedDPQ首次联合优化数据、计算和通信，为不可靠无线网络下的联邦学习提供了高效解决方案。

Abstract: Emerging real-time computer vision (CV) applications on wireless edge devices
demand energy-efficient and privacy-preserving learning. Federated learning
(FL) enables on-device training without raw data sharing, yet remains
challenging in resource-constrained environments due to energy-intensive
computation and communication, as well as limited and non-i.i.d. local data. We
propose FedDPQ, an ultra energy-efficient FL framework for real-time CV over
unreliable wireless networks. FedDPQ integrates diffusion-based data
augmentation, model pruning, communication quantization, and transmission power
control to enhance training efficiency. It expands local datasets using
synthetic data, reduces computation through pruning, compresses updates via
quantization, and mitigates transmission outages with adaptive power control.
We further derive a closed-form energy-convergence model capturing the coupled
impact of these components, and develop a Bayesian optimization(BO)-based
algorithm to jointly tune data augmentation strategy, pruning ratio,
quantization level, and power control. To the best of our knowledge, this is
the first work to jointly optimize FL performance from the perspectives of
data, computation, and communication under unreliable wireless conditions.
Experiments on representative CV tasks show that FedDPQ achieves superior
convergence speed and energy efficiency.

</details>


### [188] [Mitigating Persistent Client Dropout in Asynchronous Decentralized Federated Learning](https://arxiv.org/abs/2508.01807)
*Ignacy Stępka,Nicholas Gisolfi,Kacper Trębacz,Artur Dubrawski*

Main category: cs.LG

TL;DR: 该论文研究了异步去中心化联邦学习（DFL）中的客户端持续退出问题，提出了基于客户端重建的自适应策略来缓解性能损失。


<details>
  <summary>Details</summary>
Motivation: 异步和去中心化导致联邦学习中对客户端退出后的信息恢复困难，现有方法效果有限。

Method: 提出基于客户端重建的自适应策略，并在不同数据分布场景下进行实验。

Result: 实验显示这些策略能有效恢复部分性能损失，尽管无法精确重建缺失客户端的数据。

Conclusion: 论文指出了当前方法的局限性，并提出了未来解决客户端退出问题的研究方向。

Abstract: We consider the problem of persistent client dropout in asynchronous
Decentralized Federated Learning (DFL). Asynchronicity and decentralization
obfuscate information about model updates among federation peers, making
recovery from a client dropout difficult. Access to the number of learning
epochs, data distributions, and all the information necessary to precisely
reconstruct the missing neighbor's loss functions is limited. We show that
obvious mitigations do not adequately address the problem and introduce
adaptive strategies based on client reconstruction. We show that these
strategies can effectively recover some performance loss caused by dropout. Our
work focuses on asynchronous DFL with local regularization and differs
substantially from that in the existing literature. We evaluate the proposed
methods on tabular and image datasets, involve three DFL algorithms, and three
data heterogeneity scenarios (iid, non-iid, class-focused non-iid). Our
experiments show that the proposed adaptive strategies can be effective in
maintaining robustness of federated learning, even if they do not reconstruct
the missing client's data precisely. We also discuss the limitations and
identify future avenues for tackling the problem of client dropout.

</details>


### [189] [Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal](https://arxiv.org/abs/2508.00858)
*Christina Butsko,Kristof Van Tricht,Gabriel Tseng,Giorgia Milli,David Rolnick,Ruben Cartuyvels,Inbal Becker Reshef,Zoltan Szantoi,Hannah Kerner*

Main category: cs.LG

TL;DR: 该论文提出了一个结构化方法，将地理空间基础模型集成到实际应用中，并通过案例展示了其在作物测绘中的显著效果。


<details>
  <summary>Details</summary>
Motivation: 尽管地理空间基础模型在基准测试中表现出色，但其在实际应用中的部署仍面临数据异质性、资源限制等复杂问题，因此需要一种可操作化的方法。

Method: 提出三步协议：定义应用需求、根据领域数据调整模型、进行严格的实证测试，并以Presto模型为例进行作物测绘案例研究。

Result: 微调预训练模型在作物测绘中表现优于传统监督方法，且模型具备强时空泛化能力。

Conclusion: 该方法为实践者提供了可复制的框架，并展示了其在大规模作物测绘系统中的应用潜力。

Abstract: The increasing availability of geospatial foundation models has the potential
to transform remote sensing applications such as land cover classification,
environmental monitoring, and change detection. Despite promising benchmark
results, the deployment of these models in operational settings is challenging
and rare. Standardized evaluation tasks often fail to capture real-world
complexities relevant for end-user adoption such as data heterogeneity,
resource constraints, and application-specific requirements. This paper
presents a structured approach to integrate geospatial foundation models into
operational mapping systems. Our protocol has three key steps: defining
application requirements, adapting the model to domain-specific data and
conducting rigorous empirical testing. Using the Presto model in a case study
for crop mapping, we demonstrate that fine-tuning a pre-trained model
significantly improves performance over conventional supervised methods. Our
results highlight the model's strong spatial and temporal generalization
capabilities. Our protocol provides a replicable blueprint for practitioners
and lays the groundwork for future research to operationalize foundation models
in diverse remote sensing applications. Application of the protocol to the
WorldCereal global crop-mapping system showcases the framework's scalability.

</details>


### [190] [Communication and Computation Efficient Split Federated Learning in O-RAN](https://arxiv.org/abs/2508.02534)
*Shunxian Gu,Chaoqun You,Bangbang Ren,Deke Guo*

Main category: cs.LG

TL;DR: 提出了SplitMe框架，通过分层联合学习解决O-RAN中训练时间长和通信成本高的问题，优化资源分配并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统联合学习在O-RAN中因模型增大导致的训练时间过长和通信成本高的问题。

Method: 利用SplitMe框架，通过分层训练和反向模型技术消除频繁传输，优化资源分配和本地更新。

Result: SplitMe在成本和收敛性上显著优于SFL、FedAvg和O-RANFed。

Conclusion: SplitMe有效解决了O-RAN中的联合学习问题，优化了资源利用和训练效率。

Abstract: The hierarchical architecture of Open Radio Access Network (O-RAN) has
enabled a new Federated Learning (FL) paradigm that trains models using data
from non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs).
However, the ever-increasing model size leads to longer training time,
jeopardizing the deadline requirements for both non-RT and near-RT RICs. To
address this issue, split federated learning (SFL) offers an approach by
offloading partial model layers from near-RT-RIC to high-performance
non-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent
data/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur
significant communication cost in O-RAN. (ii) Proper allocation of
computational and communication resources in O-RAN is vital to satisfying the
deadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL
framework that exploits mutual learning to alternately and independently train
the near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating
frequent transfers. The ''inverse'' of the inverse model is derived via a
zeroth-order technique to integrate the final model. Then, we solve a joint
optimization problem for SplitMe to minimize overall resource costs with
deadline-aware selection of near-RT-RICs and adaptive local updates. Our
numerical results demonstrate that SplitMe remarkably outperforms FL frameworks
like SFL, FedAvg and O-RANFed regarding costs and convergence.

</details>


### [191] [Entity Representation Learning Through Onsite-Offsite Graph for Pinterset Ads](https://arxiv.org/abs/2508.02609)
*Jiayin Jin,Zhimeng Pan,Yang Tang,Jiarui Feng,Kungang Li,Chongyuan Xiang,Jiacheng Li,Runze Su,Siping Ji,Han Sun,Ling Leng,Prathibha Deshikachar*

Main category: cs.LG

TL;DR: 该论文提出了一种结合现场和场外用户活动的异构图构建方法，并设计了TransRA模型来优化广告排名，通过创新方法显著提升了CTR和CVR预测性能。


<details>
  <summary>Details</summary>
Motivation: 为了更好地利用用户的场外转化数据并探索现场和场外活动之间的关系，以提升广告推荐系统的效果。

Method: 构建了一个大规模的异构图，介绍了TransRA模型，并采用大ID嵌入表和基于注意力的KGE微调方法。

Result: 在CTR和CVR预测模型中实现了显著的AUC提升，并在Pinterest的广告模型中贡献了2.69%的CTR提升和1.34%的CPC降低。

Conclusion: 论文提出的技术可以应用于其他大规模工业模型，具有广泛的实践价值。

Abstract: Graph Neural Networks (GNN) have been extensively applied to industry
recommendation systems, as seen in models like GraphSage\cite{GraphSage},
TwHIM\cite{TwHIM}, LiGNN\cite{LiGNN} etc. In these works, graphs were
constructed based on users' activities on the platforms, and various graph
models were developed to effectively learn node embeddings. In addition to
users' onsite activities, their offsite conversions are crucial for Ads models
to capture their shopping interest. To better leverage offsite conversion data
and explore the connection between onsite and offsite activities, we
constructed a large-scale heterogeneous graph based on users' onsite ad
interactions and opt-in offsite conversion activities. Furthermore, we
introduced TransRA (TransR\cite{TransR} with Anchors), a novel Knowledge Graph
Embedding (KGE) model, to more efficiently integrate graph embeddings into Ads
ranking models. However, our Ads ranking models initially struggled to directly
incorporate Knowledge Graph Embeddings (KGE), and only modest gains were
observed during offline experiments. To address this challenge, we employed the
Large ID Embedding Table technique and innovated an attention based KGE
finetuning approach within the Ads ranking models. As a result, we observed a
significant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)
prediction models. Moreover, this framework has been deployed in Pinterest's
Ads Engagement Model and contributed to $2.69\%$ CTR lift and $1.34\%$ CPC
reduction. We believe the techniques presented in this paper can be leveraged
by other large-scale industrial models.

</details>


### [192] [RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation](https://arxiv.org/abs/2508.01240)
*Juntong Chen,Huayuan Ye,He Zhu,Siwei Fu,Changbo Wang,Chenhui Li*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间插值管道，结合GNN和地理编码，提高了时空数据的可视化可靠性，并通过热图编码不确定性信息。


<details>
  <summary>Details</summary>
Motivation: 传统空间插值方法因传感器覆盖有限且不规则，难以提供可靠结果，亟需改进。

Method: 整合GNN的插补参考数据和PNA、GPE技术，学习时空依赖，提出静态可视化技术展示不确定性。

Result: 在真实数据集和用户研究中，模型在数据插补和不确定性可视化方面表现优越。

Conclusion: 新方法显著提升了时空数据插值的可靠性和不确定性传达效果。

Abstract: Accurate and reliable visualization of spatiotemporal sensor data such as
environmental parameters and meteorological conditions is crucial for informed
decision-making. Traditional spatial interpolation methods, however, often fall
short of producing reliable interpolation results due to the limited and
irregular sensor coverage. This paper introduces a novel spatial interpolation
pipeline that achieves reliable interpolation results and produces a novel
heatmap representation with uncertainty information encoded. We leverage
imputation reference data from Graph Neural Networks (GNNs) to enhance
visualization reliability and temporal resolution. By integrating Principal
Neighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), our
model effectively learns the spatiotemporal dependencies. Furthermore, we
propose an extrinsic, static visualization technique for interpolation-based
heatmaps that effectively communicates the uncertainties arising from various
sources in the interpolated map. Through a set of use cases, extensive
evaluations on real-world datasets, and user studies, we demonstrate our
model's superior performance for data imputation, the improvements to the
interpolant with reference data, and the effectiveness of our visualization
design in communicating uncertainties.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [193] [CM$^3$: Calibrating Multimodal Recommendation](https://arxiv.org/abs/2508.01226)
*Xin Zhou,Yongjie Wang,Zhiqi Shen*

Main category: cs.IR

TL;DR: 该研究重新审视了多模态推荐系统中的对齐和均匀性原则，提出了一种基于多模态相似性的校准均匀性损失方法，并通过球形Bézier方法增强多模态特征融合，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多模态推荐系统中过度强调均匀性而忽略了对齐性，导致性能受限。研究旨在通过校准均匀性损失和改进特征融合方法来解决这一问题。

Method: 利用多模态数据的内在相似性校准均匀性分布，引入球形Bézier方法融合多模态特征，确保特征位于同一超球面流形上。

Result: 在五个真实数据集上的实验表明，该方法显著优于基线模型，NDCG@20性能最高提升5.4%。

Conclusion: 校准均匀性损失和改进的多模态特征融合方法有效提升了多模态推荐系统的性能，同时开源了代码以促进研究。

Abstract: Alignment and uniformity are fundamental principles within the domain of
contrastive learning. In recommender systems, prior work has established that
optimizing the Bayesian Personalized Ranking (BPR) loss contributes to the
objectives of alignment and uniformity. Specifically, alignment aims to draw
together the representations of interacting users and items, while uniformity
mandates a uniform distribution of user and item embeddings across a unit
hypersphere. This study revisits the alignment and uniformity properties within
the context of multimodal recommender systems, revealing a proclivity among
extant models to prioritize uniformity to the detriment of alignment. Our
hypothesis challenges the conventional assumption of equitable item treatment
through a uniformity loss, proposing a more nuanced approach wherein items with
similar multimodal attributes converge toward proximal representations within
the hyperspheric manifold. Specifically, we leverage the inherent similarity
between items' multimodal data to calibrate their uniformity distribution,
thereby inducing a more pronounced repulsive force between dissimilar entities
within the embedding space. A theoretical analysis elucidates the relationship
between this calibrated uniformity loss and the conventional uniformity
function. Moreover, to enhance the fusion of multimodal features, we introduce
a Spherical B\'ezier method designed to integrate an arbitrary number of
modalities while ensuring that the resulting fused features are constrained to
the same hyperspherical manifold. Empirical evaluations conducted on five
real-world datasets substantiate the superiority of our approach over competing
baselines. We also shown that the proposed methods can achieve up to a 5.4%
increase in NDCG@20 performance via the integration of MLLM-extracted features.
Source code is available at: https://github.com/enoche/CM3.

</details>


### [194] [Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches](https://arxiv.org/abs/2508.02096)
*Raj Mahmud,Yufeng Wu,Abdullah Bin Sawad,Shlomo Berkovsky,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.IR

TL;DR: 本文系统综述了2017-2025年间23项实证研究，分析了对话推荐系统(CRSs)中用户体验(UX)的评估现状，指出了现有研究的局限性，并提出了未来LLM感知的UX评估方向。


<details>
  <summary>Details</summary>
Motivation: 填补对话推荐系统用户体验评估研究的空白，特别是在自适应和基于大语言模型(LLM)的系统中。

Method: 遵循PRISMA指南，系统综述了23项实证研究，分析了UX的定义、测量及其与领域、自适应性和LLM的关系。

Result: 发现现有研究存在局限性，如过度依赖事后调查、缺乏对转向级情感UX的评估，以及LLM带来的新挑战（如不透明性和冗长性）未被充分研究。

Conclusion: 提出了结构化UX指标体系、自适应与非自适应系统比较分析，以及面向LLM的UX评估议程，以支持更透明、更用户中心的CRS评估实践。

Abstract: Conversational Recommender Systems (CRSs) are receiving growing research
attention across domains, yet their user experience (UX) evaluation remains
limited. Existing reviews largely overlook empirical UX studies, particularly
in adaptive and large language model (LLM)-based CRSs. To address this gap, we
conducted a systematic review following PRISMA guidelines, synthesising 23
empirical studies published between 2017 and 2025. We analysed how UX has been
conceptualised, measured, and shaped by domain, adaptivity, and LLM.
  Our findings reveal persistent limitations: post hoc surveys dominate,
turn-level affective UX constructs are rarely assessed, and adaptive behaviours
are seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,
including epistemic opacity and verbosity, yet evaluations infrequently address
these issues. We contribute a structured synthesis of UX metrics, a comparative
analysis of adaptive and nonadaptive systems, and a forward-looking agenda for
LLM-aware UX evaluation. These findings support the development of more
transparent, engaging, and user-centred CRS evaluation practices.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [195] [Solving Sudoku Using Oscillatory Neural Networks](https://arxiv.org/abs/2508.02250)
*Stefan Porfir,Bram F. Haverkort,Federico Sbravati,Aida Todri-Sanial*

Main category: cond-mat.dis-nn

TL;DR: 本文探讨了振荡神经网络（ONNs）在解决数独谜题中的应用，通过相位同步的生物启发性方法展示了其高性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证ONNs在约束优化问题中的有效性，特别是针对数独这类问题，同时探索非冯·诺依曼计算范式的潜力。

Method: 使用Kuramoto模型控制振荡器的相位同步，将数独约束编码到网络的权重矩阵中，并提出新的数字相位映射方法。

Result: 实验表明，ONNs在中等难度数独中表现优异，尤其在初始未知值不超过20时优于Hopfield神经网络。

Conclusion: ONNs为约束优化问题提供了可行的替代方案，并突显了其在非冯·诺依曼计算范式中的重要性。

Abstract: This paper explores the application of Oscillatory Neural Networks (ONNs) to
solving Sudoku puzzles, presenting a biologically inspired approach based on
phase synchronization. Each cell is represented by an oscillator whose phase
encodes a digit, and the synchronization is governed by the Kuramoto model. The
system dynamically evolves towards a valid solution by having the puzzle
constraints encoded into the weight matrix of the network, and through a
proposed novel phase mapping of the Sudoku digits. Experimental results show
that ONNs achieve high performance for puzzles with moderate difficulty and
outperform Hopfield Neural Networks, particularly in cases with up to 20
initially unknown values. Although the performance decreases with increased
ambiguity, ONNs still produce correct solutions in some of the iterations,
cases in which the baseline Hopfield Neural Network algorithm fails. The
findings support ONNs as a viable alternative for solving constraint
optimization problems and reinforce their relevance within emerging non-von
Neumann computing paradigms.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [196] [RC-Gossip: Information Freshness in Clustered Networks with Rate-Changing Gossip](https://arxiv.org/abs/2508.02657)
*Irtiza Hasan,Ahmed Arafa*

Main category: cs.IT

TL;DR: 引入了一种名为RC-Gossip的智能信息传播机制，通过动态调整传播速率提升节点信息新鲜度，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何在集群化网络中通过优化信息传播机制，使节点保持信息新鲜度。

Method: 提出RC-Gossip机制，动态调整传播速率，并结合更新奖励理论分析新鲜度。

Result: RC-Gossip显著提高了不同集群网络中节点的新鲜度。

Conclusion: RC-Gossip是一种高效的信息传播机制，适用于集群化网络。

Abstract: A clustered gossip network is considered in which a source updates its
information over time, and end-nodes, organized in clusters through
clusterheads, are keeping track of it. The goal for the nodes is to remain as
fresh as possible, i.e., have the same information as the source, which we
assess by the long-term average binary freshness metric. We introduce a smart
mechanism of information dissemination which we coin rate-changing gossip
(RC-Gossip). Its main idea is that gossiping is directed towards nodes that
need it the most, and hence the rate of gossiping changes based on the number
of fresh nodes in the network at a given time. While Stochastic Hybrid System
(SHS) analysis has been the norm in studying freshness of gossip networks, we
present an equivalent way to analyze freshness using a renewal-reward-based
approach. Using that, we show that RC-gossip significantly increases freshness
of nodes in different clustered networks, with optimal cluster sizes, compared
to traditional gossiping techniques.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [197] [GPU in the Blind Spot: Overlooked Security Risks in Transportation](https://arxiv.org/abs/2508.01995)
*Sefatun-Noor Puspa,Mashrur Chowdhury*

Main category: cs.CR

TL;DR: 论文探讨了GPU在智能交通系统中的安全盲点，揭示了未经授权加密挖矿对AI工作负载的影响，并提出了一种基于设备遥测的检测框架。


<details>
  <summary>Details</summary>
Motivation: GPU因其高性能和能效在智能交通系统中广泛应用，但其安全性常被忽视，易受攻击，本文旨在揭示这一问题。

Method: 通过案例研究，展示了加密挖矿对YOLOv8视频处理管道的性能影响，并利用GPU遥测数据训练分类器进行检测。

Result: 加密挖矿导致帧率下降50%，功耗增加90%，而分类器在检测中表现出高精度和高召回率。

Conclusion: 论文呼吁加强对GPU安全的监控，并提供了可复用的检测框架，以应对智能交通系统中的安全挑战。

Abstract: Graphics processing units (GPUs) are becoming an essential part of the
intelligent transportation system (ITS) for enabling video-based and artificial
intelligence (AI) based applications. GPUs provide high-throughput and
energy-efficient computing for tasks like sensor fusion and roadside video
analytics. However, these GPUs are one of the most unmonitored components in
terms of security. This makes them vulnerable to cyber and hardware attacks,
including unauthorized crypto mining. This paper highlights GPU security as a
critical blind spot in transportation cybersecurity. To support this concern,
it also presents a case study showing the impact of stealthy unauthorized
crypto miners on critical AI workloads, along with a detection strategy. We
used a YOLOv8-based video processing pipeline running on an RTX 2060 GPU for
the case study. A multi-streaming application was executed while a T-Rex crypto
miner ran in the background. We monitored how the miner degraded GPU
performance by reducing the frame rate and increasing power consumption, which
could be a serious concern for GPUs operating in autonomous vehicles or
battery-powered edge devices. We observed measurable impacts using GPU
telemetry (nvidia-smi) and Nsight Compute profiling, where frame rate dropped
by 50 percent, and power usage increased by up to 90%. To detect, we trained
lightweight classifiers using extracted telemetry features. All models achieved
high accuracy, precision, recall, and F1-score. This paper raises urgent
awareness about GPU observability gaps in ITS and offers a replicable framework
for detecting GPU misuse through on-device telemetry.

</details>


### [198] [Prompt to Pwn: Automated Exploit Generation for Smart Contracts](https://arxiv.org/abs/2508.01371)
*Zeke Xiao,Yuekang Li,Qin Wang,Shiping Chen*

Main category: cs.CR

TL;DR: 研究探索了使用大语言模型（LLM）进行自动化漏洞利用生成（AEG）的可行性，并提出了集成LLM与Foundry测试套件的框架ReX，成功率高。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在智能合约漏洞利用生成中的潜力，以自动化生成和验证漏洞利用。

Method: 提出ReX框架，结合LLM和Foundry测试套件，评估了五种先进LLM在合成和真实场景中的表现。

Result: 现代LLM能高效生成功能性的漏洞利用，成功率高达92%，Gemini 2.5 Pro和GPT-4.1表现最佳。

Conclusion: LLM在AEG中表现优异，为未来研究提供了首个真实漏洞利用数据集。

Abstract: We explore the feasibility of using LLMs for Automated Exploit Generation
(AEG) against vulnerable smart contracts. We present \textsc{ReX}, a framework
integrating LLM-based exploit synthesis with the Foundry testing suite,
enabling the automated generation and validation of proof-of-concept (PoC)
exploits. We evaluate five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro,
Claude Opus 4, DeepSeek, and Qwen3 Plus) on both synthetic benchmarks and
real-world smart contracts affected by known high-impact exploits. Our results
show that modern LLMs can reliably generate functional PoC exploits for diverse
vulnerability types, with success rates reaching up to 92\%. Notably, Gemini
2.5 Pro and GPT-4.1 consistently outperform others in both synthetic and
real-world scenarios. We further analyze factors influencing AEG effectiveness,
including model capabilities, contract structure, and vulnerability types. We
also collect the first curated dataset of real-world PoC exploits to support
future research.

</details>


### [199] [eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices](https://arxiv.org/abs/2508.00851)
*Abdurrahman Tolay*

Main category: cs.CR

TL;DR: 该论文提出了一个基于eBPF/XDP的轻量级IoT安全框架，用于在边缘设备上高效缓解DDoS攻击，展示了超过97%的缓解效果。


<details>
  <summary>Details</summary>
Motivation: IoT设备的快速扩展带来了安全挑战，尤其是资源受限设备发起的DDoS攻击，传统的防御方法不适用于IoT环境。

Method: 使用eBPF和XDP设计了一个内核级别的DDoS攻击缓解系统，采用基于速率的检测算法在早期识别并拦截恶意流量。

Result: 在仿真和实际部署（Raspberry Pi 4）中，系统对100 Mbps的攻击流量实现了超过97%的缓解效果，同时不影响合法流量和系统稳定性。

Conclusion: eBPF/XDP为IoT边缘设备提供了高效且可行的解决方案，有效防御大规模网络攻击。

Abstract: The rapid expansion of the Internet of Things (IoT) has intensified security
challenges, notably from Distributed Denial of Service (DDoS) attacks launched
by compromised, resource-constrained devices. Traditional defenses are often
ill-suited for the IoT paradigm, creating a need for lightweight,
high-performance, edge-based solutions. This paper presents the design,
implementation, and evaluation of an IoT security framework that leverages the
extended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for
in-kernel mitigation of DDoS attacks. The system uses a rate-based detection
algorithm to identify and block malicious traffic at the earliest stage of the
network stack. The framework is evaluated using both Docker-based simulations
and real-world deployment on a Raspberry Pi 4, showing over 97% mitigation
effectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected,
and system stability is preserved even under attack. These results confirm that
eBPF/XDP provides a viable and highly efficient solution for hardening IoT edge
devices against volumetric network attacks.

</details>


### [200] [VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments](https://arxiv.org/abs/2508.01469)
*Imtiaz Karim,Hyunwoo Lee,Hassan Asghar,Kazi Samin Mubasshir,Seulgi Han,Mashroor Hasan Bhuiyan,Elisa Bertino*

Main category: cs.CR

TL;DR: VWAttacker是一个测试VoWiFi UE实现安全性的框架，通过自动化方法和属性引导测试发现多个安全问题。


<details>
  <summary>Details</summary>
Motivation: 研究VoWiFi UE实现的安全性漏洞，以提升蜂窝网络的安全。

Method: 结合LLM的半自动属性提取和测试用例生成，系统性变异和确定性检测。

Result: 检测到21个UE中的13个问题，包括身份暴露和弱信道建立等。

Conclusion: VWAttacker有效发现安全问题，部分问题已被厂商确认为高风险。

Abstract: We present VWAttacker, the first systematic testing framework for analyzing
the security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations.
VWAttacker includes a complete VoWiFi network testbed that communicates with
Commercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the
behavior of diverse VoWiFi UE implementations; uses property-guided adversarial
testing to uncover security issues in different UEs systematically. To reduce
manual effort in extracting and testing properties, we introduce an LLM-based,
semi-automatic, and scalable approach for property extraction and testcase (TC)
generation. These TCs are systematically mutated by two domain-specific
transformations. Furthermore, we introduce two deterministic oracles to detect
property violations automatically. Coupled with these techniques, VWAttacker
extracts 63 properties from 11 specifications, evaluates 1,116 testcases, and
detects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret
to 0 to supporting weak algorithms. These issues result in attacks that expose
the victim UE's identity or establish weak channels, thus severely hampering
the security of cellular networks. We responsibly disclose the findings to all
the related vendors. At the time of writing, one of the vulnerabilities has
been acknowledged by MediaTek with high severity.

</details>


### [201] [RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging](https://arxiv.org/abs/2508.01784)
*Xin He,Junxi Shen,Zhenheng Tang,Xiaowen Chu,Bo Li,Ivor W. Tsang,Yew-Soon Ong*

Main category: cs.CR

TL;DR: 论文提出RouteMark框架，通过设计专家路由指纹保护合并MoE模型中的知识产权（IP）。


<details>
  <summary>Details</summary>
Motivation: 为了解决合并MoE模型中专家知识产权归属和保护的未探索挑战。

Method: 基于专家路由行为构建两种轻量级指纹（RSF和RPF），并设计相似性匹配算法进行IP验证。

Result: 实验证明RouteMark能有效识别被重用专家，并对抗多种篡改操作，优于基准方法。

Conclusion: RouteMark为MoE模型合并中的IP保护提供了实用且广泛适用的解决方案。

Abstract: Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution
for consolidating multiple task-specific models into a unified sparse
architecture, where each expert is derived from a model fine-tuned on a
distinct task. While effective for multi-task integration, this paradigm
introduces a critical yet underexplored challenge: how to attribute and protect
the intellectual property (IP) of individual experts after merging. We propose
RouteMark, a framework for IP protection in merged MoE models through the
design of expert routing fingerprints. Our key insight is that task-specific
experts exhibit stable and distinctive routing behaviors under probing inputs.
To capture these patterns, we construct expert-level fingerprints using two
complementary statistics: the Routing Score Fingerprint (RSF), quantifying the
intensity of expert activation, and the Routing Preference Fingerprint (RPF),
characterizing the input distribution that preferentially activates each
expert. These fingerprints are reproducible, task-discriminative, and
lightweight to construct. For attribution and tampering detection, we introduce
a similarity-based matching algorithm that compares expert fingerprints between
a suspect and a reference (victim) model. Extensive experiments across diverse
tasks and CLIP-based MoE architectures show that RouteMark consistently yields
high similarity for reused experts and clear separation from unrelated ones.
Moreover, it remains robust against both structural tampering (expert
replacement, addition, deletion) and parametric tampering (fine-tuning,
pruning, permutation), outperforming weight- and activation-based baseliness.
Our work lays the foundation for RouteMark as a practical and broadly
applicable framework for IP verification in MoE-based model merging.

</details>


### [202] [Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS](https://arxiv.org/abs/2508.01863)
*Sanjay Singh,Mitendra Mahto*

Main category: cs.CR

TL;DR: 论文提出了一种基于零信任架构的多维解决方案，结合反向代理、mTLS和集中式SSO，解决了分布式动态劳动力的安全访问问题。


<details>
  <summary>Details</summary>
Motivation: 传统VPN和SSO方法在安全扩展分布式劳动力访问方面存在不足，需要更现代的解决方案。

Method: 实现了一种零信任对齐架构，整合反向代理、mTLS和集中式SSO，涉及设备和用户认证、集中安全策略和全面监控。

Result: 该方案能够为内部应用提供安全无缝的访问。

Conclusion: 通过现代零信任架构成功解决了传统方法的局限性，并总结了部署和扩展中的关键挑战和经验。

Abstract: In today's enterprise environment, traditional access methods such as Virtual
Private Networks (VPNs) and application-specific Single Sign-On (SSO) often
fall short when it comes to securely scaling access for a distributed and
dynamic workforce. This paper presents our experience implementing a modern,
Zero Trust-aligned architecture that leverages a reverse proxy integrated with
Mutual TLS (mTLS) and centralized SSO, along with the key challenges we
encountered and lessons learned during its deployment and scaling. This
multidimensional solution involves both per-device and per-user authentication,
centralized enforcement of security policies, and comprehensive observability,
hence enabling organizations to deliver secure and seamless access to their
internal applications.

</details>


### [203] [AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference](https://arxiv.org/abs/2508.01107)
*Shima Yousefi,Motahare Mounesan,Saptarshi Debroy*

Main category: cs.CR

TL;DR: 该论文提出了一种名为AdVAR-DNN的攻击方法，利用对抗变分自编码器和分类器，通过敏感信息交换漏洞破坏协作DNN推理过程。


<details>
  <summary>Details</summary>
Motivation: 物联网设备计算能力有限，常采用协作DNN推理将部分计算卸载到远程服务器，而动态分区信息的交换可能导致隐私泄露。

Method: 提出AdVAR-DNN攻击，结合对抗变分自编码器和分类器，无需DNN模型和分区知识即可生成不可追踪的操纵样本。

Result: 在CIFAR-100数据集上测试表明，AdVAR-DNN攻击成功率高且几乎无法被检测到。

Conclusion: AdVAR-DNN暴露了协作推理中的隐私漏洞，需要加强安全性研究。

Abstract: In recent years, Deep Neural Networks (DNNs) have become increasingly
integral to IoT-based environments, enabling realtime visual computing.
However, the limited computational capacity of these devices has motivated the
adoption of collaborative DNN inference, where the IoT device offloads part of
the inference-related computation to a remote server. Such offloading often
requires dynamic DNN partitioning information to be exchanged among the
participants over an unsecured network or via relays/hops, leading to novel
privacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial
variational autoencoder (VAE)-based misclassification attack, leveraging
classifiers to detect model information and a VAE to generate untraceable
manipulated samples, specifically designed to compromise the collaborative
inference process. AdVAR-DNN attack uses the sensitive information exchange
vulnerability of collaborative DNN inference and is black-box in nature in
terms of having no prior knowledge about the DNN model and how it is
partitioned. Our evaluation using the most popular object classification DNNs
on the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms
of high attack success rate with little to no probability of detection.

</details>


### [204] [DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems](https://arxiv.org/abs/2508.01997)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed,Anthony Green*

Main category: cs.CR

TL;DR: 论文提出了数字身份权利框架（DIRF），以应对生成式AI对个人身份完整性的威胁，包括法律、技术和管理机制的保护。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了数字克隆、身份冒充等问题，亟需保护数字身份的安全和权益。

Method: 开发了DIRF框架，包含九大领域和63项控制措施，整合法律和技术手段。

Result: DIRF为保护数字身份提供了全面解决方案，涵盖了身份同意、追踪和商业化的需求。

Conclusion: DIRF框架为平台开发者、法律实体和监管机构提供了关键的控制措施，以保障AI系统中的身份权利。

Abstract: The rapid advancement and widespread adoption of generative artificial
intelligence (AI) pose significant threats to the integrity of personal
identity, including digital cloning, sophisticated impersonation, and the
unauthorized monetization of identity-related data. Mitigating these risks
necessitates the development of robust AI-generated content detection systems,
enhanced legal frameworks, and ethical guidelines. This paper introduces the
Digital Identity Rights Framework (DIRF), a structured security and governance
model designed to protect behavioral, biometric, and personality-based digital
likeness attributes to address this critical need. Structured across nine
domains and 63 controls, DIRF integrates legal, technical, and hybrid
enforcement mechanisms to secure digital identity consent, traceability, and
monetization. We present the architectural foundations, enforcement strategies,
and key use cases supporting the need for a unified framework. This work aims
to inform platform builders, legal entities, and regulators about the essential
controls needed to enforce identity rights in AI-driven systems.

</details>


### [205] [Nakamoto Consensus from Multiple Resources](https://arxiv.org/abs/2508.01448)
*Mirza Ahad Baig,Christoph U. Günther,Krzysztof Pietrzak*

Main category: cs.CR

TL;DR: 论文分析了比特币和Chia区块链中权重函数的安全性，提出了确保区块链免受双花攻击的权重函数条件，并给出了具体实例和应用建议。


<details>
  <summary>Details</summary>
Motivation: 研究目的是确定哪些权重函数可以确保区块链在诚实方资源权重占优时免受双花攻击，为设计新的最长链区块链提供理论支持。

Method: 通过理想化的连续模型和离散时间点的实际模型，分析权重函数的性质，包括齐次性和对空间的依赖关系。

Result: 确定了权重函数需满足的条件：在连续模型中需是V和W的一次齐次函数，离散模型中还需满足对空间的限制。提出了多种安全权重函数的实例。

Conclusion: 研究为设计区块链提供了工具，并建议在实践中采用更优的权重组合方式，如平方根或最小值法。

Abstract: The blocks in the Bitcoin blockchain record the amount of work W that went
into creating them through proofs of work. When honest parties control a
majority of the work, consensus is achieved by picking the chain with the
highest recorded weight. Resources other than work have been considered to
secure such longest-chain blockchains. In Chia, blocks record the amount of
space S (via a proof of space) and sequential computational steps V (via a
VDF).
  In this paper, we ask what weight functions {\Gamma}(S,V,W) (that assign a
weight to a block as a function of the recorded space, speed, and work) are
secure in the sense that whenever the weight of the resources controlled by
honest parties is larger than the weight of adversarial parties, the blockchain
is secure against private double-spending attacks.
  We completely classify such functions in an idealized "continuous" model:
{\Gamma}(S,V,W) is secure against private double-spending attacks if and only
if it is homogeneous of degree one in the timed resources V and W, i.e.,
{\alpha}{\Gamma}(S,V,W)={\Gamma}(S,{\alpha}V, {\alpha}W). This includes Bitcoin
rule {\Gamma}(S,V,W)=W and Chia rule {\Gamma}(S,V,W) = SV. In a more realistic
model where blocks are created at discrete time-points, one additionally needs
some mild assumptions on the dependency on S (basically, the weight should not
grow too much if S is slightly increased, say linear as in Chia).
  Our classification is more general and allows various instantiations of the
same resource. It provides a powerful tool for designing new longest-chain
blockchains. E.g., consider combining different PoWs to counter centralization,
say the Bitcoin PoW W_1 and a memory-hard PoW W_2. Previous work suggested to
use W_1+W_2 as weight. Our results show that using
{\sqrt}(W_1){\cdot}{\sqrt}(W_2), {\min}{W_1,W_2} are also secure, and we argue
that in practice these are much better choices.

</details>


### [206] [AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection](https://arxiv.org/abs/2508.01249)
*Peiran Wang,Yang Liu,Yunfei Lu,Yifeng Cai,Hongbo Chen,Qingyou Yang,Jie Zhang,Jue Hong,Ye Wu*

Main category: cs.CR

TL;DR: AgentArmor是一个程序分析框架，通过将LLM代理的运行跟踪转换为结构化程序依赖表示，来检测提示注入攻击并执行安全策略。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理的动态和不透明行为存在安全风险，尤其是提示注入攻击，因此需要一种方法来分析其行为并增强安全性。

Method: 提出AgentArmor框架，包括图构造器、属性注册表和类型系统，将代理行为转换为结构化程序进行分析。

Result: 在AgentDojo基准测试中，AgentArmor实现了95.75%的真实阳性率和3.66%的假阳性率。

Conclusion: AgentArmor能有效检测提示注入漏洞并执行细粒度安全约束。

Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving
various problems by combining natural language reasoning with the execution of
external tools. However, their dynamic and non-transparent behavior introduces
critical security risks, particularly in the presence of prompt injection
attacks. In this work, we propose a novel insight that treats the agent runtime
traces as structured programs with analyzable semantics. Thus, we present
AgentArmor, a program analysis framework that converts agent traces into graph
intermediate representation-based structured program dependency representations
(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.
AgentArmor consists of three key components: (1) a graph constructor that
reconstructs the agent's working traces as graph-based intermediate
representations with control flow and data flow described within; (2) a
property registry that attaches security-relevant metadata of interacted tools
& data, and (3) a type system that performs static inference and checking over
the intermediate representation. By representing agent behavior as structured
programs, AgentArmor enables program analysis over sensitive data flow, trust
boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo
benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with
only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect
prompt injection vulnerabilities and enforce fine-grained security constraints.

</details>


### [207] [Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models](https://arxiv.org/abs/2508.01451)
*Mohammed Sayagh,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 论文提出了一种多智能体LLM方法，通过三步流程改进漏洞检测，结合上下文信息显著减少误报并提高CWE识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在漏洞检测中存在三个问题：一次性预测的深度分析不足、缺乏函数范围外的上下文信息、以及错误的CWE关联。

Method: 采用三步多智能体LLM方法：(1)搜索潜在CWE，(2)通过外部上下文验证CWE，(3)基于上下文做出最终决策。

Result: 在PrimeVul数据集中，步骤1正确识别CWE的准确率为40.9%；全流程测试中，误报从6-9降至1-2，且9/10案例正确识别CWE。

Conclusion: 多智能体结合上下文的方法显著提升LLM在漏洞检测中的表现，尤其在减少误报和提高CWE准确性方面效果显著。

Abstract: Machine learning and Large language models (LLMs) for vulnerability detection
has received significant attention in recent years. Unfortunately,
state-of-the-art techniques show that LLMs are unsuccessful in even
distinguishing the vulnerable function from its benign counterpart, due to
three main problems: Vulnerability detection requires deep analysis, which LLMs
often struggle with when making a one-shot prediction. Existing techniques
typically perform function-level analysis, whereas effective vulnerability
detection requires contextual information beyond the function scope. The focus
on binary classification can result in identifying a vulnerability but
associating it with the wrong security weaknesses (CWE), which may mislead
developers. We propose a novel multi-agent LLM approach to address the
challenges of identifying CWEs. This approach consists of three steps: (1) a
team of LLM agents performs an exhaustive search for potential CWEs in the
function under review, (2) another team of agents identifies relevant external
context to support or refute each candidate CWE, and (3) a final agent makes
informed acceptance or rejection decisions for each CWE based on the gathered
context. A preliminary evaluation of our approach shows promising results. In
the PrimeVul dataset, Step 1 correctly identifies the appropriate CWE in 40.9\%
of the studied vulnerable functions. We further evaluated the full pipeline on
ten synthetic programs and found that incorporating context information
significantly reduced false positives from 6 to 9 CWEs to just 1 to 2, while
still correctly identifying the true CWE in 9 out of 10 cases.

</details>


### [208] [JSidentify-V2: Leveraging Dynamic Memory Fingerprinting for Mini-Game Plagiarism Detection](https://arxiv.org/abs/2508.01655)
*Zhihao Li,Chaozheng Wang,Zongjie Li,Xinyong Peng,Qun Xia,Haochuan Lu,Ting Xiong,Shuzheng Gao,Cuiyun Gao,Shuai Wang,Yuetang Deng,Huafeng Ma*

Main category: cs.CR

TL;DR: JSidentify-V2是一个动态分析框架，用于检测小游戏平台的代码抄袭，通过捕捉程序执行时的内存不变量，解决了传统静态分析工具无法应对的深度混淆问题。


<details>
  <summary>Details</summary>
Motivation: 由于小游戏平台的爆炸性增长，代码抄袭现象普遍，恶意用户通过修改源代码重新发布。现有静态分析工具对简单混淆有效，但对深度混淆（如加密代码）无效。

Method: JSidentify-V2采用四阶段流程：静态预分析与插桩、自适应热点对象切片、内存依赖图构建和基于图的相似性分析。

Result: JSidentify-V2在1,200个小游戏的数据集上评估，能够应对八种混淆方法。

Conclusion: 动态分析内存行为模式可有效检测抄袭，JSidentify-V2为复杂混淆提供了鲁棒的解决方案。

Abstract: The explosive growth of mini-game platforms has led to widespread code
plagiarism, where malicious users access popular games' source code and
republish them with modifications. While existing static analysis tools can
detect simple obfuscation techniques like variable renaming and dead code
injection, they fail against sophisticated deep obfuscation methods such as
encrypted code with local or cloud-based decryption keys that completely
destroy code structure and render traditional Abstract Syntax Tree analysis
ineffective. To address these challenges, we present JSidentify-V2, a novel
dynamic analysis framework that detects mini-game plagiarism by capturing
memory invariants during program execution. Our key insight is that while
obfuscation can severely distort static code characteristics, runtime memory
behavior patterns remain relatively stable. JSidentify-V2 employs a four-stage
pipeline: (1) static pre-analysis and instrumentation to identify potential
memory invariants, (2) adaptive hot object slicing to maximize execution
coverage of critical code segments, (3) Memory Dependency Graph construction to
represent behavioral fingerprints resilient to obfuscation, and (4) graph-based
similarity analysis for plagiarism detection.
  We evaluate JSidentify-V2 against eight obfuscation methods on a
comprehensive dataset of 1,200 mini-games ...

</details>


### [209] [LLM-Assisted Model-Based Fuzzing of Protocol Implementations](https://arxiv.org/abs/2508.01750)
*Changze Huang,Di Wang,Zhi Quan Zhou*

Main category: cs.CR

TL;DR: 提出了一种利用大语言模型（LLM）自动生成网络协议测试序列的新方法，成功识别了12个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 测试网络协议实现对于确保分布式系统的可靠性、安全性和互操作性至关重要，传统方法需要大量手动建模，难以扩展。

Method: 利用LLM从协议状态集中选择子集建模，生成状态序列代码作为测试序列生成器，进而生成测试输入。

Result: 在三种广泛使用的网络协议实现中成功发现了12个未知漏洞。

Conclusion: 该方法通过LLM辅助的模糊测试框架，能够有效发现实际安全漏洞，具有实际应用价值。

Abstract: Testing network protocol implementations is critical for ensuring the
reliability, security, and interoperability of distributed systems. Faults in
protocol behavior can lead to vulnerabilities and system failures, especially
in real-time and mission-critical applications. A common approach to protocol
testing involves constructing Markovian models that capture the state
transitions and expected behaviors of the protocol. However, building such
models typically requires significant domain expertise and manual effort,
making the process time-consuming and difficult to scale across diverse
protocols and implementations.
  We propose a novel method that leverages large language models (LLMs) to
automatically generate sequences for testing network protocol implementations.
Our approach begins by defining the full set of possible protocol states, from
which the LLM selects a subset to model the target implementation. Using this
state-based model, we prompt the LLM to generate code that produces sequences
of states. This program serves as a protocol-specific sequences generator. The
sequences generator then generates test inputs to call the protocol
implementation under various conditions. We evaluated our approach on three
widely used network protocol implementations and successfully identified 12
previously unknown vulnerabilities. We have reported them to the respective
developers for confirmation. This demonstrates the practical effectiveness of
our LLM-assisted fuzzing framework in uncovering real-world security issues.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [210] [FluidFormer: Transformer with Continuous Convolution for Particle-based Fluid Simulation](https://arxiv.org/abs/2508.01537)
*Nianyi Wang,Yu Chen,Shuai Zheng*

Main category: cs.CE

TL;DR: 提出了一种结合局部-全局层次的Fluid Attention Block（FAB）和专为流体模拟设计的Transformer架构FluidFormer，显著提升了神经流体模拟的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经网络的流体模拟方法依赖于局部粒子相互作用，但忽视全局上下文集成可能导致复杂流体模拟的不稳定性。因此，需要一种结合局部和全局特征的新方法。

Method: 设计了Fluid Attention Block（FAB），通过连续卷积提取局部特征，自注意力机制捕捉全局依赖关系。进一步提出了专为连续流体模拟设计的Transformer架构FluidFormer，采用双管道结构集成局部与全局特征。

Result: FluidFormer在复杂流体场景中展现出卓越的稳定性和性能，成为神经流体模拟的新范式。

Conclusion: 通过结合卷积和注意力机制，实现了局部与全局特征的统一建模，显著提升了神经流体模拟的效果，为相关研究提供了新思路。

Abstract: Learning-based fluid simulation networks have been proven as viable
alternatives to traditional numerical solvers for the Navier-Stokes equations.
Existing neural methods follow Smoothed Particle Hydrodynamics (SPH)
frameworks, which inherently rely only on local inter-particle interactions.
However, we emphasize that global context integration is also essential for
learning-based methods to stabilize complex fluid simulations. We propose the
first Fluid Attention Block (FAB) with a local-global hierarchy, where
continuous convolutions extract local features while self-attention captures
global dependencies. This fusion suppresses the error accumulation and models
long-range physical phenomena. Furthermore, we pioneer the first Transformer
architecture specifically designed for continuous fluid simulation, seamlessly
integrated within a dual-pipeline architecture. Our method establishes a new
paradigm for neural fluid simulation by unifying convolution-based local
features with attention-based global context modeling. FluidFormer demonstrates
state-of-the-art performance, with stronger stability in complex fluid
scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [211] [Energy-Predictive Planning for Optimizing Drone Service Delivery](https://arxiv.org/abs/2508.01671)
*Guanting Ren,Babar Shahzaad,Balsam Alkouz,Abdallah Lakhdari,Athman Bouguettaya*

Main category: cs.RO

TL;DR: 提出了一种基于能量预测的无人机服务框架(EPDS)，用于优化空路网络中的包裹配送。


<details>
  <summary>Details</summary>
Motivation: 为了提高无人机在空路网络中配送包裹的效率和能量利用率。

Method: 结合双向LSTM模型预测能量状态和随机到达时间，并使用启发式优化方法选择最优路径和充电计划。

Result: 通过真实数据集验证了框架的有效性。

Conclusion: EPDS框架显著提升了无人机配送的时间和能量效率。

Abstract: We propose a novel Energy-Predictive Drone Service (EPDS) framework for
efficient package delivery within a skyway network. The EPDS framework
incorporates a formal modeling of an EPDS and an adaptive bidirectional Long
Short-Term Memory (Bi-LSTM) machine learning model. This model predicts the
energy status and stochastic arrival times of other drones operating in the
same skyway network. Leveraging these predictions, we develop a heuristic
optimization approach for composite drone services. This approach identifies
the most time-efficient and energy-efficient skyway path and recharging
schedule for each drone in the network. We conduct extensive experiments using
a real-world drone flight dataset to evaluate the performance of the proposed
framework.

</details>


### [212] [Service Discovery-Based Hybrid Network Middleware for Efficient Communication in Distributed Robotic Systems](https://arxiv.org/abs/2508.00947)
*Shiyao Sang,Yinggang Ling*

Main category: cs.RO

TL;DR: RIMAOS2C是一种基于服务发现的混合网络通信中间件，解决了L4自动驾驶中通信多样性、数据传输效率和调度确定性问题，通过Message Bridge机制优化数据流，提升大消息传输效率并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件在大规模L4自动驾驶中难以满足多样化通信需求、优化数据传输效率和确保调度确定性，亟需改进。

Method: 提出RIMAOS2C，利用多级服务发现多播支持多种通信模式，通过Message Bridge优化数据流和共享内存分发消息。

Result: 在L4车辆和Jetson Orin控制器测试中，RIMAOS2C消除消息冗余，大消息传输效率提升36-40％，回调延迟变化减少42-906％。

Conclusion: RIMAOS2C提升了机器人操作系统的通信能力，为自动驾驶分布式计算架构通信优化提供了新方案。

Abstract: Robotic middleware is fundamental to ensuring reliable communication among
system components and is crucial for intelligent robotics, autonomous vehicles,
and smart manufacturing. However, existing robotic middleware often struggles
to meet the diverse communication demands, optimize data transmission
efficiency, and maintain scheduling determinism between Orin computing units in
large-scale L4 autonomous vehicle deployments. This paper presents RIMAOS2C, a
service discovery-based hybrid network communication middleware designed to
tackle these challenges. By leveraging multi-level service discovery multicast,
RIMAOS2C supports a wide variety of communication modes, including multiple
cross-chip Ethernet protocols and PCIe communication capabilities. Its core
mechanism, the Message Bridge, optimizes data flow forwarding and employs
shared memory for centralized message distribution, reducing message redundancy
and minimizing transmission delay uncertainty. Tested on L4 vehicles and Jetson
Orin domain controllers, RIMAOS2C leverages TCP-based ZeroMQ to overcome the
large-message transmission bottleneck in native CyberRT. In scenarios with two
cross-chip subscribers, it eliminates message redundancy and improves
large-data transmission efficiency by 36 to 40 percent while reducing callback
latency variation by 42 to 906 percent. This research advances the
communication capabilities of robotic operating systems and proposes a novel
approach to optimizing communication in distributed computing architectures for
autonomous driving.

</details>


### [213] [Set the Stage: Enabling Storytelling with Multiple Robots through Roleplaying Metaphors](https://arxiv.org/abs/2508.01736)
*Tyrone Justin Sta Maria,Faith Griffin,Jordan Aiko Deja*

Main category: cs.RO

TL;DR: 通过角色扮演隐喻（如导演、木偶师和巫师）设计多机器人系统的丰富手势交互，强调创造性、表现力和直观性。


<details>
  <summary>Details</summary>
Motivation: 传统手势控制在多机器人系统中受限于僵硬的映射和识别约束，需更富表现力的交互方式。

Method: 提出三种角色（导演、木偶师、巫师）作为设计框架，指导多样化的手势集和交互风格。

Result: 角色扮演隐喻为多机器人系统提供了新的交互可能性。

Conclusion: 角色扮演方法为未来人机交互设计提供了创造性、表现力和直观性的新方向。

Abstract: Gestures are an expressive input modality for controlling multiple robots,
but their use is often limited by rigid mappings and recognition constraints.
To move beyond these limitations, we propose roleplaying metaphors as a
scaffold for designing richer interactions. By introducing three roles:
Director, Puppeteer, and Wizard, we demonstrate how narrative framing can guide
the creation of diverse gesture sets and interaction styles. These roles enable
a variety of scenarios, showing how roleplay can unlock new possibilities for
multi-robot systems. Our approach emphasizes creativity, expressiveness, and
intuitiveness as key elements for future human-robot interaction design.

</details>


### [214] [Unraveling the Connection: How Cognitive Workload Shapes Intent Recognition in Robot-Assisted Surgery](https://arxiv.org/abs/2508.01823)
*Mansi Sharma,Antonio Kruger*

Main category: cs.RO

TL;DR: 论文提出了一种智能自适应系统，通过多模态监测认知负荷以提高机器人辅助手术中的意图识别和手术效果。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术的成功依赖于准确理解外科医生意图，但认知负荷会影响意图识别，因此需要一种新方法来监测和优化这一过程。

Method: 开发了一种多模态辅助框架，利用脑活动、心率、肌肉活动和眼动追踪技术，实现对医生意图的语义理解和认知负荷监测。

Result: 该系统有望提高意图识别的准确性，尤其在高压环境下，进一步提升机器人辅助手术的效果。

Conclusion: 通过优化意图识别和监测认知负荷，智能自适应系统将显著改善机器人辅助手术的学习效果和手术结果。

Abstract: Robot-assisted surgery has revolutionized the healthcare industry by
providing surgeons with greater precision, reducing invasiveness, and improving
patient outcomes. However, the success of these surgeries depends heavily on
the robotic system ability to accurately interpret the intentions of the
surgical trainee or even surgeons. One critical factor impacting intent
recognition is the cognitive workload experienced during the procedure. In our
recent research project, we are building an intelligent adaptive system to
monitor cognitive workload and improve learning outcomes in robot-assisted
surgery. The project will focus on achieving a semantic understanding of
surgeon intents and monitoring their mental state through an intelligent
multi-modal assistive framework. This system will utilize brain activity, heart
rate, muscle activity, and eye tracking to enhance intent recognition, even in
mentally demanding situations. By improving the robotic system ability to
interpret the surgeons intentions, we can further enhance the benefits of
robot-assisted surgery and improve surgery outcomes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [215] [Effective and Efficient Conductance-based Community Search at Billion Scale](https://arxiv.org/abs/2508.01244)
*Longlong Lin,Yue He,Wei Chen,Pingpeng Yuan,Rong-Hua Li,Tao Jia*

Main category: cs.SI

TL;DR: 该论文提出了基于电导率的社区搜索问题（CCS），旨在找到包含查询顶点的连通子图中电导率最小的子图，并证明其NP难解性。


<details>
  <summary>Details</summary>
Motivation: 现有社区搜索方法主要关注社区内部的紧密性，而忽略了社区外部的稀疏性，导致结果不佳。

Method: 提出SCCS算法，通过四阶段（采样、播种、扩展、验证）策略优化社区质量。

Result: 实验证明，算法在大规模图和合成数据集上具有高效性、有效性和可扩展性。

Conclusion: CCS问题和SCCS算法有效解决了社区搜索中内外质量权衡问题，为实际应用提供了高效解决方案。

Abstract: Community search is a widely studied semi-supervised graph clustering
problem, retrieving a high-quality connected subgraph containing the
user-specified query vertex. However, existing methods primarily focus on
cohesiveness within the community but ignore the sparsity outside the
community, obtaining sub-par results. Inspired by this, we adopt the well-known
conductance metric to measure the quality of a community and introduce a novel
problem of conductance-based community search (CCS). CCS aims at finding a
subgraph with the smallest conductance among all connected subgraphs that
contain the query vertex. We prove that the CCS problem is NP-hard. To
efficiently query CCS, a four-stage subgraph-conductance-based community search
algorithm, SCCS, is proposed. Specifically, we first greatly reduce the entire
graph using local sampling techniques. Then, a three-stage local optimization
strategy is employed to continuously refine the community quality. Namely, we
first utilize a seeding strategy to obtain an initial community to enhance its
internal cohesiveness. Then, we iteratively add qualified vertices in the
expansion stage to guarantee the internal cohesiveness and external sparsity of
the community. Finally, we gradually remove unqualified vertices during the
verification stage. Extensive experiments on real-world datasets containing one
billion-scale graph and synthetic datasets show the effectiveness, efficiency,
and scalability of our solutions.

</details>


### [216] [A Parallel Algorithm for Finding Robust Spanners in Large Social Networks](https://arxiv.org/abs/2508.01485)
*Arindam Khanda,Satyaki Roy,Prithwiraj Roy,Sajal K. Das*

Main category: cs.SI

TL;DR: 提出了一种名为稳健跨越者（RS）的节点，用于在动态社交网络中维持社区间通信，并开发了一种并行算法高效识别RS节点。


<details>
  <summary>Details</summary>
Motivation: 社交网络中的结构洞跨越者在社区间信息传播中起关键作用，但网络动态性可能导致其失效，因此需要更稳健的方法。

Method: 提出了一种新的评分技术识别RS节点，并设计了CUDA并行算法以高效检测大规模网络中的RS。

Result: 实验表明，高评分节点的跨越能力与基准算法相当，但更稳健；GPU实现速度比传统方法快244倍。

Conclusion: RS节点及其高效识别算法为动态网络中的社区间通信提供了有效解决方案。

Abstract: Social networks, characterized by community structures, often rely on nodes
called structural hole spanners to facilitate inter-community information
dissemination. However, the dynamic nature of these networks, where spanner
nodes may be removed, necessitates resilient methods to maintain
inter-community communication. To this end, we introduce robust spanners (RS)
as nodes uniquely equipped to bridge communities despite disruptions, such as
node or edge removals. We propose a novel scoring technique to identify RS
nodes and present a parallel algorithm with a CUDA implementation for efficient
RS detection in large networks. Empirical analysis of real-world social
networks reveals that high-scoring nodes exhibit a spanning capacity comparable
to those identified by benchmark spanner detection algorithms while offering
superior robustness. Our implementation on Nvidia GPUs achieves an average
speedup of 244X over traditional spanner detection techniques, demonstrating
its efficacy to identify RS in large social networks.

</details>


### [217] [Star Network Motifs on X during COVID-19](https://arxiv.org/abs/2508.00975)
*Lynnette Hui Xian Ng,Divyaansh Sinha,Kathleen M. Carley*

Main category: cs.SI

TL;DR: 研究COVID-19期间X平台上星形网络模因的表现，对比机器人和人类用户的行为模式。


<details>
  <summary>Details</summary>
Motivation: 分析社交网络中星形模因的重复出现模式，以揭示社交媒体的基本行为特征。

Method: 研究X平台上COVID-19讨论中的星形网络模因，分为机器人和人类用户作为核心或外围的六种模式。

Result: 识别出六种星形模因模式，展示了它们如何为社交媒体行为分析提供信息。

Conclusion: 星形网络模因模式有助于理解机器人和人类用户在社交媒体中的行为差异。

Abstract: Social network motifs are recurring patterns of small subgraphs that indicate
fundamental patterns of social communication. In this work, we study the simple
star network motifs that recur on X during the COVID-19 discourse. We study the
profile of the manifestation of the star network among bot and human users.
There are six primary patterns of the star motif, differentiating by the bots
and humans being either egos and alters. We describe the presentation of each
of these six patterns in our data, demonstrating how the motif patterns can
inform social media behavioral analysis.

</details>
