<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 9]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.NE](#cs.NE) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 本文评估了LLMs在真实世界问题代码生成中的进展，重点关注微服务架构。提出了难度评分标准，并开发了自动化测试框架。结果显示高级LLMs在中等难度任务表现良好，但在高难度任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在生成真实世界问题代码方面的能力，尤其是微服务架构的应用。

Method: 定义了微服务应用的标准模板，提出难度评分标准，并开发自动化测试框架。

Result: 高级LLMs在中等难度任务中表现良好，但在高难度任务中表现较差。

Conclusion: LLMs在复杂业务逻辑和外部服务集成等方面仍有挑战，需进一步研究改进。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出了一种面向效率的强化学习框架，通过动态探索和两阶段调优方法，显著提升了代码生成模型的运行效率和正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码生成方面取得了显著进展，但生成的代码运行时效率较差，限制了其在性能敏感场景中的应用。

Method: 采用动态探索克服静态数据限制，结合误差不敏感的强化学习方法和高效信号，提出两阶段调优方法。

Result: 实验结果显示，方法的有效性使得代码正确性提升了10.18%，运行时效率提升了7.75%，性能接近更大的模型。

Conclusion: 通过动态探索和两阶段调优方法，成功在保持代码正确性的同时提升了运行效率，解决了代码生成模型的实际应用问题。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一个基于LLM的SMT求解器模糊测试框架，通过生成可重用的逻辑表达式生成器来解决现有技术中的语法无效和计算开销问题，显著提升了测试效果。


<details>
  <summary>Details</summary>
Motivation: SMT求解器在现代系统和编程语言研究中至关重要，其正确性依赖于高质量的测试公式。然而，现有测试技术无法跟上求解器快速发展的新功能，LLM为基础的测试方法虽有望但存在语法无效和计算开销大的问题。

Method: Chimera通过LLM从文档中提取上下文无关文法（CFG）并合成可组合的布尔项生成器，利用这些生成器填充现有公式的结构骨架，确保语法有效性并提高语义多样性。

Result: 在Z3和cvc5两个主流SMT求解器上，Chimera发现了43个已确认的bug，其中40个已被开发者修复。

Conclusion: Chimera通过一次性LLM交互大幅降低了运行时开销，同时解决了语法无效和语义多样性问题，为SMT求解器的测试提供了高效的新方法。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: RCLAgent是一种基于多智能体递归思维框架的自适应根因定位方法，在微服务系统中表现出色，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统日益复杂且故障频发，现有根因定位方法依赖预定义模式或缺乏可解释性，SRE的实际分析过程启发研究了RCLAgent的设计。

Method: RCLAgent基于递归思维策略和多智能体协作，结合LLM推理和多模态数据分析，精准定位根因。

Result: 在多个公开数据集上，RCLAgent仅需单次请求即可实现优于依赖多请求聚合的现有方法的性能。

Conclusion: RCLAgent通过递归思维和多智能体框架，显著提升了复杂微服务环境中根因定位的效率和精度。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: 总结XP2025研讨会关于AI与敏捷开发的关键发现，提出多主题研究路线图。


<details>
  <summary>Details</summary>
Motivation: 解决生成式人工智能与敏捷软件开发结合时的挑战和机遇。

Method: 通过互动分组讨论分析痛点及其原因，共创研究路线图。

Result: 识别了工具碎片化、治理、数据质量等痛点，并制定短期和长期研究计划。

Conclusion: 提出推动GenAI负责任且以人为本地融入敏捷实践的研究方向。

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 该论文将大语言模型(LLM)应用分解为三层架构，分析了传统软件测试方法在各层的适用性，并提出四种协作策略和一个闭环质量保障框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的非确定性、动态性和上下文依赖性对质量保障提出了挑战，需要新的测试方法。

Method: 将LLM应用分解为三层架构，分析测试方法适用性，并提出协作策略和测试协议AICL。

Result: 提出了四种协作策略、闭环质量保障框架以及测试协议AICL。

Conclusion: 论文为LLM应用测试提供了实用指导和标准化协议，支持工具化和协作测试。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: 研究表明，LLMs能够从法律文本中生成高质量的Gherkin规范，显著减少人工工作量且生成的规范具备实用性。


<details>
  <summary>Details</summary>
Motivation: 法律文本的技术中立特性为工程师生成合规性文档带来挑战，手动操作效率低且易错，而GenAI技术（如LLMs）提供了自动化解决方案。

Method: 采用准实验设计，10名参与者评估了Claude和Llama从食品安全法规生成的60条Gherkin规范，每条由两人评估，共120次评价。

Result: 生成的规范在相关性、清晰度、完整性、单一性和时间节省方面表现优异，Claude和Llama各有优势，但无显著差异。

Conclusion: LLMs能有效将法律需求转化为开发者友好的规范，支持实现、验证和测试生成。

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 论文提出了一种可持续性视角（sustainability perspective vision），旨在为软件设计阶段提供结构化的指导，以有效解决可持续性问题。


<details>
  <summary>Details</summary>
Motivation: 软件密集型系统中的可持续性日益受到重视，但架构师在设计阶段缺乏结构化的指导。

Method: 通过文献综述（雪球法）和专家焦点小组研究，提出并验证了可持续性视角的构成元素。

Result: 研究发现视角元素在实践中具有相关性，并提出了满足工业需求的可持续性视角的构建方向。

Conclusion: 可持续性视角为软件架构师提供了解决可持续性问题的有效工具，并具有独立于框架和行业背景的适用性。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 论文提出了一种基于断言的测试预言方法，通过逻辑和算术谓词来减少CPS测试的成本和模拟器的不稳定性，并证明使用遗传编程（GP）与Ochiai方法的组合在准确性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 由于CPS模拟器的执行成本高且可能出现不稳定行为，需要一种无需执行系统就能提供可靠测试结果的自动化测试预言方法。

Method: 提出了基于断言的测试预言，包括两种生成方法：一种使用遗传编程（GP）结合Ochiai、Tarantula和Naish等故障定位公式；另一种使用决策树（DT）和决策规则（DR）。

Result: 实验表明，GP与Ochiai组合生成的测试预言在准确性和对不稳定性的鲁棒性上均优于其他方法。

Conclusion: 基于GP与Ochiai的测试预言是解决CPS测试高成本和模拟器不稳定性的有效方法。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 本文提出了一种结合预训练模型和异构图神经网络（GNN）的新方法，用于并发漏洞检测和定位，通过构建专用数据集和改进语义表示，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 并发漏洞在多线程或分布式系统中难以检测，现有深度学习方法存在数据集不足、语义表示不充分和缺乏细粒度调试信息等问题。

Method: 构建并发漏洞专用数据集，结合预训练模型和GNN，设计并发感知代码属性图（CCPG）表示语义，并利用SubgraphX进行漏洞定位。

Result: 相比现有方法，平均准确率和精度提升10%，召回率提升26%。

Conclusion: 该方法有效解决了并发漏洞检测的三大限制，显著提升了检测效果和调试便利性。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了一种通过配置日志增强软件诊断能力的方法，开发了工具ConfLogger，结合静态污点分析和LLM生成日志，显著提升了诊断效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统虽灵活但易引发配置问题，现有诊断方法未能确保软件提供足够的故障信息，需改进。

Method: ConfLogger通过配置敏感的静态污点分析和LLM日志生成，追踪配置相关数据流并生成诊断日志。

Result: 在8个系统中验证，ConfLogger显著提升诊断准确性（100%定位错误）和覆盖率（74%），优于基线方法。

Conclusion: ConfLogger有效增强配置诊断能力，提升调试效率和准确性，具有实用价值。

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 这篇论文探讨了软件工程领域中的性别偏见，通过历史背景分析和定量研究，揭示了该领域在性别包容性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程领域中的性别偏见问题，揭示其历史根源和现状，以推动领域的包容性发展。

Method: 结合历史背景分析、领导人物档案调查，以及对国际软件工程会议作者性别的定量分析（1976-2010）。

Result: 发现软件工程领域存在多年显著的性别排斥现象，尤其在研究作者构成中表现明显。

Conclusion: 建议从政策层面研究并解决计算领域的性别偏见问题，推动领域的多样性与包容性。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 该论文介绍了可解元组模式（STPs），用于表达类似列表的递归数据结构的不变式，无需负样本即可高效推断STPs，并将其融入支持列表类数据结构的CHC求解器中，显著提升了自动化程序验证能力。


<details>
  <summary>Details</summary>
Motivation: 递归数据结构的自动化验证是一个挑战，作者希望通过STPs高效表达和推断不变式，以改进自动化验证工具。

Method: 提出STPs概念，设计推断算法，并集成到支持序列理论的CHC求解器中。

Result: 集成STP推断的CHC求解器在CHC-COMP 2025的ADT-LIN类别中大幅领先获胜。

Conclusion: STPs为自动化验证递归数据结构提供了一种高效方法，显著提升了CHC求解器的性能。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [14] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 研究了概率程序与贝叶斯网络之间的图形表示关系，提出了新的静态分析方法和优化技术。


<details>
  <summary>Details</summary>
Motivation: 探讨概率程序（如Gen、Turing、Pyro中的特性）是否能够以图形方式表示，并解决反向映射的开放性问题。

Method: 扩展操作语义，将程序转换为控制流图，定义静态分析以近似随机变量的依赖结构，并开发程序切片技术。

Result: 提出了一种新的图形表示方法，针对有循环或动态标签的程序，并实现了三种优化技术，显著提升性能。

Conclusion: 研究成果为概率程序的静态分析和优化提供了理论基础，并在实践中优于现有技术。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [The Unwritten Contract of Cloud-based Elastic Solid-State Drives](https://arxiv.org/abs/2508.17372)
*Yingjia Wang,Ming-Chang Yang*

Main category: cs.PF

TL;DR: 论文首次对AWS和阿里云的ESSD性能进行了分析，揭示了与传统SSD不同的特性，并提出了四条观察和五条建议，以优化云存储设计。


<details>
  <summary>Details</summary>
Motivation: 随着云服务的普及，EBS（弹性块存储）使用ESSD（弹性固态硬盘）替代本地SSD的存储功能，但其性能是否可媲美本地SSD尚未有全面研究，因此需要深入分析。

Method: 通过分析AWS和阿里云的两种ESSD，总结其特征，并提出观察和设计建议。

Result: 研究发现ESSD的性能特征与传统SSD不同，提出了四条反直觉的观察和五条优化建议。

Conclusion: 研究为云存储用户提供了ESSD的性能特征和建议，帮助优化云软件设计以提升系统性能。

Abstract: Elastic block storage (EBS) with the storage-compute disaggregated
architecture stands as a pivotal piece in today's cloud. EBS furnishes users
with storage capabilities through the elastic solid-state drive (ESSD).
Nevertheless, despite the widespread integration into cloud services, the
absence of a thorough ESSD performance characterization raises critical doubt:
when more and more services are shifted onto the cloud, can ESSD satisfactorily
substitute the storage responsibilities of the local SSD and offer comparable
performance?
  In this paper, we for the first time target this question by characterizing
two ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract
of cloud-based ESSDs, encapsulating four observations and five implications for
cloud storage users. Specifically, the observations are counter-intuitive and
contrary to the conventional perceptions of what one would expect from the
local SSD. The implications we hope could guide users in revisiting the designs
of their deployed cloud software, i.e., harnessing the distinct characteristics
of ESSDs for better system performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era](https://arxiv.org/abs/2508.20205)
*Md. Emadul Haque,Faisal Tariq,Muhammad R A Khandaker,Md. Sakir Hossain,Muhammad Ali Imran,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 本文综述了5G系统中超可靠低延迟通信（URLLC）的方法，讨论了从物理层到跨层技术的分层策略，并展望了6G的挑战与前景。


<details>
  <summary>Details</summary>
Motivation: 无线通信正从以人为中心转向以机器为中心，对速率、延迟和可靠性的要求急剧变化，促使URLLC成为5G和6G的核心主题。

Method: 文章采用了分层分析方法，详细探讨了物理层、MAC层及跨层技术，并涵盖了5G及未来垂直领域的设计考量。

Result: 综述了URLLC的历史演变和技术进展，提出了满足高可靠性（99.999%）和低延迟（1毫秒）目标的研究方向。

Conclusion: 文章总结了URLLC的挑战与未来展望，特别关注了6G时代的潜在发展方向。

Abstract: As the wireless communication paradigm is being transformed from human
centered communication services towards machine centered communication
services, the requirements of rate, latency and reliability for these services
have also been transformed drastically. Thus the concept of Ultra Reliable and
Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G
systems. Though the latency and reliability requirement varies from one use
case to another, URLLC services generally aim to achieve very high reliability
in the range of 99.999\% while ensuring the latency of up to 1 ms. These two
targets are however inherently opposed to one another. Significant amounts of
work have been carried out to meet these ambitious but conflicting targets. In
this article a comprehensive survey of the URLLC approaches in 5G systems are
analysed in detail. Effort has been made to trace the history and evolution of
latency and reliability issues in wireless communication. A layered approach is
taken where physical layer, Medium Access Control (MAC) layer as well as cross
layer techniques are discussed in detail. It also covers the design
consideration for various 5G and beyond verticals. Finally the article
concludes by providing a detailed discussion on challenges and future outlook
with particular focus on the emerging 6G paradigm.

</details>


### [17] [DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)](https://arxiv.org/abs/2508.20272)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: DRR-MDPF是一种结合MDPF模型和DRR算法的混合策略，显著提升NDN网络性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为解决NDN在网络动态和高流量条件下的队列和资源管理问题，提出智能自适应方案。

Method: 采用MDPF模型预测最优转发决策，结合DRR算法公平分配带宽。

Result: 仿真表明，DRR-MDPF在吞吐量、ISR等指标上优于现有策略，且适应性强。

Conclusion: DRR-MDPF为NDN提供了智能、可扩展的队列管理方案，解决资源分配和拥塞控制问题。

Abstract: Named Data Networking (NDN) represents a transformative shift in network
architecture, prioritizing content names over host addresses to enhance data
dissemination. Efficient queue and resource management are critical to NDN
performance, especially under dynamic and high-traffic conditions. This paper
introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov
Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)
algorithm. MDPF enables routers to intelligently predict optimal forwarding
decisions based on key metrics such as bandwidth, delay, and the number of
unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation
among competing data flows. The proposed method models each router as a
learning agent capable of adjusting its strategies through continuous feedback
and probabilistic updates. Simulation results using ndnSIM demonstrate that
DRR-MDPF significantly outperforms state-of-the-art strategies including SAF,
RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest
Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load
balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and
heavy traffic, offering enhanced adaptability and lower computational
complexity due to its single-path routing design. Furthermore, its multi-metric
decision-making capability enables more accurate interface selection, leading
to optimized network performance. Overall, DRR-MDPF serves as an intelligent,
adaptive, and scalable queue management solution for NDN, effectively
addressing core challenges such as resource allocation, congestion control, and
route optimization in dynamic networking environments.

</details>


### [18] [Relay Selection in Wireless Networks as Restless Bandits](https://arxiv.org/abs/2508.20625)
*Mandar R. Nalavade,Ravindra S. Tomar,Gaurav S. Kasbekar*

Main category: cs.NI

TL;DR: 论文研究如何通过Whittle索引策略优化无线网络中继选择，以最小化长期平均包持有成本。


<details>
  <summary>Details</summary>
Motivation: 无线网络中源节点与目的节点间的直接链路被阻塞，需通过中继节点转发数据包，但中继节点的包持有成本较高，需优化选择策略以降低成本。

Method: 将问题建模为多臂老虎机（RMAB）问题，证明其具备Whittle索引性，并提出一种计算Whittle索引的方法，选择索引最小的中继节点转发数据包。

Result: 仿真表明，所提策略在平均成本、时延和吞吐量方面优于现有中继选择策略。

Conclusion: Whittle索引策略有效优化了中继选择，为无线网络中的低成本数据传输提供新解决方案。

Abstract: We consider a wireless network in which a source node needs to transmit a
large file to a destination node. The direct wireless link between the source
and the destination is assumed to be blocked. Multiple candidate relays are
available to forward packets from the source to the destination. A holding cost
is incurred for each packet stored at every relay in each time slot. The
objective is to design a policy for selecting a relay in each time slot to
which the source attempts to send a packet, so as to minimize the expected
long-run time-averaged total packet holding cost at the relays. This problem is
an instance of the restless multi-armed bandit (RMAB) problem, which is
provably hard to solve. We prove that this relay selection problem is
Whittle-indexable, and propose a method to compute the Whittle index of each
relay in every time slot. In each time slot, our relay selection policy
transmits a packet to the relay with the smallest Whittle index. Using
simulations, we show that the proposed policy outperforms the relay selection
policies proposed in prior work in terms of average cost, delay, as well as
throughput.

</details>


### [19] [Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks](https://arxiv.org/abs/2508.20957)
*Faisal Ahmed,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 提出了一种基于数字孪生和深度强化学习的智能VNF迁移框架，旨在联合优化端到端延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现代边缘-核心网络基础设施中，虚拟化网络功能（VNF）的快速部署和服务需求增长对低延迟和高能效编排提出挑战。

Method: 将VNF迁移问题建模为马尔可夫决策过程，利用优势演员-评论家模型，并结合多任务变分自编码器和LSTM的数字孪生模块。

Result: 仿真结果表明，该框架显著降低了平均端到端延迟和能耗。

Conclusion: 该框架为边缘-核心网络中的智能VNF迁移设定了新基准。

Abstract: The growing demand for services and the rapid deployment of virtualized
network functions (VNFs) pose significant challenges for achieving low-latency
and energy-efficient orchestration in modern edge-core network infrastructures.
To address these challenges, this study proposes a Digital Twin (DT)-empowered
Deep Reinforcement Learning framework for intelligent VNF migration that
jointly minimizes average end-to-end (E2E) delay and energy consumption. By
formulating the VNF migration problem as a Markov Decision Process and
utilizing the Advantage Actor-Critic model, the proposed framework enables
adaptive and real-time migration decisions. A key innovation of the proposed
framework is the integration of a DT module composed of a multi-task
Variational Autoencoder and a multi-task Long Short-Term Memory network. This
combination collectively simulates environment dynamics and generates
high-quality synthetic experiences, significantly enhancing training efficiency
and accelerating policy convergence. Simulation results demonstrate substantial
performance gains, such as significant reductions in both average E2E delay and
energy consumption, thereby establishing new benchmarks for intelligent VNF
migration in edge-core networks.

</details>


### [20] [RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN](https://arxiv.org/abs/2508.20985)
*Douglas Liao,Jiping Luo,Jens Vevstad,Nikolaos Pappas*

Main category: cs.NI

TL;DR: RANGAN框架结合GAN和Transformer架构，用于RAN系统的异常检测，通过滑动窗口方法捕获时间依赖性，实验显示F1分数达83%。


<details>
  <summary>Details</summary>
Motivation: RAN系统复杂且数据量大，传统方法难以准确诊断性能异常，需自适应方法捕捉时间依赖性。

Method: 提出RANGAN框架，整合GAN和Transformer，采用滑动窗口预处理数据以增强时间依赖性捕获能力。

Result: 在公开数据集上验证，RANGAN对网络拥塞问题的检测F1分数高达83%。

Conclusion: RANGAN为RAN系统的异常检测提供了高效且可靠的方法。

Abstract: Radio Access Network (RAN) systems are inherently complex, requiring
continuous monitoring to prevent performance degradation and ensure optimal
user experience. The RAN leverages numerous key performance indicators (KPIs)
to evaluate system performance, generating vast amounts of data each second.
This immense data volume can make troubleshooting and accurate diagnosis of
performance anomalies more difficult. Furthermore, the highly dynamic nature of
RAN performance demands adaptive methodologies capable of capturing temporal
dependencies to detect anomalies reliably. In response to these challenges, we
introduce \textbf{RANGAN}, an anomaly detection framework that integrates a
Generative Adversarial Network (GAN) with a transformer architecture. To
enhance the capability of capturing temporal dependencies within the data,
RANGAN employs a sliding window approach during data preprocessing. We
rigorously evaluated RANGAN using the publicly available RAN performance
dataset from the Spotlight project \cite{sun-2024}. Experimental results
demonstrate that RANGAN achieves promising detection accuracy, notably
attaining an F1-score of up to $83\%$ in identifying network contention issues.

</details>


### [21] [DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks](https://arxiv.org/abs/2508.21047)
*Dhiraj Bhattacharjee,Pablo G. Madoery,Abhishek Naik,Halim Yanikomeroglu,Gunes Karabulut Kurt,Stephane Martel,Khaled Ahmed*

Main category: cs.NI

TL;DR: 论文提出了一种结合路由和带宽分配的优化方法DSROQ，利用MCTS算法解决NP难题，并通过实验证明其在提升用户体验和公平性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现代互联网应用对QoS要求多样化，LEO卫星网络在覆盖和补充地面网络方面具有潜力，但需优化路由和带宽分配以满足QoS需求。

Method: 采用MCTS启发的方法解决路由与带宽分配的NP难题，并结合Lyapunov优化进行调度，提出DSROQ算法。

Result: 实验结果显示DSROQ在性能和公平性上优于基准方案，且在不同流量敏感情况下，路由和带宽分配成为主要性能影响因素。

Conclusion: DSROQ算法通过联合优化路由和带宽分配，显著提升用户体验和网络公平性，验证了联合决策的优势。

Abstract: The modern Internet supports diverse applications with heterogeneous quality
of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations
offer a promising solution to meet these needs, enhancing coverage in rural
areas and complementing terrestrial networks in urban regions. Ensuring QoS in
such networks requires joint optimization of routing, bandwidth allocation, and
dynamic queue scheduling, as traffic handling is critical for maintaining
service performance. This paper formulates a joint routing and bandwidth
allocation problem where QoS requirements are treated as soft constraints,
aiming to maximize user experience. An adaptive scheduling approach is
introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree
search (MCTS)-inspired method to solve the NP-hard route and bandwidth
allocation problem, with Lyapunov optimization-based scheduling applied during
reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we
compare end-user experience and fairness between our proposed DSROQ algorithm
and a benchmark scheme. Results show that DSROQ improves both performance
metrics and demonstrates the advantage of joint routing and bandwidth
decisions. Furthermore, we observe that the dominant performance factor shifts
from scheduling to routing and bandwidth allocation as traffic sensitivity
changes from latency-driven to bandwidth-driven.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [MM-HSD: Multi-Modal Hate Speech Detection in Videos](https://arxiv.org/abs/2508.20546)
*Berta Céspedes-Sarrias,Carlos Collado-Capell,Pablo Rodenas-Ruiz,Olena Hrynenko,Andrea Cavallaro*

Main category: cs.MM

TL;DR: 本文提出了一种多模态仇恨言论检测模型MM-HSD，结合视频帧、音频、文本及跨模态注意力（CMA）特征，首次在视频HSD中使用CMA作为早期特征提取器，并在HateMM数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频仇恨言论检测方法多模态融合不足，且忽略某些相关模态（如屏幕文本和音频），而简单融合方法无法充分捕捉模态间依赖关系。

Method: 提出MM-HSD模型，整合视频帧、音频、转录文本和屏幕文本，通过跨模态注意力（CMA）提取特征，并首次系统比较查询/键配置及模态交互。

Result: 在HateMM数据集上，MM-HSD的M-F1得分达到0.874，优于现有方法，尤其当屏幕文本作为查询时表现最佳。

Conclusion: 多模态融合与CMA特征提取能有效提升视频仇恨言论检测性能，屏幕文本作为查询模态尤为关键。

Abstract: While hate speech detection (HSD) has been extensively studied in text,
existing multi-modal approaches remain limited, particularly in videos. As
modalities are not always individually informative, simple fusion methods fail
to fully capture inter-modal dependencies. Moreover, previous work often omits
relevant modalities such as on-screen text and audio, which may contain subtle
hateful content and thus provide essential cues, both individually and in
combination with others. In this paper, we present MM-HSD, a multi-modal model
for HSD in videos that integrates video frames, audio, and text derived from
speech transcripts and from frames (i.e.~on-screen text) together with features
extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an
early feature extractor for HSD in videos, to systematically compare query/key
configurations, and to evaluate the interactions between different modalities
in the CMA block. Our approach leads to improved performance when on-screen
text is used as a query and the rest of the modalities serve as a key.
Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art
methods on M-F1 score (0.874), using concatenation of transcript, audio, video,
on-screen text, and CMA for feature extraction on raw embeddings of the
modalities. The code is available at https://github.com/idiap/mm-hsd

</details>


### [23] [diveXplore at the Video Browser Showdown 2024](https://arxiv.org/abs/2508.20560)
*Klaus Schoeffmann,Sahar Nasirihaghighi*

Main category: cs.MM

TL;DR: 对VBS2023的经验和反馈进行了系统改进，推出了diveXplore系统VBS2024版本，整合了OpenCLIP等多种功能。


<details>
  <summary>Details</summary>
Motivation: 基于VBS2023的经验和CBMI2023会议上IVR4B特别环节的反馈，旨在改进视频搜索与浏览系统。

Method: 整合OpenCLIP用于图像/文本嵌入，优化查询服务器和用户界面，新增探索视图功能。

Result: 改进了系统的自由文本和视觉相似性搜索能力，提升了用户浏览体验和查询效率。

Conclusion: diveXplore系统在VBS2024中得到显著改进，功能更强大且用户友好。

Abstract: According to our experience from VBS2023 and the feedback from the IVR4B
special session at CBMI2023, we have largely revised the diveXplore system for
VBS2024. It now integrates OpenCLIP trained on the LAION-2B dataset for
image/text embeddings that are used for free-text and visual similarity search,
a query server that is able to distribute different queries and merge the
results, a user interface optimized for fast browsing, as well as an
exploration view for large clusters of similar videos (e.g., weddings,
paraglider events, snow and ice scenery, etc.).

</details>


### [24] [Less is More - diveXplore 5.0 at VBS 2021](https://arxiv.org/abs/2508.20569)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore系统在VBS和LSC挑战赛中表现下滑，因其核心组件与新功能不兼容，5.0版本重构以简化复杂度并保留有用功能。


<details>
  <summary>Details</summary>
Motivation: 系统性能因新增功能而下降，需重构以提升性能。

Method: 从零开始重建系统，采用模块化设计。

Result: 发布5.0版本，简化复杂度并保留核心功能。

Conclusion: 重构解决了性能问题，提升了系统灵活性。

Abstract: As a longstanding participating system in the annual Video Browser Showdown
(VBS2017-VBS2020) as well as in two iterations of the more recently established
Lifelog Search Challenge (LSC2018-LSC2019), diveXplore is developed as a
feature-rich Deep Interactive Video Exploration system. After its initial
successful employment as a competitive tool at the challenges, its performance,
however, declined as new features were introduced increasing its overall
complexity. We mainly attribute this to the fact that many additions to the
system needed to revolve around the system's core element - an interactive
self-organizing browseable featuremap, which, as an integral component did not
accommodate the addition of new features well. Therefore, counteracting said
performance decline, the VBS 2021 version constitutes a completely rebuilt
version 5.0, implemented from scratch with the aim of greatly reducing the
system's complexity as well as keeping proven useful features in a modular
manner.

</details>


### [25] [diveXplore 6.0: ITEC's Interactive Video Exploration System at VBS 2022](https://arxiv.org/abs/2508.20687)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore系统从VBS2017到VBS2021经历了重构，版本5.0减少了功能但更现代化、轻量和快速，性能提升。版本6.0重新设计了镜头分割和地图搜索，并引入了新功能以改进概念和时间上下文搜索。


<details>
  <summary>Details</summary>
Motivation: 通过系统重构和新功能引入，提升交互式搜索系统的性能和现代性。

Method: 系统经历了重构，减少了功能但更现代化和快速；版本6.0引入了新的镜头分割、地图搜索和改进的概念与时间上下文搜索功能。

Result: 新系统在VBS2021中表现出性能提升。

Conclusion: 精简和现代化的重构以及新功能的引入提升了系统性能。

Abstract: Continuously participating since the sixth Video Browser Showdown (VBS2017),
diveXplore is a veteran interactive search system that throughout its lifetime
has offered and evaluated numerous features. After undergoing major refactoring
for the most recent VBS2021, however, the system since version 5.0 is less
feature rich, yet, more modern, leaner and faster than the original system.
This proved to be a sensible decision as the new system showed increasing
performance in VBS2021 when compared to the most recent former competitions.
With version 6.0 we reconsider shot segmentation, map search and introduce new
features for improving concept as well as temporal context search.

</details>


### [26] [AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression](https://arxiv.org/abs/2508.20741)
*Chenhao Zhang,Wei Gao*

Main category: cs.MM

TL;DR: 该论文提出了一种新型动态点云压缩框架，支持可变比特率和计算复杂度，通过多路径编码和粗到精运动估计优化效率，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态点云压缩方法在复杂度管理和比特率控制方面的挑战。

Method: 采用多路径编码的可扩展框架，结合粗到精运动估计模块和内容自适应比特率控制模块。

Result: 相比现有技术，BD-Rate平均降低5.81%，BD-PSNR提升0.42dB，编码时间减少44.6%。

Conclusion: 该方法在实时和比特率受限场景中表现出高效性，具有实际应用潜力。

Abstract: Dynamic point cloud compression (DPCC) is crucial in applications like
autonomous driving and AR/VR. Current compression methods face challenges with
complexity management and rate control. This paper introduces a novel dynamic
coding framework that supports variable bitrate and computational complexities.
Our approach includes a slimmable framework with multiple coding routes,
allowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within a
single model. To address data sparsity in inter-frame prediction, we propose
the coarse-to-fine motion estimation and compensation module that deconstructs
geometric information while expanding the perceptive field. Additionally, we
propose a precise rate control module that content-adaptively navigates point
cloud frames through various coding routes to meet target bitrates. The
experimental results demonstrate that our approach reduces the average BD-Rate
by 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-art
method, while keeping the average bitrate error at 0.40%. Moreover, the average
coding time is reduced by up to 44.6% compared to D-DPCC, underscoring its
efficiency in real-time and bitrate-constrained DPCC scenarios. Our code is
available at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [27] [Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL](https://arxiv.org/abs/2508.20738)
*Lukas Bartl,Jasmin Blanchette,Tobias Nipkow*

Main category: cs.LO

TL;DR: Metis是Isabelle/HOL证明助手内置的有序参数化证明器，通过分析成功的Metis证明推导变量实例化，提升Sledgehammer的成功率、速度和用户理解。


<details>
  <summary>Details</summary>
Motivation: 提升Sledgehammer工具的证明成功率、生成速度，并帮助用户理解证明过程。

Method: 分析成功的Metis证明，从中提取变量实例化信息。

Result: 提高了Sledgehammer的成功率和证明速度，增强了用户对证明的理解。

Conclusion: 通过分析Metis证明生成的变量实例化，显著优化了Sledgehammer的性能和用户体验。

Abstract: Metis is an ordered paramodulation prover built into the Isabelle/HOL proof
assistant. It attempts to close the current goal using a given list of lemmas.
Typically these lemmas are found by Sledgehammer, a tool that integrates
external automatic provers. We present a new tool that analyzes successful
Metis proofs to derive variable instantiations. These increase Sledgehammer's
success rate, improve the speed of Sledgehammer-generated proofs, and help
users understand why a goal follows from the lemmas.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [28] [Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM](https://arxiv.org/abs/2508.20263)
*Jazbo Beason,Ruijia Cheng,Eldon Schoop,Jeffrey Nichols*

Main category: cs.HC

TL;DR: Athena是一个原型应用生成环境，通过共享中间表示（如应用故事板、数据模型和GUI骨架）帮助开发者迭代生成完整用户界面，并优化LLM的代码生成。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM生成复杂用户界面代码时存在挑战，例如需要详细提示且生成结果通常是单一难理解的大文件。

Method: 引入共享中间表示（应用故事板、数据模型和GUI骨架），以迭代方式与LLM协作生成结构化的多文件代码。

Result: 用户研究表明，75%的参与者更倾向于使用Athena而非传统聊天机器人基线进行应用原型开发。

Conclusion: Athena通过中间表示有效提升了生成用户界面代码的结构化和可维护性，成为LLM辅助开发的有力工具。

Abstract: It is challenging to generate the code for a complete user interface using a
Large Language Model (LLM). User interfaces are complex and their
implementations often consist of multiple, inter-related files that together
specify the contents of each screen, the navigation flows between the screens,
and the data model used throughout the application. It is challenging to craft
a single prompt for an LLM that contains enough detail to generate a complete
user interface, and even then the result is frequently a single large and
difficult to understand file that contains all of the generated screens. In
this paper, we introduce Athena, a prototype application generation environment
that demonstrates how the use of shared intermediate representations, including
an app storyboard, data model, and GUI skeletons, can help a developer work
with an LLM in an iterative fashion to craft a complete user interface. These
intermediate representations also scaffold the LLM's code generation process,
producing organized and structured code in multiple files while limiting
errors. We evaluated Athena with a user study that found 75% of participants
preferred our prototype over a typical chatbot-style baseline for prototyping
apps.

</details>


### [29] [Identifying Framing Practices in Visualization Design Through Practitioner Reflections](https://arxiv.org/abs/2508.20383)
*Prakash Shukla,Paul Parsons*

Main category: cs.HC

TL;DR: 可视化设计中框架化的重要性及其在设计过程中的作用研究。


<details>
  <summary>Details</summary>
Motivation: 当前可视化研究主要关注框架化对观众的修辞和感知影响，忽视了其在设计过程中的作用。本研究旨在填补这一空白。

Method: 分析80多位专业可视化设计师在公开播客和书籍章节中对其工作的反思。

Result: 发现框架化是设计过程中的核心活动，涉及问题界定、数据解释、利益相关者目标对齐和叙事方向塑造。

Conclusion: 框架化是可视化实践的核心维度，需加强研究和教育以支持设计师在设计过程中的战略判断。

Abstract: Framing -- how designers define and reinterpret problems, shape narratives,
and guide audience understanding -- is central to design practice. Yet in
visualization research, framing has been examined mostly through its rhetorical
and perceptual effects on audiences, leaving its role in the design process
underexplored. This study addresses that gap by analyzing publicly available
podcasts and book chapters in which over 80 professional visualization
designers reflect on their work. We find that framing is a pervasive, iterative
activity, evident in scoping problems, interpreting data, aligning with
stakeholder goals, and shaping narrative direction. Our analysis identifies the
conditions that trigger reframing and the strategies practitioners use to
navigate uncertainty and guide design. These findings position framing as a
core dimension of visualization practice and underscore the need for research
and education to support the interpretive and strategic judgment that
practitioners exercise throughout the design process.

</details>


### [30] [Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions](https://arxiv.org/abs/2508.20464)
*Sanaz Motamedi,Viktoria Marcus,Griffin Pitts*

Main category: cs.HC

TL;DR: 论文研究了自动驾驶系统（ADS）如何通过外部因素（如安全、信任等）影响行人决策，为设计人机界面提供依据。


<details>
  <summary>Details</summary>
Motivation: 旨在解决自动驾驶系统与行人交互中的安全问题，减少因人为错误导致的交通事故。

Method: 扩展计划行为理论（TPB），引入四个外部因素，通过在线调查（n=212）分析行人决策行为。

Result: 感知行为控制、态度和社会信息显著预测行人过街意图，安全感和理解程度是关键影响因素。

Conclusion: 研究为设计安全透明的自动驾驶交互界面提供了理论基础，推动以人为本的智能交通系统发展。

Abstract: Road traffic remains a leading cause of death worldwide, with pedestrians and
other vulnerable road users accounting for over half of the 1.19 million annual
fatalities, much of it due to human error. Level-5 automated driving systems
(ADSs), capable of full self-driving without human oversight, have the
potential to reduce these incidents. However, their effectiveness depends not
only on automation performance but also on their ability to communicate intent
and coordinate safely with pedestrians in the absence of traditional driver
cues. Understanding how pedestrians interpret and respond to ADS behavior is
therefore critical to the development of connected vehicle systems. This study
extends the Theory of Planned Behavior (TPB) by incorporating four external
factors (i.e. safety, trust, compatibility, and understanding) to model
pedestrian decision-making in road-crossing scenarios involving level-5 ADSs.
Using data from an online survey (n = 212), results show that perceived
behavioral control, attitude, and social information significantly predict
pedestrians' crossing intentions. External factors, particularly perceived
safety and understanding, strongly influence these constructs. Findings provide
actionable insights for designing external human-machine interfaces (eHMIs) and
cooperative V2X communication strategies that support safe, transparent
interactions between automated vehicles and pedestrians. This work contributes
to the development of inclusive, human-centered connected mobility systems.

</details>


### [31] [What is "Spatial" about Spatial Computing?](https://arxiv.org/abs/2508.20477)
*Yibo Wang,Yuhan Luo,Janghee Cho,Junnan Yu*

Main category: cs.HC

TL;DR: 地理信息和混合现实技术的进步推动了空间计算的发展，但其概念碎片化阻碍了跨学科整合，本文通过历史回顾和概念分析提出了一种统一视角。


<details>
  <summary>Details</summary>
Motivation: 由于空间计算在不同学科中的解读不同，导致概念混乱和跨学科整合困难，研究旨在梳理其发展脉络并提出统一框架。

Method: 回溯空间计算的起源和历史演变，分析两种对‘空间’的理解学派：一是空间作为上下文，二是空间作为混合交互环境。

Result: 提出空间计算作为重新定义环境、计算与人类体验交互的计算范式，增强了概念清晰度并为未来技术提供指导。

Conclusion: 通过综合两种视角，空间计算为跨学科研究和创新提供了理论基础，推动了其在技术发展中的应用前景。

Abstract: Recent advancements in geographic information systems and mixed reality
technologies have positioned spatial computing as a transformative paradigm in
computational science. However, the field remains conceptually fragmented, with
diverse interpretations across disciplines like Human-Computer Interaction,
Geographic Information Science, and Computer Science, which hinders a
comprehensive understanding of spatial computing and poses challenges for its
coherent advancement and interdisciplinary integration. In this paper, we trace
the origins and historical evolution of spatial computing and examine how
"spatial" is understood, identifying two schools of thought: "spatial" as the
contextual understanding of space, where spatial data guides interaction in the
physical world; and "spatial" as a mixed space for interaction, emphasizing the
seamless integration of physical and digital environments to enable embodied
engagement. By synthesizing these perspectives, we propose spatial computing as
a computational paradigm that redefines the interplay between environment,
computation, and human experience, offering a holistic lens to enhance its
conceptual clarity and inspire future technological innovations that support
meaningful interactions with and shaping of environments.

</details>


### [32] [VisiTrail: A Cognitive Visualization Tool for Time-Series Analysis of Eye Tracking Data from Attention Game](https://arxiv.org/abs/2508.20522)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.HC

TL;DR: 论文提出了一种新型眼动追踪分析工具，用于研究视觉搜索任务中的注意力动态及其与任务表现的关系。


<details>
  <summary>Details</summary>
Motivation: 传统眼动分析方法在复杂视觉搜索场景中存在局限性，无法全面捕捉注意力分配的时空动态及其与任务表现的联系。

Method: 开发了一种综合工具，包括眼动数据的时间序列分析、时间模式分析、对象点击序列跟踪及性能指标量化。

Result: 该工具通过可视化技术揭示了注意力动态与用户行为的直接联系，并量化了任务表现的准确性和效率。

Conclusion: 提出的工具为理解复杂视觉搜索任务中的注意力动态提供了更全面的方法，有助于提升任务设计和用户表现分析的效率。

Abstract: Eye Tracking (ET) can help to understand visual attention and cognitive
processes in interactive environments. In attention tasks, distinguishing
between relevant target objects and distractors is crucial for effective
performance, yet the underlying gaze patterns that drive successful task
completion remain incompletely understood. Traditional gaze analyses lack
comprehensive insights into the temporal dynamics of attention allocation and
the relationship between gaze behavior and task performance. When applied to
complex visual search scenarios, current gaze analysis methods face several
limitations, including the isolation of measurements, visual stability, search
efficiency, and the decision-making processes involved in these scenarios. This
paper proposes an analysis tool that considers time series for eye tracking
data from task performance and also gaze measures (fixations, saccades and
smooth pursuit); temporal pattern analysis that reveals how attention evolves
throughout task performance; object-click sequence tracking that directly links
visual attention to user actions; and performance metrics that quantify both
accuracy and efficiency. This tool provides comprehensive visualization
techniques that make complex patterns of stimuli and gaze connections
interpretable.

</details>


### [33] [Persode: Personalized Visual Journaling with Episodic Memory-Aware AI Agent](https://arxiv.org/abs/2508.20585)
*Seokho Jin,Manseo Kim,Sungho Byun,Hansol Kim,Jungmin Lee,Sujeong Baek,Semi Kim,Sanghum Park,Sung Park*

Main category: cs.HC

TL;DR: Persode是一个面向Gen Alpha和Z的个性化日记系统，结合了视觉叙事和记忆感知对话，提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统的文字日记缺乏个性化和视觉吸引力，无法满足年轻一代对沉浸式、快速互动的需求。

Method: Persode通过个性化引导、RAG框架和自动视觉叙事，动态生成符合用户偏好的视觉内容。

Result: 系统能够提供情感丰富的对话和视觉叙事，满足年轻用户的需求。

Conclusion: Persode成功填补了传统日记与年轻一代偏好之间的差距。

Abstract: Reflective journaling often lacks personalization and fails to engage
Generation Alpha and Z, who prefer visually immersive and fast-paced
interactions over traditional text-heavy methods. Visual storytelling enhances
emotional recall and offers an engaging way to process personal expe- riences.
Designed with these digital-native generations in mind, this paper introduces
Persode, a journaling system that integrates personalized onboarding,
memory-aware conversational agents, and automated visual storytelling. Persode
captures user demographics and stylistic preferences through a tailored
onboarding process, ensuring outputs resonate with individual identities. Using
a Retrieval-Augmented Generation (RAG) framework, it prioritizes emotionally
significant memories to provide meaningful, context-rich interactions.
Additionally, Persode dynamically transforms user experiences into visually
engaging narratives by generating prompts for advanced text-to-image models,
adapting characters, backgrounds, and styles to user preferences. By addressing
the need for personalization, visual engagement, and responsiveness, Persode
bridges the gap between traditional journaling and the evolving preferences of
Gen Alpha and Z.

</details>


### [34] [Schema-Guided Response Generation using Multi-Frame Dialogue State for Motivational Interviewing Systems](https://arxiv.org/abs/2508.20635)
*Jie Zeng,Yukiko I. Nakano*

Main category: cs.HC

TL;DR: 本研究通过模式引导方法更新多帧对话状态，提出一种动态决定回答焦点的策略决策机制，以支持动机访谈（MI）原则。用户研究表明，该方法能生成符合MI的回应并有效促进用户的思考。


<details>
  <summary>Details</summary>
Motivation: 动机访谈（MI）旨在帮助客户激发行为改变的动机，本研究旨在指导大型语言模型生成符合MI原则的咨询师回应。

Method: 采用模式引导方法，提出多帧对话状态更新机制和动态响应焦点决策策略。

Result: 在用户研究中，系统成功生成符合MI原则的回应，并通过提问有效促进用户思考。

Conclusion: 该方法能有效支持MI原则在对话系统中的应用，提升用户的动机激发效果。

Abstract: The primary goal of Motivational Interviewing (MI) is to help clients build
their own motivation for behavioral change. To support this in dialogue
systems, it is essential to guide large language models (LLMs) to generate
counselor responses aligned with MI principles. By employing a schema-guided
approach, this study proposes a method for updating multi-frame dialogue states
and a strategy decision mechanism that dynamically determines the response
focus in a manner grounded in MI principles. The proposed method was
implemented in a dialogue system and evaluated through a user study. Results
showed that the proposed system successfully generated MI-favorable responses
and effectively encouraged the user's (client's) deliberation by asking
eliciting questions.

</details>


### [35] [Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop](https://arxiv.org/abs/2508.21036)
*Lev Tankelevitch,Elena L. Glassman,Jessica He,Aniket Kittur,Mina Lee,Srishti Palani,Advait Sarkar,Gonzalo Ramos,Yvonne Rogers,Hari Subramonyam*

Main category: cs.HC

TL;DR: 摘要探讨生成式AI（GenAI）对人类认知的潜在影响及其增强作用，强调需要新的理论、工具和方法来分析这一变革。CHI 2025研讨会的目标是连接科学研究与设计实践，促进多学科合作。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（GenAI）正在改变工作、教育和日常任务的自动化方式，这既带来风险也带来机遇。研究需要关注其对人类认知（如元认知、批判性思维等）的影响，并探索如何通过设计工具保护和增强人类思维。

Method: 研讨会汇集了56名跨领域研究人员和设计师，以及34篇论文和设计作品。通过讨论、构思和社区建设，探讨了GenAI对认知的影响及其设计机会。

Result: 研讨会成果为研究领域和设计实践提供了初步框架，并推动了多学科社区的建立。

Conclusion: 需要进一步研究GenAI对人类认知的影响，并开发相关工具和理论，以促进其安全和有益的应用。

Abstract: Generative AI (GenAI) radically expands the scope and capability of
automation for work, education, and everyday tasks, a transformation posing
both risks and opportunities for human cognition. How will human cognition
change, and what opportunities are there for GenAI to augment it? Which
theories, metrics, and other tools are needed to address these questions? The
CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of
how the use of GenAI affects human thought, from metacognition to critical
thinking, memory, and creativity, with an emerging design practice for building
GenAI tools that both protect and augment human thought. Fifty-six researchers,
designers, and thinkers from across disciplines as well as industry and
academia, along with 34 papers and portfolios, seeded a day of discussion,
ideation, and community-building. We synthesize this material here to begin
mapping the space of research and design opportunities and to catalyze a
multidisciplinary community around this pressing area of research.

</details>


### [36] [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)
*Adam Coscia,Shunan Guo,Eunyee Koh,Alex Endert*

Main category: cs.HC

TL;DR: OnGoal是一款帮助用户在多轮对话中更好地管理目标进度的LLM聊天界面，通过实时反馈和示例解释提高对话效率。


<details>
  <summary>Details</summary>
Motivation: 随着多轮对话的复杂性和长度增加，用户需要更好的工具来评估和跟踪对话目标的进展。

Method: OnGoal提供实时目标对齐反馈、评估结果的解释以及目标进展概览，用户可通过与基线聊天界面的对比研究进行评估。

Result: 使用OnGoal的用户在写作任务中花费更少时间和精力达成目标，并能探索新的提示策略以克服沟通障碍。

Conclusion: 目标跟踪和可视化可以增强LLM对话的参与度和韧性，未来设计应改进目标沟通、降低认知负荷并增强反馈。

Abstract: As multi-turn dialogues with large language models (LLMs) grow longer and
more complex, how can users better evaluate and review progress on their
conversational goals? We present OnGoal, an LLM chat interface that helps users
better manage goal progress. OnGoal provides real-time feedback on goal
alignment through LLM-assisted evaluation, explanations for evaluation results
with examples, and overviews of goal progression over time, enabling users to
navigate complex dialogues more effectively. Through a study with 20
participants on a writing task, we evaluate OnGoal against a baseline chat
interface without goal tracking. Using OnGoal, participants spent less time and
effort to achieve their goals while exploring new prompting strategies to
overcome miscommunication, suggesting tracking and visualizing goals can
enhance engagement and resilience in LLM dialogues. Our findings inspired
design implications for future LLM chat interfaces that improve goal
communication, reduce cognitive load, enhance interactivity, and enable
feedback to improve LLM performance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [37] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 该论文提出了一种名为'上下文混合(MoC)'的稀疏注意力路由模块，用于解决长视频生成中的长上下文记忆问题，通过动态选择信息块和锚点，实现了高效的长时记忆检索和视频生成。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临长上下文记忆的挑战，传统自注意力机制由于二次计算成本导致内存和计算难以处理长序列。论文旨在提出一种高效的长时记忆检索方法。

Method: 采用了上下文混合(MoC)模块，动态选择信息块和锚点进行注意力路由，避免循环闭合，从而提高计算效率和记忆能力。

Result: 在逐步增加数据和稀疏化路由的情况下，模型能够高效分配计算资源，保留身份、动作和场景信息，生成分钟级别的视频内容。

Conclusion: MoC模块通过稀疏注意力路由实现了长时记忆检索和高效计算，为长视频生成提供了一种可行的解决方案。

Abstract: Long video generation is fundamentally a long context memory problem: models
must retain and retrieve salient events across a long range without collapsing
or drifting. However, scaling diffusion transformers to generate long-context
videos is fundamentally limited by the quadratic cost of self-attention, which
makes memory and computation intractable and difficult to optimize for long
sequences. We recast long-context video generation as an internal information
retrieval task and propose a simple, learnable sparse attention routing module,
Mixture of Contexts (MoC), as an effective long-term memory retrieval engine.
In MoC, each query dynamically selects a few informative chunks plus mandatory
anchors (caption, local windows) to attend to, with causal routing that
prevents loop closures. As we scale the data and gradually sparsify the
routing, the model allocates compute to salient history, preserving identities,
actions, and scenes over minutes of content. Efficiency follows as a byproduct
of retrieval (near-linear scaling), which enables practical training and
synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [38] [Reverse Designing Ferroelectric Capacitors with Machine Learning-based Compact Modeling](https://arxiv.org/abs/2508.20216)
*Diego Ferrer,Jack Hutchins,Revanth Koduru,Sumeet Kumar Gupta,Admedullah Aziz*

Main category: cs.ET

TL;DR: 论文提出两种基于机器学习紧凑模型的反向设计算法，能快速高效地确定器件参数以满足特定电性能需求，相比传统方法大幅减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用机器学习紧凑模型快速估算器件行为，解决传统计算密集方法（如相场建模）在迭代设计过程中时间成本过高的问题。

Method: 提出两种反向设计算法，通过机器学习紧凑模型从期望的电特性反向推导出器件参数（如层厚），并评估其准确性和计算效率。

Result: 相比相场建模，机器学习方法显著减少计算时间，展示了高效优势；两种算法在准确性和效率上均表现出色。

Conclusion: 基于机器学习的紧凑建模方法为器件设计提供了一种高效、准确的替代方案，特别适合需要快速迭代的场景。

Abstract: Machine learning-based compact models provide a rapid and efficient approach
for estimating device behavior across multiple input parameter variations. In
this study, we introduce two reverse-design algorithms that utilize these
compact models to identify device parameters corresponding to desired
electrical characteristics. The algorithms effectively determine parameter
sets, such as layer thicknesses, required to achieve specific device
performance criteria. Significantly, the proposed methods are uniquely enabled
by machine learning-based compact modeling; alternative computationally
intensive approaches, such as phase-field modeling, would impose impractical
time constraints for iterative design processes. Our comparative analysis
demonstrates a substantial reduction in computation time when employing machine
learning-based compact models compared to traditional phase-field methods,
underscoring a clear and substantial efficiency advantage. Additionally, the
accuracy and computational efficiency of both reverse-design algorithms are
evaluated and compared, highlighting the practical advantages of machine
learning-based compact modeling approaches.

</details>


### [39] [Blind Source Separation-Enabled Joint Communication and Sensing in IBFD MIMO Systems](https://arxiv.org/abs/2508.20409)
*Siyao Li,Conrad Prisby,Thomas Yang*

Main category: cs.ET

TL;DR: 提出了一种基于盲源分离的框架，用于在IBFD MIMO系统中同时进行自干扰消除和感知信息提取，利用FastICA算法提升通信和感知性能。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中的联合通信与感知（JCAS）面临自干扰（SI）的挑战，但高功率SI信号可用于高效感知。

Method: 提出基于盲源分离（BSS）的框架，利用FastICA算法分离SI信号与目标信号（SOI），实现自干扰消除和信道估计。

Result: 仿真结果表明，随着信号帧大小的增加，框架显著提升了感知和通信性能。

Conclusion: 该框架为IBFD MIMO系统中的JCAS提供了一种有效的解决方案，无需专用雷达波形即可实现高效感知和信号恢复。

Abstract: This paper addresses the challenge of joint communication and sensing (JCAS)
in next-generation wireless networks, with an emphasis on in-band full-duplex
(IBFD) multiple-input multiple-output (MIMO) systems. Traditionally,
self-interference (SI) in IBFD systems is a major obstacle to recovering the
signal of interest (SOI). Under the JCAS paradigm, however, this high-power SI
signal presents an opportunity for efficient sensing. Since each transceiver
node has access to the original SI signal, its environmental reflections can be
exploited to estimate channel conditions and detect changes, without requiring
dedicated radar waveforms. We propose a blind source separation (BSS)-based
framework to simultaneously perform self-interference cancellation (SIC) and
extract sensing information in IBFD MIMO settings. The approach applies the
Fast Independent Component Analysis (FastICA) algorithm to separate the SI and
SOI signals while enabling simultaneous signal recovery and channel estimation.
Simulation results confirm the framework's effectiveness, showing improved
sensing and communication performance as signal frame size increases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation](https://arxiv.org/abs/2508.20253)
*Ruihao Li,Qinzhe Wu,Krishna Kavi,Gayatri Mehta,Jonathan C. Beard,Neeraja J. Yadwadkar,Lizy K. John*

Main category: cs.DC

TL;DR: SpeedMalloc通过轻量级支持核心处理多线程应用的内存分配任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代多线程多核系统中，内存分配虽只占总指令的5%，却可能导致2.7倍的性能差异，原因是分配器元数据与用户数据交织，导致缓存污染或跨线程同步开销。

Method: SpeedMalloc使用轻量级的支持核心处理分配任务，将所有元数据保存在其缓存中，减少与用户数据的缓存冲突，并避免跨核心元数据同步。

Result: SpeedMalloc在多线程工作负载上，比五种先进分配器（Jemalloc、TCMalloc等）分别快1.75倍、1.18倍、1.15倍、1.23倍和1.18倍。

Conclusion: SpeedMalloc通过设计支持核心，有效解决了分配器性能问题，并具备更好的通用性和扩展性。

Abstract: Memory allocation, though constituting only a small portion of the executed
code, can have a "butterfly effect" on overall program performance, leading to
significant and far-reaching impacts. Despite accounting for just approximately
5% of total instructions, memory allocation can result in up to a 2.7x
performance variation depending on the allocator used. This effect arises from
the complexity of memory allocation in modern multi-threaded multi-core
systems, where allocator metadata becomes intertwined with user data, leading
to cache pollution or increased cross-thread synchronization overhead.
Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a
potential direction to improve the allocator performance and mitigate cache
pollution. However, these accelerators currently have limited support for
multi-threaded applications, and synchronization between cores and accelerators
remains a significant challenge.
  We present SpeedMalloc, using a lightweight support-core to process memory
allocation tasks in multi-threaded applications. The support-core is a
lightweight programmable processor with efficient cross-core data
synchronization and houses all allocator metadata in its own caches. This
design minimizes cache conflicts with user data and eliminates the need for
cross-core metadata synchronization. In addition, using a general-purpose core
instead of domain-specific accelerators makes SpeedMalloc capable of adopting
new allocator designs. We compare SpeedMalloc with state-of-the-art software
and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and
Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on
multithreaded workloads over these five allocators, respectively.

</details>


### [41] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: SwizzlePerf利用LLM的硬件感知能力，自动优化GPU内核性能，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索的方法缺乏硬件感知，而人类工程师依赖这一特性实现最佳性能。

Method: 通过结合内存访问模式、架构规格、过滤的性能日志和历史性能反思，SwizzlePerf自动生成硬件优化的GPU内核。

Result: SwizzlePerf在5分钟内完成专家需2周的任务，10个测试内核中9个性能提升2.06倍，L2命中率提高70%。

Conclusion: SwizzlePerf是系统化构建硬件感知LLM性能工程代理的第一步。

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


### [42] [Predictable LLM Serving on GPU Clusters](https://arxiv.org/abs/2508.20274)
*Erfan Darzi,Shreeanant Bharadwaj,Sree Bhargavi Balija*

Main category: cs.DC

TL;DR: 论文提出了一种PCIe不可知的、可部署于虚拟机的主机级控制器，通过动态MIG配置、PCIe感知的放置和轻量级保护机制，显著降低了延迟敏感型任务的SLO违规率和p99延迟。


<details>
  <summary>Details</summary>
Motivation: 共享A100集群上的延迟敏感型推理常因PCIe网络的噪声邻居干扰导致尾部延迟和SLO违规增加，需解决这一问题。

Method: 结合动态MIG重新配置、PCIe感知放置和轻量级保护机制（如MPS配额、cgroup I/O），采样每个租户的尾部延迟和系统信号，利用拓扑提示避免PCIe热点，并通过冷却机制防止抖动。

Result: 单主机和2节点（16 GPU）集群上，SLO违规率降低约32%（约1.5倍），p99延迟改善约15%，吞吐量损失≤5%；LLM服务测试显示p99延迟改善10--15%。

Conclusion: 该控制器能有效减少干扰，提升性能，适用于延迟敏感型任务，即使在LLM服务中也能显著改善延迟。

Abstract: Latency-sensitive inference on shared A100 clusters often suffers
noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO
violations. We present a fabric-agnostic, VM-deployable host-level controller
that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware
placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples
per-tenant tails and system signals, uses topology hints to avoid PCIe hot
spots, and gates actions with dwell/cool-down to avoid thrash. On a single host
and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \(\approx\)32\%
(\(\approx\)1.5) and p99 latency improves \(\approx\)15\% with \(\leq\)5\%
throughput cost versus static MIG and naive placement; ablations show MIG and
placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo
2 7B Instruct: TTFT p99 improves \(\approx\)10--15\% at \(\leq\)5\% cost
without changing the controller.

</details>


### [43] [CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference](https://arxiv.org/abs/2508.20375)
*Guanyu Xu,Zhiwei Hao,Li Shen,Yong Luo,Fuhui Sun,Xiaoyan Wang,Han Hu,Yonggang Wen*

Main category: cs.DC

TL;DR: CoFormer是一个协作推理系统，通过分解大型Transformer模型并在边缘设备上分布式推理，解决了资源受限设备的计算和通信问题。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer模型的计算需求和资源限制，边缘设备上实现高质量实时服务存在挑战。现有方法在通信开销和精度效率权衡上表现不佳。

Method: 提出CoFormer系统，利用Transformer的可分割和可集成特性，将大型模型分解为小模型进行分布式推理，并使用DeBo算法优化分解策略和性能校准。

Result: 实验显示，CoFormer支持多种Transformer模型在异构边缘设备上运行，推理速度提升3.1倍，内存需求减少76.3%，能耗降低40%。

Conclusion: CoFormer通过分布式推理和优化策略，成功解决了边缘设备上Transformer模型的资源限制问题，实现了高效节能的推理性能。

Abstract: The impressive performance of transformer models has sparked the deployment
of intelligent applications on resource-constrained edge devices. However,
ensuring high-quality service for real-time edge systems is a significant
challenge due to the considerable computational demands and resource
requirements of these models. Existing strategies typically either offload
transformer computations to other devices or directly deploy compressed models
on individual edge devices. These strategies, however, result in either
considerable communication overhead or suboptimal trade-offs between accuracy
and efficiency. To tackle these challenges, we propose a collaborative
inference system for general transformer models, termed CoFormer. The central
idea behind CoFormer is to exploit the divisibility and integrability of
transformer. An off-the-shelf large transformer can be decomposed into multiple
smaller models for distributed inference, and their intermediate results are
aggregated to generate the final output. We formulate an optimization problem
to minimize both inference latency and accuracy degradation under heterogeneous
hardware constraints. DeBo algorithm is proposed to first solve the
optimization problem to derive the decomposition policy, and then progressively
calibrate decomposed models to restore performance. We demonstrate the
capability to support a wide range of transformer models on heterogeneous edge
devices, achieving up to 3.1$\times$ inference speedup with large transformer
models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6
billion parameters on edge devices, reducing memory requirements by 76.3\%.
CoFormer can also reduce energy consumption by approximately 40\% while
maintaining satisfactory inference performance.

</details>


### [44] [pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification](https://arxiv.org/abs/2508.20403)
*Tiancheng Zhao,Zekun Yin,Huihai An,Xiaoyu Yang,Zhou Jin,Jiasi Shen,Helen Xu*

Main category: cs.DC

TL;DR: 论文提出了一种名为pdGRASS的并行算法，解决了现有feGRASS方法在并行化和性能上的问题，显著提升了图光谱稀疏化的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的feGRASS方法在并行化处理和性能上存在局限，尤其是在数据依赖性和输入偏斜时表现不佳，因此需改进。

Method: 提出pdGRASS算法，通过将边组织为无数据依赖的独立子任务，实现高效并行化和单次通过的边恢复。

Result: pdGRASS在边恢复时间和稀疏化质量上均优于feGRASS，平均加速比达3.9x至8.8x，并显著改善了最坏情况下的运行时间。

Conclusion: pdGRASS在并行化、效率和可扩展性方面为图光谱稀疏化问题提供了显著改进，是现有方法的有效替代方案。

Abstract: Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or
sparsifier, whose Laplacian matrix closely approximates the spectral properties
of the original graph, enabling substantial reductions in computational
complexity for computationally intensive problems in scientific computing. The
state-of-the-art method for efficient GSS is feGRASS, consisting of two steps:
1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS
suffers from two main issues: 1) difficulties in parallelizing the recovery
step for strict data dependencies, and 2) performance degradation on skewed
inputs, often requiring multiple passes to recover sufficient edges. To address
these challenges, we propose parallel density-aware Graph Spectral
Sparsification (pdGRASS), a parallel algorithm that organizes edges into
disjoint subtasks without data dependencies between them, enabling efficient
parallelization and sufficient edge recovery in a single pass. We empirically
evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)
sparsifier quality, measured by the iteration count required for convergence in
a preconditioned conjugate gradient (PCG) application. The evaluation
demonstrates that, depending on the number of edges recovered, pdGRASS achieves
average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show
between 1.2x higher and 1.8x lower PCG iteration counts, with further
improvements as more edges are recovered. Additionally, pdGRASS mitigates the
worst-case runtimes of feGRASS with over 1000x speedup. These results highlight
pdGRASS's significant improvements in scalability and performance for the graph
spectral sparsification problem.

</details>


### [45] [Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems](https://arxiv.org/abs/2508.20508)
*Yilin Li,Song Han,Sibo Wang,Ming Wang,Renzi Meng*

Main category: cs.DC

TL;DR: 该论文提出了一种基于多智能体协同进化机制的服务优化方法，解决了大规模微服务架构中的治理挑战，并在实验中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决微服务架构中复杂的服务依赖、动态拓扑结构和波动工作负载等治理挑战。

Method: 将每个服务建模为智能体，使用图表示学习构建依赖图，并通过马尔可夫决策过程学习策略，结合集中训练与分散执行框架。

Result: 实验结果表明，该方法在协调效率、适应性和策略收敛性上优于现有方法，显著提升了治理效率和系统稳定性。

Conclusion: 该方法具有实用价值和工程可行性，适用于大规模微服务系统的治理。

Abstract: This paper proposes an intelligent service optimization method based on a
multi-agent collaborative evolution mechanism to address governance challenges
in large-scale microservice architectures. These challenges include complex
service dependencies, dynamic topology structures, and fluctuating workloads.
The method models each service as an agent and introduces graph representation
learning to construct a service dependency graph. This enables agents to
perceive and embed structural changes within the system. Each agent learns its
policy based on a Markov Decision Process. A centralized training and
decentralized execution framework is used to integrate local autonomy with
global coordination. To enhance overall system performance and adaptability, a
game-driven policy optimization mechanism is designed. Through a
selection-mutation process, agent strategy distributions are dynamically
adjusted. This supports adaptive collaboration and behavioral evolution among
services. Under this mechanism, the system can quickly respond and achieve
stable policy convergence when facing scenarios such as sudden workload spikes,
topology reconfigurations, or resource conflicts. To evaluate the effectiveness
of the proposed method, experiments are conducted on a representative
microservice simulation platform. Comparative analyses are performed against
several advanced approaches, focusing on coordination efficiency, adaptability,
and policy convergence performance. Experimental results show that the proposed
method outperforms others in several key metrics. It significantly improves
governance efficiency and operational stability in large-scale microservice
systems. The method demonstrates strong practical value and engineering
feasibility.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [46] [Efficient Forkless Blockchain Databases](https://arxiv.org/abs/2508.20686)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 提出了一种用于无分叉区块链的数据库方案，显著提升了存储和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的区块链节点操作成本高，StateDB是资源密集型组件，而无分叉区块链仍使用低效的分叉数据库。

Method: 设计了一种专门针对无分叉区块链的数据库方案。

Result: 相比geth-based Fantom Blockchain客户端，存储提升100倍，吞吐量提升10倍。

Conclusion: 该方案显著优化了区块链节点的性能。

Abstract: Operating nodes in an L1 blockchain remains costly despite recent advances in
blockchain technology. One of the most resource-intensive components of a node
is the blockchain database, also known as StateDB, that manages balances,
nonce, code, and the persistent storage of accounts/smart contracts. Although
the blockchain industry has transitioned from forking to forkless chains due to
improved consensus protocols, forkless blockchains still rely on legacy forking
databases that are suboptimal for their purposes. In this paper, we propose a
forkless blockchain database, showing a 100x improvement in storage and a 10x
improvement in throughput compared to the geth-based Fantom Blockchain client.

</details>


### [47] [Research Challenges in Relational Database Management Systems for LLM Queries](https://arxiv.org/abs/2508.20912)
*Kerem Akillioglu,Anurag Chakraborty,Sairaj Voruganti,M. Tamer Özsu*

Main category: cs.DB

TL;DR: 论文研究了大型语言模型（LLMs）在关系数据库管理系统中的应用，指出开源解决方案的功能和性能不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 提升开源SQL-LLM集成的功能与性能，解决现有系统的局限性。

Method: 通过评估两个开源系统和一个企业平台，使用五个代表性查询分析功能、性能和扩展性问题。

Result: 发现了三大问题并实施了初步解决方案，观察到性能提升。

Conclusion: 更紧密的LLM与DBMS集成是实现高效扩展的关键。

Abstract: Large language models (LLMs) have become essential for applications such as
text summarization, sentiment analysis, and automated question-answering.
Recently, LLMs have also been integrated into relational database management
systems to enhance querying and support advanced data processing. Companies
such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly
within SQL, denoted as LLM queries, to boost data insights. However,
open-source solutions currently have limited functionality and poor
performance. In this work, we present an early exploration of two open-source
systems and one enterprise platform, using five representative queries to
expose functional, performance, and scalability limits in today's SQL-invoked
LLM integrations. We identify three main issues: enforcing structured outputs,
optimizing resource utilization, and improving query planning. We implemented
initial solutions and observed improvements in accommodating LLM powered SQL
queries. These early gains demonstrate that tighter integration of LLM+DBMS is
the key to scalable and efficient processing of LLM queries.

</details>


### [48] [Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets](https://arxiv.org/abs/2508.20986)
*Lianpeng Qiao,Ziqi Cao,Kaiyu Feng,Ye Yuan,Guoren Wang*

Main category: cs.DB

TL;DR: 论文提出了一种自动化特征增强框架ReCoGNN，通过建模表内和表间关系来提升预测任务的效果。


<details>
  <summary>Details</summary>
Motivation: 数据是跨领域创新核心资产，但现有的预测模型需手动特征工程，自动化特征增强仍有挑战。

Method: ReCoGNN通过表内语义建模和构建异质图，结合图神经网络实现特征选择和增强。

Result: 在十多个数据集上，ReCoGNN在分类和回归任务中均优于现有方法。

Conclusion: ReCoGNN展示了自动化特征增强的潜力，能有效提升预测任务性能。

Abstract: Data has become a foundational asset driving innovation across domains such
as finance, healthcare, and e-commerce. In these areas, predictive modeling
over relational tables is commonly employed, with increasing emphasis on
reducing manual effort through automated machine learning (AutoML) techniques.
This raises an interesting question: can feature augmentation itself be
automated and identify and utilize task-related relational signals?
  To address this challenge, we propose an end-to-end automated feature
augmentation framework, ReCoGNN, which enhances initial datasets using features
extracted from multiple relational tables to support predictive tasks. ReCoGNN
first captures semantic dependencies within each table by modeling intra-table
attribute relationships, enabling it to partition tables into structured,
semantically coherent segments. It then constructs a heterogeneous weighted
graph that represents inter-row relationships across all segments. Finally,
ReCoGNN leverages message-passing graph neural networks to propagate
information through the graph, guiding feature selection and augmenting the
original dataset. Extensive experiments conducted on ten real-life and
synthetic datasets demonstrate that ReCoGNN consistently outperforms existing
methods on both classification and regression tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [49] [Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs](https://arxiv.org/abs/2508.20304)
*Siyuan Lu,Kangwei Xu,Peng Xie,Rui Wang,Yuanqing Cheng*

Main category: cs.AR

TL;DR: 本文提出了一种基于环形振荡器的测试技术，用于检测多壁碳纳米管（MWCNT）互连的延迟故障，并改进了测试方法和电路设计，以提高碳纳米管FPGA的测试效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着半导体工艺节点进入纳米尺度，CMOS FPGA在性能和功耗扩展方面面临挑战，而碳纳米管技术因其优越的导电性和高功率效率成为潜在替代方案。然而，MWCNT互连和CNFET制造过程中存在工艺变异性问题，导致延迟故障和金属性碳纳米管（m-CNTs）问题。

Method: 提出了基于环形振荡器（RO）的测试技术，针对MWCNT互连的延迟故障；改进了进位链和查找表（LUT）的测试方法；设计了用于检测m-CNTs的算法；并引入冗余备用行共享架构以提高成品率。

Result: 实验结果表明，6输入LUT的测试时间减少了35.49%，算法实现了高测试覆盖率且开销低，冗余架构能高效修复故障段。

Conclusion: 提出的方法有效解决了碳纳米管FPGA的工艺变异性和测试难题，显著提高了测试效率和成品率。

Abstract: As the semiconductor manufacturing process technology node shrinks into the
nanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big
challenges in scalability of performance and power consumption. Multi-walled
Carbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects
thanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor
(CNFET) also emerges as a prospective alternative to the conventional CMOS
device because of high power efficiency and large noise margin. The combination
of MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT
interconnects exhibit significant process variations due to immature
fabrication process, leading to delay faults. Also, the non-ideal CNFET
fabrication process may generate a few metallic CNTs (m-CNTs), rendering
correlated faulty blocks. In this article, we propose a ring oscillator (RO)
based testing technique to detect delay faults due to the process variation of
MWCNT interconnects. Furthermore, we propose an effective testing technique for
the carry chains in CLBs, and an improved circuit design based on the lookup
table (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In
addition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we
propose a redundant spare row sharing architecture to improve the yield of
CNT-based FPGA further. Experimental results show that the test time for a
6-input LUT can be reduced by 35.49% compared with conventional testing, and
the proposed algorithm can achieve a high test coverage with little overhead.
The proposed redundant architecture can repair the faulty segment effectively
and efficiently.

</details>


### [50] [The Future of Memory: Limits and Opportunities](https://arxiv.org/abs/2508.20425)
*Shuhan Liu,Samuel Dayo,Peijing Li,Philip Levis,Subhasish Mitra,Thierry Tambe,David Tennenhouse,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 论文提出了一种通过紧密耦合计算单元与小内存切片来优化内存访问成本和能源效率的方法，取代传统的共享大内存架构。


<details>
  <summary>Details</summary>
Motivation: 传统的大规模共享内存架构在扩展和信号传输方面存在工程挑战，而作者希望通过创新设计解决这些问题。

Method: 利用2.5D/3D集成技术，将内存分割为更小的切片并与计算单元紧密耦合，提供私有本地内存和高效共享状态。

Result: 该方法显著降低了内存访问成本，提升了带宽和能源效率，同时支持软件高效管理数据层次。

Conclusion: 紧密耦合计算单元和小内存切片的设计为高性能计算系统提供了更优的内存解决方案。

Abstract: Memory latency, bandwidth, capacity, and energy increasingly limit
performance. In this paper, we reconsider proposed system architectures that
consist of huge (many-terabyte to petabyte scale) memories shared among large
numbers of CPUs. We argue two practical engineering challenges, scaling and
signaling, limit such designs. We propose the opposite approach. Rather than
create large, shared, homogenous memories, systems explicitly break memory up
into smaller slices more tightly coupled with compute elements. Leveraging
advances in 2.5D/3D integration, this compute-memory node provisions private
local memory, enabling accesses of node-exclusive data through micrometer-scale
distances, and dramatically reduced access cost. In-package memory elements
support shared state within a processor, providing far better bandwidth and
energy-efficiency than DRAM, which is used as main memory for large working
sets and cold data. Hardware making memory capacities and distances explicit
allows software to efficiently compose this hierarchy, managing data placement
and movement.

</details>


### [51] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 论文研究了将SHA-3加密算法作为定制指令集成到通用处理器中的微架构挑战，并在RISC-V架构上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案主要依赖独立协处理器或软件优化，直接集成SHA-3到微架构仍面临挑战。

Method: 研究通过循环精确的GEM5模拟和FPGA原型设计，实现了SHA-3定制指令在RISC-V架构上的集成。

Result: 实验结果显示性能提升高达8.02倍（RISC-V优化软件）和46.31倍（Keccak特定软件），硬件开销仅增加15.09%寄存器和11.51% LUT利用率。

Conclusion: 研究证明了SHA-3微架构加速的可行性，为未来密码学指令集扩展提供了实用设计参考。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [52] [Spatio-Temporal Pruning for Compressed Spiking Large Language Models](https://arxiv.org/abs/2508.20122)
*Yi Jiang,Malyaban Bal,Brian Matejek,Susmit Jha,Adam Cobb,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 该论文提出了一个时空剪枝框架，用于优化脉冲神经网络（SNNs）与大型语言模型（LLMs）的结合，以降低计算复杂性和推理延迟，实现高效能的低功耗计算。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在能量受限环境中的部署面临模型规模大和推理延迟高的问题，而脉冲神经网络因其稀疏的事件驱动特性能效优势，成为潜在的解决方案。

Method: 提出了时空剪枝框架，包括空间剪枝（减少活跃神经元和注意力头）和时间剪枝（动态调整时间步数），并结合极端量化和知识蒸馏技术。

Result: 在SpikingBERT模型和GLUE基准测试上的实验表明，该方法显著减少了计算操作和推理延迟。

Conclusion: 该研究为实时低功耗自然语言处理应用提供了实用解决方案，使得脉冲LLM更适合部署在边缘设备和能量受限环境中。

Abstract: Large Language Models (LLMs) present significant challenges for deployment in
energy-constrained environments due to their large model sizes and high
inference latency. Spiking Neural Networks (SNNs), inspired by the sparse
event-driven neural processing and energy-efficient information transmission in
the brain, offer a promising alternative for achieving low-power computing.
Integrating the event-driven efficiency of spiking neurons with the advanced
capabilities of LLMs represents a promising direction for power-efficient LLMs.
This work specifically delves into the design of compressed spiking LLMs. Here,
we revisit spatial and temporal pruning from the perspective of SNNs and
propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize
computational efficiency while preserving high performance. Our spatial pruning
technique reduces the number of active neurons and attention heads, effectively
lowering the computational complexity of the model. Meanwhile, temporal pruning
minimizes inference latency by dynamically adjusting the number of timesteps
required for different layers. By combining these approaches with other
compression techniques, we present the first work in the domain of Spiking LLMs
to jointly explore spatial pruning, temporal pruning, extreme quantization and
knowledge distillation strategies. Extensive experimental evaluation of our
proposed framework for SpikingBERT on the large-scale GLUE benchmark
demonstrates the efficacy of our approach in terms of computational operations
and inference latency. Our approach offers a compelling solution for real-time,
low-power natural language processing applications, making Spiking LLMs more
practical for deployment on edge devices and in power-constrained settings.

</details>


### [53] [Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition](https://arxiv.org/abs/2508.20850)
*Tianyi Liu,Hemma Philamore,Benjamin Ward-Cherrier*

Main category: cs.NE

TL;DR: 提出一种通用编码策略，将触觉传感器数据映射到电刺激模式，使神经器官能执行开环人工触觉盲文分类任务，多器官组合显著提升分类准确率和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 探索器官作为低功耗、适应性生物混合计算元件的潜力，为未来 scalable 生物混合计算架构提供基础编码框架。

Method: 利用低密度微电极阵列培养人前脑器官，系统电刺激并记录响应（尖峰活动和活动中心位移），结合事件触觉输入实现分类。

Result: 单器官分类准确率61%，三器官组合提升至83%，并增强对人工噪声的鲁棒性。

Conclusion: 器官可作为高效生物混合计算单元，为未来研究奠定基础。

Abstract: This study proposes a generalizable encoding strategy that maps tactile
sensor data to electrical stimulation patterns, enabling neural organoids to
perform an open-loop artificial tactile Braille classification task. Human
forebrain organoids cultured on a low-density microelectrode array (MEA) are
systematically stimulated to characterize the relationship between electrical
stimulation parameters (number of pulse, phase amplitude, phase duration, and
trigger delay) and organoid responses, measured as spike activity and spatial
displacement of the center of activity. Implemented on event-based tactile
inputs recorded from the Evetac sensor, our system achieved an average Braille
letter classification accuracy of 61 percent with a single organoid, which
increased significantly to 83 percent when responses from a three-organoid
ensemble were combined. Additionally, the multi-organoid configuration
demonstrated enhanced robustness against various types of artificially
introduced noise. This research demonstrates the potential of organoids as
low-power, adaptive bio-hybrid computational elements and provides a
foundational encoding framework for future scalable bio-hybrid computing
architectures.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [54] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: 论文探讨了使用iPhone 15 Pro Max的LiDAR技术作为传统背景去除和合成技术的替代方案，通过深度信息在低光和光照充足环境下表现优异，并提出了通过GPU加速实现实时处理的方案。当前主要限制是深度数据的分辨率较低。


<details>
  <summary>Details</summary>
Motivation: 传统背景去除技术如绿幕抠像和AI模型依赖光照条件，而LiDAR的深度信息不受光照影响，因此探索其在移动设备上的应用潜力。

Method: 整合iPhone 15 Pro Max的LiDAR和彩色摄像头，使用SwiftUI和Swift开发用户界面和后台，结合Metal Shader Language实现60帧每秒的实时图像增强。

Result: LiDAR在低光和光照环境下表现一致，但受限于深度数据的分辨率和LiDAR激光对某些材料的反射准确性。

Conclusion: 若LiDAR分辨率能提升至匹配彩色图像，将成为视频和摄影中背景去除的首选技术。

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile
devices can be used as a replacement for traditional background removal and
compositing techniques. Unlike approaches such as chroma keying and trained AI
models, LiDAR's depth information is independent of subject lighting, and
performs equally well in low-light and well-lit environments. We integrate the
LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image
processing. We use Apple's SwiftUI and Swift frameworks for user interface and
backend development, and Metal Shader Language (MSL) for realtime image
enhancement at the standard iPhone streaming frame rate of 60 frames per
second. The only meaningful limitations of the technology are the streaming
bandwidth of the depth data, which currently reduces the depth map resolution
to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect
accurate depth from some materials. If the LiDAR resolution on a mobile device
like the iPhone can be improved to match the color image resolution, LiDAR
could feasibly become the preeminent method of background removal for video
applications and photography.

</details>


### [55] [Is the medical image segmentation problem solved? A survey of current developments and future directions](https://arxiv.org/abs/2508.20139)
*Guoping Xu,Jayaram K. Udupa,Jax Luo,Songlin Zhao,Yajun Yu,Scott B. Raymond,Hao Peng,Lipeng Ning,Yogesh Rathi,Wei Liu,You Zhang*

Main category: eess.IV

TL;DR: 过去二十年的医学图像分割主要依赖深度学习，取得了显著进展，但仍存在挑战和未解决的问题。本文回顾了医学图像分割的十年发展，探讨了七项关键维度。


<details>
  <summary>Details</summary>
Motivation: 探讨当前医学图像分割模型的进展、挑战及未来方向，为研究者提供全面的视角和灵感。

Method: 通过回顾和分类过去十年的研究，分析核心原理（如多尺度分析、注意力机制等）和七项关键维度。

Result: 提供了一个全面的概述，指出了从监督学习到半监督/无监督学习、从器官分割到病灶任务等趋势。

Conclusion: 文章总结了医学图像分割的发展轨迹，并提供了开源资源库以支持未来研究。

Abstract: Medical image segmentation has advanced rapidly over the past two decades,
largely driven by deep learning, which has enabled accurate and efficient
delineation of cells, tissues, organs, and pathologies across diverse imaging
modalities. This progress raises a fundamental question: to what extent have
current models overcome persistent challenges, and what gaps remain? In this
work, we provide an in-depth review of medical image segmentation, tracing its
progress and key developments over the past decade. We examine core principles,
including multiscale analysis, attention mechanisms, and the integration of
prior knowledge, across the encoder, bottleneck, skip connections, and decoder
components of segmentation networks. Our discussion is organized around seven
key dimensions: (1) the shift from supervised to semi-/unsupervised learning,
(2) the transition from organ segmentation to lesion-focused tasks, (3)
advances in multi-modality integration and domain adaptation, (4) the role of
foundation models and transfer learning, (5) the move from deterministic to
probabilistic segmentation, (6) the progression from 2D to 3D and 4D
segmentation, and (7) the trend from model invocation to segmentation agents.
Together, these perspectives provide a holistic overview of the trajectory of
deep learning-based medical image segmentation and aim to inspire future
innovation. To support ongoing research, we maintain a continually updated
repository of relevant literature and open-source resources at
https://github.com/apple1986/medicalSegReview

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [56] [High performance visualization for Astronomy and Cosmology: the VisIVO's pathway toward Exascale systems](https://arxiv.org/abs/2508.20603)
*Eva Sciacca,Nicola Tuccari,Fabio Vitello,Valentina Cesare*

Main category: astro-ph.IM

TL;DR: VisIVO是为处理天体物理学大数据设计的可视化工具，未来将优化以适应高性能计算，提升便携性和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现代天文学和天体物理学产生海量数据，亟需新一代工具来优化存储、访问和数据分析。

Method: 利用容器化和虚拟化技术，VisIVO已能支持分布式计算基础设施；未来将进一步适配高性能计算系统。

Result: VisIVO计划提升应用的便携性、可维护性，优化资源利用和数据传输性能。

Conclusion: VisIVO的优化将推动天体物理学大数据的高效分析，支持更复杂的计算需求。

Abstract: Petabyte-scale data volumes are generated by observations and simulations in
modern astronomy and astrophysics. Storage, access, and data analysis are
significantly hampered by such data volumes and are leading to the development
of a new generation of software tools. The Visualization Interface for the
Virtual Observatory (VisIVO) has been designed, developed and maintained by
INAF since 2005 to perform multi-dimensional data analysis and knowledge
discovery in multivariate astrophysical datasets. Utilizing containerization
and virtualization technologies, VisIVO has already been used to exploit
distributed computing infrastructures including the European Open Science Cloud
(EOSC).
  We intend to adapt VisIVO solutions for high performance visualization of
data generated on the (pre-)Exascale systems by HPC applications in
Astrophysics and Cosmology (A\&C), including GADGET (GAlaxies with Dark matter
and Gas) and PLUTO simulations, thanks to the collaboration within the SPACE
Center of Excellence, the H2020 EUPEX Project, and the ICSC National Research
Centre. In this work, we outline the evolution's course as well as the
execution strategies designed to achieve the following goals: enhance the
portability of the VisIVO modular applications and their resource requirements;
foster reproducibility and maintainability; take advantage of a more flexible
resource exploitation over heterogeneous HPC facilities; and, finally, minimize
data-movement overheads and improve I/O performances.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [57] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: MoTAS是一种通过语音早期筛查阿尔茨海默病（AD）的框架，利用TTS增强数据和MoE机制优化特征选择，提升了分类性能，在ADReSSo数据集上达到85.71%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决AD筛查中数据有限和特征选择不精细的问题，提出一种高效的非侵入式筛查方法。

Method: 结合ASR获取转录文本，使用TTS合成语音增强数据，通过MoE机制动态选择多模态特征，优化特征融合。

Result: 在ADReSSo数据集上表现优异，准确率达85.71%，超过现有基线方法。

Conclusion: MoTAS在数据受限的AD筛查场景中具有实用价值，TTS增强和MoE机制对性能提升有显著贡献。

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [58] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: 本文介绍了Amadeus框架，通过两阶段架构（自回归模型和双向离散扩散模型）提升音乐生成性能，并提出MLSDES和CIEM增强模型表现，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成模型假设音符属性为有序序列，但实验发现属性间无严格时序依赖，因此提出无序集的并发建模方式。

Method: 采用两阶段架构：自回归模型处理音符序列，双向离散扩散模型处理属性；引入MLSDES增强潜在空间区分性，CIEM强化注意力机制。

Result: 实验表明Amadeus在多项指标上显著优于现有模型，速度提升4倍，并实现无需训练的细粒度属性控制。

Conclusion: Amadeus通过并发属性建模和优化策略，突破现有模型性能上限，并发布大规模数据集AMD支持未来研究。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C框架通过因果建模和有序动作序列，解决了现有反事实解释方法忽略因果依赖和同时干预的局限，生成现实可行的行动方案。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策中，透明度和可操作性需平衡，现有反事实解释方法忽视因果关系和顺序干预，导致结果不现实。

Method: P2C利用s(CASP)系统建模因果关系，生成因果一致的动作序列计划，并优化成本计算。

Result: P2C能生成现实可行的行动方案，优于缺乏因果知识的传统方法。

Conclusion: P2C通过因果建模和有序干预，显著提升了反事实解释的实用性和可行性。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [60] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 论文提出了一种可微分的神经常微分架构和专用损失函数，用于学习解决NP难问题的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前，如何将离散推理与神经网络结合是一个热门研究方向，大语言模型在这类任务上表现不佳，因此需要新的架构来解决这一问题。

Method: 作者设计了一个概率损失函数，能够学习约束和目标，从而构建一个可解释且可扩展的完整模型，同时避免了组合求解器的训练瓶颈。

Result: 实验证明，该方法能高效学习解决NP难问题；在多种变体的Sudoku任务和视觉Min-Cut/Max-cut任务上表现优异，且在蛋白质设计问题上也取得良好效果。

Conclusion: 该架构为神经符号推理提供了高效且可扩展的解决方案，具有实际应用潜力。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [61] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 本文提出了QAgent，一个基于大语言模型的多代理系统，用于自动化OpenQASM编程，显著提高了量子代码生成的准确性。


<details>
  <summary>Details</summary>
Motivation: NISQ设备已展现出量子优势，但非专家难以编程，现有LLM代理功能有限。

Method: QAgent整合任务规划、上下文中少量学习、RAG、预定义工具和CoT推理，提升编译和功能正确性。

Result: QAgent在不同规模的LLM中，QASM代码生成准确率提高了71.6%。

Conclusion: QAgent有助于普及量子编程，缩小专业知识差距，加速量子计算的实用化。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [62] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种多代理框架的个人健康代理（PHA），通过分析多模态数据提供个性化健康建议，并进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过大型语言模型（LLMs）开发满足日常非临床环境中多样化需求的健康代理，填补现有研究的空白。

Method: 通过分析网络搜索和健康论坛查询，结合用户和健康专家的定性反馈，设计了三类专业子代理（数据分析、健康领域专家、健康教练），并开发了PHA框架。

Result: 在10个基准任务中进行了自动化与人工评估，涉及7000多份标注和1100小时的专家与用户投入，验证了系统的有效性。

Conclusion: PHA为未来个人健康代理的广泛应用奠定了坚实基础，展示了多代理框架在个性化健康管理中的潜力。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [63] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于数字孪生的边缘辅助跨系统框架，用于工业元宇宙中的实时人机交互，通过预测操作员动作优化性能和适应性，并引入HITL-MAML算法提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决工业元宇宙中人机交互面临的高计算负载、带宽限制和严格延迟问题。

Method: 利用数字孪生技术将系统解耦为视觉显示和机器人控制，并采用HITL-MAML算法动态调整预测范围。

Result: 在轨迹绘制控制任务中，加权RMSE从0.0712米降至0.0101米；在核退役实时3D场景任务中，PSNR为22.11、SSIM为0.8729、LPIPS为0.1298。

Conclusion: 该框架在实时高风险的工业环境中能确保空间精度和视觉保真度。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [64] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 提出了一种新的世界建模方法PEWM，通过限制视频生成为短时间片段来解决大规模交互数据依赖问题，提升语言与动作的细粒度对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成的体化世界模型依赖大规模交互数据，存在数据稀缺、收集困难和高维度限制，影响了语言与动作的细粒度对齐和长时序视频生成能力。

Method: 提出PEWM范式，限制视频生成为固定短时间片段，结合VLM规划和SGG机制，实现细粒度对齐和模块化控制。

Result: PEWM在数据效率、学习复杂度和推理延迟方面均有所改善，支持复杂任务的组合泛化。

Conclusion: PEWM通过利用视频模型的时空先验和VLM的语义感知，为可扩展、可解释的通用体化智能奠定了基础。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [65] [The Mathematician's Assistant: Integrating AI into Research Practice](https://arxiv.org/abs/2508.20236)
*Jonas Henkel*

Main category: math.HO

TL;DR: AI在数学研究中的应用潜力巨大，但需结合人类批判性思维，提出增强数学家的框架，强调AI作为辅助工具而非替代品。


<details>
  <summary>Details</summary>
Motivation: 探索AI在数学研究中的应用潜力，解决现有大型语言模型在数学研究中的系统性问题。

Method: 分析现有大型语言模型的基准测试，提出基于人类与AI协作的增强数学家框架，并制定五项指导原则。

Result: AI在数学研究中表现出解决问题的能力，但也存在系统性缺陷，需要通过人类监督实现有效应用。

Conclusion: AI当前的主要作用是增强人类研究能力，而非完全自动化，需掌握新的技能以有效利用这些工具。

Abstract: The rapid development of artificial intelligence (AI), marked by
breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer
powerful new tools that have the potential to significantly alter the research
practice in many areas of mathematics. This paper explores the current
landscape of publicly accessible large language models (LLMs) in a mathematical
research context, based on developments up to August 2, 2025. Our analysis of
recent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\'c et
al., 2025; Dekoninck et al., 2025), reveals a complex duality: while
state-of-the-art models demonstrate strong abilities in solving problems and
evaluating proofs, they also exhibit systematic flaws, including a lack of
self-critique and a model depending discrepancy between final-answer accuracy
and full-proof validity.
  Based on these findings, we propose a durable framework for integrating AI
into the research workflow, centered on the principle of the augmented
mathematician. In this model, the AI functions as a copilot under the critical
guidance of the human researcher, an approach distilled into five guiding
principles for effective and responsible use. We then systematically explore
seven fundamental ways AI can be applied across the research lifecycle, from
creativity and ideation to the final writing process, demonstrating how these
principles translate into concrete practice.
  We conclude that the primary role of AI is currently augmentation rather than
automation. This requires a new skill set focused on strategic prompting,
critical verification, and methodological rigor in order to effectively use
these powerful tools.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [Formal equivalence between global optimization consistency and random search](https://arxiv.org/abs/2508.20671)
*Gaëtan Serré*

Main category: cs.FL

TL;DR: 本文证明任何随机迭代全局优化算法在Lipschitz连续函数上一致当且仅当其采样整个搜索空间，并利用L$∃$∀N定理证明器和Mathlib库完成形式化。


<details>
  <summary>Details</summary>
Motivation: 研究随机迭代全局优化算法的一致性条件，为形式化证明提供理论基础。

Method: 定义算法为初始概率测度和马尔可夫核序列，利用Ionescu-Tulcea定理构建迭代序列的概率测度。

Result: 算法在Lipschitz连续函数上一致当且仅当采样整个搜索空间。

Conclusion: 形式化证明了算法的全局一致性与采样完备性的等价关系。

Abstract: We formalize a proof that any stochastic and iterative global optimization
algorithm is consistent over Lipschitz continuous functions if and only if it
samples the whole search space. To achieve this, we use the
L$\exists$$\forall$N theorem prover and the Mathlib library. The major
challenge of this formalization, apart from the technical aspects of the proof
itself, is to converge to a definition of a stochastic and iterative global
optimization algorithm that is both general enough to encompass all algorithms
of this type and specific enough to be used in a formal proof. We define such
an algorithm as a pair of an initial probability measure and a sequence of
Markov kernels that describe the distribution of the next point sampled by the
algorithm given the previous points and their evaluations. We then construct a
probability measure on finite and infinite sequences of iterations of the
algorithm using the Ionescu-Tulcea theorem.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [67] [Lattice Random Walk Discretisations of Stochastic Differential Equations](https://arxiv.org/abs/2508.20883)
*Samuel Duffield,Maxwell Aifer,Denis Melanson,Zach Belateche,Patrick J. Coles*

Main category: math.NA

TL;DR: 提出了一种基于格点随机游走的SDE离散化方法，通过二进制或三元增量简化计算，适用于随机计算架构。


<details>
  <summary>Details</summary>
Motivation: 传统浮点离散化方法计算复杂，且在随机计算架构中效率低。本文方法简化计算，避免高斯采样需求。

Method: 使用格点随机游走，每步采样二进制或三元增量，替代浮点运算。

Result: 实验证明该方法在多种SDE中有效，包括扩散模型，具有抗量化误差和鲁棒性。

Conclusion: 该方法为SDE离散化提供了高效且兼容随机计算架构的解决方案。

Abstract: We introduce a lattice random walk discretisation scheme for stochastic
differential equations (SDEs) that samples binary or ternary increments at each
step, suppressing complex drift and diffusion computations to simple 1 or 2 bit
random values. This approach is a significant departure from traditional
floating point discretisations and offers several advantages; including
compatibility with stochastic computing architectures that avoid floating-point
arithmetic in place of directly manipulating the underlying probability
distribution of a bitstream, elimination of Gaussian sampling requirements,
robustness to quantisation errors, and handling of non-Lipschitz drifts. We
prove weak convergence and demonstrate the advantages through experiments on
various SDEs, including state-of-the-art diffusion models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [68] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 论文提出基于图结构学习（GSL）的防护框架，以增强能源互联网（IoE）抵御网络攻击的能力，通过优化图拓扑和节点表示提高安全性。


<details>
  <summary>Details</summary>
Motivation: 能源互联网的互联性使其面临复杂的网络威胁，这些威胁可能导致公共安全问题，需要一种比传统方法更强大的防护方案。

Method: 提出GSL框架，联合优化图拓扑和节点表示，以抵抗对抗性网络模型操纵。

Result: 案例研究表明，GSL在稳健性上优于其他代表性方法，为保护IoE网络提供了可行路径。

Conclusion: GSL有潜力提升未来IoE网络的弹性和可靠性，同时指出了该领域的关键开放挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [69] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 利用神经机器翻译和规范化流技术，跨指令集架构（ISA）检测恶意软件，减少数据收集负担。


<details>
  <summary>Details</summary>
Motivation: 随着针对物联网设备的网络攻击增加，跨多种ISA的恶意软件检测需求增长，但数据收集和标注工作量巨大。

Method: 通过神经机器翻译（NMT）和规范化流（NFs）技术，将目标ISA的恶意软件翻译为已有丰富样本的ISA（如X86-64），再利用单ISA训练模型进行检测。

Result: 方法减少了跨ISA检测恶意软件时的数据收集需求，提升了效率。

Conclusion: 提出的技术有效解决了跨ISA恶意软件检测中的数据不足问题，为实际应用提供了可行方案。

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [70] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 论文分析了TEE容器的隔离策略及安全性问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究TEE容器的隔离策略及其安全性问题，以提升TEE平台的安全性。

Method: 设计自动化分析工具，评估TEE容器的隔离边界并识别设计缺陷。

Result: 发现部分TEE容器存在信息泄漏、回滚攻击等安全隐患。

Conclusion: 提出了开发更安全容器的建议，并探讨了TEE容器化的新趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: 介绍了一种名为VSF的新方法，通过翻转负提示的注意力值符号来动态抑制不需要的内容，适用于少步扩散和流匹配图像生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的负提示引导方法（如CFG、NASA和NAG）在少步模型中效果有限，VSF旨在更高效地抑制不需要的内容。

Method: VSF通过翻转负提示的注意力值符号动态抑制不需要的内容，计算开销小，适用于MMDiT和跨注意力模型。

Result: 实验表明，VSF在少步模型中显著优于现有方法，即使在非少步模型中也优于CFG，同时保持图像质量。

Conclusion: VSF是一种简单高效的负提示引导方法，适用于多种模型，显著提升性能。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [72] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出了一种名为CHAIR-DPO的方法，通过利用CHAIR指标和直接偏好优化（DPO）技术，有效减少了多模态大语言模型（MLLMs）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs在多种任务中表现出色，但存在幻觉问题，即生成的回答与视觉输入不符。本文旨在通过对齐方法解决这一问题。

Method: 利用CHAIR指标区分幻觉与非幻觉样本，并通过DPO技术对现成的MLLMs进行微调。

Result: CHAIR-DPO方法在多个幻觉基准测试中显著减少了幻觉回答，验证了CHAIR奖励的有效性。

Conclusion: CHAIR-DPO是一种简单而有效的方法，能够显著降低MLLMs中的幻觉现象，且其代码和模型已公开。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [73] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 这篇论文提出了一种统一框架，用于处理手语、唇语和音频等多种模态，以生成口语文本，并在性能上达到或超过特定任务的先进模型。


<details>
  <summary>Details</summary>
Motivation: 传统语音识别技术对聋人或听力障碍者不友好，而手语和唇语等视觉替代方式虽有效，但研究多孤立进行，缺乏统一框架。

Method: 设计了一种统一、模态无关的架构，处理异构输入，探索模态间的协同作用，特别是唇语在手语理解中的非手动线索作用。

Result: 框架在SLT、VSR、ASR和AVSR等任务上达到或超越先进模型，特别是将唇语作为独立模态显著提升SLT性能。

Conclusion: 统一框架不仅提升了多模态任务的性能，还揭示了模态协同的重要性。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [74] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 该论文提出了S-HArM数据集，用于意图感知分类，并通过不同提示策略生成合成数据进行比较研究。结果表明，保留视觉上下文的模型泛化能力更强，但整体性能仍有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了AI生成图像背后的意图，因此提出S-HArM数据集以填补这一空白。

Method: 使用Twitter/X和Reddit的9,576个图像-文本对构建数据集，并通过三种提示策略生成合成数据，对比不同模型的表现。

Result: 图像和多模态引导的数据训练模型在泛化能力上表现更好，但整体性能仍有提升空间。

Conclusion: 推断意图具有复杂性，需要专用架构进一步研究。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [75] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: FakeParts是一种新的深度伪造技术，通过局部修改真实视频的空间区域或时间片段，使其难以检测。为此，作者提出了FakePartsBench数据集以评估检测方法。研究发现FakeParts显著降低了人类和现有检测模型的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法对局部修改的视频（如面部表情修改、对象替换等）缺乏有效检测能力，亟需开发更鲁棒的检测手段。

Method: 提出了FakePartsBench数据集，包含超过25K视频，提供像素级和帧级修改标注，以全面评估检测方法。

Result: FakeParts使人类检测准确率降低了30%以上，现有检测模型性能也显著下降。

Conclusion: 研究揭示了当前深度伪造检测技术的漏洞，并为开发更强的局部修改检测方法提供了资源。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [76] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 医疗视觉语言模型（VLMs）在临床应用中潜力巨大，但存在隐私安全风险。MedFoundationHub是一个GUI工具包，使医生和工程师能便捷、安全地部署和使用VLMs，并通过专业评估揭示了模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLMs虽具应用前景，但其导致的PHI泄露、数据泄漏和网络安全风险尤为严重，亟需解决方案以确保安全使用。

Method: 开发了MedFoundationHub工具包，支持医生无编程使用不同模型，工程师快速部署开源VLMs，并通过Docker实现隐私保护。

Result: 专家评估了5种先进VLMs，发现其存在回答偏离、推理模糊和术语不一致等问题。

Conclusion: MedFoundationHub为医疗VLMs的部署提供了安全、易用的解决方案，同时揭示了当前模型的不足。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [77] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 提出模块化框架解耦因果推理与答案生成，引入可解释的因果链，提升推理透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有因果视频问答模型依赖不透明的管道，缺乏解释性且依赖浅层启发式方法。

Method: 采用两阶段架构，包括因果链提取器和因果链驱动的回答生成器，利用大语言模型生成高质量因果链。

Result: 在多个基准测试中超越现有模型，显著提升解释性、用户信任和泛化能力。

Conclusion: 该框架不仅性能优越，还可作为可复用的因果推理引擎跨领域应用。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为Subversive Alignment Injection (SAI)的攻击方法，通过利用大语言模型（LLM）的对齐机制，植入偏见或实现定向审查，即使对无关话题的响应不受影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示对齐机制可能被滥用的漏洞，从而在保持模型性能的同时，诱导其对特定话题或查询产生拒绝行为。

Method: 提出的SAI方法是一种投毒攻击，通过操纵对齐机制让模型在预设的特定主题上拒绝回答，同时不影响其他功能。

Result: 实验显示SAI能够规避现有防御技术，并在实际应用（如医疗聊天机器人、简历筛选等）中显著增加偏见（ΔDP高达38%）。

Conclusion: 研究表明LLM的对齐机制存在安全风险，需要更强大的防御手段来防止恶意利用。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [79] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 提出的TV-HSGT算法针对时变有向网络中的分布式在线优化问题，结合了混合随机梯度跟踪和方差缩减机制，无需梯度有界假设即可实现动态遗憾界。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和动态性的增加，分布式在线优化在实时决策中变得至关重要，但现有算法多依赖梯度有界假设且忽视随机梯度影响。

Method: TV-HSGT通过结合行随机和列随机通信方案，减少了梯度方差并准确跟踪全局下降方向。

Result: 理论分析显示TV-HSGT可提升动态遗憾界，实验验证了其在动态和资源受限环境中的有效性。

Conclusion: TV-HSGT在时变有向网络中表现优异，适用于动态和资源受限场景。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [80] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出了一个名为ProactiveEval的统一框架，用于评估大语言模型（LLMs）的主动对话能力，并通过实验展示了不同模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在特定领域或任务导向的情景，导致评估分散且无法全面探索模型的主动对话能力。

Method: 提出ProactiveEval框架，将主动对话分解为目标规划和对话引导，并建立跨领域的评估指标，同时自动生成多样化的评估数据。

Result: 实验覆盖22种LLMs，显示DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话引导任务中表现优异。

Conclusion: 探讨了推理能力对主动行为的影响，并讨论了其对未来模型开发的启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>
