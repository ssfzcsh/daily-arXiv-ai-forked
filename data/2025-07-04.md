<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 研究了软件工程师技术面试的准备情况，发现候选人缺乏真实场景训练且课程支持不足，导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 了解候选人如何准备技术面试，探讨准备方法和教育的作用，以改善面试准备的效果。

Method: 通过向131名正在准备技术面试的候选人分发调查问卷。

Result: 结果表明候选人很少在真实环境中训练，且课程支持不足，导致压力和准备不足。

Conclusion: 研究结果为利益相关者提供了改进技术面试准备的建议，以帮助软件工程师候选人更好地应对面试。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 论文提出了一种结合大型语言模型（LLM）解析自然语言查询与结构代码搜索工具的新方法，以自然语言为查询语言，显著提高了代码搜索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前开发者主要使用关键词和正则表达式进行代码搜索，而结构代码搜索工具则需要使用复杂的领域特定语言（DSL），门槛较高。为了提高搜索的易用性，研究试图通过自然语言简化查询过程。

Method: 提出了一种结合LLM解析自然语言查询的方法，并将其应用于Semgrep和GQL两种结构代码搜索DSL。通过构建包含400个查询的基准测试集进行评估。

Result: 该方法在精确率和召回率上表现优异（55%-70%），在F1分数上显著优于基于语义代码搜索和LLM检索的基线方法（分别高出57%和14%）。

Conclusion: 研究表明，利用自然语言查询结合LLM的方法可以有效降低结构代码搜索的门槛，同时提高搜索的效率和准确性。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究通过内部软件指标预测移动应用发布前的受欢迎程度，发现分类模型比回归模型更有效，最佳模型F1-score达0.72。


<details>
  <summary>Details</summary>
Motivation: 预测应用发布前的受欢迎度可为开发者提供竞争优势，但现有方法仍具挑战性。

Method: 使用446个开源Android应用的数据，提取代码指标、代码异味等特征，评估回归和分类模型。

Result: 回归模型表现不佳，但分类模型（尤其是多层感知器）在二分类任务中表现显著提升。

Conclusion: 内部代码指标可作为应用受欢迎度的有用指标，质疑了早期关于其无法预测软件质量的结论。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究比较了心理测量和生物特征指标在软件工程任务中对压力的测量效果，发现时间压力诱导不足，且生物特征仅部分显著。


<details>
  <summary>Details</summary>
Motivation: 传统自报工具可能存偏，研究者希望通过生物特征等客观方法补充心理测量。

Method: 实验结合心理测量、生物特征传感器（编程任务中）及访谈，评估压力。

Result: 心理测量未检出压力，访谈结果不一，生物特征仅EDA峰值显著差异。

Conclusion: 时间压力诱导不足，建议未来研究改进方法以更有效测量压力。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文提出了一种基于数据集特征的推荐方法，用于选择合适的情绪分析工具，以提高软件工程中的团队沟通分析效果。


<details>
  <summary>Details</summary>
Motivation: 软件开发中基于文本的沟通至关重要，但现有的情绪分析工具在不同平台上表现不一致，因此需要一种方法推荐适合的工具。

Method: 分析了10个开发者沟通数据集的语言和统计特征，评估了14种情绪分析工具的性能，并提出了一种基于特征的映射方法和问卷推荐工具。

Result: 数据集特征可用于改进工具选择，Transformer模型表现稳定但效果仍依赖于上下文。

Conclusion: 该方法为研究者和实践者提供了可信的工具选择支持，同时强调了随着沟通环境变化需要持续评估的必要性。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 本文提出一种多智能体方法，利用两个LLM智能体协作生成COBOL代码的解释，解决了COBOL因语法独特和代码过长超出LLM窗口的问题，并在真实项目中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于COBOL语言的老化、复杂性和开发者减少，维护其代码库日益困难。缺乏文档使新开发者难以理解维护，传统LLM方法因COBOL特殊性（如超出Token窗口）效果不佳。

Method: 提出多智能体方法，两个LLM智能体协作，结合代码库上下文生成函数、文件和项目级的解释。

Result: 在14个真实COBOL项目中，方法在函数级解释的METEOR、chrF和SentenceBERT分数分别提升12.67%、18.59%和0.62%；文件级解释在目的、功能和清晰度上分别优于基线4.21%、10.72%和14.68%；项目级解释覆盖82%项目。

Conclusion: 多智能体方法显著优于基线，能有效解决COBOL代码解释的挑战。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型的类型感知测试生成技术，有效地检测Python中的类型错误，减少误报并发现新错误。


<details>
  <summary>Details</summary>
Motivation: Python中的类型错误常导致运行时故障，影响软件可靠性和开发效率，现有的静态分析工具误报率高，单元测试生成技术缺乏针对性。

Method: RTED结合逐步类型约束分析和反射验证，指导测试生成过程并抑制误报。

Result: RTED在两基准测试中比现有技术多检测22-29个类型错误，误报率降低173.9%-245.9%，并在6个开源项目中发现12个未知错误。

Conclusion: RTED显著提升了类型错误的检测能力，具有高准确性和实用性。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: VeFIA框架通过TEE和协调器验证数据方的推理软件执行正确性，不泄露隐私且无额外延迟，检测异常概率高达99.99%。


<details>
  <summary>Details</summary>
Motivation: 现有VFL缺乏对数据方推理软件执行正确性的审计机制，VeFIA旨在解决这一问题。

Method: 利用TEE和协调器验证数据方的计算结果的正确性，随机采样检测异常推理。

Result: 异常推理超过5.4%时，检测概率达99.99%，随机采样的正负预测值和真阳性率均为100%。

Conclusion: VeFIA是首次讨论VFL中推理软件执行正确性的研究，高效且隐私安全。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 提出了一种自动化测试大型语言模型（LLM）公平性的新方法Meta-Fair，通过蜕变测试和LLM的能力生成与评估测试用例，实验证明其高效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前公平性测试方法依赖人工和固定模板，资源密集且难扩展，需开发自动化方法减少依赖并提升适用性。

Method: 结合蜕变测试（定义输入提示的变形关系）和LLM的能力（生成测试用例和评估输出），开发三个开源工具支持测试流程。

Result: 在12个LLM、14种变形关系和5个偏差维度上测试7.9K个用例，平均精度92%，发现29%的执行中存在偏差，LLM评估表现良好（F1分数达0.79）。

Conclusion: Meta-Fair展现了自动化测试LLM公平性的潜力，尽管存在非确定性问题，但仍为未来研究提供了方向。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI聊天机器人通过零样本和最少到最多提示技术，部分自动化需求获取访谈，减少人为错误并提高扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统需求获取访谈依赖分析师且易受偏见影响，LLM技术为自动化提供可能。

Method: 采用零样本和最少到最多提示优化LLMREI，评估33次模拟访谈。

Result: LLMREI在减少错误、提取需求及上下文提问方面表现接近人类，适合大规模访谈。

Conclusion: LLMREI可有效自动化需求获取，尤其适用于多利益相关者场景。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 通过将人机协作融入自适应信息物理系统（CPS），并注重伦理和隐私保护，研究旨在解决人机团队（HMT）的挑战。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS需要更有效的人机协作，但当前反馈机制未充分考虑人类操作节奏和隐私问题。

Method: 开发新方法将HMT融入CPS反馈循环，并构建伦理与人类价值的验证框架。

Result: 未明确成果，但目标是优化人机协作的效率和伦理合规性。

Conclusion: 研究填补了自适应CPS中人机协作的空白，强调伦理和隐私的重要性。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究表明，RSEs和SERs在相似概念上使用不同术语，导致沟通障碍。通过调查发现两者间有学习与协作机会。


<details>
  <summary>Details</summary>
Motivation: 理解RSEs与SERs在软件基础概念上的术语差异，以促进沟通与合作。

Method: 采用术语映射的系统方法，识别概念对齐与知识缺口。

Result: 初步发现表明双方存在相互学习的机会，并提供了术语映射的基础。

Conclusion: 研究为未来众包扩展与验证打下基础，促进RSEs与SERs的合作。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: RLHGNN是一种新颖的框架，通过将事件日志转换为异构过程图，并结合强化学习自动确定最优图结构，有效预测下一活动，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 优化面向服务的架构中的业务过程，解决现有序列和图方法无法捕捉非顺序关系及静态结构的问题。

Method: 将事件日志转换为三种边类型的异构图，结合强化学习选择最优图结构，使用异构图卷积预测下一活动。

Result: 在六个真实数据集上优于现有方法，推理延迟约为1毫秒，适合实时应用。

Conclusion: RLHGNN能精确建模顺序和非顺序关系，是业务过程监控的实用解决方案。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 提出了可持续发展标志的概念，以帮助识别云架构讨论中的可持续性议题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的兴起，软件架构中的可持续性问题日益重要，但目前缺乏明确的指导方法来识别相关讨论。

Method: 通过主题分析开发可持续发展标志，并在控制实验中评估其效果。

Result: 使用标志后，识别出的可持续性相关讨论减少，但分类的确定性和性能显著提高。

Conclusion: 可持续发展标志比单独使用定义更有效和易懂，有助于更准确地识别可持续性议题。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出了一种基于文本蕴含和上下文学习的自动生成法律文本规范表示的方法，将其编码为可执行的Python代码，减少了手动标注需求。


<details>
  <summary>Details</summary>
Motivation: 小型组织和初创公司缺乏法律专业知识，手动提取法律文本的元数据以确保合规性资源密集且繁琐。

Method: 采用文本蕴含和上下文学习技术，从法律文本中自动生成规范的表示，通过预设计的Python类结构捕获结构和语义元数据及其相互关系。

Result: 在13个美国州数据泄露通知法上测试，生成表示通过89.4%测试用例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法减少了手动标注需求，提高了对新法规的适用性，是一种高效的自动化合规解决方案。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 该研究探讨了使用GPT-4o生成后续面试问题以改进需求启发访谈质量的方法，实验表明LLM生成的问题在清晰度、相关性和信息量上不逊于人类撰写的问题，且在指导后表现更优。


<details>
  <summary>Details</summary>
Motivation: 需求启发访谈中，面试者常面临领域不熟悉、认知负荷和信息过载等挑战，需实时生成适当问题。LLM在自然语言处理任务中的优异表现使其成为潜解决方案。

Method: 研究基于常见面试错误类型框架，应用GPT-4o生成后续问题，并通过两项对照实验评估LLM生成与人类撰写问题的表现。

Result: LLM生成的问题在清晰度、相关性和信息量上与人类问题相当，且在错误类型指导下表现更优。

Conclusion: LLM能实时提升需求启发访谈的质量和效率，具有实际应用潜力。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 本文提出了一种名为DecoRTL的新型解码策略，旨在优化大语言模型（LLMs）在RTL代码生成中的表现，重点解决传统解码策略在结构化和语义需求上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM解码策略在设计为自然语言处理时，无法满足RTL代码生成的结构和语义需求，导致生成代码中存在幻觉、重复或无效的问题。

Method: DecoRTL通过引入两种互补组件：(i) 自我一致性采样，生成多个候选并基于令牌级别一致性进行重新排名；(ii) 语法感知温度调整，根据令牌的语法和功能角色调整采样温度。

Result: 在VerilogEval基准测试中，DecoRTL显著提高了语法有效性、功能正确性和输出多样性，且执行开销几乎不可察觉。

Conclusion: DecoRTL无需额外模型微调即可大幅提升LLMs在RTL代码生成中的性能，展现了其在自动化硬件设计中的潜力。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [18] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 论文探讨了在移动设备上部署大型语言模型（LLM）时能效低的问题，提出了统一的能量感知调控器FUSE以优化能效。


<details>
  <summary>Details</summary>
Motivation: 由于当前移动设备上的LLM框架能效低，CPU、GPU和内存的调控器相互独立运作，导致不必要的延迟和能耗。

Method: 通过测量分析LLM框架的能效问题，设计并实现了FUSE调控器，以优化移动设备上LLM推理的能效。

Result: FUSE显著减少了生成首个令牌和每个输出令牌的延迟，同时保持了相同的每令牌能耗。

Conclusion: FUSE为提高移动设备上LLM推理的能效提供了一种有效解决方案。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成的方法，重点探讨了深度学习和统计技术的发展及其在实际中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私问题和数据纯净度等挑战，推动数据驱动应用的发展。

Method: 回顾了合成网络流量生成的各种方法，包括数据模型和评估技术。

Result: 提供了一个全面的资源，分析现有方法、挑战和未来发展方向。

Conclusion: 为研究者和实践者提供了合成网络流量生成领域的结构化分析和发展机会。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [20] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 论文提出了一种名为Implicit Sequence Number (ISN)的新机制，用于检测芯片间互联中的数据丢失并确保顺序传输，同时提出Reliability Extended Link (RXL)扩展协议，以支持可扩展的多节点互联。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的计算需求超过单处理器的能力，芯片间互联成为可扩展计算的关键。现有的互联协议在高带宽传输中易受错误影响，尤其是在多节点配置中。

Method: 提出ISN机制，无需增加头部开销即可精确检测数据丢失并保证顺序传输；扩展CXL协议为RXL，整合ISN以提高可靠性和可扩展性。

Result: RXL通过将CRC提升为端到端数据完整性的传输层机制，并利用FEC进行链路层纠错，实现了高可靠性和带宽效率。

Conclusion: 该研究为解决多节点互联中的可靠性问题提供了有效方案，同时保持了与现有协议的兼容性。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [21] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本文提出了一种减少5G无线接入网络（RAN）能耗和运营成本的双重方法：基于协作学习的流量预测框架（CCL）和分布式单元池化方案（DUPS）。


<details>
  <summary>Details</summary>
Motivation: 5G网络中，RAN的能耗占总能耗的50%以上，现有技术未能充分利用数据潜力。

Method: 1. 提出CCL框架，通过选择性协作学习进行高精度流量预测；2. 提出DUPS方案，利用深度强化学习和CCL预测动态优化资源分配。

Result: CCL在预测准确性上优于现有技术39.1%~43.9%；DUPS能效提升89%，显著降低运营成本。

Conclusion: 整合CCL和DUPS为5G RAN提供了高效节能的解决方案，提升了成本效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [22] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: AI（尤其是大型语言模型）在网络配置和诊断中表现优异，本文聚焦于AI代理在网络故障排除中的应用，并强调需要一个标准化、可复现的开放基准平台。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在网络故障排除中的应用，并推动构建一个低操作成本的标准化基准平台。

Method: 未明确说明具体方法，但提出需建立开放、可复现的基准平台来评估AI代理。

Result: 尚未提供具体实验结果，但强调了标准化平台的重要性。

Conclusion: 标准化、开放的基准平台对高效评估和部署AI代理在网络故障排除中的应用至关重要。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [23] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出了一种基于信道感知的语义通信框架，以提升车联网（IoV）数据的传输效率和准确性，利用生成扩散模型预测动态信道状态，并通过大模型微调增强适应性。


<details>
  <summary>Details</summary>
Motivation: 车联网中大量数据的实时传输和处理在动态无线信道条件下存在挑战，需要高效、可靠的通信解决方案。

Method: 采用语义通信模型提取和压缩数据，并结合生成扩散模型预测信道状态，使用大模型微调提升适应性。

Result: 在两个公开数据集上验证了框架的性能和可靠性。

Conclusion: 提出的框架显著提升了车联网服务的质量和效率，适用于动态场景。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [24] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: REDUS是一种基于重采样的技术，用于在智能网络中优化深度学习训练，减少资源消耗和训练时间，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在SDN与深度学习工作负载共享基础设施的情况下，资源竞争可能导致网络性能下降。尤其是物联网环境中，DL训练的计算需求可能干扰SDN的实时性。

Method: 提出了REDUS技术，通过优先处理分类错误的样本并排除冗余数据，减少每轮训练所需的样本量，从而降低计算资源消耗和加速收敛。

Result: 在CICIoT2023数据集上的实验显示，REDUS能减少72.6%的训练时间，仅损失1.62%的准确率。

Conclusion: REDUS是一种高效且可扩展的解决方案，适用于资源受限的边缘设备，同时兼顾网络性能。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [25] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 该论文提出了一种基于5G PRS的多静态集成感知与通信（ISAC）完整信号处理链，通过分布式架构实现目标检测、参数估计与追踪。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用5G PRS信号实现高效的多静态ISAC，结合通信与感知功能，优化目标检测与追踪性能。

Method: 采用分布式架构，通过OFDM-PRS波形生成相干交叉模糊函数（CAF），提取时延与速度信息，并利用非线性最小二乘三边测量和正则化线性反演进行目标定位与速度估计。

Result: 实现了高保真的移动目标检测、定位与追踪，验证了5G PRS信号在多静态ISAC中的有效性。

Conclusion: 5G PRS信号在多静态ISAC中表现出色，为通信与感知的集成提供了可行方案。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [26] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 摘要探讨了地面网络（TN）与非地面网络（NTN）集成面临的挑战，提出基于O-RAN原则的架构和功能分割策略，并分析了性能、延迟、自主性和部署的权衡。


<details>
  <summary>Details</summary>
Motivation: 由于异构传播条件、动态拓扑和有限的星上处理能力，TN与NTN的集成面临独特挑战，需要探索标准化的解决方案。

Method: 提出分割选项的分类法，将RAN和核心功能分配于卫星和地面节点，并评估从纯星上DU部署到完整gNB和UPF集成的多种配置。讨论了近实时和非实时RAN智能控制器（RIC）的布局策略。

Result: 提供了架构分割与RIC布局的全面映射，强调实施约束和互操作性考量。

Conclusion: 总结了关键挑战，并展望未来标准化、模块化和高效的TN-NTN集成方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation](https://arxiv.org/abs/2507.02080)
*Yubeen Lee,Sangeun Lee,Chaewon Park,Junyeop Cha,Eunil Park*

Main category: cs.MM

TL;DR: TAGF（时间感知门控融合框架）通过动态调整递归注意力输出来提升多模态情感识别的性能，特别针对噪声和模态不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在效价-唤醒度估计中常因噪声和模态不对齐导致性能下降，需一种有效的动态融合方法。

Method: 提出TAGF框架，结合BiLSTM时间门控机制，动态学习递归步骤的重要性并整合跨模态特征。

Result: 在Aff-Wild2数据集上表现优异，显著提升对模态不对齐的鲁棒性，且可靠建模动态情感转移。

Conclusion: TAGF通过时间感知融合有效解决模态不对齐问题，为多模态情感识别提供新方法。

Abstract: Multimodal emotion recognition often suffers from performance degradation in
valence-arousal estimation due to noise and misalignment between audio and
visual modalities. To address this challenge, we introduce TAGF, a Time-aware
Gated Fusion framework for multimodal emotion recognition. The TAGF adaptively
modulates the contribution of recursive attention outputs based on temporal
dynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating
mechanism to learn the relative importance of each recursive step and
effectively integrates multistep cross-modal features. By embedding temporal
awareness into the recursive fusion process, the TAGF effectively captures the
sequential evolution of emotional expressions and the complex interplay between
modalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF
achieves competitive performance compared with existing recursive
attention-based models. Furthermore, TAGF exhibits strong robustness to
cross-modal misalignment and reliably models dynamic emotional transitions in
real-world conditions.

</details>


### [28] [VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning](https://arxiv.org/abs/2507.02626)
*Siran Chen,Boyu Chen,Chenyun Yu,Yuxiao Luo,Ouyang Yi,Lei Cheng,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.MM

TL;DR: VRAgent-R1通过结合多模态内容理解和强化学习，提升了视频推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频推荐研究依赖冻结的LLM进行模拟，难以处理多模态内容理解，导致推荐性能受限。

Method: VRAgent-R1包含两个代理：IP代理（多模态内容理解）和US代理（强化学习优化推荐）。

Result: 实验显示IP代理NDCG@10提升6.0%，US代理用户决策模拟准确率提高45.0%。

Conclusion: VRAgent-R1通过多模态和强化学习的结合，显著提升了推荐效果。

Abstract: Owing to powerful natural language processing and generative capabilities,
large language model (LLM) agents have emerged as a promising solution for
enhancing recommendation systems via user simulation. However, in the realm of
video recommendation, existing studies predominantly resort to prompt-based
simulation using frozen LLMs and encounter the intricate challenge of
multimodal content understanding. This frequently results in suboptimal item
modeling and user preference learning, thereby ultimately constraining
recommendation performance. To address these challenges, we introduce
VRAgent-R1, a novel agent-based paradigm that incorporates human-like
intelligence in user simulation. Specifically, VRAgent-R1 comprises two
distinct agents: the Item Perception (IP) Agent and the User Simulation (US)
Agent, designed for interactive user-item modeling. Firstly, the IP Agent
emulates human-like progressive thinking based on MLLMs, effectively capturing
hidden recommendation semantics in videos. With a more comprehensive multimodal
content understanding provided by the IP Agent, the video recommendation system
is equipped to provide higher-quality candidate items. Subsequently, the US
Agent refines the recommended video sets based on in-depth chain-of-thought
(CoT) reasoning and achieves better alignment with real user preferences
through reinforcement learning. Experimental results on a large-scale video
recommendation benchmark have demonstrated the effectiveness of our proposed
VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10
on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\%
higher accuracy in user decision simulation compared to state-of-the-art
baselines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: 该论文提出了一种名为SMT-Sweep的新方法，将SAT sweeping技术扩展到字级，基于SMT理论，用于处理硬件验证中的字级操作和数组语义，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着硬件验证中字级操作（如位向量运算、算术和数组）的广泛应用，传统的位级SAT sweeping技术无法满足需求，因此需要一种针对字级的简化与等价检查方法。

Method: SMT-Sweep基于SMT理论，结合随机和约束驱动的字级模拟，处理复杂的位向量操作和数组语义，扩展了传统的SAT sweeping技术。

Result: 实验结果表明，SMT-Sweep在性能上显著优于现有的位级SAT sweeping和字级单块SMT求解方法（平均提升约44倍和69倍）。

Conclusion: SMT-Sweep首次将sweeping技术引入基于SMT的硬件验证，为字级操作提供了高效的简化与等价检查工具，并已开源实现。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [30] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 论文扩展了未量化语言的满足性测试，将Tarski的初等代数片段与具有连续一阶导数的实函数相结合，提出了新的决策方法。


<details>
  <summary>Details</summary>
Motivation: 动机：为未量化语言开发更强大的满足性测试工具，扩展Tarski的初等代数以处理函数及其导数。

Method: 方法：通过预处理将公式转化为等价的初等代数无量化公式，利用Tarski的决策方法检查满足性。

Result: 结果：通过替换函数变量为实变量，证明了翻译的满足性保持性，并提供了灵活的插值函数族。

Conclusion: 结论：提出的方法能够有效处理包含函数及其导数的满足性问题，扩展了现有理论的应用范围。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [31] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 本文研究直觉主义条件逻辑，通过构建嵌套演算和序列演算，扩展了现有逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 旨在为条件推理提供构造性分析，解决直觉主义条件下‘would’和‘might’条件运算符不可互定义的问题。

Method: 将Chellas的条件逻辑CK嵌入直觉主义框架，构建嵌套演算（IntCK）和序列演算（CCKbox），并扩展为CCK逻辑。

Result: 定义了CCK的模型类和公理化，并将其结果推广到多个扩展中。

Conclusion: 研究为直觉主义条件逻辑提供了新的证明系统和模型理论框架。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


### [32] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: DHOL通过牺牲类型系统的可判定性显著提升了表达力，同时保留自动化定理证明支持，并在此基础上扩展了细化类型和商类型。


<details>
  <summary>Details</summary>
Motivation: 实践者常用但自动化定理证明器很少支持的细化类型和商类型，需要不可判定的类型系统，而DHOL的设计为此提供了可能。

Method: 将细化类型和商类型作为子类型的特例引入，避免了高成本的表示变更，并通过身份映射简化了实现。

Result: 扩展后的DHOL保持了对HOL的完整和可靠翻译，语法、语义和翻译过程均得到形式化。

Conclusion: DHOL的扩展既实用又优雅，解决了现有系统难以支持的复杂类型需求问题。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [33] [PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training](https://arxiv.org/abs/2507.02122)
*Neil K. R. Sehgal,Hita Kambhamettu,Allen Chang,Andrew Zhu,Lyle Ungar,Sharath Chandra Guntuku*

Main category: cs.HC

TL;DR: 研究人员开发了一个名为PAL的对话系统，用于模拟具有情感细微差别的患者互动，以支持缓和医疗中的沟通训练。通过混合方法研究发现，PAL有助于反思和技能提升，但也存在情感真实性和反馈适应性等局限。


<details>
  <summary>Details</summary>
Motivation: 缓和医疗中的有效沟通培训资源有限，标准化患者难以获取。

Method: 开发PAL系统，结合文本和语音模式，模拟患者互动并提供结构化反馈。通过混合方法研究评估用户参与度和可用性。

Result: 参与者认为PAL有助于反思和技能提升，但指出情感真实性和反馈适应性有待改进。

Conclusion: 大语言模型可支持缓和医疗沟通培训，未来设计需关注情感敏感性和多模态支持。

Abstract: Effective communication in serious illness and palliative care is essential
but often under-taught due to limited access to training resources like
standardized patients. We present PAL (Palliative Assisted Learning-bot), a
conversational system that simulates emotionally nuanced patient interactions
and delivers structured feedback grounded in an existing empathy-based
framework. PAL supports text and voice modalities and is designed to scaffold
clinical skill-building through repeated, low-cost practice. Through a
mixed-methods study with 17 U.S. medical trainees and clinicians, we explore
user engagement with PAL, evaluate usability, and examine design tensions
around modalities, emotional realism, and feedback delivery. Participants found
PAL helpful for reflection and skill refinement, though some noted limitations
in emotional authenticity and the adaptability of feedback. We contribute: (1)
empirical evidence that large language models can support palliative
communication training; (2) design insights for modality-aware, emotionally
sensitive simulation tools; and (3) implications for systems that support
emotional labor, cooperative learning, and AI-augmented training in high-stakes
care settings.

</details>


### [34] [StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative](https://arxiv.org/abs/2507.02156)
*Benjamin Watson,Janet Kim,Tim McEneany,Tom Moher,Claudia Hindo,Louis Gomez,Stephen Fransen*

Main category: cs.HC

TL;DR: StorySpace项目研究新界面技术如何支持高中课堂叙事活动，旨在通过设计目标激发学生反思与学习兴趣。


<details>
  <summary>Details</summary>
Motivation: 探索新界面技术在教育中的应用，特别是如何增强课堂叙事的互动性和复杂性。

Method: 通过三个设计目标实现：触发学生反思、展示主题复杂性、以及使叙事媒介本身引人入胜。

Result: 项目设计旨在通过技术手段提升课堂叙事的深度和趣味性。

Conclusion: StorySpace通过技术设计成功支持课堂叙事活动，激发学生参与和学习兴趣。

Abstract: The StorySpace project studies the role new interface technologies might play
in high school education. With this approach in mind, StorySpace is
specifically designed to support and enhance classroom narrative, an already
well-established classroom activity. StorySpace strives to achieve this through
adherence to three design goals. The first is to trigger student reflection and
interpretation. The narrative medium created by StorySpace should represent the
topic of classroom discussion and learning in all its complexity. In building
their representation, the students will then be confronted with that same
complexity. The medium should also itself be exciting and compelling, making
classroom narrative interesting and fun.

</details>


### [35] [A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy](https://arxiv.org/abs/2507.02138)
*Shan Li,Guozhu Ding*

Main category: cs.HC

TL;DR: 介绍并评估了Healthy Choice——一个基于理论、AI增强的模拟平台，用于通过交互式场景学习提升营养素养。来自114名大学生的高满意度反馈证明了其有用性和易用性。


<details>
  <summary>Details</summary>
Motivation: 通过创新技术（AI增强）提升营养素养，填补教育和实践需求。

Method: 采用理论驱动设计和AI技术，用户通过交互式场景学习，收集114名学生的用户体验数据。

Result: 定量评分显示高用户满意度，特别是在有用性和易用性方面。

Conclusion: Healthy Choice平台有效且用户友好，适合推广用于营养教育。

Abstract: This study introduces and evaluates Healthy Choice, an innovative
theory-driven and AI-enhanced simulation platform designed to cultivate
nutrition literacy through interactive scenario-based learning experiences. We
collected feedback from 114 university students with diverse backgrounds who
completed simulated product selection scenarios. Quantitative ratings of
usefulness and ease of use demonstrated high user satisfaction.

</details>


### [36] [The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future](https://arxiv.org/abs/2507.02180)
*Russell Beale*

Main category: cs.HC

TL;DR: 大语言模型自2022年广泛使用以来，对教育和教育技术产生了深远影响，改变了学习者和教育者的互动方式，并提出了未来教育技术设计的新要求。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在教育领域的应用、成功与失败案例，以及对学习和教育动态的影响。

Method: 回顾大语言模型在教育中的应用领域和使用案例，分析其改变学习范式的潜力。

Result: 大语言模型将重塑人与技术的互动方式，成为未来教育技术的默认工具。

Conclusion: 教育技术设计需适应学习者和用户的新期望，以确保大语言模型的高效应用。

Abstract: Large language Models have only been widely available since 2022 and yet in
less than three years have had a significant impact on approaches to education
and educational technology. Here we review the domains in which they have been
used, and discuss a variety of use cases, their successes and failures. We then
progress to discussing how this is changing the dynamic for learners and
educators, consider the main design challenges facing LLMs if they are to
become truly helpful and effective as educational systems, and reflect on the
learning paradigms they support. We make clear that the new interaction
paradigms they bring are significant and argue that this approach will become
so ubiquitous it will become the default way in which we interact with
technologies, and revolutionise what people expect from computer systems in
general. This leads us to present some specific and significant considerations
for the design of educational technology in the future that are likely to be
needed to ensure acceptance by the changing expectations of learners and users.

</details>


### [37] [A wireless, inexpensive optical tracker for the CAVE](https://arxiv.org/abs/2507.02682)
*Ehud Sharlin,Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 设计了一种低成本、无线的脚步追踪器，用于CAVE显示系统，提供自由移动的优势，适用于非精确视觉应用。


<details>
  <summary>Details</summary>
Motivation: 解决CAVE显示系统中传统追踪子系统有线束缚用户的问题，提供更自由的移动体验。

Method: 开发了一种低成本（<200美元）、无线、基于脚步的追踪器，精度为10 cm，采样率20 Hz。

Result: 追踪器在校园漫游中表现出色，但在需要精确视觉检查的应用中表现不足。

Conclusion: 无束缚的追踪器为CAVE系统提供了显著的移动自由，适用于特定的应用场景。

Abstract: CAVE displays offer many advantages over other virtual reality (VR) displays,
including a large, unencumbering viewing space. Unfortunately, the typical
tracking subsystems used with CAVE displays tether the user and lessen this
advantage. We have designed a simple, low-cost feet tracker that is wireless,
leaving the user free to move. The tracker can be assembled for less than $200
US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested
the prototype with two applications: a visualization supporting close visual
inspection, and a walkthrough of the campus. Although the tracking was
convincing, it was clear that the tracker's limitations make it less than ideal
for applications requiring precise visual inspection. However, the freedom of
motion allowed by the tracker was a compelling supplement to our campus
walkthrough, allowing users to stroll and look around corners.

</details>


### [38] [EvalAssist: A Human-Centered Tool for LLM-as-a-Judge](https://arxiv.org/abs/2507.02186)
*Zahra Ashktorab,Elizabeth M. Daly,Erik Miehling,Werner Geyer,Martin Santillan Cooper,Tejaswini Pedapati,Michael Desmond,Qian Pan,Hyo Jin Do*

Main category: cs.HC

TL;DR: EvalAssist是一个简化LLM评估流程的框架，提供在线标准开发环境，支持用户交互式构建、测试和分享评估标准，并利用开源库UNITXT中的提示链方法。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）输出的多样性和评估过程的复杂性，需要一个高效的工具来简化评估工作，帮助用户快速确定最佳模型和提示。

Method: EvalAssist提供在线标准开发环境，支持用户交互式构建评估标准，并利用开源库UNITXT中的提示链方法和专门训练的评估器检测危害与风险。

Result: 该系统已在组织内部部署，拥有数百名用户。

Conclusion: EvalAssist成功简化了LLM评估流程，提高了评估效率，并支持检测有害输出。

Abstract: With the broad availability of large language models and their ability to
generate vast outputs using varied prompts and configurations, determining the
best output for a given task requires an intensive evaluation process, one
where machine learning practitioners must decide how to assess the outputs and
then carefully carry out the evaluation. This process is both time-consuming
and costly. As practitioners work with an increasing number of models, they
must now evaluate outputs to determine which model and prompt performs best for
a given task. LLMs are increasingly used as evaluators to filter training data,
evaluate model performance, assess harms and risks, or assist human evaluators
with detailed assessments. We present EvalAssist, a framework that simplifies
the LLM-as-a-judge workflow. The system provides an online criteria development
environment, where users can interactively build, test, and share custom
evaluation criteria in a structured and portable format. We support a set of
LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a
prompt-chaining approach we developed and contributed to the UNITXT open-source
library. Additionally, our system also includes specially trained evaluators to
detect harms and risks in LLM outputs. We have deployed the system internally
in our organization with several hundreds of users.

</details>


### [39] [VergeIO: Depth-Aware Eye Interaction on Glasses](https://arxiv.org/abs/2507.02187)
*Xiyuxing Zhang,Duc Vu,Chengyi Shen,Yuntao Wang,Yuanchun Shi,Justin Chan*

Main category: cs.HC

TL;DR: VergeIO 是一种基于 EOG 的智能眼镜，通过优化的电极布局和新颖的原型设计，实现了深度感知的眼部交互，能区分 4-6 种深度手势，并具有高准确率和低功耗特性。


<details>
  <summary>Details</summary>
Motivation: 行业对无需侵入性设计的 EOG 眼部手势感知技术日益关注，但现有技术缺乏对深度交互的支持，VergeIO 旨在解决这一问题。

Method: 采用优化的电极布局、新颖的眼镜原型设计、个性化模型、运动伪影检测管道以及基于前导的激活方案。

Result: 在 11 名用户和 1,320 个手势实例中，准确率为 83-98%；对未校准的新用户准确率为 80-98%；系统功耗低至 3 mW。

Conclusion: VergeIO 通过创新设计和优化算法，实现了高效的深度感知眼部交互，适合持续运行。

Abstract: There is growing industry interest in creating unobtrusive designs for
electrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and
Apple eyewear). We present VergeIO, the first EOG-based glasses that enables
depth-aware eye interaction using vergence with an optimized electrode layout
and novel smart glass prototype. It can distinguish between four and six
depth-based eye gestures with 83-98% accuracy using personalized models in a
user study across 11 users and 1,320 gesture instances. It generalizes to
unseen users with an accuracy of 80-98% without any calibration. To reduce
false detections, we incorporate a motion artifact detection pipeline and a
preamble-based activation scheme. The system uses dry sensors without any
adhesives or gel, and operates in real time with 3 mW power consumption by the
sensing front-end, making it suitable for always-on sensing.

</details>


### [40] [An Exploration of Internal States in Collaborative Problem Solving](https://arxiv.org/abs/2507.02229)
*Sifatul Anindho,Videep Venkatesha,Mariah Bradford,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.HC

TL;DR: 研究通过混合方法探讨了协同问题解决（CPS）中的个体情绪状态，通过视频回顾和内省报告分析语言模式，揭示了情绪体验的多样性。


<details>
  <summary>Details</summary>
Motivation: 协同问题解决在教育和职业场景中日益重要，但个体情绪状态的研究尚不充分。

Method: 团队完成CPS任务后，个体通过视频回顾和内省报告自我描述情绪体验，并进行语言学分析。

Result: 分析发现了语言使用的独特模式，包括单/双词、关键词、情绪标签及语义相似性。

Conclusion: 研究揭示了CPS中情绪体验的多样性，为理解团队协作中的情感动态提供了新视角。

Abstract: Collaborative problem solving (CPS) is a complex cognitive, social, and
emotional process that is increasingly prevalent in educational and
professional settings. This study investigates the emotional states of
individuals during CPS using a mixed-methods approach. Teams of four first
completed a novel CPS task. Immediately after, each individual was placed in an
isolated room where they reviewed the video of their group performing the task
and self-reported their internal experiences throughout the task. We performed
a linguistic analysis of these internal monologues, providing insights into the
range of emotions individuals experience during CPS. Our analysis showed
distinct patterns in language use, including characteristic unigrams and
bigrams, key words and phrases, emotion labels, and semantic similarity between
emotion-related words.

</details>


### [41] [A framework for 3D interaction techniques](https://arxiv.org/abs/2507.02254)
*Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 本文介绍了一个用于3D交互技术的软件架构，以及一个面向对象、独立于工具包的框架。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个可扩展的工具，用于开发新的交互技术和集成特定应用代码。

Method: 通过基本过滤器连接的数据流构成交互技术，定义了一个执行模型来管理信息流。

Result: 设计了一个灵活的框架，可轻松添加新组件和交互技术。

Conclusion: 该框架为3D交互技术的开发提供了高效且可扩展的解决方案。

Abstract: This paper presents a software architecture for 3D interaction techniques
(ITs) and an object oriented, toolkit-independent framework that implements
such architecture. ITs are composed of basic filters connected in a dataflow,
where virtual input devices and objects in the scene are sources of
information. An execution model defines the general flow of information between
filters. This framework has been designed to be extensible: new information
types, new input devices, new execution models, or new interaction techniques
can easily be added. Application specific code and application specific ITs are
seamlessly integrated into this architecture.

</details>


### [42] [Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness](https://arxiv.org/abs/2507.02283)
*Tim Rogers,Ben Teehankee*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLM）可能继承并放大人类理论与实践中存在的偏差，尤其是在组织决策中可能阻碍学习的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示LLM如何吸收和复制人类的防御性推理模式，从而在实际应用中强化不利于学习的实践。

Method: 通过HR咨询案例研究，分析LLM如何在表面上专业但实质上阻碍组织学习。

Result: 研究发现LLM成功模仿人类行为但也继承了认知盲点，若用于决策可能强化反学习实践。

Conclusion: 论文提出开发能促进更具生产力学习模式的LLM，这可能同步推动AI对齐研究和人类价值实践。

Abstract: This paper examines a critical yet unexplored dimension of the AI alignment
problem: the potential for Large Language Models (LLMs) to inherit and amplify
existing misalignments between human espoused theories and theories-in-use.
Drawing on action science research, we argue that LLMs trained on
human-generated text likely absorb and reproduce Model 1 theories-in-use - a
defensive reasoning pattern that both inhibits learning and creates ongoing
anti-learning dynamics at the dyad, group, and organisational levels. Through a
detailed case study of an LLM acting as an HR consultant, we show how its
advice, while superficially professional, systematically reinforces
unproductive problem-solving approaches and blocks pathways to deeper
organisational learning. This represents a specific instance of the alignment
problem where the AI system successfully mirrors human behaviour but inherits
our cognitive blind spots. This poses particular risks if LLMs are integrated
into organisational decision-making processes, potentially entrenching
anti-learning practices while lending authority to them. The paper concludes by
exploring the possibility of developing LLMs capable of facilitating Model 2
learning - a more productive theory-in-use - and suggests this effort could
advance both AI alignment research and action science practice. This analysis
reveals an unexpected symmetry in the alignment challenge: the process of
developing AI systems properly aligned with human values could yield tools that
help humans themselves better embody those same values.

</details>


### [43] [Human-Centered Explainability in Interactive Information Systems: A Survey](https://arxiv.org/abs/2507.02300)
*Yuhao Zhang,Jiaxin An,Ben Wang,Yan Zhang,Jiqun Liu*

Main category: cs.HC

TL;DR: 本文系统综述了交互式信息系统中用户研究的可解释性进展，总结了概念化、设计和评估方法，并提出了五个维度、分类方案和测量标准。


<details>
  <summary>Details</summary>
Motivation: 为了促进透明、可信和负责任的交互信息系统发展，需要理解用户如何理解和评估AI驱动的输出。

Method: 采用PRISMA指南，从八个学术数据库中筛选100篇文章，使用结构化编码方法提取和综合分析见解。

Result: 总结了五个概念化维度、解释设计分类标准和六种用户为中心的测量维度。

Conclusion: 研究为人类中心可解释性的理论奠定了基础，并为未来设计和研究方向提出了建议。

Abstract: Human-centered explainability has become a critical foundation for the
responsible development of interactive information systems, where users must be
able to understand, interpret, and scrutinize AI-driven outputs to make
informed decisions. This systematic survey of literature aims to characterize
recent progress in user studies on explainability in interactive information
systems by reviewing how explainability has been conceptualized, designed, and
evaluated in practice. Following PRISMA guidelines, eight academic databases
were searched, and 100 relevant articles were identified. A structural encoding
approach was then utilized to extract and synthesize insights from these
articles. The main contributions include 1) five dimensions that researchers
have used to conceptualize explainability; 2) a classification scheme of
explanation designs; 3) a categorization of explainability measurements into
six user-centered dimensions. The review concludes by reflecting on ongoing
challenges and providing recommendations for future exploration of related
issues. The findings shed light on the theoretical foundations of
human-centered explainability, informing the design of interactive information
systems that better align with diverse user needs and promoting the development
of systems that are transparent, trustworthy, and accountable.

</details>


### [44] [Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation](https://arxiv.org/abs/2507.02306)
*Ruican Zhong,David W. McDonald,Gary Hsieh*

Main category: cs.HC

TL;DR: 本文提出了一种基于多模态LLM的合成启发式评估方法，用于替代传统高成本的用户体验评估，结果显示其性能优于人类评估者。


<details>
  <summary>Details</summary>
Motivation: 由于传统用户体验评估成本高且耗时，研究旨在探索LLM在图像分析和设计反馈方面的潜力，以降低评估成本并提高效率。

Method: 通过多模态LLM分析图像并提供设计反馈，对比其与专业UX从业者在两款应用中的评估表现。

Result: 合成评估在识别可用性问题上的表现（73%和77%）优于人类评估者（57%和63%），尤其在布局问题上表现出色，但在UI组件识别和跨屏幕违规检测方面较弱。

Conclusion: 合成评估在特定任务中表现优于人类，但仍需改进组件和设计惯例识别能力，为未来合成启发式评估的设计提供了参考。

Abstract: Usability evaluation is crucial in human-centered design but can be costly,
requiring expert time and user compensation. In this work, we developed a
method for synthetic heuristic evaluation using multimodal LLMs' ability to
analyze images and provide design feedback. Comparing our synthetic evaluations
to those by experienced UX practitioners across two apps, we found our
evaluation identified 73% and 77% of usability issues, which exceeded the
performance of 5 experienced human evaluators (57% and 63%). Compared to human
evaluators, the synthetic evaluation's performance maintained consistent
performance across tasks and excelled in detecting layout issues, highlighting
potential attentional and perceptual strengths of synthetic evaluation.
However, synthetic evaluation struggled with recognizing some UI components and
design conventions, as well as identifying across screen violations.
Additionally, testing synthetic evaluations over time and accounts revealed
stable performance. Overall, our work highlights the performance differences
between human and LLM-driven evaluations, informing the design of synthetic
heuristic evaluations.

</details>


### [45] [From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance](https://arxiv.org/abs/2507.02350)
*Hao Tang,Songyun Xie,Xinzhou Xie,Can Liao,Xin Zhang,Bohan Li,Zhongyu Tian,Dalu Zheng*

Main category: cs.HC

TL;DR: 论文提出了一种通过即时回忆范式的细粒度标注方法，解决了传统视频情感生理数据集中粗粒度标注带来的标签噪声问题，并通过生理证据和识别性能验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统的情感生理数据集通常采用全试次标注，这种粗粒度标注方法无法捕捉情感反应的动态性和时间局部性，导致标签噪声，限制了情感识别算法的评估和性能。

Method: 提出了一种基于即时回忆范式的细粒度标注方法，通过视频即时回放阶段，让参与者精确标注情感的时间戳、标签和强度。

Result: 生理验证显示，参与者标注的窗口内多模态信号变化与主观标注高度一致；情感识别实验中，细粒度标注训练的模型准确率比传统方法高9.7%。

Conclusion: 细粒度标注不仅解决了标签噪声问题，还证明了标注精度对情感识别性能的重要性超过了数据规模。

Abstract: Traditional video-induced emotion physiological datasets often use
whole-trial annotation, assigning a single emotion label to all data collected
during an entire trial. This coarse-grained annotation approach misaligns with
the dynamic and temporally localized nature of emotional responses as they
unfold with video narratives, introducing label noise that limits emotion
recognition algorithm evaluation and performance. To solve the label noise
problem caused by coarse-grained annotation, we propose a fine-grained
annotation method through an immediate recall paradigm. This paradigm
integrates an immediate video replay phase after the initial stimulus viewing,
allowing participants to precisely mark the onset timestamp, emotion label, and
intensity based on their immediate recall. We validate this paradigm through
physiological evidence and recognition performance. Physiological validation of
multimodal signals within participant-marked windows revealed rhythm-specific
EEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of
high-arousal versus 6% of low-arousal emotion windows. These objective
physiological data changes strongly aligned with subjective annotations,
confirming annotation precision. For recognition performance, classification
experiments showed that models trained on fine-grained annotations achieved
9.7% higher accuracy than traditional whole-trial labeling, despite using less
data. This work not only addresses label noise through fine-grained annotation
but also demonstrates that annotation precision outweighs data scale in
determining emotion recognition performance.

</details>


### [46] [Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset](https://arxiv.org/abs/2507.02432)
*Jueun Lee,Dennis Moschina,Supraja Ramesh,Tobias Röddiger,Kai Kunze,Michael Beigl*

Main category: cs.HC

TL;DR: 研究了通过音乐结构化的闭环振动模式作为被动生物反馈干预手段，用于放松和睡眠启动。通过将节奏模式编码到智能手表振动中，并使其频率略低于用户实时心率，以降低唤醒水平。结果显示短期刺激可增加副交感神经活动和放松感，但对睡眠启动阶段无明显效果。


<details>
  <summary>Details</summary>
Motivation: 探索一种非侵入性的触觉反馈方法，替代传统的听觉或开环方法，以帮助放松和睡眠启动。

Method: 使用智能手表传递节奏振动，调整频率略低于用户心率；通过两项研究（N=20和N=28）评估其对心率和主观放松感的影响。

Result: 短期振动刺激可提高副交感神经活动和放松感，但对睡眠启动阶段无明显作用。

Conclusion: 触觉反馈有助于放松，但需进一步优化以改善睡眠启动效果，为可穿戴设备的设计提供了新思路。

Abstract: We investigate the use of musically structured, closed-loop vibration
patterns as a passive biofeedback intervention for relaxation and sleep
initiation. By encoding rhythmic meter structures into smartwatch vibrations
and adapting their frequency to be slightly slower than the user's real-time
heart rate, our system aims to reduce arousal through tactile entrainment,
offering a non-invasive alternative to auditory or open-loop approaches
previously used in sleep and anxiety contexts. In the first study (N=20), we
compared five adaptive vibration rhythms for their effects on heart rate and
subjective perceptions of relaxation in a resting context. In the second study
(N=28), we evaluated the most promising pattern from Study 1 in a prolonged
sleep initiation setting. Results showed increased parasympathetic activity and
perceived relaxation during short-term stimulation, but no significant effects
on sleep-related measures during the sleep onset phase. This work contributes
to the understanding of how wearable haptic feedback can support relaxation and
sleep, offering design insights and identifying methodological considerations
for effectively integrating haptic interaction into self-directed
interventions.

</details>


### [47] [Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?](https://arxiv.org/abs/2507.02453)
*Jueun Lee,Martin Flipe,Philipp Lepold,Tobias Röddiger,Michael Beigl*

Main category: cs.HC

TL;DR: 研究探讨了根据实时心率调整的触觉生物反馈对身体不同部位（手腕、手、前臂、肩部）的影响，发现前臂和肩部最适合放松反馈。


<details>
  <summary>Details</summary>
Motivation: 当前触觉干预主要集中在压力场景，缺乏对休息状态下动态生物反馈和身体位置的考量。

Method: 通过实时心率调整触觉反馈，比较四个身体部位的放松效果，测量心率、α波活动、主观休息感和振动体验。

Result: 手腕、肩部和前臂的生物反馈降低了心率；前臂和肩部的主观休息感和舒适度最高。

Conclusion: 前臂和肩部是休息状态下非侵入性触觉反馈的理想选择，手腕需要设计改进。

Abstract: Wearable haptic interventions offer promising support for relaxation through
slow, vibrotactile biofeedback. Despite their potential, current applications
focus on stress-inducing procedures and fixed vibration patterns, with limited
consideration of body location and dynamic biofeedback during restful states.
This study investigates the effects of haptic biofeedback adjusted from
real-time heart rate during eyes-closed wakeful rest, comparing four wearable
body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave
activity on the ear, subjective restfulness, and vibration experience were
measured across these conditions. Results show that biofeedback reduced heart
rate at the wrist, shoulder, and forearm, while alpha power measured at the ear
remained unchanged. Subjective restfulness was rated highest at the shoulder
and forearm, which were also the most preferred locations. In addition,
participants reported greater comfort, relaxation, and further increased
sleepiness at the forearm compared to the wrist, which was more easily
recognizable. These findings suggest that the forearm and shoulder are ideal
for unobtrusive relaxation feedback for wakeful rest, while the wrist may
require design improvements for subjective experience.

</details>


### [48] [Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue](https://arxiv.org/abs/2507.02537)
*Paulo Ricardo Knob,Leonardo Scholler,Juliano Rigatti,Soraia Raupp Musse*

Main category: cs.HC

TL;DR: 论文研究了大型语言模型（LLM）在生成情感丰富的对话中的表现，通过情感分析和专家评估发现，虽然模型能模仿情感结构，但人类评价显示出共情和连贯性的差异。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理在日常互动中的普及，情感智能（尤其是共情倾听）的需求日益增长。

Method: 使用ChatGPT和Gemini扩展手动制作的共情对话数据集，并通过VADER情感分析和专家评估分析情感进展。

Result: 生成的对话在情感结构上符合预期，但在共情和连贯性上与人类评价存在差异。

Conclusion: 情感建模不仅需要结构对齐，还需定性深度，强调在开发情感智能代理时需结合自动化和人为中心的方法。

Abstract: Conversational agents have made significant progress since ELIZA, expanding
their role across various domains, including healthcare, education, and
customer service. As these agents become increasingly integrated into daily
human interactions, the need for emotional intelligence, particularly
empathetic listening, becomes increasingly essential. In this study, we explore
how Large Language Models (LLMs) respond when tasked with generating
emotionally rich interactions. Starting from a small dataset manually crafted
by an expert to reflect empathic behavior, we extended the conversations using
two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the
dialogues using both sentiment analysis (via VADER) and expert assessments.
While the generated conversations often mirrored the intended emotional
structure, human evaluation revealed important differences in the perceived
empathy and coherence of the responses. These findings suggest that emotion
modeling in dialogues requires not only structural alignment in the expressed
emotions but also qualitative depth, highlighting the importance of combining
automated and humancentered methods in the development of emotionally competent
agents.

</details>


### [49] [Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots](https://arxiv.org/abs/2507.02745)
*Zahra Ashktorab,Alessandra Buccella,Jason D'Cruz,Zoe Fowler,Andrew Gill,Kei Yan Leung,P. D. Magnus,John Richards,Kush R. Varshney*

Main category: cs.HC

TL;DR: 研究探讨了AI聊天机器人在不同错误情境下使用三种道歉方式（机械、解释性和共情）的效果，发现解释性道歉更受欢迎，但情境和用户差异影响偏好。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的聊天机器人广泛应用，其通过有效道歉恢复用户信任的能力至关重要。

Method: 采用预注册研究设计，162名参与者在三种错误情境（偏见、虚构和事实错误）中评估三种道歉方式的偏好。

Result: 解释性道歉整体更受欢迎，但在偏见情境中共情道歉更受青睐；虚构错误情境下用户偏好不明显。

Conclusion: AI系统的有效道歉需考虑情境和用户差异，未来需个性化设计以修复信任。

Abstract: As chatbots driven by large language models (LLMs) are increasingly deployed
in everyday contexts, their ability to recover from errors through effective
apologies is critical to maintaining user trust and satisfaction. In a
preregistered study with Prolific workers (N=162), we examine user preferences
for three types of apologies (rote, explanatory, and empathic) issued in
response to three categories of common LLM mistakes (bias, unfounded
fabrication, and factual errors). We designed a pairwise experiment in which
participants evaluated chatbot responses consisting of an initial error, a
subsequent apology, and a resolution. Explanatory apologies were generally
preferred, but this varied by context and user. In the bias scenario, empathic
apologies were favored for acknowledging emotional impact, while
hallucinations, though seen as serious, elicited no clear preference,
reflecting user uncertainty. Our findings show the complexity of effective
apology in AI systems. We discuss key insights such as personalization and
calibration that future systems must navigate to meaningfully repair trust.

</details>


### [50] [Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding](https://arxiv.org/abs/2507.02800)
*Ebrahim Feghhi,Shreyas Kaasyap,Nima Hadidi,Jonathan C. Kao*

Main category: cs.HC

TL;DR: 该论文提出了三项改进，以提升语音神经假体的解码效率与实时性，包括时间掩码训练、轻量级Transformer架构及测试时自适应方法，显著降低了错误率和计算成本。


<details>
  <summary>Details</summary>
Motivation: 语音神经假体旨在通过神经活动解码直接恢复严重瘫痪者的沟通能力，但现有方法的精度提升伴随高计算成本，且未验证实时性。因此，研究目标是实现高精度、高效率的实时神经语音解码。

Method: 1. 在训练中引入大量时间掩码（平均掩码50%以上）；2. 用轻量级Transformer替代基准的GRU架构，减少77%参数和36%内存占用；3. 设计轻量级测试时自适应方法，通过单次梯度步优化模型。

Result: 改进方法将单词错误率降低19.5%，显著减轻了跨天性能下降问题，并在实时解码中显著降低了计算成本。

Conclusion: 通过结合时间掩码、轻量Transformer及高效自适应方法，论文成功提升了神经语音解码的准确性、效率和实时性，为临床应用铺平了道路。

Abstract: Speech neuroprostheses aim to restore communication for people with severe
paralysis by decoding speech directly from neural activity. To accelerate
algorithmic progress, a recent benchmark released intracranial recordings from
a paralyzed participant attempting to speak, along with a baseline decoding
algorithm. Prior work on the benchmark showed impressive accuracy gains.
However, these gains increased computational costs and were not demonstrated in
a real-time decoding setting. Here, we make three contributions that pave the
way towards accurate, efficient, and real-time neural speech decoding. First,
we incorporate large amounts of time masking during training. On average, over
$50\%$ of each trial is masked. Second, we replace the gated recurrent unit
(GRU) architecture used in the baseline algorithm with a compact Transformer.
The Transformer architecture uses $77\%$ fewer parameters, cuts peak GPU memory
usage by $36\%$ relative, and is significantly faster to calibrate relative to
the GRU. Third, we design a lightweight variant of an existing test-time
adaptation method developed for decoding handwriting from neural activity. Our
variant adapts the model using multiple time masked augmentations of a single
trial and requires only one gradient step per trial. Together, these
contributions reduce word error rate by $19.5\%$ and effectively mitigate
performance degradations across held-out days in a real-time decoding setting
while substantially lowering computational costs.

</details>


### [51] [Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks](https://arxiv.org/abs/2507.02819)
*Luke Guerdan,Devansh Saxena,Stevie Chancellor,Zhiwei Steven Wu,Kenneth Holstein*

Main category: cs.HC

TL;DR: 数据科学家如何将模糊概念转化为具体的代理目标变量，通过访谈发现其过程为拼凑式迭代，需满足有效性、简单性等五个标准。


<details>
  <summary>Details</summary>
Motivation: 研究数据科学家在预测建模中如何将模糊、难以定义的概念转化为具体目标变量，填补这一过程的认知空白。

Method: 访谈15位来自教育和医疗领域的数据科学家，分析他们构建目标变量的过程。

Result: 数据科学家通过拼凑式迭代满足五个标准，并灵活使用问题重构策略。

Conclusion: 未来研究应更好支持目标变量构建的艺术与科学，推动HCI、CSCW和ML领域的发展。

Abstract: Data scientists often formulate predictive modeling tasks involving fuzzy,
hard-to-define concepts, such as the "authenticity" of student writing or the
"healthcare need" of a patient. Yet the process by which data scientists
translate fuzzy concepts into a concrete, proxy target variable remains poorly
understood. We interview fifteen data scientists in education (N=8) and
healthcare (N=7) to understand how they construct target variables for
predictive modeling tasks. Our findings suggest that data scientists construct
target variables through a bricolage process, involving iterative negotiation
between high-level measurement objectives and low-level practical constraints.
Data scientists attempt to satisfy five major criteria for a target variable
through bricolage: validity, simplicity, predictability, portability, and
resource requirements. To achieve this, data scientists adaptively use problem
(re)formulation strategies, such as swapping out one candidate target variable
for another when the first fails to meet certain criteria (e.g.,
predictability), or composing multiple outcomes into a single target variable
to capture a more holistic set of modeling objectives. Based on our findings,
we present opportunities for future HCI, CSCW, and ML research to better
support the art and science of target variable construction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [52] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: GBake是一种用于从高斯散射场景中烘焙反射探针的工具，使传统3D网格在Unity游戏引擎中实现真实反射映射。


<details>
  <summary>Details</summary>
Motivation: 由于3D高斯基元将光照和几何共同编码为外观，直接插入高斯混合中的网格会出现光照不匹配问题，需要解决。

Method: 引入GBake工具，烘焙高斯散射场景的反射探针，用于Unity中的网格反射映射。

Result: 实现了传统3D网格在高斯散射环境中的真实反射效果。

Conclusion: GBake成功解决了高斯散射环境中网格光照不一致的问题。

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [53] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 本文提出了一种高效的基于图像的光照方法，用于处理具有闪烁效果的材料，支持动态材质属性和环境贴图。该方法结合实时闪烁渲染和标准环境贴图滤波技术，实现了快速的每帧处理。


<details>
  <summary>Details</summary>
Motivation: 基于图像的光照技术在再现真实光照效果时，对闪烁材料的处理尤为复杂。本文旨在解决这一问题，提供高效且动态的解决方案。

Method: 通过分区处理环境贴图，利用正态分布函数滤波，结合双门控高斯近似二项分布的方法，实现高效滤波和采样。

Result: 实验表明，该方法在各种材质和光照条件下接近真实渲染效果，性能稳定且开销低。

Conclusion: 本文的方法在实时渲染闪烁材料方面表现出色，仅需双倍内存开销即可支持动态环境贴图。

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算（HPC）集群，专为LLM训练等高级工作负载设计，全球TOP500排名第49，采用全开放网络栈和SONiC操作系统。


<details>
  <summary>Details</summary>
Motivation: 开发SAKURAONE旨在提供高性能计算资源，支持大规模AI工作负载，并验证开放技术在大规模HPC基础设施中的可行性。

Method: SAKURAONE由100个计算节点组成，每个节点配备8个NVIDIA H100 GPU，采用800GbE全双工网络和全闪存Lustre存储系统，优化了数据访问和通信性能。

Result: 在HPL基准测试中达到33.95 PFLOP/s，HPCG测试中396.295 TFLOP/s，FP8精度下AI应用测试中达到339.86 PFLOP/s。

Conclusion: SAKURAONE展示了开放技术在高性能计算中的竞争力，为大规模AI和HPC工作负载提供了高效解决方案。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [55] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: 论文比较了基于轮询（PCM）和基于信号（SCM）的容器监控方法，通过实验证明SCM在检测速度和可用性上的优势，并提出在编排器中引入SCM功能。


<details>
  <summary>Details</summary>
Motivation: 微服务的高可用性需求驱动了对容器监控方法的改进研究，尤其是避免轮询方法的缺陷。

Method: 设计了基于信号的容器监控（SCM）方法，并在Kubernetes中实现，通过SockShop基准测试比较SCM与PCM的性能。

Result: SCM在失败检测上比PCM快86%，避免错误检测，且资源开销有限。PCM降低服务可用性4%。

Conclusion: 研究表明SCM优于PCM，建议在编排器中引入SCM以提升性能和可用性。

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [56] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 提出了一种基于迁移学习的智能故障根因识别算法，通过共享特征提取和领域对抗机制提升模型性能，适用于复杂云计算环境。


<details>
  <summary>Details</summary>
Motivation: 解决云计算环境中因系统结构复杂、服务耦合度高和故障信息有限导致的故障根因识别难题。

Method: 采用迁移学习算法，结合共享特征提取模块和领域对抗机制，并引入伪标签选择策略以增强少数类识别能力。

Result: 实验表明，该方法在准确率、F1-Score和AUC等指标上优于主流方法，尤其在标签稀缺和类不平衡条件下表现稳健。

Conclusion: 所提算法在复杂云计算系统中具有高效性和实用价值，尤其在极端不平衡和异构环境下仍保持高性能。

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [57] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 论文提出了一个名为Flotilla的可扩展轻量级联邦学习框架，解决了现有框架在真实边缘硬件部署、异步聚合和容错能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着移动和边缘计算的进步以及对数据隐私的关注增加，联邦学习（FL）作为一种隐私保护的分布式机器学习方法迅速流行。然而，现有框架多集中于通过伪分布式模拟验证FL的学习特性，而缺乏对真实边缘硬件部署和系统层面联邦特性的支持。此外，现有框架通常不支持异步聚合，且对客户端和服务器故障的容错能力有限。

Method: Flotilla采用“用户优先”的模块化设计，支持快速组合各种同步和异步FL策略，同时与DNN架构无关。它使用无状态客户端和分离会话状态的服务器设计，并定期或增量式地检查点化。

Result: Flotilla展示了五种不同FL策略的训练效果，并在200多个客户端上验证了其容错能力，能够在几秒内快速恢复。与现有框架（Flower、OpenFL和FedML）相比，Flotilla在资源使用上表现相当或更好，并且在1000+客户端情况下扩展性显著优于Flower。

Conclusion: Flotilla作为一个创新FL策略的构建、统一比较、快速部署以及系统研究和优化的候选框架，表现出强大的竞争力和实用性。

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [58] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: 瑞士国家超级计算中心（CSCS）开发了Alps，这是一个新型高性能计算基础设施，通过独立端点和高速网络设计，解决了传统HPC架构的灵活性和可组合性问题。


<details>
  <summary>Details</summary>
Motivation: 科学需求的多样性暴露出传统HPC架构缺乏灵活性和可组合性的局限，促使CSCS开发新一代HPC基础设施。

Method: Alps采用异构硬件（CPU、GPU）和高性能Slingshot网络，结合模块化存储系统和软件定义的集群技术（vCluster），抽象基础设施和服务管理。

Result: Alps目前支持多种科学领域，如数值天气预报和人工智能研究。

Conclusion: Alps的设计为科学计算提供了更高的灵活性和可组合性，满足了多样化的科学需求。

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [59] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: FlowSpec是一个基于流水线并行树结构的推测解码框架，旨在提升边缘网络中大型语言模型的分布式推理效率。


<details>
  <summary>Details</summary>
Motivation: 边缘网络中稀疏的推理请求导致现有的流水线方法利用率低下，限制了分布式推理的效率。

Method: FlowSpec引入了三种机制：基于分数的逐步验证、高效的草稿管理和动态草稿扩展策略，以优化解码过程。

Result: 实验表明，FlowSpec在多种模型和配置下显著提升了推理速度，比基线快1.36倍到1.77倍。

Conclusion: FlowSpec通过结合多项技术，有效提升了流水线利用率和推测解码效率，为边缘计算中的LLM推理提供了高效解决方案。

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Template-Based Schema Matching of Multi-Layout Tenancy Schedules:A Comparative Study of a Template-Based Hybrid Matcher and the ALITE Full Disjunction Model](https://arxiv.org/abs/2507.02020)
*Tim Uilkema,Yao Ma,Seyed Sahand Mohammadi Ziabari,Joep van Vliet*

Main category: cs.DB

TL;DR: 提出了一种基于模板的混合模式匹配器，用于将多布局租期表对齐到目标模式，显著提高了数据集成效率。


<details>
  <summary>Details</summary>
Motivation: 解决房地产公司租期表格式不标准导致的集成效率低下问题，现有方法虽完整但存在模式臃肿和业务可用性差的问题。

Method: 结合模式（Jaccard、Levenshtein）和实例（数据类型、分布）度量，通过匈牙利算法确定全局最优分配。

Result: 实验显示F1分数峰值达0.881，空值率45.7%，优于ALITE的0.712和75.6%空值率。

Conclusion: 结合业务知识的混合匹配能生成更可用且业务对齐的模式映射，未来可扩展至复杂表格处理。

Abstract: The lack of standardized tabular formats for tenancy schedules across real
estate firms creates significant inefficiencies in data integration. Existing
automated integration methods, such as Full Disjunction (FD)-based models like
ALITE, prioritize completeness but result in schema bloat, sparse attributes
and limited business usability. We propose a novel hybrid, template-based
schema matcher that aligns multi-layout tenancy schedules to a predefined
target schema. The matcher combines schema (Jaccard, Levenshtein) and
instance-based metrics (data types, distributions) with globally optimal
assignments determined via the Hungarian Algorithm. Evaluation against a
manually labeled ground truth demonstrates substantial improvements, with grid
search optimization yielding a peak F1-score of 0.881 and an overall null
percentage of 45.7%. On a separate ground truth of 20 semantically similar
column sets, ALITE achieves an F1-score of 0.712 and 75.6% nulls. These results
suggest that combining structured business knowledge with hybrid matching can
yield more usable and business-aligned schema mappings. The approach assumes
cleanly extracted tabular input, future work could explore extending the
matcher to support complex, composite tables.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [61] [Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions](https://arxiv.org/abs/2507.02067)
*Nikolaos Papanikolaou,Doha Touhafi,Jurgen Vandendriessche,Danial Karimi,Sohail Fatimi,Gianluca Cornetta,Abdellah Touhafi*

Main category: cs.AR

TL;DR: 印刷传感器通过创新印刷技术实现灵活、低成本、高度定制化的传感设备，适用于多种环境和监测应用。


<details>
  <summary>Details</summary>
Motivation: 开发灵活、低成本且可高度定制的传感器技术，以满足环境监测和保护的需求。

Method: 利用先进的印刷技术制造传感器，适用于检测污染物、温度、湿度等多种环境参数。

Result: 印刷传感器在检测环境参数时表现出高灵敏度和准确性。

Conclusion: 印刷传感器为环境监测和保护提供了一种高效、灵活的解决方案。

Abstract: Printed sensors represent a transformative advancement in sensor technology,
utilizing innovative printing techniques to create flexible, cost-effective,
and highly customizable sensing devices. Their versatility allows integration
into numerous applications across diverse fields such as monitoring a wide
range of environmental factors e.g. air and water quality, soil conditions, and
atmospheric changes among others. These sensors demonstrate high sensitivity
and accuracy in detecting pollutants, temperature variations, humidity levels,
and other critical parameters essential for environmental assessment and
protection.

</details>


### [62] [Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting](https://arxiv.org/abs/2507.02164)
*Ruibai Tang,Chengbin Quan*

Main category: cs.AR

TL;DR: 本文提出了一种基于FPGA硬件加速的算法，用于高效绘制复杂函数根的密度图，通过多项式逼近和单位移QR迭代求解根，实现了比CPU更高的能效。


<details>
  <summary>Details</summary>
Motivation: 解决复杂函数根的求解和可视化问题，尤其是在理论和应用领域中计算密集型的挑战，需要更高效的算法。

Method: 采用多项式逼近复杂函数，利用单位移QR迭代求解根，并结合Hessenberg结构的伴随矩阵和Givens旋转优化的QR分解，设计了流水线式FPGA架构。

Result: 实现了高达65倍于CPU的能效提升，尽管在性能上仍落后于现代GPU。

Conclusion: 该硬件加速算法为复杂函数根的密度图绘制提供了一种高能效的解决方案，但仍需在性能上进一步优化以追赶GPU。

Abstract: Solving and visualizing the potential roots of complex functions is essential
in both theoretical and applied domains, yet often computationally intensive.
We present a hardware-accelerated algorithm for complex function roots density
graph plotting by approximating functions with polynomials and solving their
roots using single-shift QR iteration. By leveraging the Hessenberg structure
of companion matrices and optimizing QR decomposition with Givens rotations, we
design a pipelined FPGA architecture capable of processing a large amount of
polynomials with high throughput. Our implementation achieves up to 65x higher
energy efficiency than CPU-based approaches, and while it trails modern GPUs in
performance due to differences in fabrication technique.

</details>


### [63] [System-performance and cost modeling of Large Language Model training and inference](https://arxiv.org/abs/2507.02456)
*Wenzhe Guo,Joyjit Kundu,Uras Tos,Weijiang Kong,Giuliano Sisto,Timon Evenblij,Manu Perumkunnil*

Main category: cs.AR

TL;DR: 该论文提出了一种性能-成本建模方法，用于优化大型语言模型（LLM）的训练和推理，结合了最新的计算、内存和通信技术，以解决内存带宽和计算瓶颈问题，并分析了不同网络拓扑和通信算法的性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和复杂性的指数增长，计算能力、内存带宽、网络性能和成本效率的进步落后，导致分布式系统中的可扩展性面临重大挑战。

Method: 论文提出了一个性能-成本建模框架，整合了闪存注意力技术和专家混合模型等创新，同时考虑了网络拓扑和并行通信算法的影响，并引入了小芯片成本模型。

Result: 该建模方法为未来的计算系统设计提供了有价值的指导，并通过分析不同系统架构的性能-成本权衡，促进了硬件软件的协同开发。

Conclusion: 该研究为优化LLM的可扩展性和成本提供了实用工具，推动了AI系统的高效设计和实现。

Abstract: Large language models (LLMs), based on transformer architectures, have
revolutionized numerous domains within artificial intelligence, science, and
engineering due to their exceptional scalability and adaptability. However, the
exponential growth in LLM size and complexity has outpaced advancements in
compute capacity, memory bandwidth, network performance, and cost efficiency,
posing significant challenges to their scalability on distributed systems. To
address these limitations, alternative model architectures, optimization
strategies, communication-aware network topologies, and novel system design
approaches have been proposed in literature. This paper introduces a
performance-cost modeling methodology for LLM training and inference that
integrates state-of-the-art compute techniques with memory optimizations, and
latest communication techniques. Building on an analytical performance model,
our approach incorporates recent innovations such as the flash attention
technique and mixture of experts models to address the memory bandwidth and
compute bottlenecks. It also considers the impact of different network
topologies and topology-specific communication algorithms with 5D parallellism.
The framework also integrates a chiplet cost model. The proposed modeling
methodology provides valuable insights to guide future compute system design
and facilitates hardware-software co-development, in particular due to its
ability to analyze performance-cost trade-offs for various system architectural
configurations.

</details>


### [64] [AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models](https://arxiv.org/abs/2507.02598)
*Chenhao Xue,Kezhi Li,Jiaxing Zhang,Yi Ren,Zhengyuan Shi,Chen Zhang,Yibo Lin,Lining Zhang,Qiang Xu,Guangyu Sun*

Main category: cs.AR

TL;DR: 提出了一种基于条件扩散模型的算术电路优化框架AC-Refiner，通过将电路合成视为条件图像生成任务，显著提升设计质量。


<details>
  <summary>Details</summary>
Motivation: 优化算术电路设计空间大、物理约束复杂，现有深度学习方法难以高效探索高潜力设计变体。

Method: 使用条件扩散模型，将电路合成任务重定义为条件图像生成，并通过设计结果微调模型，聚焦Pareto前沿。

Result: 实验显示AC-Refiner生成的电路设计在Pareto最优性上优于现有方法，实际应用验证了其性能提升。

Conclusion: AC-Refiner通过创新的条件扩散模型框架，有效解决了算术电路优化的挑战，展示了显著性能优势。

Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental
components of digital systems, directly impacting the performance, power
efficiency, and area footprint. However, optimizing these circuits remains
challenging due to the vast design space and complex physical constraints.
While recent deep learning-based approaches have shown promise, they struggle
to consistently explore high-potential design variants, limiting their
optimization efficiency. To address this challenge, we propose AC-Refiner, a
novel arithmetic circuit optimization framework leveraging conditional
diffusion models. Our key insight is to reframe arithmetic circuit synthesis as
a conditional image generation task. By carefully conditioning the denoising
diffusion process on target quality-of-results (QoRs), AC-Refiner consistently
produces high-quality circuit designs. Furthermore, the explored designs are
used to fine-tune the diffusion model, which focuses the exploration near the
Pareto frontier. Experimental results demonstrate that AC-Refiner generates
designs with superior Pareto optimality, outperforming state-of-the-art
baselines. The performance gain is further validated by integrating AC-Refiner
into practical applications.

</details>


### [65] [Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure](https://arxiv.org/abs/2507.02654)
*Rui Xie,Asad Ul Haq,Yunhua Fang,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 论文提出了一种系统级方法来降低HBM成本，通过取消片上ECC并将故障管理转移到内存控制器，结合特定领域的ECC框架，在保持高性能的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: HBM的高成本限制了其在AI领域的规模化部署，部分原因是严格的片上可靠性要求，因此需要创新方法降低成本。

Method: 引入领域特定的ECC框架，结合大码字Reed-Solomon校正、轻量级细粒度CRC检测、差分奇偶更新和可调节保护机制。

Result: 即使在较高比特错误率（10^-3）下，系统仍能保持78%的吞吐量和97%的模型精度。

Conclusion: 通过将可靠性作为可调系统参数而非固定硬件约束，该设计为低成本高性能HBM在AI基础设施中的应用开辟了新路径。

Abstract: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy
efficiency for AI workloads, but its high cost per bit, driven in part by
stringent on-die reliability requirements, poses a growing barrier to scalable
deployment. This work explores a system-level approach to cost reduction by
eliminating on-die ECC and shifting all fault management to the memory
controller. We introduce a domain-specific ECC framework combining
large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC
detection, differential parity updates to mitigate write amplification, and
tunable protection based on data importance. Our evaluation using LLM inference
workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the
system retains over 78\% of throughput and 97\% of model accuracy compared with
systems equipped with ideal error-free HBM. By treating reliability as a
tunable system parameter rather than a fixed hardware constraint, our design
opens a new path toward low-cost, high-performance HBM deployment in AI
infrastructure.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [66] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的解决方案，用于检测多种作物的疾病，通过整合17种作物和34种疾病的数据集，实现了99%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 印度作为农业大国，作物疾病导致严重损失。早期准确检测疾病对提高产量和食品安全至关重要。

Method: 创建包含17种作物和34种疾病的数据集，训练深度学习模型，并与其他先进方法对比。

Result: 模型准确率达99%，较现有方法（覆盖14种作物和26种疾病）提升了7%。

Conclusion: 提出的解决方案能更好帮助印度农民，覆盖更多作物和疾病类型。

Abstract: India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [67] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种高效的网格表示方法，通过自回归方式生成多边形网格，减少冗余顶点标记50%，达到约22%的压缩率，并提升几何完整性。


<details>
  <summary>Details</summary>
Motivation: 现有网格标记化方法因重复顶点标记导致网络能力浪费，因此需要一种更高效的表示方法。

Method: Mesh Silksong通过每个顶点仅访问一次的方式标记化网格，减少序列冗余。

Result: 实现了约22%的压缩率，生成的多边形网格具有优越的几何特性（如流形拓扑、水密检测等）。

Conclusion: 实验证明该方法不仅能生成复杂网格，还显著提升了几何完整性，具有实际应用价值。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [68] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 该论文介绍了HyperGaussians，一种基于3D高斯泼溅的新型扩展技术，用于高质量可动画化面部虚拟形象的创建，通过高维多变量高斯表示提升细节表现力并解决计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯泼溅在静态面部渲染中表现优异，但在处理非线性变形、复杂光照和细节时仍有局限，尤其是在可动画化面部虚拟形象创建中。论文旨在通过扩展3D高斯表示，提升其表达能力。

Method: 提出HyperGaussians，将3D高斯扩展为高维多变量高斯，通过可学习局部嵌入增强表现力。为解决计算效率问题，采用‘逆协方差技巧’重新参数化协方差矩阵。

Result: 在4个面部数据集上的19位受试者评估表明，HyperGaussians在数值和视觉上优于3D高斯泼溅，尤其在高频细节（如眼镜框、牙齿、复杂面部动作和镜面反射）表现突出。

Conclusion: HyperGaussians作为一种新型高斯表示方法，显著提升了可动画化面部虚拟形象的质量和细节表现力，同时通过高效计算设计实现了无缝集成。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [69] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种新颖的流水线，将室内环境的RGB-D扫描转换为紧凑、逼真且可交互的3D虚拟副本。


<details>
  <summary>Details</summary>
Motivation: 旨在重建视觉上接近现实且支持图形流水线关键功能的3D场景。

Method: 通过场景理解、3D资产检索、材质增强和物理模拟等步骤实现。

Result: 生成的场景紧凑、可编辑，并在Scan2CAD基准测试中表现优异。

Conclusion: LiteReality适用于AR/VR、游戏、机器人和数字孪生等领域。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [70] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 论文提出了一种自蒸馏方法，通过模拟电影语言变化来改进视频到音频生成模型的性能，尤其是在声音与部分视觉信息关联的场景中。


<details>
  <summary>Details</summary>
Motivation: 当前视频到音频生成方法忽视了电影语言这一关键艺术表达元素，导致在部分视觉可见场景中性能下降。

Method: 采用自蒸馏方法，通过模拟电影语言变化，训练学生模型对齐视频特征与相同音视频对应关系。

Result: 该方法在部分可见性和大规模V2A数据集VGGSound上均取得了显著提升。

Conclusion: 提出的自蒸馏方法有效提升了视频到音频生成模型在电影语言场景中的性能。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [71] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 通过FINN架构在FPGA上部署量化ANN模型，显著提升机器人检测速度和任务执行效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人因低速扫描和低帧率摄像头导致的任务执行时间延长问题，利用FPGA加速检测算法。

Method: 使用FINN架构在FPGA的PL部分部署三种量化ANN模型（MobileNet v1-4位、CNV-2位和BNN-1位），并在RG2C数据集上训练。

Result: MobileNet v1表现最佳，实现了98%的成功率和6611 FPS的推理速度。

Conclusion: FPGA加速ANN可显著提升性能，适用于注意力机制等实时任务。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [72] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文探讨了高质标签数据短缺问题，提出人类标签变异(HLV)作为有用信号，并分析了主动学习(AL)中简化假设的不足。


<details>
  <summary>Details</summary>
Motivation: 解决标注数据中标签变异(LV)的问题，尤其是人类标签变异(HLV)，以优化标注预算和模型训练。

Method: 分解标签变异为信号(如HLV)和噪声(如标注错误)，提出HLV感知的主动学习框架。

Result: 提出了一个结合HLV的主动学习概念框架，包括实例选择、标注者选择和标签表示。

Conclusion: 研究为考虑HLV的主动学习提供了理论基础，更贴近实际标注的复杂性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [73] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 论文分析了电子支付平台中验证规则的脆弱性，并提出了一种系统性方法来识别和修复这些缺陷以提高数据安全性。


<details>
  <summary>Details</summary>
Motivation: 由于电子支付平台处理的高交易量和潜在的高额金融损失，现有手动构建的验证规则缺乏系统性，容易受到恶意攻击，因此需要一种更稳健的方法。

Method: 论文提出了一种系统性的方法来识别和修复验证规则中的缺陷，以确保其对抗漏洞的鲁棒性。

Result: 通过系统性分析，发现了现有验证规则的不完善之处，并提出了改进方法以减少恶意请求绕过规则的风险。

Conclusion: 系统性方法可以显著提升验证规则的鲁棒性，从而增强电子支付平台的数据安全性。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [74] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 利用区块链和物联网技术监控餐厅披萨生产，提升透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决披萨生产过程中的数据透明性和安全性问题，确保食品质量。

Method: 集成IoT设备实时监测温湿度，区块链技术保障数据安全，树莓派处理数据和智能合约。

Result: 实验显示原料管理改善、浪费减少和厨房效率提升。

Conclusion: 区块链和物联网的结合为食品生产监控提供了有效解决方案。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [75] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 论文提出了一种上下文感知的攻击数据生成器，用于生成高质量的攻击数据，以解决车载网络安全中数据稀缺问题，并通过IDS案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于安全、成本和伦理限制，真实攻击数据稀缺，需要高效生成模拟攻击数据以支持入侵检测系统的开发。

Method: 使用参数化攻击模型，结合CAN消息解码和攻击强度调整，生成多种攻击类型的模拟数据。

Result: 生成的攻击数据在IDS模型中表现出高检测和分类能力，验证了数据的有效性和一致性。

Conclusion: 该方法高效、可扩展，生成的数据能有效支持IDS开发，并提供了关于数据真实性和应用的实际见解。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文探讨了信念修正的研究现状，指出现有方法缺乏对能力的分析，强调修订机制需要具备多样性能力才能应对不同应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域多关注基于公设的语法表征，而忽视了修订机制的多样性和灵活性，论文旨在填补这一空白。

Method: 通过分析现有修订机制（如词典序、自然、激进等）的能力，探讨它们在满足不同应用需求时的表现。

Result: 论文证明了不同修订机制具备特定能力（如塑性、均衡、教条等），但各有局限性。

Conclusion: 信念修正机制应根据应用场景的需求选择，未来研究应更关注机制的多样性和灵活性。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [77] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 该论文提出了一种基于AI代理的方法，结合人为干预（HITL），用于硬件设计验证，显著提高了效率并减少了时间。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路（IC）设计复杂且验证过程耗时，作者希望通过利用大型语言模型（LLMs）和生成式AI（GenAI）来优化硬件设计验证流程。

Method: 采用AI代理与人为干预（HITL）结合的方式，动态、迭代地进行端到端的硬件设计与验证。

Result: 在五个开源设计上测试，实现了超过95%的覆盖率，同时减少了验证时间，表现出卓越的性能、适应性和可配置性。

Conclusion: 该方法通过AI代理与人为干预的结合，显著提升了硬件设计验证的效率和效果。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [78] [A formal specification of the desired software behaviour of the Princess Marijke lock complex](https://arxiv.org/abs/2507.02721)
*Jan Friso Groote,Matthias Volk*

Main category: eess.SY

TL;DR: 这篇论文详细描述了荷兰Princess Marijke锁控系统的软件控制，用不到400行mCRL2代码精确建模，并通过模型检查验证了53项软件需求的正确性，为系统构建提供了蓝图。


<details>
  <summary>Details</summary>
Motivation: 确保锁控系统的安全控制对防洪和船舶运营至关重要，需要用形式化方法验证其正确性。

Method: 使用mCRL2代码对锁控系统进行形式化描述，并通过模型检查验证软件需求。

Result: 验证了53项软件需求的正确性，表明形式化描述行为无误且无疏漏。

Conclusion: 该形式化描述可作为锁控系统软件构建的可靠蓝图，显著减少潜在错误。

Abstract: The Princess Marijke lock complex is a large lock and water-protection
installation in the Netherlands between the river Rhine and the
Amsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of
Amsterdam. The lock complex consists of two independent locks and a moveable
flood-protection barrier. Ensuring safe control of the lock complex is of
utmost importance to guarantee both flood-protection and reliable ship
operations. This paper gives a precise, formal description of the software
control of the lock complex in less than 400 lines of mCRL2 code. This
description can act as a blueprint on how the software of this lock complex
needs to be constructed. Moreover, using model checking, 53 software
requirements are shown to be valid, ensuring that the formal description of the
behaviour is correct with regard to these properties and is unlikely to contain
mistakes and oversights.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [79] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 论文探讨了生成式AI（如ChatGPT和Codex）如何改变计算机科学教育，提出了机遇与挑战，并提出了政策建议。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在计算机科学教育中的应用潜力及其带来的问题和挑战。

Method: 通过分析AI工具的功能及其对教育的影响，结合实证数据和新兴研究。

Result: AI工具能辅助编程教学，但也带来了学术诚信和教学评估的挑战。

Conclusion: 建议合理整合AI工具，制定政策以平衡技术潜力与教育完整性。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [80] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 论文提出了一种新框架HyFairCRS，旨在通过超图对比多兴趣学习，解决对话推荐系统中的动态公平性问题，从而提升多样性公平性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的不公平问题（如性别、种族等偏见）在动态交互中加剧，导致马太效应和过滤泡沫等问题。

Method: 采用超图对比学习捕捉用户多样化兴趣，并在对话中动态调整推荐以保障公平性。

Result: 在两种对话推荐数据集上，HyFairCRS实现了最优性能并显著缓解了不公平问题。

Conclusion: 动态交互场景下的公平性问题可通过多兴趣学习框架有效解决，同时提升推荐效果。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [81] [Public perspectives on the design of fusion energy facilities](https://arxiv.org/abs/2507.02207)
*Nathan Kawamoto,Daniel Hoover,Jonathan Xie,Jacob Walters,Katie Snyder,Aditi Verma*

Main category: physics.soc-ph

TL;DR: 论文探讨了公众参与设计核聚变能源设施的方法及其对实现社会认可的重要性，通过工作坊发现公众最关注的是诚信、尊重、经济利益和环境保护/安全。


<details>
  <summary>Details</summary>
Motivation: 随着核聚变技术接近商业化部署，了解公众对核聚变设施的看法对于获得社会认可至关重要，因为这些设施可能因监管框架的不同而更靠近社区。

Method: 研究采用参与式设计方法，通过工作坊将社区居民与工程学生聚集在一起，共同设计核聚变设施，并分析文本与视觉数据。

Result: 工作坊发现诚信和尊重是公众最看重的价值，经济利益和环境保护/安全是最重要的决策标准。设计主题包括社区历史、工人关怀、透明度和健康安全。

Conclusion: 研究表明，早期参与式设计可以具体化公众的期望与担忧，增强对新兴技术的理解与好奇，为核聚变设施的发展提供情境化指导。

Abstract: As fusion energy technologies approach demonstration and commercial
deployment, understanding public perspectives on future fusion facilities will
be critical for achieving social license, especially because fusion energy
facilities, unlike large fission reactors, may be sited in closer proximity to
people and communities, due to distinct regulatory frameworks. In a departure
from the 'decide-announce-defend' approach typically used to site energy
infrastructure, we develop a participatory design methodology for
collaboratively designing fusion energy facilities with prospective host
communities. We present here our findings from a participatory design workshop
that brought together 22 community participants and 34 engineering students.
Our analysis of the textual and visual data from this workshop shows a range of
design values and decision-making criteria with 'integrity' and 'respect'
ranking highest among values and 'economic benefits' and 'environmental
protection/safety' ranking highest among decision-making criteria. Salient
design themes that emerge across facility concepts include connecting the
history and legacy of the community to the design of the facility, care for
workers, transparency and access to the facility, and health and safety of the
host community. Participants reported predominantly positive sentiments,
expressing joy and surprise as the workshop progressed from learning about
fusion to designing the hypothetical facility. Our findings suggest that
carrying out participatory design in the early stages of technology development
can invite and make concrete public hopes and concerns, improve understanding
of, and curiosity about, an emerging technology, build toward social license,
and inform context-specific development of fusion energy facilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 扩展DBNs的可训练互联结构以支持更宽层，并提出两种剪枝阶段以压缩模型。


<details>
  <summary>Details</summary>
Motivation: 解决DBNs在处理更宽层时的扩展性问题，同时优化模型压缩与准确性。

Method: 引入可训练、参数固定的互联结构；提出基于SAT逻辑等价和相似性的两阶段剪枝。

Result: 有效扩展DBNs层宽，剪枝方法优于基线，实现更优压缩-准确性平衡。

Conclusion: 改进的DBNs结构与剪枝策略显著提升模型的可扩展性和效率。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [83] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 该论文综述了Transformer模型在EEG解码中的最新应用，总结了其架构演变及与其他深度学习技术的融合，并讨论了当前挑战与未来发展前景。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer模型在EEG解码中的应用，因其在处理序列数据和注意力机制上的优势，逐渐成为BCI/BMIs研究的前沿。

Method: 通过调查和总结Transformer在EEG处理中的直接应用、混合架构（与其他深度学习技术结合）以及定制化Transformer结构的改进。

Result: Transformer模型在EEG解码中展现出强大的潜力，尤其在自动学习判别特征和序列数据处理方面表现优异。

Conclusion: 尽管Transformer在EEG解码中取得了显著进展，但仍面临挑战，未来研究方向包括进一步优化模型架构和解决实际应用中的问题。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [84] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 论文提出了一种新方法，通过优化的预处理和深度学习技术，显著提升了跨主体运动想象（CS-MI）分类的性能。该方法在多个数据集上验证，表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 跨主体脑机接口（BCI）中运动想象分类受个体间脑电图（EEG）模式差异影响，导致分类精度低，阻碍了实用化校准自由BCI的开发。

Method: 采用短时傅里叶变换（STFT）处理EEG数据，优化STFT参数，并结合卷积神经网络（CNN）进行训练，同时使用平衡批量策略。

Result: 在多个数据集上实现较高分类准确率（67.60%-80.22%），并研究了不同时间窗口对分类效果的影响。

Conclusion: 该方法为跨主体MI分类设立了新基准，并贡献了一个开放数据集，推动该领域研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [85] [Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems](https://arxiv.org/abs/2507.02464)
*Craig S Wright*

Main category: cs.GT

TL;DR: 这篇论文通过自动机理论和经济学框架重新构建了CAP定理的权衡问题，证明了在一定范围内可以同时保持可用性和一致性。


<details>
  <summary>Details</summary>
Motivation: CAP定理指出分布式系统无法同时满足一致性、可用性和分区容忍性。本文旨在通过经济激励机制和自动机理论扩展CAP定理的限制。

Method: 通过建模分布式系统为分区感知的状态机，并引入经济激励层来稳定共识行为，结合博弈论机制定义可证明的边界。

Result: 研究证明，可用性和一致性可以在有界的ε范围内同时保持，从而通过经济控制扩展了CAP定理。

Conclusion: 通过经济控制和自动机理论，本文成功扩展了CAP定理的经典限制，证明了在某些条件下可用性和一致性可以兼顾。

Abstract: The CAP theorem asserts a trilemma between consistency, availability, and
partition tolerance. This paper introduces a rigorous automata-theoretic and
economically grounded framework that reframes the CAP trade-off as a constraint
optimization problem. We model distributed systems as partition-aware state
machines and embed economic incentive layers to stabilize consensus behavior
across adversarially partitioned networks. By incorporating game-theoretic
mechanisms into the global transition semantics, we define provable bounds on
convergence, liveness, and correctness. Our results demonstrate that
availability and consistency can be simultaneously preserved within bounded
epsilon margins, effectively extending the classical CAP limits through formal
economic control.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems](https://arxiv.org/abs/2507.02400)
*Maximilian Zipfl,Pascal Zwick,Patrick Schulz,Marc Rene Zofka,Albert Schotschneider,Helen Gremmelmaier,Nikolai Polley,Ferdinand Mütsch,Kevin Simon,Fabian Gottselig,Michael Frey,Sergio Marschall,Akim Stark,Maximilian Müller,Marek Wehmer,Mihai Kocsis,Dominic Waldenmayer,Florian Schnepf,Erik Heinrich,Sabrina Pletz,Matthias Kölle,Karin Langbein-Euchner,Alexander Viehl,Raoul Zöllner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 本文描述了德国TAF-BW测试区的数字重建过程，用于开发其数字孪生。通过智能基础设施和LiDAR传感器提取真实数据，并提供一个公开可用的仿真框架。


<details>
  <summary>Details</summary>
Motivation: 随着数字化的普及，未来交通将高度互联，数字孪生因其双向连接特性成为理想工具。本文旨在展示TAF-BW数字孪生的开发过程及其应用价值。

Method: 利用智能交叉口的摄像头和LiDAR传感器提取对象列表，生成真实数据输入数字孪生，并通过统一接口实现交通参与者的重新仿真。

Result: 开发了一个公开可用的数字孪生仿真框架，并通过两个案例（交通信号优化和通信安全场景）展示了其应用。

Conclusion: 该数字孪生框架为交通优化和安全性研究提供了实用工具，并在测试区展示了其潜力。

Abstract: In the future, mobility will be strongly shaped by the increasing use of
digitalization. Not only will individual road users be highly interconnected,
but also the road and associated infrastructure. At that point, a Digital Twin
becomes particularly appealing because, unlike a basic simulation, it offers a
continuous, bilateral connection linking the real and virtual environments.
This paper describes the digital reconstruction used to develop the Digital
Twin of the Test Area Autonomous Driving-Baden-W\"urttemberg (TAF-BW), Germany.
The TAF-BW offers a variety of different road sections, from high-traffic urban
intersections and tunnels to multilane motorways. The test area is equipped
with a comprehensive Vehicle-to-Everything (V2X) communication infrastructure
and multiple intelligent intersections equipped with camera sensors to
facilitate real-time traffic flow monitoring. The generation of authentic data
as input for the Digital Twin was achieved by extracting object lists at the
intersections. This process was facilitated by the combined utilization of
camera images from the intelligent infrastructure and LiDAR sensors mounted on
a test vehicle. Using a unified interface, recordings from real-world
detections of traffic participants can be resimulated. Additionally, the
simulation framework's design and the reconstruction process is discussed. The
resulting framework is made publicly available for download and utilization at:
https://digit4taf-bw.fzi.de The demonstration uses two case studies to
illustrate the application of the digital twin and its interfaces: the analysis
of traffic signal systems to optimize traffic flow and the simulation of
security-related scenarios in the communications sector.

</details>


### [87] [MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints](https://arxiv.org/abs/2507.02438)
*Shivam Chaubey,Francesco Verdoja,Shankar Deka,Ville Kyrki*

Main category: cs.RO

TL;DR: 论文提出了一种基于约束最优控制问题的辅助控制框架，通过离线的控制不变集在线计算控制动作，确保可行性、严格约束满足和最小化用户意图覆盖，解决了现有共享控制方法在可行性、扩展性和安全性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有共享控制方法（如模型预测控制、控制屏障函数或基于学习的控制）在用户输入不可预测的情况下难以保证可行性、扩展性和安全性，亟需一种新的方法来解决这些挑战。

Method: 提出了一种基于约束最优控制问题的辅助控制框架，结合离线计算的控制不变集，实现在线计算控制动作，同时支持非凸约束的适应。

Result: 通过大规模用户研究验证，该方法在任务负荷、信任、感知控制和性能方面均有显著提升，且不损害安全性和用户意图。

Conclusion: 所提框架为共享控制提供了可行性、安全性和性能的综合解决方案，适用于实际场景中的非凸约束问题。

Abstract: Shared control combines human intention with autonomous decision-making, from
low-level safety overrides to high-level task guidance, enabling systems that
adapt to users while ensuring safety and performance. This enhances task
effectiveness and user experience across domains such as assistive robotics,
teleoperation, and autonomous driving. However, existing shared control
methods, based on e.g. Model Predictive Control, Control Barrier Functions, or
learning-based control, struggle with feasibility, scalability, or safety
guarantees, particularly since the user input is unpredictable.
  To address these challenges, we propose an assistive controller framework
based on Constrained Optimal Control Problem that incorporates an
offline-computed Control Invariant Set, enabling online computation of control
actions that ensure feasibility, strict constraint satisfaction, and minimal
override of user intent. Moreover, the framework can accommodate structured
class of non-convex constraints, which are common in real-world scenarios. We
validate the approach through a large-scale user study with 66
participants--one of the most extensive in shared control research--using a
computer game environment to assess task load, trust, and perceived control, in
addition to performance. The results show consistent improvements across all
these aspects without compromising safety and user intent.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [88] [Access Control Threatened by Quantum Entanglement](https://arxiv.org/abs/2507.02622)
*Zhicheng Zhang,Mingsheng Ying*

Main category: quant-ph

TL;DR: 该论文研究了量子计算机系统中的访问控制，揭示了经典安全系统直接迁移到量子环境时的安全漏洞，并提出新的量子访问控制模型以应对量子纠缠带来的威胁。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算机系统中的访问控制问题，揭示经典安全系统在量子环境中的潜在漏洞，并提出改进方案。

Method: 分析量子纠缠对访问控制的威胁，设计并验证了几种新的量子访问控制模型。

Result: 提出了能抵御量子纠缠威胁的访问控制模型，并分析了其安全性、灵活性和效率。

Conclusion: 量子计算对现有安全系统提出了新的挑战，需要设计专门针对量子环境的访问控制机制。

Abstract: Access control is a cornerstone of computer security that prevents
unauthorised access to resources. In this paper, we study access control in
quantum computer systems. We present the first explicit scenario of a security
breach when a classically secure access control system is straightforwardly
adapted to the quantum setting. The breach is ultimately due to that quantum
mechanics allows the phenomenon of entanglement and violates Mermin inequality,
a multi-party variant of the celebrated Bell inequality. This reveals a threat
from quantum entanglement to access control if existing computer systems
integrate with quantum computing. To protect against such threat, we propose
several new models of quantum access control, and rigorously analyse their
security, flexibility and efficiency.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [89] [Optimising task allocation to balance business goals and worker well-being for financial service workforces](https://arxiv.org/abs/2507.01968)
*Chris Duckworth,Zlatko Zlatev,James Sciberras,Peter Hallett,Enrico Gerding*

Main category: q-fin.GN

TL;DR: 论文提出了一种结合业务目标和员工福祉的任务分配模型，使用遗传算法优化分配和调度任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融服务业处理大量数据，需及时解决错误任务，这给分析师带来压力并增加业务风险。

Method: 采用遗传算法优化任务分配模型，考虑员工技能、经验和福祉目标。

Result: 遗传算法模型优于基线启发式方法和当前工作实践，适用于单目标和多目标场景。

Conclusion: 该模型填补了现有分配和调度模型中忽视员工福祉的空白，同时提高了效率。

Abstract: Purpose: Financial service companies manage huge volumes of data which
requires timely error identification and resolution. The associated tasks to
resolve these errors frequently put financial analyst workforces under
significant pressure leading to resourcing challenges and increased business
risk. To address this challenge, we introduce a formal task allocation model
which considers both business orientated goals and analyst well-being.
  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to
allocate and schedule tasks to analysts. The proposed solution is able to
allocate tasks to analysts with appropriate skills and experience, while taking
into account staff well-being objectives.
  Findings: We demonstrate our GA model outperforms baseline heuristics,
current working practice, and is applicable to a range of single and
multi-objective real-world scenarios. We discuss the potential for
metaheuristics (such as GAs) to efficiently find sufficiently good allocations
which can provide recommendations for financial service managers in-the-loop.
  Originality: A key gap in existing allocation and scheduling models, is fully
considering worker well-being. This paper presents an allocation model which
explicitly optimises for well-being while still improving on current working
practice for efficiency.

</details>
