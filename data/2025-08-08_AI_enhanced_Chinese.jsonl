{"id": "2508.05130", "pdf": "https://arxiv.org/pdf/2508.05130", "abs": "https://arxiv.org/abs/2508.05130", "authors": ["Ali Raza", "Muhammad Farhan Khan", "Zeeshan Alam", "Muhammad Saad", "Ilyas Saleem", "Muhammad Ahmed Mohsin", "Muhammad Ali Jamshed"], "title": "TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks", "categories": ["cs.NI", "eess.SP"], "comment": "Accepted at PIMRC", "summary": "This paper presents a joint framework that integrates reconfigurable\nintelligent surfaces (RISs) with Terahertz (THz) communications and\nnon-orthogonal multiple access (NOMA) to enhance smart industrial\ncommunications. The proposed system leverages the advantages of RIS and THz\nbands to improve spectral efficiency, coverage, and reliability key\nrequirements for industrial automation and real-time communications in future\n6G networks and beyond. Within this framework, two power allocation strategies\nare investigated: the first optimally distributes power between near and far\nindustrial nodes, and the second prioritizes network demands to enhance system\nperformance further. A performance evaluation is conducted to compare the sum\nrate and outage probability against a fixed power allocation scheme. Our scheme\nachieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results\nvalidate the theoretical analysis, demonstrating the effectiveness and\nrobustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial\ncommunications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u3001\u592a\u8d6b\u5179(THz)\u901a\u4fe1\u548c\u975e\u6b63\u4ea4\u591a\u5740(NOMA)\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u667a\u80fd\u5de5\u4e1a\u901a\u4fe1\u3002\u901a\u8fc7\u4f18\u5316\u529f\u7387\u5206\u914d\u7b56\u7565\uff0c\u7cfb\u7edf\u5728\u9891\u8c31\u6548\u7387\u548c\u53ef\u9760\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u672a\u67656G\u7f51\u7edc\u4e2d\uff0c\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u5b9e\u65f6\u901a\u4fe1\u5bf9\u9891\u8c31\u6548\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002RIS\u548cTHz\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u5728\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u529f\u7387\u5206\u914d\u7b56\u7565\uff1a\u4e00\u79cd\u662f\u5728\u8fd1\u7aef\u548c\u8fdc\u7aef\u5de5\u4e1a\u8282\u70b9\u4e4b\u95f4\u4f18\u5316\u5206\u914d\u529f\u7387\uff0c\u53e6\u4e00\u79cd\u662f\u6839\u636e\u7f51\u7edc\u9700\u6c42\u4f18\u5148\u7ea7\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "result": "\u76f8\u6bd4\u4e8e\u56fa\u5b9a\u529f\u7387\u5206\u914d\u65b9\u6848\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u572830dBm\u65f6\u5b9e\u73b0\u4e8623%\u7684\u9891\u8c31\u6548\u7387\u589e\u76ca\uff0c\u5e76\u964d\u4f4e\u4e86\u4e2d\u65ad\u6982\u7387\u3002", "conclusion": "RIS\u8f85\u52a9\u7684NOMA MIMO\u6846\u67b6\u5728THz\u5de5\u4e1a\u901a\u4fe1\u4e2d\u8868\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05249", "pdf": "https://arxiv.org/pdf/2508.05249", "abs": "https://arxiv.org/abs/2508.05249", "authors": ["Jos\u00e9 Ruela", "Ivan Cojocaru", "Andr\u00e9 Coelho", "Rui Campos", "Manuel Ricardo"], "title": "Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models", "categories": ["cs.NI"], "comment": null, "summary": "This paper presents the concept, architectural design, and performance\nevaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to\nUser Equipment (UE) in areas with limited fixed 5G infrastructures or subject\nto adverse radio conditions. We consider two main approaches to MC design: an\noverlay model, where the MC obtains backhaul connectivity from a 5G overlay\nnetwork, and an Integrated Access and Backhaul (IAB)-based model, discussing\ntheir protocol stacks and architectural implications. In order to validate the\nMC's performance, we employ an emulation-based testbed using the\nOpenAirInterface (OAI) implementation, considering different MC positions. The\nresults validate the MC concept and demonstrate that MC positioning\nsignificantly influences network performance. This paper has the potential to\naid network operators and service providers in selecting and deploying MC\narchitectures for temporary coverage extension and capacity reinforcement in\ndifferent environments, including seaports, industrial scenarios, and public\nsafety.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u5728\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u6216\u65e0\u7ebf\u6761\u4ef6\u8f83\u5dee\u533a\u57df\u63d0\u4f9b5G\u8fde\u63a5\u7684\u79fb\u52a8\u8702\u7a9d\uff08MC\uff09\u7684\u6982\u5ff5\u3001\u8bbe\u8ba1\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u63a2\u8ba8\u4e86\u4e24\u79cd\u4e3b\u8981\u8bbe\u8ba1\u6a21\u578b\u53ca\u5176\u5728\u7f51\u7edc\u6027\u80fd\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u5728\u56fa\u5b9a5G\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u6216\u65e0\u7ebf\u6761\u4ef6\u6076\u52a3\u533a\u57df\u63d0\u4f9b\u53ef\u97605G\u8fde\u63a5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cdMC\u8bbe\u8ba1\u6a21\u578b\uff08\u8986\u76d6\u6a21\u578b\u548c\u57fa\u4e8eIAB\u7684\u6a21\u578b\uff09\uff0c\u5e76\u4f7f\u7528OpenAirInterface\u4eff\u771f\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u9a8c\u8bc1\u4e86MC\u6982\u5ff5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660eMC\u4f4d\u7f6e\u5bf9\u7f51\u7edc\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "MC\u67b6\u6784\u53ef\u4e3a\u4e34\u65f6\u8986\u76d6\u6269\u5c55\u548c\u5bb9\u91cf\u589e\u5f3a\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6d77\u6e2f\u3001\u5de5\u4e1a\u573a\u666f\u548c\u516c\u5171\u5b89\u5168\u7b49\u9886\u57df\u3002"}}
{"id": "2508.04967", "pdf": "https://arxiv.org/pdf/2508.04967", "abs": "https://arxiv.org/abs/2508.04967", "authors": ["Yuan Li", "Chen Zhang", "Hao Zhang", "Tao Huang", "Yunjie Liu"], "title": "A Design for an Early Quantum Network", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "With the rapid advancement of quantum information technology, quantum\nnetworks have become essential for supporting diverse applications, which often\nhave stringent demands for key metrics such as fidelity and request completion\ntime. In this work, we propose a design for early-stage quantum networks that\nis compatible with the three existing quantum repeater technologies. The design\naims to maximize the ability of the network to accommodate the diverse needs of\nquantum applications, even under conditions of limited quantum resources and\nsuboptimal network performance. We have also described the required identifiers\nin the quantum network and the specific process for implementing quantum\nrequests. To assess the feasibility of our design, we conduct simulations based\non discrete-event modeling of quantum networks. The simulations consider\nvarious types of noise and imperfect parameters that might exist in early-stage\nnetworks. We analyze the impact of these parameters on the fidelity of the\ngenerated entangled states and the request completion time. Furthermore, we\ninvestigated additional decisions that the central controller can make beyond\npath selection, such as the choice of cutoff time and the allocation of network\nresources to requests.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u65e9\u671f\u91cf\u5b50\u7f51\u7edc\u7684\u8bbe\u8ba1\uff0c\u517c\u5bb9\u73b0\u6709\u91cf\u5b50\u4e2d\u7ee7\u6280\u672f\uff0c\u65e8\u5728\u6700\u5927\u5316\u6ee1\u8db3\u91cf\u5b50\u5e94\u7528\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u8bc4\u4f30\u5176\u53ef\u884c\u6027\u548c\u6027\u80fd\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u4fe1\u606f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u91cf\u5b50\u7f51\u7edc\u9700\u6ee1\u8db3\u9ad8\u4fdd\u771f\u5ea6\u548c\u5feb\u901f\u5b8c\u6210\u8bf7\u6c42\u7b49\u4e25\u683c\u8981\u6c42\uff0c\u4f46\u76ee\u524d\u8d44\u6e90\u6709\u9650\u4e14\u6027\u80fd\u4e0d\u7406\u60f3\u3002", "method": "\u8bbe\u8ba1\u517c\u5bb9\u4e09\u79cd\u73b0\u6709\u91cf\u5b50\u4e2d\u7ee7\u6280\u672f\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u660e\u786e\u6807\u8bc6\u7b26\u548c\u91cf\u5b50\u8bf7\u6c42\u5b9e\u73b0\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u5206\u6790\u566a\u58f0\u548c\u4e0d\u5b8c\u7f8e\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5206\u6790\u4e86\u4fdd\u771f\u5ea6\u548c\u8bf7\u6c42\u5b8c\u6210\u65f6\u95f4\u53d7\u53c2\u6570\u5f71\u54cd\u7684\u60c5\u51b5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e2d\u592e\u63a7\u5236\u5668\u5728\u8def\u5f84\u9009\u62e9\u5916\u7684\u5176\u4ed6\u51b3\u7b56\uff08\u5982\u622a\u6b62\u65f6\u95f4\u548c\u8d44\u6e90\u5206\u914d\uff09\u7684\u4f5c\u7528\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u65e9\u671f\u91cf\u5b50\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\uff0c\u540c\u65f6\u4e3a\u63a7\u5236\u5668\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.04829", "pdf": "https://arxiv.org/pdf/2508.04829", "abs": "https://arxiv.org/abs/2508.04829", "authors": ["Devora Chait-Roth", "Kedar S. Namjoshi", "Thomas Wies"], "title": "Consistent Updates for Scalable Microservices", "categories": ["cs.PL"], "comment": null, "summary": "Online services are commonly implemented with a scalable microservice\narchitecture, where isomorphic worker processes service client requests,\nrecording persistent state in a backend data store. To maintain service, any\nmodifications to the service functionality must be made on the fly -- i.e., as\nthe service continues to process client requests -- but doing so is\nchallenging. The central difficulty is that of avoiding potential\ninconsistencies caused by ''mixed mode'' operation, where workers of current\nand new versions are concurrently active and interact via the data store. Some\nupdate methods avoid mixed mode altogether, but only at the cost of substantial\ninefficiency -- by doubling resources (memory and compute), or by halving\nthroughput. The alternative is a so-called ''rolling'' update, which is\nuncontrolled and runs the risk of serious service failures arising from\ninconsistent mixed-mode behavior.\n  In this paper, we present the first algorithms that guarantee consistency for\nmixed mode updates. The algorithms rely on semantic properties of service\nactions, such as commutativity. We show that semantic awareness is required, by\nproving that any semantically oblivious, mixed-mode update method cannot avoid\ninconsistencies. Ideally, it should appear to every client that a service\nupdate takes effect atomically; this ensures that a client is not exposed to\ninconsistent mixed-mode behavior. We introduce a framework that formalizes this\nintuition and develop foundational theory for reasoning about the consistency\nof mixed-mode updates, applying that theory to derive the new algorithms and\nestablish their correctness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4fdd\u8bc1\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u4e00\u81f4\u6027\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u670d\u52a1\u52a8\u4f5c\u7684\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u4ea4\u6362\u6027\uff09\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u670d\u52a1\u52a8\u6001\u66f4\u65b0\u4e2d\u7684\u6df7\u5408\u6a21\u5f0f\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u7ebf\u670d\u52a1\u52a8\u6001\u66f4\u65b0\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u65b0\u65e7\u7248\u672c\u5de5\u4f5c\u8fdb\u7a0b\u540c\u65f6\u8fd0\u884c\u5e76\u4ea4\u4e92\u5bfc\u81f4\u7684\u6df7\u5408\u6a21\u5f0f\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u670d\u52a1\u52a8\u4f5c\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u4ea4\u6362\u6027\uff09\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6b63\u786e\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u4ec5\u51ed\u8bed\u4e49\u65e0\u5173\u7684\u66f4\u65b0\u65b9\u6cd5\u65e0\u6cd5\u907f\u514d\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u786e\u4fdd\u66f4\u65b0\u539f\u5b50\u6027\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\u548c\u6709\u6548\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u670d\u52a1\u66f4\u65b0\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2508.05062", "pdf": "https://arxiv.org/pdf/2508.05062", "abs": "https://arxiv.org/abs/2508.05062", "authors": ["Thom Badings", "Alessandro Abate"], "title": "Probabilistic Alternating Simulations for Policy Synthesis in Uncertain Stochastic Dynamical Systems", "categories": ["eess.SY", "cs.LO", "cs.SY", "math.OC"], "comment": "Presented at CDC 2025", "summary": "A classical approach to formal policy synthesis in stochastic dynamical\nsystems is to construct a finite-state abstraction, often represented as a\nMarkov decision process (MDP). The correctness of these approaches hinges on a\nbehavioural relation between the dynamical system and its abstraction, such as\na probabilistic simulation relation. However, probabilistic simulation\nrelations do not suffice when the system dynamics are, next to being\nstochastic, also subject to nondeterministic (i.e., set-valued) disturbances.\nIn this work, we extend probabilistic simulation relations to systems with both\nstochastic and nondeterministic disturbances. Our relation, which is inspired\nby a notion of alternating simulation, generalises existing relations used for\nverification and policy synthesis used in several works. Intuitively, our\nrelation allows reasoning probabilistically over stochastic uncertainty, while\nreasoning robustly (i.e., adversarially) over nondeterministic disturbances. We\nexperimentally demonstrate the applicability of our relations for policy\nsynthesis in a 4D-state Dubins vehicle.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u6982\u7387\u6a21\u62df\u5173\u7cfb\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u968f\u673a\u548c\u4e0d\u786e\u5b9a\u6270\u52a8\u7684\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u653f\u7b56\u5408\u6210\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u57284D\u72b6\u6001Dubins\u8f66\u8f86\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u7387\u6a21\u62df\u5173\u7cfb\u65e0\u6cd5\u5145\u5206\u5904\u7406\u540c\u65f6\u5177\u6709\u968f\u673a\u548c\u975e\u786e\u5b9a\u6027\u6270\u52a8\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u5173\u7cfb\u6765\u652f\u6301\u9a8c\u8bc1\u548c\u7b56\u7565\u5408\u6210\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6982\u7387\u6a21\u62df\u5173\u7cfb\uff0c\u7ed3\u5408\u4ea4\u66ff\u6a21\u62df\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5173\u7cfb\uff0c\u5141\u8bb8\u5728\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u6982\u7387\u6027\u63a8\u7406\uff0c\u5728\u975e\u786e\u5b9a\u6027\u6270\u52a8\u4e0b\u8fdb\u884c\u9c81\u68d2\u6027\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b0\u5173\u7cfb\u57284D\u72b6\u6001Dubins\u8f66\u8f86\u7684\u653f\u7b56\u5408\u6210\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5904\u7406\u6df7\u5408\u968f\u673a\u548c\u975e\u786e\u5b9a\u6027\u6270\u52a8\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7406\u8bba\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u653f\u7b56\u5408\u6210\u7684\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2508.04713", "pdf": "https://arxiv.org/pdf/2508.04713", "abs": "https://arxiv.org/abs/2508.04713", "authors": ["Carlo Esposito"], "title": "AI Should Be More Human, Not More Complex", "categories": ["cs.HC", "cs.AI"], "comment": "2025 - Knowledge Commons - Eyed Research Collection", "summary": "Large Language Models (LLMs) in search applications increasingly prioritize\nverbose, lexically complex responses that paradoxically reduce user\nsatisfaction and engagement. Through a comprehensive study of 10.000 (est.)\nparticipants comparing responses from five major AI-powered search systems, we\ndemonstrate that users overwhelmingly prefer concise, source-attributed\nresponses over elaborate explanations. Our analysis reveals that current AI\ndevelopment trends toward \"artificial sophistication\" create an uncanny valley\neffect where systems sound knowledgeable but lack genuine critical thinking,\nleading to reduced trust and increased cognitive load. We present evidence that\noptimal AI communication mirrors effective human discourse: direct, properly\nsourced, and honest about limitations. Our findings challenge the prevailing\nassumption that more complex AI responses indicate better performance, instead\nsuggesting that human-like brevity and transparency are key to user engagement\nand system reliability.", "AI": {"tldr": "\u7528\u6237\u66f4\u559c\u6b22\u7b80\u6d01\u3001\u6709\u6765\u6e90\u6807\u6ce8\u7684AI\u56de\u7b54\uff0c\u800c\u975e\u590d\u6742\u5197\u957f\u7684\u89e3\u91ca\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fc7\u4e8e\u590d\u6742\u7684AI\u56de\u7b54\u4f1a\u964d\u4f4e\u7528\u6237\u4fe1\u4efb\u5e76\u589e\u52a0\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u641c\u7d22\u5e94\u7528\u4e2d\u8fc7\u4e8e\u8ffd\u6c42\u590d\u6742\u56de\u7b54\u7684\u73b0\u8c61\uff0c\u53ca\u5176\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u53c2\u4e0e\u5ea6\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf910,000\u540d\u53c2\u4e0e\u8005\u6bd4\u8f83\u4e94\u79cdAI\u641c\u7d22\u7cfb\u7edf\u7684\u56de\u7b54\uff0c\u5206\u6790\u7528\u6237\u504f\u597d\u3002", "result": "\u7528\u6237\u504f\u597d\u7b80\u6d01\u4e14\u6709\u6765\u6e90\u6807\u6ce8\u7684\u56de\u7b54\uff0c\u590d\u6742\u56de\u7b54\u4f1a\u5bfc\u81f4\u201c\u4eba\u5de5\u590d\u6742\u201d\u6548\u5e94\uff0c\u964d\u4f4e\u4fe1\u4efb\u3002", "conclusion": "AI\u56de\u7b54\u5e94\u50cf\u4eba\u7c7b\u4ea4\u6d41\u4e00\u6837\u7b80\u6d01\u3001\u900f\u660e\uff0c\u5e76\u6807\u6ce8\u6765\u6e90\uff0c\u4ee5\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2508.05087", "pdf": "https://arxiv.org/pdf/2508.05087", "abs": "https://arxiv.org/abs/2508.05087", "authors": ["Renmiao Chen", "Shiyao Cui", "Xuancheng Huang", "Chengwei Pan", "Victor Shea-Jay Huang", "QingLin Zhang", "Xuan Ouyang", "Zhexin Zhang", "Hongning Wang", "Minlie Huang"], "title": "JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CR", "I.2.7; K.4.1; K.6.5"], "comment": "10 pages, 3 tables, 2 figures, to appear in the Proceedings of the\n  33rd ACM International Conference on Multimedia (MM '25)", "summary": "Jailbreak attacks against multimodal large language Models (MLLMs) are a\nsignificant research focus. Current research predominantly focuses on\nmaximizing attack success rate (ASR), often overlooking whether the generated\nresponses actually fulfill the attacker's malicious intent. This oversight\nfrequently leads to low-quality outputs that bypass safety filters but lack\nsubstantial harmful content. To address this gap, we propose JPS,\n\\underline{J}ailbreak MLLMs with collaborative visual \\underline{P}erturbation\nand textual \\underline{S}teering, which achieves jailbreaks via corporation of\nvisual image and textually steering prompt. Specifically, JPS utilizes\ntarget-guided adversarial image perturbations for effective safety bypass,\ncomplemented by \"steering prompt\" optimized via a multi-agent system to\nspecifically guide LLM responses fulfilling the attackers' intent. These visual\nand textual components undergo iterative co-optimization for enhanced\nperformance. To evaluate the quality of attack outcomes, we propose the\nMalicious Intent Fulfillment Rate (MIFR) metric, assessed using a\nReasoning-LLM-based evaluator. Our experiments show JPS sets a new\nstate-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with\nanalyses confirming its efficacy. Codes are available at\n\\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.\n\\color{warningcolor}{Warning: This paper contains potentially sensitive\ncontents.}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86JPS\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u5bf9\u6297\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u6765\u5b9e\u73b0\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807MIFR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\u548c\u6076\u610f\u610f\u56fe\u5b9e\u73b0\u7387\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9MLLMs\u7684\u8d8a\u72f1\u653b\u51fb\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u5ffd\u89c6\u4e86\u751f\u6210\u54cd\u5e94\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u653b\u51fb\u8005\u7684\u6076\u610f\u610f\u56fe\uff0c\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4f4e\u4e0b\u3002", "method": "JPS\u65b9\u6cd5\u901a\u8fc7\u534f\u540c\u4f18\u5316\u89c6\u89c9\u5bf9\u6297\u56fe\u50cf\u6270\u52a8\u548c\u6587\u672c\u63d0\u793a\uff08steering prompt\uff09\uff0c\u5b9e\u73b0\u5bf9\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u5e76\u4f7f\u7528\u591a\u4ee3\u7406\u7cfb\u7edf\u4f18\u5316\u63d0\u793a\u4ee5\u5f15\u5bfc\u6a21\u578b\u6ee1\u8db3\u6076\u610f\u610f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJPS\u5728\u591a\u79cdMLLMs\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u9ad8\u6c34\u5e73\u7684ASR\u548cMIFR\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "JPS\u4e0d\u4ec5\u5728\u653b\u51fb\u6210\u529f\u7387\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u901a\u8fc7\u65b0\u6307\u6807MIFR\u786e\u4fdd\u653b\u51fb\u54cd\u5e94\u771f\u6b63\u6ee1\u8db3\u6076\u610f\u610f\u56fe\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.05266", "pdf": "https://arxiv.org/pdf/2508.05266", "abs": "https://arxiv.org/abs/2508.05266", "authors": ["Jiazheng Zhang", "Cheng Liu", "Huawei Li"], "title": "Understanding and Mitigating Errors of LLM-Generated RTL Code", "categories": ["cs.AR", "cs.CL", "cs.LG"], "comment": "14 pages, 26 figures", "summary": "Despite the promising potential of large language model (LLM) based\nregister-transfer-level (RTL) code generation, the overall success rate remains\nunsatisfactory. Errors arise from various factors, with limited understanding\nof specific failure causes hindering improvement. To address this, we conduct a\ncomprehensive error analysis and manual categorization. Our findings reveal\nthat most errors stem not from LLM reasoning limitations, but from insufficient\nRTL programming knowledge, poor understanding of circuit concepts, ambiguous\ndesign descriptions, or misinterpretation of complex multimodal inputs.\nLeveraging in-context learning, we propose targeted error correction\ntechniques. Specifically, we construct a domain-specific knowledge base and\nemploy retrieval-augmented generation (RAG) to supply necessary RTL knowledge.\nTo mitigate ambiguity errors, we introduce design description rules and\nimplement a rule-checking mechanism. For multimodal misinterpretation, we\nintegrate external tools to convert inputs into LLM-compatible meta-formats.\nFor remaining errors, we adopt an iterative debugging loop (simulation-error\nlocalization-correction). Integrating these techniques into an LLM-based\nframework significantly improves performance. We incorporate these error\ncorrection techniques into a foundational LLM-based RTL code generation\nframework, resulting in significantly improved performance. Experimental\nresults show that our enhanced framework achieves 91.0\\% accuracy on the\nVerilogEval benchmark, surpassing the baseline code generation approach by\n32.7\\%, demonstrating the effectiveness of our methods.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u9519\u8bef\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u9488\u5bf9\u6027\u7ea0\u6b63\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210RTL\u4ee3\u7801\u65f6\u7684\u9ad8\u9519\u8bef\u7387\u95ee\u9898\uff0c\u5206\u6790\u9519\u8bef\u6839\u6e90\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u5e93\u3001\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5f15\u5165\u8bbe\u8ba1\u63cf\u8ff0\u89c4\u5219\u548c\u68c0\u67e5\u673a\u5236\uff0c\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u8fed\u4ee3\u8c03\u8bd5\u5faa\u73af\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6846\u67b6\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523091.0%\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534732.7%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u9488\u5bf9\u6027\u7ea0\u9519\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.04825", "pdf": "https://arxiv.org/pdf/2508.04825", "abs": "https://arxiv.org/abs/2508.04825", "authors": ["Seungyong Lee", "Jeong-gi Kwak"], "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://nxnai.github.io/Voost/", "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.", "AI": {"tldr": "Voost\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\uff0c\u4f7f\u7528\u5355\u4e00\u6269\u6563\u53d8\u6362\u5668\u63d0\u5347\u670d\u88c5\u4e0e\u8eab\u4f53\u7684\u5173\u8054\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e24\u9879\u63a8\u7406\u6280\u672f\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u865a\u62df\u8bd5\u7a7f\u4e2d\u670d\u88c5\u4e0e\u8eab\u4f53\u7684\u5bf9\u5e94\u5173\u7cfb\u5efa\u6a21\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u59ff\u6001\u548c\u5916\u89c2\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVoost\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\uff0c\u5229\u7528\u5355\u4e00\u6269\u6563\u53d8\u6362\u5668\uff0c\u652f\u6301\u7075\u6d3b\u7684\u6761\u4ef6\u751f\u6210\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u6e29\u5ea6\u7f29\u653e\u548c\u81ea\u4fee\u6b63\u91c7\u6837\u6280\u672f\u3002", "result": "Voost\u5728\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728\u5bf9\u9f50\u7cbe\u5ea6\u3001\u89c6\u89c9\u903c\u771f\u5ea6\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Voost\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u548c\u63a8\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.04878", "pdf": "https://arxiv.org/pdf/2508.04878", "abs": "https://arxiv.org/abs/2508.04878", "authors": ["Md Mazharul Islam", "Md Shafayat Hossain", "Kathleen E Hamilton", "Ahmedullah Aziz"], "title": "Injection Locking and Coupling Dynamics in Superconducting Nanowire based Cryogenic Oscillators", "categories": ["cs.ET", "physics.app-ph"], "comment": null, "summary": "Oscillators designed to function at cryogenic temperatures play a critical\nrole in superconducting electronics and quantum computing by providing stable,\nlow noise signals with minimal energy loss.Here we present a comprehensive\nnumerical study of injection locking and mutual coupling dynamics in\nsuperconducting nanowire based cryogenic oscillators.Using the design space of\nstandalone ScNW based oscillator, we investigate two critical mechanisms that\ngovern frequency synchronization and signal coordination in cryogenic computing\narchitectures.First, an injection locking induced by an external AC signal with\na frequency near the oscillators natural frequency, and second, the mutual\ncoupling dynamics between two ScNW oscillators under varying coupling\nstrengths.We identify key design parameters such as shunt resistance, nanowire\ninductance, and coupling strength that govern the locking range.Additionally,\nwe examine how the amplitude of the injected signal affects the amplitude of\nthe locked oscillation, offering valuable insights for power aware oscillator\nsynchronization.Furthermore, we analyze mutual synchronization between coupled\nScNW oscillators using capacitive and resistive coupling elements.Our results\nreveal that the phase difference between oscillators can be precisely\ncontrolled by tuning the coupling strength, enabling programmable phase encoded\ninformation processing.These findings could enable building ScNW based\noscillatory neural networks, synchronized cryogenic logic blocks, and on chip\ncryogenic resonator arrays.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f4e\u6e29\u8d85\u5bfc\u7eb3\u7c73\u7ebf\u632f\u8361\u5668\u7684\u6ce8\u5165\u9501\u5b9a\u548c\u4e92\u8026\u5408\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u9891\u7387\u540c\u6b65\u548c\u4fe1\u53f7\u534f\u8c03\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u4f4e\u6e29\u8ba1\u7b97\u67b6\u6784\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5bfc\u3002", "motivation": "\u4f4e\u6e29\u632f\u8361\u5668\u5728\u8d85\u5bfc\u7535\u5b50\u5b66\u548c\u91cf\u5b50\u8ba1\u7b97\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u5176\u540c\u6b65\u548c\u8026\u5408\u673a\u5236\u7684\u7814\u7a76\u4ecd\u6709\u5f85\u6df1\u5165\u3002", "method": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5206\u6790\u4e86\u6ce8\u5165\u9501\u5b9a\u548c\u4e92\u8026\u5408\u52a8\u6001\uff0c\u8003\u5bdf\u4e86\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\uff08\u5982\u5e76\u8054\u7535\u963b\u3001\u7eb3\u7c73\u7ebf\u7535\u611f\u548c\u8026\u5408\u5f3a\u5ea6\uff09\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9501\u9891\u8303\u56f4\u53ef\u7531\u6ce8\u5165\u4fe1\u53f7\u5e45\u5ea6\u8c03\u8282\uff0c\u4e14\u8026\u5408\u5f3a\u5ea6\u53ef\u7cbe\u786e\u63a7\u5236\u76f8\u4f4d\u5dee\uff0c\u4e3a\u4f4e\u6e29\u8ba1\u7b97\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u8bbe\u8ba1\u601d\u8def\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6784\u5efa\u57fa\u4e8e\u8d85\u5bfc\u7eb3\u7c73\u7ebf\u7684\u632f\u8361\u795e\u7ecf\u7f51\u7edc\u548c\u540c\u6b65\u4f4e\u6e29\u903b\u8f91\u5757\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.04833", "pdf": "https://arxiv.org/pdf/2508.04833", "abs": "https://arxiv.org/abs/2508.04833", "authors": ["Nicolas Nicolaou", "Onyeka Obi", "Aayush Rajasekaran", "Alejandro Bergasov", "Aleksandr Bezobchuk", "Kishori M. Konwar", "Michael Meier", "Santiago Paiva", "Har Preet Singh", "Swarnabha Sinha"], "title": "OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks", "categories": ["cs.DC", "E.4, C.2.4"], "comment": null, "summary": "Gossip algorithms are pivotal in the dissemination of information within\ndecentralized systems. Consequently, numerous gossip libraries have been\ndeveloped and widely utilized especially in blockchain protocols for the\npropagation of blocks and transactions. A well-established library is libp2p,\nwhich provides two gossip algorithms: floodsup and gossibsup. These algorithms\nenable the delivery of published messages to a set of peers. In this work we\naim to enhance the performance and reliability of libp2p by introducing\nOPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random\nLinear Network Coding (RLNC) to expedite the dissemination of information in a\npeer-to-peer (P2P) network while ensuring reliable delivery, even in the\npresence of malicious actors capable of corrupting the transmitted data.\nPreliminary research from the Ethereum Foundation has demonstrated the use of\nRLNC in the significant improvement in the block propagation time [14]. Here we\npresent extensive evaluation results both in simulation and real-world\nenvironments that demonstrate the performance gains of OPTIMUMP2P over the\nGossipsub protocol.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPTIMUMP2P\u7684\u65b0\u6d41\u8a00\u7b97\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u7f51\u7edc\u7f16\u7801\uff08RLNC\uff09\u63d0\u5347\u4fe1\u606f\u4f20\u64ad\u6548\u7387\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709Gossipsub\u534f\u8bae\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d41\u8a00\u7b97\u6cd5\uff08\u5982floodsup\u548cgossibsup\uff09\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u867d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5728\u6027\u80fd\u548c\u4fe1\u606f\u4f20\u9012\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u5c24\u5176\u662f\u5728\u9762\u5bf9\u6076\u610f\u884c\u4e3a\u5e72\u6270\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u53ef\u9760\u3002", "method": "OPTIMUMP2P\u91c7\u7528\u968f\u673a\u7ebf\u6027\u7f51\u7edc\u7f16\u7801\uff08RLNC\uff09\u6280\u672f\uff0c\u52a0\u901fP2P\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u4f20\u64ad\uff0c\u540c\u65f6\u786e\u4fdd\u5373\u4f7f\u5728\u6076\u610f\u6570\u636e\u7be1\u6539\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u53ef\u9760\u4f20\u9012\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPTIMUMP2P\u5728\u533a\u5757\u4f20\u64ad\u65f6\u95f4\u4e0a\u663e\u8457\u4f18\u4e8eGossipsub\u534f\u8bae\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "OPTIMUMP2P\u901a\u8fc7\u5f15\u5165RLNC\u6280\u672f\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u4fe1\u606f\u4f20\u64ad\u901f\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u8fd8\u5c55\u73b0\u4e86\u5728\u533a\u5757\u94fe\u7b49\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.04820", "pdf": "https://arxiv.org/pdf/2508.04820", "abs": "https://arxiv.org/abs/2508.04820", "authors": ["Mayra Sofia Ruiz Rodriguez", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Logging is essential in software development, helping developers monitor\nsystem behavior and aiding in debugging applications. Given the ability of\nlarge language models (LLMs) to generate natural language and code, researchers\nare exploring their potential to generate log statements. However, prior work\nfocuses on evaluating logs introduced in code functions, leaving file-level log\ngeneration underexplored -- especially in machine learning (ML) applications,\nwhere comprehensive logging can enhance reliability. In this study, we evaluate\nthe capacity of GPT-4o mini as a case study to generate log statements for ML\nprojects at file level. We gathered a set of 171 ML repositories containing\n4,073 Python files with at least one log statement. We identified and removed\nthe original logs from the files, prompted the LLM to generate logs for them,\nand evaluated both the position of the logs and log level, variables, and text\nquality of the generated logs compared to human-written logs. In addition, we\nmanually analyzed a representative sample of generated logs to identify common\npatterns and challenges. We find that the LLM introduces logs in the same place\nas humans in 63.91% of cases, but at the cost of a high overlogging rate of\n82.66%. Furthermore, our manual analysis reveals challenges for file-level\nlogging, which shows overlogging at the beginning or end of a function,\ndifficulty logging within large code blocks, and misalignment with\nproject-specific logging conventions. While the LLM shows promise for\ngenerating logs for complete files, these limitations remain to be addressed\nfor practical implementation.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86GPT-4o mini\u5728\u6587\u4ef6\u7ea7\u522b\u4e3aML\u9879\u76ee\u751f\u6210\u65e5\u5fd7\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u80fd\u90e8\u5206\u6a21\u4eff\u4eba\u7c7b\u65e5\u5fd7\u4f4d\u7f6e\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u65e5\u5fd7\u8bb0\u5f55\u7b49\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u4ef6\u7ea7\u522b\u65e5\u5fd7\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u3002", "method": "\u6536\u96c6171\u4e2aML\u4ed3\u5e93\u7684Python\u6587\u4ef6\uff0c\u79fb\u9664\u539f\u6709\u65e5\u5fd7\u540e\u7531LLM\u751f\u6210\u65e5\u5fd7\uff0c\u5e76\u5bf9\u6bd4\u5206\u6790\u65e5\u5fd7\u4f4d\u7f6e\u3001\u7ea7\u522b\u3001\u53d8\u91cf\u548c\u6587\u672c\u8d28\u91cf\u3002", "result": "LLM\u572863.91%\u7684\u60c5\u51b5\u4e0b\u4e0e\u4eba\u7c7b\u65e5\u5fd7\u4f4d\u7f6e\u4e00\u81f4\uff0c\u4f46\u8fc7\u5ea6\u65e5\u5fd7\u7387\u9ad8\u8fbe82.66%\uff0c\u4e14\u5b58\u5728\u5176\u4ed6\u95ee\u9898\u5982\u4ee3\u7801\u5757\u5185\u65e5\u5fd7\u8bb0\u5f55\u56f0\u96be\u3002", "conclusion": "LLM\u5728\u6587\u4ef6\u7ea7\u522b\u65e5\u5fd7\u751f\u6210\u4e2d\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5982\u8fc7\u5ea6\u65e5\u5fd7\u548c\u9879\u76ee\u9002\u914d\u7b49\u9650\u5236\u3002"}}
{"id": "2508.04917", "pdf": "https://arxiv.org/pdf/2508.04917", "abs": "https://arxiv.org/abs/2508.04917", "authors": ["Atharva Gondhalekar", "Kjetil Haugen", "Thomas Gibson", "Wu-chun Feng"], "title": "Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition", "categories": ["cs.PF", "cs.NA", "math.NA", "G.1.3; D.1.3"], "comment": "14 pages, 14 figures", "summary": "Sparse linear systems are typically solved using preconditioned iterative\nmethods, but applying preconditioners via sparse triangular solves introduces\nbottlenecks due to irregular memory accesses and data dependencies. This work\nleverages fine-grained domain decomposition to adapt triangular solves to the\nGPU architecture. We develop a fine-grained domain decomposition strategy that\ngenerates non-overlapping subdomains, increasing parallelism in the application\nof preconditioner at the expense of a modest increase in the iteration count\nfor convergence. Each subdomain is assigned to a thread block and is sized such\nthat the subdomain vector fits in the GPU shared memory, eliminating the need\nfor inter-block synchronization and reducing irregular global memory accesses.\nCompared to other state-of-the-art implementations using the ROCm$^{\\text{TM}}$\nsoftware stack, we achieve a 10.7$\\times$ speedup for triangular solves and a\n3.2$\\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized\n(BiCGSTAB) solver on the AMD Instinct$^{\\text{TM}}$ MI210 GPU.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u57df\u5206\u89e3\u7684\u7b56\u7565\uff0c\u4ee5\u4f18\u5316GPU\u4e0a\u7684\u7a00\u758f\u4e09\u89d2\u6c42\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u7684\u9884\u5904\u7406\u8fed\u4ee3\u65b9\u6cd5\u4e2d\uff0c\u7a00\u758f\u4e09\u89d2\u6c42\u89e3\u7531\u4e8e\u5185\u5b58\u8bbf\u95ee\u4e0d\u89c4\u5f8b\u548c\u6570\u636e\u4f9d\u8d56\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002", "method": "\u91c7\u7528\u7ec6\u7c92\u5ea6\u57df\u5206\u89e3\u6280\u672f\u751f\u6210\u975e\u91cd\u53e0\u5b50\u57df\uff0c\u6bcf\u4e2a\u5b50\u57df\u7531\u7ebf\u7a0b\u5757\u5904\u7406\u5e76\u9002\u914dGPU\u5171\u4eab\u5185\u5b58\uff0c\u4ee5\u51cf\u5c11\u540c\u6b65\u548c\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\u3002", "result": "\u5728AMD GPU\u4e0a\u5b9e\u73b0\u4e86\u4e09\u89d2\u6c42\u89e310.7\u500d\u548cILU0\u9884\u6761\u4ef6\u7684BiCGSTAB\u6c42\u89e33.2\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86GPU\u4e0a\u9884\u5904\u7406\u8fed\u4ee3\u6c42\u89e3\u7684\u6027\u80fd\uff0c\u9002\u5408\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u3002"}}
{"id": "2508.05002", "pdf": "https://arxiv.org/pdf/2508.05002", "abs": "https://arxiv.org/abs/2508.05002", "authors": ["Ji Sun", "Guoliang Li", "Peiyao Zhou", "Yihui Ma", "Jingzhe Xu", "Yuan Li"], "title": "AgenticData: An Agentic Data Analytics System for Heterogeneous Data", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Existing unstructured data analytics systems rely on experts to write code\nand manage complex analysis workflows, making them both expensive and\ntime-consuming. To address these challenges, we introduce AgenticData, an\ninnovative agentic data analytics system that allows users to simply pose\nnatural language (NL) questions while autonomously analyzing data sources\nacross multiple domains, including both unstructured and structured data.\nFirst, AgenticData employs a feedback-driven planning technique that\nautomatically converts an NL query into a semantic plan composed of relational\nand semantic operators. We propose a multi-agent collaboration strategy by\nutilizing a data profiling agent for discovering relevant data, a semantic\ncross-validation agent for iterative optimization based on feedback, and a\nsmart memory agent for maintaining short-term context and long-term knowledge.\nSecond, we propose a semantic optimization model to refine and execute semantic\nplans effectively. Our system, AgenticData, has been tested using three\nbenchmarks. Experimental results showed that AgenticData achieved superior\naccuracy on both easy and difficult tasks, significantly outperforming\nstate-of-the-art methods.", "AI": {"tldr": "AgenticData\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6570\u636e\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u81ea\u4e3b\u5206\u6790\u591a\u9886\u57df\u6570\u636e\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u89c4\u5212\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u975e\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u7cfb\u7edf\u4f9d\u8d56\u4e13\u5bb6\u7f16\u5199\u4ee3\u7801\u548c\u7ba1\u7406\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u9ad8\u6210\u672c\u548c\u8017\u65f6\u95ee\u9898\u3002", "method": "1. \u53cd\u9988\u9a71\u52a8\u89c4\u5212\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u8bed\u4e49\u8ba1\u5212\uff1b2. \u591a\u4ee3\u7406\u534f\u4f5c\uff08\u6570\u636e\u5256\u6790\u4ee3\u7406\u3001\u8bed\u4e49\u4ea4\u53c9\u9a8c\u8bc1\u4ee3\u7406\u548c\u667a\u80fd\u8bb0\u5fc6\u4ee3\u7406\uff09\uff1b3. \u8bed\u4e49\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgenticData\u5728\u7b80\u5355\u548c\u590d\u6742\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AgenticData\u901a\u8fc7\u81ea\u4e3b\u5206\u6790\u548c\u4f18\u5316\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6570\u636e\u5206\u6790\u7684\u95e8\u69db\u548c\u6210\u672c\u3002"}}
{"id": "2508.04865", "pdf": "https://arxiv.org/pdf/2508.04865", "abs": "https://arxiv.org/abs/2508.04865", "authors": ["Aleksander Boruch-Gruszecki", "Yangtian Zi", "Zixuan Wu", "Tejas Oberoi", "Carolyn Jane Anderson", "Joydeep Biswas", "Arjun Guha"], "title": "Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment", "categories": ["cs.LG", "cs.PL"], "comment": "18 pages, 19 figures. For artifacts, see https://agnostics.abgru.me", "summary": "Large language models (LLMs) already excel at writing code in high-resource\nlanguages such as Python and JavaScript, yet stumble on low-resource languages\nthat remain essential to science and engineering. Besides the obvious shortage\nof pre-training data, post-training itself is a bottleneck: every new language\nseems to require new datasets, test harnesses, and reinforcement-learning (RL)\ninfrastructure.\n  We introduce Agnostics, a language-agnostic post-training pipeline that\neliminates this per-language engineering. The key idea is to judge code solely\nby its externally observable behavior, so a single verifier can test solutions\nwritten in any language. Concretely, we (i) use an LLM to rewrite existing\nunit-test datasets into an I/O format, (ii) supply a short configuration that\ntells the verifier how to compile and run a target language, and (iii) apply\nreinforcement learning with verifiable rewards (RLVR) in a robust code\nexecution environment.\n  Applied to five low-resource languages--Lua, Julia, R, OCaml, and\nFortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other\n16B-70B open-weight models; (2) scales cleanly to larger and diverse model\nfamilies (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for\n${\\le} 16$B parameter models, sets new state-of-the-art pass@1 results on\nMultiPL-E and a new multi-language version LiveCodeBench that we introduce.\n  We will release the language-agnostic training datasets (Ag-MBPP-X,\nAg-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use\nconfigurations, making RL post-training in any programming language as simple\nas editing a short YAML file.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Agnostics\uff0c\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u5916\u90e8\u884c\u4e3a\u9a8c\u8bc1\u4ee3\u7801\uff0c\u907f\u514d\u4e86\u9488\u5bf9\u6bcf\u79cd\u8bed\u8a00\u7684\u989d\u5916\u5de5\u7a0b\u5de5\u4f5c\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u907f\u514d\u4e3a\u6bcf\u79cd\u8bed\u8a00\u5355\u72ec\u6784\u5efa\u6570\u636e\u96c6\u548cRL\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u4f7f\u7528LLM\u5c06\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u6570\u636e\u96c6\u8f6c\u6362\u4e3aI/O\u683c\u5f0f\uff0c\u63d0\u4f9b\u7b80\u77ed\u7684\u914d\u7f6e\u8bf4\u660e\u5982\u4f55\u7f16\u8bd1\u548c\u8fd0\u884c\u76ee\u6807\u8bed\u8a00\uff0c\u5e76\u5728\u7a33\u5065\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\u4e2d\u5e94\u7528\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002", "result": "\u5728\u4e94\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Lua\u3001Julia\u3001R\u3001OCaml\u548cFortran\uff09\u4e0a\uff0cAgnostics\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u572816B\u53c2\u6570\u4ee5\u4e0b\u7684\u6a21\u578b\u4e2d\u521b\u9020\u4e86\u65b0\u7684state-of-the-art\u7ed3\u679c\u3002", "conclusion": "Agnostics\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4e3a\u6bcf\u79cd\u8bed\u8a00\u5355\u72ec\u5f00\u53d1\uff0c\u7b80\u5316\u4e86\u5728\u4efb\u610f\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u540e\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u4e14\u76f8\u5173\u8d44\u6e90\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.05350", "pdf": "https://arxiv.org/pdf/2508.05350", "abs": "https://arxiv.org/abs/2508.05350", "authors": ["Federica Di Stefano", "Quentin Mani\u00e8re", "Magdalena Ortiz", "Mantas \u0160imkus"], "title": "Minimal Model Reasoning in Description Logics: Don't Try This at Home!", "categories": ["cs.AI", "cs.CC", "cs.LO"], "comment": "44 pages", "summary": "Reasoning with minimal models has always been at the core of many knowledge\nrepresentation techniques, but we still have only a limited understanding of\nthis problem in Description Logics (DLs). Minimization of some selected\npredicates, letting the remaining predicates vary or be fixed, as proposed in\ncircumscription, has been explored and exhibits high complexity. The case of\n`pure' minimal models, where the extension of all predicates must be minimal,\nhas remained largely uncharted. We address this problem in popular DLs and\nobtain surprisingly negative results: concept satisfiability in minimal models\nis undecidable already for $\\mathcal{EL}$. This undecidability also extends to\na very restricted fragment of tuple-generating dependencies. To regain\ndecidability, we impose acyclicity conditions on the TBox that bring the\nworst-case complexity below double exponential time and allow us to establish a\nconnection with the recently studied pointwise circumscription; we also derive\nresults in data complexity. We conclude with a brief excursion to the DL-Lite\nfamily, where a positive result was known for DL-Lite$_{\\text{core}}$, but our\ninvestigation establishes ExpSpace-hardness already for its extension\nDL-Lite$_{\\text{horn}}$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u63cf\u8ff0\u903b\u8f91\uff08DLs\uff09\u4e2d\u7684\u7eaf\u6781\u5c0f\u6a21\u578b\u63a8\u7406\u95ee\u9898\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u7b80\u5355\u7684DLs\uff08\u5982$\u2060mathcal{EL}$\uff09\u4e2d\uff0c\u6982\u5ff5\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u4e5f\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002\u901a\u8fc7\u65bd\u52a0\u975e\u5faa\u73af\u6761\u4ef6\uff0c\u53ef\u4ee5\u6062\u590d\u53ef\u5224\u5b9a\u6027\uff0c\u4f46\u590d\u6742\u5ea6\u8f83\u9ad8\u3002", "motivation": "\u7814\u7a76\u7eaf\u6781\u5c0f\u6a21\u578b\u5728\u63cf\u8ff0\u903b\u8f91\u4e2d\u7684\u63a8\u7406\u95ee\u9898\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u957f\u671f\u672a\u6df1\u5165\u63a2\u7d22\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5176\u590d\u6742\u6027\u548c\u4e0d\u53ef\u5224\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7eaf\u6781\u5c0f\u6a21\u578b\u5728\u6d41\u884c\u63cf\u8ff0\u903b\u8f91\u4e2d\u7684\u7279\u6027\uff0c\u7814\u7a76\u5176\u6982\u5ff5\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u901a\u8fc7\u65bd\u52a0\u975e\u5faa\u73af\u6761\u4ef6\u6765\u6062\u590d\u53ef\u5224\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0$\u2060mathcal{EL}$\u4e2d\u7684\u6982\u5ff5\u53ef\u6ee1\u8db3\u6027\u5728\u7eaf\u6781\u5c0f\u6a21\u578b\u4e0b\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff1b\u901a\u8fc7\u975e\u5faa\u73af\u6761\u4ef6\uff0c\u53ef\u5224\u5b9a\u6027\u5f97\u4ee5\u6062\u590d\uff0c\u4f46\u590d\u6742\u5ea6\u8f83\u9ad8\uff08\u4f4e\u4e8e\u53cc\u6307\u6570\u65f6\u95f4\uff09\u3002", "conclusion": "\u7eaf\u6781\u5c0f\u6a21\u578b\u5728\u63cf\u8ff0\u903b\u8f91\u4e2d\u7684\u63a8\u7406\u95ee\u9898\u8868\u73b0\u51fa\u6781\u9ad8\u7684\u590d\u6742\u6027\uff0c\u5373\u4f7f\u7b80\u5355\u903b\u8f91\u4e5f\u65e0\u6cd5\u907f\u514d\u4e0d\u53ef\u5224\u5b9a\u6027\u3002\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u9650\u5236\u6761\u4ef6\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.04787", "pdf": "https://arxiv.org/pdf/2508.04787", "abs": "https://arxiv.org/abs/2508.04787", "authors": ["Vishnu Menon", "Andy Cherney", "Elizabeth B. Cloude", "Li Zhang", "Tiffany D. Do"], "title": "Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to NCME Special Interest Group on AI in Measurement:\n  AIME-CON 2025 conference", "summary": "This study examined whether embedding LLM-guided reflection prompts in an\ninteractive AI-generated podcast improved learning and user experience compared\nto a version without prompts. Thirty-six undergraduates participated, and while\nlearning outcomes were similar across conditions, reflection prompts reduced\nperceived attractiveness, highlighting a call for more research on reflective\ninteractivity design.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728AI\u751f\u6210\u7684\u64ad\u5ba2\u4e2d\u5d4c\u5165LLM\u5f15\u5bfc\u7684\u53cd\u601d\u63d0\u793a\u662f\u5426\u6bd4\u65e0\u63d0\u793a\u7248\u672c\u66f4\u80fd\u63d0\u5347\u5b66\u4e60\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u63a2\u8ba8\u53cd\u601d\u63d0\u793a\u5bf9\u5b66\u4e60\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "36\u540d\u672c\u79d1\u751f\u53c2\u4e0e\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u6709\u63d0\u793a\u548c\u65e0\u63d0\u793a\u7248\u672c\u7684\u64ad\u5ba2\u3002", "result": "\u5b66\u4e60\u6548\u679c\u76f8\u4f3c\uff0c\u4f46\u53cd\u601d\u63d0\u793a\u964d\u4f4e\u4e86\u7528\u6237\u7684\u611f\u77e5\u5438\u5f15\u529b\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u53cd\u601d\u4ea4\u4e92\u8bbe\u8ba1\u3002"}}
{"id": "2508.05473", "pdf": "https://arxiv.org/pdf/2508.05473", "abs": "https://arxiv.org/abs/2508.05473", "authors": ["Sam Kouteili", "Hiren Madhu", "George Typaldos", "Mark Santolucito"], "title": "Embedding Alignment in Code Generation for Audio", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u591a\u53d8\u7684\u521b\u610f\u7f16\u7801\uff08\u5982\u73b0\u573a\u7f16\u7801\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u4e0e\u97f3\u9891\u5d4c\u5165\u7a7a\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u7684\u97f3\u4e50\u591a\u6837\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u5efa\u7acb\u4ee3\u7801\u4e0e\u97f3\u9891\u5d4c\u5165\u7a7a\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u5b9e\u73b0\u97f3\u4e50\u610f\u56fe\uff0c\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5728\u63d0\u4f9b\u591a\u6837\u6027\u548c\u72ec\u7279\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8c03\u67e5\u4ee3\u7801\u4e0e\u97f3\u9891\u5d4c\u5165\u7a7a\u95f4\u7684\u62d3\u6251\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u9884\u6d4b\u6a21\u578b\u6765\u5b66\u4e60\u5d4c\u5165\u5bf9\u9f50\u6620\u5c04\uff0c\u4ee5\u9884\u6d4b\u4ee3\u7801\u751f\u6210\u7684\u97f3\u9891\u5d4c\u5165\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u4e0e\u97f3\u9891\u5d4c\u5165\u4e4b\u95f4\u4e0d\u5b58\u5728\u7b80\u5355\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u7684\u9884\u6d4b\u6a21\u578b\u5b66\u4e60\u5d4c\u5165\u5bf9\u9f50\u6620\u5c04\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u6784\u5efa\u4ee3\u7801-\u97f3\u9891\u5d4c\u5165\u5bf9\u9f50\u6620\u5c04\u6a21\u578b\uff0c\u80fd\u591f\u589e\u5f3a\u751f\u6210\u4ee3\u7801\u7684\u97f3\u4e50\u591a\u6837\u6027\uff0c\u4ece\u800c\u66f4\u597d\u5730\u652f\u6301\u521b\u610f\u7f16\u7801\u4efb\u52a1\u3002"}}
{"id": "2508.05354", "pdf": "https://arxiv.org/pdf/2508.05354", "abs": "https://arxiv.org/abs/2508.05354", "authors": ["Michael Rogenmoser", "Angelo Garofalo", "Luca Benini"], "title": "relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication", "categories": ["cs.AR"], "comment": "2 pages extended abstract, accepted at IIRW 2025", "summary": "On-chip communication is a critical element of modern systems-on-chip (SoCs),\nallowing processor cores to interact with memory and peripherals. Interconnects\nrequire special care in radiation-heavy environments, as any soft error within\nthe SoC interconnect is likely to cause a functional failure of the whole SoC.\nThis work proposes relOBI, an extension to Open Bus Interface (OBI) combining\ntriple modular redundancy (TMR) for critical handshake signals with error\ncorrection codes (ECC) protection on other signals for complete reliability.\nImplementing and testing a fully reliable crossbar shows improved reliability\nto injected faults from a vulnerability of 34.85 % to 0 % compared to a\nreference design, with an area increase of 2.6x and 1.4x timing impact. The\narea overhead is 1.8x lower than that reported in the literature for\nfine-grained triplication and voting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3arelOBI\u7684\u6269\u5c55Open Bus Interface\uff08OBI\uff09\u65b9\u6848\uff0c\u7ed3\u5408\u4e09\u91cd\u6a21\u5757\u5197\u4f59\uff08TMR\uff09\u548c\u7ea0\u9519\u7801\uff08ECC\uff09\u4fdd\u62a4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SoC\u4e92\u8fde\u5728\u8f90\u5c04\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u8f90\u5c04\u73af\u5883\u4e2d\uff0cSoC\u4e92\u8fde\u7684\u8f6f\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u6574\u4e2a\u7cfb\u7edf\u529f\u80fd\u6545\u969c\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u53ef\u9760\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51farelOBI\uff0c\u7ed3\u5408TMR\u548cECC\u4fdd\u62a4\uff0c\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u9760\u7684\u4ea4\u53c9\u5f00\u5173\u3002", "result": "\u76f8\u6bd4\u53c2\u8003\u8bbe\u8ba1\uff0c\u6ce8\u5165\u6545\u969c\u7684\u8106\u5f31\u6027\u4ece34.85%\u964d\u4f4e\u52300%\uff0c\u4f46\u9762\u79ef\u589e\u52a0\u4e862.6\u500d\uff0c\u65f6\u5e8f\u5f71\u54cd\u589e\u52a0\u4e861.4\u500d\u3002", "conclusion": "relOBI\u5728\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u9762\u79ef\u5f00\u9500\u6bd4\u6587\u732e\u4e2d\u62a5\u544a\u7684\u7ec6\u7c92\u5ea6TMR\u65b9\u6848\u4f4e1.8\u500d\u3002"}}
{"id": "2508.04965", "pdf": "https://arxiv.org/pdf/2508.04965", "abs": "https://arxiv.org/abs/2508.04965", "authors": ["Zijian Wang", "Beizhen Zhao", "Hao Wang"], "title": "Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable\ncapabilities in real-time and photorealistic novel view synthesis. However,\ntraditional 3DGS representations often struggle with large-scale scene\nmanagement and efficient storage, particularly when dealing with complex\nenvironments or limited computational resources. To address these limitations,\nwe introduce a novel perceive-sample-compress framework for 3D Gaussian\nSplatting. Specifically, we propose a scene perception compensation algorithm\nthat intelligently refines Gaussian parameters at each level. This algorithm\nintelligently prioritizes visual importance for higher fidelity rendering in\ncritical areas, while optimizing resource usage and improving overall visible\nquality. Furthermore, we propose a pyramid sampling representation to manage\nGaussian primitives across hierarchical levels. Finally, to facilitate\nefficient storage of proposed hierarchical pyramid representations, we develop\na Generalized Gaussian Mixed model compression algorithm to achieve significant\ncompression ratios without sacrificing visual fidelity. The extensive\nexperiments demonstrate that our method significantly improves memory\nefficiency and high visual quality while maintaining real-time rendering speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u611f\u77e5-\u91c7\u6837-\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u573a\u666f\u7ba1\u7406\u548c\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5927\u89c4\u6a21\u573a\u666f\u7ba1\u7406\u548c\u5b58\u50a8\u6548\u7387\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u6216\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u3002", "method": "\u901a\u8fc7\u573a\u666f\u611f\u77e5\u8865\u507f\u7b97\u6cd5\u548c\u91d1\u5b57\u5854\u91c7\u6837\u8868\u793a\u4f18\u5316\u9ad8\u65af\u53c2\u6570\uff0c\u63d0\u51fa\u901a\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u538b\u7f29\u7b97\u6cd5\u4ee5\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u8d44\u6e90\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6548\u679c\u3002"}}
{"id": "2508.05014", "pdf": "https://arxiv.org/pdf/2508.05014", "abs": "https://arxiv.org/abs/2508.05014", "authors": ["Yunwen Liu", "Jiang Xiao"], "title": "Wave Computing based on Dynamical Networks: Applications in Optimization Problems", "categories": ["cs.ET"], "comment": "20 pages, 4 figures", "summary": "We develop a computing framework that leverages wave propagation within an\ninterconnected network, where nodes and edges possess wave manipulation\ncapabilities, such as frequency mixing or time delay. This computing paradigm\ncan not only achieve intrinsic parallelism like existing works by the\nexploration of an exponential number of possibilities simultaneously with very\nsmall number of hardware units, but also extend this unique characteristic to a\nmultidimensional space including spatial, temporal and frequency domains,\nmaking it particularly effective for addressing NP-hard problems. The proposed\narchitecture has been validated through SPICE simulations, demonstrating its\npotential capability in solving several NP-hard problems, such as the Number\nPartitioning Problem, the 0/1 Knapsack Problem, and the Traveling Salesman\nProblem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6ce2\u4f20\u64ad\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u548c\u8fb9\u7684\u6ce2\u64cd\u4f5c\u80fd\u529b\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\uff0c\u6709\u6548\u89e3\u51b3NP\u96be\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u80fd\u591f\u5728\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u9891\u7387\u7b49\u591a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u89e3\u51b3NP\u96be\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6ce2\u4f20\u64ad\u7684\u7f51\u7edc\u8ba1\u7b97\u6846\u67b6\uff0c\u8282\u70b9\u548c\u8fb9\u5177\u6709\u6ce2\u64cd\u4f5c\u80fd\u529b\uff08\u5982\u9891\u7387\u6df7\u5408\u6216\u65f6\u5ef6\uff09\uff0c\u5e76\u901a\u8fc7SPICE\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6a21\u62df\u4e2d\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4e2aNP\u96be\u95ee\u9898\uff0c\u5305\u62ec\u6570\u5b57\u5212\u5206\u95ee\u9898\u30010/1\u80cc\u5305\u95ee\u9898\u548c\u65c5\u884c\u5546\u95ee\u9898\u3002", "conclusion": "\u8be5\u8ba1\u7b97\u6846\u67b6\u901a\u8fc7\u6ce2\u4f20\u64ad\u7684\u591a\u7ef4\u5e76\u884c\u6027\uff0c\u4e3aNP\u96be\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u786c\u4ef6\u53cb\u597d\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2508.04870", "pdf": "https://arxiv.org/pdf/2508.04870", "abs": "https://arxiv.org/abs/2508.04870", "authors": ["Khaled Jawhar", "Evangelos Kranakis"], "title": "Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model", "categories": ["cs.DC"], "comment": null, "summary": "We consider linear search for capturing an oblivious moving target by two\nautonomous robots with different communicating abilities. Both robots can\ncommunicate Face-to-Face (F2F) when co-located but in addition one robot is a\nSender (can also send messages wirelessly) and the other also a Receiver (can\nalso receive messages wirelessly). This is known as Sender/Receiver (S/R, for\nshort) communication model. The robots can move with max speed $1$. The moving\ntarget starts at distance $d$ from the origin and can move either with speed\n$v<1$ away from the origin in the ``away'' model or with speed $v \\geq 0$\ntoward the origin in the ``toward'' model. We assume that the direction of\nmotion of the target (i.e., whether it is the away or toward model) is known to\nthe robots in advance. To capture the target the two robots must be co-located\nwith it.\n  We design new linear search algorithms and analyze the competitive ratio of\nthe time required to capture the target. The approach takes into account\nvarious scenarios related to what the robots know about the search environment\n(e.g., starting distance or speed of the mobile, away or toward model, or a\ncombination thereof). Our study contributes to understanding how asymmetric\ncommunication affects the competitive ratio of linear search.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u901a\u4fe1\u80fd\u529b\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u7ebf\u6027\u641c\u7d22\u6355\u83b7\u79fb\u52a8\u76ee\u6807\uff0c\u5206\u6790\u4e86\u5728\u4e0d\u540c\u901a\u4fe1\u6a21\u578b\u548c\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u6355\u83b7\u65f6\u95f4\u7684\u6700\u4f18\u7ade\u4e89\u6bd4\u3002", "motivation": "\u63a2\u8ba8\u4e86\u5728\u975e\u5bf9\u79f0\u901a\u4fe1\u6a21\u578b\u4e0b\uff0c\u4e24\u4e2a\u673a\u5668\u4eba\u5982\u4f55\u9ad8\u6548\u534f\u4f5c\u6355\u83b7\u79fb\u52a8\u76ee\u6807\uff0c\u4ee5\u7406\u89e3\u901a\u4fe1\u80fd\u529b\u5dee\u5f02\u5bf9\u641c\u7d22\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u7ebf\u6027\u641c\u7d22\u7b97\u6cd5\uff0c\u8003\u8651\u4e86\u76ee\u6807\u7684\u8d77\u59cb\u8ddd\u79bb\u3001\u901f\u5ea6\u53ca\u8fd0\u52a8\u65b9\u5411\uff08\u8fdc\u79bb\u6216\u63a5\u8fd1\u539f\u70b9\uff09\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7ade\u4e89\u6bd4\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u975e\u5bf9\u79f0\u901a\u4fe1\u6a21\u578b\uff08Sender/Receiver\uff09\u5bf9\u6355\u83b7\u65f6\u95f4\u7684\u7ade\u4e89\u6bd4\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7b97\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7406\u89e3\u975e\u5bf9\u79f0\u901a\u4fe1\u5bf9\u641c\u7d22\u4efb\u52a1\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.04895", "pdf": "https://arxiv.org/pdf/2508.04895", "abs": "https://arxiv.org/abs/2508.04895", "authors": ["Wentao Lu", "Alexander Senchenko", "Abram Hindle", "Cor-Paul Bezemer"], "title": "Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Modern game studios deliver new builds and patches at a rapid pace,\ngenerating thousands of bug reports, many of which embed gameplay videos. To\nverify and triage these bug reports, developers must watch the submitted\nvideos. This manual review is labour-intensive, slow, and hard to scale. In\nthis paper, we introduce an automated pipeline that reduces each video to a\nsingle frame that best matches the reported bug description, giving developers\ninstant visual evidence that pinpoints the bug.\n  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video\nto a median of just 1.90% of its original frames while still capturing bug\nmoments in 98.79 of cases. These keyframes are then evaluated by a\nvision--language model (GPT-4o), which ranks them based on how well they match\nthe textual bug description and selects the most representative frame. We\nevaluated this approach using real-world developer-submitted gameplay videos\nand JIRA bug reports from a popular First-Person Shooter (FPS) game. The\npipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the\ntop-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =\n0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and\nlowest for Animation & VFX (0.51).\n  By replacing video viewing with an immediately informative image, our\napproach dramatically reduces manual effort and speeds up triage and regression\nchecks, offering practical benefits to quality assurance (QA) teams and\ndevelopers across the game industry.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u901a\u8fc7\u5c06\u6e38\u620f\u89c6\u9891\u7f29\u51cf\u4e3a\u4e0ebug\u63cf\u8ff0\u6700\u5339\u914d\u7684\u5355\u5e27\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u5ba1\u6838\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3\u6e38\u620f\u5f00\u53d1\u4e2d\u56e0\u9891\u7e41\u66f4\u65b0\u548c\u5927\u91cfbug\u62a5\u544a\u5bfc\u81f4\u7684\u4eba\u5de5\u5ba1\u6838\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528FFmpeg\u63d0\u53d6\u5173\u952e\u5e27\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\uff09\u7b5b\u9009\u6700\u5339\u914dbug\u63cf\u8ff0\u7684\u4ee3\u8868\u5e27\u3002", "result": "\u65b9\u6cd5\u5728\u771f\u5b9e\u6e38\u620f\u6570\u636e\u4e2d\u8868\u73b0\u826f\u597d\uff0cF1\u5206\u6570\u4e3a0.79\uff0c\u51c6\u786e\u7387\u4e3a0.89\uff0c\u5c24\u5176\u5728\u5149\u7167\u4e0e\u9634\u5f71\u7c7bbug\u4e2d\u8868\u73b0\u6700\u4f73\uff08F1=0.94\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86QA\u56e2\u961f\u7684\u5de5\u4f5c\u91cf\uff0c\u63d0\u5347\u4e86bug\u5206\u7c7b\u6548\u7387\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6e38\u620f\u884c\u4e1a\u3002"}}
{"id": "2508.05621", "pdf": "https://arxiv.org/pdf/2508.05621", "abs": "https://arxiv.org/abs/2508.05621", "authors": ["Max Hawkins", "Richard Vuduc"], "title": "Back to Bits: Extending Shannon's communication performance framework to computing", "categories": ["cs.PF", "D.4.8; K.6.2"], "comment": "5 pages, 4 figures", "summary": "This work proposes a novel computing performance unit grounded in information\ntheory. Modern computing systems are increasingly diverse, supporting\nlow-precision formats, hardware specialization, and emerging paradigms such as\nanalog, quantum, and reversible logic. Traditional metrics like floating-point\noperations (flops) no longer accurately capture this complexity. We frame\ncomputing as the transformation of information through a channel and define\nperformance in terms of the mutual information between a system's inputs and\noutputs. This approach measures not just the quantity of data processed, but\nthe amount of meaningful information encoded, manipulated, and retained through\ncomputation. Our framework provides a principled, implementation-agnostic\nfoundation for evaluating performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u65b0\u578b\u8ba1\u7b97\u6027\u80fd\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ee5\u4e92\u4fe1\u606f\u8861\u91cf\u7cfb\u7edf\u8f93\u5165\u4e0e\u8f93\u51fa\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u8ba1\u7b97\u7cfb\u7edf\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u65e5\u76ca\u591a\u6837\u5316\uff08\u5982\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u3001\u786c\u4ef6\u4e13\u4e1a\u5316\u3001\u6a21\u62df/\u91cf\u5b50/\u53ef\u9006\u903b\u8f91\u7b49\uff09\uff0c\u4f20\u7edf\u6307\u6807\uff08\u5982\u6d6e\u70b9\u8fd0\u7b97\uff09\u5df2\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u6027\u80fd\u3002", "method": "\u5c06\u8ba1\u7b97\u89c6\u4e3a\u4fe1\u606f\u901a\u8fc7\u4fe1\u9053\u7684\u8f6c\u6362\uff0c\u901a\u8fc7\u8f93\u5165\u4e0e\u8f93\u51fa\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u5b9a\u4e49\u8ba1\u7b97\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u5b9e\u73b0\u65e0\u5173\u7684\u7406\u8bba\u6846\u67b6\uff0c\u53ef\u8861\u91cf\u8ba1\u7b97\u4e2d\u7f16\u7801\u3001\u5904\u7406\u548c\u4fdd\u7559\u7684\u6709\u610f\u4e49\u4fe1\u606f\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u8ba1\u7b97\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u65b0\u5174\u8ba1\u7b97\u8303\u5f0f\u3002"}}
{"id": "2508.05012", "pdf": "https://arxiv.org/pdf/2508.05012", "abs": "https://arxiv.org/abs/2508.05012", "authors": ["Ugur Cetintemel", "Shu Chen", "Alexander W. Lee", "Deepti Raghavan"], "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.", "AI": {"tldr": "SPEAR\u662f\u4e00\u79cd\u8bed\u8a00\u548c\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u4ee3LLM\u6d41\u7a0b\u4e2d\u63d0\u793a\u7ba1\u7406\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u81ea\u9002\u5e94\u548c\u8fd0\u884c\u65f6\u4f18\u5316\u63d0\u793a\uff0c\u63d0\u5347\u6548\u7387\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u4ee3LLM\u6d41\u7a0b\u4e2d\u63d0\u793a\uff08prompt\uff09\u7f3a\u4e4f\u7ed3\u6784\u5316\u548c\u52a8\u6001\u9002\u5e94\u6027\uff0c\u9650\u5236\u4e86\u590d\u7528\u3001\u4f18\u5316\u548c\u8fd0\u884c\u65f6\u63a7\u5236\uff0cSPEAR\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SPEAR\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u4ee3\u6570\u7ed3\u6784\uff0c\u652f\u6301\u52a8\u6001\u63d0\u793a\u8c03\u6574\u548c\u7ed3\u6784\u5316\u63d0\u793a\u7ba1\u7406\uff0c\u5305\u62ec\u8fd0\u884c\u65f6\u63d0\u793a\u4f18\u5316\u3001\u7248\u672c\u5316\u89c6\u56fe\u548c\u65e5\u5fd7\u8bb0\u5f55\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SPEAR\u5728\u4e0d\u540c\u63d0\u793a\u4f18\u5316\u6a21\u5f0f\u4e0b\u7684\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5bf9\u9759\u6001\u63d0\u793a\u548c\u52a8\u6001\u4ee3\u7406\u91cd\u8bd5\u7684\u6539\u8fdb\u3002", "conclusion": "SPEAR\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7ba1\u7406\u548c\u52a8\u6001\u8c03\u6574\u80fd\u529b\uff0c\u4e3aLLM\u6d41\u7a0b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u64cd\u4f5c\u6027\u548c\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2508.04821", "pdf": "https://arxiv.org/pdf/2508.04821", "abs": "https://arxiv.org/abs/2508.04821", "authors": ["Yang Liu", "Thorbj\u00f8rn Mikkelsen", "Zehai Liu", "Gengchen Tian", "Diako Mardanbegi", "Qiushi Zhou", "Hans Gellersen", "Ken Pfeuffer"], "title": "At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp", "categories": ["cs.HC"], "comment": "12 pages, 11 figures, The 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25), September 28-October 01, 2025, Busan,\n  Republic of Korea", "summary": "In 3D user interfaces, reaching out to grab and manipulate something works\ngreat until it is out of reach. Indirect techniques like gaze and pinch offer\nan alternative for distant interaction, but do not provide the same immediacy\nor proprioceptive feedback as direct gestures. To support direct gestures for\nfaraway objects, we introduce SightWarp, an interaction technique that exploits\neye-hand coordination to seamlessly summon object proxies to the user's\nfingertips. The idea is that after looking at a distant object, users either\nshift their gaze to the hand or move their hand into view-triggering the\ncreation of a scaled near-space proxy of the object and its surrounding\ncontext. The proxy remains active until the eye-hand pattern is released. The\nkey benefit is that users always have an option to immediately operate on the\ndistant object through a natural, direct hand gesture. Through a user study of\na 3D object docking task, we show that users can easily employ SightWarp, and\nthat subsequent direct manipulation improves performance over gaze and pinch.\nApplication examples illustrate its utility for 6DOF manipulation,\noverview-and-detail navigation, and world-in-miniature interaction. Our work\ncontributes to expressive and flexible object interactions across near and far\nspaces.", "AI": {"tldr": "SightWarp\u662f\u4e00\u79cd\u5229\u7528\u773c\u624b\u534f\u8c03\u7684\u4ea4\u4e92\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u8fdc\u8ddd\u79bb\u5bf9\u8c61\u7684\u8fd1\u7a7a\u95f4\u4ee3\u7406\uff0c\u5b9e\u73b0\u81ea\u7136\u76f4\u63a5\u7684\u624b\u52bf\u64cd\u4f5c\uff0c\u63d0\u5347\u8fdc\u8ddd\u79bb\u4ea4\u4e92\u7684\u4f53\u9a8c\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8fdc\u8ddd\u79bb\u7269\u4f53\u4ea4\u4e92\u4e2d\u76f4\u63a5\u624b\u52bf\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u64cd\u4f5c\u65b9\u5f0f\u3002", "method": "\u5229\u7528\u773c\u624b\u534f\u8c03\u89e6\u53d1\u8fdc\u8ddd\u79bb\u5bf9\u8c61\u7684\u8fd1\u7a7a\u95f4\u4ee3\u7406\u751f\u6210\uff0c\u652f\u6301\u76f4\u63a5\u624b\u52bf\u64cd\u4f5c\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cSightWarp\u64cd\u4f5c\u76f4\u89c2\uff0c\u76f4\u63a5\u624b\u52bf\u7684\u6027\u80fd\u4f18\u4e8e\u89c6\u7ebf\u548c\u634f\u5408\u3002", "conclusion": "SightWarp\u4e3a\u8fdc\u8fd1\u7a7a\u95f4\u7684\u7269\u4f53\u4ea4\u4e92\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04966", "pdf": "https://arxiv.org/pdf/2508.04966", "abs": "https://arxiv.org/abs/2508.04966", "authors": ["Yifan Zhou", "Beizhen Zhao", "Pengcheng Wu", "Hao Wang"], "title": "Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its\nextension to dynamic scenes introduces significant challenges. Existing dynamic\n3DGS methods suffer from either over-smoothing due to low-rank decomposition or\nfeature collision from high-dimensional grid sampling. This is because of the\ninherent spectral conflicts between preserving motion details and maintaining\ndeformation consistency at different frequency. To address these challenges, we\npropose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.\nOur approach contains three key innovations: a spectral-aware Laplacian\nencoding architecture which merges Hash encoding and Laplacian-based module for\nflexible frequency motion control, an enhanced Gaussian dynamics attribute that\ncompensates for photometric distortions caused by geometric deformation, and an\nadaptive Gaussian split strategy guided by KDTree-based primitive control to\nefficiently query and optimize dynamic areas. Through extensive experiments,\nour method demonstrates state-of-the-art performance in reconstructing complex\ndynamic scenes, achieving better reconstruction fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u663e\u9690\u5f0f\u529f\u80fd\u7684\u65b0\u578b\u52a8\u60013D\u9ad8\u65af\u5206\u5e03\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u7684\u5149\u8c31\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u52a8\u60013D\u9ad8\u65af\u5206\u5e03\u5728\u5efa\u6a21\u52a8\u6001\u573a\u666f\u65f6\u5b58\u5728\u5149\u8c31\u51b2\u7a81\uff0c\u5bfc\u81f4\u8fd0\u52a8\u7ec6\u8282\u4e22\u5931\u6216\u7279\u5f81\u78b0\u649e\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u517c\u987e\u8fd0\u52a8\u7ec6\u8282\u548c\u53d8\u5f62\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408\u4e86Hash\u7f16\u7801\u548c\u62c9\u666e\u62c9\u65af\u6a21\u5757\u7684\u5149\u8c31\u611f\u77e5\u7f16\u7801\u67b6\u6784\u3001\u589e\u5f3a\u7684\u9ad8\u65af\u52a8\u6001\u5c5e\u6027\u4ee5\u53ca\u57fa\u4e8eKDTree\u7684\u81ea\u9002\u5e94\u9ad8\u65af\u5206\u88c2\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u91cd\u5efa\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u60013D\u9ad8\u65af\u5206\u5e03\u7684\u5149\u8c31\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04974", "pdf": "https://arxiv.org/pdf/2508.04974", "abs": "https://arxiv.org/abs/2508.04974", "authors": ["Hoa T. Nguyen", "Muhammad Usman", "Rajkumar Buyya"], "title": "QFOR: A Fidelity-aware Orchestrator for Quantum Computing Environments using Deep Reinforcement Learning", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Quantum cloud computing enables remote access to quantum processors, yet the\nheterogeneity and noise of available quantum hardware create significant\nchallenges for efficient resource orchestration. These issues complicate the\noptimization of quantum task allocation and scheduling, as existing heuristic\nmethods fall short in adapting to dynamic conditions or effectively balancing\nexecution fidelity and time. Here, we propose QFOR, a Quantum Fidelity-aware\nOrchestration of tasks across heterogeneous quantum nodes in cloud-based\nenvironments using Deep Reinforcement learning. We model the quantum task\norchestration as a Markov Decision Process and employ the Proximal Policy\nOptimization algorithm to learn adaptive scheduling policies, using IBM quantum\nprocessor calibration data for noise-aware performance estimation. Our\nconfigurable framework balances overall quantum task execution fidelity and\ntime, enabling adaptation to different operational priorities. Extensive\nevaluation demonstrates that QFOR is adaptive and achieves significant\nperformance with 29.5-84% improvements in relative fidelity performance over\nheuristic baselines. Furthermore, it maintains comparable quantum execution\ntimes, contributing to cost-efficient use of quantum computation resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u91cf\u5b50\u4efb\u52a1\u8c03\u5ea6\u6846\u67b6QFOR\uff0c\u5728\u5f02\u6784\u91cf\u5b50\u4e91\u73af\u5883\u4e2d\u4f18\u5316\u4efb\u52a1\u5206\u914d\u4e0e\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50\u4e91\u8ba1\u7b97\u4e2d\u8d44\u6e90\u5f02\u6784\u6027\u548c\u566a\u58f0\u5bfc\u81f4\u4efb\u52a1\u8c03\u5ea6\u4f18\u5316\u56f0\u96be\uff0c\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u6216\u5e73\u8861\u6267\u884c\u4fdd\u771f\u5ea6\u4e0e\u65f6\u95f4\u3002", "method": "\u5c06\u91cf\u5b50\u4efb\u52a1\u8c03\u5ea6\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5b66\u4e60\u81ea\u9002\u5e94\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u57fa\u4e8eIBM\u91cf\u5b50\u5904\u7406\u5668\u6821\u51c6\u6570\u636e\u8fdb\u884c\u566a\u58f0\u611f\u77e5\u6027\u80fd\u4f30\u8ba1\u3002", "result": "QFOR\u5728\u76f8\u5bf9\u4fdd\u771f\u5ea6\u6027\u80fd\u4e0a\u6bd4\u542f\u53d1\u5f0f\u57fa\u7ebf\u63d0\u534729.5-84%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u91cf\u5b50\u6267\u884c\u65f6\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u3002", "conclusion": "QFO R\u6846\u67b6\u80fd\u52a8\u6001\u9002\u5e94\u64cd\u4f5c\u4f18\u5148\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u91cf\u5b50\u4efb\u52a1\u8c03\u5ea6\u6027\u80fd\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.04944", "pdf": "https://arxiv.org/pdf/2508.04944", "abs": "https://arxiv.org/abs/2508.04944", "authors": ["Craig Barnes", "Kyle Burton", "Michael S. Fitzsimons", "Hara Prasad Juvvala", "Brienna Larrick", "Christopher Meyer", "Pauline Ribeyre", "Ao Liu", "Clint Malson", "Noah Metoki-Shlubsky", "Andrii Prokhorenkov", "Jawad Qureshi", "Radhika Reddy", "L. Philip Schumm", "Mingfei Shao", "Trevar Simmons", "Alexander VanTol", "Peter Vassilatos", "Aarti Venkat", "Robert L. Grossman"], "title": "Managing, Analyzing and Sharing Research Data with Gen3 Data Commons", "categories": ["cs.DC"], "comment": "20 pages, 9 figures", "summary": "Gen3 is an open-source data platform for building data commons. A data\ncommons is a cloud-based data platform for managing, analyzing, and sharing\ndata with a research community. Gen3 has been used to build over a dozen data\ncommons that in aggregate contain over 28 PB of data and 64 million FAIR data\nobjects. To set up a Gen3 data commons, you first define a data model. Gen3\nthen autogenerates 1) a data portal for searching and exploring data in the\ncommons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs\nfor accessing the data programmatically. Gen3 is built over a small number of\nstandards-based software services, which are designed to support current and\nfuture Gen3 components so that Gen3 can interoperate with other data platforms\nand data ecosystems.", "AI": {"tldr": "Gen3\u662f\u4e00\u4e2a\u5f00\u6e90\u6570\u636e\u5e73\u53f0\uff0c\u7528\u4e8e\u6784\u5efa\u6570\u636e\u5171\u4eab\u7a7a\u95f4\uff0c\u652f\u6301\u7ba1\u7406\u3001\u5206\u6790\u548c\u5171\u4eab\u7814\u7a76\u6570\u636e\u3002", "motivation": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e00\u4e2a\u4e91\u6570\u636e\u5e73\u53f0\uff0c\u5b9e\u73b0\u6570\u636e\u7684FAIR\uff08\u53ef\u67e5\u627e\u3001\u53ef\u8bbf\u95ee\u3001\u53ef\u4e92\u64cd\u4f5c\u3001\u53ef\u91cd\u7528\uff09\u7ba1\u7406\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u6570\u636e\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6570\u636e\u95e8\u6237\u548cAPI\uff0c\u57fa\u4e8e\u6807\u51c6\u5316\u8f6f\u4ef6\u670d\u52a1\u5b9e\u73b0\u5e73\u53f0\u4e92\u64cd\u4f5c\u6027\u3002", "result": "\u5df2\u652f\u6301\u5341\u591a\u4e2a\u6570\u636e\u5171\u4eab\u7a7a\u95f4\uff0c\u7ba1\u7406\u8d85\u8fc728PB\u7684\u6570\u636e\u548c6400\u4e07\u4e2aFAIR\u6570\u636e\u5bf9\u8c61\u3002", "conclusion": "Gen3\u901a\u8fc7\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316\u670d\u52a1\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86\u6570\u636e\u7684\u5171\u4eab\u4e0e\u4e92\u64cd\u4f5c\u3002"}}
{"id": "2508.04921", "pdf": "https://arxiv.org/pdf/2508.04921", "abs": "https://arxiv.org/abs/2508.04921", "authors": ["Zixuan Feng", "Reed Milewicz", "Emerson Murphy-Hill", "Tyler Menezes", "Alexander Serebrenik", "Igor Steinmacher", "Anita Sarma"], "title": "Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities", "categories": ["cs.SE"], "comment": "13 pages, 1 figure", "summary": "Open Source Software communities face a wave of uncertainty as Generative AI\nrapidly transforms how software is created, maintained, and governed. Without\nclear frameworks, communities risk being overwhelmed by the complexity and\nambiguity introduced by GenAI, threatening the collaborative ethos that\nunderpins OSS. We conduct a scenario-driven, conceptual exploration using a\nsocio-technical framework inspired by McLuhan's Tetrad to surface both risks\nand opportunities for community resilience amid GenAI-driven disruption of OSS\ndevelopment across four domains: software practices, documentation, community\nengagement, and governance. By adopting this lens, OSS leaders and researchers\ncan proactively shape the future of their ecosystems, rather than simply\nreacting to technological upheaval.", "AI": {"tldr": "\u5f00\u6e90\u8f6f\u4ef6\u793e\u533a\u9762\u4e34\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u672c\u6587\u901a\u8fc7\u60c5\u666f\u9a71\u52a8\u7684\u6982\u5ff5\u63a2\u7d22\uff0c\u63d0\u51fa\u4e86\u793e\u4f1a\u6280\u672f\u6846\u67b6\u4ee5\u5e94\u5bf9\u98ce\u9669\u548c\u673a\u9047\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u3001\u7ef4\u62a4\u548c\u6cbb\u7406\u65b9\u5f0f\uff0c\u5f00\u6e90\u793e\u533a\u4e9f\u9700\u660e\u786e\u6846\u67b6\u4ee5\u907f\u514d\u88ab\u590d\u6742\u6027\u6df9\u6ca1\uff0c\u540c\u65f6\u4fdd\u6301\u534f\u4f5c\u7cbe\u795e\u3002", "method": "\u91c7\u7528\u57fa\u4e8eMcLuhan\u56db\u5143\u6cd5\u7684\u793e\u4f1a\u6280\u672f\u6846\u67b6\uff0c\u4ece\u8f6f\u4ef6\u5b9e\u8df5\u3001\u6587\u6863\u3001\u793e\u533a\u53c2\u4e0e\u548c\u6cbb\u7406\u56db\u4e2a\u9886\u57df\u8fdb\u884c\u60c5\u666f\u9a71\u52a8\u7684\u6982\u5ff5\u63a2\u7d22\u3002", "result": "\u63d0\u51fa\u4e86\u5f00\u6e90\u793e\u533a\u5728\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u53d8\u9769\u4e2d\u589e\u5f3a\u97e7\u6027\u7684\u98ce\u9669\u548c\u673a\u4f1a\uff0c\u5e2e\u52a9\u9886\u5bfc\u8005\u548c\u7814\u7a76\u8005\u4e3b\u52a8\u5851\u9020\u751f\u6001\u7cfb\u7edf\u672a\u6765\u3002", "conclusion": "\u901a\u8fc7\u793e\u4f1a\u6280\u672f\u6846\u67b6\uff0c\u5f00\u6e90\u793e\u533a\u53ef\u4ee5\u66f4\u4e3b\u52a8\u5e94\u5bf9\u6280\u672f\u98a0\u8986\uff0c\u800c\u975e\u88ab\u52a8\u53cd\u5e94\uff0c\u4ece\u800c\u7ef4\u62a4\u534f\u4f5c\u7cbe\u795e\u548c\u751f\u6001\u53d1\u5c55\u3002"}}
{"id": "2508.05061", "pdf": "https://arxiv.org/pdf/2508.05061", "abs": "https://arxiv.org/abs/2508.05061", "authors": ["Ruiyuan Zhang", "Chrysanthi Kosyfaki", "Xiaofang Zhou"], "title": "Data-Aware Socratic Query Refinement in Database Systems", "categories": ["cs.DB", "cs.IR"], "comment": null, "summary": "In this paper, we propose Data-Aware Socratic Guidance (DASG), a\ndialogue-based query enhancement framework that embeds \\linebreak interactive\nclarification as a first-class operator within database systems to resolve\nambiguity in natural language queries. DASG treats dialogue as an optimization\ndecision, asking clarifying questions only when the expected execution cost\nreduction exceeds the interaction overhead. The system quantifies ambiguity\nthrough linguistic fuzziness, schema grounding confidence, and projected costs\nacross relational and vector backends. Our algorithm selects the optimal\nclarifications by combining semantic relevance, catalog-based information gain,\nand potential cost reduction. We evaluate our proposed framework on three\ndatasets. The results show that DASG demonstrates improved query precision\nwhile maintaining efficiency, establishing a cooperative analytics paradigm\nwhere systems actively participate in query formulation rather than passively\ntranslating user requests.", "AI": {"tldr": "DASG\u662f\u4e00\u79cd\u901a\u8fc7\u5bf9\u8bdd\u4f18\u5316\u6570\u636e\u5e93\u67e5\u8be2\u7684\u6846\u67b6\uff0c\u4ec5\u5728\u9884\u671f\u6536\u76ca\u8d85\u8fc7\u4ea4\u4e92\u6210\u672c\u65f6\u63d0\u95ee\uff0c\u4ee5\u63d0\u9ad8\u67e5\u8be2\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u901a\u8fc7\u4e3b\u52a8\u5bf9\u8bdd\u4f18\u5316\u67e5\u8be2\uff0c\u800c\u975e\u88ab\u52a8\u7ffb\u8bd1\u7528\u6237\u8bf7\u6c42\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u6a21\u7cca\u6027\u3001\u6a21\u5f0f\u7f6e\u4fe1\u5ea6\u548c\u6210\u672c\u9884\u6d4b\uff0c\u9009\u62e9\u6700\u4f73\u6f84\u6e05\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cDASG\u63d0\u9ad8\u4e86\u67e5\u8be2\u7cbe\u5ea6\u4e14\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "DASG\u5efa\u7acb\u4e86\u7cfb\u7edf\u4e3b\u52a8\u53c2\u4e0e\u67e5\u8be2\u5236\u5b9a\u7684\u534f\u4f5c\u5206\u6790\u8303\u5f0f\u3002"}}
{"id": "2508.04842", "pdf": "https://arxiv.org/pdf/2508.04842", "abs": "https://arxiv.org/abs/2508.04842", "authors": ["Amit Kumar Das", "Mohammad Tarun", "Klaus Mueller"], "title": "Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction", "categories": ["cs.HC"], "comment": "11 pages, 8 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria", "summary": "This paper evaluates the visualization literacy of modern Large Language\nModels (LLMs) and introduces a novel prompting technique called\nCharts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,\nGPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment\nTest (VLAT) using standard prompts and our structured approach. The\nCharts-of-Thought method guides LLMs through a systematic data extraction,\nverification, and analysis process before answering visualization questions.\nOur results show Claude-3.7-sonnet achieved a score of 50.17 using this method,\nfar exceeding the human baseline of 28.82. This approach improved performance\nacross all models, with score increases of 21.8% for GPT-4.5, 9.4% for\nGemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The\nperformance gains were consistent across original and modified VLAT charts,\nwith Claude correctly answering 100% of questions for several chart types that\npreviously challenged LLMs. Our study reveals that modern multimodal LLMs can\nsurpass human performance on visualization literacy tasks when given the proper\nanalytical framework. These findings establish a new benchmark for LLM\nvisualization literacy and demonstrate the importance of structured prompting\nstrategies for complex visual interpretation tasks. Beyond improving LLM\nvisualization literacy, Charts-of-Thought could also enhance the accessibility\nof visualizations, potentially benefiting individuals with visual impairments\nor lower visualization literacy.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165Charts-of-Thought\u63d0\u793a\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53ef\u89c6\u5316\u7d20\u517b\uff0c\u5e76\u5728VLAT\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\uff0c\u5c24\u5176Claude-3.7-sonnet\u8d85\u8fc7\u4eba\u7c7b\u57fa\u51c6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5LLMs\u7684\u53ef\u89c6\u5316\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u6280\u672f\u63d0\u5347\u5176\u8868\u73b0\u3002", "method": "\u91c7\u7528Charts-of-Thought\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5316\u5f15\u5bfcLLMs\u8fdb\u884c\u6570\u636e\u63d0\u53d6\u3001\u9a8c\u8bc1\u548c\u5206\u6790\uff0c\u518d\u56de\u7b54\u53ef\u89c6\u5316\u95ee\u9898\u3002", "result": "Claude-3.7-sonnet\u5f97\u520650.17\uff0c\u8fdc\u8d85\u4eba\u7c7b\u57fa\u51c628.82\uff1b\u6240\u6709\u6a21\u578b\u8868\u73b0\u5747\u6709\u63d0\u5347\uff0c\u589e\u5e45\u6700\u9ad8\u8fbe21.8%\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u6280\u672f\u4e0d\u4ec5\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u53ef\u89c6\u5316\u7d20\u517b\uff0c\u8fd8\u53ef\u589e\u5f3a\u89c6\u89c9\u4fe1\u606f\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5c24\u5176\u5bf9\u89c6\u89c9\u969c\u788d\u8005\u6216\u4f4e\u7d20\u517b\u4eba\u7fa4\u6709\u76ca\u3002"}}
{"id": "2508.05064", "pdf": "https://arxiv.org/pdf/2508.05064", "abs": "https://arxiv.org/abs/2508.05064", "authors": ["Mahmoud Chick Zaouali", "Todd Charter", "Yehor Karpichev", "Brandon Haworth", "Homayoun Najjjaran"], "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding", "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": null, "summary": "Gaussian Splatting has rapidly emerged as a transformative technique for\nreal-time 3D scene representation, offering a highly efficient and expressive\nalternative to Neural Radiance Fields (NeRF). Its ability to render complex\nscenes with high fidelity has enabled progress across domains such as scene\nreconstruction, robotics, and interactive content creation. More recently, the\nintegration of Large Language Models (LLMs) and language embeddings into\nGaussian Splatting pipelines has opened new possibilities for text-conditioned\ngeneration, editing, and semantic scene understanding. Despite these advances,\na comprehensive overview of this emerging intersection has been lacking. This\nsurvey presents a structured review of current research efforts that combine\nlanguage guidance with 3D Gaussian Splatting, detailing theoretical\nfoundations, integration strategies, and real-world use cases. We highlight key\nlimitations such as computational bottlenecks, generalizability, and the\nscarcity of semantically annotated 3D Gaussian data and outline open challenges\nand future directions for advancing language-guided 3D scene understanding\nusing Gaussian Splatting.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8bed\u8a00\u5f15\u5bfc\u4e0e3D Gaussian Splatting\u6280\u672f\u7684\u7ed3\u5408\uff0c\u63a2\u8ba8\u4e86\u5176\u7406\u8bba\u3001\u96c6\u6210\u7b56\u7565\u53ca\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7684\u6280\u672f\u74f6\u9888\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1Gaussian Splatting\u57283D\u573a\u666f\u8868\u793a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u7814\u7a76\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u7efc\u8ff0\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u4e0eGaussian Splatting\u96c6\u6210\u7684\u7406\u8bba\u3001\u7b56\u7565\u53ca\u6848\u4f8b\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u6280\u672f\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5982\u8ba1\u7b97\u74f6\u9888\u548c\u8bed\u4e49\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684Gaussian Splatting\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u89e3\u51b3\u73b0\u6709\u6311\u6218\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2508.05248", "pdf": "https://arxiv.org/pdf/2508.05248", "abs": "https://arxiv.org/abs/2508.05248", "authors": ["Pradeep Kumar Shukla", "Tanujit Chakraborty", "Mustafa Sari", "Joel Sarout", "Partha Pratim Mandal"], "title": "Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications", "categories": ["physics.geo-ph", "cs.ET", "cs.LG"], "comment": null, "summary": "This study provides an in-depth analysis of time series forecasting methods\nto predict the time-dependent deformation trend (also known as creep) of salt\nrock under varying confining pressure conditions. Creep deformation assessment\nis essential for designing and operating underground storage facilities for\nnuclear waste, hydrogen energy, or radioactive materials. Salt rocks, known for\ntheir mechanical properties like low porosity, low permeability, high\nductility, and exceptional creep and self-healing capacities, were examined\nusing multi-stage triaxial (MSTL) creep data. After resampling, axial strain\ndatasets were recorded at 5--10 second intervals under confining pressure\nlevels ranging from 5 to 35 MPa over 5.8--21 days. Initial analyses, including\nSeasonal-Trend Decomposition (STL) and Granger causality tests, revealed\nminimal seasonality and causality between axial strain and temperature data.\nFurther statistical tests, such as the Augmented Dickey-Fuller (ADF) test,\nconfirmed the stationarity of the data with p-values less than 0.05, and\nwavelet coherence plot (WCP) analysis indicated repeating trends. A suite of\ndeep neural network (DNN) models (Neural Basis Expansion Analysis for Time\nSeries (N-BEATS), Temporal Convolutional Networks (TCN), Recurrent Neural\nNetworks (RNN), and Transformers (TF)) was utilized and compared against\nstatistical baseline models. Predictive performance was evaluated using Root\nMean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage\nError (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE). Results\ndemonstrated that N-BEATS and TCN models outperformed others across various\nstress levels, respectively. DNN models, particularly N-BEATS and TCN, showed a\n15--20\\% improvement in accuracy over traditional analytical models,\neffectively capturing complex temporal dependencies and patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5206\u6790\u4e86\u76d0\u5ca9\u5728\u4e0d\u540c\u56f4\u538b\u6761\u4ef6\u4e0b\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u53d8\u5f62\u8d8b\u52bf\uff08\u8815\u53d8\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u76d0\u5ca9\u7684\u8815\u53d8\u53d8\u5f62\u8bc4\u4f30\u5bf9\u6838\u5e9f\u6599\u3001\u6c22\u80fd\u6216\u653e\u5c04\u6027\u6750\u6599\u7684\u5730\u4e0b\u5b58\u50a8\u8bbe\u65bd\u8bbe\u8ba1\u4e0e\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u591a\u9636\u6bb5\u4e09\u8f74\u8815\u53d8\u6570\u636e\uff0c\u901a\u8fc7\u5b63\u8282\u6027\u8d8b\u52bf\u5206\u89e3\u3001\u683c\u5170\u6770\u56e0\u679c\u68c0\u9a8c\u7b49\u521d\u6b65\u5206\u6790\uff0c\u5e76\u91c7\u7528\u591a\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u5982N-BEATS\u3001TCN\uff09\u8fdb\u884c\u9884\u6d4b\u3002", "result": "N-BEATS\u548cTCN\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u529b\u6c34\u5e73\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u6bd4\u4f20\u7edf\u5206\u6790\u6a21\u578b\u51c6\u786e\u5ea6\u63d0\u9ad8\u4e8615-20%\u3002", "conclusion": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u76d0\u5ca9\u8815\u53d8\u9884\u6d4b\u3002"}}
{"id": "2508.04953", "pdf": "https://arxiv.org/pdf/2508.04953", "abs": "https://arxiv.org/abs/2508.04953", "authors": ["Song Bian", "Saurabh Agarwal", "Md. Tareq Mahmood", "Shivaram Venkataraman"], "title": "Tesserae: Scalable Placement Policies for Deep Learning Workloads", "categories": ["cs.DC", "cs.AI"], "comment": "16 pages, 18 figures", "summary": "Training deep learning (DL) models has become a dominant workload in\ndata-centers and improving resource utilization is a key goal of DL cluster\nschedulers. In order to do this, schedulers typically incorporate placement\npolicies that govern where jobs are placed on the cluster. Existing placement\npolicies are either designed as ad-hoc heuristics or incorporated as\nconstraints within a complex optimization problem and thus either suffer from\nsuboptimal performance or poor scalability. Our key insight is that many\nplacement constraints can be formulated as graph matching problems and based on\nthat we design novel placement policies for minimizing job migration overheads\nand job packing. We integrate these policies into Tesserae and describe how our\ndesign leads to a scalable and effective GPU cluster scheduler. Our\nexperimental results show that Tesserae improves average JCT by up to 1.62x and\nthe Makespan by up to 1.15x compared with the existing schedulers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5339\u914d\u4f18\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u96c6\u7fa4\u8c03\u5ea6\u5668Tesserae\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5728\u6570\u636e\u4e2d\u5fc3\u5360\u6bd4\u9ad8\uff0c\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u662f\u8c03\u5ea6\u5668\u7684\u5173\u952e\u76ee\u6807\u3002\u73b0\u6709\u8c03\u5ea6\u7b56\u7565\u5b58\u5728\u6027\u80fd\u4e0d\u4f73\u6216\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5c06\u653e\u7f6e\u7ea6\u675f\u8f6c\u5316\u4e3a\u56fe\u5339\u914d\u95ee\u9898\uff0c\u8bbe\u8ba1\u65b0\u7684\u653e\u7f6e\u7b56\u7565\u4ee5\u51cf\u5c11\u4efb\u52a1\u8fc1\u79fb\u5f00\u9500\u5e76\u4f18\u5316\u4efb\u52a1\u6253\u5305\u3002", "result": "Tesserae\u5c06\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\uff08JCT\uff09\u63d0\u53471.62\u500d\uff0cMakespan\u63d0\u53471.15\u500d\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u7b56\u7565\u53ef\u6709\u6548\u63d0\u5347\u8c03\u5ea6\u5668\u6027\u80fd\uff0cTesserae\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.04925", "pdf": "https://arxiv.org/pdf/2508.04925", "abs": "https://arxiv.org/abs/2508.04925", "authors": ["Sigma Jahan", "Saurabh Singh Rajput", "Tushar Sharma", "Mohammad Masudur Rahman"], "title": "Taxonomy of Faults in Attention-Based Neural Networks", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Attention mechanisms are at the core of modern neural architectures, powering\nsystems ranging from ChatGPT to autonomous vehicles and driving a major\neconomic impact. However, high-profile failures, such as ChatGPT's nonsensical\noutputs or Google's suspension of Gemini's image generation due to attention\nweight errors, highlight a critical gap: existing deep learning fault\ntaxonomies might not adequately capture the unique failures introduced by\nattention mechanisms. This gap leaves practitioners without actionable\ndiagnostic guidance. To address this gap, we present the first comprehensive\nempirical study of faults in attention-based neural networks (ABNNs). Our work\nis based on a systematic analysis of 555 real-world faults collected from 96\nprojects across ten frameworks, including GitHub, Hugging Face, and Stack\nOverflow. Through our analysis, we develop a novel taxonomy comprising seven\nattention-specific fault categories, not captured by existing work. Our results\nshow that over half of the ABNN faults arise from mechanisms unique to\nattention architectures. We further analyze the root causes and manifestations\nof these faults through various symptoms. Finally, by analyzing symptom-root\ncause associations, we identify four evidence-based diagnostic heuristics that\nexplain 33.0% of attention-specific faults, offering the first systematic\ndiagnostic guidance for attention-based models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u7f51\u7edc\uff08ABNNs\uff09\u4e2d\u7684\u6545\u969c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e03\u79cd\u72ec\u7279\u7684\u6545\u969c\u5206\u7c7b\uff0c\u5e76\u4e3a\u8bca\u65ad\u8fd9\u4e9b\u6545\u969c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6545\u969c\u5206\u7c7b\u672a\u80fd\u5145\u5206\u6355\u6349\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u7684\u72ec\u7279\u6545\u969c\uff0c\u5bfc\u81f4\u5b9e\u8df5\u8005\u7f3a\u4e4f\u6709\u6548\u7684\u8bca\u65ad\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u6765\u81ea96\u4e2a\u9879\u76ee\u3001\u5341\u4e2a\u6846\u67b6\u7684555\u4e2a\u5b9e\u9645\u6545\u969c\uff0c\u5f00\u53d1\u4e86\u4e03\u79cd\u6ce8\u610f\u529b\u7279\u6709\u7684\u6545\u969c\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u5176\u6839\u672c\u539f\u56e0\u548c\u8868\u73b0\u75c7\u72b6\u3002", "result": "\u8d85\u8fc7\u4e00\u534a\u7684ABNNs\u6545\u969c\u7531\u6ce8\u610f\u529b\u673a\u5236\u7279\u6709\u673a\u5236\u5f15\u8d77\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u89e3\u91ca33.0%\u6ce8\u610f\u529b\u7279\u6709\u6545\u969c\u7684\u8bca\u65ad\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86\u6ce8\u610f\u529b\u673a\u5236\u6545\u969c\u5206\u7c7b\u548c\u8bca\u65ad\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6307\u5bfc\u3002"}}
{"id": "2508.05029", "pdf": "https://arxiv.org/pdf/2508.05029", "abs": "https://arxiv.org/abs/2508.05029", "authors": ["Felipe Arambur\u00fa", "William Malpica", "Kaouther Abrougui", "Amin Aramoon", "Romulo Auccapuclla", "Claude Brisson", "Matthijs Brobbel", "Colby Farrell", "Pradeep Garigipati", "Joost Hoozemans", "Supun Kamburugamuve", "Akhil Nair", "Alexander Ocsa", "Johan Peltenburg", "Rub\u00e9n Quesada L\u00f3pez", "Deepak Sihag", "Ahmet Uyar", "Dhruv Vats", "Michael Wendt", "Jignesh M. Patel", "Rodrigo Arambur\u00fa"], "title": "Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement", "categories": ["cs.DC", "cs.DB", "H.2.4"], "comment": "6 Pages,6 Figures", "summary": "Online analytical processing of queries on datasets in the many-terabyte\nrange is only possible with costly distributed computing systems. To decrease\nthe cost and increase the throughput, systems can leverage accelerators such as\nGPUs, which are now ubiquitous in the compute infrastructure. This introduces\nmany challenges, the majority of which are related to when, where, and how to\nbest move data around the system. We present Theseus -- a production-ready\nenterprise-scale distributed accelerator-native query engine designed to\nbalance data movement, memory utilization, and computation in an\naccelerator-based system context. Specialized asynchronous control mechanisms\nare tightly coupled to the hardware resources for the purpose of network\ncommunication, data pre-loading, data spilling across memories and storage, and\nGPU compute tasks. The memory subsystem contains a mechanism for fixed-size\npage-locked host memory allocations to increase throughput and reduce memory\nfragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k\non cloud infrastructure, Theseus outperforms Databricks Photon by up to\n$4\\times$ at cost parity. Theseus is capable of processing all queries of the\nTPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as\n2 DGX A100 640GB nodes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTheseus\u7684\u5206\u5e03\u5f0f\u52a0\u901f\u5668\u539f\u751f\u67e5\u8be2\u5f15\u64ce\uff0c\u65e8\u5728\u4f18\u5316\u6570\u636e\u79fb\u52a8\u3001\u5185\u5b58\u5229\u7528\u548c\u8ba1\u7b97\uff0c\u6027\u80fd\u4f18\u4e8eDatabricks Photon\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u5927\u89c4\u6a21\u6570\u636e\u5728\u7ebf\u5206\u6790\u5904\u7406\u7684\u6210\u672c\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u7cfb\u7edf\u53ef\u4ee5\u4f7f\u7528\u52a0\u901f\u5668\uff08\u5982GPU\uff09\uff0c\u4f46\u6570\u636e\u79fb\u52a8\u7b49\u6311\u6218\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "Theseus\u91c7\u7528\u5f02\u6b65\u63a7\u5236\u673a\u5236\u4e0e\u786c\u4ef6\u8d44\u6e90\u7d27\u5bc6\u7ed3\u5408\uff0c\u4f18\u5316\u7f51\u7edc\u901a\u4fe1\u3001\u6570\u636e\u9884\u52a0\u8f7d\u3001\u5b58\u50a8\u6ea2\u51fa\u548cGPU\u8ba1\u7b97\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u56fa\u5b9a\u5927\u5c0f\u7684\u9875\u9762\u9501\u5b9a\u4e3b\u673a\u5185\u5b58\u5206\u914d\u673a\u5236\u3002", "result": "\u5728TPC-H\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTheseus\u5728\u76f8\u540c\u6210\u672c\u4e0b\u6027\u80fd\u4f18\u4e8eDatabricks Photon\u9ad8\u8fbe4\u500d\uff0c\u5e76\u53ef\u5904\u7406100TB\u89c4\u6a21\u7684\u6570\u636e\u3002", "conclusion": "Theseus\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u4f01\u4e1a\u7ea7\u7684\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u67e5\u8be2\u5f15\u64ce\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2508.04859", "pdf": "https://arxiv.org/pdf/2508.04859", "abs": "https://arxiv.org/abs/2508.04859", "authors": ["Panicz Maciej Godek"], "title": "An Implementation of a Visual Stepper in the GRASP Programming System", "categories": ["cs.HC", "68", "H.5.2"], "comment": "Scheme Workshop 2024 (ICFP), 23 pages", "summary": "The direct purpose of this paper - as its title suggests - is to present how\nthe visual evaluator extension is implemented in the GRASP programming system.\nThe indirect purpose is to provide a tutorial around the design of GRASP, and\nin particular - around the architecture of its extension mechanism. Neither\nGRASP nor its extension mechanisms are, at the moment of writing this paper,\nfinal or complete, and we are certain that some details of the solutions\ndescribed in here will change even before the first release. What will not\nchange, though, is the set of problems that need to be solved in order to build\na system with capabilities similar to those of GRASP. We believe that these\nproblems might be of interest to the Scheme community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u5728GRASP\u7f16\u7a0b\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u89c6\u89c9\u8bc4\u4f30\u5668\u6269\u5c55\uff0c\u5e76\u56f4\u7ed5GRASP\u7684\u8bbe\u8ba1\u5c24\u5176\u662f\u5176\u6269\u5c55\u673a\u5236\u67b6\u6784\u63d0\u4f9b\u6559\u7a0b\u3002", "motivation": "\u5c55\u793aGRASP\u7f16\u7a0b\u7cfb\u7edf\u4e2d\u89c6\u89c9\u8bc4\u4f30\u5668\u6269\u5c55\u7684\u5b9e\u73b0\u65b9\u6cd5\uff0c\u540c\u65f6\u63a2\u8ba8\u7c7b\u4f3cGRASP\u7cfb\u7edf\u7684\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "\u63cf\u8ff0GRASP\u7684\u6269\u5c55\u673a\u5236\u67b6\u6784\uff0c\u5e76\u8ba8\u8bba\u5176\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5c3d\u7ba1GRASP\u53ca\u5176\u6269\u5c55\u673a\u5236\u5c1a\u672a\u6700\u7ec8\u5b8c\u6210\uff0c\u4f46\u63d0\u4f9b\u4e86\u76f8\u5173\u8bbe\u8ba1\u95ee\u9898\u7684\u89c1\u89e3\u3002", "conclusion": "GRASP\u7684\u8bbe\u8ba1\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u5bf9Scheme\u793e\u533a\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2508.05377", "pdf": "https://arxiv.org/pdf/2508.05377", "abs": "https://arxiv.org/abs/2508.05377", "authors": ["Hongyu Zhou", "Yinan Zhang", "Aixin Sun", "Zhiqi Shen"], "title": "Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions", "categories": ["cs.IR", "cs.MM"], "comment": null, "summary": "Multimodal recommendation systems are increasingly popular for their\npotential to improve performance by integrating diverse data types. However,\nthe actual benefits of this integration remain unclear, raising questions about\nwhen and how it truly enhances recommendations. In this paper, we propose a\nstructured evaluation framework to systematically assess multimodal\nrecommendations across four dimensions: Comparative Efficiency, Recommendation\nTasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a\nset of reproducible multimodal models against strong traditional baselines and\nevaluate their performance on different platforms. Our findings show that\nmultimodal data is particularly beneficial in sparse interaction scenarios and\nduring the recall stage of recommendation pipelines. We also observe that the\nimportance of each modality is task-specific, where text features are more\nuseful in e-commerce and visual features are more effective in short-video\nrecommendations. Additionally, we explore different integration strategies and\nmodel sizes, finding that Ensemble-Based Learning outperforms Fusion-Based\nLearning, and that larger models do not necessarily deliver better results. To\ndeepen our understanding, we include case studies and review findings from\nother recommendation domains. Our work provides practical insights for building\nefficient and effective multimodal recommendation systems, emphasizing the need\nfor thoughtful modality selection, integration strategies, and model design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u6570\u636e\u5728\u7a00\u758f\u4ea4\u4e92\u548c\u53ec\u56de\u9636\u6bb5\u6548\u679c\u663e\u8457\uff0c\u4e14\u4e0d\u540c\u6a21\u6001\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u91cd\u8981\u6027\u5404\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u9010\u6e10\u6d41\u884c\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u56db\u4e2a\u7ef4\u5ea6\u5206\u6790\u591a\u6a21\u6001\u63a8\u8350\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u4f20\u7edf\u6a21\u578b\u7684\u6548\u679c\u3002", "result": "\u591a\u6a21\u6001\u6570\u636e\u5728\u7a00\u758f\u4ea4\u4e92\u548c\u53ec\u56de\u9636\u6bb5\u8868\u73b0\u66f4\u597d\uff1b\u6587\u672c\u6a21\u6001\u5728\u7535\u5546\u4e2d\u66f4\u6709\u6548\uff0c\u89c6\u89c9\u6a21\u6001\u5728\u77ed\u89c6\u9891\u63a8\u8350\u4e2d\u66f4\u4f18\uff1b\u57fa\u4e8e\u96c6\u6210\u7684\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u878d\u5408\u7684\u65b9\u6cd5\uff1b\u6a21\u578b\u5927\u5c0f\u4e0e\u6548\u679c\u65e0\u5173\u3002", "conclusion": "\u672c\u6587\u4e3a\u6784\u5efa\u9ad8\u6548\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6a21\u6001\u9009\u62e9\u3001\u96c6\u6210\u7b56\u7565\u548c\u6a21\u578b\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.05115", "pdf": "https://arxiv.org/pdf/2508.05115", "abs": "https://arxiv.org/abs/2508.05115", "authors": ["Fangyu Du", "Taiqing Li", "Ziwei Zhang", "Qian Qiao", "Tan Yu", "Dingcheng Zhen", "Xu Jia", "Yang Yang", "Shunshun Yin", "Siyuan Liu"], "title": "RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 9 figures", "summary": "Audio-driven portrait animation aims to synthesize realistic and natural\ntalking head videos from an input audio signal and a single reference image.\nWhile existing methods achieve high-quality results by leveraging\nhigh-dimensional intermediate representations and explicitly modeling motion\ndynamics, their computational complexity renders them unsuitable for real-time\ndeployment. Real-time inference imposes stringent latency and memory\nconstraints, often necessitating the use of highly compressed latent\nrepresentations. However, operating in such compact spaces hinders the\npreservation of fine-grained spatiotemporal details, thereby complicating\naudio-visual synchronization RAP (Real-time Audio-driven Portrait animation), a\nunified framework for generating high-quality talking portraits under real-time\nconstraints. Specifically, RAP introduces a hybrid attention mechanism for\nfine-grained audio control, and a static-dynamic training-inference paradigm\nthat avoids explicit motion supervision. Through these techniques, RAP achieves\nprecise audio-driven control, mitigates long-term temporal drift, and maintains\nhigh visual fidelity. Extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance while operating under real-time constraints.", "AI": {"tldr": "RAP\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u9759\u6001-\u52a8\u6001\u8bad\u7ec3\u63a8\u65ad\u8303\u5f0f\uff0c\u89e3\u51b3\u538b\u7f29\u7a7a\u95f4\u4e2d\u7684\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5b9e\u65f6\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u590d\u6742\u5ea6\u9ad8\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\uff0c\u800c\u538b\u7f29\u7a7a\u95f4\u4f1a\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\uff0c\u5f71\u54cd\u97f3\u9891-\u89c6\u89c9\u540c\u6b65\u3002RAP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7cbe\u7ec6\u97f3\u9891\u63a7\u5236\uff0c\u9759\u6001-\u52a8\u6001\u8bad\u7ec3\u63a8\u65ad\u8303\u5f0f\u907f\u514d\u663e\u5f0f\u8fd0\u52a8\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRAP\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u753b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAP\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u5b9e\u65f6\u6027\u4e0e\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4e3a\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05020", "pdf": "https://arxiv.org/pdf/2508.05020", "abs": "https://arxiv.org/abs/2508.05020", "authors": ["Anjiang Wei", "Hang Song", "Mert Hidayetoglu", "Elliott Slaughter", "Sanjiva K. Lele", "Alex Aiken"], "title": "Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations", "categories": ["cs.DC", "cs.CE", "cs.MS"], "comment": null, "summary": "High-order solvers for compressible flows are vital in scientific\napplications. Adaptive mesh refinement (AMR) is a key technique for reducing\ncomputational cost by concentrating resolution in regions of interest. In this\nwork, we develop an AMR-based numerical solver using Regent, a high-level\nprogramming language for the Legion programming model. We address several\nchallenges associated with implementing AMR in Regent. These include dynamic\ndata structures for patch refinement/coarsening, mesh validity enforcement, and\nreducing task launch overhead via task fusion. Experimental results show that\ntask fusion achieves 18x speedup, while automated GPU kernel generation via\nsimple annotations yields 9.7x speedup for the targeted kernel. We demonstrate\nour approach through simulations of two canonical compressible flow problems\ngoverned by the Euler equations.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eAMR\u7684\u9ad8\u9636\u6570\u503c\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u6570\u636e\u7ed3\u6784\u3001\u7f51\u683c\u6709\u6548\u6027\u6267\u884c\u7b49\u6311\u6218\uff0c\u901a\u8fc7\u4efb\u52a1\u878d\u5408\u548cGPU\u5185\u6838\u751f\u6210\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u9ad8\u9636\u6c42\u89e3\u5668\u5bf9\u53ef\u538b\u7f29\u6d41\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0cAMR\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5b9e\u73b0\u4e2d\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f7f\u7528Regent\u8bed\u8a00\u5f00\u53d1AMR\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u52a8\u6001\u6570\u636e\u7ed3\u6784\u3001\u4efb\u52a1\u878d\u5408\u548cGPU\u5185\u6838\u751f\u6210\u95ee\u9898\u3002", "result": "\u4efb\u52a1\u878d\u5408\u5b9e\u73b018\u500d\u52a0\u901f\uff0cGPU\u5185\u6838\u751f\u6210\u8fbe\u52309.7\u500d\u52a0\u901f\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AMR\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6d41\u95ee\u9898\u3002"}}
{"id": "2508.05005", "pdf": "https://arxiv.org/pdf/2508.05005", "abs": "https://arxiv.org/abs/2508.05005", "authors": ["Gang Xu", "Airong Wang", "Yushan Pan"], "title": "Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic", "categories": ["cs.SE"], "comment": null, "summary": "We find ourselves in the midst of an explosion in artificial intelligence\nresearch, particularly with large language models (LLMs). These models have\ndiverse applications spanning finance, commonsense knowledge graphs, medicine,\nand visual analysis. In the world of Object-Oriented Programming(OOP), a robust\nbody of knowledge and methods has been developed for managing complex tasks\nthrough object-oriented thinking. However, the intersection of LLMs with OOP\nremains an underexplored territory. Empirically, we currently possess limited\nunderstanding of how LLMs can enhance the effectiveness of OOP learning and\ncode writing, as well as how we can evaluate such AI-powered tools. Our work\naims to address this gap by presenting a vision from the perspectives of key\nstakeholders involved in an OOP task: programmers, mariners, and experienced\nprogrammers. We identify critical junctures within typical coding workflows\nwhere the integration of LLMs can offer significant benefits. Furthermore, we\npropose ways to augment existing logical reasoning and code writing, ultimately\nenhancing the programming experience.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u63d0\u5347\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\uff08OOP\uff09\u7684\u6548\u7387\u548c\u4ee3\u7801\u7f16\u5199\uff0c\u586b\u8865\u4e86LLMs\u4e0eOOP\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7684AI\u7814\u7a76\u5feb\u901f\u53d1\u5c55\uff0cLLMs\u5728\u591a\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46LLMs\u4e0eOOP\u7684\u7ed3\u5408\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8LLMs\u5982\u4f55\u63d0\u5347\u7f16\u7a0b\u6548\u7387\u3002", "method": "\u4ece\u7a0b\u5e8f\u5458\u3001\u521d\u5b66\u8005\u548c\u8001\u624b\u7684\u89c6\u89d2\u51fa\u53d1\uff0c\u8bc6\u522b\u7f16\u7801\u6d41\u7a0b\u4e2dLLMs\u53ef\u63d0\u4f9b\u663e\u8457\u5e2e\u52a9\u7684\u5173\u952e\u8282\u70b9\uff0c\u5e76\u63d0\u51fa\u589e\u5f3a\u903b\u8f91\u63a8\u7406\u548c\u4ee3\u7801\u7f16\u5199\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86LLMs\u5728OOP\u4efb\u52a1\u4e2d\u5e94\u7528\u7684\u5177\u4f53\u573a\u666f\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3a\u7f16\u7a0b\u4f53\u9a8c\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408LLMs\u4e0eOOP\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7f16\u7a0b\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u6269\u5c55\u8fd9\u4e9b\u5e94\u7528\u3002"}}
{"id": "2508.04902", "pdf": "https://arxiv.org/pdf/2508.04902", "abs": "https://arxiv.org/abs/2508.04902", "authors": ["Luis Morales-Navarro", "Michelle Gan", "Evelyn Yu", "Lauren Vogelstein", "Yasmin B. Kafai", "Dana\u00e9 Metaxa"], "title": "Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model", "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.2"], "comment": null, "summary": "This study investigates how high school-aged youth engage in algorithm\nauditing to identify and understand biases in artificial intelligence and\nmachine learning (AI/ML) tools they encounter daily. With AI/ML technologies\nbeing increasingly integrated into young people's lives, there is an urgent\nneed to equip teenagers with AI literacies that build both technical knowledge\nand awareness of social impacts. Algorithm audits (also called AI audits) have\ntraditionally been employed by experts to assess potential harmful biases, but\nrecent research suggests that non-expert users can also participate\nproductively in auditing. We conducted a two-week participatory design workshop\nwith 14 teenagers (ages 14-15), where they audited the generative AI model\nbehind TikTok's Effect House, a tool for creating interactive TikTok filters.\nWe present a case study describing how teenagers approached the audit, from\ndeciding what to audit to analyzing data using diverse strategies and\ncommunicating their results. Our findings show that participants were engaged\nand creative throughout the activities, independently raising and exploring new\nconsiderations, such as age-related biases, that are uncommon in professional\naudits. We drew on our expertise in algorithm auditing to triangulate their\nfindings as a way to examine if the workshop supported participants to reach\ncoherent conclusions in their audit. Although the resulting number of changes\nin race, gender, and age representation uncovered by the teens were slightly\ndifferent from ours, we reached similar conclusions. This study highlights the\npotential for auditing to inspire learning activities to foster AI literacies,\nempower teenagers to critically examine AI systems, and contribute fresh\nperspectives to the study of algorithmic harms.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9ad8\u4e2d\u751f\u5982\u4f55\u901a\u8fc7\u7b97\u6cd5\u5ba1\u8ba1\u8ba4\u8bc6AI/ML\u5de5\u5177\u7684\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u5c55\u793a\u4e86\u4ed6\u4eec\u7684\u521b\u9020\u6027\u548c\u72ec\u7acb\u6027\u3002", "motivation": "\u968f\u7740AI/ML\u6280\u672f\u878d\u5165\u9752\u5c11\u5e74\u751f\u6d3b\uff0c\u9700\u8981\u57f9\u517b\u5176AI\u7d20\u517b\uff0c\u63d0\u5347\u6280\u672f\u8ba4\u77e5\u548c\u793e\u4f1a\u5f71\u54cd\u610f\u8bc6\u3002", "method": "\u4e0e14\u540d\u9752\u5c11\u5e74\u8fdb\u884c\u4e3a\u671f\u4e24\u5468\u7684\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u8ba8\u4f1a\uff0c\u5ba1\u8ba1TikTok\u7684\u751f\u6210AI\u6a21\u578b\u3002", "result": "\u9752\u5c11\u5e74\u72ec\u7acb\u53d1\u73b0\u5e74\u9f84\u504f\u89c1\u7b49\u4e13\u4e1a\u5ba1\u8ba1\u4e2d\u5c11\u89c1\u7684\u95ee\u9898\uff0c\u5ba1\u8ba1\u7ed3\u8bba\u4e0e\u4e13\u5bb6\u76f8\u4f3c\u3002", "conclusion": "\u7b97\u6cd5\u5ba1\u8ba1\u53ef\u4f5c\u4e3a\u57f9\u517bAI\u7d20\u517b\u7684\u5b66\u4e60\u6d3b\u52a8\uff0c\u5e2e\u52a9\u9752\u5c11\u5e74\u6279\u5224\u6027\u5ba1\u89c6AI\u7cfb\u7edf\u5e76\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.05187", "pdf": "https://arxiv.org/pdf/2508.05187", "abs": "https://arxiv.org/abs/2508.05187", "authors": ["Mohamed Abdul Gafoor", "Marius Preda", "Titus Zaharia"], "title": "Refining Gaussian Splatting: A Volumetric Densification Approach", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)\noften depends on effective point primitive management. The underlying Adaptive\nDensity Control (ADC) process addresses this issue by automating densification\nand pruning. Yet, the vanilla 3DGS densification strategy shows key\nshortcomings. To address this issue, in this paper we introduce a novel density\ncontrol method, which exploits the volumes of inertia associated to each\nGaussian function to guide the refinement process. Furthermore, we study the\neffect of both traditional Structure from Motion (SfM) and Deep Image Matching\n(DIM) methods for point cloud initialization. Extensive experimental\nevaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses\n3DGS in reconstruction quality, delivering encouraging performance across\ndiverse scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60ef\u6027\u4f53\u79ef\u7684\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e863D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u70b9\u539f\u59cb\u7ba1\u7406\uff0c\u63d0\u5347\u4e86\u65b0\u89c6\u56fe\u5408\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u4e2d\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236(ADC)\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u65b0\u89c6\u56fe\u5408\u6210\u7684\u8d28\u91cf\u3002", "method": "\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u60ef\u6027\u4f53\u79ef\u6307\u5bfc\u5bc6\u5ea6\u63a7\u5236\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edfSfM\u548c\u6df1\u5ea6\u56fe\u50cf\u5339\u914d(DIM)\u7684\u70b9\u4e91\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "result": "\u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u4f18\u4e8e3DGS\uff0c\u91cd\u5efa\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u5bc6\u5ea6\u63a7\u5236\u548c\u70b9\u4e91\u521d\u59cb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863DGS\u7684\u6027\u80fd\u3002"}}
{"id": "2508.05034", "pdf": "https://arxiv.org/pdf/2508.05034", "abs": "https://arxiv.org/abs/2508.05034", "authors": ["Arabat", "Ali", "Sayagh", "Mohammed", "Hassine", "Jameleddine"], "title": "An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "As software systems grow in complexity, accurately identifying and managing\ndependencies among changes becomes increasingly critical. For instance, a\nchange that leverages a function must depend on the change that introduces it.\nEstablishing such dependencies allows CI/CD pipelines to build and orchestrate\nchanges effectively, preventing build failures and incomplete feature\ndeployments. In modern software systems, dependencies often span multiple\ncomponents across teams, creating challenges for development and deployment.\nThey serve various purposes, from enabling new features to managing\nconfigurations, and can even involve traditionally independent changes like\ndocumentation updates. To address these challenges, we conducted a preliminary\nstudy on dependency management in OpenStack, a large-scale software system. Our\nstudy revealed that a substantial portion of software changes in OpenStack over\nthe past 10 years are interdependent. Surprisingly, 51.08% of these\ndependencies are identified during the code review phase-after a median delay\nof 5.06 hours-rather than at the time of change creation. Developers often\nspend a median of 57.12 hours identifying dependencies, searching among a\nmedian of 463 other changes. To help developers proactively identify\ndependencies, we propose a semi-automated approach that leverages two ML\nmodels. The first model predicts the likelihood of dependencies among changes,\nwhile the second identifies the exact pairs of dependent changes. Our proposed\nmodels demonstrate strong performance, achieving average AUC scores of 79.33%\nand 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the\nsecond model has a good top-k recall across all types of pairs, while the top-k\nprecision has room for improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u65b9\u6cd5\u6765\u7ba1\u7406\u8f6f\u4ef6\u53d8\u66f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u4e24\u4e2aML\u6a21\u578b\u9884\u6d4b\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86OpenStack\u4e2d\u5927\u91cf\u53d8\u66f4\u4f9d\u8d56\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u51c6\u786e\u8bc6\u522b\u548c\u7ba1\u7406\u53d8\u66f4\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u9632\u6b62\u6784\u5efa\u5931\u8d25\u548c\u4e0d\u5b8c\u6574\u7684\u90e8\u7f72\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5728OpenStack\u4e2d\u8fdb\u884c\u521d\u6b65\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u4e2aML\u6a21\u578b\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u8bc6\u522b\u53d8\u66f4\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c51.08%\u7684\u4f9d\u8d56\u5173\u7cfb\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u88ab\u8bc6\u522b\uff0c\u5ef6\u8fdf\u4e2d\u4f4d\u6570\u4e3a5.06\u5c0f\u65f6\uff1b\u63d0\u51fa\u7684\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0cAUC\u5f97\u5206\u4e3a79.33%\u548c91.89%\uff0cBrier\u5f97\u5206\u4e3a0.11\u548c0.014\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u63d0\u524d\u8bc6\u522b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u6a21\u578b\u7684\u51c6\u786e\u7387\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2508.04904", "pdf": "https://arxiv.org/pdf/2508.04904", "abs": "https://arxiv.org/abs/2508.04904", "authors": ["Yuqi Hu", "Qiwen Xiong", "Zhenzhen Qin", "Brandon Watanabe", "Yujing Wang", "Mirjana Prpa", "Ilmi Yoon"], "title": "Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept", "categories": ["cs.HC"], "comment": null, "summary": "Root Cause Analysis (RCA) is a critical tool for investigating adverse events\nin healthcare and improving patient safety. However, existing RCA training\nprograms are often limited by high resource demands, leading to insufficient\ntraining and inconsistent implementation. To address this challenge, we present\nan AI-powered 3D simulation game that helps healthcare professionals develop\nRCA skills through interactive, immersive simulations. This approach offers a\ncost-effective, scalable, and accessible alternative to traditional training.\nThe prototype simulates an RCA investigation following a death in the ICU,\nwhere learners interview five virtual avatars representing ICU team members to\ninvestigate the incident and complete a written report. The system enables\nnatural, life-like interactions with avatars via large language models (LLMs),\nemotional text-to-speech, and AI-powered animations. An additional LLM\ncomponent provides formative and summative feedback to support continual\nimprovement. We conclude by outlining plans to empirically evaluate the\nsystem's efficacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u76843D\u6a21\u62df\u6e38\u620f\uff0c\u901a\u8fc7\u4e92\u52a8\u6c89\u6d78\u5f0f\u6a21\u62df\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u63d0\u5347RCA\u6280\u80fd\uff0c\u89e3\u51b3\u4f20\u7edf\u57f9\u8bad\u8d44\u6e90\u5bc6\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RCA\u57f9\u8bad\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u5bfc\u81f4\u57f9\u8bad\u4e0d\u8db3\u548c\u5b9e\u65bd\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u4e00\u79cd\u6210\u672c\u4f4e\u3001\u53ef\u6269\u5c55\u4e14\u6613\u83b7\u53d6\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5f00\u53d1AI\u9a71\u52a8\u76843D\u6a21\u62df\u6e38\u620f\uff0c\u5229\u7528LLM\u3001\u60c5\u611f\u8bed\u97f3\u5408\u6210\u548cAI\u52a8\u753b\u5b9e\u73b0\u4e0e\u865a\u62df\u89d2\u8272\u7684\u81ea\u7136\u4e92\u52a8\uff0c\u5e76\u63d0\u4f9b\u53cd\u9988\u673a\u5236\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u6a21\u62dfICU\u4e0d\u826f\u4e8b\u4ef6\u8c03\u67e5\uff0c\u7528\u6237\u53ef\u4e0e\u865a\u62df\u89d2\u8272\u4e92\u52a8\u5b8c\u6210\u62a5\u544a\uff0c\u5e76\u83b7\u5f97\u53cd\u9988\u4ee5\u6539\u8fdb\u6280\u80fd\u3002", "conclusion": "\u672a\u6765\u8ba1\u5212\u5b9e\u8bc1\u8bc4\u4f30\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u65e8\u5728\u4e3a\u533b\u7597RCA\u57f9\u8bad\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05524", "pdf": "https://arxiv.org/pdf/2508.05524", "abs": "https://arxiv.org/abs/2508.05524", "authors": ["Sefat Rahman", "Tushar M. Athawale", "Paul Rosen"], "title": "GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs", "categories": ["cs.GR", "cs.CG", "cs.HC"], "comment": null, "summary": "Reeb graphs are an important tool for abstracting and representing the\ntopological structure of a function defined on a manifold. We have identified\nthree properties for faithfully representing Reeb graphs in a visualization.\nNamely, they should be constrained to the boundary, compact, and aligned with\nthe function gradient. Existing algorithms for drawing Reeb graphs are agnostic\nto or violate these properties. In this paper, we introduce an algorithm to\ngenerate Reeb graph visualizations, called \\textit{GASP}, that is cognizant of\nthese properties, thereby producing visualizations that are more representative\nof the underlying data. To demonstrate the improvements, the resulting Reeb\ngraphs are evaluated both qualitatively and quantitatively against the\ngeometric barycenter algorithm, using its implementation available in the\nTopology ToolKit (TTK), a widely adopted tool for calculating and visualizing\nReeb graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGASP\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u6ee1\u8db3\u8fb9\u754c\u7ea6\u675f\u3001\u7d27\u51d1\u6027\u548c\u68af\u5ea6\u5bf9\u9f50\u7684Reeb\u56fe\u53ef\u89c6\u5316\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u4ee3\u8868\u6027\u3002", "motivation": "\u73b0\u6709Reeb\u56fe\u53ef\u89c6\u5316\u7b97\u6cd5\u5ffd\u7565\u4e86\u8fb9\u754c\u7ea6\u675f\u3001\u7d27\u51d1\u6027\u548c\u68af\u5ea6\u5bf9\u9f50\u8fd9\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff0c\u5bfc\u81f4\u53ef\u89c6\u5316\u7ed3\u679c\u65e0\u6cd5\u5fe0\u5b9e\u8868\u8fbe\u6570\u636e\u7684\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u63d0\u51faGASP\u7b97\u6cd5\uff0c\u4e13\u95e8\u8003\u8651\u8fb9\u754c\u7ea6\u675f\u3001\u7d27\u51d1\u6027\u548c\u68af\u5ea6\u5bf9\u9f50\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684Reeb\u56fe\u53ef\u89c6\u5316\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0cGASP\u7b97\u6cd5\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u51e0\u4f55\u91cd\u5fc3\u7b97\u6cd5\uff08TTK\u5b9e\u73b0\uff09\u3002", "conclusion": "GASP\u7b97\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8868\u793a\u6570\u636e\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u4e3aReeb\u56fe\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.05370", "pdf": "https://arxiv.org/pdf/2508.05370", "abs": "https://arxiv.org/abs/2508.05370", "authors": ["Sumit Kumar", "Arjun Temura", "Naman Sharma", "Ramanjeet Singh", "Meet Dadhania", "Praveen Tammana", "Satananda Burla", "Abed Mohammad Kamaluddin", "Rinku Shah"], "title": "Simulating LLM training workloads for heterogeneous compute and network infrastructure", "categories": ["cs.DC"], "comment": null, "summary": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u611f\u77e5\u7684\u5206\u5e03\u5f0fLLM\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u8bad\u7ec3\u65f6\u95f4\u5e76\u652f\u6301\u8bbe\u5907\u7ec4\u548c\u5e76\u884c\u6620\u5c04\u7684\u81ea\u5b9a\u4e49\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86GPU\u96c6\u7fa4\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\u4e2dGPU\u96c6\u7fa4\u7684\u5f02\u6784\u6027\uff08\u5982\u8d44\u6e90\u5171\u4eab\u3001\u8bbe\u5907\u4ee3\u9645\u5dee\u5f02\uff09\u5bf9\u6a21\u578b\u4f18\u5316\u548c\u7cfb\u7edf\u589e\u5f3a\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u73b0\u6709\u6a21\u62df\u5668\u5047\u8bbe\u57fa\u7840\u8bbe\u65bd\u540c\u8d28\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u5f02\u6784\u611f\u77e5\u7684\u5206\u5e03\u5f0fLLM\u6a21\u62df\u5668\uff0c\u5305\u62ec\u975e\u5747\u5300\u5de5\u4f5c\u8d1f\u8f7d\u5206\u533a\u7b49\u7ec4\u4ef6\uff0c\u652f\u6301\u8bbe\u5907\u7ec4\u548c\u5e76\u884c\u6620\u5c04\u7684\u81ea\u5b9a\u4e49\u914d\u7f6e\u3002", "result": "\u521d\u6b65\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5f02\u6784\u6027\u5bf9\u6a21\u578b\u8ba1\u7b97\u548c\u901a\u4fe1\u65f6\u95f4\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86\u73b0\u6709\u6a21\u62df\u5668\u4e0e\u5b9e\u9645\u95ee\u9898\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5f02\u6784\u73af\u5883\u4e0b\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.05085", "pdf": "https://arxiv.org/pdf/2508.05085", "abs": "https://arxiv.org/abs/2508.05085", "authors": ["Junayed Mahmud", "James Chen", "Terry Achille", "Camilo Alvarez-Velez", "Darren Dean Bansil", "Patrick Ijieh", "Samar Karanch", "Nadeeshan De Silva", "Oscar Chaparro", "Andrian Marcus", "Kevin Moran"], "title": "LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps", "categories": ["cs.SE"], "comment": "5 pages, to appear in the Proceedings of the 41st International\n  Conference on Software Maintenance and Evolution (ICSME'25) - Tool\n  Demonstration Track", "summary": "This paper introduces LadyBug, a GitHub bot that automatically localizes bugs\nfor Android apps by combining UI interaction information with text retrieval.\nLadyBug connects to an Android app's GitHub repository, and is triggered when a\nbug is reported in the corresponding issue tracker. Developers can then record\na reproduction trace for the bug on a device or emulator and upload the trace\nto LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both\nthe text from the original bug description, and UI information from the\nreproduction trace to accurately retrieve a ranked list of files from the\nproject that most likely contain the reported bug.\n  We empirically evaluated LadyBug using an automated testing pipeline and\nbenchmark called RedWing that contains 80 fully-localized and reproducible bug\nreports from 39 Android apps. Our results illustrate that LadyBug outperforms\ntext-retrieval-based baselines and that the utilization of UI information leads\nto a substantial increase in localization accuracy. LadyBug is an open-source\ntool, available at https://github.com/LadyBugML/ladybug.\n  A video showing the capabilities of Ladybug can be viewed here:\nhttps://youtu.be/hI3tzbRK0Cw", "AI": {"tldr": "LadyBug\u662f\u4e00\u6b3eGitHub\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7ed3\u5408UI\u4ea4\u4e92\u4fe1\u606f\u548c\u6587\u672c\u68c0\u7d22\uff0c\u5e2e\u52a9Android\u5e94\u7528\u81ea\u52a8\u5b9a\u4f4dbug\u3002", "motivation": "\u5f00\u53d1LadyBug\u662f\u4e3a\u4e86\u63d0\u9ad8Android\u5e94\u7528bug\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7ed3\u5408\u6587\u672c\u63cf\u8ff0\u548cUI\u4ea4\u4e92\u4fe1\u606f\u3002", "method": "LadyBug\u8fde\u63a5\u5230GitHub\u4ed3\u5e93\uff0c\u5229\u7528\u5f00\u53d1\u8005\u4e0a\u4f20\u7684bug\u91cd\u73b0\u8ffd\u8e2a\u548c\u539f\u59cb\u6587\u672c\uff0c\u7ed3\u5408UI\u4fe1\u606f\u8fdb\u884c\u6587\u4ef6\u68c0\u7d22\u3002", "result": "\u5728RedWing\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLadyBug\u8868\u73b0\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6587\u672c\u68c0\u7d22\u7684\u65b9\u6cd5\uff0cUI\u4fe1\u606f\u7684\u5229\u7528\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "conclusion": "LadyBug\u662f\u4e00\u6b3e\u5f00\u6e90\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u5408UI\u548c\u6587\u672c\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86Android\u5e94\u7528bug\u5b9a\u4f4d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.04920", "pdf": "https://arxiv.org/pdf/2508.04920", "abs": "https://arxiv.org/abs/2508.04920", "authors": ["Oliver Huang", "Carolina Nobre"], "title": "Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities", "categories": ["cs.HC"], "comment": "VIS 2025 Poster Summary", "summary": "Analysts increasingly explore data through evolving, narrative-driven\ninquiries, moving beyond static dashboards and predefined metrics as their\nquestions deepen and shift. As these explorations progress, insights often\nbecome dispersed across views, making it challenging to maintain context or\nclarify how conclusions arise. Through a formative study with 48 participants,\nwe identify key barriers that hinder narrative-driven exploration, including\ndifficulty maintaining context across views, tracing reasoning paths, and\nexternalizing evolving interpretations. Our findings surface design\nopportunities to support narrative-driven analysis better.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6570\u636e\u5206\u6790\u5e08\u5728\u53d9\u4e8b\u9a71\u52a8\u67e5\u8be2\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6539\u8fdb\u673a\u4f1a\u3002", "motivation": "\u968f\u7740\u6570\u636e\u5206\u6790\u4ece\u9759\u6001\u4eea\u8868\u677f\u8f6c\u5411\u66f4\u6df1\u5165\u7684\u53d9\u4e8b\u9a71\u52a8\u67e5\u8be2\uff0c\u5206\u6790\u5e08\u5728\u7ef4\u62a4\u4e0a\u4e0b\u6587\u548c\u8ddf\u8e2a\u63a8\u7406\u8def\u5f84\u65f6\u9047\u5230\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5bf948\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u5f62\u6210\u6027\u7814\u7a76\uff0c\u8bc6\u522b\u4e86\u53d9\u4e8b\u9a71\u52a8\u63a2\u7d22\u4e2d\u7684\u5173\u952e\u969c\u788d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5206\u6790\u5e08\u5728\u7ef4\u62a4\u8de8\u89c6\u56fe\u4e0a\u4e0b\u6587\u3001\u8ddf\u8e2a\u63a8\u7406\u8def\u5f84\u548c\u5916\u90e8\u5316\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u7814\u7a76\u4e3a\u652f\u6301\u53d9\u4e8b\u9a71\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u673a\u4f1a\u3002"}}
{"id": "2508.05531", "pdf": "https://arxiv.org/pdf/2508.05531", "abs": "https://arxiv.org/abs/2508.05531", "authors": ["Davide Garavaso", "Federico Masi", "Pietro Musoni", "Umberto Castellani"], "title": "Point cloud segmentation for 3D Clothed Human Layering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Cloth modeling and simulation is essential for avatars creation in several\nfields, such as fashion, entertainment, and animation. Achieving high-quality\nresults is challenging due to the large variability of clothed body especially\nin the generation of realistic wrinkles. 3D scan acquisitions provide more\naccuracy in the representation of real-world objects but lack semantic\ninformation that can be inferred with a reliable semantic reconstruction\npipeline. To this aim, shape segmentation plays a crucial role in identifying\nthe semantic shape parts. However, current 3D shape segmentation methods are\ndesigned for scene understanding and interpretation and only few work is\ndevoted to modeling. In the context of clothed body modeling the segmentation\nis a preliminary step for fully semantic shape parts reconstruction namely the\nunderlying body and the involved garments. These parts represent several layers\nwith strong overlap in contrast with standard segmentation methods that provide\ndisjoint sets. In this work we propose a new 3D point cloud segmentation\nparadigm where each 3D point can be simultaneously associated to different\nlayers. In this fashion we can estimate the underlying body parts and the\nunseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer\nabove. We name this segmentation paradigm clothed human layering. We create a\nnew synthetic dataset that simulates very realistic 3D scans with the ground\ntruth of the involved clothing layers. We propose and evaluate different neural\nnetwork settings to deal with 3D clothing layering. We considered both coarse\nand fine grained per-layer garment identification. Our experiments demonstrates\nthe benefit in introducing proper strategies for the segmentation on the\ngarment domain on both the synthetic and real-world scan datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u70b9\u4e91\u5206\u5272\u8303\u5f0f\u2014\u2014Clothed Human Layering\uff0c\u7528\u4e8e\u540c\u65f6\u5173\u8054\u4e0d\u540c\u5c42\u6b21\u7684\u670d\u88c5\u548c\u8eab\u4f53\u90e8\u5206\uff0c\u5e76\u521b\u5efa\u4e86\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u67093D\u5f62\u72b6\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u7528\u4e8e\u573a\u666f\u7406\u89e3\uff0c\u800c\u5728\u670d\u88c5\u5efa\u6a21\u4e2d\u9700\u8981\u89e3\u51b3\u591a\u5c42\u91cd\u53e0\u7684\u5206\u5272\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\uff0c\u652f\u6301\u70b9\u540c\u65f6\u5173\u8054\u591a\u4e2a\u5c42\u6b21\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7684\u7c97/\u7ec6\u7c92\u5ea6\u8bc6\u522b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u670d\u88c5\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u5206\u5272\u8303\u5f0f\u80fd\u6709\u6548\u5904\u7406\u670d\u88c5\u548c\u8eab\u4f53\u90e8\u5206\u7684\u91cd\u53e0\u5206\u5272\u95ee\u9898\uff0c\u4e3a3D\u670d\u88c5\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.05511", "pdf": "https://arxiv.org/pdf/2508.05511", "abs": "https://arxiv.org/abs/2508.05511", "authors": ["Rasman Mubtasim Swargo", "Engin Arslan", "Md Arifuzzaman"], "title": "Adaptive Parallel Downloader for Large Genomic Datasets", "categories": ["cs.DC"], "comment": null, "summary": "Modern next-generation sequencing (NGS) projects routinely generate terabytes\nof data, which researchers commonly download from public repositories such as\nSRA or ENA. Existing download tools often employ static concurrency settings,\nleading to inefficient bandwidth utilization and prolonged download times due\nto their inability to adapt to dynamic network conditions. We introduce\nFastBioDL, a parallel file downloader designed for large biological datasets,\nfeaturing an adaptive concurrency controller. FastBioDL frames the download\nprocess as an online optimization problem, utilizing a utility function and\ngradient descent to adjust the number of concurrent socket streams in real-time\ndynamically. This approach maximizes download throughput while minimizing\nresource overhead. Comprehensive evaluations on public genomic datasets\ndemonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art\ntools. Moreover, in high-speed network experiments, its adaptive design was up\nto $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP\nor FTP downloads on the client side, FastBioDL provides a robust and efficient\nsolution for large-scale genomic data acquisition, democratizing\nhigh-performance data retrieval for researchers without requiring specialized\ncommercial software or protocols.", "AI": {"tldr": "FastBioDL\u662f\u4e00\u6b3e\u81ea\u9002\u5e94\u5e76\u53d1\u7684\u751f\u7269\u6570\u636e\u96c6\u4e0b\u8f7d\u5de5\u5177\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f18\u5316\u5e76\u53d1\u6d41\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u8f7d\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u4e0b\u8f7d\u5de5\u5177\u56e0\u9759\u6001\u5e76\u53d1\u8bbe\u7f6e\u5bfc\u81f4\u5e26\u5bbd\u5229\u7528\u4f4e\u6548\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\uff0c\u4e0b\u8f7d\u901f\u5ea6\u6162\u3002", "method": "FastBioDL\u5c06\u4e0b\u8f7d\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u5728\u7ebf\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u6548\u7528\u51fd\u6570\u548c\u68af\u5ea6\u4e0b\u964d\u52a8\u6001\u8c03\u6574\u5e76\u53d1\u6d41\u6570\u91cf\u3002", "result": "\u6d4b\u8bd5\u663e\u793aFastBioDL\u6bd4\u73b0\u6709\u5de5\u5177\u5feb4\u500d\uff0c\u9ad8\u901f\u7f51\u7edc\u4e0b\u5feb2.1\u500d\u3002", "conclusion": "FastBioDL\u901a\u8fc7\u667a\u80fd\u4f18\u5316HTTP/FTP\u4e0b\u8f7d\uff0c\u4e3a\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u6570\u636e\u83b7\u53d6\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05170", "pdf": "https://arxiv.org/pdf/2508.05170", "abs": "https://arxiv.org/abs/2508.05170", "authors": ["Lishui Fan", "Yu Zhang", "Mouxiang Chen", "Zhongxin Liu"], "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u548c\u5956\u52b1\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u5ffd\u89c6\u63a8\u7406\u8d28\u91cf\u548c\u6613\u53d7\u5956\u52b1\u653b\u51fb\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6d4b\u8bd5\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\uff0c\u4e14\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u653b\u51fb\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86LCB-RB\u57fa\u51c6\u6765\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\uff0c\u91c7\u7528OD-based\u65b9\u6cd5\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5f15\u5165P-GRPO\u65b9\u6cd5\uff0c\u6839\u636e\u4efb\u52a1\u6210\u529f\u9009\u62e9\u6027\u5956\u52b1\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "7B\u53c2\u6570\u7684\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53474.5%\uff0c\u63a5\u8fd1GPT-4-Turbo\u7684\u6027\u80fd\uff0c\u5e76\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u63a8\u7406\u8d28\u91cf\u548c\u4efb\u52a1\u6210\u529f\u5956\u52b1\uff0c\u907f\u514d\u4e86\u5956\u52b1\u653b\u51fb\uff0c\u540c\u65f6\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.04995", "pdf": "https://arxiv.org/pdf/2508.04995", "abs": "https://arxiv.org/abs/2508.04995", "authors": ["Matthew Kelly"], "title": "Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge", "categories": ["cs.HC", "cs.AI", "cs.DL", "K.4.1; K.3; K.2"], "comment": "27 pages including references. Draft prepared for submission to\n  Science, Technology & Human Values", "summary": "Large Language Models (LLMs) such as ChatGPT have rendered visible the\nfragility of contemporary knowledge infrastructures by simulating coherence\nwhile bypassing traditional modes of citation, authority, and validation. This\npaper introduces the Situated Epistemic Infrastructures (SEI) framework as a\ndiagnostic tool for analyzing how knowledge becomes authoritative across hybrid\nhuman-machine systems under post-coherence conditions. Rather than relying on\nstable scholarly domains or bounded communities of practice, SEI traces how\ncredibility is mediated across institutional, computational, and temporal\narrangements. Integrating insights from infrastructure studies, platform\ntheory, and epistemology, the framework foregrounds coordination over\nclassification, emphasizing the need for anticipatory and adaptive models of\nepistemic stewardship. The paper contributes to debates on AI governance,\nknowledge production, and the ethical design of information systems by offering\na robust alternative to representationalist models of scholarly communication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSituated Epistemic Infrastructures (SEI)\u6846\u67b6\uff0c\u5206\u6790\u5728\u540e\u8fde\u8d2f\u6027\u6761\u4ef6\u4e0b\u77e5\u8bc6\u5982\u4f55\u5728\u4eba\u673a\u6df7\u5408\u7cfb\u7edf\u4e2d\u83b7\u5f97\u6743\u5a01\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u66b4\u9732\u4e86\u5f53\u524d\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\u7684\u8106\u5f31\u6027\uff0c\u5176\u6a21\u62df\u8fde\u8d2f\u6027\u5374\u7ed5\u8fc7\u4e86\u4f20\u7edf\u5f15\u7528\u3001\u6743\u5a01\u548c\u9a8c\u8bc1\u65b9\u5f0f\u3002", "method": "SEI\u6846\u67b6\u7ed3\u5408\u57fa\u7840\u8bbe\u65bd\u7814\u7a76\u3001\u5e73\u53f0\u7406\u8bba\u548c\u8ba4\u8bc6\u8bba\uff0c\u8ffd\u8e2a\u4fe1\u8a89\u5728\u673a\u6784\u3001\u8ba1\u7b97\u548c\u65f6\u95f4\u5b89\u6392\u4e2d\u7684\u5a92\u4ecb\u4f5c\u7528\u3002", "result": "\u6846\u67b6\u5f3a\u8c03\u534f\u8c03\u800c\u975e\u5206\u7c7b\uff0c\u63d0\u51fa\u524d\u77bb\u6027\u548c\u9002\u5e94\u6027\u7684\u77e5\u8bc6\u7ba1\u7406\u6a21\u5f0f\u3002", "conclusion": "SEI\u6846\u67b6\u4e3aAI\u6cbb\u7406\u3001\u77e5\u8bc6\u751f\u4ea7\u548c\u4fe1\u606f\u7cfb\u7edf\u4f26\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u8868\u5f81\u4e3b\u4e49\u6a21\u578b\u3002"}}
{"id": "2508.05626", "pdf": "https://arxiv.org/pdf/2508.05626", "abs": "https://arxiv.org/abs/2508.05626", "authors": ["Chris Careaga", "Ya\u011f\u0131z Aksoy"], "title": "Physically Controllable Relighting of Photographs", "categories": ["cs.GR", "cs.CV", "I.4"], "comment": "Proc. SIGGRAPH 2025, 10 pages, 9 figures", "summary": "We present a self-supervised approach to in-the-wild image relighting that\nenables fully controllable, physically based illumination editing. We achieve\nthis by combining the physical accuracy of traditional rendering with the\nphotorealistic appearance made possible by neural rendering. Our pipeline works\nby inferring a colored mesh representation of a given scene using monocular\nestimates of geometry and intrinsic components. This representation allows\nusers to define their desired illumination configuration in 3D. The scene under\nthe new lighting can then be rendered using a path-tracing engine. We send this\napproximate rendering of the scene through a feed-forward neural renderer to\npredict the final photorealistic relighting result. We develop a differentiable\nrendering process to reconstruct in-the-wild scene illumination, enabling\nself-supervised training of our neural renderer on raw image collections. Our\nmethod represents a significant step in bringing the explicit physical control\nover lights available in typical 3D computer graphics tools, such as Blender,\nto in-the-wild relighting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u56fe\u50cf\u91cd\u5149\u7167\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u6e32\u67d3\u7684\u7269\u7406\u7cbe\u5ea6\u548c\u795e\u7ecf\u6e32\u67d3\u7684\u903c\u771f\u5916\u89c2\uff0c\u5b9e\u73b0\u5bf9\u771f\u5b9e\u573a\u666f\u5149\u7167\u7684\u5b8c\u5168\u63a7\u5236\u3002", "motivation": "\u5c06\u5178\u578b\u76843D\u56fe\u5f62\u5de5\u5177\u4e2d\u5bf9\u706f\u5149\u7684\u663e\u5f0f\u7269\u7406\u63a7\u5236\u5f15\u5165\u771f\u5b9e\u573a\u666f\u7684\u91cd\u5149\u7167\u4e2d\u3002", "method": "\u901a\u8fc7\u5355\u76ee\u4f30\u8ba1\u51e0\u4f55\u548c\u56fa\u6709\u7ec4\u4ef6\u63a8\u65ad\u5f69\u8272\u7f51\u683c\u8868\u793a\uff0c\u7ed3\u5408\u8def\u5f84\u8ffd\u8e2a\u5f15\u64ce\u548c\u795e\u7ecf\u7f51\u7edc\u6e32\u67d3\u5668\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u80fd\u591f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u5b8c\u5168\u53ef\u63a7\u3001\u7269\u7406\u51c6\u786e\u7684\u5149\u7167\u7f16\u8f91\u3002", "conclusion": "\u5c06\u4f20\u7edf\u6e32\u67d3\u4e0e\u795e\u7ecf\u7f51\u7edc\u6e32\u67d3\u76f8\u7ed3\u5408\uff0c\u4e3a\u771f\u5b9e\u573a\u666f\u7684\u91cd\u5149\u7167\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05546", "pdf": "https://arxiv.org/pdf/2508.05546", "abs": "https://arxiv.org/abs/2508.05546", "authors": ["Rasman Mubtasim Swargo", "Engin Arslan", "Md Arifuzzaman"], "title": "Modular Architecture for High-Performance and Low Overhead Data Transfers", "categories": ["cs.DC"], "comment": null, "summary": "High-performance applications necessitate rapid and dependable transfer of\nmassive datasets across geographically dispersed locations. Traditional file\ntransfer tools often suffer from resource underutilization and instability\nbecause of fixed configurations or monolithic optimization methods. We propose\nAutoMDT, a novel modular data transfer architecture that employs a deep\nreinforcement learning based agent to simultaneously optimize concurrency\nlevels for read, network, and write operations. Our solution incorporates a\nlightweight network-system simulator, enabling offline training of a Proximal\nPolicy Optimization (PPO) agent in approximately 45 minutes on average, thereby\novercoming the impracticality of lengthy online training in production\nnetworks. AutoMDT's modular design decouples I/O and network tasks, allowing\nthe agent to capture complex buffer dynamics precisely and to adapt quickly to\nchanging system and network conditions. Evaluations on production-grade\ntestbeds show that AutoMDT achieves up to 8x faster convergence and a 68%\nreduction in transfer completion times compared with state-of-the-art\nsolutions.", "AI": {"tldr": "AutoMDT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u5757\u5316\u6570\u636e\u4f20\u8f93\u67b6\u6784\uff0c\u901a\u8fc7\u4f18\u5316\u8bfb\u5199\u548c\u7f51\u7edc\u64cd\u4f5c\u7684\u5e76\u53d1\u6027\u663e\u8457\u63d0\u5347\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6587\u4ef6\u4f20\u8f93\u5de5\u5177\u7531\u4e8e\u56fa\u5b9a\u914d\u7f6e\u6216\u5355\u4e00\u4f18\u5316\u65b9\u6cd5\uff0c\u5e38\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u548c\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u6027\u80fd\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff08PPO\u7b97\u6cd5\uff09\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7f51\u7edc\u7cfb\u7edf\u6a21\u62df\u5668\u8fdb\u884c\u79bb\u7ebf\u8bad\u7ec3\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u5206\u79bbI/O\u548c\u7f51\u7edc\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAutoMDT\u6bd4\u73b0\u6709\u6280\u672f\u5feb8\u500d\u6536\u655b\uff0c\u4f20\u8f93\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1168%\u3002", "conclusion": "AutoMDT\u901a\u8fc7\u6a21\u5757\u5316\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u6570\u636e\u4f20\u8f93\u7684\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.05192", "pdf": "https://arxiv.org/pdf/2508.05192", "abs": "https://arxiv.org/abs/2508.05192", "authors": ["Felix Neubauer", "J\u00fcrgen Pleiss", "Benjamin Uekermann"], "title": "AI-assisted JSON Schema Creation and Mapping", "categories": ["cs.SE", "H.2.3; I.2.6; D.2.2"], "comment": "Accepted for Tools and Demonstrations Track of ACM/IEEE MODELS'25", "summary": "Model-Driven Engineering (MDE) places models at the core of system and data\nengineering processes. In the context of research data, these models are\ntypically expressed as schemas that define the structure and semantics of\ndatasets. However, many domains still lack standardized models, and creating\nthem remains a significant barrier, especially for non-experts. We present a\nhybrid approach that combines large language models (LLMs) with deterministic\ntechniques to enable JSON Schema creation, modification, and schema mapping\nbased on natural language inputs by the user. These capabilities are integrated\ninto the open-source tool MetaConfigurator, which already provides visual model\nediting, validation, code generation, and form generation from models. For data\nintegration, we generate schema mappings from heterogeneous JSON, CSV, XML, and\nYAML data using LLMs, while ensuring scalability and reliability through\ndeterministic execution of generated mapping rules. The applicability of our\nwork is demonstrated in an application example in the field of chemistry. By\ncombining natural language interaction with deterministic safeguards, this work\nsignificantly lowers the barrier to structured data modeling and data\nintegration for non-experts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u786e\u5b9a\u6027\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u7b80\u5316JSON Schema\u7684\u521b\u5efa\u548c\u6620\u5c04\uff0c\u9002\u7528\u4e8e\u975e\u4e13\u5bb6\u7528\u6237\u3002", "motivation": "\u8bb8\u591a\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u6a21\u578b\uff0c\u4e14\u975e\u4e13\u5bb6\u521b\u5efa\u8fd9\u4e9b\u6a21\u578b\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6613\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u786e\u5b9a\u6027\u6280\u672f\uff0cMetaConfigurator\u5de5\u5177\u63d0\u4f9b\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684JSON Schema\u7f16\u8f91\u548c\u6620\u5c04\u529f\u80fd\u3002", "result": "\u5de5\u5177\u652f\u6301\u4ece\u5f02\u6784\u6570\u636e\u751f\u6210\u6620\u5c04\u89c4\u5219\uff0c\u5e76\u5728\u5316\u5b66\u9886\u57df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u663e\u8457\u964d\u4f4e\u4e86\u975e\u4e13\u5bb6\u5728\u7ed3\u6784\u6570\u636e\u5efa\u6a21\u548c\u6570\u636e\u96c6\u6210\u4e2d\u7684\u95e8\u69db\u3002"}}
{"id": "2508.05045", "pdf": "https://arxiv.org/pdf/2508.05045", "abs": "https://arxiv.org/abs/2508.05045", "authors": ["Sitong Wang"], "title": "Human-AI Schema Discovery and Application for Creative Problem Solving", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Humans often rely on underlying structural patterns-schemas-to create,\nwhether by writing stories, designing software, or composing music. Schemas\nhelp organize ideas and guide exploration, but they are often difficult to\ndiscover and apply, especially in complex or unfamiliar domains. My Ph.D.\nresearch develops a framework for human-AI schema discovery and application to\nsupport creative problem solving. I design systems that support users in\nsensemaking over examples to abstract schemas, and in operationalizing schemas\ninto human-AI co-creative workflows for application. This research offers\ninsights into how schema-guided interaction can make implicit knowledge more\naccessible and actionable, advancing more transparent and collaborative\nhuman-AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u548c\u5e94\u7528\u7ed3\u6784\u6027\u6a21\u5f0f\uff08schema\uff09\u6765\u652f\u6301\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u4eba\u7c7b\u5728\u521b\u9020\u6027\u6d3b\u52a8\u4e2d\u4f9d\u8d56\u7ed3\u6784\u6027\u6a21\u5f0f\uff08\u5982\u5199\u4f5c\u3001\u8bbe\u8ba1\u3001\u97f3\u4e50\u521b\u4f5c\uff09\uff0c\u4f46\u8fd9\u4e9b\u6a21\u5f0f\u5728\u590d\u6742\u6216\u964c\u751f\u9886\u57df\u4e2d\u96be\u4ee5\u53d1\u73b0\u548c\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u652f\u6301\u7528\u6237\u4ece\u793a\u4f8b\u4e2d\u62bd\u8c61\u51fa\u6a21\u5f0f\uff0c\u5e76\u5c06\u6a21\u5f0f\u8f6c\u5316\u4e3a\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\u7684\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u8868\u660e\u6a21\u5f0f\u5f15\u5bfc\u7684\u4ea4\u4e92\u53ef\u4ee5\u4f7f\u9690\u6027\u77e5\u8bc6\u66f4\u6613\u83b7\u53d6\u548c\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u4eba\u673a\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u534f\u4f5c\u6027\u3002"}}
{"id": "2508.04732", "pdf": "https://arxiv.org/pdf/2508.04732", "abs": "https://arxiv.org/abs/2508.04732", "authors": ["Xiaoqi Dong", "Xiangyu Zhou", "Nicholas Evans", "Yujia Lin"], "title": "LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Text-to-Image (T2I) generation has made significant advancements with\ndiffusion models, yet challenges persist in handling complex instructions,\nensuring fine-grained content control, and maintaining deep semantic\nconsistency. Existing T2I models often struggle with tasks like accurate text\nrendering, precise pose generation, or intricate compositional coherence.\nConcurrently, Vision-Language Models (LVLMs) have demonstrated powerful\ncapabilities in cross-modal understanding and instruction following. We propose\nLumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I\nmodel performance, particularly in areas requiring fine-grained control,\nthrough a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an\nIntelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt\nenhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which\nacts as a \"visual critic\" to iteratively correct and optimize generated images.\nEvaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a\nsuperior average score of 3.08, outperforming state-of-the-art baselines.\nNotably, our framework demonstrates significant improvements in critical\ndimensions such as text rendering and pose expression, validating the\neffectiveness of LVLM integration for more controllable and higher-quality\nimage generation.", "AI": {"tldr": "LumiGen \u662f\u4e00\u4e2a\u57fa\u4e8e LVLM \u589e\u5f3a\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u63d0\u793a\u89e3\u6790\u4e0e\u89c6\u89c9\u53cd\u9988\u673a\u5236\uff0c\u63d0\u5347 T2I \u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709 T2I \u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u5904\u7406\u3001\u7ec6\u7c92\u5ea6\u5185\u5bb9\u63a7\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LumiGen \u5305\u542b\u667a\u80fd\u63d0\u793a\u89e3\u6790\u589e\u5f3a\u6a21\u5757\uff08IPPA\uff09\u548c\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u4f18\u5316\u6a21\u5757\uff08IVFR\uff09\uff0c\u901a\u8fc7 LVLM \u9a71\u52a8\u7684\u95ed\u73af\u53cd\u9988\u673a\u5236\u4f18\u5316\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5728 LongBench-T2I Benchmark \u4e0a\uff0cLumiGen \u53d6\u5f97\u4e86 3.08 \u7684\u5e73\u5747\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u6587\u672c\u6e32\u67d3\u548c\u59ff\u52bf\u8868\u8fbe\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LumiGen \u901a\u8fc7 LVLM \u7684\u96c6\u6210\uff0c\u589e\u5f3a\u4e86 T2I \u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05135", "pdf": "https://arxiv.org/pdf/2508.05135", "abs": "https://arxiv.org/abs/2508.05135", "authors": ["Thinh Nguyen", "Trung Phan", "Binh T. Nguyen", "Khoa D Doan", "Kok-Seng Wong"], "title": "HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation", "categories": ["cs.LG", "cs.DC", "C.2.4; I.2.11"], "comment": "11 pages, 3 figures", "summary": "Federated Learning (FL) is a decentralized approach where multiple clients\ncollaboratively train a shared global model without sharing their raw data.\nDespite its effectiveness, conventional FL faces scalability challenges due to\nexcessive computational and communication demands placed on a single central\nserver as the number of participating devices grows. Hierarchical Federated\nLearning (HFL) addresses these issues by distributing model aggregation tasks\nacross intermediate nodes (stations), thereby enhancing system scalability and\nrobustness against single points of failure. However, HFL still suffers from a\ncritical yet often overlooked limitation: domain shift, where data\ndistributions vary significantly across different clients and stations,\nreducing model performance on unseen target domains. While Federated Domain\nGeneralization (FedDG) methods have emerged to improve robustness to domain\nshifts, their integration into HFL frameworks remains largely unexplored. In\nthis paper, we formally introduce Hierarchical Federated Domain Generalization\n(HFedDG), a novel scenario designed to investigate domain shift within\nhierarchical architectures. Specifically, we propose HFedATM, a hierarchical\naggregation method that first aligns the convolutional filters of models from\ndifferent stations through Filter-wise Optimal Transport Alignment and\nsubsequently merges aligned models using a Shrinkage-aware Regularized Mean\nAggregation. Our extensive experimental evaluations demonstrate that HFedATM\nsignificantly boosts the performance of existing FedDG baselines across\nmultiple datasets and maintains computational and communication efficiency.\nMoreover, theoretical analyses indicate that HFedATM achieves tighter\ngeneralization error bounds compared to standard hierarchical averaging,\nresulting in faster convergence and stable training behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHFedATM\u7684\u5206\u5c42\u8054\u90a6\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ee4\u6ce2\u5668\u5bf9\u9f50\u548c\u6b63\u5219\u5316\u805a\u5408\u89e3\u51b3HFL\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5206\u5c42\u67b6\u6784\u867d\u7136\u89e3\u51b3\u4e86\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f46\u4ecd\u9762\u4e34\u57df\u504f\u79fb\u7684\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u5728\u65b0\u57df\u4e0a\u7684\u8868\u73b0\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u8fd9\u4e00\u67b6\u6784\u4e2d\u6709\u6548\u5e94\u5bf9\u57df\u504f\u79fb\u3002", "method": "\u63d0\u51faHFedATM\u65b9\u6cd5\uff0c\u9996\u5148\u901a\u8fc7\u6ee4\u6ce2\u5668\u6700\u4f18\u8fd0\u8f93\u5bf9\u9f50\u5bf9\u4e0d\u540c\u57fa\u7ad9\u6a21\u578b\u8fdb\u884c\u5377\u79ef\u6ee4\u6ce2\u5668\u5bf9\u9f50\uff0c\u518d\u4f7f\u7528\u6536\u7f29\u611f\u77e5\u7684\u6b63\u5219\u5316\u5747\u503c\u805a\u5408\u65b9\u6cd5\u5408\u5e76\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHFedATM\u5728\u591a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u57df\u6cdb\u5316\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u6548\u7387\u3002\u7406\u8bba\u5206\u6790\u663e\u793a\u5176\u6cdb\u5316\u8bef\u5dee\u66f4\u5c0f\uff0c\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "HFedATM\u4e3a\u89e3\u51b3\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u6027\u80fd\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2508.05193", "pdf": "https://arxiv.org/pdf/2508.05193", "abs": "https://arxiv.org/abs/2508.05193", "authors": ["Kaiwen Yan", "Yuhang Chang", "Zirui Guo", "Yaling Mou", "Jiang Ming", "Jingwei Sun"], "title": "STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning", "categories": ["cs.SE"], "comment": null, "summary": "In recent years, large language models (LLMs) have made significant progress\nin code intelligence, yet systematically evaluating their code understanding\nand reasoning abilities remains challenging. Mainstream benchmarks such as\nHumanEval and MBPP primarily assess functional correctness, while reasoning\nbenchmarks like CRUXEVAL are limited to single-function, low-complexity\nscenarios. As a result, advanced models achieve nearly saturated scores,\nlimiting their discriminative power. To address this, we present\nSTEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex\nmulti-function understanding and fine-grained execution reasoning. SX-Bench\nfeatures tasks involving collaboration among multiple sub-functions (e.g.,\nchained calls, nested loops), shifting evaluation towards overall control and\ndata flow modeling. It defines \"computation steps\" as the minimal execution\nunit and requires models to predict the total number of steps in reasoning\ntasks, thereby assessing a model's in-depth understanding of dynamic execution\nbeyond simple I/O matching. Evaluation on over 20 mainstream models (including\n14 reasoning-enhanced models) demonstrates that SX-Bench is highly\ndiscriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent\naccuracy on Hard-Reasoning tasks, much lower than its saturated scores on\nprevious benchmarks, thereby revealing bottlenecks in complex and fine-grained\nreasoning. We also release an automated pipeline combining program synthesis,\nsymbolic execution, and LLM-aided validation for efficient benchmark generation\nand quality assurance. SX-Bench advances code evaluation from \"single-function\nverification\" to \"multi-function dynamic reasoning,\" providing a key tool for\nthe in-depth assessment of advanced code intelligence models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86STEPWISE-CODEX-Bench\uff08SX-Bench\uff09\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u590d\u6742\u591a\u51fd\u6570\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u6267\u884c\u63a8\u7406\u7684\u65b0\u57fa\u51c6\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u9ad8\u7ea7\u4ee3\u7801\u667a\u80fd\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\uff08\u5982HumanEval\u3001MBPP\uff09\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u800c\u63a8\u7406\u57fa\u51c6\uff08\u5982CRUXEVAL\uff09\u5c40\u9650\u4e8e\u4f4e\u590d\u6742\u5ea6\u573a\u666f\uff0c\u5bfc\u81f4\u9ad8\u7ea7\u6a21\u578b\u7684\u5206\u6570\u8d8b\u8fd1\u9971\u548c\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u5176\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "method": "SX-Bench\u8bbe\u8ba1\u4e86\u6d89\u53ca\u591a\u5b50\u51fd\u6570\u534f\u4f5c\u7684\u4efb\u52a1\uff0c\u5e76\u4ee5\u201c\u8ba1\u7b97\u6b65\u9aa4\u201d\u4e3a\u6700\u5c0f\u6267\u884c\u5355\u4f4d\uff0c\u8981\u6c42\u6a21\u578b\u9884\u6d4b\u63a8\u7406\u4efb\u52a1\u7684\u603b\u6b65\u9aa4\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u7a0b\u5e8f\u5408\u6210\u3001\u7b26\u53f7\u6267\u884c\u548cLLM\u8f85\u52a9\u9a8c\u8bc1\u7684\u81ea\u52a8\u751f\u6210\u7ba1\u9053\u3002", "result": "\u572820\u591a\u4e2a\u4e3b\u6d41\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSX-Bench\u5177\u6709\u9ad8\u5ea6\u533a\u5206\u6027\uff1a\u5373\u4f7f\u662fOpenAI-O3\u5728Hard-Reasoning\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u523078.37%\u7684\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u5176\u5728\u4f20\u7edf\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "SX-Bench\u5c06\u4ee3\u7801\u8bc4\u4f30\u4ece\u201c\u5355\u51fd\u6570\u9a8c\u8bc1\u201d\u63a8\u8fdb\u5230\u201c\u591a\u51fd\u6570\u52a8\u6001\u63a8\u7406\u201d\uff0c\u4e3a\u6df1\u5165\u8bc4\u4f30\u9ad8\u7ea7\u4ee3\u7801\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2508.05056", "pdf": "https://arxiv.org/pdf/2508.05056", "abs": "https://arxiv.org/abs/2508.05056", "authors": ["Vaanee Tripathi", "Aalok Thakkar"], "title": "Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments", "categories": ["cs.HC"], "comment": null, "summary": "Computer science education has evolved extensively; however, systemic\nbarriers still prevent students with visual impairments from fully\nparticipating. While existing research has developed specialized programming\ntools and assistive technologies, these solutions remain fragmented and often\nrequire complex technical infrastructure, which limits their classroom\nimplementation. Current approaches treat accessibility as individual\naccommodations rather than integral curriculum design, creating gaps in\nholistic educational support. This paper presents a comprehensive framework for\nredesigning introductory computer science curricula to provide equitable\nlearning experiences for students with visual impairments without requiring\nspecialized technical infrastructure. The framework outlines five key\ncomponents that together contribute a systematic approach to curriculum\naccessibility: accessible learning resources with pre-distributed materials and\ntactile diagrams, in-class learning kits with hands-on demonstrations,\nstructured support systems with dedicated teaching assistance, an online tool\nrepository, and psychosocial support for classroom participation. Unlike\nexisting tool-focused solutions, this framework addresses both technical and\npedagogical dimensions of inclusive education while emphasizing practical\nimplementation in standard university settings. The design is grounded in\nuniversal design principles and validated through expert consultation with\naccessibility specialists and disability services professionals, establishing\nfoundations for future empirical evaluation of learning outcomes and student\nengagement while serving as a template for broader institutional adoption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u91cd\u65b0\u8bbe\u8ba1\u8ba1\u7b97\u673a\u79d1\u5b66\u5165\u95e8\u8bfe\u7a0b\uff0c\u4e3a\u89c6\u969c\u5b66\u751f\u63d0\u4f9b\u516c\u5e73\u5b66\u4e60\u4f53\u9a8c\uff0c\u4e0d\u4f9d\u8d56\u7279\u6b8a\u6280\u672f\u8bbe\u65bd\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u969c\u788d\uff0c\u89c6\u969c\u5b66\u751f\u53c2\u4e0e\u53d7\u9650\uff1b\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5206\u6563\u4e14\u6280\u672f\u590d\u6742\uff0c\u7f3a\u4e4f\u6574\u4f53\u8bfe\u7a0b\u8bbe\u8ba1\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e94\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u53ef\u8bbf\u95ee\u5b66\u4e60\u8d44\u6e90\u3001\u8bfe\u5802\u5b66\u4e60\u5de5\u5177\u5305\u3001\u652f\u6301\u7cfb\u7edf\u3001\u5728\u7ebf\u5de5\u5177\u5e93\u548c\u5fc3\u7406\u652f\u6301\uff0c\u57fa\u4e8e\u901a\u7528\u8bbe\u8ba1\u539f\u5219\u5e76\u901a\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u4ece\u6280\u672f\u548c\u6559\u5b66\u7ef4\u5ea6\u89e3\u51b3\u5305\u5bb9\u6027\u6559\u80b2\u95ee\u9898\uff0c\u9002\u5408\u6807\u51c6\u5927\u5b66\u73af\u5883\u5b9e\u65bd\u3002", "conclusion": "\u6846\u67b6\u4e3a\u89c6\u969c\u5b66\u751f\u63d0\u4f9b\u7cfb\u7edf\u5316\u652f\u6301\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u8bc4\u4f30\u6548\u679c\u5e76\u63a8\u5e7f\u5230\u66f4\u5e7f\u673a\u6784\u3002"}}
{"id": "2508.04962", "pdf": "https://arxiv.org/pdf/2508.04962", "abs": "https://arxiv.org/abs/2508.04962", "authors": ["Peng Zhang", "Songru Yang", "Jinsheng Sun", "Weiqing Li", "Zhiyong Su"], "title": "Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework", "categories": ["cs.CV", "cs.GR"], "comment": "To be published in IEEE Transactions on Circuits and Systems for\n  Video Technology", "summary": "Open-world point cloud semantic segmentation (OW-Seg) aims to predict point\nlabels of both base and novel classes in real-world scenarios. However,\nexisting methods rely on resource-intensive offline incremental learning or\ndensely annotated support data, limiting their practicality. To address these\nlimitations, we propose HOW-Seg, the first human-in-the-loop framework for\nOW-Seg. Specifically, we construct class prototypes, the fundamental\nsegmentation units, directly on the query data, avoiding the prototype bias\ncaused by intra-class distribution shifts between the support and query data.\nBy leveraging sparse human annotations as guidance, HOW-Seg enables\nprototype-based segmentation for both base and novel classes. Considering the\nlack of granularity of initial prototypes, we introduce a hierarchical\nprototype disambiguation mechanism to refine ambiguous prototypes, which\ncorrespond to annotations of different classes. To further enrich contextual\nawareness, we employ a dense conditional random field (CRF) upon the refined\nprototypes to optimize their label assignments. Through iterative human\nfeedback, HOW-Seg dynamically improves its predictions, achieving high-quality\nsegmentation for both base and novel classes. Experiments demonstrate that with\nsparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or\nsurpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)\nmethod under the 5-shot setting. When using advanced backbones (e.g.,\nStratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),\nHOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,\nsignificantly outperforming alternatives.", "AI": {"tldr": "HOW-Seg\u662f\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u53cd\u9988\u7684\u5f00\u653e\u4e16\u754c\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u6807\u6ce8\u548c\u5c42\u7ea7\u539f\u578b\u6d88\u6b67\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u589e\u91cf\u5b66\u4e60\u6216\u5bc6\u96c6\u6807\u6ce8\u652f\u6301\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u67e5\u8be2\u6570\u636e\u4e0a\u7684\u7c7b\u522b\u539f\u578b\uff0c\u5229\u7528\u7a00\u758f\u4eba\u5de5\u6807\u6ce8\u6307\u5bfc\uff0c\u7ed3\u5408\u5c42\u7ea7\u539f\u578b\u6d88\u6b67\u548c\u5bc6\u96c6\u6761\u4ef6\u968f\u673a\u573a\u4f18\u5316\u3002", "result": "\u5728\u7a00\u758f\u6807\u6ce8\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff1b\u4f7f\u7528\u5148\u8fdb\u9aa8\u5e72\u7f51\u7edc\u548c\u5bc6\u96c6\u6807\u6ce8\u65f6\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HOW-Seg\u5728\u5f00\u653e\u4e16\u754c\u70b9\u4e91\u5206\u5272\u4e2d\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.05568", "pdf": "https://arxiv.org/pdf/2508.05568", "abs": "https://arxiv.org/abs/2508.05568", "authors": ["Qinghua Yao", "Xiangrui Xu", "Zhize Li"], "title": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment", "categories": ["cs.LG", "cs.CV", "cs.DC", "math.OC"], "comment": "20 pages", "summary": "Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference.", "AI": {"tldr": "X-VFL\u662f\u4e00\u79cd\u65b0\u7684\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6837\u672c\u4e0d\u5bf9\u9f50\u548c\u672c\u5730\u72ec\u7acb\u63a8\u7406\u7684\u95ee\u9898\uff0c\u901a\u8fc7XCom\u548cDS-Align\u6a21\u5757\u5b9e\u73b0\u7f3a\u5931\u7279\u5f81\u7684\u8865\u5168\u548c\u51b3\u7b56\u5b50\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u9700\u8981\u6570\u636e\u6837\u672c\u5b8c\u7f8e\u5bf9\u9f50\u4e14\u4e0d\u652f\u6301\u672c\u5730\u72ec\u7acb\u63a8\u7406\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002X-VFL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u3002", "method": "X-VFL\u5f15\u5165\u4e86\u4e24\u4e2a\u6a21\u5757\uff1aXCom\u7528\u4e8e\u8865\u5168\u7f3a\u5931\u7279\u5f81\uff0cDS-Align\u5728\u51b3\u7b56\u5b50\u7a7a\u95f4\u5bf9\u9f50\u672c\u5730\u548c\u5168\u5c40\u7279\u5f81\u3002\u652f\u6301SGD\u548cPAGE\u7b49\u8bad\u7ec3\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cX-VFL\u5728CIFAR-10\u548cMIMIC-III\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u5347\u4e8615%\u548c43%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "X-VFL\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86VFL\u5728\u975e\u5bf9\u9f50\u6570\u636e\u548c\u672c\u5730\u63a8\u7406\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.05199", "pdf": "https://arxiv.org/pdf/2508.05199", "abs": "https://arxiv.org/abs/2508.05199", "authors": ["Igor Costa", "Christopher Baran"], "title": "EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0", "categories": ["cs.SE", "cs.AI", "D.2.2; D.2.7; I.2.2"], "comment": "15 pages, 3 tables, 1 algorithm. Submitted to ICSE 2025", "summary": "We introduce **EvoGraph**, a framework that enables software systems to\nevolve their own source code, build pipelines, documentation, and tickets.\nEvoGraph represents every artefact in a typed directed graph, applies learned\nmutation operators driven by specialized small language models (SLMs), and\nselects survivors with a multi-objective fitness. On three benchmarks, EvoGraph\nfixes 83% of known security vulnerabilities, translates COBOL to Java with 93%\nfunctional equivalence (test verified), and maintains documentation freshness\nwithin two minutes. Experiments show a 40% latency reduction and a sevenfold\ndrop in feature lead time compared with strong baselines. We extend our\napproach to **evoGraph**, leveraging language-specific SLMs for modernizing\n.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%\nsemantic equivalence across languages while reducing computational costs by 90%\ncompared to large language models. EvoGraph's design responds to empirical\nfailure modes in legacy modernization, such as implicit contracts, performance\npreservation, and integration evolution. Our results suggest a practical path\ntoward Software 3.0, where systems adapt continuously yet remain under\nmeasurable control.", "AI": {"tldr": "EvoGraph\u6846\u67b6\u901a\u8fc7\u7c7b\u578b\u5316\u6709\u5411\u56fe\u548cSLM\u9a71\u52a8\u53d8\u5f02\u64cd\u4f5c\uff0c\u5b9e\u73b0\u8f6f\u4ef6\u7cfb\u7edf\u7684\u81ea\u6211\u8fdb\u5316\uff0c\u4fee\u590d\u6f0f\u6d1e\u3001\u5b8c\u6210\u8bed\u8a00\u8f6c\u6362\u5e76\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u9057\u7559\u7cfb\u7edf\u73b0\u4ee3\u5316\u548c\u6301\u7eed\u9002\u5e94\u6027\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7c7b\u578b\u5316\u6709\u5411\u56fe\u8868\u793a\u8f6f\u4ef6\u4ea7\u7269\uff0c\u7ed3\u5408SLM\u9a71\u52a8\u7684\u53d8\u5f02\u64cd\u4f5c\u548c\u591a\u76ee\u6807\u9002\u5e94\u5ea6\u9009\u62e9\u3002", "result": "\u5b89\u5168\u6f0f\u6d1e\u4fee\u590d\u738783%\uff0cCOBOL\u8f6cJava\u529f\u80fd\u7b49\u6548\u738793%\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e90%\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "EvoGraph\u4e3a\u8f6f\u4ef63.0\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u6027\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2508.05088", "pdf": "https://arxiv.org/pdf/2508.05088", "abs": "https://arxiv.org/abs/2508.05088", "authors": ["Sam Johnson-Lacoss", "Santiago V. Lombeyda", "S. George Djorgovski"], "title": "A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments", "categories": ["cs.HC"], "comment": null, "summary": "Mixed reality (MR) environments are bound to become ubiquitous as MR\ntechnology becomes lighter, higher resolution, more affordable, and overall\nbecomes a seamless extension of our current work and living spaces. For\nresearch scientists and clinicians focused on understanding 3D phenomena or\npatient pathologies within the context of the larger human anatomy, that means\na necessary evolution of their workstations currently only utilizing 2D\ninterfaces for everyday communication, logistics and data analysis. MR\ntechnologies bring forth immersive 3D representations coexisting in our natural\nspaces, while allowing for richer interconnected information displays, where 3D\nrepresentations greatly aid in the detailed understanding of physical\nstructures, spatial relationships, and 3D contextualization of 2D measurements,\nprojections, abstractions, and other data details. We present a breakdown of\nthe different interaction zones and modalities into a design space that best\naccommodates the creation of applications for users engaged through MR\ntechnologies in precise object-centric data analysis within the ergonomic\nconfines of their desktop physical spaces.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u6280\u672f\u5728\u79d1\u7814\u548c\u4e34\u5e8a\u5de5\u4f5c\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u7a7a\u95f4\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u968f\u7740MR\u6280\u672f\u7684\u666e\u53ca\uff0c\u5982\u4f55\u5728\u684c\u9762\u7269\u7406\u7a7a\u95f4\u4e2d\u9ad8\u6548\u5229\u75283D\u4ea4\u4e92\u8fdb\u884c\u7cbe\u786e\u6570\u636e\u5206\u6790\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7684\u4ea4\u4e92\u533a\u57df\u548c\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5408MR\u6280\u672f\u5e94\u7528\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5206\u7c7b\u3002", "result": "\u8bbe\u8ba1\u7a7a\u95f4\u5206\u7c7b\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u7528\u6237\u5728MR\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u9ad8\u6548\u7684\u5bf9\u8c61\u4e2d\u5fc3\u6570\u636e\u5206\u6790\u3002", "conclusion": "MR\u6280\u672f\u4e3a\u79d1\u7814\u548c\u4e34\u5e8a\u5de5\u4f5c\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u76843D\u4ea4\u4e92\u65b9\u5f0f\uff0c\u8bbe\u8ba1\u7a7a\u95f4\u5206\u7c7b\u6709\u52a9\u4e8e\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.05301", "pdf": "https://arxiv.org/pdf/2508.05301", "abs": "https://arxiv.org/abs/2508.05301", "authors": ["Victoria Torres Bosch", "Ronny Seiger", "Manuela Albert Albiol", "Antoni Mestre Gascon", "Pedro Jose Valderas Aranda"], "title": "A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes", "categories": ["cs.SE", "cs.CY"], "comment": "Submitted to Information Systems Frontiers (1572-9419)", "summary": "The real-time data collection and automation capabilities offered by the\nInternet of Things (IoT) are revolutionizing and transforming Business\nProcesses (BPs) into IoT-enhanced BPs, showing high potential for improving\nsustainability. Although already studied in Business Process Management (BPM),\nsustainability research has primarily focused on environmental concerns.\nHowever, achieving a holistic and lasting impact requires a systematic approach\nto address sustainability beyond the environmental dimension. This work\nproposes a conceptual model and a structured methodology with the goal of\nanalyzing the potential of IoT to measure and improve the sustainability of\nBPs. The conceptual model formally represents key sustainability concepts,\nlinking BPM and IoT by highlighting how IoT devices support and contribute to\nsustainability. The methodology guides the systematic analysis of existing BPs,\nidentifies opportunities, and implements sustainability-aware, IoT-enhanced\nBPs. The approach is illustrated through a running example from the tourism\ndomain and a case study in healthcare.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u5ff5\u6a21\u578b\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5206\u6790\u7269\u8054\u7f51\uff08IoT\uff09\u5728\u8861\u91cf\u548c\u6539\u8fdb\u4e1a\u52a1\u6d41\u7a0b\uff08BPs\uff09\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u65c5\u6e38\u548c\u533b\u7597\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u5c3d\u7ba1\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\uff08BPM\uff09\u4e2d\u5df2\u7814\u7a76\u53ef\u6301\u7eed\u6027\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u73af\u5883\u95ee\u9898\u4e0a\uff1b\u800c\u5b9e\u73b0\u5168\u9762\u4e14\u6301\u4e45\u7684\u53ef\u6301\u7eed\u6027\u9700\u8981\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u6db5\u76d6\u66f4\u5e7f\u6cdb\u7684\u7ef4\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u6a21\u578b\u5c06\u5173\u952e\u53ef\u6301\u7eed\u6027\u6982\u5ff5\u4e0eBPM\u548cIoT\u8054\u7cfb\u8d77\u6765\uff0c\u65b9\u6cd5\u5219\u6307\u5bfc\u7cfb\u7edf\u5206\u6790\u73b0\u6709BPs\u5e76\u5b9e\u65bd\u53ef\u6301\u7eed\u6027\u611f\u77e5\u7684IoT\u589e\u5f3aBPs\u3002", "result": "\u901a\u8fc7\u65c5\u6e38\u9886\u57df\u7684\u793a\u4f8b\u548c\u533b\u7597\u4fdd\u5065\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5229\u7528IoT\u63d0\u5347BPs\u7684\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5f25\u8865\u4e86\u5f53\u524d\u7814\u7a76\u5728\u975e\u73af\u5883\u7ef4\u5ea6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.05098", "pdf": "https://arxiv.org/pdf/2508.05098", "abs": "https://arxiv.org/abs/2508.05098", "authors": ["Anand Kumar", "Antony Albert Raj Irudayaraj", "Ishita Chandra", "Adwait Sharma", "Aditya Shekhar Nittala"], "title": "SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures", "categories": ["cs.HC"], "comment": "UIST'25: Proceedings of the 38th Annual ACM Symposium on User\n  Interface Software and Technology", "summary": "Gesture recognition with electromyography (EMG) is a complex problem\ninfluenced by gesture sets, electrode count and placement, and machine learning\nparameters (e.g., features, classifiers). Most existing toolkits focus on\nstreamlining model development but overlook the impact of electrode selection\non classification accuracy. In this work, we present the first data-driven\nanalysis of how electrode selection and classifier choice affect both accuracy\nand sparsity. Through a systematic evaluation of 28 combinations (4 selection\nschemes, 7 classifiers), across six datasets, we identify an approach that\nminimizes electrode count without compromising accuracy. The results show that\nPermutation Importance (selection scheme) with Random Forest (classifier)\nreduces the number of electrodes by 53.5\\%. Based on these findings, we\nintroduce SparseEMG, a design tool that generates sparse electrode layouts\nbased on user-selected gesture sets, electrode constraints, and ML parameters\nwhile also predicting classification performance. SparseEMG supports 50+ unique\ngestures and is validated in three real-world applications using different\nhardware setups. Results from our multi-dataset evaluation show that the\nlayouts generated from the SparseEMG design tool are transferable across users\nwith only minimal variation in gesture recognition performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u7535\u6781\u9009\u62e9\u548c\u5206\u7c7b\u5668\u9009\u62e9\u5bf9EMG\u624b\u52bf\u8bc6\u522b\u7cbe\u5ea6\u548c\u7a00\u758f\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177SparseEMG\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u6548\u7684\u7a00\u758f\u7535\u6781\u5e03\u5c40\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5305\u5927\u591a\u5ffd\u89c6\u7535\u6781\u9009\u62e9\u5bf9\u5206\u7c7b\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f18\u5316\u7535\u6781\u6570\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f3028\u79cd\u7ec4\u5408\uff084\u79cd\u9009\u62e9\u65b9\u6848\uff0c7\u79cd\u5206\u7c7b\u5668\uff09\uff0c\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc6\u522b\u51fa\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u6700\u5c0f\u5316\u7535\u6781\u6570\u91cf\u7684\u65b9\u6cd5\u3002", "result": "Permutation Importance\u9009\u62e9\u65b9\u6848\u4e0eRandom Forest\u5206\u7c7b\u5668\u7ed3\u5408\uff0c\u53ef\u5c06\u7535\u6781\u6570\u91cf\u51cf\u5c1153.5%\uff0c\u5f00\u53d1\u4e86\u5de5\u5177SparseEMG\u5e76\u5728\u4e09\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "SparseEMG\u751f\u6210\u7684\u5e03\u5c40\u5728\u4e0d\u540c\u7528\u6237\u95f4\u5177\u6709\u53ef\u8f6c\u79fb\u6027\uff0c\u4e14\u8bc6\u522b\u6027\u80fd\u53d8\u5316\u5c0f\uff0c\u4e3aEMG\u624b\u52bf\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.04889", "pdf": "https://arxiv.org/pdf/2508.04889", "abs": "https://arxiv.org/abs/2508.04889", "authors": ["Theia Henderson", "David R. Karger", "David D. Clark"], "title": "Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications", "categories": ["cs.SI", "cs.HC", "cs.SE"], "comment": "Accepted to The 38th Annual ACM Symposium on User Interface Software\n  and Technology (UIST '25), September 28-October 1, 2025, Busan, Republic of\n  Korea. 21 pages", "summary": "Most social applications, from Twitter to Wikipedia, have rigid\none-size-fits-all designs, but building new social applications is both\ntechnically challenging and results in applications that are siloed away from\nexisting communities. We present Graffiti, a system that can be used to build a\nwide variety of personalized social applications with relative ease that also\ninteroperate with each other. People can freely move between a plurality of\ndesigns -- each with its own aesthetic, feature set, and moderation -- all\nwithout losing their friends or data.\n  Our concept of total reification makes it possible for seemingly\ncontradictory designs, including conflicting moderation rules, to interoperate.\nConversely, our concept of channels prevents interoperation from occurring by\naccident, avoiding context collapse.\n  Graffiti applications interact through a minimal client-side API, which we\nshow admits at least two decentralized implementations. Above the API, we built\na Vue.js plugin, which we use to develop applications similar to Twitter,\nMessenger, and Wikipedia using only client-side code. Our case studies explore\nhow these and other novel applications interoperate, as well as the broader\necosystem that Graffiti enables.", "AI": {"tldr": "Graffiti\u662f\u4e00\u79cd\u7cfb\u7edf\uff0c\u7528\u4e8e\u8f7b\u677e\u6784\u5efa\u4e2a\u6027\u5316\u7684\u793e\u4ea4\u5e94\u7528\uff0c\u5e76\u5b9e\u73b0\u5e94\u7528\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u540c\u65f6\u907f\u514d\u5185\u5bb9\u548c\u4e0a\u4e0b\u6587\u7684\u6df7\u4e71\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u793e\u4ea4\u5e94\u7528\u8bbe\u8ba1\u50f5\u5316\u3001\u96be\u4ee5\u4e2a\u6027\u5316\u4e14\u4e92\u64cd\u4f5c\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u201c\u5b8c\u5168\u5177\u4f53\u5316\u201d\u548c\u201c\u901a\u9053\u201d\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6700\u5c0f\u5ba2\u6237\u7aefAPI\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u591a\u79cd\u793e\u4ea4\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86Graffiti\u652f\u6301Twitter\u3001Messenger\u548cWikipedia\u7b49\u5e94\u7528\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u80fd\u907f\u514d\u4e0a\u4e0b\u6587\u5d29\u6e83\u3002", "conclusion": "Graffiti\u4e3a\u6784\u5efa\u591a\u6837\u5316\u4e14\u4e92\u64cd\u4f5c\u7684\u793e\u4ea4\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05112", "pdf": "https://arxiv.org/pdf/2508.05112", "abs": "https://arxiv.org/abs/2508.05112", "authors": ["Margarida Romero", "George Kalmpourtzis"], "title": "Metacognition and self-regulated learning in manipulative robotic problem-solving task", "categories": ["cs.HC"], "comment": null, "summary": "Metacognition is an important aspect in creative problem solving (CPS) and\nthrough this chapter we analyse the meta-reasoning aspects applied in the\ndifferent processes of monitoring the progress of learners' reasoning and CPS\nactivities. Meta-reasoning monitors the way that problem-solving processes\nadvance and regulate time and efforts towards a solution. In the context of an\nill-defined problem, exploration is required to develop a better-defined\nproblem space and advance towards the solution space. The way learners engage\nin exploration and exploitations is regulated by the meta-reasoning within the\nCPS activity. The objective of this chapter is to examine and identify the CPS\nprocess with educational robots through a metacognitive and interactionist\napproach. This chapter presents a case study, where, to solve a problem, a\nparticipant had to explore a set of robot cubes to develop the technological\nknowledge associated with each single component of the system, but also\nconceptualize a system-level behaviour of the cubes when they are assembled.\nThe chapter presents the emergence of knowledge through the metacognitive\nregulation of the process of exploration and exploitation of prior knowledge\nand emergent knowledge until finding a solution", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u5143\u8ba4\u77e5\u5728\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u5206\u6790\u5143\u63a8\u7406\u5982\u4f55\u76d1\u63a7\u548c\u8c03\u8282\u5b66\u4e60\u8005\u5728\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5143\u8ba4\u77e5\u548c\u4e92\u52a8\u4e3b\u4e49\u65b9\u6cd5\uff0c\u5206\u6790\u6559\u80b2\u673a\u5668\u4eba\u60c5\u5883\u4e2d\u7684CPS\u8fc7\u7a0b\uff0c\u63ed\u793a\u5143\u63a8\u7406\u5982\u4f55\u8c03\u8282\u5b66\u4e60\u8005\u7684\u63a2\u7d22\u4e0e\u77e5\u8bc6\u5229\u7528\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u6cd5\uff0c\u53c2\u4e0e\u8005\u901a\u8fc7\u63a2\u7d22\u673a\u5668\u4eba\u7acb\u65b9\u4f53\uff0c\u9010\u6b65\u6784\u5efa\u6280\u672f\u77e5\u8bc6\u5e76\u5f62\u6210\u7cfb\u7edf\u7ea7\u884c\u4e3a\u7406\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5143\u8ba4\u77e5\u8c03\u8282\u4e0b\u7684\u63a2\u7d22\u4e0e\u77e5\u8bc6\u5229\u7528\u884c\u4e3a\u4fc3\u8fdb\u4e86\u77e5\u8bc6\u7684\u6d8c\u73b0\uff0c\u6700\u7ec8\u4fc3\u6210\u95ee\u9898\u89e3\u51b3\u3002", "conclusion": "\u5143\u8ba4\u77e5\u5728CPS\u4e2d\u8d77\u5230\u5173\u952e\u4f5c\u7528\uff0c\u901a\u8fc7\u8c03\u8282\u63a2\u7d22\u4e0e\u5229\u7528\u884c\u4e3a\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u9010\u6b65\u6784\u5efa\u548c\u6574\u5408\u77e5\u8bc6\u3002"}}
{"id": "2508.05286", "pdf": "https://arxiv.org/pdf/2508.05286", "abs": "https://arxiv.org/abs/2508.05286", "authors": ["Katsiaryna Dzialets", "Aleksandra Makeeva", "Ilya Vlasov", "Anna Potriasaeva", "Aleksei Rostovskii", "Yaroslav Golubev", "Anastasiia Birillo"], "title": "Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants", "categories": ["cs.CY", "cs.HC", "cs.SE"], "comment": "Accepted to CompEd'25, 7 pages, 1 figure", "summary": "Computer science education is a dynamic field with many aspects that\ninfluence the learner's path. While these aspects are usually studied in depth\nseparately, it is also important to carry out broader large-scale studies that\ntouch on many topics, because they allow us to put different results into each\nother's perspective. Past large-scale surveys have provided valuable insights,\nhowever, the emergence of new trends (e.g., AI), new learning formats (e.g.,\nin-IDE learning), and the increasing learner diversity highlight the need for\nan updated comprehensive study. To address this, we conducted a survey with\n18,032 learners from 173 countries, ensuring diverse representation and\nexploring a wide range of topics - formal education, learning formats, AI\nusage, challenges, motivation, and more. This paper introduces the results of\nthis survey as an open dataset, describes our methodology and the survey\nquestions, and highlights, as a motivating example, three possible research\ndirections within this data: challenges in learning, emerging formats, and\ninsights into the in-IDE format. The dataset aims to support further research\nand foster advancements in computer education.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4e00\u9879\u8986\u76d618,032\u540d\u5b66\u4e60\u8005\u7684\u5168\u7403\u8c03\u67e5\uff0c\u66f4\u65b0\u4e86\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u9886\u57df\u7684\u5168\u9762\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u65b0\u8d8b\u52bf\u548c\u65b9\u6cd5\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8e\u65b0\u8d8b\u52bf\uff08\u5982AI\uff09\u3001\u65b0\u5b66\u4e60\u5f62\u5f0f\uff08\u5982IDE\u5185\u5b66\u4e60\uff09\u548c\u5b66\u4e60\u8005\u591a\u6837\u6027\u7684\u589e\u52a0\uff0c\u9700\u8981\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u8fdb\u884c\u66f4\u65b0\u7684\u7efc\u5408\u6027\u7814\u7a76\u3002", "method": "\u5bf9\u6765\u81ea173\u4e2a\u56fd\u5bb6\u768418,032\u540d\u5b66\u4e60\u8005\u8fdb\u884c\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u6db5\u76d6\u6b63\u89c4\u6559\u80b2\u3001\u5b66\u4e60\u5f62\u5f0f\u3001AI\u4f7f\u7528\u3001\u6311\u6218\u548c\u52a8\u673a\u7b49\u591a\u4e2a\u4e3b\u9898\u3002", "result": "\u63d0\u4f9b\u4e86\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff1a\u5b66\u4e60\u4e2d\u7684\u6311\u6218\u3001\u65b0\u5174\u5b66\u4e60\u5f62\u5f0f\u548cIDE\u5185\u5b66\u4e60\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u65e8\u5728\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u63a8\u52a8\u8ba1\u7b97\u673a\u6559\u80b2\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.05156", "pdf": "https://arxiv.org/pdf/2508.05156", "abs": "https://arxiv.org/abs/2508.05156", "authors": ["Nikolaos Avouris"], "title": "AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study", "categories": ["cs.HC"], "comment": "To be cited as: Avouris N., (2025). AI Conversational Tutors in\n  Foreign Language Learning: A Mixed-Methods Evaluation Study, in Proceedings\n  14th Panhellenic Conference ICT in Education HCICTE 2025, Rhodes, October\n  2025", "summary": "This paper focuses on AI tutors in foreign language learning, a field of\napplication of AI tutors with great development, especially during the last\nyears, when great advances in natural language understanding and processing in\nreal time, have been achieved. These tutors attempt to address needs for\nimproving language skills (speaking, or communicative competence,\nunderstanding). In this paper, a mixed-methos empirical study on the use of\ndifferent kinds of state-of-the-art AI tutors for language learning is\nreported. This study involves a user experience evaluation of typical such\ntools, with special focus in their conversation functionality and an evaluation\nof their quality, based on chat transcripts. This study can help establish\ncriteria for assessing the quality of such systems and inform the design of\nfuture tools, including concerns about data privacy and secure handling of\nlearner information.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u5916\u8bed\u5b66\u4e60\u5bfc\u5e08\u7684\u5e94\u7528\u53ca\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u5bf9\u8bdd\u529f\u80fd\u548c\u8d28\u91cf\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u8fdb\u6b65\uff0cAI\u5bfc\u5e08\u5728\u5916\u8bed\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5176\u6548\u679c\u5e76\u63d0\u5347\u8bbe\u8ba1\u6807\u51c6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u591a\u79cd\u5148\u8fdbAI\u5bfc\u5e08\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u5206\u6790\u804a\u5929\u8bb0\u5f55\u4ee5\u786e\u5b9a\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u4e3aAI\u5bfc\u5e08\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\uff0c\u5e76\u5173\u6ce8\u6570\u636e\u9690\u79c1\u548c\u5b66\u5458\u4fe1\u606f\u5b89\u5168\u7684\u672a\u6765\u8bbe\u8ba1\u3002", "conclusion": "AI\u5bfc\u5e08\u5728\u5916\u8bed\u5b66\u4e60\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u9700\u5728\u529f\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2508.05228", "pdf": "https://arxiv.org/pdf/2508.05228", "abs": "https://arxiv.org/abs/2508.05228", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "title": "CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Due to the intracranial volume conduction effects, high-dimensional\nmulti-channel electroencephalography (EEG) features often contain substantial\nredundant and irrelevant information. This issue not only hinders the\nextraction of discriminative emotional representations but also compromises the\nreal-time performance. Feature selection has been established as an effective\napproach to address the challenges while enhancing the transparency and\ninterpretability of emotion recognition models. However, existing EEG feature\nselection research overlooks the influence of latent EEG feature structures on\nemotional label correlations and assumes uniform importance across various\nchannels, directly limiting the precise construction of EEG feature selection\nmodels for multi-dimensional affective computing. To address these limitations,\na novel channel-wise EEG feature selection (CWEFS) method is proposed for\nmulti-dimensional emotion recognition. Specifically, inspired by brain volume\nconduction effects, CWEFS integrates EEG emotional feature selection into a\nshared latent structure model designed to construct a consensus latent space\nacross diverse EEG channels. To preserve the local geometric structure, this\nconsensus space is further integrated with the latent semantic analysis of\nmulti-dimensional emotional labels. Additionally, CWEFS incorporates adaptive\nchannel-weight learning to automatically determine the significance of\ndifferent EEG channels in the emotional feature selection task. The\neffectiveness of CWEFS was validated using three popular EEG datasets with\nmulti-dimensional emotional labels. Comprehensive experimental results,\ncompared against nineteen feature selection methods, demonstrate that the EEG\nfeature subsets chosen by CWEFS achieve optimal emotion recognition performance\nacross six evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684CWEFS\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u901a\u9053EEG\u7279\u5f81\u9009\u62e9\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7ed3\u6784\u6a21\u578b\u63d0\u5347\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684EEG\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6f5c\u5728EEG\u7279\u5f81\u7ed3\u6784\u5bf9\u60c5\u611f\u6807\u7b7e\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u4e14\u5047\u8bbe\u5404\u901a\u9053\u91cd\u8981\u6027\u76f8\u540c\uff0c\u9650\u5236\u4e86\u591a\u7ef4\u60c5\u611f\u8ba1\u7b97\u7684\u7cbe\u786e\u5efa\u6a21\u3002", "method": "CWEFS\u7ed3\u5408\u4e86EEG\u60c5\u611f\u7279\u5f81\u9009\u62e9\u548c\u5171\u4eab\u6f5c\u5728\u7ed3\u6784\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5171\u8bc6\u6f5c\u5728\u7a7a\u95f4\u548c\u81ea\u9002\u5e94\u901a\u9053\u6743\u91cd\u5b66\u4e60\uff0c\u4f18\u5316\u7279\u5f81\u9009\u62e9\u3002", "result": "\u5728\u4e09\u79cd\u6d41\u884c\u7684\u591a\u7ef4\u5ea6\u60c5\u611f\u6807\u7b7eEEG\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCWEFS\u5728\u516d\u79cd\u8bc4\u4ef7\u6307\u6807\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "CWEFS\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.05229", "pdf": "https://arxiv.org/pdf/2508.05229", "abs": "https://arxiv.org/abs/2508.05229", "authors": ["Tianze Yu", "Junming Zhang", "Wenjia Dong", "Xueyuan Xu", "Li Zhuo"], "title": "ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "EEG based multi-dimension emotion recognition has attracted substantial\nresearch interest in human computer interfaces. However, the high\ndimensionality of EEG features, coupled with limited sample sizes, frequently\nleads to classifier overfitting and high computational complexity. Feature\nselection constitutes a critical strategy for mitigating these challenges. Most\nexisting EEG feature selection methods assume complete multi-dimensional\nemotion labels. In practice, open acquisition environment, and the inherent\nsubjectivity of emotion perception often result in incomplete label data, which\ncan compromise model generalization. Additionally, existing feature selection\nmethods for handling incomplete multi-dimensional labels primarily focus on\ncorrelations among various dimensions during label recovery, neglecting the\ncorrelation between samples in the label space and their interaction with\nvarious dimensions. To address these issues, we propose a novel incomplete\nmulti-dimensional feature selection algorithm for EEG-based emotion\nrecognition. The proposed method integrates an adaptive dual self-expression\nlearning (ADSEL) with least squares regression. ADSEL establishes a\nbidirectional pathway between sample-level and dimension-level self-expression\nlearning processes within the label space. It could facilitate the\ncross-sharing of learned information between these processes, enabling the\nsimultaneous exploitation of effective information across both samples and\ndimensions for label reconstruction. Consequently, ADSEL could enhances label\nrecovery accuracy and effectively identifies the optimal EEG feature subset for\nmulti-dimensional emotion recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684EEG\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\uff08ADSEL\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u7ef4\u5ea6\u60c5\u7eea\u8bc6\u522b\u4e2d\u6807\u7b7e\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53cc\u81ea\u8868\u8fbe\u5b66\u4e60\u548c\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u63d0\u9ad8\u6807\u7b7e\u6062\u590d\u7cbe\u5ea6\u548c\u7279\u5f81\u9009\u62e9\u6548\u679c\u3002", "motivation": "\u9488\u5bf9EEG\u60c5\u7eea\u8bc6\u522b\u4e2d\u7279\u5f81\u7ef4\u5ea6\u9ad8\u3001\u6837\u672c\u5c11\u5bfc\u81f4\u8fc7\u62df\u5408\u4ee5\u53ca\u6807\u7b7e\u4e0d\u5b8c\u6574\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u6709\u6548\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u3002", "method": "\u63d0\u51faADSEL\u7b97\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u53cc\u81ea\u8868\u8fbe\u5b66\u4e60\u548c\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\uff0c\u5728\u6807\u7b7e\u7a7a\u95f4\u4e2d\u5efa\u7acb\u6837\u672c\u7ea7\u548c\u7ef4\u5ea6\u7ea7\u7684\u53cc\u5411\u4fe1\u606f\u5171\u4eab\u8def\u5f84\uff0c\u7528\u4e8e\u6807\u7b7e\u91cd\u6784\u548c\u7279\u5f81\u9009\u62e9\u3002", "result": "ADSEL\u80fd\u591f\u63d0\u9ad8\u6807\u7b7e\u6062\u590d\u7cbe\u5ea6\uff0c\u5e76\u6709\u6548\u8bc6\u522b\u6700\u4f18EEG\u7279\u5f81\u5b50\u96c6\uff0c\u63d0\u5347\u591a\u7ef4\u5ea6\u60c5\u7eea\u8bc6\u522b\u7684\u6548\u679c\u3002", "conclusion": "ADSEL\u7b97\u6cd5\u901a\u8fc7\u53cc\u81ea\u8868\u8fbe\u5b66\u4e60\u89e3\u51b3\u4e86\u6807\u7b7e\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u60c5\u7eea\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.05231", "pdf": "https://arxiv.org/pdf/2508.05231", "abs": "https://arxiv.org/abs/2508.05231", "authors": ["Wenjia Dong", "Xueyuan Xu", "Tianze Yu", "Junming Zhang", "Li Zhuo"], "title": "FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Electroencephalogram (EEG)-based emotion recognition holds significant value\nin affective computing and brain-computer interfaces. However, in practical\napplications, EEG recordings are susceptible to the effects of various\nphysiological artifacts. Current approaches typically treat denoising and\nemotion recognition as independent tasks using cascaded architectures, which\nnot only leads to error accumulation, but also fails to exploit potential\nsynergies between these tasks. Moreover, conventional EEG-based emotion\nrecognition models often rely on the idealized assumption of \"perfectly\ndenoised data\", lacking a systematic design for noise robustness. To address\nthese challenges, a novel framework that deeply couples denoising and emotion\nrecognition tasks is proposed for end-to-end noise-robust emotion recognition,\ntermed as Feedback-Driven Collaborative Network for Denoising-Classification\nNexus (FDC-Net). Our primary innovation lies in establishing a dynamic\ncollaborative mechanism between artifact removal and emotion recognition\nthrough: (1) bidirectional gradient propagation with joint optimization\nstrategies; (2) a gated attention mechanism integrated with frequency-adaptive\nTransformer using learnable band-position encoding. Two most popular EEG-based\nemotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels\nwere employed to compare the artifact removal and emotion recognition\nperformance between ASLSL and nine state-of-the-art methods. In terms of the\ndenoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of\n96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the\nemotion recognition task under physiological artifact interference, FDC-Net\nachieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on\nDREAMER.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFDC-Net\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u68af\u5ea6\u4f20\u64ad\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6df1\u5ea6\u8026\u5408\u53bb\u566a\u548c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u566a\u58f0\u9c81\u68d2\u60c5\u611f\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5c06\u53bb\u566a\u548c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u5206\u5f00\u5904\u7406\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u672a\u80fd\u5229\u7528\u4efb\u52a1\u95f4\u534f\u540c\u6548\u5e94\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8EEG\u4fe1\u53f7\u5728\u566a\u58f0\u5e72\u6270\u4e0b\u7684\u60c5\u611f\u8bc6\u522b\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u53cc\u5411\u68af\u5ea6\u4f20\u64ad\u548c\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u548c\u9891\u7387\u81ea\u9002\u5e94Transformer\uff0c\u52a8\u6001\u534f\u540c\u53bb\u566a\u548c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u3002", "result": "\u5728DEAP\u548cDREAMER\u6570\u636e\u96c6\u4e0a\uff0c\u53bb\u566a\u4efb\u52a1\u7684\u76f8\u5173\u7cfb\u6570\u5206\u522b\u8fbe\u523096.30%\u548c90.31%\uff0c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a82.3\u00b17.1%\u548c88.1\u00b10.8%\u3002", "conclusion": "FDC-Net\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u4e3aEEG\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05238", "pdf": "https://arxiv.org/pdf/2508.05238", "abs": "https://arxiv.org/abs/2508.05238", "authors": ["Wei Xiang", "Muchen Li", "Jie Yan", "Manling Zheng", "Hanfei Zhu", "Mengyun Jiang", "Lingyun Sun"], "title": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems,\n  Man, and Cybernetics (SMC)", "summary": "Level 3 automated driving systems allows drivers to engage in secondary tasks\nwhile diminishing their perception of risk. In the event of an emergency\nnecessitating driver intervention, the system will alert the driver with a\nlimited window for reaction and imposing a substantial cognitive burden. To\naddress this challenge, this study employs a Large Language Model (LLM) to\nassist drivers in maintaining an appropriate attention on road conditions\nthrough a \"humanized\" persuasive advice. Our tool leverages the road conditions\nencountered by Level 3 systems as triggers, proactively steering driver\nbehavior via both visual and auditory routes. Empirical study indicates that\nour tool is effective in sustaining driver attention with reduced cognitive\nload and coordinating secondary tasks with takeover behavior. Our work provides\ninsights into the potential of using LLMs to support drivers during multi-task\nautomated driving.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728Level 3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u4f9b\u4eba\u6027\u5316\u5efa\u8bae\uff0c\u4ee5\u5e2e\u52a9\u9a7e\u9a76\u5458\u4fdd\u6301\u5bf9\u8def\u51b5\u7684\u5173\u6ce8\uff0c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "Level 3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c\u9a7e\u9a76\u5458\u53ef\u80fd\u56e0\u6267\u884c\u5176\u4ed6\u4efb\u52a1\u800c\u964d\u4f4e\u98ce\u9669\u611f\u77e5\uff0c\u800c\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u9700\u8981\u5feb\u901f\u53cd\u5e94\u65f6\uff0c\u8ba4\u77e5\u8d1f\u62c5\u8f83\u91cd\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u201c\u4eba\u6027\u5316\u201d\u7684\u529d\u5bfc\u5efa\u8bae\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u542c\u89c9\u9014\u5f84\u5f15\u5bfc\u9a7e\u9a76\u5458\u884c\u4e3a\uff0c\u89e6\u53d1\u673a\u5236\u57fa\u4e8e\u8def\u51b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u5de5\u5177\u80fd\u6709\u6548\u7ef4\u6301\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u3001\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u534f\u8c03\u6b21\u8981\u4efb\u52a1\u4e0e\u63a5\u7ba1\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86LLM\u5728\u591a\u4efb\u52a1\u81ea\u52a8\u9a7e\u9a76\u4e2d\u652f\u6301\u9a7e\u9a76\u5458\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.05281", "pdf": "https://arxiv.org/pdf/2508.05281", "abs": "https://arxiv.org/abs/2508.05281", "authors": ["Ahmed Abdal Shafi Rasel", "Ahmed Mustafa Amlan", "Tasmim Shajahan Mim", "Tanvir Hasan"], "title": "A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness", "categories": ["cs.HC"], "comment": "34 pages, Submitted for review", "summary": "This study explores perceptions of fairness in algorithmic decision-making\namong users in Bangladesh through a comprehensive mixed-methods approach. By\nintegrating quantitative survey data with qualitative interview insights, we\nexamine how cultural, social, and contextual factors influence users'\nunderstanding of fairness, transparency, and accountability in AI systems. Our\nfindings reveal nuanced attitudes toward human oversight, explanation\nmechanisms, and contestability, highlighting the importance of culturally aware\ndesign principles for equitable and trustworthy algorithmic systems. These\ninsights contribute to ongoing discussions on algorithmic fairness by\nforegrounding perspectives from a non-Western context, thus broadening the\nglobal dialogue on ethical AI deployment.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5b5f\u52a0\u62c9\u56fd\u7528\u6237\u5bf9\u7b97\u6cd5\u51b3\u7b56\u516c\u5e73\u6027\u7684\u611f\u77e5\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u63ed\u793a\u4e86\u6587\u5316\u3001\u793e\u4f1a\u53ca\u60c5\u5883\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u5176\u5bf9AI\u7cfb\u7edf\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u7684\u7406\u89e3\u3002", "motivation": "\u65e8\u5728\u7406\u89e3\u975e\u897f\u65b9\u6587\u5316\u80cc\u666f\u4e0b\u7528\u6237\u5bf9\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u72ec\u7279\u6001\u5ea6\uff0c\u4e3a\u5168\u7403AI\u4f26\u7406\u5bf9\u8bdd\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9a\u91cf\u8c03\u67e5\u4e0e\u5b9a\u6027\u8bbf\u8c08\u6570\u636e\uff0c\u7efc\u5408\u5206\u6790\u6587\u5316\u548c\u793e\u4f1a\u56e0\u7d20\u5bf9\u516c\u5e73\u6027\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7528\u6237\u5bf9\u4eba\u4e3a\u76d1\u7763\u3001\u89e3\u91ca\u673a\u5236\u548c\u53ef\u4e89\u8bae\u6027\u6709\u590d\u6742\u6001\u5ea6\uff0c\u5f3a\u8c03\u9700\u8003\u8651\u6587\u5316\u80cc\u666f\u7684\u7b97\u6cd5\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u7814\u7a76\u4e30\u5bcc\u4e86\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u5168\u7403\u8ba8\u8bba\uff0c\u7a81\u51fa\u4e86\u975e\u897f\u65b9\u89c6\u89d2\u5728\u6784\u5efa\u516c\u5e73\u53ef\u4fe1AI\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.05325", "pdf": "https://arxiv.org/pdf/2508.05325", "abs": "https://arxiv.org/abs/2508.05325", "authors": ["Jonathan C. Roberts", "Hanan Alnjar", "Aron E. Owen", "Panagiotis D. Ritsos"], "title": "Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs", "categories": ["cs.HC", "H.5.2; K.3.0; D.0; I.3.8"], "comment": "11 pages, 6 pages supplemental material, 2 CDS versions", "summary": "We present the Critical Design Strategy (CDS) - a structured method designed\nto facilitate the examination of visualisation designs through reflection and\ncritical thought. The CDS helps designers think critically and make informed\nimprovements using heuristic evaluation. When developing a visual tool or\npioneering a novel visualisation approach, identifying areas for enhancement\ncan be challenging. Critical thinking is particularly crucial for visualisation\ndesigners and tool developers, especially those new to the field, such as\nstudying visualisation in higher education. The CDS consists of three stages\nacross six perspectives: Stage 1 captures the essence of the idea by assigning\nan indicative title and selecting five adjectives (from twenty options) to form\ninitial impressions of the design. Stage 2 involves an in-depth critique using\n30 heuristic questions spanning six key perspectives - user, environment,\ninterface, components, design, and visual marks. Stage 3 focuses on\nsynthesising insights, reflecting on design decisions, and determining the next\nsteps forward. We introduce the CDS and explore its use across three\nvisualisation modules in both undergraduate and postgraduate courses. Our\nlongstanding experience with the CDS has allowed us to refine and develop it\nover time: from its initial creation through workshops in 2017/18 to\nimprovements in wording and the development of two applications by 2020,\nfollowed by the expansion of support notes and refinement of heuristics through\n2023; while using it in our teaching each year. This sustained use allows us to\nreflect on its practical application and offer guidance on how others can\nincorporate it into their own work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDS\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6279\u5224\u6027\u601d\u7ef4\u6539\u8fdb\u53ef\u89c6\u5316\u8bbe\u8ba1\uff0c\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff0c\u7ed3\u5408\u516d\u4e2a\u89c6\u89d2\uff0c\u5e76\u5728\u6559\u5b66\u4e2d\u6301\u7eed\u4f18\u5316\u3002", "motivation": "\u9488\u5bf9\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e2d\u7684\u6539\u8fdb\u96be\u70b9\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\uff08\u5c24\u5176\u662f\u521d\u5b66\u8005\uff09\u901a\u8fc7\u6279\u5224\u6027\u601d\u7ef4\u548c\u542f\u53d1\u5f0f\u8bc4\u4ef7\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "method": "CDS\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a1)\u6355\u6349\u8bbe\u8ba1\u6838\u5fc3\uff1b2)\u516d\u89c6\u89d2\u542f\u53d1\u5f0f\u63d0\u95ee\uff1b3)\u7efc\u5408\u6d1e\u5bdf\u4e0e\u53cd\u601d\u3002", "result": "\u5728\u672c\u79d1\u548c\u7814\u7a76\u751f\u8bfe\u7a0b\u4e2d\u5e94\u7528CDS\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u4f7f\u7528\u4e0d\u65ad\u4f18\u5316\u5176\u5185\u5bb9\u4e0e\u5de5\u5177\u3002", "conclusion": "CDS\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u7528\u6846\u67b6\uff0c\u9002\u5408\u6559\u80b2\u4e0e\u5b9e\u8df5\u4e2d\u7684\u6279\u5224\u6027\u601d\u7ef4\u8bad\u7ec3\u3002"}}
{"id": "2508.05332", "pdf": "https://arxiv.org/pdf/2508.05332", "abs": "https://arxiv.org/abs/2508.05332", "authors": ["Masanori Ibara", "Yuichi Hiroi", "Takushi Kamegai", "Takefumi Hiraki"], "title": "Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform", "categories": ["cs.HC"], "comment": "10 pages, to appear in IEEE International Symposium on Emerging\n  Metaverse (ISEMV)", "summary": "Traditionally, specialized 3D design data, such as BIM and CAD, have been\naccessible only to a select group of experts, creating significant barriers\nthat prevent general users from participating in decision-making processes.\nThis paper provides a systematic overview of practical insights for utilizing\n3D data in industrial and architectural domains by presenting implementation\ncases of the industrial metaverse on Cluster, a commercial cross-device\nmetaverse platform. This paper analyzes the characteristics and constraints of\nmajor data formats in the industrial and architectural fields and organizes\nintegration workflows for the metaverse. Through application cases utilizing 3D\ndata across multiple domains, we present practical examples of collaborative\ndecision-making support enabled by the fusion of metaverse and digital twin\ntechnologies. Specifically, we demonstrate that multi-device access and\nsimultaneous multi-user participation capabilities foster democratic\nenvironments in the industrial metaverse, which are challenging to achieve with\nconventional, expert-dependent systems.", "AI": {"tldr": "\u4f20\u7edf3D\u8bbe\u8ba1\u6570\u636e\uff08\u5982BIM\u548cCAD\uff09\u4ec5\u9650\u4e8e\u4e13\u5bb6\u4f7f\u7528\uff0c\u9650\u5236\u4e86\u666e\u901a\u7528\u6237\u7684\u53c2\u4e0e\u3002\u672c\u6587\u901a\u8fc7\u5de5\u4e1a\u5143\u5b87\u5b99\u5e73\u53f0Cluster\u7684\u6848\u4f8b\uff0c\u5c55\u793a\u4e863D\u6570\u636e\u5728\u591a\u9886\u57df\u7684\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u534f\u4f5c\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u6253\u7834\u4e13\u5bb6\u5784\u65ad\uff0c\u8ba9\u666e\u901a\u7528\u6237\u53c2\u4e0e3D\u8bbe\u8ba1\u6570\u636e\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u5206\u6790\u5de5\u4e1a\u4e0e\u5efa\u7b51\u9886\u57df\u7684\u4e3b\u8981\u6570\u636e\u683c\u5f0f\u7279\u70b9\uff0c\u63d0\u51fa\u5143\u5b87\u5b99\u96c6\u6210\u5de5\u4f5c\u6d41\uff0c\u5e76\u901a\u8fc7\u591a\u9886\u57df3D\u6570\u636e\u5e94\u7528\u6848\u4f8b\u5c55\u793a\u534f\u4f5c\u51b3\u7b56\u652f\u6301\u3002", "result": "\u591a\u8bbe\u5907\u8bbf\u95ee\u4e0e\u591a\u7528\u6237\u540c\u65f6\u53c2\u4e0e\u80fd\u529b\u5728\u5de5\u4e1a\u5143\u5b87\u5b99\u4e2d\u521b\u9020\u4e86\u6c11\u4e3b\u73af\u5883\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4e13\u5bb6\u4f9d\u8d56\u7cfb\u7edf\u7684\u9650\u5236\u3002", "conclusion": "3D\u6570\u636e\u4e0e\u5143\u5b87\u5b99\u6280\u672f\u7684\u7ed3\u5408\u4e3a\u534f\u4f5c\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05497", "pdf": "https://arxiv.org/pdf/2508.05497", "abs": "https://arxiv.org/abs/2508.05497", "authors": ["Federico Scar\u00ec", "Olger Siebinga", "Arkady Zgonnikov"], "title": "Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "As automated vehicles (AVs) increasingly integrate into mixed-traffic\nenvironments, evaluating their interaction with human-driven vehicles (HDVs)\nbecomes critical. In most research focused on developing new AV control\nalgorithms (controllers), the performance of these algorithms is assessed\nsolely based on performance metrics such as collision avoidance or lane-keeping\nefficiency, while largely overlooking the human-centred dimensions of\ninteraction with HDVs. This paper proposes a structured evaluation framework\nthat addresses this gap by incorporating metrics grounded in the human-robot\ninteraction literature. The framework spans four key domains: a) interaction\neffect, b) interaction perception, c) interaction effort, and d) interaction\nability. These domains capture both the performance of the AV and its impact on\nhuman drivers around it. To demonstrate the utility of the framework, we apply\nit to a case study evaluating how a state-of-the-art AV controller interacts\nwith human drivers in a merging scenario in a driving simulator. Measuring\nHDV-HDV interactions as a baseline, this study included one representative\nmetric per domain: a) perceived safety, b) subjective ratings, specifically how\nparticipants perceived the other vehicle's driving behaviour (e.g.,\naggressiveness or predictability) , c) driver workload, and d) merging success.\nThe results showed that incorporating metrics covering all four domains in the\nevaluation of AV controllers can illuminate critical differences in driver\nexperience when interacting with AVs. This highlights the need for a more\ncomprehensive evaluation approach. Our framework offers researchers,\ndevelopers, and policymakers a systematic method for assessing AV behaviour\nbeyond technical performance, fostering the development of AVs that are not\nonly functionally capable but also understandable, acceptable, and safe from a\nhuman perspective.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HDV\uff09\u7684\u4ea4\u4e92\uff0c\u6db5\u76d6\u56db\u4e2a\u5173\u952e\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8AV\u63a7\u5236\u7b97\u6cd5\u7684\u6280\u672f\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u4ea4\u4e92\u4f53\u9a8c\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4ea4\u4e92\u6548\u679c\u3001\u4ea4\u4e92\u611f\u77e5\u3001\u4ea4\u4e92\u52aa\u529b\u548c\u4ea4\u4e92\u80fd\u529b\u7684\u56db\u57df\u6846\u67b6\uff0c\u5e76\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u5e94\u7528\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86AV\u4e0eHDV\u4ea4\u4e92\u4e2d\u9a7e\u9a76\u5458\u4f53\u9a8c\u7684\u5173\u952e\u5dee\u5f02\uff0c\u5f3a\u8c03\u9700\u7efc\u5408\u8bc4\u4f30\u6280\u672f\u4e0e\u4eba\u56e0\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30AV\u884c\u4e3a\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u52a9\u529b\u5f00\u53d1\u66f4\u4eba\u6027\u5316\u3001\u5b89\u5168\u7684\u6280\u672f\u3002"}}
{"id": "2508.05572", "pdf": "https://arxiv.org/pdf/2508.05572", "abs": "https://arxiv.org/abs/2508.05572", "authors": ["Yifan Wang", "Hongfeng Ai", "Ruiqi Li", "Maowei Jiang", "Ruiyuan Kang", "Jiahua Dong", "Cheng Jiang", "Chenzhong Li"], "title": "Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis", "categories": ["cs.HC"], "comment": "10 pages", "summary": "In medical time series disease diagnosis, two key challenges are identified.\nFirst, the high annotation cost of medical data leads to overfitting in models\ntrained on label-limited, single-center datasets. To address this, we propose\nincorporating external data from related tasks and leveraging AE-GAN to extract\nprior knowledge, providing valuable references for downstream tasks. Second,\nmany existing studies employ contrastive learning to derive more generalized\nmedical sequence representations for diagnostic tasks, usually relying on\nmanually designed diverse positive and negative sample pairs. However, these\napproaches are complex, lack generalizability, and fail to adaptively capture\ndisease-specific features across different conditions. To overcome this, we\nintroduce LMCF (Learnable Multi-views Contrastive Framework), a framework that\nintegrates a multi-head attention mechanism and adaptively learns\nrepresentations from different views through inter-view and intra-view\ncontrastive learning strategies. Additionally, the pre-trained AE-GAN is used\nto reconstruct discrepancies in the target data as disease probabilities, which\nare then integrated into the contrastive learning process. Experiments on three\ntarget datasets demonstrate that our method consistently outperforms other\nseven baselines, highlighting its significant impact on healthcare applications\nsuch as the diagnosis of myocardial infarction, Alzheimer's disease, and\nParkinson's disease. We release the source code at xxxxx.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u4e24\u5927\u6311\u6218\u2014\u2014\u9ad8\u6807\u6ce8\u6210\u672c\u4e0e\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408AE-GAN\u548cLMCF\u6846\u67b6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u590d\u6742\u4e14\u7f3a\u4e4f\u666e\u9002\u6027\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u7ed3\u5408AE-GAN\u63d0\u53d6\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165LMCF\u6846\u67b6\u901a\u8fc7\u591a\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u81ea\u9002\u5e94\u5b66\u4e60\u75be\u75c5\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4e03\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u808c\u6897\u6b7b\u3001\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u548c\u5e15\u91d1\u68ee\u75c5\u7684\u8bca\u65ad\u6548\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.05025", "pdf": "https://arxiv.org/pdf/2508.05025", "abs": "https://arxiv.org/abs/2508.05025", "authors": ["Zhehan Qu", "Tianyi Hu", "Christian Fronk", "Maria Gorlatova"], "title": "Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Augmented Reality (AR) systems, while enhancing task performance through\nreal-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on\nvirtual content that compromises situational awareness (SA) in safety-critical\nscenarios. This paper investigates SA in AR-guided cardiopulmonary\nresuscitation (CPR), where responders must balance effective compressions with\nvigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR\napp on a Magic Leap 2 that overlays real-time CPR feedback (compression depth\nand rate) and conducted a user study with simulated unexpected incidents (e.g.,\nbleeding) to evaluate SA, in which SA metrics were collected via observation\nand questionnaires administered during freeze-probe events. Eye tracking\nanalysis revealed that higher SA levels were associated with greater saccadic\namplitude and velocity, and with reduced proportion and frequency of fixations\non virtual content. To predict SA, we propose FixGraphPool, a graph neural\nnetwork that structures gaze events (fixations, saccades) into spatiotemporal\ngraphs, effectively capturing dynamic attentional patterns. Our model achieved\n83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and\nstate-of-the-art time-series models by leveraging domain knowledge and\nspatial-temporal information encoded in ET data. These findings demonstrate the\npotential of eye tracking for SA modeling in AR and highlight its utility in\ndesigning AR systems that ensure user safety and situational awareness.", "AI": {"tldr": "AR\u7cfb\u7edf\u5728\u589e\u5f3a\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u53ef\u80fd\u5f15\u53d1\u8ba4\u77e5\u96a7\u9053\u6548\u5e94\uff0c\u964d\u4f4e\u60c5\u5883\u610f\u8bc6\u3002\u8be5\u7814\u7a76\u901a\u8fc7AR\u5f15\u5bfc\u7684\u5fc3\u80ba\u590d\u82cf\u5b9e\u9a8c\u8bc4\u4f30\u60c5\u5883\u610f\u8bc6\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u773c\u52a8\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9884\u6d4b\u60c5\u5883\u610f\u8bc6\u3002", "motivation": "\u7814\u7a76AR\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u53ef\u80fd\u5f15\u53d1\u7684\u8ba4\u77e5\u96a7\u9053\u6548\u5e94\u53ca\u5176\u5bf9\u60c5\u5883\u610f\u8bc6\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u5fc3\u80ba\u590d\u82cf\u7b49\u9700\u8981\u540c\u65f6\u5173\u6ce8\u591a\u9879\u4efb\u52a1\u7684\u573a\u666f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3eAR\u5e94\u7528\uff0c\u63d0\u4f9b\u5b9e\u65f6\u5fc3\u80ba\u590d\u82cf\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u6a21\u62df\u610f\u5916\u4e8b\u4ef6\uff0c\u6536\u96c6\u773c\u52a8\u6570\u636e\u4ee5\u5206\u6790\u60c5\u5883\u610f\u8bc6\u3002\u63d0\u51fa\u4e86FixGraphPool\u6a21\u578b\uff0c\u5229\u7528\u773c\u52a8\u6570\u636e\u7684\u65f6\u7a7a\u4fe1\u606f\u9884\u6d4b\u60c5\u5883\u610f\u8bc6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u60c5\u5883\u610f\u8bc6\u4e0e\u66f4\u5927\u7684\u773c\u8df3\u5e45\u5ea6\u548c\u901f\u5ea6\u76f8\u5173\uff0c\u51cf\u5c11\u5bf9\u865a\u62df\u5185\u5bb9\u7684\u6ce8\u89c6\u3002FixGraphPool\u6a21\u578b\u5728\u9884\u6d4b\u60c5\u5883\u610f\u8bc6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u738783.0%\uff09\u3002", "conclusion": "\u773c\u52a8\u6570\u636e\u53ef\u7528\u4e8e\u5efa\u6a21AR\u4e2d\u7684\u60c5\u5883\u610f\u8bc6\uff0c\u4e3a\u8bbe\u8ba1\u517c\u987e\u5b89\u5168\u6027\u548c\u60c5\u5883\u610f\u8bc6\u7684AR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.05310", "pdf": "https://arxiv.org/pdf/2508.05310", "abs": "https://arxiv.org/abs/2508.05310", "authors": ["Jelle Luijkx", "Zlatan Ajanovi\u0107", "Laura Ferranti", "Jens Kober"], "title": "ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.RO", "68T05", "I.2.6; I.2.8; I.2.9"], "comment": "Accepted for publication in Transactions on Machine Learning Research\n  (TMLR, 2025)", "summary": "Human teaching effort is a significant bottleneck for the broader\napplicability of interactive imitation learning. To reduce the number of\nrequired queries, existing methods employ active learning to query the human\nteacher only in uncertain, risky, or novel situations. However, during these\nqueries, the novice's planned actions are not utilized despite containing\nvaluable information, such as the novice's capabilities, as well as\ncorresponding uncertainty levels. To this end, we allow the novice to say: \"I\nplan to do this, but I am uncertain.\" We introduce the Active Skill-level Data\nAggregation (ASkDAgger) framework, which leverages teacher feedback on the\nnovice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating\nthreshold to track sensitivity, specificity, or a minimum success rate; (2)\nForesight Interactive Experience Replay (FIER), which recasts valid and\nrelabeled novice action plans into demonstrations; and (3) Prioritized\nInteractive Experience Replay (PIER), which prioritizes replay based on\nuncertainty, novice success, and demonstration age. Together, these components\nbalance query frequency with failure incidence, reduce the number of required\ndemonstration annotations, improve generalization, and speed up adaptation to\nchanging domains. We validate the effectiveness of ASkDAgger through\nlanguage-conditioned manipulation tasks in both simulation and real-world\nenvironments. Code, data, and videos are available at\nhttps://askdagger.github.io.", "AI": {"tldr": "ASkDAgger\u6846\u67b6\u901a\u8fc7\u5229\u7528\u65b0\u624b\u8ba1\u5212\u7684\u53cd\u9988\u4fe1\u606f\uff0c\u4f18\u5316\u67e5\u8be2\u9891\u7387\u4e0e\u5931\u8d25\u7387\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u51cf\u5c11\u4eba\u5de5\u6559\u5b66\u6210\u672c\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51cf\u5c11\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u4eba\u529b\u6559\u5b66\u6210\u672c\uff0c\u5229\u7528\u65b0\u624b\u8ba1\u5212\u4e2d\u7684\u4fe1\u606f\uff08\u5982\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\uff09\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51faASkDAgger\u6846\u67b6\uff0c\u5305\u542bS-Aware Gating\u3001Foresight Interactive Experience Replay\u548cPrioritized Interactive Experience Replay\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u4f18\u5316\u67e5\u8be2\u7b56\u7565\u548c\u793a\u8303\u6570\u636e\u5229\u7528\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u7684\u8bed\u8a00\u6761\u4ef6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86ASkDAgger\u7684\u6709\u6548\u6027\uff0c\u964d\u4f4e\u4e86\u793a\u8303\u6807\u6ce8\u9700\u6c42\u5e76\u63d0\u5347\u4e86\u9002\u5e94\u6027\u3002", "conclusion": "ASkDAgger\u901a\u8fc7\u5229\u7528\u65b0\u624b\u8ba1\u5212\u7684\u4fe1\u606f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u5e76\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.05358", "pdf": "https://arxiv.org/pdf/2508.05358", "abs": "https://arxiv.org/abs/2508.05358", "authors": ["Fenya Wasserroth", "Eleftherios Avramidis", "Vera Czehmann", "Tanja Kojic", "Fabrizio Nunnari", "Sebastian M\u00f6ller"], "title": "Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \\& Acceptability", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper presents an investigation into the impact of adding adjustment\nfeatures to an existing sign language (SL) avatar on a Microsoft Hololens 2\ndevice. Through a detailed analysis of interactions of expert German Sign\nLanguage (DGS) users with both adjustable and non-adjustable avatars in a\nspecific use case, this study identifies the key factors influencing the\ncomprehensibility, the user experience (UX), and the acceptability of such a\nsystem. Despite user preference for adjustable settings, no significant\nimprovements in UX or comprehensibility were observed, which remained at low\nlevels, amid missing SL elements (mouthings and facial expressions) and\nimplementation issues (indistinct hand shapes, lack of feedback and menu\npositioning). Hedonic quality was rated higher than pragmatic quality,\nindicating that users found the system more emotionally or aesthetically\npleasing than functionally useful. Stress levels were higher for the adjustable\navatar, reflecting lower performance, greater effort and more frustration.\nAdditionally, concerns were raised about whether the Hololens adjustment\ngestures are intuitive and easy to familiarise oneself with. While\nacceptability of the concept of adjustability was generally positive, it was\nstrongly dependent on usability and animation quality. This study highlights\nthat personalisation alone is insufficient, and that SL avatars must be\ncomprehensible by default. Key recommendations include enhancing mouthing and\nfacial animation, improving interaction interfaces, and applying participatory\ndesign.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u624b\u8bed\u865a\u62df\u89d2\u8272\u4e2d\u589e\u52a0\u8c03\u8282\u529f\u80fd\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u53ef\u7406\u89e3\u6027\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u5c3d\u7ba1\u7528\u6237\u504f\u597d\u53ef\u8c03\u8282\u8bbe\u7f6e\uff0c\u4f46\u672a\u663e\u8457\u63d0\u5347\u4f53\u9a8c\u6216\u7406\u89e3\u3002", "motivation": "\u63a2\u7d22\u53ef\u8c03\u8282\u624b\u8bed\u865a\u62df\u89d2\u8272\u5728Microsoft Hololens 2\u8bbe\u5907\u4e0a\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u53ef\u7406\u89e3\u6027\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u7528\u6237\u4ea4\u4e92\u5206\u6790\uff0c\u6bd4\u8f83\u53ef\u8c03\u8282\u4e0e\u4e0d\u53ef\u8c03\u8282\u865a\u62df\u89d2\u8272\u7684\u8868\u73b0\u3002", "result": "\u53ef\u8c03\u8282\u8bbe\u7f6e\u672a\u663e\u8457\u6539\u5584\u7528\u6237\u4f53\u9a8c\u6216\u7406\u89e3\u6027\uff0c\u4f46\u60c5\u611f\u8d28\u91cf\u8bc4\u5206\u8f83\u9ad8\u3002", "conclusion": "\u4e2a\u6027\u5316\u4e0d\u8db3\uff0c\u9700\u4f18\u5148\u63d0\u5347\u9ed8\u8ba4\u53ef\u7406\u89e3\u6027\u548c\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u6539\u8fdb\u53e3\u578b\u548c\u8868\u60c5\u52a8\u753b\u3002"}}
{"id": "2508.05535", "pdf": "https://arxiv.org/pdf/2508.05535", "abs": "https://arxiv.org/abs/2508.05535", "authors": ["Albert Yu", "Chengshu Li", "Luca Macesanu", "Arnav Balaji", "Ruchira Ray", "Raymond Mooney", "Roberto Mart\u00edn-Mart\u00edn"], "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation", "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG", "cs.MA", "I.2.9; I.2.7; I.2.6"], "comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/", "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.", "AI": {"tldr": "MICoBot\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u51b3\u7b56\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u4eba\u7c7b\u4f19\u4f34\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u52a8\u6001\u53d8\u5316\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u7684\u6c9f\u901a\u548c\u51b3\u7b56\u7cfb\u7edf\u3002", "method": "MICoBot\u91c7\u7528\u6df7\u5408\u4e3b\u52a8\u5bf9\u8bdd\u8303\u5f0f\uff0c\u901a\u8fc7\u5143\u89c4\u5212\u5668\u3001\u4efb\u52a1\u89c4\u5212\u5668\u548c\u52a8\u4f5c\u6267\u884c\u5668\u4e09\u5c42\u6b21\u51b3\u7b56\uff0c\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMICoBot\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u7528\u6237\u4f53\u9a8c\u4e0a\u663e\u8457\u4f18\u4e8e\u7eafLLM\u57fa\u7ebf\u548c\u5176\u4ed6\u4efb\u52a1\u5206\u914d\u6a21\u578b\u3002", "conclusion": "MICoBot\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u7075\u6d3b\u51b3\u7b56\uff0c\u80fd\u591f\u6709\u6548\u9002\u5e94\u591a\u6837\u5316\u7684\u4eba\u7c7b\u4f19\u4f34\uff0c\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002"}}
