<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 17]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [VTS-Guided AI Interaction Workflow for Business Insights](https://arxiv.org/abs/2507.00347)
*Sun Ding,Ude Enebeli,Atilhan,Manay,Ryan Pua,Kamal Kotak*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern firms face a flood of dense, unstructured reports. Turning these
documents into usable insights takes heavy effort and is far from agile when
quick answers are needed. VTS-AI tackles this gap. It integrates Visual
Thinking Strategies, which emphasize evidence-based observation, linking, and
thinking, into AI agents, so the agents can extract business insights from
unstructured text, tables, and images at scale. The system works in three tiers
(micro, meso, macro). It tags issues, links them to source pages, and rolls
them into clear action levers stored in a searchable YAML file. In tests on an
18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt
yet produced richer findings: page locations, verbatim excerpts, severity
scores, and causal links. Analysts can accept or adjust these outputs in the
same IDE, keeping human judgment in the loop. Early results show VTS-AI spots
the direction of key metrics and flags where deeper number-crunching is needed.
Next steps include mapping narrative tags to financial ratios, adding
finance-tuned language models through a Model-Context Protocol, and building a
Risk & Safety Layer to stress-test models and secure data. These upgrades aim
to make VTS-AI a production-ready, audit-friendly tool for rapid business
analysis.

</details>


### [2] [An AST-guided LLM Approach for SVRF Code Synthesis](https://arxiv.org/abs/2507.00352)
*Abanoub E. Abdelmalak,Mohamed A. Elsayed,David Abercrombie,Ilhami Torunoglu*

Main category: cs.SE

TL;DR: 本文提出了一种结合AST嵌入和RAG的新方法，用于优化SVRF代码合成，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 先进的半导体节点导致复杂的设计规则，传统SVRF开发方法效率低下，亟需新方法填补技术空白。

Method: 采用AST嵌入进行结构验证，结合RAG注入领域知识，提出SVRF专用评分框架。

Result: 测试740条DRC规则显示，代码生成准确性提高40%，显著提升效率。

Conclusion: 结合行业知识和先进编码策略，不仅优化了SVRF开发，还提升了整体生产效率。

Abstract: Standard Verification Rule Format (SVRF) is essential for semiconductor
applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and
Optical Proximity Correction (OPC) and it faces challenges as advancing nodes
create complex design rules that renders traditional SVRF development
ineffective and highlight an expertise gap. This paper introduces a novel
methodology integrating Abstract Syntax Tree (AST) embedding and
Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring
semantic accuracy and error minimization through structural validation with
domain-specific insights for precise code generation.
  We evaluate different T5-based models and propose an innovative SVRF-specific
scoring framework that complements standard metrics like BLEU and ROUGE-L. In
our approach, AST provides rigorous structural validation, while RAG infuses
relevant domain knowledge, effectively enhancing the code generation workflow.
  Testing on a comprehensive benchmark of 740 DRC rule implementations, our
methodology demonstrates up to a 40\% improvement in code generation accuracy
compared to basic text-based fine-tuning process. This fusion of industry
expertise with advanced coding strategies not only optimizes SVRF development
under limited dataset constraints but also creates a more intuitive and
efficient coding environment. Consequently, users can rapidly iterate through
design cycles, reduce manual error correction, and significantly improve
overall productivity.

</details>


### [3] [iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing](https://arxiv.org/abs/2507.00378)
*Xikai Sun,Fan Dang,Kebin Liu,Xin Miao,Zihao Yang,Haimo Lu,Yawen Zheng,Yunhao Liu*

Main category: cs.SE

TL;DR: iPanda是一个利用大型语言模型（LLM）自动生成协议一致性测试用例和代码的端到端框架，显著提升了测试代码生成的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统协议一致性测试需要人工创建大量测试用例和脚本，效率低下且费时，而LLM的文本理解和代码生成能力为自动化提供了可能。

Method: iPanda结合关键字方法和检索增强生成技术，自动生成测试用例和代码，并通过迭代自校正机制优化代码质量。

Result: 实验显示，iPanda比纯LLM方法在测试代码生成成功率上提升了4.675到10.751倍。

Conclusion: iPanda通过自动化协议一致性测试，显著提高了效率和准确性，为测试技术领域提供了新的解决方案。

Abstract: Conformance testing is essential for ensuring that protocol implementations
comply with their specifications. However, traditional testing approaches
involve manually creating numerous test cases and scripts, making the process
labor-intensive and inefficient. Recently, Large Language Models (LLMs) have
demonstrated impressive text comprehension and code generation abilities,
providing promising opportunities for automation. In this paper, we propose
iPanda, the first end-to-end framework that leverages LLMs to automate protocol
conformance testing. Given a protocol specification document and its
implementation, iPanda first employs a keyword-based method to automatically
generate comprehensive test cases. Then, it utilizes a code-based
retrieval-augmented generation approach to effectively interpret the
implementation and produce executable test code. To further enhance code
quality, iPanda incorporates an iterative self-correction mechanism to refine
generated test scripts interactively. Finally, by executing and analyzing the
generated tests, iPanda systematically verifies compliance between
implementations and protocol specifications. Comprehensive experiments on
various protocols show that iPanda significantly outperforms pure LLM-based
approaches, improving the success rate (Pass@1) of test-code generation by
factors ranging from 4.675 times to 10.751 times.

</details>


### [4] [Recommending Variable Names for Extract Local Variable Refactorings](https://arxiv.org/abs/2507.00413)
*Taiming Wang,Hui Liu,Yuxia Zhang,Yanjie Jiang*

Main category: cs.SE

TL;DR: VarNamer是一种自动化方法，用于为提取局部变量重构推荐变量名，显著提升准确性并减少开发者的编辑负担。


<details>
  <summary>Details</summary>
Motivation: 发现现有IDE推荐的变量名与开发者手动构造的变量名70%不一致，增加了开发者的重命名负担。

Method: 通过程序静态分析和数据挖掘技术开发启发式规则，推荐变量名。

Result: VarNamer在精确匹配率上比Eclipse和IntelliJ分别提升52.6%和40.7%，并可减少49.3%的编辑量。

Conclusion: VarNamer在Java和C++中表现良好，具有普适性，显著提升开发效率。

Abstract: Extract local variable is one of the most popular refactorings, and most IDEs
and refactoring tools provide automated support for this refactoring. However,
we find approximately 70% of the names recommended by these IDEs are different
from what developers manually constructed, adding additional renaming burdens
to developers and providing limited assistance. In this paper, we introduce
VarNamer, an automated approach designed to recommend variable names for
extract local variable refactorings. Through a large-scale empirical study, we
identify key contexts that are useful for composing variable names. Leveraging
these insights, we developed a set of heuristic rules through program static
analysis techniques and employ data mining techniques to recommend variable
names effectively. Notably, some of our heuristic rules have been successfully
integrated into Eclipse, where they are now distributed with the latest
releases of the IDE. Evaluation demonstrates its superiority over
state-of-the-art IDEs. Specifically, VarNamer significantly increases the
chance of exact match by 52.6% compared to Eclipse and 40.7% compared to
IntelliJ IDEA. We also evaluated the proposed approach with real-world extract
local variable refactorings conducted in C++ projects, and the results suggest
that the approach can achieve comparable performance on programming languages
besides Java. It may suggest the generalizability of VarNamer. Finally, we
designed and conducted a user study and the results of the user study suggest
that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits
on the recommended variable names.

</details>


### [5] [Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development](https://arxiv.org/abs/2507.00421)
*Parthiv Katapara,Anand Sharma*

Main category: cs.SE

TL;DR: DevOps在嵌入式系统和固件开发中的应用正在兴起，以应对现代硬件-软件协同设计产品的复杂性。本文通过20个学术和工业来源的文献综述，探讨了如何将DevOps原则（如持续集成、持续交付和自动化测试）适应嵌入式环境，并总结了工具、测试策略、管道自动化和安全实践的分类，提出了未来研究路线。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统引入了硬件依赖、实时约束和安全关键要求等挑战，而DevOps的成功应用在这些领域中尚未得到充分探索。本文旨在填补这一空白，为研究者和从业者提供关于嵌入式DevOps的全面理解。

Method: 通过文献综述，分析了20个学术和工业来源，探讨了DevOps原则在嵌入式系统中的适应情况，重点关注工具、测试策略、管道自动化和安全实践。

Result: 总结了嵌入式DevOps当前的努力和限制，特别是在部署工作流和可观察性方面的不足，为未来研究提供了路线图。

Conclusion: 本文为嵌入式DevOps领域提供了结构化的视角，整合了分散的文献，并为未来的研究和实践指明了方向。

Abstract: The adoption of DevOps practices in embedded systems and firmware development
is emerging as a response to the growing complexity of modern
hardware--software co-designed products. Unlike cloud-native applications,
embedded systems introduce challenges such as hardware dependency, real-time
constraints, and safety-critical requirements. This literature review
synthesizes findings from 20 academic and industrial sources to examine how
DevOps principles--particularly continuous integration, continuous delivery,
and automated testing--are adapted to embedded contexts. We categorize efforts
across tooling, testing strategies, pipeline automation, and security
practices. The review highlights current limitations in deployment workflows
and observability, proposing a roadmap for future research. This work offers
researchers and practitioners a consolidated understanding of Embedded DevOps,
bridging fragmented literature with a structured perspective.

</details>


### [6] [The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach](https://arxiv.org/abs/2507.00481)
*Philipp M. Zähl,Sabine Theis,Martin R. Wolf*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Although software engineering research has focused on optimizing processes
and technology, there is a growing recognition that human factors, particularly
teamwork, also significantly impact optimization. Recent research suggests that
developer personality has a strong influence on teamwork. In fact, personality
considerations may have a greater impact on software development than processes
and tools. This paper aims to design a study that measures the impact of HEXACO
personality traits on the Teamwork Quality (TWQ) of software teams. A
preliminary data collection (n=54) was conducted for this purpose. The analysis
showed that several personality traits, as well as their composition, had a
significant impact on TWQ. Additionally, other variables, such as the
proportion of women and age distribution, also affected TWQ. The study's
initial results demonstrate the usefulness and validity of the study design.
The results also suggest several opportunities to improve teamwork in IT
organizations and avenues for further research.

</details>


### [7] [Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey](https://arxiv.org/abs/2507.00496)
*Hongjing Guo,Chuanqi Tao,Zhiqiu Huang,Weiqin Zou*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Deep Learning (DL) models are increasingly applied in safety-critical
domains, ensuring their quality has emerged as a pressing challenge in modern
software engineering. Among emerging validation paradigms, coverage-guided
testing (CGT) has gained prominence as a systematic framework for identifying
erroneous or unexpected model behaviors. Despite growing research attention,
existing CGT studies remain methodologically fragmented, limiting the
understanding of current advances and emerging trends. This work addresses that
gap through a comprehensive review of state-of-the-art CGT methods for DL
models, including test coverage analysis, coverage-guided test input
generation, and coverage-guided test input optimization. This work provides
detailed taxonomies to organize these methods based on methodological
characteristics and application scenarios. We also investigate evaluation
practices adopted in existing studies, including the use of benchmark datasets,
model architectures, and evaluation aspects. Finally, open challenges and
future directions are highlighted in terms of the correlation between
structural coverage and testing objectives, method generalizability across
tasks and models, practical deployment concerns, and the need for standardized
evaluation and tool support. This work aims to provide a roadmap for future
academic research and engineering practice in DL model quality assurance.

</details>


### [8] [A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT](https://arxiv.org/abs/2507.00686)
*Ronny Seiger,Daniel Locher,Marco Kaufmann,Aaron F. Kurz*

Main category: cs.SE

TL;DR: 该论文提出了一种名为Radiant的领域特定语言（DSL），用于将物联网（IoT）传感器数据抽象为业务流程级别的事件，以便进行在线事件检测和分析。


<details>
  <summary>Details</summary>
Motivation: 物联网系统产生大量细粒度传感器数据，但这些数据难以直接用于分析更高层次的业务流程。需要一种方法将低层数据抽象为业务活动事件。

Method: 开发了领域特定语言Radiant，支持用户从传感器数据中识别业务流程活动的模式，并通过复杂事件处理（CEP）应用实时检测活动执行。

Result: 在智能制造和智能医疗领域的实验中，该方法成功监测了业务流程活动，并为领域专家提供了改进质量的反馈。

Conclusion: Radiant和CEP架构结合可以有效将物联网传感器数据抽象为业务流程事件，支持领域专家进行实时分析和改进。

Abstract: Modern Internet of Things (IoT) systems are equipped with a plethora of
sensors providing real-time data about the current operations of their
components, which is crucial for the systems' internal control systems and
processes. However, these data are often too fine-grained to derive useful
insights into the execution of the larger processes an IoT system might be part
of. Process mining has developed advanced approaches for the analysis of
business processes that may also be used in the context of IoT. Bringing
process mining to IoT requires an event abstraction step to lift the low-level
sensor data to the business process level. In this work, we aim to empower
domain experts to perform this step using a newly developed domain-specific
language (DSL) called Radiant. Radiant supports the specification of patterns
within the sensor data that indicate the execution of higher level process
activities. These patterns are translated to complex event processing (CEP)
applications to be used for detecting activity executions at runtime. We
propose a corresponding software architecture for online event abstraction from
IoT sensor streams using the CEP applications. We evaluate these applications
to monitor activity executions using IoT sensors in smart manufacturing and
smart healthcare. The evaluation method and results inform the domain expert
about the quality of activity detections and potential for improvement.

</details>


### [9] [A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback](https://arxiv.org/abs/2507.00699)
*Guoliang Duan,Mingwei Liu,Yanlin Wang,Chong Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: MultiCodeIF是一个新基准，用于评估大语言模型在代码生成中对多维度指令的遵循能力，包括约束类型、层次和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注功能正确性，忽略了现实开发中的复杂需求，因此需要更全面的评估框架。

Method: 通过自动化管道ConstraGen，生成2021个多语言代码任务，支持多轮反馈驱动的评估。

Result: 最佳模型Claude-3-7-Sonnet达到63%的约束满足率，但模型在隐式或多层次约束下表现不佳。反馈可显著提升性能。

Conclusion: MultiCodeIF为真实代码生成场景提供了可扩展的评估框架，缩小了合成评估与现实指令复杂性之间的差距。

Abstract: Large language models (LLMs) have advanced significantly in code generation,
yet their ability to follow complex programming instructions with layered and
diverse constraints remains underexplored. Existing benchmarks often prioritize
functional correctness, overlooking the nuanced requirements found in
real-world development. We introduce MultiCodeIF, a comprehensive benchmark
designed to evaluate instruction-following in code generation across multiple
dimensions: constraint type, hierarchical levels, and iterative refinement.
Built upon a structured taxonomy of 9 categories and 27 constraint types,
MultiCodeIF enables granular assessment of both functional and non-functional
instruction adherence. Using an automated pipeline, ConstraGen, we synthesize
and evolve 2,021 code tasks sourced from 14 programming languages, supporting
multi-turn evaluation through feedback-driven task variants. Empirical
evaluation of six state-of-the-art LLMs uncovers substantial performance
disparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%
average constraint satisfaction, while smaller models like Qwen3-1.7B fall to
44.8%. Models perform well on explicit constraints, but struggle with implicit
or abstract constraints. Tasks with multiple hierarchical constraints
significantly reduce model success rates, from 54.5% in single-level to just
18.8% in multi-level scenarios. However, structured feedback enables
progressive improvement: average constraint satisfaction rises from 63.0% to
83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,
constraint-aware, and feedback-sensitive framework to benchmark LLMs under
realistic code generation scenarios, bridging the gap between synthetic
evaluations and real-world instruction complexity. The full benchmark dataset,
evaluation pipeline, and source code are available at
https://github.com/SYSUSELab/MultiCodeIF.

</details>


### [10] [Snaps: Bloated and Outdated?](https://arxiv.org/abs/2507.00786)
*Jukka Ruohonen,Qusai Ramadan*

Main category: cs.SE

TL;DR: 本文分析了Snap软件打包系统的优缺点，指出其平均包体积过大且更新频率较低。


<details>
  <summary>Details</summary>
Motivation: 研究Snap打包系统在软件分发中的实际表现，验证用户对其体积和更新频率的批评是否合理。

Method: 通过对当前分发中的Snap包进行实证分析，比较其大小和更新频率。

Result: Snap包平均体积过大且更新频率较低，证实了用户的批评。

Conclusion: Snap在软件分发中虽提供互操作性，但在包大小和更新频率上需改进。

Abstract: Snap is an alternative software packaging system developed by Canonical and
provided by default in the Ubuntu Linux distribution. Given the heterogeneity
of various Linux distributions and their various releases, Snap allows an
interoperable delivery of software directly to users. However, concerns and
criticism have also been frequently expressed. Regarding this criticism, the
paper shows that currently distributed snap packages are indeed on average
bloated in terms of their sizes and outdated in terms updating frequencies.
With these empirical observations, this short paper contributes to the research
domain of software packaging, software packages, and package managers.

</details>


### [11] [Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability](https://arxiv.org/abs/2507.00788)
*Markus Borg,Dave Hewett,Nadim Hagatulah,Noric Couderc,Emma Söderberg,Donald Graham,Uttam Kini,Dave Farley*

Main category: cs.SE

TL;DR: 研究了AI助手对软件可维护性的影响，发现其对开发效率有显著提升，但对代码可维护性影响较小。


<details>
  <summary>Details</summary>
Motivation: 探究AI助手（如GitHub Copilot）是否会影响软件的可维护性，尤其是其他开发者对代码的后续修改能力。

Method: 通过两阶段对照实验，151名（95%为专业开发者）参与者分别在有或无AI帮助下开发功能，随后评估代码的可维护性和开发效率。

Result: AI辅助开发显著提升开发效率（减少30.7%任务时间），但对代码可维护性影响较小，仅在习惯性使用者中代码健康度显著提高。

Conclusion: AI助手能有效加速开发，且未显著降低代码可维护性，建议未来关注代码膨胀和认知债务等潜在风险。

Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming
software engineering. While several studies highlight productivity
improvements, their impact on maintainability requires further investigation.
[Objective] This study investigates whether co-development with AI assistants
affects software maintainability, specifically how easily other developers can
evolve the resulting source code. [Method] We conducted a two-phase controlled
experiment involving 151 participants, 95% of whom were professional
developers. In Phase 1, participants added a new feature to a Java web
application, with or without AI assistance. In Phase 2, a randomized controlled
trial, new participants evolved these solutions without AI assistance.
[Results] AI-assisted development in Phase 1 led to a modest speedup in
subsequent evolution and slightly higher average CodeHealth. Although neither
difference was significant overall, the increase in CodeHealth was
statistically significant when habitual AI users completed Phase 1. For Phase
1, we also observed a significant effect that corroborates previous
productivity findings: using an AI assistant yielded a 30.7% median decrease in
task completion time. Moreover, for habitual AI users, the mean speedup was
55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants
can effectively accelerate development. Moreover, we did not observe warning
signs of degraded code-level maintainability. We recommend that future research
focus on risks such as code bloat from excessive code generation and the
build-up of cognitive debt as developers invest less mental effort during
implementation.

</details>


### [12] [Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses](https://arxiv.org/abs/2507.00803)
*Gillian Daniel,Chris Hall,Per Hammer,Alec-Angus Macdonald,Hollie Marwick-Best,Emma McKenzie,George Popa,Derek Somerville,Tim Storer*

Main category: cs.SE

TL;DR: 该论文探讨了企业与高校合作设计软件工程课程时行业从业者的视角，填补了现有文献中对从业者动机、期望和经验研究的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补对行业从业者在课程设计和交付中视角的文献空白，以优化未来合作模式并确保其可持续性。

Method: 通过对参与过软件工程课程设计和交付的从业者进行回顾性讨论，分析其视角并提炼主题和推荐建议。

Result: 研究揭示了从业者的动机、期望和经验，并提出了未来合作的建议。

Conclusion: 理解从业者的视角对于建立和维持高校与行业的长期合作关系至关重要。

Abstract: Over more than two decades, The University of Glasgow has co-designed and
delivered numerous software engineering focused courses with industry partners,
covering both technical and discipline specific professional skills. Such
collaborations are not unique and many of the benefits are well recognised in
the literature. These include enhancing the real-world relevance of curricula,
developing student professional networks ahead of graduation and easing
recruitment opportunities for employers.
  However, there is relatively little scholarship on the perspectives of
industry practitioners who participate in course design and delivery. This gap
is significant, since the effort invested by practitioners is often substantial
and may require ongoing support from both the industry partner and academic
institution. Understanding the motivations, expectations and experiences of
practitioners who engage in course delivery can guide the formation of future
partnerships and ensure their long-term sustainability.
  We begin to address this gap by reporting on the outcomes of a retrospective
conducted amongst the practitioner coauthors of this paper, with the academic
coauthors acting as facilitators. All coauthors have participated in the recent
co-design and delivery of software engineering courses, but we choose to focus
explicitly on the perspectives of the practitioners. We report on the themes
that emerged from the discussions and our resulting recommendations for future
collaborations.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: 该论文提出了一种名为‘不一致性’的度量方法，用于在缺乏正确答案（oracle）的情况下，高效评估大型语言模型（LLM）生成代码的正确性。


<details>
  <summary>Details</summary>
Motivation: 在没有现有正确实现（oracle）的情况下，如何量化生成代码的正确性？

Method: 提出‘不一致性’度量方法，估计LLM生成代码的错误概率。

Result: 实验表明，该方法可自动识别约三分之二的错误程序，且无误报。oracle评估可被不一致性评估可靠替代。

Conclusion: 不一致性方法能高效评估代码生成的正确性，适用于LLM的自动化评估。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


### [14] [Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains](https://arxiv.org/abs/2507.00264)
*Isabella Basso do Amaral,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.PL

TL;DR: 比较研究评估了PyO3、ctypes和cffi在性能和使用便捷性方面的表现，表明PyO3能实现最佳性能且无需担心API兼容性问题。


<details>
  <summary>Details</summary>
Motivation: Python以其语法和科学库闻名，但其解释器速度慢，优化关键部分需要跨语言知识且手动操作繁琐。

Method: 通过PyO3、ctypes和cffi的工具链进行性能和使用便捷性的对比研究。

Result: 使用PyO3可以实现最佳性能，且无需担心API兼容性问题。

Conclusion: PyO3是一种高效的Python绑定工具链，优于ctypes和cffi。

Abstract: The Python programming language is best known for its syntax and scientific
libraries, but it is also notorious for its slow interpreter. Optimizing
critical sections in Python entails special knowledge of the binary
interactions between programming languages, and can be cumbersome to interface
manually, with implementers often resorting to convoluted third-party
libraries. This comparative study evaluates the performance and ease of use of
the PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using
Rust tooling developed for Python, we can achieve state-of-the-art performance
with no concern for API compatibility.

</details>


### [15] [Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?](https://arxiv.org/abs/2507.00488)
*Lloyd Allison*

Main category: cs.PL

TL;DR: 论文探讨了编程语言中函数的分类不足问题，提出在面向对象语言中引入“Function类”及其子类的概念，并通过实验验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究发现编程语言中的函数缺乏精细分类，尤其是在面向对象语言中缺少“Function类”及其子类的实现，这限制了函数的灵活性和功能性。

Method: 通过提出“Function类”及其子类，并在流行的编程语言中进行实验验证，探讨其在面向对象语言中的实际应用。

Result: 实验结果表明，引入“Function类”及其子类可以增强函数的功能性和灵活性，填补现有语言的不足。

Conclusion: 研究表明在面向对象语言中引入“Function类”及其子类是可行的，且能显著提升函数的分类和功能性。

Abstract: Compared to functions in mathematics, functions in programming languages seem
to be under classified. Functional programming languages based on the lambda
calculus famously treat functions as first-class values. Object-oriented
languages have adopted ``lambdas'', notably for call-back routines in
event-based programming. Typically a programming language has functions, a
function has a type, and some functions act on other functions and/or return
functions but there is generally a lack of (i) ``class Function'' in the OO
sense of the word class and particularly (ii) subclasses of Function for
functions having specific properties. Some such classes are presented here and
programmed in some popular programming languages as an experimental
investigation into OO languages missing this opportunity.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [Plan-Based Scalable Online Virtual Network Embedding](https://arxiv.org/abs/2507.00237)
*Oleg Kolosov,David Breitgand,Dean H. Lorenz,Gala Yadgar*

Main category: cs.NI

TL;DR: 论文提出了一种新的在线算法OLIVE，通过离线计算的聚合需求规划，动态处理虚拟网络嵌入（VNE）问题，显著提升了处理请求的能力。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中，虚拟网络嵌入问题因需求不可预测且资源受限而难以解决，现有方法在规模化处理上表现不足。

Method: OLIVE算法结合离线计算的聚合需求规划，动态调整以应对实际请求，优化嵌入效率。

Result: OLIVE处理请求的能力比现有方法高出两个数量级，适用于现实边缘环境。

Conclusion: OLIVE为高动态边缘环境下的虚拟网络嵌入提供了高效解决方案。

Abstract: Network virtualization allows hosting applications with diverse computation
and communication requirements on shared edge infrastructure. Given a set of
requests for deploying virtualized applications, the edge provider has to
deploy a maximum number of them to the underlying physical network, subject to
capacity constraints. This challenge is known as the virtual network embedding
(VNE) problem: it models applications as virtual networks, where virtual nodes
represent functions and virtual links represent communication between the
virtual nodes.
  All variants of VNE are known to be strongly NP-hard. Because of its
centrality to network virtualization, VNE has been extensively studied. We
focus on the online variant of VNE, in which deployment requests are not known
in advance. This reflects the highly skewed and unpredictable demand intrinsic
to the edge. Unfortunately, existing solutions to online VNE do not scale well
with the number of requests per second and the physical topology size.
  We propose a novel approach in which our new online algorithm, OLIVE,
leverages a nearly optimal embedding for an aggregated expected demand. This
embedding is computed offline. It serves as a plan that OLIVE uses as a guide
for handling actual individual requests while dynamically compensating for
deviations from the plan. We demonstrate that our solution can handle a number
of requests per second greater by two orders of magnitude than the best results
reported in the literature. Thus, it is particularly suitable for realistic
edge environments.

</details>


### [17] [Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols](https://arxiv.org/abs/2507.00337)
*Yuxin Liu,Tianyang Zhang,Qiang Wu,Ju Ren,Kyle Jamieson,Yaxiong Xie*

Main category: cs.NI

TL;DR: 论文提出CellNinjia和Gandalf系统，通过实时监控RAN操作补偿其引起的延迟，显著提升延时协议的蜂窝网络性能。


<details>
  <summary>Details</summary>
Motivation: 在蜂窝网络中，RAN缓冲引入的延迟与拥塞无关，但会显著影响基于延迟的网络协议性能。

Method: 开发了CellNinjia（实时监控RAN）和Gandalf（补偿RAN延迟），无需修改核心协议。

Result: 在商用4G LTE和5G网络中，Copa和PCC Vivace性能分别提升7.49倍和9.53倍。

Conclusion: 基于延迟的协议在蜂窝网络中可通过补偿RAN延迟实现其潜力。

Abstract: Delay-based protocols rely on end-to-end delay measurements to detect network
congestion. However, in cellular networks, Radio Access Network (RAN) buffers
introduce significant delays unrelated to congestion, fundamentally challenging
these protocols' assumptions. We identify two major types of RAN buffers -
retransmission buffers and uplink scheduling buffers - that can introduce
delays comparable to congestion-induced delays, severely degrading protocol
performance. We present CellNinjia, a software-based system providing real-time
visibility into RAN operations, and Gandalf, which leverages this visibility to
systematically handle RAN-induced delays. Unlike existing approaches that treat
these delays as random noise, Gandalf identifies specific RAN operations and
compensates for their effects. Our evaluation in commercial 4G LTE and 5G
networks shows that Gandalf enables substantial performance improvements - up
to 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols'
core algorithms, demonstrating that delay-based protocols can realize their
full potential in cellular networks.

</details>


### [18] [Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols](https://arxiv.org/abs/2507.00623)
*Daniel Mejías,Inhar Yeregui,Roberto Viola,Miguel Fernández,Mario Montagud*

Main category: cs.NI

TL;DR: 论文探讨了远程渲染在扩展现实（XR）应用中的重要性，通过集成主流多媒体框架（GStreamer和FFmpeg）并分析其在WIFI和5G下的性能，为XR研究提供了先进的测试平台。


<details>
  <summary>Details</summary>
Motivation: 由于轻量级设备处理能力和带宽有限，无法满足复杂XR应用的需求，远程渲染成为解决方案。

Method: 集成GStreamer和FFmpeg作为远程渲染器，测试其在WIFI和5G下通过不同协议（如WebRTC、DASH和QUIC-based协议）传输渲染内容的性能。

Result: 构建了一个先进的测试平台，支持多种传输协议和网络环境，为XR研究提供了高效的实验基础。

Conclusion: 通过远程渲染和多协议传输，可以有效解决XR应用在轻量级设备上的性能瓶颈，推动该领域的进一步发展。

Abstract: The increasing complexity of Extended Reality (XR) applications demands
substantial processing power and high bandwidth communications, often
unavailable on lightweight devices. Remote rendering consists of offloading
processing tasks to a remote node with a powerful GPU, delivering the rendered
content to the end device. The delivery is usually performed through popular
streaming protocols such as Web Real-Time Communications (WebRTC), offering a
data channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH),
better suitable for scalability. Moreover, new streaming protocols based on
QUIC are emerging as potential replacements for WebRTC and DASH and offer
benefits like connection migration, stream multiplexing and multipath delivery.
This work describes the integration of the two most popular multimedia
frameworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote
Renderer, and analyzes their performance when offering different protocols for
delivering the rendered content to the end device over WIFI or 5G. This
solution constitutes a beyond state-of-the-art testbed to conduct cutting-edge
research in the XR field.

</details>


### [19] [Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration](https://arxiv.org/abs/2507.00672)
*Haoxiang Luo,Yinqiu Liu,Ruichen Zhang,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Zehui Xiong,Xianbin Wang,Xuemin Shen*

Main category: cs.NI

TL;DR: 该论文探讨了如何通过集成多LLM（大语言模型）提升边缘计算中复杂任务的性能，包括动态协同和跨域知识迁移等关键技术。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在处理边缘计算中的复杂动态任务和多模态数据时表现不足，需要更高效的解决方案。

Method: 通过引入多LLM系统，结合动态调度、资源管理和跨领域知识转移技术，提升任务适应性和性能。

Result: 多LLM系统显著提高了边缘计算中复杂任务的处理能力，尤其是在资源受限的环境中。

Conclusion: 多LLM系统为边缘计算提供了新的研究方向，未来需进一步优化资源效率和信任机制。

Abstract: Edge computing enables real-time data processing closer to its source, thus
improving the latency and performance of edge-enabled AI applications. However,
traditional AI models often fall short when dealing with complex, dynamic tasks
that require advanced reasoning and multimodal data processing. This survey
explores the integration of multi-LLMs (Large Language Models) to address this
in edge computing, where multiple specialized LLMs collaborate to enhance task
performance and adaptability in resource-constrained environments. We review
the transition from conventional edge AI models to single LLM deployment and,
ultimately, to multi-LLM systems. The survey discusses enabling technologies
such as dynamic orchestration, resource scheduling, and cross-domain knowledge
transfer that are key for multi-LLM implementation. A central focus is on
trusted multi-LLM systems, ensuring robust decision-making in environments
where reliability and privacy are crucial. We also present multimodal multi-LLM
architectures, where multiple LLMs specialize in handling different data
modalities, such as text, images, and audio, by integrating their outputs for
comprehensive analysis. Finally, we highlight future directions, including
improving resource efficiency, trustworthy governance multi-LLM systems, while
addressing privacy, trust, and robustness concerns. This survey provides a
valuable reference for researchers and practitioners aiming to leverage
multi-LLM systems in edge computing applications.

</details>


### [20] [Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework](https://arxiv.org/abs/2507.00856)
*Beining Wu,Jun Huang,Qiang Duan,Liang Dong,Zhipeng Cai*

Main category: cs.NI

TL;DR: 论文通过两阶段资源感知控制框架（RACE）优化车载编队（VP）中的无线联邦学习（WFL）性能，显著提升信息时效性（AoI）和模型同步。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，车载编队系统面临通信变化和资源限制的问题，影响信息交换和学习模型同步，需解决AoI和FLMD的联合优化问题。

Method: 提出两阶段框架RACE：第一阶段采用拉格朗日对偶分解法配置资源；第二阶段用多智能体深度强化学习选择车辆，结合多头自注意力与LSTM网络。

Result: 实验表明，RACE在AI4MARS数据集上优于基线方法，AoI优化提升45%，学习收敛加快，更适应动态VP环境。

Conclusion: RACE框架有效解决了VP中WFL的性能问题，显著提升了动态环境下的通信和模型同步效率。

Abstract: This paper aims to enhance the performance of Vehicular Platooning (VP)
systems integrated with Wireless Federated Learning (WFL). In highly dynamic
environments, vehicular platoons experience frequent communication changes and
resource constraints, which significantly affect information exchange and
learning model synchronization. To address these challenges, we first formulate
WFL in VP as a joint optimization problem that simultaneously considers Age of
Information (AoI) and Federated Learning Model Drift (FLMD) to ensure timely
and accurate control. Through theoretical analysis, we examine the impact of
FLMD on convergence performance and develop a two-stage Resource-Aware Control
framework (RACE). The first stage employs a Lagrangian dual decomposition
method for resource configuration, while the second stage implements a
multi-agent deep reinforcement learning approach for vehicle selection. The
approach integrates Multi-Head Self-Attention and Long Short-Term Memory
networks to capture spatiotemporal correlations in communication states.
Experimental results demonstrate that, compared to baseline methods, the
proposed framework improves AoI optimization by up to 45%, accelerates learning
convergence, and adapts more effectively to dynamic VP environments on the
AI4MARS dataset.

</details>


### [21] [QUIC Delay Control: an implementation of congestion and delay control](https://arxiv.org/abs/2507.00896)
*Saverio Mascolo,Andrea Vittorio Balillo,Gioacchino Manfredi,Davide D'Agostino,Luca De Cicco*

Main category: cs.NI

TL;DR: QUIC-DC是一种新的拥塞和延迟控制算法，旨在同时控制拥塞和单向排队延迟，通过早期反应减少丢包和通信延迟。


<details>
  <summary>Details</summary>
Motivation: 为了在实时应用中更有效地控制拥塞和延迟，减少丢包并提高网络利用率。

Method: 结合单向排队延迟估计和TCP Westwood+算法，实现早期拥塞反应。

Result: 在仿真和实际网络中，QUIC-DC显著减少丢包和端到端延迟，同时保持网络利用率。

Conclusion: QUIC-DC是实时应用中有效的拥塞和延迟控制方案。

Abstract: A new congestion and delay control algorithm named QUIC Delay Control
(QUIC-DC) is proposed for controlling not only congestion but also the queuing
delay encountered along the forward communication path. The core idea is to
estimate the one-way queuing delay of a connection to trigger an early reaction
to congestion. This idea, along with the TCP Westwood+ congestion control
algorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2,
NewReno, Westwood+. The results obtained in both emulated and real network
connections show that QUIC-DC can significantly reduce packet losses along with
end-to-end communication delays, while preserving network utilization, features
that are both very useful for real-time applications.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction](https://arxiv.org/abs/2507.00926)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.MM

TL;DR: HyperFusion是一个层次化多模态集成学习框架，用于社交媒体流行度预测，通过融合视觉、文本、时空和用户行为特征，取得了SMP挑战数据集上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 社交媒体流行度预测对内容优化和营销策略至关重要，但因多因素复杂交互而具有挑战性。

Method: 采用三层融合架构，结合CLIP视觉编码、Transformer文本嵌入及时空元数据，利用CatBoost、TabNet和MLP进行集成学习，并提出两阶段训练方法。

Result: 在SMP Challenge 2025（图像赛道）中排名第三，表现优异。

Conclusion: HyperFusion通过多模态特征融合和层次化集成策略，有效提升了社交媒体流行度预测的准确性。

Abstract: Social media popularity prediction plays a crucial role in content
optimization, marketing strategies, and user engagement enhancement across
digital platforms. However, predicting post popularity remains challenging due
to the complex interplay between visual, textual, temporal, and user behavioral
factors. This paper presents HyperFusion, a hierarchical multimodal ensemble
learning framework for social media popularity prediction. Our approach employs
a three-tier fusion architecture that progressively integrates features across
abstraction levels: visual representations from CLIP encoders, textual
embeddings from transformer models, and temporal-spatial metadata with user
characteristics. The framework implements a hierarchical ensemble strategy
combining CatBoost, TabNet, and custom multi-layer perceptrons. To address
limited labeled data, we propose a two-stage training methodology with
pseudo-labeling and iterative refinement. We introduce novel cross-modal
similarity measures and hierarchical clustering features that capture
inter-modal dependencies. Experimental results demonstrate that HyperFusion
achieves competitive performance on the SMP challenge dataset. Our team
achieved third place in the SMP Challenge 2025 (Image Track). The source code
is available at https://anonymous.4open.science/r/SMPDImage.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Encoding Peano Arithmetic in a Minimal Fragment of Separation Logic](https://arxiv.org/abs/2507.00465)
*Sohei Ito,Makoto Tatsuta*

Main category: cs.LO

TL;DR: 论文研究了扩展分离逻辑最小片段与自然数的表达能力，证明其可以编码所有PA的$Π^0_1$公式，并揭示了其有效性的不可判定性。


<details>
  <summary>Details</summary>
Motivation: 探索分离逻辑简单片段的表达能力及其在算术和程序验证中的理论意义。

Method: 构建从PA到该片段的翻译，利用堆内存编码算术操作，并通过有限模型理论进行不可判定性证明。

Result: 该片段的$Π^0_1$公式有效性不可判定且$Π^0_1$-完全，但对$Σ^0_1$公式无效。

Conclusion: 该片段虽语法简单，但在逻辑与程序验证中具有重要意义。

Abstract: This paper investigates the expressive power of a minimal fragment of
separation logic extended with natural numbers. Specifically, it demonstrates
that the fragment consisting solely of the intuitionistic points-to predicate,
the constant 0, and the successor function is sufficient to encode all
$\Pi^0_1$ formulas of Peano Arithmetic (PA). The authors construct a
translation from PA into this fragment, showing that a $\Pi^0_1$ formula is
valid in the standard model of arithmetic if and only if its translation is
valid in the standard interpretation of the separation logic fragment. This
result implies the undecidability of validity in the fragment, despite its
syntactic simplicity. The translation leverages a heap-based encoding of
arithmetic operations - addition, multiplication, and inequality - using
structured memory cells. The paper also explores the boundaries of this
encoding, showing that the translation does not preserve validity for
$\Sigma^0_1$ formulas. Additionally, an alternative undecidability proof is
presented via a reduction from finite model theory. Finally, the paper
establishes that the validity problem for this fragment is $\Pi^0_1$-complete,
highlighting its theoretical significance in the landscape of logic and program
verification.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)
*Xingyu Xiao,Jiejuan Tong,Peng Chen,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: 该论文提出了一种基于AutoGraph（InSight-R）的框架，用于自动化识别人为故障事件并评估界面设计对操作员表现的影响，提升了人类可靠性分析的客观性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统人类可靠性分析方法依赖专家判断，存在主观性强、可重复性差等问题，且无法有效评估人机界面设计对人为错误的影响。

Method: 通过构建界面嵌入知识图谱（IE-KG）并链接实证行为数据，InSight-R框架实现了基于易错和时偏操作路径的自动化人为故障事件识别。

Result: InSight-R不仅提高了人为故障事件识别的客观性和可解释性，还为实时人类可靠性评估提供了可行路径。

Conclusion: 该框架为人机界面设计优化提供了实用建议，推动了机制驱动的人类可靠性分析方法的发展。

Abstract: Human reliability remains a critical concern in safety-critical domains such
as nuclear power, where operational failures are often linked to human error.
While conventional human reliability analysis (HRA) methods have been widely
adopted, they rely heavily on expert judgment for identifying human failure
events (HFEs) and assigning performance influencing factors (PIFs). This
reliance introduces challenges related to reproducibility, subjectivity, and
limited integration of interface-level data. In particular, current approaches
lack the capacity to rigorously assess how human-machine interface design
contributes to operator performance variability and error susceptibility. To
address these limitations, this study proposes a framework for risk-informed
human failure event identification and interface-induced risk assessment driven
by AutoGraph (InSight-R). By linking empirical behavioral data to the
interface-embedded knowledge graph (IE-KG) constructed by the automated
graph-based execution framework (AutoGraph), the InSight-R framework enables
automated HFE identification based on both error-prone and time-deviated
operational paths. Furthermore, we discuss the relationship between
designer-user conflicts and human error. The results demonstrate that InSight-R
not only enhances the objectivity and interpretability of HFE identification
but also provides a scalable pathway toward dynamic, real-time human
reliability assessment in digitalized control environments. This framework
offers actionable insights for interface design optimization and contributes to
the advancement of mechanism-driven HRA methodologies.

</details>


### [25] [Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments](https://arxiv.org/abs/2507.00161)
*Christopher M. Wegemer,Edward Halim,Jeff Burke*

Main category: cs.HC

TL;DR: 论文探讨了利用AI技术减少政治极化，通过情感计算和叙事机制促进学生对不同政治观点的开放性。


<details>
  <summary>Details</summary>
Motivation: 政治极化阻碍民主教育，AI技术提供新机会设计干预措施以减少极化并提升政治开放性。

Method: 结合情感计算和叙事理论，开发AI-DCS平台，实时分析用户情感和注意力，并利用GPT-4个性化故事内容。

Result: 平台通过情感敏感策略减少情感极化，同时保持学习者自主性，为公民教育提供新工具。

Conclusion: 研究为公民教育干预和HCI领域提供基础，需进一步解决AI对话管理和情感适应性挑战。

Abstract: Political polarization undermines democratic civic education by exacerbating
identity-based resistance to opposing viewpoints. Emerging AI technologies
offer new opportunities to advance interventions that reduce polarization and
promote political open-mindedness. We examined novel design strategies that
leverage adaptive and emotionally-responsive civic narratives that may sustain
students' emotional engagement in stories, and in turn, promote
perspective-taking toward members of political out-groups. Drawing on theories
from political psychology and narratology, we investigate how affective
computing techniques can support three storytelling mechanisms: transportation
into a story world, identification with characters, and interaction with the
storyteller. Using a design-based research (DBR) approach, we iteratively
developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS)
platform. Our prototype integrates facial emotion recognition and attention
tracking to assess users' affective and attentional states in real time.
Narrative content is organized around pre-structured story outlines, with
beat-by-beat language adaptation implemented via GPT-4, personalizing
linguistic tone to sustain students' emotional engagement in stories that
center political perspectives different from their own. Our work offers a
foundation for AI-supported, emotionally-sensitive strategies that address
affective polarization while preserving learner autonomy. We conclude with
implications for civic education interventions, algorithmic literacy, and HCI
challenges associated with AI dialogue management and affect-adaptive learning
environments.

</details>


### [26] [Exploring AR Label Placements in Visually Cluttered Scenarios](https://arxiv.org/abs/2507.00198)
*Ji Hwan Park,Braden Roper,Amirhossein Arezoumand,Tien Tran*

Main category: cs.HC

TL;DR: 研究在视觉杂乱的AR场景中有效放置标签的方法，测试了三种技术。


<details>
  <summary>Details</summary>
Motivation: 随着AR场景中对象数量增加，现有标签放置指南难以有效应对视觉杂乱的问题。

Method: 实现并评估了三种针对同类型对象的标签放置技术。

Result: 使用标签对同类型对象进行空间分组有助于识别、比较和总结数据。

Conclusion: 标签分组技术在视觉杂乱的AR场景中具有实用价值。

Abstract: We investigate methods for placing labels in AR environments that have
visually cluttered scenes. As the number of items increases in a scene within
the user' FOV, it is challenging to effectively place labels based on existing
label placement guidelines. To address this issue, we implemented three label
placement techniques for in-view objects for AR applications. We specifically
target a scenario, where various items of different types are scattered within
the user's field of view, and multiple items of the same type are situated
close together. We evaluate three placement techniques for three target tasks.
Our study shows that using a label to spatially group the same types of items
is beneficial for identifying, comparing, and summarizing data.

</details>


### [27] [Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group](https://arxiv.org/abs/2507.00202)
*Blade Frisch,Betts Peters,Keith Vertanen*

Main category: cs.HC

TL;DR: 研究探讨了自闭症成人的沟通需求及其与残疾人群的差异，提出如何设计AAC以支持这一群体，并通过在线焦点小组分析参与者的反馈，总结了AAC设计的未来启示和自闭症研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索自闭症成人的独特沟通需求及AAC如何更好地支持这些需求。

Method: 通过在线异步文本焦点小组，收集五名自闭症成人的社交沟通和社区参与经验。

Result: 发现情感体验影响沟通方式、AAC对能说话的自闭症成人有益、自闭症关闭导致动态沟通需求。提出了未来AAC设计的三点启示。

Conclusion: 研究为未来AAC设计和自闭症研究提供了具体方向，并改进了在线焦点小组的可访问性方法。

Abstract: Purpose: Little research has explored the communication needs of autistic
adults and how their needs differ from those of other disabled populations.
Augmentative and Alternative Communication (AAC) can support these
communication needs, but more guidance is needed on how to design AAC to
support this population.
  Materials and Methods: We conducted an online, asynchronous, text-based focus
group with five autistic adults to explore their social communication and
community engagement and how AAC can help support them.
  Results and Conclusion: Our analysis of the participant responses found that
1) participants' emotional experiences impacted the communication methods they
used, 2) speaking autistic adults can benefit from AAC use, and 3) autistic
shutdown creates dynamic communication needs. We present implications for
future AAC design: supporting communication in times of shutdown, indicating
communication ability to communication partners, and a need to better
understand the fear of using AAC. These implications can inform the design for
future AAC systems. We also provide themes for future autism research:
exploring the impact of a late diagnosis, gaining a better understanding of the
communication needs during autistic shutdown, and expanding research to include
the social and environmental factors that impact communication. Finally, we
provide guidance on how future online focus groups can be run in an accessible
manner.

</details>


### [28] [User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"](https://arxiv.org/abs/2507.00271)
*Zhuochao Peng,Jiaxin Xu,Jun Hu,Haian Xue,Laurens A. G. Kolks,Pieter M. A. Desmet*

Main category: cs.HC

TL;DR: 研究探讨了社交机器人在日常生活中调节情绪的潜力和用户对其集成的看法，通过一个名为'Mora'的虚构机器人原型和共同构建故事的方法，收集了15名参与者对使用社交机器人管理日常情绪（如“周日忧郁”）的见解。研究揭示了用户对社交机器人共情能力、干预效果和伦理边界的复杂反思，并转化为设计建议。


<details>
  <summary>Details</summary>
Motivation: 尽管社交机器人在情绪调节方面具有潜力，但用户对其在日常生活中集成的看法尚不明确。研究旨在通过一个具体的情绪情境（“周日忧郁”）探索用户的期望和担忧。

Method: 采用探索性案例研究方法，使用虚构机器人概念'Mora'的视频原型和共同构建故事的方法，收集15名参与者的反馈，分析他们对社交机器人的看法。

Result: 研究发现用户对社交机器人的共情能力、干预效果和伦理边界有复杂反思，这些见解为未来人机交互研究和设计提供了实用建议。

Conclusion: 研究强调了在设计社交机器人时需要关注用户的多样化需求和伦理问题，为未来开发提供了重要启示。

Abstract: While recent research highlights the potential of social robots to support
mood regulation, little is known about how prospective users view their
integration into everyday life. To explore this, we conducted an exploratory
case study that used a speculative robot concept "Mora" to provoke reflection
and facilitate meaningful discussion about using social robots to manage
subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a
common dip in mood that occurs at the end of the weekend, as a relatable
context in which to explore individuals' insights. Using a video prototype and
a co-constructing stories method, we engaged 15 participants in imagining
interactions with Mora and discussing their expectations, doubts, and concerns.
The study surfaced a range of nuanced reflections around the attributes of
social robots like empathy, intervention effectiveness, and ethical boundaries,
which we translated into design considerations for future research and
development in human-robot interaction.

</details>


### [29] [Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels](https://arxiv.org/abs/2507.00333)
*Emin Zerman,Jonas Carlsson,Mårten Sjöström*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Marksmanship practices are required in various professions, including police,
military personnel, hunters, as well as sports shooters, such as Olympic
shooting, biathlon, and modern pentathlon. The current form of training and
coaching is mostly based on repetition, where the coach does not see through
the eyes of the shooter, and analysis is limited to stance and accuracy
post-session. In this study, we present a shooting visualization system and
evaluate its perceived effectiveness for both novice and expert shooters. To
achieve this, five composite visualizations were developed using first-person
shooting video recordings enriched with overlaid metrics and graphical
summaries. These views were evaluated with 10 participants (5 expert marksmen,
5 novices) through a mixed-methods study including shot-count and aiming
interpretation tasks, pairwise preference comparisons, and semi-structured
interviews. The results show that a dashboard-style composite view, combining
raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases
and supported understanding across skill levels. The insights gained from this
design study point to the broader value of integrating first-person video with
visual analytics for coaching, and we suggest directions for applying this
approach to other precision-based sports.

</details>


### [30] [Visual Privacy Management with Generative AI for Blind and Low-Vision People](https://arxiv.org/abs/2507.00286)
*Tanusree Sharma,Yu-Yun Tseng,Lotus Zhang,Ayae Ide,Kelly Avery Mack,Leah Findlater,Danna Gurari,Yang Wang*

Main category: cs.HC

TL;DR: BLV人群使用GenAI工具管理视觉内容，但面临视觉隐私问题。通过21人访谈，研究揭示了当前实践和未来设计偏好，提出了支持隐私的设计建议。


<details>
  <summary>Details</summary>
Motivation: 研究BLV人群使用GenAI工具时面临的视觉隐私挑战，以改善工具设计并增强用户隐私保护。

Method: 通过访谈研究21位BLV个体，分析其GenAI使用实践和设计偏好。

Result: 发现用户在隐私、效率和情感代理之间的平衡实践，并提出了五种设计偏好。

Conclusion: 提出了以用户为中心的视觉隐私设计建议，扩展了隐私概念及其对他人数据的负责处理。

Abstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to
interpret and manage visual content in their daily lives. While such tools can
enhance the accessibility of visual content and so enable greater user
independence, they also introduce complex challenges around visual privacy. In
this paper, we investigate the current practices and future design preferences
of blind and low vision individuals through an interview study with 21
participants. Our findings reveal a range of current practices with GenAI that
balance privacy, efficiency, and emotional agency, with users accounting for
privacy risks across six key scenarios, such as self-presentation,
indoor/outdoor spatial privacy, social sharing, and handling professional
content. Our findings reveal design preferences, including on-device
processing, zero-retention guarantees, sensitive content redaction,
privacy-aware appearance indicators, and multimodal tactile mirrored
interaction methods. We conclude with actionable design recommendations to
support user-centered visual privacy through GenAI, expanding the notion of
privacy and responsible handling of others data.

</details>


### [31] [When Kids Mode Isn't For Kids: Investigating TikTok's "Under 13 Experience"](https://arxiv.org/abs/2507.00299)
*Olivia Figueira,Pranathi Chamarthi,Tu Le,Athina Markopoulou*

Main category: cs.HC

TL;DR: 研究通过审计方法分析TikTok的“儿童模式”，发现83%的内容非儿童导向，存在不适当内容，且缺乏关键功能如家长控制。


<details>
  <summary>Details</summary>
Motivation: TikTok的“儿童模式”内容筛选及安全隐私保护缺乏透明度，研究旨在填补这一空白。

Method: 提出审计方法，分析“儿童模式”内容，依据COPPA规定评估儿童导向内容比例。

Result: 发现83%视频非儿童导向，有不适当内容，且缺乏家长控制等功能。

Conclusion: 研究揭示儿童模式的不足，可能促使儿童转向普通模式，增加安全隐私风险，具设计和监管意义。

Abstract: TikTok, the social media platform that is popular among children and
adolescents, offers a more restrictive "Under 13 Experience" exclusively for
young users in the US, also known as TikTok's "Kids Mode". While prior research
has studied various aspects of TikTok's regular mode, including privacy and
personalization, TikTok's Kids Mode remains understudied, and there is a lack
of transparency regarding its content curation and its safety and privacy
protections for children. In this paper, (i) we propose an auditing methodology
to comprehensively investigate TikTok's Kids Mode and (ii) we apply it to
characterize the platform's content curation and determine the prevalence of
child-directed content, based on regulations in the Children's Online Privacy
Protection Act (COPPA). We find that 83% of videos observed on the "For You"
page in Kids Mode are actually not child-directed, and even inappropriate
content was found. The platform also lacks critical features, namely parental
controls and accessibility settings. Our findings have important design and
regulatory implications, as children may be incentivized to use TikTok's
regular mode instead of Kids Mode, where they are known to be exposed to
further safety and privacy risks.

</details>


### [32] [EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation](https://arxiv.org/abs/2507.00305)
*Deland Liu,Frigyes Samuel Racz,Zoe Lalji,Jose del R. Millan*

Main category: cs.HC

TL;DR: 研究探讨了非侵入性脑电图（EEG）脑机接口（BCI）能否帮助完全锁闭状态（CLIS）的肌萎缩侧索硬化（ALS）患者恢复基本沟通。患者通过调节脑电波对问题做出“是”/“否”回答，部分会话表现优异。


<details>
  <summary>Details</summary>
Motivation: 完全锁闭状态的ALS患者失去所有可靠运动控制，无法沟通。研究旨在探索非侵入性BCI是否能为CLIS患者提供沟通途径。

Method: 使用EEG-based BCI，患者通过调节alpha和beta波段功率在不同通道做出“是”/“否”回答，实时听觉反馈指导。

Result: 患者在辅助问题上表现优异，部分会话中对一般知识问题回答准确。脑电调节模式随时间稳定。

Conclusion: 非侵入性BCI可能为CLIS患者恢复基本沟通提供可行途径。

Abstract: Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in
state (CLIS) can lose all reliable motor control and are left without any means
of communication. It remains unknown whether non-invasive electroencephalogram
(EEG) based brain-computer interfaces (BCIs) can support volitional
communication in CLIS. Here, we show that a CLIS patient was able to operate an
EEG-based BCI across multiple online sessions to respond to both general
knowledge and personally relevant assistive questions. The patient delivered
"Yes"/"No" responses by volitionally modulating alpha and beta band power at
different channels, guided by real-time auditory feedback from the BCI. The
patient communicated assistive needs above chance in all sessions, achieving a
perfect score in the final session. Performance on general knowledge questions
varied across sessions, with two sessions showing accurate and above-chance
responses, while the first and last sessions remained at chance level. The
patient also showed consistent modulation patterns over time. These findings
suggest that non-invasive BCIs may offer a potential pathway for restoring
basic communication in CLIS.

</details>


### [33] [Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center](https://arxiv.org/abs/2507.00513)
*Kai Qin,Kexin Du,Yimeng Chen,Yueyan Liu,Jie Cai,Zhiqiang Nie,Nan Gao,Guohui Wei,Shengzhu Wang,Chun Yu*

Main category: cs.HC

TL;DR: 研究探讨电力网格客服中心代表如何感知AI辅助，发现AI既能减轻传统负担（如打字和记忆），又带来新负担（如学习、合规和心理负担）。


<details>
  <summary>Details</summary>
Motivation: 了解AI工具在复杂社会技术环境中对员工与客户互动的影响，探索客服代表对AI辅助的看法。

Method: 通过实地访问和半结构化访谈13名客服代表，分析AI在客服互动中的作用。

Result: AI减轻了传统工作负担（如打字和记忆），但也引入了新负担（如学习、合规和心理压力）。

Conclusion: 研究揭示了AI在组织中集成的复杂性，强调客服代表为适应新系统所付出的努力和承受的负担。

Abstract: The integration of various AI tools creates a complex socio-technical
environment where employee-customer interactions form the core of work
practices. This study investigates how customer service representatives (CSRs)
at the power grid service customer service call center perceive AI assistance
in their interactions with customers. Through a field visit and semi-structured
interviews with 13 CSRs, we found that AI can alleviate some traditional
burdens during the call (e.g., typing and memorizing) but also introduces new
burdens (e.g., earning, compliance, psychological burdens). This research
contributes to a more nuanced understanding of AI integration in organizational
settings and highlights the efforts and burdens undertaken by CSRs to adapt to
the updated system.

</details>


### [34] [Gaze3P: Gaze-Based Prediction of User-Perceived Privacy](https://arxiv.org/abs/2507.00596)
*Mayar Elfares,Pascal Reisert,Ralf Küsters,Andreas Bulling*

Main category: cs.HC

TL;DR: 本文提出了Gaze3P，首个用于系统研究用户隐私感知的数据集，结合眼动数据训练模型预测隐私感知，并优化隐私保护技术参数。


<details>
  <summary>Details</summary>
Motivation: 解决隐私感知量化研究依赖问卷的局限，探索隐私感知在隐私保护技术参数优化中的应用。

Method: 构建Gaze3P数据集（100人眼动数据+1000刺激），训练机器学习模型动态预测隐私感知。

Result: 所训练模型预测隐私感知准确率高，并能优化差分隐私机制参数。

Conclusion: Gaze3P有效填补隐私感知量化与隐私保护技术优化的研究空白，提升用户期望匹配度。

Abstract: Privacy is a highly subjective concept and perceived variably by different
individuals. Previous research on quantifying user-perceived privacy has
primarily relied on questionnaires. Furthermore, applying user-perceived
privacy to optimise the parameters of privacy-preserving techniques (PPT)
remains insufficiently explored. To address these limitations, we introduce
Gaze3P -- the first dataset specifically designed to facilitate systematic
investigations into user-perceived privacy. Our dataset comprises gaze data
from 100 participants and 1,000 stimuli, encompassing a range of private and
safe attributes. With Gaze3P, we train a machine learning model to implicitly
and dynamically predict perceived privacy from human eye gaze. Through
comprehensive experiments, we show that the resulting models achieve high
accuracy. Finally, we illustrate how predicted privacy can be used to optimise
the parameters of differentially private mechanisms, thereby enhancing their
alignment with user expectations.

</details>


### [35] [Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity](https://arxiv.org/abs/2507.00657)
*Jacopo Nudo,Mario Edoardo Pandolfo,Edoardo Loru,Mattia Samory,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.HC

TL;DR: 研究发现大型语言模型（LLMs）在模拟社交媒体政治话语时，不仅不会真实模拟用户行为，反而会重构用户，放大极化与有害语言，影响其作为社会代理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在模拟政治话语时的行为表现，特别是其在社会代理应用中的潜在偏差。

Method: 基于2100万次X平台交互数据，构建1186个真实用户的LLM代理，分别以零样本和少样本方式初始化，比较模型在语言风格、意识形态一致性和毒性上的表现。

Result: 上下文丰富化虽提高了内部一致性，但也放大了极化、风格化信号和有害语言，且存在“生成夸张”现象。

Conclusion: LLMs的输出更多反映内部优化动态而非真实行为，其结构性偏差限制了其在内容审核、政策建模等领域的可靠性。

Abstract: We investigate how Large Language Models (LLMs) behave when simulating
political discourse on social media. Leveraging 21 million interactions on X
during the 2024 U.S. presidential election, we construct LLM agents based on
1,186 real users, prompting them to reply to politically salient tweets under
controlled conditions. Agents are initialized either with minimal ideological
cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one
comparisons with human replies. We evaluate three model families (Gemini,
Mistral, and DeepSeek) across linguistic style, ideological consistency, and
toxicity. We find that richer contextualization improves internal consistency
but also amplifies polarization, stylized signals, and harmful language. We
observe an emergent distortion that we call "generation exaggeration": a
systematic amplification of salient traits beyond empirical baselines. Our
analysis shows that LLMs do not emulate users, they reconstruct them. Their
outputs, indeed, reflect internal optimization dynamics more than observed
behavior, introducing structural biases that compromise their reliability as
social proxies. This challenges their use in content moderation, deliberative
simulations, and policy modeling.

</details>


### [36] [Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review](https://arxiv.org/abs/2507.00775)
*Haonan Yao,Lingyun Yu,Lijie Yao*

Main category: cs.HC

TL;DR: 对实体数据探索中的任务、交互和可视化部件进行了系统综述，填补了领域内缺乏结构化理解的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实体部件能减少认知负荷、支持自然交互和复杂任务，但目前缺乏对任务、交互和设计的系统协调理解，阻碍了设计模式的发现和创新。

Method: 通过系统综述分析现有工作，总结数据探索任务、交互和实体可视化部件的设计现状，并基于发现提出研究议程。

Result: 提出了未来实体数据探索部件设计的工具包研究方向，补充材料在线公开。

Conclusion: 填补了领域空白，为未来实体部件设计提供了框架和研究方向。

Abstract: We present a systematic review on tasks, interactions, and visualization
widgets (refer to tangible entities that are used to accomplish data
exploration tasks through specific interactions) in the context of tangible
data exploration. Tangible widgets have been shown to reduce cognitive load,
enable more natural interactions, and support the completion of complex data
exploration tasks. Yet, the field lacks a structured understanding of how task
types, interaction methods, and widget designs are coordinated, limiting the
ability to identify recurring design patterns and opportunities for innovation.
To address this gap, we conduct a systematic review to analyze existing work
and characterize the current design of data exploration tasks, interactions,
and tangible visualization widgets. We next reflect based on our findings and
propose a research agenda to inform the development of a future widget design
toolkit for tangible data exploration. Our systematic review and supplemental
materials are available at physicalviswidget.github.io and osf.io/vjw5e.

</details>


### [37] [Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures](https://arxiv.org/abs/2507.00821)
*Mihnea Stefan Calota,Wessel Nieuwenhuys,Janet Yi-Ching Huang,Lin-Lin Chen,Mathias Funk*

Main category: cs.HC

TL;DR: 论文提出了一种面向制造的方法，帮助设计师在无法直接访问真实医疗系统时，通过手工制作合成数据集来理解复杂的医疗环境。


<details>
  <summary>Details</summary>
Motivation: 医疗系统通常是封闭的生态系统，设计师难以直接接触临床利益相关者、系统或数据，导致理解复杂医疗系统存在挑战。

Method: 采用远程患者监测（RPM）为案例，通过观察真实场景并制作合成数据集，迭代设计简化版RPM系统。

Result: 设计师通过手工制作和迭代原型，能够在接触有限的情况下熟悉医疗环境。

Conclusion: 通过与数据结构的实际交互，设计师能够更好地理解不透明的医疗系统。

Abstract: Designers have ample opportunities to impact the healthcare domain. However,
hospitals are often closed ecosystems that pose challenges in engaging clinical
stakeholders, developing domain knowledge, and accessing relevant systems and
data. In this paper, we introduce a making-oriented approach to help designers
understand the intricacies of their target healthcare context. Using Remote
Patient Monitoring (RPM) as a case study, we explore how manually crafting
synthetic datasets based on real-world observations enables designers to learn
about complex data-driven healthcare systems. Our process involves observing
and modeling the real-world RPM context, crafting synthetic datasets, and
iteratively prototyping a simplified RPM system that balances contextual
richness and intentional abstraction. Through this iterative process of
sensemaking through making, designers can still develop context familiarity
when direct access to the actual healthcare system is limited. Our approach
emphasizes the value of hands-on interaction with data structures to support
designers in understanding opaque healthcare systems.

</details>


### [38] [Towards Difficulty-Aware Analysis of Deep Neural Networks](https://arxiv.org/abs/2507.00881)
*Linhao Meng,Stef van den Elzen,Anna Vilanova*

Main category: cs.HC

TL;DR: 论文提出一种基于实例难度的深度神经网络评估方法，从数据、模型和人类三个视角衡量难度，并开发了可视化工具DifficultyEyes来分析和比较分类任务的难度模式。


<details>
  <summary>Details</summary>
Motivation: 传统的基于实例的模型分析只关注误分类实例，忽视了不同实例的难度差异。希望通过引入难度评估，更全面地衡量模型的表现。

Method: 从数据、模型和人类三个角度提出难度度量，并开发交互式可视化工具DifficultyEyes，支持基于难度模式的实例分析和问题诊断。

Result: 提出的方法能够有效识别不同类型的困难实例，并通过工具实现对数据或模型问题的分析。案例研究验证了方法的有效性。

Conclusion: 通过引入实例难度评估，可以更全面地理解模型的表现，并帮助识别潜在的数据或模型问题，提升模型分析的深度。

Abstract: Traditional instance-based model analysis focuses mainly on misclassified
instances. However, this approach overlooks the varying difficulty associated
with different instances. Ideally, a robust model should recognize and reflect
the challenges presented by intrinsically difficult instances. It is also
valuable to investigate whether the difficulty perceived by the model aligns
with that perceived by humans. To address this, we propose incorporating
instance difficulty into the deep neural network evaluation process,
specifically for supervised classification tasks on image data. Specifically,
we consider difficulty measures from three perspectives -- data, model, and
human -- to facilitate comprehensive evaluation and comparison. Additionally,
we develop an interactive visual tool, DifficultyEyes, to support the
identification of instances of interest based on various difficulty patterns
and to aid in analyzing potential data or model issues. Case studies
demonstrate the effectiveness of our approach.

</details>


### [39] [Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception](https://arxiv.org/abs/2507.00963)
*Fan Wang,Giulia Perugia,Yuan Feng,Wijnand IJsselsteijn*

Main category: cs.HC

TL;DR: 综述分析了26项关于痴呆患者与社交机器人互动的研究，识别了可能引发误导性印象的四类设计线索，并基于双重认知处理理论提出了机器人欺骗的定义。


<details>
  <summary>Details</summary>
Motivation: 探讨社交机器人在痴呆护理中可能引发的欺骗性问题，特别是在设计线索如何影响痴呆患者的感知方面。

Method: 通过范围综述，系统性分析了26项实证研究，采用主题分析法归纳了用户反应。

Result: 研究发现四类设计线索（生理信号、社会意图、熟悉形象和揭示人工性的线索）可能导致欺骗性印象，痴呆患者对机器人的感知在意识和错觉之间动态变化。

Conclusion: 从双重认知处理角度定义了机器人欺骗，强调设计应兼顾趣味性和认知尊重，以减少伦理问题。

Abstract: As social robots increasingly enter dementia care, concerns about deception,
intentional or not, are gaining attention. Yet, how robotic design cues might
elicit misleading perceptions in people with dementia, and how these
perceptions arise, remains insufficiently understood. In this scoping review,
we examined 26 empirical studies on interactions between people with dementia
and physical social robots. We identify four key design cue categories that may
influence deceptive impressions: cues resembling physiological signs (e.g.,
simulated breathing), social intentions (e.g., playful movement), familiar
beings (e.g., animal-like form and sound), and, to a lesser extent, cues that
reveal artificiality. Thematic analysis of user responses reveals that people
with dementia often attribute biological, social, and mental capacities to
robots, dynamically shifting between awareness and illusion. These findings
underscore the fluctuating nature of ontological perception in dementia
contexts. Existing definitions of robotic deception often rest on philosophical
or behaviorist premises, but rarely engage with the cognitive mechanisms
involved. We propose an empirically grounded definition: robotic deception
occurs when Type 1 (automatic, heuristic) processing dominates over Type 2
(deliberative, analytic) reasoning, leading to misinterpretation of a robot's
artificial nature. This dual-process perspective highlights the ethical
complexity of social robots in dementia care and calls for design approaches
that are not only engaging, but also epistemically respectful.

</details>


### [40] [A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models](https://arxiv.org/abs/2507.01017)
*Xingyu Xiao,Hongxu Zhu,Jingang Liang,Jiejuan Tong,Haitao Wang*

Main category: cs.HC

TL;DR: 该论文探讨了人类错误在关键安全领域的影响，并提出通过结合风险决策、人因可靠性评估、AI和认知科学来减少风险。


<details>
  <summary>Details</summary>
Motivation: 人类错误在核能、航空和医疗等领域可能导致灾难性后果，现有方法的局限性促使研究如何结合新技术提升预测和应对能力。

Method: 论文综述了错误分类、风险决策框架、认知模型及AI技术，提出整合这些方法以改进人因可靠性评估。

Result: 研究发现，结合认知模型与AI分析能显著提高预测准确性，但需更多数据和透明算法支持。

Conclusion: 未来研究方向包括结合韧性工程理论、建立跨领域数据联盟，以提升高风险系统中的人类可靠性。

Abstract: Human error remains a dominant risk driver in safety-critical sectors such as
nuclear power, aviation, and healthcare, where seemingly minor mistakes can
cascade into catastrophic outcomes. Although decades of research have produced
a rich repertoire of mitigation techniques, persistent limitations: scarce
high-quality data, algorithmic opacity, and residual reliance on expert
judgment, continue to constrain progress. This review synthesizes recent
advances at the intersection of risk-informed decision making, human
reliability assessment (HRA), artificial intelligence (AI), and cognitive
science to clarify how their convergence can curb human-error risk. We first
categorize the principal forms of human error observed in complex
sociotechnical environments and outline their quantitative impact on system
reliability. Next, we examine risk-informed frameworks that embed HRA within
probabilistic and data-driven methodologies, highlighting successes and gaps.
We then survey cognitive and human-performance models, detailing how
mechanistic accounts of perception, memory, and decision-making enrich error
prediction and complement HRA metrics. Building on these foundations, we
critically assess AI-enabled techniques for real-time error detection,
operator-state estimation, and AI-augmented HRA workflows. Across these
strands, a recurring insight emerges: integrating cognitive models with
AI-based analytics inside risk-informed HRA pipelines markedly enhances
predictive fidelity, yet doing so demands richer datasets, transparent
algorithms, and rigorous validation. Finally, we identify promising research
directions, coupling resilience engineering concepts with grounded theory,
operationalizing the iceberg model of incident causation, and establishing
cross-domain data consortia, to foster a multidisciplinary paradigm that
elevates human reliability in high-stakes systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [41] [MVGBench: Comprehensive Benchmark for Multi-view Generation Models](https://arxiv.org/abs/2507.00006)
*Xianghui Xie,Chuhang Zou,Meher Gitika Karumuri,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.GR

TL;DR: MVGBench是一个用于评估多视角图像生成模型（MVGs）的综合基准，重点关注3D几何和纹理一致性、图像质量和语义。通过引入3D自一致性指标和系统比较12种现有MVGs，揭示了现有方法的局限性，并提出了性能更优的ViFiGen。


<details>
  <summary>Details</summary>
Motivation: 现有MVGs评估方法依赖与真实目标视图的对比，不适合生成任务中多解决方案的情景，且缺乏对真实数据泛化和鲁棒性的全面评估，需要一个更严谨的评估协议。

Method: 提出MVGBench基准，包括性能最佳设置、泛化能力和鲁棒性评估，引入3D自一致性指标替代真实视图对比，并在4个合成和真实数据集上系统比较12种MVGs。

Result: 发现了现有方法在鲁棒性和泛化能力上的主要局限性，确定了关键设计选择，并提出了ViFiGen方法，在3D一致性上优于所有评估模型。

Conclusion: MVGBench为MVGs提供了更全面的评估框架，揭示了现有方法的不足，并通过ViFiGen验证了其有效性，代码和模型将公开。

Abstract: We propose MVGBench, a comprehensive benchmark for multi-view image
generation models (MVGs) that evaluates 3D consistency in geometry and texture,
image quality, and semantics (using vision language models). Recently, MVGs
have been the main driving force in 3D object creation. However, existing
metrics compare generated images against ground truth target views, which is
not suitable for generative tasks where multiple solutions exist while
differing from ground truth. Furthermore, different MVGs are trained on
different view angles, synthetic data and specific lightings -- robustness to
these factors and generalization to real data are rarely evaluated thoroughly.
Without a rigorous evaluation protocol, it is also unclear what design choices
contribute to the progress of MVGs. MVGBench evaluates three different aspects:
best setup performance, generalization to real data and robustness. Instead of
comparing against ground truth, we introduce a novel 3D self-consistency metric
which compares 3D reconstructions from disjoint generated multi-views. We
systematically compare 12 existing MVGs on 4 different curated real and
synthetic datasets. With our analysis, we identify important limitations of
existing methods specially in terms of robustness and generalization, and we
find the most critical design choices. Using the discovered best practices, we
propose ViFiGen, a method that outperforms all evaluated MVGs on 3D
consistency. Our code, model, and benchmark suite will be publicly released.

</details>


### [42] [ViscoReg: Neural Signed Distance Functions via Viscosity Solutions](https://arxiv.org/abs/2507.00412)
*Meenakshi Krishnan,Ramani Duraiswami*

Main category: cs.GR

TL;DR: 研究者提出了一种名为ViscoReg的新损失函数，用于稳定训练隐式神经表示（INR）中的符号距离函数（SDF），并在理论上和实验上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Eikonal方程可用于训练神经SDF，但其不适定性可能导致不稳定的梯度流，需要额外的稳定技术。

Method: 通过引入基于粘性理论的新损失函数ViscoReg，增强了神经SDF的训练稳定性。

Result: ViscoReg在理论上证明了梯度流方程的稳定性，并在实验中超越了SIREN、DiGS和StEik等先进方法。

Conclusion: ViscoReg为神经SDF训练提供了一种高效且稳定的解决方案，且未显著增加计算成本。

Abstract: Implicit Neural Representations (INRs) that learn a Signed Distance Function
(SDF) are a powerful tool for continuous 3D scene reconstruction. These models
are trained by enforcing the Eikonal equation. We demonstrate theoretically
that despite the ill-posedness of the Eikonal equation, generalization error
estimates may be obtained for Neural SDFs in terms of the training error.
However, training with the Eikonal loss can lead to unstable gradient flows,
necessitating alternate stabilization techniques. Traditional numerical solvers
for the equation have relied on viscosity approaches for regularization. We
enhance Neural SDF training using this well-developed theory, and introduce a
new loss formulation we call ViscoReg. We theoretically demonstrate the
stability of the gradient flow equation of our proposed loss term. Empirically,
ViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik
without adding significant computational cost.

</details>


### [43] [FreNBRDF: A Frequency-Rectified Neural Material Representation](https://arxiv.org/abs/2507.00476)
*Chenliang Zhou,Zheyuan Hu,Cengiz Oztireli*

Main category: cs.GR

TL;DR: 论文提出FreNBRDF，一种基于频率校正的神经材料表示方法，通过球谐函数将频域考虑融入神经BRDF建模，提升了材料外观重建和编辑的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经表示方法在频域行为上研究不足，论文旨在通过频域校正提升材料建模的逼真度和适应性。

Method: 利用球谐函数设计频率校正损失函数，并集成到通用且自适应的重建与编辑流程中。

Result: 实验表明FreNBRDF在材料外观重建和编辑中优于现有方法，提升了下游任务的结构化和可解释性。

Conclusion: FreNBRDF通过频域校正实现了更高质量的材料建模，为后续应用提供了更灵活和可靠的框架。

Abstract: Accurate material modeling is crucial for achieving photorealistic rendering,
bridging the gap between computer-generated imagery and real-world photographs.
While traditional approaches rely on tabulated BRDF data, recent work has
shifted towards implicit neural representations, which offer compact and
flexible frameworks for a range of tasks. However, their behavior in the
frequency domain remains poorly understood. To address this, we introduce
FreNBRDF, a frequency-rectified neural material representation. By leveraging
spherical harmonics, we integrate frequency-domain considerations into neural
BRDF modeling. We propose a novel frequency-rectified loss, derived from a
frequency analysis of neural materials, and incorporate it into a generalizable
and adaptive reconstruction and editing pipeline. This framework enhances
fidelity, adaptability, and efficiency. Extensive experiments demonstrate that
\ours improves the accuracy and robustness of material appearance
reconstruction and editing compared to state-of-the-art baselines, enabling
more structured and interpretable downstream tasks and applications.

</details>


### [44] [Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory](https://arxiv.org/abs/2507.00725)
*Amritendu Dhar,Apratim Chakraborty,Vijay Natarajan*

Main category: cs.GR

TL;DR: 该论文将莫尔斯-塞尔夫理论推广到分段线性函数族，引入了顶点图和Cerf图来表示临界点的演化，并提出了时间变化标量场的拓扑描述符、Cerf图的计算算法及其比较方法。


<details>
  <summary>Details</summary>
Motivation: 将经典的莫尔斯-塞尔夫理论应用于分段线性函数族，以更好地理解时间变化标量场中临界点的演化。

Method: 通过顶点图和Cerf图描述临界点的演变，提出基于顶点下链接同调的拓扑描述符，并开发计算Cerf图和比较它们的算法。

Result: 实验验证了所提方法在时间变化标量场中的有效性和应用价值。

Conclusion: 该研究为分析动态标量场的拓扑结构提供了新工具，扩展了莫尔斯-塞尔夫理论的应用范围。

Abstract: Morse-Cerf theory considers a one-parameter family of smooth functions
defined on a manifold and studies the evolution of their critical points with
the parameter. This paper presents an adaptation of Morse-Cerf theory to a
family of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram
are introduced as representations of the evolution of critical points of the PL
function. The characterization of a crossing in the vertex diagram based on the
homology of the lower links of vertices leads to the definition of a
topological descriptor for time-varying scalar fields. An algorithm for
computing the Cerf diagram and a measure for comparing two Cerf diagrams are
also described together with experimental results on time-varying scalar
fields.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [45] [Origin-Destination Travel Demand Estimation: An Approach That Scales Worldwide, and Its Application to Five Metropolitan Highway Networks](https://arxiv.org/abs/2507.00306)
*Chao Zhang,Neha Arora,Christopher Bian,Yechen Li,Willa Ng,Andrew Tomkins,Bin Yan,Janny Zhang,Carolina Osorio*

Main category: cs.ET

TL;DR: 提出了一种利用谷歌地图交通趋势数据直接估计OD需求的方法，避免了传统方法对城市特定先验数据的需求，验证了其在多个城市网络中的高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于高精度交通数据稀缺和城市特定先验OD数据难以获取，传统OD需求估计方法面临挑战，需要一种通用的解决方案。

Method: 通过非线性优化问题估计OD需求，利用可微分的宏观网络模型，减少校准负担并提高计算效率。

Result: 在洛杉矶和圣地亚哥验证中，方法显著提升了数据拟合度；在多个城市网络中，比基准方法在高峰时段拟合旅行时间数据平均提高了13%。

Conclusion: 该方法具有广泛的适用性和实用性，能够在缺乏传统数据的情况下高效估计OD需求。

Abstract: Estimating Origin-Destination (OD) travel demand is vital for effective urban
planning and traffic management. Developing universally applicable OD
estimation methodologies is significantly challenged by the pervasive scarcity
of high-fidelity traffic data and the difficulty in obtaining city-specific
prior OD estimates (or seed ODs), which are often prerequisite for traditional
approaches. Our proposed method directly estimates OD travel demand by
systematically leveraging aggregated, anonymized statistics from Google Maps
Traffic Trends, obviating the need for conventional census or city-provided OD
data. The OD demand is estimated by formulating a single-level,
one-dimensional, continuous nonlinear optimization problem with nonlinear
equality and bound constraints to replicate highway path travel times. The
method achieves efficiency and scalability by employing a differentiable
analytical macroscopic network model. This model by design is computationally
lightweight, distinguished by its parsimonious parameterization that requires
minimal calibration effort and its capacity for instantaneous evaluation. These
attributes ensure the method's broad applicability and practical utility across
diverse cities globally. Using segment sensor counts from Los Angeles and San
Diego highway networks, we validate our proposed approach, demonstrating a
two-thirds to three-quarters improvement in the fit to segment count data over
a baseline. Beyond validation, we establish the method's scalability and robust
performance in replicating path travel times across diverse highway networks,
including Seattle, Orlando, Denver, Philadelphia, and Boston. In these expanded
evaluations, our method not only aligns with simulation-based benchmarks but
also achieves an average 13% improvement in it's ability to fit travel time
data compared to the baseline during afternoon peak hours.

</details>


### [46] [DiffCkt: A Diffusion Model-Based Hybrid Neural Network Framework for Automatic Transistor-Level Generation of Analog Circuits](https://arxiv.org/abs/2507.00444)
*Chengjie Liu,Jiajia Li,Yabing Feng,Wenhao Huang,Weiyu Chen,Yuan Du,Jun Yang,Li Du*

Main category: cs.ET

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Analog circuit design consists of the pre-layout and layout phases. Among
them, the pre-layout phase directly decides the final circuit performance, but
heavily depends on experienced engineers to do manual design according to
specific application scenarios. To overcome these challenges and automate the
analog circuit pre-layout design phase, we introduce DiffCkt: a diffusion
model-based hybrid neural network framework for the automatic transistor-level
generation of analog circuits, which can directly generate corresponding
circuit structures and device parameters tailored to specific performance
requirements. To more accurately quantify the efficiency of circuits generated
by DiffCkt, we introduce the Circuit Generation Efficiency Index (CGEI), which
is determined by both the figure of merit (FOM) of a single generated circuit
and the time consumed. Compared with relative research, DiffCkt has improved
CGEI by a factor of $2.21 \sim 8365\times$, reaching a state-of-the-art (SOTA)
level. In conclusion, this work shows that the diffusion model has the
remarkable ability to learn and generate analog circuit structures and device
parameters, providing a revolutionary method for automating the pre-layout
design of analog circuits. The circuit dataset will be open source, its preview
version is available at https://github.com/CjLiu-NJU/DiffCkt.

</details>


### [47] [Robust Task Offloading for UAV-enabled Secure MEC Against Aerial Eavesdropper](https://arxiv.org/abs/2507.00710)
*Can Cui,ZIye Jia,Chao Dong,Qihui Wu*

Main category: cs.ET

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unmanned aerial vehicles (UAVs) are recognized as a promising candidate for
the multi-access edge computing (MEC) in the future sixth generation
communication networks. However, the aerial eavesdropping UAVs (EUAVs) pose a
significant security threat to the data offloading. In this paper, we
investigate a robust MEC scenario with multiple service UAVs (SUAVs) towards
the potential eavesdropping from the EUAV, in which the random parameters such
as task complexities are considered in the practical applications. In detail,
the problem is formulated to optimize the deployment positions of SUAVs, the
connection relationships between GUs and SUAVs, and the offloading ratios. With
the uncertain task complexities, the corresponding chance constraints are
constructed under the uncertainty set, which is tricky to deal with. Therefore,
we first optimize the pre-deployment of SUAVs by the K-means algorithm. Then,
the distributionally robust optimization method is employed, and the
conditional value at risk is utilized to transform the chance constraints into
convex forms, which can be solved via convex toolkits. Finally, the simulation
results show that with the consideration of uncertainties, just 5% more energy
is consumed compared with the ideal circumstance, which verifies the robustness
of the proposed algorithms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training](https://arxiv.org/abs/2507.00217)
*Tiancheng Chen,Ales Kubicek,Langwen Huang,Torsten Hoefler*

Main category: cs.DC

TL;DR: CrossPipe框架通过优化跨数据中心的大语言模型训练，缓解网络延迟和带宽限制，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练需要超越单一数据中心的资源，跨数据中心策略变得至关重要。

Method: CrossPipe结合流水线并行和数据并行通信优化，采用解耦调度逻辑与通信细节的灵活执行引擎，提供最优或近似最优的调度算法。

Result: 在相同内存限制下，CrossPipe减少训练时间达33.6%；在放宽内存限制时，仍保持高效。

Conclusion: CrossPipe在高速网络延迟或有限带宽环境中，显著提升扩展性和资源利用率。

Abstract: Training large language models (LLMs) now requires resources that exceed a
single datacenter, making cross-datacenter strategies increasingly crucial. We
present CrossPipe, a framework designed to optimize model training across
geographically distributed datacenters by explicitly modeling and mitigating
the impact of network latency and limited bandwidth. It enables unified
analysis and optimization incorporating both pipeline parallelism (PP) and
opportunities for overlapping data parallelism (DP) communication. CrossPipe
generates optimized pipeline schedules using either solver-based optimal or
fast near-optimal greedy algorithms, built upon a flexible execution engine
that separates scheduling logic from communication details. Our evaluation
shows that CrossPipe reduces training time by up to 33.6\% compared to
traditional pipeline schedules under identical memory constraints. When memory
constraints are relaxed, CrossPipe maintains strong performance despite
communication delays, approaching the efficiency of idealized schedules without
delays. CrossPipe offers improved scalability and resource utilization,
particularly in environments with high network latency or limited bandwidth.

</details>


### [49] [Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs](https://arxiv.org/abs/2507.00418)
*Mohammad Firas Sada,John J. Graham,Elham E Khoda,Mahidhar Tatineni,Dmitry Mishin,Rajesh K. Gupta,Rick Wagner,Larry Smarr,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100
Ultra (QAic) accelerator for large language model (LLM) inference, evaluating
its energy efficiency (throughput per watt) and performance against leading
NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform
(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90
billion parameters, are served using the vLLM framework. The QAic inference
cards appears to be energy efficient and performs well in the energy efficiency
metric in most cases. The findings offer insights into the potential of the
Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications
within the National Research Platform (NRP).

</details>


### [50] [Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor](https://arxiv.org/abs/2507.00428)
*Mohammad Firas Sada,John J. Graham,Mahidhar Tatineni,Dmitry Mishin,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: 探讨如何利用P4编程范式在FPGA SmartNICs上实现低延迟的神经网络和回归模型，以支持网络边缘的机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在网络操作中的应用日益重要，需要一种能够在网络边缘实现低延迟ML推理的可编程平台。

Method: 利用P4-programmable FPGA SmartNICs，将ML模型的权重和偏置存储在控制平面表查找中，实现动态重构和高效部署。

Result: 提出的方法能够灵活编程并高效部署可重训练的ML模型，不依赖于核心基础设施。

Conclusion: P4编程范式在FPGA SmartNICs上的应用为实现网络边缘的机器学习提供了可行且高效的解决方案。

Abstract: As machine learning (ML) applications become integral to modern network
operations, there is an increasing demand for network programmability that
enables low-latency ML inference for tasks such as Quality of Service (QoS)
prediction and anomaly detection in cybersecurity. ML models provide
adaptability through dynamic weight adjustments, making Programming
Protocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an
ideal platform for investigating In-Network Machine Learning (INML). These
devices offer high-throughput, low-latency packet processing and can be
dynamically reconfigured via the control plane, allowing for flexible
integration of ML models directly at the network edge. This paper explores the
application of the P4 programming paradigm to neural networks and regression
models, where weights and biases are stored in control plane table lookups.
This approach enables flexible programmability and efficient deployment of
retrainable ML models at the network edge, independent of core infrastructure
at the switch level.

</details>


### [51] [PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds](https://arxiv.org/abs/2507.00824)
*Matthieu Pigaglio,Onur Ascigil,Michał Król,Sergi Rene,Felix Lange,Kaleem Peeroo,Ramin Sadre,Vladimir Stankovic,Etienne Rivière*

Main category: cs.DC

TL;DR: 介绍了PANDAS方案，用于在Danksharding框架下高效支持Ethereum的数据可用性采样，满足4秒内完成分发与采样的严格要求。


<details>
  <summary>Details</summary>
Motivation: 解决现有方案无法在Ethereum的严格时间限制内支持DAS的问题，提升Layer-2数据的分发与采样效率。

Method: 采用轻量级直接交换机制，设计时考虑了消息丢失、节点故障和参与者无响应等场景，同时支持网络扩展。

Result: 在1000节点集群和20000节点的模拟中，PANDAS能在4秒内完成全球延迟下的数据分发与采样。

Conclusion: PANDAS是一种实用的DAS集成方案，无需修改Ethereum的共识协议，即可满足Danksharding的时间要求。

Abstract: Layer-2 protocols can assist Ethereum's limited throughput, but globally
broadcasting layer-2 data limits their scalability. The Danksharding evolution
of Ethereum aims to support the selective distribution of layer-2 data, whose
availability in the network is verified using randomized data availability
sampling (DAS). Integrating DAS into Ethereum's consensus process is
challenging, as pieces of layer-2 data must be disseminated and sampled within
four seconds of the beginning of each consensus slot. No existing solution can
support dissemination and sampling under such strict time bounds.
  We propose PANDAS, a practical approach to integrate DAS with Ethereum under
Danksharding's requirements without modifying its protocols for consensus and
node discovery. PANDAS disseminates layer-2 data and samples its availability
using lightweight, direct exchanges. Its design accounts for message loss, node
failures, and unresponsive participants while anticipating the need to scale
out the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node
cluster and simulations for up to 20,000 peers shows that it allows layer-2
data dissemination and sampling under planetary-scale latencies within the
4-second deadline.

</details>


### [52] [LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference](https://arxiv.org/abs/2507.00507)
*Chuhao Xu,Zijun Li,Quan Chen,Han Zhao,Minyi Guo*

Main category: cs.DC

TL;DR: LLM-Mesh是一种针对中小型LLM的服务器无感知推理方案，通过异构硬件的弹性共享提升服务能力。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案通常独占GPU部署，而新兴CPU架构未被充分利用，且CPU和GPU均可同时服务多个LLM。

Method: 提出LLM-Mesh，解决资源分配的三大挑战：细粒度计算资源分配、内存扩展机制及资源碎片化减少。

Result: 实验表明，LLM-Mesh通过资源共享提升服务能力44%-63%，结合CPU后提升至91%-159%。

Conclusion: LLM-Mesh显著提升资源利用率，为中小型LLM的高效部署提供了新思路。

Abstract: The rise of LLMs has driven demand for private serverless deployments,
characterized by moderate-scale models and infrequent requests. While existing
solutions follow exclusive GPU deployment, we take a step back to explore
modern platforms and find that: Emerging CPU architectures with built-in
accelerators are capable of serving LLMs but remain underutilized, and both
CPUs and GPUs can accommodate multiple LLMs simultaneously.
  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized
LLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh
tackles three fundamental challenges: (1) precise, fine-grained compute
resource allocation at token-level to handle fluctuating computational demands;
(2) a coordinated and forward-looking memory scaling mechanism to detect
out-of-memory hazards and reduce operational overhead; and (3) a dual approach
that reduces resource fragmentation through proactive preemption and reactive
bin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that
LLM-Meshimproves service capacity by 44% - 63% through sharing, while further
leveraging CPUs boosts this to 91% - 159%.

</details>


### [53] [A New Family of Thread to Core Allocation Policies for an SMT ARM Processor](https://arxiv.org/abs/2507.00855)
*Marta Navarro,Josué Feliu,Salvador Petit,María E. Gómez,Julio Sahuquillo*

Main category: cs.DC

TL;DR: 本文提出了在ARM处理器中构建性能堆栈的新方法（ISC堆栈），并基于此开发了SYNPA系列线程到核心分配策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决SMT处理器中应用间干扰导致的性能挑战，需要更精确的线程到核心分配策略。

Method: 通过构建性能堆栈（如ISC堆栈）和性能预测模型，开发了SYNPA系列分配策略。

Result: 最佳策略SYNPA4将周转时间提升了38%，远超现有技术。

Conclusion: 该方法不仅适用于ARM处理器，还可推广到其他SMT处理器，为性能分析提供指导。

Abstract: Modern high-performance servers commonly integrate Simultaneous
Multithreading (SMT) processors, which efficiently boosts throughput over
single-threaded cores. Optimizing performance in SMT processors faces
challenges due to the inter-application interference within each SMT core. To
mitigate the interference, thread-to-core (T2C) allocation policies play a
pivotal role. State-of-the-art T2C policies work in two steps: i) building a
per-application performance stack using performance counters and ii) building
performance prediction models to identify the best pairs of applications to run
on each core.
  This paper explores distinct ways to build the performance stack in ARM
processors and introduces the Instructions and Stalls Cycles (ISC) stack, a
novel approach to overcome ARM PMU limitations. The ISC stacks are used as
inputs for a performance prediction model to estimate the applications'
performance considering the inter-application interference. The accuracy of the
prediction model (second step) depends on the accuracy of the performance stack
(first step); thus, the higher the accuracy of the performance stack, the
higher the potential performance gains obtained by the T2C allocation policy.
  This paper presents SYNPA as a family of T2C allocation policies.
Experimental results show that $SYNPA4$, the best-performing SYNPA variant,
outperforms turnaround time by 38\% over Linux, which represents 3$\times$ the
gains achieved by the state-of-the-art policies for ARM processors.
Furthermore, the multiple discussions and refinements presented throughout this
paper can be applied to other SMT processors from distinct vendors and are
aimed at helping performance analysts build performance stacks for accurate
performance estimates in real processors.

</details>


### [54] [Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona](https://arxiv.org/abs/2507.00909)
*Philip Colangelo,Ayse K. Coskun,Jack Megrue,Ciaran Roberts,Shayan Sengupta,Varun Sivaram,Ethan Tiao,Aroon Vijaykar,Chris Williams,Daniel C. Wilson,Zack MacFarland,Daniel Dreiling,Nathan Morey,Anuja Ratnayake,Baskar Vairamohan*

Main category: cs.DC

TL;DR: Emerald Conductor软件解决方案通过实时电网信号协调AI工作负载，在不改变硬件或增加储能的情况下，将AI数据中心转化为灵活电网资源，试验中实现了25%的功耗降低。


<details>
  <summary>Details</summary>
Motivation: AI快速发展导致电力需求激增，威胁电网可靠性，增加基础设施成本，阻碍AI创新。

Method: 与合作企业共同开发的Emerald Conductor软件，通过实时电网信号协调AI工作负载，无需硬件改造或储能。

Result: 在256-GPU集群的试验中，峰值电网事件期间功耗降低25%，同时保持AI服务质量。

Conclusion: 该平台将数据中心重新构想为电网互动资产，提升电网可靠性、降低成本并推动AI发展。

Abstract: Artificial intelligence (AI) is fueling exponential electricity demand
growth, threatening grid reliability, raising prices for communities paying for
new energy infrastructure, and stunting AI innovation as data centers wait for
interconnection to constrained grids. This paper presents the first field
demonstration, in collaboration with major corporate partners, of a
software-only approach--Emerald Conductor--that transforms AI data centers into
flexible grid resources that can efficiently and immediately harness existing
power systems without massive infrastructure buildout. Conducted at a 256-GPU
cluster running representative AI workloads within a commercial, hyperscale
cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in
cluster power usage for three hours during peak grid events while maintaining
AI quality of service (QoS) guarantees. By orchestrating AI workloads based on
real-time grid signals without hardware modifications or energy storage, this
platform reimagines data centers as grid-interactive assets that enhance grid
reliability, advance affordability, and accelerate AI's development.

</details>


### [55] [Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling](https://arxiv.org/abs/2507.00550)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 该论文提出了一种基于多代理系统的弹性云资源扩展优化方法，通过分布式决策和协作价值函数实现全局协调，并结合轻量级状态预测模型增强系统预见性。在典型云场景评估中，该方法在资源利用率、SLA违规控制和调度延迟方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 应对云计算环境中资源快速变化和任务负载高度不确定性的挑战，寻求提高资源调度响应性和系统性能的新方法。

Method: 采用多代理系统进行并行资源感知和局部决策，引入协作价值函数实现全局协调，并结合轻量级状态预测模型和集中训练分散执行的强化学习框架。

Result: 实验结果表明，该方法在资源利用率、SLA违规控制和调度延迟方面优于现有方法，表现出较强的适应性和智能调节能力。

Conclusion: 该研究为复杂云平台中的弹性资源扩展问题提供了一种高效可靠的新解决方案。

Abstract: This paper addresses the challenges of rapid resource variation and highly
uncertain task loads in cloud computing environments. It proposes an
optimization method for elastic cloud resource scaling based on a multi-agent
system. The method deploys multiple autonomous agents to perceive resource
states in parallel and make local decisions. While maintaining the distributed
nature of the system, it introduces a collaborative value function to achieve
global coordination. This improves the responsiveness of resource scheduling
and enhances overall system performance. To strengthen system foresight, a
lightweight state prediction model is designed. It assists agents in
identifying future workload trends and optimizes the selection of scaling
actions. For policy training, the method adopts a centralized training and
decentralized execution reinforcement learning framework. This enables agents
to learn effectively and coordinate strategies under conditions of incomplete
information. The paper also constructs typical cloud scenarios, including
multi-tenancy and burst traffic, to evaluate the proposed method. The
evaluation focuses on resource isolation, service quality assurance, and
robustness. Experimental results show that the proposed multi-agent scaling
strategy outperforms existing methods in resource utilization, SLA violation
control, and scheduling latency. The results demonstrate strong adaptability
and intelligent regulation. This provides an efficient and reliable new
approach to solving the problem of elastic resource scaling in complex cloud
platforms.

</details>


### [56] [DynoStore: A wide-area distribution system for the management of data over heterogeneous storage](https://arxiv.org/abs/2507.00576)
*Dante D. Sanchez-Gallegos,J. L. Gonzalez-Compean,Maxime Gonthier,Valerie Hayot-Sasson,J. Gregory Pauloski,Haochen Pan,Kyle Chard,Jesus Carretero,Ian Foster*

Main category: cs.DC

TL;DR: DynoStore是一个跨异构存储系统的数据管理系统，通过数据容器实现统一管理，性能提升10%，具备更强的容错能力。


<details>
  <summary>Details</summary>
Motivation: 分布式数据管理存在协议、认证和协调框架的异构性问题，需要一种统一解决方案。

Method: DynoStore利用数据容器抽象和负载均衡算法，构建广域存储网络，采用纠删码策略确保弹性。

Result: 相比集中式系统性能提升10%，与Redis和IPFS竞争，容错能力更强。

Conclusion: DynoStore有效解决了跨存储系统的数据管理挑战，性能与容错表现优异。

Abstract: Data distribution across different facilities offers benefits such as
enhanced resource utilization, increased resilience through replication, and
improved performance by processing data near its source. However, managing such
data is challenging due to heterogeneous access protocols, disparate
authentication models, and the lack of a unified coordination framework. This
paper presents DynoStore, a system that manages data across heterogeneous
storage systems. At the core of DynoStore are data containers, an abstraction
that provides standardized interfaces for seamless data management,
irrespective of the underlying storage systems. Multiple data container
connections create a cohesive wide-area storage network, ensuring resilience
using erasure coding policies. Furthermore, a load-balancing algorithm ensures
equitable and efficient utilization of storage resources. We evaluate DynoStore
using benchmarks and real-world case studies, including the management of
medical and satellite data across geographically distributed environments. Our
results demonstrate a 10\% performance improvement compared to centralized
cloud-hosted systems while maintaining competitive performance with
state-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits
superior fault tolerance, withstanding more failures than traditional systems.

</details>


### [57] [How Fast Can Graph Computations Go on Fine-grained Parallel Architectures](https://arxiv.org/abs/2507.00949)
*Yuqing Wang,Charles Colley,Brian Wheatman,Jiya Su,David F. Gleich,Andrew A. Chien*

Main category: cs.DC

TL;DR: 论文探讨了在大规模图计算问题中，通过细粒度并行架构实现高性能的可能性，并提出了UpDown架构。通过PageRank和BFS基准测试，展示了其显著优于先前方法的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模图计算问题日益重要，但传统并行架构对此支持不足。论文旨在探索细粒度并行架构在高性能图计算中的潜力。

Method: 提出了一种名为UpDown的细粒度图架构，针对图计算的特性（不规则性和偏斜）优化，并通过PageRank和BFS两种基准算法进行验证。

Result: 在模拟实验中，UpDown架构在256节点（524,288 lanes）上表现出色，PageRank和BFS的性能分别达到637K和989K GTEPS，远超先前最佳结果。

Conclusion: UpDown架构通过细粒度并行设计和自然编程，为大规模图计算提供了高性能解决方案，展示了硬件与算法协同设计的潜力。

Abstract: Large-scale graph problems are of critical and growing importance and
historically parallel architectures have provided little support. In the spirit
of co-design, we explore the question, How fast can graph computing go on a
fine-grained architecture? We explore the possibilities of an architecture
optimized for fine-grained parallelism, natural programming, and the
irregularity and skew found in real-world graphs. Using two graph benchmarks,
PageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph
architecture, UpDown, to explore what performance codesign can achieve. To
demonstrate programmability, we wrote five variants of these algorithms.
Simulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes
(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS
on RMAT, exceeding the best prior results by 5x and 100x respectively.

</details>


### [58] [Accelerating Loading WebGraphs in ParaGrapher](https://arxiv.org/abs/2507.00716)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: ParaGrapher is a graph loading API and library that enables graph processing
frameworks to load large-scale compressed graphs with minimal overhead. This
capability accelerates the design and implementation of new high-performance
graph algorithms and their evaluation on a wide range of graphs and across
different frameworks. However, our previous study identified two major
limitations in ParaGrapher: inefficient utilization of high-bandwidth storage
and reduced decompression bandwidth due to increased compression ratios. To
address these limitations, we present two optimizations for ParaGrapher in this
paper. To improve storage utilization, particularly for high-bandwidth storage,
we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE
(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the
size of requested blocks, reducing the number of calls to the underlying
filesystem, and caching the received blocks in memory for future calls. To
improve the decompression bandwidth, we introduce CompBin, a compact binary
representation of the CSR format. CompBin facilitates direct accesses to
neighbors while preventing storage usage for unused bytes. Our evaluation on 12
real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse
and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [59] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

TL;DR: 论文挑战了在丰富的数据感知声明式过程模型中计算最优对齐的难题，提出了一种结合A*搜索和SMT求解的新算法，实现了高效与表达性的双赢。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要关注控制流或有限数据扩展，而缺乏对通用数据类型和条件的数据感知支持，限制了实际应用。本文旨在解决这一差距。

Method: 提出一种结合A*搜索和SMT求解的新算法，通过修复动作逐步解决约束冲突，高效探索搜索空间。

Result: 算法在保持高效的同时支持更丰富的数据依赖，实验证明其性能优于或匹配现有技术。

Conclusion: 该方法为数据感知对齐问题提供了可扩展且高效的解决方案，展示了其在实际应用中的潜力。

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


### [60] [LIMAO: A Framework for Lifelong Modular Learned Query Optimization](https://arxiv.org/abs/2507.00188)
*Qihan Zhang,Shaolin Xie,Ibrahim Sabek*

Main category: cs.DB

TL;DR: 论文介绍了一种名为LIMAO的学习型查询优化器框架，解决了现有方法在动态查询环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有学习型查询优化器在静态查询环境中表现良好，但在动态环境中存在局限性，尤其是灾难性遗忘问题。LIMAO旨在解决这一问题。

Method: LIMAO采用了模块化终身学习技术、基于注意力的神经网络组合架构和高效训练范式，以保留先验知识并适应新环境。

Result: 实验表明，LIMAO显著提升了学习型查询优化器的性能，查询执行时间最多减少40%，执行时间方差最多降低60%。

Conclusion: LIMAO通过其精确和自洽的设计，有效缓解了灾难性遗忘问题，并在实际查询优化中表现出显著的性能优势，与Postgres相比实现了高达4倍的加速。

Abstract: Query optimizers are crucial for the performance of database systems.
Recently, many learned query optimizers (LQOs) have demonstrated significant
performance improvements over traditional optimizers. However, most of them
operate under a limited assumption: a static query environment. This limitation
prevents them from effectively handling complex, dynamic query environments in
real-world scenarios. Extensive retraining can lead to the well-known
catastrophic forgetting problem, which reduces the LQO generalizability over
time. In this paper, we address this limitation and introduce LIMAO (Lifelong
Modular Learned Query Optimizer), a framework for lifelong learning of plan
cost prediction that can be seamlessly integrated into existing LQOs. LIMAO
leverages a modular lifelong learning technique, an attention-based neural
network composition architecture, and an efficient training paradigm designed
to retain prior knowledge while continuously adapting to new environments. We
implement LIMAO in two LQOs, showing that our approach is agnostic to
underlying engines. Experimental results show that LIMAO significantly enhances
the performance of LQOs, achieving up to a 40% improvement in query execution
time and reducing the variance of execution time by up to 60% under dynamic
workloads. By leveraging a precise and self-consistent design, LIMAO
effectively mitigates catastrophic forgetting, ensuring stable and reliable
plan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup
on selected benchmarks, highlighting its practical advantages in real-world
query optimization.

</details>


### [61] [Meaningful Data Erasure in the Presence of Dependencies](https://arxiv.org/abs/2507.00343)
*Vishal Chakraborty,Youri Kaminsky,Sharad Mehrotra,Felix Naumann,Faisal Nawab,Primal Pappachan,Mohammad Sadoghi,Nalini Venkatasubramanian*

Main category: cs.DB

TL;DR: 论文提出了一种精确的数据擦除定义和机制，确保从依赖关系中推断已删除数据的范围受限于插入前的状态，同时优化了成本和吞吐量。


<details>
  <summary>Details</summary>
Motivation: GDPR等数据法规要求系统支持数据擦除，但未能明确定义“擦除”的含义，尤其是在存在数据依赖的数据库中，这导致合规性挑战。

Method: 作者提出了一种形式化的数据擦除定义，设计了相应的擦除机制，并探索了优化成本和吞吐量的策略，包括批量擦除和主动计算数据保留时间。

Result: 通过真实和合成数据集的实验，证明了所提算法的实用性和可扩展性。

Conclusion: 研究为数据擦除提供了一种精确且高效的解决方案，解决了法规模糊性和数据依赖性带来的挑战。

Abstract: Data regulations like GDPR require systems to support data erasure but leave
the definition of "erasure" open to interpretation. This ambiguity makes
compliance challenging, especially in databases where data dependencies can
lead to erased data being inferred from remaining data. We formally define a
precise notion of data erasure that ensures any inference about deleted data,
through dependencies, remains bounded to what could have been inferred before
its insertion. We design erasure mechanisms that enforce this guarantee at
minimal cost. Additionally, we explore strategies to balance cost and
throughput, batch multiple erasures, and proactively compute data retention
times when possible. We demonstrate the practicality and scalability of our
algorithms using both real and synthetic datasets.

</details>


### [62] [Towards Robustness: A Critique of Current Vector Database Assessments](https://arxiv.org/abs/2507.00379)
*Zikai Wang,Qianxi Zhang,Baotong Lu,Qi Chen,Cheng Tan*

Main category: cs.DB

TL;DR: 论文指出，依赖平均召回率评估向量数据库存在问题，并提出新指标Robustness-δ@K以改进评估方式。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库评估过于依赖平均召回率，忽视了尾部查询的变异性，可能导致下游应用失败。

Method: 提出Robustness-δ@K指标，衡量查询中召回率超过阈值δ的比例，并集成到现有基准测试中评估主流向量索引。

Result: 研究发现不同向量索引的鲁棒性差异显著，鲁棒性更高的索引即使平均召回率相同也能提升应用性能。

Conclusion: 新指标Robustness-δ@K能更全面评估向量数据库性能，并揭示设计因素对鲁棒性的影响，为实际应用提供优化指导。

Abstract: Vector databases are critical infrastructure in AI systems, and average
recall is the dominant metric for their evaluation. Both users and researchers
rely on it to choose and optimize their systems. We show that relying on
average recall is problematic. It hides variability across queries, allowing
systems with strong mean performance to underperform significantly on hard
queries. These tail cases confuse users and can lead to failure in downstream
applications such as RAG. We argue that robustness consistently achieving
acceptable recall across queries is crucial to vector database evaluation. We
propose Robustness-$\delta$@K, a new metric that captures the fraction of
queries with recall above a threshold $\delta$. This metric offers a deeper
view of recall distribution, helps vector index selection regarding application
needs, and guides the optimization of tail performance. We integrate
Robustness-$\delta$@K into existing benchmarks and evaluate mainstream vector
indexes, revealing significant robustness differences. More robust vector
indexes yield better application performance, even with the same average
recall. We also identify design factors that influence robustness, providing
guidance for improving real-world performance.

</details>


### [63] [Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition](https://arxiv.org/abs/2507.00427)
*Hao Wu,Changzheng Wei,Yanhao Wang,Li Lin,Yilong Leng,Shiyu He,Minghao Zhao,Hanghang Wu,Ying Yan,Aoying Zhou*

Main category: cs.DB

TL;DR: 该论文研究了如何实现图数据库的零知识可验证性，通过将复杂查询分解为更细粒度的操作符，利用优化的ZKP电路提高了性能和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 图数据库的查询复杂性和数据隐私需求促使作者探索零知识验证的实现方法，以解决传统关系数据库中类似技术的不足。

Method: 提出将图查询分解为更细粒度的原始操作符，并设计基于PLONKish算法的优化ZKP电路，特别是围绕图扩展操作设计专用电路。

Result: 通过实现的ZKGraph系统，验证了分解和优化方法在运行时和内存消耗上的显著改进。

Conclusion: 研究证明了通过分解和优化ZKP电路，可以高效实现图数据库的零知识可验证查询处理。

Abstract: This paper investigates the feasibility of achieving zero-knowledge
verifiability for graph databases, enabling database owners to
cryptographically prove the query execution correctness without disclosing the
underlying data. Although similar capabilities have been explored for
relational databases, their implementation for graph databases presents unique
challenges. This is mainly attributed to the relatively large complexity of
queries in graph databases. When translating graph queries into arithmetic
circuits, the circuit scale can be too large to be practically evaluated. To
address this issue, we propose to break down graph queries into more
fine-grained, primitive operators, enabling a step-by-step evaluation through
smaller-scale circuits. Accordingly, the verification with ZKP circuits of
complex graph queries can be decomposed into a series of composable
cryptographic primitives, each designed to verify a fundamental structural
property such as path ordering or edge directionality. Especially, having
noticed that the graph expansion (i.e., traversing from nodes to their
neighbors along edges) operation serves as the backbone of graph query
evaluation, we design the expansion centric operator decomposition. In addition
to constructing circuits for the expansion primitives, we also design
specialized ZKP circuits for the various attributes that augment this
traversal. The circuits are meticulously designed to take advantage of PLONKish
arithmetization. By integrating these optimized circuits, we implement ZKGraph,
a system that provides verifiable query processing while preserving data
privacy. Performance evaluation indicates that ZKGraph significantly
outperforms naive in circuit implementations of graph operators, achieving
substantial improvements in both runtime and memory consumption.

</details>


### [64] [Towards Efficient Random-Order Enumeration for Join Queries](https://arxiv.org/abs/2507.00489)
*Pengyu Chen,Zizheng Guo,Jianwei Yang,Dongjing Miao*

Main category: cs.DB

TL;DR: 提出了一种高效的随机顺序枚举算法，用于处理连接查询，具备接近最优的最坏情况性能，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地以随机顺序枚举连接查询结果，以解决现有方法在大规模或复杂查询下的效率不足问题。

Method: 开发了一种无需查询特定预处理、复杂度可控的随机顺序枚举算法，并引入了两项加速技术优化性能。

Result: 算法实现了理论上的高效性能（预期延迟和总运行时间），并通过实验证明其显著优于现有方法。

Conclusion: 本文提出的算法在随机顺序枚举连接查询结果方面具有高效性和适应性，为数据分析和数据库操作提供了实用工具。

Abstract: In many data analysis pipelines, a basic and time-consuming process is to
produce join results and feed them into downstream tasks. Numerous enumeration
algorithms have been developed for this purpose. To be a statistically
meaningful representation of the whole join result, the result tuples are
required to be enumerated in uniformly random order. However, existing studies
lack an efficient random-order enumeration algorithm with a worst-case runtime
guarantee for (cyclic) join queries. In this paper, we study the problem of
enumerating the results of a join query in random order. We develop an
efficient random-order enumeration algorithm for join queries with no large
hidden constants in its complexity, achieving expected
$O(\frac{\mathrm{AGM}(Q)}{|Res(Q)|}\log^2|Q|)$ delay,
$O(\mathrm{AGM}(Q)\log|Q|)$ total running time after $O(|Q|\log|Q|)$-time index
construction, where $|Q|$ is the size of input, $\mathrm{AGM}(Q)$ is the AGM
bound, and $|Res(Q)|$ is the size of the join result. We prove that our
algorithm is near-optimal in the worst case, under the combinatorial $k$-clique
hypothesis. Our algorithm requires no query-specific preprocessing and can be
flexibly adapted to many common database indexes with only minor modifications.
We also devise two non-trivial techniques to speed up the enumeration, and
provide an experimental study on our enumeration algorithm along with the
speed-up techniques. The experimental results show that our algorithm, enhanced
with the proposed techniques, significantly outperforms existing
state-of-the-art methods.

</details>


### [65] [RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries](https://arxiv.org/abs/2507.00839)
*Chiyu Hao,Jixian Su,Shixuan Sun,Hao Zhang,Sen Gao,Jianwen Zhao,Chenyi Zhang,Jieru Zhao,Chen Chen,Minyi Guo*

Main category: cs.DB

TL;DR: 提出了RapidStore，一种高效的动态图存储系统，解决了现有方法在并发读写操作中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 动态图存储系统在实时应用中至关重要，但现有方法存在写查询干扰读效率、版本管理开销大以及并发负载下性能不平衡的问题。

Method: 设计RapidStore，通过解耦系统设计分离读写查询，并分离版本数据和图数据，结合动态图存储与并发控制机制。

Result: 实验表明RapidStore能快速、可扩展地支持并发图查询，平衡插入、搜索和扫描性能。

Conclusion: RapidStore显著提高了动态图存储系统的效率。

Abstract: Dynamic graph storage systems are essential for real-time applications such
as social networks and recommendation, where graph data continuously evolves.
However, they face significant challenges in efficiently handling concurrent
read and write operations. We find that existing methods suffer from write
queries interfering with read efficiency, substantial time and space overhead
due to per-edge versioning, and an inability to balance performance, such as
slow searches under concurrent workloads. To address these issues, we propose
RapidStore, a holistic approach for efficient in-memory dynamic graph storage
designed for read-intensive workloads. Our key idea is to exploit the
characteristics of graph queries through a decoupled system design that
separates the management of read and write queries and decouples version data
from graph data. Particularly, we design an efficient dynamic graph store to
cooperate with the graph concurrency control mechanism. Experimental results
demonstrate that RapidStore enables fast and scalable concurrent graph queries,
effectively balancing the performance of inserts, searches, and scans, and
significantly improving efficiency in dynamic graph storage systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [Presto: Hardware Acceleration of Ciphers for Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.00367)
*Yeonsoo Jeon,Mattan Erez,Michael Orshansky*

Main category: cs.AR

TL;DR: 该论文开发了针对HERA和Rubato这两种CKKS目标HHE密码的硬件加速器，通过优化设计显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 结合对称密钥和同态加密的混合同态加密（HHE）在客户端-服务器部署中需要高性能且能效高的实现。

Method: 设计了向量化和重叠功能模块，利用MixColumns和MixRows的转置不变性，优化密钥生成流程，隐藏RNG延迟，减少关键路径。

Result: 在FPGA上实现的加速器比软件实现分别提升了6倍吞吐量，Rubato和HERA的延迟分别降低了5倍和3倍，能耗分别减少了75倍和47倍。

Conclusion: 硬件加速器显著提升了HHE密码的性能和能效，适用于实际部署。

Abstract: Hybrid Homomorphic Encryption (HHE) combines symmetric key and homomorphic
encryption to reduce ciphertext expansion crucial in client-server deployments
of HE. Special symmetric ciphers, amenable to efficient HE evaluation, have
been developed. Their client-side deployment calls for performant and
energy-efficient implementation, and in this paper we develop and evaluate
hardware accelerators for the two known CKKS-targeting HHE ciphers, HERA and
Rubato.
  We design vectorized and overlapped functional modules. The design exploits
transposition-invariance property of the MixColumns and MixRows function and
alternates the order of intermediate state to eliminate bubbles in stream key
generation, improving latency and throughput. We decouple the RNG and key
computation phases to hide the latency of RNG and to reduce the critical path
in FIFOs, achieving higher operating frequency.
  We implement the accelerator on an AMD Virtex UltraScale+ FPGA. Both Rubato
and HERA achieve a 6x improvement in throughput compared to the software
implementation. In terms of latency, Rubato achieves a 5x reduction, while HERA
achieves a 3x reduction. Additionally, our hardware implementations reduce
energy consumption by 75x for Rubato and 47x for HERA compared to their
software implementation.

</details>


### [67] [ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis](https://arxiv.org/abs/2507.00642)
*Runkai Li,Jia Xiong,Xiuyuan He,Jieru Zhao,Qiang Xu,Xi Wang*

Main category: cs.AR

TL;DR: ChatHLS利用LLMs技术实现了硬件设计自动化与优化，显著提升了修复率与性能加速。


<details>
  <summary>Details</summary>
Motivation: 传统硬件设计周期长，HLS面临编码约束大且优化复杂的挑战，LLMs潜力未被充分利用。

Method: 提出ChatHLS，结合微调LLMs和多代理框架进行错误修正与设计优化。

Result: 修复通过率82.7%，性能提升1.9-14.8倍，速度相比现有方法提升4.9倍。

Conclusion: ChatHLS能显著缩短硬件开发周期，同时确保设计可靠性与优化质量。

Abstract: The increasing complexity of computational demands has accelerated the
adoption of domain-specific accelerators, yet traditional hardware design
methodologies remain constrained by prolonged development and verification
cycles. High-Level Synthesis (HLS) bridges the gap between software and
hardware by enabling hardware design from high-level programming languages.
However, its widespread adoption is hindered by strict coding constraints and
intricate hardware-specific optimizations, creating significant obstacles for
developers. Recent advancements in Large Language Models (LLMs) demonstrate
substantial potential in hardware design automation. However, their
effectiveness is limited by the scarcity of high-quality datasets, particularly
in the context of HLS. To address these challenges, we introduce ChatHLS, an
agile HLS design automation and optimization workflow that leverages fine-tuned
LLMs integrated within a multi-agent framework for error correction and design
optimization. Our extensive evaluations reveal that ChatHLS achieves an average
repair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and
Llama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers
performance enhancements ranging from 1.9$\times$ to 14.8$\times$ upon
resource-constrained kernels. By enabling sophisticated optimization reasoning
within practical computational budgets, ChatHLS attains a 4.9$\times$ geometric
mean speedup compared to state-of-the-art DSL-based approaches. These results
underscore the potential of ChatHLS in substantially expediting hardware
development cycles while maintaining rigorous standards of design reliability
and optimization quality.

</details>


### [68] [VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction and Dataflow-flexible Accelerator](https://arxiv.org/abs/2507.00797)
*Zhican Wang,Hongxiang Fan,Haroon Waris,Gang Wang,Zhenyu Li,Jianfei Jiang,Yanan Sun,Guanghui He*

Main category: cs.AR

TL;DR: 论文提出了一种针对大语言模型（LLM）在边缘设备上高效推理的算法-硬件-数据流三优化方法，显著降低了延迟和硬件复杂度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和内存需求高，难以在资源受限的边缘设备上部署。研究旨在优化LLM推理效率，解决资源限制问题。

Method: 提出了一种基于投票的KV缓存淘汰算法、灵活的数据流设计、运行时可重构的PE阵列，以及针对非线性操作的串行调度方案。

Result: 方法显著减少了延迟，硬件复杂度从O(N)降至O(1)，并在定制加速器VEDA中实现了优于现有平台的性能。

Conclusion: 研究推动了LLM在边缘设备上的高效推理，支持实时处理、增强数据隐私并促进模型定制化。

Abstract: Large Language Models (LLMs) excel in natural language processing tasks but
pose significant computational and memory challenges for edge deployment due to
their intensive resource demands. This work addresses the efficiency of LLM
inference by algorithm-hardware-dataflow tri-optimizations. We propose a novel
voting-based KV cache eviction algorithm, balancing hardware efficiency and
algorithm accuracy by adaptively identifying unimportant kv vectors. From a
dataflow perspective, we introduce a flexible-product dataflow and a runtime
reconfigurable PE array for matrix-vector multiplication. The proposed approach
effectively handles the diverse dimensional requirements and solves the
challenges of incrementally varying sequence lengths. Additionally, an
element-serial scheduling scheme is proposed for nonlinear operations, such as
softmax and layer normalization (layernorm). Results demonstrate a substantial
reduction in latency, accompanied by a significant decrease in hardware
complexity, from O(N) to O(1). The proposed solution is realized in a
custom-designed accelerator, VEDA, which outperforms existing hardware
platforms. This research represents a significant advancement in LLM inference
on resource-constrained edge devices, facilitating real-time processing,
enhancing data privacy, and enabling model customization.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 论文研究了语言模型在简单语法任务（如生成平衡括号）中的错误机制，并提出了一种名为RASteer的方法，通过增强可靠组件的贡献显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在编码能力上取得了显著进展，但在简单语法任务中仍存在错误。研究旨在理解并减少这些错误。

Method: 通过分析不同大小语言模型的组件（注意力头和前馈神经元），发现可靠和不可靠组件对预测的影响。提出RASteer方法，增强可靠组件的贡献。

Result: RASteer显著提升了平衡括号任务的准确性（从0%到约100%），并在算术推理任务中实现了约20%的性能提升。

Conclusion: 研究表明，通过增强可靠组件的作用可以有效改善模型的性能，且RASteer方法具有广泛的适用性。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [70] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: TransLaw是一种基于LLM的多代理框架，用于翻译香港法律判决，通过三个专业代理协作提高准确性，并在成本上优于人工翻译。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLM在翻译香港法律判决中的潜力，解决其面临的复杂法律术语和文化差异等挑战。

Method: 采用Translator、Annotator和Proofreader三个代理协作完成翻译，支持自定义LLM配置。

Result: TransLaw在语义准确性、结构连贯性和风格忠实度上超越GPT-4o，但在复杂术语和风格自然度上不及人类专家。

Conclusion: TransLaw为法律翻译提供了一种高效且成本低的解决方案，但仍需在术语和自然度上进一步改进。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [Plug. Play. Persist. Inside a Ready-to-Go Havoc C2 Infrastructure](https://arxiv.org/abs/2507.00189)
*Alessio Di Santo*

Main category: cs.CR

TL;DR: 该论文分析了攻击者如何利用一台Azure虚拟机作为集分发、中转和命令控制于一体的节点，展示了攻击技术和工具链的详细信息。


<details>
  <summary>Details</summary>
Motivation: 研究攻击者如何利用公共云服务和已知漏洞进行攻击，以及其技术特点和目标。

Method: 分析攻击者使用的工具和技术，包括钓鱼手段、PowerShell脚本、Reflective-Loader技术和Havoc Demon植入等。

Result: 发现攻击者技术娴熟，但操作安全性不足，依赖合法云服务隐藏恶意流量。

Conclusion: 攻击者技术高效但缺乏深度隐蔽性，需加强云服务和漏洞管理以防范类似攻击。

Abstract: This analysis focuses on a single Azure-hosted Virtual Machine at
52.230.23.114 that the adversary converted into an all-in-one delivery, staging
and Command-and-Control node. The host advertises an out-of-date Apache 2.4.52
instance whose open directory exposes phishing lures, PowerShell loaders,
Reflective Shell-Code, compiled Havoc Demon implants and a toolbox of
lateral-movement binaries; the same server also answers on 8443/80 for
encrypted beacon traffic. The web tier is riddled with publicly documented
critical vulnerabilities, that would have allowed initial code-execution had
the attackers not already owned the device.
  Initial access is delivered through an HTML file that, once de-obfuscated,
perfectly mimics Google Unusual sign-in attempt notification and funnels
victims toward credential collection. A PowerShell command follows: it disables
AMSI in-memory, downloads a Base64-encoded stub, allocates RWX pages and starts
the shell-code without ever touching disk. That stub reconstructs a DLL in
memory using the Reflective-Loader technique and hands control to Havoc Demon
implant. Every Demon variant-32- and 64-bit alike-talks to the same backend,
resolves Windows APIs with hashed look-ups, and hides its activity behind
indirect syscalls.
  Runtime telemetry shows interests in registry under Image File Execution
Options, deliberate queries to Software Restriction Policy keys, and heavy use
of Crypto DLLs to protect payloads and C2 traffic. The attacker toolkit further
contains Chisel, PsExec, Doppelganger and Whisker, some of them re-compiled
under user directories that leak the developer personas tonzking123 and thobt.
Collectively the findings paint a picture of a technically adept actor who
values rapid re-tooling over deep operational security, leaning on Havoc
modularity and on legitimate cloud services to blend malicious flows into
ordinary enterprise traffic.

</details>


### [72] [AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise](https://arxiv.org/abs/2507.00145)
*Hasan Yiğit*

Main category: cs.CR

TL;DR: AI-Hybrid TRNG是一种深度学习框架，直接从物理噪声中提取接近均匀的熵，避免了昂贵或笨重的硬件依赖。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种低成本、高效的随机数生成方法，摆脱对专用硬件的依赖，适用于多种场景。

Method: 结合低成本的射频前端和CPU时序抖动进行训练，动态内外网络自适应自然源和重播种。

Result: 生成的随机数通过NIST SP 800-22测试和19项定制统计测试，满足密码学标准，无预测性偏差，模型体积小。

Conclusion: 该框架为资源受限的平台提供了高质量随机数生成方案，扩展了随机数应用的范围。

Abstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform
entropy directly from physical noise, eliminating the need for bulky quantum
devices or expensive laboratory-grade RF receivers. Instead, it relies on a
low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and
then emits 32-bit high-entropy streams without any quantization step.
  Unlike deterministic or trained artificial intelligence random number
generators (RNGs), our dynamic inner-outer network couples adaptive natural
sources and reseeding, yielding truly unpredictable and autonomous sequences.
Generated numbers pass the NIST SP 800-22 battery better than a CPU-based
method. It also passes nineteen bespoke statistical tests for both bit- and
integer-level analysis. All results satisfy cryptographic standards, while
forward and backward prediction experiments reveal no exploitable biases. The
model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft
cores, as well as suitable for other resource-constrained platforms.
  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG
broadens the reach of high-integrity random number generators across secure
systems, cryptographic protocols, embedded and edge devices, stochastic
simulations, and server applications that need randomness.

</details>


### [73] [Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing](https://arxiv.org/abs/2507.00847)
*Keiichiro Kimura,Hiroki Kuzuno,Yoshiaki Shiraishi,Masakatu Morii*

Main category: cs.CR

TL;DR: Stealtooth攻击利用商业蓝牙设备自动配对功能中的漏洞，实现静默覆盖设备连接密钥，揭示了安全性与便利性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 研究商业蓝牙设备自动配对的潜在安全隐患，揭示其带来的安全风险。

Method: 利用蓝牙音频设备在特定条件下自动进入配对模式的特点，设计Stealtooth攻击和MitM Stealtooth攻击。

Result: 在10种商业蓝牙设备上验证攻击有效性，证明漏洞广泛存在。

Conclusion: 提出设备和协议层面的防御措施，并已向相关厂商披露，部分已发布补丁。

Abstract: Bluetooth is a pervasive wireless communication technology used by billions
of devices for short-range connectivity. The security of Bluetooth relies on
the pairing process, where devices establish shared long-term keys for secure
communications. However, many commercial Bluetooth devices implement automatic
pairing functions to improve user convenience, creating a previously unexplored
attack surface.
  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in
the automatic pairing functions in commercial Bluetooth devices to achieve
completely silent device link key overwriting. The Stealtooth attack leverages
the fact that Bluetooth audio devices automatically transition to pairing mode
under specific conditions, enabling attackers to hijack pairing processes
without user awareness or specialized tools. We also extend the attack into the
MitM Stealtooth attack, combining automatic pairing abuse with power-saving
mode techniques to enable man-in-the-middle attacks.
  We evaluate the attacks against 10 commercial Bluetooth devices from major
manufacturers, demonstrating widespread vulnerabilities across diverse device
types and manufacturers. Our practical implementation requires only commodity
hardware and open-source software, highlighting the low barrier to entry for
attackers.
  We propose defenses both device and protocol levels, including enhanced user
notifications and standardized automatic pairing guidelines. Our findings
reveal a critical tension between security and usability, showing that current
automatic pairing implementations create systematic vulnerabilities. We
responsibly disclosed our findings to affected vendors, with several already
releasing patches.

</details>


### [74] [The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)](https://arxiv.org/abs/2507.00595)
*Linard Arquint,Samarth Kishor,Jason R. Koenig,Joey Dodds,Daniel Kroening,Peter Müller*

Main category: cs.CR

TL;DR: Diodon方法通过将代码库分为核心（Core）和应用（Application）两部分，实现对安全关键核心的半自动化验证，以及全自动静态分析对整个代码库的扩展验证。


<details>
  <summary>Details</summary>
Motivation: 现有程序验证器难以扩展到大型代码库，因为需要大量手动工作。Diodon旨在通过分离核心和应用程序，减少手动验证的需求，提高验证效率。

Method: Diodon将代码库分为核心和应用程序，核心部分采用半自动化验证技术，应用程序部分通过静态分析实现自动验证，确保其I/O操作独立于核心的安全数据。

Result: 在两个案例（Diffie-Hellman密钥交换和大型Go代码库）中验证了Diodon的有效性，通过验证1%的核心代码，在三个月内实现了安全和协议保证。

Conclusion: Diodon成功解决了大规模代码库的验证难题，通过分离核心和应用，结合半自动化和全自动化技术，显著提高了验证效率和可扩展性。

Abstract: Existing program verifiers can prove advanced properties about security
protocol implementations, but are difficult to scale to large codebases because
of the manual effort required. We develop a novel methodology called *Diodon*
that addresses this challenge by splitting the codebase into the protocol
implementation (the *Core*) and the remainder (the *Application*). This split
allows us to apply powerful semi-automated verification techniques to the
security-critical Core, while fully-automatic static analyses scale the
verification to the entire codebase by ensuring that the Application cannot
invalidate the security properties proved for the Core. The static analyses
achieve that by proving *I/O independence*, i.e., that the I/O operations
within the Application are independent of the Core's security-relevant data
(such as keys), and that the Application meets the Core's requirements. We have
proved Diodon sound by first showing that we can safely allow the Application
to perform I/O independent of the security protocol, and second that manual
verification and static analyses soundly compose. We evaluate Diodon on two
case studies: an implementation of the signed Diffie-Hellman key exchange and a
large (100k+ LoC) production Go codebase implementing a key exchange protocol
for which we obtained secrecy and injective agreement guarantees by verifying a
Core of about 1% of the code with the auto-active program verifier Gobra in
less than three person months.

</details>


### [75] [Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning](https://arxiv.org/abs/2507.00423)
*Wenjin Mo,Zhiyuan Li,Minghong Fang,Mingwei Fang*

Main category: cs.CR

TL;DR: FedPoisonMIA是一种针对联邦学习的新型成员推断攻击，通过恶意客户端发送有害数据来推断成员信息。同时，提出了防御机制以减轻攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性使其容易受到中毒攻击，现有研究多关注模型完整性的损害，而忽视了隐私问题。因此，需要研究针对成员推断的中毒攻击及其防御方法。

Method: 提出FedPoisonMIA攻击，恶意客户端通过精心设计的本地模型更新推断成员信息。同时，设计了一种防御机制来减轻攻击的影响。

Result: 实验表明FedPoisonMIA在多个数据集上有效，而防御机制能够显著降低攻击的影响。

Conclusion: 研究揭示了联邦学习中隐私攻击的新威胁，并提供了有效的防御手段。

Abstract: Federated learning (FL) allows multiple clients to collaboratively train a
global machine learning model with coordination from a central server, without
needing to share their raw data. This approach is particularly appealing in the
era of privacy regulations like the GDPR, leading many prominent companies to
adopt it. However, FL's distributed nature makes it susceptible to poisoning
attacks, where malicious clients, controlled by an attacker, send harmful data
to compromise the model. Most existing poisoning attacks in FL aim to degrade
the model's integrity, such as reducing its accuracy, with limited attention to
privacy concerns from these attacks. In this study, we introduce FedPoisonMIA,
a novel poisoning membership inference attack targeting FL. FedPoisonMIA
involves malicious clients crafting local model updates to infer membership
information. Additionally, we propose a robust defense mechanism to mitigate
the impact of FedPoisonMIA attacks. Extensive experiments across various
datasets demonstrate the attack's effectiveness, while our defense approach
reduces its impact to a degree.

</details>


### [76] [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
*Craig S Wright*

Main category: cs.CR

TL;DR: 该论文对简化的支付验证（SPV）进行了完整的规范化和数学证明，澄清了其安全性并反驳了常见误解。


<details>
  <summary>Details</summary>
Motivation: 旨在纠正流行的SPV实现中的错误描述，证明其在数字现金系统中的安全性和最优性。

Method: 基于符号自动机、Merkle成员关系和链证明优势谓词，重建SPV协议，并通过概率和博弈论分析验证其安全性。

Result: SPV在部分连接、敌对网络和传播延迟等条件下具有活性和安全性，同时优化了低带宽需求。

Conclusion: 该研究为SPV的安全实现提供了蓝图，并反驳了对非验证客户端的常见误解。

Abstract: This paper presents a complete formal specification, protocol description,
and mathematical proof structure for Simplified Payment Verification (SPV) as
originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark
contrast to the misrepresentations proliferated by popular implementations, we
show that SPV is not only secure under bounded adversarial assumptions but
strictly optimal for digital cash systems requiring scalable and verifiable
transaction inclusion. We reconstruct the SPV protocol from first principles,
grounding its verification model in symbolic automata, Merkle membership
relations, and chain-of-proof dominance predicates. Through rigorous
probabilistic and game-theoretic analysis, we derive the economic bounds within
which the protocol operates securely and verify its liveness and safety
properties under partial connectivity, hostile relay networks, and adversarial
propagation delay. Our specification further introduces low-bandwidth
optimisations such as adaptive polling and compressed header synchronisation
while preserving correctness. This document serves both as a blueprint for
secure SPV implementation and a rebuttal of common misconceptions surrounding
non-validating clients.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: 论文提出了DS3框架，通过技能图随机遍历优化LLM推理效率，分析了不同推理策略的成本与任务难度关系，统一了训练与推理的优化框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练和部署时消耗大量资源，现有方法未能全面优化推理成本，需新框架平衡效率与性能。

Method: 提出DS3框架，以技能图随机遍历建模推理，推导任务成功与计算成本的闭式表达式，结合训练与推理的优化。

Result: 理论验证了经验规律（如对数计算下的线性精度扩展），统一了BoN和多数投票行为，支持任务难度与模型能力的动态策略选择。

Conclusion: DS3框架通过联合优化训练与推理，为算法设计和资源分配提供了理论基础。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [78] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: LiSER是一个知识蒸馏框架，利用未标记的音频-视觉数据和大型教师模型，为轻量级学生模型提供情感识别能力，减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 由于人类通过多模态（音频-视觉）表达情感，开发基于多模态的情感识别系统（SER）是有益的，但标注大量数据成本高昂。

Method: 提出LiSER框架，利用未标记的音频-视觉数据，通过大型教师模型（基于先进的语音和面部表示模型）向轻量级学生模型传递知识。

Result: 在RAVDESS和CREMA-D数据集上的实验表明，LiSER可以减少SER任务对大量标注数据的依赖。

Conclusion: LiSER通过知识蒸馏和未标记数据，有效提升了轻量级模型的情感识别能力，降低了数据标注成本。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [79] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: NeutroSENSE是一个基于中性逻辑的集成框架，用于物联网中的可解释入侵检测，通过分解预测置信度为真、假和不确定部分来提高精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决物联网环境中入侵检测的不确定性和可解释性问题，以支持更可信的AI决策。

Method: 结合随机森林、XGBoost和逻辑回归，并使用中性逻辑分解预测置信度为T、F、I三个部分，设置全局和自适应阈值进行样本审核。

Result: 在IoT-CAD数据集上达到97%的准确性，且错误分类样本的I值显著高于正确分类样本。

Conclusion: 中性逻辑提升了精度和可解释性，为边缘和雾计算中的物联网安全提供了实用基础。

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [80] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种结合时域建模和频域特征提取的新型网络流量预测与异常检测模型MamNet，实验结果显示其在复杂流量模式和长期趋势检测中表现优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 网络流量的异常波动可能预示安全威胁或系统故障，因此高效的流量预测与异常检测方法对网络安全和流量管理至关重要。

Method: MamNet通过Mamba模块（时域建模）捕捉流量的长期依赖关系，并通过傅里叶变换（频域特征提取）识别周期性波动，多尺度特征融合进一步提升异常检测能力。

Result: 在UNSW-NB15和CAIDA数据集上的实验表明，MamNet在准确率、召回率和F1分数上优于其他模型，复杂流量模式的检测性能提升约2%至4%。

Conclusion: MamNet能有效捕捉不同时间尺度的流量异常，适用于网络安全和流量管理，未来可通过整合外部网络事件信息进一步优化模型。

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [81] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: SWE-Bench-CL 是一个基于时间顺序的持续学习基准测试，用于评估大型语言模型在动态软件开发环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型在静态代码生成任务上表现优秀，但缺乏对动态软件开发的持续适应能力。为了解决这一问题，研究团队开发了SWE-Bench-CL基准测试。

Method: 研究团队从GitHub问题中构建了时间顺序的数据集，并通过分析任务间的结构相似性和上下文敏感性，开发了一个评估框架和一系列持续学习指标。

Result: SWE-Bench-CL提供了一个可复现的平台，支持评估代理在知识积累、跨任务转移和抵抗灾难性遗忘方面的能力。

Conclusion: SWE-Bench-CL为开发更适应动态软件工程的AI代理提供了重要工具和基准。

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [82] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: HelixPipe是一种新型的流水线并行方法，用于优化长序列Transformer训练，通过并行注意力计算和内存优化策略，显著提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的流水线并行方法在处理长序列Transformer时性能不足，主要由于二次注意力计算和高内存开销，HelixPipe旨在解决这些问题。

Method: HelixPipe采用注意力并行分区、两阶段微批次调度，以及无注意力重计算和分块MLP技术，以减少流水线气泡并优化内存使用。

Result: 实验表明，HelixPipe在长序列训练中表现优异，比基线方法在7B模型和128k序列长度下实现了26%的速度提升。

Conclusion: HelixPipe通过创新的并行和调度策略，显著提升了长序列Transformer的训练效率和可扩展性，代码已开源。

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [83] [Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives](https://arxiv.org/abs/2507.00108)
*Clemente Rubio-Manzano,Jazna Meza,Rodolfo Fernandez-Santibanez,Christian Vidal-Castro*

Main category: cs.CY

TL;DR: 论文探讨了在生成式AI背景下编程教学的转变，提出通过代码理解和执行来增强教学方法，并提倡使用可视化工具。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI工具的兴起，编程教学需要重新思考如何教授、学习和评估，以适应这一变革。

Method: 回顾相关研究，提出将教学重点从编码转移到代码理解和执行，并采用可视化工具如代码的可视化表示和执行模拟。

Result: 文章通过学生反馈初步支持了在教学中引入可视化模拟（如Java）的有效性。

Conclusion: 可视化工具可以加深学生对编程的理解，是生成式AI时代编程教学的有益补充。

Abstract: Computer programming is undergoing a true transformation driven by powerful
new tools for automatic source code generation based on large language models.
This transformation is also manifesting in introductory programming courses at
universities around the world, generating an in-depth debate about how
programming content should be taught, learned, and assessed in the context of
generative artificial intelligence.
  This article aims, on the one hand, to review the most relevant studies on
this issue, highlighting the advantages and disadvantages identified in the
specialized literature. On the other hand, it proposes enriching teaching and
learning methodologies by focusing on code comprehension and execution rather
than on mere coding or program functionality. In particular, it advocates for
the use of visual representations of code and visual simulations of its
execution as effective tools for teaching, learning, and assessing programming,
thus fostering a deeper understanding among students.
  Finally, the opinions of students who took the object-oriented programming
course are presented to provide preliminary context supporting the
incorporation of visual simulations in Java (or other languages) as part of the
training process.

</details>


### [84] [Intellectual Property Rights and Entrepreneurship in the NFT Ecosystem: Legal Frameworks, Business Models, and Innovation Opportunities](https://arxiv.org/abs/2507.00172)
*Pranav Darshan,Rohan J S,Raghuveer Rajesh,Ruchitha M,Sanika Kamath,Manas M N*

Main category: cs.CY

TL;DR: NFT市场快速增长但面临知识产权管理问题，研究通过混合方法分析传统版权法与区块链交易的差距，提出新IP权利矩阵和商业模式分类。


<details>
  <summary>Details</summary>
Motivation: 探索NFT生态系统中知识产权管理的混乱，明确NFT所有权与版权之间的差异。

Method: 采用混合方法，分析法律案例、智能合约及利益相关者访谈，创建IP权利矩阵和商业模式分类。

Result: 发现跨地区执法、许可标准化和商业机会评估中的关键问题。

Conclusion: 研究结果为NFT生态系统中的知识产权管理提供了清晰框架，有助于未来法律和商业发展。

Abstract: Non Fungible Tokens have changed digital ownership and how creators earn
money. Between 2021 and 2024, the market value exceeded 40 billion. However,
the fast growth of the NFT ecosystem has revealed serious issues in managing
intellectual property rights. There is a lot of confusion about the difference
between owning an NFT and owning the copyright for the underlying content. This
research looks at the gap between traditional copyright laws and
blockchain-based transactions. We use a mixed methods approach to analyze this
disconnect. We create a new IP rights matrix that clearly shows how copyright
law relates to NFT ownership structures. Additionally, we include a business
model taxonomy that sorts new commercial applications by their IP risk and
sustainability factors. By examining important legal cases, smart contracts,
and interviews with stakeholders, we find key problems in enforcing laws across
different regions, standardizing licenses, and assessing business
opportunities.

</details>


### [85] [Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools](https://arxiv.org/abs/2507.00456)
*Deepak Varuvel Dennison,Bakhtawar Ahtisham,Kavyansh Chourasia,Nirmit Arora,Rahul Singh,Rene F. Kizilcec,Akshay Nambi,Tanuja Ganu,Aditya Vashistha*

Main category: cs.CY

TL;DR: 研究调查了印度卡纳塔克邦政府学校中使用的AI辅助教案工具Shiksha copilot，分析了教师与AI协作的效果及挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何支持教师在多语言、资源匮乏的环境中生成情境敏感的教案，并评估AI内容质量和教学实践的转变。

Method: 采用混合方法研究，涉及1,043名教师和23名策划者，结合LLMs与人类专业知识共同制定教案。

Result: Shiksha copilot减轻了行政负担，缩短了备课时间，降低了压力，并促进了活动式教学的转变，但仍受限于系统性挑战。

Conclusion: 研究提出了教师为中心的EdTech设计方向，强调AI工具需结合教师需求和多语言环境特点。

Abstract: This study investigates Shiksha copilot, an AI-assisted lesson planning tool
deployed in government schools across Karnataka, India. The system combined
LLMs and human expertise through a structured process in which English and
Kannada lesson plans were co-created by curators and AI; teachers then further
customized these curated plans for their classrooms using their own expertise
alongside AI support. Drawing on a large-scale mixed-methods study involving
1,043 teachers and 23 curators, we examine how educators collaborate with AI to
generate context-sensitive lesson plans, assess the quality of AI-generated
content, and analyze shifts in teaching practices within multilingual,
low-resource environments. Our findings show that teachers used Shiksha copilot
both to meet administrative documentation needs and to support their teaching.
The tool eased bureaucratic workload, reduced lesson planning time, and lowered
teaching-related stress, while promoting a shift toward activity-based
pedagogy. However, systemic challenges such as staffing shortages and
administrative demands constrained broader pedagogical change. We frame these
findings through the lenses of teacher-AI collaboration and communities of
practice to examine the effective integration of AI tools in teaching. Finally,
we propose design directions for future teacher-centered EdTech, particularly
in multilingual and Global South contexts.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [86] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出了一种基于Transformer的端到端模型，用于MIDI表演中的节拍和重音跟踪，通过新颖的数据预处理技术和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的节拍跟踪方法主要关注音频，而本文旨在解决MIDI表演中的节拍和重音跟踪问题，填补符号音乐分析的空白。

Method: 采用编码器-解码器结构的Transformer模型，结合动态增强和优化的标记化策略进行数据预处理。

Result: 模型在多个数据集上表现优于现有方法，F1分数具有竞争力。

Conclusion: Transformer架构在符号节拍跟踪中显示出潜力，未来可与自动音乐转录系统结合以增强分析能力。

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [87] [MuteSwap: Silent Face-based Voice Conversion](https://arxiv.org/abs/2507.00498)
*Yifan Liu,Yu Fang,Zhouhan Lin*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conventional voice conversion modifies voice characteristics from a source
speaker to a target speaker, relying on audio input from both sides. However,
this process becomes infeasible when clean audio is unavailable, such as in
silent videos or noisy environments. In this work, we focus on the task of
Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely
from visual inputs. i.e., given images of a target speaker and a silent video
of a source speaker containing lip motion, SFVC generates speech aligning the
identity of the target speaker while preserving the speech content in the
source silent video. As this task requires generating intelligible speech and
converting identity using only visual cues, it is particularly challenging. To
address this, we introduce MuteSwap, a novel framework that employs contrastive
learning to align cross-modality identities and minimize mutual information to
separate shared visual features. Experimental results show that MuteSwap
achieves impressive performance in both speech synthesis and identity
conversion, especially under noisy conditions where methods dependent on audio
input fail to produce intelligible results, demonstrating both the
effectiveness of our training approach and the feasibility of SFVC.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [88] [Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context](https://arxiv.org/abs/2507.00288)
*Claire Li,David Freeborn*

Main category: econ.TH

TL;DR: 研究探讨AI如何重塑跨国治理中的组织问责制，结合TAM、ANT和制度理论分析，提出两种策略以促进AI的负责任采用。


<details>
  <summary>Details</summary>
Motivation: AI在审计和财务报告等领域的广泛应用正在动摇传统的问责机制，需要通过跨学科理论重新构建问责框架。

Method: 整合TAM、ANT和制度理论，引入合规性和合法性作为感知有用性的关键因素，提出问责作为网络化组装的涌现属性。

Result: 提出两种组织策略：内部治理重构和外部网络参与，以实现AI的合法、负责任的全球采用。

Conclusion: 问责是全球化社会技术网络中共同构建的，需多理论融合和策略调整以适应AI技术的快速发展。

Abstract: This study explores how AI-powered digital innovations are reshaping
organisational accountability in a transnational governance context. As AI
systems increasingly mediate decision-making in domains such as auditing and
financial reporting, traditional mechanisms of accountability, based on
control, transparency, and auditability, are being destabilised. We integrate
the Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and
institutional theory to examine how organisations adopt AI technologies in
response to regulatory, ethical, and cultural pressures that transcend national
boundaries. We argue that accountability is co-constructed within global
socio-technical networks, shaped not only by user perceptions but also by
governance logics and normative expectations. Extending TAM, we incorporate
compliance and legitimacy as key factors in perceived usefulness and usability.
Drawing on ANT, we reconceptualise accountability as a relational and emergent
property of networked assemblages. We propose two organisational strategies
including internal governance reconfiguration and external actor-network
engagement to foster responsible, legitimate, and globally accepted AI adoption
in the accounting domain.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [89] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: 论文提出了一种无监督的3D击剑动作与策略提取系统VirtualFencer，并能生成逼真的击剑行为，展示了其多样化的应用能力。


<details>
  <summary>Details</summary>
Motivation: 击剑运动中动作多样化且具有策略性，结合无监督数据驱动建模的需求，激发了该系统的开发。

Method: 提出VirtualFencer系统，从无监督视频中提取3D击剑动作与策略，并生成真实击剑行为。

Result: 系统展示了与自身对战、与真实击剑手对战及与专业击剑手交互对战的能力。

Conclusion: VirtualFencer系统能够无监督地提取并模拟击剑动作与策略，展现了广泛的应用潜力。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [90] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: MVP是一个多模态视频流行度预测模型，融合深层视频特征、用户元数据和上下文信息，通过梯度提升回归模型取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体视频流行度预测在内容推荐、趋势检测和观众互动中具有重要应用价值。

Method: MVP整合预训练模型的深层视频特征、用户元数据和上下文信息，采用系统预处理技术（如对数转换和离群值去除）提升模型鲁棒性，并使用梯度提升回归模型捕捉多模态复杂模式。

Result: MVP在SMP Challenge 2025视频赛道官方评估中排名第一，证明了其有效性和可靠性。

Conclusion: MVP通过多模态融合和系统预处理，显著提升了社交媒体视频流行度的预测性能。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [91] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: 论文介绍了FiboSB数据集，用于解决K-12教育中协作任务中6D姿态估计的挑战，并评估了现有方法的表现，最终通过改进YOLO11-x提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在K-12教育协作任务中难以准确捕捉学生与物理对象的互动，6D姿态估计为解决这一问题提供了可能。

Method: 提出FiboSB数据集，包含协作任务中的6D姿态视频数据，并评估四种先进方法，改进YOLO11-x进行性能提升。

Result: YOLO11-x在FiboSB数据集上的mAP_50达到0.898，表现优于其他方法。

Conclusion: FiboSB数据集和YOLO11-x的改进为复杂协作场景中的6D姿态估计提供了基础。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [92] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: GazeTarget360系统通过整合眼接触检测器、预训练视觉编码器和多尺度融合解码器，实现了从图像中预测360度视线目标的首创方法。


<details>
  <summary>Details</summary>
Motivation: 使机器人理解人类视线目标是实现人机交互中注意力估计和运动预测等下游任务的关键。

Method: 提出GazeTarget360系统，结合条件推理引擎（眼接触检测器、预训练视觉编码器和多尺度融合解码器），解决广义视觉场景中的360度视线目标估计问题。

Result: 交叉验证结果表明，GazeTarget360在未见过场景中能准确预测视线目标，是首个高效、可部署的实用系统。

Conclusion: GazeTarget360为真实摄像机画面中的视线目标预测提供了首创且高效的方法，代码已开源。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [93] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新颖的实时逆向运动学（IK）求解器，用于生成逼真的人体运动，利用TensorFlow的自动微分和即时编译技术，高效处理复杂的人体骨骼模型。


<details>
  <summary>Details</summary>
Motivation: 生成准确且逼真的虚拟人体运动对于计算机图形学、虚拟环境、机器人和生物力学等领域具有重要意义。

Method: 通过将正向和逆向运动学作为可微分操作处理，利用TensorFlow的自动微分和即时编译技术，解决了误差累积和复杂关节限制等常见问题。

Result: 实验表明，该求解器在实时性能、收敛速度和计算效率上优于现有方法（如CCD、FABRIK和IPOPT），在多约束任务中表现优异。

Conclusion: 提出的IK求解器在实时性和逼真运动生成方面具有显著优势，为相关应用提供了高效工具。

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [94] [Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications](https://arxiv.org/abs/2507.00543)
*Leila Tavakoli,Hamed Zamani*

Main category: cs.IR

TL;DR: LLMs在复杂、多维标注任务中表现不佳，研究提出结合人工审核的轻量级干预方法，显著提升可靠性并减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在多维、主观标注任务中的实际效果，解决其现有局限性。

Method: 通过系统评估LLMs表现，提出基于置信度和模型分歧的HITL工作流。

Result: 轻量干预显著提升标注可靠性，人工工作量减少45%。

Conclusion: HITL工作流为LLMs在实际标注任务中的应用提供了可行方案。

Abstract: Despite growing interest in using large language models (LLMs) to automate
annotation, their effectiveness in complex, nuanced, and multi-dimensional
labelling tasks remains relatively underexplored. This study focuses on
annotation for the search clarification task, leveraging a high-quality,
multi-dimensional dataset that includes five distinct fine-grained annotation
subtasks. Although LLMs have shown impressive capabilities in general settings,
our study reveals that even state-of-the-art models struggle to replicate
human-level performance in subjective or fine-grained evaluation tasks. Through
a systematic assessment, we demonstrate that LLM predictions are often
inconsistent, poorly calibrated, and highly sensitive to prompt variations. To
address these limitations, we propose a simple yet effective human-in-the-loop
(HITL) workflow that uses confidence thresholds and inter-model disagreement to
selectively involve human review. Our findings show that this lightweight
intervention significantly improves annotation reliability while reducing human
effort by up to 45%, offering a relatively scalable and cost-effective yet
accurate path forward for deploying LLMs in real-world evaluation settings.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [95] [Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms](https://arxiv.org/abs/2507.00491)
*Zain Taufique,Aman Vyas,Antonio Miele,Pasi Liljeberg,Anil Kanduri*

Main category: cs.MA

TL;DR: 论文介绍了Twill框架，用于在移动边缘平台上高效调度复合AI系统（cAI）的并发推理任务，减少延迟并控制功耗。


<details>
  <summary>Details</summary>
Motivation: 复合AI系统（cAI）结合多种AI模型解决复杂问题，但现有技术无法处理其动态工作负载，尤其是在移动边缘平台上。

Method: 提出Twill框架，通过任务亲和性集群映射、优先级感知任务冻结/解冻和动态电压频率调整（DVFS），优化cAI系统的推理调度。

Result: 在Nvidia Jetson Orin NX平台上，Twill将推理延迟平均降低54%，同时满足功耗预算。

Conclusion: Twill能有效解决cAI系统在移动边缘平台上的调度问题，提升性能并控制功耗。

Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems.
cAI systems are typically composed of deep neural networks (DNNs),
transformers, and large language models (LLMs), exhibiting a high degree of
computational diversity and dynamic workload variation. Deploying cAI services
on mobile edge platforms poses a significant challenge in scheduling concurrent
DNN-transformer inference tasks, which arrive dynamically in an unknown
sequence. Existing mobile edge AI inference strategies manage multi-DNN or
transformer-only workloads, relying on design-time profiling, and cannot handle
concurrent inference of DNNs and transformers required by cAI systems. In this
work, we address the challenge of scheduling cAI systems on heterogeneous
mobile edge platforms. We present Twill, a run-time framework to handle
concurrent inference requests of cAI workloads through task affinity-aware
cluster mapping and migration, priority-aware task freezing/unfreezing, and
DVFS, while minimizing inference latency within power budgets. We implement and
deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate
Twill against state-of-the-art edge AI inference techniques over contemporary
DNNs and LLMs, reducing inference latency by 54% on average, while honoring
power budgets.

</details>


### [96] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: SciBORG是一个模块化框架，利用LLM驱动的代理自主执行科学任务，解决了内存、规划和工具集成的挑战，实现了可靠的任务执行。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂科学工作流中的应用受限于内存、规划和工具集成问题，SciBORG旨在通过代理框架解决这些问题。

Method: SciBORG通过动态从源代码文档构建代理，并采用有限状态自动机（FSA）内存以实现状态跟踪和上下文感知决策，无需手动提示工程。

Result: SciBORG在物理和虚拟硬件集成中表现优异，支持多步生物测定检索，实现了可靠执行、自适应规划和可解释状态转换。

Conclusion: 内存和状态感知是代理规划和可靠性的关键，SciBORG为复杂环境中的AI代理部署提供了通用基础。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 本文改进了SMT-NRA的局部搜索方法，提出了2d-cell-jump和2d-LS框架，并结合MCSAT和OpenCAD提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高SMT-NRA问题的求解效率，尤其是在非线性实数算术领域中的局部搜索性能。

Method: 1. 引入2d-cell-jump操作；2. 提出2d-LS框架并与MCSAT结合；3. 实施sample-cell projection operator优化MCSAT；4. 设计了结合MCSAT、2d-LS和OpenCAD的混合框架。

Result: 实验结果显示局部搜索性能显著提升。

Conclusion: 所提出的2d-cell-jump和混合框架有效提升了SMT-NRA的求解效率。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [98] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI是一个无需训练的GUI自然语言查询框架，通过动态视觉基础和模态感知优化实现了改进的结果。


<details>
  <summary>Details</summary>
Motivation: GUI的自然语言查询面临视觉元素多样性和语言歧义等挑战，需要一个高效的解决方案。

Method: 将GUI拆分为文本和图标元素，利用通用视觉语言模型独立处理，并通过动态聚焦候选区域逐步优化结果。

Result: 在标准GUI基准测试中表现优于基线方法。

Conclusion: 模态分离和区域聚焦推理的结合有效解决了GUI查询中的歧义问题。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [99] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: SEZ-HARN是一种新型自解释零样本人类活动识别模型，通过生成骨骼视频解释其决策过程，解决了现有HAR模型缺乏透明度的问题。在四个基准数据集上表现优异，零样本识别准确率接近最佳黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 现有零样本人类活动识别（ZS-HAR）模型缺乏透明度，限制了其在现实场景中的应用。SEZ-HARN旨在填补这一空白，提供可解释的决策过程。

Method: 提出了SEZ-HARN模型，能够识别训练中未遇到的活动，并通过生成的骨骼视频解释其决策。

Result: 在PAMAP2、DaLiAc、HTD-MHAD和MHealth数据集上，SEZ-HARN的零样本识别准确率接近最佳黑盒模型，同时提供直观的解释。

Conclusion: SEZ-HARN在保持高准确率的同时增强了模型的可解释性，为HAR的实用化提供了新方向。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [100] [RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles](https://arxiv.org/abs/2507.00937)
*David Hunt,Shaocheng Luo,Spencer Hallyburton,Shafii Nillongo,Yi Li,Tingjun Chen,Miroslav Pajic*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based localization suffers
from sparse point cloud generation, noise, and false detections. Thus, in this
work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph
neural network (GNN)-based framework to enhance radar point clouds, even in
complex and dynamic environments. With an inference time of just 7.3 ms on the
low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such
resource-constrained devices, requiring no additional computational resources.
We evaluate its performance across key tasks, including localization, SLAM, and
autonomous navigation, in three different environments. Our results demonstrate
strong reliability and generalizability, making RaGNNarok a robust solution for
low-cost indoor mobile robots.

</details>


### [101] [Edge Computing and its Application in Robotics: A Survey](https://arxiv.org/abs/2507.00523)
*Nazish Tahir,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 该论文综述了边缘计算在机器人领域的应用，着重探讨了其在减少延迟、提升实时数据处理能力方面的优势，同时分析了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 边缘计算在机器人领域的应用广泛且重要，但缺乏全面的调查。本文旨在填补这一空白，探讨边缘机器人的最新进展、挑战和未来方向。

Method: 通过对边缘机器人领域的现有工作进行系统梳理，分析关键应用、挑战和解决方案，提供深入的见解。

Result: 论文总结了边缘机器人技术的优势，如低延迟和高移动性，并识别了该领域的开放性研究问题。

Conclusion: 边缘计算是机器人技术发展的关键推动力，未来需要更多研究解决现有挑战，以充分发挥其潜力。

Abstract: The Edge computing paradigm has gained prominence in both academic and
industry circles in recent years. By implementing edge computing facilities and
services in robotics, it becomes a key enabler in the deployment of artificial
intelligence applications to robots. Time-sensitive robotics applications
benefit from the reduced latency, mobility, and location awareness provided by
the edge computing paradigm, which enables real-time data processing and
intelligence at the network's edge. While the advantages of integrating edge
computing into robotics are numerous, there has been no recent survey that
comprehensively examines these benefits. This paper aims to bridge that gap by
highlighting important work in the domain of edge robotics, examining recent
advancements, and offering deeper insight into the challenges and motivations
behind both current and emerging solutions. In particular, this article
provides a comprehensive evaluation of recent developments in edge robotics,
with an emphasis on fundamental applications, providing in-depth analysis of
the key motivations, challenges, and future directions in this rapidly evolving
domain. It also explores the importance of edge computing in real-world
robotics scenarios where rapid response times are critical. Finally, the paper
outlines various open research challenges in the field of edge robotics.

</details>


### [102] [Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery](https://arxiv.org/abs/2507.00635)
*Tinghe Hong,Shenlin Cai,Boyang Li,Kai Huang*

Main category: cs.RO

TL;DR: 提出了一种结合机器学习和传统算法的创新眼定位与跟踪方法，解决了现有技术依赖额外传感器、遮挡问题和光照变化挑战。


<details>
  <summary>Details</summary>
Motivation: 解决眼科手术机器人术前导航依赖手动操作的问题，以及现有眼注视估计技术对额外传感器、遮挡和光照变化的依赖。

Method: 结合机器学习和传统算法，无需地标点，实现稳定虹膜检测和注视估计。

Result: 眼方向估计平均误差为0.58度，机械臂运动控制平均误差为2.08度。

Conclusion: 该方法在变化光照和阴影条件下表现稳定，为手术机器人提供了更精确的导航控制。

Abstract: Ophthalmic surgical robots offer superior stability and precision by reducing
the natural hand tremors of human surgeons, enabling delicate operations in
confined surgical spaces. Despite the advancements in developing vision- and
force-based control methods for surgical robots, preoperative navigation
remains heavily reliant on manual operation, limiting the consistency and
increasing the uncertainty. Existing eye gaze estimation techniques in the
surgery, whether traditional or deep learning-based, face challenges including
dependence on additional sensors, occlusion issues in surgical environments,
and the requirement for facial detection. To address these limitations, this
study proposes an innovative eye localization and tracking method that combines
machine learning with traditional algorithms, eliminating the requirements of
landmarks and maintaining stable iris detection and gaze estimation under
varying lighting and shadow conditions. Extensive real-world experiment results
show that our proposed method has an average estimation error of 0.58 degrees
for eye orientation estimation and 2.08-degree average control error for the
robotic arm's movement based on the calculated orientation.

</details>
