<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 17]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad提出了一种将形式化方法集成到LLM代码生成循环中的框架，通过将代码视为可微分变量并利用伪梯度引导迭代优化，显著提高了代码的正确性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的代码往往缺乏正确性、鲁棒性和效率的保证，尤其是在需要严格约束的领域中，这一问题尤为突出。

Method: FormalGrad框架将形式化方法直接集成到LLM的迭代生成循环中，将代码视为可微分变量，并将结构化反馈和形式化约束转化为文本伪梯度，以指导模型迭代优化代码。

Result: 在HumanEval、HumanEval+和LiveCodeBench基准测试中，FormalGrad表现优异，绝对性能提升高达27%（HumanEval），相对性能提升41%（LiveCodeBench V6）。

Conclusion: FormalGrad能够生成形式化验证的、鲁棒且高效的代码，为高风险的AI辅助软件开发提供了可靠的方法。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [2] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder通过分层特征优化和多维度检索改进，显著提升了仓库级代码补全的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本相似性的检索增强生成（RAG）方法存在语义误导、冗余和同质化问题，且无法解决外部符号歧义。

Method: 提出了Saracoder框架，包括分层特征优化模块（通过深层次语义关系筛选候选）和外部感知标识符消歧模块（解决跨文件符号歧义）。

Result: 在CrossCodeEval和RepoEval-Updated基准测试中显著优于现有基线。

Conclusion: 多维度系统性优化为构建更准确、鲁棒的仓库级代码补全系统提供了新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [3] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出了一种名为“Next Edit Prediction”的新任务，旨在通过开发者的交互历史预测下一次编辑的位置和内容，以弥补当前代码助手在预测开发者意图方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的代码辅助工具（如代码补全和聊天编辑）无法主动预测开发者的连续编辑意图，导致用户体验不佳。

Method: 作者构建了一个高质量的监督微调数据集和评估基准，并对一系列模型进行了监督微调。

Result: 通过实验验证了微调模型和其他基线模型的性能，得出了新的发现。

Conclusion: 这项工作为一种新的人机交互范式奠定了基础，能够主动预测开发者下一步动作，而非仅响应显式指令。

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [4] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 预训练语言模型（PTLM）的开发与分发在GitHub和Hugging Face平台上存在协调问题，导致版本不一致和功能不完整。研究发现，两个平台的贡献者关注点不同，且同步活动呈现八种模式，部分同步模式可能导致用户使用不完整或过时的模型。


<details>
  <summary>Details</summary>
Motivation: 探索GitHub和Hugging Face平台在PTLM开发与分发过程中的协调问题，以改进跨平台发布实践。

Method: 对325个PTLM家族（904个HF变体）进行混合方法研究，分析提交活动的协调性，包括延迟、同步类型和强度三个维度。

Result: GitHub贡献者关注模型版本、代码质量和性能优化，而HF贡献者关注模型描述和数据集处理。发现八种同步模式，部分同步模式可能导致功能不完整。

Conclusion: 识别同步模式对改进PTLM发布流程的监督和可追溯性至关重要。

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [5] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: 论文研究了Solidity版本更新带来的编译错误挑战，通过实证分析发现81.68%的合约在不同版本中编译失败，并提出SMCFIXER框架，结合专家知识检索与LLM修复机制，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: Solidity频繁的版本更新虽然提升了安全性和功能，但也引入了编译错误和维护难题，亟需解决方案。

Method: 通过实证分析评估LLM在修复Solidity编译错误中的表现，并开发SMCFIXER框架，整合专家知识检索和LLM修复机制。

Result: SMCFIXER在真实数据集上比基线GPT-4o显著提升24.24%，达到96.97%的准确率。

Conclusion: SMCFIXER为智能合约的版本迁移提供了高效、可靠的解决方案，突显了领域特定适配的重要性。

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [6] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: 提出EvoScat工具，通过交互式密度散点图可视化大规模历史数据集，支持软件演化研究。


<details>
  <summary>Details</summary>
Motivation: 长期软件项目涉及大量不断修订的工件，研究者需要有效工具来分析和比较这些数据。

Method: 开发EvoScat工具，利用交互式密度散点图，支持时间轴配置、工件排序和交互式颜色映射。

Result: 展示了工具在变化速度比较、克隆检测和新鲜度评估中的适用性，并分析了多个数据集。

Conclusion: EvoScat为研究者提供了高效的可视化手段，支持对大规模软件演化数据的探索和分析。

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [Generating Compilers for Qubit Mapping and Routing](https://arxiv.org/abs/2508.10781)
*Abtin Molavi,Amanda Xu,Ethan Cecchetti,Swamit Tannu,Aws Albarghouthi*

Main category: cs.PL

TL;DR: 量子计算优化编译器可通过自动化生成技术，简化针对多样化量子架构的量子比特映射和路由（QMR）问题的解决。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在材料科学和化学等领域有巨大潜力，但不同量子设备的多样性和快速演进使得QMR问题复杂化。

Method: 提出了一种基于设备状态机的通用QMR问题抽象，并开发了领域专用语言Marol，结合参数化求解器自动生成编译器。

Result: 生成的编译器在运行时和解质量上与手工编写的专用编译器相当，适用于各种主要硬件平台。

Conclusion: 该方法简化了未来量子编译器的开发，适应新兴量子架构的快速变化。

Abstract: Quantum computers promise to solve important problems faster than classical
computers, potentially unlocking breakthroughs in materials science, chemistry,
and beyond. Optimizing compilers are key to realizing this potential, as they
minimize expensive resource usage and limit error rates. A critical compilation
step is qubit mapping and routing (QMR), which finds mappings from circuit
qubits to qubits on a target device and plans instruction execution while
satisfying the device's connectivity constraints. The challenge is that the
landscape of quantum architectures is incredibly diverse and fast-evolving.
Given this diversity, hundreds of papers have addressed the QMR problem for
different qubit hardware, connectivity constraints, and quantum error
correction schemes.
  We present an approach for automatically generating qubit mapping and routing
compilers for arbitrary quantum architectures. Though each QMR problem is
different, we identify a common core structure-device state machine-that we use
to formulate an abstract QMR problem. Our formulation naturally leads to a
domain-specific language, Marol, for specifying QMR problems-for example, the
well-studied NISQ mapping and routing problem requires only 12 lines of Marol.
We demonstrate that QMR problems, defined in Marol, can be solved with a
powerful parametric solver that can be instantiated for any Marol program. We
evaluate our approach through case studies of important QMR problems from prior
and recent work, covering noisy and fault-tolerant quantum architectures on all
major hardware platforms. Our thorough evaluation shows that generated
compilers are competitive with handwritten, specialized compilers in terms of
runtime and solution quality. We envision that our approach will simplify
development of future quantum compilers as new quantum architectures continue
to emerge.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [8] [Meta-Metrics and Best Practices for System-Level Inference Performance Benchmarking](https://arxiv.org/abs/2508.10251)
*Shweta Salaria,Zhuoran Liu,Nelson Mimura Gonzalez*

Main category: cs.PF

TL;DR: FMwork是一种系统化方法，用于高效评估基础模型（如LLM）的性能，通过元指标、参数选择和成本性能分析，显著提升实验效率，最高可达24倍加速。


<details>
  <summary>Details</summary>
Motivation: 由于评估基础模型的推理性能涉及大量实验配置，传统方法不切实际且低效，因此开发FMwork解决这一问题。

Method: FMwork包含元指标、参数选择和战略成本性能评估三个核心组件，用于系统化设计和优化实验流程。

Result: 使用FMwork后，实验速度最高提升24倍，同时减少实验输出也能保持96.6%的准确性。

Conclusion: FMwork提供了一种高效且准确的基准测试方法，显著降低了评估基础模型性能的成本和时间。

Abstract: Benchmarking inference performance (speed) of Foundation Models such as Large
Language Models (LLM) involves navigating a vast experimental landscape to
understand the complex interactions between hardware and software components.
However, evaluating every possible test configuration is impractical,
unfeasible and unnecessary. To address this challenge, we introduce FMwork, a
comprehensive and methodical approach to creating a controlled testing
environment that accurately reflects and characterizes performance. FMwork
comprises a set of benchmkaring best practices with three key components: 1)
meta-metrics, 2) parameter selection, and 3) strategic cost-performance
evaluation. Meta-metrics account for time and resources spent on benchmarking
and the relative accuracy of the results compared to a larger body of
measurements, representing the complete experimental space. FMwork
operationalizes the meta-metrics and provides efficient strategies for
parameter selection and cost-performance analysis. Using the framework, we show
up to 24x improvement (speedup and/or resource savings) running sweeps of
experiments compared to the ground truth. Even already considering a subset of
experiments as reference point (using the power of two for batch sizes),
reducing experimental output size from 1024 to 128 tokens yields another 2.7x
gain while keeping 96.6% accuracy for an evaluation using Llama 3.1 8B model.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [Rethinking Reliability Using Network Coding: a Practical 5G Evaluation](https://arxiv.org/abs/2508.10247)
*Laura Landon,Vipindev Adat Vasudevan,Junmo Sung,Muriel Médard*

Main category: cs.NI

TL;DR: 该研究在5G测试平台的IP层中集成了一种实时网络编码系统，替代了传统的基于重传的可靠性机制（如ARQ和HARQ）。通过随机线性网络编码（RLNC）和块编码方案，结果表明RLNC能以更少的传输次数恢复丢包，并在中高丢包率下保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 探索网络编码在5G无线系统中替代传统重传机制的潜力，以提高资源利用效率和系统可靠性。

Method: 在5G测试平台中，基于netfilter的包拦截框架，将RLNC应用于gNB和UE之间的实时流量，并分析块编码方案对吞吐量、抖动和资源使用的影响。

Result: 结果表明，通过选择合适的编码率，RLNC能以比ARQ/HARQ更少的传输次数完全恢复丢包，并在中高丢包率下保持高吞吐量。

Conclusion: 网络编码可以有效替代未来的无线系统中的重传机制，具有更高的资源利用效率潜力。

Abstract: This work presents the design and implementation of a real-time network
coding system integrated into the IP layer of a 5G testbed, offering an
alternative to conventional retransmission-based reliability mechanisms such as
ARQ and HARQ. Using a netfilter-based packet interception framework, we inject
forward erasure correction using Random Linear Network Coding (RLNC) into live
traffic between a gNB and UE over a 3GPP RF link. We evaluate a block coding
scheme, analyzing its impact on throughput, jitter, and resource usage. Results
show that with appropriate code rate selection, RLNC can fully recover from
packet losses using fewer transmissions than ARQ/HARQ and maintain a high
throughput, particularly under moderate-to-high packet loss rates. These
findings demonstrate that network coding can effectively replace
retransmission-based reliability in future wireless systems, with the potential
for more efficient resource utilization.

</details>


### [10] [Design of a Timer Queue Supporting Dynamic Update Operations](https://arxiv.org/abs/2508.10283)
*Zekun Wang,Binghao Yue,Weitao Pan,Jiangyi Shi,Yue Hao*

Main category: cs.NI

TL;DR: 提出了一种基于脉动阵列和移位寄存器的混合架构硬件优先级队列，用于高效管理计时器队列，支持五种操作，并在FPGA上实现高频率和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大规模计时器在网络处理中无处不在，但传统实现存在计时精度低和计算开销高的问题，需要更高效的解决方案。

Method: 采用脉动阵列和移位寄存器的混合架构，集中布尔逻辑编码生成控制信号，支持五种队列操作（入队、出队、删除、更新、查看），并首次实现队列内优先级更新。

Result: 在FPGA上实现超过400 MHz的工作频率，资源消耗比现有方案减少2.2-2.8倍。

Conclusion: 该设计显著提升计时器队列管理的效率和性能，适用于多种网络处理场景。

Abstract: Large-scale timers are ubiquitous in network processing, including flow table
entry expiration control in software defined network (SDN) switches, MAC
address aging in Ethernet bridges, and retransmission timeout management in
TCP/IP protocols. Conventional implementations suffer from critical
limitations: low timing accuracy due to large-scale timer traversal and high
computational overhead for new timer insertion. This paper presents a
hybrid-architecture hardware priority queue based on systolic arrays and shift
registers for efficient timer queue management. The design uniquely supports
five operations: enqueue, dequeue, delete, update, and peek.To the best of our
knowledge, it is the first hardware priority queue enabling in-queue priority
updates. By leveraging centralized Boolean logic encoding within systolic
blocks, the design efficiently generates set/shift control signals while the
novel push-first operation ensures FIFO ordering for same-priority timers
without additional metadata. Experimental results demonstrate that the design
operates at over 400 MHz on FPGAs, achieving a 2.2-2.8x reduction in resource
consumption compared to state-of-the-art implementations.

</details>


### [11] [Near-realtime Earth Observation Via Starlink LEO Satellite Constellation](https://arxiv.org/abs/2508.10338)
*Bo Wu,Pengfei Zhou*

Main category: cs.NI

TL;DR: 摘要探讨了利用Starlink卫星基础设施支持地球观测卫星数据传输的可行性，提出了名为“Starlink Space User”(SSU)的新系统。SSU通过优化链路选择、PoP选择及调度算法，显著减少了未传输数据的积压。


<details>
  <summary>Details</summary>
Motivation: 由于地面站数量有限且通信窗口短暂，地球观测卫星数据传输面临挑战，而Starlink的连续连接能力为解决问题提供了新思路。

Method: 提出SSU系统，将观测卫星视为Starlink的空间用户，并设计了新型算法优化链路选择、PoP选择和系统调度。

Result: 通过仿真和实际测量，SSU显著降低了每颗卫星的未传输数据中位数积压。

Conclusion: 研究表明，利用Starlink基础设施支持地球观测卫星数据传输是可行的，SSU系统有效提升了数据传输效率。

Abstract: Earth observation (EO) satellites in Low Earth Orbit (LEO) are collecting
vast amounts of data, which are invaluable for applications such as monitoring
forest fires. However, data downloading from EO satellites faces significant
challenges due to the limited number of ground stations and the brief
communication windows with them. Conversely, emerging LEO constellations like
Starlink have enabled continuous connectivity and revolutionized access for
ordinary users globally, who can connect via a simple satellite dish. In this
paper, we study the feasibility of supporting EO satellites with Starlink
satellite infrastructure and introduce a novel data delivery system, designated
as "Starlink Space User" (SSU), for relaying data from observation satellites.
SSU treats EO satellites as space users of Starlink, facilitating efficient
data transfer to Earth. At the core of SSU is a novel class of algorithms
designed for link and PoP selection, as well as system scheduling optimization,
that operate effectively atop Starlink's proprietary infrastructure. We assess
the performance of SSU using trace-driven simulations alongside real-world
Starlink performance measurements. Our results demonstrate that the proposed
Starlink-aided design can significantly reduce the median backlog (data not
delivered) per satellite.

</details>


### [12] [Probabilistic Latency Analysis of the Data Distribution Service in ROS 2](https://arxiv.org/abs/2508.10413)
*Sanghoon Lee,Hyung-Seok Park,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: ROS 2中DDS通信的可靠性传输过程通过概率延迟分析（PLA）建模，解决了无线网络中参数调优的难题，并通过实验验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: ROS 2的DDS通信在无线网络中面临参数调优困难，缺乏端到端延迟行为的明确指导，需要一种理论方法优化性能。

Method: 提出概率延迟分析（PLA），通过离散状态方法建模DDS通信的可靠传输过程，分析中间件和传输层事件。

Result: PLA在270种场景下验证，分析预测与实验结果高度一致，为无线工业机器人提供了优化依据。

Conclusion: PLA为ROS 2 DDS通信在无线网络中的可靠性、延迟和性能优化提供了理论支持。

Abstract: Robot Operating System 2 (ROS 2) is now the de facto standard for robotic
communication, pairing UDP transport with the Data Distribution Service (DDS)
publish-subscribe middleware. DDS achieves reliability through periodic
heartbeats that solicit acknowledgments for missing samples and trigger
selective retransmissions. In lossy wireless networks, the tight coupling among
heartbeat period, IP fragmentation, and retransmission interval obscures end to
end latency behavior and leaves practitioners with little guidance on how to
tune these parameters. To address these challenges, we propose a probabilistic
latency analysis (PLA) that analytically models the reliable transmission
process of ROS 2 DDS communication using a discrete state approach. By
systematically analyzing both middleware level and transport level events, PLA
computes the steady state probability distribution of unacknowledged messages
and the retransmission latency. We validate our PLA across 270 scenarios,
exploring variations in packet delivery ratios, message sizes, and both
publishing and retransmission intervals, demonstrating a close alignment
between analytical predictions and experimental results. Our findings establish
a theoretical basis to systematically optimize reliability, latency, and
performance in wireless industrial robotics.

</details>


### [13] [Federated Learning Over LoRa Networks: Simulator Design and Performance Evaluation](https://arxiv.org/abs/2508.10574)
*Anshika Singh,Siddhartha S. Borkotoky*

Main category: cs.NI

TL;DR: 开发了一个Python模拟器，用于评估基于LoRa网络的集中式联邦学习（FL），并研究了前向帧擦除纠正（FEC）对FL性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决长距离低功耗广域网（LoRa）下联邦学习面临的带宽限制、干扰和严格占空比约束问题。

Method: 整合并扩展了Flower和LoRaSim框架，开发了一个详细的链路级模型，支持多种技术（如稀疏化、量化、压缩、FEC和占空比控制）。

Result: 证明了FEC在LoRa网络中FL收敛和设备空中时间中的关键作用。

Conclusion: 为基于LoRa网络的FL通信协议设计提供了重要见解。

Abstract: Federated learning (FL) over long-range (LoRa) low-power wide area networks
faces unique challenges due to limited bandwidth, interference, and strict
duty-cycle constraints. We develop a Python-based simulator that integrates and
extends the Flower and LoRaSim frameworks to evaluate centralized FL over LoRa
networks. The simulator employs a detailed link-level model for FL update
transfer over LoRa channels, capturing LoRa's receiver sensitivity,
interference characteristics, block-fading effects, and constraints on the
maximum transmission unit. It supports update sparsification, quantization,
compression, forward frame-erasure correction (FEC), and duty cycling.
Numerical results illustrate the impact of transmission parameters (spreading
factor, FEC rate) and interference on FL performance. Demonstrating the
critical role of FEC in enabling FL over LoRa networks, we perform an in-depth
evaluation of the impact of FEC on FL convergence and device airtime, providing
insights for communication protocol design for FL over LoRa networks.

</details>


### [14] [Balancing the Energy Consumption and Latency of Over-the-Air Firmware Updates in LoRaWAN](https://arxiv.org/abs/2508.10588)
*Siddhartha S. Borkotoky*

Main category: cs.NI

TL;DR: 论文提出了一种灵活的LoRaWAN固件空中更新方案，通过调整传播因子平衡能耗与更新延迟。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN终端设备受能量限制和传输周期约束，需要解决固件更新中的能耗与延迟问题。

Method: 采用序列化传播因子传输更新帧，调整最小传播因子和每因子传输次数，实现能耗与延迟的权衡。

Result: 方案可灵活适应不同需求，如低延迟高能耗用于安全补丁，高延迟低能耗用于非关键更新。

Conclusion: 提出的方案为LoRaWAN固件更新提供了高效、灵活的解决方案。

Abstract: Over-the-air firmware updates are crucial for mitigating security threats and
maintaining up-to-date device functionality in Long Range Wide Area Networks
(LoRaWANs). LoRaWAN end devices are usually energy-constrained, and LoRaWAN
transmissions are subject to duty-cycle restrictions. Consequently, controlling
the energy expenditure and update-delivery latency of FUOTA are key challenges.
We propose a flexible scheme that achieves a tunable trade-off between the
energy consumption and delivery delay. The scheme employs the LoRa spreading
factors sequentially to transmit update-carrying frames, sending a fixed number
of frames with a given spreading factor before moving to the next. By adjusting
the smallest spreading factor to be used and the number of transmissions per
spreading factor, a suitable energy-delay trade-off can be achieved. Thus,
time-sensitive updates, such as security patches, may be sent with a
low-delay-high-energy setting, whereas a more energy-efficient but higher-delay
setting may be used for non-critical updates.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [Ensembling Synchronisation-based and Face-Voice Association Paradigms for Robust Active Speaker Detection in Egocentric Recordings](https://arxiv.org/abs/2508.10580)
*Jason Clarke,Yoshihiko Gotoh,Stefan Goetze*

Main category: cs.MM

TL;DR: 提出了一种简单的集成方法，通过加权平均融合同步依赖和同步无关的模型输出，解决了第一人称录音中的ASD问题。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称录音中因遮挡、运动模糊和音频干扰导致的ASD性能下降问题。

Method: 采用加权平均融合同步依赖和同步无关的模型输出，并优化FVA组件的预处理流程。

Result: 在Ego4D-AVD验证集上，分别达到70.2%和66.7%的mAP。

Conclusion: 该集成方法有效利用了互补信息，证明了两种组件的互补优势。

Abstract: Audiovisual active speaker detection (ASD) in egocentric recordings is
challenged by frequent occlusions, motion blur, and audio interference, which
undermine the discernability of temporal synchrony between lip movement and
speech. Traditional synchronisation-based systems perform well under clean
conditions but degrade sharply in first-person recordings. Conversely,
face-voice association (FVA)-based methods forgo synchronisation modelling in
favour of cross-modal biometric matching, exhibiting robustness to transient
visual corruption but suffering when overlapping speech or front-end
segmentation errors occur. In this paper, a simple yet effective ensemble
approach is proposed to fuse synchronisation-dependent and
synchronisation-agnostic model outputs via weighted averaging, thereby
harnessing complementary cues without introducing complex fusion architectures.
A refined preprocessing pipeline for the FVA-based component is also introduced
to optimise ensemble integration. Experiments on the Ego4D-AVD validation set
demonstrate that the ensemble attains 70.2% and 66.7% mean Average Precision
(mAP) with TalkNet and Light-ASD backbones, respectively. A qualitative
analysis stratified by face image quality and utterance masking prevalence
further substantiates the complementary strengths of each component.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Repairing General Game Descriptions (extended version)](https://arxiv.org/abs/2508.10438)
*Yifan He,Munyque Mittelmann,Aniello Murano,Abdallah Saffidine,Michael Thielscher*

Main category: cs.LO

TL;DR: 论文研究了Game Description Language (GDL)描述中自动修复违反形式要求的问题，提出了最小修复的概念，并展示了基于Answer Set Programming的解决方案。


<details>
  <summary>Details</summary>
Motivation: GDL描述的正确性对非专家来说具有挑战性，论文旨在通过自动修复工具解决这一问题。

Method: 定义了GDL描述的最小修复问题，提供了相关计算问题的复杂性结果，并提出了基于Answer Set Programming的编码方法。

Result: 展示了该方法在自动修复定义不完善的游戏描述中的应用。

Conclusion: 论文为GDL描述的自动修复提供了理论框架和实用工具，有助于简化游戏设计过程。

Abstract: The Game Description Language (GDL) is a widely used formalism for specifying
the rules of general games. Writing correct GDL descriptions can be
challenging, especially for non-experts. Automated theorem proving has been
proposed to assist game design by verifying if a GDL description satisfies
desirable logical properties. However, when a description is proved to be
faulty, the repair task itself can only be done manually. Motivated by the work
on repairing unsolvable planning domain descriptions, we define a more general
problem of finding minimal repairs for GDL descriptions that violate formal
requirements, and we provide complexity results for various computational
problems related to minimal repair. Moreover, we present an Answer Set
Programming-based encoding for solving the minimal repair problem and
demonstrate its application for automatically repairing ill-defined game
descriptions.

</details>


### [17] [Modal definability in Euclidean modal logics](https://arxiv.org/abs/2508.10813)
*Philippe Balbiani,Tinko Tinchev*

Main category: cs.LO

TL;DR: 这篇论文研究了欧几里得模态逻辑框架类中模态可定义性问题的可计算性，并确定了导致该问题不可判定的逻辑类。


<details>
  <summary>Details</summary>
Motivation: 探讨欧几里得模态逻辑中模态可定义性问题的可计算性，以理解其复杂性。

Method: 通过分析欧几里得模态逻辑的框架类，确定哪些逻辑会导致模态可定义性问题不可判定。

Result: 确定了导致模态可定义性问题不可判定的欧几里得模态逻辑类。

Conclusion: 研究揭示了欧几里得模态逻辑中模态可定义性问题不可判定的条件，为相关领域提供了理论基础。

Abstract: This paper is about the computability of the modal definability problem in
classes of frames determined by Euclidean modal logics. We characterize those
Euclidean modal logics such that the classes of frames they determine give rise
to an undecidable modal definability problem.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training](https://arxiv.org/abs/2508.10160)
*Timon Merk,Saeed Salehi,Richard M. Koehler,Qiming Cui,Maria Olaru,Amelia Hahn,Nicole R. Provenza,Simon Little,Reza Abbasi-Asl,Phil A. Starr,Wolf-Julian Neumann*

Main category: cs.HC

TL;DR: 该论文提出了一种基于预训练基础模型的神经解码方法，用于个性化闭环神经调控治疗，能够无需患者特定训练即可解码帕金森病症状。


<details>
  <summary>Details</summary>
Motivation: 通过利用预训练的大规模基础模型，开发一种通用状态估计方法，解决传统神经解码中的患者个体训练需求问题。

Method: 使用慢性长期深部脑刺激记录数据训练基础模型，优化预训练损失函数以校正频率偏差，并采用30分钟的扩展上下文窗口。

Result: 在无需患者特定训练的情况下，通过留一受试者交叉验证成功解码帕金森病症状。

Conclusion: 该方法展示了基础模型在神经解码中的潜力，为闭环神经调控治疗提供了通用解决方案。

Abstract: Neural decoding of pathological and physiological states can enable
patient-individualized closed-loop neuromodulation therapy. Recent advances in
pre-trained large-scale foundation models offer the potential for generalized
state estimation without patient-individual training. Here we present a
foundation model trained on chronic longitudinal deep brain stimulation
recordings spanning over 24 days. Adhering to long time-scale symptom
fluctuations, we highlight the extended context window of 30 minutes. We
present an optimized pre-training loss function for neural electrophysiological
data that corrects for the frequency bias of common masked auto-encoder loss
functions due to the 1-over-f power law. We show in a downstream task the
decoding of Parkinson's disease symptoms with leave-one-subject-out
cross-validation without patient-individual training.

</details>


### [19] [Training Spatial Ability in Virtual Reality](https://arxiv.org/abs/2508.10195)
*Yiannos Demetriou,Manasvi Parikh,Sara Eskandari,Westley Weimer,Madeline Endres*

Main category: cs.HC

TL;DR: VR可以高效教授空间推理能力，效果与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 空间推理是STEM成功的关键技能，但弱势群体在此方面能力较低。VR被认为是一种可能的工具，但缺乏结构化课程评估。

Method: 将现有纸笔课程的三模块改编为VR课程，结合教育脚手架和实时反馈，通过24名本科生的实验评估效果。

Result: VR课程显著提升了空间能力，效果与传统课程无显著差异，且VR课程时间更短。学生反馈良好，晕动症发生率较低。

Conclusion: VR是一种高效教授空间推理的工具，具有推广潜力。

Abstract: Background: Spatial reasoning has been identified as a critical skill for
success in STEM. Unfortunately, under-represented groups often have lower
incoming spatial ability. Courses that improve spatial skills exist but are not
widely used. Virtual reality (VR) has been suggested as a possible tool for
teaching spatial reasoning since students are more accurate and complete
spatial tasks more quickly in three dimensions. However, no prior work has
developed or evaluated a fully-structured VR spatial skills course. Objectives:
We seek to assess the effectiveness of teaching spatial reasoning in VR, both
in isolation as a structured training curriculum and also in comparison to
traditional methods. Methods: We adapted three modules of an existing
pencil-and-paper course to VR, leveraging educational scaffolding and real-time
feedback in the design. We evaluated our three-week course in a study with
$n=24$ undergraduate introductory STEM students, capturing both quantitative
spatial ability gains (using pre- and post test scores on validated
assessments) and qualitative insights (from a post-study questionnaire). We
also compared our VR course to an offering of a baseline non-VR course (using
data collected in a previous study). Results and Conclusions: Students who took
our VR course had significant spatial ability gains. Critically, we find no
significant difference in outcomes between our VR course (3 meetings of 120
minutes each) and a baseline pencil and paper course (10 meetings of 90 minutes
each), suggesting that spatial reasoning can be very efficiently taught in VR.
We observed cybersickness at lower rates than are generally reported and most
students reported enjoying learning in VR.

</details>


### [20] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 研究探讨了领域特定术语对跨学科交流的阻碍，设计了实时个性化术语解释系统ParseJargon，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决跨学科交流中因专业术语导致的障碍。

Method: 通过日记研究发现问题，设计并测试了基于LLM的个性化术语解释系统ParseJargon。

Result: 个性化术语支持显著提升理解与参与感，而通用支持则产生负面影响。实地研究验证了实用性。

Conclusion: 个性化术语支持工具对跨学科交流和教育应用具有广泛潜力。

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>


### [21] [Facilitating Longitudinal Interaction Studies of AI Systems](https://arxiv.org/abs/2508.10252)
*Tao Long,Sitong Wang,Émilie Fabre,Tony Wang,Anup Sathya,Jason Wu,Savvas Petridis,Dingzeyu Li,Tuhin Chakrabarty,Yue Jiang,Jingyi Li,Tiffany Tseng,Ken Nakagaki,Qian Yang,Nikolas Martelaro,Jeffrey V. Nickerson,Lydia B. Chilton*

Main category: cs.HC

TL;DR: 摘要指出当前的UIST研究工具未能适应用户与AI动态互动的长期变化，提出了通过研讨会来解决纵向研究中部署、评估和数据收集的挑战。


<details>
  <summary>Details</summary>
Motivation: 用户与AI的交互随时间动态变化，一次性评估不足以捕捉这些变化，因此需要长期研究来更好地理解和优化工具。

Method: 通过组织研讨会，包括主题演讲、小组讨论和互动分组活动，聚焦于协议设计和工具原型制作，以提供实用的纵向研究策略。

Result: 研讨会的目标是为研究人员提供长期研究的方法和工具，促进纵向研究在UIST领域的普及和应用。

Conclusion: 研讨会旨在推动纵向研究作为一种更受欢迎的方法，用于UIST工具的设计、开发和评估。

Abstract: UIST researchers develop tools to address user challenges. However, user
interactions with AI evolve over time through learning, adaptation, and
repurposing, making one time evaluations insufficient. Capturing these dynamics
requires longer-term studies, but challenges in deployment, evaluation design,
and data collection have made such longitudinal research difficult to
implement. Our workshop aims to tackle these challenges and prepare researchers
with practical strategies for longitudinal studies. The workshop includes a
keynote, panel discussions, and interactive breakout groups for discussion and
hands-on protocol design and tool prototyping sessions. We seek to foster a
community around longitudinal system research and promote it as a more embraced
method for designing, building, and evaluating UIST tools.

</details>


### [22] [Artificial Emotion: A Survey of Theories and Debates on Realising Emotion in Artificial Intelligence](https://arxiv.org/abs/2508.10286)
*Yupei Li,Qiyang Sun,Michelle Schlicher,Yee Wen Lim,Björn W. Schuller*

Main category: cs.HC

TL;DR: 该论文探讨了人工智能是否需要发展内部情感状态以提升情感计算能力，并讨论了当前AE的早期表现及其未来潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨情感计算（AC）在人工智能（AI）中的作用，尤其是发展内部情感状态（AE）的价值，以推动人工通用智能（AGI）的发展。

Method: 综述了当前机器学习系统中AE的表现，研究了情感调制的架构，并总结了建模和整合AE的机制。

Result: 研究表明，AE可能为AI带来优势，但目前缺乏清晰的实现框架。

Conclusion: AE有望在未来对AI产生积极影响，但需关注伦理和安全问题。

Abstract: Affective Computing (AC) has enabled Artificial Intelligence (AI) systems to
recognise, interpret, and respond to human emotions - a capability also known
as Artificial Emotional Intelligence (AEI). It is increasingly seen as an
important component of Artificial General Intelligence (AGI). We discuss
whether in order to peruse this goal, AI benefits from moving beyond emotion
recognition and synthesis to develop internal emotion-like states, which we
term as Artificial Emotion (AE). This shift potentially allows AI to benefit
from the paradigm of `inner emotions' in ways we - as humans - do. Although
recent research shows early signs that AI systems may exhibit AE-like
behaviours, a clear framework for how emotions can be realised in AI remains
underexplored. In this paper, we discuss potential advantages of AE in AI,
review current manifestations of AE in machine learning systems, examine
emotion-modulated architectures, and summarise mechanisms for modelling and
integrating AE into future AI. We also explore the ethical implications and
safety risks associated with `emotional' AGI, while concluding with our opinion
on how AE could be beneficial in the future.

</details>


### [23] [Beyond Self-Regulated Learning Processes: Unveiling Hidden Tactics in Generative AI-Assisted Writing](https://arxiv.org/abs/2508.10310)
*Kaixun Yang,Yizhou Fan,Luzhen Tang,Mladen Raković,Xinyu Li,Dragan Gašević,Guanliang Chen*

Main category: cs.HC

TL;DR: 研究提出了一种新的方法，将自我调节学习（SRL）视为一个分层系统，利用隐马尔可夫模型（HMMs）分析学生在生成式AI辅助写作中的行为，揭示了三种不同的学习策略及其对成绩的影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GenAI）在教育中的应用，学生自我调节学习（SRL）的能力变得更为关键。然而，现有方法对SRL的假设过于简化，忽略了其动态性和非线性特征。

Method: 采用隐马尔可夫模型（HMMs），分析高等教育学生在GenAI辅助写作中的数字痕迹数据，将SRL建模为分层系统（策略-战术-行为）。

Result: 研究发现学生学习策略分为三类，且不同策略组的成绩存在显著差异。

Conclusion: 研究为SRL建模提供了新方法，并为GenAI教育环境中的自适应学习技术设计提供了理论支持。

Abstract: The integration of Generative AI (GenAI) into education is reshaping how
students learn, making self-regulated learning (SRL) - the ability to plan,
monitor, and adapt one's learning - more important than ever. To support
learners in these new contexts, it is essential to understand how SRL unfolds
during interaction with GenAI tools. Learning analytics offers powerful
techniques for analyzing digital trace data to infer SRL behaviors. However,
existing approaches often assume SRL processes are linear, segmented, and
non-overlapping-assumptions that overlook the dynamic, recursive, and
non-linear nature of real-world learning. We address this by conceptualizing
SRL as a layered system: observable learning patterns reflect hidden tactics
(short, purposeful action states), which combine into broader SRL strategies.
Using Hidden Markov Models (HMMs), we analyzed trace data from higher education
students engaged in GenAI-assisted academic writing. We identified three
distinct groups of learners, each characterized by different SRL strategies.
These groups showed significant differences in performance, indicating that
students' use of different SRL strategies in GenAI-assisted writing led to
varying task outcomes. Our findings advance the methodological toolkit for
modeling SRL and inform the design of adaptive learning technologies that more
effectively support learners in GenAI-enhanced educational environments.

</details>


### [24] [Mental Effort Estimation in Motion Exploration and Concept Generation Design Tasks using Inter-Band Relative Power Difference of EEG](https://arxiv.org/abs/2508.10353)
*G. Kalyan Ramana,Sumit Yempalle,Prasad S. Onkar*

Main category: cs.HC

TL;DR: 研究了设计师在探索运动相关产品概念时的认知现象，通过EEG测量脑力活动，提出新指标inter-BRPD来量化脑力负担。


<details>
  <summary>Details</summary>
Motivation: 静态草图难以准确表现运动，需理解设计师的认知过程以开发支持工具。

Method: 通过EEG记录32名参与者在运动探索和概念生成任务中的脑电活动，并用新指标inter-BRPD分析。

Result: inter-BRPD能有效量化脑力负担，参数更少且统计验证可靠。

Conclusion: 研究为支持概念设计及其评估提供了新方法。

Abstract: Conceptual design is a cognitively complex task, especially in the
engineering design of products having relative motion between components.
Designers prefer sketching as a medium for conceptual design and use gestures
and annotations to represent such relative motion. Literature suggests that
static representations of motion in sketches may not achieve the intended
functionality when realised, because it primarily depends on the designers'
mental capabilities for motion simulation. Thus, it is important to understand
the cognitive phenomena when designers are exploring concepts of articulated
products. The current work is an attempt to understand design neurocognition by
categorising the tasks and measuring the mental effort involved in these tasks
using EEG. The analysis is intended to validate design intervention tools to
support the conceptual design involving motion exploration. A novel EEG-based
metric, inter-Band Relative Power Difference (inter-BRPD), is introduced to
quantify mental effort. A design experiment is conducted with 32 participants,
where they have to perform one control task and 2 focus tasks corresponding to
the motion exploration task (MET) and the concept generation task (CGT),
respectively. EEG data is recorded during the 3 tasks, cleaned, processed and
analysed using the MNE library in Python. It is observed from the results that
inter-BRPD captures the essence of mental effort with half the number of
conventionally used parameters. The reliability and efficacy of the inter-BRPD
metric are also statistically validated against literature-based cognitive
metrics. With these new insights, the study opens up possibilities for creating
support for conceptual design and its evaluation.

</details>


### [25] ["Here Comes the Makeup Tutorial You Asked For!": Exploring Communication Strategies and Viewer Engagement in Beauty Videos on Rednote](https://arxiv.org/abs/2508.10364)
*Xueer Lin,Chenyu Li,Yuhan Lyu,Zhicong Lu,Zhenhui Peng*

Main category: cs.HC

TL;DR: 研究分析了美妆视频中的沟通策略及其对观众参与的影响，提出了分类标签并通过回归分析揭示了策略效果。


<details>
  <summary>Details</summary>
Motivation: 探究美妆视频创作者使用的沟通策略及其如何影响观众参与，以促进美妆知识的传播。

Method: 通过编码352个美妆视频，建立沟通策略分类（如家庭背景、展示化妆效果等），并计算分类观众评论。

Result: 回归分析显示沟通策略对观众参与的影响，例如结尾呼吁行动会增加讨论产品效果的评论。

Conclusion: 研究为美妆视频创作和知识传播提供了实用见解。

Abstract: More and more people, especially females, create and view beauty videos
covering topics like makeup tutorials and vlogs on social media platforms.
Understanding the communication strategies that creators use in these videos
and how they affect viewers' engagement can help spread beauty knowledge. By
coding 352 beauty videos in Rednote, this study presents a comprehensive
taxonomy of communication strategies used by the creators, such as using home
as the video background and displaying makeup effects when starting the
narrative at the beginning. We further label and computationally classify six
categories of comments that reveal viewers' engagement with beauty videos. The
regression analyses reveal the effects of beauty video communication strategies
on viewers' engagement; for example, calling viewers to take action at the end
tends to attract more comments that debate the product's efficacy. We discuss
insights into fostering the creation of beauty videos and the communication of
beauty knowledge.

</details>


### [26] [MCP2OSC: Parametric Control by Natural Language](https://arxiv.org/abs/2508.10414)
*Yuan-Yi Fan*

Main category: cs.HC

TL;DR: 该论文提出了一种结合文本提示和精确控制的MCP2OSC服务器，通过自然语言处理OSC消息，提升了人机协作的效率。


<details>
  <summary>Details</summary>
Motivation: 解决文本提示在复杂任务中精度不足的问题，同时避免传统旋钮控制的复杂性。

Method: 开发MCP服务器和提示设计准则，通过自然语言提示探索OSC控制。

Result: 成功生成、解释、验证和管理OSC消息，展示了LLM在多媒体设备控制中的潜力。

Conclusion: MCP2OSC为创意应用提供了新视角，可能成为基于LLM的通用多媒体控制机制。

Abstract: Text prompts enable intuitive content creation but may fall short in
achieving high precision for intricate tasks; knob or slider controls offer
precise adjustments at the cost of increased complexity. To address the gap
between knobs and prompts, a new MCP (Model Context Protocol) server and a
unique set of prompt design criteria are presented to enable exploring
parametric OSC (OpenSoundControl) control by natural language prompts.
Demonstrated by 14 practical QA examples with best practices and the
generalized prompt templates, this study finds Claude integrated with the
MCP2OSC server effective in generating OSC messages by natural language,
interpreting, searching, and visualizing OSC messages, validating and debugging
OSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine
collaboration by leveraging LLM (Large Language Model) to handle intricate OSC
development tasks, and by empowering human creativity with an intuitive
language interface featuring flexible precision controls: a prompt-based OSC
tool. This study provides a novel perspective on the creative MCP application
at the network protocol level by utilizing LLM's strength in directly
processing and generating human-readable OSC messages. The results suggest its
potential for a LLM-based universal control mechanism for multimedia devices.

</details>


### [27] [Stress Detection from Multimodal Wearable Sensor Data](https://arxiv.org/abs/2508.10468)
*Paul Schreiber,Beyza Cinar,Lennart Mackert,Maria Maleshkova*

Main category: cs.HC

TL;DR: 本文介绍了一个用于可穿戴情感计算研究的公开多模态数据集，旨在标准化压力实验和数据收集，并提供了压力检测基准。


<details>
  <summary>Details</summary>
Motivation: 目前公开的数据集和标准化协议在压力监测领域有限，因此需要一个多模态数据集以自动化压力识别系统的开发。

Method: 基于标准化框架收集生理和运动信号，区分四种情感/活动状态，并标注元数据和心理自评。

Result: 提出了一个基准测试，二元分类准确率为89％，多类分类准确率为82％。

Conclusion: 该数据集和框架为压力识别研究提供了重要资源。

Abstract: Human-Computer Interaction (HCI) is a multi-modal, interdisciplinary field
focused on designing, studying, and improving the interactions between people
and computer systems. This involves the design of systems that can recognize,
interpret, and respond to human emotions or stress. Developing systems to
monitor and react to stressful events can help prevent severe health
implications caused by long-term stress exposure. Currently, the publicly
available datasets and standardized protocols for data collection in this
domain are limited. Therefore, we introduce a multi-modal dataset intended for
wearable affective computing research, specifically the development of
automated stress recognition systems. We systematically review the publicly
available datasets recorded in controlled laboratory settings. Based on a
proposed framework for the standardization of stress experiments and data
collection, we collect physiological and motion signals from wearable devices
(e.g., electrodermal activity, photoplethysmography, three-axis accelerometer).
During the experimental protocol, we differentiate between the following four
affective/activity states: neutral, physical, cognitive stress, and
socio-evaluative stress. These different phases are meticulously labeled,
allowing for detailed analysis and reconstruction of each experiment. Meta-data
such as body positions, locations, and rest phases are included as further
annotations. In addition, we collect psychological self-assessments after each
stressor to evaluate subjects' affective states. The contributions of this
paper are twofold: 1) a novel multi-modal, publicly available dataset for
automated stress recognition, and 2) a benchmark for stress detection with 89\%
in a binary classification (baseline vs. stress) and 82\% in a multi-class
classification (baseline vs. stress vs. physical exercise).

</details>


### [28] [Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling](https://arxiv.org/abs/2508.10561)
*Andrea Gargano,Jasin Machkour,Mimma Nardelli,Enzo Pasquale Scilingo,Michael Muma*

Main category: cs.HC

TL;DR: 本研究通过分析情感计算中主观情绪与客观生理标记的关联，发现仅两种皮肤电信号特征能可靠预测情绪唤醒水平，强调了特征选择中严格可重复性的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算中主观情绪体验与客观生理标记的关联问题，并提高相关研究的可重复性。

Method: 使用T-Rex方法从心血管和皮肤电信号中提取164个特征，并与连续自报告的唤醒水平关联分析。

Result: 仅两种皮肤电信号特征表现出显著且可重复的关联，确认率为100%。

Conclusion: 研究强调了生理特征选择中严格可重复性的重要性，对安全关键应用（如心理健康识别和人机交互）具有重要意义。

Abstract: In Affective Computing, a key challenge lies in reliably linking subjective
emotional experiences with objective physiological markers. This preliminary
study addresses the issue of reproducibility by identifying physiological
features from cardiovascular and electrodermal signals that are associated with
continuous self-reports of arousal levels. Using the Continuously Annotated
Signal of Emotion dataset, we analyzed 164 features extracted from cardiac and
electrodermal signals of 30 participants exposed to short emotion-evoking
videos. Feature selection was performed using the Terminating-Random
Experiments (T-Rex) method, which performs variable selection systematically
controlling a user-defined target False Discovery Rate. Remarkably, among all
candidate features, only two electrodermal-derived features exhibited
reproducible and statistically significant associations with arousal, achieving
a 100\% confirmation rate. These results highlight the necessity of rigorous
reproducibility assessments in physiological features selection, an aspect
often overlooked in Affective Computing. Our approach is particularly promising
for applications in safety-critical environments requiring trustworthy and
reliable white box models, such as mental disorder recognition and human-robot
interaction systems.

</details>


### [29] [Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions](https://arxiv.org/abs/2508.10586)
*Birgit Nierula,Mustafa Tevfik Lafci,Anna Melnik,Mert Akgül,Farelle Toumaleu Siewe,Sebastian Bosse*

Main category: cs.HC

TL;DR: 研究发现，虚拟现实中个人空间侵犯和面部表情会影响生理和主观反应，不同生理指标反映不同方面。


<details>
  <summary>Details</summary>
Motivation: 探究虚拟现实中个人空间侵犯和面部表情对生理反应的影响，填补现有研究的空白。

Method: 采用2x2因子设计，测量皮肤电反应、心率变异性和不适感评分。

Result: 站立阶段空间侵犯时皮肤电反应更强，愤怒表情降低心率变异性并增加不适感。

Conclusion: 研究为虚拟环境中的多模态评估和更真实的交互设计提供了依据。

Abstract: Proxemics, the study of spatial behavior, is fundamental to social
interaction and increasingly relevant for virtual reality (VR) applications.
While previous research has established that users respond to personal space
violations in VR similarly as in real-world settings, phase-specific
physiological responses and the modulating effects of facial expressions remain
understudied. We investigated physiological and subjective responses to
personal space violations by virtual avatars, to understand how threatening
facial expressions and interaction phases (approach vs. standing) influence
these responses. Sixteen participants experienced a 2x2 factorial design
manipulating Personal Space (intrusion vs. respect) and Facial Expression
(neutral vs. angry) while we recorded skin conductance response (SCR), heart
rate variability (HRV), and discomfort ratings. Personal space boundaries were
individually calibrated using a stop-distance procedure. Results show that SCR
responses are significantly higher during the standing phase compared to the
approach phase when personal space was violated, indicating that prolonged
proximity within personal space boundaries is more physiologically arousing
than the approach itself. Angry facial expressions significantly reduced HRV,
reflecting decreased parasympathetic activity, and increased discomfort
ratings, but did not amplify SCR responses. These findings demonstrate that
different physiological modalities capture distinct aspects of proxemic
responses: SCR primarily reflects spatial boundary violations, while HRV
responds to facial threat cues. Our results provide insights for developing
comprehensive multi-modal assessments of social behavior in virtual
environments and inform the design of more realistic avatar interactions.

</details>


### [30] [DEV: A Driver-Environment-Vehicle Closed-Loop Framework for Risk-Aware Adaptive Automation of Driving](https://arxiv.org/abs/2508.10618)
*Anaïs Halin,Christel Devue,Marc Van Droogenbroeck*

Main category: cs.HC

TL;DR: DEV框架是一种闭环的风险感知自适应驾驶自动化系统，通过动态调整自动化水平以优化驾驶员与系统的协作。


<details>
  <summary>Details</summary>
Motivation: 自动化车辆的集成虽提升安全与舒适性，但也带来驾驶员分心、情境意识下降等新风险。

Method: 提出DEV框架，结合驾驶员、环境及车辆的动态交互，通过实时风险评估调整自动化级别。

Result: 框架支持平滑过渡与有效协作，并通过指标分类（驾驶员参与、环境复杂度、车辆介入）量化驾驶风险。

Conclusion: DEV框架为开发动态、风险感知的驾驶自动化系统提供了多学科研究指导。

Abstract: The increasing integration of automation in vehicles aims to enhance both
safety and comfort, but it also introduces new risks, including driver
disengagement, reduced situation awareness, and mode confusion. In this work,
we propose the DEV framework, a closed-loop framework for risk-aware adaptive
driving automation that captures the dynamic interplay between the driver, the
environment, and the vehicle. The framework promotes to continuously adjusting
the operational level of automation based on a risk management strategy. The
real-time risk assessment supports smoother transitions and effective
cooperation between the driver and the automation system. Furthermore, we
introduce a nomenclature of indexes corresponding to each core component,
namely driver involvement, environment complexity, and vehicle engagement, and
discuss how their interaction influences driving risk. The DEV framework offers
a comprehensive perspective to align multidisciplinary research efforts and
guide the development of dynamic, risk-aware driving automation systems.

</details>


### [31] [Are Electrodermal Activity-Based Indicators of Driver Cognitive Distraction Robust to Varying Traffic Conditions and Adaptive Cruise Control Use?](https://arxiv.org/abs/2508.10620)
*Anaïs Halin,Marc Van Droogenbroeck,Christel Devue*

Main category: cs.HC

TL;DR: 该研究通过模拟驾驶实验，探讨了皮肤电活动（EDA）如何反映驾驶员在不同交通条件和自适应巡航控制（ACC）使用下的认知分心。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解EDA能否有效反映驾驶员的认知分心，并探究其在不同驾驶环境和自动化使用下的变化。

Method: 研究通过六种驾驶场景，结合认知分心任务（有无心算任务）和驾驶环境复杂性（不同交通条件），并允许参与者自由开关ACC，分析了三种EDA指标（SCL、SCR幅度和SCR率）。

Result: 结果显示，所有EDA指标均显著受认知分心和ACC使用影响，环境复杂性仅影响SCL和SCR幅度，不影响SCR率。

Conclusion: 研究表明，EDA指标能反映驾驶员的心理负荷变化，不仅由认知分心引起，还与驾驶环境和自动化使用相关。

Abstract: In this simulator study, we investigate whether and how electrodermal
activity (EDA) reflects driver cognitive distraction under varying traffic
conditions and adaptive cruise control (ACC) use. Participants drove in six
scenarios, combining two levels of cognitive distraction (presence/absence of a
mental calculation task) and three levels of driving environment complexity
(different traffic conditions). Throughout the experiment, they were free to
activate or deactivate ACC (ACC use, two levels). We analyzed three EDA-based
indicators of cognitive distraction: SCL (mean skin conductance level), SCR
amplitude (mean amplitude of skin conductance responses), and SCR rate (rate of
skin conductance responses). Results indicate that all three indicators were
significantly influenced by cognitive distraction and ACC use, while
environment complexity influenced SCL and SCR amplitude, but not SCR rate.
These findings suggest that EDA-based indicators reflect variations in drivers'
mental workload due not only to cognitive distraction, but also to driving
environment and automation use.

</details>


### [32] [Gaze-Based Indicators of Driver Cognitive Distraction: Effects of Different Traffic Conditions and Adaptive Cruise Control Use](https://arxiv.org/abs/2508.10624)
*Anaïs Halin,Adrien Deliège,Christel Devue,Marc Van Droogenbroeck*

Main category: cs.HC

TL;DR: 研究了驾驶员认知分心在不同交通条件和自适应巡航控制（ACC）使用下的眼动参数变化。


<details>
  <summary>Details</summary>
Motivation: 探讨眼动参数如何反映驾驶员的认知分心，特别是在不同交通复杂度和ACC使用情况下的表现。

Method: 参与者完成六种驾驶场景，结合两种认知分心水平和三种交通复杂度，自由启用或停用ACC。分析了两种眼动指标：道路中心注视百分比和视线分散（水平和垂直）。

Result: 交通复杂度增加导致垂直视线分散增加，ACC使用使视线更集中于道路中心。认知分心减少道路中心注视并增加垂直分散。补充分析发现这些现象主要在非计算期间出现。

Conclusion: 眼动参数能有效反映认知分心，尤其在复杂交通和ACC使用情境下。计算期间的眼动集中可能是临时的分心表现。

Abstract: In this simulator study, we investigate how gaze parameters reflect driver
cognitive distraction under varying traffic conditions and adaptive cruise
control (ACC) use. Participants completed six driving scenarios that combined
two levels of cognitive distraction (with/without mental calculations) and
three levels of driving environment complexity. Throughout the experiment,
participants were free to activate or deactivate an ACC. We analyzed two
gaze-based indicators of driver cognitive distraction: the percent road center,
and the gaze dispersions (horizontal and vertical). Our results show that
vertical gaze dispersion increases with traffic complexity, while ACC use leads
to gaze concentration toward the road center. Cognitive distraction reduces
road center gaze and increases vertical dispersion. Complementary analyses
revealed that these observations actually arise mainly between mental
calculations, while periods of mental calculations are characterized by a
temporary increase in gaze concentration.

</details>


### [33] [Visualization of Electronic Health Record Sequences at Scale](https://arxiv.org/abs/2508.10700)
*Ambre Assor,Mickael Sereno,Jean-Daniel Fekete*

Main category: cs.HC

TL;DR: ParcoursVis是一种渐进式视觉分析工具，用于大规模探索电子健康记录序列，通过渐进算法快速展示近似结果并逐步优化，支持处理千万级患者数据。


<details>
  <summary>Details</summary>
Motivation: 现有工具处理大规模数据时耗时长且交互性差，无法满足实时需求，ParcoursVis通过渐进算法解决了这一问题。

Method: 使用渐进算法快速生成近似聚合结果（以冰柱树形式展示），并逐步优化，同时提供设计指南支持系统开发。

Result: ParcoursVis能处理数千万患者数据，性能提升3至5个数量级，支持罕见病或异常路径探索。

Conclusion: ParcoursVis显著提升了大规模医疗数据分析的交互性和可扩展性，已应用于真实数据集并开源。

Abstract: We present ParcoursVis, a Progressive Visual Analytics tool designed to
explore electronic health record sequences of patients at scale. Existing tools
process and aggregate the whole dataset upfront before showing the
visualization, taking a time proportional to the data size. Therefore, to
remain interactive, existing tools are limited to data sizes that can be
processed in under a few seconds to meet the latency constraints of human
attention. To overcome this limitation and scale to larger sizes, ParcoursVis
relies on a progressive algorithm that quickly shows an approximate initial
result of the aggregation, visualized as an Icicle tree, and improves it
iteratively, updating the visualization until the whole computation is done.
With its architecture, ParcoursVis remains interactive while visualizing the
sequences of tens of millions of patients, each described with thousands of
events; three to five orders of magnitude more than similar systems. Managing
large datasets allows for exploring rare medical conditions or unexpected
patient pathways, contributing to improving treatments. We describe the
algorithms we use and our evaluation concerning their scalability, convergence,
and stability. We also report on a set of guidelines to support visualization
designers in developing scalable progressive systems. ParcoursVis already
allows practitioners to perform analyses on two large real medical datasets.
Our prototype is open-source.

</details>


### [34] ["I Want My Chart to Be Just for Me": Community-Engaged Design to Support Outpatient Healthcare for Resettled Communities](https://arxiv.org/abs/2508.10757)
*Zhanming Chen,Juan F. Maestre,May Hang,Alisha Ghaju,Ji Youn Shin*

Main category: cs.HC

TL;DR: 论文探讨了重新安置人口在医疗保健中的挑战，提出了一种基于社区资源的资产导向方法，而非弥补不足，以提升医疗技术的相关性和可持续性。


<details>
  <summary>Details</summary>
Motivation: 重新安置的人口在门诊护理中面临文化和语言障碍，现有技术方案多关注弥补不足而非利用社区已有优势，限制了解方案的实用性。

Method: 通过两个社区参与式设计工作坊，与30名Hmong社区成员合作，识别社区已有的资源。

Result: 发现社区具备四种资源（如代际健康管理和基于故事的沟通方式），展示了参与式设计如何促进资产导向的方法。

Conclusion: 资产导向的技术设计能够更好地支持重新安置人口的门诊健康管理。

Abstract: Individuals resettled in a new environment often face challenges in accessing
adequate healthcare services, particularly within the complex processes of
outpatient clinic care. Cultural differences, language barriers, and low
socioeconomic status contribute to these difficulties. While previous studies
have identified barriers and proposed technology-mediated solutions for
resettled populations, many focus on addressing deficits rather than building
on the strengths these communities already possess, which limits the
sustainability and relevance of these solutions in everyday life. We conducted
two community-based participatory design workshops with 30 Hmong community
members in a large metropolitan area in the US. Through this process, we
identified four types of assets the community has gradually developed,
including intergenerational support for health management and
storytelling-based communication practices that facilitate relatable and
culturally grounded interactions. We show how participatory design workshops
can foster asset-based approaches, and discuss design implications for
technologies that leverage patients' existing strengths to support their health
management during outpatient visits.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [35] [B-repLer: Semantic B-rep Latent Editor using Large Language Models](https://arxiv.org/abs/2508.10201)
*Yilin Liu,Niladri Shekhar Dutt,Changjian Li,Niloy J. Mitra*

Main category: cs.GR

TL;DR: 该论文研究了如何将多模态大语言模型（mLLMs）应用于边界表示（B-rep）CAD对象的高层次编辑，提出了B-repLer模型，该模型通过独特的多模态架构和自动生成的数据集实现文本驱动的语义编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管mLLMs在许多视觉和图形任务中表现出色，但在3D分析及编辑任务中仍表现有限。论文旨在探索如何使mLLMs适应B-rep CAD对象的编辑需求，解决其因数据稀缺和3D表示特殊性带来的挑战。

Method: 提出了B-repLer模型，这是一种针对B-rep CAD对象设计的微调mLLM。通过新颖的多模态架构处理B-rep数据，并利用现有CAD工具自动生成训练所需的推理数据集，无需外部标注。

Result: 实验表明，B-repLer能够理解文本提示并对B-rep进行语义编辑，生成有效的CAD对象，解决了此前无法实现的基于文本的复杂B-rep编辑问题。

Conclusion: B-repLer成功地将mLLMs应用于B-rep CAD编辑任务，通过自动生成数据集和独特架构填补了该领域的空白，展示了在实际工程应用中的潜力。

Abstract: Multimodal large language models (mLLMs), trained in a mixed modal setting as
a universal model, have been shown to compete with or even outperform many
specialized algorithms for imaging and graphics tasks. As demonstrated across
many applications, mLLMs' ability to jointly process image and text data makes
them suitable for zero-shot applications or efficient fine-tuning towards
specialized tasks. However, they have had limited success in 3D analysis and
editing tasks. This is due to both the lack of suitable (annotated) 3D data as
well as the idiosyncrasies of 3D representations. In this paper, we investigate
whether mLLMs can be adapted to support high-level editing of Boundary
Representation (B-rep) CAD objects. B-reps remain the industry-standard for
precisely encoding engineering objects, but are challenging as the
representation is fragile (i.e. can easily lead to invalid CAD objects) and no
publicly available data source exists with semantically-annotated B-reps or CAD
construction history. We present B-repLer as a finetuned mLLM that can
understand text prompts and make semantic edits on given B-Reps to produce
valid outputs. We enable this via a novel multimodal architecture, specifically
designed to handle B-rep models, and demonstrate how existing CAD tools, in
conjunction with mLLMs, can be used to automatically generate the required
reasoning dataset, without relying on external annotations. We extensively
evaluate B-repLer and demonstrate several text-based B-rep edits of various
complexity, which were not previously possible.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [36] [Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization](https://arxiv.org/abs/2508.10141)
*Laura Lawniczak,Tobias Distler*

Main category: cs.DC

TL;DR: ShellFT框架通过微复制概念，让系统设计者自由选择需要抵御拜占庭故障的部分，减少了传统混合方法的多样化成本70%以上。


<details>
  <summary>Details</summary>
Motivation: 现有混合故障模型在灵活性和多样性开销方面存在不足，需要更有效的解决方案。

Method: 提出ShellFT框架，利用微复制技术，允许选择性地增强复制逻辑的拜占庭容错能力。

Result: 设计了三个定制协议，分析其复杂度，并证明能减少70%以上的多样化成本。

Conclusion: ShellFT显著提升了混合系统的灵活性和效率，适用于不同用例需求。

Abstract: Hybrid fault models are known to be an effective means for enhancing the
robustness of consensus-based replicated systems. However, existing
hybridization approaches suffer from limited flexibility with regard to the
composition of crash-tolerant and Byzantine fault-tolerant system parts and/or
are associated with a significant diversification overhead. In this paper we
address these issues with ShellFT, a framework that leverages the concept of
micro replication to allow system designers to freely choose the parts of the
replication logic that need to be resilient against Byzantine faults. As a key
benefit, such a selective hybridization makes it possible to develop hybrid
solutions that are tailored to the specific characteristics and requirements of
individual use cases. To illustrate this flexibility, we present three custom
ShellFT protocols and analyze the complexity of their implementations. Our
evaluation shows that compared with traditional hybridization approaches,
ShellFT is able to decrease diversification costs by more than 70%.

</details>


### [37] [Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices](https://arxiv.org/abs/2508.10202)
*Sreeram Venkat,Kasia Swirydowicz,Noah Wolfe,Omar Ghattas*

Main category: cs.DC

TL;DR: 论文提出了一种基于Hipify的性能可移植框架，用于将FFTMatvec应用从CUDA扩展到AMD GPU，并通过动态混合精度框架优化性能。


<details>
  <summary>Details</summary>
Motivation: 领导级计算设施的硬件多样性和GPU在低精度计算中的性能优势，促使科学HPC工作流采用混合精度算法和性能可移植模型。

Method: 使用Hipify框架实现性能可移植，应用于FFTMatvec应用；通过动态混合精度框架和Pareto前沿分析优化配置。

Result: FFTMatvec在AMD GPU上表现出色，优化直接集成到rocBLAS库中；混合精度框架在OLCF Frontier超级计算机上扩展到2,048 GPU。

Conclusion: 该框架成功实现了性能可移植和混合精度优化，为HPC应用提供了高效解决方案。

Abstract: The hardware diversity displayed in leadership-class computing facilities,
alongside the immense performance boosts exhibited by today's GPUs when
computing in lower precision, provide a strong incentive for scientific HPC
workflows to adopt mixed-precision algorithms and performance portability
models. We present an on-the-fly framework using Hipify for performance
portability and apply it to FFTMatvec-an HPC application that computes
matrix-vector products with block-triangular Toeplitz matrices. Our approach
enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD
GPUs with excellent observed performance. Performance optimizations for AMD
GPUs are integrated directly into the open-source rocBLAS library, keeping the
application code unchanged. We then present a dynamic mixed-precision framework
for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision
configuration for a desired error tolerance. Results are shown for AMD Instinct
MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,
mixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier
supercomputer.

</details>


### [38] [GPZ: GPU-Accelerated Lossy Compressor for Particle Data](https://arxiv.org/abs/2508.10305)
*Ruoyu Li,Yafan Huang,Longtao Zhang,Zhuoxun Yang,Sheng Di,Jiajun Huang,Jinyang Liu,Jiannan Tian,Xin Liang,Guanpeng Li,Hanqi Guo,Franck Cappello,Kai Zhao*

Main category: cs.DC

TL;DR: 介绍了一种名为GPZ的高性能、误差有界的无损压缩器，专门设计用于现代GPU上的大规模粒子数据压缩，显著提升了压缩效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统压缩技术在应对不规则粒子分布和GPU架构限制时表现不佳，导致吞吐量有限和压缩比不理想，需要针对GPU优化的压缩解决方案。

Method: GPZ采用了一种新颖的四阶段并行流水线设计，结合计算、内存访问和GPU占用率的优化策略，以实现接近硬件极限的吞吐量。

Result: GPZ在三种不同GPU架构上测试了六种大规模科学数据集，结果显著优于五种现有GPU压缩器，吞吐量最高提升8倍，同时保持更好的压缩比和数据质量。

Conclusion: GPZ展示了其在处理大规模粒子数据时的高效性和适应性，为相关领域的实时分析提供了可行的解决方案。

Abstract: Particle-based simulations and point-cloud applications generate massive,
irregular datasets that challenge storage, I/O, and real-time analytics.
Traditional compression techniques struggle with irregular particle
distributions and GPU architectural constraints, often resulting in limited
throughput and suboptimal compression ratios. In this paper, we present GPZ, a
high-performance, error-bounded lossy compressor designed specifically for
large-scale particle data on modern GPUs. GPZ employs a novel four-stage
parallel pipeline that synergistically balances high compression efficiency
with the architectural demands of massively parallel hardware. We introduce a
suite of targeted optimizations for computation, memory access, and GPU
occupancy that enables GPZ to achieve near-hardware-limit throughput. We
conduct an extensive evaluation on three distinct GPU architectures
(workstation, data center, and edge) using six large-scale, real-world
scientific datasets from five distinct domains. The results demonstrate that
GPZ consistently and significantly outperforms five state-of-the-art GPU
compressors, delivering up to 8x higher end-to-end throughput while
simultaneously achieving superior compression ratios and data quality.

</details>


### [39] [Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models](https://arxiv.org/abs/2508.10349)
*Tianjun Yuan,Jiaxiang Geng,Pengchao Han,Xianhao Chen,Bing Luo*

Main category: cs.DC

TL;DR: 论文提出了一种灵活的个性化联邦学习框架FlexP-SFL，通过分割学习和资源适应策略，解决了客户端数据有限和异构分布的问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在客户端数据有限且分布异构时效果不佳，需要一种灵活的方法来适应不同资源约束并提升个性化性能。

Method: 提出FlexP-SFL框架，结合分割学习，客户端根据资源约束部分本地训练模型，其余部分交给服务器处理，并引入对齐策略优化全局数据表现。

Result: 实验表明，FlexP-SFL在个性化微调效率和最终准确率上优于基线模型。

Conclusion: FlexP-SFL通过灵活的资源分配和对齐策略，有效提升了联邦学习在个性化任务中的性能。

Abstract: Fine-tuning foundation models is critical for superior performance on
personalized downstream tasks, compared to using pre-trained models.
Collaborative learning can leverage local clients' datasets for fine-tuning,
but limited client data and heterogeneous data distributions hinder effective
collaboration. To address the challenge, we propose a flexible personalized
federated learning paradigm that enables clients to engage in collaborative
learning while maintaining personalized objectives. Given the limited and
heterogeneous computational resources available on clients, we introduce
\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based on
split learning, FlexP-SFL allows each client to train a portion of the model
locally while offloading the rest to a server, according to resource
constraints. Additionally, we propose an alignment strategy to improve
personalized model performance on global data. Experimental results show that
FlexP-SFL outperforms baseline models in personalized fine-tuning efficiency
and final accuracy.

</details>


### [40] [Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster](https://arxiv.org/abs/2508.10481)
*Adrien Cassagne,Noé Amiot,Manuel Bouyer*

Main category: cs.DC

TL;DR: Dalek是一个实验性计算集群，用于评估消费级异构硬件在软件设计、原型开发和算法开发中的性能，对比传统计算中心更经济且多功能。


<details>
  <summary>Details</summary>
Motivation: 传统计算中心依赖昂贵的服务器级组件，而Dalek通过集成消费级硬件提供更经济的替代方案。

Method: 详细介绍了集群架构、软件栈及合成基准测试结果，并使用高精度能源监控平台进行实验。

Result: 展示了消费级硬件的性能潜力，并提供了能源感知研究的实验能力。

Conclusion: Dalek为计算机科学应用提供了一个经济高效且多样化的实验平台。

Abstract: Dalek is an experimental compute cluster designed to evaluate the performance
of heterogeneous, consumer-grade hardware for software design, prototyping, and
algorithm development. In contrast to traditional computing centers that rely
on costly, server-class components, Dalek integrates CPUs and GPUs typically
found in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet
versatile platform. This document details the cluster's architecture and
software stack, and presents results from synthetic benchmarks. Furthermore, it
introduces a custom energy monitoring platform capable of delivering 1000
averaged samples per second with milliwatt-level resolution. This
high-precision monitoring capability enables a wide range of energy-aware
research experiments in applied Computer Science.

</details>


### [41] [Introducing CQ: A C-like API for Quantum Accelerated HPC](https://arxiv.org/abs/2508.10854)
*Oliver Thomson Brown,Mateusz Meller,James Richings*

Main category: cs.DC

TL;DR: 本文介绍了CQ规范及其参考实现CQ-SimBE，旨在将量子计算逐步集成到经典HPC代码中。


<details>
  <summary>Details</summary>
Motivation: 通过支持从C和Fortran等语言运行时卸载量子计算，CQ专注于在严格和强类型编译语言中实现量子计算的兼容性。

Method: CQ-SimBE是基于QuEST状态向量模拟器的C99实现，支持实验新功能如模拟量子计算。

Result: CQ和CQ-SimBE均为开源，提供了量子计算与经典HPC代码集成的实用工具。

Conclusion: CQ规范及其实现为量子计算在HPC领域的应用提供了灵活的实验平台。

Abstract: In this paper we present CQ, a specification for a C-like API for quantum
accelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written
in C99, and built on top of the statevector simulator QuEST. CQ focuses on
enabling the incremental integration of quantum computing into classical HPC
codes by supporting runtime offloading from languages such as C and Fortran. It
provides a way of describing and offloading quantum computations which is
compatible with strictly and strongly typed compiled languages, and gives the
programmer fine-grained control over classical data movement. The CQ Simulated
Backend (CQ-SimBE) provides both a way to demonstrate the usage and utility of
CQ, and a space to experiment with new features such as support for analogue
quantum computing. Both the CQ specification and CQ-SimBE are open-source, and
available in public repositories.

</details>


### [42] [Minimmit: Fast Finality with Even Faster Blocks](https://arxiv.org/abs/2508.10862)
*Brendan Kobayashi Chou,Andrew Lewis-Pye,Patrick O'Grady*

Main category: cs.DC

TL;DR: Minimmit是一种新的状态机复制协议，通过更快地切换视图减少延迟，扩展了Alpenglow等协议的'两轮终结'方法。


<details>
  <summary>Details</summary>
Motivation: 目标是进一步减少延迟，提升状态机复制的效率。

Method: 采用伪代码实现，证明了一致性和活跃性。

Result: 协议初步验证有效，但仍有优化和实验待完成。

Conclusion: 该协议展示了潜在的低延迟优势，后续将进一步完善。

Abstract: Minimmit is a new protocol for State-Machine-Replication (SMR) that extends
the '2-round finality' approach of protocols such as Alpenglow to further
reduce latency, by allowing for faster progression through 'views'. This
preliminary draft provides motivation and pseudocode, together with proofs of
consistency and liveness. An updated draft with a proof of optimistic
responsiveness, suggested optimizations, and experiments, is to follow.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [43] [Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional Data](https://arxiv.org/abs/2508.10373)
*Yingfan Liu,Yandi Zhang,Jiadong Xie,Hui Li,Jeffrey Xu Yu,Jiangtao Cui*

Main category: cs.DB

TL;DR: 提出了一种在云计算中保护隐私的近似最近邻搜索新方法，通过单云服务器执行优化了效率，同时引入新型加密和索引技术确保数据隐私与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护近似最近邻搜索（PP-ANNS）方案无法同时满足隐私、效率、准确性和用户参与度的需求。

Method: 提出了基于单云服务器的PP-ANNS解决方案，使用距离比较加密保护数据隐私，设计了隐私保护索引和过滤-精炼搜索策略。

Result: 实验表明该方法比现有方案快3个数量级，且不影响准确性。

Conclusion: 新方法在隐私保护、效率与准确性上均优于现有方案，适用于云计算环境。

Abstract: In the era of cloud computing and AI, data owners outsource ubiquitous
vectors to the cloud, which furnish approximate $k$-nearest neighbors
($k$-ANNS) services to users. To protect data privacy against the untrusted
server, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental
and urgent problem. However, existing PP-ANNS solutions fall short of meeting
the requirements of data privacy, efficiency, accuracy, and minimal user
involvement concurrently. To tackle this challenge, we introduce a novel
solution that primarily executes PP-ANNS on a single cloud server to avoid the
heavy communication overhead between the cloud and the user. To ensure data
privacy, we introduce a novel encryption method named distance comparison
encryption, facilitating secure, efficient, and exact distance comparisons. To
optimize the trade-off between data privacy and search performance, we design a
privacy-preserving index that combines the state-of-the-art $k$-ANNS method
with an approximate distance computation method. Then, we devise a search
method using a filter-and-refine strategy based on the index. Moreover, we
provide the security analysis of our solution and conduct extensive experiments
to demonstrate its superiority over existing solutions. Based on our
experimental results, our method accelerates PP-ANNS by up to 3 orders of
magnitude compared to state-of-the-art methods, while not compromising the
accuracy.

</details>


### [44] [Cross-Organizational Analysis of Parliamentary Processes: A Case Study](https://arxiv.org/abs/2508.10381)
*Paul-Julius Hillmann,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: 该论文首次将流程挖掘应用于议会流程，分析了德国三个州议会的立法流程，揭示了它们的差异和最佳实践。


<details>
  <summary>Details</summary>
Motivation: 尽管流程挖掘在企业中广泛应用，但跨组织流程比较研究较少，因为企业通常不愿共享数据。本文利用德国州议会（依法必须公开数据）的流程数据，填补了这一研究空白。

Method: 通过流程挖掘技术分析三个德国州议会的立法流程，并结合政治学家和德国联邦议会专家的知识交流。

Result: 研究揭示了不同州议会之间的流程差异和最佳实践，并讨论了结果的现实意义。

Conclusion: 论文为政治学与流程挖掘的跨学科研究开辟了新方向，并为未来相关研究提供了参考。

Abstract: Process Mining has been widely adopted by businesses and has been shown to
help organizations analyze and optimize their processes. However, so far,
little attention has gone into the cross-organizational comparison of
processes, since many companies are hesitant to share their data. In this
paper, we explore the processes of German state parliaments that are often
legally required to share their data and run the same type of processes for
different geographical regions. This paper is the first attempt to apply
process mining to parliamentary processes and, therefore, contributes toward a
novel interdisciplinary research area that combines political science and
process mining. In our case study, we analyze legislative processes of three
German state parliaments and generate insights into their differences and best
practices. We provide a discussion of the relevance of our results that are
based on knowledge exchange with a political scientist and a domain expert from
the German federal parliament.

</details>


### [45] [Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching](https://arxiv.org/abs/2508.10460)
*Wei Tian,Jieming Shi,Man Lung Yiu*

Main category: cs.DB

TL;DR: 论文提出了 TRMMA 和 MMA 两种方法，分别用于稀疏轨迹的恢复和地图匹配，通过高效技术显著提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 现实中的 GPS 轨迹通常稀疏且与路网不对齐，需要高效方法提升数据质量以支持应用需求。

Method: MMA 通过分类任务将稀疏轨迹映射到小候选路段集；TRMMA 利用 MMA 结果高效推断缺失点，结合双变换器编码和解码技术。

Result: 实验表明 TRAMMA 和 MMA 在 4 个大数据集上优于现有方法，效果显著。

Conclusion: TRMMA 和 MMA 为稀疏轨迹恢复和地图匹配提供了高效准确的解决方案。

Abstract: Real-world trajectories are often sparse with low-sampling rates (i.e., long
intervals between consecutive GPS points) and misaligned with road networks,
yet many applications demand high-quality data for optimal performance. To
improve data quality with sparse trajectories as input, we systematically study
two related research problems: trajectory recovery on road network, which aims
to infer missing points to recover high-sampling trajectories, and map
matching, which aims to map GPS points to road segments to determine underlying
routes. In this paper, we present efficient methods TRMMA and MMA for accurate
trajectory recovery and map matching, respectively, where MMA serves as the
first step of TRMMA. In MMA, we carefully formulate a classification task to
map a GPS point from sparse trajectories to a road segment over a small
candidate segment set, rather than the entire road network. We develop
techniques in MMA to generate effective embeddings that capture the patterns of
GPS data, directional information, and road segments, to accurately align
sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the
segments in the route returned by MMA to infer missing points with position
ratios on road segments, producing high-sampling trajectories efficiently by
avoiding evaluation of all road segments. Specifically, in TRMMA, we design a
dual-transformer encoding process to cohesively capture latent patterns in
trajectories and routes, and an effective decoding technique to sequentially
predict the position ratios and road segments of missing points. We conduct
extensive experiments to compare TRMMA and MMA with numerous existing methods
for trajectory recovery and map matching, respectively, on 4 large real-world
datasets. TRMMA and MMA consistently achieve the best result quality, often by
a significant margin.

</details>


### [46] [Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria](https://arxiv.org/abs/2508.10504)
*Zhliang Xiang,Meghyn Bienvenu,Gianluca Cima,Víctor Gutiérrez-Basulto,Yazmín Ibáñez-García*

Main category: cs.DB

TL;DR: ASPEN+扩展了ASPEN系统，引入了局部合并和新优化标准，改进了实体解析功能。


<details>
  <summary>Details</summary>
Motivation: 现有ASPEN仅支持全局合并，无法处理局部合并需求，ASPEN+通过新功能和优化标准解决这一问题。

Method: 引入局部合并和多种优化标准（如最小化规则冲突、最大化支持规则的合并数），并进行形式化和计算分析。

Result: 实验证实，局部合并和新优化标准显著提高了准确性和运行效率。

Conclusion: ASPEN+通过支持局部合并和优化标准，有效提升了实体解析的性能和适用性。

Abstract: In this paper, we present ASPEN+, which extends an existing ASP-based system,
ASPEN,for collective entity resolution with two important functionalities:
support for local merges and new optimality criteria for preferred solutions.
Indeed, ASPEN only supports so-called global merges of entity-referring
constants (e.g. author ids), in which all occurrences of matched constants are
treated as equivalent and merged accordingly. However, it has been argued that
when resolving data values, local merges are often more appropriate, as e.g.
some instances of 'J. Lee' may refer to 'Joy Lee', while others should be
matched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+
offers new optimality criteria for selecting solutions, such as minimizing rule
violations or maximising the number of rules supporting a merge. Our main
contributions are thus (1) the formalisation and computational analysis of
various notions of optimal solution, and (2) an extensive experimental
evaluation on real-world datasets, demonstrating the effect of local merges and
the new optimality criteria on both accuracy and runtime.

</details>


### [47] [Emerging Skycube](https://arxiv.org/abs/2508.10516)
*Mickaël Martin Nevot*

Main category: cs.DB

TL;DR: 该论文提出了一种结合多准则决策分析和趋势反转发现的方法——Emerging Skycube，用于高效提取全局最优数据并分析其演变。


<details>
  <summary>Details</summary>
Motivation: 为了解决DBMS中缺乏集成工具来计算Emerging Skycube的问题，并充分利用ROLAP分析工具。

Method: 通过结合Skycube和新兴数据立方体，提出了Emerging Skycube概念，并设计了两种计算优化方法：部分无损物化和小型化但无损的L-Skycube。

Result: 研究表明，Emerging Skycube的计算成本比新兴数据立方体低，且通过优化方法可进一步节省计算时间和存储空间。

Conclusion: Emerging Skycube为多准则决策分析提供了一种高效且灵活的解决方案，同时通过优化技术显著提升了计算效率。

Abstract: Combining multi-criteria decision analysis and trend reversal discovery make
it possible to extract globally optimal, or non-dominated, data in relation to
several criteria, and then to observe their evolution according to a
decision-making property. Thus, we introduce Emerging Skycube, a concept
associating Skycube and emerging datacube. As far as we know, no
DBMS-integrated solution exists to compute an emerging Skycube, and hence
taking advantage of ROLAP analysis tools. An emerging datacube has only one
measure: we propose to use several to comply to multi-criteria decision
analysis constraints which requires multiple attributes. A datacube is
expensive to compute. An emerging datacube is about twice as expensive. On the
other hand, an emerging Skycube is cheaper as the trend reversal is computed
after two Skycube calculations, which considerably reduces the relation volume
in comparison with the initial one. It is possible to save even more computing
time and storage space. To this end, we propose two successive reductions.
First, a Skycube lossless partial materialisation using Skylines concepts
lattice, based on the agree concepts lattice and partitions lattice. Then,
either the closed emerging Skycube for an information-loss reduction, or the
closed emerging L-Skycube for a smaller but lossless reduction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [48] [DiffAxE: Diffusion-driven Hardware Accelerator Generation and Design Space Exploration](https://arxiv.org/abs/2508.10303)
*Arkapravo Ghosh,Abhishek Moitra,Abhiroop Bhattacharjee,Ruokai Yin,Priyadarshini Panda*

Main category: cs.AR

TL;DR: 该论文提出了一种生成式方法，用于硬件设计空间探索（DSE），通过1D图像合成建模硬件性能映射，显著提升了优化效率和结果质量。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载（如DNN和LLM）的复杂性增加，硬件加速器的设计空间变得庞大且复杂，传统优化方法效率低下或无法适用，因此需要一种更高效的方法来解决这一挑战。

Method: 论文提出了一种生成式方法，将硬件设计建模为基于目标性能的1D图像合成，从而高效学习非可微、非双射的硬件性能映射。

Result: 该方法在生成误差上比贝叶斯优化低0.86%，速度提升17000倍；在结构化DSE设置中，能效延迟积（EDP）降低9.8%，性能提高6%，搜索速度最高提升1312倍。

Conclusion: 生成式DSE方法在复杂设计空间中显著优于传统优化技术，为AI硬件加速提供了高效的设计探索工具。

Abstract: Design space exploration (DSE) is critical for developing optimized hardware
architectures, especially for AI workloads such as deep neural networks (DNNs)
and large language models (LLMs), which require specialized acceleration. As
model complexity grows, accelerator design spaces have expanded to O(10^17),
becoming highly irregular, non-convex, and exhibiting many-to-one mappings from
design configurations to performance metrics. This complexity renders direct
inverse derivation infeasible and necessitates heuristic or sampling-based
optimization. Conventional methods - including Bayesian optimization, gradient
descent, reinforcement learning, and genetic algorithms - depend on iterative
sampling, resulting in long runtimes and sensitivity to initialization. Deep
learning-based approaches have reframed DSE as classification using
recommendation models, but remain limited to small-scale (O(10^3)), less
complex design spaces. To overcome these constraints, we propose a generative
approach that models hardware design as 1-D image synthesis conditioned on
target performance, enabling efficient learning of non-differentiable,
non-bijective hardware-performance mappings. Our framework achieves 0.86% lower
generation error than Bayesian optimization with a 17000x speedup, and
outperforms GANDSE with 30% lower error at only 1.83x slower search. We further
extend the method to a structured DSE setting, attaining 9.8% lower
energy-delay product (EDP) and 6% higher performance, with up to 145.6x and
1312x faster search compared to existing optimization methods on O(10^17)
design spaces. For LLM inference, our method achieves 3.37x and 7.75x lower EDP
on a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the
state-of-the-art DOSA framework.

</details>


### [49] [AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design](https://arxiv.org/abs/2508.10409)
*Zihao Chen,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Xinyi Yang,Mingjie Zhou,Zhuoyao Du,Xu Yan,Zhouyang Wu,Zhenyu Xu,Jiangli Huang,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AR

TL;DR: 提出了开源模拟电路设计基础模型AnalogSeeker，通过知识蒸馏和数据集构建方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计领域数据稀缺，需要整合领域知识并提供设计辅助。

Method: 采用领域知识蒸馏方法，构建细粒度学习节点，并开发了自定义的微调训练范式。

Result: 模型在AMSBench-TQA基准上达到85.04%准确率，比原模型提升15.67%。

Conclusion: AnalogSeeker在模拟电路设计任务中表现优异，并开源供研究使用。

Abstract: In this paper, we propose AnalogSeeker, an effort toward an open-source
foundation language model for analog circuit design, with the aim of
integrating domain knowledge and giving design assistance. To overcome the
scarcity of data in this field, we employ a corpus collection strategy based on
the domain knowledge framework of analog circuits. High-quality, accessible
textbooks across relevant subfields are systematically curated and cleaned into
a textual domain corpus. To address the complexity of knowledge of analog
circuits, we introduce a granular domain knowledge distillation method. Raw,
unlabeled domain corpus is decomposed into typical, granular learning nodes,
where a multi-agent framework distills implicit knowledge embedded in
unstructured text into question-answer data pairs with detailed reasoning
processes, yielding a fine-grained, learnable dataset for fine-tuning. To
address the unexplored challenges in training analog circuit foundation models,
we explore and share our training methods through both theoretical analysis and
experimental validation. We finally establish a fine-tuning-centric training
paradigm, customizing and implementing a neighborhood self-constrained
supervised fine-tuning algorithm. This approach enhances training outcomes by
constraining the perturbation magnitude between the model's output
distributions before and after training. In practice, we train the
Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%
accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,
with a 15.67% point improvement over the original model and is competitive with
mainstream commercial models. Furthermore, AnalogSeeker also shows
effectiveness in the downstream operational amplifier design task. AnalogSeeker
is open-sourced at https://huggingface.co/analogllm/analogseeker for research
use.

</details>


### [50] [THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures](https://arxiv.org/abs/2508.10691)
*Alish Kanani,Lukas Pfromm,Harsh Sharma,Janardhan Rao Doppa,Partha Pratim Pande,Umit Y. Ogras*

Main category: cs.AR

TL;DR: 提出了一种名为THERMOS的框架，用于在异构多芯片PIM架构上调度AI工作负载，通过多目标强化学习实现优化。


<details>
  <summary>Details</summary>
Motivation: 为了解决在异构多芯片PIM架构上调度AI工作负载的挑战，包括性能目标冲突、动态负载特性以及热和功率约束。

Method: 开发了THERMOS框架，利用多目标强化学习（MORL）训练单一策略，以实现运行时根据目标偏好优化执行时间、能耗或平衡目标。

Result: 评估显示，THERMOS比基线算法平均执行时间快89%，能耗降低57%，仅带来0.14%运行时和0.022%能耗开销。

Conclusion: THERMOS在异构多芯片PIM架构上高效调度AI工作负载，显著提升了性能和能效。

Abstract: Chiplet-based integration enables large-scale systems that combine diverse
technologies, enabling higher yield, lower costs, and scalability, making them
well-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a
promising solution for AI inference, leveraging technologies such as ReRAM,
SRAM, and FeFET, each offering unique advantages and trade-offs. A
heterogeneous chiplet-based PIM architecture can harness the complementary
strengths of these technologies to enable higher performance and energy
efficiency. However, scheduling AI workloads across such a heterogeneous system
is challenging due to competing performance objectives, dynamic workload
characteristics, and power and thermal constraints. To address this need, we
propose THERMOS, a thermally-aware, multi-objective scheduling framework for AI
workloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a
single multi-objective reinforcement learning (MORL) policy that is capable of
achieving Pareto-optimal execution time, energy, or a balanced objective at
runtime, depending on the target preferences. Comprehensive evaluations show
that THERMOS achieves up to 89% faster average execution time and 57% lower
average energy consumption than baseline AI workload scheduling algorithms with
only 0.14% runtime and 0.022% energy overhead.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 提出了首个统一的评估框架，用于衡量个体倾向学习方法是否真正捕捉到标注者行为和提供有意义的解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏评估个体倾向学习和行为解释的有效性，因此需要新的评估标准。

Method: 提出两种新指标（DIC和BAE），通过比较预测和真实标注者相似性来评估模型表现。

Result: 实验验证了评估框架的有效性。

Conclusion: 新框架填补了现有空白，为个体倾向学习提供了有效评估工具。

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [52] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 该论文提出了一种针对扩散大语言模型（Diffusion LLMs）的约束解码方法，能够确保生成的输出符合上下文无关文法（CFGs）定义的正式语言。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在许多领域表现出色，但其输出无法保证符合正式语言的语法约束。现有约束解码方法不适用于新兴的扩散LLMs，因此需要一种新方法来解决这一问题。

Method: 论文将约束解码问题转化为更一般的“加法填空”问题，并通过判断目标语言与正则语言的交集是否为空来高效解决上下文无关语言的约束解码问题。

Result: 实验表明，该方法在C++代码填空和JSON结构化数据提取等任务中，几乎完美地保证了语法正确性，同时保持或提升了功能正确性，计算开销也保持在实用范围内。

Conclusion: 该研究首次实现了对扩散模型的约束解码，为解决正式语言生成问题提供了高效且实用的解决方案。

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [53] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: 论文提出了一种多层次的IoMT入侵检测框架，能有效检测零日攻击并区分已知和未知威胁，在CICIoMT2024数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式入侵检测系统（IDS）在IoMT环境中存在延迟、隐私风险等问题，而本地运行IDS又受限于设备资源。因此，需要一种更高效的解决方案来解决这些问题。

Method: 提出了一种多层次的IoMT IDS框架。第一层（近Edge）使用元学习或单类分类（OCC）粗粒度过滤流量，后续层（远Edge，云）识别攻击类型和新颖性。

Result: 在CICIoMT2024数据集上，框架实现了99.77%的准确率和97.8%的F1分数，并能高精度检测零日攻击。

Conclusion: 该框架在IoMT环境中表现出色，解决了传统方法的局限性，具有实际应用潜力。

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [54] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 该论文研究了语义通信中的分布学习问题，揭示了源统计学习的基本条件，并证明了学习性与传输矩阵秩的关系，同时量化了估计误差对语义失真的影响。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信侧重于单个语义的传输优化，但在先验未知的情况下，学习源统计的能力是未知的。本文旨在填补这一空白。

Method: 通过分析有效传输矩阵的秩和分布估计的收敛速率，量化语义失真的影响，并通过CIFAR-10实验验证理论框架。

Result: 研究发现，优化的编码方案可能在短期语义性能上表现良好，但会牺牲长期学习能力，系统条件对学习速率和性能有重要影响。

Conclusion: 本文首次严格描述了语义通信中的统计学习问题，并提出了平衡即时性能与适应能力的设计原则。

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [55] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: EDAPT是一种任务和模型无关的框架，通过持续模型适配消除BCI中的校准需求，结合群体预训练和在线微调显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: BCI因神经信号随时间漂移和用户间差异导致准确性下降，频繁校准限制了实际应用，需要一种免校准的解决方案。

Method: EDAPT先通过多用户数据训练基线解码器，随后在使用过程中通过监督微调持续个性化模型，并结合无监督域适应进一步优化。

Result: 在三个BCI任务的九个数据集上，EDAPT比静态方法准确性更高，且计算高效，模型更新仅需200毫秒。

Conclusion: EDAPT为免校准BCI提供实用路径，显著降低部署门槛。

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [56] [Approximating Entanglement Based on Abstract Interpretation](https://arxiv.org/abs/2508.10056)
*Aske Nord Raahauge,Martin Bom Marchioro,Rasmus Ross Nylandsted*

Main category: quant-ph

TL;DR: 提出了一种基于抽象解释的静态分析方法，用于近似分析量子比特的纠缠情况，避免精确分析带来的指数级减速。


<details>
  <summary>Details</summary>
Motivation: 纠缠是量子系统的核心特性，对于量子程序优化和算法正确性验证至关重要。

Method: 通过扩展现有抽象解释方法，提出一种静态分析技术，用于近似纠缠分析。

Result: 方法被证明是可靠的，并在Standard ML中实现了线性时间可扩展的实现。

Conclusion: 为量子电路优化和算法验证提供了一种高效的纠缠近似分析方法。

Abstract: Entanglement is a fundamental property of quantum systems, essential for
non-trivial quantum programs. Identifying when qubits become entangled is
critical for circuit optimization, and for arguing for the correctness of
quantum algorithms. This paper presents a static analysis method for
approximating entanglement by extending an already existing abstract
interpretation, thus avoiding the exponential slowdown of an exact analysis.
The approach is shown to be sound and an implementation is provided in Standard
ML with linear-time scalability.

</details>


### [57] [Simulating Mass-Dependent Decoherence in Quantum Computers: Baseline Signatures for Testing Gravity-Induced Collapse](https://arxiv.org/abs/2508.10590)
*Viswak R Balaji,Samuel Punch*

Main category: quant-ph

TL;DR: 该研究通过量子计算模拟检验了Penrose重力诱导塌缩假说中的质量相关退相干模型，发现其产生的塌缩特征与恒定速率退相不同。


<details>
  <summary>Details</summary>
Motivation: 探究Penrose客观还原理论中关于量子叠加态稳定性的预测，利用量子计算机验证质量相关塌缩效应。

Method: 在Qiskit AerSimulator中实现质量相关退相噪声通道，应用于GHZ态、分支质量纠缠测试和Grover搜索实验。

Result: 研究为未来硬件的质量相关塌缩效应提供了基线参考和可复现协议。

Conclusion: 该工作为用量子计算机测试量子力学基本问题提供了方法和数据支持。

Abstract: We present a quantum computing simulation study of mass-dependent decoherence
models inspired by Penrose's gravity-induced collapse hypothesis. According to
objective reduction (OR) theory, quantum superpositions become unstable when
the gravitational self-energy difference between branches exceeds a certain
threshold, leading to a collapse time $\tau \approx \hbar / E_G$. In this work,
we implement a mass-dependent dephasing noise channel, $p(m) = 1 - e^{-k
m^{\alpha}}$, within the Qiskit AerSimulator, where $m$ is a proxy for the
effective mass of a superposition, mapped to circuit parameters such as the
number of entangled qubits or branch size. We apply this model to three
canonical quantum computing experiments: GHZ state parity measurements,
branch-mass entanglement tests, and Grover's search to generate distinctive
collapse signatures that differ qualitatively from constant-rate dephasing. The
resulting patterns serve as a baseline reference: if future hardware
experiments exhibit the same scaling trends under ideal isolation, this could
indicate a contribution from mass-dependent collapse processes. Conversely,
deviation toward constant-noise behaviour would suggest the absence of such
gravitationally induced effects. Our results provide a reproducible protocol
and reference for using quantum computers as potential testbeds for probing
fundamental questions in quantum mechanics.

</details>


### [58] [Routing and Wavelength Assignment with Minimal Attack Radius for QKD Networks](https://arxiv.org/abs/2508.10613)
*Mengyao Li,Qiaolun Zhang,Zongshuai Yang,Stefano Bregni,Alberto Gatto,Raouf Boutaba,Massimo Tornatore*

Main category: quant-ph

TL;DR: 论文提出了一种新方法，通过最大受影响请求数(maxNAR)量化物理层攻击的影响，并解决了路由和波长分配最小攻击半径(RWA-MAR)问题，使用ILP模型和启发式算法优化安全性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发(QKD)在保证安全性的同时，易受高功率干扰等物理层攻击的干扰，导致密钥交换中断。本文旨在解决这一问题。

Method: 提出了maxNAR指标，并研究了RWA-MAR问题。使用ILP模型和启发式算法，结合量子密钥池(QKPs)和不同网络架构（如光学旁路和可信中继）进行优化。

Result: 仿真结果表明，该方法在安全性和可扩展性上显著优于基线方法。

Conclusion: 通过maxNAR和RWA-MAR的优化，结合灵活的启发式算法，有效提升了QKD网络的安全性和适应性。

Abstract: Quantum Key Distribution (QKD) can distribute keys with guaranteed security
but remains susceptible to key exchange interruption due to physical-layer
threats, such as high-power jamming attacks. To address this challenge, we
first introduce a novel metric, namely Maximum Number of Affected Requests
(maxNAR), to quantify the worst-case impact of a single physical-layer attack,
and then we investigate a new problem of Routing and Wavelength Assignment with
Minimal Attack Radius (RWA-MAR). We formulate the problem using an Integer
Linear Programming (ILP) model and propose a scalable heuristic to efficiently
minimize maxNAR. Our approach incorporates key caching through Quantum Key
Pools (QKPs) to enhance resilience and optimize resource utilization. Moreover,
we model the impact of different QKD network architectures, employing Optical
Bypass (OB) for optical switching of quantum channels and Trusted Relay (TR)
for secure key forwarding. Moreover, a tunable parameter is designed in the
heuristic to guide the preference for OB or TR, offering enhanced adaptability
and dynamic control in diverse network scenarios. Simulation results confirm
that our method significantly outperforms the baseline in terms of security and
scalability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [59] [Enabling Generic Robot Skill Implementation Using Object Oriented Programming](https://arxiv.org/abs/2508.10497)
*Abdullah Farrukh,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 提出一个软件框架，简化机器人系统部署，特别是针对中小企业缺乏机器人专业知识的问题。


<details>
  <summary>Details</summary>
Motivation: 中小企业缺乏机器人专业知识，依赖外部集成商可能导致供应商锁定；学术研究中也面临机器人接口复杂的问题。

Method: 使用Python实现一个抽象层框架，简化不同制造商和型号的机器人接口。

Result: 开发了一个原型，应用于包含Yaskawa Motoman GP4的分拣单元。

Conclusion: 该框架降低了部署机器人系统的复杂性，适合中小企业和学术研究。

Abstract: Developing robotic algorithms and integrating a robotic subsystem into a
larger system can be a difficult task. Particularly in small and medium-sized
enterprises (SMEs) where robotics expertise is lacking, implementing,
maintaining and developing robotic systems can be a challenge. As a result,
many companies rely on external expertise through system integrators, which, in
some cases, can lead to vendor lock-in and external dependency. In the academic
research on intelligent manufacturing systems, robots play a critical role in
the design of robust autonomous systems. Similar challenges are faced by
researchers who want to use robotic systems as a component in a larger smart
system, without having to deal with the complexity and vastness of the robot
interfaces in detail. In this paper, we propose a software framework that
reduces the effort required to deploy a working robotic system. The focus is
solely on providing a concept for simplifying the different interfaces of a
modern robot system and using an abstraction layer for different manufacturers
and models. The Python programming language is used to implement a prototype of
the concept. The target system is a bin-picking cell containing a Yaskawa
Motoman GP4.

</details>


### [60] [Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality](https://arxiv.org/abs/2508.10603)
*Agnes Axelsson,Merle Reimann,Ronald Cumbal,Hannah Pelikan,Divesh Lala*

Main category: cs.RO

TL;DR: 该论文提出使用民族志小故事（ethnographic vignettes）来突出人机交互（HRI）中的失败案例，尤其是在研究中容易被忽略的问题，以提升透明度和多学科视角的沟通。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs提升了人机交互的质量，但与人人互动相比，现有系统仍存在许多不足。失败的原因和重要性取决于交互的具体场景，难以一概而论。因此，需要一种方法能够清晰地记录和突出这些失败，尤其是那些未被充分研究的问题。

Method: 论文提出使用民族志小故事（ethnographic vignettes）这一技术，基于作者在HRI系统中遇到的实际失败案例，设计并描述了编写小故事的方法论。

Result: 民族志小故事能够从多学科角度记录交互失败，提升对机器人能力的透明度，并捕捉研究中容易被忽略的意外行为。

Conclusion: 论文鼓励将民族志小故事作为现有交互评估方法的补充工具，以更好地记录和沟通HRI中的问题。

Abstract: Although the quality of human-robot interactions has improved with the advent
of LLMs, there are still various factors that cause systems to be sub-optimal
when compared to human-human interactions. The nature and criticality of
failures are often dependent on the context of the interaction and so cannot be
generalized across the wide range of scenarios and experiments which have been
implemented in HRI research. In this work we propose the use of a technique
overlooked in the field of HRI, ethnographic vignettes, to clearly highlight
these failures, particularly those that are rarely documented. We describe the
methodology behind the process of writing vignettes and create our own based on
our personal experiences with failures in HRI systems. We emphasize the
strength of vignettes as the ability to communicate failures from a
multi-disciplinary perspective, promote transparency about the capabilities of
robots, and document unexpected behaviours which would otherwise be omitted
from research reports. We encourage the use of vignettes to augment existing
interaction evaluation methods.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [61] [zERExtractor:An Automated Platform for Enzyme-Catalyzed Reaction Data Extraction from Scientific Literature](https://arxiv.org/abs/2508.09995)
*Rui Zhou,Haohui Ma,Tianle Xin,Lixin Zou,Qiuyue Hu,Hongxi Cheng,Mingzhi Lin,Jingjing Guo,Sheng Wang,Guoqing Zhang,Yanjie Wei,Liangzhen Zheng*

Main category: q-bio.BM

TL;DR: 文章介绍了zERExtractor，一个自动化平台，用于从科学文献中提取酶催化反应和活性数据，填补了酶动力学领域的数据空白。


<details>
  <summary>Details</summary>
Motivation: 酶动力学文献快速扩展，传统数据库难以跟上，阻碍了AI建模和知识发现。

Method: zERExtractor采用模块化架构，结合深度学习、OCR、语义识别和LLMs，并通过主动学习和专家校正优化数据提取。

Result: 系统在表格识别、分子图像解释和关系提取方面表现优异（准确率高达99.1%）。

Conclusion: zERExtractor为未来AI驱动的酶建模和生化知识发现奠定了基础。

Abstract: The rapid expansion of enzyme kinetics literature has outpaced the curation
capabilities of major biochemical databases, creating a substantial barrier to
AI-driven modeling and knowledge discovery. We present zERExtractor, an
automated and extensible platform for comprehensive extraction of
enzyme-catalyzed reaction and activity data from scientific literature.
zERExtractor features a unified, modular architecture that supports
plug-and-play integration of state-of-the-art models, including large language
models (LLMs), as interchangeable components, enabling continuous system
evolution alongside advances in AI. Our pipeline combines domain-adapted deep
learning, advanced OCR, semantic entity recognition, and prompt-driven LLM
modules, together with human expert corrections, to extract kinetic parameters
(e.g., kcat, Km), enzyme sequences, substrate SMILES, experimental conditions,
and molecular diagrams from heterogeneous document formats. Through active
learning strategies integrating AI-assisted annotation, expert validation, and
iterative refinement, the system adapts rapidly to new data sources. We also
release a large benchmark dataset comprising over 1,000 annotated tables and
5,000 biological fields from 270 P450-related enzymology publications.
Benchmarking demonstrates that zERExtractor consistently outperforms existing
baselines in table recognition (Acc 89.9%), molecular image interpretation (up
to 99.1%), and relation extraction (accuracy 94.2%). zERExtractor bridges the
longstanding data gap in enzyme kinetics with a flexible, plugin-ready
framework and high-fidelity extraction, laying the groundwork for future
AI-powered enzyme modeling and biochemical knowledge discovery.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 本文指出大型语言模型（LLMs）的所谓“理解能力”和“推理能力”仅仅是模糊概念的幻觉，其本质上无法实现真正的正确推理。


<details>
  <summary>Details</summary>
Motivation: 揭露LLMs基于其工作原理的本质局限性，无法具备真正的理解与推理能力。

Method: 通过分析LLMs的工作原理及其本质局限性。

Result: LLMs无法实现真正的正确推理能力。

Conclusion: LLMs的根本设计决定了它们无法拥有真正的理解或推理能力。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [63] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 该论文提出了一种多智能体协作的图形设计评估系统（AgenticDRS），通过图匹配和提示扩展方法提升评估效果，并引入DRS-BENCH基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 图形设计评估需要从多个方面（如对齐、构图、美学和色彩选择）综合考量，目前缺乏系统化的方法。

Method: 提出Agentic Design Review System（AgenticDRS），由多个智能体协作分析设计，辅以图匹配和提示扩展方法。

Result: 实验表明Agentic-DRS在评估图形设计和生成可操作反馈方面优于现有基准。

Conclusion: 该研究为图形设计评估提供了实用且创新的解决方案，呼吁关注这一未充分探索的研究方向。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [64] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 文章研究了人类对AI生成内容的反应，提出了新指标和工具T-Lens来量化用户判断和互动，并提供了缓解AI驱动错误信息风险的策略。


<details>
  <summary>Details</summary>
Motivation: AI生成内容广泛传播带来错误信息风险，但现有研究多集中于内容真实性识别，对人类反应的研究较少。

Method: 引入MhAIM数据集（含15.4万条帖子），提出Trustworthiness、Impact、Openness三个指标，开发基于LLM的T-Lens系统（核心为HR-MCP协议）。

Result: 研究表明人类在文字和视觉信息不一致时更容易识别AI内容；T-Lens能更好地预测和适应用户反应。

Conclusion: 研究为LLM提供人类感知能力支持，揭示了AI与人类认知的复杂互动，提出了缓解AI错误信息风险的具体策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [65] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: 联邦学习结合差分隐私在医疗研究中保护患者隐私，但面临隐私与临床效用的权衡问题。研究通过SMOTETomek和优化FedProx算法解决了数据不平衡问题，找到了隐私与效用的最佳平衡点。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在医疗数据中因隐私保护和数据不平衡导致的临床效用下降问题。

Method: 采用SMOTETomek处理数据不平衡，优化FedProx算法适应非独立同分布数据。

Result: 找到隐私预算与模型召回率的非线性权衡点，优化算法性能优于标准方法。

Conclusion: 研究提供了一个高效、安全的医疗诊断工具框架，适用于现实世界中的异构数据。

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [66] [An Architecture for Distributed Digital Identities in the Physical World](https://arxiv.org/abs/2508.10185)
*René Mayrhofer,Michael Roland,Tobias Höller,Philipp Hofer,Mario Lins*

Main category: cs.CR

TL;DR: 该论文提出了一种分布式数字身份架构，用于物理世界交易，通过避免集中化解决了可用性和隐私问题，并结合多种组件和技术实现安全且去中心化的身份管理。


<details>
  <summary>Details</summary>
Motivation: 集中式数字身份管理存在单点故障和隐私风险，容易成为攻击目标。研究旨在设计一种去中心化架构，以解决这些问题。

Method: 设计了包含传感器、身份权威、属性验证器和个人身份代理（PIA）的分布式架构，并通过初始协议和安全验证实现交易去中心化。

Result: 架构和协议在理论上满足安全性要求，并能通过概念验证实现，适用于容忍数秒延迟的应用场景。

Conclusion: 分布式数字身份架构在避免集中化风险的同时，兼具实用性和安全性，适合物理世界的身份管理需求。

Abstract: Digital identities are increasingly important for mediating not only digital
but also physical service transactions. Managing such identities through
centralized providers can cause both availability and privacy concerns: single
points of failure and control are ideal targets for global attacks on
technical, organizational, or legal fronts. We design, analyze, and build a
distributed digital identity architecture for physical world transactions in
common scenarios like unlocking doors, public transport, or crossing country
borders. This architecture combines (biometric and other) sensors, (established
and upcoming) identity authorities, attribute verifiers, and a new core
component we call the \emph{Personal Identity Agent (PIA)} that represents
individuals with their identity attributes in the digital domain. All
transactions are conducted in a completely decentralized manner, and the
components for which we currently assume central coordination are optional and
only used for assisting with service discovery and latency reduction. We
present a first protocol between these parties and formally verify that it
achieves relevant security properties based on a realistic threat model
including strong global adversaries. A proof-of-concept implementation
demonstrates practical feasibility of both architecture and initial protocol
for applications that can tolerate end-to-end latencies in the range of a few
seconds.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [67] [The phi-Process: Operator-Algebraic Embeddings of Possibilities, Transfinite Stabilization, and a Quantitative Application to Sensory Depletion](https://arxiv.org/abs/2508.10650)
*Bugra Kilictas,Faruk Alpay*

Main category: math.FA

TL;DR: 本文提出了一个超越有限Phi过程，用于处理结构化状态空间中的可能性嵌入，并证明了多个定理和定量应用。


<details>
  <summary>Details</summary>
Motivation: 旨在形式化处理复杂状态空间中的可能性嵌入，并在理论和实际应用中展示其有效性。

Method: 通过形式化Phi过程，研究了其在Banach空间、Hilbert空间等中的行为，并提出了多个定理和定量应用。

Result: 证明了确定性全局动态、序数稳定定理等，并展示了在感知耗竭中的定量应用。

Conclusion: Phi过程在理论和实践中具有广泛适用性，展示了其在复杂系统中的潜力。

Abstract: We formalize a transfinite Phi process that treats all possibility embeddings
as operators on structured state spaces including complete lattices, Banach and
Hilbert spaces, and orthomodular lattices. We prove a determinization lemma
showing that lifting to sets or distributions yields a deterministic global
dynamic, an ordinal stabilization theorem sending operator transforms to the
fixed subspace by stage omega under normal spectral contraction, and a product
of Riesz projections theorem for commuting layers. We establish a
compositionality law for lifted maps, show closure of Phi packings, and present
a quantitative application to sensory depletion that models tissue removal as a
projection and derives strict decreases in the attainable fixed point under
minimal monotonicity and positivity assumptions. We also state measurable
conditions for probabilistic lifts, give explicit non normal and non commuting
counterexamples, and provide finite dimensional and stochastic witnesses
together with per theorem scope tables and a small reproducible code appendix.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [68] [DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality](https://arxiv.org/abs/2508.10605)
*Xinyi Wang,Angeliki Katsenou,David Bull*

Main category: eess.IV

TL;DR: 该论文提出了一种基于帧间变化的时空碎片化NR-VQA模型，通过多层级分析视频质量敏感区域，结合2D和3D特征，性能优异且计算复杂度低。


<details>
  <summary>Details</summary>
Motivation: 用户生成视频内容（UGC）的快速增长推动了无参考（NR）视频质量评估（VQA）的研究需求，因为UGC通常缺乏原始参考视频。

Method: 利用帧间变化驱动时空碎片化，从帧、块和碎片化帧三个层级分析质量敏感区域，结合全局和局部信息，同时提取2D和3D特征。

Result: 在五个UGC数据集上测试，性能排名前2（DIVA-VQA-L：0.898和DIVA-VQA-B：0.886），且计算复杂度低。

Conclusion: 提出的NR-VQA模型在性能和效率上均优于现有方法，代码和模型已开源。

Abstract: The rapid growth of user-generated (video) content (UGC) has driven increased
demand for research on no-reference (NR) perceptual video quality assessment
(VQA). NR-VQA is a key component for large-scale video quality monitoring in
social media and streaming applications where a pristine reference is not
available. This paper proposes a novel NR-VQA model based on spatio-temporal
fragmentation driven by inter-frame variations. By leveraging these inter-frame
differences, the model progressively analyses quality-sensitive regions at
multiple levels: frames, patches, and fragmented frames. It integrates frames,
fragmented residuals, and fragmented frames aligned with residuals to
effectively capture global and local information. The model extracts both 2D
and 3D features in order to characterize these spatio-temporal variations.
Experiments conducted on five UGC datasets and against state-of-the-art models
ranked our proposed method among the top 2 in terms of average rank correlation
(DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered
at a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on
average compared to the fastest existing NR-VQA method. Code and models are
publicly available at: https://github.com/xinyiW915/DIVA-VQA.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: Puppeteer是一个自动化框架，用于3D模型的绑定和动画生成，通过创新的骨架预测和权重推断技术，显著提升了动画质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现代交互应用需要动态3D内容，但传统方法依赖专家手动操作，效率低。生成式AI在静态3D模型创建中取得进展，但绑定和动画仍需改进。

Method: 采用自回归变换器预测骨架结构，结合分层排序和随机扰动；基于注意力机制的架构推断蒙皮权重；通过可微分优化生成高保真动画。

Result: 在多个基准测试中，Puppeteer在骨架预测和蒙皮质量上优于现有技术，能处理多样3D内容并生成稳定的动画。

Conclusion: Puppeteer提供高效、自动化的3D绑定和动画解决方案，显著减少人工干预，适用于游戏资产和AI生成模型。

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [70] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: 本研究探讨了结合真实与合成图像提升西瓜病害分类模型性能的效果，发现混合使用少量真实图像与大量合成图像能显著提高模型表现。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）在农业中可用于合成高分辨率图像，减少实地数据采集，但如何结合真实与合成图像提升病害分类性能的研究较少。

Method: 使用五种不同比例的真实与合成图像训练EfficientNetV2-L模型，包括纯真实图像、纯合成图像及不同比例的混合图像。

Result: 混合少量真实图像与大量合成图像的模型表现最佳，加权F1分数从0.65提升到1.00，表明二者结合可显著提升分类性能。

Conclusion: 纯合成图像不足以替代真实图像，混合使用少量真实与大量合成图像可最大化模型性能。

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [71] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

TL;DR: 提出了一种基于高质量合成数据生成的可扩展框架，用于解决视觉语言模型在工业泄漏检测等关键领域中因数据稀缺导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在通用视觉识别中表现出色，但在安全关键领域（如工业泄漏检测）中性能大幅下降，主要由于数据稀缺（隐私、敏感性和罕见性）导致传统微调不可行。

Method: 引入了一个高质量的合成数据生成流程框架，支持参数高效微调（PEFT），用于提升YOLO和DETR等检测器的性能。

Result: 合成数据（SynSpill数据集）显著提升了视觉语言模型和检测器的性能，使其在未见泄漏场景中表现接近。

Conclusion: 高保真合成数据是弥合安全关键应用领域差异的有效手段，结合轻量级适应提供了一种成本效益高的解决方案。

Abstract: Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [72] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文研究了基于外观的视线估计（PoG）中个体差异和头部姿态变化的影响，提出了通过引入更多头部姿态变化的动态校准策略来提高估计器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的PoG估计器在跨个体泛化方面表现不佳，且对头部姿态变化敏感，因此需要研究如何通过校准策略提升其鲁棒性。

Method: 构建了MobilePoG基准数据集，分析了校准点和头部姿态多样性对估计精度的影响，并提出了一种动态校准策略。

Result: 实验表明，校准时引入更广泛的头部姿态变化可以提高估计器对姿态变化的适应性。

Conclusion: 动态校准策略能够有效减少PoG估计器对头部姿态变化的敏感性，提升其准确性。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [73] [Molecule Mixture Detection and Alphabet Design for Non-linear, Cross-reactive Receiver Arrays in MC](https://arxiv.org/abs/2508.10856)
*Bastian Heinlein,Kaikai Zhu,Sümeyye Carkit-Yilmaz,Sebastian Lotter,Helene M. Loos,Andrea Buettner,Yansha Deng,Robert Schober,Vahid Jamali*

Main category: eess.SP

TL;DR: 论文提出了一种用于分子混合通信的检测器及字母表设计算法，解决了商用传感器非线性、交叉反应的问题，模拟结果表明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 空气分子通信（MC）具有实际应用前景，但商用传感器存在非线性和交叉反应行为，与MC文献中的理想假设不符。论文旨在填补这一研究空白。

Method: 提出了一种基于最大似然检测的检测器，适用于非线性和交叉反应的接收器阵列，并设计了一种考虑接收器特性的混合字母表算法。

Result: 模拟实验表明，该检测器在无需大量训练样本的情况下，性能与数据驱动方法相当；字母表设计算法优于未考虑接收器特性的方法。

Conclusion: 论文提出的检测器和字母表设计算法为可靠空气分子通信铺平了道路，并适用于其他化学传感器。

Abstract: Air-based molecular communication (MC) has the potential to be one of the
first MC systems to be deployed in real-world applications, enabled by existing
sensor technologies such as metal-oxide semi-conductor (MOS) sensors. However,
commercially available sensors usually exhibit non-linear and cross-reactive
behavior, contrary to the idealizing assumptions about linear and perfectly
molecule type-specific sensing often made in the MC literature. To address this
gap, we propose a detector for molecule mixture communication with a general
non-linear, cross-reactive receiver (RX) array that performs approximate
maximum likelihood detection on the sensor outputs. Additionally, we introduce
an algorithm for the design of mixture alphabets that accounts for the RX
characteristics. We evaluate our detector and alphabet design algorithm through
simulations that are based on measurements reported for two commercial MOS
sensors. Our simulations demonstrate that the proposed detector achieves
similar symbol error rates as data-driven methods without requiring large
numbers of training samples and that the alphabet design algorithm outperforms
methods that do not account for the RX characteristics. Since the proposed
detector and alphabet design algorithm are also applicable to other chemical
sensors, they pave the way for reliable air-based MC.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [74] [Advancing Data Equity: Practitioner Responsibility and Accountability in NLP Data Practices](https://arxiv.org/abs/2508.10071)
*Jay L. Cunningham,Kevin Zhongyang Shao,Rock Yuren Pang,Nathaniel Mengist*

Main category: cs.CY

TL;DR: 研究探讨NLP从业者对数据公平的认知与应对策略，揭示商业目标与公平承诺的张力，呼吁结构治理改革。


<details>
  <summary>Details</summary>
Motivation: 关注NLP从业者在数据处理中对公平问题的看法与行动，填补研究空白。

Method: 基于2024年的问卷和焦点小组研究美国NLP从业者的实践与挑战。

Result: 发现商业与公平的冲突，要求更参与的治理和问责机制。

Conclusion: 呼吁结构改革，提升从业者能动性和社区参与，以实现NLP公平。

Abstract: While research has focused on surfacing and auditing algorithmic bias to
ensure equitable AI development, less is known about how NLP practitioners -
those directly involved in dataset development, annotation, and deployment -
perceive and navigate issues of NLP data equity. This study is among the first
to center practitioners' perspectives, linking their experiences to a
multi-scalar AI governance framework and advancing participatory
recommendations that bridge technical, policy, and community domains. Drawing
on a 2024 questionnaire and focus group, we examine how U.S.-based NLP data
practitioners conceptualize fairness, contend with organizational and systemic
constraints, and engage emerging governance efforts such as the U.S. AI Bill of
Rights. Findings reveal persistent tensions between commercial objectives and
equity commitments, alongside calls for more participatory and accountable data
workflows. We critically engage debates on data diversity and diversity
washing, arguing that improving NLP equity requires structural governance
reforms that support practitioner agency and community consent.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [75] [An Intelligent Infrastructure as a Foundation for Modern Science](https://arxiv.org/abs/2508.10051)
*Satrajit S. Ghosh*

Main category: q-bio.NC

TL;DR: 论文呼吁将传统静态、碎片化的科学基础设施转变为动态、AI对齐的生态系统，以加速科学发现，并以神经科学为例提出实施指南。


<details>
  <summary>Details</summary>
Motivation: 传统的科学基础设施存在数据孤岛、互操作性和可重复性差等问题，阻碍科学进步。AI技术的兴起为变革提供了契机，尤其是在神经科学领域多模态、多尺度数据快速增长的情况下。

Method: 提出将基础设施升级为动态、AI对齐的生态系统，结合数据共享、集体利益和数字存储原则，制定实施指南。

Result: 通过智能基础设施，可以克服当前限制，加速科学发现，确保可重复性和伦理实践，并将神经科学的成果转化为社会效益。

Conclusion: 优先发展智能基础设施是科学进步的关键，为其他科学领域提供借鉴。

Abstract: Infrastructure shapes societies and scientific discovery. Traditional
scientific infrastructure, often static and fragmented, leads to issues like
data silos, lack of interoperability and reproducibility, and unsustainable
short-lived solutions. Our current technical inability and social reticence to
connect and coordinate scientific research and engineering leads to
inefficiencies and impedes progress. With AI technologies changing how we
interact with the world around us, there is an opportunity to transform
scientific processes. Neuroscience's exponential growth of multimodal and
multiscale data, and urgent clinical relevance demand an infrastructure itself
learns, coordinates, and improves. Using neuroscience as a stress test, this
perspective argues for a paradigm shift: infrastructure must evolve into a
dynamic, AI-aligned ecosystem to accelerate science. Building on several
existing principles for data, collective benefit, and digital repositories, I
recommend operational guidelines for implementing them to create this dynamic
ecosystem, aiming to foster a decentralized, self-learning, and self-correcting
system where humans and AI can collaborate seamlessly. Addressing the chronic
underfunding of scientific infrastructure, acknowledging diverse contributions
beyond publications, and coordinating global efforts are critical steps for
this transformation. By prioritizing an intelligent infrastructure as a central
scientific instrument for knowledge generation, we can overcome current
limitations, accelerate discovery, ensure reproducibility and ethical
practices, and ultimately translate neuroscientific understanding into tangible
societal benefits, setting a blueprint for other scientific domains.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本文分享了在BC癌症注册中心实施NLP模型进行信息提取和分类任务的关键经验，强调明确业务目标、迭代开发、多学科合作，以及模型选择和数据质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档数据提取可提升医疗效率，但NLP解决方案的实际部署面临挑战。

Method: 采用迭代开发和多学科协作，结合混合模型和简单方法，注重数据质量和错误缓解策略。

Result: 为医疗机构成功实施AI/NLP解决方案提供了实用指导，以改善数据管理和患者护理。

Conclusion: 明确业务目标、多学科合作和务实方法是成功部署NLP的关键。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [77] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究发现，注意力权重作为Transformer模型的解释工具在生物医学文献分类中未被普遍认为是有效的，但其感知有用性受可视化方式影响。


<details>
  <summary>Details</summary>
Motivation: 探讨注意力机制在解释AI模型预测中的作用，尤其是在生物医学文献分类中，是否有助于医生理解和交互。

Method: 通过用户研究，评估注意力权重及其可视化形式对医学专家分类文献的帮助程度。

Result: Transformer模型（XLNet）分类准确，但注意力权重解释性不佳；可视化方式（如文本亮度或背景颜色）显著影响用户感知。

Conclusion: 注意力权重的解释效用有限，但其可视化形式是关键影响因素；与传统视觉编码原则不同，用户偏好更直观的可视化方式。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [78] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF是一个个性化的、无需参考的评估框架，用于衡量文本生成的质量和用户对齐性。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成的评估方法大多忽略了用户的个性化需求，PREF旨在填补这一空白。

Method: PREF通过三步流程（覆盖率、偏好和评分）生成个性化评估标准，并利用LLM进行评分。

Result: 实验表明，PREF在准确性和用户对齐性上优于基线方法。

Conclusion: PREF为个性化语言生成系统的可靠评估和开发奠定了基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [79] [Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children's Speech](https://arxiv.org/abs/2508.10332)
*Abhijit Sinha,Harishankar Kumar,Mohit Joshi,Hemant Kumar Kathania,Shrikanth Narayanan,Sudarsana Reddy Kadiri*

Main category: eess.AS

TL;DR: 论文研究了自监督学习（SSL）模型在儿童语音中对年龄和性别分类的表现，发现浅层更有效捕捉说话者特征，PCA可改进分类。


<details>
  <summary>Details</summary>
Motivation: 儿童语音的高变异性对年龄和性别分类带来挑战，现有SSL模型在儿童语音中的表现尚未充分探索。

Method: 对四个Wav2Vec2变体进行分层分析，使用PFSTAR和CMU Kids数据集，并应用PCA。

Result: 浅层（1-7）更有效捕捉说话者特征，Wav2Vec2-large-lv60在CMU Kids上表现最佳（年龄97.14%，性别98.20%）。

Conclusion: 研究揭示了说话者特征在SSL分层中的分布，为儿童语音接口提供了针对性策略。

Abstract: Children's speech presents challenges for age and gender classification due
to high variability in pitch, articulation, and developmental traits. While
self-supervised learning (SSL) models perform well on adult speech tasks, their
ability to encode speaker traits in children remains underexplored. This paper
presents a detailed layer-wise analysis of four Wav2Vec2 variants using the
PFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture
speaker-specific cues more effectively than deeper layers, which increasingly
focus on linguistic information. Applying PCA further improves classification,
reducing redundancy and highlighting the most informative components. The
Wav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU
Kids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These
results reveal how speaker traits are structured across SSL model depth and
support more targeted, adaptive strategies for child-aware speech interfaces.

</details>
