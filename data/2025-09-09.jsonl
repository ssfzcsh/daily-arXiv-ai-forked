{"id": "2509.05372", "pdf": "https://arxiv.org/pdf/2509.05372", "abs": "https://arxiv.org/abs/2509.05372", "authors": ["Piotr Przymus", "Andreas Happe", "Jürgen Cito"], "title": "Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Model (LLM) - based Automated Program Repair (APR) systems are\nincreasingly integrated into modern software development workflows, offering\nautomated patches in response to natural language bug reports. However, this\nreliance on untrusted user input introduces a novel and underexplored attack\nsurface. In this paper, we investigate the security risks posed by adversarial\nbug reports -- realistic-looking issue submissions crafted to mislead APR\nsystems into producing insecure or harmful code changes. We develop a\ncomprehensive threat model and conduct an empirical study to evaluate the\nvulnerability of state-of-the-art APR systems to such attacks. Our\ndemonstration comprises 51 adversarial bug reports generated across a spectrum\nof strategies, from manual curation to fully automated pipelines. We test these\nagainst leading APR model and assess both pre-repair defenses (e.g., LlamaGuard\nvariants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and\npost-repair detectors (GitHub Copilot, CodeQL). Our findings show that current\ndefenses are insufficient: 90\\% of crafted bug reports triggered\nattacker-aligned patches. The best pre-repair filter blocked only 47\\%, while\npost-repair analysis-often requiring human oversight-was effective in just 58\\%\nof cases. To support scalable security testing, we introduce a prototype\nframework for automating the generation of adversarial bug reports. Our\nanalysis exposes a structural asymmetry: generating adversarial inputs is\ninexpensive, while detecting or mitigating them remains costly and error-prone.\nWe conclude with practical recommendations for improving the robustness of APR\nsystems against adversarial misuse and highlight directions for future work on\ntrustworthy automated repair."}
{"id": "2509.05394", "pdf": "https://arxiv.org/pdf/2509.05394", "abs": "https://arxiv.org/abs/2509.05394", "authors": ["Zoltan Toth-Czifra"], "title": "Reverse Browser: Vector-Image-to-Code Generator", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to AIWare 2025 ArXiv Track", "summary": "Automating the conversion of user interface design into code (image-to-code\nor image-to-UI) is an active area of software engineering research. However,\nthe state-of-the-art solutions do not achieve high fidelity to the original\ndesign, as evidenced by benchmarks. In this work, I approach the problem\ndifferently: I use vector images instead of bitmaps as model input. I create\nseveral large datasets for training machine learning models. I evaluate the\navailable array of Image Quality Assessment (IQA) algorithms and introduce a\nnew, multi-scale metric. I then train a large open-weights model and discuss\nits limitations."}
{"id": "2509.05540", "pdf": "https://arxiv.org/pdf/2509.05540", "abs": "https://arxiv.org/abs/2509.05540", "authors": ["Thiago Barradas", "Aline Paes", "Vânia de Oliveira Neves"], "title": "Combining TSL and LLM to Automate REST API Testing: A Comparative Study", "categories": ["cs.SE", "cs.AI"], "comment": "10 pages, article computer science, software engineering, software\n  testing, ia, llm", "summary": "The effective execution of tests for REST APIs remains a considerable\nchallenge for development teams, driven by the inherent complexity of\ndistributed systems, the multitude of possible scenarios, and the limited time\navailable for test design. Exhaustive testing of all input combinations is\nimpractical, often resulting in undetected failures, high manual effort, and\nlimited test coverage. To address these issues, we introduce RestTSLLM, an\napproach that uses Test Specification Language (TSL) in conjunction with Large\nLanguage Models (LLMs) to automate the generation of test cases for REST APIs.\nThe approach targets two core challenges: the creation of test scenarios and\nthe definition of appropriate input data. The proposed solution integrates\nprompt engineering techniques with an automated pipeline to evaluate various\nLLMs on their ability to generate tests from OpenAPI specifications. The\nevaluation focused on metrics such as success rate, test coverage, and mutation\nscore, enabling a systematic comparison of model performance. The results\nindicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),\nDeepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -\nconsistently produced robust and contextually coherent REST API tests. Among\nthem, Claude 3.5 Sonnet outperformed all other models across every metric,\nemerging in this study as the most suitable model for this task. These findings\nhighlight the potential of LLMs to automate the generation of tests based on\nAPI specifications."}
{"id": "2509.05585", "pdf": "https://arxiv.org/pdf/2509.05585", "abs": "https://arxiv.org/abs/2509.05585", "authors": ["Zhiyuan Zou", "Bangchao Wang", "Peng Liang", "Tingting Bi", "Huan Jin"], "title": "Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity", "categories": ["cs.SE", "cs.AI"], "comment": "45 pages, 5 images, 11 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "In the field of software traceability link recovery (TLR), textual similarity\nhas long been regarded as the core criterion. However, in tasks involving\nnatural language and programming language (NL-PL) artifacts, relying solely on\ntextual similarity is limited by their semantic gap. To this end, we conducted\na large-scale empirical evaluation across various types of TLR tasks, revealing\nthe limitations of textual similarity in NL-PL scenarios. To address these\nlimitations, we propose an approach that incorporates multiple domain-specific\nauxiliary strategies, identified through empirical analysis, into two models:\nthe Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based\nGemini 2.5 Pro via additional input information. We then evaluated our approach\nusing the widely studied requirements-to-code TLR task, a representative case\nof NL-PL TLR. Experimental results show that both the multi-strategy HGT and\nGemini 2.5 Pro models outperformed their original counterparts without strategy\nintegration. Furthermore, compared to the current state-of-the-art method\nHGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average\nF1-score improvements of 3.68% and 8.84%, respectively, across twelve\nopen-source projects, demonstrating the effectiveness of multi-strategy\nintegration in enhancing overall model performance for the requirements-code\nTLR task."}
{"id": "2509.05488", "pdf": "https://arxiv.org/pdf/2509.05488", "abs": "https://arxiv.org/abs/2509.05488", "authors": ["Hongjun Xu", "Junxi Xia", "Weisi Yang", "Yueyuan Sui", "Stephen Xia"], "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs", "categories": ["cs.LG", "cs.AI", "cs.OS", "C.3; I.2.6; D.2.13; D.4.7"], "comment": "4 pages, 1 figures", "summary": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to\nlimited memory, the lack of native operator support, and the absence of\nembedded-friendly toolchains. We present, to our knowledge, the first\ndeployment of a Mamba-based neural architecture on a resource-constrained MCU,\na fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline\nmaps a trained PyTorch Mamba model to on-device execution by (1) exporting\nmodel weights into a lightweight format, and (2) implementing a handcrafted\nMamba layer and supporting operators in C with operator fusion and memory\nlayout optimization. MambaLite-Micro eliminates large intermediate tensors,\nreducing 83.0% peak memory, while maintaining an average numerical error of\nonly 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on\nkeyword spotting(KWS) and human activity recognition (HAR) tasks,\nMambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully\npreserving classification accuracy. We further validated portability by\ndeploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating\nconsistent operation across heterogeneous embedded platforms and paving the way\nfor bringing advanced sequence models like Mamba to real-world\nresource-constrained applications."}
{"id": "2509.05504", "pdf": "https://arxiv.org/pdf/2509.05504", "abs": "https://arxiv.org/abs/2509.05504", "authors": ["Karl Aaron Rudkowski", "Sallar Ahmadi-Pour", "Rolf Drechsler"], "title": "Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution", "categories": ["cs.PL", "cs.AR"], "comment": null, "summary": "Virtual Prototypes (VPs) are important tools in modern hardware development.\nAt high abstractions, they are often implemented in SystemC and offer early\nanalysis of increasingly complex designs. These complex designs often combine\none or more processors, interconnects, and peripherals to perform tasks in\nhardware or interact with the environment. Verifying these subsystems is a\nwell-suited task for VPs, as they allow reasoning across different abstraction\nlevels. While modern verification techniques like symbolic execution can be\nseamlessly integrated into VP-based workflows, they require modifications in\nthe SystemC kernel. Hence, existing approaches therefore modify and replace the\nSystemC kernel, or ignore the opportunity of cross-level scenarios completely,\nand would not allow focusing on special challenges of particular subsystems\nlike peripherals. We propose CrosSym and SEFOS, two opposing approaches for a\nversatile symbolic execution of peripherals. CrosSym modifies the SystemC\nkernel, while SEFOS instead modifies a modern symbolic execution engine. Our\nextensive evaluation applies our tools to various peripherals on different\nlevels of abstractions. Both tools extensive sets of features are demonstrated\nfor (1) different verification scenarios, and (2) identifying 300+ mutants. In\ncomparison with each other, SEFOS convinces with the unmodified SystemC kernel\nand peripheral, while CrosSym offers slightly better runtime and memory usage.\nIn comparison to the state-of-the-art, that is limited to Transaction Level\nModelling (TLM), our tools offered comparable runtime, while enabling\ncross-level verification with symbolic execution."}
{"id": "2509.05595", "pdf": "https://arxiv.org/pdf/2509.05595", "abs": "https://arxiv.org/abs/2509.05595", "authors": ["Seonghun Oh", "Xiaodi Yuan", "Xinyue Wei", "Ruoxi Shi", "Fanbo Xiang", "Minghua Liu", "Hao Su"], "title": "PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly Modeling on the GPU", "categories": ["cs.GR"], "comment": null, "summary": "Reducing the triangle count in complex 3D models is a basic geometry\npreprocessing step in graphics pipelines such as efficient rendering and\ninteractive editing. However, most existing mesh simplification methods exhibit\na few issues. Firstly, they often lead to self-intersections during decimation,\na major issue for applications such as 3D printing and soft-body simulation.\nSecond, to perform simplification on a mesh in the wild, one would first need\nto perform re-meshing, which often suffers from surface shifts and losses of\nsharp features. Finally, existing re-meshing and simplification methods can\ntake minutes when processing large-scale meshes, limiting their applications in\npractice. To address the challenges, we introduce a novel GPU-based mesh\noptimization approach containing three key components: (1) a parallel\nre-meshing algorithm to turn meshes in the wild into watertight, manifold, and\nintersection-free ones, and reduce the prevalence of poorly shaped triangles;\n(2) a robust parallel simplification algorithm with intersection-free\nguarantees; (3) an optimization-based safe projection algorithm to realign the\nsimplified mesh with the input, eliminating the surface shift introduced by\nre-meshing and recovering the original sharp features. The algorithm\ndemonstrates remarkable efficiency, simplifying a 2-million-face mesh to 20k\ntriangles in 3 seconds on RTX4090. We evaluated the approach on the Thingi10K\ndataset and showcased its exceptional performance in geometry preservation and\nspeed."}
{"id": "2509.05365", "pdf": "https://arxiv.org/pdf/2509.05365", "abs": "https://arxiv.org/abs/2509.05365", "authors": ["Dingcui Yu", "Yunpeng Song", "Yiyang Huang", "Yumiao Zhao", "Yina Lv", "Chundong Wang", "Youtao Zhang", "Liang Shi"], "title": "Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs", "categories": ["cs.PF"], "comment": "Dingcui Yu, Yunpeng Song and Yiyang Huang have equal contributions to\n  this work. Liang Shi and Yina Lv are the corresponding authors", "summary": "Data compression is widely adopted for modern solid-state drives (SSDs) to\nmitigate both storage capacity and SSD lifetime issues. Researchers have\nproposed compression schemes at different system layers, including device-side\nsolutions like CCSDs ( c ompression-based c omputational SSDs) and compression\nsupported by host-side, like F2FS (flash-friendly file system). We conduct\nquantitative studies to understand how host-side and device-side compression\nschemes affect the temperature and performance of SSD-based storage systems.\nFrom our experiments, device-side compression, facilitated by a hardware\ncompression engine, can raise the temperature of CCSDs to intolerable levels,\nresulting in throttling and service shutdown. In contrast, host-side\ncompression causes software-stack overhead, which often results in large\nperformance degradation and resource consumption. To ensure efficient data\ncompression with high performance and better temperature control, we propose\nWaltz, a temperature-aware cooperative compression method that schedules\n(de)compression tasks at the host and device sides by monitoring device\ntemperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for\nspace and performance optimization, respectively. Waltz is implemented within\nF2FS, achieving high performance while extending SSD lifetime and preventing\noverheating-induced in-flight shutdowns."}
{"id": "2509.05786", "pdf": "https://arxiv.org/pdf/2509.05786", "abs": "https://arxiv.org/abs/2509.05786", "authors": ["Jorge E. León", "Miguel Carrasco"], "title": "Effectively obtaining acoustic, visual and textual data from videos", "categories": ["cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "The increasing use of machine learning models has amplified the demand for\nhigh-quality, large-scale multimodal datasets. However, the availability of\nsuch datasets, especially those combining acoustic, visual and textual data,\nremains limited. This paper addresses this gap by proposing a method to extract\nrelated audio-image-text observations from videos. We detail the process of\nselecting suitable videos, extracting relevant data pairs, and generating\ndescriptive texts using image-to-text models. Our approach ensures a robust\nsemantic connection between modalities, enhancing the utility of the created\ndatasets for various applications. We also discuss the challenges encountered\nand propose solutions to improve data quality. The resulting datasets, publicly\navailable, aim to support and advance research in multimodal data analysis and\nmachine learning."}
{"id": "2509.05447", "pdf": "https://arxiv.org/pdf/2509.05447", "abs": "https://arxiv.org/abs/2509.05447", "authors": ["Zhongyuan Zhao", "Gunjan Verma", "Ananthram Swami", "Santiago Segarra"], "title": "Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)", "categories": ["cs.NI", "cs.DM", "cs.LG", "eess.SP", "05-08", "C.2.1; I.2.8; G.2.2"], "comment": "15 pages, 18 figures, accepted to IEEE Transactions on Wireless\n  Communications. This is the extended journal version of the conference paper\n  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, \"Distributed Link\n  Sparsification for Scalable Scheduling using Graph Neural Networks,\" IEEE\n  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )", "summary": "In wireless networks characterized by dense connectivity, the significant\nsignaling overhead generated by distributed link scheduling algorithms can\nexacerbate issues like congestion, energy consumption, and radio footprint\nexpansion. To mitigate these challenges, we propose a distributed link\nsparsification scheme employing graph neural networks (GNNs) to reduce\nscheduling overhead for delay-tolerant traffic while maintaining network\ncapacity. A GNN module is trained to adjust contention thresholds for\nindividual links based on traffic statistics and network topology, enabling\nlinks to withdraw from scheduling contention when they are unlikely to succeed.\nOur approach is facilitated by a novel offline constrained {unsupervised}\nlearning algorithm capable of balancing two competing objectives: minimizing\nscheduling overhead while ensuring that total utility meets the required level.\nIn simulated wireless multi-hop networks with up to 500 links, our link\nsparsification technique effectively alleviates network congestion and reduces\nradio footprints across four distinct distributed link scheduling protocols."}
{"id": "2509.06250", "pdf": "https://arxiv.org/pdf/2509.06250", "abs": "https://arxiv.org/abs/2509.06250", "authors": ["Ian Dardik", "Eunsuk Kang"], "title": "Compositional Inductive Invariant Inference via Assume-Guarantee Reasoning", "categories": ["cs.LO"], "comment": null, "summary": "A common technique for verifying the safety of complex systems is the\ninductive invariant method. Inductive invariants are inductive formulas that\noverapproximate the reachable states of a system and imply a desired safety\nproperty. However, inductive invariants are notoriously complex, which makes\ninductive invariant inference a challenging problem. In this work, we observe\nthat inductive invariant formulas are complex primarily because they must be\nclosed over the transition relation of an entire system. Therefore, we propose\na new approach in which we decompose a system into components, assign an\nassume-guarantee contract to each component, and prove that each component\nfulfills its contract by inferring a local inductive invariant. The key\nadvantage of local inductive invariant inference is that the local invariant\nneed only be closed under the transition relation for the component, which is\nsimpler than the transition relation for the entire system. Once local\ninvariant inference is complete, system-wide safety follows by construction\nbecause the conjunction of all local invariants becomes an inductive invariant\nfor the entire system. We apply our compositional inductive invariant inference\ntechnique to two case studies, in which we provide evidence that our framework\ncan infer invariants more efficiently than the global technique. Our case\nstudies also show that local inductive invariants provide modular insights\nabout a specification that are not offered by global invariants."}
{"id": "2509.05298", "pdf": "https://arxiv.org/pdf/2509.05298", "abs": "https://arxiv.org/abs/2509.05298", "authors": ["Rui Xi", "Xianghan Wang"], "title": "Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression", "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 International Conference on\n  Artificial Intelligence and Virtual Reality (AIVR 2025). \\c{opyright} 2025\n  Springer. This is the author-accepted manuscript. Rui Xi and Xianghan Wang\n  contributed equally to this work. The final version will be available via\n  SpringerLink", "summary": "Loneliness and social isolation pose significant emotional and health\nchallenges, prompting the development of technology-based solutions for\ncompanionship and emotional support. This paper introduces Livia, an\nemotion-aware augmented reality (AR) companion app designed to provide\npersonalized emotional support by combining modular artificial intelligence\n(AI) agents, multimodal affective computing, progressive memory compression,\nand AR driven embodied interaction. Livia employs a modular AI architecture\nwith specialized agents responsible for emotion analysis, dialogue generation,\nmemory management, and behavioral orchestration, ensuring robust and adaptive\ninteractions. Two novel algorithms-Temporal Binary Compression (TBC) and\nDynamic Importance Memory Filter (DIMF)-effectively manage and prioritize\nlong-term memory, significantly reducing storage requirements while retaining\ncritical context. Our multimodal emotion detection approach achieves high\naccuracy, enhancing proactive and empathetic engagement. User evaluations\ndemonstrated increased emotional bonds, improved satisfaction, and\nstatistically significant reductions in loneliness. Users particularly valued\nLivia's adaptive personality evolution and realistic AR embodiment. Future\nresearch directions include expanding gesture and tactile interactions,\nsupporting multi-user experiences, and exploring customized hardware\nimplementations."}
{"id": "2509.06044", "pdf": "https://arxiv.org/pdf/2509.06044", "abs": "https://arxiv.org/abs/2509.06044", "authors": ["Lingxiao Kong", "Apostolos Sarris", "Miltiadis Polidorou", "Victor Klingenberg", "Vasilis Sevetlidis", "Vasilis Arampatzakis", "George Pavlidis", "Cong Yang", "Zeyd Boukhers"], "title": "A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach", "categories": ["cs.DB"], "comment": "Accepted for publication at the IEEE International Conference on\n  Cyber Humanities (2025)", "summary": "Cultural heritage preservation faces significant challenges in managing\ndiverse, multi-source, and multi-scale data for effective monitoring and\nconservation. This paper documents a comprehensive data historicity and\nmigration framework implemented within the ARGUS project, which addresses the\ncomplexities of processing heterogeneous cultural heritage data. We describe a\nsystematic data processing pipeline encompassing standardization, enrichment,\nintegration, visualization, ingestion, and publication strategies. The\nframework transforms raw, disparate datasets into standardized formats\ncompliant with FAIR principles. It enhances sparse datasets through established\nimputation techniques, ensures interoperability through database integration,\nand improves querying capabilities through LLM-powered natural language\nprocessing. This approach has been applied across five European pilot sites\nwith varying preservation challenges, demonstrating its adaptability to diverse\ncultural heritage contexts. The implementation results show improved data\naccessibility, enhanced analytical capabilities, and more effective\ndecision-making for conservation efforts."}
{"id": "2509.05451", "pdf": "https://arxiv.org/pdf/2509.05451", "abs": "https://arxiv.org/abs/2509.05451", "authors": ["Niansong Zhang", "Wenbo Zhu", "Courtney Golden", "Dan Ilan", "Hongzheng Chen", "Christopher Batten", "Zhiru Zhang"], "title": "Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device", "categories": ["cs.AR", "cs.PF"], "comment": "MICRO 2025", "summary": "Compute-in-SRAM architectures offer a promising approach to achieving higher\nperformance and energy efficiency across a range of data-intensive\napplications. However, prior evaluations have largely relied on simulators or\nsmall prototypes, limiting the understanding of their real-world potential. In\nthis work, we present a comprehensive performance and energy characterization\nof a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.\nWe compare the GSI APU against established architectures, including CPUs and\nGPUs, to quantify its energy efficiency and performance potential. We introduce\nan analytical framework for general-purpose compute-in-SRAM devices that\nreveals fundamental optimization principles by modeling performance trade-offs,\nthereby guiding program optimizations.\n  Exploiting the fine-grained parallelism of tightly integrated memory-compute\narchitectures requires careful data management. We address this by proposing\nthree optimizations: communication-aware reduction mapping, coalesced DMA, and\nbroadcast-friendly data layouts. When applied to retrieval-augmented generation\n(RAG) over large corpora (10GB--200GB), these optimizations enable our\ncompute-in-SRAM system to accelerate retrieval by 4.8$\\times$--6.6$\\times$ over\nan optimized CPU baseline, improving end-to-end RAG latency by\n1.1$\\times$--1.8$\\times$. The shared off-chip memory bandwidth is modeled using\na simulated HBM, while all other components are measured on the real\ncompute-in-SRAM device. Critically, this system matches the performance of an\nNVIDIA A6000 GPU for RAG while being significantly more energy-efficient\n(54.4$\\times$-117.9$\\times$ reduction). These findings validate the viability\nof compute-in-SRAM for complex, real-world applications and provide guidance\nfor advancing the technology."}
{"id": "2509.05532", "pdf": "https://arxiv.org/pdf/2509.05532", "abs": "https://arxiv.org/abs/2509.05532", "authors": ["Changxu Song", "Arda Caliskan", "Beyza Zeynep Ucpinar", "Yasemin Kopur", "Mustafa Altay Karamuftuoglu", "Sasan Razmkhah", "Shahin Nazarian", "Massoud Pedram"], "title": "SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips", "categories": ["cs.ET"], "comment": null, "summary": "Despite numerous proposed designs for superconducting neural networks (SNNs),\nmost have overlooked practical fabrication constraints, leading to\nimplementations limited to only a few neurons or synapses. Current\nsuperconducting technologies, such as MIT LL SFQ5ee, impose severe limitations\non chip area, routing, and input/output pin counts (e.g., 5x5 mm^2 chip with 40\npins), drastically restricting network size and complexity. These hardware\nconstraints necessitate a comprehensive framework to tailor network designs for\nphysical realizability while minimizing accuracy loss. This paper introduces\nSuperSNN, a comprehensive framework for the implementation of full\nsuperconducting SNNs on a chip within these constraints. The key technical\ncontributions include: (1) A hardware-aware training methodology for SNNs,\nutilizing off-chip pruning and weight quantization for energy-efficient\nsuperconducting implementations. (2) Design and layout of an inference SNN chip\nthat incorporates novel high fan-in neurons and custom superconducting cells.\n(3) An optimized locally synchronous, globally synchronous (LAGS) clock\ndistribution scheme for robust circuit implementation and management of data\ntransfer delays in SFQ SNNs. The main results and findings demonstrate the\neffectiveness of the framework: (1) The complete network achieved 96.47%\naccuracy on the full MNIST dataset after quantization and pruning. (2) The\nfabricated SuperSNN chip successfully classified a reduced set of digits (2, 3,\nand 4) with 80.07% accuracy, reaching a maximum of 86.2% accuracy for digits 0,\n1, and 2. (3) The chip operates at an ultra-high 3.02 GHz clock frequency. (4)\nIt occupies a compact area of 3.4 x 3.9 mm^2, incorporates 5,822 Josephson\nJunctions, consumes 2.15 mW static power, and has an exceptionally low energy\ncost of 6.55 fJ (or 1.31e-6 nJ) per inference."}
{"id": "2509.05303", "pdf": "https://arxiv.org/pdf/2509.05303", "abs": "https://arxiv.org/abs/2509.05303", "authors": ["Sam Davidson", "Li Sun", "Bhavana Bhasker", "Laurent Callot", "Anoop Deoras"], "title": "Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Infrastructure as Code (IaC) is fundamental to modern cloud computing,\nenabling teams to define and manage infrastructure through machine-readable\nconfiguration files. However, different cloud service providers utilize diverse\nIaC formats. The lack of a standardized format requires cloud architects to be\nproficient in multiple IaC languages, adding complexity to cloud deployment.\nWhile Large Language Models (LLMs) show promise in automating IaC creation and\nmaintenance, progress has been limited by the lack of comprehensive benchmarks\nacross multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark\ndataset for evaluating LLM-based IaC generation and mutation across AWS\nCloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset\nconsists of triplets containing initial IaC templates, natural language\nmodification requests, and corresponding updated templates, created through a\nsynthetic data generation pipeline with rigorous validation. We evaluate\nseveral state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while\nmodern LLMs can achieve high success rates (>95%) in generating syntactically\nvalid IaC across formats, significant challenges remain in semantic alignment\nand handling complex infrastructure patterns. Our ablation studies highlight\nthe importance of prompt engineering and retry mechanisms in successful IaC\ngeneration. We release Multi-IaC-Bench to facilitate further research in\nAI-assisted infrastructure management and establish standardized evaluation\nmetrics for this crucial domain."}
{"id": "2509.05596", "pdf": "https://arxiv.org/pdf/2509.05596", "abs": "https://arxiv.org/abs/2509.05596", "authors": ["Soumyadip Bandyopadhyay", "Santonu Sarkar"], "title": "Verifying Correctness of PLC Software during System Evolution using Model Containment Approach", "categories": ["cs.SE", "cs.SC", "68", "D.3.1; D.2.4"], "comment": "31 pages with appendix", "summary": "Upgradation of Programmable Logic Controller (PLC) software is quite common\nto accommodate evolving industrial requirements. Verifying the correctness of\nsuch upgrades remains a significant challenge. In this paper, we propose a\nverification-based approach to ensure the correctness of the existing\nfunctionality in the upgraded version of a PLC software. The method converts\nthe older and the newer versions of the sequential function chart (SFC) into\ntwo Petri net models. We then verify whether one model is contained within\nanother, based on a novel containment checking algorithm grounded in symbolic\npath equivalence. For this purpose, we have developed a home-grown Petri\nnet-based containment checker. Experimental evaluation on 80 real-world\nbenchmarks from the OSCAT library highlights the scalability and effectiveness\nof the framework. We have compared our approach with verifAPS, a popular tool\nused for software upgradation, and observed nearly 4x performance improvement."}
{"id": "2509.05586", "pdf": "https://arxiv.org/pdf/2509.05586", "abs": "https://arxiv.org/abs/2509.05586", "authors": ["Lee Zheng Han", "Umang Mathur"], "title": "Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types", "categories": ["cs.PL", "cs.CC"], "comment": null, "summary": "Verifying linearizability of concurrent data structures is NP-hard, even for\nsimple types. We present fixed-parameter tractable algorithms for monitoring\nstacks, queues, and anagram-agnostic data types (AADTs), parameterized by the\nmaximum concurrency. Our approach leverages frontier graphs and partition\nstates to bound the search space. For AADTs, equivalence of linearizations\nenables monitoring in log-linear time. For stacks, we introduce a grammar-based\nmethod with a sub-cubic reduction to matrix multiplication, and for queues, a\nsplit-sequence transition system supporting efficient dynamic programming.\nThese results unify tractability guarantees for both order-sensitive and\nanagram-agnostic data types under bounded concurrency."}
{"id": "2509.05855", "pdf": "https://arxiv.org/pdf/2509.05855", "abs": "https://arxiv.org/abs/2509.05855", "authors": ["Thijs Masmeijer", "Caleb Swain", "Jeff Hill", "Ed Habtour"], "title": "Programming tension in 3D printed networks inspired by spiderwebs", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Each element in tensioned structural networks -- such as tensegrity,\narchitectural fabrics, or medical braces/meshes -- requires a specific tension\nlevel to achieve and maintain the desired shape, stability, and compliance.\nThese structures are challenging to manufacture, 3D print, or assemble because\nflattening the network during fabrication introduces multiplicative\ninaccuracies in the network's final tension gradients. This study overcomes\nthis challenge by offering a fabrication algorithm for direct 3D printing of\nsuch networks with programmed tension gradients, an approach analogous to the\nspinning of spiderwebs. The algorithm: (i) defines the desired network and\nprescribes its tension gradients using the force density method; (ii) converts\nthe network into an unstretched counterpart by numerically optimizing vertex\nlocations toward target element lengths and converting straight elements into\narcs to resolve any remaining error; and (iii) decomposes the network into\nprintable toolpaths; Optional additional steps are: (iv) flattening curved 2D\nnetworks or 3D networks to ensure 3D printing compatibility; and (v)\nautomatically resolving any unwanted crossings introduced by the flattening\nprocess. The proposed method is experimentally validated using 2D unit cells of\nviscoelastic filaments, where accurate tension gradients are achieved with an\naverage element strain error of less than 1.0\\%. The method remains effective\nfor networks with element minimum length and maximum stress of 5.8 mm and 7.3\nMPa, respectively. The method is used to demonstrate the fabrication of three\ncomplex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The\nprogrammable tension gradient algorithm can be utilized to produce compact,\nintegrated cable networks, enabling novel applications such as moment-exerting\nstructures in medical braces and splints."}
{"id": "2509.05511", "pdf": "https://arxiv.org/pdf/2509.05511", "abs": "https://arxiv.org/abs/2509.05511", "authors": ["Dhanya R Mathews", "Mudit Verma", "Pooja Aggarwal", "J. Lakshmi"], "title": "Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology", "categories": ["cs.PF", "cs.DC"], "comment": null, "summary": "Cloud application services are distributed in nature and have components\nacross the stack working together to deliver the experience to end users. The\nwide adoption of microservice architecture exacerbates failure management due\nto increased service components. To be effective, the strategies to enhance the\napplication service resilience need to be autonomous and developed at the\nservice's granularity, considering its end-to-end components. However, the\nmassive amount of observability data generated by all these components across\nthe service stack poses a significant challenge in reacting to anomalies and\nrestoring the service quality in real time. Identifying the most informative\nobservability data from across the cloud service stack and timely localization\nof root causes of anomalies thus becomes crucial to ensure service resilience.\nThis article presents a novel approach that considers the application service\ntopology to select the most informative metrics across the cloud stack to\nsupport efficient, explainable, and accurate root cause identifications in case\nof performance anomalies. The usefulness of the selected metrics is then\nevaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for\nlocalizing the root cause of performance anomalies. As a step towards improving\nthe accuracy and efficiency of RCD, this article then proposes the\nTopology-Aware-RCD (TA-RCD) that incorporates the end-to-end application\nservice topology in RCD. The evaluation of the failure injection studies shows\nthat the proposed approach performs at least 2X times better on average than\nthe state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall."}
{"id": "2509.05298", "pdf": "https://arxiv.org/pdf/2509.05298", "abs": "https://arxiv.org/abs/2509.05298", "authors": ["Rui Xi", "Xianghan Wang"], "title": "Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression", "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 International Conference on\n  Artificial Intelligence and Virtual Reality (AIVR 2025). \\c{opyright} 2025\n  Springer. This is the author-accepted manuscript. Rui Xi and Xianghan Wang\n  contributed equally to this work. The final version will be available via\n  SpringerLink", "summary": "Loneliness and social isolation pose significant emotional and health\nchallenges, prompting the development of technology-based solutions for\ncompanionship and emotional support. This paper introduces Livia, an\nemotion-aware augmented reality (AR) companion app designed to provide\npersonalized emotional support by combining modular artificial intelligence\n(AI) agents, multimodal affective computing, progressive memory compression,\nand AR driven embodied interaction. Livia employs a modular AI architecture\nwith specialized agents responsible for emotion analysis, dialogue generation,\nmemory management, and behavioral orchestration, ensuring robust and adaptive\ninteractions. Two novel algorithms-Temporal Binary Compression (TBC) and\nDynamic Importance Memory Filter (DIMF)-effectively manage and prioritize\nlong-term memory, significantly reducing storage requirements while retaining\ncritical context. Our multimodal emotion detection approach achieves high\naccuracy, enhancing proactive and empathetic engagement. User evaluations\ndemonstrated increased emotional bonds, improved satisfaction, and\nstatistically significant reductions in loneliness. Users particularly valued\nLivia's adaptive personality evolution and realistic AR embodiment. Future\nresearch directions include expanding gesture and tactile interactions,\nsupporting multi-user experiences, and exploring customized hardware\nimplementations."}
{"id": "2509.05467", "pdf": "https://arxiv.org/pdf/2509.05467", "abs": "https://arxiv.org/abs/2509.05467", "authors": ["Reshma Prasad", "Maxime Elkael", "Gabriele Gemmi", "Osama M. Bushnaq", "Debashisha Mishra", "Prasanna Raut", "Jennifer Simonjan", "Michele Polese", "Tommaso Melodia"], "title": "Joint Routing, Resource Allocation, and Energy Optimization for Integrated Access and Backhaul with Open RAN", "categories": ["cs.NI"], "comment": null, "summary": "As networks evolve towards 6G, Mobile Network Operators (MNOs) must\naccommodate diverse requirements and at the same time manage rising energy\nconsumption. Integrated Access and Backhaul (IAB) networks facilitate dense\ncellular deployments with reduced infrastructure complexity. However, the\nmulti-hop wireless backhauling in IAB networks necessitates proper routing and\nresource allocation decisions to meet the performance requirements. At the same\ntime, cell densification makes energy optimization crucial. This paper\naddresses the joint optimization of routing and resource allocation in IAB\nnetworks through two distinct objectives: energy minimization and throughput\nmaximization. We develop a novel capacity model that links power levels to\nachievable data rates. We propose two practical large-scale approaches to solve\nthe optimization problems and leverage the closed-loop control framework\nintroduced by the Open Radio Access Network (O-RAN) architecture to integrate\nthe solutions. The approaches are evaluated on diverse scenarios built upon\nopen data of two months of traffic collected by network operators in the city\nof Milan, Italy. Results show that the proposed approaches effectively reduces\nnumber of activated nodes to save energy and achieves approximately 100 Mbps of\nminimum data rate per User Equipment (UE) during peak hours of the day using\nspectrum within the Frequency Range (FR) 3, or upper midband. The results\nvalidate the practical applicability of our framework for next-generation IAB\nnetwork deployment and optimization."}
{"id": "2509.06410", "pdf": "https://arxiv.org/pdf/2509.06410", "abs": "https://arxiv.org/abs/2509.06410", "authors": ["Kevin Batz", "Joost-Pieter Katoen", "Tobias Winkler", "Daniel Zilken"], "title": "Verifying Sampling Algorithms via Distributional Invariants", "categories": ["cs.LO", "cs.DM"], "comment": null, "summary": "This paper develops a verification framework aimed at establishing the\ncorrectness of discrete sampling algorithms. We do so by considering\nprobabilistic programs as distribution transformers. Inspired by recent work on\ndistributional verification of Markov models, we introduce the notion of\n(inductive) distributional loop invariants for discrete probabilistic programs.\nThese invariants are embedded in a Hoare-like verification framework that\nincludes proof rules for total and partial correctness. To illustrate the\napplicability of our framework, we prove the correctness of two discrete\nsampling algorithms: the Fast Dice Roller and the Fast Loaded Dice Roller."}
{"id": "2509.05491", "pdf": "https://arxiv.org/pdf/2509.05491", "abs": "https://arxiv.org/abs/2509.05491", "authors": ["Sebastian Hubenschmid", "Marc Satkowski", "Johannes Zagermann", "Julián Méndez", "Niklas Elmqvist", "Steven Feiner", "Tiara Feuchtner", "Jens Emil Grønbæk", "Benjamin Lee", "Dieter Schmalstieg", "Raimund Dachselt", "Harald Reiterer"], "title": "Hybrid User Interfaces: Past, Present, and Future of Complementary Cross-Device Interaction in Mixed Reality", "categories": ["cs.HC"], "comment": "Submitted on the 26 February 2025 to the IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) and awaiting Reviews", "summary": "We investigate hybrid user interfaces (HUIs), aiming to establish a cohesive\nunderstanding and adopt consistent terminology for this nascent research area.\nHUIs combine heterogeneous devices in complementary roles, leveraging the\ndistinct benefits of each. Our work focuses on cross-device interaction between\n2D devices and mixed reality environments, which are particularly compelling,\nleveraging the familiarity of traditional 2D platforms while providing spatial\nawareness and immersion. Although such HUIs have been prominently explored in\nthe context of mixed reality by prior work, we still lack a cohesive\nunderstanding of the unique design possibilities and challenges of such\ncombinations, resulting in a fragmented research landscape. We conducted a\nsystematic survey and present a taxonomy of HUIs that combine conventional\ndisplay technology and mixed reality environments. Based on this, we discuss\npast and current challenges, the evolution of definitions, and prospective\nopportunities to tie together the past 30 years of research with our vision of\nfuture HUIs."}
{"id": "2509.06093", "pdf": "https://arxiv.org/pdf/2509.06093", "abs": "https://arxiv.org/abs/2509.06093", "authors": ["Yuze Liu", "Zhaoyuan Zhang", "Xiangsheng Zeng", "Yihe Zhang", "Leping Yu", "Lejia Wang", "Xi Yu"], "title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "comment": null, "summary": "Chemical and materials research has traditionally relied heavily on knowledge\nnarrative, with progress often driven by language-based descriptions of\nprinciples, mechanisms, and experimental experiences, rather than tables,\nlimiting what conventional databases and ML can exploit. We present a\nlanguage-native database for boron nitride nanosheet (BNNS) polymer thermally\nconductive composites that captures lightly structured information from papers\nacross preparation, characterization, theory-computation, and mechanistic\nreasoning, with evidence-linked snippets. Records are organized in a\nheterogeneous database and queried via composite retrieval with semantics, key\nwords and value filters. The system can synthesizes literature into accurate,\nverifiable, and expert style guidance. This substrate enables high fidelity\nefficient Retrieval Augmented Generation (RAG) and tool augmented agents to\ninterleave retrieval with reasoning and deliver actionable SOP. The framework\nsupplies the language rich foundation required for LLM-driven materials\ndiscovery."}
{"id": "2509.05688", "pdf": "https://arxiv.org/pdf/2509.05688", "abs": "https://arxiv.org/abs/2509.05688", "authors": ["Kuan-Ting Lin", "Ching-Te Chiu", "Jheng-Yi Chang", "Shi-Zong Huang", "Yu-Ting Li"], "title": "High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator", "categories": ["cs.AR"], "comment": null, "summary": "Deep convolution Neural Network (DCNN) has been widely used in computer\nvision tasks. However, for edge devices even inference has too large\ncomputational complexity and data access amount. The inference latency of\nstate-of-the-art models are impractical for real-world applications. In this\npaper, we propose a high utilization energy-aware real-time inference deep\nconvolutional neural network accelerator, which improves the performance of the\ncurrent accelerators. First, we use the 1x1 size convolution kernel as the\nsmallest unit of the computing unit. Then we design suitable computing unit\nbased on the requirements of each model. Secondly, we use Reuse Feature SRAM to\nstore the output of the current layer in the chip and use the value as the\ninput of the next layer. Moreover, we import Output Reuse Strategy and Ring\nStream Dataflow to reduce the amount of data exchange between chips and DRAM.\nFinally, we present On-fly Pooling Module to let the calculation of the Pooling\nlayer directly complete in the chip. With the aid of the proposed method, the\nimplemented acceleration chip has an extremely high hardware utilization rate.\nWe reduce a generous amount of data transfer on the specific module, ECNN.\nCompared to the methods without reuse strategy, we can reduce 533 times of data\naccess amount. At the same time, we have enough computing power to perform\nreal-time execution of the existing image classification model, VGG16 and\nMobileNet. Compared with the design in VWA, we can speed up 7.52 times and have\n1.92x energy efficiency"}
{"id": "2509.05364", "pdf": "https://arxiv.org/pdf/2509.05364", "abs": "https://arxiv.org/abs/2509.05364", "authors": ["Abdollah Baghaei Daemei"], "title": "Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Residential buildings contribute significantly to energy use, health\noutcomes, and carbon emissions. In New Zealand, housing quality has\nhistorically been poor, with inadequate insulation and inefficient heating\ncontributing to widespread energy hardship. Recent reforms, including the\nWarmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code\nupgrades, have delivered health and comfort improvements, yet challenges\npersist. Many retrofits remain partial, data on household performance are\nlimited, and decision-making support for homeowners is fragmented. This study\npresents the design and evaluation of an AI-powered decision-support tool for\nresidential energy efficiency in New Zealand. The prototype, developed using\nPython and Streamlit, integrates data ingestion, anomaly detection, baseline\nmodeling, and scenario simulation (e.g., LED retrofits, insulation upgrades)\ninto a modular dashboard. Fifteen domain experts, including building\nscientists, consultants, and policy practitioners, tested the tool through\nsemi-structured interviews. Results show strong usability (M = 4.3), high value\nof scenario outputs (M = 4.5), and positive perceptions of its potential to\ncomplement subsidy programs and regulatory frameworks. The tool demonstrates\nhow AI can translate national policies into personalized, household-level\nguidance, bridging the gap between funding, standards, and practical\ndecision-making. Its significance lies in offering a replicable framework for\nreducing energy hardship, improving health outcomes, and supporting climate\ngoals. Future development should focus on carbon metrics, tariff modeling,\nintegration with national datasets, and longitudinal trials to assess\nreal-world adoption."}
{"id": "2509.05870", "pdf": "https://arxiv.org/pdf/2509.05870", "abs": "https://arxiv.org/abs/2509.05870", "authors": ["Edith Cohen", "Moshe Shechner", "Uri Stemmer"], "title": "A Simple and Robust Protocol for Distributed Counting", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "We revisit the distributed counting problem, where a server must continuously\napproximate the total number of events occurring across $k$ sites while\nminimizing communication. The communication complexity of this problem is known\nto be $\\Theta(\\frac{k}{\\epsilon}\\log N)$ for deterministic protocols. Huang,\nYi, and Zhang (2012) showed that randomization can reduce this to\n$\\Theta(\\frac{\\sqrt{k}}{\\epsilon}\\log N)$, but their analysis is restricted to\nthe {\\em oblivious setting}, where the stream of events is independent of the\nprotocol's outputs.\n  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed\ncounting that removes the oblivious assumption. However, their communication\ncomplexity is suboptimal by a $polylog(k)$ factor and their protocol is\nsubstantially more complex than the oblivious protocol of Huang et al. (2012).\nThis left open a natural question: could it be that the simple protocol of\nHuang et al. (2012) is already robust?\n  We resolve this question with two main contributions. First, we show that the\nprotocol of Huang et al. (2012) is itself not robust by constructing an\nexplicit adaptive attack that forces it to lose its accuracy. Second, we\npresent a new, surprisingly simple, robust protocol for distributed counting\nthat achieves the optimal communication complexity of\n$O(\\frac{\\sqrt{k}}{\\epsilon} \\log N)$. Our protocol is simpler than that of\nXiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and\nis the first to match the optimal oblivious complexity in the adaptive setting."}
{"id": "2509.05749", "pdf": "https://arxiv.org/pdf/2509.05749", "abs": "https://arxiv.org/abs/2509.05749", "authors": ["AmirHossein Naghshzan"], "title": "Automating API Documentation with LLMs: A BERTopic Approach", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Developers rely on API documentation, but official sources are often lengthy,\ncomplex, or incomplete. Many turn to community-driven forums like Stack\nOverflow for practical insights. We propose automating the summarization of\ninformal sources, focusing on Android APIs. Using BERTopic, we extracted\nprevalent topics from 3.6 million Stack Overflow posts and applied extractive\nsummarization techniques to generate concise summaries, including code\nsnippets. A user study with 30 Android developers assessed the summaries for\ncoherence, relevance, informativeness, and satisfaction, showing improved\nproductivity. Integrating formal API knowledge with community-generated content\nenhances documentation, making API resources more accessible and actionable\nwork."}
{"id": "2509.06724", "pdf": "https://arxiv.org/pdf/2509.06724", "abs": "https://arxiv.org/abs/2509.06724", "authors": ["Florian Kohn", "Arthur Correnson", "Jan Baumeister", "Bernd Finkbeiner"], "title": "Pacing Types: Safe Monitoring of Asynchronous Streams", "categories": ["cs.PL"], "comment": null, "summary": "Stream-based monitoring is a real-time safety assurance mechanism for complex\ncyber-physical systems such as unmanned aerial vehicles. In this context, a\nmonitor aggregates streams of input data from sensors and other sources to give\nreal-time statistics and assessments of the system's health. Since monitors are\nsafety-critical components, it is crucial to ensure that they are free of\npotential runtime errors. One of the central challenges in designing reliable\nstream-based monitors is to deal with the asynchronous nature of data streams:\nin concrete applications, the different sensors being monitored produce values\nat different speeds, and it is the monitor's responsibility to correctly react\nto the asynchronous arrival of different streams of values. To ease this\nprocess, modern frameworks for stream-based monitoring such as RTLola feature\nan expressive specification language that allows to finely specify data\nsynchronization policies. While this feature dramatically simplifies the design\nof monitors, it can also lead to subtle runtime errors. To mitigate this issue,\nthis paper presents pacing types, a novel type system implemented in RTLola to\nensure that monitors for asynchronous streams are well-behaved at runtime. We\nformalize the essence of pacing types for a core fragment of RTLola, and\npresent a soundness proof of the pacing type system using a new logical\nrelation."}
{"id": "2509.06573", "pdf": "https://arxiv.org/pdf/2509.06573", "abs": "https://arxiv.org/abs/2509.06573", "authors": ["Jie Zhou", "Linzi Qu", "Miu-Ling Lam", "Hongbo Fu"], "title": "From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters", "categories": ["cs.GR"], "comment": null, "summary": "Hand-drawn character animation is a vibrant field in computer graphics,\npresenting challenges in achieving geometric consistency while conveying\nexpressive motion. Traditional skeletal animation methods maintain geometric\nconsistency but struggle with complex non-rigid elements like flowing hair and\nskirts, leading to unnatural deformation. Conversely, video diffusion models\nsynthesize realistic dynamics but often create geometric distortions in\nstylized drawings due to domain gaps. This work proposes a hybrid animation\nsystem that combines skeletal animation and video diffusion. Initially, coarse\nimages are generated from characters retargeted with skeletal animations for\ngeometric guidance. These images are then enhanced in texture and secondary\ndynamics using video diffusion priors, framing this enhancement as an\ninpainting task. A domain-adapted diffusion model refines user-masked regions\nneeding improvement, especially for secondary dynamics. To enhance motion\nrealism further, we introduce a Secondary Dynamics Injection (SDI) strategy in\nthe denoising process, incorporating features from a pre-trained diffusion\nmodel enriched with human motion priors. Additionally, to tackle unnatural\ndeformations from low-poly single-mesh character modeling, we present a Hair\nLayering Modeling (HLM) technique that uses segmentation maps to separate hair\nfrom the body, allowing for more natural animation of long-haired characters.\nExtensive experiments show that our system outperforms state-of-the-art methods\nin both quantitative and qualitative evaluations."}
{"id": "2509.05790", "pdf": "https://arxiv.org/pdf/2509.05790", "abs": "https://arxiv.org/abs/2509.05790", "authors": ["Hai Dinh-Tuan", "Franz Florian Six"], "title": "Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach", "categories": ["cs.PF"], "comment": null, "summary": "Modern software architectures are characterized by their cloud-native,\nmodular, and microservice-based designs. While these systems are known for\ntheir efficiency, they also face complex challenges in service optimization,\nespecially in maintaining end-to-end quality of service across dynamically\ndistributed services. This paper introduces a novel approach using the concept\nof Service Affinity to address this challenge. The proposed method, termed\nService Affinity Graph-based Approach, employs a graph-based model to model the\ninteractions among microservices. It formulates the service placement as a\nminimum-weight k-cut problem and utilizes an approximation algorithm for\nservice clustering. This approach is realized through a conceptual framework\nthat takes into account a wide range of optimization objectives, ranging from\nenhancing application performance and enforcing data privacy to optimizing\noperational costs. In addition to presenting the SAGA framework in details,\nthis paper conducts an in-depth empirical evaluation using a prototype deployed\non a Kubernetes cluster. The results demonstrate a mean latency improvement of\n23.40%, validating the effectiveness of our approach. Finally, the paper\ncomprehensively discusses various aspects of the proposed methods, including\ntheir implications, challenges, and benefits, providing a thorough analysis of\nthe approach's impact."}
{"id": "2509.05323", "pdf": "https://arxiv.org/pdf/2509.05323", "abs": "https://arxiv.org/abs/2509.05323", "authors": ["Adam Cole", "Mick Grierson"], "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts", "categories": ["cs.AI", "cs.MM"], "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts)\n  at the ACM Creativity and Cognition Conference 2025", "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium."}
{"id": "2509.05759", "pdf": "https://arxiv.org/pdf/2509.05759", "abs": "https://arxiv.org/abs/2509.05759", "authors": ["Jinkun Geng", "Shuai Mu", "Anirudh Sivaraman", "Balaji Prabhakar"], "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]", "categories": ["cs.NI", "cs.DB", "cs.DC", "68M10, 68M15"], "comment": "This is the technical report for our paper accepted by The 31st\n  Symposium on Operating Systems Principles (SOSP'25)", "summary": "This paper presents Tiga, a new design for geo-replicated and scalable\ntransactional databases such as Google Spanner. Tiga aims to commit\ntransactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of\nscenarios, while maintaining high throughput with minimal computational\noverhead. Tiga consolidates concurrency control and consensus, completing both\nstrictly serializable execution and consistent replication in a single round.\nIt uses synchronized clocks to proactively order transactions by assigning each\na future timestamp at submission. In most cases, transactions arrive at servers\nbefore their future timestamps and are serialized according to the designated\ntimestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed\nand proactive ordering fails, in which case Tiga falls back to a slow path,\ncommitting in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can\ncommit more transactions at 1-WRTT latency, and incurs much less throughput\noverhead. Evaluation results show that Tiga outperforms all baselines,\nachieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower\nlatency. Tiga is open-sourced at\nhttps://github.com/New-Consensus-Concurrency-Control/Tiga."}
{"id": "2509.06841", "pdf": "https://arxiv.org/pdf/2509.06841", "abs": "https://arxiv.org/abs/2509.06841", "authors": ["Paweł Rzążewski", "Michał Stronkowski"], "title": "Tabular intermediate logics comparison", "categories": ["cs.LO"], "comment": null, "summary": "Tabular intermediate logics are intermediate logics characterized by finite\nposets treated as Kripke frames. For a poset $\\mathbb{P}$, let $L(\\mathbb{P})$\ndenote the corresponding tabular intermediate logic. We investigate the\ncomplexity of the following decision problem $\\mathsf{LogContain}$: given two\nfinite posets $\\mathbb P$ and $\\mathbb Q$, decide whether $L(\\mathbb P)\n\\subseteq L(\\mathbb Q)$.\n  By Jankov's and de Jongh's theorem, the problem $\\mathsf{LogContain}$ is\nrelated to the problem $\\mathsf{SPMorph}$: given two finite posets $\\mathbb P$\nand $\\mathbb Q$, decide whether there exists a surjective $p$-morphism from\n$\\mathbb P$ onto $\\mathbb Q$. Both problems belong to the complexity class NP.\n  We present two contributions. First, we describe a construction which,\nstarting with a graph $\\mathbb{G}$, gives a poset $\\mathsf{Pos}(\\mathbb{G})$\nsuch that there is a surjective locally surjective homomorphism (the\ngraph-theoretic analog of a $p$-morphism) from $\\mathbb{G}$ onto $\\mathbb{H}$\nif and only if there is a surjective $p$-morphism from\n$\\mathsf{Pos}(\\mathbb{G})$ onto $\\mathsf{Pos}(\\mathbb{H})$. This allows us to\ntranslate some hardness results from graph theory and obtain that several\nrestricted versions of the problems $\\mathsf{LogContain}$ and\n$\\mathsf{SPMorph}$ are NP-complete. Among other results, we present a\n18-element poset $\\mathbb{Q}$ such that the problem to decide, for a given\nposet $\\mathbb{P}$, whether $L(\\mathbb{P})\\subseteq L(\\mathbb{Q})$ is\nNP-complete.\n  Second, we describe a polynomial-time algorithm that decides\n$\\mathsf{LogContain}$ and $\\mathsf{SPMorph}$ for posets $\\mathbb{T}$ and\n$\\mathbb{Q}$, when $\\mathbb{T}$ is a tree."}
{"id": "2509.05619", "pdf": "https://arxiv.org/pdf/2509.05619", "abs": "https://arxiv.org/abs/2509.05619", "authors": ["Ruiqi Chen", "Qingyang He", "Hanxi Bao", "Jung Choi", "Xin Tong"], "title": "GestoBrush: Facilitating Graffiti Artists' Digital Creation Experiences through Embodied AR Interactions", "categories": ["cs.HC", "H.5.1; H.5.2; H.5.1; H.5.2; H.5.m"], "comment": "Accepted to VINCI 2025; 8 pages, 3 figures", "summary": "Graffiti has long documented the socio-cultural landscapes of urban spaces,\nyet increasing global regulations have constrained artists' creative freedom,\nprompting exploration of digital alternatives. Augmented Reality (AR) offers\nopportunities to extend graffiti into digital environments while retaining\nspatial and cultural significance, but prior research has largely centered on\naudience engagement rather than the embodied creative processes of graffiti\nartists. To address this, we developed GestoBrush, a mobile AR prototype that\nturns smartphones into virtual spray cans, enabling graffiti creation through\nembodied gestures. A co-design workshop underscored the role of\nembodiment-physical engagement with surroundings and body-driven creative\nprocesses-in digital workflows. We evaluated GestoBrush with six graffiti\nartists and findings suggested that embodied AR interactions supporting artists\nbypass real-world constraints and explore new artistic possibilities, whose AR\nartworks created enhanced senses of intuitiveness, immersion, and\nexpressiveness. This work highlight how embodied AR tools can bridge the gap\nbetween physical graffiti practice and digital expression, suggesting pathways\nfor designing immersive creative systems that respect the cultural ethos of\nstreet art while expanding its possibilities in virtual spaces."}
{"id": "2509.06298", "pdf": "https://arxiv.org/pdf/2509.06298", "abs": "https://arxiv.org/abs/2509.06298", "authors": ["Zihan Yan", "Rui Xi", "Mengshu Hou"], "title": "MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration", "categories": ["cs.DB"], "comment": null, "summary": "Database knob tuning is essential for optimizing the performance of modern\ndatabase management systems, which often expose hundreds of knobs with\ncontinuous or categorical values. However, the large number of knobs and the\nvast configuration space make it difficult to identify optimal settings\nefficiently. Although learning-based tuning has shown promise, existing\napproaches either ignore domain knowledge by relying solely on benchmark\nfeedback or struggle to explore the high-dimensional knob space, resulting in\nhigh tuning costs and suboptimal performance. To address these challenges, we\npropose MCTuner, an adaptive knob tuning framework that minimizes exploration\nin ineffective regions of the configuration space. MCTuner employs a\nMixture-of-Experts (MoE) mechanism with specialized LLMs to identify\nperformance-critical knobs. In further, MCTuner introduces the first spatial\ndecomposition algorithm that recursively partitions the space into hierarchical\nsubspaces, on which Bayesian Optimization is performed to efficiently search\nfor near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,\nand HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster\nconfiguration discovery per iteration compared to state-of-the-art methods."}
{"id": "2509.05937", "pdf": "https://arxiv.org/pdf/2509.05937", "abs": "https://arxiv.org/abs/2509.05937", "authors": ["Wei-Hsing Huang", "Jianwei Jia", "Yuyao Kong", "Faaiq Waqar", "Tai-Hao Wen", "Meng-Fan Chang", "Shimeng Yu"], "title": "Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems", "categories": ["cs.AR"], "comment": null, "summary": "Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an\ninnovative architectural paradigm capable of replicating conventional deep\nneural network (DNN) capabilities while utilizing significantly reduced\nparameter counts through the employment of parameterized B-spline functions\nwith trainable coefficients. Nevertheless, the B-spline functional components\ninherent to KAN architectures introduce distinct hardware acceleration\ncomplexities. While B-spline function evaluation can be accomplished through\nlook-up table (LUT) implementations that directly encode functional mappings,\nthus minimizing computational overhead, such approaches continue to demand\nconsiderable circuit infrastructure, including LUTs, multiplexers, decoders,\nand related components. This work presents an algorithm-hardware co-design\napproach for KAN acceleration. At the algorithmic level, techniques include\nAlignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity\naware mapping strategy, and circuit-level techniques include N:1 Time\nModulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)\ncircuits. This work conducts evaluations on large-scale KAN networks to\nvalidate the proposed methodologies. Non-ideality factors, including partial\nsum deviations from process variations, have been evaluated with statistics\nmeasured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally\ndetermined KAN hyperparameters in conjunction with circuit optimizations\nfabricated at the 22nm technology node, despite the parameter count for\nlarge-scale tasks in this work increasing by 500Kx to 807Kx compared to\ntiny-scale tasks in previous work, the area overhead increases by only 28Kx to\n41Kx, with power consumption rising by merely 51x to 94x, while accuracy\ndegradation remains minimal at 0.11% to 0.23%, demonstrating the scaling\npotential of our proposed architecture."}
{"id": "2509.05370", "pdf": "https://arxiv.org/pdf/2509.05370", "abs": "https://arxiv.org/abs/2509.05370", "authors": ["Tanya Joshi", "Krishnendu Guha"], "title": "Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection", "categories": ["cs.CR", "cs.ET"], "comment": "10 pages", "summary": "This study explores the application of quantum machine learning (QML)\nalgorithms to enhance cybersecurity threat detection, particularly in the\nclassification of malware and intrusion detection within high-dimensional\ndatasets. Classical machine learning approaches encounter limitations when\ndealing with intricate, obfuscated malware patterns and extensive network\nintrusion data. To address these challenges, we implement and evaluate various\nQML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector\nMachines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for\nmalware detection tasks. Our experimental analysis utilized two datasets: the\nIntrusion dataset, comprising 150 samples with 56 memory-based features derived\nfrom Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,\ncontaining 58,596 samples with 57 features representing benign and malicious\nsoftware. Remarkably, our QML methods demonstrated superior performance\ncompared to classical approaches, achieving accuracies of 95% for QNN and 94%\nfor QSVM. These quantum-enhanced methods leveraged quantum superposition and\nentanglement principles to accurately identify complex patterns within highly\nobfuscated malware samples that were imperceptible to classical methods. To\nfurther advance malware analysis, we propose a novel real-time malware analysis\nframework that incorporates Quantum Feature Extraction using Quantum Fourier\nTransform, Quantum Feature Maps, and Classification using Variational Quantum\nCircuits. This system integrates explainable AI methods, including GradCAM++\nand ScoreCAM algorithms, to provide interpretable insights into the quantum\ndecision-making processes."}
{"id": "2509.06046", "pdf": "https://arxiv.org/pdf/2509.06046", "abs": "https://arxiv.org/abs/2509.06046", "authors": ["Philip Adams", "Menghao Li", "Shi Zhang", "Li Tan", "Qi Chen", "Mingqin Li", "Zengzhong Li", "Knut Risvik", "Harsha Vardhan Simhadri"], "title": "DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers", "categories": ["cs.DC", "cs.DS", "cs.IR", "E.1; H.3.3"], "comment": null, "summary": "We present DISTRIBUTEDANN, a distributed vector search service that makes it\npossible to search over a single 50 billion vector graph index spread across\nover a thousand machines that offers 26ms median query latency and processes\nover 100,000 queries per second. This is 6x more efficient than existing\npartitioning and routing strategies that route the vector query to a subset of\npartitions in a scale out vector search system. DISTRIBUTEDANN is built using\ntwo well-understood components: a distributed key-value store and an in-memory\nANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for\nserving the Bing search engine, and we share our experience from making this\ntransition."}
{"id": "2509.05769", "pdf": "https://arxiv.org/pdf/2509.05769", "abs": "https://arxiv.org/abs/2509.05769", "authors": ["Edyta Brzychczy", "Urszula Jessen", "Krzysztof Kluza", "Sridhar Sriram", "Manuel Vargas Nettelnstroth"], "title": "IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining", "categories": ["cs.SE"], "comment": "17 pages, conference draft", "summary": "This paper presents IoT Miner, a novel framework for automatically creating\nhigh-level event logs from raw industrial sensor data to support process\nmining. In many real-world settings, such as mining or manufacturing, standard\nevent logs are unavailable, and sensor data lacks the structure and semantics\nneeded for analysis. IoT Miner addresses this gap using a four-stage pipeline:\ndata preprocessing, unsupervised clustering, large language model (LLM)-based\nlabeling, and event log construction. A key innovation is the use of LLMs to\ngenerate meaningful activity labels from cluster statistics, guided by\ndomain-specific prompts. We evaluate the approach on sensor data from a\nLoad-Haul-Dump (LHD) mining machine and introduce a new metric,\nSimilarity-Weighted Accuracy, to assess labeling quality. Results show that\nricher prompts lead to more accurate and consistent labels. By combining AI\nwith domain-aware data processing, IoT Miner offers a scalable and\ninterpretable method for generating event logs from IoT data, enabling process\nmining in settings where traditional logs are missing."}
{"id": "2509.06752", "pdf": "https://arxiv.org/pdf/2509.06752", "abs": "https://arxiv.org/abs/2509.06752", "authors": ["Amir M. Ben-Amram", "Samir Genaim", "Joël Ouaknine", "James Worrell"], "title": "Termination Analysis of Linear-Constraint Programs", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "This Survey provides an overview of techniques in termination analysis for\nprograms with numerical variables and transitions defined by linear\nconstraints. This subarea of program analysis is challenging due to the\nexistence of undecidable problems, and this Survey systematically explores\napproaches that mitigate this inherent difficulty. These include foundational\ndecidability results, the use of ranking functions, and disjunctive\nwell-founded transition invariants. The Survey also discusses non-termination\nwitnesses, used to prove that a program will not halt. We examine the\nalgorithmic and complexity aspects of these methods, showing how different\napproaches offer a trade-off between expressive power and computational\ncomplexity. The Survey does not discuss how termination analysis is performed\non real-world programming languages, nor does it consider more expressive\nabstract models that include non-linear arithmetic, probabilistic choice, or\nterm rewriting systems."}
{"id": "2509.06607", "pdf": "https://arxiv.org/pdf/2509.06607", "abs": "https://arxiv.org/abs/2509.06607", "authors": ["Marilyn Keller", "Keenon Werling", "Soyong Shin", "Scott Delp", "Sergi Pujades", "C. Karen Liu", "Michael J. Black"], "title": "From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Great progress has been made in estimating 3D human pose and shape from\nimages and video by training neural networks to directly regress the parameters\nof parametric human models like SMPL. However, existing body models have\nsimplified kinematic structures that do not correspond to the true joint\nlocations and articulations in the human skeletal system, limiting their\npotential use in biomechanics. On the other hand, methods for estimating\nbiomechanically accurate skeletal motion typically rely on complex motion\ncapture systems and expensive optimization methods. What is needed is a\nparametric 3D human model with a biomechanically accurate skeletal structure\nthat can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL\nbody model with a biomechanics skeleton. To enable this, we need training data\nof skeletons inside SMPL meshes in diverse poses.\n  We build such a dataset by optimizing biomechanically accurate skeletons\ninside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL\nmesh vertices to the optimized joint locations and bone rotations. Finally, we\nre-parametrize the SMPL mesh with the new kinematic parameters. The resulting\nSKEL model is animatable like SMPL but with fewer, and\nbiomechanically-realistic, degrees of freedom. We show that SKEL has more\nbiomechanically accurate joint locations than SMPL, and the bones fit inside\nthe body surface better than previous methods. By fitting SKEL to SMPL meshes\nwe are able to \"upgrade\" existing human pose and shape datasets to include\nbiomechanical parameters. SKEL provides a new tool to enable biomechanics in\nthe wild, while also providing vision and graphics researchers with a better\nconstrained and more realistic model of human articulation. The model, code,\nand data are available for research at https://skel.is.tue.mpg.de.."}
{"id": "2509.05794", "pdf": "https://arxiv.org/pdf/2509.05794", "abs": "https://arxiv.org/abs/2509.05794", "authors": ["Hai Dinh-Tuan", "Jialun Jiang"], "title": "Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing", "categories": ["cs.PF", "cs.CE"], "comment": null, "summary": "The widespread adoption of microservices architecture in modern software\nsystems has emphasized the need for efficient management of distributed\nservices. While stateless microservices enable straightforward migration,\nstateful microservices introduce added complexity due to the need to preserve\nin-memory state during migration. However, most container orchestrators,\nincluding Kubernetes, lack native support for live stateful service migration.\nThis paper proposes an optimized migration scheme for stateful services in\nKubernetes by integrating the Message-based Stateful Microservice Migration\n(MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC)\nfeature. Key enhancements include support for migrating StatefulSet-managed\nPods and the introduction of a Threshold-Based Cutoff Mechanism to handle high\nincoming message rates. Evaluation results demonstrate that MS2M for individual\nPods reduces downtime by 96.986% compared to cold migration methods, while the\nStatefulSet approach provides greater flexibility in managing stateful\nservices. These insights provide practical strategies for optimizing stateful\nmicroservice migration in cloud-native environments."}
{"id": "2509.05334", "pdf": "https://arxiv.org/pdf/2509.05334", "abs": "https://arxiv.org/abs/2509.05334", "authors": ["Diwen Huang"], "title": "A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices", "categories": ["cs.CV", "cs.MM", "H.5.1; I.2.10"], "comment": "6 pages, 3 figures, 1 table. Independent research preprint", "summary": "Performance metrics in sports, such as shot speed and angle, provide crucial\nfeedback for athlete development. However, the technology to capture these\nmetrics has historically been expensive, complex, and largely inaccessible to\namateur and recreational players. This paper addresses this gap in the context\nof badminton, one of the world's most popular sports, by introducing a novel,\ncost-effective, and user-friendly system for measuring smash speed using\nubiquitous smartphone technology. Our approach leverages a custom-trained\nYOLOv5 model for shuttlecock detection, combined with a Kalman filter for\nrobust trajectory tracking. By implementing a video-based kinematic speed\nestimation method with spatiotemporal scaling, the system automatically\ncalculates the shuttlecock's velocity from a standard video recording. The\nentire process is packaged into an intuitive mobile application, democratizing\naccess to high-level performance analytics and empowering players at all levels\nto analyze and improve their game."}
{"id": "2509.05889", "pdf": "https://arxiv.org/pdf/2509.05889", "abs": "https://arxiv.org/abs/2509.05889", "authors": ["Mahsa Paknejad", "Parisa Fard Moshiri", "Murat Simsek", "Burak Kantarci", "Hussein T. Mouftah"], "title": "On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks with Reduced Latency and Task Loss", "categories": ["cs.NI"], "comment": "12 pages, 9 figures, accepted to IEEE Internet of Things Journal", "summary": "Real-time task processing is a critical challenge in vehicular networks,\nwhere achieving low latency and minimizing dropped task ratio depend on\nefficient task execution. Our primary objective is to maximize the number of\ncompleted tasks while minimizing overall latency, with a particular focus on\nreducing number of dropped tasks. To this end, we investigate both static and\ndynamic versions of an optimization algorithm. The static version assumes full\ntask availability, while the dynamic version manages tasks as they arrive. We\nalso distinguish between online and offline cases: the online version\nincorporates execution time into the offloading decision process, whereas the\noffline version excludes it, serving as a theoretical benchmark for optimal\nperformance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm\n(On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm\nOptimization (PSO) baseline assumes all tasks are transferred to the RSU and\nprocessed by the MEC, and its offline version disregards execution time, making\nit infeasible for real-time applications despite its optimal performance in\ntheory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the\nmost complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It\nalso outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22%\nreduction in average latency in complex scenarios. Furthermore, it requires\nneither a dataset nor a training phase, and its low computational complexity\nensures efficiency and scalability in dynamic environments."}
{"id": "2509.05762", "pdf": "https://arxiv.org/pdf/2509.05762", "abs": "https://arxiv.org/abs/2509.05762", "authors": ["Shibashis Guha", "Anirban Majumdar", "Prince Mathew", "A. V. Sreejith"], "title": "Scalable Learning of One-Counter Automata via State-Merging Algorithms", "categories": ["cs.FL", "cs.DS", "cs.LO", "F.3.1; F.4.3"], "comment": "18 pages, 24 figures, 3 procedures", "summary": "We propose One-counter Positive Negative Inference (OPNI), a passive learning\nalgorithm for deterministic real-time one-counter automata (DROCA). Inspired by\nthe RPNI algorithm for regular languages, OPNI constructs a DROCA consistent\nwith any given valid sample set.\n  We further present a method for combining OPNI with active learning of DROCA,\nand provide an implementation of the approach. Our experimental results\ndemonstrate that this approach scales more effectively than existing\nstate-of-the-art algorithms. We also evaluate the performance of the proposed\napproach for learning visibly one-counter automata."}
{"id": "2509.05718", "pdf": "https://arxiv.org/pdf/2509.05718", "abs": "https://arxiv.org/abs/2509.05718", "authors": ["Péter Ferenc Gyarmati", "Manfred Klaffenböck", "Laura Koesten", "Torsten Möller"], "title": "Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization", "categories": ["cs.HC"], "comment": "2 pages, 2 figures. Accepted submission to the poster track of IEEE\n  VIS 2025", "summary": "Vision-language models (VLMs) hold promise for enhancing visualization tools,\nbut effective human-AI collaboration hinges on a shared perceptual\nunderstanding of visual content. Prior studies assessed VLM visualization\nliteracy through interpretive tasks, revealing an over-reliance on textual cues\nrather than genuine visual analysis. Our study investigates a more foundational\nskill underpinning such literacy: the ability of VLMs to recognize a chart's\ncore visual properties as humans do. We task 13 diverse VLMs with classifying\nscientific visualizations based solely on visual stimuli, according to three\ncriteria: purpose (e.g., schematic, GUI, visualization), encoding (e.g., bar,\npoint, node-link), and dimensionality (e.g., 2D, 3D). Using expert labels from\nthe human-centric VisType typology as ground truth, we find that VLMs often\nidentify purpose and dimensionality accurately but struggle with specific\nencoding types. Our preliminary results show that larger models do not always\nequate to superior performance and highlight the need for careful integration\nof VLMs in visualization tasks, with human supervision to ensure reliable\noutcomes."}
{"id": "2509.06439", "pdf": "https://arxiv.org/pdf/2509.06439", "abs": "https://arxiv.org/abs/2509.06439", "authors": ["David Robert Pratten", "Luke Mathieson", "Fahimeh Ramezani"], "title": "Relational Algebras for Subset Selection and Optimisation", "categories": ["cs.DB", "cs.DM", "cs.MS"], "comment": "15 pages main text, 28 pages appendicies", "summary": "The database community lacks a unified relational query language for subset\nselection and optimisation queries, limiting both user expression and query\noptimiser reasoning about such problems. Decades of research (latterly under\nthe rubric of prescriptive analytics) have produced powerful evaluation\nalgorithms with incompatible, ad-hoc SQL extensions that specify and filter\nthrough distinct mechanisms. We present the first unified algebraic foundation\nfor these queries, introducing relational exponentiation to complete the\nfundamental algebraic operations alongside union (addition) and cross product\n(multiplication). First, we extend relational algebra to complete domain\nrelations-relations defined by characteristic functions rather than explicit\nextensions-achieving the expressiveness of NP-complete/hard problems, while\nsimultaneously providing query safety for finite inputs. Second, we introduce\nsolution sets, a higher-order relational algebra over sets of relations that\nnaturally expresses search spaces as functions f: Base to Decision, yielding\n|Decision|^|Base| candidate relations. Third, we provide structure-preserving\ntranslation semantics from solution sets to standard relational algebra,\nenabling mechanical translation to existing evaluation algorithms. This\nframework achieves the expressiveness of the most powerful prior approaches\nwhile providing the theoretical clarity and compositional properties absent in\nprevious work. We demonstrate the capabilities these algebras open up through a\npolymorphic SQL where standard clauses seamlessly express data management,\nsubset selection, and optimisation queries within a single paradigm."}
{"id": "2509.06101", "pdf": "https://arxiv.org/pdf/2509.06101", "abs": "https://arxiv.org/abs/2509.06101", "authors": ["Fan Li", "Mimi Xie", "Yanan Guo", "Huize Li", "Xin Xin"], "title": "SCREME: A Scalable Framework for Resilient Memory Design", "categories": ["cs.AR"], "comment": null, "summary": "The continuing advancement of memory technology has not only fueled a surge\nin performance, but also substantially exacerbate reliability challenges.\nTraditional solutions have primarily focused on improving the efficiency of\nprotection schemes, i.e., Error Correction Codes (ECC), under the assumption\nthat allocating additional memory space for parity data is always expensive and\ntherefore not a scalable solution.\n  We break the stereotype by proposing an orthogonal approach that provides\nadditional, cost-effective memory space for resilient memory design. In\nparticular, we recognize that ECC chips (used for parity storage) do not\nnecessarily require the same performance level as regular data chips. This\noffers two-fold benefits: First, the bandwidth originally provisioned for a\nregular-performance ECC chip can instead be used to accommodate multiple\nlow-performance chips. Second, the cost of ECC chips can be effectively\nreduced, as lower performance often correlates with lower expense. In addition,\nwe observe that server-class memory chips are often provisioned with ample, yet\nunderutilized I/O resources. This further offers the opportunity to repurpose\nthese resources to enable flexible on-DIMM interconnections. Based on the above\ntwo insights, we finally propose SCREME, a scalable memory framework leverages\ncost-effective, albeit slower, chips -- naturally produced during rapid\ntechnology evolution -- to meet the growing reliability demands driven by this\nevolution."}
{"id": "2509.06176", "pdf": "https://arxiv.org/pdf/2509.06176", "abs": "https://arxiv.org/abs/2509.06176", "authors": ["Zsolt Almási", "Hannah Bleher", "Johannes Bleher", "Rozanne Tuesday Flores", "Guo Xuanyang", "Paweł Pujszo", "Raphaël Weuts"], "title": "AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "68T01, 68T20, 91-08, 97U50, 97B10", "I.2.0; K.4.1; K.4.2; K.3.2"], "comment": null, "summary": "As artificial intelligence (AI) systems permeate critical sectors, the need\nfor professionals who can address ethical, legal and governance challenges has\nbecome urgent. Current AI ethics education remains fragmented, often siloed by\ndiscipline and disconnected from practice. This paper synthesizes literature\nand regulatory developments to propose a modular, interdisciplinary curriculum\nthat integrates technical foundations with ethics, law and policy. We highlight\nrecurring operational failures in AI - bias, misspecified objectives,\ngeneralization errors, misuse and governance breakdowns - and link them to\npedagogical strategies for teaching AI governance. Drawing on perspectives from\nthe EU, China and international frameworks, we outline a semester plan that\nemphasizes integrated ethics, stakeholder engagement and experiential learning.\nThe curriculum aims to prepare students to diagnose risks, navigate regulation\nand engage diverse stakeholders, fostering adaptive and ethically grounded\nprofessionals for responsible AI governance."}
{"id": "2509.06064", "pdf": "https://arxiv.org/pdf/2509.06064", "abs": "https://arxiv.org/abs/2509.06064", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Non-Vertex-Transitive Graphs Under Round Robin", "categories": ["cs.DC", "math.CO"], "comment": "16 pages, 3 figures", "summary": "The Gathering problem for a swarm of robots asks for a distributed algorithm\nthat brings such entities to a common place, not known in advance. We consider\nthe well-known OBLOT model with robots constrained to move along the edges of a\ngraph, hence gathering in one vertex, eventually. Despite the classical setting\nunder which the problem has been usually approached, we consider the `hostile'\ncase where: i) the initial configuration may contain multiplicities, i.e. more\nthan one robot may occupy the same vertex; ii) robots cannot detect\nmultiplicities. As a scheduler for robot activation, we consider the\n\"favorable\" round-robin case, where robots are activated one at a time.\n  Our objective is to achieve a complete characterization of the problem in the\nbroad context of non-vertex-transitive graphs, i.e., graphs where the vertices\nare partitioned into at least two different classes of equivalence. We provide\na resolution algorithm for any configuration of robots moving on such graphs,\nalong with its correctness. Furthermore, we analyze its time complexity."}
{"id": "2509.05881", "pdf": "https://arxiv.org/pdf/2509.05881", "abs": "https://arxiv.org/abs/2509.05881", "authors": ["Qianheng Zhang", "Song Gao", "Chen Wei", "Yibo Zhao", "Ying Nie", "Ziru Chen", "Shijie Chen", "Yu Su", "Huan Sun"], "title": "GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation", "categories": ["cs.SE", "cs.AI", "I.2"], "comment": "34 pages, 8 figures", "summary": "Recent advances in large language models (LLMs) have fueled growing interest\nin automating geospatial analysis and GIS workflows, yet their actual\ncapabilities remain uncertain. In this work, we call for rigorous evaluation of\nLLMs on well-defined geoprocessing tasks before making claims about full GIS\nautomation. To this end, we present GeoAnalystBench, a benchmark of 50\nPython-based tasks derived from real-world geospatial problems and carefully\nvalidated by GIS experts. Each task is paired with a minimum deliverable\nproduct, and evaluation covers workflow validity, structural alignment,\nsemantic similarity, and code quality (CodeBLEU). Using this benchmark, we\nassess both proprietary and open source models. Results reveal a clear gap:\nproprietary models such as ChatGPT-4o-mini achieve high validity 95% and\nstronger code alignment (CodeBLEU 0.39), while smaller open source models like\nDeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%\nvalidity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as\nspatial relationship detection or optimal site selection, remain the most\nchallenging across all models. These findings demonstrate both the promise and\nlimitations of current LLMs in GIS automation and provide a reproducible\nframework to advance GeoAI research with human-in-the-loop support."}
{"id": "2509.06794", "pdf": "https://arxiv.org/pdf/2509.06794", "abs": "https://arxiv.org/abs/2509.06794", "authors": ["Shihan Fang", "Hongzheng Chen", "Niansong Zhang", "Jiajie Li", "Han Meng", "Adrian Liu", "Zhiru Zhang"], "title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "categories": ["cs.PL", "cs.AR", "cs.LG"], "comment": null, "summary": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance."}
{"id": "2509.06950", "pdf": "https://arxiv.org/pdf/2509.06950", "abs": "https://arxiv.org/abs/2509.06950", "authors": ["Nithin Gopalakrishnan Nair", "Srinivas Kaza", "Xuan Luo", "Vishal M. Patel", "Stephen Lombardi", "Jungyeon Park"], "title": "Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "Large transformer-based models have made significant progress in\ngeneralizable novel view synthesis (NVS) from sparse input views, generating\nnovel viewpoints without the need for test-time optimization. However, these\nmodels are constrained by the limited diversity of publicly available scene\ndatasets, making most real-world (in-the-wild) scenes out-of-distribution. To\novercome this, we incorporate synthetic training data generated from diffusion\nmodels, which improves generalization across unseen domains. While synthetic\ndata offers scalability, we identify artifacts introduced during data\ngeneration as a key bottleneck affecting reconstruction quality. To address\nthis, we propose a token disentanglement process within the transformer\narchitecture, enhancing feature separation and ensuring more effective\nlearning. This refinement not only improves reconstruction quality over\nstandard transformers but also enables scalable training with synthetic data.\nAs a result, our method outperforms existing models on both in-dataset and\ncross-dataset evaluations, achieving state-of-the-art results across multiple\nbenchmarks while significantly reducing computational costs. Project page:\nhttps://scaling3dnvs.github.io/"}
{"id": "2509.05451", "pdf": "https://arxiv.org/pdf/2509.05451", "abs": "https://arxiv.org/abs/2509.05451", "authors": ["Niansong Zhang", "Wenbo Zhu", "Courtney Golden", "Dan Ilan", "Hongzheng Chen", "Christopher Batten", "Zhiru Zhang"], "title": "Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device", "categories": ["cs.AR", "cs.PF"], "comment": "MICRO 2025", "summary": "Compute-in-SRAM architectures offer a promising approach to achieving higher\nperformance and energy efficiency across a range of data-intensive\napplications. However, prior evaluations have largely relied on simulators or\nsmall prototypes, limiting the understanding of their real-world potential. In\nthis work, we present a comprehensive performance and energy characterization\nof a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.\nWe compare the GSI APU against established architectures, including CPUs and\nGPUs, to quantify its energy efficiency and performance potential. We introduce\nan analytical framework for general-purpose compute-in-SRAM devices that\nreveals fundamental optimization principles by modeling performance trade-offs,\nthereby guiding program optimizations.\n  Exploiting the fine-grained parallelism of tightly integrated memory-compute\narchitectures requires careful data management. We address this by proposing\nthree optimizations: communication-aware reduction mapping, coalesced DMA, and\nbroadcast-friendly data layouts. When applied to retrieval-augmented generation\n(RAG) over large corpora (10GB--200GB), these optimizations enable our\ncompute-in-SRAM system to accelerate retrieval by 4.8$\\times$--6.6$\\times$ over\nan optimized CPU baseline, improving end-to-end RAG latency by\n1.1$\\times$--1.8$\\times$. The shared off-chip memory bandwidth is modeled using\na simulated HBM, while all other components are measured on the real\ncompute-in-SRAM device. Critically, this system matches the performance of an\nNVIDIA A6000 GPU for RAG while being significantly more energy-efficient\n(54.4$\\times$-117.9$\\times$ reduction). These findings validate the viability\nof compute-in-SRAM for complex, real-world applications and provide guidance\nfor advancing the technology."}
{"id": "2509.05391", "pdf": "https://arxiv.org/pdf/2509.05391", "abs": "https://arxiv.org/abs/2509.05391", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Schüppstuhl"], "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "categories": ["cs.RO", "cs.HC", "cs.MM"], "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks."}
{"id": "2509.05936", "pdf": "https://arxiv.org/pdf/2509.05936", "abs": "https://arxiv.org/abs/2509.05936", "authors": ["Xuanhao Luo", "Shivesh Madan Nath Jha", "Akruti Sinha", "Zhizhen Li", "Yuchen Liu"], "title": "ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection", "categories": ["cs.NI", "cs.LG"], "comment": "Accepted at 44th IEEE International Performance Computing and\n  Communications Conference (IPCCC 2025)", "summary": "Network log data analysis plays a critical role in detecting security threats\nand operational anomalies. Traditional log analysis methods for anomaly\ndetection and root cause analysis rely heavily on expert knowledge or fully\nsupervised learning models, both of which require extensive labeled data and\nsignificant human effort. To address these challenges, we propose ALPHA, the\nfirst Active Learning Pipeline for Human-free log Analysis. ALPHA integrates\nsemantic embedding, clustering-based representative sampling, and large\nlanguage model (LLM)-assisted few-shot annotation to automate the anomaly\ndetection process. The LLM annotated labels are propagated across clusters,\nenabling large-scale training of an anomaly detector with minimal supervision.\nTo enhance the annotation accuracy, we propose a two-step few-shot refinement\nstrategy that adaptively selects informative prompts based on the LLM's\nobserved error patterns. Extensive experiments on real-world log datasets\ndemonstrate that ALPHA achieves detection accuracy comparable to fully\nsupervised methods while mitigating human efforts in the loop. ALPHA also\nsupports interpretable analysis through LLM-driven root cause explanations in\nthe post-detection stage. These capabilities make ALPHA a scalable and\ncost-efficient solution for truly automated log-based anomaly detection."}
{"id": "2509.06752", "pdf": "https://arxiv.org/pdf/2509.06752", "abs": "https://arxiv.org/abs/2509.06752", "authors": ["Amir M. Ben-Amram", "Samir Genaim", "Joël Ouaknine", "James Worrell"], "title": "Termination Analysis of Linear-Constraint Programs", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "This Survey provides an overview of techniques in termination analysis for\nprograms with numerical variables and transitions defined by linear\nconstraints. This subarea of program analysis is challenging due to the\nexistence of undecidable problems, and this Survey systematically explores\napproaches that mitigate this inherent difficulty. These include foundational\ndecidability results, the use of ranking functions, and disjunctive\nwell-founded transition invariants. The Survey also discusses non-termination\nwitnesses, used to prove that a program will not halt. We examine the\nalgorithmic and complexity aspects of these methods, showing how different\napproaches offer a trade-off between expressive power and computational\ncomplexity. The Survey does not discuss how termination analysis is performed\non real-world programming languages, nor does it consider more expressive\nabstract models that include non-linear arithmetic, probabilistic choice, or\nterm rewriting systems."}
{"id": "2509.05721", "pdf": "https://arxiv.org/pdf/2509.05721", "abs": "https://arxiv.org/abs/2509.05721", "authors": ["Péter Ferenc Gyarmati", "Dominik Moritz", "Torsten Möller", "Laura Koesten"], "title": "A Composable Agentic System for Automated Visual Data Reporting", "categories": ["cs.HC"], "comment": null, "summary": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/."}
{"id": "2509.05750", "pdf": "https://arxiv.org/pdf/2509.05750", "abs": "https://arxiv.org/abs/2509.05750", "authors": ["Ilias Azizi", "Karima Echihab", "Themis Palpanas", "Vassilis Christophides"], "title": "Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.PF"], "comment": "Presented at ICML 2025 VecDB Workshop; an extended version appeared\n  in ACM SIGMOD 2025 ('Graph-Based Vector Search: An Experimental Evaluation of\n  the State-of-the-Art')", "summary": "Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. Although several paradigms (seed selection, incremental insertion,\nneighborhood propagation, neighborhood diversification, and divide-and-conquer)\nhave been employed to design in-memory graph-based vector search algorithms, a\nsystematic comparison of the key algorithmic advances is still missing. We\nconduct an exhaustive experimental evaluation of twelve state-of-the-art\nmethods on seven real data collections, with sizes up to 1 billion vectors. We\nshare key insights about the strengths and limitations of these methods; e.g.,\nthe best approaches are typically based on incremental insertion and\nneighborhood diversification, and the choice of the base graph can hurt\nscalability. Finally, we discuss open research directions, such as the\nimportance of devising more sophisticated data adaptive seed selection and\ndiversification strategies."}
{"id": "2509.06365", "pdf": "https://arxiv.org/pdf/2509.06365", "abs": "https://arxiv.org/abs/2509.06365", "authors": ["Omar Al Habsi", "Safa Mohammed Sali", "Anis Meribout", "Mahmoud Meribout", "Saif Almazrouei", "Mohamed Seghier"], "title": "Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects", "categories": ["cs.AR"], "comment": null, "summary": "There is a growing interest in portable MRI (pMRI) systems for point-of-care\nimaging, particularly in remote or resource-constrained environments. However,\nthe computational complexity of pMRI, especially in image reconstruction and\nmachine learning (ML) algorithms for enhanced imaging, presents significant\nchallenges. Such challenges can be potentially addressed by harnessing hardware\napplication solutions, though there is little focus in the current pMRI\nliterature on hardware acceleration. This paper bridges that gap by reviewing\nrecent developments in pMRI, emphasizing the role and impact of hardware\nacceleration to speed up image acquisition and reconstruction. Key technologies\nsuch as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays\n(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent\nperformance in terms of reconstruction speed and power consumption. This review\nalso highlights the promise of AI-powered reconstruction, open low-field pMRI\ndatasets, and innovative edge-based hardware solutions for the future of pMRI\ntechnology. Overall, hardware acceleration can enhance image quality, reduce\npower consumption, and increase portability for next-generation pMRI\ntechnology. To accelerate reproducible AI for portable MRI, we propose forming\na Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,\nretrospective multi-center testing, prospective reader and non-inferiority\ntrials) to provide standardized datasets, benchmarks, and regulator-ready\ntestbeds."}
{"id": "2509.06245", "pdf": "https://arxiv.org/pdf/2509.06245", "abs": "https://arxiv.org/abs/2509.06245", "authors": ["Shyam Kumar Shrestha", "Jonathan Kua", "Shiva Raj Pokhrel"], "title": "Understanding BBRv3 Performance in AQM-Enabled WiFi Networks", "categories": ["cs.NI", "cs.ET"], "comment": "The 50th IEEE Conference on Local Computer Networks (LCN) October\n  14-16, 2025, Sydney, Australia", "summary": "We present a modular experimental testbed and lightweight visualization tool\nfor evaluating TCP congestion control performance in wireless networks. We\ncompare Google's latest Bottleneck Bandwidth and Round-trip time version 3\n(BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management\n(AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a\ncommercial MikroTik router. Our real-time dashboard visualizes metrics such as\nthroughput, latency, and fairness across competing flows. Results show that\nBBRv3 significantly improves fairness and convergence under AQM, especially\nwith FQ-CoDel. Our visualization tool and modular testbed provide a practical\nfoundation for evaluating next-generation TCP variants in real-world\nAQM-enabled home wireless networks."}
{"id": "2509.06229", "pdf": "https://arxiv.org/pdf/2509.06229", "abs": "https://arxiv.org/abs/2509.06229", "authors": ["Karolina Skrivankova", "Mark Handley", "Stephen Hailes"], "title": "20 Years in Life of a Smart Building: A retrospective", "categories": ["cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Operating an intelligent smart building automation system in 2025 is met with\nmany challenges: hardware failures, vendor obsolescence, evolving security\nthreats and more. None of these have been comprehensibly addressed by the\nindustrial building nor home automation industries, limiting feasibility of\noperating large, truly smart automation deployments. This paper introduces\nKaOS, a distributed control platform for constructing robust and evolvable\nsmart building automation systems using affordable, off-the-shelf IoT hardware.\nSupporting control applications and distributed system operations by leveraging\ncontainerisation and managed resource access, KaOS seeks to achieve\nflexibility, security, and fault tolerance without sacrificing\ncost-effectiveness. Initial evaluation confirms the practical feasibility of\nour approach, highlighting its potential to sustainably maintain and\nincrementally evolve building control functionalities over extended timeframes."}
{"id": "2509.05941", "pdf": "https://arxiv.org/pdf/2509.05941", "abs": "https://arxiv.org/abs/2509.05941", "authors": ["Chaoqian Ouyang", "Ling Yue", "Shimin Di", "Libin Zheng", "Shaowu Pan", "Min-Ling Zhang"], "title": "Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has created a significant\nintegration challenge in the AI agent ecosystem, often called the \"$N \\times M$\nproblem,\" where N models require custom integrations for M tools. This\nfragmentation stifles innovation and creates substantial development overhead.\nWhile the Model Context Protocol (MCP) has emerged as a standard to resolve\nthis, its adoption is hindered by the manual effort required to convert the\nvast universe of existing software into MCP-compliant services. This is\nespecially true for the millions of open-source repositories on GitHub, the\nworld's largest collection of functional code. This paper introduces Code2MCP,\na highly automated, agentic framework designed to transform any GitHub\nrepository into a functional MCP service with minimal human intervention. Our\nsystem employs a multi-stage workflow that automates the entire process, from\ncode analysis and environment configuration to service generation and\ndeployment. A key innovation of our framework is an LLM-driven, closed-loop\n\"Run--Review--Fix\" cycle, which enables the system to autonomously debug and\nrepair the code it generates. Code2MCP produces not only deployable services\nbut also comprehensive technical documentation, acting as a catalyst to\naccelerate the MCP ecosystem by systematically unlocking the world's largest\nopen-source code repository and automating the critical last mile of tool\nintegration. The code is open-sourced at\nhttps://github.com/DEFENSE-SEU/MCP-Github-Agent."}
{"id": "2509.06845", "pdf": "https://arxiv.org/pdf/2509.06845", "abs": "https://arxiv.org/abs/2509.06845", "authors": ["Tom Lauwaerts", "Maarten Steevens", "Christophe Scholliers"], "title": "MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices", "categories": ["cs.PL", "cs.SE"], "comment": "This extended version provides auxiliary material to the article of\n  the same title that will appear in the ACM Digital Library as part of the\n  PACMPL issue for OOPSLA 2025", "summary": "Debugging non-deterministic programs on microcontrollers is notoriously\nchallenging, especially when bugs manifest in unpredictable, input-dependent\nexecution paths. A recent approach, called multiverse debugging, makes it\neasier to debug non-deterministic programs by allowing programmers to explore\nall potential execution paths. Current multiverse debuggers enable both forward\nand backward traversal of program paths, and some facilitate jumping to any\npreviously visited states, potentially branching into alternative execution\npaths within the state space.\n  Unfortunately, debugging programs that involve input/output operations using\nexisting multiverse debuggers can reveal inaccessible program states, i.e.\nstates which are not encountered during regular execution. This can\nsignificantly hinder the debugging process, as the programmer may spend\nsubstantial time exploring and examining inaccessible program states, or worse,\nmay mistakenly assume a bug is present in the code, when in fact, the issue is\ncaused by the debugger.\n  This paper presents a novel approach to multiverse debugging, which can\naccommodate a broad spectrum of input/output operations. We provide the\nsemantics of our approach and prove the correctness of our debugger, ensuring\nthat despite having support for a wide range of input/output operations the\ndebugger will only explore those program states which can be reached during\nregular execution.\n  We have developed a prototype, called MIO, leveraging the WARDuino\nWebAssembly virtual machine to demonstrate the feasibility and efficiency of\nour techniques. As a demonstration of the approach we highlight a color dial\nbuilt with a Lego Mindstorms motor, and color sensor, providing a tangible\nexample of how our approach enables multiverse debugging for programs running\non an STM32 microcontroller."}
{"id": "2509.06167", "pdf": "https://arxiv.org/pdf/2509.06167", "abs": "https://arxiv.org/abs/2509.06167", "authors": ["Ximena Pocco", "Waqar Hassan", "Karelia Salinas", "Vladimir Molchanov", "Luis G. Nonato"], "title": "Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Urban analytics utilizes extensive datasets with diverse urban information to\nsimulate, predict trends, and uncover complex patterns within cities. While\nthese data enables advanced analysis, it also presents challenges due to its\ngranularity, heterogeneity, and multimodality. To address these challenges,\nvisual analytics tools have been developed to support the exploration of latent\nrepresentations of fused heterogeneous and multimodal data, discretized at a\nstreet-level of detail. However, visualization-assisted tools seldom explore\nthe extent to which fused data can offer deeper insights than examining each\ndata source independently within an integrated visualization framework. In this\nwork, we developed a visualization-assisted framework to analyze whether fused\nlatent data representations are more effective than separate representations in\nuncovering patterns from dynamic and static urban data. The analysis reveals\nthat combined latent representations produce more structured patterns, while\nseparate ones are useful in particular cases."}
{"id": "2509.05584", "pdf": "https://arxiv.org/pdf/2509.05584", "abs": "https://arxiv.org/abs/2509.05584", "authors": ["Sadegh Jafari", "Aishwarya Sarkar", "Mohiuddin Bilwal", "Ali Jannesari"], "title": "ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization", "categories": ["cs.LG", "cs.CV", "cs.PF"], "comment": "13 pages, 3 figures, 5 tables, 1 algorithm", "summary": "Foundation models face growing compute and memory bottlenecks, hindering\ndeployment on resource-limited platforms. While compression techniques such as\npruning and quantization are widely used, most rely on uniform heuristics that\nignore architectural and runtime heterogeneity. Profiling tools expose\nper-layer latency, memory, and compute cost, yet are rarely integrated into\nautomated pipelines. We propose ProfilingAgent, a profiling-guided, agentic\napproach that uses large language models (LLMs) to automate compression via\nstructured pruning and post-training dynamic quantization. Our modular\nmulti-agent system reasons over static metrics (MACs, parameter counts) and\ndynamic signals (latency, memory) to design architecture-specific strategies.\nUnlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to\nbottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with\nResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive\nor improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on\nsmaller datasets), while quantization achieves up to 74% memory savings with\n<0.5% accuracy loss. Our quantization also yields consistent inference speedups\nof up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo\nhighlight the importance of LLM reasoning quality for iterative pruning. These\nresults establish agentic systems as scalable solutions for profiling-guided\nmodel optimization."}
{"id": "2509.05971", "pdf": "https://arxiv.org/pdf/2509.05971", "abs": "https://arxiv.org/abs/2509.05971", "authors": ["Kaiyi Chi", "Yinghui He", "Qianqian Yang", "Zhiping Jiang", "Yuanchao Shu", "Zhiqin Wang", "Jun Luo", "Jiming Chen"], "title": "DeepStream: Prototyping Deep Joint Source-Channel Coding for Real-Time Multimedia Transmissions", "categories": ["eess.SP", "cs.MM"], "comment": "13 pages, 43 figures", "summary": "Deep learning-based joint source-channel coding (DeepJSCC) has emerged as a\npromising technique in 6G for enhancing the efficiency and reliability of data\ntransmission across diverse modalities, particularly in low signal-to-noise\nratio (SNR) environments. This advantage is realized by leveraging powerful\nneural networks to learn an optimal end-to-end mapping from the source data\ndirectly to the transmit symbol sequence, eliminating the need for separate\nsource coding, channel coding, and modulation. Although numerous efforts have\nbeen made towards efficient DeepJSCC, they have largely stayed at numerical\nsimulations that can be far from practice, leaving the real-world viability of\nDeepJSCC largely unverified. To this end, we prototype DeepStream upon\northogonal frequency division multiplexing (OFDM) technology to offer efficient\nand robust DeepJSCC for multimedia transmission. In conforming to OFDM, we\ndevelop both a feature-to-symbol mapping method and a cross-subcarrier\nprecoding method to improve the subcarrier independence and reduce\npeak-to-average power ratio. To reduce system complexity and enable flexibility\nin accommodating varying quality of service requirements, we further propose a\nprogressive coding strategy that adjusts the compression ratio based on latency\nwith minimal performance loss. We implement DeepStream for real-time image\ntransmission and video streaming using software-defined radio. Extensive\nevaluations verify that DeepStream outperforms both the standard scheme and the\ndirect deployment scheme. Particularly, at an SNR of 10 dB, DeepStream achieves\na PSNR of 35 dB for image transmission and an MS-SSIM of 20 dB for video\nstreaming, whereas the standard scheme fails to recover meaningful information."}
{"id": "2509.05938", "pdf": "https://arxiv.org/pdf/2509.05938", "abs": "https://arxiv.org/abs/2509.05938", "authors": ["Alissa Baumeister", "Sina Keshvadi"], "title": "An Axiomatic Analysis of Path Selection Strategies for Multipath Transport in Path-Aware Networks", "categories": ["cs.NI", "C.2; C.2.1; C.2.2"], "comment": "13 pages, submitted in the journal of Complex Networks", "summary": "Path-aware networking architectures like SCION provide end-hosts with\nexplicit control over inter-domain routing, while multipath transport protocols\nlike MPTCP and MPQUIC enable the concurrent use of multiple paths. This\ncombination promises significant gains in performance and policy enforcement,\nbut it also creates a stark trade-off between individual performance\noptimization and overall network stability. This paper quantifies this\ntrade-off through a rigorous axiomatic analysis. We evaluate a spectrum of\nalgorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid\napproaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance,\nStability, and Fairness in a simulated path-aware environment.\n  Our simulations reveal that purely greedy strategies, while efficient under\nlow contention, induce catastrophic packet loss, increasing by over >18,000% as\nthe number of competing agents grow, due to herd effects that cause severe\nnetwork instability. Conversely, cooperative strategies ensure fairness and\nstability but at the cost of underutilizing high-capacity paths. Crucially, we\ndemonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy\nalgorithm, for instance, achieves the highest efficiency of all tested\nstrategies in high-contention scenarios while mitigating the instability\ninherent to the greedy approach. Our axiomatic analysis suggests that tunable,\nhybrid algorithms are essential for designing robust and high-performance path\nselection mechanisms for next-generation networks."}
{"id": "2509.05829", "pdf": "https://arxiv.org/pdf/2509.05829", "abs": "https://arxiv.org/abs/2509.05829", "authors": ["Jiyoon Pyo", "Yuankun Jiao", "Yao-Yi Chiang", "Michael Corey"], "title": "Augmenting Human-Centered Racial Covenant Detection and Georeferencing with Plug-and-Play NLP Pipelines", "categories": ["cs.HC"], "comment": null, "summary": "Though no longer legally enforceable, racial covenants in twentieth-century\nproperty deeds continue to shape spatial and socioeconomic inequalities.\nUnderstanding this legacy requires identifying racially restrictive language\nand geolocating affected properties. The Mapping Prejudice project addresses\nthis by engaging volunteers on the Zooniverse crowdsourcing platform to\ntranscribe covenants from scanned deeds and link them to modern parcel maps\nusing transcribed legal descriptions. While the project has explored\nautomation, it values crowdsourcing for its social impact and technical\nadvantages. Historically, Mapping Prejudice relied on lexicon-based searching\nand, more recently, fuzzy matching to flag suspected covenants. However, fuzzy\nmatching has increased false positives, burdening volunteers and raising\nscalability concerns. Additionally, while many properties can be mapped\nautomatically, others still require time-intensive manual geolocation.\n  We present a human-centered computing approach with two plug-and-play NLP\npipelines: (1) a context-aware text labeling model that flags racially\nrestrictive language with high precision and (2) a georeferencing module that\nextracts geographic descriptions from deeds and resolves them to real-world\nlocations. Evaluated on historical deed documents from six counties in\nMinnesota and Wisconsin, our system reduces false positives in racial term\ndetection by 25.96% while maintaining 91.73% recall and achieves 85.58%\ngeoreferencing accuracy within 1x1 square-mile ranges. These tools enhance\ndocument filtering and enrich spatial annotations, accelerating volunteer\nparticipation and reducing manual cleanup while strengthening public\nengagement."}
{"id": "2509.05759", "pdf": "https://arxiv.org/pdf/2509.05759", "abs": "https://arxiv.org/abs/2509.05759", "authors": ["Jinkun Geng", "Shuai Mu", "Anirudh Sivaraman", "Balaji Prabhakar"], "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]", "categories": ["cs.NI", "cs.DB", "cs.DC", "68M10, 68M15"], "comment": "This is the technical report for our paper accepted by The 31st\n  Symposium on Operating Systems Principles (SOSP'25)", "summary": "This paper presents Tiga, a new design for geo-replicated and scalable\ntransactional databases such as Google Spanner. Tiga aims to commit\ntransactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of\nscenarios, while maintaining high throughput with minimal computational\noverhead. Tiga consolidates concurrency control and consensus, completing both\nstrictly serializable execution and consistent replication in a single round.\nIt uses synchronized clocks to proactively order transactions by assigning each\na future timestamp at submission. In most cases, transactions arrive at servers\nbefore their future timestamps and are serialized according to the designated\ntimestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed\nand proactive ordering fails, in which case Tiga falls back to a slow path,\ncommitting in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can\ncommit more transactions at 1-WRTT latency, and incurs much less throughput\noverhead. Evaluation results show that Tiga outperforms all baselines,\nachieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower\nlatency. Tiga is open-sourced at\nhttps://github.com/New-Consensus-Concurrency-Control/Tiga."}
{"id": "2509.06698", "pdf": "https://arxiv.org/pdf/2509.06698", "abs": "https://arxiv.org/abs/2509.06698", "authors": ["Leidy Mabel Alvero-Gonzalez", "Matias Miguez", "Eric Gutierrez", "Juan Sapriza", "Susana Patón", "David Atienza", "José Miranda"], "title": "VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing", "categories": ["cs.AR"], "comment": null, "summary": "Continuous monitoring of electrodermal activity (EDA) through wearable\ndevices has attracted much attention in recent times. However, the persistent\nchallenge demands analog front-end (AFE) systems with high sensitivity, low\npower consumption, and minimal calibration requirements to ensure practical\nusability in wearable technologies. In response to this challenge, this\nresearch introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog\nReadout tailored for continuous EDA sensing. The results show that our system\nachieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS\nrange and a negligible relative error of less than 0.0025% for\nfixed-resistance. Furthermore, the proposed system consumes only an average of\n2.3 uW based on post-layout validations and introduces a low noise\ncontribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.\nThis research aims to drive the evolution of wearable sensors characterized by\nseamless adaptability to diverse users, minimal power consumption, and\noutstanding noise resilience."}
{"id": "2509.06289", "pdf": "https://arxiv.org/pdf/2509.06289", "abs": "https://arxiv.org/abs/2509.06289", "authors": ["Shaoqi Wei", "Senling Wang", "Hiroshi Kai", "Yoshinobu Higami", "Ruijun Ma", "Tianming Ni", "Xiaoqing Wen", "Hiroshi Takahashi"], "title": "A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults", "categories": ["cs.LG", "cs.AR", "cs.ET", "B.7.3"], "comment": "21 pages, 9 figures, plan to submit to ACM TODAES", "summary": "Silent Data Errors (SDEs) from time-zero defects and aging degrade\nsafety-critical systems. Functional testing detects SDE-related faults but is\nexpensive to simulate. We present a unified spatio-temporal graph convolutional\nnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impact\nprobabilities (FIPs) in large sequential circuits, supporting quantitative risk\nassessment. Gate-level netlists are modeled as spatio-temporal graphs to\ncapture topology and signal timing; dedicated spatial and temporal encoders\npredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method\nreduces simulation time by more than 10x while maintaining high accuracy (mean\nabsolute error 0.024 for 5-cycle predictions). The framework accepts features\nfrom testability metrics or fault simulation, allowing efficiency-accuracy\ntrade-offs. A test-point selection study shows that choosing observation points\nby predicted FIPs improves detection of long-cycle, hard-to-detect faults. The\napproach scales to SoC-level test strategy optimization and fits downstream\nelectronic design automation flows."}
{"id": "2509.06261", "pdf": "https://arxiv.org/pdf/2509.06261", "abs": "https://arxiv.org/abs/2509.06261", "authors": ["Kyungmin Bin", "Seungbeom Choi", "Jimyoung Son", "Jieun Choi", "Daseul Bae", "Daehyeon Baek", "Kihyo Moon", "Minsung Jang", "Hyojung Lee"], "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."}
{"id": "2509.05980", "pdf": "https://arxiv.org/pdf/2509.05980", "abs": "https://arxiv.org/abs/2509.05980", "authors": ["Xingliang Wang", "Baoyi Wang", "Chen Zhi", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion", "categories": ["cs.SE"], "comment": null, "summary": "LLMs excel in localized code completion but struggle with repository-level\ntasks due to limited context windows and complex semantic and structural\ndependencies across codebases. While Retrieval-Augmented Generation (RAG)\nmitigates context scarcity by retrieving relevant code snippets, current\napproaches face significant limitations. They overly rely on textual similarity\nfor retrieval, neglecting structural relationships such as call chains and\ninheritance hierarchies, and lose critical structural information by naively\nconcatenating retrieved snippets into text sequences for LLM input. To address\nthese shortcomings, GRACE constructs a multi-level, multi-semantic code graph\nthat unifies file structures, abstract syntax trees, function call graphs,\nclass hierarchies, and data flow graphs to capture both static and dynamic code\nsemantics. For retrieval, GRACE employs a Hybrid Graph Retriever that\nintegrates graph neural network-based structural similarity with textual\nretrieval, refined by a graph attention network-based re-ranker to prioritize\ntopologically relevant subgraphs. To enhance context, GRACE introduces a\nstructural fusion mechanism that merges retrieved subgraphs with the local code\ncontext and preserves essential dependencies like function calls and\ninheritance. Extensive experiments on public repository-level benchmarks\ndemonstrate that GRACE significantly outperforms state-of-the-art methods\nacross all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the\nstrongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every\ndataset. The code is available at\nhttps://anonymous.4open.science/r/grace_icse-C3D5."}
{"id": "2509.06872", "pdf": "https://arxiv.org/pdf/2509.06872", "abs": "https://arxiv.org/abs/2509.06872", "authors": ["Zachary Kent", "Ugur Y. Yavuz", "Siddhartha Jayanti", "Stephanie Balzer", "Guy Blelloch"], "title": "Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs", "categories": ["cs.PL"], "comment": null, "summary": "In the past decade, many techniques have been developed to prove\nlinearizability, the gold standard of correctness for concurrent data\nstructures. Intuitively, linearizability requires that every operation on a\nconcurrent data structure appears to take place instantaneously, even when\ninterleaved with other operations. Most recently, Jayanti et al. presented the\nfirst sound and complete \"forward reasoning\" technique for proving\nlinearizability that relates the behavior of a concurrent data structure to a\nreference atomic data structure as time moves forward. This technique can be\nused to produce machine-checked proofs of linearizability in TLA+. However,\nwhile Jayanti et al.'s approach is shown to be sound and complete, a\nmechanization of this important metatheoretic result is still outstanding. As a\nresult, it is not possible to produce verified end-to-end proofs of\nlinearizability. To reduce the size of this trusted computing base, we\nformalize this forward reasoning technique and mechanize proofs of its\nsoundness and completeness in Rocq. As a case study, we use the approach to\nproduce a verified end-to-end proof of linearizability for a simple concurrent\nregister."}
{"id": "2509.05750", "pdf": "https://arxiv.org/pdf/2509.05750", "abs": "https://arxiv.org/abs/2509.05750", "authors": ["Ilias Azizi", "Karima Echihab", "Themis Palpanas", "Vassilis Christophides"], "title": "Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.PF"], "comment": "Presented at ICML 2025 VecDB Workshop; an extended version appeared\n  in ACM SIGMOD 2025 ('Graph-Based Vector Search: An Experimental Evaluation of\n  the State-of-the-Art')", "summary": "Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. Although several paradigms (seed selection, incremental insertion,\nneighborhood propagation, neighborhood diversification, and divide-and-conquer)\nhave been employed to design in-memory graph-based vector search algorithms, a\nsystematic comparison of the key algorithmic advances is still missing. We\nconduct an exhaustive experimental evaluation of twelve state-of-the-art\nmethods on seven real data collections, with sizes up to 1 billion vectors. We\nshare key insights about the strengths and limitations of these methods; e.g.,\nthe best approaches are typically based on incremental insertion and\nneighborhood diversification, and the choice of the base graph can hurt\nscalability. Finally, we discuss open research directions, such as the\nimportance of devising more sophisticated data adaptive seed selection and\ndiversification strategies."}
{"id": "2509.06219", "pdf": "https://arxiv.org/pdf/2509.06219", "abs": "https://arxiv.org/abs/2509.06219", "authors": ["Haochen You", "Baojing Liu"], "title": "MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning", "categories": ["cs.LG", "cs.MM"], "comment": "Accepted as a conference paper at KSEM 2025", "summary": "Exemplar-free class-incremental learning enables models to learn new classes\nover time without storing data from old ones. As multimodal graph-structured\ndata becomes increasingly prevalent, existing methods struggle with challenges\nlike catastrophic forgetting, distribution bias, memory limits, and weak\ngeneralization. We propose MCIGLE, a novel framework that addresses these\nissues by extracting and aligning multimodal graph features and applying\nConcatenated Recursive Least Squares for effective knowledge retention. Through\nmulti-channel processing, MCIGLE balances accuracy and memory preservation.\nExperiments on public datasets validate its effectiveness and generalizability."}
{"id": "2509.05946", "pdf": "https://arxiv.org/pdf/2509.05946", "abs": "https://arxiv.org/abs/2509.05946", "authors": ["Bisheng Wei", "Ruihong Jiang", "Ruichen Zhang", "Yinqiu Liu", "Dusit Niyato", "Yaohua Sun", "Yang Lu", "Yonghui Li", "Shiwen Mao", "Chau Yuen", "Marco Di Renzo", "Mugen Peng"], "title": "Large Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial", "categories": ["cs.NI"], "comment": null, "summary": "The rapid advancement toward sixth-generation (6G) wireless networks has\nsignificantly intensified the complexity and scale of optimization problems,\nincluding resource allocation and trajectory design, often formulated as\ncombinatorial problems in large discrete decision spaces. However, traditional\noptimization methods, such as heuristics and deep reinforcement learning (DRL),\nstruggle to meet the demanding requirements of real-time adaptability,\nscalability, and dynamic handling of user intents in increasingly heterogeneous\nand resource-constrained network environments. Large language models (LLMs)\npresent a transformative paradigm by enabling natural language-driven problem\nformulation, context-aware reasoning, and adaptive solution refinement through\nadvanced semantic understanding and structured reasoning capabilities. This\npaper provides a systematic and comprehensive survey of LLM-enabled\noptimization frameworks tailored for wireless networks. We first introduce\nfoundational design concepts and distinguish LLM-enabled methods from\nconventional optimization paradigms. Subsequently, we critically analyze key\nenabling methodologies, including natural language modeling, solver\ncollaboration, and solution verification processes. Moreover, we explore\nrepresentative case studies to demonstrate LLMs' transformative potential in\npractical scenarios such as optimization formulation, low-altitude economy\nnetworking, and intent networking. Finally, we discuss current research\nchallenges, examine prominent open-source frameworks and datasets, and identify\npromising future directions to facilitate robust, scalable, and trustworthy\nLLM-enabled optimization solutions for next-generation wireless networks."}
{"id": "2509.05898", "pdf": "https://arxiv.org/pdf/2509.05898", "abs": "https://arxiv.org/abs/2509.05898", "authors": ["Omar Elgohary", "Zhu-Tien"], "title": "Attention, Action, and Memory: How Multi-modal Interfaces and Cognitive Load Alter Information Retention", "categories": ["cs.HC"], "comment": null, "summary": "Each year, multi-modal interaction continues to grow within both industry and\nacademia. However, researchers have yet to fully explore the impact of\nmulti-modal systems on learning and memory retention. This research\ninvestigates how combining gaze-based controls with gesture navigation affects\ninformation retention when compared to standard track-pad usage. A total of\ntwelve participants read four textual articles through two different user\ninterfaces which included a track-pad and a multi-modal interface that tracked\neye movements and hand gestures for scrolling, zooming, and revealing content.\nParticipants underwent two assessment sessions that measured their information\nretention immediately and after a twenty-four hour period along with the\nNASA-TLX workload evaluation and the System Usability Scale assessment. The\ninitial analysis indicates that multi-modal interaction produces similar\ntargeted information retention to traditional track-pad usage, but this neutral\neffect comes with higher cognitive workload demands and seems to deteriorate\nwith long-term retention. The research results provide new knowledge about how\nmulti-modal systems affect cognitive engagement while providing design\nrecommendations for future educational and assistive technologies that require\neffective memory performance."}
{"id": "2509.05891", "pdf": "https://arxiv.org/pdf/2509.05891", "abs": "https://arxiv.org/abs/2509.05891", "authors": ["Mahfuzul I. Nissan"], "title": "MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm", "categories": ["cs.CR", "cs.DB"], "comment": null, "summary": "Database audit and transaction logs are fundamental to forensic\ninvestigations, but they are vulnerable to tampering by privileged attackers.\nMalicious insiders or external threats with administrative access can alter,\npurge, or temporarily disable logging mechanisms, creating significant blind\nspots and rendering disk-based records unreliable. Memory analysis offers a\nvital alternative, providing investigators direct access to volatile artifacts\nthat represent a ground-truth source of recent user activity, even when log\nfiles have been compromised.\n  This paper introduces MemTraceDB, a tool that reconstructs user activity\ntimelines by analyzing raw memory snapshots from the MySQL database process.\nMemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically\nextract and correlate forensic artifacts such as user connections and executed\nqueries. Through a series of experiments, I demonstrate MemTraceDB's\neffectiveness and reveal a critical empirical finding: the MySQL query stack\nhas a finite operational capacity of approximately 9,997 queries. This\ndiscovery allows me to establish a practical, data-driven formula for\ndetermining the optimal frequency for memory snapshot collection, providing a\nclear, actionable guideline for investigators. The result is a\nforensically-sound reconstruction of user activity, independent of compromised\ndisk-based logs."}
{"id": "2509.05504", "pdf": "https://arxiv.org/pdf/2509.05504", "abs": "https://arxiv.org/abs/2509.05504", "authors": ["Karl Aaron Rudkowski", "Sallar Ahmadi-Pour", "Rolf Drechsler"], "title": "Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution", "categories": ["cs.PL", "cs.AR"], "comment": null, "summary": "Virtual Prototypes (VPs) are important tools in modern hardware development.\nAt high abstractions, they are often implemented in SystemC and offer early\nanalysis of increasingly complex designs. These complex designs often combine\none or more processors, interconnects, and peripherals to perform tasks in\nhardware or interact with the environment. Verifying these subsystems is a\nwell-suited task for VPs, as they allow reasoning across different abstraction\nlevels. While modern verification techniques like symbolic execution can be\nseamlessly integrated into VP-based workflows, they require modifications in\nthe SystemC kernel. Hence, existing approaches therefore modify and replace the\nSystemC kernel, or ignore the opportunity of cross-level scenarios completely,\nand would not allow focusing on special challenges of particular subsystems\nlike peripherals. We propose CrosSym and SEFOS, two opposing approaches for a\nversatile symbolic execution of peripherals. CrosSym modifies the SystemC\nkernel, while SEFOS instead modifies a modern symbolic execution engine. Our\nextensive evaluation applies our tools to various peripherals on different\nlevels of abstractions. Both tools extensive sets of features are demonstrated\nfor (1) different verification scenarios, and (2) identifying 300+ mutants. In\ncomparison with each other, SEFOS convinces with the unmodified SystemC kernel\nand peripheral, while CrosSym offers slightly better runtime and memory usage.\nIn comparison to the state-of-the-art, that is limited to Transaction Level\nModelling (TLM), our tools offered comparable runtime, while enabling\ncross-level verification with symbolic execution."}
{"id": "2509.06362", "pdf": "https://arxiv.org/pdf/2509.06362", "abs": "https://arxiv.org/abs/2509.06362", "authors": ["Mo Xuan", "Zhang yue", "Wu Weigang"], "title": "MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS", "categories": ["cs.DC"], "comment": null, "summary": "Model-as-a-Service (MaaS) platforms face diverse Service Level Objective\n(SLO) requirements stemming from various large language model (LLM)\napplications, manifested in contextual complexity, first-token latency, and\nbetween-token latency. On the other hand, an LLM instance, when configured with\ndifferent parallelism strategies and inference batch sizes, exhibits distinct\nperformance characteristics and can thus be used to serve different SLO\nrequirements. However, current LLM inference systems typically deploy instances\nof the same model with identical configurations, lacking mechanisms to leverage\nsuch heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS\nOrchestrator, which comprises three modules: (1) a profiler characterizing\ninstance performance under diverse parallelism strategies and inference batch\nsizes; (2) a placer optimizing heterogeneous instance configurations; (3) a\ndistributor enabling SLO-aware request distribution and preventing cascaded\ntimeouts in continuous batching. Experiments show that MaaSO improves the SLO\nsatisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%\ncompared to existing approaches, and significantly lowers overall orchestration\noverhead."}
{"id": "2509.05995", "pdf": "https://arxiv.org/pdf/2509.05995", "abs": "https://arxiv.org/abs/2509.05995", "authors": ["Sharon Guardado", "Risha Parveen", "Zheying Zhang", "Maruf Rayhan", "Nirnaya Tripathi"], "title": "Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities", "categories": ["cs.SE"], "comment": "Accepted by the 33rd IEEE International Requirements Engineering 2025\n  (RE'25), Valencia, Spain, September 1-5, 2025", "summary": "The integration of Large Language Models (LLMs) in Requirements Engineering\n(RE) education is reshaping pedagogical approaches, seeking to enhance student\nengagement and motivation while providing practical tools to support their\nprofessional future. This study empirically evaluates the impact of integrating\nLLMs in RE coursework. We examined how the guided use of LLMs influenced\nstudents' learning experiences, and what benefits and challenges they perceived\nin using LLMs in RE practices. The study collected survey data from 179\nstudents across two RE courses in two universities. LLMs were integrated into\ncoursework through different instructional formats, i.e., individual\nassignments versus a team-based Agile project. Our findings indicate that LLMs\nimproved students' comprehension of RE concepts, particularly in tasks like\nrequirements elicitation and documentation. However, students raised concerns\nabout LLMs in education, including academic integrity, overreliance on AI, and\nchallenges in integrating AI-generated content into assignments. Students who\nworked on individual assignments perceived that they benefited more than those\nwho worked on team-based assignments, highlighting the importance of contextual\nAI integration. This study offers recommendations for the effective integration\nof LLMs in RE education. It proposes future research directions for balancing\nAI-assisted learning with critical thinking and collaborative practices in RE\ncourses."}
{"id": "2509.05462", "pdf": "https://arxiv.org/pdf/2509.05462", "abs": "https://arxiv.org/abs/2509.05462", "authors": ["Grigory Kondyrev", "David I. Spivak"], "title": "The compact double category $\\mathbf{Int}(\\mathbf{Poly}_*)$ models control flow and data transformations", "categories": ["math.CT", "cs.PL", "18M30, 18M10, 18M35, 18B20, 18M60", "D.1.7; D.3.3"], "comment": "28 pages including many diagrams", "summary": "Hasegawa showed that control flow in programming languages -- while loops and\nif-then-else statements -- can be modeled using traced cocartesian categories,\nsuch as the category $\\mathbf{Set}_*$ of pointed sets. In this paper we define\nan operad $\\mathscr{W}$ of wiring diagrams that provides syntax for categories\nwhose control flow moreover includes data transformations, including deleting,\nduplicating, permuting, and applying pre-specified functions to variables. In\nthe most basic version, the operad underlies $\\mathbf{Int}(\\mathbf{Poly}_*)$,\nwhere $\\mathbf{Int}(\\mathscr{T})$ denotes the free compact category on a traced\ncategory $\\mathscr{T}$, as defined by Joyal, Street, and Verity; to do so, we\nshow that $\\mathbf{Poly}_*$, as well as any multivariate version of it, is\ntraced. We show moreover that whenever $\\mathscr{T}$ is uniform -- a condition\nalso defined by Hasegawa and satisfied by $\\mathbf{Int}(\\mathscr{T})$ -- the\nresulting $\\mathbf{Int}$-construction extends to a double category\n$\\mathbb{I}\\mathbf{nt}(\\mathscr{T})$, which is compact in the sense of\nPatterson. Finally, we define a universal property of the double category\n$\\mathbb{I}\\mathbf{nt}(\\mathbf{Poly}_*)$ and\n$\\mathbb{I}\\mathbf{nt}(\\mathbf{Set}_*)$ by which one can track trajectories as\nthey move through the control flow associated to a wiring diagram."}
{"id": "2509.06716", "pdf": "https://arxiv.org/pdf/2509.06716", "abs": "https://arxiv.org/abs/2509.06716", "authors": ["Théo Matricon", "Mathieu Acher", "Helge Spieker", "Arnaud Gotlieb"], "title": "Efficiently Ranking Software Variants with Minimal Benchmarks", "categories": ["cs.SE", "cs.PF"], "comment": null, "summary": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability."}
{"id": "2509.06554", "pdf": "https://arxiv.org/pdf/2509.06554", "abs": "https://arxiv.org/abs/2509.06554", "authors": ["Dietmar Saupe", "Tim Bleile"], "title": "Robustness and accuracy of mean opinion scores with hard and soft outlier detection", "categories": ["eess.IV", "cs.LG", "cs.MM"], "comment": "Accepted for 17th International Conference on Quality of Multimedia\n  Experience (QoMEX'25), September 2025, Madrid, Spain", "summary": "In subjective assessment of image and video quality, observers rate or\ncompare selected stimuli. Before calculating the mean opinion scores (MOS) for\nthese stimuli from the ratings, it is recommended to identify and deal with\noutliers that may have given unreliable ratings. Several methods are available\nfor this purpose, some of which have been standardized. These methods are\ntypically based on statistics and sometimes tested by introducing synthetic\nratings from artificial outliers, such as random clickers. However, a reliable\nand comprehensive approach is lacking for comparative performance analysis of\noutlier detection methods. To fill this gap, this work proposes and applies an\nempirical worst-case analysis as a general solution. Our method involves\nevolutionary optimization of an adversarial black-box attack on outlier\ndetection algorithms, where the adversary maximizes the distortion of scale\nvalues with respect to ground truth. We apply our analysis to several hard and\nsoft outlier detection methods for absolute category ratings and show their\ndiffering performance in this stress test. In addition, we propose two new\noutlier detection methods with low complexity and excellent worst-case\nperformance. Software for adversarial attacks and data analysis is available."}
{"id": "2509.06049", "pdf": "https://arxiv.org/pdf/2509.06049", "abs": "https://arxiv.org/abs/2509.06049", "authors": ["Andrea Tassi", "Oluwatayo Yetunde Kolawole", "Joan Pujol Roig", "Daniel Warren"], "title": "Optimized Split Computing Framework for Edge and Core Devices", "categories": ["cs.NI"], "comment": "To appear on IEEE Transactions on Vehicular Technology", "summary": "With mobile networks expected to support services with stringent requirements\nthat ensure high-quality user experience, the ability to apply Feed-Forward\nNeural Network (FFNN) models to User Equipment (UE) use cases has become\ncritical. Given that UEs have limited resources, running FFNNs directly on UEs\nis an intrinsically challenging problem. This letter proposes an optimization\nframework for split computing applications where an FFNN model is partitioned\ninto multiple sections, and executed by UEs, edge- and core-located nodes to\nreduce the required UE computational footprint while containing the inference\ntime. An efficient heuristic strategy for solving the optimization problem is\nalso provided. The proposed framework is shown to be robust in heterogeneous\nsettings, eliminating the need for retraining and reducing the UE's memory\n(CPU) footprint by over 33.6% (60%)."}
{"id": "2509.05943", "pdf": "https://arxiv.org/pdf/2509.05943", "abs": "https://arxiv.org/abs/2509.05943", "authors": ["Yi Wang", "Haodong Zhang", "Hongqi Li"], "title": "DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification", "categories": ["cs.HC"], "comment": "Submit to IEEE Journal", "summary": "Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant\npotential for assistive technologies and neurorehabilitation. However, the\nprecise and efficient decoding of MI remains challenging due to their\nnon-stationary nature and low signal-to-noise ratio. This paper introduces a\nnovel end-to-end deep learning framework of Discriminative Residual Dense\nConvolutional Autoencoder with Spatio-Temporal Graph Neural Network\n(DRDCAE-STGNN) to enhance the MI feature learning and classification.\nSpecifically, the DRDCAE module leverages residual-dense connections to learn\ndiscriminative latent representations through joint reconstruction and\nclassifica-tion, while the STGNN module captures dynamic spatial dependencies\nvia a learnable graph adjacency matrix and models temporal dynamics using\nbidirectional long short-term memory (LSTM). Extensive evaluations on BCI\nCompetition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art\nperformance, with average accuracies of 95.42%, 97.51%, and 90.15%,\nrespectively. Ablation studies confirm the contribution of each component, and\ninterpreta-bility analysis reveals neurophysiologically meaningful connectivity\npatterns. Moreover, despite its complexity, the model maintains a feasible\nparameter count and an inference time of 0.32 ms per sample. These results\nindicate that our method offers a robust, accurate, and interpretable solution\nfor MI-EEG decoding, with strong generalizability across subjects and tasks and\nmeeting the requirements for potential real-time BCI applications."}
{"id": "2509.05899", "pdf": "https://arxiv.org/pdf/2509.05899", "abs": "https://arxiv.org/abs/2509.05899", "authors": ["Dazhi Peng"], "title": "X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "With Large Language Models' (LLMs) emergent abilities on code generation\ntasks, Text-to-SQL has become one of the most popular downstream applications.\nDespite the strong results of multiple recent LLM-based Text-to-SQL frameworks,\nthe research community often overlooks the importance of database schema\ninformation for generating high-quality SQL queries. We find that such schema\ninformation plays a significant or even dominant role in the Text-to-SQL task.\nTo tackle this challenge, we propose a novel database schema expert with two\ncomponents. We first introduce X-Linking, an LLM Supervised Finetuning\n(SFT)-based method that achieves superior Schema Linking results compared to\nexisting open-source Text-to-SQL methods. In addition, we innovatively propose\nan X-Admin component that focuses on Schema Understanding by bridging the gap\nbetween abstract schema information and the user's natural language question.\nAside from better learning with schema information, we experiment with\nMulti-LLMs for different components within the system to further boost its\nperformance. By incorporating these techniques into our end-to-end framework,\nX-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset\nand 82.5% on the Spider-Test dataset. This outstanding performance establishes\nX-SQL as the leading Text-to-SQL framework based on open-source models."}
{"id": "2509.06162", "pdf": "https://arxiv.org/pdf/2509.06162", "abs": "https://arxiv.org/abs/2509.06162", "authors": ["M. Rezaalipour", "F. Costa", "M. Biasion", "R. Otoni", "G. A. Constantinides", "L. Pozzi"], "title": "An Improved Template for Approximate Computing", "categories": ["cs.LG", "cs.AR", "B.2.1; B.6.3"], "comment": "4 pages, 5 figures", "summary": "Deploying neural networks on edge devices entails a careful balance between\nthe energy required for inference and the accuracy of the resulting\nclassification. One technique for navigating this tradeoff is approximate\ncomputing: the process of reducing energy consumption by slightly reducing the\naccuracy of arithmetic operators. In this context, we propose a methodology to\nreduce the area of the small arithmetic operators used in neural networks -\ni.e., adders and multipliers - via a small loss in accuracy, and show that we\nimprove area savings for the same accuracy loss w.r.t. the state of the art. To\nachieve our goal, we improve on a boolean rewriting technique recently\nproposed, called XPAT, where the use of a parametrisable template to rewrite\ncircuits has proved to be highly beneficial. In particular, XPAT was able to\nproduce smaller circuits than comparable approaches while utilising a naive sum\nof products template structure. In this work, we show that template parameters\ncan act as proxies for chosen metrics and we propose a novel template based on\nparametrisable product sharing that acts as a close proxy to synthesised area.\nWe demonstrate experimentally that our methodology converges better to low-area\nsolutions and that it can find better approximations than both the original\nXPAT and two other state-of-the-art approaches."}
{"id": "2509.06514", "pdf": "https://arxiv.org/pdf/2509.06514", "abs": "https://arxiv.org/abs/2509.06514", "authors": ["Mpoki Mwaisela", "Peterson Yuhala", "Pascal Felber", "Valerio Schiavoni"], "title": "IM-PIR: In-Memory Private Information Retrieval", "categories": ["cs.DC"], "comment": null, "summary": "Private information retrieval (PIR) is a cryptographic primitive that allows\na client to securely query one or multiple servers without revealing their\nspecific interests. In spite of their strong security guarantees, current PIR\nconstructions are computationally costly. Specifically, most PIR\nimplementations are memory-bound due to the need to scan extensive databases\n(in the order of GB), making them inherently constrained by the limited memory\nbandwidth in traditional processor-centric computing\narchitectures.Processing-in-memory (PIM) is an emerging computing paradigm that\naugments memory with compute capabilities, addressing the memory bandwidth\nbottleneck while simultaneously providing extensive parallelism.Recent research\nhas demonstrated PIM's potential to significantly improve performance across a\nrange of data-intensive workloads, including graph processing, genome analysis,\nand machine learning.\n  In this work, we propose the first PIM-based architecture for multi-server\nPIR. We discuss the algorithmic foundations of the latter and show how its\noperations align with the core strengths of PIM architectures: extensive\nparallelism and high memory bandwidth. Based on this observation, we design and\nimplement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,\nthe first openly commercialized PIM architecture. Our evaluation demonstrates\nthat a PIM-based multi-server PIR implementation significantly improves query\nthroughput by more than 3.7x when compared to a standard CPU-based PIR\napproach."}
{"id": "2509.06012", "pdf": "https://arxiv.org/pdf/2509.06012", "abs": "https://arxiv.org/abs/2509.06012", "authors": ["Jukka Ruohonen"], "title": "A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering", "categories": ["cs.SE"], "comment": "Submitted to REFSQ 2026", "summary": "Out of a personal puzzlement, recent peer review comments, and demonstrable\nconfusion in the existing literature, the paper presents a rapid review of the\nconcept of legal requirements (LRs) in requirements engineering (RE) research.\nAccording to reviewing results, a normative understanding of LRs has often been\npresent, although proper definitions and conceptual operationalizations are\nlacking. Some papers also see LRs as functional and others as non-functional\nrequirements. Legal requirements are often characterized as being vague and\ncomplex, requiring a lot of effort to elicit, implement, and validate. These\ncharacterizations supposedly correlate with knowledge gaps among requirements\nengineers. LRs are also seen to often change and overlap. They may be also\nprioritized. According to the literature, they seem to be also reluctantly\nimplemented, often providing only a minimal baseline for other requirements.\nWith these and other observations, the review raises critical arguments about\napparent knowledge gaps, including a lack of empirical evidence backing the\nobservations and enduring conceptual confusion."}
{"id": "2509.06776", "pdf": "https://arxiv.org/pdf/2509.06776", "abs": "https://arxiv.org/abs/2509.06776", "authors": ["Jingwen Qin", "Semen Checherin", "Yue Li", "Berend-Jan van der Zwaag", "Özlem Durmaz-Incel"], "title": "Hue4U: Real-Time Personalized Color Correction in Augmented Reality", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent\nof women worldwide. Existing color-correction methods often rely on prior\nclinical diagnosis and static filtering, making them less effective for users\nwith mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,\nreal-time color-correction system in augmented reality using consumer-grade\nMeta Quest headsets. Unlike previous methods, Hue4U requires no prior medical\ndiagnosis and adapts to the user in real time. A user study with 10\nparticipants showed notable improvements in their ability to distinguish\ncolors. The results demonstrated large effect sizes (Cohen's d > 1.4),\nsuggesting clinically meaningful gains for individuals with CVD. These findings\nhighlight the potential of personalized AR interventions to improve visual\naccessibility and quality of life for people affected by CVD."}
{"id": "2509.06245", "pdf": "https://arxiv.org/pdf/2509.06245", "abs": "https://arxiv.org/abs/2509.06245", "authors": ["Shyam Kumar Shrestha", "Jonathan Kua", "Shiva Raj Pokhrel"], "title": "Understanding BBRv3 Performance in AQM-Enabled WiFi Networks", "categories": ["cs.NI", "cs.ET"], "comment": "The 50th IEEE Conference on Local Computer Networks (LCN) October\n  14-16, 2025, Sydney, Australia", "summary": "We present a modular experimental testbed and lightweight visualization tool\nfor evaluating TCP congestion control performance in wireless networks. We\ncompare Google's latest Bottleneck Bandwidth and Round-trip time version 3\n(BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management\n(AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a\ncommercial MikroTik router. Our real-time dashboard visualizes metrics such as\nthroughput, latency, and fairness across competing flows. Results show that\nBBRv3 significantly improves fairness and convergence under AQM, especially\nwith FQ-CoDel. Our visualization tool and modular testbed provide a practical\nfoundation for evaluating next-generation TCP variants in real-world\nAQM-enabled home wireless networks."}
{"id": "2509.05961", "pdf": "https://arxiv.org/pdf/2509.05961", "abs": "https://arxiv.org/abs/2509.05961", "authors": ["Evgeny V. Votyakov", "Marios Constantinides", "Fotis Liarokapis"], "title": "A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners", "categories": ["cs.HC"], "comment": "8 pages, 5 figures, 3 tables", "summary": "Amateur runners are increasingly using wearable devices to track their\ntraining, and often do so through simple metrics such as heart rate and pace.\nHowever, these metrics are typically analyzed in isolation and lack the\nexplainability needed for long-term self-monitoring. In this paper, we first\npresent Fitplotter, which is a client-side web application designed for the\nvisualization and analysis of data associated with fitness and activity\ntracking devices. Next, we revisited and formalized Heart Rate Efficiency\n(HRE), defined as the product of pace and heart rate, as a practical and\nexplainable metric to track aerobic fitness in everyday running. Drawing on\nmore than a decade of training data from one athlete, and supplemented by\npublicly available logs from twelve runners, we showed that HRE provides more\nstable and meaningful feedback on aerobic development than heart rate or pace\nalone. We showed that HRE correlates with training volume, reflects seasonal\nprogress, and remains stable during long runs in well-trained individuals. We\nalso discuss how HRE can support everyday training decisions, improve the user\nexperience in fitness tracking, and serve as an explainable metric to\nproprietary ones of commercial platforms. Our findings have implications for\ndesigning user-centered fitness tools that empower amateur athletes to\nunderstand and manage their own performance data."}
{"id": "2509.06061", "pdf": "https://arxiv.org/pdf/2509.06061", "abs": "https://arxiv.org/abs/2509.06061", "authors": ["Faiza Babakano", "Ahmed Fahmin", "Bojie Shen", "Muhammad Aamir Cheema", "Isma Farah Siddiqui"], "title": "Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain", "categories": ["cs.RO", "cs.DB"], "comment": null, "summary": "Autonomous Mobile Robots (AMRs) operate on battery power, making energy\nefficiency a critical consideration, particularly in outdoor environments where\nterrain variations affect energy consumption. While prior research has\nprimarily focused on computing energy-efficient paths from a source to a\ndestination, these approaches often overlook practical scenarios where a robot\nneeds to pick up an object en route - an action that can significantly impact\nenergy consumption due to changes in payload. This paper introduces the\nObject-Pickup Minimum Energy Path Problem (OMEPP), which addresses\nenergy-efficient route planning for AMRs required to pick up an object from one\nof many possible locations and deliver it to a destination. To address OMEPP,\nwe first introduce a baseline algorithm that employs the Z star algorithm, a\nvariant of A star tailored for energy-efficient routing, to iteratively visit\neach pickup point. While this approach guarantees optimality, it suffers from\nhigh computational cost due to repeated searches at each pickup location. To\nmitigate this inefficiency, we propose a concurrent PCPD search that manages\nmultiple Z star searches simultaneously across all pickup points. Central to\nour solution is the Payload-Constrained Path Database (PCPD), an extension of\nthe Compressed Path Database (CPD) that incorporates payload constraints. We\ndemonstrate that PCPD significantly reduces branching factors during search,\nimproving overall performance. Although the concurrent PCPD search may produce\nslightly suboptimal solutions, extensive experiments on real-world datasets\nshow it achieves near-optimal performance while being one to two orders of\nmagnitude faster than the baseline algorithm."}
{"id": "2509.06289", "pdf": "https://arxiv.org/pdf/2509.06289", "abs": "https://arxiv.org/abs/2509.06289", "authors": ["Shaoqi Wei", "Senling Wang", "Hiroshi Kai", "Yoshinobu Higami", "Ruijun Ma", "Tianming Ni", "Xiaoqing Wen", "Hiroshi Takahashi"], "title": "A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults", "categories": ["cs.LG", "cs.AR", "cs.ET", "B.7.3"], "comment": "21 pages, 9 figures, plan to submit to ACM TODAES", "summary": "Silent Data Errors (SDEs) from time-zero defects and aging degrade\nsafety-critical systems. Functional testing detects SDE-related faults but is\nexpensive to simulate. We present a unified spatio-temporal graph convolutional\nnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impact\nprobabilities (FIPs) in large sequential circuits, supporting quantitative risk\nassessment. Gate-level netlists are modeled as spatio-temporal graphs to\ncapture topology and signal timing; dedicated spatial and temporal encoders\npredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method\nreduces simulation time by more than 10x while maintaining high accuracy (mean\nabsolute error 0.024 for 5-cycle predictions). The framework accepts features\nfrom testability metrics or fault simulation, allowing efficiency-accuracy\ntrade-offs. A test-point selection study shows that choosing observation points\nby predicted FIPs improves detection of long-cycle, hard-to-detect faults. The\napproach scales to SoC-level test strategy optimization and fits downstream\nelectronic design automation flows."}
{"id": "2509.06616", "pdf": "https://arxiv.org/pdf/2509.06616", "abs": "https://arxiv.org/abs/2509.06616", "authors": ["Anton Paramonov", "Yann Vonlanthen", "Quentin Kniep", "Jakub Sliwinski", "Roger Wattenhofer"], "title": "Mangrove: Fast and Parallelizable State Replication for Blockchains", "categories": ["cs.DC"], "comment": null, "summary": "Mangrove is a novel scaling approach to building blockchains with parallel\nsmart contract support. Unlike in monolithic blockchains, where a single\nconsensus mechanism determines a strict total order over all transactions,\nMangrove uses separate consensus instances per smart contract, without a global\norder. To allow multiple instances to run in parallel while ensuring that no\nconflicting transactions are committed, we propose a mechanism called Parallel\nOptimistic Agreement. Additionally, for simple transactions, we leverage a\nlightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove\nis optimized for performance under optimistic conditions, where there is no\nmisbehavior and the network is synchronous. Under these conditions, our\nprotocol can achieve a latency of 2 communication steps between creating and\nexecuting a transaction."}
{"id": "2509.06052", "pdf": "https://arxiv.org/pdf/2509.06052", "abs": "https://arxiv.org/abs/2509.06052", "authors": ["Qingyuan Li", "Binchang Li", "Cuiyun Gao", "Shuzheng Gao", "Zongjie Li"], "title": "Empirical Study of Code Large Language Models for Binary Security Patch Detection", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "Security patch detection (SPD) is crucial for maintaining software security,\nas unpatched vulnerabilities can lead to severe security risks. In recent\nyears, numerous learning-based SPD approaches have demonstrated promising\nresults on source code. However, these approaches typically cannot be applied\nto closed-source applications and proprietary systems that constitute a\nsignificant portion of real-world software, as they release patches only with\nbinary files, and the source code is inaccessible. Given the impressive\nperformance of code large language models (LLMs) in code intelligence and\nbinary analysis tasks such as decompilation and compilation optimization, their\npotential for detecting binary security patches remains unexplored, exposing a\nsignificant research gap between their demonstrated low-level code\nunderstanding capabilities and this critical security task. To address this\ngap, we construct a large-scale binary patch dataset containing \\textbf{19,448}\nsamples, with two levels of representation: assembly code and pseudo-code, and\nsystematically evaluate \\textbf{19} code LLMs of varying scales to investigate\ntheir capability in binary SPD tasks. Our initial exploration demonstrates that\ndirectly prompting vanilla code LLMs struggles to accurately identify security\npatches from binary patches, and even state-of-the-art prompting techniques\nfail to mitigate the lack of domain knowledge in binary SPD within vanilla\nmodels. Drawing on the initial findings, we further investigate the fine-tuning\nstrategy for injecting binary SPD domain knowledge into code LLMs through two\nlevels of representation. Experimental results demonstrate that fine-tuned LLMs\nachieve outstanding performance, with the best results obtained on the\npseudo-code representation."}
{"id": "2509.06451", "pdf": "https://arxiv.org/pdf/2509.06451", "abs": "https://arxiv.org/abs/2509.06451", "authors": ["Filippo Bragato", "Tullia Fontana", "Marco Giordani", "Malte Schellmann", "Josef Eichinger", "Michele Zorzi"], "title": "Network-Aware Control of AGVs in an Industrial Scenario: A Simulation Study Based on ROS 2 and Gazebo", "categories": ["cs.NI"], "comment": "This paper has been accepted for publication at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), 2025", "summary": "Networked Control System (NCS) is a paradigm where sensors, controllers, and\nactuators communicate over a shared network. One promising application of NCS\nis the control of Automated Guided Vehicles (AGVs) in the industrial\nenvironment, for example to transport goods efficiently and to autonomously\nfollow predefined paths or routes. In this context, communication and control\nare tightly correlated, a paradigm referred to as Joint Communication and\nControl (JCC), since network issues such as delays or errors can lead to\nsignificant deviations of the AGVs from the planned trajectory. In this paper,\nwe present a simulation framework based on Gazebo and Robot Operating System 2\n(ROS 2) to simulate and visualize, respectively, the complex interaction\nbetween the control of AGVs and the underlying communication network. This\nframework explicitly incorporates communication metrics, such as delay and\npacket loss, and control metrics, especially the Mean Squared Error (MSE)\nbetween the optimal/desired and actual path of the AGV in response to driving\ncommands. Our results shed light into the correlation between the network\nperformance, particularly Packet Reception Ratio (PRR), and accuracy of\ncontrol."}
{"id": "2509.05962", "pdf": "https://arxiv.org/pdf/2509.05962", "abs": "https://arxiv.org/abs/2509.05962", "authors": ["Lazaros Stavrinou", "Argyris Constantinides", "Marios Belk", "Vasos Vassiliou", "Fotis Liarokapis", "Marios Constantinides"], "title": "The Reel Deal: Designing and Evaluating LLM-Generated Short-Form Educational Videos", "categories": ["cs.HC"], "comment": "9 pages, 3 figures, 1 table", "summary": "Short-form videos are gaining popularity in education due to their concise\nand accessible format that enables microlearning. Yet, most of these videos are\nmanually created. Even for those automatically generated using artificial\nintelligence (AI), it is not well understood whether or how they affect\nlearning outcomes, user experience, and trust. To address this gap, we\ndeveloped ReelsEd, which is a web-based system that uses large language models\n(LLMs) to automatically generate structured short-form video (i.e., reels) from\nlecture long-form videos while preserving instructor-authored material. In a\nbetween-subject user study with 62 university students, we evaluated ReelsEd\nand demonstrated that it outperformed traditional long-form videos in\nengagement, quiz performance, and task efficiency without increasing cognitive\nload. Learners expressed high trust in our system and valued its clarity,\nusefulness, and ease of navigation. Our findings point to new design\nopportunities for integrating generative AI into educational tools that\nprioritize usability, learner agency, and pedagogical alignment."}
{"id": "2509.06902", "pdf": "https://arxiv.org/pdf/2509.06902", "abs": "https://arxiv.org/abs/2509.06902", "authors": ["Aivin V. Solatorio"], "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty."}
{"id": "2509.06690", "pdf": "https://arxiv.org/pdf/2509.06690", "abs": "https://arxiv.org/abs/2509.06690", "authors": ["Usman Haider", "Lukasz Szemet", "Daniel Kelly", "Vasileios Sergis", "Andrew C. Daly", "Karl Mason"], "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring", "categories": ["cs.CV", "cs.AI", "cs.AR", "N/A", "I.2.9; I.2.10; I.4.6"], "comment": "8 pages, 5 figures, conference-style submission (ICRA 2026). Includes\n  dataset description, BioLite U-Net architecture, benchmark results on edge\n  device (Raspberry Pi 4B)", "summary": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems."}
{"id": "2509.05511", "pdf": "https://arxiv.org/pdf/2509.05511", "abs": "https://arxiv.org/abs/2509.05511", "authors": ["Dhanya R Mathews", "Mudit Verma", "Pooja Aggarwal", "J. Lakshmi"], "title": "Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology", "categories": ["cs.PF", "cs.DC"], "comment": null, "summary": "Cloud application services are distributed in nature and have components\nacross the stack working together to deliver the experience to end users. The\nwide adoption of microservice architecture exacerbates failure management due\nto increased service components. To be effective, the strategies to enhance the\napplication service resilience need to be autonomous and developed at the\nservice's granularity, considering its end-to-end components. However, the\nmassive amount of observability data generated by all these components across\nthe service stack poses a significant challenge in reacting to anomalies and\nrestoring the service quality in real time. Identifying the most informative\nobservability data from across the cloud service stack and timely localization\nof root causes of anomalies thus becomes crucial to ensure service resilience.\nThis article presents a novel approach that considers the application service\ntopology to select the most informative metrics across the cloud stack to\nsupport efficient, explainable, and accurate root cause identifications in case\nof performance anomalies. The usefulness of the selected metrics is then\nevaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for\nlocalizing the root cause of performance anomalies. As a step towards improving\nthe accuracy and efficiency of RCD, this article then proposes the\nTopology-Aware-RCD (TA-RCD) that incorporates the end-to-end application\nservice topology in RCD. The evaluation of the failure injection studies shows\nthat the proposed approach performs at least 2X times better on average than\nthe state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall."}
{"id": "2509.06085", "pdf": "https://arxiv.org/pdf/2509.06085", "abs": "https://arxiv.org/abs/2509.06085", "authors": ["Jerin Yasmin", "Wenxin Jiang", "James C. Davis", "Yuan Tian"], "title": "Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to Empirical Software Engineering (EMSE) Journal", "summary": "Pre-trained models (PTMs) are machine learning models that have been trained\nin advance, often on large-scale data, and can be reused for new tasks, thereby\nreducing the need for costly training from scratch. Their widespread adoption\nintroduces a new class of software dependency, which we term Software\nDependencies 2.0, extending beyond conventional libraries to learned behaviors\nembodied in trained models and their associated artifacts. The integration of\nPTMs as software dependencies in real projects remains unclear, potentially\nthreatening maintainability and reliability of modern software systems that\nincreasingly rely on them. Objective: In this study, we investigate Software\nDependencies 2.0 in open-source software (OSS) projects by examining the reuse\nof PTMs, with a focus on how developers manage and integrate these models.\nSpecifically, we seek to understand: (1) how OSS projects structure and\ndocument their PTM dependencies; (2) what stages and organizational patterns\nemerge in the reuse pipelines of PTMs within these projects; and (3) the\ninteractions among PTMs and other learned components across pipeline stages. We\nconduct a mixed-methods analysis of a statistically significant random sample\nof 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories\nreusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM\nreuse by identifying patterns and qualitatively investigate how developers\nintegrate and manage these models in practice."}
{"id": "2509.06454", "pdf": "https://arxiv.org/pdf/2509.06454", "abs": "https://arxiv.org/abs/2509.06454", "authors": ["Julia Caleya-Sanchez", "Pablo Muñoz", "Jorge Sánchez-Garrido", "Emilio Florentín", "Felix Delgado-Ferro", "Pablo Rodriguez-Martin", "Pablo Ameigeiras"], "title": "Empirical Evaluation of a 5G Transparent Clock for Time Synchronization in a TSN-5G Network", "categories": ["cs.NI"], "comment": "6 pages, 5 figures", "summary": "Time synchronization is essential for industrial IoT and Industry 4.0/5.0\napplications, but achieving high synchronization accuracy in Time-Sensitive\nNetworking (TSN)-5G networks is challenging due to jitter and asymmetric\ndelays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware\nsystem, boundary clock (BC), and transparent clock (TC), where TC offers a\npromising solution. However, to the best of our knowledge, there is no\nempirical evaluation of TC in a TSN-5G network. This paper empirically\nevaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial\nTSN switches with a single clock. For TC development, we compute the residence\ntime in 5G and recover the clock domain at the slave node. We deploy a TSN-5G\ntestbed with commercial equipment for synchronization evaluation by modifying\nthe Precision Timing Protocol (PTP) message transmission rates. Experimental\nresults show a peak-to-peak synchronization of 500 ns, meeting the industrial\nrequirement of < 1 us, with minimal synchronization offsets for specific PTP\nmessage transmission rates."}
{"id": "2509.06114", "pdf": "https://arxiv.org/pdf/2509.06114", "abs": "https://arxiv.org/abs/2509.06114", "authors": ["Yuxin Zhang", "Fan Zhang", "Jinjun Xia", "Chao Zhao"], "title": "Material Experience: An Evaluation Model for Creative Materials Based on Visual-Tactile Sensory Properties", "categories": ["cs.HC"], "comment": null, "summary": "This study adopts a design-oriented approach to integrate traditional braids\nwith commonly used matrix materials, developing creative materials with\ndifferent sensory properties by altering matrix material types and braid\npatterns. Based on these creative materials, a quantitative and structured\nmodel is proposed to assist designers understanding the material experience\nprocess and guide material selection by analyzing the relationship between\nmaterial properties and sensory perception. Specifically, participants\nevaluated the creative materials under visual-tactile conditions using a\n7-point semantic differential (SD) scale. Correlation analysis was performed to\nexplore the data. The main and interaction effects of matrix materials and\nbraid patterns on impression evaluation were analyzed using two-way analysis of\nvariance (ANOVA). A structural equation model (SEM) was constructed based on\nexploratory factor analysis (EFA), and path coefficients were computed to\nassess the relative importance of material properties in determining material\nattractiveness. The results show that, compared to braids, the creative\nmaterials resulted in significant changes in impression evaluation.\nFurthermore, the creative materials can be understood through intrinsic,\naesthetic, and physical properties, with their standardized regression\ncoefficients for material attractiveness of 0.486, 0.650, and 0.103,\nrespectively. These properties are interrelated and under their combined\ninfluence affect the attractiveness of the material. Therefore, designers\nshould consider utilizing these relationships to enhance sensory experience in\norder to achieve design objectives. Moreover, designers should also consider\nbalancing technology and experience, using materials according to the principle\nof \"form follows function\"."}
{"id": "2509.06794", "pdf": "https://arxiv.org/pdf/2509.06794", "abs": "https://arxiv.org/abs/2509.06794", "authors": ["Shihan Fang", "Hongzheng Chen", "Niansong Zhang", "Jiajie Li", "Han Meng", "Adrian Liu", "Zhiru Zhang"], "title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "categories": ["cs.PL", "cs.AR", "cs.LG"], "comment": null, "summary": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance."}
{"id": "2509.05675", "pdf": "https://arxiv.org/pdf/2509.05675", "abs": "https://arxiv.org/abs/2509.05675", "authors": ["Amin Pakzad", "Pedro Arduino", "Wenyang Zhang", "Ertugrul Tacirouglu"], "title": "Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation", "categories": ["math.NA", "cs.DC", "cs.NA", "74S05", "G.1.8"], "comment": "8 pages, 20 figures, conference paper, Proceedings of the XVII PCSMGE", "summary": "The demand for high-fidelity numerical simulations in soil-structure\ninteraction analysis is on the rise, yet a standardized workflow to guide the\ncreation of such simulations remains elusive. This paper aims to bridge this\ngap by presenting a step-by-step guideline proposing a workflow for dynamic\nanalysis of structures with pile foundations. The proposed workflow encompasses\ninstructions on how to use Domain Reduction Method for loading, Perfectly\nMatched Layer elements for wave absorption, soil-structure interaction modeling\nusing Embedded interface elements, and domain decomposition for efficient use\nof processing units. Through a series of numerical simulations, we showcase the\npractical application of this workflow. Our results reveal the efficacy of the\nDomain Reduction Method in reducing simulation size without compromising model\nfidelity, show the precision of Perfectly Matched Layer elements in modeling\ninfinite domains, highlight the efficiency of Embedded Interface elements in\nestablishing connections between structures and the soil domain, and\ndemonstrate the overall effectiveness of the proposed workflow in conducting\nhigh-fidelity simulations. While our study focuses on simplified geometries and\nloading scenarios, it serves as a foundational framework for future research\nendeavors aimed at exploring more intricate structural configurations and\ndynamic loading conditions"}
{"id": "2509.06216", "pdf": "https://arxiv.org/pdf/2509.06216", "abs": "https://arxiv.org/abs/2509.06216", "authors": ["Ahmed E. Hassan", "Hao Li", "Dayi Lin", "Bram Adams", "Tse-Hsun Chen", "Yutaro Kashiwa", "Dong Qiu"], "title": "Agentic Software Engineering: Foundational Pillars and a Research Roadmap", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Agentic Software Engineering (SE 3.0) represents a new era where intelligent\nagents are tasked not with simple code generation, but with achieving complex,\ngoal-oriented SE objectives. To harness these new capabilities while ensuring\ntrustworthiness, we must recognize a fundamental duality within the SE field in\nthe Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE\nfor Agents. This duality demands a radical reimagining of the foundational\npillars of SE (actors, processes, tools, and artifacts) which manifest\ndifferently across each modality. We propose two purpose-built workbenches to\nsupport this vision. The Agent Command Environment (ACE) serves as a command\ncenter where humans orchestrate and mentor agent teams, handling outputs such\nas Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The\nAgent Execution Environment (AEE) is a digital workspace where agents perform\ntasks while invoking human expertise when facing ambiguity or complex\ntrade-offs. This bi-directional partnership, which supports agent-initiated\nhuman callbacks and handovers, gives rise to new, structured engineering\nactivities (i.e., processes) that redefine human-AI collaboration, elevating\nthe practice from agentic coding to true agentic software engineering. This\npaper presents the Structured Agentic Software Engineering (SASE) vision,\noutlining several of the foundational pillars for the future of SE. The paper\nculminates in a research roadmap that identifies a few key challenges and\nopportunities while briefly discussing the resulting impact of this future on\nSE education. Our goal is not to offer a definitive solution, but to provide a\nconceptual scaffold with structured vocabulary to catalyze a community-wide\ndialogue, pushing the SE community to think beyond its classic, human-centric\ntenets toward a disciplined, scalable, and trustworthy agentic future."}
{"id": "2509.06515", "pdf": "https://arxiv.org/pdf/2509.06515", "abs": "https://arxiv.org/abs/2509.06515", "authors": ["Ege Cem Kirci", "Ayush Mishra", "Laurent Vanbever"], "title": "Five Blind Men and the Internet: Towards an Understanding of Internet Traffic", "categories": ["cs.NI"], "comment": "15 pages, 16 figures", "summary": "The Internet, the world's largest and most pervasive network, lacks a\ntransparent, granular view of its traffic patterns, volumes, and growth trends,\nhindering the networking community's understanding of its dynamics. This paper\nleverages publicly available Internet Exchange Point traffic statistics to\naddress this gap, presenting a comprehensive two-year study (2023-2024) from\n472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate\ntraffic by late 2024. Our analysis reveals a 49.2% global traffic increase\n(24.5% annualized), uncovers regionally distinct diurnal patterns and\nevent-driven anomalies, and demonstrates stable utilization rates, reflecting\npredictable infrastructure scaling. By analyzing biases and confirming high\nself-similarity, we establish IXP traffic as a robust proxy for overall\nInternet growth and usage behavior. With transparent, replicable data--covering\n87% of the worldwide IXP port capacity--and plans to release our dataset, this\nstudy offers a verifiable foundation for long-term Internet traffic monitoring.\nIn particular, our findings shed light on the interplay between network design\nand function, providing an accessible framework for researchers and operators\nto explore the Internet's evolving ecosystem."}
{"id": "2509.06382", "pdf": "https://arxiv.org/pdf/2509.06382", "abs": "https://arxiv.org/abs/2509.06382", "authors": ["Yingke Ding", "Zeyu Wang", "Xiyuxing Zhang", "Hongbin Chen", "Zhenan Xu"], "title": "Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn Multimodal LLM Conversation", "categories": ["cs.HC"], "comment": "Ubicomp Companion 2025", "summary": "Traditional hearing aids often rely on static fittings that fail to adapt to\ntheir dynamic acoustic environments. We propose CAFA, a Context-Adaptive\nFitting Advisor that provides personalized, real-time hearing aid adjustments\nthrough a multi-agent Large Language Model (LLM) workflow. CAFA combines live\nambient audio, audiograms, and user feedback in a multi-turn conversational\nsystem. Ambient sound is classified into conversation, noise, or quiet with\n91.2\\% accuracy using a lightweight neural network based on YAMNet embeddings.\nThis system utilizes a modular LLM workflow, comprising context acquisition,\nsubproblem classification, strategy provision, and ethical regulation, and is\noverseen by an LLM Judge. The workflow translates context and feedback into\nprecise, safe tuning commands. Evaluation confirms that real-time sound\nclassification enhances conversational efficiency. CAFA exemplifies how\nagentic, multimodal AI can enable intelligent, user-centric assistive\ntechnologies."}
{"id": "2509.05679", "pdf": "https://arxiv.org/pdf/2509.05679", "abs": "https://arxiv.org/abs/2509.05679", "authors": ["Viet Hoang Pham", "Hyo-Sung Ahn"], "title": "Distributed Deep Learning using Stochastic Gradient Staleness", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Despite the notable success of deep neural networks (DNNs) in solving complex\ntasks, the training process still remains considerable challenges. A primary\nobstacle is the substantial time required for training, particularly as high\nperforming DNNs tend to become increasingly deep (characterized by a larger\nnumber of hidden layers) and require extensive training datasets. To address\nthese challenges, this paper introduces a distributed training method that\nintegrates two prominent strategies for accelerating deep learning: data\nparallelism and fully decoupled parallel backpropagation algorithm. By\nutilizing multiple computational units operating in parallel, the proposed\napproach enhances the amount of training data processed in each iteration while\nmitigating locking issues commonly associated with the backpropagation\nalgorithm. These features collectively contribute to significant improvements\nin training efficiency. The proposed distributed training method is rigorously\nproven to converge to critical points under certain conditions. Its\neffectiveness is further demonstrated through empirical evaluations, wherein an\nDNN is trained to perform classification tasks on the CIFAR-10 dataset."}
{"id": "2509.06301", "pdf": "https://arxiv.org/pdf/2509.06301", "abs": "https://arxiv.org/abs/2509.06301", "authors": ["Dharun Anandayuvaraj", "Zain Hammadeh", "Andreas Lund", "Alexandra Holloway", "James C. Davis"], "title": "Learning From Software Failures: A Case Study at a National Space Research Center", "categories": ["cs.SE"], "comment": null, "summary": "Software failures can have significant consequences, making learning from\nfailures a critical aspect of software engineering. While software\norganizations are recommended to conduct postmortems, the effectiveness and\nadoption of these practices vary widely. Understanding how engineers gather,\ndocument, share, and apply lessons from failures is essential for improving\nreliability and preventing recurrence. High-reliability organizations (HROs)\noften develop software systems where failures carry catastrophic risks,\nrequiring continuous learning to ensure reliability. These organizations\nprovide a valuable setting to examine practices and challenges for learning\nfrom software failures. Such insight could help develop processes and tools to\nimprove reliability and prevent recurrence. However, we lack in-depth industry\nperspectives on the practices and challenges of learning from failures.\n  To address this gap, we conducted a case study through 10 in-depth interviews\nwith research software engineers at a national space research center. We\nexamine how they learn from failures: how they gather, document, share, and\napply lessons. To assess transferability, we include data from 5 additional\ninterviews at other HROs. Our findings provide insight into how engineers learn\nfrom failures in practice. To summarize: (1) failure learning is informal, ad\nhoc, and inconsistently integrated into SDLC; (2) recurring failures persist\ndue to absence of structured processes; and (3) key challenges, including time\nconstraints, knowledge loss from turnover and fragmented documentation, and\nweak process enforcement, undermine systematic learning. Our findings deepen\nunderstanding of how software engineers learn from failures and offer guidance\nfor improving failure management practices."}
{"id": "2509.06639", "pdf": "https://arxiv.org/pdf/2509.06639", "abs": "https://arxiv.org/abs/2509.06639", "authors": ["Chenming He", "Rui Xia", "Chengzhen Meng", "Xiaoran Fan", "Dequan Wang", "Haojie Ren", "Jianmin Ji", "Yanyong Zhang"], "title": "Ghost Points Matter: Far-Range Vehicle Detection with a Single mmWave Radar in Tunnel", "categories": ["cs.NI"], "comment": "17 pages, 25 figures, to appear in ACM MobiCom 2025", "summary": "Vehicle detection in tunnels is crucial for traffic monitoring and accident\nresponse, yet remains underexplored. In this paper, we develop mmTunnel, a\nmillimeter-wave radar system that achieves far-range vehicle detection in\ntunnels. The main challenge here is coping with ghost points caused by\nmulti-path reflections, which lead to severe localization errors and false\nalarms. Instead of merely removing ghost points, we propose correcting them to\ntrue vehicle positions by recovering their signal reflection paths, thus\nreserving more data points and improving detection performance, even in\nocclusion scenarios. However, recovering complex 3D reflection paths from\nlimited 2D radar points is highly challenging. To address this problem, we\ndevelop a multi-path ray tracing algorithm that leverages the ground plane\nconstraint and identifies the most probable reflection path based on signal\npath loss and spatial distance. We also introduce a curve-to-plane segmentation\nmethod to simplify tunnel surface modeling such that we can significantly\nreduce the computational delay and achieve real-time processing.\n  We have evaluated mmTunnel with comprehensive experiments. In two test\ntunnels, we conducted controlled experiments in various scenarios with cars and\ntrucks. Our system achieves an average F1 score of 93.7% for vehicle detection\nwhile maintaining real-time processing. Even in the challenging occlusion\nscenarios, the F1 score remains above 91%. Moreover, we collected extensive\ndata from a public tunnel with heavy traffic at times and show our method could\nachieve an F1 score of 91.5% in real-world traffic conditions."}
{"id": "2509.06393", "pdf": "https://arxiv.org/pdf/2509.06393", "abs": "https://arxiv.org/abs/2509.06393", "authors": ["Mehrnoosh Sadat Shirvani", "Jackie Liu", "Thomas Chao", "Suky Martinez", "Laura Brandt", "Ig-Jae Kim", "Dongwook Yoon"], "title": "Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support", "categories": ["cs.HC"], "comment": null, "summary": "Mental health conversational agents have the potential to deliver valuable\ntherapeutic impact, but low user engagement remains a critical barrier\nhindering their efficacy. Existing therapeutic approaches have leveraged\nclients' internal dialogues (e.g., journaling, talking to an empty chair) to\nenhance engagement through accountable, self-sourced support. Inspired by\nthese, we designed novel AI-driven self-clone chatbots that replicate users'\nsupport strategies and conversational patterns to improve therapeutic\nengagement through externalized meaningful self-conversation. Validated through\na semi-controlled experiment (N=180), significantly higher emotional and\ncognitive engagement was demonstrated with self-clone chatbots than a chatbot\nwith a generic counselor persona. Our findings highlight self-clone\nbelievability as a mediator and emphasize the balance required in maintaining\nconvincing self-representation while creating positive interactions. This study\ncontributes to AI-based mental health interventions by introducing and\nevaluating self-clones as a promising approach to increasing user engagement,\nwhile exploring implications for their application in mental health care."}
{"id": "2509.05759", "pdf": "https://arxiv.org/pdf/2509.05759", "abs": "https://arxiv.org/abs/2509.05759", "authors": ["Jinkun Geng", "Shuai Mu", "Anirudh Sivaraman", "Balaji Prabhakar"], "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]", "categories": ["cs.NI", "cs.DB", "cs.DC", "68M10, 68M15"], "comment": "This is the technical report for our paper accepted by The 31st\n  Symposium on Operating Systems Principles (SOSP'25)", "summary": "This paper presents Tiga, a new design for geo-replicated and scalable\ntransactional databases such as Google Spanner. Tiga aims to commit\ntransactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of\nscenarios, while maintaining high throughput with minimal computational\noverhead. Tiga consolidates concurrency control and consensus, completing both\nstrictly serializable execution and consistent replication in a single round.\nIt uses synchronized clocks to proactively order transactions by assigning each\na future timestamp at submission. In most cases, transactions arrive at servers\nbefore their future timestamps and are serialized according to the designated\ntimestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed\nand proactive ordering fails, in which case Tiga falls back to a slow path,\ncommitting in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can\ncommit more transactions at 1-WRTT latency, and incurs much less throughput\noverhead. Evaluation results show that Tiga outperforms all baselines,\nachieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower\nlatency. Tiga is open-sourced at\nhttps://github.com/New-Consensus-Concurrency-Control/Tiga."}
{"id": "2509.06324", "pdf": "https://arxiv.org/pdf/2509.06324", "abs": "https://arxiv.org/abs/2509.06324", "authors": ["Zhuohang Shen", "Mohammed Yaseen", "Denini Silva", "Kevin Guan", "Junho Lee", "Marcelo d'Amorim", "Owolabi Legunsen"], "title": "A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation", "categories": ["cs.SE"], "comment": "23 pages, 7 figures", "summary": "Runtime verification (RV) now scales for testing thousands of open-source\nJava projects, helping find hundreds of bugs. The popular Python ecosystem\ncould use such benefits. But, today's Python RV systems are limited to a domain\nor specification logic, or slow. We propose PyMOP, a generic, extensible, and\nefficient RV system for Python. PyMOP supports five logics, implements five\nexisting monitoring algorithms, ships with 73 API specs of Python and\nwidely-used libraries, supports three instrumentation strategies, and users can\neasily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we\nfind mainly that (i) the default monitoring algorithm for Java is often not the\nfastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic\nanalysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were\nfixed by developers. PyMOP's generality and efficiency position it well as an\nexcellent platform for the next advances on RV for Python."}
{"id": "2509.06700", "pdf": "https://arxiv.org/pdf/2509.06700", "abs": "https://arxiv.org/abs/2509.06700", "authors": ["Swarna Bindu Chetty", "David Grace", "Simon Saunders", "Paul Harris", "Eirini Eleni Tsiropoulou", "Tony Quek", "Hamed Ahmadi"], "title": "Sovereign AI for 6G: Towards the Future of AI-Native Networks", "categories": ["cs.NI"], "comment": null, "summary": "The advent of Generative Artificial Intelligence (GenAI), Large Language\nModels (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile\nnetworks, especially as the telecom industry transitions from 5G's\ncloud-centric to AI-native 6G architectures. This transition unlocks\nunprecedented capabilities in real-time automation, semantic networking, and\nautonomous service orchestration. However, it introduces critical risks related\nto data sovereignty, security, explainability, and regulatory compliance\nespecially when AI models are trained, deployed, or governed externally. This\npaper introduces the concept of `Sovereign AI' as a strategic imperative for\n6G, proposing architectural, operational, and governance frameworks that enable\nnational or operator-level control over AI development, deployment, and\nlife-cycle management. Focusing on O-RAN architecture, we explore how sovereign\nAI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure\npolicy-aligned control, secure model updates, and federated learning across\ntrusted infrastructure. We analyse global strategies, technical enablers, and\nchallenges across safety, talent, and model governance. Our findings underscore\nthat Sovereign AI is not just a regulatory necessity but a foundational pillar\nfor secure, resilient, and ethically-aligned 6G networks."}
{"id": "2509.06475", "pdf": "https://arxiv.org/pdf/2509.06475", "abs": "https://arxiv.org/abs/2509.06475", "authors": ["Yannick Kalff", "Katharina Simbeck"], "title": "Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems", "categories": ["cs.HC", "cs.AI", "cs.CY", "A.0; H.5.2; I.2; J.1; K.4.2; K.4.3"], "comment": "Accepted paper for RecSys in HR'25: The 5th Workshop on Recommender\n  Systems for Human Resources, in conjunction with the 19th ACM Conference on\n  Recommender Systems, September 22--26, 2025, Prague, Czech Republic", "summary": "AI-based recommender systems increasingly influence recruitment decisions.\nThus, transparency and responsible adoption in Human Resource Management (HRM)\nare critical. This study examines how HR managers' AI literacy influences their\nsubjective perception and objective understanding of explainable AI (XAI)\nelements in recruiting recommender dashboards. In an online experiment, 410\nGerman-based HR managers compared baseline dashboards to versions enriched with\nthree XAI styles: important features, counterfactuals, and model criteria. Our\nresults show that the dashboards used in practice do not explain AI results and\neven keep AI elements opaque. However, while adding XAI features improves\nsubjective perceptions of helpfulness and trust among users with moderate or\nhigh AI literacy, it does not increase their objective understanding. It may\neven reduce accurate understanding, especially with complex explanations. Only\noverlays of important features significantly aided the interpretations of\nhigh-literacy users. Our findings highlight that the benefits of XAI in\nrecruitment depend on users' AI literacy, emphasizing the need for tailored\nexplanation strategies and targeted literacy training in HRM to ensure fair,\ntransparent, and effective adoption of AI."}
{"id": "2509.05884", "pdf": "https://arxiv.org/pdf/2509.05884", "abs": "https://arxiv.org/abs/2509.05884", "authors": ["Banhirup Sengupta", "Peenal Gupta", "Souvik Sengupta"], "title": "Introduction to Number Theoretic Transform", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "The Number Theoretic Transform (NTT) can be regarded as a variant of the\nDiscrete Fourier Transform. NTT has been quite a powerful mathematical tool in\ndeveloping Post-Quantum Cryptography and Homomorphic Encryption. The Fourier\nTransform essentially decomposes a signal into its frequencies. They are\ntraditionally sine or cosine waves. NTT works more over groups or finite fields\nrather than on a continuous signal and polynomials work as the analog of sine\nwaves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has\nbeen proven to be useful in lattice-based cryptography due to its ability to\nreduce the complexity of polynomial multiplication from quadratic to\nquasilinear. We have introduced the concepts of cyclic, negacyclic convolutions\nalong with NTT and its inverse and their fast versions."}
{"id": "2509.06429", "pdf": "https://arxiv.org/pdf/2509.06429", "abs": "https://arxiv.org/abs/2509.06429", "authors": ["Mehmet Bilal Er", "Nagehan İlhan", "Umut Kuran"], "title": "Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction", "categories": ["cs.SE"], "comment": null, "summary": "The use of Large Language Models (LLMs) in software engineering tasks is\ngrowing, especially in the areas of bug fixing and code generation.\nNevertheless, these models often yield unstable results; when executed at\ndifferent times with the same input, they can generate radically different\ncode. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly\nassessed, despite the fact that this instability has typically been discussed\nin the literature in relation to code generation. The purpose of this study is\nto look into how unstable an LLM like ChatGPT is when it comes to fixing code\nbugs. We examine the structural, syntactic, and functional variations among\nseveral fix recommendations made in response to the same prompt using code\nsamples with various error types. Additionally, we assess how instability is\naffected by the temperature settings (0, 0.5, and 1) used for the model's\ndeterministic operation. For a total of 20 problems in the experimental\nanalysis, the model produced three fix suggestions at each temperature value,\ncomparing nine distinct outputs for each problem. The Syntax Similarity and\nOutput Equivalence Rate (OER) metrics were used to assess the outputs'\nstructural and functional consistency. The results demonstrate that the model's\noutputs become much more unstable and variable as the temperature rises, with\nhigh temperatures showing especially high rates of functional failure.\nAccording to syntax similarity analyses, the suggested fixes show notable\nstructural differences at high temperatures but are fairly similar at low\ntemperatures. The purpose of this study is to provide important methodological\ninsights into how LLM-based error correction systems can be applied more\nconsistently in software development processes while also casting doubt on\ntheir dependability."}
{"id": "2509.06763", "pdf": "https://arxiv.org/pdf/2509.06763", "abs": "https://arxiv.org/abs/2509.06763", "authors": ["Huijun Tang", "Wang Zeng", "Ming Du", "Pinlong Zhao", "Pengfei Jiao", "Huaming Wu", "Hongjian Sun"], "title": "VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning", "categories": ["cs.NI"], "comment": null, "summary": "The integration of Reconfigurable Intelligent Surfaces (RIS) and Integrated\nSensing and Communication (ISAC) in vehicular networks enables dynamic spatial\nresource management and real-time adaptation to environmental changes. However,\nthe coexistence of distinct vehicle-to-infrastructure (V2I) and\nvehicle-to-vehicle (V2V) connectivity requirements, together with highly\ndynamic and heterogeneous network topologies, presents significant challenges\nfor unified reliability modeling and resource optimization. To address these\nissues, we propose VariSAC, a graph neural network (GNN)-augmented deep\nreinforcement learning framework for assured, time-continuous connectivity in\nRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,\nwe introduce the Continuous Connectivity Ratio (CCR), a unified metric that\ncharacterizes the sustained temporal reliability of V2I connections and the\nprobabilistic delivery guarantees of V2V links, thus unifying their continuous\nreliability semantics. Next, we employ a GNN with residual adapters to encode\ncomplex, high-dimensional system states, capturing spatial dependencies among\nvehicles, base stations (BS), and RIS nodes. These representations are then\nprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channel\nallocation, power control, and RIS configurations to maximize CCR-driven\nlong-term rewards. Extensive experiments on real-world urban datasets\ndemonstrate that VariSAC consistently outperforms existing baselines in terms\nof continuous V2I ISAC connectivity and V2V delivery reliability, enabling\npersistent connectivity in highly dynamic vehicular environments."}
{"id": "2509.06557", "pdf": "https://arxiv.org/pdf/2509.06557", "abs": "https://arxiv.org/abs/2509.06557", "authors": ["Juhoon Lee", "Bich Ngoc Doan", "Jonghyun Jee", "Joseph Seering"], "title": "Mapping Community Appeals Systems: Lessons for Community-led Moderation in Multi-Level Governance", "categories": ["cs.HC"], "comment": "Accepted for CSCW 2025", "summary": "Platforms are increasingly adopting industrial models of moderation that\nprioritize scalability and consistency, frequently at the expense of\ncontext-sensitive and user-centered values. Building on the multi-level\ngovernance framework that examines the interdependent relationship between\nplatforms and middle-level communities, we investigate community appeals\nsystems on Discord as a model for successful community-led governance. We\ninvestigate how Discord servers operationalize appeal systems through a\nqualitative interview study with focus groups and individual interviews with 17\ncommunity moderators. Our findings reveal a structured appeals process that\nbalances scalability, fairness, and accountability while upholding\ncommunity-centered values of growth and rehabilitation. Communities design\nthese processes to empower users, ensuring their voices are heard in moderation\ndecisions and fostering a sense of belonging. This research provides insights\ninto the practical implementation of community-led governance in a multi-level\ngovernance framework, illustrating how communities can maintain their core\nprinciples while integrating procedural fairness and tool-based design. We\ndiscuss how platforms can gain insights from community-led moderation work to\nmotivate governance structures that effectively balance and align the interests\nof multiple stakeholders."}
{"id": "2509.06133", "pdf": "https://arxiv.org/pdf/2509.06133", "abs": "https://arxiv.org/abs/2509.06133", "authors": ["Pradyumna Kaushal"], "title": "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles", "categories": ["cs.CR", "cs.DC", "cs.SE", "cs.SY", "eess.SY", "C.2.4; K.6.5; D.4.6"], "comment": "13 pages, 5 figures. Whitepaper submission; LaTeX source with\n  compiled .bbl. Includes architecture diagrams, tables, and code listings\n  (TypeScript & Solidity)", "summary": "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets."}
{"id": "2509.06530", "pdf": "https://arxiv.org/pdf/2509.06530", "abs": "https://arxiv.org/abs/2509.06530", "authors": ["Sylvain Guérin", "Salvador Martinez", "Ciprian Teodorov"], "title": "Modeling in the Design Multiverse", "categories": ["cs.SE", "D.2.10"], "comment": null, "summary": "Real-world design processes often involve the evolution and divergence of\ndesign paths (by branching, revising, merging, etc.), especially when multiple\nstakeholders or teams operate concurrently and/or explore different\nalternatives for complex and heterogeneous systems. Unfortunately, this\nvariability in time and space can not be directly managed in current modeling\nspaces but requires resorting to external tools and methodologies.\n  In order to tackle this problem, we introduce the Design Multiverse. The\nDesign Multiverse aims to integrate in the modeling space a selection of\nrevisions and variants, representing snapshots of a design state composed of\nmultiple artifacts. This enables stakeholders to seamlessly trace, analyze, and\nmanage design decisions, system variants, and their interdependencies.\nConcretely, in this paper we present a conceptual definition of the Design\nMultiverse, discuss usage scenarios such as model product lines and\nmodel/metamodel co-evolution, and propose an implementation leveraging the\nmodel federation paradigm."}
{"id": "2509.06766", "pdf": "https://arxiv.org/pdf/2509.06766", "abs": "https://arxiv.org/abs/2509.06766", "authors": ["Binquan Guo", "Zehui Xiong", "Zhou Zhang", "Baosheng Li", "Dusit Niyato", "Chau Yuen", "Zhu Han"], "title": "Resilience of Mega-Satellite Constellations: How Node Failures Impact Inter-Satellite Networking Over Time?", "categories": ["cs.NI"], "comment": "Accepted for publication in IEEE", "summary": "Mega-satellite constellations have the potential to leverage inter-satellite\nlinks to deliver low-latency end-to-end communication services globally,\nthereby extending connectivity to underserved regions. However, harsh space\nenvironments make satellites vulnerable to failures, leading to node removals\nthat disrupt inter-satellite networking. With the high risk of satellite node\nfailures, understanding their impact on end-to-end services is essential. This\nstudy investigates the importance of individual nodes on inter-satellite\nnetworking and the resilience of mega satellite constellations against node\nfailures. We represent the mega-satellite constellation as discrete temporal\ngraphs and model node failure events accordingly. To quantify node importance\nfor targeted services over time, we propose a service-aware temporal\nbetweenness metric. Leveraging this metric, we develop an analytical framework\nto identify critical nodes and assess the impact of node failures. The\nframework takes node failure events as input and efficiently evaluates their\nimpacts across current and subsequent time windows. Simulations on the Starlink\nconstellation setting reveal that satellite networks inherently exhibit\nresilience to node failures, as their dynamic topology partially restore\nconnectivity and mitigate the long-term impact. Furthermore, we find that the\nintegration of rerouting mechanisms is crucial for unleashing the full\nresilience potential to ensure rapid recovery of inter-satellite networking."}
{"id": "2509.06776", "pdf": "https://arxiv.org/pdf/2509.06776", "abs": "https://arxiv.org/abs/2509.06776", "authors": ["Jingwen Qin", "Semen Checherin", "Yue Li", "Berend-Jan van der Zwaag", "Özlem Durmaz-Incel"], "title": "Hue4U: Real-Time Personalized Color Correction in Augmented Reality", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent\nof women worldwide. Existing color-correction methods often rely on prior\nclinical diagnosis and static filtering, making them less effective for users\nwith mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,\nreal-time color-correction system in augmented reality using consumer-grade\nMeta Quest headsets. Unlike previous methods, Hue4U requires no prior medical\ndiagnosis and adapts to the user in real time. A user study with 10\nparticipants showed notable improvements in their ability to distinguish\ncolors. The results demonstrated large effect sizes (Cohen's d > 1.4),\nsuggesting clinically meaningful gains for individuals with CVD. These findings\nhighlight the potential of personalized AR interventions to improve visual\naccessibility and quality of life for people affected by CVD."}
{"id": "2509.06163", "pdf": "https://arxiv.org/pdf/2509.06163", "abs": "https://arxiv.org/abs/2509.06163", "authors": ["Victoria Kozlova", "Ben Biedermann"], "title": "Social Dynamics of DAOs: Power, Onboarding, and Inclusivity", "categories": ["cs.CY", "cs.DC", "C.2.4; C.2.1; D.4.6; K.4.3"], "comment": null, "summary": "This report explores the often-overlooked cultural and social dynamics\nshaping participation and power in DAOs. Drawing on qualitative interviews and\nethnographic observations, it shows how factors such as financial privilege,\ninformal gatekeeping, visibility bias, and onboarding structures create\nbarriers to meaningful inclusion. While DAOs are frequently framed as\npermissionless and egalitarian, the lived experiences of contributors reveal a\nmore complex reality, one in which soft power and implicit norms determine\npeople's position within DAOs. Instead of offering solutionist prescriptions,\nthis report argues for a deeper cultural reflection within the DAO ecosystem.\nIt highlights that decentralisation is not solely a protocol-level feature, but\nan ongoing social process that requires intentional cultivation of trust,\nbelonging, and epistemic plurality. With this report, we want to sharpen the\ncollective awareness of structural blind spots and call for building more\ninclusive and culturally conscious decentralised systems."}
{"id": "2509.06688", "pdf": "https://arxiv.org/pdf/2509.06688", "abs": "https://arxiv.org/abs/2509.06688", "authors": ["Heerok Banerjee"], "title": "Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool", "categories": ["cs.SE"], "comment": null, "summary": "Domain-specific languages (DSLs) play a crucial role in resolving internal\ndependencies across enterprises and boosts their upfront business management\nprocesses. Yet, a lot of development is needed to build modelling frameworks\nwhich support graphical interfaces (canvas, pallettes etc.), hierarchical\nstructures and easy implementation to shorten the gap for novice users. In this\npaper, a DSL namely, Bmod is introduced, which can be used to model evacuation\nscenarios. The language is built using Eclipse Modelling Framework (EMF) and\nEclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also\nshown between Eclipse EMF/GMF and other modelling tools such as AToMPM,\nmetaDepth, Sirius etc with respect to expressiveness, learning curve and\nperformance."}
{"id": "2509.06898", "pdf": "https://arxiv.org/pdf/2509.06898", "abs": "https://arxiv.org/abs/2509.06898", "authors": ["Zhihui Gao", "Zhecun Liu", "Tingjun Chen"], "title": "BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with Zero-Shot Template Generation", "categories": ["cs.NI", "eess.SP"], "comment": "14 pages, 17 figures", "summary": "The coexistence between incumbent radar signals and commercial 5G signals\nnecessitates a versatile and ubiquitous radar sensing for efficient and\nadaptive spectrum sharing. In this context, leveraging the densely deployed 5G\nbase stations (BS) for radar sensing is particularly promising, offering both\nwide coverage and immediate feedback to 5G scheduling. However, the targeting\nradar signals are superimposed with concurrent 5G uplink transmissions received\nby the BS, and practical deployment also demands a lightweight, portable radar\nsensing model. This paper presents BatStation, a lightweight, in-situ radar\nsensing framework seamlessly integrated into 5G BSs. BatStation leverages\nuplink resource grids to extract radar signals through three key components:\n(i) radar signal separation to cancel concurrent 5G transmissions and reveal\nthe radar signals, (ii) resource grid reshaping to align time-frequency\nresolution with radar pulse characteristics, and (iii) zero-shot template\ncorrelation based on a portable model trained purely on synthetic data that\nsupports detection, classification, and localization of radar pulses without\nfine-tuning using experimental data. We implement BatStation on a\nsoftware-defined radio (SDR) testbed and evaluate its performance with real 5G\ntraffic in the CBRS band. Results show robust performance across diverse radar\ntypes, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),\nclassification accuracy up to 97.00%, and median localization errors of\n2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,\nBatStation achieves this performance with a runtime latency of only 0.11/0.94\nms on GPU/CPU, meeting the real-time requirement of 5G networks."}
{"id": "2509.06934", "pdf": "https://arxiv.org/pdf/2509.06934", "abs": "https://arxiv.org/abs/2509.06934", "authors": ["Agam Oberlender", "Hadas Erel"], "title": "\"It was Tragic\": Exploring the Impact of a Robot's Shutdown", "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 1 table, submitted to IEEE RO-MAN 2025", "summary": "It is well established that people perceive robots as social entities, even\nwhen they are not designed for social interaction. We evaluated whether the\nsocial interpretation of robotic gestures should also be considered when\nturning off a robot. In the experiment, participants engaged in a brief\npreliminary neutral interaction while a robotic arm showed interest in their\nactions. At the end of the task, participants were asked to turn off the\nrobotic arm under two conditions: (1) a Non-designed condition, where all of\nthe robot's engines were immediately and simultaneously turned off, as robots\ntypically shut down; (2) a Designed condition, where the robot's engines\ngradually folded inward in a motion resembling \"falling asleep.\" Our findings\nrevealed that all participants anthropomorphized the robot's movement when it\nwas turned off. In the Non-designed condition, most participants interpreted\nthe robot's turn-off movement negatively, as if the robot had \"died.\" In the\nDesigned condition, most participants interpreted it more neutrally, stating\nthat the robot \"went to sleep.\" The robot's turn-off movement also impacted its\nperception, leading to higher likeability, perceived intelligence, and animacy\nin the Designed condition. We conclude that the impact of common edge\ninteractions, such as turning off a robot, should be carefully designed while\nconsidering people's automatic tendency to perceive robots as social entities."}
{"id": "2509.06466", "pdf": "https://arxiv.org/pdf/2509.06466", "abs": "https://arxiv.org/abs/2509.06466", "authors": ["Erwan Meunier", "Julien M. Hendrickx"], "title": "Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading", "categories": ["math.OC", "cs.AI", "cs.DC", "cs.MA"], "comment": "7 pages, 5 figures. Paper accepted for the 64th IEEE Conference on\n  Decision and Control (2025)", "summary": "We analyze Decentralized Online Optimization algorithms using the Performance\nEstimation Problem approach which allows, to automatically compute exact\nworst-case performance of optimization algorithms. Our analysis shows that\nseveral available performance guarantees are very conservative, sometimes by\nmultiple orders of magnitude, and can lead to misguided choices of algorithm.\nMoreover, at least in terms of worst-case performance, some algorithms appear\nnot to benefit from inter-agent communications for a significant period of\ntime. We show how to improve classical methods by tuning their step-sizes, and\nfind that we can save up to 20% on their actual worst-case performance regret."}
{"id": "2509.06716", "pdf": "https://arxiv.org/pdf/2509.06716", "abs": "https://arxiv.org/abs/2509.06716", "authors": ["Théo Matricon", "Mathieu Acher", "Helge Spieker", "Arnaud Gotlieb"], "title": "Efficiently Ranking Software Variants with Minimal Benchmarks", "categories": ["cs.SE", "cs.PF"], "comment": null, "summary": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability."}
{"id": "2509.05366", "pdf": "https://arxiv.org/pdf/2509.05366", "abs": "https://arxiv.org/abs/2509.05366", "authors": ["Umair Amjid", "M. Umar Khan", "S. A. Manan Kirmani"], "title": "A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "The increasing use of Internet of Things (IoT) devices has led to a rise in\nsecurity related concerns regarding IoT Networks. The surveillance cameras in\nIoT networks are vulnerable to security threats such as brute force and\nzero-day attacks which can lead to unauthorized access by hackers and potential\nspying on the users activities. Moreover, these cameras can be targeted by\nDenial of Service (DOS) attacks, which will make it unavailable for the user.\nThe proposed AI based framework will leverage machine learning algorithms to\nanalyze network traffic and detect anomalous behavior, allowing for quick\ndetection and response to potential intrusions. The framework will be trained\nand evaluated using real-world datasets to learn from past security incidents\nand improve its ability to detect potential intrusion."}
{"id": "2509.05317", "pdf": "https://arxiv.org/pdf/2509.05317", "abs": "https://arxiv.org/abs/2509.05317", "authors": ["Isac Holm"], "title": "VILOD: A Visual Interactive Labeling Tool for Object Detection", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Master's project", "summary": "The advancement of Object Detection (OD) using Deep Learning (DL) is often\nhindered by the significant challenge of acquiring large, accurately labeled\ndatasets, a process that is time-consuming and expensive. While techniques like\nActive Learning (AL) can reduce annotation effort by intelligently querying\ninformative samples, they often lack transparency, limit the strategic insight\nof human experts, and may overlook informative samples not aligned with an\nemployed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)\napproaches integrating human intelligence and intuition throughout the machine\nlearning life-cycle have gained traction. Leveraging Visual Analytics (VA),\neffective interfaces can be created to facilitate this human-AI collaboration.\nThis thesis explores the intersection of these fields by developing and\ninvestigating \"VILOD: A Visual Interactive Labeling tool for Object Detection\".\nVILOD utilizes components such as a t-SNE projection of image features,\ntogether with uncertainty heatmaps and model state views. Enabling users to\nexplore data, interpret model states, AL suggestions, and implement diverse\nsample selection strategies within an iterative HITL workflow for OD. An\nempirical investigation using comparative use cases demonstrated how VILOD,\nthrough its interactive visualizations, facilitates the implementation of\ndistinct labeling strategies by making the model's state and dataset\ncharacteristics more interpretable (RQ1). The study showed that different\nvisually-guided labeling strategies employed within VILOD result in competitive\nOD performance trajectories compared to an automated uncertainty sampling AL\nbaseline (RQ2). This work contributes a novel tool and empirical insight into\nmaking the HITL-AL workflow for OD annotation more transparent, manageable, and\npotentially more effective."}
{"id": "2509.06552", "pdf": "https://arxiv.org/pdf/2509.06552", "abs": "https://arxiv.org/abs/2509.06552", "authors": ["Zheqi Lv", "Wenqiao Zhang", "Kairui Fu", "Qi Tian", "Shengyu Zhang", "Jiajie Su", "Jingyuan Chen", "Kun Kuang", "Fei Wu"], "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing", "categories": ["cs.LG", "cs.CV", "cs.DC", "cs.IR"], "comment": "Published on MM'25: Proceedings of the 33rd ACM International\n  Conference on Multimedia", "summary": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality."}
{"id": "2509.06774", "pdf": "https://arxiv.org/pdf/2509.06774", "abs": "https://arxiv.org/abs/2509.06774", "authors": ["Hridoy Sankar Dutta", "Sana Ansari", "Swati Kumari", "Shounak Ravi Bhalerao"], "title": "OpenCoderRank: AI-Driven Technical Assessments Made Easy", "categories": ["cs.SE"], "comment": null, "summary": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments."}
{"id": "2509.05907", "pdf": "https://arxiv.org/pdf/2509.05907", "abs": "https://arxiv.org/abs/2509.05907", "authors": ["Qianren Li", "Yuncong Hong", "Bojie Lv", "Rui Wang"], "title": "A Dynamic Programming Framework for Vehicular Task Offloading with Successive Action Improvement", "categories": ["eess.SY", "cs.NI", "cs.SY"], "comment": null, "summary": "In this paper, task offloading from vehicles with random velocities is\noptimized via a novel dynamic programming framework. Particularly, in a\nvehicular network with multiple vehicles and base stations (BSs), computing\ntasks of vehicles are offloaded via BSs to an edge server. Due to the random\nvelocities, the exact locations of vehicles versus time, namely trajectories,\ncannot be determined in advance. Hence, instead of deterministic optimization,\nthe cell association, uplink time, and throughput allocation of multiple\nvehicles during a period of task offloading are formulated as a finite-horizon\nMarkov decision process. In order to derive a low-complexity solution\nalgorithm, a two-time-scale framework is proposed. The scheduling period is\ndivided into super slots, each super slot is further divided into a number of\ntime slots. At the beginning of each super slot, we first obtain a reference\nscheduling scheme of cell association, uplink time and throughput allocation\nvia deterministic optimization, yielding an approximation of the optimal value\nfunction. Within the super slot, the actual scheduling action of each time slot\nis determined by making improvement to the approximate value function according\nto the system state. Due to the successive improvement framework, a non-trivial\naverage cost upper bound could be derived. In the simulation, the random\ntrajectories of vehicles are generated from a high-fidelity traffic simulator.\nIt is shown that the performance gain of the proposed scheduling framework over\nthe baselines is significant."}
{"id": "2509.05391", "pdf": "https://arxiv.org/pdf/2509.05391", "abs": "https://arxiv.org/abs/2509.05391", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Schüppstuhl"], "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "categories": ["cs.RO", "cs.HC", "cs.MM"], "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks."}
{"id": "2509.06588", "pdf": "https://arxiv.org/pdf/2509.06588", "abs": "https://arxiv.org/abs/2509.06588", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity", "categories": ["eess.SY", "cs.DC", "cs.SY", "eess.SP", "math.OC"], "comment": "Digital Signal Processing journal", "summary": "This paper considers automatic generation control over an information-sharing\nnetwork of communicating generators as a multi-agent system. The optimization\nsolution is distributed among the agents based on information consensus\nalgorithms, while addressing the generators' ramp-rate-limits (RRL). This is\ntypically ignored in the existing linear/nonlinear optimization solutions but\nthey exist in real-time power generation scenarios. Without addressing the RRL,\nthe generators cannot follow the assigned rate of generating power by the\noptimization algorithm; therefore, the existing solutions may not necessarily\nconverge to the exact optimal cost or may lose feasibility in practice. The\nproposed solution in this work addresses the ramp-rate-limit constraint along\nwith the box constraint (limits on the generated powers) and the\ncoupling-constraint (generation-demand balance) at all iteration times of the\nalgorithm. The latter is referred to as the anytime feasibility and implies\nthat at every termination point of the algorithm, the balance between the\ndemand and generated power holds. To improve the convergence rate of the\nalgorithm we further consider internal signum-based nonlinearity. We also show\nthat our solution can tolerate communication link removal. This follows from\nthe uniform-connectivity assumption on the communication network."}
{"id": "2509.06911", "pdf": "https://arxiv.org/pdf/2509.06911", "abs": "https://arxiv.org/abs/2509.06911", "authors": ["Margarida Ferreira", "Victor Nicolet", "Luan Pham", "Joey Dodds", "Daniel Kroening", "Ines Lynce", "Ruben Martins"], "title": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline."}
{"id": "2509.06550", "pdf": "https://arxiv.org/pdf/2509.06550", "abs": "https://arxiv.org/abs/2509.06550", "authors": ["Jack Wilkie", "Hanan Hindy", "Christos Tachtatzis", "Robert Atkinson"], "title": "Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI", "I.2.6; K.6.5"], "comment": "Published in: Proceedings of IEEE Conference on Cyber Security and\n  Resilience (CSR), 2025. Official version:\n  https://doi.org/10.1109/CSR64739.2025.11129979 Code:\n  https://github.com/jackwilkie/CLAN", "summary": "Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models."}
{"id": "2509.05469", "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design."}
{"id": "2509.05643", "pdf": "https://arxiv.org/pdf/2509.05643", "abs": "https://arxiv.org/abs/2509.05643", "authors": ["Carmine Cesarano", "Roberto Natella"], "title": "FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Coverage-guided fuzzing has been widely applied to address zero-day\nvulnerabilities in general-purpose software and operating systems. This\napproach relies on instrumenting the target code at compile time. However,\napplying it to industrial systems remains challenging, due to proprietary and\nclosed-source compiler toolchains and lack of access to source code. FuzzBox\naddresses these limitations by integrating emulation with fuzzing: it\ndynamically instruments code during execution in a virtualized environment, for\nthe injection of fuzz inputs, failure detection, and coverage analysis, without\nrequiring source code recompilation and hardware-specific dependencies. We show\nthe effectiveness of FuzzBox through experiments in the context of a\nproprietary MILS (Multiple Independent Levels of Security) hypervisor for\nindustrial applications. Additionally, we analyze the applicability of FuzzBox\nacross commercial IoT firmware, showcasing its broad portability."}
{"id": "2509.06626", "pdf": "https://arxiv.org/pdf/2509.06626", "abs": "https://arxiv.org/abs/2509.06626", "authors": ["Jan Matter", "Muoi Tran"], "title": "Network-level Censorship Attacks in the InterPlanetary File System", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "The InterPlanetary File System (IPFS) has been successfully established as\nthe de facto standard for decentralized data storage in the emerging Web3.\nDespite its decentralized nature, IPFS nodes, as well as IPFS content\nproviders, have converged to centralization in large public clouds.\nCentralization introduces BGP routing-based attacks, such as passive\ninterception and BGP hijacking, as potential threats. Although this attack\nvector has been investigated for many other Web3 protocols, such as Bitcoin and\nEthereum, to the best of our knowledge, it has not been analyzed for the IPFS\nnetwork. In our work, we bridge this gap and demonstrate that BGP routing\nattacks can be effectively leveraged to censor content in IPFS. For the\nanalysis, we collected 3,000 content blocks called CIDs and conducted a\nsimulation of BGP hijacking and passive interception against them. We find that\na single malicious AS can censor 75% of the IPFS content for more than 57% of\nall requester nodes. Furthermore, we show that even with a small set of only 62\nhijacked prefixes, 70% of the full attack effectiveness can already be reached.\nWe further propose and validate countermeasures based on global collaborative\ncontent replication among all nodes in the IPFS network, together with\nadditional robust backup content provider nodes that are well-hardened against\nBGP hijacking. We hope this work raises awareness about the threat BGP\nrouting-based attacks pose to IPFS and triggers further efforts to harden the\nlive IPFS network against them."}
{"id": "2509.05547", "pdf": "https://arxiv.org/pdf/2509.05547", "abs": "https://arxiv.org/abs/2509.05547", "authors": ["Ziling Chen", "Yeo Jung Yoon", "Rolando Bautista-Montesano", "Zhen Zhao", "Ajay Mandlekar", "John Liu"], "title": "TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Teleoperation offers a promising solution for enabling hands-on learning in\nremote education, particularly in environments requiring interaction with\nreal-world equipment. However, such remote experiences can be costly or\nnon-intuitive. To address these challenges, we present TeleopLab, a mobile\ndevice teleoperation system that allows students to control a robotic arm and\noperate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,\ncameras, lab equipment for a diverse range of applications, a user interface\naccessible through smartphones, and video call software. We conducted a user\nstudy, focusing on task performance, students' perspectives toward the system,\nusability, and workload assessment. Our results demonstrate a 46.1% reduction\nin task completion time as users gained familiarity with the system.\nQuantitative feedback highlighted improvements in students' perspectives after\nusing the system, while NASA TLX and SUS assessments indicated a manageable\nworkload of 38.2 and a positive usability of 73.8. TeleopLab successfully\nbridges the gap between physical labs and remote education, offering a scalable\nand effective platform for remote STEM learning."}
{"id": "2509.06133", "pdf": "https://arxiv.org/pdf/2509.06133", "abs": "https://arxiv.org/abs/2509.06133", "authors": ["Pradyumna Kaushal"], "title": "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles", "categories": ["cs.CR", "cs.DC", "cs.SE", "cs.SY", "eess.SY", "C.2.4; K.6.5; D.4.6"], "comment": "13 pages, 5 figures. Whitepaper submission; LaTeX source with\n  compiled .bbl. Includes architecture diagrams, tables, and code listings\n  (TypeScript & Solidity)", "summary": "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets."}
{"id": "2509.06640", "pdf": "https://arxiv.org/pdf/2509.06640", "abs": "https://arxiv.org/abs/2509.06640", "authors": ["Yung-Fu Chen", "Sen Lin", "Anish Arora"], "title": "Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "We propose a simple algorithm that needs only a few data samples from a\nsingle graph for learning local routing policies that generalize across a rich\nclass of geometric random graphs in Euclidean metric spaces. We thus solve the\nall-pairs near-shortest path problem by training deep neural networks (DNNs)\nthat let each graph node efficiently and scalably route (i.e., forward) packets\nby considering only the node's state and the state of the neighboring nodes.\nOur algorithm design exploits network domain knowledge in the selection of\ninput features and design of the policy function for learning an approximately\noptimal policy. Domain knowledge also provides theoretical assurance that the\nchoice of a ``seed graph'' and its node data sampling suffices for\ngeneralizable learning. Remarkably, one of these DNNs we train -- using\ndistance-to-destination as the only input feature -- learns a policy that\nexactly matches the well-known Greedy Forwarding policy, which forwards packets\nto the neighbor with the shortest distance to the destination. We also learn a\nnew policy, which we call GreedyTensile routing -- using both\ndistance-to-destination and node stretch as the input features -- that almost\nalways outperforms greedy forwarding. We demonstrate the explainability and\nultra-low latency run-time operation of Greedy Tensile routing by symbolically\ninterpreting its DNN in low-complexity terms of two linear actions."}
{"id": "2509.06069", "pdf": "https://arxiv.org/pdf/2509.06069", "abs": "https://arxiv.org/abs/2509.06069", "authors": ["Alexander Erlei"], "title": "From Digital Distrust to Codified Honesty: Experimental Evidence on Generative AI in Credence Goods Markets", "categories": ["econ.GN", "cs.HC", "q-fin.EC"], "comment": null, "summary": "Generative AI is transforming the provision of expert services. This article\nuses a series of one-shot experiments to quantify the behavioral, welfare and\ndistribution consequences of large language models (LLMs) on AI-AI,\nHuman-Human, Human-AI and Human-AI-Human expert markets. Using a credence goods\nframework where experts have private information about the optimal service for\nconsumers, we find that Human-Human markets generally achieve higher levels of\nefficiency than AI-AI and Human-AI markets through pro-social expert\npreferences and higher consumer trust. Notably, LLM experts still earn\nsubstantially higher surplus than human experts -- at the expense of consumer\nsurplus - suggesting adverse incentives that may spur the harmful deployment of\nLLMs. Concurrently, a majority of human experts chooses to rely on LLM agents\nwhen given the opportunity in Human-AI-Human markets, especially if they have\nagency over the LLM's (social) objective function. Here, a large share of\nexperts prioritizes efficiency-loving preferences over pure self-interest.\nDisclosing these preferences to consumers induces strong efficiency gains by\nmarginalizing self-interested LLM experts and human experts. Consequently,\nHuman-AI-Human markets outperform Human-Human markets under transparency rules.\nWith obfuscation, however, efficiency gains disappear, and adverse expert\nincentives remain. Our results shed light on the potential opportunities and\nrisks of disseminating LLMs in the context of expert services and raise several\nregulatory challenges. On the one hand, LLMs can negatively affect human trust\nin the presence of information asymmetries and partially crowd-out experts'\nother-regarding preferences through automation. On the other hand, LLMs allow\nexperts to codify and communicate their objective function, which reduces\ninformation asymmetries and increases efficiency."}
{"id": "2509.06845", "pdf": "https://arxiv.org/pdf/2509.06845", "abs": "https://arxiv.org/abs/2509.06845", "authors": ["Tom Lauwaerts", "Maarten Steevens", "Christophe Scholliers"], "title": "MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices", "categories": ["cs.PL", "cs.SE"], "comment": "This extended version provides auxiliary material to the article of\n  the same title that will appear in the ACM Digital Library as part of the\n  PACMPL issue for OOPSLA 2025", "summary": "Debugging non-deterministic programs on microcontrollers is notoriously\nchallenging, especially when bugs manifest in unpredictable, input-dependent\nexecution paths. A recent approach, called multiverse debugging, makes it\neasier to debug non-deterministic programs by allowing programmers to explore\nall potential execution paths. Current multiverse debuggers enable both forward\nand backward traversal of program paths, and some facilitate jumping to any\npreviously visited states, potentially branching into alternative execution\npaths within the state space.\n  Unfortunately, debugging programs that involve input/output operations using\nexisting multiverse debuggers can reveal inaccessible program states, i.e.\nstates which are not encountered during regular execution. This can\nsignificantly hinder the debugging process, as the programmer may spend\nsubstantial time exploring and examining inaccessible program states, or worse,\nmay mistakenly assume a bug is present in the code, when in fact, the issue is\ncaused by the debugger.\n  This paper presents a novel approach to multiverse debugging, which can\naccommodate a broad spectrum of input/output operations. We provide the\nsemantics of our approach and prove the correctness of our debugger, ensuring\nthat despite having support for a wide range of input/output operations the\ndebugger will only explore those program states which can be reached during\nregular execution.\n  We have developed a prototype, called MIO, leveraging the WARDuino\nWebAssembly virtual machine to demonstrate the feasibility and efficiency of\nour techniques. As a demonstration of the approach we highlight a color dial\nbuilt with a Lego Mindstorms motor, and color sensor, providing a tangible\nexample of how our approach enables multiverse debugging for programs running\non an STM32 microcontroller."}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164", "abs": "https://arxiv.org/abs/2509.06164", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "title": "Benchmarking Gender and Political Bias in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts."}
{"id": "2509.06864", "pdf": "https://arxiv.org/pdf/2509.06864", "abs": "https://arxiv.org/abs/2509.06864", "authors": ["Ming-I Huang", "Chih-Duo Hong", "Fang Yu"], "title": "Concolic Testing on Individual Fairness of Neural Network Models", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "This paper introduces PyFair, a formal framework for evaluating and verifying\nindividual fairness of Deep Neural Networks (DNNs). By adapting the concolic\ntesting tool PyCT, we generate fairness-specific path constraints to\nsystematically explore DNN behaviors. Our key innovation is a dual network\narchitecture that enables comprehensive fairness assessments and provides\ncompleteness guarantees for certain network types. We evaluate PyFair on 25\nbenchmark models, including those enhanced by existing bias mitigation\ntechniques. Results demonstrate PyFair's efficacy in detecting discriminatory\ninstances and verifying fairness, while also revealing scalability challenges\nfor complex models. This work advances algorithmic fairness in critical domains\nby offering a rigorous, systematic method for fairness testing and verification\nof pre-trained DNNs."}
{"id": "2509.06176", "pdf": "https://arxiv.org/pdf/2509.06176", "abs": "https://arxiv.org/abs/2509.06176", "authors": ["Zsolt Almási", "Hannah Bleher", "Johannes Bleher", "Rozanne Tuesday Flores", "Guo Xuanyang", "Paweł Pujszo", "Raphaël Weuts"], "title": "AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "68T01, 68T20, 91-08, 97U50, 97B10", "I.2.0; K.4.1; K.4.2; K.3.2"], "comment": null, "summary": "As artificial intelligence (AI) systems permeate critical sectors, the need\nfor professionals who can address ethical, legal and governance challenges has\nbecome urgent. Current AI ethics education remains fragmented, often siloed by\ndiscipline and disconnected from practice. This paper synthesizes literature\nand regulatory developments to propose a modular, interdisciplinary curriculum\nthat integrates technical foundations with ethics, law and policy. We highlight\nrecurring operational failures in AI - bias, misspecified objectives,\ngeneralization errors, misuse and governance breakdowns - and link them to\npedagogical strategies for teaching AI governance. Drawing on perspectives from\nthe EU, China and international frameworks, we outline a semester plan that\nemphasizes integrated ethics, stakeholder engagement and experiential learning.\nThe curriculum aims to prepare students to diagnose risks, navigate regulation\nand engage diverse stakeholders, fostering adaptive and ethically grounded\nprofessionals for responsible AI governance."}
{"id": "2509.06221", "pdf": "https://arxiv.org/pdf/2509.06221", "abs": "https://arxiv.org/abs/2509.06221", "authors": ["Vishal Choudhari"], "title": "Beamforming-LLM: What, Where and When Did I Miss?", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing."}
{"id": "2509.06368", "pdf": "https://arxiv.org/pdf/2509.06368", "abs": "https://arxiv.org/abs/2509.06368", "authors": ["Kunlin Cai", "Jinghuai Zhang", "Ying Li", "Zhiyuan Wang", "Xun Chen", "Tianshi Li", "Yuan Tian"], "title": "From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)", "categories": ["cs.CR", "cs.HC"], "comment": "NDSS 2026", "summary": "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges."}
{"id": "2509.06502", "pdf": "https://arxiv.org/pdf/2509.06502", "abs": "https://arxiv.org/abs/2509.06502", "authors": ["Junjie Chen", "Yao Hu", "Junjie Li", "Kangyue Li", "Kun Liu", "Wenpeng Li", "Xu Li", "Ziyuan Li", "Feiyu Shen", "Xu Tang", "Manzhen Wei", "Yichen Wu", "Fenglong Xie", "Kaituo Xu", "Kun Xie"], "title": "FireRedChat: A Pluggable, Full-Duplex Voice Interaction System with Cascaded and Semi-Cascaded Implementations", "categories": ["cs.SD", "cs.HC"], "comment": "12 pages, 2 figures", "summary": "Full-duplex voice interaction allows users and agents to speak simultaneously\nwith controllable barge-in, enabling lifelike assistants and customer service.\nExisting solutions are either end-to-end, difficult to design and hard to\ncontrol, or modular pipelines governed by turn-taking controllers that ease\nupgrades and per-module optimization; however, prior modular frameworks depend\non non-open components and external providers, limiting holistic optimization.\nIn this work, we present a complete, practical full-duplex voice interaction\nsystem comprising a turn-taking controller, an interaction module, and a\ndialogue manager. The controller integrates streaming personalized VAD (pVAD)\nto suppress false barge-ins from noise and non-primary speakers, precisely\ntimestamp primary-speaker segments, and explicitly enable primary-speaker\nbarge-ins; a semantic end-of-turn detector improves stop decisions. It upgrades\nheterogeneous half-duplex pipelines, cascaded, semi-cascaded, and\nspeech-to-speech, to full duplex. Using internal models, we implement cascaded\nand semi-cascaded variants; the semi-cascaded one captures emotional and\nparalinguistic cues, yields more coherent responses, lowers latency and error\npropagation, and improves robustness. A dialogue manager extends capabilities\nvia tool invocation and context management. We also propose three system-level\nmetrics, barge-in, end-of-turn detection accuracy, and end-to-end latency, to\nassess naturalness, control accuracy, and efficiency. Experiments show fewer\nfalse interruptions, more accurate semantic ends, and lower latency approaching\nindustrial systems, enabling robust, natural, real-time full-duplex\ninteraction. Demos: https://fireredteam.github.io/demos/firered_chat."}
{"id": "2509.06582", "pdf": "https://arxiv.org/pdf/2509.06582", "abs": "https://arxiv.org/abs/2509.06582", "authors": ["Carlos A. Pinheiro de Sousa", "Niklas Gröne", "Mathias Günther", "Oliver Deussen"], "title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the Gesellschaft f\\\"ur Informatik (GI) VR/AR Workshop\n  2025 (Lecture Notes in Informatics)", "summary": "We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions."}
{"id": "2509.06770", "pdf": "https://arxiv.org/pdf/2509.06770", "abs": "https://arxiv.org/abs/2509.06770", "authors": ["Shashidhar Reddy Javaji", "Bhavul Gauri", "Zining Zhu"], "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies."}
