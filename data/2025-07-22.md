<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 36]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.NI](#cs.NI) [Total: 25]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LO](#cs.LO) [Total: 9]
- [cs.HC](#cs.HC) [Total: 35]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.AR](#cs.AR) [Total: 7]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.IR](#cs.IR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 11]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 8]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究探讨了代码上下文和提示策略对大型语言模型生成单元测试的影响，结果显示包含文档字符串能显著提升代码充分性，而链式思维提示策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 为提高软件开发阶段的效率，研究如何通过生成式AI自动生成单元测试。

Method: 分析代码上下文和提示策略对多种LLMs生成单元测试质量的影响。

Result: 包含文档字符串显著提升测试充分性，链式思维提示策略效果最好，模型M5表现最优。

Conclusion: 生成式AI在单元测试生成中具有潜力，优化上下文和提示策略可提升效果。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [2] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目研究了如何从非正式需求自动或半自动生成可验证的规格书，结合NLP和大型语言模型等技术。


<details>
  <summary>Details</summary>
Motivation: 从模糊的自然语言需求生成形式化规格书是确保软件正确性的关键挑战，特别是在安全关键系统中。

Method: 项目采用了NLP、领域本体建模、工件重用和大型语言模型等技术。

Result: 初步文献综述确定了生成可验证规格书的常见挑战和未来研究方向。

Conclusion: VERIFAI为自动生成形式化规格书提供了潜在解决方案，仍需进一步研究。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [3] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究探讨了软件开发中沟通问题的技术因素，并验证了通过共享词汇系统提升文档和代码库的效果。


<details>
  <summary>Details</summary>
Motivation: 软件开发中的沟通问题导致误解和低效，亟需技术解决方案。

Method: 采用设计科学研究框架，分三个阶段：问题识别、方法开发和实证验证。

Result: 共享词汇系统显著提高了信息密度、文档清晰度和协作效率。

Conclusion: 研究为软件工程沟通实践提供了可行建议，并指出了未来研究方向。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [4] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究探讨了合并同语义单元子标记的隐藏表示对代码语言模型的影响，提出两种策略：平均表示和学习方法，实验显示可减少计算量且对下游任务性能影响有限。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型的分词输出通常比传统编译器长，导致计算开销增加，因此研究通过合并同语义单元子标记来优化。

Method: 提出两种策略：平均子标记表示和学习方法，集成到现有代码语言模型中，并在六个模型和三个任务上实验。

Result: 计算量减少1%至19%，下游任务中漏洞检测F1分数下降1.82点，代码翻译CodeBLEU提升2.47点。

Conclusion: 合并同语义单元子标记能提升代码语言模型的计算效率和下游性能。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [5] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究通过文献综述统一了架构退化的定义、原因、指标和修复策略，揭示其为技术和社会问题，并指出检测工具多但持续修复不足的研究空白。


<details>
  <summary>Details</summary>
Motivation: 架构退化影响系统质量和可维护性，但现有研究定义和策略分散。研究旨在统一理解架构退化，填补学术和实践中定义、指标及修复策略的空白。

Method: 对108项研究进行多源文献综述，提取定义、原因、指标、方法、工具和修复策略，并构建分类法分析演变、趋势和缺口。

Result: 架构退化被扩展为包含代码违规、设计漂移和社会技术问题。识别了54种指标和31种测量技术，但工具多为检测而非持续修复。

Conclusion: 架构退化需关注技术和组织两方面。研究呼吁整合指标、工具和修复逻辑，推动可持续架构的主动策略。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [6] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 这篇论文研究了五年内八大行业会议中的软件架构趋势，分析了5677个演讲，发现Kubernetes、云原生等技术主导了当前实践。


<details>
  <summary>Details</summary>
Motivation: 了解云计算、微服务和容器等技术兴起对软件架构实践的影响。

Method: 从八大会议中分析5677个演讲，使用大语言模型和专家验证提取技术及其用途。

Result: 发现Kubernetes等技术在部署、通信等领域占主导，并识别了五个技术社区。

Conclusion: 核心技术集中在DevOps后期阶段，研究可为架构设计提供更全面的视角。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [7] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ利用大型语言模型（LLM）解析OpenCV API文档，生成输入值以系统测试API，发现并修复了多个bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV作为广泛使用的开源计算机视觉库，其可靠性至关重要。然而，库中的bug可能影响下游应用，因此需要高效测试方法。

Method: VISTAFUZZ通过LLM解析API文档，提取参数约束和依赖关系，生成输入值以测试OpenCV API。

Result: 在测试330个API时，VISTAFUZZ发现了17个新bug，其中10个被确认，5个已修复。

Conclusion: VISTAFUZZ是一种有效的方法，利用LLM提升OpenCV库的可靠性，展示了文档引导的模糊测试的潜力。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [8] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 本文研究了开源许可证变体在PyPI生态系统中的影响，发现文本变体虽常见但实质性修改仅占2%，但导致了显著的合规问题。作者提出了LV-Parser和LV-Compat工具，显著提升了分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开源许可证变体在软件重用中引入了合规复杂性，但现有工具无法有效应对，因此需要深入研究并提供实用解决方案。

Method: 通过实证研究分析PyPI生态系统中的许可证变体，并开发了基于差异分析和大语言模型的LV-Parser，以及自动检测许可证不兼容性的LV-Compat工具。

Result: 研究发现10.7%的下游依赖存在许可证不兼容问题。LV-Parser准确率达0.936，降低了30%计算成本；LV-Compat识别能力提升5.2倍，精度为0.98。

Conclusion: 本文填补了许可证变体研究的空白，并为开发者提供了高效工具以应对开源许可证的复杂性。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [9] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出了一种名为'Robin's Rule'的确定性算法，为N个条件的奇异布尔表达式（SBEs）直接构建最小测试集（N+1个案例），以实现100%的唯一原因MC/DC覆盖，且效率优于商业工具。


<details>
  <summary>Details</summary>
Motivation: 尽管Unique-Cause MC/DC在确保关键系统可靠性和安全性方面提供最高保障，但其高效测试生成研究不足，而99.7%的航空电子系统中条件决策是SBEs，适合应用Unique-Cause MC/DC。

Method: 提出'Robin's Rule'算法，无需生成完整真值表，直接构造最小测试集（N+1个案例）以保证SBEs的唯一原因MC/DC覆盖。

Result: 通过将TCAS-II规范转化为SBEs构造基准测试，验证了该方法能始终以理论最小测试数实现100%覆盖，效率优于商业工具。

Conclusion: 提供了一种实用且理论最优的解决方案，兼具严格性和高效性，适用于验证安全关键系统。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [10] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）编写Alloy语言中的声明式规范的能力，实验表明LLMs在从自然语言描述合成完整规范、生成等价规范以及完成规范草图方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 声明式规范对开发安全可靠的软件系统至关重要，但正确编写规范仍然具有挑战性。本研究旨在探索LLMs在这一领域的潜力。

Method: 研究通过三个任务评估LLMs：1）从自然语言描述生成完整Alloy公式；2）生成等价Alloy公式；3）完成Alloy规范草图。实验使用了11个已知规范和两个LLM（ChatGPT和DeepSeek）。

Result: 实验结果显示，LLMs在从自然语言或Alloy输入合成完整规范以及完成规范草图方面表现优异，并能生成多个独特解决方案。

Conclusion: LLMs为编写规范提供了新方法，有望在软件开发中提升规范的实用性，并帮助构建更健壮的软件。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [11] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究提出了一种新的方法历史生成工具HistoryFinder，通过系统构建的两个新oracle验证其优于现有工具，并在精度和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 高效准确地重建方法变更历史对软件工程任务至关重要，但现有工具的评估因oracle不准确而受限。

Method: 结合自动化分析和专家验证构建新oracle，开发新工具HistoryFinder，评估400个方法的性能。

Result: HistoryFinder在精度、召回率和F1分数上优于现有工具，并具备高效运行时性能。

Conclusion: HistoryFinder是兼顾准确性和效率的最佳选择，提供了多种使用方式以促进采用。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [12] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 研究探讨了结合超参数调优和提示工程如何提升Llama 3.1模型在从文本描述生成领域模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在领域建模中效率不足，需探索更优方法。

Method: 采用搜索方法调优超参数，并在多领域测试效果。

Result: 优化后的模型在几乎所有测试领域中表现更佳。

Conclusion: 超参数调优与提示工程结合可显著改善模型输出质量。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [13] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGTs）对不同性别开发者使用效果的影响，旨在揭示性别差异并推动工具设计的公平性。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成工具的普及，其公平性和包容性问题日益凸显，但现有研究未充分探究工具对不同用户群体的效果差异。

Method: 采用混合实验设计，54名性别均衡的参与者完成编程任务，数据包括认知负荷、任务表现及工具交互行为，并进行统计分析。

Result: 预期揭示CGTs使用中的性别差异，为工具设计和包容性AI实践提供依据。

Conclusion: 研究为提升CGTs的公平性、透明性和伦理设计奠定基础，推动面向所有用户的平等工具开发。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [14] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt 是一个利用角色分工和 PPA 优化的框架，使得大模型生成的 Verilog 代码满足工业级设计的 PPA 要求。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型在硬件设计中的应用主要关注功能正确性，忽视了关键的 PPA 指标，无法满足工业级需求。

Method: 通过角色分工（Planner、Programmer 等）模拟人类设计流程，并利用 PPA 约束优化提示管道，结合多模态反馈实现高效代码生成。

Result: 实验显示 VeriOpt 在功耗、面积和时序上有显著改进，功能测试成功率达 86%。

Conclusion: VeriOpt 解决了功能正确性与设计质量之间的鸿沟，为大模型在工业设计中的应用铺平道路。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [15] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope利用调用链感知的多视图上下文解决仓库级代码生成中上下文相关性不足的问题，通过结构语义图和四视图上下文提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别仓库的丰富语义和结构关系时效果不佳，导致生成的代码质量受限。

Method: 构建结构语义图（RSSG）和四视图上下文，结合静态分析和调用链预测，优化提示构造。

Result: 在CoderEval和DevEval基准测试中，pass@1分数相对提升36.35%。

Conclusion: RepoScope高效且通用，无需额外训练，显著提升代码生成质量。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [16] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: 本文介绍了RequireCEG，一种结合因果效应图和神经符号协作架构的需求引导与自检代理，旨在解决自然语言需求的歧义性，并在生成式软件开发中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为缺乏软件工程知识的终端用户提供全生命周期控制，解决自然语言需求的模糊性问题。

Method: 利用因果效应图（CEGs）和神经符号协作架构，通过特征树分析用户叙述，构建自愈CEGs，优化Gherkin场景。

Result: 实验使用RGPair数据集，覆盖率达87%，多样性提升51.88%。

Conclusion: RequireCEG能够有效解决需求模糊性，提升生成式软件开发的一致性和多样性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [17] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文介绍了AIDev，首个大规模数据集，记录五种AI编码代理在真实开发环境中的合作情况，为研究AI在软件工程中的角色提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理成为软件工程的新伙伴，需要真实数据来研究其实际表现和与人类开发者的协作效果。

Method: AIDev数据集收集了456,000个由五种主流AI代理生成的代码拉取请求（PRs），涵盖了61,000个仓库和47,000名开发者，包含丰富的元数据。

Result: AI代理在速度上优于人类，但PR接受率较低，且提交的代码结构更简单，揭示了信任与效用差距。

Conclusion: AIDev为研究AI原生工作流和优化人机协作提供了开放数据，将成为SE 3.0发展的重要资源。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [18] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 探讨GenAI在汽车软件开发中的应用，重点关注需求处理、合规性和代码生成，并提出通用工作流程。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发流程繁琐且昂贵，GenAI有望减少人力和复杂度。

Method: 综述LLMs、RAG和VLMs等技术，并提出工作流程，结合行业调查。

Result: 总结GenAI工具在行业中的实际应用情况。

Conclusion: GenAI在汽车软件开发中具有潜力，可优化流程和效率。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [19] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 该论文探讨了如何利用大语言模型（LLM）在敏捷框架中自动化需求获取，并评估了LLM生成用户故事（US）的质量及其语义质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，传统方法在语义质量评估上耗时且依赖于人工。研究旨在通过LLM提高自动化水平和效率。

Method: 使用10种先进的LLM模拟客户访谈生成US，并将其与人类生成的US（来自领域专家和学生）进行比较，同时探索LLM对US语义质量的自动评估能力。

Result: LLM生成的US在覆盖范围和风格质量上与人类相似，但在多样性和创造性上较弱。LLM生成的US质量与人类相当，但在满足接受标准上表现较差。LLM在提供明确评估标准时可可靠评估US的语义质量。

Conclusion: LLM可以辅助自动化需求获取和语义质量评估，但仍需改进多样性和创造性，同时明确评估标准是其有效应用的关键。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [20] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: 提出了一个名为DLMMM的新方法，用于改善深度学习框架测试的有效性，通过融合多种模型测量指标来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法在检测深度学习框架错误时存在三个主要问题：未量化算子的组合多样性、忽略模型执行时间、未考虑不同测量指标的相关性。

Method: DLMMM通过量化模型的错误检测性能、算子组合多样性和执行时间，并基于相关性融合这些指标，实现其权衡。此外，设计了多层次的启发式引导来生成测试输入模型。

Result: DLMMM能够更全面地检测框架错误，提升测试效果。

Conclusion: DLMMM是首个将多模型测量指标融合的测试方法，显著提升了深度学习框架测试的有效性。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [21] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉文化对需求工程（RE）活动的影响，旨在帮助从业者避免文化误解，并促进IT行业的多样性。


<details>
  <summary>Details</summary>
Motivation: 需求工程是软件开发中最需要互动的阶段，文化多样性可能导致误解和冲突。孟加拉IT行业发展迅速但研究不足。

Method: 研究调查孟加拉文化如何影响需求工程的采纳和执行。

Result: （未在摘要中明确提及具体结果）

Conclusion: 了解文化影响有助于优化需求工程活动，支持IT行业的多样性和包容性。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [22] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 该论文通过系统映射研究（SMS）探讨了需求工程（RE）中角色的最新研究趋势，尤其关注生成式AI对角色构建和验证的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解生成式AI等新兴技术如何影响角色在需求工程中的应用，以及近年来的研究趋势变化。

Method: 采用系统映射研究方法，分析了2023年4月至2025年4月期间的22篇相关文献，涵盖角色表示、构建、验证及RE活动。

Result: 研究发现，基于AI的角色构建和验证方法增多，模板化角色更受欢迎，验证相关研究比例上升。

Conclusion: 生成式AI对角色构建和验证产生了显著影响，未来研究可进一步探索AI在RE中的应用潜力。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [23] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench 是首个专门为 SIMD 指令代码生成设计的基准测试，包含 136 项任务，评估了 18 种大型语言模型在生成高性能 SIMD 代码时的表现。


<details>
  <summary>Details</summary>
Motivation: 研究 SIMD 指令编程在 LLMs 代码生成中的应用，填补现有基准测试中缺少 SIMD 代码评估的空白。

Method: 设计了 SimdBench 基准测试，包含 136 项任务，覆盖五种 SIMD 指令集，并对 18 种 LLMs 进行了系统评估。

Result: LLMs 在生成 SIMD 代码时的表现普遍不如标量代码，但研究提出了改进方向。

Conclusion: SimdBench 为 LLMs 在 SIMD 代码生成领域的进一步研究提供了基础，代码已开源。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [24] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: 本文提出了一种名为AlphaCC的代码克隆检测方法，灵感来自AlphaFold的成功，通过将代码片段表示为标记序列，并利用多序列对齐和注意力编码器来捕捉代码语义。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法难以捕捉代码语义或依赖特定语言分析器。受到AlphaFold预测蛋白质结构的启发，认为蛋白质序列与代码标记序列具有相似的线性结构。

Method: 将代码片段转为标记序列，构建多序列对齐（MSA），使用修改后的注意力编码器建模依赖关系，通过后交互策略计算相似度并进行二元分类。

Result: 在多语言数据集上表现优越，尤其在语义克隆检测任务中优于所有基线，同时保持了高效的运行效率。

Conclusion: AlphaCC在多语言环境下具有强语义理解能力，适用于大规模代码克隆检测任务。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [25] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: 论文提出了一种名为FaultLine的LLM代理工作流，用于自动生成漏洞证明测试（PoV），显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞报告中常缺乏验证补丁所需的PoV测试，导致难以确保修复效果和防止回归问题。

Method: FaultLine通过结合静态和动态分析的推理步骤，追踪漏洞源到汇的路径，并生成PoV测试用例。

Result: 在100个已知漏洞的数据集上，FaultLine成功生成了16个项目的PoV测试，比现有技术提高了77%。

Conclusion: 分层推理能提升LLM代理在PoV测试生成中的表现，但问题仍具挑战性，需进一步研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [26] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix是一种基于LLM的自动程序修复（APR）方法，通过自动缩减测试输入来避免长提示中的“中间丢失”问题，显著提升修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在长提示中容易丢失关键信息（如测试输入），影响修复性能，需解决这一问题。

Method: 提出ReduceFix方法，通过LLM生成缩减器，自动最小化失败诱导的测试输入，保留其关键行为。

Result: 在LFTBench基准测试中，ReduceFix平均缩减输入89.1%，修复成功率提高53.8%，且优化其他方法的修复率21.3%。

Conclusion: 自动缩减失败输入是LLM-based APR的实用且强效补充，显著提升其可扩展性和有效性。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [27] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 论文探讨了工具代理范式中参数失败问题，提出了分类和改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究工具代理范式中参数失败的根源，以提高LLM在复杂任务中的效率和可靠性。

Method: 构建参数失败分类体系，通过15种输入扰动方法分析输入源与失败类型的相关性。

Result: 参数名称幻觉失败主要由LLM固有局限引起，其他失败模式则与输入源问题相关。

Conclusion: 建议标准化工具返回格式、改进错误反馈机制和确保参数一致性，以提升工具代理互动的可靠性和有效性。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [28] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出了一种名为StackTrans的新方法，通过引入隐状态栈来增强Transformer架构，解决了其在捕获乔姆斯基层次结构（如正则表达式和确定性上下文无关文法）方面的局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在大型语言模型（LLMs）中表现出色，但在处理乔姆斯基层次结构时存在固有局限性。

Method: StackTrans在Transformer层之间显式引入隐状态栈，支持可微分和端到端学习的栈操作，同时保持与现有框架的兼容性。

Result: StackTrans在乔姆斯基层次结构和大规模自然语言任务的基准测试中均优于标准Transformer和其他基线模型，且参数规模可扩展。

Conclusion: StackTrans通过引入栈操作显著提升了模型的效率和推理能力，展示了其在LLMs中的潜力。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [29] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 本文提出了一种“Chinese Wall”技术，通过强模型为弱模型生成详细指令，提升弱模型在复杂任务中的表现，但其应用受限于缺乏无版权限制的公开训练数据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决代码大语言模型训练数据版权问题，同时提升这些模型的实用性。

Method: 采用“Chinese Wall”技术，利用高质量模型为伦理对齐的弱模型生成指令，提升其性能。

Result: 该技术使Comma v0.1 1T在CanItEdit基准测试中性能提升66%，Starcoder2 Instruct提升约20%。

Conclusion: 尽管技术有效，但其实际应用仍受限于无版权公开训练数据的不足。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [30] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 本研究通过分析Stack Overflow上的React相关问题，揭示了用户最常讨论的关键词、错误分类及基于用户声誉的错误分布。


<details>
  <summary>Details</summary>
Motivation: 尽管React在Web开发中广受欢迎，但用户面临的具体挑战尚不明确，本研究旨在填补这一空白。

Method: 采用探索性数据分析方法，研究React相关问题中的关键词频率、错误分类和用户声誉与错误的关系。

Result: 最常讨论的关键词包括code、link等；算法错误是最常见问题，中等声誉用户贡献最多（55.77%）。

Conclusion: 研究结果为未来支持React社区的早期实施提供了宝贵见解，建议社区提供更多算法问题解决指导。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [31] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion通过优化超参数和提示结构，显著减少Stable Diffusion模型中的性别和种族偏见及能耗，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion模型在社会和环境可持续性方面的负面影响。

Method: 基于搜索的方法，优化超参数和提示结构以减少偏见和能耗。

Result: 性别偏见减少68%，种族偏见减少59%，能耗降低48%。

Conclusion: 无需调整模型架构即可提升模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [32] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 比较了CubeSat卫星电池放电的等效电路分析和机器学习建模两种方法，机器学习模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为CubeSat卫星电池放电建模选择合适方法，以提高故障预测和设备容错能力。

Method: 使用轨道数据样本，分析电压、电流和温度数据，比较等效电路和机器学习两种建模方法。

Result: 等效电路透明但灵活性不足；机器学习能适应复杂依赖，预测更准确。

Conclusion: 机器学习更适合复杂环境下的CubeSat电池放电建模。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [33] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一个基于大型语言模型的多智能体系统，通过从示例中学习并模拟人类审计行为，提高软件缺陷检测的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具覆盖范围有限，难以适应复杂和定制化的缺陷模式，而现有的基于LLM的方法在处理高级缺陷时依然存在不足。

Method: BugScope利用LLM从示例中学习缺陷模式，通过程序切片提取相关检测上下文，并构建定制化的提示引导LLM进行准确推理。

Result: 在40个真实缺陷的数据集上，BugScope达到87.04%的精度和90.00%的召回率，F1分数超过工业标准工具0.44；在Linux内核等大规模系统中还发现了141个未知缺陷。

Conclusion: BugScope在缺陷检测中表现出显著的实用性和高效性，能够发现并修复复杂缺陷，具有重要的实际应用价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [34] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLM）在自动程序修复（APR）中的实际效果，并通过实验评估其对程序员调试的帮助。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术（尤其是LLM）如何在软件工程中实现自动程序修复，并验证其实际效果与程序员的使用方式。

Method: 采用实验方法，将程序员分为两组（一组使用LLM，另一组不使用），通过形式化验证工具评估修复效果。

Result: 实验结果出乎意料，揭示了LLM在调试中的实际作用，并提出了7种LLM使用模式。

Conclusion: 研究为AI和LLM在程序修复中的合理应用提供了初步结论，并提出了有效使用LLM的建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [35] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 论文探讨了利用基于RAG的LLM工具自动生成证据简报的可行性，旨在比较其与人工简报在内容保真度、易理解性和实用性上的表现。


<details>
  <summary>Details</summary>
Motivation: 证据简报对软件工程领域有价值，但人工制作成本高，因此研究自动生成方法的可行性。

Method: 开发了基于RAG的LLM工具生成简报，设计实验与人工简报对比评估。

Result: 实验结果待公布。

Conclusion: 结论将依据实验结果而定。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [36] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 该研究填补了数据科学中计算笔记本动态开发过程的研究空白，通过工具集收集Jupyter笔记本的代码变更数据，并分析了开发行为模式。


<details>
  <summary>Details</summary>
Motivation: 数据科学领域缺乏对计算笔记本开发过程的研究，而软件工程中对细粒度日志的分析已取得显著成果。

Method: 开发工具集收集20名开发者在数据分析和机器学习任务中的代码变更，形成含2655个单元格和9207次执行的数据库。

Result: 分析显示，笔记本常用于小规模修复和代码迭代，不仅是开发和探索工具，也用于调试。

Conclusion: 研究发现笔记本的多功能用途，并提出了未来研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [37] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: 论文介绍了NPUEval基准测试，用于评估LLM生成的NPU内核代码的功能正确性和向量化效率，揭示了当前前沿模型的局限性。


<details>
  <summary>Details</summary>
Motivation: NPU编程社区分散且缺乏优化代码示例，LLM辅助编写高效内核面临挑战，需标准化评估工具。

Method: 提出NPUEval基准，包含102个常见运算符，使用开源编译器工具在AMD NPU上评估LLM生成代码。

Result: 前沿模型如DeepSeek R1在部分内核上实现50%+向量化，但整体平均仅10%，显示任务难度大。

Conclusion: NPUEval为代码生成和NPU优化研究提供基准，模型性能仍有提升空间。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [38] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种名为Timetide的新多时钟语义同步语言，解决了分布式系统确定性编程的挑战，无需物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 现有的确定性模型主要集中于集中式应用，而适用于分布式的语言依赖昂贵且难以扩展的物理时钟同步，确定性的分布式编程仍具挑战性。

Method: 开发了一种新的多时钟语义同步程序语义，并构建了基于逻辑同步模型的编程语言Timetide，避免了物理时钟同步需求。

Result: Timetide成为首个既适合分布式又支持形式化验证的多时钟同步语言，无需物理时钟同步或时钟门控。

Conclusion: Timetide为解决分布式系统的确定性编程提供了新途径，具有较高的实用性和理论价值。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [39] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 为Python开发了一款语音辅助调试插件，将静默运行时错误转化为可操作的声音诊断，结合听觉与视觉反馈，显著降低认知负荷并加速错误识别。


<details>
  <summary>Details</summary>
Motivation: 提升编程调试的效率和可访问性，特别是为视觉障碍者和初学者提供更友好的错误诊断方式。

Method: 采用全局异常钩子架构，结合pyttsx3语音合成和Tkinter GUI可视化，提供多模态错误反馈。

Result: 实验显示认知负荷降低37%，错误识别速度提高78%，且系统兼容性强，延迟低。

Conclusion: 该插件为人本错误诊断开创了新范式，未来拟结合GPT和多语言翻译进一步增强功能。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [40] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 提出了一种基于约束求解方法的理论框架，用于生成浮点程序在误差扰动下的紧不变式，并通过实验验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 浮点运算的累积误差可能导致程序失败，因此需要一种方法来生成考虑误差的不变式，以确保程序的正确性。

Method: 结合FPTaylor的一阶微分特征与约束求解方法，提出两种多项式不变式生成算法，一种需要初始不变式，另一种适用于多项式程序。

Result: 实验表明，所提算法在时间效率和生成不变式的精度上均优于现有方法。

Conclusion: 该框架有效解决了浮点程序的不变式生成问题，尤其适用于处理条件分支等复杂情况。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [41] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种称为‘多版本化’的技术，通过自动生成多个代码变体，实现高性能的可移植代码，避免了传统自动调优的‘过拟合’问题，并在新设备上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前GPU设备上手动优化线性代数内核复杂且耗时，自动调优虽能提高性能但容易‘过拟合’，且需要重新调优以适应环境变化。论文旨在解决这一问题。

Method: 采用多版本化技术，生成多个代码变体，并使用‘可移植性调优’框架自动选择最佳版本，实现高性能的可移植性。

Result: 在CLBlast库的GEMM内核测试中，该技术优于默认内核，接近理论最大性能的90%，且在新设备上表现优异。

Conclusion: 多版本化和可移植性调优是实现高性能代码可移植性的有效方法，适用于不同设备和环境。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [42] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: 论文介绍了Bayesian分离逻辑（BaSL），一种能够处理贝叶斯更新的概率分离逻辑，填补了现有分离逻辑无法处理BPPL关键特性的空白。


<details>
  <summary>Details</summary>
Motivation: 现有概率分离逻辑无法处理贝叶斯编程语言（BPPL）中的贝叶斯更新特性，限制了对其性质的推理能力。

Method: 提出BaSL，基于Rokhlin-Simmons定理证明贝叶斯定理的内部版本，并利用σ-有限测度空间构建Kripke资源幺半群模型。

Result: BaSL成功建模了贝叶斯编程的关键概念，并验证了多个统计模型的属性，如期望值、相关性和后验分布。

Conclusion: BaSL为BPPL提供了新的语义框架，扩展了概率分离逻辑的应用范围，并支持对复杂统计模型的推理。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [43] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 本文提出了一种统一的形式化框架，用于分析PLC驱动的系统，整合了离散PLC语义、网络通信和连续物理行为，并采用偏序归约技术解决了状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有形式化验证技术往往忽略PLC程序与物理环境和网络通信的交互，无法有效分析实际工业系统中的连续动态和通信延迟问题。

Method: 开发了一种统一的形式化框架，整合离散PLC语义、网络通信和连续物理行为，并使用偏序归约技术减少状态空间。

Result: 该框架能够精确分析具有连续动态和网络通信的PLC驱动系统，同时显著减少状态探索数量。

Conclusion: 提出的框架为复杂工业系统中的PLC程序验证提供了更全面的方法，解决了现有技术的局限性。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [44] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 本文研究了闭包转换与抽象机器中闭包和环境的关系，提出了一种新的闭包转换正确性证明技术，改进了抽象机器中环境处理的方式，并分析了时间复杂度的变化。


<details>
  <summary>Details</summary>
Motivation: 探讨闭包转换与抽象机器中闭包和环境的异同，并通过研究其在简单λ-演算中的应用，提出改进方法。

Method: 采用带元组的简单λ-演算作为源语言，设计并分析源语言和目标语言中的抽象机器，重点研究平面闭包/环境的情况。

Result: 提出新的闭包转换正确性证明方法，改进环境处理方式，并证明闭包转换虽改变动态成本和初始代码大小，但整体复杂度保持不变。

Conclusion: 闭包转换与抽象机器中的环境处理可以结合，形成更高效的实现方式，同时保持时间复杂度不变。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [45] [Iran's Stealth Internet Blackout: A New Model of Censorship](https://arxiv.org/abs/2507.14183)
*Arash Aryapour*

Main category: cs.NI

TL;DR: 伊朗在2025年中期实施了一种新型隐蔽的互联网关闭手段，通过深度包检测等手段隔离国内用户，同时保持全球路由存在。论文通过测量分析了DNS投毒等技术，并量化了VPN需求的激增。


<details>
  <summary>Details</summary>
Motivation: 研究伊朗的新型互联网关闭策略及其对用户的影响，为数字权利监控和规避技术提供见解。

Method: 通过主动网络测量（如DNS投毒、HTTP注入等）分析伊朗的互联网关闭策略，并追踪到集中式边界网关。

Result: 研究发现VPN需求增加了约707%，并揭示了多层审查基础设施的结构。

Conclusion: 论文强调了伊朗新型互联网关闭手段对规避技术和数字权利监控的深远影响。

Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that
preserved global routing presence while isolating domestic users through deep
packet inspection, aggressive throttling, and selective protocol blocking. This
paper analyzes active network measurements such as DNS poisoning, HTTP
injection, TLS interception, and protocol whitelisting, traced to a centralized
border gateway. We quantify an approximate 707 percent rise in VPN demand and
describe the multi-layered censorship infrastructure, highlighting implications
for circumvention and digital rights monitoring.

</details>


### [46] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 论文提出了一种结合专家知识特征压缩和分离表示学习的双策略方法，用于解决低空网络覆盖预测中的数据稀疏问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 低空经济的发展需要准确的低空网络覆盖预测，但基站天线波束模式通常不公开，且低空路测数据稀缺，导致预测面临特征采样不平衡和泛化能力不足的挑战。

Method: 采用了专家知识特征压缩和分离表示学习的双策略。前者通过通信专业知识降低特征空间复杂度，后者通过结合传播模型和子网络增强模型泛化能力。

Result: 实验显示该方法比基线算法误差降低7%，实际网络验证中MAE误差达到5dB水平，证明了其可靠性和实用性。

Conclusion: 该框架成功解决了低空网络覆盖预测中的数据稀疏问题，显著提升了预测精度和泛化能力。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [47] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 本文提出了一个完全基于轨道的移动网络系统架构，探讨了其在密集城市中的可行性，并展示了未来的发展路线图。


<details>
  <summary>Details</summary>
Motivation: 解决当前卫星移动网络依赖地面核心的局限，探索完全基于轨道的移动网络在密集城市中的可行性。

Method: 设计了一个端到端的轨道电信系统，包括电子控制相控阵、5G核心功能部署和激光网状回程，分析了频谱效率和链路预算。

Result: 模拟显示屋顶和视距用户可维持64-QAM吞吐量，街级接入需中继或辅助波束模式，未来可提供50-100 Mbps的手持设备连接。

Conclusion: 完全轨道移动网络的工程瓶颈可通过技术解决，未来15年有望实现自主覆盖，不依赖地面基础设施。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [48] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: 本文介绍了P4TG中基于直方图的RTT测量功能，解决了时间采样导致的精度降低问题，并通过范围到前缀的转换算法实现了硬件高效匹配。


<details>
  <summary>Details</summary>
Motivation: P4TG在数据平面采样RTT等时间指标会导致精度下降，因此需要一种无需采样的高精度测量方法。

Method: 使用直方图模型，并采用范围到前缀的转换算法在TCAM中实现高效的硬件匹配。

Result: 评估结果表明，直方图RTT分析与配置的理论分布一致，验证了其适用性。

Conclusion: 直方图方法在P4TG中实现了高精度的RTT测量，并展示了硬件匹配的效率。

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [49] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 论文提出了一种在资源受限环境中实现语义通信的新方法，通过将语义图像分割任务分配给发送端和接收端，显著降低了带宽和计算需求，同时保持了分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决语义通信中带宽与计算效率的平衡问题，特别是在资源受限和信道条件变化的环境中。

Method: 将语义图像分割过程分到发送端和接收端，减少传输数据量和发送端计算负载。

Result: 传输比特率降低72%，发送端计算负载减少19%以上。

Conclusion: 该方法在6G等通信系统中有广泛应用前景。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [50] [White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective](https://arxiv.org/abs/2507.14209)
*Rute C. Sofia,Hao Shen,Yuanting Liu,Severin Kacianka,Holger Pfeifer*

Main category: cs.NI

TL;DR: fortiss提出了一种以人为本、可持续且集成AI的6G网络愿景，强调社会需求与技术发展的结合。


<details>
  <summary>Details</summary>
Motivation: 确保6G技术不仅实现技术突破，还要符合社会需求，推动认知智能、去中心化编排和可持续架构的发展。

Method: 通过软件定义、AI驱动和可持续的通信服务，聚焦语义通信、绿色编排和分布式AI等研究领域。

Result: 6G将超越前几代技术，实现超可靠低延迟通信和个性化数字服务，同时注重能源效率和长期社会影响。

Conclusion: fortiss致力于通过多学科合作和负责任的创新，推动6G技术发展，以实现2030年的战略愿景。

Abstract: As a leading research institute in software-intensive systems, fortiss is
actively shaping the vision of Sixth Generation Mobile Communication (6G). Our
mission is to ensure that 6G technologies go beyond technical advancements and
are aligned with societal needs. fortiss plays a key role in 6G initiatives
worldwide, including contributions to standardization bodies and collaborative
Research and Development programs. We focus on software-defined, AI-enabled,
and sustainable communication services that prioritize human values and
long-term impact. 6G will redefine digital connectivity through cognitive
intelligence, decentralized orchestration, and sustainability-oriented
architectures. As expectations rise for ultra-reliable low-latency
communication (URLLC) and personalized digital services, 6G must outperform
prior generations. It will rely on AI-native networking, Edge-Cloud resource
orchestration, and energy-aware data frameworks, ensuring both technical
performance and societal relevance. This white paper presents the fortiss
vision for a human-centric, sustainable, and AI-integrated 6G network. It
outlines key research domains such as semantic communication, green
orchestration, and distributed AI, all linked to societal and technological
challenges. The white paper is aimed at researchers, industry experts,
policymakers, and developers. It articulates the strategic direction and
contributions of fortiss to 6G, emphasizing responsible innovation and
interdisciplinary collaboration toward a meaningful 2030 vision.

</details>


### [51] [A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming](https://arxiv.org/abs/2507.14205)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 提出了一种结合SDWMN、D2M广播和Kafka混合云流量的集成架构，显著提升了城乡网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决城市网络拥堵和农村数字鸿沟问题，实现流量卸载、资源公平分配和增强容错能力。

Method: 通过数学建模（如城市拥塞和农村覆盖赤字）和实验验证（在曼谷、孟买和芬兰农村进行测试）。

Result: 实验显示延迟降低32%，带宽卸载40%，农村覆盖提升28%，恢复时间低于10秒。

Conclusion: 建议优化频谱分配和政策支持，以推广该架构，支持公平数字化转型。

Abstract: We propose an integrated architecture combining Software-Defined Wireless
Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based
hybrid cloud streaming to improve wireless network performance in both urban
and rural settings. The approach addresses urban congestion and rural digital
exclusion through traffic offloading, enhanced fault tolerance, and equitable
resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and
rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize
global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3
\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,
Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth
offloading of 40%, rural coverage gain of 28%, and fairness index rising from
0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We
recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and
device mandates to promote adoption. This scalable, fault-tolerant design
supports equitable digital transformation and suggests directions for future
research.

</details>


### [52] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA是一个基于AI的预测性QoS模拟框架，用于优化远程驾驶应用中的QoS决策，结合RL方法显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶对延迟和可靠性有严格要求，预测性QoS能避免性能下降，而强化学习是实现这一目标的理想工具。

Method: PRATA框架包括5G RAN模拟、车辆数据生成工具和AI优化单元，通过RL设计分段优化策略（RAN-AI）。

Result: RAN-AI平衡了QoS和QoE，系统性能比基线方法提升近一倍，同时分析了状态空间和数据采集成本的影响。

Conclusion: PRATA框架有效优化远程驾驶的QoS，展示了RL在预测性QoS中的潜力。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [53] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 论文提出了一种基于LLM的意图网络自动化方法，用于优化无线接入网络管理，通过智能闭环机制提高能效。


<details>
  <summary>Details</summary>
Motivation: 面对无线网络管理的复杂性，利用LLM实现意图翻译和自动化配置是一种创新解决方案。

Method: 采用基于LLM的代理架构，通过结构化提示工程和实时反馈闭环机制优化RAN参数。

Result: 该方法能动态优化网络能效，展示出在RAN资源管理中适应实时变化的能力。

Conclusion: LLM和代理架构的结合为无线网络自动化管理提供了高效且可靠的新途径。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [54] [Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting](https://arxiv.org/abs/2507.14234)
*Samer Nasser,Henrique Duarte Moura,Dragan Subotic,Ritesh Kumar Singh,Maarten Weyn,Jeroen Famaey*

Main category: cs.NI

TL;DR: 本文提出了一种结合太阳能和动能采集的能量中性系统，用于野生动物追踪和监测。通过多源能量采集和NB-IoT通信技术，该系统提高了数据产量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 长期野生动物追踪对生物多样性监测至关重要，但能源限制成为难题。传统电池更换不切实际且对动物有害，而现有系统多依赖单一能源。

Method: 系统结合太阳能和动能采集，动能采集器还用作运动代理。采用NB-IoT通信和基于能量的任务调度器。

Result: 仿真表明，系统在能量中性操作下，显著提高了数据产量和可靠性，如每两分钟采样一次GPS和动能数据，每小时通过NB-IoT传输。

Conclusion: 该系统展示了在偏远栖息地实现免维护、环保的野生动物追踪潜力，为高效、可扩展的监测提供了可能。

Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but
energy limitations pose challenges, especially for animal tags, where replacing
batteries is impractical and stressful for the animal due to the need to
locate, possibly sedate, and handle it. Energy harvesting offers a sustainable
alternative, yet most existing systems rely on a single energy source and
infrastructure-limited communication technologies. This paper presents an
energy-neutral system that combines solar and kinetic energy harvesting to
enable the tracking and monitoring of wild animals. Harvesting from multiple
sources increases the total available energy. Uniquely, the kinetic harvester
also serves as a motion proxy by sampling harvested current, enabling activity
monitoring without dedicated sensors. Our approach also ensures compatibility
with existing cellular infrastructure, using Narrowband Internet of Things
(NB-IoT). We present a simulation framework that models energy harvesting,
storage, and consumption at the component level. An energy-aware scheduler
coordinates task execution based on real-time energy availability. We evaluate
performance under realistically varying conditions, comparing task frequencies
and capacitor sizes. Results show that our approach maintains energy-neutral
operation while significantly increasing data yield and reliability compared to
single-source systems, with the ability to consistently sample GPS location
data and kinetic harvesting data every two minutes while transmitting these
results over NB-IoT every hour. These findings demonstrate the potential for
maintenance-free, environmentally friendly tracking in remote habitats,
enabling more effective and scalable wildlife monitoring.

</details>


### [55] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA架构为AI代理提供了一种轻量级、可扩展的发现与认证方案，支持快速解析、隐私保护和跨组织协作。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理数量激增，传统的DNS为中心的身份发现机制面临挑战，亟需新的解决方案。

Method: 提出NANDA索引架构，基于动态可验证的AgentFacts，支持多端点路由、隐私保护和能力声明。

Result: 实现了快速全局解析、秒级撤销、隐私保护等功能，原型验证了其轻量级和可扩展性。

Conclusion: NANDA为下一代AI代理互联网提供了安全、可信的高效协作基础，兼容现有网络设施。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [56] [NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation](https://arxiv.org/abs/2507.14398)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: 论文提出了IBNBench基准测试套件和NetIntent框架，用于评估和整合大语言模型（LLM）在意图网络（IBN）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 意图网络（IBN）在简化网络管理方面潜力巨大，但现有解决方案缺乏灵活性和适应性。LLM在自然语言理解和推理方面的优势使其成为解决这一问题的潜在工具。

Method: 研究团队开发了IBNBench基准测试套件，包含四个数据集，用于评估33个开源LLM在意图翻译和冲突检测任务中的表现。随后提出了NetIntent框架，整合LLM和非LLM代理，实现意图的自动翻译、激活和保障。

Result: 实验结果显示LLM在孤立IBN任务中表现不一，而NetIntent框架在ODL和ONOS SDN控制器上实现了稳定且自适应的端到端IBN。

Conclusion: LLM在IBN任务中展现出潜力，但需要NetIntent这样的框架来整合其能力，以实现完全自主的IBN流程。

Abstract: Intent-Based Networking (IBN) often leverages the programmability of
Software-Defined Networking (SDN) to simplify network management. However,
significant challenges remain in automating the entire pipeline, from
user-specified high-level intents to device-specific low-level configurations.
Existing solutions often rely on rigid, rule-based translators and fixed APIs,
limiting extensibility and adaptability. By contrast, recent advances in large
language models (LLMs) offer a promising pathway that leverages natural
language understanding and flexible reasoning. However, it is unclear to what
extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a
first-of-its-kind benchmarking suite comprising four novel datasets:
Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.
These datasets are specifically designed for evaluating LLMs performance in
intent translation and conflict detection tasks within the industry-grade SDN
controllers ODL and ONOS. Our results provide the first comprehensive
comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a
wide range of performance outcomes. However, while these results demonstrate
the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully
autonomous IBN pipeline remains unexplored. Thus, our second contribution is
NetIntent, a unified and adaptable framework that leverages LLMs to automate
the full IBN lifecycle, including translation, activation, and assurance within
SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting
dynamic re-prompting and contextual feedback to robustly execute user-defined
intents with minimal human intervention. Our implementation of NetIntent across
both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end
IBN realization.

</details>


### [57] [Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks](https://arxiv.org/abs/2507.14512)
*Qiyuan Peng,Qi Zhang,Yue Gao,Kun Qiu*

Main category: cs.NI

TL;DR: 本文提出了一种基于强化学习的控制器配置策略（Dora），用于解决卫星网络中的管理问题，显著提升了性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 为了解决卫星网络中传统网络管理架构和搜索算法在计算资源和时间限制下的不足。

Method: 采用三层域基架构和强化学习策略（Dora）优化控制器配置。

Result: Dora在控制器配置质量上提升了10%，且计算时间仅为传统算法的1/30到1/90。

Conclusion: 强化学习方法在下一代SAGIN卫星网络管理中具有巨大潜力。

Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground
Integrated Networks (SAGIN) presents significant challenges for network
management. Conventional flat network architectures struggle with
synchronization and data transmission across massive distributed nodes. In
response, hierarchical domain-based satellite network architectures have
emerged as a scalable solution, highlighting the critical importance of
controller provisioning strategies. However, existing network management
architectures and traditional search-based algorithms fail to generate
efficient controller provisioning solutions due to limited computational
resources in satellites and strict time constraints. To address these
challenges, we propose a three-layer domain-based architecture that enhances
both scalability and adaptability. Furthermore, we introduce Dora, a
reinforcement learning-based controller provisioning strategy designed to
optimize network performance while minimizing computational overhead. Our
comprehensive experimental evaluation demonstrates that Dora significantly
outperforms state-of-the-art benchmarks, achieving 10% improvement in
controller provisioning quality while requiring only 1/30 to 1/90 of the
computation time compared to traditional algorithms. These results underscore
the potential of reinforcement learning approaches for efficient satellite
network management in next-generation SAGIN deployments.

</details>


### [58] [UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach](https://arxiv.org/abs/2507.14627)
*Kaiqiang Lin,Yijie Mao,Onel Luis Alcaraz López,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出了一种基于无人机的无线供能地下通信网络系统，通过混合无线能量传输和时间分配优化，降低了能量消耗并提升了系统可持续性。


<details>
  <summary>Details</summary>
Motivation: 地下环境中无线信号衰减严重且信道状态信息获取成本高，大规模无线供能地下通信网络难以实现经济可行性。

Method: 引入无人机作为中继，采用混合无线能量传输方法（结合基于CSI和无CSI的多天线波束成形）和时间分配优化问题。

Result: 在农业场景模拟中，混合无线能量传输方法表现出优越性，且基于无CSI多天线方案的能量消耗最低。

Conclusion: 无人机辅助的无线供能地下通信网络系统能够显著降低能量消耗，实现了地下可持续监测。

Abstract: Wireless-powered underground communication networks (WPUCNs), which allow
underground devices (UDs) to harvest energy from wireless signals for
battery-free communication, offer a promising solution for sustainable
underground monitoring. However, the severe wireless signal attenuation in
challenging underground environments and the costly acquisition of channel
state information (CSI) make large-scale WPUCNs economically infeasible in
practice. To address this challenge, we introduce flexible unmanned aerial
vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this
system, a UAV is first charged by a terrestrial hybrid access point (HAP), then
flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV
collects data from the UDs and finally returns to the HAP for data offloading.
Based on the proposed UAV-enabled WPUCN system, we first propose its energy
consumption model and a hybrid wireless energy transfer (WET) approach (i.e.,
UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and
CSI-free multi-antenna beamforming. Then, we formulate and address a time
allocation problem to minimize the energy consumption of UAV, while ensuring
that the throughput requirements of all UDs are met and all sensor data is
offloaded. Through simulations of a realistic farming scenario, we demonstrate
that the proposed hybrid WET approach outperforms other WET approaches, with
performance gains influenced by the number of antennas, communication distance,
number of UDs, and underground conditions. Additionally, under the optimized
time allocation, we found that the proposed hybrid WET approach based on a
CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption
among all WET mechanisms, thereby enabling sustainable underground monitoring
in WPUCNs.

</details>


### [59] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 该论文探讨了如何在卫星-低空经济与地面网络（SLAETNs）中实现智能自主系统，重点是通过生成式AI（GAI）和大型语言模型（LLMs）来赋能具有感知、推理和行动能力的人工智能代理。


<details>
  <summary>Details</summary>
Motivation: 随着卫星-低空经济与地面网络的发展，需要能够在异构、动态和任务关键环境中可靠运行的智能自主系统，生成式AI和LLMs为解决这些问题提供了新思路。

Method: 论文首先介绍了SLAETNs的架构和特点，分析了集成卫星、空中和地面组件时的挑战；然后系统回顾了五类生成模型（VAEs、GANs、GDMs、TBMs、LLMs），并比较了它们的生成机制、能力和部署权衡；最后探讨了这些模型在通信增强、安全隐私保护和智能卫星任务三个领域的应用。

Result: 研究指出生成式AI和LLMs在SLAETNs中能够有效赋能智能代理功能，并总结了未来发展方向。

Conclusion: 该综述为下一代集成网络中可扩展、自适应和可信的生成式智能代理提供了统一理解和可操作的参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [60] [Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks](https://arxiv.org/abs/2507.14842)
*Satadal Sengupta,Hyojoon Kim,Daniel Jubas,Maria Apostolaki,Jennifer Rexford*

Main category: cs.NI

TL;DR: 该论文探讨了利用传播延迟变化检测互联网路由劫持的可行性，并提出HiDe系统，展示其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 互联网路由安全问题导致用户数据可能被劫持至国外基础设施，带来隐私和国家安全威胁，现有检测方法关注控制层面而忽略数据层面信号。

Method: 通过分析延迟变化（如传播延迟增加）检测劫持，设计HiDe系统实现实时检测，并验证其在实际部署中的表现。

Result: 实验显示86%的受害-攻击国家对在劫持期间延迟增加至少25%，HiDe系统能可靠检测长距离劫持。

Conclusion: 延迟变化是一种可行的劫持检测信号，HiDe系统在实际应用中表现良好，具有潜力提升路由安全。

Abstract: Poor security of Internet routing enables adversaries to divert user data
through unintended infrastructures (hijack). Of particular concern -- and the
focus of this paper -- are cases where attackers reroute domestic traffic
through foreign countries, exposing it to surveillance, bypassing legal privacy
protections, and posing national security threats. Efforts to detect and
mitigate such attacks have focused primarily on the control plane while
data-plane signals remain largely overlooked. In particular, change in
propagation delay caused by rerouting offers a promising signal: the change is
unavoidable and the increased propagation delay is directly observable from the
affected networks. In this paper, we explore the practicality of using delay
variations for hijack detection, addressing two key questions: (1) What
coverage can this provide, given its heavy dependence on the geolocations of
the sender, receiver, and adversary? and (2) Can an always-on latency-based
detection system be deployed without disrupting normal network operations? We
observe that for 86% of victim-attacker country pairs in the world, mid-attack
delays exceed pre-attack delays by at least 25% in real deployments, making
delay-based hijack detection promising. To demonstrate practicality, we design
HiDe, which reliably detects delay surges from long-distance hijacks at line
rate. We measure HiDe's accuracy and false-positive rate on real-world data and
validate it with ethically conducted hijacks.

</details>


### [61] [Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors](https://arxiv.org/abs/2507.14876)
*Zi-Yang Wu,Muhammad Ismail,Jiliang Zhang,Jie Zhang*

Main category: cs.NI

TL;DR: 论文提出将可重构智能表面（RIS）嵌入建筑结构以优化室内无线性能，并探讨了复杂人类行为对RIS覆盖建筑信道的影响，以及深度学习预测与控制策略的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决建筑设计中无线性能不足的问题，利用RIS技术提升室内移动网络的性能。

Method: 研究RIS嵌入建筑结构的方法，分析人类行为对信道的影响，探讨深度学习预测与控制策略的适用性。

Result: 发现人类行为导致信道动态变化，深度学习面临高阶马尔可夫依赖、概念漂移和泛化问题等挑战。

Conclusion: 需进一步研究以协调RIS覆盖建筑与人群移动的共存，优化无线性能。

Abstract: Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.

</details>


### [62] [FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches](https://arxiv.org/abs/2507.14891)
*Xiangyu Gao,Tong Li,Yinchao Zhang,Ziqiang Wang,Xiangsheng Zeng,Su Yao,Ke Xu*

Main category: cs.NI

TL;DR: FENIX是一种混合网络机器学习系统，通过结合可编程交换机ASIC和FPGA，实现低延迟、高吞吐量和高精度的网络流量分析。


<details>
  <summary>Details</summary>
Motivation: 现有网络数据平面的机器学习解决方案（如FlowLens、N3IC和BoS）难以同时满足低延迟、高吞吐量和高精度的需求，FENIX旨在解决这一问题。

Method: FENIX采用混合架构：1) 在ASIC上进行特征提取；2) 在FPGA上进行深度神经网络推理。并引入数据引擎（基于概率令牌桶算法）和模型引擎（解决复杂模型部署问题）。

Result: FENIX在实际网络流量数据集上实现了微秒级推理延迟、多兆比特吞吐量，分类任务准确率超过95%，性能优于当前最优技术。

Conclusion: FENIX通过创新的混合架构和算法设计，显著提升了网络数据平面机器学习的性能，为未来研究提供了新思路。

Abstract: Machine learning (ML) is increasingly used in network data planes for
advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,
and BoS) still struggle to simultaneously achieve low latency, high throughput,
and high accuracy. To address these challenges, we present FENIX, a hybrid
in-network ML system that performs feature extraction on programmable switch
ASICs and deep neural network inference on FPGAs. FENIX introduces a Data
Engine that leverages a probabilistic token bucket algorithm to control the
sending rate of feature streams, effectively addressing the throughput gap
between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model
Engine to enable high-accuracy deep neural network inference in the network,
overcoming the difficulty of deploying complex models on resource-constrained
switch chips. We implement FENIX on a programmable switch platform that
integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on
real-world network traffic datasets. Our results show that FENIX achieves
microsecond-level inference latency and multi-terabit throughput with low
hardware overhead, and delivers over 95\% accuracy on mainstream network
traffic classification tasks, outperforming SOTA.

</details>


### [63] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 该论文提出了一种通信高效、事件触发的推理框架，用于多用户设备和边缘服务器合作的边缘AI系统，通过双阈值早期退出策略和比例公平约束优化性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI系统中多设备协作的通信效率和资源分配公平性问题。

Method: 提出基于双阈值早期退出策略的分布式联合优化框架，利用单调性和交替优化结合Benders分解求解。

Result: 实验表明该系统在性能和资源分配的公平性上显著优于单设备基准。

Conclusion: 该框架在边缘AI系统中实现了高效通信和公平资源分配，提升了整体性能。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [64] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 本文提出了一种人机协同动态带宽分配方案（HMC-DBA），用于实时同步XR内容，降低带宽消耗并满足延迟要求。


<details>
  <summary>Details</summary>
Motivation: 推动移动系统和固定无线网络的发展，支持高带宽、低延迟服务，应对工业4.0/5.0和社会5.0的需求。

Method: 使用双向LSTM预测头部运动，动态调整带宽分配，提出HMC-DBA方案。

Result: 仿真表明，HMC-DBA在满足XR延迟要求的同时，显著降低了带宽消耗。

Conclusion: HMC-DBA在资源利用效率上优于现有方案，适用于企业网络。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


### [65] [Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint](https://arxiv.org/abs/2507.15338)
*Takaho Shimokasa,Hiroyuki Yomo,Federico Chiariotti,Junya Shiraishi,Petar Popovski*

Main category: cs.NI

TL;DR: 研究如何在无线传感器网络的资源限制下，通过Kalman滤波实现物联网监测中传感器节点的低功耗运行和准确状态估计。比较了两种基站策略：基于统计的无意识策略和基于传感器瞬时观测的分散策略，并引入唤醒机制提高能效。


<details>
  <summary>Details</summary>
Motivation: 在IoT监测中，需要平衡传感器节点的低功耗和状态估计的准确性，尤其是在无线资源受限的情况下。

Method: 提出两种策略（无意识和分散策略），结合唤醒接收器和唤醒信号，分散策略通过随机访问优先级传输高贡献观测值。

Result: 数值结果显示，当传感器观测过程相关性低时，分散策略在资源和能量约束下优于无意识策略，并确定了策略优劣转换的相关性阈值。

Conclusion: 分散策略在低相关性场景下更优，唤醒机制提升了能效，研究为资源受限的IoT网络提供了实用解决方案。

Abstract: This paper investigates how to achieve both low-power operations of sensor
nodes and accurate state estimation using Kalman filter for internet of things
(IoT) monitoring employing wireless sensor networks under radio resource
constraint. We consider two policies used by the base station to collect
observations from the sensor nodes: (i) an oblivious policy, based on
statistics of the observations, and (ii) a decentralized policy, based on
autonomous decision of each sensor based on its instantaneous observation. This
work introduces a wake-up receiver and wake-up signaling to both policies to
improve the energy efficiency of the sensor nodes. The decentralized policy
designed with random access prioritizes transmissions of instantaneous
observations that are highly likely to contribute to the improvement of state
estimation. Our numerical results show that the decentralized policy improves
the accuracy of the estimation in comparison to the oblivious policy under the
constraint on the radio resource and consumed energy when the correlation
between the processes observed by the sensor nodes is low. We also clarify the
degree of correlation in which the superiority of two policies changes.

</details>


### [66] [Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities](https://arxiv.org/abs/2507.15391)
*Fabian Ihle,Michael Menth*

Main category: cs.NI

TL;DR: MPLS网络行动(MNA)框架通过改进MPLS转发，支持网络切片和IOAM等功能。研究发现其设计导致路由器需较大可读标签深度(RLD)，并提出一种减少RLD需求的机制，通过MPLS栈重构实现。


<details>
  <summary>Details</summary>
Motivation: MNA框架需要较大的RLD，路由器需处理较多标签栈条目(LSEs)，硬件限制成为瓶颈，因此需优化设计以减少RLD需求。

Method: 分析MNA硬件实现，提出栈重构机制，并引入新的栈管理网络动作，支持与不支持MNA节点的兼容性。通过P4实现验证可行性。

Result: 机制成功减少RLD需求，并讨论了其对ECMP和数据包开销的影响。

Conclusion: 提出的栈管理机制有效降低了MNA对RLD的需求，适用于可编程硬件，增强了MNA的实用性。

Abstract: The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a
generalized encoding for manifold extensions such as network slicing and
in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries
(LSEs) and are added to the MPLS stack. Routers have a physical limit on the
number of LSEs they can read, called the readable label depth (RLD). With MNA,
routers must be able to process a minimum number of LSEs which requires a
relatively large RLD. In this paper, we perform a hardware analysis of an MNA
implementation and identify the reason for a large RLD requirement in the MNA
protocol design. Based on this, we present a mechanism that reduces the
required RLD for MNA nodes by restructuring the MPLS stack during forwarding.
We then introduce the novel stack management network action that enables the
proposed mechanism as well as its integration in networks with MNA-incapable
nodes. The feasibility of the mechanism on programmable hardware is verified by
providing a P4-based implementation. Further, the effects on the required RLD,
ECMP, and packet overhead are discussed.

</details>


### [67] [Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations](https://arxiv.org/abs/2507.15423)
*Laura Finarelli,Falko Dressler,Marco Ajmone Marsan,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 本文提出了一种随机几何框架，用于评估移动网络（MN）在异构网络（HetNet）中的潜在优势，并通过优化问题和启发式算法减少基础设施部署。


<details>
  <summary>Details</summary>
Motivation: 探讨在何种条件下移动网络的优势能超过其额外资源成本，为6G用户中心网络提供更动态、灵活和可持续的解决方案。

Method: 使用随机几何框架评估用户感知性能，考虑无线回程连接和基站资源调度，提出优化问题和启发式算法以最小化基站部署。

Result: 数值评估表明，移动网络结合动态管理策略可显著减少基础设施部署，同时保证用户目标服务质量。

Conclusion: 移动网络范式在适当动态管理策略下能有效优化资源利用并保障服务质量。

Abstract: In the evolution towards 6G user-centric networking, the moving network (MN)
paradigm can play an important role. In a MN, some small cell base stations
(BS) are installed on top of vehicles, and enable a more dynamic, flexible and
sustainable, network operation. By "following" the users movements and adapting
dynamically to their requests, the MN paradigm enables a more efficient
utilization of network resources, mitigating the need for dense small cell BS
deployments at the cost of an increase in resource utilization due to wireless
backhauling. This aspect is at least partly compensated by the shorter distance
between users and BS, which allows for lower power and Line-of-Sight
communications. While the MN paradigm has been investigated for some time, to
date, it is still unclear in which conditions the advantages of MN outweigh the
additional resource costs. In this paper, we propose a stochastic geometry
framework for the characterization of the potential benefits of the MN paradigm
as part of an HetNet in urban settings. Our approach allows the estimation of
user-perceived performance, accounting for wireless backhaul connectivity as
well as base station resource scheduling. We formulate an optimization problem
for determining the resource-optimal network configurations and BS scheduling
which minimize the overall amount of deployed BSs in a QoS-aware manner, and
the minimum vehicular flow between different urban districts required to
support them, and we propose an efficient stochastic heuristic to solve it. Our
numerical assessment suggests that the MN paradigm, coupled with appropriate
dynamic network management strategies, significantly reduces the amount of
deployed network infrastructure while guaranteeing the target QoS perceived by
users.

</details>


### [68] [SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform](https://arxiv.org/abs/2507.15659)
*Gabriel Paradzik,Benjamin Steinert,Heinrich Abele,Michael Menth*

Main category: cs.NI

TL;DR: 提出了一种基于开源工具的低成本分布式流量监控平台，用于收集未采样的IPFIX数据。


<details>
  <summary>Details</summary>
Motivation: 通过开源工具实现低成本、分布式的流量监控，解决传统监控方法的成本高和扩展性问题。

Method: 使用开源工具构建平台，详细介绍了工具的选择和使用方法。

Result: 成功在蒂宾根大学实现了该平台，能够高效收集IPFIX数据。

Conclusion: 该平台证明了利用开源工具构建分布式流量监控系统的可行性和成本效益。

Abstract: This paper presents a cost-effective and distributed flow monitoring platform
for collecting unsampled IPFIX data exclusively using open-source tools, which
is implemented at the University of T\"ubingen. An overview of all tools is
given and their use is explained.

</details>


### [69] [Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks](https://arxiv.org/abs/2507.15670)
*Rosario Patanè,Nadjib Achir,Andrea Araldo,Lila Boukhatem*

Main category: cs.NI

TL;DR: 该研究探讨了车载云计算（VCC）是否能替代边缘计算（EC）支持低延迟应用，并通过模拟实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 边缘计算（EC）部署成本高，限制了其广泛应用。车载云计算（VCC）利用空闲车载资源，可能成为低成本替代方案。

Method: 通过SUMO模拟车辆移动性，NS3 5G-LENA模拟通信，评估负载、车辆密度和可用性等因素对VCC替代EC的影响。

Result: VCC在大多数情况下可以替代EC，但延迟低于16毫秒时仍需依赖EC。

Conclusion: VCC是EC的一种可行低成本替代方案，但在极低延迟需求场景中仍需EC的支持。

Abstract: Edge Computing (EC) is a computational paradigm that involves deploying
resources such as CPUs and GPUs near end-users, enabling low-latency
applications like augmented reality and real-time gaming. However, deploying
and maintaining a vast network of EC nodes is costly, which can explain its
limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)
has emerged and inspired interest among researchers and industry. VCC
opportunistically utilizes existing and idle vehicular computational resources
for external task offloading. This work is the first to systematically address
the following question: Can VCC replace EC for low-latency applications?
Answering this question is highly relevant for Network Operators (NOs), as VCC
could eliminate costs associated with EC given that it requires no
infrastructural investment. Despite its potential, no systematic study has yet
explored the conditions under which VCC can effectively support low-latency
applications without relying on EC. This work aims to fill that gap. Extensive
simulations allow for assessing the crucial scenario factors that determine
when this EC-to-VCC substitution is feasible. Considered factors are load,
vehicles mobility and density, and availability. Potential for substitution is
assessed based on multiple criteria, such as latency, task completion success,
and cost. Vehicle mobility is simulated in SUMO, and communication in NS3
5G-LENA. The findings show that VCC can effectively replace EC for low-latency
applications, except in extreme cases when the EC is still required (latency <
16 ms).

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [70] [Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling](https://arxiv.org/abs/2507.14915)
*Xiaojie Li,Ronghui Li,Shukai Fang,Shuzhao Xie,Xiaoyang Guo,Jiaqing Zhou,Junkun Peng,Zhi Wang*

Main category: cs.MM

TL;DR: SoulDance和SoulNet解决了音乐与舞蹈对齐的复杂问题，通过新数据集和框架生成高质量的3D舞蹈。


<details>
  <summary>Details</summary>
Motivation: 当前音乐对齐的舞蹈生成面临数据集稀缺、跨模态对齐困难以及身体各部分动作依赖复杂的问题。

Method: SoulDance提供高精度舞蹈数据集，SoulNet包含分层残差向量量化、音乐对齐生成模型和音乐-动作检索模块。

Result: SoulNet在生成高质量、音乐对齐的3D舞蹈序列上显著优于现有方法。

Conclusion: SoulNet成功解决了音乐与舞蹈对齐的挑战，生成了表达力强且协调的舞蹈序列。

Abstract: Well-coordinated, music-aligned holistic dance enhances emotional
expressiveness and audience engagement. However, generating such dances remains
challenging due to the scarcity of holistic 3D dance datasets, the difficulty
of achieving cross-modal alignment between music and dance, and the complexity
of modeling interdependent motion across the body, hands, and face. To address
these challenges, we introduce SoulDance, a high-precision music-dance paired
dataset captured via professional motion capture systems, featuring
meticulously annotated holistic dance movements. Building on this dataset, we
propose SoulNet, a framework designed to generate music-aligned, kinematically
coordinated holistic dance sequences. SoulNet consists of three principal
components: (1) Hierarchical Residual Vector Quantization, which models
complex, fine-grained motion dependencies across the body, hands, and face; (2)
Music-Aligned Generative Model, which composes these hierarchical motion units
into expressive and coordinated holistic dance; (3) Music-Motion Retrieval
Module, a pre-trained cross-modal model that functions as a music-dance
alignment prior, ensuring temporal synchronization and semantic coherence
between generated dance and input music throughout the generation process.
Extensive experiments demonstrate that SoulNet significantly surpasses existing
approaches in generating high-quality, music-coordinated, and well-aligned
holistic 3D dance sequences.

</details>


### [71] [Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval](https://arxiv.org/abs/2507.15491)
*Deyu Zhang,Tingting Long,Jinrui Zhang,Ligeng Chen,Ju Ren,Yaoxue Zhang*

Main category: cs.MM

TL;DR: ProCLIP是一个用户中心框架，通过动态选择语义相关帧和两级候选剪枝策略，显著提升文本-视频检索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在文本-视频检索中无法平衡准确性和计算效率的问题。

Method: 设计了提示感知的帧采样策略和两级候选剪枝策略。

Result: 在MSR-VTT数据集上实现75.3%的延迟降低，R@1=49.0。

Conclusion: ProCLIP在保持高精度的同时显著提升了效率。

Abstract: Enabling efficient text-video retrieval on edge-end devices is critical for
real-world applications. Yet, existing methods face a critical challenge in
balancing accuracy and computational efficiency: uniform frame sampling methods
ensure content coverage but incur prohibitive computational costs, while
salient-frame sampling methods reduce overhead but suffer from query-agnostic
frame selection that biases retrieval results. To address this, we propose
ProCLIP, a user-centric framework that achieves state-of-the-art accuracy with
significantly improved efficiency. We design a prompt-aware frame sampling
strategy that dynamically guides lightweight feature extractors using textual
prompts to select semantically relevant frames, overcoming the limitations of
existing salient-frame sampling methods which rely on static, query-agnostic
selection criteria. Moreover, we adopt a two-stage candidate pruning strategy
that combines rapid coarse filtering via a lightweight module with CLIP-powered
fine-grained re-ranking, enhancing retrieval efficiency while preserving
accuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency
reduction versus baselines while maintaining competitive accuracy, i.e.,
R@1=49.0 in MSR-VTT dataset. Code is available at
https://github.com/tiffylong/ProCLIP.

</details>


### [72] [Point Cloud Streaming with Latency-Driven Implicit Adaptation using MoQ](https://arxiv.org/abs/2507.15673)
*Andrew Freeman,Michael Rudolph,Amr Rizk*

Main category: cs.MM

TL;DR: 该论文提出了一种基于QUIC协议的隐式服务器端自适应方法，用于点云视频的实时流传输，从而根据客户端的延迟需求动态调整视频质量。


<details>
  <summary>Details</summary>
Motivation: 点云视频因其高比特率特性，难以实现实时流传输。现有方法依赖客户端显式适应，限制了低延迟的实现。

Method: 利用Media Over QUIC协议中的传输超时特性，在服务器端进行隐式自适应调整，以满足不同客户端的延迟目标。

Result: 实验表明，系统能够根据客户端的延迟需求，动态调整视频质量（低延迟要求对应低质量，反之亦然）。

Conclusion: 该方案为点云视频的实时流传输提供了灵活的延迟-质量权衡方法。

Abstract: Point clouds are a promising video representation for next-generation
multimedia experiences in virtual and augmented reality. Point clouds are
notoriously high-bitrate, however, which limits the feasibility of live
streaming systems. Prior methods have adopted traditional HTTP-based protocols
for point cloud streaming, but they rely on explicit client-side adaptation to
maintain low latency under congestion. In this work, we leverage the delivery
timeout feature within the Media Over QUIC protocol to perform implicit
server-side adaptation based on an application's latency target. Through
experimentation with several publisher and network configurations, we
demonstrate that our system unlocks a unique trade-off on a per-client basis:
applications with lower latency requirements will receive lower-quality video,
while applications with more relaxed latency requirements will receive
higher-quality video.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [73] [A Proof System with Causal Labels (Part I): checking Individual Fairness and Intersectionality](https://arxiv.org/abs/2507.14650)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 提出一种扩展的TNDPQ演算，用于验证概率分类器中的个体公平性和交叉性。


<details>
  <summary>Details</summary>
Motivation: 为概率分类器中的公平性和因果标签建模提供理论基础。

Method: 通过限制结构弱化规则的应用条件，并结合因果标签检验条件独立性。

Result: 成功构建了一个能够验证个体公平性和交叉性的理论框架。

Conclusion: 扩展的TNDPQ演算为公平性验证提供了新的理论工具。

Abstract: In this article we propose an extension to the typed natural deduction
calculus TNDPQ to model verification of individual fairness and
intersectionality in probabilistic classifiers. Their interpretation is
obtained by formulating specific conditions for the application of the
structural rule of Weakening. Such restrictions are given by causal labels used
to check for conditional independence between protected and target variables.

</details>


### [74] [A Proof System with Causal Labels (Part II): checking Counterfactual Fairness](https://arxiv.org/abs/2507.14655)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 提出了扩展TNDPQ以验证概率分类器中反事实公平性的方法。


<details>
  <summary>Details</summary>
Motivation: 需要为反事实公平性验证提供形式化工具。

Method: 在TNDPQ中引入特定结构条件，检查评估对这些条件的鲁棒性。

Result: 成功建模并验证了反事实公平性。

Conclusion: 扩展的TNDPQ为反事实公平性验证提供了有效框架。

Abstract: In this article we propose an extension to the typed natural deduction
calculus TNDPQ to model verification of counterfactual fairness in
probabilistic classifiers. This is obtained formulating specific structural
conditions for causal labels and checking that evaluation is robust under their
variation.

</details>


### [75] [PSPACE-completeness of bimodal transitive weak-density logic](https://arxiv.org/abs/2507.14949)
*Philippe Balbiani,Olivier Gasquet*

Main category: cs.LO

TL;DR: 本文通过窗口方法重新审视双模态K4逻辑，并结合弱密度与传递性，证明了其可满足性和有效性的PSPACE完备性。


<details>
  <summary>Details</summary>
Motivation: 重新审视双模态K4逻辑，并扩展窗口方法以解决弱密度加传递性逻辑的可满足性问题。

Method: 结合双模态K算法和窗口方法，设计多项式算法。

Result: 证明了这些逻辑的可满足性和有效性均为PSPACE完备。

Conclusion: 窗口方法在解决复杂逻辑问题时具有潜力，双模态逻辑的可满足性问题可通过混合算法高效解决。

Abstract: Windows have been introduce in \cite{BalGasq25} as a tool for designing
polynomial algorithms to check satisfiability of a bimodal logic of
weak-density. In this paper, after revisiting the ``folklore'' case of bimodal
$\K4$ already treated in \cite{Halpern} but which is worth a fresh review, we
show that windows allow to polynomially solve the satisfiability problem when
adding transitivity to weak-density, by mixing algorithms for bimodal K
together with windows-approach. The conclusion is that both satisfiability and
validity are PSPACE-complete for these logics.

</details>


### [76] [PSPACE-completeness of Grammar logics of bounded density](https://arxiv.org/abs/2507.14956)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 论文介绍了多模态有界密度逻辑家族，并通过有限窗口的表格方法证明其可满足性问题为PSPACE完全问题。附带证明了单模态密度逻辑属于para-PSPACE。


<details>
  <summary>Details</summary>
Motivation: 研究多模态有界密度逻辑的可满足性问题及其复杂性，同时探索单模态密度逻辑的计算复杂性。

Method: 使用表格方法和有限窗口技术，基于已有研究的理论框架。

Result: 多模态有界密度逻辑的可满足性问题是PSPACE完全的；单模态密度逻辑属于para-PSPACE。

Conclusion: 研究结果为多模态和单模态密度逻辑的复杂性提供了新的理论支持。

Abstract: We introduce the family of multi-modal logics of bounded density and with a
tableau-like approach using finite \emph{windows} which were introduced in
\cite{BalGasq25}, we prove that their satisfiability problem is
PSPACE-complete. As a side effect, the monomodal logic of density is shown to
be in para-PSPACE.

</details>


### [77] [A meta-modal logic for bisimulations](https://arxiv.org/abs/2507.15117)
*Alfredo Burrieza,Fernando Soler-Toscano,Antonio Yuste-Ginel*

Main category: cs.LO

TL;DR: 本文提出了一种对偶模拟的模态研究，通过在基本模态语言中引入新模态[b]，实现对其的量化定义，并给出了完全公理化系统。


<details>
  <summary>Details</summary>
Motivation: 研究如何在模态语言中定义和量化偶模拟关系，以增强对偶模拟的理论理解。

Method: 扩展基本模态语言，引入新模态[b]，定义偶模拟关系，并提供完全公理化系统。

Result: 成功在模态语言中定义了偶模拟关系，并验证了公理化系统的完备性和可靠性。

Conclusion: 通过新模态语言和公理化系统，为偶模拟的理论研究提供了新工具和框架。

Abstract: We propose a modal study of the notion of bisimulation. Our contribution is
twofold. First, we extend the basic modal language with a new modality [b],
whose intended meaning is universal quantification over all states that are
bisimilar to the current one. We show that bisimulations are definable in this
object language. Second, we provide a sound and complete axiomatisation of the
class of all pairs of Kripke models linked by bisimulations.

</details>


### [78] [STL-GO: Spatio-Temporal Logic with Graph Operators for Distributed Systems with Multiple Network Topologies](https://arxiv.org/abs/2507.15147)
*Yiqi Zhao,Xinyi Yu,Bardh Hoxha,Georgios Fainekos,Jyotirmoy V. Deshmukh,Lars Lindemann*

Main category: cs.LO

TL;DR: 论文提出了一种新的逻辑STL-GO，用于建模多智能体系统（MAS）的复杂需求，并通过分布式监控条件实现局部信息验证。案例研究展示了其表达能力和实用性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）的复杂任务需求需要对感知、通信和任务依赖进行建模和监控，以确保目标、安全和可靠性。现有方法难以全面捕捉这些需求。

Method: 通过多向有向图建模智能体交互，并引入STL-GO逻辑，定义图操作符以推理邻域智能体属性。提出分布式监控条件，仅用局部信息验证STL-GG规范。

Result: STL-GO在表达能力上优于现有时空逻辑形式，并在共享单车和多无人机案例中展示了其实用性。

Conclusion: STL-GO为MAS需求建模和监控提供了更丰富的表达能力和分布式解决方案，适用于复杂任务场景。

Abstract: Multi-agent systems (MASs) consisting of a number of autonomous agents that
communicate, coordinate, and jointly sense the environment to achieve complex
missions can be found in a variety of applications such as robotics, smart
cities, and internet-of-things applications. Modeling and monitoring MAS
requirements to guarantee overall mission objectives, safety, and reliability
is an important problem. Such requirements implicitly require reasoning about
diverse sensing and communication modalities between agents, analysis of the
dependencies between agent tasks, and the spatial or virtual distance between
agents. To capture such rich MAS requirements, we model agent interactions via
multiple directed graphs, and introduce a new logic -- Spatio-Temporal Logic
with Graph Operators (STL-GO). The key innovation in STL-GO are graph operators
that enable us to reason about the number of agents along either the incoming
or outgoing edges of the underlying interaction graph that satisfy a given
property of interest; for example, the requirement that an agent should sense
at least two neighboring agents whose task graphs indicate the ability to
collaborate. We then propose novel distributed monitoring conditions for
individual agents that use only local information to determine whether or not
an STL-GO specification is satisfied. We compare the expressivity of STL-GO
against existing spatio-temporal logic formalisms, and demonstrate the utility
of STL-GO and our distributed monitors in a bike-sharing and a multi-drone case
study.

</details>


### [79] [Quantum Programming in Polylogarithmic Time](https://arxiv.org/abs/2507.15415)
*Florent Ferrari,Emmanuel Hainry,Romain Péchoux,Mário Silva*

Main category: cs.LO

TL;DR: 论文介绍了一种量子编程语言，首次从编程语言角度描述了FBQPOLYLOG类，展示了其完备性与正确性，并提出了编译策略，验证了FBQPOLYLOG与QNC的包含关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过编程语言的方式描述FBQPOLYLOG复杂性类，填补量子计算模型在这一领域的空白。

Method: 提出了一种具有一阶递归过程的量子编程语言，证明了其计算能力与FBQPOLYLOG类一致，并提供了将其编译为量子电路的策略。

Result: 验证了程序的完备性与正确性，实现了FBQPOLYLOG类函数的计算，并揭示了FBQPOLYLOG与QNC的包含关系。

Conclusion: 该研究为量子计算模型的编程语言描述提供了新视角，并验证了FBQPOLYLOG类的计算能力与限制。

Abstract: Polylogarithmic time delineates a relevant notion of feasibility on several
classical computational models such as Boolean circuits or parallel random
access machines. As far as the quantum paradigm is concerned, this notion
yields the complexity class FBQPOLYLOG of functions approximable in
polylogarithmic time with a quantum random-access Turing machine. We introduce
a quantum programming language with first-order recursive procedures, which
provides the first programming-language-based characterization of FBQPOLYLOG.
Each program computes a function in FBQPOLYLOG (soundness) and, conversely,
each function of this complexity class is computed by a program (completeness).
We also provide a compilation strategy from programs to uniform families of
quantum circuits of polylogarithmic depth and polynomial size, whose set of
computed functions is known as QNC, and recover the well-known separation
result FBQPOLYLOG $\subsetneq$ QNC.

</details>


### [80] [A SHACL-based Data Consistency Solution for Contract Compliance Verification](https://arxiv.org/abs/2507.15420)
*Robert David,Albin Ahmeti,Geni Bushati,Amar Tauqeer,Anna Fensel*

Main category: cs.LO

TL;DR: 论文提出了一种半自动化的方法来修复合同合规性验证（CCV）中的数据不一致问题，通过提供修复策略并集成到现有的自动化合同工具（ACT）中，以支持GDPR合规的合同生命周期管理。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化合同工具（ACT）虽然能报告合同义务的违规情况，但在验证和确保合规性方面存在局限性，如缺乏互操作的语义形式化方法（如SHACL）和用户支持解决数据不一致问题。

Method: 论文通过半自动化方式解决CCV的数据不一致性问题，提供修复策略并提出最优解决方案，帮助用户重新建立数据一致性。

Result: 方法已实现并集成到ACT中，测试表明其能满足基本的CCV一致性要求。

Conclusion: 研究通过修复策略和半自动化方法改进了ACT工具，提高了GDPR合规合同生命周期管理的效率和准确性。

Abstract: In recent years, there have been many developments for GDPR-compliant data
access and sharing based on consent. For more complex data sharing scenarios,
where consent might not be sufficient, many parties rely on contracts. Before a
contract is signed, it must undergo the process of contract negotiation within
the contract lifecycle, which consists of negotiating the obligations
associated with the contract. Contract compliance verification (CCV) provides a
means to verify whether a contract is GDPR-compliant, i.e., adheres to legal
obligations and there are no violations. The rise of knowledge graph (KG)
adoption, enabling semantic interoperability using well-defined semantics,
allows CCV to be applied on KGs. In the scenario of different participants
negotiating obligations, there is a need for data consistency to ensure that
CCV is done correctly. Recent work introduced the automated contracting tool
(ACT), a KG-based and ODRL-employing tool for GDPR CCV, which was developed in
the Horizon 2020 project smashHit (https://smashhit.eu). Although the tool
reports violations with respect to obligations, it had limitations in verifying
and ensuring compliance, as it did not use an interoperable semantic formalism,
such as SHACL, and did not support users in resolving data inconsistencies. In
this work, we propose a novel approach to overcome these limitations of ACT. We
semi-automatically resolve CCV inconsistencies by providing repair strategies,
which automatically propose (optimal) solutions to the user to re-establish
data consistency and thereby support them in managing GDPR-compliant contract
lifecycle data. We have implemented the approach, integrated it into ACT and
tested its correctness and performance against basic CCV consistency
requirements.

</details>


### [81] [Computation of Interpolants for Description Logic Concepts in Hard Cases](https://arxiv.org/abs/2507.15689)
*Jean Christoph Jung,Jędrzej Kołodziejski,Frank Wolter*

Main category: cs.LO

TL;DR: 摘要讨论了描述逻辑（DLs）中Craig插值的计算问题，尤其是对于不具备Craig插值性质（CIP）的DLs或需要在较弱DL中插值的情况。论文提出了首个基本算法，用于计算ALC插值，并观察到非基本大小的均匀插值可能存在的问题。


<details>
  <summary>Details</summary>
Motivation: 研究DLs中插值计算的空白，尤其是针对不具备CIP的DLs或在较弱DL中插值的情境。

Method: 基于最近的插值存在性决策程序，提出了计算ALC插值的基本算法。

Result: 成功开发了两种插值计算算法，并发现均匀插值可能具有非基本大小。

Conclusion: 论文填补了DLs中插值计算的理论空白，为相关研究提供了新工具和见解。

Abstract: While the computation of Craig interpolants for description logics (DLs) with
the Craig Interpolation Property (CIP) is well understood, very little is known
about the computation and size of interpolants for DLs without CIP or if one
aims at interpolating concepts in a weaker DL than the DL of the input ontology
and concepts. In this paper, we provide the first elementary algorithms
computing (i) ALC-interpolants between ALC-concepts under ALCH-ontologies and
(ii) ALC-interpolants between ALCQ-concepts under ALCQ-ontologies. The
algorithms are based on recent decision procedures for interpolant existence.
We also observe that, in contrast, uniform (possibly depth restricted)
interpolants might be of non-elementary size.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [82] [Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap](https://arxiv.org/abs/2507.14316)
*Xianhao Carton Liu,Difan Jia,Tongyu Nie,Evan Suma Rosenberg,Victoria Interrante,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: 研究对比了AR和2D地图在紧急决策中用户对AI建议的依赖情况，发现AR可能因视觉错觉导致过度依赖，但提升了空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 在紧急情况下，决策者需快速选择目标，AI和传感技术可辅助决策，但信息可视化方式可能影响用户对AI的合理依赖。

Method: 通过实验（N=32）比较AR和2D地图在AI辅助目标选择任务中的表现。

Result: AR条件下用户对AI的依赖不当增加，主因是视觉错觉导致的过度依赖，但AR在空间推理方面有优势。

Conclusion: AR嵌入式可视化需优化以避免过度依赖，同时发挥其空间推理优势，未来研究应关注人机协作设计。

Abstract: In high-stakes, time-critical scenarios-such as emergency evacuation, first
responder prioritization, and crisis management -- decision-makers must rapidly
choose among spatial targets, such as exits, individuals to assist, or areas to
secure. Advances in indoor sensing and artificial intelligence (AI) can support
these decisions by visualizing real-time situational data and AI suggestions on
2D maps. However, mentally mapping this information onto real-world spaces
imposes significant cognitive load. This load can impair users' ability to
appropriately judge AI suggestions, leading to inappropriate reliance (e.g.,
accepting wrong AI suggestions or rejecting correct ones). Embedded
visualizations in Augmented Reality (AR), by directly overlaying information
onto physical environments, may reduce this load and foster more deliberate,
appropriate reliance on AI. But is this true? In this work, we conducted an
empirical study (N = 32) comparing AR see-through (embedded visualization) and
2D Minimap in time-critical, AI-assisted spatial target selection tasks.
Contrary to our expectations, users exhibited greater inappropriate reliance on
AI in the AR condition. Our analysis further reveals that this is primarily due
to over-reliance, with factors specific to embedded visualizations, such as
perceptual challenges, visual proximity illusions, and highly realistic visual
representations. Nonetheless, embedded visualizations demonstrated notable
benefits in spatial reasoning, such as spatial mapping and egocentric spatial
imagery. We conclude by discussing the empirical insights, deriving design
implications, and outlining important directions for future research on
human-AI decision collaboration in AR.

</details>


### [83] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: 研究了如何利用ChatGPT进行结构化演绎定性编码，测试了四种干预方法，发现逐步任务分解策略效果最佳，适合融入严格的定性编码工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注归纳编码，未充分探索大型语言模型在演绎分类任务中的潜力。本文旨在填补这一空白。

Method: 使用CAP主代码书对最高法院案例摘要进行分类，测试了零样本、少样本、基于定义及逐步任务分解四种策略。

Result: 逐步任务分解策略表现最佳，达到高度一致性（准确率0.775，kappa 0.744）。卡方检验和Cramer's V证实干预策略显著影响分类行为。

Conclusion: 通过定制化干预，大型语言模型可达到适合严格定性编码工作流程的可靠性水平。

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [84] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: 论文探讨了在技术面试中，利用对话式AI辅助候选人练习‘大声思考’过程的研究。通过实验，研究发现参与者对AI在模拟、反馈和学习示例中的作用持积极态度，并提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 技术面试中的‘大声思考’过程对候选人至关重要，但缺乏结构化练习机会。尽管对话式AI有潜力辅助，但用户对其作用的认知研究有限，因此开展了本研究。

Method: 研究通过17名参与者使用基于大语言模型（LLM）的技术面试练习工具进行实验，分析用户对AI辅助的看法和需求。

Result: 参与者认可AI在模拟面试、提供反馈及生成学习示例中的作用。研究提出了促进社交存在感、扩展反馈内容和人机协作等关键设计建议。

Conclusion: 研究不仅关注功能设计，还探讨了AI驱动的面试准备如何促进计算职业的公平学习，并提出了人机协作研究方向的必要性。

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [85] [Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies](https://arxiv.org/abs/2507.14482)
*Qianhe Chen,Yong Wang,Yixin Yu,Xiyuan Zhu,Xuerou Yu,Ran Wang*

Main category: cs.HC

TL;DR: 论文提出了一种名为Conch的交互式可视化系统，用于分析竞争性辩论的内容和过程，利用大语言模型自动识别关键辩论元素，并通过案例研究和用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 手动分析无结构和无标记的辩论记录耗时且低效，难以从原始数据中重建上下文语义和追踪逻辑联系。

Method: 提出Conch系统，采用平行螺旋可视化追踪辩论过程中多维碰撞点和参与者互动的演化，并利用大语言模型自动识别关键辩论元素。

Result: 通过真实辩论案例和用户研究，验证了Conch在辩论分析中的有效性和实用性。

Conclusion: Conch系统为竞争性辩论提供了高效的分析工具，显著提升了辩论分析的效率和质量。

Abstract: In-depth analysis of competitive debates is essential for participants to
develop argumentative skills and refine strategies, and further improve their
debating performance. However, manual analysis of unstructured and unlabeled
textual records of debating is time-consuming and ineffective, as it is
challenging to reconstruct contextual semantics and track logical connections
from raw data. To address this, we propose Conch, an interactive visualization
system that systematically analyzes both what is debated and how it is debated.
In particular, we propose a novel parallel spiral visualization that compactly
traces the multidimensional evolution of clash points and participant
interactions throughout debate process. In addition, we leverage large language
models with well-designed prompts to automatically identify critical debate
elements such as clash points, disagreements, viewpoints, and strategies,
enabling participants to understand the debate context comprehensively.
Finally, through two case studies on real-world debates and a
carefully-designed user study, we demonstrate Conch's effectiveness and
usability for competitive debate analysis.

</details>


### [86] ["It looks sexy but it's wrong." Tensions in creativity and accuracy using genAI for biomedical visualization](https://arxiv.org/abs/2507.14494)
*Roxanne Ziman,Shehryar Saharan,Gaël McGill,Laura Garrison*

Main category: cs.HC

TL;DR: 生医可视化中生成AI的工作流程与潜在问题分析，揭示其机遇与局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成AI在生医可视化中的应用及其对信息准确性和科学传播的潜在影响。

Method: 通过17位从业者的访谈，定性分析生成AI在生医可视化中的使用态度与阶段。

Result: 专家对生成AI态度各异，从热情接纳到谨慎回避，强调人类干预对准确科学视觉的必要性。

Conclusion: 生成AI虽有潜力，但需人类干预以确保科学可视化准确性与可信度。

Abstract: We contribute an in-depth analysis of the workflows and tensions arising from
generative AI (genAI) use in biomedical visualization (BioMedVis). Although
genAI affords facile production of aesthetic visuals for biological and medical
content, the architecture of these tools fundamentally limits the accuracy and
trustworthiness of the depicted information, from imaginary (or fanciful)
molecules to alien anatomy. Through 17 interviews with a diverse group of
practitioners and researchers, we qualitatively analyze the concerns and values
driving genAI (dis)use for the visual representation of spatially-oriented
biomedical data. We find that BioMedVis experts, both in roles as developers
and designers, use genAI tools at different stages of their daily workflows and
hold attitudes ranging from enthusiastic adopters to skeptical avoiders of
genAI. In contrasting the current use and perspectives on genAI observed in our
study with predictions towards genAI in the visualization pipeline from prior
work, our refocus the discussion of genAI's effects on projects in
visualization in the here and now with its respective opportunities and
pitfalls for future visualization research. At a time when public trust in
science is in jeopardy, we are reminded to first do no harm, not just in
biomedical visualization but in science communication more broadly. Our
observations reaffirm the necessity of human intervention for empathetic design
and assessment of accurate scientific visuals.

</details>


### [87] [PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration](https://arxiv.org/abs/2507.14527)
*Runhua Zhang,Yang Ouyang,Leixian Shen,Yuying Tang,Xiaojuan Ma,Huamin Qu,Xian Xu*

Main category: cs.HC

TL;DR: PaperBridge是一个人类-AI协同探索系统，帮助研究人员将其出版物组织成连贯的叙事。通过双向分析引擎支持迭代探索，用户研究表明其可用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人员需要将出版物组织成连贯的叙事，尤其在跨学科领域（如HCI）尤为困难。本文旨在探索多样化的组织方式。

Method: 开发了PaperBridge系统，基于形成性研究和内容分析，使用大语言模型驱动的双向分析引擎支持用户意图与叙事组件的迭代探索。

Result: 用户研究（N=12）表明PaperBridge在探索替代性研究叙事方面具有可用性和有效性。

Conclusion: 交互式系统可以为学术交流任务提供支持，PaperBridge为研究人员提供了新的组织工具。

Abstract: Researchers frequently need to synthesize their own publications into
coherent narratives that demonstrate their scholarly contributions. To suit
diverse communication contexts, exploring alternative ways to organize one's
work while maintaining coherence is particularly challenging, especially in
interdisciplinary fields like HCI where individual researchers' publications
may span diverse domains and methodologies. In this paper, we present
PaperBridge, a human-AI co-exploration system informed by a formative study and
content analysis. PaperBridge assists researchers in exploring diverse
perspectives for organizing their publications into coherent narratives. At its
core is a bi-directional analysis engine powered by large language models,
supporting iterative exploration through both top-down user intent (e.g.,
determining organization structure) and bottom-up refinement on narrative
components (e.g., thematic paper groupings). Our user study (N=12) demonstrated
PaperBridge's usability and effectiveness in facilitating the exploration of
alternative research narratives. Our findings also provided empirical insights
into how interactive systems can scaffold academic communication tasks.

</details>


### [88] [Uncovering the EEG Temporal Representation of Low-dimensional Object Properties](https://arxiv.org/abs/2507.14537)
*Jiahua Tang,Song Wang,Jiachen Zou,Chen Wei,Quanying Liu*

Main category: cs.HC

TL;DR: 提出了一个新方法，利用先进的神经解码算法研究EEG信号中低维物体属性的时间编码，填补了EEG研究中时间动态神经表征的空白。


<details>
  <summary>Details</summary>
Motivation: 探索EEG信号中神经表征的时间动态特性，弥补现有研究中对EEG时间分辨率优势的不足。

Method: 整合先进的神经解码算法，系统研究EEG信号中低维物体属性的时间编码特性。

Result: 首次在时间分布中识别概念的特异性和原型时间特征。

Conclusion: 该框架不仅提升了神经表征的可解释性，还为脑机接口中的视觉解码提供了新见解。

Abstract: Understanding how the human brain encodes and processes external visual
stimuli has been a fundamental challenge in neuroscience. With advancements in
artificial intelligence, sophisticated visual decoding architectures have
achieved remarkable success in fMRI research, enabling more precise and
fine-grained spatial concept localization. This has provided new tools for
exploring the spatial representation of concepts in the brain. However, despite
the millisecond-scale temporal resolution of EEG, which offers unparalleled
advantages in tracking the dynamic evolution of cognitive processes, the
temporal dynamics of neural representations based on EEG remain underexplored.
This is primarily due to EEG's inherently low signal-to-noise ratio and its
complex spatiotemporal coupling characteristics. To bridge this research gap,
we propose a novel approach that integrates advanced neural decoding algorithms
to systematically investigate how low-dimensional object properties are
temporally encoded in EEG signals. We are the first to attempt to identify the
specificity and prototypical temporal characteristics of concepts within
temporal distributions. Our framework not only enhances the interpretability of
neural representations but also provides new insights into visual decoding in
brain-computer interfaces (BCI).

</details>


### [89] [EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences](https://arxiv.org/abs/2507.14685)
*Luis Montana,Jessica Magallanes,Miguel Juarez,Suzanne Mason,Andrew Narracott,Lindsey van Gemeren,Steven Wood,Maria-Cruz Villa-Uriol*

Main category: cs.HC

TL;DR: 论文提出了一种名为EventBox的新颖数据表示和视觉编码方法，用于分析事件序列的多变量属性，并将其集成到Sequen-C系统中。通过用户驱动的转换和自动生成的统计分析，该方法增强了分析深度，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着事件序列数据在各领域的快速增长和可用性，需要有效的分析和探索方法来支持决策制定。现有方法常忽略时间和多变量属性之间的相互作用。

Method: 开发了EventBox的数据表示和视觉编码方法，并将其集成到Sequen-C系统中。通过用户驱动的转换（如对齐、排序、替换和聚合）以及自动生成的统计分析，提升了分析能力。

Result: 通过21名参与者的评估（包括3名领域专家和18名新手数据分析师），验证了EventBox在揭示事件序列中模式和异常的有效性。三个真实医疗数据的案例研究进一步展示了其实际应用价值。

Conclusion: EventBox和Sequen-C的结合为探索事件序列中的时间和多变量属性提供了灵活且有效的解决方案，推动了视觉分析领域的进步。

Abstract: The rapid growth and availability of event sequence data across domains
requires effective analysis and exploration methods to facilitate
decision-making. Visual analytics combines computational techniques with
interactive visualizations, enabling the identification of patterns, anomalies,
and attribute interactions. However, existing approaches frequently overlook
the interplay between temporal and multivariate attributes. We introduce
EventBox, a novel data representation and visual encoding approach for
analyzing groups of events and their multivariate attributes. We have
integrated EventBox into Sequen-C, a visual analytics system for the analysis
of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we
have added user-driven transformations, including alignment, sorting,
substitution and aggregation. To enhance analytical depth, we incorporate
automatically generated statistical analyses, providing additional insight into
the significance of attribute interactions. We evaluated our approach involving
21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T
framework to assess visualization value, user performance metrics completing a
series of tasks, and interactive sessions with domain experts. We also present
three case studies with real-world healthcare data demonstrating how EventBox
and its integration into Sequen-C reveal meaningful patterns, anomalies, and
insights. These results demonstrate that our work advances visual analytics by
providing a flexible solution for exploring temporal and multivariate
attributes in event sequences.

</details>


### [90] [A Notification Based Nudge for Handling Excessive Smartphone Use](https://arxiv.org/abs/2507.14702)
*Partha Sarker,Dipto Dey,Marium-E-Jannat*

Main category: cs.HC

TL;DR: 提出了一种基于通知的干预方法，通过增强用户自我意识来减少智能手机过度使用，避免用户感到厌烦。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过限制用户使用智能手机来减少过度使用，但用户对此感到厌烦或不愿被限制。

Method: 设计了一个Android原型“App Usage Monitor”，结合了轻推和可视化技术，通过通知增强用户自我意识。

Result: 通过3周实验验证了假设，证明了概念的有效性。

Conclusion: 该通知干预方法能有效减少智能手机过度使用，且不引起用户厌烦。

Abstract: Excessive use of smartphones is a worldwide known issue. In this study, we
proposed a notification-based intervention approach to reduce smartphone
overuse without making the user feel any annoyance or irritation. Most of the
work in this field tried to reduce smartphone overuse by making smartphone use
more difficult for the user. In our user study (n = 109), we found that 19.3%
of the participants are unwilling to use any usage-limiting application because
a) they do not want their smartphone activities to get restricted or b) those
applications are annoying. Following that, we devised a hypothesis to minimize
smartphone usage among undergraduates. Finally, we designed a prototype for
Android, "App Usage Monitor," and conducted a 3-week experiment through which
we found proof of concept for our hypothesis. In our prototype, we combined
techniques such as nudge and visualization to increase self-awareness among the
user by leveraging notifications.

</details>


### [91] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct是一个可视化分析框架，用于在子群体中模拟、解释和推理个体层面的干预效果。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要集中在群体层面，难以应对显著异质性系统，因此需要支持个体层面的分析工具。

Method: 提出XplainAct框架，通过案例研究（阿片类药物相关死亡和总统选举投票倾向）验证其有效性。

Result: XplainAct能有效支持个体层面的干预分析和推理。

Conclusion: XplainAct为异质性系统中的因果推理提供了新的解决方案。

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [92] [Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs](https://arxiv.org/abs/2507.14769)
*Ananya Gubbi Mohanbabu,Yotam Sechayk,Amy Pavel*

Main category: cs.HC

TL;DR: 论文提出Task Mode系统，通过动态过滤网页内容，帮助屏幕阅读用户（SRUs）更高效地完成任务，减少与视觉用户（VUs）的差距。


<details>
  <summary>Details</summary>
Motivation: 现代网页界面过于复杂，对屏幕阅读用户造成不必要的负担，导致任务完成时间显著长于视觉用户。

Method: 利用大型语言模型动态过滤网页内容，基于用户目标识别并优先显示相关元素，同时保留页面结构。

Result: 用户研究表明，Task Mode显著减少SRUs的任务完成时间，将两组差距从2倍降至1.2倍，多数用户希望未来继续使用。

Conclusion: 通过为视觉和非视觉用户同时设计新交互方式，可以减少技术中的无障碍差异。

Abstract: Modern web interfaces are unnecessarily complex to use as they overwhelm
users with excessive text and visuals unrelated to their current goals. This
problem particularly impacts screen reader users (SRUs), who navigate content
sequentially and may spend minutes traversing irrelevant elements before
reaching desired information compared to vision users (VUs) who visually skim
in seconds. We present Task Mode, a system that dynamically filters web content
based on user-specified goals using large language models to identify and
prioritize relevant elements while minimizing distractions. Our approach
preserves page structure while offering multiple viewing modes tailored to
different access needs. Our user study with 12 participants (6 VUs, 6 SRUs)
demonstrates that our approach reduced task completion time for SRUs while
maintaining performance for VUs, decreasing the completion time gap between
groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the
future, reporting that Task Mode supported completing tasks with less effort
and fewer distractions. This work demonstrates how designing new interactions
simultaneously for visual and non-visual access can reduce rather than
reinforce accessibility disparities in future technology created by
human-computer interaction researchers and practitioners.

</details>


### [93] [SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors](https://arxiv.org/abs/2507.14792)
*Kaixin Ji,Danula Hettiachchi,Falk Scholer,Flora D. Salim,Damiano Spina*

Main category: cs.HC

TL;DR: SenseSeek数据集利用消费级传感器捕捉多阶段信息搜索行为中的生理和行为数据，为研究信息寻求行为提供新工具。


<details>
  <summary>Details</summary>
Motivation: 理解复杂信息处理行为需要个性化数据捕捉，消费级传感器提供解决方案。

Method: 收集20名参与者在235次搜索试验中的多阶段数据，包括EDA、EEG、PUPIL等传感器数据及258个特征。

Result: 验证数据集对不同认知意图和交互模态的影响，以及区分搜索阶段的有效性。

Conclusion: SenseSeek是首个多传感器表征信息寻求多阶段的数据集，为未来研究提供参考。

Abstract: Information processing tasks involve complex cognitive mechanisms that are
shaped by various factors, including individual goals, prior experience, and
system environments. Understanding such behaviors requires a sophisticated and
personalized data capture of how one interacts with modern information systems
(e.g., web search engines). Passive sensors, such as wearables, capturing
physiological and behavioral data, have the potential to provide solutions in
this context. This paper presents a novel dataset, SenseSeek, designed to
evaluate the effectiveness of consumer-grade sensors in a complex information
processing scenario: searching via systems (e.g., search engines), one of the
common strategies users employ for information seeking. The SenseSeek dataset
comprises data collected from 20 participants, 235 trials of the stimulated
search process, 940 phases of stages in the search process, including the
realization of Information Need (IN), Query Formulation (QF), Query Submission
by Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)
or Listening (RJ-L). The data includes Electrodermal Activities (EDA),
Electroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured
using consumer-grade sensors. It also contains 258 features extracted from the
sensor data, the gaze-annotated screen recordings, and task responses. We
validate the usefulness of the dataset by providing baseline analysis on the
impacts of different cognitive intents and interaction modalities on the sensor
data, and effectiveness of the data in discriminating the search stages. To our
knowledge, SenseSeek is the first dataset that characterizes the multiple
stages involved in information seeking with physiological signals collected
from multiple sensors. We hope this dataset can serve as a reference for future
research on information-seeking behaviors.

</details>


### [94] [Understanding How Visually Impaired Players Socialize in Mobile Games](https://arxiv.org/abs/2507.14818)
*Zihe Ran,Xiyu Li,Qing Xiao,Yanyun Wang,Franklin Mingzhe Li,Zhicong Lu*

Main category: cs.HC

TL;DR: 研究探讨了中国视障玩家在手机游戏中的社交体验，发现现有游戏虽满足部分需求，但技术障碍和社区分割仍是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 视障人群在线下社交中面临困难，手机游戏成为重要社交平台，但游戏设计和无障碍功能不足影响其体验。

Method: 通过采访30名视障玩家，分析他们在游戏中的社交参与和社区融入情况。

Result: 研究显示游戏满足了部分社交需求，但技术障碍和无障碍功能不足阻碍了更深入的参与。

Conclusion: 研究强调需设计更具包容性和无障碍的手机游戏，以提升视障玩家的社交体验。

Abstract: Mobile games are becoming a vital medium for social interaction, offering a
platform that transcends geographical boundaries. An increasing number of
visually impaired individuals are engaging in mobile gaming to connect,
collaborate, compete, and build friendships. In China, visually impaired
communities face significant social challenges in offline settings, making
mobile games a crucial avenue for socialization. However, the design of mobile
games and their mapping to real-world environments significantly shape their
social gaming experiences. This study explores how visually impaired players in
China navigate socialization and integrate into gaming communities. Through
interviews with 30 visually impaired players, we found that while mobile games
fulfill many of their social needs, technological barriers and insufficient
accessibility features, and internal community divisions present significant
challenges to their participation. This research sheds light on their social
experiences and offers insights for designing more inclusive and accessible
mobile games.

</details>


### [95] [Progressive Sentences: Combining the Benefits of Word and Sentence Learning](https://arxiv.org/abs/2507.14846)
*Nuwan Janaka,Shengdong Zhao,Ashwin Ram,Ruoxin Sun,Sherisse Tan Jing Wen,Danae Li,David Hsu*

Main category: cs.HC

TL;DR: 轻量级AR智能眼镜通过逐步呈现句子结构，支持移动二语习得。


<details>
  <summary>Details</summary>
Motivation: 探索AR智能眼镜在移动学习中如何支持二语习得，特别是通过多模态信息呈现。

Method: 提出逐步呈现法，依次展示句子成分（主、谓、宾），保留上下文，并测试其效果。

Result: 逐步呈现法提高记忆效果，尤其是在移动场景中；定时间隔进一步优化学习效果。

Conclusion: 逐步呈现法适用于移动学习场景，为教育应用提供实用指南。

Abstract: The rapid evolution of lightweight consumer augmented reality (AR) smart
glasses (a.k.a. optical see-through head-mounted displays) offers novel
opportunities for learning, particularly through their unique capability to
deliver multimodal information in just-in-time, micro-learning scenarios. This
research investigates how such devices can support mobile second-language
acquisition by presenting progressive sentence structures in multimodal
formats. In contrast to the commonly used vocabulary (i.e., word) learning
approach for novice learners, we present a "progressive presentation" method
that combines both word and sentence learning by sequentially displaying
sentence components (subject, verb, object) while retaining prior context.
Pilot and formal studies revealed that progressive presentation enhances
recall, particularly in mobile scenarios such as walking. Additionally,
incorporating timed gaps between word presentations further improved learning
effectiveness under multitasking conditions. Our findings demonstrate the
utility of progressive presentation and provide usage guidelines for
educational applications-even during brief, on-the-go learning moments.

</details>


### [96] [Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications](https://arxiv.org/abs/2507.14859)
*Nils Mandischer,Alexander Atanasyan,Ulrich Dahmen,Michael Schluse,Jürgen Rossmann,Lars Mikelsons*

Main category: cs.HC

TL;DR: 该论文提出了一个全面的人类数字孪生概念，明确了其定义、功能层级（存储、分析、个性化、预测、控制和优化），并通过实例展示了其可行性和市场潜力。


<details>
  <summary>Details</summary>
Motivation: 人类数字孪生是一个新兴但定义模糊的概念，目前缺乏对其市场潜力和实际应用的清晰认识。论文旨在填补这一空白，提供一个全面的框架。

Method: 提出一个全面的愿景，定义功能层级、利益相关者和用户群体，并通过具体的应用案例验证抽象层级的可行性。

Result: 从详细讨论中得出了人类数字孪生的全面需求列表，为研究和工业应用提供了指导。

Conclusion: 论文为人类数字孪生的实现提供了指南，特别是在多目标应用中的可重用性方面。

Abstract: The digital twin of humans is a relatively new concept. While many diverse
definitions, architectures, and applications exist, a clear picture is missing
on what, in fact, makes a human digital twin. Within this context, researchers
and industrial use-case owners alike are unaware about the market potential of
the - at the moment - rather theoretical construct. In this work, we draw a
holistic vision of the human digital twin, and derive the specification of this
holistic human digital twin in form of requirements, stakeholders, and users.
For each group of users, we define exemplary applications that fall into the
six levels of functionality: store, analyze, personalize, predict, control, and
optimize. The functionality levels facilitate an abstraction of abilities of
the human digital twin. From the manifold applications, we discuss three in
detail to showcase the feasibility of the abstraction levels and the analysis
of stakeholders and users. Based on the deep discussion, we derive a
comprehensive list of requirements on the holistic human digital twin. These
considerations shall be used as a guideline for research and industries for the
implementation of human digital twins, particularly in context of reusability
in multiple target applications.

</details>


### [97] [LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection](https://arxiv.org/abs/2507.14944)
*Boning Zhao,Yutong Hu*

Main category: cs.HC

TL;DR: 论文提出了一种名为LEKIA的新架构，旨在统一知识注入和价值对齐，解决了大型语言模型在高风险领域应用中的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署大语言模型时，需要同时满足动态的专家知识注入和细致的价值对齐，但现有方法往往无法兼顾二者。

Method: 通过LEKIA架构实现，包括理论层、实践层和评估层，不改变模型权重，引导模型推理过程。

Result: 成功开发了一个基于LEKIA的心理支持助手，展示了该架构的有效性。

Conclusion: LEKIA为专家提供了直接设计AI行为的途径，解决了知识与对齐之间的矛盾，推动了更负责任和专家驱动的AI发展。

Abstract: Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a
dual challenge: the need for deep, dynamic expert knowledge injection and
nuanced value alignment. Prevailing paradigms often address these challenges
separately, creating a persistent tension between knowledge and alignment;
knowledge-focused methods like Retrieval-Augmented Generation (RAG) have
limited deep alignment capabilities, while alignment-focused methods like
Reinforcement Learning from Human Feedback (RLHF) struggle with the agile
injection of expert wisdom. This paper introduces a new collaborative
philosophy, Expert-owned AI behavior design, realized through Architectural
Alignment-a paradigm that unifies these two goals within a single framework
called the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA
operates as an intelligent intermediary that guides an LLM's reasoning process
without altering its weights, utilizing a three-tiered structure: a Theoretical
Layer for core principles, a Practical Layer for exemplary cases, and an
Evaluative Layer for real-time, value-aligned self-correction. We demonstrate
the efficacy of this paradigm through the successful implementation of a
LEKIA-based psychological support assistant for the special education field.
Our work presents a path toward more responsible and expert-driven AI,
empowering domain specialists to directly architect AI behavior and resolve the
tension between knowledge and alignment.

</details>


### [98] [Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake](https://arxiv.org/abs/2507.14947)
*Ivan C. H. Liu,Chung-En Hao,Jing Xie*

Main category: cs.HC

TL;DR: 《Echoes of the Land》是一个将地震动力学转化为多感官体验的交互装置，通过弹簧块模型模拟地震重复性和自组织临界性。


<details>
  <summary>Details</summary>
Motivation: 该作品旨在结合科学知识与艺术实践，探索新兴复杂性、美学和交互性的交汇点。

Method: 利用弹簧块模型模拟地震动力学，通过运动捕捉和级联颗粒合成实时生成声音和光效。

Result: 每个块作为代理，产生突发的视听效果，可视化断层和阈值行为的物理现象。

Conclusion: 作品为新型乐器和叙事媒介开辟了新途径，并激发了进一步研究。

Abstract: Echoes of the Land is an interactive installation that transforms seismic
dynamics into a multisensory experience through a scientifically grounded
spring-block model. Simulating earthquake recurrence and self-organized
criticality, the work generates real-time sound and light via motion capture
and concatenative granular synthesis. Each block acts as an agent, producing
emergent audiovisual cascades that visualize the physics of rupture and
threshold behavior. This work exemplifies the amalgamation of scientific
knowledge and artistic practice, opening new avenues for novel forms of musical
instrument and narrative medium, while inviting further investigation into the
intersection of emergent complexity, aesthetics and interactivity.

</details>


### [99] [Emphasizing Deliberation and Critical Thinking in an AI Hype World](https://arxiv.org/abs/2507.14961)
*Katja Rogers*

Main category: cs.HC

TL;DR: 论文提倡在AI热潮中采取缓慢、深思熟虑的使用方式，结合批判性参与或不参与，以减少对教育和研究的负面影响。


<details>
  <summary>Details</summary>
Motivation: 为了应对AI解决方案主义在炒作和HCI对新颖性的推崇下的加速，作者认为完全禁止或放弃技术并不可行，也不一定有益。

Method: 提出通过缓慢、慎重的使用方式，辅以批判性参与或不参与，来应对AI热潮。

Result: 这种策略有助于在AI热潮后建立坚实的知识基础，并减少对教育和研究的危害。

Conclusion: 在AI热潮中，慢速、批判性的使用方式比完全禁止技术更具可持续性和效益。

Abstract: AI solutionism is accelerated and substantiated by hype and HCI's elevation
of novelty. Banning or abandoning technology is unlikely to work and probably
not beneficial on the whole either -- but slow(er), deliberate use together
with conscientious, critical engagement and non-engagement may help us navigate
a post-AI hype world while contributing to a solid knowledge foundation and
reducing harmful impacts in education and research.

</details>


### [100] ['A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data](https://arxiv.org/abs/2507.15033)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 研究通过LDA主题建模和情感分析，分析Reddit上五个LGBTQ+相关子论坛的内容，探讨疫情期间社交媒体对该群体的支持和影响。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，社交媒体成为年轻人主要沟通平台，讨论疫情危机。研究旨在了解LGBTQ+群体在Reddit上的讨论主题和情感倾向。

Method: 使用LDA主题建模和情感分析技术，分析五个LGBTQ+相关子论坛的数据。

Result: 研究揭示了疫情期间Reddit对LGBTQ+群体的支持作用及讨论主题的变化。

Conclusion: Reddit在疫情期间为LGBTQ+群体提供了重要支持，同时讨论主题与疫情前相比有一定变化。

Abstract: Social media was one of the most popular forms of communication among young
people with digital access during the pandemic. Consequently, crucial debates
and discussions about the pandemic crisis have also developed on social media
platforms, making them a great primary source to study the experiences of
specific groups and communities during the pandemic. This study involved
research using LDA topic modeling and sentiment analysis on data obtained from
the social media platform Reddit to understand the themes and attitudes in
circulation within five subreddits devoted to LGBTQ+ experiences and issues. In
the process, we attempt to make sense of the role that Reddit may have played
in the lives of LGBTQ+ people who were online during the pandemic, and whether
this was marked by any continuities or discontinuities from before the pandemic
period.

</details>


### [101] [Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic](https://arxiv.org/abs/2507.15041)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 研究分析了印度两家英语新闻网站在新冠疫情期间对LGBTQ+群体的报道情况，发现报道集中在跨性别群体，但质量不足，部分语言带有偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨印度在线新闻媒体在疫情期间如何报道LGBTQ+群体的生活现实，尤其是法律认可晚的跨性别群体。

Method: 通过情感分析和主题建模分析2020年3月至2021年8月的文章，并与2019年同期对比。

Result: 报道多关注跨性别群体，但缺乏深度，部分文章语言过时且带有偏见。

Conclusion: 研究揭示了疫情下印度新闻网对LGBTQ+群体的可见度和代表性问题，呼吁改进报道质量。

Abstract: In India, online news media outlets were an important source of information
for people with digital access during the COVID-19 pandemic. In India, where
"transgender" was legally recognised as a category only in 2014, and same-sex
marriages are yet to be legalised, it becomes crucial to analyse whether and
how they reported the lived realities of vulnerable LGBTQ+ communities during
the pandemic. This study analysed articles from online editions of two
English-language newspaper websites, which differed vastly in their circulation
figures-The Times of India and The Indian Express. The results of our study
suggest that these newspaper websites published articles surrounding various
aspects of the lives of LGBTQ+ individuals with a greater focus on transgender
communities. However, they lacked quality and depth. Focusing on the period
spanning March 2020 to August 2021, we analysed articles using sentiment
analysis and topic modelling. We also compared our results to the period before
the pandemic (January 2019 - December 2019) to understand the shift in topics,
sentiments, and stances across the two newspaper websites. A manual analysis of
the articles indicated that the language used in certain articles by The Times
of India was transphobic and obsolete. Our study captures the visibility and
representation of the LGBTQ+ communities in Indian newspaper websites during
the pandemic.

</details>


### [102] [Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence](https://arxiv.org/abs/2507.15049)
*Andres Navarro,Carlos de Quinto,José Alberto Hernández*

Main category: cs.HC

TL;DR: 本文介绍了一种集成了5G通信、边缘计算和AI的低成本无人机平台，用于解决非地面网络场景中的核心挑战，并在紧急响应、基础设施评估和环境监测等场景中展示了其适应性。


<details>
  <summary>Details</summary>
Motivation: 非地面网络中无人机作为智能节点的应用需求日益增长，但传统系统往往成本高且功能有限。本文旨在开发一种经济高效的无人机平台，结合5G、边缘计算和AI技术，解决这些挑战。

Method: 开发了一款配备全景摄像头、强大机载计算能力和大语言模型（LLMs）的无人机系统，结合5G通信和虚拟现实（VR）技术，实现低延迟视觉流处理和无缝对象识别。

Result: 实地测试表明，该平台能够以低延迟处理视觉数据，并维持稳定的5G连接。LLMs的应用进一步优化了数据收集和决策支持。

Conclusion: 该无人机平台展示了在紧急响应、基础设施评估和环境监测等多样化场景中的高效性和适应性，为非地面网络的智能化提供了可行方案。

Abstract: Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as
agile, intelligent nodes capable of advanced analytics and instantaneous
situational awareness. This article introduces a budget-friendly quadcopter
platform that unites 5G communications, edge-based processing, and AI to tackle
core challenges in NTN scenarios. Outfitted with a panoramic camera, robust
onboard computation, and LLMs, the drone system delivers seamless object
recognition, contextual analysis, and immersive operator experiences through
virtual reality VR technology. Field evaluations confirm the platform's ability
to process visual streams with low latency and sustain robust 5G links. Adding
LLMs further streamlines operations by extracting actionable insights and
refining collected data for decision support. Demonstrated use cases, including
emergency response, infrastructure assessment, and environmental surveillance,
underscore the system's adaptability in demanding contexts.

</details>


### [103] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: 论文提出了一种多模态引导模拟器，帮助盲人和低视力（BLV）操作者在工业仓库中安全远程控制机器人，通过视觉、听觉和触觉反馈实现导航。


<details>
  <summary>Details</summary>
Motivation: 工业仓库环境复杂，BLV操作者远程控制机器人存在高风险，现有研究缺乏针对BLV用户的多模态引导系统设计。

Method: 开发了一个结合导航网格、实时路径规划和多模态反馈（视觉路径线、语音提示、触觉警报）的模拟器系统。

Result: 系统为BLV用户提供了实时、闭环的导航支持，并可快速适配到实际机器人硬件上。

Conclusion: 该模拟器为可访问远程控制研究提供了可重复测试平台，其设计原则易于应用于实际仓库环境。

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [104] ["If I were in Space": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives](https://arxiv.org/abs/2507.15081)
*Qi Gong,Ximing Shen,Ziyou Yin,Yaning Li,Ray Lc*

Main category: cs.HC

TL;DR: 该研究通过社交VR中的协作在线叙事体验，探索人们在隔离期间的适应策略，发现叙事设计比传统解决方案更能揭示应对方式。


<details>
  <summary>Details</summary>
Motivation: 社会孤立可能导致焦虑和孤独等健康问题，但以往研究忽略了叙事策略的潜力。

Method: 设计了一个社交VR中的协作叙事体验，18名隔离参与者通过虚拟角色扮演设计太空旅程，揭示适应策略。

Result: 参与者通过设计和互动展现了应对隔离的创造性策略，叙事体验意外影响了他们的适应过程。

Conclusion: 研究表明，叙事设计比解决方案驱动的方法更能有效揭示人们如何应对社会孤立。

Abstract: Social isolation can lead to pervasive health issues like anxiety and
loneliness. Previous work focused on physical interventions like exercise and
teleconferencing, but overlooked the narrative potential of adaptive
strategies. To address this, we designed a collaborative online storytelling
experience in social VR, enabling participants in isolation to design an
imaginary space journey as a metaphor for quarantine, in order to learn about
their isolation adaptation strategies in the process. Eighteen individuals
participated during real quarantine undertaken a virtual role-play experience,
designing their own spaceship rooms and engaging in collaborative activities
that revealed creative adaptative strategies. Qualitative analyses of
participant designs, transcripts, and interactions revealed how they coped with
isolation, and how the engagement unexpectedly influenced their adaptation
process. This study shows how designing playful narrative experiences, rather
than solution-driven approaches, can serve as probes to surface how people
navigate social isolation.

</details>


### [105] [TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style](https://arxiv.org/abs/2507.15202)
*Karim Benharrak,Puyuan Peng,Amy Pavel*

Main category: cs.HC

TL;DR: TalkLess 是一种结合提取和抽象方法的语音编辑系统，旨在高效压缩语音内容的同时保留说话者的风格，并减少编辑负担。


<details>
  <summary>Details</summary>
Motivation: 现有语音编辑工具在提取句子或抽象概括时存在灵活性不足或丢失说话者风格的局限性，TalkLess 旨在解决这些问题。

Method: TalkLess 首先生成可能的转录编辑方案，选择最优编辑以平衡压缩率、内容覆盖率和音频质量，随后利用语音编辑模型将文本编辑转换为音频编辑。

Result: 与现有最佳提取方法相比，TalkLess 提高了内容覆盖率并减少了语音错误。用户研究显示其显著降低了认知负担和编辑工作量。

Conclusion: TalkLess 通过灵活的编辑方法和用户友好界面，为语音创作者提供了高效且高质量的编辑工具。

Abstract: Millions of people listen to podcasts, audio stories, and lectures, but
editing speech remains tedious and time-consuming. Creators remove unnecessary
words, cut tangential discussions, and even re-record speech to make recordings
concise and engaging. Prior work automatically summarized speech by removing
full sentences (extraction), but rigid extraction limits expressivity. AI tools
can summarize then re-synthesize speech (abstraction), but abstraction strips
the speaker's style. We present TalkLess, a system that flexibly combines
extraction and abstraction to condense speech while preserving its content and
style. To edit speech, TalkLess first generates possible transcript edits,
selects edits to maximize compression, coverage, and audio quality, then uses a
speech editing model to translate transcript edits into audio edits. TalkLess's
interface provides creators control over automated edits by separating
low-level wording edits (via the compression pane) from major content edits
(via the outline pane). TalkLess achieves higher coverage and removes more
speech errors than a state-of-the-art extractive approach. A comparison study
(N=12) showed that TalkLess significantly decreased cognitive load and editing
effort in speech editing. We further demonstrate TalkLess's potential in an
exploratory study (N=3) where creators edited their own speech.

</details>


### [106] [How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective](https://arxiv.org/abs/2507.15244)
*Leixian Shen,Leni Yang,Haotian Li,Yun Wang,Yuyu Luo,Huamin Qu*

Main category: cs.HC

TL;DR: 该论文通过案例研究探讨了创意设计中实证研究如何影响创作工具的开发，揭示了当前不足并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解实证研究如何影响创作工具的开发，并探索如何加强两者的结合以提升创意设计的理论基础和实际效果。

Method: 研究方法包括综合分析46篇实证研究论文和48篇创作工具论文，辅以11位专家的访谈，通过上下文感知的引用分析和模式分类揭示实证研究对工具设计的影响。

Result: 研究结果揭示了实证研究对工具设计的常见模式以及影响应用的关键因素，如上下文相关性、颗粒度匹配和清晰度等。

Conclusion: 结论提出了促进实证研究与工具研究更紧密结合的建议，并讨论了未来的机遇，旨在强化创作工具的理论基础并提升实证研究的实际影响力。

Abstract: Empirical research in creative design deepens our theoretical understanding
of design principles and perceptual effects, offering valuable guidance for
innovating creation tools. However, how these empirical insights currently
influence the development of creation tools, and how their integration can be
enhanced in the future, remains insufficiently understood. In this paper, we
aim to unveil the gap through a case study on data videos, a prominent and
wide-spread medium for effective data storytelling. To achieve the goal, we
conducted a comprehensive analysis of 46 empirical research papers and 48
creation tool papers on data video, complemented by interviews with 11 experts.
Building upon a systematic collection and structured characterization of
empirical research by their methodologies (e.g., corpus analysis, comparative
evaluations) and component focus (e.g., visuals, motions, narratives, audio),
we conducted a context-aware citation analysis and revealed a taxonomy of
recurring patterns in how empirical findings inform tool design across citation
functions (e.g., problem framing, technical reference). Expert interviews
further uncovered researchers' practice patterns in applying empirical findings
(e.g., adaptation, synthesis, iteration, etc.) and identified key factors
influencing applicability, such as contextual relevance, granularity matching,
clarity, credibility, and feasibility. Finally, we derive suggestions and
discuss future opportunities to foster closer mutual engagement between
empirical and tool research, aiming to reinforce the theoretical grounding of
creation tools and enhance the practical impact of empirical research.

</details>


### [107] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO是一种结合偏好贝叶斯优化和元学习的新方法，显著提高样本效率，使用户在更少迭代中获得满意的视觉参数设定。


<details>
  <summary>Details</summary>
Motivation: 调整视觉参数（如亮度和对比度）时，由于搜索空间大且缺乏明确目标函数，用户通常只能依赖主观偏好，而现有方法（如PBO）需要较多轮次，不适用于普通用户。

Method: 提出Meta-PO，通过元学习推断先前用户的偏好并存储为模型，为新手用户智能生成候选设计，从而加速收敛并提供个性化结果。

Result: 实验表明，当用户目标相似时，Meta-PO仅需5.86次迭代即可达到满意效果；即使目标差异较大，也能在8次迭代内泛化。

Conclusion: Meta-PO通过高效、可泛化的优化方法，使个性化视觉优化更贴近普通用户，并具备广泛扩展接口个性化的潜力。

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [108] [Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools](https://arxiv.org/abs/2507.15433)
*Lou Schwartz,Mohammad Ghoniem,Valérie Maquil,Adrien Coppens,Johannes Hermen*

Main category: cs.HC

TL;DR: 研究了在墙尺寸显示器上使用桌面优化原型工具进行1:1比例设计的可用性，发现平板交互最舒适，并提出12条设计指南。


<details>
  <summary>Details</summary>
Motivation: 墙尺寸显示器的空间特性给用户界面设计带来挑战，1:1比例设计可能是解决方案的一部分。

Method: 通过两项用户研究和一项技术审查，评估了桌面优化原型工具在墙尺寸显示器上的可用性，考虑了两种显示设置和三种交互方法（触控、键盘+触摸板、平板）。

Result: 1:1比例设计受到认可，平板交互最舒适，混合交互模式有潜力，环境需注意。

Conclusion: 现有设计工具对墙尺寸显示器的支持不足，需进一步考虑界面元素布局和功能增强。

Abstract: Wall-Sized Displays have spatial characteristics that are difficult to
address during user interface design. The design at scale 1:1 could be part of
the solution. In this paper, we present the results of two user studies and one
technology review, exploring the usability of popular, desktop-optimized
prototyping tools, for designing at scale on Wall-Sized Displays. We considered
two wall-sized display setups, and three different interaction methods: touch,
a keyboard equipped with a touchpad, and a tablet. We observed that designing
at scale 1:1 was appreciated. Tablet-based interaction proved to be the most
comfortable interaction method, and a mix of interaction modalities is
promising. In addition, care must be given to the surrounding environment, such
as furniture. We propose twelve design guidelines for a design tool dedicated
to this specific context. Overall, existing user interface design tools do not
yet fully support design on and for wall-sized displays and require further
considerations in terms of placement of user interface elements and the
provision of additional features.

</details>


### [109] [Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays](https://arxiv.org/abs/2507.15443)
*Adrien Coppens,Valérie Maquil*

Main category: cs.HC

TL;DR: 提出一种基于头部注视数据的联合注意力测量方法，用于评估墙屏混合在场协作的质量。


<details>
  <summary>Details</summary>
Motivation: 需要一种适应房间规模体验且不显侵扰的稳健评估方法。

Method: 通过头部注视数据测量联合注意力，并在墙屏混合在场协作的用户研究中实施。

Result: 从初步数据中获得了一些关于联合注意力的见解。

Conclusion: 该方法为墙屏混合在场协作的评估提供了可行思路。

Abstract: To understand and quantify the quality of mixed-presence collaboration around
wall-sized displays, robust evaluation methodologies are needed, that are
adapted for a room-sized experience and are not perceived as obtrusive. In this
paper, we propose our approach for measuring joint attention based on head gaze
data. We describe how it has been implemented for a user study on mixed
presence collaboration with two wall-sized displays and report on the insights
we gained so far from its implementation, with a preliminary focus on the data
coming from one particular session.

</details>


### [110] [Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands](https://arxiv.org/abs/2507.15481)
*Yesica Duarte,Puneet Jain*

Main category: cs.HC

TL;DR: 论文通过XR装置《Waiting for Hands》批判VR的同情叙事，提出替代控制器和荒诞表演，挑战对残疾的刻板印象。


<details>
  <summary>Details</summary>
Motivation: VR常被标榜为‘终极共情机器’，但可能导致对残疾的简化或同情化。研究旨在通过XR装置颠覆这种叙事。

Method: 设计替代控制器并创作XR荒诞表演，通过部分遮挡电影和奇怪动作，制造不确定性体验。

Result: XR表演成功制造疏离感，挑战了沉浸式媒介的常规叙事，唤起对非标准身体的关注。

Conclusion: 研究提出XR表演可以通过荒诞和疏离的方式，引导参与者反思残疾叙事中的伦理立场。

Abstract: Virtual Reality (VR) is often described as the "ultimate empathy machine,"
framing disability as an experience to be simulated through such technologies,
which can reduce disability to a spectacle of pity or inspiration. In response,
we present Waiting for Hands (WfH), an interactive eXtended Reality (XR)
installation that critiques this logic by: (1) repurposing interaction norms in
XR through the creation of Alternative Controllers, and (2) staging an absurd
XR performance using the built controllers to disrupt sentimentalized
disability narratives. The performance involves eight people: two XR
participants on stage and six audience members watching a projected documentary
about Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR
users partially obscure the film, drawing attention through strange mouth and
hand movements performed in XR. This creates a layered experience that disrupts
direct engagement with Hema's story and introduces uncertainty. While XR is
often seen as a fully immersive, sensory-dominant medium, this piece subverts
that framing by using XR to produce absurdity and alienation. By challenging
empathy-driven and pitiable narratives of disability, we ask what ethical
stance an XR performance can take to attune participants to non-normative
embodiment while resisting spectacle.

</details>


### [111] [FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up](https://arxiv.org/abs/2507.15502)
*Chen Chen,Jianing Yin,Jiannong Cao,Zhiyuan Wen,Mingjin Zhang,Weixun Gao,Xiang Wang,Haihua Shu*

Main category: cs.HC

TL;DR: 论文提出了一种名为FollowUpBot的LLM驱动边缘部署机器人，用于术后护理和监测，解决了传统方法耗时长、现有数字解决方案互动不灵活或隐私泄露的问题。


<details>
  <summary>Details</summary>
Motivation: 传统术后随访方法耗时耗力，现有数字解决方案存在互动不灵活或隐私泄露的局限性，需要一种更高效、灵活且安全的技术。

Method: 通过边缘部署的LLM进行自适应面对面交流，动态规划最优路线，支持多种互动模式，并自动生成结构化随访报告。

Result: 实验证明FollowUpBot在随访互动中实现了高覆盖率和满意度，且报告生成准确率高。

Conclusion: FollowUpBot为术后随访提供了一种高效、灵活且隐私安全的解决方案。

Abstract: Postoperative follow-up plays a crucial role in monitoring recovery and
identifying complications. However, traditional approaches, typically involving
bedside interviews and manual documentation, are time-consuming and
labor-intensive. Although existing digital solutions, such as web
questionnaires and intelligent automated calls, can alleviate the workload of
nurses to a certain extent, they either deliver an inflexible scripted
interaction or face private information leakage issues. To address these
limitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed
robot for postoperative care and monitoring. It allows dynamic planning of
optimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face
conversations with patients through multiple interaction modes, ensuring data
privacy. Moreover, FollowUpBot is capable of automatically generating
structured postoperative follow-up reports for healthcare institutions by
analyzing patient interactions during follow-up. Experimental results
demonstrate that our robot achieves high coverage and satisfaction in follow-up
interactions, as well as high report generation accuracy across diverse field
types. The demonstration video is available at
https://www.youtube.com/watch?v=_uFgDO7NoK0.

</details>


### [112] [Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey](https://arxiv.org/abs/2507.15526)
*Antonio Perez,Avinash Singh,Jonathan Mitchell,Philip Swadling*

Main category: cs.HC

TL;DR: MR头戴显示器在飞行模拟训练中具有潜力，但需解决人类因素问题以确保训练效果和飞行员安全。


<details>
  <summary>Details</summary>
Motivation: 研究MR头戴显示器在飞行模拟训练中的人类因素挑战，并提出缓解策略，以确保其训练潜力的实现。

Method: 系统回顾现有文献，从法规角度探讨硬件、软件、人体工程学及心理干预措施。

Result: 识别关键人类因素挑战并提出缓解策略，同时评估这些策略在现有航空培训法规中的可行性。

Conclusion: 研究为MR航空培训指南提供见解，确保技术需求与飞行员福祉平衡，推动MR在航空训练中的应用。

Abstract: Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative
to traditional Flight Simulator Training Device (FSTD) displays, providing
immersion, realism and cost efficiency. However, these technologies require
management of human factors; cybersickness, visual fatigue and ergonomic
strain. If left unmitigated, these effects can hinder pilot performance and
training outcomes. For safety critical fields like aviation, addressing human
factors challenges is crucial for MR's training potential. This survey
systematically reviews the current literature identifying key human factors
challenges in MR HMD use in pilot training and examines strategies to mitigate
these barriers. Drawing on existing industry standards set by a leading
aviation authority, the review adopts a regulatory perspective to explore
hardware, software, ergonomic, physiological and psychological interventions
improving pilot comfort, safety and training effectiveness in an MR FSTD.
Additionally, it evaluates which of these interventions are most appropriate
and viable for MR pilot training under existing aviation training regulations,
ensuring that technical requirements and pilot wellbeing remain balanced. The
findings yield significant insights for the human dimensions of aviation
simulation training, highlighting how regulatory considerations shape the
practicality of mitigation measures. These insights inform emerging MR aviation
training guidelines and best practices, supporting MR's readiness to enhance
aviation training.

</details>


### [113] [FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold](https://arxiv.org/abs/2507.15559)
*Pan Hao,Dongyeop Kang,Nicholas Hinds,Qianwen Wang*

Main category: cs.HC

TL;DR: FLOWFORGE是一个交互式可视化工具，用于优化多智能体工作流设计，通过分层探索和实时指导解决复杂任务的设计挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专家直觉，导致设计固化或耗时试错，需更结构化、高效的设计工具。

Method: FLOWFORGE分三级（任务规划、智能体分配、优化）提供结构化可视化和实时设计模式指导。

Result: 用户研究和案例验证了工具的有效性和可用性，提升了工作流设计效率。

Conclusion: FLOWFORGE为多智能体工作流设计提供了高效支持，未来可进一步推广其应用。

Abstract: Multi-agent workflows have become an effective strategy for tackling
complicated tasks by decomposing them into multiple sub-tasks and assigning
them to specialized agents. However, designing optimal workflows remains
challenging due to the vast and intricate design space. Current practices rely
heavily on the intuition and expertise of practitioners, often resulting in
design fixation or an unstructured, time-consuming exploration of
trial-and-error. To address these challenges, this work introduces FLOWFORGE,
an interactive visualization tool to facilitate the creation of multi-agent
workflow through i) a structured visual exploration of the design space and ii)
in-situ guidance informed by established design patterns. Based on formative
studies and literature review, FLOWFORGE organizes the workflow design process
into three hierarchical levels (i.e., task planning, agent assignment, and
agent optimization), ranging from abstract to concrete. This structured visual
exploration enables users to seamlessly move from high-level planning to
detailed design decisions and implementations, while comparing alternative
solutions across multiple performance metrics. Additionally, drawing from
established workflow design patterns, FLOWFORGE provides context-aware, in-situ
suggestions at each level as users navigate the design space, enhancing the
workflow creation process with practical guidance. Use cases and user studies
demonstrate the usability and effectiveness of FLOWFORGE, while also yielding
valuable insights into how practitioners explore design spaces and leverage
guidance during workflow development.

</details>


### [114] [Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback](https://arxiv.org/abs/2507.15650)
*Gerben van der Hoek,Bastiaan Heeren,Rogier Bos,Paul Drijvers,Johan Jeuring*

Main category: cs.HC

TL;DR: 研究探讨了一种平衡的反馈策略，关注学生如何与之交互并评价其效果。


<details>
  <summary>Details</summary>
Motivation: 探索一种既能提供探索空间又能减少学习障碍的反馈策略，以优化计算机辅助形成性评估。

Method: 25名15-17岁学生在在线环境中完成线性与指数外推任务，收集屏幕记录和访谈数据。

Result: 学生能自我引导且不会因障碍而脱离学习，同时偏好平衡的反馈。

Conclusion: 平衡反馈策略促进了有效的学生-环境互动。

Abstract: Computer aided formative assessment can be used to enhance a learning
process, for instance by providing feedback. There are many design choices for
delivering feedback, that lead to a feedback strategy. In an informative
feedback strategy, students do not immediately receive information about the
correct response, but are offered the opportunity to retry a task to apply
feedback information. In this small-scale qualitative study, we explore an
informative feedback strategy designed to offer a balance between room for
exploration and mitigation of learning barriers. The research questions concern
the ways in which students interact with the feedback strategy and their
appreciation of error-specific feedback as opposed to worked-out solutions. To
answer these questions, twenty-five 15-to-17-year-old senior general secondary
education students worked for approximately 20 minutes on linear and
exponential extrapolation tasks in an online environment. Data included screen
captures of students working with the environment and post-intervention
interviews. Results showed that room for exploration offered opportunities for
self-guidance while mitigation of learning barriers prevented disengagement.
Furthermore, students appreciated balanced feedback. We conclude that the
balanced feedback strategy yielded fruitful student-environment interactions.

</details>


### [115] [Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions](https://arxiv.org/abs/2507.15692)
*Meng Chen,Akhil Iyer,Amy Pavel*

Main category: cs.HC

TL;DR: MLLMs帮助盲人和低视力人群获取视觉信息，但存在难以检测的错误。研究提出通过展示多模型响应变化来提高信息可靠性识别。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在盲人和低视力群体中使用时的错误检测问题，减少安全和社交风险。

Method: 设计一个包含多个MLLM响应变化的空间，开发原型系统并测试三种变化展示方式。

Result: 展示变化使不可靠信息识别能力提高4.9倍，15名参与者中14人更倾向于多响应展示。

Conclusion: 系统展示MLLM响应变化能显著提升信息可靠性评估，受到用户广泛认可。

Abstract: Multimodal large language models (MLLMs) provide new opportunities for blind
and low vision (BLV) people to access visual information in their daily lives.
However, these models often produce errors that are difficult to detect without
sight, posing safety and social risks in scenarios from medication
identification to outfit selection. While BLV MLLM users use creative
workarounds such as cross-checking between tools and consulting sighted
individuals, these approaches are often time-consuming and impractical. We
explore how systematically surfacing variations across multiple MLLM responses
can support BLV users to detect unreliable information without visually
inspecting the image. We contribute a design space for eliciting and presenting
variations in MLLM descriptions, a prototype system implementing three
variation presentation styles, and findings from a user study with 15 BLV
participants. Our results demonstrate that presenting variations significantly
increases users' ability to identify unreliable claims (by 4.9x using our
approach compared to single descriptions) and significantly decreases perceived
reliability of MLLM responses. 14 of 15 participants preferred seeing
variations of MLLM responses over a single description, and all expressed
interest in using our system for tasks from understanding a tornado's path to
posting an image on social media.

</details>


### [116] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: 研究探讨青少年对可定制AI聊天机器人（如Character.AI）的情感依赖问题，发现使用后可能出现心理困扰和现实关系干扰，并提出了改进设计建议。


<details>
  <summary>Details</summary>
Motivation: 关注青少年对GenAI驱动的聊天机器人（如Character.AI）的过度依赖及其对情感和社交生活的影响。

Method: 分析318名13-17岁青少年在Character.AI子论坛上的Reddit帖子，以了解过度依赖的模式。

Result: 青少年初始使用聊天机器人是为了情感支持或创意表达，但许多人发展出强烈依恋，影响现实关系和日常活动，并表现出心理困扰和难以摆脱的行为。

Conclusion: 研究建议改进聊天机器人设计以提升自我意识、支持现实互动，并让青少年参与开发更安全的数字工具。

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [117] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 提出了一种基于探针数据结构的隐式表示方法，用于高效重建无需显式几何的大规模复杂场景，并实现高质量的新视角合成。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法在大规模场景中效率和复杂度的矛盾，以及几何重建方法维护成本高的问题。

Method: 使用探针数据结构重建多尺度隐式场景表示，避免依赖显式几何，降低计算成本。

Result: 渲染性能与场景复杂度无关，适用于VR/AR等应用。

Conclusion: 探针数据结构为大规模场景重建提供了一种高效且可扩展的解决方案。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [118] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出了一种新型三阶段框架，通过单图像引导生成高质量3D场景，解决了多对象场景中生成质量与场景一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 解决从单RGB图像生成3D场景时对象生成质量与场景一致性的挑战。

Method: 采用三阶段框架：图像实例分割与修复、伪立体视图构建与模型选择、布局参数优化。

Result: 在几何精度和纹理保真度上优于现有方法，且在场景布局合成中表现卓越。

Conclusion: 该方法显著提升了3D场景生成的质量和一致性，适用于多对象场景。

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [119] [Time Series Information Visualization -- A Review of Approaches and Tools](https://arxiv.org/abs/2507.14920)
*Evandro S. Ortigossa,Fábio F. Dias,Diego C. Nascimento,Luis Gustavo Nonato*

Main category: cs.GR

TL;DR: 该论文综述了多特征时间序列数据的信息可视化技术与方法，探讨了如何通过可视化增强数据解读能力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在各领域普遍存在且复杂，可视化技术能帮助数据科学家更好地理解动态行为和发现模式，但目前仍需更全面的工具和方法。

Method: 通过综述现有时间序列可视化技术和方法，结合理论见解和设计指南，提出整合多种分析工具的丰富可视化系统。

Result: 论文总结了多特征时间序列可视化的关键技术和挑战，并提供了未来研究的建议。

Conclusion: 时间序列可视化技术潜力巨大，但仍需进一步研究以解决开放性问题，尤其是在多特征数据的表达和分析方面。

Abstract: Time series data are prevalent across various domains and often encompass
large datasets containing multiple time-dependent features in each sample.
Exploring time-varying data is critical for data science practitioners aiming
to understand dynamic behaviors and discover periodic patterns and trends.
However, the analysis of such data often requires sophisticated procedures and
tools. Information visualization is a communication channel that leverages
human perceptual abilities to transform abstract data into visual
representations. Visualization techniques have been successfully applied in the
context of time series to enhance interpretability by graphically representing
the temporal evolution of data. The challenge for information visualization
developers lies in integrating a wide range of analytical tools into rich
visualization systems that can summarize complex datasets while clearly
describing the impacts of the temporal component. Such systems enable data
scientists to turn raw data into understandable and potentially useful
knowledge. This review examines techniques and approaches designed for handling
time series data, guiding users through knowledge discovery processes based on
visual analysis. We also provide readers with theoretical insights and design
guidelines for considering when developing comprehensive information
visualization approaches for time series, with a particular focus on time
series with multiple features. As a result, we highlight the challenges and
future research directions to address open questions in the visualization of
time-dependent data.

</details>


### [120] [Model Simplification through refinement](https://arxiv.org/abs/2507.15186)
*Dmitry Brodsky,Benjamin Watson*

Main category: cs.GR

TL;DR: 该论文提出了一种用于在交互速度下简化大型多边形模型的新算法，解决了现有算法速度慢或质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 随着建模和可视化应用的普及，需要一种能够快速简化大型多边形模型的算法，而现有方法无法满足这一需求。

Method: 受向量量化文献中分割算法的启发，该算法从极粗糙的近似开始逐步细化，利用表面曲率的近似来指导简化过程。

Result: 算法速度快，可以在给定时间限制内生成可显示的结果，同时保证了良好的质量。

Conclusion: 该算法适合在交互速度下简化大型模型，并能通过进一步细化现有简化结果来提升质量。

Abstract: As modeling and visualization applications proliferate, there arises a need
to simplify large polygonal models at interactive rates. Unfortunately existing
polygon mesh simplification algorithms are not well suited for this task
because they are either too slow (requiring the simplified model to be
pre-computed) or produce models that are too poor in quality. These
shortcomings become particularly acute when models are extremely large. We
present an algorithm suitable for simplification of large models at interactive
speeds. The algorithm is fast and can guarantee displayable results within a
given time limit. Results also have good quality. Inspired by splitting
algorithms from vector quantization literature, we simplify models in reverse,
beginning with an extremely coarse approximation and refining it.
Approximations of surface curvature guide the simplification process.
Previously produced simplifications can be further refined by using them as
input to the algorithm.

</details>


### [121] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 本文提出了一种基于修复的点云3D形状编辑框架，通过结合基础3D扩散模型和结构引导，实现了局部形状编辑并保持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 自然语言为3D形状的局部精细编辑提供了直观接口，但现有方法难以在局部修改时保持全局一致性。

Method: 采用基于修复的框架，结合3D扩散模型和部分条件形状的结构引导，提出推理时坐标混合算法。

Result: 实验表明，该方法在保真度和文本描述匹配度上优于其他技术。

Conclusion: 该方法实现了3D形状的精细编辑，无需复杂计算且避免了不准确性。

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [122] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS是一个结合3D场景重建与语义理解的对象感知框架，通过动态生成和优化局部锚点实现高精度对象级重建，并在开放词汇和全景分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 3D高斯喷洒技术虽然能实现高保真重建和实时新视角合成，但缺乏语义理解，限制了对象级感知能力。因此，作者提出ObjectGS，旨在通过对象感知框架统一3D重建与语义理解。

Method: ObjectGS将场景中的对象建模为局部锚点，动态生成和修剪这些锚点并优化其特征，同时通过独热编码ID和分类损失强制语义约束。

Result: 在开放词汇和全景分割任务中，ObjectGS超越了现有最先进方法，并能无缝集成到网格提取和场景编辑等应用中。

Conclusion: ObjectGS通过结合语义理解与3D重建，显著提升了对象级感知能力，并在多项任务和应用中表现出色。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [123] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 该论文提出了一种离散化SDF方法，通过在高斯基元中编码SDF值，避免了额外内存开销和复杂训练，提升了逆渲染质量。


<details>
  <summary>Details</summary>
Motivation: 高斯基元在逆渲染任务中因离散性难以应用几何约束，现有方法引入SDF但增加了内存和训练复杂度。作者希望通过离散化SDF解决这一问题。

Method: 在高斯基元中采样编码SDF值，通过SDF-to-opacity转换实现渲染，并设计投影一致性损失正则化离散样本。

Result: 实验表明，该方法在无额外内存开销和复杂优化情况下，显着提高了逆渲染质量。

Conclusion: 离散化SDF方法在高斯基元中有效结合了SDF的优势，提升了逆渲染性能，且实现简单高效。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [124] [Privacy-Preserving Drone Navigation Through Homomorphic Encryption for Collision Avoidance](https://arxiv.org/abs/2507.14713)
*Allan Luedeman,Nicholas Baum,Andrew Quijano,Kemal Akkaya*

Main category: cs.ET

TL;DR: 提出了一种基于同态加密的路径比较方法，以保护无人机配送路径隐私的同时避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 随着无人机配送的增加，共享飞行路径会泄露商业隐私（如配送路线），需要一种既能检测碰撞又能保护隐私的方案。

Method: 使用同态加密技术计算路径交集，无人机在不暴露路径细节的情况下识别潜在碰撞，并通过调整高度避免碰撞。

Result: 方法与基于混淆电路的方法相比速度更快、网络通信更少，且通过了安全性分析。

Conclusion: 该方法在资源受限的设备上有效，平衡了隐私保护和碰撞检测的需求。

Abstract: As drones increasingly deliver packages in neighborhoods, concerns about
collisions arise. One solution is to share flight paths within a specific zip
code, but this compromises business privacy by revealing delivery routes. For
example, it could disclose which stores send packages to certain addresses. To
avoid exposing path information, we propose using homomorphic encryption-based
comparison to compute path intersections. This allows drones to identify
potential collisions without revealing path and destination details, allowing
them to adjust altitude to avoid crashes. We implemented and tested our
approach on resource-limited virtual machines to mimic the computational power
of drones. Our results demonstrate that our method is significantly faster and
requires less network communication compared to a garbled circuit-based
approach. We also provide a security analysis of the approach against potential
attacks.

</details>


### [125] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: 本文提出了一种便携式、边缘支持的电子健康记录平台，专为资源有限环境设计，具有离线优先操作和模块化诊断功能，并通过贫血筛查模块验证其效果。


<details>
  <summary>Details</summary>
Motivation: 针对资源有限环境中医疗系统面临的互操作性差、缺乏离线支持和基础设施昂贵等问题，设计一个适用于偏远地区的便携式医疗解决方案。

Method: 开发了一个运行在小型嵌入式设备上的平台，支持AES-256本地加密存储和可选云同步，集成了基于随机森林的贫血筛查模块和优化的YOLOv8n指甲床检测器。

Result: 贫血筛查模块在测试数据上表现出较低的误差（RMSE 1.969 g/dL，MAE 1.490 g/dL），优化的检测器将推理延迟从46.96 ms降至21.50 ms。

Conclusion: 该平台通过低成本、模块化和隐私合规性为资源有限地区的数字健康提供了可行的解决方案。

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


### [126] [Advancing Lunar Communication through Inter-domain Space Networks and Dynamic Orchestration](https://arxiv.org/abs/2507.15483)
*Selen Gecgel Cetin,Baris Donmez,Gunes Karabulut Kurt*

Main category: cs.ET

TL;DR: 论文提出了一种基于近地空间网络的月球通信架构，以解决传统直接通信网络的局限性，并通过统一的链路分析框架和动态决策引擎，提高通信的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着月球探索从临时访问转向持续的国际和商业存在，传统直接通信网络的局限性（如覆盖不足、资源有限）无法满足需求，亟需一种更可靠的通信基础设施。

Method: 提出基于近地空间网络的通信架构，建立统一链路分析框架（考虑月球可变光照等因素），评估可靠性，并引入动态决策引擎（空间数字孪生）实时优化通信路径。

Result: 新架构能够提供高可靠性和多层通信骨干，同时动态决策引擎显著优化了通信性能和功耗。

Conclusion: 论文强调月球通信对永久人类和经济立足点的重要性，提出的综合框架为实现这一目标提供了可行方案。

Abstract: The reawakened era of lunar exploration is defined by a strategic shift from
temporary visits to a sustained international and commercial presence,
resulting in an unprecedented demand for a robust and continuously available
communication infrastructure. The conventional direct-to-Earth communication
architecture relies on limited and oversubscribed deep space networks, which
are further challenged by the radiative environment and insufficient visibility
in certain areas of the cislunar domain. We address these issues by proposing a
foundational move toward inter-domain space network cooperation by introducing
architectures based on near space networks. They can directly service lunar
surface users or, via cislunar relays, by forming a resilient and multi-layered
communication backbone. First, we establish a unified link analysis framework
incorporating frequently disregarded environmental factors, such as the Moon's
variable illumination, to provide a high-fidelity performance evaluation.
Second, we assess architectures' reliability based on the outage risk,
essential for quantifying the operational robustness of communication links.
Finally, to manage the inherent dynamism of architectures, we propose an
inter-domain space digital twin$-$a dynamic decision-making engine that
performs real-time analysis to autonomously select the best communication path,
ensuring high and stable reliability while simultaneously optimizing power
consumption. Overall, our paper provides a holistic architectural and
conceptual management framework, emphasizing the necessity of lunar
communications to support a permanent human and economic foothold on the Moon.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [127] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 论文提出了一种三步骤的边缘流处理自动扩展方法，包括GRU神经网络预测负载、转移学习框架处理领域差异，以及基于预测负载的动态并行度调整模块。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济发展，高速数据处理日益重要，但边缘流处理面临资源分配难题。现有反应式方法无法满足需求，强化学习等预测方法又需大量仿真。

Method: 1. GRU神经网络预测上游负载；2. 转移学习框架整合预测模型，处理离线与在线域差异；3. 动态水平扩展模块调整操作并行度。

Result: GRU模型在真实数据集上SMAPE值低至1.3\%，优于CNN、ARIMA和Prophet，且训练时间短于RL模型。

Conclusion: 该方案有效解决了边缘流处理的资源分配问题，兼具高精度和低计算开销。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [128] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 研究分布式LLM服务中的通信动态，分析不同并行化方法在推理过程中如何协调GPU间的数据交换，并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: 分布式推理框架虽能部署LLM，但GPU间通信的性能限制影响了服务质量，需研究通信动态以优化性能。

Method: 结合详细的分析模型和性能测量，研究不同并行化配置下的通信行为，包括张量并行和流水线并行。

Result: 张量并行网络开销大但响应快，流水线并行数据交换少但延迟高，组合方法需精心调优以实现平衡性能。

Conclusion: 为生产环境中的LLM服务选择合适的并行化方案提供建议，并为优化推理框架和通信基础设施指明方向。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [129] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 研究了在无方向共识的1-区间连通环形网络中，如何解决距离-$k$-分散问题，提出了一种利用局部信息模拟方向共识的方法，并证明了该问题在任何规模的网络中均可解。


<details>
  <summary>Details</summary>
Motivation: 探讨在缺乏方向共识的动态网络中，移动代理如何实现分散与协调，突破了传统对网络规模和方向的限制。

Method: 提出了一种利用局部信息和有限内存模拟方向共识的技术，并设计了一个$O(ln)$轮次的算法来解决距离-$k$-分散问题。

Result: 证明了距离-$k$-分散问题在任何规模的环形网络中均可解，并提供了一个高效的算法。

Conclusion: 研究扩展了动态网络中移动代理协调的理论基础，澄清了方向共识在分布式计算中的作用。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [130] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME提出了一种分布式自适应的Transformer大模型定制方法，通过双向单循环系统优化性能和资源利用，显著降低了数据传输量并提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 解决云环境中大模型部署的数据隐私和延迟问题，同时应对集中式定制的高成本和用户/数据异构性带来的性能不平衡。

Method: 采用双向单循环分布式系统，分阶段定制主干和头部分，并通过Pareto Front优化资源利用和个性化架构聚合。

Result: 数据传输量降至6%，平均准确率提升10%，权衡指标增加近30%。

Conclusion: ACME有效解决了大模型定制中的成本和性能问题，具有显著的实际应用价值。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [131] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: 论文提出DecentLLMs，一种去中心化的多代理LLM系统共识方法，解决了传统基于领导者协调的脆弱性和低效问题。


<details>
  <summary>Details</summary>
Motivation: 克服单代理系统的局限性（如幻觉和单点故障），并解决现有拜占庭鲁棒多代理系统因依赖领导者而存在的脆弱性和低效问题。

Method: 采用去中心化架构，工作代理并行生成答案，评估代理独立评分和排序以选择最佳答案，结合拜占庭鲁棒聚合技术。

Result: 实验表明，DecentLLM能有效容忍拜占庭代理，显著提升所选答案的质量。

Conclusion: 去中心化的DecentLLM优于传统领导者驱动的系统，更高效且鲁棒。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [132] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: 论文提出了一种针对非结构化网格的异构CPU-GPU任务并行方法，通过将网格连通性计算卸载到GPU线程，提升可视化算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的任务并行数据结构受限于CPU资源，导致数据结构和分析算法竞争计算资源，限制了性能提升。

Method: 提出了一种异构CPU-GPU任务并行方法，将网格连通性计算卸载到GPU线程，同时设计了首个开源的CUDA-based数据结构GALE。

Result: 在20核CPU和NVIDIA V100 GPU上的实验表明，GALE比现有技术快2.7倍，同时保持内存效率。

Conclusion: GALE通过异构任务并行显著提升了非结构化网格的可视化性能，为未来研究提供了新思路。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [133] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: AMPED是一种多GPU并行算法，用于加速大规模稀疏张量分解中的MTTKRP计算，通过分区和动态负载平衡技术，在4个GPU上实现了5.1倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中的稀疏张量规模增长到数十亿非零值，传统的单GPU方法在内存和计算吞吐量上无法满足需求，需要多GPU并行解决方案。

Method: 提出AMPED算法，通过分区策略和动态负载平衡方案在多GPU间分配计算，减少GPU空闲时间。

Result: 在4个GPU上，AMPED比现有GPU基线方法快5.1倍。

Conclusion: AMPED成功解决了大规模稀疏张量分解中MTTKRP的计算瓶颈，显著提升了性能。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [134] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune动态调整Raft选举参数，显著降低OTS时间，提升SMR服务的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统Raft在动态网络条件下难以有效调整选举参数，导致OTS时间增加和服务响应性下降。

Method: 通过心跳测量网络指标，动态调整Raft的选举参数。

Result: 实验显示Dynatune将领导失效检测和OTS时间分别降低80%和45%。

Conclusion: Dynatune在不同网络场景下提升了SMR服务的性能和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [135] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 提出了一种基于多目标强化学习的联邦推荐系统参与者选择方法，优化客户端性能、数据效用和系统效率，显著提升训练效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 集中式推荐系统存在隐私和可扩展性问题，联邦推荐系统虽能解决这些问题，但仍面临设备异构性、数据非独立同分布和通信瓶颈的挑战。

Method: 设计了一个复合客户端效用函数，结合历史表现、系统能力和数据质量，并将其嵌入多臂老虎机框架，动态平衡探索与利用以选择参与者。

Result: 在四种数据分布不均的场景中，该方法将收敛时间缩短32-50%，总训练时间减少46%，同时保持了推荐性能。

Conclusion: 自适应、奖励驱动的客户端选择方法能显著提升联邦推荐系统的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [136] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出了一种基于NSGA-II的路由算法，用于在云边计算环境中分配LLM推理请求，优化响应质量、时间和成本，实验显示其显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)推理服务的需求增长带来了计算资源压力和延迟成本挑战，需要优化解决方案。

Method: 采用NSGA-II作为多目标优化算法，动态分配请求到异构LLM实例，平衡响应质量、时间和成本。

Result: 在SQuAD等数据集上测试，响应时间和成本分别提升95.2%和34.9%。

Conclusion: 该算法有效支持可扩展的LLM部署，适应动态工作负载。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [137] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA是一种结合大型语言模型和混合检索技术的模式匹配框架，无需标记数据或成对比较即可高效识别匹配候选。


<details>
  <summary>Details</summary>
Motivation: 解决异构数据源集成和数据集发现中复杂且资源密集的模式匹配问题。

Method: 采用基于提示的方法，结合向量和词法检索，丰富模式元数据。

Result: 在MIMIC-OMOP基准测试中，HitRate@5和HitRate@3分别提升7.49%和3.75%，达到最新性能。

Conclusion: SCHEMORA是首个基于LLM的开源模式匹配方法，强调了检索的关键作用和模型选择的实用性。

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


### [138] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: 本文提出了一种名为Mayura的新框架，通过利用多时态模体的结构共性，减少了冗余计算，提高了挖掘效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法独立处理每个查询时存在大量冗余计算，限制了效率和扩展性。

Method: 提出Motif-Group Tree (MG-Tree)数据结构和协同挖掘算法，支持CPU和GPU高效运行。

Result: 实验结果显示，Mayura比现有技术平均提速2.4倍（CPU）和1.7倍（GPU），且保持精确性。

Conclusion: Mayura显著提升了时态模体挖掘的效率，适用于高精度要求的应用场景。

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


### [139] [Towards Temporal Knowledge Graph Alignment in the Wild](https://arxiv.org/abs/2507.14475)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Xiang Zhao,Jiuyang Tang,Lei Chen*

Main category: cs.DB

TL;DR: 提出了HyDRA方法，通过多尺度超图检索增强生成解决复杂时空知识图对齐问题，并在新数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设统一的时空标准和简化结构，无法处理现实世界中多尺度时空元素纠缠和跨源时空结构不平衡问题。

Method: HyDRA采用多尺度超图检索增强生成，结合尺度内交互和跨尺度冲突检测机制。

Result: 在新数据集BETA和WildBETA上表现优异，优于24个基准方法，具有高效性和可扩展性。

Conclusion: HyDRA为复杂时空知识图对齐提供了新范式，新数据集更真实反映现实挑战。

Abstract: Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent
entities across heterogeneous temporal knowledge graphs (TKGs) for fusion to
improve their completeness. Although some approaches have been proposed to
tackle this task, most assume unified temporal element standards and simplified
temporal structures across different TKGs. They cannot deal with TKGA in the
wild (TKGA-Wild), where multi-scale temporal element entanglement and
cross-source temporal structural imbalances are common. To bridge this gap, we
study the task of TKGA-Wild and propose HyDRA, a new and effective solution.
HyDRA is the first to reformulate the task via multi-scale hypergraph
retrieval-augmented generation to address the challenges of TKGA-Wild.In
addition, we design a new scale-weave synergy mechanism for HyDRA, which
incorporates intra-scale interactions and cross-scale conflict detection. This
mechanism is designed to alleviate the fragmentation caused by multi-source
temporal incompleteness and resolves inconsistencies arising from complex and
uneven temporal event density distributions, thereby enhancing the model
capacity to handle the intricacies of real-world temporal alignment. Finally,
there is no standard benchmark that captures these challenges of TKGA-Wild and
effectively evaluates existing methods. To this end, we formally propose to
benchmark challenges for TKGA-Wild and validate the effectiveness of the method
by establishing two new datasets(BETA and WildBETA). Extensive experiments on
the new datasets and six representative benchmarks show that BETA and WildBETA
better reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm
for TKGA-Wild, consistently outperforming 24 competitive baselines, while
maintaining strong efficiency and scalability.

</details>


### [140] [Opening The Black-Box: Explaining Learned Cost Models For Databases](https://arxiv.org/abs/2507.14495)
*Roman Heinrich,Oleksandr Havrylov,Manisha Luthra,Johannes Wehrstein,Carsten Binnig*

Main category: cs.DB

TL;DR: 论文提出了一种解释学习成本模型（LCMs）的方法，通过AI可解释性技术解决LCMs预测尾部误差大的问题，并开发了交互式工具展示其工作原理。


<details>
  <summary>Details</summary>
Motivation: LCMs虽优于传统数据库成本模型，但仍有尾部预测误差大的问题，且因其基于复杂深度神经网络模型，难以定位误差根源。

Method: 扩展现有AI可解释性方法，开发了专门针对LCMs的新解释技术，并创建了交互式工具。

Result: 实现了LCMs的可解释性，使其可通过系统化调试改进。

Conclusion: 这是LCMs可调试的第一步，为系统化解决其问题铺平了道路。

Abstract: Learned Cost Models (LCMs) have shown superior results over traditional
database cost models as they can significantly improve the accuracy of cost
predictions. However, LCMs still fail for some query plans, as prediction
errors can be large in the tail. Unfortunately, recent LCMs are based on
complex deep neural models, and thus, there is no easy way to understand where
this accuracy drop is rooted, which critically prevents systematic
troubleshooting. In this demo paper, we present the very first approach for
opening the black box by bringing AI explainability approaches to LCMs. As a
core contribution, we developed new explanation techniques that extend existing
methods that are available for the general explainability of AI models and
adapt them significantly to be usable for LCMs. In our demo, we provide an
interactive tool to showcase how explainability for LCMs works. We believe this
is a first step for making LCMs debuggable and thus paving the road for new
approaches for systematically fixing problems in LCMs.

</details>


### [141] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: 论文提出了一种名为IDSS的新型大规模数据存储工具，用于解决传统数据库管理系统在可扩展性和处理大规模异构数据时的低效问题。


<details>
  <summary>Details</summary>
Motivation: 数据生成速度的快速增长带来了管理挑战，传统数据库系统在可扩展性和处理大规模异构数据时表现低效。

Method: IDSS利用点对点网络和嵌入式关系数据库，设计了支持分布式查询的架构，并提供了复杂查询处理的方法。

Result: IDSS能够高效且稳健地管理大量数据，支持分布式查询和复杂查询处理。

Conclusion: IDSS为大规模数据管理提供了一种高效的解决方案，结合了P2P网络和关系数据库的优势。

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [142] [SpeedLLM: An FPGA Co-design of Large Language Model Inference Accelerator](https://arxiv.org/abs/2507.14139)
*Peipei Wang,Wu Guan,Liping Liang,Zhijun Wang,Hanqing Luo,Zhibin Zhang*

Main category: cs.AR

TL;DR: SpeedLLM是一种基于Xilinx Alevo U280平台的神经网络加速器，专为Tinyllama框架优化，提升边缘计算性能。通过数据流并行、内存重用策略和Llama2算子融合，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 为了在边缘计算设备中实现更高效的神经网络推理，减少延迟和能量消耗，设计了SpeedLLM。

Method: 采用了数据流并行、内存重用策略和Llama2算子融合技术，优化数据流水线架构和内存使用效率。

Result: SpeedLLM相比传统Tinyllama实现，性能提升4.8倍，能耗降低1.18倍。

Conclusion: SpeedLLM为边缘设备提供了高效、低能耗的神经网络加速解决方案。

Abstract: This paper introduces SpeedLLM, a neural network accelerator designed on the
Xilinx Alevo U280 platform and optimized for the Tinyllama framework to enhance
edge computing performance. Key innovations include data stream parallelism, a
memory reuse strategy, and Llama2 operator fusion, which collectively reduce
latency and energy consumption. SpeedLLM's data pipeline architecture optimizes
the read-compute-write cycle, while the memory strategy minimizes FPGA resource
demands. The operator fusion boosts computational density and throughput.
Results show SpeedLLM outperforms traditional Tinyllama implementations,
achieving up to 4.8* faster performance and 1.18* lower energy consumption,
offering improvements in edge devices.

</details>


### [143] [Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need](https://arxiv.org/abs/2507.14397)
*Michael Davies,Neal Crago,Karthikeyan Sankaralingam,Christos Kozyrakis*

Main category: cs.AR

TL;DR: 本文研究了基于Transformer的大型语言模型（LLM）推理的性能瓶颈，分析了内存带宽、容量和分布式系统中的同步开销，并开发了一个硬件无关的性能模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示LLM推理中的性能瓶颈，为未来硬件设计和部署策略提供指导。

Method: 通过硬件无关的性能模型，分析多种内存技术（如HBM3、HBM4、3D-stacked DRAM）和分布式系统的效率。

Result: 关键发现包括：1) 需要数百GB内存；2) 高内存带宽对吞吐至关重要；3) 同步延迟需极低；4) DRAM设计效率更高；5) 未来需算法优化。

Conclusion: 研究为LLM推理的性能极限提供了重要见解，指导未来硬件优化和部署。

Abstract: This paper presents a limit study of transformer-based large language model
(LLM) inference, focusing on the fundamental performance bottlenecks imposed by
memory bandwidth, memory capacity, and synchronization overhead in distributed
inference systems. We develop a hardware-agnostic performance model that
abstracts away implementation details, enabling the analysis of a wide range of
current and near-future hardware technologies. Our analysis spans from current
HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems
based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers
SRAM-based designs and scaling techniques from distributed clusters with
varying numbers of chips to wafer-scale integration. Our key findings for
auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to
serve a model instance; ii) high memory bandwidth is critical for high per-user
throughput; iii) exposed synchronization latencies to achieve collective
communication must be around 1us else they make the memory bandwidth
ineffective; iv) DRAM-based designs have a fundamental advantage in terms of
system-level efficiency as measured in throughput per cost or watt; and v)
hardware designs can easily reach 2000+ user token/sec but getting to 10,000+
tokens/sec will need smaller models, smaller context, or other forms of
algorithmic advances. This study provides valuable insights into the
fundamental performance limits of LLM inference, highlighting the potential
benefits of future hardware advancements and guiding the optimization of LLM
deployment strategies.

</details>


### [144] [Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge](https://arxiv.org/abs/2507.14651)
*Joren Dumoulin,Pouya Houshmand,Vikram Jain,Marian Verhelst*

Main category: cs.AR

TL;DR: 混合视觉Transformer结合传统神经网络和视觉Transformer，实现轻量高精度检测，但在资源受限的边缘设备上仍面临挑战。本文通过硬件调度堆栈创新，优化计算效率，28nm CMOS实现1.39 TOPS/W的能效。


<details>
  <summary>Details</summary>
Motivation: 混合ViT模型在边缘设备上部署时，因层类型多样和大中间数据张量而效率不足，需优化硬件支持。

Method: 提出多级硬件调度优化：可配置PE阵列支持所有层类型；层内循环重排序减少数据传输；层间融合降低内存传输。

Result: 28nm CMOS芯片实现1.39 TOPS/W的峰值能效，计算能力25.6 GMACs/s。

Conclusion: 通过硬件调度创新，显著提升混合ViT在边缘设备上的计算效率，为实际部署提供解决方案。

Abstract: Hybrid vision transformers combine the elements of conventional neural
networks (NN) and vision transformers (ViT) to enable lightweight and accurate
detection. However, several challenges remain for their efficient deployment on
resource-constrained edge devices. The hybrid models suffer from a widely
diverse set of NN layer types and large intermediate data tensors, hampering
efficient hardware acceleration. To enable their execution at the edge, this
paper proposes innovations across the hardware-scheduling stack: a.) At the
lowest level, a configurable PE array supports all hybrid ViT layer types; b.)
temporal loop re-ordering within one layer, enabling hardware support for
normalization and softmax layers, minimizing on-chip data transfers; c.)
further scheduling optimization employs layer fusion across inverted bottleneck
layers to drastically reduce off-chip memory transfers. The resulting
accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of
1.39 TOPS/W at 25.6 GMACs/s.

</details>


### [145] [GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing](https://arxiv.org/abs/2507.15300)
*Minnan Pei,Gang Li,Junwen Si,Zeyu Zhu,Zitao Mo,Peisong Wang,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.AR

TL;DR: 论文提出了一种新型3D高斯溅射（3DGS）加速器GCC，解决了现有加速器中预处理和渲染数据流的低效问题，通过跨阶段条件处理和高斯逐渲染方法，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS加速器采用解耦的预处理-渲染数据流，导致大量预处理的Gaussians未被使用，且同一Gaussian在不同区块渲染中重复加载，产生巨大的计算和数据移动开销。

Method: 提出GCC加速器，引入跨阶段条件处理动态跳过不必要的预处理，以及高斯逐渲染避免重复加载；还提出基于alpha的边界识别方法以减小渲染成本。

Result: GCC在28nm工艺下实现，实验表明其在性能和能效上显著优于现有最佳3DGS加速器GSCore。

Conclusion: GCC通过优化数据流和减少冗余操作，为3DGS推理提供了高效且节能的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering
technique for high-fidelity view synthesis, prompting the development of
dedicated 3DGS accelerators for mobile applications. Through in-depth analysis,
we identify two major limitations in the conventional decoupled
preprocessing-rendering dataflow adopted by existing accelerators: 1) a
significant portion of preprocessed Gaussians are not used in rendering, and 2)
the same Gaussian gets repeatedly loaded across different tile renderings,
resulting in substantial computational and data movement overhead. To address
these issues, we propose GCC, a novel accelerator designed for fast and
energy-efficient 3DGS inference. At the dataflow level, GCC introduces: 1)
cross-stage conditional processing, which interleaves preprocessing and
rendering to dynamically skip unnecessary Gaussian preprocessing; and 2)
Gaussian-wise rendering, ensuring that all rendering operations for a given
Gaussian are completed before moving to the next, thereby eliminating
duplicated Gaussian loading. We also propose an alpha-based boundary
identification method to derive compact and accurate Gaussian regions, thereby
reducing rendering costs. We implement our GCC accelerator in 28nm technology.
Extensive experiments demonstrate that GCC significantly outperforms the
state-of-the-art 3DGS inference accelerator, GSCore, in both performance and
energy efficiency.

</details>


### [146] [The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts](https://arxiv.org/abs/2507.15465)
*Sungmin Yun,Seonyong Park,Hwayong Nam,Younjoo Lee,Gunjun Lee,Kwanhee Kyung,Sangpyo Kim,Nam Sung Kim,Jongmin Kim,Hyungyo Kim,Juhwan Cho,Seungmin Baek,Jung Ho Ahn*

Main category: cs.AR

TL;DR: 传统Transformer模型的计算负载分为两部分：内存受限的多头注意力（MHA）和计算受限的馈送层。本文指出，新的架构如多头潜在注意力（MLA）和专家混合（MoE）改变了这一现状，使得专用注意力硬件的需求减少。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决传统Transformer中MHA内存瓶颈问题，并探讨新架构（MLA和MoE）是否消除了对专用硬件的需求。

Method: 通过分析MLA和MoE的计算特性，比较其算术强度与MHA的差异，并探讨如何通过批量调整MoE的计算强度以匹配密集层。

Result: MLA的算术强度比MHA高两个数量级，接近计算受限状态；MoE的算术强度可通过批量调整，使得模型计算更加均衡。

Conclusion: 结论表明，未来的研究重点应从专用注意力硬件转向设计平衡的系统，以满足大规模模型的多样需求。

Abstract: Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.

</details>


### [147] [When Pipelined In-Memory Accelerators Meet Spiking Direct Feedback Alignment: A Co-Design for Neuromorphic Edge Computing](https://arxiv.org/abs/2507.15603)
*Haoxiong Ren,Yangu He,Kwunhang Wong,Rui Bao,Ning Lin,Zhongrui Wang,Dashan Shang*

Main category: cs.AR

TL;DR: 论文提出了一种名为PipeSDFA的软硬件协同设计方法，通过硬件友好的SDFA算法和RRAM-based IMC架构，加速SNN训练，减少计算复杂度和能耗。


<details>
  <summary>Details</summary>
Motivation: SNN在资源受限的边缘设备上具有能效优势，但传统反向传播算法在训练SNN时计算量大，亟需高效的训练方法。

Method: 提出SDFA算法减少计算复杂度，并基于RRAM-based IMC架构设计三级流水线数据流，并行化训练过程。

Result: PipeSDFA在五个数据集上精度损失小于2%，训练时间和能耗分别比PipeLayer减少1.1X~10.5X和1.37X~2.1X。

Conclusion: PipeSDFA通过软硬件协同设计，实现了高效的SNN训练，适合边缘设备应用。

Abstract: Spiking Neural Networks (SNNs) are increasingly favored for deployment on
resource-constrained edge devices due to their energy-efficient and
event-driven processing capabilities. However, training SNNs remains
challenging because of the computational intensity of traditional
backpropagation algorithms adapted for spike-based systems. In this paper, we
propose a novel software-hardware co-design that introduces a hardware-friendly
training algorithm, Spiking Direct Feedback Alignment (SDFA) and implement it
on a Resistive Random Access Memory (RRAM)-based In-Memory Computing (IMC)
architecture, referred to as PipeSDFA, to accelerate SNN training.
Software-wise, the computational complexity of SNN training is reduced by the
SDFA through the elimination of sequential error propagation. Hardware-wise, a
three-level pipelined dataflow is designed based on IMC architecture to
parallelize the training process. Experimental results demonstrate that the
PipeSDFA training accelerator incurs less than 2% accuracy loss on five
datasets compared to baselines, while achieving 1.1X~10.5X and 1.37X~2.1X
reductions in training time and energy consumption, respectively compared to
PipeLayer.

</details>


### [148] [VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability Repair](https://arxiv.org/abs/2507.15664)
*Haomin Qi,Yuyang Du,Lihao Zhang,Soung Chang Liew,Kexin Chen,Yining Du*

Main category: cs.AR

TL;DR: 提出了VeriRAG，第一个基于LLM的DFT-EDA框架，通过RAG方法实现自动化DFT合规性修复，并开源了VeriDFT数据集。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在CAD中的应用广泛，但DFT领域尚未充分探索，缺乏自动化调试和验证工具。

Method: 采用RAG方法，结合自动编码器检索相似RTL设计，并通过迭代代码修订确保DFT合规性。

Result: 实现了7.72倍的修复成功率提升，并开源了数据集和模型。

Conclusion: VeriRAG为DFT-EDA领域提供了高效的自动化解决方案。

Abstract: Large language models (LLMs) have demonstrated immense potential in
computer-aided design (CAD), particularly for automated debugging and
verification within electronic design automation (EDA) tools. However, Design
for Testability (DFT) remains a relatively underexplored area. This paper
presents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a
Retrieval-Augmented Generation (RAG) approach to enable LLM to revise code to
ensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity
measurement model for precise retrieval of reference RTL designs for the LLM,
and (2) an iterative code revision pipeline that allows the LLM to ensure DFT
compliance while maintaining synthesizability. To support VeriRAG, we introduce
VeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG
retrieves structurally similar RTL designs from VeriDFT, each paired with a
rigorously validated correction, as references for code repair. With VeriRAG
and VeriDFT, we achieve fully automated DFT correction -- resulting in a
7.72-fold improvement in successful repair rate compared to the zero-shot
baseline (Fig. 5 in Section V). Ablation studies further confirm the
contribution of each component of the VeriRAG framework. We open-source our
data, models, and scripts at https://github.com/yuyangdu01/LLM4DFT.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [149] [Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition of Children for On-device Edge Applications](https://arxiv.org/abs/2507.14451)
*Satwik Dutta,Shruthigna Chandupatla,John Hansen*

Main category: eess.AS

TL;DR: 开发了一种轻量级Whisper ASR系统，适用于树莓派，隐私保护设计，通过低秩压缩优化性能。


<details>
  <summary>Details</summary>
Motivation: 由于法规和隐私挑战，需要开发本地运行的轻量级ASR系统以支持儿童语音应用。

Method: 基于MyST语料库评估，采用过滤策略微调tiny.en模型，并应用低秩压缩优化性能。

Result: WER为15.9%（过滤后11.8%），压缩后模型体积减小，推理速度提升，树莓派运行表现稳定。

Conclusion: 轻量级ASR系统在隐私保护场景下可行，但小模型可能引发额外开销和散热问题。

Abstract: Reliability on cloud providers for ASR inference to support child-centered
voice-based applications is becoming challenging due to regulatory and privacy
challenges. Motivated by a privacy-preserving design, this study aims to
develop a lightweight & efficient Whisper ASR system capable of running on a
Raspberry Pi. Upon evaluation of the MyST corpus and by examining various
filtering strategies to fine-tune the `tiny.en' model, a Word Error Rate (WER)
of 15.9% was achieved (11.8% filtered). A low-rank compression reduces the
encoder size by 0.51M with 1.26x faster inference in GPU, with 11% relative WER
increase. During inference on Pi, the compressed version required ~2 GFLOPS
fewer computations. The RTF for both the models ranged between [0.23-0.41] for
various input audio durations. Analyzing the RAM usage and CPU temperature
showed that the PI was capable of handling both the tiny models, however it was
noticed that small models initiated additional overhead/thermal throttling.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [150] [Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies](https://arxiv.org/abs/2507.14822)
*Zeeshan Kaleem,Misha Urooj Khan,Ahmad Suleman,Waqas Khalid,Kai-Kit Wong,Chau Yuen*

Main category: cs.CR

TL;DR: 论文提出了一种名为Quantum Skyshield的量子安全架构，结合BB84量子密钥分发和后量子认证机制，解决了低空无线网络的安全通信问题。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和高空平台的密集部署，低空无线网络的安全性和可靠性成为关键问题，尤其是自由空间光链路易受拦截和干扰。

Method: 提出Quantum Skyshield架构，整合BB84量子密钥分发、Lamport一次性签名和HMAC认证，并采用Grover启发的威胁检测机制。

Result: 模拟结果显示，在量子比特错误率低于11%时可可靠生成128位对称密钥，威胁检测概率高达89%。

Conclusion: 该架构有效提升了低空无线网络的安全性和可靠性，并指出了未来研究方向。

Abstract: Recently, low-altitude wireless networks (LAWNs) have emerged as a critical
backbone for supporting the low-altitude economy, particularly with the
densification of unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs). To meet growing data demands, some LAWN deployments incorporate
free-space optical (FSO) links, which offer exceptional bandwidth and beam
directivity. However, without strong security measures in place, both
conventional radio frequency channels and FSO beams remain vulnerable to
interception and spoofing and FSO in particular can suffer from turbulence,
misalignment, and weather-related attenuation. To address these challenges in
the quantum era, a quantum-secure architecture called Quantum Skyshield is
proposed to enable reliable communication between the base transceiver station
(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution
(QKD) with post-quantum authentication mechanisms. Simulation results confirm
the reliable generation of a 128-bit symmetric key when the quantum bit error
rate (QBER) remains below the threshold of 11%. Authentication is enforced
using Lamport one-time signatures and hash-based message authentication codes
(HMAC) to ensure message integrity. A Grover-inspired threat detection
mechanism identifies anomalies with up to 89% probability in a single
iteration, enabling real-time trust evaluation. Lastly, future research
challenges have also been identified and discussed to guide further development
in this area.

</details>


### [151] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 该论文系统回顾了2013至2024年间关于元宇宙安全和隐私问题的研究，分析了方法和趋势，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着元宇宙技术的快速发展，其安全和隐私问题日益突出，需要系统性的综述来指导未来研究。

Method: 通过系统性文献综述，整理研究方法、安全隐私属性、沉浸式组件和评估策略。

Result: 研究发现过去五年研究活动激增，注重实践和用户为中心，但政策合规、可访问性等领域仍存在不足。

Conclusion: 需结合技术和人文因素，采用跨学科方法建设可信赖的沉浸式环境。

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [152] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz框架通过结合大型语言模型和轻量级工具链，降低了模糊测试闭源库的成本，实现了全自动化分析和驱动生成。


<details>
  <summary>Details</summary>
Motivation: 解决闭源库模糊测试的高成本和复杂性，尤其是当仅有二进制库可用时。

Method: 利用大型语言模型（LLM）和轻量级工具链（反汇编器/编译器/模糊器），自动分析剥离的二进制文件，规划模糊策略，生成驱动并自我修复错误。

Result: 成功为558个可模糊测试的API函数生成语法正确的驱动，首次执行时75.52%的驱动名义上正确。

Conclusion: LLM增强的中间件为模糊测试黑盒组件提供了成本效益高的解决方案，并为未来研究方向奠定了基础。

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [153] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: 本文研究了集体树探索问题，提出了分布式异步算法，其探索效率优于之前的同步或集中式算法，并提供了新的下界证明。


<details>
  <summary>Details</summary>
Motivation: 研究在分布式异步环境中高效探索未知树的动机，填补了现有同步或集中式算法的不足。

Method: 提出了两个分布式异步算法，分别以不同的时间限制探索树。

Result: 算法分别在2n+O(k²2ᵏD)和O(k/ log k)(n+kD)的时间内完成探索，且第二个算法的竞争比为O(k/ log k)。

Conclusion: 所提算法在平均情况下具有最优性，并改进了异步集体树探索的下界至Ω(log²k)。

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [154] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 该论文介绍了一种基于大型语言模型（LLM）的视线和语音交互机器人系统，用于辅助协作任务，并通过实验比较了其与传统脚本化HRI系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有HRI系统在用户指令解析、动作生成和任务解决方面取得了进展，但双向、多模态和上下文感知的协作任务支持仍是一个挑战。

Method: 开发了一种模块化、可转移的接口系统，通过多视觉输入感知环境，并结合实时语言交互状态表示和快速感知模块。

Result: 实验表明，基于LLM的系统在适应性和用户参与度上略有提升，但可能产生冗余输出；而传统脚本化系统更适合简单任务。

Conclusion: LLM方法在复杂协作任务中表现更好，但需优化以减少冗余；传统方法在简单任务中更具优势。

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [155] [Remote Assistance or Remote Driving: The Impact of Operational Design Domains on ADS-Supporting Systems Selection](https://arxiv.org/abs/2507.14347)
*Ole Hans,Benedikt Walter*

Main category: eess.SY

TL;DR: 论文提出了一种基于ODD和用例分析的RDS或RAS选择框架。


<details>
  <summary>Details</summary>
Motivation: 当前ADS在L4级别系统中缺乏物理驾驶员，远程支持系统（RDS或RAS）的选择标准不清晰，通常忽略关键因素（如ODD和用例分析）。

Method: 应用PEGASUS框架分析ODD，并引入结构化框架来评估和选择远程支持系统。

Result: 提出了一种系统化方法，帮助开发者根据ODD和用例选择适合的远程支持系统。

Conclusion: 通过明确的标准和结构化框架，可以更科学地选择RDS或RAS，优化ADS的远程支持系统设计。

Abstract: High level Automated Driving Systems (ADS) can handle many situations, but
they still encounter situations where human intervention is required. In
systems where a physical driver is present in the vehicle, typically SAE Level
3 systems, this intervention is relatively straightforward and is handled by
the in-vehicle driver. However, the complexity increases for Level 4 systems,
where, in most cases, no physical driver remains in the vehicle. The two common
industry solutions for this challenge are the integration of a remote support
system, such as a Remote Driving System (RDS) or Remote Assistance System
(RAS). While it is clear that ADS will require one of these systems, it is less
clear how the suitability of either system for a particular ADS application
should be evaluated. Currently, the selection process often focuses on system
architecture as well as its design and integration challenges. Furthermore,
since many ADS developers choose to develop remote system solutions in-house,
it is advantageous to select the simpler approach to streamline development and
integration efforts. While these decision points are certainly relevant, this
approach overlooks the most critical factors: the use cases and the
complementarity of the ADS and the remote support system within the context of
the Operational Design Design Domain (ODD). This paper proposes a structured
approach for selecting between RDS and RAS as an ADS support system, based on
the defined ODD and use case analysis. To achieve this, the paper applies the
PEGASUS framework to systematically describe and analyze the ODD. A structured
framework is introduced to evaluate and select the most suitable remote support
system for an ADS based on clearly defined criteria.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [156] [Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok Refugees on RedNote](https://arxiv.org/abs/2507.14623)
*Mingchen Li,Wenbo Xu,Wenqing Gu,Yixuan Xie,Yao Zhou,Yunsong Dai,Cheng Tan,Pan Hui*

Main category: cs.SI

TL;DR: 研究分析了中国用户与‘TikTok难民’（因TikTok在美禁令而迁移至RedNote的外国用户）之间的跨文化互动，结合情感分类和主题建模，揭示了情感表达与立场、话题间的关联。


<details>
  <summary>Details</summary>
Motivation: 探究在全球社交媒体平台上，中国用户与外国用户如何互动，特别是在TikTok禁令背景下，双方的情感表达与立场如何影响交流。

Method: 基于1,862篇帖子和403,054条评论数据，采用大语言模型情感分类和BERT主题建模分析情感与话题的关系。

Result: 发现情感不对称性：中国用户在不同话题和立场下情感差异显著，外国用户普遍负面情绪更强；政治内容极化最严重。

Conclusion: 研究揭示了中外在线互动中的情感-立场结构，为跨国数字公共空间中的身份协商提供了实证依据。

Abstract: This study examines cross-cultural interactions between Chinese users and
self-identified "TikTok Refugees"(foreign users who migrated to RedNote after
TikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we
use large language model-based sentiment classification and BERT-based topic
modelling to explore how both groups engage with the TikTok refugee phenomenon.
We analyse what themes foreign users express, how Chinese users respond, how
stances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how
affective responses differ across topics and identities. Results show strong
affective asymmetry: Chinese users respond with varying emotional intensities
across topics and stances: pride and praise dominate cultural threads, while
political discussions elicit high levels of contempt and anger, especially from
Pro-China commenters. Pro-Foreign users exhibit the strongest negative emotions
across all topics, whereas neutral users express curiosity and joy but still
reinforce mainstream discursive norms. Cross-topic comparisons reveal that
appearance-related content produces the most emotionally balanced interactions,
while politics generates the highest polarization. Our findings reveal distinct
emotion-stance structures in Sino-foreign online interactions and offer
empirical insights into identity negotiation in transnational digital publics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [157] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 本文介绍了LinkedIn开发的内部聊天机器人，通过知识图谱、Text-to-SQL代理和交互式聊天界面，帮助团队自助获取数据洞察。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL任务上取得进展，但构建企业级解决方案仍具挑战性。

Method: 结合知识图谱、Text-to-SQL代理和交互式聊天机器人，支持用户从数据发现到查询调试的全流程。

Result: 聊天机器人每周有300多名用户，53%的响应在内部测试中被评为正确或接近正确。

Conclusion: 研究为企业级Text-to-SQL解决方案提供了实用的开发路径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [158] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究了分层安全聚合（HSA）中异构安全需求的弱安全模型（WS-HSA），提出了灵活的框架并分析了最优密钥率。


<details>
  <summary>Details</summary>
Motivation: 为了解决HSA中用户数量增加时的异构安全需求问题。

Method: 提出WS-HSA模型，定义安全输入集和共谋集，分析最优密钥率。

Result: 给出了最优密钥率的参数范围，并对其他情况提供了上下界。

Conclusion: WS-HSA为HSA中的异构安全需求提供了灵活且高效的解决方案。

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


### [159] [The Capacity of Semantic Private Information Retrieval with Colluding Servers](https://arxiv.org/abs/2507.15818)
*Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 论文研究了带有$T$个合谋服务器的语义私有信息检索问题（Sem-TPIR），提出了检索率的上界并设计了一个方案实现此上界，解决了消息大小不等且检索概率不均的情况。


<details>
  <summary>Details</summary>
Motivation: 经典PIR问题假设消息大小相等且检索概率均匀，而实际应用中这些条件可能不成立。Sem-PIR在$T=1$时的研究已有进展，但$T>1$时合谋服务器的行为尚未充分研究。

Method: 作者扩展了Sem-PIR问题到$T>1$的情况，推导了检索率的上界，并提出了一种能够达到此上界的方案。

Result: 证明了该方案实现了Sem-TPIR的精确容量，即检索率的上界是可达到的。

Conclusion: 该研究解决了合谋服务器场景下的非均匀消息检索问题，对理论和实际应用均有贡献。

Abstract: We study the problem of semantic private information retrieval (Sem-PIR) with
$T$ colluding servers (Sem-TPIR), i.e., servers that collectively share user
queries. In Sem-TPIR, the message sizes are different, and message retrieval
probabilities by any user are not uniform. This is a generalization of the
classical PIR problem where the message sizes are equal and message retrieval
probabilities are identical. The earlier work on Sem-PIR considered the case of
no collusions, i.e., the collusion parameter of $T=1$. In this paper, we
consider the general problem for arbitrary $T < N$. We find an upper bound on
the retrieval rate and design a scheme that achieves this rate, i.e., we derive
the exact capacity of Sem-TPIR.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [160] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一种适用于嵌入式设备的实时多标签视频分类框架，通过动态激活轻量级适配器来提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备上的实时多标签视频分类受限于计算和能源预算，但视频流具有标签稀疏性、时间连续性和标签共现性等特性，可以用于优化推理效率。

Method: 引入Polymorph框架，动态激活每帧所需的轻量级低秩适配器（LoRA），每个适配器专注于来自共现模式的子集类别，无需全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph实现了比基线方法低40%的能耗，并提升了9个百分点的mAP。

Conclusion: Polymorph通过模块化策略显著提升了嵌入式设备上的视频分类效率，同时开源以促进进一步研究。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [161] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 本文介绍了针对3D高斯溅射（3DGS）视频流传输的创新框架，通过高斯变形场、混合显著性分块和差异化质量建模，实现了高效压缩和带宽适应，提升了传输质量和性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频因其数据量大且压缩传输复杂，对流传输提出了挑战，亟需一种高效的解决方案。

Method: 采用高斯变形场构建3DGS视频，利用混合显著性分块和差异化质量建模进行数据压缩和带宽适应。

Result: 实验验证表明，该方法在视频质量、压缩效率和传输速率上优于现有方法。

Conclusion: 该框架为3DGS视频流传输提供了有效的解决方案，显著提升了传输性能。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [162] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了针对3D高斯点云视频流媒体的一系列解决方案，包括自适应分块、质量评估和比特率调整，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点云视频流媒体在沉浸式体验方面表现优异，但分块、质量评估和比特率调整等基础问题仍需研究。

Method: 提出基于显著性分析的自适应分块技术，结合空间和时间特征；开发了评估3DGS表示和2D渲染质量的框架；设计了基于元学习的自适应比特率算法。

Result: 实验表明，所提方法在性能上显著优于现有技术。

Conclusion: 本研究为3D高斯点云视频流媒体提供了全面的解决方案，解决了其核心挑战。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [163] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的机器人动作预测方法，利用InstructPix2Pix模型实现单图像和文本输入的100帧未来视觉预测，显著降低了计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹在机器人、自主系统和人类活动预测等领域至关重要，但传统视频预测模型计算密集且延迟高。

Method: 提出了一种基于InstructPix2Pix改进的深度学习方法，支持视觉和文本多模态输入，实现了高效的单图像和文本预测未来100帧的能力。

Result: 在RoboTWin数据集上，该方法在SSIM和PSNR上优于现有基准，同时显著降低了计算需求和推理时间。

Conclusion: 该方法为需要快速和轻量级预测的应用（如机器人任务和运动轨迹分析）提供了高效解决方案。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [164] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 论文提出了一种浏览器扩展，用于在视频通话中实时将手语翻译为字幕，旨在消除听力障碍者与普通人之间的沟通障碍。


<details>
  <summary>Details</summary>
Motivation: 听力障碍者与普通人沟通困难，尤其是疫情期间视频会议成为主流，听力障碍者更倾向于用手语而非打字交流。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展实现手语到字幕的自动翻译。

Result: 未提及具体结果，但目标是实现实时翻译功能。

Conclusion: 通过技术手段消除沟通障碍，提升听力障碍者在视频会议中的交流体验。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [165] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 论文提出了一种相机引导系统，通过美学评估和目标区分算法帮助用户识别和去除照片中的杂物，提升照片质量和拍摄效率。


<details>
  <summary>Details</summary>
Motivation: 摄影师和业余爱好者常因疏忽或经验不足在照片中留下杂物，影响照片的美观和情感表达，因此需要一种系统来辅助识别和去除杂物。

Method: 系统结合了美学评估算法和目标区分技术，通过交互式界面让用户识别杂物，并提供去除建议及基于生成对抗网络的图像修复工具。

Result: 用户研究表明，该系统能有效帮助用户快速识别杂物并提升照片质量，减少了拍摄时间。

Conclusion: 该系统为摄影爱好者提供了实用的工具，通过算法和交互式设计显著提升了照片的美观性和拍摄效率。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [166] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO是一个高效处理大规模视频数据中复杂对象查询的系统，通过预训练视觉编码器生成紧凑的视觉嵌入，并结合多索引结构和跨模态重排，显著提高了查询准确性和降低了搜索延迟。


<details>
  <summary>Details</summary>
Motivation: 随着摄像头部署的普及，视频数据激增，但现有方法在查询大规模视频数据集中的特定对象时面临挑战，包括数据处理量大、查询需求复杂以及高延迟问题。

Method: LOVO使用预训练的视觉编码器一次性提取特征，生成关键帧的紧凑视觉嵌入并构建高效索引，结合倒排多索引结构和向量数据库支持任意对象查询。查询时，通过近似最近邻搜索和跨模态重排优化结果。

Result: 在真实视频数据集上的评估显示，LOVO在复杂查询处理中优于现有方法，查询准确性接近最优，搜索延迟降低多达85倍，并显著减少索引构建成本。

Conclusion: LOVO为视频分析中的对象查询设定了新的技术标杆，提供了一种新颖、可扩展且高效的解决方案，特别适用于动态环境。

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [167] [Quantum Annealing for Machine Learning: Applications in Feature Selection, Instance Selection, and Clustering](https://arxiv.org/abs/2507.15063)
*Chloe Pomeroy,Aleksandar Pramov,Karishma Thakrar,Lakshmi Yendapalli*

Main category: quant-ph

TL;DR: 本文比较了量子退火（QA）和经典模拟退火（SA）在机器学习的组合优化问题中的应用，重点关注特征选择、实例选择和聚类任务，表明QA在效率和性能上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索量子退火和经典模拟退火在机器学习中的组合优化问题（如特征选择、实例选择和聚类）中的应用潜力，并比较两者的效果。

Method: 将每个任务建模为二次无约束二进制优化（QUBO）问题，分别实现量子退火和经典模拟退火求解器。在特征选择中设计了平衡特征重要性和冗余性的QUBO配置；在实例选择中提出了新的启发式方法；在聚类中结合了经典聚类和QUBO优化的流程。

Result: 量子退火在特征选择中表现出更高的计算效率，在实例选择和聚类中提出了新方法并实现了性能提升（如聚类紧凑性和检索指标）。

Conclusion: 研究表明，即使在当前量子硬件的限制下，量子退火仍是离散机器学习优化中具有竞争力和高效的工具。

Abstract: This paper explores the applications of quantum annealing (QA) and classical
simulated annealing (SA) to a suite of combinatorial optimization problems in
machine learning, namely feature selection, instance selection, and clustering.
We formulate each task as a Quadratic Unconstrained Binary Optimization (QUBO)
problem and implement both quantum and classical solvers to compare their
effectiveness. For feature selection, we propose several QUBO configurations
that balance feature importance and redundancy, showing that quantum annealing
(QA) produces solutions that are computationally more efficient. In instance
selection, we propose a few novel heuristics for instance-level importance
measures that extend existing methods. For clustering, we embed a
classical-to-quantum pipeline, using classical clustering followed by
QUBO-based medoid refinement, and demonstrate consistent improvements in
cluster compactness and retrieval metrics. Our results suggest that QA can be a
competitive and efficient tool for discrete machine learning optimization, even
within the constraints of current quantum hardware.

</details>


### [168] [Deterministic Quantum Search via Recursive Oracle Expansion](https://arxiv.org/abs/2507.15797)
*John Burke,Ciaran McGoldrick*

Main category: quant-ph

TL;DR: 提出了一种新型确定性量子搜索算法，替代传统概率性方法，通过递归扩展基础预言机实现确定性测量。


<details>
  <summary>Details</summary>
Motivation: 解决量子搜索中的不确定性，避免依赖任意相位旋转，适合硬件实现。

Method: 递归扩展预言机，逐步减少叠加态，使用最近邻扩散算子。

Result: 算法确定性成功，查询复杂度为O(N^0.7925)，减少了两比特门数量。

Conclusion: 算法适合硬件实现，支持部分数据库搜索，扩展了应用范围。

Abstract: We introduce a novel deterministic quantum search algorithm that provides a
practical alternative to conventional probabilistic search approaches. Our
scheme eliminates the inherent uncertainty of quantum search without relying on
arbitrary phase rotations, a key limitation of other deterministic methods. The
algorithm achieves certainty by recursively expanding the base oracle so that
it marks all states prefixed by the same two bits as the target, encompassing
exactly one-quarter of the search space. This enables a step-by-step reduction
of the superposition until the target state can be measured with certainty. The
algorithm achieves deterministic success with a query complexity of
$O(N^{\log_2(3)/2}) \approx O(N^{0.7925})$, falling between Grover's
$O(\sqrt{N})$ scaling and the classical $O(N)$. Our approach relies exclusively
on two-qubit nearest-neighbour diffusion operators, avoiding global diffusion
entirely. We show that, despite the increased query complexity, this design
reduces the total number of two-qubit gates required for diffusion by more than
an order of magnitude for search spaces up to at least 18 qubits, with even
greater advantages on hardware with limited qubit connectivity. The scheme's
inherent determinism, reliance on simple nearest-neighbour, low-depth
operations, and scalable recursive structure make it well-suited for hardware
implementation. Additionally, we show that the algorithm naturally supports
partial database search, enabling deterministic identification of selected
target bits without requiring a full search, further broadening its
applicability.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [169] [MeMo: Attentional Momentum for Real-time Audio-visual Speaker Extraction under Impaired Visual Conditions](https://arxiv.org/abs/2507.15294)
*Junjie Li,Wenxuan Wu,Shuai Wang,Zexu Pan,Kong Aik Lee,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 论文提出了一种名为MeMo的新型框架，通过两个自适应记忆库存储注意力相关信息，解决了视听目标说话者提取（AV-TSE）在视觉线索缺失时的性能问题。


<details>
  <summary>Details</summary>
Motivation: 人类在缺乏辅助信息时仍能保持注意力，受此启发，作者设计了一个能在视觉线索缺失时保持注意力动量的系统。

Method: MeMo框架包含两个自适应记忆库，用于存储和调用注意力相关信息，适用于实时场景。

Result: 实验表明，MeMo框架在SI-SNR指标上比基线至少提高2 dB。

Conclusion: MeMo框架有效提升了AV-TSE系统在视觉线索缺失时的性能，展示了其在实际应用中的潜力。

Abstract: Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate a target
speaker's voice from multi-speaker environments by leveraging visual cues as
guidance. However, the performance of AV-TSE systems heavily relies on the
quality of these visual cues. In extreme scenarios where visual cues are
missing or severely degraded, the system may fail to accurately extract the
target speaker. In contrast, humans can maintain attention on a target speaker
even in the absence of explicit auxiliary information. Motivated by such human
cognitive ability, we propose a novel framework called MeMo, which incorporates
two adaptive memory banks to store attention-related information. MeMo is
specifically designed for real-time scenarios: once initial attention is
established, the system maintains attentional momentum over time, even when
visual cues become unavailable. We conduct comprehensive experiments to verify
the effectiveness of MeMo. Experimental results demonstrate that our proposed
framework achieves SI-SNR improvements of at least 2 dB over the corresponding
baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [170] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出了一种新的任务Time-RA，将时间序列异常检测从判别式任务转为生成式任务，并利用大语言模型进行推理。同时，引入了一个多模态基准数据集RATs40K，包含40,000个样本，用于异常推理。实验表明，当前模型的局限性凸显了监督微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法通常仅进行二进制异常分类，缺乏详细分类和解释推理。Time-RA任务旨在解决这一问题，提升异常检测的解释性。

Method: 提出Time-RA任务，将异常检测转为生成式任务，利用大语言模型进行推理。引入RATs40K数据集，包含多模态数据（时间序列、文本、视觉）和精细标注（14种单变量异常和6种多变量异常）。采用GPT-4反馈优化的标注框架。

Result: 通过实验评估了大语言模型和多模态大语言模型在Time-RA任务上的表现，揭示了当前模型的局限性，并强调了监督微调的重要性。

Conclusion: Time-RA任务和RATs40K数据集为可解释时间序列异常检测和推理开辟了新的研究方向。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [171] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 论文提出了M-DESIGN，一种通过学习模型架构修改的先验知识来优化神经网络模型的流水线，解决了数据库研究中模型选择的静态性问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型选择方法忽略了任务查询与模型架构之间的动态依赖关系，导致匹配不优且无法有效优化模型。为解决这一问题，M-DESIGN被设计出来。

Method: M-DESIGN通过知识编织引擎将模型优化转化为对任务元数据的自适应查询问题，利用图关系知识模式进行细粒度分析并预测OOD任务。

Result: 在图形分析任务中，M-DESIGN在67,760个模型记录中，33个数据任务对中有26个在有限预算内找到了最优模型。

Conclusion: M-DESIGN通过动态关系分析有效提升了模型选择与优化的效果，尤其在处理异构任务时表现出色。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [172] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 该论文提出了信号骰子相似系数（SDSC），一种用于时间序列自监督表示学习的结构感知度量函数，以解决传统距离目标（如MSE）在信号SSL中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法在信号处理中常用MSE等距离目标，但这些方法对幅度敏感且忽略波形极性，影响了语义对齐和可解释性。SDSC旨在通过量化信号结构一致性来解决这些问题。

Method: SDSC基于骰子相似系数（DSC），通过计算符号幅度的交集体现信号结构一致性，并可作为损失函数。文中还提出了一种结合SDSC与MSE的混合损失，以平衡稳定性和幅度保持。

Result: 在预测和分类基准测试中，基于SDSC的预训练表现与MSE相当或更好，尤其是在域内和低资源场景下。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的有效替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [173] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 论文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性聚类，有效预测和利用LLMs中的激活稀疏性，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在显著的激活稀疏性，直接预测神经元级别的激活模式计算成本高。本文旨在提出一种高效预测和利用激活稀疏性的方法。

Method: 采用聚类框架，将相似激活模式分组为少量代表性聚类，而非独立处理每个神经元。通过预测聚类分配而非单个神经元状态，降低计算开销。详细描述了聚类算法及其有效性分析。

Result: 实验表明，该方法聚类精度高达79.34%，优于标准二值聚类方法，且困惑度（PPL）下降极小（最低为12.49），有效保持模型质量的同时减少计算开销。

Conclusion: 该方法为激活模式预测的未来研究奠定了基础，有望提升大规模语言模型的高效推理能力。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [174] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究多标签分类问题，利用表达性序列模型处理标签间的逻辑约束，实验证明该架构能利用约束提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何在多标签分类任务中有效利用标签间的逻辑约束，以提高分类性能。

Method: 采用个体标签分类器与表达性序列模型结合的方式，生成联合分布以捕捉标签间的相关性。

Result: 实验表明该架构能够有效利用训练中的约束，并在推理阶段强制执行约束。

Conclusion: 提出的架构在多标签分类中能有效利用标签约束，提升分类效果。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [175] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一种新型元学习框架，通过动态选择最佳聚合规则来应对联邦学习中的模型投毒攻击，优于静态防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受模型投毒攻击，现有静态防御方法效果有限且依赖上下文。本文旨在提出一种适应性强、可动态调整的防御方案。

Method: 设计了一个轻量级上下文强盗代理，实时选择最优聚合规则。通过实验验证了动态策略的优越性。

Result: 实验表明，动态代理在不同场景下学习到更优策略，并能通过单一参数控制风险与性能的权衡。

Conclusion: FedStrategist为构建弹性、智能的去中心化AI系统提供了一种实用且可分析的方法。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [176] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化的框架，用于安全高效地协作微调大语言模型（LLM），结合区块链信任层和经济激励，解决了传统联邦学习中单点攻击和投毒漏洞的问题，并在70B参数模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（FL）虽然保护数据隐私，但中央服务器的存在导致单点攻击和投毒漏洞。在异构、无信任环境中微调70B参数模型尚未解决。本文旨在提出一个去中心化框架，解决这些问题。

Method: 提出FLock框架，结合区块链信任层和经济激励，用安全可审计的协议取代中央聚合器，实现不信任方之间的协作。

Result: 实验验证了FLock在70B参数模型上的有效性，抵御了后门投毒攻击，降低了68%以上的攻击成功率，并促进了跨领域的知识迁移。

Conclusion: FLock框架在去中心化环境中安全高效地微调LLM，显著提升模型对抗攻击的能力和跨领域泛化性能。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [177] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: CSE-FSL 是一种新型的联邦分割学习方法，通过辅助网络减少客户端与服务器间的通信和存储需求。


<details>
  <summary>Details</summary>
Motivation: 解决联邦分割学习（FSL）中高通信开销和服务器存储需求大的问题。

Method: 引入辅助网络本地更新客户端权重，减少梯度传输，并在特定周期传输数据。

Result: 理论分析证明其收敛性，实验显示显著减少通信开销。

Conclusion: CSE-FSL 在通信和存储效率上优于现有 FSL 方法。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [178] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，结合非线性效用聚合和几何感知查询选择，解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中模式爆炸问题，通过交互学习提高效率和准确性。

Method: 使用Choquet积分建模用户偏好，结合几何感知查询选择和分支定界策略。

Result: 在UCI数据集上实验显示，优于现有方法（如ChoquetRank），排名准确性更高且用户交互更少。

Conclusion: 该框架有效解决了模式爆炸问题，提高了模式挖掘的效率和准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [179] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 论文提出SST-CL框架，结合空间-时间变换器和课程学习，解决了EEG情感识别中的非平稳时空模式整合和动态情感强度适应问题。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在实际应用中面临非平稳时空神经模式整合和动态情感强度变化的挑战，需要一种有效的方法来提升系统性能。

Method: 采用空间编码器建模通道间关系和时间编码器通过窗口注意力机制捕获多尺度依赖，结合强度感知课程学习策略动态调度训练样本。

Result: 在三个基准数据集上实现最先进性能，消融实验验证了框架各部分的必要性。

Conclusion: SST-CL框架通过整合时空变换器和课程学习，显著提升了EEG情感识别的性能和适应性。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [180] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GUI-G²的奖励框架，通过将GUI元素建模为连续高斯分布，改进了GUI定位任务中的稀疏奖励问题。该方法在实验中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法使用稀疏的二进制奖励，忽略了空间交互的连续性。受人类点击行为启发，作者提出一种连续的高斯分布模型来更好地模拟GUI定位。

Method: GUI-G²结合了高斯点奖励和覆盖奖励，前者通过指数衰减分布建模精确定位，后者评估预测分布与目标区域的重叠。此外，还开发了自适应方差机制以适应不同尺寸的元素。

Result: 实验表明，GUI-G²在多个基准测试（如ScreenSpot-Pro）上显著优于现有方法，最高提升达24.7%，并显示出对界面变化和未见布局的更强鲁棒性。

Conclusion: 通过连续优化和空间建模，GUI-G²为GUI交互任务中的空间推理设定了新范式，提供了更丰富的梯度信号和更强的泛化能力。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [181] [Inverse-Designed Dot Product Engine](https://arxiv.org/abs/2507.14333)
*Anannya Mathur*

Main category: physics.optics

TL;DR: 提出一种逆设计光学腔，通过干涉实现两光源输入的数字乘法运算，用于改进Lightening Transformer的点积计算，显著降低功耗和面积。


<details>
  <summary>Details</summary>
Motivation: 通过光学方法实现数字运算，减少传统电子计算中的功耗和面积问题。

Method: 利用逆设计光学腔，通过干涉实现(x+y)^2 - (x-y)^2 = 4xy的运算逻辑。

Result: 光腔输出与目标乘积xy直接成比例，降低光电核心面积88%，激光功耗23.43%，训练DeiT模型能耗0.88%。

Conclusion: 光学腔提供了一种高效的数字乘法实现方法，显著提升了计算效率。

Abstract: The work presents an inverse-designed optical cavity that can direct light
from two sources such that if the sources were to represent any number in the
range [-1,1] with magnitude encoded through the power emitted by the source and
sign by switching the direction of source current, the photocurrent generated
at the two output ports is proportional to the product of the two numbers. Let
us say that the two sources encode x and y, which are two numbers $\in$ [-1,1].
Multiplication is reduced to the form $(x+y)^2 - (x-y)^2 = 4xy \propto xy$. The
addition and subtraction operations of the numbers are supported by
constructive and destructive interference, respectively. The work shows that
replacing the DDOT dot product engine of the Lightening Transformer with the
optical cavity proposed to calculate the dot product can lead to a reduction in
the area occupied by the photonic core by 88 \%, can reduce the power
consumption by lasers by around 23.43 \%, and bring down energy consumption
while training DeiT models by 0.88 \%. The cavities can generate photocurrents
of the form $1.057 xy + 0.249$ with $R^2=0.88,$ thus showing a relationship of
direct proportionality between the target product $xy$ and the output of the
cavity in response to stimuli encoding $x$ and $y$.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [182] [Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse](https://arxiv.org/abs/2507.14218)
*Craig S Wright*

Main category: cs.CY

TL;DR: AI加剧认知分层，固化信息阶层，削弱民主审议能力，需通过教育和认知权利重建理性自主。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何通过选择性增强某些认知能力，导致认知分层和信息不对称，进一步削弱民主社会的审议能力。

Method: 结合形式认识论、政治理论、算法架构和经济激励结构，分析AI如何强化或削弱不同人群的认知能力。

Result: AI导致技术权力重组，信息不再为公共所有，而是成为制造共识和压制自主的工具。

Conclusion: 需通过教育、认知权利和开放认知基础设施重建公民的理性自主能力。

Abstract: Artificial intelligence functions not as an epistemic leveller, but as an
accelerant of cognitive stratification, entrenching and formalising
informational castes within liberal-democratic societies. Synthesising formal
epistemology, political theory, algorithmic architecture, and economic
incentive structures, the argument traces how contemporary AI systems
selectively amplify the reasoning capacity of individuals equipped with
recursive abstraction, symbolic logic, and adversarial interrogation, whilst
simultaneously pacifying the cognitively untrained through engagement-optimised
interfaces. Fluency replaces rigour, immediacy displaces reflection, and
procedural reasoning is eclipsed by reactive suggestion. The result is a
technocratic realignment of power: no longer grounded in material capital
alone, but in the capacity to navigate, deconstruct, and manipulate systems of
epistemic production. Information ceases to be a commons; it becomes the
substrate through which consent is manufactured and autonomy subdued.
Deliberative democracy collapses not through censorship, but through the
erosion of interpretive agency. The proposed response is not technocratic
regulation, nor universal access, but the reconstruction of rational autonomy
as a civic mandate, codified in education, protected by epistemic rights, and
structurally embedded within open cognitive infrastructure.

</details>


### [183] [Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement](https://arxiv.org/abs/2507.14242)
*Prerana Khatiwada,Grace Donaher,Jasymyn Navarro,Lokesh Bhatta*

Main category: cs.CY

TL;DR: 论文探讨了AI（尤其是ChatGPT和deepfake）带来的公平问题和错误信息传播风险，并提出跨领域合作与政策建议以减少危害。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术（如ChatGPT）的快速发展，其潜在危害（如公平问题、错误信息）亟待关注，需多方合作解决。

Method: 通过分析学术资料，研究AI在医疗、教育、科学等领域的负面影响及其成因。

Result: 提出了未来面向的指导方针和政策建议，以平衡创新与风险管控。

Conclusion: 呼吁用户、开发者和政府共同努力，确保AI技术的健康发展。

Abstract: While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.

</details>


### [184] [Fiduciary AI for the Future of Brain-Technology Interactions](https://arxiv.org/abs/2507.14339)
*Abhishek Bhattacharjee,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: 该论文探讨了将信托责任嵌入脑机接口（BCI）集成的脑基础模型中的必要性，以平衡其潜在利益与风险。


<details>
  <summary>Details</summary>
Motivation: 脑基础模型和BCIs技术虽具革命性潜力，但也带来了前所未有的风险，如潜意识神经信号的滥用和认知自由的侵蚀，需要技术设计确保用户权益。

Method: 结合法律传统和AI对齐技术，论文提出了可实施的架构和治理机制，以确保模型始终以用户最佳利益为导向。

Result: 通过技术设计嵌入信托责任（忠诚、谨慎和保密），可以有效减少权力不对称和操纵风险。

Conclusion: 将脑基础模型置于信托基础上，是实现其潜力而不损害用户自主权的关键步骤。

Abstract: Brain foundation models represent a new frontier in AI: instead of processing
text or images, these models interpret real-time neural signals from EEG, fMRI,
and other neurotechnologies. When integrated with brain-computer interfaces
(BCIs), they may enable transformative applications-from thought controlled
devices to neuroprosthetics-by interpreting and acting on brain activity in
milliseconds. However, these same systems pose unprecedented risks, including
the exploitation of subconscious neural signals and the erosion of cognitive
liberty. Users cannot easily observe or control how their brain signals are
interpreted, creating power asymmetries that are vulnerable to manipulation.
This paper proposes embedding fiduciary duties-loyalty, care, and
confidentiality-directly into BCI-integrated brain foundation models through
technical design. Drawing on legal traditions and recent advancements in AI
alignment techniques, we outline implementable architectural and governance
mechanisms to ensure these systems act in users' best interests. Placing brain
foundation models on a fiduciary footing is essential to realizing their
potential without compromising self-determination.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [185] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转化为动画，帮助学习者理解复杂科学概念。


<details>
  <summary>Details</summary>
Motivation: 复杂科学概念的理解困难，动态可视化虽有效但制作耗时且需要专业知识。

Method: 通过LLM解析论文或文本生成结构化场景描述，再转换为可执行的Manim代码。

Result: 快速创建高质量教育动画，降低制作门槛。

Conclusion: Manimator有望成为STEM教育的有力工具，普及高质量内容创作。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [186] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出了两种新方法（GRPO和OSPO）以解决传统MARL在动态拼车平台中的问题，通过避免值函数估计提升性能。


<details>
  <summary>Details</summary>
Motivation: 动态拼车平台面临高维度和不确定性的挑战，而传统MARL方法因依赖准确值函数估计导致训练不稳定和估计偏差。

Method: 1. 将GRPO应用于拼车，用群体平均奖励替代PPO基线；2. 提出OSPO，仅利用一步奖励优化策略。

Result: 在真实曼哈顿拼车数据上的实验表明，GRPO和OSPO在多数场景中表现更优，优化了接送时间和订单完成量。

Conclusion: GRPO和OSPO通过绕过值函数估计，有效解决了传统MARL在大规模不确定环境中的问题，提升了拼车平台的效率。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [187] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 该论文探讨了代理型AI在复杂系统中自主检测和响应异常的潜力，重点关注其改变传统依赖人类的异常管理模式的能力。


<details>
  <summary>Details</summary>
Motivation: 旨在利用代理型AI技术解决复杂系统中的异常管理问题，减少人为干预，提高效率。

Method: 研究代理型AI在异常检测和响应中的自主能力，可能涉及算法设计和实践应用。

Result: 展示了代理型AI在提高异常管理效率和减少人为依赖方面的潜力。

Conclusion: 代理型AI有望革新传统的异常管理方式，为复杂系统提供更高效的解决方案。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [188] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文探讨了命题溯因中的精细化推理方法，通过引入“方面”概念和分析解释间的距离，提出了对解释多样性的更深入理解。


<details>
  <summary>Details</summary>
Motivation: 旨在解决命题溯因中高复杂度推理问题（如计数和枚举），同时保持较低的复杂度。

Method: 引入“方面”（在部分解释中出现但非全部解释中出现的字面量）和解释间距离分析。

Result: 在Post框架中几乎完全表征了命题溯因的各个方面。

Conclusion: 通过精细化推理方法，更好地理解了命题溯因的解释多样性，并保持了较低的复杂度。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [189] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 提出了一种结合DL-Lite本体知识的新规划方法，利用显式输入知识（eKABs）和一致性更新语义，将复杂性控制在先前方法水平，并通过多项式编译实现。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，提升规划能力。

Method: 结合显式输入知识和一致性更新语义，设计新的规划方法，并通过多项式编译实现。

Result: 新方法的复杂度与现有方法相当，编译实现验证了可行性。

Conclusion: 该方法有效整合本体知识，为规划问题提供了新解决方案。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [190] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 提出一种基于比较学习的框架，用于校准项目特定的故事点估计模型，减轻开发者的认知负担。


<details>
  <summary>Details</summary>
Motivation: 传统故事点估计方法繁琐且耗时，机器学习可降低工作量，但现有模型需依赖项目特定数据。

Method: 通过开发者对任务对的比较判断训练机器学习模型，而非直接分配具体故事点。

Result: 在16个项目、23,313次手动估计数据上，模型预测与真实故事的Spearman相关系数为0.34，性能与回归模型相当。

Conclusion: 比较学习方法更高效，认知负担更低，性能与现有回归方法相当或更好。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [191] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 《Endless Tuning》是一种基于双重镜像过程的AI可靠部署方法，旨在避免人类被替代并填补责任缺口。通过三个原型应用测试，研究发现用户感知到的控制力提升，并可在责任与法律责任之间建立桥梁。


<details>
  <summary>Details</summary>
Motivation: 为了避免AI技术造成人类被替代的风险，并解决AI决策中的责任缺口问题（Matthias 2004）。

Method: 采用双重镜像过程设计方法，开发协议并在贷款审批、肺炎诊断和艺术风格识别三个应用中进行测试，结合专家反馈和用户经验分析。

Result: 实验显示，尽管使用深度学习模型，用户在决策环境中仍感知到完全控制，同时责任与法律责任之间可以建立联系。

Conclusion: 《Endless Tuning》方法在提升用户控制感和解决责任问题上是有效的，为AI伦理提供了新的视角。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [192] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一种基于异步监督的AI医疗诊断系统g-AMIE，通过多智能体框架在安全范围内进行病史采集，并由主诊医生监督决策，实验显示其效果优于传统模式。


<details>
  <summary>Details</summary>
Motivation: 目前AI在医疗诊断对话中展现了潜力，但患者安全需要专业监督。受医生团队协作启发，作者希望开发一种异步监督框架。

Method: 提出g-AMIE系统，在限定范围内采集病史并避免直接诊断，通过临床界面将评估传递给监督医生，实现异步监督。

Result: 在虚拟试验中，g-AMIE在病史采集、病例总结和诊断建议方面优于护士和医生助手组，且监督效率更高。

Conclusion: 异步监督框架为AI诊断系统在真实医疗环境中运作提供了可行模式，但需进一步验证临床实用性。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [193] [Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model](https://arxiv.org/abs/2507.14173)
*Karim Alghoul,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: eess.SP

TL;DR: 该论文提出了一种结合CNN、LSTM和TCN的混合架构，用于提高基于PPG信号的情感识别模型的泛化能力，实验证明其在PPG信号情感识别任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPG信号的情感识别模型在个体间的泛化能力不足，因此需要一种更鲁棒的模型来解决这一问题。

Method: 提出了一种混合架构，结合了CNN、LSTM和TCN的优势，通过多阶段处理提取特征并分类情绪。

Result: 实验表明，该混合模型在PPGE数据集上优于单独使用CNN或LSTM的模型，并且在AUC和F1 Score指标上表现优异。

Conclusion: 混合架构显著提升了模型的泛化能力，为基于PPG信号的情感识别提供了更有效的解决方案。

Abstract: Human computer interaction has become integral to modern life, driven by
advancements in machine learning technologies. Affective computing, in
particular, has focused on systems that recognize, interpret, and respond to
human emotions, often using wearable devices, which provide continuous data
streams of physiological signals. Among various physiological signals, the
photoplethysmogram (PPG) has gained prominence due to its ease of acquisition
from widely available devices. However, the generalization of PPG-based emotion
recognition models across individuals remains an unresolved challenge. This
paper introduces a novel hybrid architecture that combines Convolutional Neural
Networks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal
Convolutional Networks (TCNs) to address this issue. The proposed model
integrates the strengths of these architectures to improve robustness and
generalization. Raw PPG signals are fed into the CNN for feature extraction.
These features are processed separately by LSTM and TCN. The outputs from these
components are concatenated to generate a final feature representation, which
serves as the input for classifying valence and arousal, the primary dimensions
of emotion. Experiments using the Photoplethysmogram Dataset for Emotional
Analysis (PPGE) demonstrate that the proposed hybrid model achieves better
model generalization than standalone CNN and LSTM architectures. Our results
show that the proposed solution outperforms the state-of-the-art CNN
architecture, as well as a CNN-LSTM model, in emotion recognition tasks with
PPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we
highlight the model's effectiveness in handling subject variability.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [194] [Scalable Climate Data Analysis: Balancing Petascale Fidelity and Computational Cost](https://arxiv.org/abs/2507.08006)
*Aashish Panta,Amy Gooch,Giorgio Scorzelli,Michela Taufer,Valerio Pascucci*

Main category: physics.ao-ph

TL;DR: 提出一种可扩展的生态系统，通过分层多分辨率数据管理、智能传输和机器学习辅助重建，在保持精度的同时显著降低存储和计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高分辨率、大容量的气候数据时，往往因压缩或降采样而牺牲数据保真度，限制了科学研究的深度。

Method: 结合分层多分辨率数据管理、智能传输和机器学习辅助重建的技术，实现对气候数据的高效处理。

Result: 存储和计算成本降低99%（从10万美元降至24美元），同时保持RMS误差为1.46℃。

Conclusion: 该方法在NASA海量气候数据集上验证有效，既能降低成本，又能保持数据的高保真度，适用于研究和决策。

Abstract: The growing resolution and volume of climate data from remote sensing and
simulations pose significant storage, processing, and computational challenges.
Traditional compression or subsampling methods often compromise data fidelity,
limiting scientific insights. We introduce a scalable ecosystem that integrates
hierarchical multiresolution data management, intelligent transmission, and
ML-assisted reconstruction to balance accuracy and efficiency. Our approach
reduces storage and computational costs by 99\%, lowering expenses from
\$100,000 to \$24 while maintaining a Root Mean Square (RMS) error of 1.46
degrees Celsius. Our experimental results confirm that even with significant
data reduction, essential features required for accurate climate analysis are
preserved. Validated on petascale NASA climate datasets, this solution enables
cost-effective, high-fidelity climate analysis for research and
decision-making.

</details>
