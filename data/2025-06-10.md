<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 12]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 11]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.CR](#cs.CR) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.CY](#cs.CY) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Enhancing Software Supply Chain Security Through STRIDE-Based Threat Modelling of CI/CD Pipelines](https://arxiv.org/abs/2506.06478)
*Sowmiya Dhandapani*

Main category: cs.SE

TL;DR: 研究通过威胁建模方法识别和缓解CI/CD生命周期中的风险，评估安全控制并提出工具链集成策略。


<details>
  <summary>Details</summary>
Motivation: 随着CI/CD管道的广泛应用，保障软件供应链安全成为DevOps团队的关键挑战。

Method: 使用STRIDE框架建模典型CI/CD管道，结合NIST、OWASP、SLSA标准分析漏洞并制定安全控制措施。

Result: 提出了一套基于Security as Code和Shift Left-Shield Right原则的自动化安全工具链集成策略。

Conclusion: 该研究为提升CI/CD管道安全性提供了实用路线图，以应对不断变化的软件供应链威胁。

Abstract: With the increasing adoption of Continuous Integration and Continuous
Deployment pipelines, securing software supply chains has become a critical
challenge for modern DevOps teams. This study addresses these challenges by
applying a structured threat modeling approach to identify and mitigate risks
throughout the CI/CD lifecycle. By modeling a representative pipeline
architecture incorporating tools such as GitHub, Jenkins, Docker, and
Kubernetes and applying the STRIDE framework, we systematically analyze
vulnerabilities at each stage, from source code management to deployment.
Threats are documented and mapped to comprehensive security controls drawn from
standards like NIST SP 800-218, OWASP Top 10 CI/CD risks, and the SLSA
framework. Controls are further evaluated against SLSA maturity levels to
assess improvements in trust and provenance. To operationalize these findings,
the study outlines a practical security toolchain integration strategy grounded
in Security as Code and Shift Left-Shield Right principles, enabling automated,
enforceable security across the pipeline. This approach provides a pragmatic
roadmap for enhancing CI/CD pipeline security against evolving software supply
chain threats.

</details>


### [2] [Information-Theoretic Detection of Unusual Source Code Changes](https://arxiv.org/abs/2506.06508)
*Adriano Torres,Sebastian Baltes,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 该论文通过信息论的视角，测量开源项目中源代码的信息内容，定义了文本和结构熵，并分析了其与经典代码复杂度指标的关系，发现熵能捕捉不同的复杂度维度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于从信息论角度测量源代码的演化，以熵作为衡量指标，探索其与代码复杂度的关系，并验证熵在异常检测中的有效性。

Method: 论文定义了基于token和抽象语法树节点的文本和结构熵，通过分析95个开源项目的演化模式，评估熵与传统复杂度指标的关系，并进行熵基异常检测。

Result: 研究发现熵能捕捉与经典复杂度指标不同的维度，熵基异常检测的精度超过60%。

Conclusion: 论文为信息论方法在代码演化测量中的应用奠定了基础，开创了一种静态衡量程序复杂度的新方法。

Abstract: The code base of software projects evolves essentially through inserting and
removing information to and from the source code. We can measure this evolution
via the elements of information - tokens, words, nodes - of the respective
representation of the code. In this work, we approach the measurement of the
information content of the source code of open-source projects from an
information-theoretic standpoint. Our focus is on the entropy of two
fundamental representations of code: tokens and abstract syntax tree nodes,
from which we derive definitions of textual and structural entropy. We proceed
with an empirical assessment where we evaluate the evolution patterns of the
entropy of 95 actively maintained open source projects. We calculate the
statistical relationships between our derived entropy metrics and classic
methods of measuring code complexity and learn that entropy may capture
different dimensions of complexity than classic metrics. Finally, we conduct
entropy-based anomaly detection of unusual changes to demonstrate that our
approach may effectively recognise unusual source code change events with over
60% precision, and lay the groundwork for improvements to information-theoretic
measurement of source code evolution, thus paving the way for a new approach to
statically gauging program complexity throughout its development.

</details>


### [3] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: 研究了私有GPT基于需求自动生成可执行测试代码的能力。发现两步法（先转换为Gherkin语法）效果更好，尤其在代码可读性和最佳实践方面。


<details>
  <summary>Details</summary>
Motivation: 探索私有GPT如何通过需求直接生成测试代码，为产品所有者提供一种高效的测试标准生成方法。

Method: 使用接受标准作为输入，对比两种方法：直接生成代码与通过Gherkin语法间接生成。

Result: 两步法在代码可读性和最佳实践方面表现更优，结构化提示能生成更高质量的测试输出。

Conclusion: 通过中间步骤（如Gherkin语法）可以显著提升GPT生成测试代码的质量。

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [4] [Mind the Gap: A Readability-Aware Metric for Test Code Complexity](https://arxiv.org/abs/2506.06764)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 介绍了CCTR，一种针对单元测试的认知复杂性度量，结合了结构特征和语义特征，能更准确地评估测试代码的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有复杂性度量如Cyclomatic Complexity和Cognitive Complexity不适用于测试代码，特别是LLM生成的测试代码。

Method: 提出CCTR度量，整合断言密度、注释角色和测试组合模式等特征，评估EvoSuite、GPT-4o和Mistral Large-1024生成的测试套件。

Result: CCTR能有效区分结构化和碎片化的测试套件，生成更符合开发者感知的复杂性评分。

Conclusion: CCTR为测试代码的可靠评估和改进提供了基础。

Abstract: Automatically generated unit tests-from search-based tools like EvoSuite or
LLMs-vary significantly in structure and readability. Yet most evaluations rely
on metrics like Cyclomatic Complexity and Cognitive Complexity, designed for
functional code rather than test code. Recent studies have shown that
SonarSource's Cognitive Complexity metric assigns near-zero scores to
LLM-generated tests, yet its behavior on EvoSuite-generated tests and its
applicability to test-specific code structures remain unexplored. We introduce
CCTR, a Test-Aware Cognitive Complexity metric tailored for unit tests. CCTR
integrates structural and semantic features like assertion density, annotation
roles, and test composition patterns-dimensions ignored by traditional
complexity models but critical for understanding test code. We evaluate 15,750
test suites generated by EvoSuite, GPT-4o, and Mistral Large-1024 across 350
classes from Defects4J and SF110. Results show CCTR effectively discriminates
between structured and fragmented test suites, producing interpretable scores
that better reflect developer-perceived effort. By bridging structural analysis
and test readability, CCTR provides a foundation for more reliable evaluation
and improvement of generated tests. We publicly release all data, prompts, and
evaluation scripts to support replication.

</details>


### [5] [Beyond Surface Similarity: Evaluating LLM-Based Test Refactorings with Structural and Semantic Awareness](https://arxiv.org/abs/2506.06767)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CTSES是一种新的复合指标，用于评估LLMs自动重构单元测试的效果，结合了CodeBLEU、METEOR和ROUGE-L，比现有指标更能反映开发者期望和人类直觉。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs自动重构单元测试的效果是一个挑战，传统指标对重命名和结构编辑过于敏感，而基于嵌入的相似性指标则忽略了可读性和模块化。

Method: 提出CTSES复合指标，结合CodeBLEU、METEOR和ROUGE-L，以平衡行为保留、词汇质量和结构对齐，并在两个Java基准数据集上测试了GPT-4o和Mistral-Large-2407的重构效果。

Result: CTSES在5,000多个测试套件上的评估显示，它比其他指标更能提供忠实和可解释的评估，更符合开发者期望和人类直觉。

Conclusion: CTSES是一种更有效的评估指标，适用于LLMs自动重构单元测试的效果评估。

Abstract: Large Language Models (LLMs) are increasingly employed to automatically
refactor unit tests, aiming to enhance readability, naming, and structural
clarity while preserving functional behavior. However, evaluating such
refactorings remains challenging: traditional metrics like CodeBLEU are overly
sensitive to renaming and structural edits, whereas embedding-based
similarities capture semantics but ignore readability and modularity. We
introduce CTSES, a composite metric that integrates CodeBLEU, METEOR, and
ROUGE-L to balance behavior preservation, lexical quality, and structural
alignment. CTSES is evaluated on over 5,000 test suites automatically
refactored by GPT-4o and Mistral-Large-2407, using Chain-of-Thought prompting,
across two established Java benchmarks: Defects4J and SF110. Our results show
that CTSES yields more faithful and interpretable assessments, better aligned
with developer expectations and human intuition than existing metrics.

</details>


### [6] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 本文探讨了将机器学习训练管道部署到生产环境的挑战，通过SPIRA项目的案例展示了从初始混乱架构到模块化单体架构，再到微服务架构的演变过程。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习训练管道在生产环境中缺乏关键软件质量属性的问题，以提升系统的可维护性、健壮性和可扩展性。

Method: 通过比较SPIRA项目中三个版本的连续训练子系统架构（从混乱架构到模块化单体架构，再到微服务架构），采用不同的设计原则和模式。

Result: 优化后的架构显著提升了系统的可维护性、健壮性和可扩展性，为机器学习工程师和数据科学家提供了生产化实践的参考。

Conclusion: 通过架构演化，证明了设计原则和模式在提升ML训练管道生产环境性能中的重要性，为相关从业者提供了有价值的经验。

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [7] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 研究了量子编程中软件库频繁更新带来的代码重构问题，提出了量子电路重构问题的分类法，并利用大语言模型（LLM）辅助分类Qiskit版本迁移中的重构需求。


<details>
  <summary>Details</summary>
Motivation: 量子计算软件的快速发展导致软件库频繁更新，代码需要重构，而这些问题与经典软件工程中的重构问题有本质不同，亟需系统化研究方法。

Method: 通过分析Qiskit文档和发布说明建立初始重构分类法，并分别由专家和LLM生成两种分类法，最终整合为统一的分类法。

Result: 生成了两种分类法并整合成统一的分类法，为AI辅助迁移和自动化重构技术的研究奠定基础。

Conclusion: 该研究为量子软件工程提供了系统性工具，优化了开发流程，提升了语言兼容性，并促进了量子编程的最佳实践。

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [8] [GUIPilot: A Consistency-based Mobile GUI Testing Approach for Detecting Application-specific Bugs](https://arxiv.org/abs/2506.07385)
*Ruofan Liu,Xiwen Teoh,Yun Lin,Guanjie Chen,Ruofei Ren,Denys Poshyvanyk,Jin Song Dong*

Main category: cs.SE

TL;DR: GUIPilot是一种检测移动设计与其实现之间不一致性的方法，通过屏幕和行为分析，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 移动设计通常包含设计模拟图，但实际实现中可能出现不一致，需要工具进行自动化检测。

Method: GUIPilot将屏幕抽象为控件容器，通过优化控件对齐问题检测屏幕不一致性；通过视觉提示模型推断行为动作验证过渡一致性。

Result: 实验显示，GUIPilot在屏幕不一致性检测中的精确率和召回率分别为94.5%和99.6%，过程不一致性检测零误差。

Conclusion: GUIPilot高效且准确，工业案例中检测到多个应用缺陷，证实其实际价值。

Abstract: In this work, we propose GUIPilot, an approach for detecting inconsistencies
between the mobile design and their implementations. The mobile design usually
consists of design mock-ups that specify (1) the expected screen appearances
(e.g., widget layouts, colors, and shapes) and (2) the expected screen
behaviors, regarding how one screen can transition into another (e.g., labeled
widgets with textual description). Given a design mock-up and the
implementation of its application, GUIPilot reports both their screen
inconsistencies as well as process inconsistencies. On the one hand, GUIPilot
detects the screen inconsistencies by abstracting every screen into a widget
container where each widget is represented by its position, width, height, and
type. By defining the partial order of widgets and the costs of replacing,
inserting, and deleting widgets in a screen, we convert the screen-matching
problem into an optimizable widget alignment problem. On the other hand, we
translate the specified GUI transition into stepwise actions on the mobile
screen (e.g., click, long-press, input text on some widgets). To this end, we
propose a visual prompt for the vision-language model to infer widget-specific
actions on the screen. By this means, we can validate the presence or absence
of expected transitions in the implementation. Our extensive experiments on 80
mobile applications and 160 design mock-ups show that (1) GUIPilot can achieve
94.5% precision and 99.6% recall in detecting screen inconsistencies,
outperforming the state-of-the-art approach, such as GVT, by 66.2% and 56.6%
respectively, and (2) GUIPilot reports zero errors in detecting process
inconsistencies. Furthermore, our industrial case study on applying GUIPilot on
a trading mobile application shows that GUIPilot has detected nine application
bugs, and all the bugs were confirmed by the original application experts.

</details>


### [9] [Generate Realistic Test Scenes for V2X Communication Systems](https://arxiv.org/abs/2506.07419)
*An Guo,Xinyu Gao,Chunrong Fang,Haoxiang Tian,Weisong Sun,Yanzhou Mu,Shuncheng Tang,Lei Ma,Zhenyu Chen*

Main category: cs.SE

TL;DR: V2XGen是一个自动化测试生成工具，用于提升V2X协同感知系统的测试效率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服单智能体感知系统在远距离和遮挡场景中的局限性，V2X协同感知系统需要高效测试工具来验证和优化性能。

Method: 使用高保真方法生成真实协同对象实例，并通过适应性引导的V2X场景生成策略提升测试效率。

Result: 实验表明，V2XGen能生成真实测试场景，有效检测错误行为，并提升系统检测精度。

Conclusion: V2XGen为V2X协同感知系统测试提供了高效解决方案，显著提升了系统性能。

Abstract: Accurately perceiving complex driving environments is essential for ensuring
the safe operation of autonomous vehicles. With the tremendous progress in deep
learning and communication technologies, cooperative perception with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
the limitations of single-agent perception systems in perceiving distant
objects and occlusions. Despite the considerable advancements, V2X cooperative
perception systems require thorough testing and continuous enhancement of
system performance. Given that V2X driving scenes entail intricate
communications with multiple vehicles across various geographic locations,
creating V2X test scenes for these systems poses a significant challenge.
Moreover, current testing methodologies rely on manual data collection and
labeling, which are both time-consuming and costly.
  In this paper, we design and implement V2XGen, an automated testing
generation tool for V2X cooperative perception systems. V2XGen utilizes a
high-fidelity approach to generate realistic cooperative object instances and
strategically place them within the background data in crucial positions.
Furthermore, V2XGen adopts a fitness-guided V2X scene generation strategy for
the transformed scene generation process and improves testing efficiency. We
conduct experiments on V2XGen using multiple cooperative perception systems
with different fusion schemes to assess its performance on various tasks. The
experimental results demonstrate that V2XGen is capable of generating realistic
test scenes and effectively detecting erroneous behaviors in different
V2X-oriented driving conditions. Furthermore, the results validate that
retraining systems under test with the generated scenes can enhance average
detection precision while reducing occlusion and long-range perception errors.

</details>


### [10] [A Framework for Creating Non-Regressive Test Cases via Branch Consistency Analysis Driven by Descriptions](https://arxiv.org/abs/2506.07486)
*Yuxiang Zhang,Pengyu Xue,Zhen Yang,Xiaoxue Ren,Xiang Li,Linhao Wu,Jiancheng Zhao,Xingda Yu*

Main category: cs.SE

TL;DR: DISTINCT是一个基于自然语言描述的测试生成框架，通过分支一致性分析提升LLM在缺陷检测中的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成工具假设被测试方法正确，而实际场景中方法可能有缺陷。研究旨在解决这一问题，提高缺陷检测率。

Method: DISTINCT包含生成器、验证器和分析器三个迭代组件，通过自然语言描述和分支分析生成并优化测试用例。

Result: DISTINCT显著提升编译成功率、通过率和缺陷检测率，并在代码覆盖率和分支覆盖率上优于现有方法。

Conclusion: DISTINCT为非回归测试生成设定了新基准，展示了自然语言描述驱动的LLM在缺陷检测中的潜力。

Abstract: Automated test-generation research overwhelmingly assumes the correctness of
focal methods, yet practitioners routinely face non-regression scenarios where
the focal method may be defective. A baseline evaluation of EvoSuite and two
leading Large Language Model (LLM)-based generators, namely ChatTester and
ChatUniTest, on defective focal methods reveals that despite achieving up to
83% of branch coverage, none of the generated tests expose defects.
  To resolve this problem, we first construct two new benchmarks, namely
Defects4J-Desc and QuixBugs-Desc, for experiments. In particular, each focal
method is equipped with an extra Natural Language Description (NLD) for code
functionality understanding.
  Subsequently, we propose DISTINCT, a Description-guided, branch-consistency
analysis framework that transforms LLMs into fault-aware test generators.
DISTINCT carries three iterative components: (1) a Generator that derives
initial tests based on the NLDs and the focal method, (2) a Validator that
iteratively fixes uncompilable tests using compiler diagnostics, and (3) an
Analyzer that iteratively aligns test behavior with NLD semantics via
branch-level analysis.
  Extensive experiments confirm the effectiveness of our approach. Compared to
state-of-the-art methods, DISTINCT achieves an average improvement of 14.64% in
Compilation Success Rate (CSR) and 6.66% in Passing Rate (PR) across both
benchmarks. It notably enhances Defect Detection Rate (DDR) on both benchmarks,
with a particularly significant gain of 149.26% observed on Defects4J-Desc. In
terms of code coverage, DISTINCT improves Statement Coverage (SC) by an average
of 3.77% and Branch Coverage (BC) by 5.36%. These results set a new baseline
for non-regressive test generation and highlight how description-driven
reasoning enables LLMs to move beyond coverage chasing toward effective defect
detection.

</details>


### [11] [Large Language Models for Multilingual Vulnerability Detection: How Far Are We?](https://arxiv.org/abs/2506.07503)
*Honglin Shu,Michael Fu,Junji Yu,Dong Wang,Chakkrit Tantithamthavorn,Junjie Chen,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文通过实证研究评估了预训练语言模型（PLMs）和大语言模型（LLMs）在多语言漏洞检测中的效果，发现GPT-4o在多语言和细粒度场景中表现最优。


<details>
  <summary>Details</summary>
Motivation: 尽管PLMs和LLMs在漏洞检测中有所应用，但其在多语言和多粒度场景中的表现尚未充分研究。

Method: 利用30,000多个真实漏洞修复补丁，对七种编程语言的函数级和行级漏洞检测进行系统评估。

Result: GPT-4o在指令调优和少样本提示下显著优于其他模型，且在检测高危漏洞方面表现突出。

Conclusion: LLMs在多语言漏洞检测中展现出巨大潜力和优势，为实际软件安全挑战提供了新解决方案。

Abstract: Various deep learning-based approaches utilizing pre-trained language models
(PLMs) have been proposed for automated vulnerability detection. With recent
advancements in large language models (LLMs), several studies have begun
exploring their application to vulnerability detection tasks. However, existing
studies primarily focus on specific programming languages (e.g., C/C++) and
function-level detection, leaving the strengths and weaknesses of PLMs and LLMs
in multilingual and multi-granularity scenarios largely unexplored. To bridge
this gap, we conduct a comprehensive fine-grained empirical study evaluating
the effectiveness of state-of-the-art PLMs and LLMs for multilingual
vulnerability detection. Using over 30,000 real-world vulnerability-fixing
patches across seven programming languages, we systematically assess model
performance at both the function-level and line-level. Our key findings
indicate that GPT-4o, enhanced through instruction tuning and few-shot
prompting, significantly outperforms all other evaluated models, including
CodeT5P. Furthermore, the LLM-based approach demonstrates superior capability
in detecting unique multilingual vulnerabilities, particularly excelling in
identifying the most dangerous and high-severity vulnerabilities. These results
underscore the promising potential of adopting LLMs for multilingual
vulnerability detection at function-level and line-level, revealing their
complementary strengths and substantial improvements over PLM approaches. This
first empirical evaluation of PLMs and LLMs for multilingual vulnerability
detection highlights LLMs' value in addressing real-world software security
challenges.

</details>


### [12] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: IntenTest是一个针对LLM代理的API压力测试框架，通过系统性地生成和变异任务，揭示意图完整性违规问题。


<details>
  <summary>Details</summary>
Motivation: 部署LLM代理时，自然语言指令的模糊性可能导致代理行为偏离用户意图，尤其是随着外部工具集的演变，传统测试方法无法处理这种模糊性。

Method: IntenTest通过工具文档生成真实任务，应用定向变异，并利用语义分区和轻量级预测器提升测试效率。

Result: 在80个工具API的实验中，IntenTest显著优于基线方法，能有效暴露错误，并对更强的目标模型表现良好。

Conclusion: IntenTest为解决LLM代理意图完整性测试提供了一种高效、可扩展的方法，适用于动态演变的API环境。

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [13] [Evaluating LLMs Effectiveness in Detecting and Correcting Test Smells: An Empirical Study](https://arxiv.org/abs/2506.07594)
*E. G. Santana Jr,Jander Pereira Santos Junior,Erlon P. Almeida,Iftekhar Ahmed,Paulo Anselmo da Mota Silveira Neto,Eduardo Santana de Almeida*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLM）在识别和重构测试代码中的坏味道（test smells）方面的能力，评估了GPT-4-Turbo、LLaMA 3 70B和Gemini-1.5 Pro的表现，发现Gemini表现最佳。


<details>
  <summary>Details</summary>
Motivation: 测试代码中的坏味道降低了可维护性和可靠性，但现有工具主要集中于检测而非自动重构。LLM在代码理解和转换方面表现出潜力，但其在测试坏味道处理中的应用尚未充分探索。

Method: 使用PyNose和TsDetect检测Python和Java测试套件中的坏味道，随后利用GPT-4-Turbo、LLaMA 3 70B和Gemini-1.5 Pro进行重构。

Result: Gemini在检测和重构方面表现最佳，检测准确率最高（Python 74.35%，Java 80.32%），并能提高测试覆盖率；而GPT-4和LLaMA则可能降低覆盖率或引入新坏味道。

Conclusion: LLM在自动化测试坏味道重构方面具有潜力，尤其是Gemini表现突出，但不同语言和坏味道类型的处理仍存在挑战。

Abstract: Test smells indicate poor development practices in test code, reducing
maintainability and reliability. While developers often struggle to prevent or
refactor these issues, existing tools focus primarily on detection rather than
automated refactoring. Large Language Models (LLMs) have shown strong potential
in code understanding and transformation, but their ability to both identify
and refactor test smells remains underexplored. We evaluated GPT-4-Turbo, LLaMA
3 70B, and Gemini-1.5 Pro on Python and Java test suites, using PyNose and
TsDetect for initial smell detection, followed by LLM-driven refactoring.
Gemini achieved the highest detection accuracy (74.35\% Python, 80.32\% Java),
while LLaMA was lowest. All models could refactor smells, but effectiveness
varied, sometimes introducing new smells. Gemini also improved test coverage,
unlike GPT-4 and LLaMA, which often reduced it. These results highlight LLMs'
potential for automated test smell refactoring, with Gemini as the strongest
performer, though challenges remain across languages and smell types.

</details>


### [14] [Leveraging Network Methods for Hub-like Microservice Detection](https://arxiv.org/abs/2506.07683)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 提出了一种检测微服务架构中Hub-like反模式的稳健方法，并比较了多种网络中心检测技术的效果。


<details>
  <summary>Details</summary>
Motivation: 由于Hub-like反模式缺乏明确的定义和检测方法，研究旨在找到一种高精度的检测方法。

Method: 利用25个微服务网络数据集，采用多种网络中心检测技术（如无标度属性、中心性度量等）进行识别。

Result: 发现研究的网络不具备无标度特性，多数方法在检测中心点时不一致，Kirkley的ER编码方法最准确。

Conclusion: 研究为Arcan工具提供了改进建议，并指出了未来研究方向。

Abstract: Context: Microservice Architecture is a popular architectural paradigm that
facilitates flexibility by decomposing applications into small, independently
deployable services. Catalogs of architectural anti-patterns have been proposed
to highlight the negative aspects of flawed microservice design. In particular,
the Hub-like anti-pattern lacks an unambiguous definition and detection method.
Aim: In this work, we aim to find a robust detection approach for the Hub-like
microservice anti-pattern that outputs a reasonable number of Hub-like
candidates with high precision. Method: We leveraged a dataset of 25
microservice networks and several network hub detection techniques to identify
the Hub-like anti-pattern, namely scale-free property, centrality metrics and
clustering coefficient, minimum description length principle, and the approach
behind the Arcan tool. Results and Conclusion: Our findings revealed that the
studied architectural networks are not scale-free, that most considered hub
detection approaches do not agree on the detected hubs, and that the method by
Kirkley leveraging the Erdos-Renyi encoding is the most accurate one in terms
of the number of detected hubs and the detection precision. Investigating
further the applicability of these methods to detecting Hub-like components in
microservice-based and other systems opens up new research directions.
Moreover, our results provide an evaluation of the approach utilized by the
widely used Arcan tool and highlight the potential to update the tool to use
the normalized degree centrality of a component in the network, or for the
approach based on ER encoding to be adopted instead.

</details>


### [15] [Centrality Change Proneness: an Early Indicator of Microservice Architectural Degradation](https://arxiv.org/abs/2506.07690)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究了时间中心性指标是否有助于早期检测微服务架构退化，发现7个规模和5个复杂性指标与中心性相关，新指标Centrality Change Proneness可作为早期指示器。


<details>
  <summary>Details</summary>
Motivation: 探讨时间中心性指标与软件指标的关系，以早期检测微服务架构退化。

Method: 对7个版本的OSS微服务项目进行架构重建，计算软件和中心性指标，分析相关性。

Result: 7个规模和5个复杂性指标与中心性相关，Centrality Change Proneness不影响软件指标。

Conclusion: 时间中心性指标为微服务架构退化提供了新视角和早期检测工具。

Abstract: Over the past decade, the wide adoption of Microservice Architecture has
required the identification of various patterns and anti-patterns to prevent
Microservice Architectural Degradation. Frequently, the systems are modelled as
a network of connected services. Recently, the study of temporal networks has
emerged as a way to describe and analyze evolving networks. Previous research
has explored how software metrics such as size, complexity, and quality are
related to microservice centrality in the architectural network. This study
investigates whether temporal centrality metrics can provide insight into the
early detection of architectural degradation by correlating or affecting
software metrics. We reconstructed the architecture of 7 releases of an OSS
microservice project with 42 services. For every service in every release, we
computed the software and centrality metrics. From one of the latter, we
derived a new metric, Centrality Change Proneness. We then explored the
correlation between the metrics. We identified 7 size and 5 complexity metrics
that have a consistent correlation with centrality, while Centrality Change
Proneness did not affect the software metrics, thus providing yet another
perspective and an early indicator of microservice architectural degradation.

</details>


### [16] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: 该研究提出了一个模块化的SLMs生命周期框架，整合了学术和实践中的见解，支持方法重用和生命周期意识。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补小型语言模型(SLMs)研究中缺乏统一生命周期视角的空白。

Method: 通过综合分析36项研究，归类和分析生命周期相关技术。

Result: 提出了一个模块化的生命周期模型，包含主要、可选和跨领域组件，支持方法重用和生命周期意识。

Conclusion: 该框架为SLMs的开发和维护提供了统一的基础，并指导未来研究和工具开发。

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


### [17] [Adversarial Attack Classification and Robustness Testing for Large Language Models for Code](https://arxiv.org/abs/2506.07942)
*Yang Liu,Armstrong Foundjem,Foutse Khomh,Heng Li*

Main category: cs.SE

TL;DR: 论文研究了对抗性扰动对代码生成大语言模型（LLM4Code）的影响，分析了字符、词和句子级别的扰动效果，发现词级别扰动最具威胁。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，确保其对对抗性输入的鲁棒性变得至关重要。本研究填补了自然语言输入对代码任务影响的空白。

Method: 采用混合方法，结合定量性能指标和定性漏洞分析，测试了字符、词和句子级别的对抗性扰动对LLM4Code的影响。

Result: 词级别扰动最易暴露模型漏洞，而句子级别扰动对模型影响较小，字符级别扰动效果因模型而异。

Conclusion: 研究为测试LLM4Code鲁棒性提供了结构化框架，并强调提高模型对语义级扰动的抵抗力是关键。

Abstract: Large Language Models (LLMs) have become vital tools in software development
tasks such as code generation, completion, and analysis. As their integration
into workflows deepens, ensuring robustness against vulnerabilities especially
those triggered by diverse or adversarial inputs becomes increasingly
important. Such vulnerabilities may lead to incorrect or insecure code
generation when models encounter perturbed task descriptions, code, or
comments. Prior research often overlooks the role of natural language in
guiding code tasks. This study investigates how adversarial perturbations in
natural language inputs including prompts, comments, and descriptions affect
LLMs for Code (LLM4Code). It examines the effects of perturbations at the
character, word, and sentence levels to identify the most impactful
vulnerabilities. We analyzed multiple projects (e.g., ReCode, OpenAttack) and
datasets (e.g., HumanEval, MBPP), establishing a taxonomy of adversarial
attacks. The first dimension classifies the input type code, prompts, or
comments while the second dimension focuses on granularity: character, word, or
sentence-level changes. We adopted a mixed-methods approach, combining
quantitative performance metrics with qualitative vulnerability analysis.
LLM4Code models show varying robustness across perturbation types.
Sentence-level attacks were least effective, suggesting models are resilient to
broader contextual changes. In contrast, word-level perturbations posed serious
challenges, exposing semantic vulnerabilities. Character-level effects varied,
showing model sensitivity to subtle syntactic deviations.Our study offers a
structured framework for testing LLM4Code robustness and emphasizes the
critical role of natural language in adversarial evaluation. Improving model
resilience to semantic-level disruptions is essential for secure and reliable
code-generation systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Optimizing Optimizations: Case Study on Detecting Specific Types of Mathematical Optimization Constraints with E-Graphs in JijModeling](https://arxiv.org/abs/2506.06495)
*Hiromi Ishii,Taro Shimizu,Toshiki Teramura*

Main category: cs.PL

TL;DR: 论文探讨了如何利用特定约束信息（如one-hot或SOS约束）来提高数学优化问题的效率，并通过JijModeling实现符号表示与输入数据的分离。文中提出了基于e-graph的约束检测机制和启发式重写系统标准，并展示了性能测试结果。此外，还介绍了egg_recursive工具库，用于简化复杂S表达式的编写和维护。


<details>
  <summary>Details</summary>
Motivation: 利用特定约束信息可以显著提高数学优化问题的执行效率，但现有方法在符号表示和数据处理方面存在不足。

Method: 通过JijModeling分离符号表示与输入数据，提出了基于e-graph的约束检测机制和启发式重写系统标准。

Result: 性能测试表明约束检测机制有效提升了优化问题的执行效率。

Conclusion: 该方法通过优化约束检测和符号表示，显著提升了数学优化问题的解决效率，并为复杂S表达式的管理提供了工具支持。

Abstract: In solving mathematical optimization problems efficiently, it is crucial to
make use of information about specific types of constraints, such as the
one-hot or Special-Ordered Set (SOS) constraints. In many cases, exploiting
such information gives asymptotically better execution time. JijModeling, an
industrial-strength mathematical optimization modeller, achieves this by
separating the symbolic representation of an optimization problem from the
input data. In this paper, we will report a real-world case study on a
constraint detection mechanism modulo the algebraic congruence using e-graphs,
and describe heuristic criteria for designing rewriting systems. We give
benchmarking result that shows the performance impact of the constraint
detection mechanism.
  We also introduce egg_recursive, a utility library for writing egg-terms as
recursive abstract syntax trees, reducing the burden of writing and maintaining
complex terms in S-expressions.

</details>


### [19] [Reasoning about External Calls](https://arxiv.org/abs/2506.06544)
*Sophia Drossopoulou,Julian Mackay,Susan Eisenbach,James Noble*

Main category: cs.PL

TL;DR: 本文提出了一种新方法来规范化和验证内部代码，通过封装和能力限制来控制外部调用的影响。


<details>
  <summary>Details</summary>
Motivation: 在复杂软件中，内部可信代码与外部不可信代码紧密交织，需限制外部调用的潜在影响。

Method: 引入新的能力访问断言、效果限制规范及Hoare逻辑，验证模块满足规范。

Result: 通过运行示例和机械化证明展示了方法的有效性，并证明了Hoare逻辑的合理性。

Conclusion: 该方法成功确保了内部代码的可靠性，即使在外部调用存在的情况下。

Abstract: In today's complex software, internal trusted code is tightly intertwined
with external untrusted code. To reason about internal code, programmers must
reason about the potential effects of calls to external code, even though that
code is not trusted and may not even be available. The effects of external
calls can be limited, if internal code is programmed defensively, limiting
potential effects by limiting access to the capabilities necessary to cause
those effects.
  This paper addresses the specification and verification of internal code that
relies on encapsulation and object capabilities to limit the effects of
external calls. We propose new assertions for access to capabilities, new
specifications for limiting effects, and a Hoare logic to verify that a module
satisfies its specification, even while making external calls. We illustrate
the approach though a running example with mechanised proofs, and prove
soundness of the Hoare logic.

</details>


### [20] [Execution-Aware Program Reduction for WebAssembly via Record and Replay](https://arxiv.org/abs/2506.07834)
*Doehyun Baek,Daniel Lehmann,Ben L. Titzer,Sukyoung Ryu,Michael Pradel*

Main category: cs.PL

TL;DR: 该论文提出两种执行感知的程序缩减技术（RR-Reduce和Hybrid-Reduce），通过记录和重放执行行为来解决现有静态技术处理大型复杂WebAssembly程序时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有技术依赖静态信息且忽略执行行为，难以有效缩减大型复杂WebAssembly程序。

Method: RR-Reduce通过识别和隔离触发错误的函数生成缩减程序；Hybrid-Reduce结合执行感知和无感知技术进一步缩减程序。

Result: RR-Reduce平均将程序缩减至原大小的1.20%，用时14.5分钟；Hybrid-Reduce缩减至0.13%，用时3.5小时，均优于现有技术。

Conclusion: RR-Reduce适合快速调试，Hybrid-Reduce适合需要最小化程序的场景。

Abstract: WebAssembly (Wasm) programs may trigger bugs in their engine implementations.
To aid debugging, program reduction techniques try to produce a smaller variant
of the input program that still triggers the bug. However, existing
execution-unaware program reduction techniques struggle with large and complex
Wasm programs, because they rely on static information and apply syntactic
transformations, while ignoring the valuable information offered by the input
program's execution behavior.
  We present RR-Reduce and Hybrid-Reduce, novel execution-aware program
reduction techniques that leverage execution behaviors via record and replay.
RR-Reduce identifies a bug-triggering function as the target function, isolates
that function from the rest of the program, and generates a reduced program
that replays only the interactions between the target function and the rest of
the program. Hybrid-Reduce combines a complementary execution-unaware reduction
technique with RR-Reduce to further reduce program size.
  We evaluate RR-Reduce and Hybrid-Reduce on 28 Wasm programs that trigger a
diverse set of bugs in three engines. On average, RR-Reduce reduces the
programs to 1.20 percent of their original size in 14.5 minutes, which
outperforms the state of the art by 33.15 times in terms of reduction time.
Hybrid-Reduce reduces the programs to 0.13 percent of their original size in
3.5 hours, which outperforms the state of the art by 3.42 times in terms of
reduced program size and 2.26 times in terms of reduction time. We envision
RR-Reduce as the go-to tool for rapid, on-demand debugging in minutes, and
Hybrid-Reduce for scenarios where developers require the smallest possible
programs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [21] [Pinching-Antenna Systems For Indoor Immersive Communications: A 3D-Modeling Based Performance Analysis](https://arxiv.org/abs/2506.07771)
*Yulei Wang,Yalin Liu,Yaru Fu,Zhiguo Ding*

Main category: cs.PF

TL;DR: 该论文研究了用于室内沉浸式通信的Pinching-antenna系统（PASS），提出了3D模型、理论模型和部署指南。


<details>
  <summary>Details</summary>
Motivation: 探索Pinching天线技术（PA）在6G室内沉浸式应用中的潜力，解决视线遮挡问题。

Method: 1. 构建3D模型描述用户、波导和PA的分布；2. 建立理论模型分析下行链路性能；3. 提供数值结果和部署指南。

Result: 理论模型得到验证，并提供了PASS系统的部署建议。

Conclusion: PASS系统在6G室内应用中具有潜力，能够灵活配置无线信道并解决视线遮挡问题。

Abstract: The emerging pinching antenna (PA) technology has high flexibility to
reconfigure wireless channels and combat line-of-sight blockage, thus holding
transformative potential for indoor immersive applications in 6G. This paper
investigates Pinching-antenna systems (PASS) for indoor immersive
communications. Our contributions are threefold: (1) we construct a 3D model to
characterize the distribution of users, waveguides, and PAs in the PASS; (2) we
develop a general theoretical model on downlink performance of PASS by
capturing PA-user relationships and system parameters' impacts; and (3) we
conduct comprehensive numerical results of the theoretical model and provide
implementation guidelines for PASS deployments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [Steps towards an Ecology for the Internet](https://arxiv.org/abs/2506.06469)
*Anil Madhavapeddy,Sam Reynolds,Alec P. Christie,David A. Coomes,Michael W. Dales,Patrick Ferris,Ryan Gibb,Hamed Haddadi,Sadiq Jaffer,Josh Millar,Cyrus Omar,William J. Sutherland,Jon Crowcroft*

Main category: cs.NI

TL;DR: 论文探讨互联网未来的安全挑战，借鉴生物系统提出构建更具弹性的互联网，并倡导去中心化以促进互利通信。


<details>
  <summary>Details</summary>
Motivation: 互联网缺乏内置的免疫系统，未来可能面临万亿节点和AI驱动的威胁，当前过度中心化导致互惠关系崩溃。

Method: 从生物系统中汲取灵感，提出将适应性机制融入互联网架构，设计数字免疫系统并提倡软件栈多样化。

Result: 提出了互联网去中心化的建议，以及如何通过数字免疫系统增强其弹性。

Conclusion: 互联网需要重新去中心化，并通过生物启发的方法提升其适应性和安全性。

Abstract: The Internet has grown from a humble set of protocols for end-to-end
connectivity into a critical global system with no builtin "immune system". In
the next decade the Internet will likely grow to a trillion nodes and need
protection from threats ranging from floods of fake generative data to
AI-driven malware. Unfortunately, growing centralisation has lead to the
breakdown of mutualism across the network, with surveillance capitalism now the
dominant business model. We take lessons from from biological systems towards
evolving a more resilient Internet that can integrate adaptation mechanisms
into its fabric. We also contribute ideas for how the Internet might
incorporate digital immune systems, including how software stacks might mutate
to encourage more architectural diversity. We strongly advocate for the
Internet to "re-decentralise" towards incentivising more mutualistic forms of
communication.

</details>


### [23] [A Comparative Analyses Of Network Formation In Low-power Lossy Networks: ContikiMAC vs Orchestra-enabled TSCH](https://arxiv.org/abs/2506.06688)
*Heerok Banerjee*

Main category: cs.NI

TL;DR: 对Contiki-MAC和Orchestra-enabled TSCH协议进行了比较分析，Contiki-MAC在网络形成方面表现优于后者13倍。


<details>
  <summary>Details</summary>
Motivation: 针对低功耗有损网络（LLNs），选择合适的MAC层协议对满足网络目标（如加入时间、网络寿命、能耗等）至关重要。

Method: 通过比较Contiki-MAC和Orchestra-enabled TSCH协议的网络加入、收敛时间及能耗。

Result: Contiki-MAC在网络形成方面的性能优于Orchestra-enabled TSCH 13倍。

Conclusion: Contiki-MAC更适合用于低功耗有损网络的构建。

Abstract: Medium Access Control (MAC) layer protocols are the underlying paradigms
which dictate the transmission & reception of data in any network. Particularly
for Low-powered Lossy Networks (LLNs), the design and selection of appropiate
MAC-layer protocols is crucial inorder to satisfy several networking objectives
such as joining time, network lifetime, energy consumption, end-to-end-delay,
etc. In this report, we have presented a comparative analysis between
Contiki-MAC and Orchestra-enabled TSCH protocol which provides insights towards
the network joining & convergence time as well as an estimate of the energy
consumption required of build such LLNs. Our results indicates that Contiki-MAC
outperforms Orchestra-enabled TSCH by a factor of 13 times in network
formation.

</details>


### [24] [ARGOS: Anomaly Recognition and Guarding through O-RAN Sensing](https://arxiv.org/abs/2506.06916)
*Stavros Dimou,Guevara Noubir*

Main category: cs.NI

TL;DR: ARGOS是一种在O-RAN环境中实时检测Rogue Base Station降级攻击的入侵检测系统，利用改进的3GPP KPM服务模型和机器学习模型进行异常检测，性能优越。


<details>
  <summary>Details</summary>
Motivation: 5G独立部署有限且设备制造商仍支持传统网络连接，Rogue Base Station攻击持续威胁网络安全。

Method: ARGOS系统部署于Near Real-Time RIC，采用改进的KPM服务模型和基于无监督学习的异常检测xApp，利用跨层特征和商用设备数据。

Result: 在真实测试环境中，变分自编码器（VAE）表现最佳，准确率达99.5%，误报率仅0.6%。

Conclusion: ARGOS能有效检测RBS降级攻击，为O-RAN环境提供了一种高效的安全解决方案。

Abstract: Rogue Base Station (RBS) attacks, particularly those exploiting downgrade
vulnerabilities, remain a persistent threat as 5G Standalone (SA) deployments
are still limited and User Equipment (UE) manufacturers continue to support
legacy network connectivity. This work introduces ARGOS, a comprehensive O-RAN
compliant Intrusion Detection System (IDS) deployed within the Near Real-Time
RIC, designed to detect RBS downgrade attacks in real time, an area previously
unexplored within the O-RAN context. The system enhances the 3GPP KPM Service
Model to enable richer, UE-level telemetry and features a custom xApp that
applies unsupervised Machine Learning models for anomaly detection.
Distinctively, the updated KPM Service Model operates on cross-layer features
extracted from Modem Layer 1 (ML1) logs and Measurement Reports collected
directly from Commercial Off-The-Shelf (COTS) UEs. To evaluate system
performance under realistic conditions, a dedicated testbed is implemented
using Open5GS, srsRAN, and FlexRIC, and validated against an extensive
real-world measurement dataset. Among the evaluated models, the Variational
Autoencoder (VAE) achieves the best balance of detection performance and
efficiency, reaching 99.5% Accuracy with only 0.6% False Positives and minimal
system overhead.

</details>


### [25] [Delay Optimization in Remote ID-Based UAV Communication via BLE and Wi-Fi Switching](https://arxiv.org/abs/2506.07715)
*Yian Zhu,Ziye Jia,Lei Zhang,Yao Wu,Qiuming Zhu,Qihui Wu*

Main category: cs.NI

TL;DR: 论文提出了一种基于自适应BLE/Wi-Fi切换算法的远程识别（Remote ID）通信优化方法，以降低动态环境下多无人机操作的延迟。


<details>
  <summary>Details</summary>
Motivation: 远程识别（Remote ID）广播能力是无人机间通信的关键技术，但在动态环境中，低延迟至关重要。

Method: 通过建立BLE 4和Wi-Fi协议的延迟模型，提出了一种基于多智能体深度Q网络的自适应BLE/Wi-Fi切换算法。

Result: 实验结果表明，在动态密度场景中，该策略比静态BLE 4和Wi-Fi模式分别降低了32.1%和37.7%的延迟。

Conclusion: 自适应协议选择显著提升了多无人机操作的效率和实时性。

Abstract: The remote identification (Remote ID) broadcast capability allows unmanned
aerial vehicles (UAVs) to exchange messages, which is a pivotal technology for
inter-UAV communications. Although this capability enhances the operational
visibility, low delay in Remote ID-based communications is critical for
ensuring the efficiency and timeliness of multi-UAV operations in dynamic
environments. To address this challenge, we first establish delay models for
Remote ID communications by considering packet reception and collisions across
both BLE 4 and Wi-Fi protocols. Building upon these models, we formulate an
optimization problem to minimize the long-term communication delay through
adaptive protocol selection. Since the delay performance varies with the UAV
density, we propose an adaptive BLE/Wi-Fi switching algorithm based on the
multi-agent deep Q-network approach. Experimental results demonstrate that in
dynamic-density scenarios, our strategy achieves 32.1% and 37.7% lower latency
compared to static BLE 4 and Wi-Fi modes respectively.

</details>


### [26] [Diffusion-RL for Scalable Resource Allocation for 6G Networks](https://arxiv.org/abs/2506.07880)
*Salar Nouri,Mojdeh Karbalaee Motalleb,Vahid Shah-Mansouri*

Main category: cs.NI

TL;DR: 提出了一种基于生成式AI和网络切片的资源分配方法，用于O-RAN中的5G和6G服务需求，引入Diffusion-RL算法优化资源分配，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决O-RAN中5G和6G多样化服务类型（如eMBB、URLLC、mMTC）的资源分配问题。

Method: 提出Diffusion-RL算法，通过控制噪声和扰动优化PRB和功耗分配。

Result: Diffusion-RL在吞吐量和延迟方面优于现有方法（如DQN、SS-VAE），展现出高效性和可扩展性。

Conclusion: Diffusion-RL为动态异构O-RAN环境提供了有效的资源分配方案，对6G网络有重要启示。

Abstract: This paper presents a novel approach to resource allocation in Open Radio
Access Networks (O-RAN), leveraging a Generative AI technique with network
slicing to address the diverse demands of 5G and 6G service types such as
Enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communications
(URLLC), and Massive Machine-Type Communications (mMTC). Additionally, we
provide a comprehensive analysis and comparison of machine learning (ML)
techniques for resource allocation within O-RAN, evaluating their effectiveness
in optimizing network performance. We introduce a diffusion-based reinforcement
learning (Diffusion-RL) algorithm designed to optimize the allocation of
physical resource blocks (PRBs) and power consumption, thereby maximizing
weighted throughput and minimizing the delay for user equipment (UE). The
Diffusion-RL model incorporates controlled noise and perturbations to explore
optimal resource distribution while meeting each service type's Quality of
Service (QoS) requirements. We evaluate the performance of our proposed method
against several benchmarks, including an exhaustive search algorithm, deep
Q-networks (DQN), and the Semi-Supervised Variational Autoencoder (SS-VAE).
Comprehensive metrics, such as throughput and latency, are presented for each
service type. Experimental results demonstrate that the Diffusion-based RL
approach outperforms existing methods in efficiency, scalability, and
robustness, offering a promising solution for resource allocation in dynamic
and heterogeneous O-RAN environments with significant implications for future
6G networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [An Efficient Digital Watermarking Technique for Small Scale devices](https://arxiv.org/abs/2506.06691)
*Kaushik Talathi,Aparna Santra Biswas*

Main category: cs.MM

TL;DR: 论文提出了一种轻量级的水印方案FWT-AQIM，适用于低功耗设备，在鲁棒性、不可感知性和计算效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 在IoT和移动平台时代，如何在有限硬件资源下确保内容真实性是一个关键问题。

Method: 采用混合快速小波变换和加性量化索引调制（FWT-AQIM）方法，在YCbCr色彩空间的亮度分量中嵌入水印。

Result: 在Raspberry Pi 5上测试，嵌入和提取过程耗时小于40毫秒，PSNR≥34 dB，SSIM≥0.97，对攻击的鲁棒性表现优异。

Conclusion: FWT-AQIM为带宽和功耗受限的场景提供了高效、可扩展的实时安全水印解决方案。

Abstract: In the age of IoT and mobile platforms, ensuring that content stay authentic
whilst avoiding overburdening limited hardware is a key problem. This study
introduces hybrid Fast Wavelet Transform & Additive Quantization index
Modulation (FWT-AQIM) scheme, a lightweight watermarking approach that secures
digital pictures on low-power, memory-constrained small scale devices to
achieve a balanced trade-off among robustness, imperceptibility, and
computational efficiency. The method embeds watermark in the luminance
component of YCbCr color space using low-frequency FWT sub-bands, minimizing
perceptual distortion, using additive QIM for simplicity. Both the extraction
and embedding processes run in less than 40 ms and require minimum RAM when
tested on a Raspberry Pi 5. Quality assessments on standard and high-resolution
images yield PSNR greater than equal to 34 dB and SSIM greater than equal to
0.97, while robustness verification includes various geometric and
signal-processing attacks demonstrating near-zero bit error rates and NCC
greater than equal to 0.998. Using a mosaic-based watermark, redundancy added
enhancing robustness without reducing throughput, which peaks at 11 MP/s. These
findings show that FWT-AQIM provides an efficient, scalable solution for
real-time, secure watermarking in bandwidth- and power-constrained contexts,
opening the way for dependable content protection in developing IoT and
multimedia applications.

</details>


### [28] [The State-of-the-Art in Lifelog Retrieval: A Review of Progress at the ACM Lifelog Search Challenge Workshop 2022-24](https://arxiv.org/abs/2506.06743)
*Allie Tran,Werner Bailer,Duc-Tien Dang-Nguyen,Graham Healy,Steve Hodges,Björn Þór Jónsson,Luca Rossetto,Klaus Schoeffmann,Minh-Triet Tran,Lucia Vadicamo,Cathal Gurrin*

Main category: cs.MM

TL;DR: 论文回顾了2022年至2024年ACM LSC竞赛中互动式生命日志检索的进展，分析比较了三种检索任务的改进趋势，突出嵌入检索方法和大型语言模型的应用，并探讨了用户界面设计对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 通过分析ACM LSC竞赛中的系统表现，总结互动式生命日志检索的最新进展和趋势，为未来研究和系统开发提供参考。

Method: 通过对2022-2024年ACM LSC竞赛的系统进行详细比较分析，归纳出嵌入检索方法（如CLIP、BLIP）、大型语言模型（LLMs）在会话检索中的应用，以及多模态和协作搜索界面的创新。

Result: 研究发现嵌入驱动方法与LLMs结合在生命日志检索中表现出潜力，同时改进UI设计能提升系统的可用性和效率。

Conclusion: 论文强调未来应注重平衡检索复杂性与可用性，并建议在专家系统中改进多实例系统评估以适应不同用户熟悉程度和配置效果。

Abstract: The ACM Lifelog Search Challenge (LSC) is a venue that welcomes and compares
systems that support the exploration of lifelog data, and in particular the
retrieval of specific information, through an interactive competition format.
This paper reviews the recent advances in interactive lifelog retrieval as
demonstrated at the ACM LSC from 2022 to 2024. Through a detailed comparative
analysis, we highlight key improvements across three main retrieval tasks:
known-item search, question answering, and ad-hoc search. Our analysis
identifies trends such as the widespread adoption of embedding-based retrieval
methods (e.g., CLIP, BLIP), increased integration of large language models
(LLMs) for conversational retrieval, and continued innovation in multimodal and
collaborative search interfaces. We further discuss how specific retrieval
techniques and user interface (UI) designs have impacted system performance,
emphasizing the importance of balancing retrieval complexity with usability.
Our findings indicate that embedding-driven approaches combined with LLMs show
promise for lifelog retrieval systems. Likewise, improving UI design can
enhance usability and efficiency. Additionally, we recommend reconsidering
multi-instance system evaluations within the expert track to better manage
variability in user familiarity and configuration effectiveness.

</details>


### [29] [Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP](https://arxiv.org/abs/2506.06938)
*Bastian Jäckl,Vojtěch Kloda,Daniel A. Keim,Jakub Lokoč*

Main category: cs.MM

TL;DR: 研究探讨了在高度同质化的领域（如水下场景）中，通过添加基于位置的提示来增强文本查询的图像检索效果，结果显示简单分区方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 专业领域中，用户因缺乏专业知识只能提供模糊的文本描述，导致多模态模型在高度同质化场景中检索效果不佳。

Method: 收集了741个人工标注（包含文本描述和感兴趣区域框），评估CLIP模型在不同静态子区域与完整图像上的检索表现。

Result: 3×3分区和5网格重叠方法能显著提升检索效果，并对标注框扰动具有鲁棒性。

Conclusion: 基于位置的提示是提升高度同质化领域图像检索的有效策略。

Abstract: Advances in multimodal text-image models have enabled effective text-based
querying in extensive image collections. While these models show convincing
performance for everyday life scenes, querying in highly homogeneous,
specialized domains remains challenging. The primary problem is that users can
often provide only vague textual descriptions as they lack expert knowledge to
discriminate between homogenous entities. This work investigates whether adding
location-based prompts to complement these vague text queries can enhance
retrieval performance. Specifically, we collected a dataset of 741 human
annotations, each containing short and long textual descriptions and bounding
boxes indicating regions of interest in challenging underwater scenes. Using
these annotations, we evaluate the performance of CLIP when queried on various
static sub-regions of images compared to the full image. Our results show that
both a simple 3-by-3 partitioning and a 5-grid overlap significantly improve
retrieval effectiveness and remain robust to perturbations of the annotation
box.

</details>


### [30] [Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets](https://arxiv.org/abs/2506.07076)
*Xinyi Wu,Haohong Wang,Aggelos K. Katsaggelos*

Main category: cs.MM

TL;DR: 提出了一种基于GAN的和谐感知框架，用于提升音乐到动作合成的节奏同步性，通过跨模态节拍检测和人类感知策略实现更自然的3D人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 随着视频用户生成内容（UGC）的流行，基于人类感知原则的和谐性对提升音频-视觉内容的节奏一致性至关重要，从而提高用户参与度。

Method: 设计了一种和谐评估策略，包括跨模态节拍检测、基于显著性的节拍加权和间隔驱动的节拍对齐；采用编码器-解码器和深度提升设计的GAN模型，并通过弱监督感知约束指导生成过程。

Result: 实验表明，该方法在节奏和谐性上显著优于其他音乐到动作合成方法，即使训练数据有限，也能生成更真实的3D人体运动。

Conclusion: 提出的和谐感知框架和评估策略有效提升了音乐到动作合成的节奏同步性，符合人类感知原则。

Abstract: With the popularity of video-based user-generated content (UGC) on social
media, harmony, as dictated by human perceptual principles, is critical in
assessing the rhythmic consistency of audio-visual UGCs for better user
engagement. In this work, we propose a novel harmony-aware GAN framework,
following a specifically designed harmony evaluation strategy to enhance
rhythmic synchronization in the automatic music-to-motion synthesis using a UGC
dance dataset. This harmony strategy utilizes refined cross-modal beat
detection to capture closely correlated audio and visual rhythms in an
audio-visual pair. To mimic human attention mechanism, we introduce
saliency-based beat weighting and interval-driven beat alignment, which ensures
accurate harmony score estimation consistent with human perception. Building on
this strategy, our model, employing efficient encoder-decoder and depth-lifting
designs, is adversarially trained based on categorized musical meter segments
to generate realistic and rhythmic 3D human motions. We further incorporate our
harmony evaluation strategy as a weakly supervised perceptual constraint to
flexibly guide the synchronized audio-visual rhythms during the generation
process. Experimental results show that our proposed model significantly
outperforms other leading music-to-motion methods in rhythmic harmony, both
quantitatively and qualitatively, even with limited UGC training data. Live
samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [31] [Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks](https://arxiv.org/abs/2506.06870)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.LO

TL;DR: 论文提出了一种递归语义锚定方法，通过固定点算子建模语言变体的语义漂移，为ISO 639:2023提供了一种机器友好的处理机制。


<details>
  <summary>Details</summary>
Motivation: ISO 639:2023缺乏处理方言漂移和克里奥尔语混合的机制，希望通过数学模型解决这一问题。

Method: 使用固定点算子和语义漂移向量建模语言变体的语义变化，并通过范畴论和RDF/Turtle模式实现框架。

Result: 实验表明，基于语义锚定的方法在语言识别和翻译任务中表现更好，尤其是在噪声或代码切换输入时。

Conclusion: 该框架为未来语言标准的语义层提供了AI可处理的解决方案，兼容ISO/TC 37。

Abstract: ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.

</details>


### [32] [Verification of Quantum Circuits through Barrier Certificates using a Scenario Approach](https://arxiv.org/abs/2506.07635)
*Siwei Hu,Victor Lopata,Sadegh Soudjani,Paolo Zuliani*

Main category: cs.LO

TL;DR: 本文提出了一种基于场景的方法，用于合成量子电路的屏障证书，以验证其正确性，适用于有限和无限时间范围，并能处理初始状态和系统动态的不确定性。


<details>
  <summary>Details</summary>
Motivation: 量子电路的验证是一个关键问题，屏障证书作为一种数学工具可以确保系统从初始状态出发且应用系统动态后不会进入不期望的状态。

Method: 采用基于场景的方法合成屏障证书，并考虑初始状态和系统动态的不确定性。

Result: 通过多个量子电路的案例研究，比较了不同类型屏障证书的性能，并分析了各自的适用性。

Conclusion: 该方法能有效验证量子电路的正确性，并为选择合适的屏障证书提供了指导。

Abstract: In recent years, various techniques have been explored for the verification
of quantum circuits, including the use of barrier certificates, mathematical
tools capable of demonstrating the correctness of such systems. These
certificates ensure that, starting from initial states and applying the
system's dynamics, the system will never reach undesired states. In this paper,
we propose a methodology for synthesizing such certificates for quantum
circuits using a scenario-based approach, for both finite and infinite time
horizons. In addition, our approach can handle uncertainty in the initial
states and in the system's dynamics. We present several case studies on quantum
circuits, comparing the performance of different types of barrier certificate
and analyzing which one is most suitable for each case.

</details>


### [33] [On-The-Fly Symbolic Algorithm for Timed ATL with Abstractions](https://arxiv.org/abs/2506.07802)
*Nicolaj Ø. Jensen,Kim G. Larsen,Didier Lime,Jiří Srba*

Main category: cs.LO

TL;DR: 提出了一个实时验证TATL逻辑的算法，通过动态依赖图和新型抽象方法显著提升性能，优于现有关工具。


<details>
  <summary>Details</summary>
Motivation: 多组件多参与方的实时系统验证计算复杂度高，需要高效算法。

Method: 结合游戏理论和时间CTL验证，在ADG框架中实现动态算法，并提出新抽象方法避免包含检查。

Result: 实验显示新方法比原始方法快近两个数量级，并优于Uppaal Tiga。

Conclusion: 提出的抽象方法显著提升验证效率，适用范围更广。

Abstract: Verification of real-time systems with multiple components controlled by
multiple parties is a challenging task due to its computational complexity. We
present an on-the-fly algorithm for verifying timed alternating-time temporal
logic (TATL), a branching-time logic with quantifiers over outcomes that
results from coalitions of players in such systems. We combine existing work on
games and timed CTL verification in the abstract dependency graph (ADG)
framework, which allows for easy creation of on-the-fly algorithms that only
explore the state space as needed. In addition, we generalize the conventional
inclusion check to the ADG framework which enables dynamic reductions of the
dependency graph. Using the insights from the generalization, we present a
novel abstraction that eliminates the need for inclusion checking altogether in
our domain. We implement our algorithms in Uppaal and our experiments show that
while inclusion checking considerably enhances performance, our abstraction
provides even more significant improvements, almost two orders of magnitude
faster than the naive method. In addition, we outperform Uppaal Tiga, which can
verify only a strict subset of TATL. After implementing our new abstraction in
Uppaal Tiga, we also improve its performance by almost an order of magnitude.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [34] [Fake Friends and Sponsored Ads: The Risks of Advertising in Conversational Search](https://arxiv.org/abs/2506.06447)
*Jacob Erickson*

Main category: cs.HC

TL;DR: 论文探讨了对话搜索中的广告问题，指出其对用户体验的潜在危害，并提出了“假朋友困境”的概念，呼吁采取行动以避免风险。


<details>
  <summary>Details</summary>
Motivation: 随着对话搜索技术（如ChatGPT）的普及，广告可能对用户的信息寻求行为产生负面影响，包括降低搜索质量、滥用用户数据或误导用户，尤其是敏感话题的指导。

Method: 通过推测性案例批判性地分析广告在对话搜索中的潜在风险，并提出广告可能的形式和“假朋友困境”的概念。

Result: 研究揭示了广告在对话搜索中可能对用户造成的危害，探讨了如何利用用户信任实现其他目标的机制。

Conclusion: 论文呼吁关注对话搜索中广告的潜在风险并提出行动建议，以避免未来可能出现的负面影响。

Abstract: Digital commerce thrives on advertising, with many of the largest technology
companies relying on it as a significant source of revenue. However, in the
context of information-seeking behavior, such as search, advertising may
degrade the user experience by lowering search quality, misusing user data for
inappropriate personalization, potentially misleading individuals, or even
leading them toward harm. These challenges remain significant as conversational
search technologies, such as ChatGPT, become widespread. This paper critically
examines the future of advertising in conversational search, utilizing several
speculative examples to illustrate the potential risks posed to users who seek
guidance on sensitive topics. Additionally, it provides an overview of the
forms that advertising might take in this space and introduces the "fake friend
dilemma," the idea that a conversational agent may exploit unaligned user trust
to achieve other objectives. This study presents a provocative discussion on
the future of online advertising in the space of conversational search and ends
with a call to action.

</details>


### [35] [RadioGami: Batteryless, Long-Range Wireless Paper Sensors Using Tunnel Diodes](https://arxiv.org/abs/2506.06473)
*Imran Fahad,Danny Scott,Azizul Zahid,Matthew Bringle,Srinayana Patil,Ella Bevins,Carmen Palileo,Sai Swaminathan*

Main category: cs.HC

TL;DR: RadioGami是一种低成本、DIY的方法，通过纸基材料实现长距离、无电池的射频传感表面，可感知纸张变形和环境变化。


<details>
  <summary>Details</summary>
Motivation: 传统纸基RF设备受限于短距离操作，需要一种可持续、可交互的解决方案。

Method: 使用铜带、纸张和现成电子器件，结合超低功耗开关电路和隧道二极管，实现远距离无线传感。

Result: 能够在45.73米范围内感知纸张变形（如弯曲、撕裂、折纸），并可持续运行。

Conclusion: RadioGami为可持续、无电池的交互界面提供了新方向。

Abstract: Paper-based interactive RF devices have opened new possibilities for wireless
sensing, yet they are typically constrained by short operational ranges. This
paper introduces RadioGami, a method for creating long-range, batteryless RF
sensing surfaces on paper using low-cost, DIY materials like copper tape,
paper, and off-the-shelf electronics paired with an affordable radio receiver
(approx. $20). We explore the design space enabled by RadioGami, including
sensing paper deformations like bending, tearing, and origami patterns (Miura,
Kresling) at ranges up to 45.73 meters. RadioGami employs a novel ultra-low
power (35uW) switching circuit with a tunnel diode for wireless functionality.
These surfaces can sustainably operate by harvesting energy using tiny
photodiodes. We demonstrate applications that monitor object status, track user
interactions (rotation, sliding), and detect environmental changes. We
characterize performance, sensitivity, range, and power consumption with
deployment studies. RadioGami advances sustainable, tangible, and batteryless
interfaces for embodied interaction.

</details>


### [36] [Mind Games! Exploring the Impact of Dark Patterns in Mixed Reality Scenarios](https://arxiv.org/abs/2506.06774)
*Luca-Maxim Meinhardt,Simon Demharter,Michael Rietzler,Mark Colley,Thomas Eßmeyer,Enrico Rukzio*

Main category: cs.HC

TL;DR: 研究探讨了混合现实（MR）中的四种黑暗模式对用户体验的负面影响，结果显示这些模式显著降低用户舒适度、增加抗拒心理，并减少使用MR眼镜的意愿。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索黑暗模式在MR中的应用及其对用户的影响，以推动伦理设计指南和技术的发展。

Method: 通过74名参与者的双因素受试内研究，分析了13个模拟MR城市漫步体验的视频。

Result: 所有黑暗模式均显著降低用户舒适度、增加抗拒心理，个人或金钱操纵效果最显著。情感与感官操纵及隐藏信息在MR中的影响相似。

Conclusion: 研究强调了制定伦理设计指南和工具的重要性，以预防黑暗模式在沉浸式技术中的滥用。

Abstract: Mixed Reality (MR) integrates virtual objects with the real world, offering
potential but raising concerns about misuse through dark patterns. This study
explored the effects of four dark patterns, adapted from prior research, and
applied to MR across three targets: places, products, and people. In a
two-factorial within-subject study with 74 participants, we analyzed 13 videos
simulating MR experiences during a city walk. Results show that all dark
patterns significantly reduced user comfort, increased reactance, and decreased
the intention to use MR glasses, with the most disruptive effects linked to
personal or monetary manipulation. Additionally, the dark patterns of Emotional
and Sensory Manipulation and Hiding Information produced similar impacts on the
user in MR, suggesting a re-evaluation of current classifications to go beyond
deceptive design techniques. Our findings highlight the importance of
developing ethical design guidelines and tools to detect and prevent dark
patterns as immersive technologies continue to evolve.

</details>


### [37] [Identity Deepfake Threats to Biometric Authentication Systems: Public and Expert Perspectives](https://arxiv.org/abs/2506.06825)
*Shijing He,Yaxiong Lei,Zihan Zhang,Yuzhou Sun,Shujun Li,Chi Zhang,Juan Ye*

Main category: cs.HC

TL;DR: 本文研究了生成式AI深度伪造技术对生物特征认证的威胁，揭示了公众与专家之间的认知差距，并提出了一个基于实证的三层缓解框架。


<details>
  <summary>Details</summary>
Motivation: 生物特征认证系统面临生成式AI深度伪造技术的快速演变威胁，而公众与专家在此风险上的认知差距导致了系统的关键漏洞。

Method: 研究采用混合方法，包括对408名专业人士的调查和37名参与者的深度访谈，并引入了一个新的深度伪造攻击链模型。

Result: 研究发现公众依赖于生物特征认证的便利性，而专家则对静态模态如面部和语音识别的伪造表示严重担忧，并提出了一个三层缓解框架。

Conclusion: 本文通过结合技术保障和以人为本的见解，为防御AI生成的身份威胁提供了首个基于实证的路线图。

Abstract: Generative AI (Gen-AI) deepfakes pose a rapidly evolving threat to biometric
authentication, yet a significant gap exists between expert understanding of
these risks and public perception. This disconnection creates critical
vulnerabilities in systems trusted by millions. To bridge this gap, we
conducted a comprehensive mixed-method study, surveying 408 professionals
across key sectors and conducting in-depth interviews with 37 participants (25
experts, 12 general public [non-experts]). Our findings reveal a paradox: while
the public increasingly relies on biometrics for convenience, experts express
grave concerns about the spoofing of static modalities like face and voice
recognition. We found significant demographic and sector-specific divides in
awareness and trust, with finance professionals, for example, showing
heightened skepticism. To systematically analyze these threats, we introduce a
novel Deepfake Kill Chain model, adapted from Hutchins et al.'s cybersecurity
frameworks to map the specific attack vectors used by malicious actors against
biometric systems. Based on this model and our empirical findings, we propose a
tri-layer mitigation framework that prioritizes dynamic biometric signals
(e.g., eye movements), robust privacy-preserving data governance, and targeted
educational initiatives. This work provides the first empirically grounded
roadmap for defending against AI-generated identity threats by aligning
technical safeguards with human-centered insights.

</details>


### [38] [In-Sensor Motion Recognition with Memristive System and Light Sensing Surfaces](https://arxiv.org/abs/2506.06829)
*Hritom Das,Imran Fahad,SNB Tushar,Sk Hasibul Alam,Graham Buchanan,Danny Scott,Garrett S. Rose,Sai Swaminathan*

Main category: cs.HC

TL;DR: 本文提出了一种新型设备架构，通过结合忆阻器件与光敏表面，实现了边缘端的节能运动识别。光敏表面通过传感器内计算捕获运动数据，忆阻系统利用HfO2基突触器件和胜者全得（WTA）电路进行低功耗运动分类。实验表明，系统平均能耗极低，并具备高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种节能的边缘端运动识别系统，结合忆阻器件与光敏表面，以实现低能耗和高隐私保护，适合可持续自主运行的应用场景。

Method: 提出了一种基于光敏表面和忆阻系统的架构，集成了HfO2基突触器件和WTA电路，通过四类手势实验验证系统性能。

Result: 系统平均能耗为4.17 nJ（预处理）和0.952 nJ（测试），分类准确率达到97.22%，且对5%噪声干扰具有鲁棒性。

Conclusion: 该架构显著降低了能耗，适合结合能量收集技术（如太阳能）实现可持续运行，同时通过本地数据处理增强了隐私保护。

Abstract: In this paper, we introduce a novel device architecture that merges
memristive devices with light-sensing surfaces, for energy-efficient motion
recognition at the edge. Our light-sensing surface captures motion data through
in-sensor computation. This data is then processed using a memristive system
equipped with a HfO2-based synaptic device, coupled with a winner-take-all
(WTA) circuit, tailored for low-power motion classification tasks. We validate
our end-to-end system using four distinct human hand gestures - left-to-right,
right-to-left, bottom-to-top, and top-to-bottom movements - to assess energy
efficiency and classification robustness. Our experiments show that the system
requires an average of only 4.17 nJ for taking our processed analog signal and
mapping weights onto our memristive system and 0.952 nJ for testing per
movement class, achieving 97.22% accuracy even under 5% noise interference. A
key advantage of our proposed architecture is its low energy requirement,
enabling the integration of energy-harvesting solutions such as solar power for
sustainable autonomous operation. Additionally, our approach enhances data
privacy by processing data locally, reducing the need for external data
transmission and storage.

</details>


### [39] [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)
*Ala Yankouskaya,Areej B. Babiker,Syeda W. F. Rizvi,Sameha Alshakhsi,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: 研究者开发了一种新的12项问卷LLM-D12，用于测量人们对大型语言模型（LLM）的依赖性，包括工具性依赖和关系性依赖两个维度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估人们对LLM依赖性的工具，现有方法主要基于行为成瘾症状，而LLM与人类的关系更为复杂，需要新视角。

Method: 通过理论工作开发了12项的LLM-D12问卷，收集了526名英国参与者的数据，使用分样本方法进行了探索性和验证性因子分析。

Result: 问卷支持双因子结构：工具性依赖和关系性依赖，显示出良好的内部一致性和区分效度，验证了其概念基础。

Conclusion: LLM-D12问卷为评估LLM依赖性提供了新工具，表明依赖不一定是功能失调，但在某些情境下可能成为问题。

Abstract: There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.

</details>


### [40] [From Inquisitorial to Adversarial: Using Legal Theory to Redesign Online Reporting Systems](https://arxiv.org/abs/2506.07041)
*Leijie Wang,Weizi Wu,Lirong Que,Nirvan Tyagi,Amy X. Zhang*

Main category: cs.HC

TL;DR: 论文探讨了在线用户举报系统的改进，提出了对抗性模型的优势，以提升用户控制和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前在线举报系统采用讯问制，用户缺乏参与感。对抗性模型可能更好地实现程序正义和隐私保护。

Method: 通过文献综述、访谈和威胁建模，探索对抗性实践在举报系统中的应用，并设计减少信息共享的方案。

Result: 研究发现对抗性模型可赋予用户更多控制权，但需平衡滥用风险。设计了支持证据认证的最小化信息共享方案。

Conclusion: 研究为在线举报系统的改进提供了新思路，尤其是结合法律框架和密码学工具的应用。

Abstract: User reporting systems are central to addressing interpersonal conflicts and
protecting users from harm in online spaces, particularly those with heightened
privacy expectations. However, users often express frustration at their lack of
insight and input into the reporting process. Drawing on offline legal
literature, we trace these frustrations to the inquisitorial nature of today's
online reporting systems, where moderators lead evidence gathering and case
development. In contrast, adversarial models can grant users greater control
and thus are better for procedural justice and privacy protection, despite
their increased risks of system abuse. This motivates us to explore the
potential of incorporating adversarial practices into online reporting systems.
Through literature review, formative interviews, and threat modeling, we find a
rich design space for empowering users to collect and present their evidence
while mitigating potential abuse in the reporting process. In particular, we
propose designs that minimize the amount of information shared for reporting
purposes, as well as supporting evidence authentication. Finally, we discuss
how our findings can inform new cryptographic tools and new efforts to apply
comparative legal frameworks to online moderation.

</details>


### [41] [earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor](https://arxiv.org/abs/2506.07193)
*Tobias King,Michael Knierim,Philipp Lepold,Christopher Clarke,Hans Gellersen,Michael Beigl,Tobias Röddiger*

Main category: cs.HC

TL;DR: 研究人员开发了一种基于耳机的眼动追踪方法（earEOG），利用电极测量眼动，实验显示水平方向效果良好，但垂直方向表现不佳。


<details>
  <summary>Details</summary>
Motivation: 传统眼动追踪技术依赖笨重硬件且计算资源需求高，希望通过耳机集成的电眼动图（EOG）技术解决这些问题。

Method: 使用14个电极分布在耳机上的设备，测量16名参与者的眼动数据，分析与标准方法的关联性。

Result: 水平方向眼动（平滑追踪和扫视）与标准方法高度相关，垂直方向相关性较弱。

Conclusion: 耳EOG在水平方向具有潜力，但垂直方向需进一步改进。

Abstract: Eye tracking technology is frequently utilized to diagnose eye and
neurological disorders, assess sleep and fatigue, study human visual
perception, and enable novel gaze-based interaction methods. However,
traditional eye tracking methodologies are constrained by bespoke hardware that
is often cumbersome to wear, complex to apply, and demands substantial
computational resources. To overcome these limitations, we investigated
Electrooculography (EOG) eye tracking using 14 electrodes positioned around the
ears, integrated into a custom-built headphone form factor device. In a
controlled experiment, 16 participants tracked stimuli designed to induce
smooth pursuits and saccades. Data analysis identified optimal electrode pairs
for vertical and horizontal eye movement tracking, benchmarked against
gold-standard EOG and camera-based methods. The electrode montage nearest the
eyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG
showed high correlation with gold-standard measures ($r_{\mathrm{EOG}} = 0.81,
p = 0.01$; $r_{\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were
weakly correlated ($r_{\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\mathrm{CAM}} =
0.35, p = 0.05$). Voltage deflections when performing saccades showed strong
correlation in the horizontal direction ($r_{\mathrm{left}} = 0.99, p = 0.0$;
$r_{\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical
direction ($r_{\mathrm{up}} = 0.6, p = 0.23$; $r_{\mathrm{down}} = 0.19, p =
0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating
its potential effectiveness, while vertical earEOG results were poor,
suggesting limited feasibility in our current setup.

</details>


### [42] [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)
*Gionnieve Lim,Bryan Chen Zhengyu Tan,Kellie Yu Hui Sim,Weiyan Shi,Ming Hui Chew,Ming Shan Hee,Roy Ka-Wei Lee,Simon T. Perrault,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLM）在虚假信息传播中的双重作用，通过模拟在线论坛的游戏实验分析其潜在危害与积极用途，并提出均衡发展的建议。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在虚假信息传播中的双重作用，既可能被滥用生成虚假信息，也可用于检测和应对虚假信息。

Method: 采用受狼人杀启发的通讯游戏，模拟在线论坛，分析25名参与者在不同角色（散播者、监管者、用户）中利用LLM的策略。

Result: 发现LLM的使用因角色和策略而异，揭示了其潜在的滥用和对抗虚假信息的双重效果。

Conclusion: 建议未来LLM开发和平台设计需平衡，既能提升用户能力、增强信任，又能降低虚假信息风险。

Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in
the fight against disinformation. These powerful tools, capable of generating
human-like text at scale, can be weaponised to produce sophisticated and
persuasive disinformation, yet they also hold promise for enhancing detection
and mitigation strategies. This paper investigates the complex dynamics between
LLMs and disinformation through a communication game that simulates online
forums, inspired by the game Werewolf, with 25 participants. We analyse how
Disinformers, Moderators, and Users leverage LLMs to advance their goals,
revealing both the potential for misuse and combating disinformation. Our
findings highlight the varying uses of LLMs depending on the participants'
roles and strategies, underscoring the importance of understanding their
effectiveness in this context. We conclude by discussing implications for
future LLM development and online platform design, advocating for a balanced
approach that empowers users and fosters trust while mitigating the risks of
LLM-assisted disinformation.

</details>


### [43] [IDEIA: A Generative AI-Based System for Real-Time Editorial Ideation in Digital Journalism](https://arxiv.org/abs/2506.07278)
*Victor B. Santos,Cauã O. Jordão,Leonardo J. O. Ibiapina,Gabriel M. Silva,Mirella E. B. Santana,Matheus A. Garrido,Lucas R. C. Farias*

Main category: cs.HC

TL;DR: IDEIA是一个生成式AI系统，通过实时趋势分析和内容建议优化新闻编辑流程，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决新闻编辑过程中时间和认知负担高的问题，IDEIA旨在通过AI智能辅助提升内容策划效率。

Method: IDEIA整合了Google Trends和Gemini API，采用Node.js、React和PostgreSQL的模块化架构，并利用Docker和CI/CD进行部署。

Result: 实验结果显示，编辑策划阶段的时间减少高达70%，显著提升效率。

Conclusion: IDEIA展示了智能自动化在新闻业中的潜力，同时讨论了生成模型在新闻工作流中的技术及伦理问题。

Abstract: This paper presents IDEIA (Intelligent Engine for Editorial Ideation and
Assistance), a generative AI-powered system designed to optimize the
journalistic ideation process by combining real-time trend analysis with
automated content suggestion. Developed in collaboration with the Sistema
Jornal do Commercio de Comunica\c{c}\~ao (SJCC), the largest media conglomerate
in Brazil's North and Northeast regions, IDEIA integrates the Google Trends API
for data-driven topic monitoring and the Google Gemini API for the generation
of context-aware headlines and summaries. The system adopts a modular
architecture based on Node.js, React, and PostgreSQL, supported by Docker
containerization and a CI/CD pipeline using GitHub Actions and Vercel.
Empirical results demonstrate a significant reduction in the time and cognitive
effort required for editorial planning, with reported gains of up to 70\% in
the content ideation stage. This work contributes to the field of computational
journalism by showcasing how intelligent automation can enhance productivity
while maintaining editorial quality. It also discusses the technical and
ethical implications of incorporating generative models into newsroom
workflows, highlighting scalability and future applicability across sectors
beyond journalism.

</details>


### [44] [Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency](https://arxiv.org/abs/2506.07281)
*Leah Hope Ajmani,Nuredin Ali Abdelkadir,Stevie Chancellor*

Main category: cs.HC

TL;DR: 研究探讨如何将参与式AI理念扩展至次要利益相关者，通过半结构化访谈提出三个参与理想：知情、同意和能动性。


<details>
  <summary>Details</summary>
Motivation: 当前参与式AI主要关注主要利益相关者（如终端用户），忽视了次要利益相关者，研究旨在填补这一空白。

Method: 通过半结构化访谈，理论化次要利益相关者的参与理想，并提出三种利益相关者原型。

Result: 提出次要利益相关者实现参与理想的路径，强调知情、同意和能动性的递进关系。

Conclusion: 研究呼吁未来AI发展中，次要利益相关者能有效参与并影响AI系统。

Abstract: As AI technologies become more human-facing, there have been numerous calls
to adapt participatory approaches to AI development -- spurring the idea of
participatory AI. However, these calls often focus only on primary
stakeholders, such as end-users, and not secondary stakeholders. This paper
seeks to translate the ideals of participatory AI to a broader population of
secondary AI stakeholders through semi-structured interviews. We theorize that
meaningful participation involves three participatory ideals: (1) informedness,
(2) consent, and (3) agency. We also explore how secondary stakeholders realize
these ideals by traversing a complicated problem space. Like walking up the
rungs of a ladder, these ideals build on one another. We introduce three
stakeholder archetypes: the reluctant data contributor, the unsupported
activist, and the well-intentioned practitioner, who must navigate systemic
barriers to achieving agentic AI relationships. We envision an AI future where
secondary stakeholders are able to meaningfully participate with the AI systems
they influence and are influenced by.

</details>


### [45] [Human Side of Smart Contract Fuzzing: An Empirical Study](https://arxiv.org/abs/2506.07389)
*Guanming Qiao,Partha Protim Paul*

Main category: cs.HC

TL;DR: 分析智能合约模糊测试工具在实践中面临的挑战，通过GitHub问题和用户研究分类问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 智能合约（SC）模糊测试是检测区块链应用中漏洞的关键技术，但其采用仍面临挑战，因为SC与传统软件系统存在根本性差异。

Method: 通过分析Echidna和Foundry两个SC模糊测试工具的381个GitHub问题，并进行用户研究，分类挑战并识别解决策略。

Result: 发现领域特定的易用性和实用性挑战，包括区块链模拟的技术问题和文档不足及自动化缺失的人为问题。

Conclusion: 研究结果为工具开发者和研究人员提供了可行的改进方向，指导未来SC模糊测试工具的设计优化。

Abstract: Smart contract (SC) fuzzing is a critical technique for detecting
vulnerabilities in blockchain applications. However, its adoption remains
challenging for practitioners due to fundamental differences between SCs and
traditional software systems. In this study, we investigate the challenges
practitioners face when adopting SC fuzzing tools by conducting an inductive
content analysis of 381 GitHub issues from two widely used SC fuzzers: Echidna
and Foundry. Furthermore, we conducted a user study to examine how these
challenges affect different practitioner groups, SC developers, and traditional
software security professionals, and identify strategies practitioners use to
overcome them. We systematically categorize these challenges into a taxonomy
based on their nature and occurrence within the SC fuzzing workflow. Our
findings reveal domain-specific ease-of-use and usefulness challenges,
including technical issues with blockchain emulation, and human issues with a
lack of accessible documentation and process automation. Our results provide
actionable insights for tool developers and researchers, guiding future
improvements in SC fuzzer tool design.

</details>


### [46] [Happiness Finder: Exploring the Role of AI in Enhancing Well-Being During Four-Leaf Clover Searches](https://arxiv.org/abs/2506.07393)
*Anna Yokokubo,Takeo Hamada,Tatsuya Ishizuka,Hiroaki Mori,Noboru Koshizuka*

Main category: cs.HC

TL;DR: 研究探讨了使用AI辅助寻找四叶草的体验，开发了名为'HappinessFinder'的App，并在工作坊中展示了其效果。


<details>
  <summary>Details</summary>
Motivation: 四叶草象征幸运，但传统搜索的成就感可能被AI替代，研究旨在探索AI辅助搜索时的用户感受。

Method: 开发'HappinessFinder'系统，利用智能手机的物体检测算法辅助搜索，并在工作坊中通过人工盆栽展示效果。

Result: 研究展示了AI辅助四叶草搜索的实际应用和用户体验。

Conclusion: AI辅助工具虽能提升效率，但需平衡技术便利与传统体验的成就感。

Abstract: A four-leaf clover (FLC) symbolizes luck and happiness worldwide, but it is
hard to distinguish it from the common three-leaf clover. While AI technology
can assist in searching for FLC, it may not replicate the traditional search's
sense of achievement. This study explores searcher feelings when AI aids the
FLC search. In this study, we developed a system called ``Happiness Finder''
that uses object detection algorithms on smartphones or tablets to support the
search. We exhibited HappinessFinder at an international workshop, allowing
participants to experience four-leaf clover searching using potted artificial
clovers and the HappinessFinder app. This paper reports the findings from this
demonstration.

</details>


### [47] [Interaction Analysis by Humans and AI: A Comparative Perspective](https://arxiv.org/abs/2506.07707)
*Maryam Teimouri,Filip Ginter,Tomi "bgt" Suovuo*

Main category: cs.HC

TL;DR: 研究探讨MR和Zoom对儿童手势猜谜游戏中沟通的影响，发现MR促进更丰富的互动，而Zoom更简便。


<details>
  <summary>Details</summary>
Motivation: 探索MR和2D视频会议技术在儿童协作任务中的沟通效果差异，以优化分布式学习体验。

Method: 使用Microsoft HoloLens MR和Zoom进行实验，通过LLM分析音频视频记录。

Result: MR表现出更高的情感表达和参与度，而Zoom更简单易用。

Conclusion: MR有潜力提升分布式环境中儿童的协作学习体验，但需要结合技术的简便性。

Abstract: This paper explores how Mixed Reality (MR) and 2D video conferencing
influence children's communication during a gesture-based guessing game.
Finnish-speaking participants engaged in a short collaborative task using two
different setups: Microsoft HoloLens MR and Zoom. Audio-video recordings were
transcribed and analyzed using Large Language Models (LLMs), enabling iterative
correction, translation, and annotation. Despite limitations in annotations'
accuracy and agreement, automated approaches significantly reduced processing
time and allowed non-Finnish-speaking researchers to participate in data
analysis. Evaluations highlight both the efficiency and constraints of
LLM-based analyses for capturing children's interactions across these
platforms. Initial findings indicate that MR fosters richer interaction,
evidenced by higher emotional expression during annotation, and heightened
engagement, while Zoom offers simplicity and accessibility. This study
underscores the potential of MR to enhance collaborative learning experiences
for children in distributed settings.

</details>


### [48] [Supporting Aging Well through Accessible Digital Games: The Supplemental Role of AI in Game Design for Older Adults](https://arxiv.org/abs/2506.07777)
*Brandon Lyman,Yichi Zhang,Celia Pearce,Miso Kim,Casper Harteveld,Leanne Chukoskie,Bob De Schutter*

Main category: cs.HC

TL;DR: 论文探讨如何利用人工智能为老年游戏玩家设计个性化辅助功能，以应对其日益多样化的需求。


<details>
  <summary>Details</summary>
Motivation: 随着老年游戏玩家群体的多样化，传统游戏辅助功能（如简化输入、冗余信息等）已无法满足个体需求，亟需更灵活的解决方案。

Method: 引入人工智能技术，结合老年学、人机交互和残疾研究的视角，设计基于玩家个体的适应性辅助功能。

Result: 提出了人工智能驱动的个性化辅助功能框架，为老年玩家提供更精准的支持。

Conclusion: 个性化辅助功能对提升老年玩家的游戏体验至关重要，有助于促进其身心健康和社交联系。

Abstract: As the population continues to age, and gaming continues to grow as a hobby
for older people, heterogeneity among older adult gamers is increasing. We
argue that traditional game-based accessibility features, such as simplified
input schemes, redundant information channels, and increased legibility of
digital user interfaces, are increasingly limited in the face of this
heterogeneity. This is because such features affect all older adult players
simultaneously and therefore are designed generically. We introduce artificial
intelligence, although it has its own limitations and ethical concerns, as a
method of creating player-based accessibility features, given the adaptive
nature of the emerging technology. These accessibility features may help to
address unique assemblage of accessibility needs an individual may accumulate
through age. We adopt insights from gerontology, HCI, and disability studies
into the digital game design discourse for older adults, and we contribute
insight that can guide the integration of player-based accessibility features
to supplement game-based counterparts. The accessibility of digital games for
heterogenous older adult audience is paramount, as the medium offers short-term
social, emotional, psychological, cognitive, and physical that support the
long-term goal of aging well.

</details>


### [49] [Integrating Artificial Intelligence as Assistive Technology for Older Adult Gamers: A Pilot Study](https://arxiv.org/abs/2506.07830)
*Yichi Zhang,Brandon Lyman,Celia Pearce,Miso Kim,Casper Harteveld,Leanne Chukoskie,Bob De Schutter*

Main category: cs.HC

TL;DR: 论文探讨了AI如何改善老年玩家的游戏体验，通过迭代开发调查问卷，了解他们的游戏偏好、挑战及对AI的看法，初步发现可用性问题和AI的双面性是关键。


<details>
  <summary>Details</summary>
Motivation: 研究老年玩家在数字游戏中的需求，探索AI如何辅助改进他们的游戏体验。

Method: 迭代设计并优化调查问卷，收集39名老年玩家的数据和反馈。

Result: 发现游戏可用性问题和AI的实际益处与自主权担忧是主要障碍，为未来研究提供了设计方向。

Conclusion: 研究为年龄包容性AI游戏设计提供了初步见解，并优化了调查工具用于更大规模研究。

Abstract: With respect to digital games, older adults are a demographic that is often
underserved due to an industry-wide focus on younger audiences' preferences and
skill sets. Meanwhile, as artificial intelligence (AI) continues to expand into
everyday technologies, its assistive capabilities have been recognized,
suggesting its potential in improving the gaming experience for older gamers.
To study this potential, we iteratively developed a pilot survey aimed at
understanding older adult gamers' current gameplay preference, challenges they
are facing, and their perspectives of AI usage in gaming. This article
contributes an overview of our iterative survey-design workflow, and pilot
results from 39 participants. During each iteration, we analyzed the survey's
efficacy and adjusted the content, language, and format to better capture
meaningful data, and was able to create a refined survey for a larger, more
representative future parent study. At the same time, preliminary findings
suggest that for older adult gamers, usability issues in gaming remain key
obstacles, while this demographic's perceptions of AI are shaped by both its
practical benefits and concerns about autonomy and complexity. These findings
also offer early insights for the design of age-inclusive, AI-supported gaming
experiences.

</details>


### [50] [Predicting Situation Awareness from Physiological Signals](https://arxiv.org/abs/2506.07930)
*Kieran J. Smith,Tristan C. Endsley,Torin K. Clark*

Main category: cs.HC

TL;DR: 通过多模态生理信号预测情境意识（SA）的三个层次，展示其在复杂任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的情境意识测量方法具有破坏性，因此需要开发实时、非干扰性的生理指标来预测SA。

Method: 使用多模态神经生理、心理生理和行为信号，结合飞机模拟任务和冻结探测评估，预测SA的连续性和全面性。

Result: 多模态生理模型能有效预测SA，特别是第三层次（预测），而第二层次（理解）最难预测。EEG和眼动信号对预测贡献显著。

Conclusion: 多模态生理信号在非干扰性预测SA方面具有实用价值，能提供全面的、客观的测量。

Abstract: Situation awareness (SA)--comprising the ability to 1) perceive critical
elements in the environment, 2) comprehend their meanings, and 3) project their
future states--is critical for human operator performance. Due to the
disruptive nature of gold-standard SA measures, researchers have sought
physiological indicators to provide real-time information about SA. We extend
prior work by using a multimodal suite of neurophysiological,
psychophysiological, and behavioral signals, predicting all three levels of SA
along a continuum, and predicting a comprehensive measure of SA in a complex
multi-tasking simulation. We present a lab study in which 31 participants
controlled an aircraft simulator task battery while wearing physiological
sensors and responding to SA 'freeze-probe' assessments. We demonstrate the
validity of task and assessment for measuring SA. Multimodal physiological
models predict SA with greater predictive performance ($Q^2$ for levels 1-3 and
total, respectively: 0.14, 0.00, 0.26, and 0.36) than models built with
shuffled labels, demonstrating that multimodal physiological signals provide
useful information in predicting all SA levels. Level 3 SA (projection) was
best predicted, and level 2 SA comprehension) was the most challenging to
predict. Ablation analysis and single sensor models found EEG and eye-tracking
signals to be particularly useful to predictions of level 3 and total SA. A
reduced sensor fusion model showed that predictive performance can be
maintained with a subset of sensors. This first rigorous cross-validation
assessment of predictive performance demonstrates the utility of multimodal
physiological signals for inferring complex, holistic, objective measures of SA
at all levels, non-disruptively, and along a continuum.

</details>


### [51] [Implementation Considerations for Automated AI Grading of Student Work](https://arxiv.org/abs/2506.07955)
*Zewei,Tian,Alex Liu,Lief Esbenshade,Shawon Sarkar,Zachary Zhang,Kevin He,Min Sun*

Main category: cs.HC

TL;DR: 研究探讨了AI评分平台在K-12课堂中的应用，发现教师认可能快速反馈但质疑自动评分，学生喜欢快速反馈但怀疑AI评分可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索AI评分平台在教育中的实际应用效果及其对教师与学生的影响。

Method: 通过平台日志、问卷调查和定性访谈，结合19名教师的协同设计实验。

Result: 教师重视AI的快速反馈但质疑其评分准确性，学生喜欢快速反馈但对AI评分持怀疑态度。

Conclusion: 设计可信赖且以教师为中心的AI评估工具，需增强反馈效果并保留教师的教学主导权。

Abstract: This study explores the classroom implementation of an AI-powered grading
platform in K-12 settings through a co-design pilot with 19 teachers. We
combine platform usage logs, surveys, and qualitative interviews to examine how
teachers use AI-generated rubrics and grading feedback. Findings reveal that
while teachers valued the AI's rapid narrative feedback for formative purposes,
they distrusted automated scoring and emphasized the need for human oversight.
Students welcomed fast, revision-oriented feedback but remained skeptical of
AI-only grading. We discuss implications for the design of trustworthy,
teacher-centered AI assessment tools that enhance feedback while preserving
pedagogical agency.

</details>


### [52] [Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System](https://arxiv.org/abs/2506.07997)
*Fan Yang,Yuan Tian,Jiansong Zhang*

Main category: cs.HC

TL;DR: 研究开发了一个多Agent对话系统，通过AI驱动的方法解决建筑行业中心理健康支持不足的问题，显著提高了工人的心理需求和社交互动。


<details>
  <summary>Details</summary>
Motivation: 建筑行业心理风险高但支持不足，AI尤其是大语言模型潜力未被充分挖掘，亟需行业专用解决方案。

Method: 开发了一个结合领域知识的多Agent对话系统，每个Agent具有独特角色，提供问题和社交支持。

Result: 用户研究表明，系统在可用性、自我决定、社交存在和信任方面分别提升了18%、40%、60%和60%，优于单Agent基线。

Conclusion: 研究表明，LLM驱动的AI系统在建筑行业中具有提供领域专用支持的巨大潜力。

Abstract: The construction industry is characterized by both high physical and
psychological risks, yet supports of mental health remain limited. While
advancements in artificial intelligence (AI), particularly large language
models (LLMs), offer promising solutions, their potential in construction
remains largely underexplored. To bridge this gap, we developed a
conversational multi-agent system that addresses industry-specific challenges
through an AI-driven approach integrated with domain knowledge. In parallel, it
fulfills construction workers' basic psychological needs by enabling
interactions with multiple agents, each has a distinct persona. This approach
ensures that workers receive both practical problem-solving support and social
engagement, ultimately contributing to their overall well-being. We evaluate
its usability and effectiveness through a within-subjects user study with 12
participants. The results show that our system significantly outperforms the
single-agent baseline, achieving improvements of 18% in usability, 40% in
self-determination, 60% in social presence, and 60% in trust. These findings
highlight the promise of LLM-driven AI systems in providing domain-specific
support for construction workers.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [53] [Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation](https://arxiv.org/abs/2506.06440)
*Chuhao Chen,Zhiyang Dou,Chen Wang,Yiming Huang,Anjun Chen,Qiao Feng,Jiatao Gu,Lingjie Liu*

Main category: cs.GR

TL;DR: Vid2Sim提出了一种基于视频的通用方法，通过无网格简化模拟高效重建几何和物理属性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂的优化流程和大量超参数调整，限制了实用性和泛化性。

Method: Vid2Sim结合前馈神经网络捕捉物理世界知识，并通过轻量级优化流程进行快速精确重建。

Result: 实验显示Vid2Sim在重建精度和效率上优于现有方法。

Conclusion: Vid2Sim提供了一种高效、通用的视频到几何和物理属性的解决方案。

Abstract: Faithfully reconstructing textured shapes and physical properties from videos
presents an intriguing yet challenging problem. Significant efforts have been
dedicated to advancing such a system identification problem in this area.
Previous methods often rely on heavy optimization pipelines with a
differentiable simulator and renderer to estimate physical parameters. However,
these approaches frequently necessitate extensive hyperparameter tuning for
each scene and involve a costly optimization process, which limits both their
practicality and generalizability. In this work, we propose a novel framework,
Vid2Sim, a generalizable video-based approach for recovering geometry and
physical properties through a mesh-free reduced simulation based on Linear
Blend Skinning (LBS), offering high computational efficiency and versatile
representation capability. Specifically, Vid2Sim first reconstructs the
observed configuration of the physical system from video using a feed-forward
neural network trained to capture physical world knowledge. A lightweight
optimization pipeline then refines the estimated appearance, geometry, and
physical properties to closely align with video observations within just a few
minutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,
mesh-free simulation with high efficiency. Extensive experiments demonstrate
that our method achieves superior accuracy and efficiency in reconstructing
geometry and physical properties from video data.

</details>


### [54] [Splat and Replace: 3D Reconstruction with Repetitive Elements](https://arxiv.org/abs/2506.06462)
*Nicolás Violante,Andreas Meuleman,Alban Gauthier,Frédo Durand,Thibault Groueix,George Drettakis*

Main category: cs.GR

TL;DR: 利用3D场景中的重复元素提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 当前NeRF和3DGS技术虽提升了新视角合成效果，但对未覆盖或遮挡区域的渲染质量仍不足。环境中常有重复元素，可用于改善这一问题。

Method: 提出一种方法，通过分割3DGS重建中的重复实例、对齐并共享信息，同时考虑实例间的外观变化。

Result: 在合成和真实场景中验证，显著提升了新视角合成的质量。

Conclusion: 利用重复元素能有效改进3D场景重建质量，尤其是在覆盖不足或存在遮挡的情况下。

Abstract: We leverage repetitive elements in 3D scenes to improve novel view synthesis.
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly
improved novel view synthesis but renderings of unseen and occluded parts
remain low-quality if the training views are not exhaustive enough. Our key
observation is that our environment is often full of repetitive elements. We
propose to leverage those repetitions to improve the reconstruction of
low-quality parts of the scene due to poor coverage and occlusions. We propose
a method that segments each repeated instance in a 3DGS reconstruction,
registers them together, and allows information to be shared among instances.
Our method improves the geometry while also accounting for appearance
variations across instances. We demonstrate our method on a variety of
synthetic and real scenes with typical repetitive elements, leading to a
substantial improvement in the quality of novel view synthesis.

</details>


### [55] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 提出两种辅助一致性损失（先验一致性正则化和主题一致性正则化）来改进Stable Diffusion的微调，解决身份捕捉不足和背景多样性下降的问题，提升图像质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法存在身份捕捉不可靠（欠拟合）和背景多样性减少（过拟合）的问题，影响生成的图像质量。

Method: 使用两种一致性损失：先验一致性正则化保持非主题图像的噪声预测与预训练模型一致，主题一致性正则化增强模型对噪声调制的潜在代码的鲁棒性。

Result: 实验结果表明，该方法在CLIP分数、背景多样性和视觉质量上优于DreamBooth，更好地保留了主题身份并提升了多样性。

Conclusion: 通过引入一致性损失，有效解决了欠拟合和过拟合问题，提升了Stable Diffusion模型在主题驱动图像生成中的表现。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [56] [JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics](https://arxiv.org/abs/2506.06494)
*Lei Lan,Zixuan Lu,Chun Yuan,Weiwei Xu,Hao Su,Huamin Wang,Chenfanfu Jiang,Yin Yang*

Main category: cs.GR

TL;DR: 提出了一种新型GPU算法，在保持与雅可比方法类似的并行性的同时，收敛速度接近牛顿法。方法通过解决“过冲”现象实现，结合Cubature采样和全坐标公式，显著提升了收敛性能。


<details>
  <summary>Details</summary>
Motivation: 并行模拟中，收敛性和并行性常被视为冲突目标。本文旨在通过改进算法，实现高并行性和快速收敛的双赢。

Method: 通过理论推导二阶最优解抑制过冲现象，结合Cubature采样和全坐标公式预计算，提升算法效率。

Result: 实验表明，算法收敛速度比当前最优GPU方法快50到100倍，适用于刚性和柔性材料的高质量模拟。

Conclusion: 该算法成功解决了收敛与并行性的权衡问题，为并行模拟提供了一种高效解决方案。

Abstract: In parallel simulation, convergence and parallelism are often seen as
inherently conflicting objectives. Improved parallelism typically entails
lighter local computation and weaker coupling, which unavoidably slow the
global convergence. This paper presents a novel GPU algorithm that achieves
convergence rates comparable to fullspace Newton's method while maintaining
good parallelizability just like the Jacobi method. Our approach is built on a
key insight into the phenomenon of overshoot. Overshoot occurs when a local
solver aggressively minimizes its local energy without accounting for the
global context, resulting in a local update that undermines global convergence.
To address this, we derive a theoretically second-order optimal solution to
mitigate overshoot. Furthermore, we adapt this solution into a pre-computable
form. Leveraging Cubature sampling, our runtime cost is only marginally higher
than the Jacobi method, yet our algorithm converges nearly quadratically as
Newton's method. We also introduce a novel full-coordinate formulation for more
efficient pre-computation. Our method integrates seamlessly with the
incremental potential contact method and achieves second-order convergence for
both stiff and soft materials. Experimental results demonstrate that our
approach delivers high-quality simulations and outperforms state-of-the-art GPU
methods with 50 to 100 times better convergence.

</details>


### [57] [CrossGen: Learning and Generating Cross Fields for Quad Meshing](https://arxiv.org/abs/2506.07020)
*Qiujie Dong,Jiepeng Wang,Rui Xu,Cheng Lin,Yuan Liu,Shiqing Xin,Zichun Zhong,Xin Li,Changhe Tu,Taku Komura,Leif Kobbelt,Scott Schaefer,Wenping Wang*

Main category: cs.GR

TL;DR: CrossGen是一个新颖的框架，通过联合潜在空间统一几何和交叉场表示，实现了四边网格生成中交叉场的快速预测和潜在生成建模。


<details>
  <summary>Details</summary>
Motivation: 现有的交叉场生成方法在计算效率和生成质量上难以平衡，通常需要缓慢的逐形状优化。

Method: 使用自编码器网络架构，将输入点云表面编码为稀疏体素网格，解码为基于SDF的几何和交叉场，并结合扩散模型处理部分输入生成新形状。

Result: 实验表明，CrossGen能在不到一秒的时间内快速计算高质量交叉场，具有高几何保真度、噪声鲁棒性和快速推理能力。

Conclusion: CrossGen为四边网格生成提供了高效且高质量的交叉场生成解决方案，适用于广泛的表面形状。

Abstract: Cross fields play a critical role in various geometry processing tasks,
especially for quad mesh generation. Existing methods for cross field
generation often struggle to balance computational efficiency with generation
quality, using slow per-shape optimization. We introduce CrossGen, a novel
framework that supports both feed-forward prediction and latent generative
modeling of cross fields for quad meshing by unifying geometry and cross field
representations within a joint latent space. Our method enables extremely fast
computation of high-quality cross fields of general input shapes, typically
within one second without per-shape optimization. Our method assumes a
point-sampled surface, or called a point-cloud surface, as input, so we can
accommodate various different surface representations by a straightforward
point sampling process. Using an auto-encoder network architecture, we encode
input point-cloud surfaces into a sparse voxel grid with fine-grained latent
spaces, which are decoded into both SDF-based surface geometry and cross
fields. We also contribute a dataset of models with both high-quality signed
distance fields (SDFs) representations and their corresponding cross fields,
and use it to train our network. Once trained, the network is capable of
computing a cross field of an input surface in a feed-forward manner, ensuring
high geometric fidelity, noise resilience, and rapid inference. Furthermore,
leveraging the same unified latent representation, we incorporate a diffusion
model for computing cross fields of new shapes generated from partial input,
such as sketches. To demonstrate its practical applications, we validate
CrossGen on the quad mesh generation task for a large variety of surface
shapes. Experimental results...

</details>


### [58] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 本文提出了一种架构-算法协同设计方法，通过轴导向光栅化、神经排序和可重构处理阵列，显著提升了3D高斯渲染在资源受限设备上的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯渲染在视觉合成中表现出色，但在资源受限设备上实现实时渲染仍存在挑战，本文旨在解决这一问题。

Method: 采用轴导向光栅化减少计算冗余，引入神经排序替代硬件排序器，并通过可重构处理阵列统一支持光栅化和神经网络推理。

Result: 实验显示，该设计在保持渲染质量的同时，性能提升23.4~27.8倍，能耗降低28.8~51.4倍。

Conclusion: 提出的协同设计有效解决了3D高斯渲染在资源受限设备上的效率问题，并计划开源以推动进一步研究。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [59] [HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance](https://arxiv.org/abs/2506.07209)
*Lei Li,Angela Dai*

Main category: cs.GR

TL;DR: HOI-PAGE提出了一种零样本方法，通过部分级适应性推理从文本提示合成4D人-物交互（HOI），显著提升了真实性和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常关注全局人体-物体运动，而忽略部分级交互细节。为了生成更真实和多样化的HOI，需要更细粒度的理解。

Method: 引入部分适应性图（PAGs）表征HOI，并结合三阶段合成方法：分解对象、生成参考视频、优化4D运动序列以满足部分级接触约束。

Result: 实验表明，该方法能灵活生成复杂的多人或多物交互序列，在零样本4D HOI生成中表现出显著改进的真实性和文本对齐。

Conclusion: HOI-PAGE通过部分级推理和结构化表征，为4D HOI合成提供了一种有效且灵活的解决方案。

Abstract: We present HOI-PAGE, a new approach to synthesizing 4D human-object
interactions (HOIs) from text prompts in a zero-shot fashion, driven by
part-level affordance reasoning. In contrast to prior works that focus on
global, whole body-object motion for 4D HOI synthesis, we observe that
generating realistic and diverse HOIs requires a finer-grained understanding --
at the level of how human body parts engage with object parts. We thus
introduce Part Affordance Graphs (PAGs), a structured HOI representation
distilled from large language models (LLMs) that encodes fine-grained part
information along with contact relations. We then use these PAGs to guide a
three-stage synthesis: first, decomposing input 3D objects into geometric
parts; then, generating reference HOI videos from text prompts, from which we
extract part-based motion constraints; finally, optimizing for 4D HOI motion
sequences that not only mimic the reference dynamics but also satisfy
part-level contact constraints. Extensive experiments show that our approach is
flexible and capable of generating complex multi-object or multi-person
interaction sequences, with significantly improved realism and text alignment
for zero-shot 4D HOI generation.

</details>


### [60] [Immersive Visualization of Flat Surfaces Using Ray Marching](https://arxiv.org/abs/2506.07558)
*Fabian Lander,Diaaeldin Taha*

Main category: cs.GR

TL;DR: 本文提出了一种基于光线行进的高效平坦表面可视化方法，适用于翻译表面、镜面房间、展开多面体和翻译棱镜，并提供实现细节和示例。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种直观且高效的方法来探索不同类型的平坦表面，并为程序员提供实现参考。

Method: 采用光线行进技术进行平坦表面的可视化，适用于多种几何结构，包括翻译表面和展开多面体等。

Result: 通过多个示例展示了方法的实用性和计算效率，并提供了在线可用的代码和模拟。

Conclusion: 该方法不仅适用于学术和研究领域，还可用于科普推广，具有广泛的应用潜力。

Abstract: We present an effective method for visualizing flat surfaces using ray
marching. Our approach provides an intuitive way to explore translation
surfaces, mirror rooms, unfolded polyhedra, and translation prisms while
maintaining computational efficiency. We demonstrate the utility of the method
through various examples and provide implementation insights for programmers.
Finally, we discuss the use of our visualizations in outreach. We make our
simulations and code available online.

</details>


### [61] [PIG: Physically-based Multi-Material Interaction with 3D Gaussians](https://arxiv.org/abs/2506.07657)
*Zeyu Xiao,Zhenyi Wu,Mingyang Sun,Qipeng Yan,Yufan Guo,Zhuoer Liang,Lihua Zhang*

Main category: cs.GR

TL;DR: 论文提出了一种名为PIG的新方法，结合3D对象分割与高精度物体交互模拟，解决了3D高斯场景中的分割不准、变形不精确和渲染伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯场景中物体交互存在分割不准、变形不精确和渲染伪影问题，需要一种更精确的方法来实现多材料交互和物理模拟。

Method: 通过从2D像素到3D高斯的快速准确映射实现分割，为分割对象分配物理属性，并在变形梯度中嵌入约束尺度，消除伪影。

Result: 实验表明，该方法在视觉质量上优于现有技术，为物理真实场景生成提供了新的方向和流程。

Conclusion: PIG方法在视觉质量和物理真实感方面取得了显著进展，为3D场景生成开辟了新途径。

Abstract: 3D Gaussian Splatting has achieved remarkable success in reconstructing both
static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian
primitives, interactions between objects suffer from inaccurate 3D
segmentation, imprecise deformation among different materials, and severe
rendering artifacts. To address these challenges, we introduce PIG:
Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach
that combines 3D object segmentation with the simulation of interacting objects
in high precision. Firstly, our method facilitates fast and accurate mapping
from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.
Secondly, we assign unique physical properties to correspondingly segmented
objects within the scene for multi-material coupled interactions. Finally, we
have successfully embedded constraint scales into deformation gradients,
specifically clamping the scaling and rotation properties of the Gaussian
primitives to eliminate artifacts and achieve geometric fidelity and visual
consistency. Experimental results demonstrate that our method not only
outperforms the state-of-the-art (SOTA) in terms of visual quality, but also
opens up new directions and pipelines for the field of physically realistic
scene generation.

</details>


### [62] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出一种新方法，通过轻量级生成模型和Hessian辅助采样策略，提升3D高斯泼溅的分辨率和几何保真度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法受限于输入分辨率，无法生成比训练视图更精细的细节，因此需要突破这一限制。

Method: 使用轻量级生成模型预测和细化额外的3D高斯点，并结合Hessian辅助采样策略智能识别需要密集化的区域。

Result: 方法在计算效率上表现优异（单消费级GPU上每推理0.015秒），显著提升了几何精度和渲染质量。

Conclusion: 提出了一种无需分辨率限制的3D场景增强新范式，适用于实时交互应用。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [63] [Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes](https://arxiv.org/abs/2506.07917)
*Allen Tu,Haiyang Ying,Alex Hanson,Yonghan Lee,Tom Goldstein,Matthias Zwicker*

Main category: cs.GR

TL;DR: SpeeDe3DGS通过两种互补技术（时间敏感度剪枝和GroupFlow运动分析）加速动态3D高斯溅射渲染，提升速度10.37倍，模型缩小7.71倍，训练时间减少2.71倍。


<details>
  <summary>Details</summary>
Motivation: 动态场景的3D高斯溅射渲染因逐高斯神经推理导致渲染速度慢、内存和计算需求高，需要优化。

Method: 提出时间敏感度剪枝和GroupFlow运动分析技术，分别减少高斯数量和预测单一刚性变换。

Result: 在多个数据集上显著提升渲染速度、减小模型尺寸并缩短训练时间。

Conclusion: SpeeDe3DGS模块化设计可集成到任何动态3DGS框架中，高效解决渲染瓶颈。

Abstract: Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity pruning score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth pruning mechanism that
improves pruning robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.

</details>


### [64] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D利用预训练的3D生成模型的隐式先验知识，通过可训练的映射网络实现3D数据的高压缩比。


<details>
  <summary>Details</summary>
Motivation: 结合现有预训练模型的潜在空间，实现高效3D数据压缩。

Method: 通过预训练编码器压缩3D数据，再通过映射网络与生成模型结合，实现压缩与解压缩。

Result: 实现高压缩比（最高2187倍），且视觉质量与现有方法相当。

Conclusion: Squeeze3D提供了一种无需数据训练的高效3D压缩方案。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [Performance Impact of Containerized METADOCK 2 on Heterogeneous Platforms](https://arxiv.org/abs/2506.06450)
*Antonio Jesús Banegas-Luna,Baldomero Imbernón Tudela,Carlos Martínez-Cortés,José María Cecilia,Horacio Pérez-Sánchez*

Main category: cs.DC

TL;DR: 研究评估了容器化对虚拟筛选软件METADOCK 2在高性能计算平台性能的影响，结果显示性能开销可忽略不计（低于1%），且能高效处理大分子复合物。


<details>
  <summary>Details</summary>
Motivation: 探讨容器化技术在高通量虚拟筛选任务中的性能表现，以提高科学计算的可移植性、可重复性和扩展性。

Method: 测试三种容器化技术（Docker、Singularity、Apptainer）在不同CPU和GPU配置下的性能表现。

Result: 容器化性能开销极小，METADOCK 2表现优于商业工具AutoDock Vina，能高效处理大分子复合物。

Conclusion: 容器化的METADOCK 2是异构高性能计算平台上虚拟筛选任务的强大且高效的解决方案。

Abstract: Virtual screening (VS) is a computationally intensive process crucial for
drug discovery, often requiring significant resources to analyze large chemical
libraries and predict ligand-protein interactions. This study evaluates the
performance impact of containerization on METADOCK 2, a high-throughput docking
software when deployed on heterogeneous high-performance computing (HPC)
platforms. By testing three containerization technologies - Docker,
Singularity, and Apptainer - across varying CPU and GPU configurations, the
experiments reveal that containerization introduces negligible performance
overhead, with deviations below 1%. Moreover, METADOCK 2 demonstrated the
capability to efficiently process large molecular complexes, surpassing the
limitations of commercial tools such as AutoDock Vina. The results underscore
the advantages of container-based deployment for ensuring portability,
reproducibility, and scalability in scientific computing. This study concludes
that containerized METADOCK 2 is a robust and efficient solution for VS tasks
on heterogeneous HPC platforms.

</details>


### [66] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: TERAIO是一种基于SSD的GPU内存扩展框架，通过分析张量生命周期优化卸载/预取，显著提升大语言模型训练性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练时，大部分GPU内存被未活跃使用的张量占用，而活跃张量占比很小，这为卸载/预取提供了机会。

Method: TERAIO通过前几次迭代分析张量生命周期并生成卸载/预取计划，利用GPUDirect存储直接在GPU和SSD间迁移张量。

Result: TERAIO平均提升训练性能1.47倍，达到理想性能（无限GPU内存假设）的80.7%。

Conclusion: TERAIO有效利用SSD扩展GPU内存，优化大语言模型训练效率。

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [67] [Generating representative macrobenchmark microservice systems from distributed traces with Palette](https://arxiv.org/abs/2506.06448)
*Vaastav Anand,Matheus Stolet,Jonathan Mace,Antoine Kaufmann*

Main category: cs.DC

TL;DR: 提出了一种利用分布式跟踪数据集生成代表性微服务系统的方法，以解决研究中缺乏代表性系统的问题。


<details>
  <summary>Details</summary>
Motivation: 由于研究和实践中缺乏具有代表性的微服务系统（如规模、拓扑和执行模式匹配的系统），作者提出了基于分布式跟踪数据集的方法来生成代表性系统，以促进微服务的进一步发展。

Method: 使用图形因果模型（GCMs）抽象系统拓扑，包括分支概率、调用执行顺序和执行时间，并将其集成到Palette系统中，从分布式跟踪生成灵活的宏观基准微服务系统。

Result: Palette系统能够生成具有代表性的微服务系统，解决了研究中缺乏真实系统的问题。

Conclusion: 该方法通过利用分布式跟踪数据集，为研究和实践提供了一种生成代表性微服务系统的有效工具。

Abstract: Microservices are the dominant design for developing cloud systems
  today. Advancements for microservice need to be evaluated in representative
systems, e.g. with matching scale, topology, and execution patterns.
  Unfortunately in practice, researchers and practitioners alike often do not
have access to representative systems. Thus they have to resort to sub-optimal
non-representative alternatives, e.g. small and oversimplified synthetic
benchmark systems or simulated system models instead.
  To solve this issue, we propose the use of distributed trace datasets,
available from large internet companies,
  to generate representative microservice systems.
  To do so, we introduce a novel abstraction of a system topology which uses
Graphical Causal Models (GCMs)
  to model the underlying system by incorporating the branching probabilities,
execution order of outgoing
  calls to every dependency, and execution times.
  We then incorporate this topology in Palette, a system that generates
  representative flexible macrobenchmarks microservice systems from distributed
traces.

</details>


### [68] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: pFedSOP是一种利用二阶优化的个性化联邦学习方法，通过减少通信轮次和本地计算加速训练，并在异构数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统个性化联邦学习因一阶优化导致的训练速度慢和通信轮次多的问题，利用二阶优化提高效率。

Method: 使用Gompertz函数计算个性化梯度更新，并利用正则化Fisher信息矩阵近似Hessian矩阵进行模型更新。

Result: 在异构分布的图像分类数据集上，pFedSOP在性能和通信效率上优于现有FL和PFL算法。

Conclusion: pFedSOP通过有效利用二阶优化，显著提升了PFL的训练效率和模型性能。

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


### [69] [Addressing tokens dynamic generation, propagation, storage and renewal to secure the GlideinWMS pilot based jobs and system](https://arxiv.org/abs/2506.07379)
*Bruno Moreira Coimbra,Marco Mambelli*

Main category: cs.DC

TL;DR: GlideinWMS从X.509转向支持令牌的生产应用，遇到挑战并计划优化基础设施以满足新需求。


<details>
  <summary>Details</summary>
Motivation: GlideinWMS团队需要支持令牌的广泛应用，并解决基础设施中的不足和更严格的要求。

Method: 设计了新的凭证模块，支持动态生成、类型化管理和最小权限原则，并计划加入存储、更新和失效机制。

Result: 实现了凭证在多系统中的通用性，支持动态范围和时间的定制。

Conclusion: GlideinWMS通过改进凭证管理，更好地满足了实验需求并增强了安全性。

Abstract: GlideinWMS has been one of the first middleware in the WLCG community to
transition from X.509 to support also tokens. The first step was to get from
the prototype in 2019 to using tokens in production in 2022. This paper will
present the challenges introduced by the wider adoption of tokens and the
evolution plans for securing the pilot infrastructure of GlideinWMS and
supporting the new requirements. In the last couple of years, the GlideinWMS
team supported the migration of experiments and resources to tokens. Inadequate
support in the current infrastructure, more stringent requirements, and the
higher spatial and temporal granularity forced GlideinWMS to revisit once more
how credentials are generated, used, and propagated. The new credential modules
have been designed to be used in multiple systems (GlideinWMS, HEPCloud) and
use a model where credentials have type, purpose, and different flows.
Credentials are dynamically generated in order to customize the duration and
limit the scope to the targeted resource. This allows to enforce the least
privilege principle. Finally, we also considered adding credential storage,
renewal, and invalidation mechanisms within the GlideinWMS infrastructure to
better serve the experiments' needs.

</details>


### [70] [New Limits on Distributed Quantum Advantage: Dequantizing Linear Programs](https://arxiv.org/abs/2506.07574)
*Alkida Balliu,Corinna Coupette,Antonio Cruciani,Francesco d'Amore,Massimo Equi,Henrik Lievonen,Augusto Modanese,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 该论文证明在分布式量子计算中，量子方法并不比经典方法更有优势，尤其在局部模型中。同时，研究还发现某些本地可检查标记问题（LCL）中，量子局部模型的表现甚至弱于经典确定性SLOCAL模型。


<details>
  <summary>Details</summary>
Motivation: 探讨在LOCAL模型中，分布式量子计算是否比经典计算更具优势。

Method: 通过理论证明和构造性方法，展示了量子LOCAL算法在解决线性规划问题时的局限性，并对比了量子LOCAL、SLOCAL和非信号模型的表现。

Result: 发现量子LOCAL在解决线性规划问题时没有优势，且在特定LCL问题中表现不如SLOCAL模型。

Conclusion: 量子LOCAL模型在分布式计算中的优势有限，甚至在某些情况下不如经典模型。

Abstract: In this work, we give two results that put new limits on distributed quantum
advantage in the context of the LOCAL model of distributed computing. First, we
show that there is no distributed quantum advantage for any linear program. Put
otherwise, if there is a quantum-LOCAL algorithm $\mathcal{A}$ that finds an
$\alpha$-approximation of some linear optimization problem $\Pi$ in $T$
communication rounds, we can construct a classical, deterministic LOCAL
algorithm $\mathcal{A}'$ that finds an $\alpha$-approximation of $\Pi$ in $T$
rounds. As a corollary, all classical lower bounds for linear programs,
including the KMW bound, hold verbatim in quantum-LOCAL. Second, using the
above result, we show that there exists a locally checkable labeling problem
(LCL) for which quantum-LOCAL is strictly weaker than the classical
deterministic SLOCAL model. Our results extend from quantum-LOCAL also to
finitely dependent and non-signaling distributions, and one of the corollaries
of our work is that the non-signaling model and the SLOCAL model are
incomparable in the context of LCL problems: By prior work, there exists an LCL
problem for which SLOCAL is strictly weaker than the non-signaling model, and
our work provides a separation in the opposite direction.

</details>


### [71] [A Terminology for Scientific Workflow Systems](https://arxiv.org/abs/2506.07838)
*Frédéric Sutera,Tainã Coleman,İlkay Altintaş,Rosa M. Badia,Bartosz Balis,Kyle Chard,Iacopo Colonnelli,Ewa Deelman,Paolo Di Tommaso,Thomas Fahringer,Carole Goble,Shantenu Jha,Daniel S. Katz,Johannes Köster,Ulf Leser,Kshitij Mehta,Hilary Oliver,J. -Luc Peterson,Giovanni Pizzi,Loïc Pottier,Raül Sirvent,Eric Suchyta,Douglas Thain,Sean R. Wilkinson,Justin M. Wozniak,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 论文总结了科学工作流管理系统（WMSs）的分类和术语，提出了一种基于五个轴的新术语来描述WMS的特征。


<details>
  <summary>Details</summary>
Motivation: 由于科学工作流的多样性和复杂性，现有WMSs功能重叠但又各具特色，导致研究人员选择困难。因此，需要一种统一的术语和分类方法来帮助选择。

Method: 作者与WMS开发者及实践者合作，提出了一个包含五个轴（工作流特征、组成、编排、数据管理和元数据捕获）的社区术语。

Result: 基于新术语，对23个现有WMSs进行了分类，展示了它们的共性和特性。

Conclusion: 论文提供了一种标准化的术语和分类方法，帮助研究人员更有效地选择和评估WMSs。

Abstract: The term scientific workflow has evolved over the last two decades to
encompass a broad range of compositions of interdependent compute tasks and
data movements. It has also become an umbrella term for processing in modern
scientific applications. Today, many scientific applications can be considered
as workflows made of multiple dependent steps, and hundreds of workflow
management systems (WMSs) have been developed to manage and run these
workflows. However, no turnkey solution has emerged to address the diversity of
scientific processes and the infrastructure on which they are implemented.
Instead, new research problems requiring the execution of scientific workflows
with some novel feature often lead to the development of an entirely new WMS. A
direct consequence is that many existing WMSs share some salient features,
offer similar functionalities, and can manage the same categories of workflows
but also have some distinct capabilities. This situation makes researchers who
develop workflows face the complex question of selecting a WMS. This selection
can be driven by technical considerations, to find the system that is the most
appropriate for their application and for the resources available to them, or
other factors such as reputation, adoption, strong community support, or
long-term sustainability. To address this problem, a group of WMS developers
and practitioners joined their efforts to produce a community-based terminology
of WMSs. This paper summarizes their findings and introduces this new
terminology to characterize WMSs. This terminology is composed of fives axes:
workflow characteristics, composition, orchestration, data management, and
metadata capture. Each axis comprises several concepts that capture the
prominent features of WMSs. Based on this terminology, this paper also presents
a classification of 23 existing WMSs according to the proposed axes and terms.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [72] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: KRAMABENCH是一个包含104个真实世界数据科学管道的基准测试，用于评估AI系统在数据处理方面的端到端能力，结果显示现有模型在复杂管道设计上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究探索AI系统是否能够成功设计和执行复杂的数据科学管道，填补现有模型在实际应用中的能力空白。

Method: 通过手动整理的104个真实数据科学管道测试5个通用模型和3个代码生成模型，使用DS-GURU框架指导AI模型分解任务并生成代码。

Result: 现有模型在明确的数据科学代码生成任务中表现良好，但在需要大量数据处理和领域知识的复杂管道设计中表现不足。

Conclusion: KRAMABENCH为开发自主数据科学代理迈出了关键一步，但仍需改进AI模型在复杂实际任务中的能力。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


### [73] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: QUITE是一种基于LLM的系统，通过多代理框架和实时数据库反馈，将SQL查询重写为性能更好的等效形式，克服了传统规则方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的SQL查询重写方法存在局限性，无法处理新查询模式且难以扩展。利用LLM的语义和推理能力实现更高效的查询重写。

Method: 提出QUITE系统，采用训练无关的多代理框架和有限状态机，结合实时反馈和提示注入技术，优化查询重写过程。

Result: QUITE将查询执行时间减少35.8%，生成24.1%更多重写，覆盖传统方法无法处理的查询案例。

Conclusion: QUITE通过LLM和反馈机制实现了更高效、覆盖面更广的SQL查询重写，性能显著优于现有方法。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [74] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: 论文提出了一种名为Bullseye的预测子系统，用于改进TAGE-SC-L分支预测器在处理难以预测（H2P）分支时的性能，通过动态阈值和并行操作减少预测错误并提高准确性。


<details>
  <summary>Details</summary>
Motivation: TAGE-SC-L预测器在处理H2P分支时表现不佳，简单的表扩大方法效果有限，需要更高效的方法来提升预测性能。

Method: 设计了一个28 KB的Bullseye子系统，通过H2P识别表（HIT）和两个分支特定感知器来动态识别和处理H2P分支，同时减少对TAGE表的污染。

Result: Bullseye与TAGE-SC-L并行运行，显著降低了预测错误率（MPKI为3.4045，CycWpPKI为145.09）。

Conclusion: 通过定制的H2P分支处理机制，Bullseye有效提升了分支预测器的性能，尤其是在处理复杂分支时。

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [75] [Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing](https://arxiv.org/abs/2506.06693)
*Priyanshu Yadav*

Main category: cs.AR

TL;DR: 论文对RISC-V指令集架构进行了全面分析，重点关注其模块化设计、实现挑战和性能特征。通过RV32I基础指令集及其扩展（乘法和原子操作）的周期精确模拟，评估了CPI和能效。结果表明，RISC-V在嵌入式系统中具有优势，且适合自定义加速器。与ARM Cortex-M0相比，功耗降低了17%。


<details>
  <summary>Details</summary>
Motivation: 研究RISC-V指令集架构的模块化设计和性能特征，以验证其在嵌入式系统和自定义加速器应用中的潜力。

Method: 使用RV32I基础指令集及其扩展（乘法和原子操作），通过周期精确的流水线模拟，评估CPI和能效。

Result: RISC-V在嵌入式系统中表现优异，功耗比ARM Cortex-M0降低了17%，且具有高度的灵活性和可扩展性。

Conclusion: RISC-V作为开放标准，具有显著的灵活性和性能优势，尤其在嵌入式系统和定制化应用中表现突出。

Abstract: This paper presents a comprehensive analysis of the RISC-V instruction set
architecture, focusing on its modular design, implementation challenges, and
performance characteristics. We examine the RV32I base instruction set with
extensions for multiplication (M) and atomic operations (A). Through
cycle-accurate simulation of a pipelined implementation, we evaluate
performance metrics including CPI (cycles per instruction) and power
efficiency. Our results demonstrate RISC-V's advantages in embedded systems and
its scalability for custom accelerators. Comparative analysis shows a 17%
reduction in power consumption compared to ARM Cortex-M0 implementations in
similar process nodes. The open-standard nature of RISC-V provides significant
flexibility for domain-specific optimizations.

</details>


### [76] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: ASPO是一种改进的贝叶斯优化方法，解决了标准BO在处理分类参数约束和优化时间增长问题，特别针对FPGA软处理器设计。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化（BO）无法处理分类参数约束，且优化时间随处理器复杂度增加，尤其是在FPGA软处理器设计中更为显著。

Method: ASPO通过定制BO的数学机制，使用新型协方差核支持分类参数，并通过惩罚采集函数和复用FPGA合成检查点加速设计评估。

Result: ASPO在BOOM处理器上为乘法基准测试减少了35%的执行时间，并将设计时间减少了74%，优于现有硬件导向BO方法。

Conclusion: ASPO为FPGA软处理器设计提供了一种高效且灵活的优化方法，显著提升了性能并减少了设计时间。

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


### [77] [Containerized In-Storage Processing and Computing-Enabled SSD Disaggregation](https://arxiv.org/abs/2506.06769)
*Miryeong Kwon,Donghyun Gouk,Eunjee Na,Jiseon Kim,Junhee Kim,Hyein Woo,Eojin Ryu,Hyunkyu Choi,Jinwoo Baek,Hanyeoreum Bae,Mahmut Kandemir,Myoungsoo Jung*

Main category: cs.AR

TL;DR: DockerSSD是一种利用OS级虚拟化和轻量级固件的ISP模型，直接在SSD上实现容器化数据处理，显著提升性能和分布式LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: ISP需解决数据分析和解耦存储的挑战，DockerSSD通过创新设计优化数据管理和处理效率。

Method: 结合OS级虚拟化、轻量级固件，支持基于网络的ISP管理及安全的容器运行。

Result: I/O密集型任务性能提升2.0倍，分布式LLM推理效率提升7.9倍。

Conclusion: DockerSSD为大规模服务提供高效、低开销的解耦存储方案，性能显著改进。

Abstract: ISP minimizes data transfer for analytics but faces challenges in adaptation
and disaggregation. We propose DockerSSD, an ISP model leveraging OS-level
virtualization and lightweight firmware to enable containerized data processing
directly on SSDs. Key features include Ethernet over NVMe for network-based ISP
management and Virtual Firmware for secure, efficient container execution.
DockerSSD supports disaggregated storage pools, reducing host overhead and
enhancing large-scale services like LLM inference. It achieves up to 2.0x
better performance for I/O-intensive workloads, and 7.9x improvement in
distributed LLM inference.

</details>


### [78] [QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine](https://arxiv.org/abs/2506.07046)
*Anushka Jha,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: QForce-RL是一种利用量化和轻量级架构的强化学习方法，显著提升了FPGA部署的吞吐量和能源效率。


<details>
  <summary>Details</summary>
Motivation: FPGA部署强化学习资源消耗大，QForce-RL旨在通过量化和轻量级架构解决这一问题。

Method: 结合E2HRL减少动作空间和QuaRL的量化SIMD加速硬件，优化模型大小和计算操作。

Result: 性能提升2.3倍，帧率提升2.6倍，适用于资源受限设备。

Conclusion: QForce-RL在资源效率和性能上优于现有技术，适用于多种应用场景。

Abstract: Reinforcement Learning (RL) has outperformed other counterparts in sequential
decision-making and dynamic environment control. However, FPGA deployment is
significantly resource-expensive, as associated with large number of
computations in training agents with high-quality images and possess new
challenges. In this work, we propose QForce-RL takes benefits of quantization
to enhance throughput and reduce energy footprint with light-weight RL
architecture, without significant performance degradation. QForce-RL takes
advantages from E2HRL to reduce overall RL actions to learn desired policy and
QuaRL for quantization based SIMD for hardware acceleration. We have also
provided detailed analysis for different RL environments, with emphasis on
model size, parameters, and accelerated compute ops. The architecture is
scalable for resource-constrained devices and provide parametrized efficient
deployment with flexibility in latency, throughput, power, and energy
efficiency. The proposed QForce-RL provides performance enhancement up to 2.3x
and better FPS - 2.6x compared to SoTA works.

</details>


### [79] [MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](https://arxiv.org/abs/2506.07126)
*Weihan Lu,Hong Cai Chen*

Main category: cs.AR

TL;DR: 提出了MAGNet，一种混合深度学习模型，结合改进的U-Net和图神经网络，用于DRC违规预测，提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 目的是通过机器学习和深度学习技术，提升集成电路设计中的设计规则检查（DRC）效率与准确性，从而降低成本。

Method: 使用改进的U-Net（动态注意力模块和多尺度卷积模块）提取空间特征，结合图神经网络建模拓扑关系，并采用标签放大策略增强稀疏违规模式的敏感性。

Result: MAGNet在DRC热点检测中显著优于对比模型（ibUnet、RouteNet和J-Net），准确率提升且误报率降低。

Conclusion: MAGNet成功结合空间、语义和结构信息，为DRC提供了一种高效且准确的解决方案。

Abstract: Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to sparse violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.

</details>


### [80] [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)
*Raghu Vamshi Hemadri,Jitendra Bhandari,Johann Knechtel,Badri P Gopalan,Ramesh Narayanaswamy,Ramesh Karri,Siddharth Garg*

Main category: cs.AR

TL;DR: VeriLoC是一种新方法，首次直接从Verilog代码预测线路级和模块级的设计质量，利用LLM嵌入提取信息，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现代芯片设计复杂，需要早期预测关键设计质量指标（如时序和布线拥塞），尤其是从Verilog代码中直接预测导致问题的具体代码行。

Method: VeriLoC利用Verilog代码生成LLM提取线路级和模块级嵌入，并训练下游分类器/回归器。

Result: VeriLoC在线路级拥塞和时序预测中F1分数达0.86-0.95，将平均百分比误差从14%-18%降至4%。

Conclusion: VeriLoC为复杂硬件设计的预测和优化任务提供了有价值的嵌入和见解。

Abstract: Modern chip design is complex, and there is a crucial need for early-stage
prediction of key design-quality metrics like timing and routing congestion
directly from Verilog code (a commonly used programming language for hardware
design). It is especially important yet complex to predict individual lines of
code that cause timing violations or downstream routing congestion. Prior works
have tried approaches like converting Verilog into an intermediate graph
representation and using LLM embeddings alongside other features to predict
module-level quality, but did not consider line-level quality prediction. We
propose VeriLoC, the first method that predicts design quality directly from
Verilog at both the line- and module-level. To this end, VeriLoC leverages
recent Verilog code-generation LLMs to extract local line-level and
module-level embeddings, and train downstream classifiers/regressors on
concatenations of these embeddings. VeriLoC achieves high F1-scores of
0.86-0.95 for line-level congestion and timing prediction, and reduces the mean
average percentage error from 14% - 18% for SOTA methods down to only 4%. We
believe that VeriLoC embeddings and insights from our work will also be of
value for other predictive and optimization tasks for complex hardware design.

</details>


### [81] [A Survey on LUT-based Deep Neural Networks Implemented in FPGAs](https://arxiv.org/abs/2506.07367)
*Zeyu Guo*

Main category: cs.AR

TL;DR: 该论文综述了基于LUT的DNN架构，解决了FPGA中DSP块限制的问题，提高了资源利用率和推理效率。


<details>
  <summary>Details</summary>
Motivation: 边缘计算需求低延迟、高效的DNN推理，传统FPGA依赖DSP块限制了可扩展性。基于LUT的DNN为解决这一问题提供了潜力。

Method: 论文综述了基于LUT的DNN架构，包括其演变、设计方法和性能权衡。

Result: 基于LUT的DNN提高了FPGA资源利用率，减少了推理延迟。

Conclusion: 基于LUT的DNN为边缘计算提供了新方向，未来研究将进一步优化其性能。

Abstract: Low-latency, energy-efficient deep neural networks (DNNs) inference are
critical for edge applications, where traditional cloud-based deployment
suffers from high latency and security risks. Field-Programmable Gate Arrays
(FPGAs) offer a compelling solution, balancing reconfigurability, power
efficiency, and real-time performance. However, conventional FPGA-based DNNs
rely heavily on digital signal processing (DSP) blocks for multiply-accumulate
(MAC) operations, limiting scalability.
  LUT-based DNNs address this challenge by fully leveraging FPGA lookup tables
(LUTs) for computation, improving resource utilization and reducing inference
latency. This survey provides a comprehensive review of LUT-based DNN
architectures, including their evolution, design methodologies, and performance
trade-offs, while outlining promising directions for future research.

</details>


### [82] [FREESS: An Educational Simulator of a RISC-V-Inspired Superscalar Processor Based on Tomasulo's Algorithm](https://arxiv.org/abs/2506.07665)
*Roberto Giorgi*

Main category: cs.AR

TL;DR: FREESS是一个免费的交互式模拟器，用于教学RISC-V启发的超标量处理器中的指令级并行性。


<details>
  <summary>Details</summary>
Motivation: 为高级计算机架构课程提供一个实践性强的教学工具，帮助学生理解动态乱序指令执行。

Method: 基于扩展的Tomasulo算法，模拟微架构组件，支持动态配置运行时参数，使用精简的RISC-V指令集。

Result: 通过一步步的示例，可视化展示指令在单周期内的并行执行，提供开源平台供实验。

Conclusion: FREESS是一个有效的教育工具，适合学生和教师通过实验学习超标量架构和指令级并行性。

Abstract: FREESS is a free, interactive simulator that illustrates instruction-level
parallelism in a RISC-V-inspired superscalar processor. Based on an extended
version of Tomasulo's algorithm, FREESS is intended as a hands-on educational
tool for Advanced Computer Architecture courses. It enables students to explore
dynamic, out-of-order instruction execution, emphasizing how instructions are
issued as soon as their operands become available.
  The simulator models key microarchitectural components, including the
Instruction Window (IW), Reorder Buffer (ROB), Register Map (RM), Free Pool
(FP), and Load/Store Queues. FREESS allows users to dynamically configure
runtime parameters, such as the superscalar issue width, functional unit types
and latencies, and the sizes of architectural buffers and queues.
  To simplify learning, the simulator uses a minimal instruction set inspired
by RISC-V (ADD, ADDI, BEQ, BNE, LW, MUL, SW), which is sufficient to
demonstrate key pipeline stages: fetch, register renaming, out-of-order
dispatch, execution, completion, commit, speculative branching, and memory
access. FREESS includes three step-by-step, illustrated examples that visually
demonstrate how multiple instructions can be issued and executed in parallel
within a single cycle. Being open source, FREESS encourages students and
educators to experiment freely by writing and analyzing their own
instruction-level programs and superscalar architectures.

</details>


### [83] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: 该论文分析了大型语言模型（LLM）在生成硬件描述语言（HDL）如SystemVerilog方面的能力，尤其是针对标准通信协议的实现。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs在通用编程语言代码生成上表现出色，但在HDL领域的应用尚未充分探索，尤其是针对生成可综合且功能正确的设计。

Method: 论文提出了首个针对SPI、I2C、UART和AXI四种广泛使用协议的基准测试套件，并定义了不同设计抽象水平和提示具体性的代码生成任务。

Result: 生成的代码通过波形仿真和测试台评估了语法正确性、可综合性和功能保真度。

Conclusion: 论文填补了LLMs在HDL领域应用的空白，为生成可综合和功能正确的设计提供了基准和分析。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


### [84] [Understanding the Error Sensitivity of Privacy-Aware Computing](https://arxiv.org/abs/2506.07957)
*Matías Mazzanti,Esteban Mocskos,Augusto Vega,Pradip Bose*

Main category: cs.AR

TL;DR: 该文探讨了同态加密（HE）在数据隐私保护中的重要性，重点关注CKKS方案对硬件和软件错误的敏感性，并研究了优化技术对错误敏感性的影响。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）是数据隐私保护的关键技术，但在实际应用中容易受到硬件和软件错误的影响，导致数据损坏。本文旨在研究HE（特别是CKKS方案）的错误敏感性，填补这一领域的研究空白。

Method: 本文通过详细研究CKKS方案，分析了其在硬件和软件错误下的表现，并探讨了残数系统（RNS）和数论变换（NTT）对错误敏感性的影响。

Result: 研究发现，CKKS方案对位错误特别敏感，且优化技术（如RNS和NTT）可能进一步加剧这一问题。这为未来提高HE的鲁棒性提供了重要参考。

Conclusion: 本文首次系统研究了HE的错误敏感性，揭示了潜在的数据损坏风险，并为未来改进HE的稳健性指明了方向。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data
without decryption, allowing a great opportunity for privacy-preserving
computation. In particular, domains such as healthcare, finance, and
government, where data privacy and security are of utmost importance, can
benefit from HE by enabling third-party computation and services on sensitive
data. In other words, HE constitutes the "Holy Grail" of cryptography: data
remains encrypted all the time, being protected while in use.
  HE's security guarantees rely on noise added to data to make relatively
simple problems computationally intractable. This error-centric intrinsic HE
mechanism generates new challenges related to the fault tolerance and
robustness of HE itself: hardware- and software-induced errors during HE
operation can easily evade traditional error detection and correction
mechanisms, resulting in silent data corruption (SDC).
  In this work, we motivate a thorough discussion regarding the sensitivity of
HE applications to bit faults and provide a detailed error characterization
study of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes
due to its fixed-point arithmetic support for AI and machine learning
applications. We also delve into the impact of the residue number system (RNS)
and the number theoretic transform (NTT), two widely adopted HE optimization
techniques, on CKKS' error sensitivity. To the best of our knowledge, this is
the first work that looks into the robustness and error sensitivity of
homomorphic encryption and, as such, it can pave the way for critical future
work in this area.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [85] [FPGA-Based Material Testing Machine Controller](https://arxiv.org/abs/2506.07139)
*Arev Hambardzumyan,Rafayel Ghasabyan,Vahagn Tamazyan*

Main category: eess.SY

TL;DR: FPGA控制器在材料测试中解决了传统控制器的适应性和速度限制问题，提供了高性能解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决传统控制器在材料测试中因多样性材料及测试标准带来的适应性和处理速度不足问题。

Method: 采用FPGA技术，实现可重构控制和并行数据采集电路集成。

Result: 提供了一种适应性强、处理速度快的多通道测试设备解决方案。

Conclusion: FPGA控制器为材料测试提供了高效、灵活的解决方案。

Abstract: In the realm of contemporary materials testing, the demand for scalability,
adaptability, parallelism, and speed has surged due to the proliferation of
diverse materials and testing standards. Traditional controller-based systems
often fall short in meeting these requirements, resulting in adaptability and
processing speed limitations. Conversely, FPGA-based controllers present a
multifaceted, high-performance solution. Key advantages of FPGA-based
controllers in materials testing encompass reconfiguration capabilities for
cost-effective adaptation to evolving materials and standards. FPGAs also
enable the integration of parallel control and data acquisition circuits, vital
for multichannel test equipment demanding simultaneous, independent operation
of multiple control channels.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: 提出了CPS-Guard框架，通过多角色编排自动化AI驱动的CPS的验证与验证过程，提高系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统验证方法难以应对AI组件的动态性和不可预测性，需要新的解决方案。

Method: 采用多角色编排，在模拟环境中通过专用代理（如安全监控、安全评估、故障注入等）持续评估和改进AI行为。

Result: 案例研究表明CPS-Guard能有效检测漏洞、管理性能影响并支持自适应恢复策略。

Conclusion: CPS-Guard为安全和关键系统提供了可扩展的结构化验证与验证方案。

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [87] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: ECOD框架利用边缘计算和多CAV协作，通过PACE和VOTE算法提升实时多视角物体检测的准确性和效率。实验表明，ECOD在分类准确率上比传统单视角方法提升高达75%。


<details>
  <summary>Details</summary>
Motivation: 传统车载感知系统因遮挡和盲区准确性受限，而云端解决方案延迟高，无法满足动态环境中自动驾驶的实时需求。

Method: ECOD框架整合PACE和VOTE算法，前者聚合多CAV数据以增强感知，后者通过共识投票机制提升分类准确性。

Result: 实验表明，ECOD在物体分类准确率上比传统方法提升75%，同时确保低延迟实时处理。

Conclusion: 研究表明边缘计算可显著增强对延迟敏感的自动驾驶系统的协作感知能力。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [88] [SMaRCSim: Maritime Robotics Simulation Modules](https://arxiv.org/abs/2506.07781)
*Mart Kartašev,David Dörner,Özer Özkahraman,Petter Ögren,Ivan Stenius,John Folkesson*

Main category: cs.RO

TL;DR: 本文介绍了一种名为SMaRCSim的仿真工具包，旨在解决水下机器人开发中的测试问题。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真工具缺乏对学习型方法、多类型自主载具团队协作以及与任务规划集成的支持。

Method: 开发了SMaRCSim仿真工具包，提供上述功能。

Result: SMaRCSim为水下机器人开发提供了全面的仿真支持。

Conclusion: SMaRCSim展示了将新功能引入水下领域的潜力。

Abstract: Developing new functionality for underwater robots and testing them in the
real world is time-consuming and resource-intensive. Simulation environments
allow for rapid testing before field deployment. However, existing tools lack
certain functionality for use cases in our project: i) developing
learning-based methods for underwater vehicles; ii) creating teams of
autonomous underwater, surface, and aerial vehicles; iii) integrating the
simulation with mission planning for field experiments. A holistic solution to
these problems presents great potential for bringing novel functionality into
the underwater domain. In this paper we present SMaRCSim, a set of simulation
packages that we have developed to help us address these issues.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [89] [#P is Sandwiched by One and Two #2DNF Calls: Is Subtraction Stronger Than We Thought?](https://arxiv.org/abs/2506.06716)
*Max Bannach,Erik D. Demaine,Timothy Gomez,Markus Hecher*

Main category: cs.CC

TL;DR: 论文研究了计算复杂性中的 #DNF 问题和 #2DNF 问题的复杂性关系，揭示了 gapP 可以通过两个 #2DNF 查询捕获，且证明了在单调片段中也能实现。同时，提出了线性时间减法的应用，并改进了 Toda 定理的结果。


<details>
  <summary>Details</summary>
Motivation: 探讨 #DNF 和 #2DNF 问题在计算复杂性中的作用，特别是在 logspace 归约下的表现，并解决单调片段中的相关问题。

Method: 通过结构感知的归约方法和技术，分析 #2DNF 查询的复杂性，并使用线性时间减法捕获 gapP。

Result: 证明两个受限的 #2DNF 查询足以捕获 gapP，且结果适用于单调片段；同时改进了 Toda 定理，展示了参数化的 SETH 紧下界。

Conclusion: 研究揭示了 #2DNF 查询的丰富性及其在复杂性理论和算法应用中的潜力，特别是在结构保留和参数化复杂性中的意义。

Abstract: The canonical class in the realm of counting complexity is #P. It is well
known that the problem of counting the models of a propositional formula in
disjunctive normal form (#DNF) is complete for #P under Turing reductions. On
the other hand, #DNF $\in$ spanL and spanL $\not\subseteq$ #P unless NL = NP.
Hence, the class of functions logspace-reducible to #DNF is a strict subset of
#P under plausible complexity-theoretic assumptions. By contrast, we show that
two calls to a (restricted) #2DNF oracle suffice to capture gapP, namely, that
the logspace many-one closure of the subtraction between the results of two
#2DNF calls is gapP. Because #P $\not\subseteq$ gapP, #P is strictly contained
between one and two #2DNF oracle calls.
  Surprisingly, the propositional formulas needed in both calls are linear-time
computable, and the reduction preserves interesting structural as well as
symmetry properties, leading to algorithmic applications. We show that a single
subtraction suffices to compensate for the absence of negation while still
capturing gapP, i.e., our results carry over to the monotone fragments of #2SAT
and #2DNF. Since our reduction is linear-time, it preserves sparsity and, as a
consequence we obtain a sparsification lemma for both #2SAT and #2DNF. This has
only been known for kSAT with k $\geq$ 3 and respective counting versions. We
further show that both #2DNF calls can be combined into a single call if we
allow a little postprocessing (computable by AC0- or TC0-circuits).
Consequently, we derive refined versions of Toda's Theorem: PH $\subseteq$
[#MON2SAT]$^{log}_{TC0}$ = [#MON2DNF]$^{log}_{TC0}$ and PH $\subseteq$
[#IMPL2SAT]$^{log}_{AC0}$. Our route to these results is via structure-aware
reductions that preserve parameters like treewidth up to an additive overhead.
The absence of multiplicative overhead indeed yields parameterized SETH-tight
lower bounds.

</details>


### [90] [Robust predicate and function computation in continuous chemical reaction networks](https://arxiv.org/abs/2506.06590)
*Kim Calabrese,David Doty,Mina Latifi*

Main category: cs.CC

TL;DR: 研究了化学反应网络（CRNs）中速率常数无关的布尔谓词和数值函数的计算方法，发现布尔谓词的计算受限，提出了一种更宽松的“稳健计算”方法，并证明了其可以解决阈值谓词和分段仿射函数问题。


<details>
  <summary>Details</summary>
Motivation: 探索CRNs在速率常数独立情况下的计算能力，特别是布尔谓词和数值函数的计算局限性，并提出一种更灵活的稳健计算方法以扩展其适用范围。

Method: 使用标准质量作用速率模型，定义稳健计算的条件，即对于任何正速率常数的选择，计算都能收敛到正确输出。研究了CRNs在稳健计算下的阈��谓词和分段仿射函数的计算能力。

Result: CRNs可以稳健地决定所有有限布尔组合的阈值谓词，并计算任何有理系数的分段仿射函数。

Conclusion: 稳健计算方法扩展了CRNs的计算能力，使其能够处理更复杂的布尔谓词和数值函数问题，尽管在严格的速率独立条件下其表现受限。

Abstract: We initiate the study of rate-constant-independent computation of Boolean
predicates and numerical functions in the continuous model of chemical reaction
networks (CRNs), which model the amount of a chemical species as a nonnegative,
real-valued *concentration*. Real-valued numerical functions have previously
been studied, finding that exactly the continuous, piecewise rational linear
functions $f: \mathbb{R}_{> 0}^k \to \mathbb{R}_{> 0}$ can be computed
*stably*, a.k.a., *rate-independently*, meaning that the CRN gets the answer
correct no matter the rate at which reactions occur.
  We show that, contrary to functions, continuous CRNs are severely limited in
the Boolean predicates they can stably decide, reporting an answer based only
on which inputs are 0 or positive.
  This limitation motivates a slightly relaxed notion of rate-independent
computation in CRNs that we call *robust computation*. The standard mass-action
rate model is used, in which each reaction is assigned a rate equal to the
product of its reactant concentrations and its rate constant. The computation
is correct in this model if it converges to the correct output for any positive
choice of rate constants. This adversary is weaker than the stable computation
adversary, the latter being able to run reactions at non-mass-action rates.
  We show that CRNs can robustly decide every finite Boolean combination of
*threshold predicates*: those predicates defined by taking a rational weighted
sum of the inputs $\mathbf{x} \in \mathbb{R}^k_{\ge 0}$ and comparing to a
constant, answering the question ``Is $\sum_{i=1}^k w_i \cdot \mathbf{x}(i) >
h$?'', for rational weights $w_i$ and real threshold $h$. Turning to function
computation, we show that CRNs can robustly compute any piecewise affine
function with rational coefficients, where threshold predicates determine which
affine piece to evaluate for a given input.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [91] [Insights on Harmonic Tones from a Generative Music Experiment](https://arxiv.org/abs/2506.07073)
*Emmanuel Deruty,Maarten Grachten*

Main category: cs.SD

TL;DR: 生成音乐AI通过艺术与科学的跨学科合作——工作室实验室，揭示了模型能够生成结构化和连贯的多重旋律线，促进音乐创造力和音乐认知的研究。


<details>
  <summary>Details</summary>
Motivation: 探讨生成音乐AI在音乐制作中的应用及其对音乐理论和人类听觉感知的潜在贡献。

Method: 通过工作室实验室的实验，研究者、音乐制作人和AI模型合作，观察模型生成低音音频的能力及其对音乐制作的影响。

Result: AI模型学会了生成结构化的多重旋律线，音乐制作人利用这些输出来探索谐波作为独立音高的可能性。

Conclusion: 生成音乐AI不仅能提升音乐创作，还为人类感知多音高的能力提供了新的研究视角。

Abstract: The ultimate purpose of generative music AI is music production. The
studio-lab, a social form within the art-science branch of
cross-disciplinarity, is a way to advance music production with AI music
models. During a studio-lab experiment involving researchers, music producers,
and an AI model for music generating bass-like audio, it was observed that the
producers used the model's output to convey two or more pitches with a single
harmonic complex tone, which in turn revealed that the model had learned to
generate structured and coherent simultaneous melodic lines using monophonic
sequences of harmonic complex tones. These findings prompt a reconsideration of
the long-standing debate on whether humans can perceive harmonics as distinct
pitches and highlight how generative AI can not only enhance musical creativity
but also contribute to a deeper understanding of music.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [92] [Hadamard-$Π$: Equational Quantum Programming](https://arxiv.org/abs/2506.06835)
*Wang Fang,Chris Heunen,Robin Kaarsgaard*

Main category: quant-ph

TL;DR: 该论文提出了一种小型量子编程语言，通过扩展经典可逆编程语言Π，引入Hadamard门原语，以实现计算通用性，并提供了完备的范畴语义和等价性自动化推理方法。


<details>
  <summary>Details</summary>
Motivation: 尽管量子计算在标准量子电路模型中通过添加Hadamard门实现通用计算，但其具体计算行为尚未完全明确。本文旨在填补这一理论空白。

Method: 引入一个小型量子编程语言，扩展经典可逆语言Π，仅添加Hadamard门原语。通过范畴语义和纯等式理论，实现程序的等价性自动化推理。

Result: 展示了正交矩阵群在环ℤ[1/√2]上的有限表示和合成算法，证明了语义的完备性。

Conclusion: 通过扩展经典编程语言并引入Hadamard门，实现了量子程序的完备语义，为量子计算行为提供了新的理论工具。

Abstract: Quantum computing offers advantages over classical computation, yet the
precise features that set the two apart remain unclear. In the standard quantum
circuit model, adding a 1-qubit basis-changing gate -- commonly chosen to be
the Hadamard gate -- to a universal set of classical reversible gates yields
computationally universal quantum computation. However, the computational
behaviours enabled by this addition are not fully characterised. We give such a
characterisation by introducing a small quantum programming language extending
the universal classical reversible programming language $\Pi$ with a single
primitive corresponding to the Hadamard gate. The language comes equipped with
a sound and complete categorical semantics that is specified by a purely
equational theory, enabling reasoning about the equivalence of quantum programs
in a way that can be automated. Completeness is shown by means of a novel
finite presentation, and corresponding synthesis algorithm, for the groups of
orthogonal matrices with entries in the ring $\mathbb{Z}[\tfrac{1}{\sqrt{2}}]$.

</details>


### [93] [Quantum Information-Theoretical Size Bounds for Conjunctive Queries with Functional Dependencies](https://arxiv.org/abs/2506.07552)
*Valter Uotila,Jiaheng Lu*

Main category: quant-ph

TL;DR: 该论文探讨了利用量子信息理论中的量子Rényi熵替代经典信息理论中的香农熵，以简化多约束条件下联合查询的最坏情况大小界限的计算，尽管实践中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决多约束条件下联合查询的最坏情况大小界限计算的复杂性问题，尤其是当约束条件超过一个时，现有方法需要无限多的线性不等式，计算困难。

Method: 提出使用量子Rényi熵替代经典香农熵，通过仅需非负性不等式的优化空间简化问题，并建立量子版本的最坏情况大小界限模型。

Result: 尽管量子方法简化了优化空间的描述，但由于量子态的优化复杂性，未能实现实际可计算的紧密最坏情况大小界限。

Conclusion: 论文展示了量子信息理论在数据库理论研究中的潜力，并提出未来的研究方向，将经典界限视为量子界限的特例。

Abstract: Deriving formulations for computing and estimating tight worst-case size
increases for conjunctive queries with various constraints has been at the core
of theoretical database research. If the problem has no constraints or only one
constraint, such as functional dependencies or degree constraints, tight
worst-case size bounds have been proven, and they are even practically
computable. If the problem has more than one constraint, computing tight bounds
can be difficult in practice and may even require an infinite number of linear
inequalities in its optimization formulation. While these challenges have been
addressed with varying methods, no prior research has employed quantum
information theory to address this problem. In this work, we establish a
connection between earlier work on estimating size bounds for conjunctive
queries with classical information theory and the field of quantum information
theory. We propose replacing the classical Shannon entropy formulation with the
quantum R\'enyi entropy. Whereas classical Shannon entropy requires infinitely
many inequalities to characterize the optimization space, R\'enyi entropy
requires only one type of inequality, which is non-negativity. Although this is
a promising modification, optimization with respect to the quantum states
instead of classical distributions creates a new set of challenges that prevent
us from finding a practically computable, tight worst-case size bound. In this
line, we propose a quantum version to derive worst-case size bounds. The
previous tight classical worst-case size bound can be viewed as a special limit
of this quantum bound. We also provide a comprehensive background on prior
research and discuss the future possibilities of quantum information theory in
theoretical database research.

</details>


### [94] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: 本文提出了一种加权同质量子集成方法，利用量子分类器和索引寄存器实现数据编码，通过量子并行执行多样化内部分类器，并在测试时电路中编码权重，实验证明了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 通过结合量子计算的并行性和集成学习的多样性，提高机器学习预测的准确性。

Method: 使用量子分类器和索引寄存器进行数据编码，通过叠加和控制门实现数据子采样，量子并行执行多样化分类器，并优化权重学习过程。

Result: 实验结果表明，该方法能够有效提升预测性能，验证了其在实际应用中的潜力。

Conclusion: 提出的加权同质量子集成方法为实现高效且多样化的机器学习提供了新思路，展示了量子计算在集成学习中的优势。

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [95] [Optimal quantum sampling on distributed databases](https://arxiv.org/abs/2506.07724)
*Longyun Chen,Jingcheng Liu,Penghui Yao*

Main category: quant-ph

TL;DR: 论文研究了分布式环境中的量子采样问题，提出了顺序和并行两种优化算法，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 由于大规模量子存储的高成本，研究分布式环境下的量子采样问题具有重要意义。

Method: 在分布式设置中，数据分布在多台机器上，每台机器维护一个基本的oracle；提出了顺序和并行两种算法，分别通过顺序或同时查询机器来实现采样。

Result: 提出的顺序和并行算法在各自设置下均被证明为最优。

Conclusion: 分布式量子采样问题可以通过顺序和并行算法高效解决，且算法在各自场景下是最优的。

Abstract: Quantum sampling, a fundamental subroutine in numerous quantum algorithms,
involves encoding a given probability distribution in the amplitudes of a pure
state. Given the hefty cost of large-scale quantum storage, we initiate the
study of quantum sampling in a distributed setting. Specifically, we assume
that the data is distributed among multiple machines, and each machine solely
maintains a basic oracle that counts the multiplicity of individual elements.
Given a quantum sampling task, which is to sample from the joint database, a
coordinator can make oracle queries to all machines. We focus on the oblivious
communication model, where communications between the coordinator and the
machines are predetermined. We present both sequential and parallel algorithms:
the sequential algorithm queries the machines sequentially, while the parallel
algorithm allows the coordinator to query all machines simultaneously.
Furthermore, we prove that both algorithms are optimal in their respective
settings.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [96] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 本文提出了一种利用自然语言处理（NLP）和大型语言模型（LLMs）的工作流，以提升战场物联网（IoBT）中情境感知的数据处理能力。


<details>
  <summary>Details</summary>
Motivation: 通过IoBT提升战场情境感知能力，需要将设备数据转换为用户友好的信息对象，并实现按需访问。

Method: 作者提出了一种两阶段工作流，利用适合边缘设备的LLMs将自然语言问题映射为Cypher数据库查询，并将数据库结果以自然语言形式返回。

Result: 在基于公开美军数据的测试中，Llama 3.1（80亿参数）表现最佳，且两阶段方法显著提高了查询准确性（19.4%）。

Conclusion: 该工作流为边缘设备部署LLMs奠定了基础，支持通过自然语言交互访问关键决策信息。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [97] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: 提出一种依赖类型的跨语言框架，用于分析事件的终结性和完成性，并在英语句子建模中应用。该框架分名义和动词两部分建模，并通过Agda形式化。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个形式化的方法来分析事件的终结性和完成性，解决语言中的界限和子类型问题。

Method: 扩展Martin-Löf依赖类型理论，名义部分建模名词短语的界限性，动词部分定义依赖事件演算。

Result: 成功形式化了框架，并通过Agda实现了规则和示例。

Conclusion: 框架能有效分析事件终结性和完成性，并通过形式化验证其严谨性。

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [98] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLM）在代码检查或调试中通过测试用例生成的能力，提出了TCGBench基准测试，发现LLM在生成有效测试用例生成器方面表现良好，但在生成针对性测试用例暴露代码缺陷方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在代码检查和调试中的应用潜力，特别是通过测试用例生成来发现竞争编程中的代码缺陷。

Method: 提出了TCGBench基准测试，包含两个任务：1）为给定竞争编程问题生成有效测试用例生成器；2）生成针对性测试用例生成器以暴露人类代码中的缺陷。

Result: 实验表明，LLM在生成有效测试用例生成器方面表现良好，但在生成针对性测试用例方面表现不佳，且与人类表现差距显著。通过高质量手动标注数据集，可以提高LLM的表现。

Conclusion: LLM在代码调试中的测试用例生成方面具有潜力，但仍需进一步改进，尤其是在生成针对性测试用例方面。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [99] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 该论文提供了一个多语言的孟加拉跨国政治话语数据集（BTPD），填补了该领域的数据空白，并展示了其主题和内容概况。


<details>
  <summary>Details</summary>
Motivation: 研究在线政治话语对分析公众意见和意识形态极化至关重要，但针对孟加拉语等资源不足语言的研究因缺乏数据集而受限。

Method: 通过社区驱动的关键词检索方法，从三个在线平台手工收集数据，构建数据集。

Result: 成功创建了一个多语言的孟加拉跨国政治话语数据集，并对其主题和内容进行了概述。

Conclusion: 该数据集为研究孟加拉语政治话语提供了重要资源，填补了现有研究的空白。

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [100] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: 论文研究了低资源语言（孟加拉语）中语言技术的社会技术系统身份偏见问题，并通过算法审计验证模型偏见的存在。


<details>
  <summary>Details</summary>
Motivation: 语言技术中的身份偏见问题对历史边缘化社区影响显著，但低资源语言环境下研究不足。

Method: 对基于mBERT和BanglaBERT的情感分析模型进行算法审计，测试其在性别、宗教和国籍身份上的偏见。

Result: 研究发现，尽管语义内容和结构相似，但模型在不同身份类别上表现出偏见。

Conclusion: 研究凸显了预训练模型与数据集结合的不一致性，并关联了认知不公和AI对齐的讨论。

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [101] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 该论文通过审计Twitch的自动审核工具AutoMod，发现其在检测仇恨内容时存在严重漏洞，94%的仇恨内容未被标记，同时对敏感词的非仇恨使用过度审查。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估实时互动平台（如Twitch）的自动审核系统在检测仇恨内容方面的有效性。

Method: 方法包括创建测试账户、通过Twitch API发送107,000条评论，并测量AutoMod对仇恨内容的标记准确性。

Result: 结果显示AutoMod漏检率高达94%，且对敏感词的上下文理解不足，误删了89.5%的良性内容。

Conclusion: 结论指出AutoMod存在重大缺陷，强调了自动化审核系统需更好地理解上下文。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 论文探讨了数字孪生如何助力现代非符号AI的数据不足问题，通过系统调查提出技术趋势和参考框架。


<details>
  <summary>Details</summary>
Motivation: 解决非符号AI在数据量和质量上的不足，数字孪生提供高保真模拟环境，优化AI训练。

Method: 系统调查了22项主要研究，分析技术趋势并设计参考框架，结合ISO 23247标准。

Result: 提出了数字孪生与AI组件结合的参考框架和架构指南，并指出研究挑战。

Conclusion: 数字孪生是AI模拟的有效工具，未来研究需进一步解决技术挑战。

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [103] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: 论文提出ReVD框架，通过合成推理数据和优化漏洞偏好，显著提高大语言模型在软件漏洞检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件漏洞检测中表现不足，主要缺乏与漏洞相关的推理数据和对语义背后原因的学习能力。

Method: 构建漏洞及其修复代码的前后向推理过程，设计三重监督微调和课程在线偏好优化。

Result: 在PrimeVul和SVEN数据集上，ReVD将检测准确率提高12.24%-22.77%。

Conclusion: ReVD通过推理数据合成和偏好优化，显著提升了漏洞检测能力，成为基于大语言模型的最先进方法。

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [104] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: 论文探讨了利用大型预训练模型进行自动游戏设计的潜力，并提出了一种名为ScriptDoctor的系统，它能够通过迭代循环生成和测试游戏设计。


<details>
  <summary>Details</summary>
Motivation: 目前，大型预训练模型在自动游戏设计中的应用主要依赖于人工监督，缺乏长期自主生成和测试的管道。

Method: ScriptDoctor利用大型语言模型（LLM）在PuzzleScript中生成和测试游戏，结合人类示例、引擎错误反馈和基于搜索的代理进行迭代优化。

Result: ScriptDemonstrate了LLM驱动的工作流在生成新颖游戏内容方面的潜力。

Conclusion: ScriptDoctor提供了一个具体的自动化工作流示例，展示了LLM在开放游戏设计中的应用前景。

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [105] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: 论文研究了如何利用同行预测机制检测和减少众包任务中LLM辅助作弊行为，特别适用于标注任务。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的广泛应用，众包工作者可能依赖LLM生成答案，导致数据集质量下降，现有检测方法对此不够适用。

Method: 提出了一种基于同行预测的无训练评分机制，通过量化工人回答间的相关性（以LLM生成的标签为条件），无需依赖真实标签即可检测作弊。

Result: 理论分析和实验表明，该方法在检测低质量作弊行为上表现稳健，适用于实际众包数据集。

Conclusion: 同行预测机制能有效应对LLM辅助作弊，为众包任务中的数据质量保障提供了新思路。

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [106] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 多模态大语言模型（LLM）在建筑工地视觉危险识别中的应用研究，通过比较五种先进模型在不同提示策略下的性能，发现链式思维（CoT）提示能显著提高准确性，并强调提示设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补多模态LLM在建筑安全领域应用的研究空白，探讨不同模型在视觉危险识别任务中的性能差异及其优化方法。

Method: 对五种模型（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3、Gemini 2.0 Pro）进行了三种提示策略（零样本、少样本、链式思维）的比较评估，使用精准率、召回率和F1分数进行量化分析。

Result: CoT提示策略显著提升模型性能，尤其是GPT-4.5和GPT-o3表现最优；提示设计对多模态LLM的准确性和一致性至关重要。

Conclusion: 研究为建筑安全领域的AI辅助系统提供了实用的提示工程建议，促进了更可靠的模型应用。

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [107] [From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion](https://arxiv.org/abs/2506.07050)
*Zheng Wang,Kai Ying,Bin Xu,Chunjiao Wang,Cong Bai*

Main category: cs.CV

TL;DR: PRE-Net通过多模态知识扩展提升了红外降水反演的准确性，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 红外降水反演技术精度低，而微波和雷达技术范围有限，因此提出PRE任务以扩展红外技术的应用范围。

Method: 采用两阶段流程（Swath-Distilling和Full-Disc Adaptation），结合CoMWE和Self-MaskTune技术提升模型性能。

Result: PRE-Net在PRE基准测试中表现优异，超越PERSIANN-CCS等主流产品。

Conclusion: PRE-Net为全盘降水反演提供了高效解决方案，并开源代码以推动研究进展。

Abstract: Accurate near-real-time precipitation retrieval has been enhanced by
satellite-based technologies. However, infrared-based algorithms have low
accuracy due to weak relations with surface precipitation, whereas passive
microwave and radar-based methods are more accurate but limited in range. This
challenge motivates the Precipitation Retrieval Expansion (PRE) task, which
aims to enable accurate, infrared-based full-disc precipitation retrievals
beyond the scanning swath. We introduce Multimodal Knowledge Expansion, a
two-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling
stage, PRE-Net transfers knowledge from a multimodal data integration model to
an infrared-based model within the scanning swath via Coordinated Masking and
Wavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune
refines predictions across the full disc by balancing multimodal and full-disc
infrared knowledge. Experiments on the introduced PRE benchmark demonstrate
that PRE-Net significantly advanced precipitation retrieval performance,
outperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code
will be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.

</details>


### [108] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 提出了一种名为STF和MBTF的方法，以减少视觉标记序列的长度并提高推理效率，同时保持多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态模型因视觉标记序列长而带来的计算成本高和效率低的问题。

Method: 通过Spatial Token Fusion (STF)学习紧凑视觉标记，并通过Multi-Block Token Fusion (MBTF)补充多粒度特征。

Result: 在8个视觉语言基准测试中，仅使用基线25%的视觉标记，性能相当或更优。

Conclusion: 组合STF和MBTF模块，能有效平衡标记减少和信息保留，提高推理效率且不牺牲性能。

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [109] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: VIVAT通过简单调整（如损失权重、填充策略和空间条件归一化）有效解决KL-VAE训练中的常见伪影问题，显著提升图像重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: VAE训练中常见的伪影问题（如颜色偏移、网格模式等）影响了重建和生成质量，VIVAT旨在通过系统性改进解决这些问题而不改变基础架构。

Method: 提出五种常见伪影的分类和根源分析，并通过调整损失权重、优化填充策略及引入空间条件归一化等简单修改进行改进。

Result: 在多个基准测试中，PSNR和SSIM指标达到最优，且文本到图像生成的CLIP分数显著提高。

Conclusion: VIVAT在保持KL-VAE框架简洁性的同时，有效解决了训练中的实际问题，为研究者提供了实用优化策略。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [110] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: 提出首个多变量时间序列扩散模型的水印算法TimeWak，通过在真实时空嵌入时序链式哈希水印，解决了现有方法与水印兼容性问题，并在数据质量和水印可检测性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法依赖同质潜在空间嵌入，但最先进的时间序列生成器在真实空间操作，导致潜在水印方法不兼容。需要直接在真实时空解决特征异质性和时序依赖性问题。

Method: 提出TimeWak算法，直接在真实时空嵌入时序链式哈希水印，并引入ε-精确反转技术处理扩散过程逆变的非均匀重构误差。

Result: 在5个数据集上验证，TimeWak在context-FID评分上提升61.96%，相关性评分提升8.44%，且水印始终保持高可检测性。

Conclusion: TimeWak成功解决了时间序列扩散模型的水印嵌入问题，显著提升了合成数据质量和抗攻击能力。

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [111] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 论文提出了一种基于早期充电阶段电压行为的电动汽车认证框架，提高了识别的速度和可靠性，同时也揭示了充电数据可能带来的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电安全性需求增加，但现有认证方法集中在充电后期，无法及时阻止能源盗窃行为，同时充电模式分析可能引发隐私问题。

Method: 利用早期充电阶段的电压测量数据提取特征，验证电动汽车指纹识别的可行性，并通过数据集测试模型性能。

Result: 在7408次充电数据中，模型准确率达0.86，仅需10个关键特征即可达到接近最优的性能。

Conclusion: 研究为电动汽车认证提供了新方法，但强调了充电数据可能被滥用的隐私风险。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [112] [Are Trees Really Green? A Detection Approach of IoT Malware Attacks](https://arxiv.org/abs/2506.07836)
*Silvia Lucia Sanna,Diego Soi,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 提出了一种基于节能的树模型优化方法，用于识别IoT恶意网络攻击，兼顾高性能与低能耗。


<details>
  <summary>Details</summary>
Motivation: IoT设备由于资源受限和难以应用安全补丁，易受网络攻击，现有机器学习方法多关注攻击识别，忽略了算法对计算资源的影响。

Method: 优化三种树模型（决策树、随机森林、Extra-Trees）的超参数，基于能耗和测试性能（马修斯相关系数）。

Result: 模型在降低能耗（瓦时）的同时保持高性能和检测准确率。

Conclusion: 基于本地的ML入侵检测系统适合IoT等资源受限设备。

Abstract: Nowadays, the Internet of Things (IoT) is widely employed, and its usage is
growing exponentially because it facilitates remote monitoring, predictive
maintenance, and data-driven decision making, especially in the healthcare and
industrial sectors. However, IoT devices remain vulnerable due to their
resource constraints and difficulty in applying security patches. Consequently,
various cybersecurity attacks are reported daily, such as Denial of Service,
particularly in IoT-driven solutions. Most attack detection methodologies are
based on Machine Learning (ML) techniques, which can detect attack patterns.
However, the focus is more on identification rather than considering the impact
of ML algorithms on computational resources. This paper proposes a green
methodology to identify IoT malware networking attacks based on flow
privacy-preserving statistical features. In particular, the hyperparameters of
three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are
optimized based on energy consumption and test-time performance in terms of
Matthew's Correlation Coefficient. Our results show that models maintain high
performance and detection accuracy while consistently reducing power usage in
terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion
Detection Systems are suitable for IoT and other resource-constrained devices.

</details>


### [113] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: 该论文介绍了TimberStrike攻击，一种针对基于树的联邦学习的优化数据集重构攻击，证明了现有系统的隐私漏洞，并讨论了差分隐私的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究基于树的联邦学习模型在安全和隐私方面的不足，填补现有研究的空白。

Method: 通过单个客户端利用决策树的分裂值和路径信息，重构其他客户端的敏感训练数据，并在多个框架中验证攻击效果。

Result: 在所有测试框架中，TimberStrike能重构73.05%至95.63%的目标数据集，差分隐私虽能部分缓解攻击但显著降低模型性能。

Conclusion: 需为基于树的联邦学习设计专门隐私保护机制，论文为此提供了初步设计思路。

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [114] [Exploiting Inaccurate Branch History in Side-Channel Attacks](https://arxiv.org/abs/2506.07263)
*Yuhui Zhu,Alessandro Biondi*

Main category: cs.CR

TL;DR: 论文研究现代分支预测单元（BPU）的资源共享和竞争如何影响其特性，揭示了这些特性可能带来的安全风险，并提出新的攻击模型。


<details>
  <summary>Details</summary>
Motivation: 探讨分支预测资源缺乏隔离导致的潜在安全漏洞，以及如何通过BPU特性（如Bias-Free Branch Prediction和Branch History Speculation）引发新的攻击面。

Method: 分析BPU的基本组件，研究资源共享和竞争对其特性的影响，并通过实验验证这些特性如何修改分支历史缓冲区（BHB）的行为。

Result: 发现了新的跨权限攻击面（BHI），提出了三种新攻击模型（Spectre-BSE、Spectre-BHS和BiasScope），并在多款处理器上验证了漏洞利用。

Conclusion: 研究表明，BPU特性虽提升了性能，但也带来安全风险，作者开发的Chimera攻击演示器验证了Spectre-BHS的可行性。

Abstract: Modern out-of-order CPUs heavily rely on speculative execution for
performance optimization, with branch prediction serving as a cornerstone to
minimize stalls and maximize efficiency. Whenever shared branch prediction
resources lack proper isolation and sanitization methods, they may originate
security vulnerabilities that expose sensitive data across different software
contexts.
  This paper examines the fundamental components of modern Branch Prediction
Units (BPUs) and investigates how resource sharing and contention affect two
widely implemented but underdocumented features: Bias-Free Branch Prediction
and Branch History Speculation. Our analysis demonstrates that these BPU
features, while designed to enhance speculative execution efficiency through
more accurate branch histories, can also introduce significant security risks.
We show that these features can inadvertently modify the Branch History Buffer
(BHB) update behavior and create new primitives that trigger malicious
mis-speculations.
  This discovery exposes previously unknown cross-privilege attack surfaces for
Branch History Injection (BHI). Based on these findings, we present three novel
attack primitives: two Spectre attacks, namely Spectre-BSE and Spectre-BHS, and
a cross-privilege control flow side-channel attack called BiasScope. Our
research identifies corresponding patterns of vulnerable control flows and
demonstrates exploitation on multiple processors. Finally, Chimera is
presented: an attack demonstrator based on eBPF for a variant of Spectre-BHS
that is capable of leaking kernel memory contents at 24,628 bit/s.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [115] [SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement](https://arxiv.org/abs/2506.07634)
*Chenyu Yang,Shuai Wang,Hangting Chen,Wei Tan,Jianwei Yu,Haizhou Li*

Main category: eess.AS

TL;DR: SongBlo…


<details>
  <summary>Details</summary>
Motivation: Generating music with coherent structure and harmonious elements remains challenging, as current methods struggle with balancing global coherence and local fidelity.

Method: SongBloom uses an autoregressive diffusion model for full-length song generation, combining diffusion models' high fidelity with language models' scalability through interleaved sketching and refinement.

Result: SongBloom outperforms existing methods in subjective and objective metrics, achieving performance close to commercial platforms.

Conclusion: SongBloom presents a promising framework for high-quality, coherent song generation.

Abstract: Generating music with coherent structure, harmonious instrumental and vocal
elements remains a significant challenge in song generation. Existing language
models and diffusion-based methods often struggle to balance global coherence
with local fidelity, resulting in outputs that lack musicality or suffer from
incoherent progression and mismatched lyrics. This paper introduces
$\textbf{SongBloom}$, a novel framework for full-length song generation that
leverages an interleaved paradigm of autoregressive sketching and
diffusion-based refinement. SongBloom employs an autoregressive diffusion model
that combines the high fidelity of diffusion models with the scalability of
language models. Specifically, it gradually extends a musical sketch from short
to long and refines the details from coarse to fine-grained. The interleaved
generation paradigm effectively integrates prior semantic and acoustic context
to guide the generation process. Experimental results demonstrate that
SongBloom outperforms existing methods across both subjective and objective
metrics and achieves performance comparable to the state-of-the-art commercial
music generation platforms. Audio samples are available on our demo page:
https://cypress-yang.github.io/SongBloom\_demo.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [116] [WiFi Pathologies Detection using LLMs](https://arxiv.org/abs/2506.06943)
*Forough Shirin Abkenar*

Main category: eess.SP

TL;DR: 通过微调编码器和解码器大型语言模型，检测WiFi网络中的异常情况。


<details>
  <summary>Details</summary>
Motivation: 研究目标是利用LLMs来检测IEEE 802.11（WiFi）网络的病理问题，以提升网络诊断的效率和准确性。

Method: 方法包括手动设计提示语并对模型进行微调。采用了编码器-解码器和因果模型两种架构。

Result: 结果显示，序列模型在标注数据上表现优异，而因果模型在未标注数据上同样表现良好。

Conclusion: 研究表明，微调后的LLMs能够有效检测WiFi网络中的异常情况，两种模型各具优势。

Abstract: In this paper, we fine-tune encoder-only and decoder-only large language
models (LLMs) to detect pathologies in IEEE 802.11 networks, commonly known as
WiFi. Our approach involves manually crafting prompts followed by fine-tuning.
Evaluations show that the sequential model achieves high detection accuracy
using labeled data, while the causal model performs equally well for unlabeled
data.

</details>


### [117] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 该研究开发并评估了机器学习方法，用于通过多模态传感器数据早期预测社区居住老年痴呆患者的激动行为，引入新的特征集，并利用公开数据进行了多模型比较。


<details>
  <summary>Details</summary>
Motivation: 痴呆患者的激动行为常见且影响生活质量，早期预测可减轻护理负担并改善患者和护理者的生活。

Method: 研究使用了多模态传感器数据和机器学习模型，包括二进制分类和异常检测，利用TIHM数据集进行验证。

Result: 最佳模型在6小时时间戳的二进制分类中表现最优（AUC-ROC: 0.9720），加入时间和历史信息进一步提升性能。

Conclusion: 研究为社区痴呆护理提供了隐私保护、高效的激动预测方法，支持主动护理和就地养老。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [118] [Meaning as Use, Application, Employment, Purpose, Usefulness](https://arxiv.org/abs/2506.07131)
*Ruy J. G. B. de Queiroz*

Main category: math.HO

TL;DR: Wittgenstein对语言的本质进行了深入研究，将其视为理解现实和世界的工具，并通过数学、心理学等多个领域测试其理论，最终强调语言的意义与使用、目的密切相关。


<details>
  <summary>Details</summary>
Motivation: 探讨Wittgenstein如何通过对语言的多角度研究，揭示其作为人类推理和生活揭示的根本途径。

Method: 通过开放和可搜索的Nachlass资料库，分析Wittgenstein的文本，追踪其对语言意义与使用的反复探讨。

Result: 发现Wittgenstein从早期到晚期的著作中始终强调语言意义与使用、目的的关系，这一核心思想贯穿其研究。

Conclusion: Wittgenstein的研究展示了语言作为意义载体的核心地位，其思想超越了单一标签，体现了对语言本质的持续探索。

Abstract: Arising from the whole body of Wittgenstein's writings is a picture of a (not
necessarily straight, linear, but admittedly tireless) journey to come to terms
with the mechanics of language as an instrument to conceive `reality' and to
communicate an acquired conception of the `world'. The journey passes through
mathematics, psychology, color perception, certainty, aesthetic, but, looking
at it from a sort of birdview, it seems reasonable to say that these are all
used as `test beds' for his reflections and `experimentations' towards an all
encompassing perspective of such a fundamental gateway to human reasoning and
life-revealing as language. Whatever labelling of Wittgenstein as a mystic, a
logicist, a conventionalist, a skeptic, an anti-metaphysics, an anti-realist, a
verificationist, a pragmatist, and many others, does not seem to do justice to
his absolute obsession with being a persistent `deep diver' into the nature of
language. Working with an open and searchable account of the Nachlass has
allowed us to identify important aspects of the philosopher's possible common
line of thinking, in spite of changes of directions, some of them acknowledged
by Wittgenstein himself. One of those aspects is the association of meaning
with use, application, purpose, usefulness of symbols in language, which
happens to show itself from the very beginning through to the very late
writings. The German terms Gebrauch, Anwendung, \emph{Verwendung}, Zweck in
relation to meaning, sense of signs, words, sentences, appear in several texts
since the WW1 Notebooks (1914--1916) up until very late manuscripts from
1950--51.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [119] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 本文研究如何通过强化学习优化自动驾驶行为，提升婴儿睡眠质量，同时兼顾出行效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶虽提升了安全性和舒适性，但其对婴儿睡眠的影响尚未充分研究。突加减速和急转弯可能扰乱婴儿睡眠，影响乘客舒适度和家长便利性。

Method: 提出了一种智能巡航控制框架，结合可穿戴设备与车辆数据，利用LSTM和Transformer神经网络建模驾驶行为与婴儿睡眠质量的关系。该方法根据传感器、车辆控制器和地图数据动态优化驾驶策略。

Result: 仿真结果表明，相比基线方法，该方案显著提升了婴儿睡眠质量，同时保持了良好的出行效率。

Conclusion: 通过强化学习与多源数据融合，该方法成功优化了自动驾驶行为，为提升婴儿睡眠质量提供了可行方案。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [120] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出了一种通过注入Gumbel噪声和直通估计器来加速逻辑门网络（LGNs）训练的方法，显著提升了训练速度、神经元利用率和减少了离散化差距。


<details>
  <summary>Details</summary>
Motivation: 尽管逻辑门网络在图像分类中表现出高效性，但其训练时间长、神经元利用率低和离散化差距大限制了实际应用。

Method: 在训练过程中注入Gumbel噪声并使用直通估计器，理论分析表明这隐含了Hessian正则化，改善了收敛性能。

Result: 实验结果显示，训练速度提升4.5倍，离散化差距减少98%，未使用门数量减少100%。

Conclusion: 该方法有效解决了LGNs的训练效率和性能问题，为其实际部署提供了可行方案。

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [121] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 该论文提出了一种基于大型语言模型（LLMs）的分层协作方法，用于多无人机在动态约束环境中的运动和通信控制，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统在动态和受限环境中的控制与优化是一个重要挑战，尤其是在集成地面和非地面网络（如高空平台站）的场景中。

Method: 采用分层协作的LLM框架，HAPS上的LLM负责无人机接入控制，每架无人机上的LLM处理运动规划与控制。

Result: 实验结果表明，该方法实现了更高的系统奖励、更低的运营成本和显著减少的无人机碰撞率。

Conclusion: 这种基于知识的范式为下一代三维空中高速公路系统的开发提供了巨大潜力。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [122] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer是一个基于神经前提选择系统的领域通用工具，首次为Lean证明助手提供了端到端的自动化推理支持。它通过动态适应用户上下文并结合符号Proof搜索，显著提高了目标解决率。


<details>
  <summary>Details</summary>
Motivation: 尽管神经方法在自动推理中取得了进展，但将其应用到实际证明助手中仍具有挑战性。Lean是受欢迎的证明助手，但缺乏自动化工具Hammers。因此，开发LeanHammer以填补这一空白。

Method: 提出LeanHammer，结合新颖的神经前提选择系统与符号Proof搜索和重建，动态适应用户上下文。

Result: 与现有前提选择器相比，LeanHammer能解决21%更多的目标，并能在多样化领域中表现出色。

Conclusion: LeanHammer成功将神经检索与符号推理结合，提升了形式化验证的可及性，为研究者和实践者提供了更高效的自动化工具。

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [123] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过动态选择轻量级或大型语言模型来优化计算资源，以降低语言模型的推理成本。


<details>
  <summary>Details</summary>
Motivation: 语言模型在计算资源和能耗方面的高额需求限制了其在移动、边缘等资源受限环境中的部署。

Method: 论文研究了两种策略：路由（根据查询选择合适模型）和级联推理（逐步提升模型复杂度直至获得满意结果）。

Result: 这些策略能在保证性能的同时显著降低计算开销。

Conclusion: 未来的研究方向包括更快响应时间、基于任务复杂度的自适应模型选择，以及跨异构环境的可扩展部署。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [124] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: SALT 是一种轻量级模型适应框架，用于在封闭约束下进行 Split Computing，通过客户端适配器优化特征，实现个性化推理，提升准确性并降低训练延迟。


<details>
  <summary>Details</summary>
Motivation: 在 Split Computing 的封闭环境中，用户无法访问头尾网络的参数或架构，传统适应方法不可行。SALT 旨在解决这一问题。

Method: SALT 在客户端引入一个紧凑、可训练的适配器，优化头网络的潜在特征，无需修改原始模型或增加通信开销。

Result: 在 CIFAR-10 和 CIFAR-100 任务中，SALT 提高了准确性，训练延迟更低，并能适应有损网络的稳健推理。

Conclusion: SALT 为边缘 AI 系统提供了一种在严格约束下实现个性化推理的实用解决方案，部署开销极小。

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [125] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 本文提出联邦学习中的多级梯度发散（CGD）概念，证明其影响FL收敛速度，并提出FedCGD算法优化调度策略。


<details>
  <summary>Details</summary>
Motivation: 解决无线网络中联邦学习的数据异构性和带宽限制问题，传统方法未充分关注设备级和样本级梯度发散的集体效应。

Method: 通过分析设备级和样本级CGD，将其转化为分类问题的加权地球移动距离（WEMD），并提出FedCGD算法。

Result: 实验表明，FedCGD在减少调度设备数量的同时，提升了CIFAR-10数据集分类准确率4.2%。

Conclusion: 多级CGD优化能有效提升FL性能，FedCGD算法在实际中表现优异。

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [126] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT是一种基于FPGA的超快CNN微调方法，适用于物联网设备，显著提升了计算效率和能源效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在资源有限的物联网平台上运行时计算和内存需求高的问题。

Method: 通过优化参数高效微调（PEFT）中的前向和反向计算，提出InstantFT方法。

Result: 实验表明，InstantFT比现有的基于LoRA的方法快17.4倍，能耗降低16.3倍，且准确率相当。

Conclusion: InstantFT能够快速适应非平稳数据分布，适用于物联网设备的实时需求。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [127] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: FuncGNN通过混合特征聚合、门感知归一化和多层集成技术，解决电路设计中AIG的结构异质性和全局逻辑信息丢失问题，显著提升了逻辑电路表示性能。


<details>
  <summary>Details</summary>
Motivation: 现代电路复杂度和集成密度增加，导致AIG在表示布尔逻辑时出现结构异质性和全局逻辑信息丢失，亟需更有效的电路表示方法。

Method: FuncGNN采用混合特征提取多粒度拓扑模式，门感知归一化适应特定门分布，多层集成融合局部和全局语义信息。

Result: 在信号概率预测和真值表距离预测任务中，FuncGNN性能提升2.06%和18.71%，训练时间减少50.6%，GPU内存使用降低32.8%。

Conclusion: FuncGNN在解决电路表示问题上表现出色，是电子设计自动化领域的高效工具。

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [128] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: 论文探讨了多GPU MoE网络中的负载均衡问题，提出MoE-GPS框架优化预测器设计，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决多GPU MoE网络中因专家分布不均导致的负载失衡问题。

Method: 提出MoE-GPS框架，量化预测策略（如Distribution-Only Prediction）对系统性能的影响。

Result: 在Mixtral 8x7B MMLU数据集上，Distribution-Only Prediction比传统方法提升23%的推理性能。

Conclusion: MoE-GPS通过优化预测策略，显著提升系统性能，适用于多种配置。

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [129] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 本文研究了基于Hessian矩阵的分析方法在注意力机制模型故障诊断中的应用潜力，发现其比梯度分析更有效。


<details>
  <summary>Details</summary>
Motivation: 随着注意力模型规模和复杂度的增加，故障诊断变得更具挑战性，本文旨在探索Hessian矩阵分析在此领域的实用性。

Method: 通过Hessian矩阵的曲率分析和参数交互分析，识别了注意力机制中的脆弱区域和参数依赖性，并在HAN、3D-CNN和DistilBERT模型上进行了实验。

Result: 实验表明，Hessian指标比梯度更能准确定位不稳定性和故障源。

Conclusion: Hessian矩阵指标能显著提升复杂神经架构的故障诊断能力，有望改进软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [130] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: 该研究结合上下文多臂老虎机（cMAB）和大型语言模型（LLM），提出了一种混合方法，用于个性化干预以减少久坐行为。


<details>
  <summary>Details</summary>
Motivation: 现有cMAB算法需要大量样本且可能忽略心理因素，希望通过结合LLM提升干预效果。

Method: 研究比较了四种模型：单独cMAB、单独LLM、cMABxLLM组合和随机对照试验（RCT），评估其对步数和消息接受的影响。

Result: 研究通过因果推理框架分析了每种模型的效果。

Conclusion: LLM个性化和cMAB适应在促进体育活动方面具有互补作用。

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [131] [Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics](https://arxiv.org/abs/2506.06286)
*Kevin Baum*

Main category: cs.CY

TL;DR: 论文提出了一种结构化概念框架，以区分AI对齐的目标、范围和选民，为解决AI安全与伦理问题提供了清晰的理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究快速发展，但AI安全、对齐和伦理学等领域的概念边界模糊，缺乏明确的研究方向。

Method: 引入分类法，区分对齐的目标（安全、伦理、合法性等）、范围（结果与执行）和选民（个体与集体）。

Result: 框架揭示了多种合理的对齐配置，为跨领域的实践和哲学整合提供了基础。

Conclusion: 该框架为AI代理的全面对齐提供了理论支持，有助于澄清研究方向和目标。

Abstract: Recent advances in AI research make it increasingly plausible that artificial
agents with consequential real-world impact will soon operate beyond tightly
controlled environments. Ensuring that these agents are not only safe but that
they adhere to broader normative expectations is thus an urgent
interdisciplinary challenge. Multiple fields -- notably AI Safety, AI
Alignment, and Machine Ethics -- claim to contribute to this task. However, the
conceptual boundaries and interrelations among these domains remain vague,
leaving researchers without clear guidance in positioning their work.
  To address this meta-challenge, we develop a structured conceptual framework
for understanding AI alignment. Rather than focusing solely on alignment goals,
we introduce a taxonomy distinguishing the alignment aim (safety, ethicality,
legality, etc.), scope (outcome vs. execution), and constituency (individual
vs. collective). This structural approach reveals multiple legitimate alignment
configurations, providing a foundation for practical and philosophical
integration across domains, and clarifying what it might mean for an agent to
be aligned all-things-considered.

</details>


### [132] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 该论文提出了一种新型审计框架来评估工人希望AI自动化或增强的职业任务，并比较了这些愿望与当前技术能力的匹配情况，最终构建了WORKBank数据库以揭示任务的不同优先级和开发机会。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的快速发展，其对劳动市场的影响日益显著，但目前缺乏对工人需求与技术能力匹配的系统性研究。

Method: 引入音频增强的小型访谈和Human Agency Scale (HAS)来量化工人对任务自动化的偏好，并构建WORKBank数据库，结合工人偏好和AI专家评估，将任务分为四个区域。

Result: 研究发现任务可以划分为四个优先级区域，揭示了不同职业对人工参与的多样化需求，并指出AI集成可能将人类核心能力从信息技能转向人际技能。

Conclusion: 研究强调AI开发应与人类需求对齐，并为工人适应未来的工作动态做好准备。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


### [133] [Privacy Perspectives and Practices of Chinese Smart Home Product Teams](https://arxiv.org/abs/2506.06591)
*Shijing He,Yaxiong Lei,Xiao Zhan,Chi Zhang,Juan Ye,Ruba Abu-Salma,Jose Such*

Main category: cs.CY

TL;DR: 本文通过27次半结构化访谈，研究了中国智能家居产品团队的隐私观点与实践，发现其更注重国家安全的合规性，并受中国文化、社会和法律因素影响。


<details>
  <summary>Details</summary>
Motivation: 探索非西方背景下智能家居产品团队的隐私观点与实践。

Method: 采用27次半结构化访谈，涵盖不同职能的团队成员。

Result: 中国团队更注重国家安全合规性，文化和社会因素影响其隐私与便利的平衡。

Conclusion: 提出针对中国智能家居隐私问题的建议和社会技术法律干预措施。

Abstract: Previous research has explored the privacy needs and concerns of device
owners, primary users, and different bystander groups with regard to smart home
devices like security cameras, smart speakers, and hubs, but little is known
about the privacy views and practices of smart home product teams, particularly
those in non-Western contexts. This paper presents findings from 27
semi-structured interviews with Chinese smart home product team members,
including product/project managers, software/hardware engineers, user
experience (UX) designers, legal/privacy experts, and marketers/operation
specialists. We examine their privacy perspectives, practices, and risk
mitigation strategies. Our results show that participants emphasized compliance
with Chinese data privacy laws, which typically prioritized national security
over individual privacy rights. China-specific cultural, social, and legal
factors also influenced participants' ethical considerations and attitudes
toward balancing user privacy and security with convenience. Drawing on our
findings, we propose a set of recommendations for smart home product teams,
along with socio-technical and legal interventions to address smart home
privacy issues-especially those belonging to at-risk groups-in Chinese
multi-user smart homes.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [134] [Quantum-Enhanced Spectral Solution of the Poisson Equation](https://arxiv.org/abs/2506.07743)
*G. Intoccia,U. Chirico,G. Pepe,S. Cuomo*

Main category: math.NA

TL;DR: 提出了一种混合数值-量子方法，利用量子傅里叶变换（QFT）提升泊松方程求解的计算效率，降低时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决传统经典方法在处理大量点时的高计算成本问题，探索量子辅助技术在偏微分方程（PDEs）求解中的应用潜力。

Method: 通过量子框架直接估计解的级数展开系数，避免传统方法的积分计算。

Result: 数值实验验证了该方法在时间、空间复杂性和解精度上的显著提升。

Conclusion: 尽管量子实现存在挑战，但该研究为未来优化和扩展量子数值方法提供了基础。

Abstract: We present a hybrid numerical-quantum method for solving the Poisson equation
under homogeneous Dirichlet boundary conditions, leveraging the Quantum Fourier
Transform (QFT) to enhance computational efficiency and reduce time and space
complexity. This approach bypasses the integration-heavy calculations of
classical methods, which have to deal with high computational costs for large
number of points. The proposed method estimates the coefficients of the series
expansion of the solution directly within the quantum framework. Numerical
experiments validate its effectiveness and reveal significant improvements in
terms of time and space complexity and solution accuracy, demonstrating the
capability of quantum-assisted techniques to contribute in solving partial
differential equations (PDEs). Despite the inherent challenges of quantum
implementation, the present work serves as a starting point for future
researches aimed at refining and expanding quantum numerical methods.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [135] [CNFs and DNFs with Exactly $k$ Solutions](https://arxiv.org/abs/2506.07268)
*L. Sunil Chandran,Rishikesh Gajjala,Kuldeep S. Meel*

Main category: cs.DM

TL;DR: 论文研究了加权模型计数问题，探讨了构建具有恰好k个满足赋值的DNF或CNF公式所需的最小项数或子句数。提出了首个o(log k)上界，并证明了某些k值需要Ω(log log k)项或子句的下界。


<details>
  <summary>Details</summary>
Motivation: 加权模型计数在概率推理、网络可靠性等领域有广泛应用。研究最小项数问题有助于提高基于公式转换的模型计数算法的效率。

Method: 通过构造单调DNF公式，证明了上界O(√(log k) log log k)；同时通过理论分析，确定了某些k值的下界Ω(log log k)。

Result: 首次提出o(log k)的上界，并证明了下界的存在，表明在某些情况下需要至少Ω(log log k)项或子句。

Conclusion: 这些结果对模型计数算法的效率改进有重要意义，揭示了公式转换中的复杂度限制。

Abstract: Model counting is a fundamental problem that consists of determining the
number of satisfying assignments for a given Boolean formula. The weighted
variant, which computes the weighted sum of satisfying assignments, has
extensive applications in probabilistic reasoning, network reliability,
statistical physics, and formal verification. A common approach for solving
weighted model counting is to reduce it to unweighted model counting, which
raises an important question: {\em What is the minimum number of terms (or
clauses) required to construct a DNF (or CNF) formula with exactly $k$
satisfying assignments?}
  In this paper, we establish both upper and lower bounds on this question. We
prove that for any natural number $k$, one can construct a monotone DNF formula
with exactly $k$ satisfying assignments using at most $O(\sqrt{\log k}\log\log
k)$ terms. This construction represents the first $o(\log k)$ upper bound for
this problem. We complement this result by showing that there exist infinitely
many values of $k$ for which any DNF or CNF representation requires at least
$\Omega(\log\log k)$ terms or clauses. These results have significant
implications for the efficiency of model counting algorithms based on formula
transformations.

</details>
