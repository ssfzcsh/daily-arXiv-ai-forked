{"id": "2508.21097", "pdf": "https://arxiv.org/pdf/2508.21097", "abs": "https://arxiv.org/abs/2508.21097", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation", "categories": ["cs.SE", "cs.AI"], "comment": "This paper is accepted to the New Ideas and Emerging Results (NIER)\n  track of the ACM/IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS)", "summary": "This paper introduces a novel research direction for model-to-text/code\ntransformations by leveraging Large Language Models (LLMs) that can be enhanced\nwith Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum\nand hybrid quantum-classical software systems, where model-driven approaches\ncan help reduce the costs and mitigate the risks associated with the\nheterogeneous platform landscape and lack of developers' skills. We validate\none of the proposed ideas regarding generating code out of UML model instances\nof software systems. This Python code uses a well-established library, called\nQiskit, to execute on gate-based or circuit-based quantum computers. The RAG\npipeline that we deploy incorporates sample Qiskit code from public GitHub\nrepositories. Experimental results show that well-engineered prompts can\nimprove CodeBLEU scores by up to a factor of four, yielding more accurate and\nconsistent quantum code. However, the proposed research direction can go beyond\nthis through further investigation in the future by conducting experiments to\naddress our other research questions and ideas proposed here, such as deploying\nsoftware system model instances as the source of information in the RAG\npipelines, or deploying LLMs for code-to-code transformations, for instance,\nfor transpilation use cases."}
{"id": "2508.21107", "pdf": "https://arxiv.org/pdf/2508.21107", "abs": "https://arxiv.org/abs/2508.21107", "authors": ["Dongjun Lee", "Changho Hwang", "Kimin Lee"], "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "categories": ["cs.SE", "cs.AI"], "comment": "Code is available at: https://github.com/dgjun32/UTRL", "summary": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task."}
{"id": "2508.21156", "pdf": "https://arxiv.org/pdf/2508.21156", "abs": "https://arxiv.org/abs/2508.21156", "authors": ["Kiana Kiashemshaki", "Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan"], "title": "Automated Bug Triaging using Instruction-Tuned Large Language Models", "categories": ["cs.SE", "D.2.7; I.2.7; I.2.6"], "comment": "11 pages, 7 figures", "summary": "Bug triaging, the task of assigning new issues to developers, is often slow\nand inconsistent in large projects. We present a lightweight framework that\ninstruction-tuned large language model (LLM) with LoRA adapters and uses\ncandidate-constrained decoding to ensure valid assignments. Tested on\nEclipseJDT and Mozilla datasets, the model achieves strong shortlist quality\n(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent\nsnapshots, accuracy rises sharply, showing the framework's potential for\nreal-world, human-in-the-loop triaging. Our results suggest that\ninstruction-tuned LLMs offer a practical alternative to costly feature\nengineering and graph-based methods."}
{"id": "2508.21433", "pdf": "https://arxiv.org/pdf/2508.21433", "abs": "https://arxiv.org/abs/2508.21433", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility"}
{"id": "2508.21095", "pdf": "https://arxiv.org/pdf/2508.21095", "abs": "https://arxiv.org/abs/2508.21095", "authors": ["Thomas Besnier", "Sylvain Arguill√®re", "Mohamed Daoudi"], "title": "ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Unregistered surface meshes, especially raw 3D scans, present significant\nchallenges for automatic computation of plausible deformations due to the lack\nof established point-wise correspondences and the presence of noise in the\ndata. In this paper, we propose a new, rig-free, data-driven framework for\nmotion prediction and transfer on such body meshes. Our method couples a robust\nmotion embedding network with a learned per-vertex feature field to generate a\nspatio-temporal deformation field, which drives the mesh deformation. Extensive\nevaluations, including quantitative benchmarks and qualitative visuals on tasks\nsuch as walking and running, demonstrate the effectiveness and versatility of\nour approach on challenging unregistered meshes."}
{"id": "2508.21516", "pdf": "https://arxiv.org/pdf/2508.21516", "abs": "https://arxiv.org/abs/2508.21516", "authors": ["Federico Chiariotti", "Fabio Saggese", "Andrea Munari", "Leonardo Badia", "Petar Popovski"], "title": "A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting", "categories": ["cs.NI", "94A05", "C.2.1; C.2.5"], "comment": "Submitted to IEEE INFOCOM 2026", "summary": "A digital twin (DT) contains a set of virtual models of real systems and\nprocesses that are synchronized to their physical counterparts. This enables\nexperimentation and examination of counterfactuals, simulating the consequences\nof decisions in real time. However, the DT accuracy relies on timely updates\nthat maintain alignment with the real system. We can distinguish between: (i)\npull-updates, which follow a request from the DT to the sensors, to decrease\nits drift from the physical state; (ii) push-updates, which are sent directly\nby the sensors since they represent urgent information, such as anomalies. In\nthis work, we devise a push-pull scheduler (PPS) medium access framework, which\ndynamically allocates the communication resources used for these two types of\nupdates. Our scheme strikes a balance in the trade-off between DT alignment in\nnormal conditions and anomaly reporting, optimizing resource usage and reducing\nthe drift age of incorrect information (AoII) by over 20% with respect to\nstate-of-the-art solutions, while maintaining the same anomaly detection\nguarantees, as well as reducing the worst-case anomaly detection AoII from 70\nms to 20 ms when considering a 1 ms average drift AoII constraint."}
{"id": "2508.21256", "pdf": "https://arxiv.org/pdf/2508.21256", "abs": "https://arxiv.org/abs/2508.21256", "authors": ["Nripesh Niketan", "Vaatsalya Shrivastva"], "title": "CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation", "categories": ["cs.PL", "cs.CL", "cs.GR", "68N20, 68N15, 68W10", "D.3.4; D.3.2; D.1.3"], "comment": "15 Pages, 5 Figures, 1 Table. Introduces CrossTL, a universal\n  programming language translator enabling bidirectional translation between 8\n  programming languages (CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan\n  SPIR-V, Rust, Mojo) through a unified intermediate representation called\n  CrossGL. Includes comprehensive evaluation with complex real-world examples", "summary": "We present CrossTL, a universal programming language translator enabling\nbidirectional translation between multiple languages through a unified\nintermediate representation called CrossGL. Traditional approaches require\nseparate translators for each language pair, leading to exponential complexity\ngrowth. CrossTL uses a single universal IR to facilitate translations between\nCUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,\nwith Slang support in development. Our system consists of: language-specific\nlexers/parsers converting source code to ASTs, bidirectional CrossGL\ntranslation modules implementing ToCrossGLConverter classes for importing code\nand CodeGen classes for target generation, and comprehensive backend\nimplementations handling full translation pipelines. We demonstrate\neffectiveness through comprehensive evaluation across programming domains,\nachieving successful compilation and execution across all supported backends.\nThe universal IR design enables adding new languages with minimal effort,\nrequiring only language-specific frontend/backend components. Our contributions\ninclude: (1) a unified IR capturing semantics of multiple programming\nparadigms, (2) a modular architecture enabling extensibility, (3) a\ncomprehensive framework supporting GPU compute, graphics programming, and\nsystems languages, and (4) empirical validation demonstrating practical\nviability of universal code translation. CrossTL represents a significant step\ntoward language-agnostic programming, enabling write-once, deploy-everywhere\ndevelopment."}
{"id": "2508.21397", "pdf": "https://arxiv.org/pdf/2508.21397", "abs": "https://arxiv.org/abs/2508.21397", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "lifeXplore at the Lifelog Search Challenge 2020", "categories": ["cs.MM"], "comment": null, "summary": "Since its first iteration in 2018, the Lifelog Search Challenge (LSC) - an\ninteractive competition for retrieving lifelogging moments - is co-located at\nthe annual ACM International Conference on Multimedia Retrieval (ICMR) and has\ndrawn international attention. With the goal of making an ever growing public\nlifelogging dataset searchable, several teams develop systems for quickly\nsolving time-limited queries during the challenge. Having participated in both\nprevious LSC iterations, i.e. LSC2018 and LSC2019, we present our lifeXplore\nsystem - a video exploration and retrieval tool combining feature map browsing,\nconcept search and filtering as well as hand-drawn sketching. The system is\nimproved by including additional deep concept YOLO9000, optical character\nrecognition (OCR) as well as adding uniform sampling as an alternative to the\nsystem's traditional underlying shot segmentation."}
{"id": "2508.21485", "pdf": "https://arxiv.org/pdf/2508.21485", "abs": "https://arxiv.org/abs/2508.21485", "authors": ["Johannes Kloibhofer", "Valentina Trucco Dalmas", "Yde Venema"], "title": "Interpolation for Converse PDL", "categories": ["cs.LO"], "comment": "Accepted at TABLEAUX 2025", "summary": "Converse PDL is the extension of propositional dynamic logic with a converse\noperation on programs. Our main result states that Converse PDL enjoys the\n(local) Craig Interpolation Property, with respect to both atomic programs and\npropositional variables. As a corollary we establish the Beth Definability\nProperty for the logic.\n  Our interpolation proof is based on an adaptation of Maehara's\nproof-theoretic method. For this purpose we introduce a sound and complete\ncyclic sequent system for this logic. This calculus features an analytic cut\nrule and uses a focus mechanism for recognising successful cycles."}
{"id": "2508.21087", "pdf": "https://arxiv.org/pdf/2508.21087", "abs": "https://arxiv.org/abs/2508.21087", "authors": ["Bin Han", "Deuksin Kwon", "Spencer Lin", "Kaleen Shrestha", "Jonathan Gratch"], "title": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?", "categories": ["cs.HC"], "comment": null, "summary": "This study proposes a framework that employs personality prompting with Large\nLanguage Models to generate verbal and nonverbal behaviors for virtual agents\nbased on personality traits. Focusing on extraversion, we evaluated the system\nin two scenarios: negotiation and ice breaking, using both introverted and\nextroverted agents. In Experiment 1, we conducted agent to agent simulations\nand performed linguistic analysis and personality classification to assess\nwhether the LLM generated language reflected the intended traits and whether\nthe corresponding nonverbal behaviors varied by personality. In Experiment 2,\nwe carried out a user study to evaluate whether these personality aligned\nbehaviors were consistent with their intended traits and perceptible to human\nobservers. Our results show that LLMs can generate verbal and nonverbal\nbehaviors that align with personality traits, and that users are able to\nrecognize these traits through the agents' behaviors. This work underscores the\npotential of LLMs in shaping personality aligned virtual agents."}
{"id": "2508.21219", "pdf": "https://arxiv.org/pdf/2508.21219", "abs": "https://arxiv.org/abs/2508.21219", "authors": ["A H M Nazmus Sakib", "Mahsin Bin Akram", "Joseph Spracklen", "Sahan Kalutarage", "Raveen Wijewickrama", "Igor Bilogrevic", "Murtuza Jadliwala"], "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation", "categories": ["cs.CR", "cs.ET", "cs.PL"], "comment": null, "summary": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks."}
{"id": "2508.21304", "pdf": "https://arxiv.org/pdf/2508.21304", "abs": "https://arxiv.org/abs/2508.21304", "authors": ["Joanie Hayoun Chung", "Chaemyung Lim", "Sumin Lee", "Sungbin Lim"], "title": "ORCA: ORchestrating Causal Agent", "categories": ["cs.DB", "cs.MA"], "comment": "24 pages, 17 figures, 1 table", "summary": "Causal inference is essential for decision-making science while the\ncomplexity of the data analysis workflow, ranging from data wrangling to causal\nanalysis, increases substantially as the scale of data grows in complicated\nbusiness environments. Especially, the execution of the workflow in relational\ndatabases by non-experts can result in repetitive bottlenecks which impede\ntimely and responsible business insights. To address this challenge, we propose\nORCA (Orchestrating Causal Agent), an LLM agentic system that can automate\nroutine workflows in RDBMS while preserving expert oversight via human-AI\ninteractions. ORCA orchestrates the full data analysis pipeline: interpreting\nnatural language queries, navigating tables from DB servers, generating proper\nSQL codes, preprocessing data, and configuring modeling processes using causal\ninference libraries. Domain experts still can control the automation through\niterative interactions with ORCA, enabling robust data-driven decision making\nwith less technical expertise in statistical computing. Empirical evaluations\non benchmark and synthetic e-commerce datasets demonstrate competitive\nperformance of ORCA in table understanding, query generation, and cause-effect\nestimation -- achieving over $7\\times$ improvement in estimating average\ntreatment compared to GPT-4o mini."}
{"id": "2508.21265", "pdf": "https://arxiv.org/pdf/2508.21265", "abs": "https://arxiv.org/abs/2508.21265", "authors": ["Sasan Razmkhah", "Mingye Li", "Zeming Cheng", "Robert S. Aviles", "Kyle Jackman", "Joey Delport", "Lieze Schindler", "Wenhui Luo", "Takuya Suzuki", "Mehdi Kamal", "Christopher L. Ayala", "Coenrad J. Fourie", "Nabuyuki Yoshikawa", "Peter A. Beerel", "Sandeep Gupta", "Massoud Pedram"], "title": "SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics", "categories": ["cs.AR", "cond-mat.supr-con", "cs.CR", "cs.ET"], "comment": "13 pages, 22 figures", "summary": "This research explores the use of superconductor electronics (SCE) for\naccelerating fully homomorphic encryption (FHE), focusing on the\nNumber-Theoretic Transform (NTT), a key computational bottleneck in FHE\nschemes. We present SCE-NTT, a dedicated hardware accelerator based on\nsuperconductive single flux quantum (SFQ) logic and memory, targeting high\nperformance and energy efficiency beyond the limits of conventional CMOS. To\naddress SFQ constraints such as limited dense RAM and restricted fanin/fanout,\nwe propose a deeply pipelined NTT-128 architecture using shift register memory\n(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7\nprocessing elements (PEs), each featuring a butterfly unit (BU), dual\ncoefficient memories operating in ping-pong mode via FIFO-based SRM queues, and\ntwiddle factor buffers. The BU integrates a Shoup modular multiplier optimized\nfor a small area, leveraging precomputed twiddle factors. A new RSFQ cell\nlibrary with over 50 parameterized cells, including compound logic units, was\ndeveloped for implementation. Functional and timing correctness were validated\nusing JoSIM analog simulations and Verilog models. A multiphase clocking scheme\nwas employed to enhance robustness and reduce path-balancing overhead,\nimproving circuit reliability. Fabricated results show the NTT-128 unit\nachieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art\nCMOS equivalents. We also project that the architecture can scale to larger\nsizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput\nis estimated at 1.63 million operations/sec, significantly exceeding existing\nhardware. These results demonstrate the strong potential of SCE-based\naccelerators for scalable, energy-efficient secure computation in the\npost-quantum era, with further gains anticipated through advances in\nfabrication."}
{"id": "2508.21230", "pdf": "https://arxiv.org/pdf/2508.21230", "abs": "https://arxiv.org/abs/2508.21230", "authors": ["Brian Curless", "Michael Gowanlock"], "title": "Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores", "categories": ["cs.DC", "cs.PF"], "comment": "To appear in the proceedings of the International Conference on\n  Parallel Processing 2025", "summary": "Modern GPUs are equipped with tensor cores (TCs) that are commonly used for\nmatrix multiplication in artificial intelligence workloads. However, because\nthey have high computational throughput, they can lead to significant\nperformance gains in other algorithms if they can be successfully exploited. We\nexamine using TCs to compute Euclidean distance calculations, which are used in\nmany data analytics applications. Prior work has only investigated using 64 bit\nfloating point (FP64) data for computation; however, TCs can operate on lower\nprecision floating point data (i.e., 16 bit matrix multiplication and 32 bit\naccumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so\nhigh that TCs are easily starved of data. We propose a Fast and Scalable Tensor\ncore Euclidean Distance (FaSTED) algorithm. To achieve high computational\nthroughput, we design FaSTED for significant hierarchical reuse of data and\nmaximize memory utilization at every level (global memory, shared memory, and\nregisters). We apply FaSTED to the application of similarity searches, which\ntypically employ an indexing data structure to eliminate superfluous Euclidean\ndistance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean\ndistance algorithm in the literature that employs FP64, as well as to two\nsingle precision (FP32) CUDA core algorithms that both employ an index. We find\nthat across four real-world high-dimensional datasets spanning 128-960\ndimensions, the mixed-precision brute force approach achieves a speedup over\nthe SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed\nprecision algorithm to be less than <0.06% when compared to the FP64 baseline."}
{"id": "2508.21454", "pdf": "https://arxiv.org/pdf/2508.21454", "abs": "https://arxiv.org/abs/2508.21454", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Enhancing Semantic Understanding in Pointer Analysis using Large Language Models", "categories": ["cs.SE"], "comment": "Accepted by LMPL 2025", "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision."}
{"id": "2508.21344", "pdf": "https://arxiv.org/pdf/2508.21344", "abs": "https://arxiv.org/abs/2508.21344", "authors": ["Jeong Uk Lee", "Sung Hee Choi"], "title": "ARGS: Advanced Regularization on Aligning Gaussians over the Surface", "categories": ["cs.GR", "cs.CV"], "comment": "9 pages, 4 figures", "summary": "Reconstructing high-quality 3D meshes and visuals from 3D Gaussian\nSplatting(3DGS) still remains a central challenge in computer graphics.\nAlthough existing models such as SuGaR offer effective solutions for rendering,\nthere is is still room to improve improve both visual fidelity and scene\nconsistency. This work builds upon SuGaR by introducing two complementary\nregularization strategies that address common limitations in both the shape of\nindividual Gaussians and the coherence of the overall surface. The first\nstrategy introduces an effective rank regularization, motivated by recent\nstudies on Gaussian primitive structures. This regularization discourages\nextreme anisotropy-specifically, \"needle-like\" shapes-by favoring more\nbalanced, \"disk-like\" forms that are better suited for stable surface\nreconstruction. The second strategy integrates a neural Signed Distance\nFunction (SDF) into the optimization process. The SDF is regularized with an\nEikonal loss to maintain proper distance properties and provides a continuous\nglobal surface prior, guiding Gaussians toward better alignment with the\nunderlying geometry. These two regularizations aim to improve both the fidelity\nof individual Gaussian primitives and their collective surface behavior. The\nfinal model can make more accurate and coherent visuals from 3DGS data."}
{"id": "2508.21783", "pdf": "https://arxiv.org/pdf/2508.21783", "abs": "https://arxiv.org/abs/2508.21783", "authors": ["Mohamed Seliem", "Utz Roedig", "Cormac Sreenan", "Dirk Pesch"], "title": "QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective", "categories": ["cs.NI"], "comment": "(c) 2025 IEEE. This is the author's version of a paper accepted for\n  presentation at the IEEE MSWiM 2025 conference. The final version will appear\n  in the conference proceedings", "summary": "Private 5G networks are emerging as key enablers for smart factories, where a\nsingle device often handles multiple concurrent traffic flows with distinct\nQuality of Service (QoS) requirements. Existing simulation frameworks, however,\nlack the fidelity to model such multi-flow behavior at the QoS Flow Identifier\n(QFI) level. This paper addresses this gap by extending Simu5G to support\nper-QFI modeling and by introducing a novel QoS-aware Proportional Fairness\n(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit\nRate (GBR), and priority metrics to optimize resource allocation across\nheterogeneous flows. We evaluate the proposed approach in a realistic smart\nfactory scenario featuring edge-hosted machine vision, real-time control loops,\nand bulk data transfer. Results show that QoS-PF improves deadline adherence\nand fairness without compromising throughput. All extensions are implemented in\na modular and open-source manner to support future research. Our work provides\nboth a methodological and architectural foundation for simulating and analyzing\nadvanced QoS policies in industrial 5G deployments."}
{"id": "2508.21593", "pdf": "https://arxiv.org/pdf/2508.21593", "abs": "https://arxiv.org/abs/2508.21593", "authors": ["Anne Baanen", "Matthew Robert Ballard", "Johan Commelin", "Bryan Gin-ge Chen", "Michael Rothgang", "Damiano Testa"], "title": "Growing Mathlib: maintenance of a large scale mathematical library", "categories": ["cs.PL", "cs.MS", "math.HO"], "comment": "21 pages, 1 figure. To appear at Conference on Intelligent Computer\n  Mathematics (CICM) 2025", "summary": "The Lean mathematical library Mathlib is one of the fastest-growing libraries\nof formalised mathematics. We describe various strategies to manage this\ngrowth, while allowing for change and avoiding maintainer overload. This\nincludes dealing with breaking changes via a deprecation system, using code\nquality analysis tools (linters) to provide direct user feedback about common\npitfalls, speeding up compilation times through conscious library (re-)design,\ndealing with technical debt as well as writing custom tooling to help with the\nreview and triage of new contributions."}
{"id": "2508.21398", "pdf": "https://arxiv.org/pdf/2508.21398", "abs": "https://arxiv.org/abs/2508.21398", "authors": ["Andreas Leibetseder", "Sabrina Kletz", "Klaus Schoeffmann", "Simon Keckstein", "J√∂rg Keckstein"], "title": "GLENDA: Gynecologic Laparoscopy Endometriosis Dataset", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is\nperformed via a live feed of a patient's abdomen surveying the insertion and\nhandling of various instruments for conducting treatment. Adopting this kind of\nsurgical intervention not only facilitates a great variety of treatments, the\npossibility of recording said video streams is as well essential for numerous\npost-surgical activities, such as treatment planning, case documentation and\neducation. Nonetheless, the process of manually analyzing surgical recordings,\nas it is carried out in current practice, usually proves tediously\ntime-consuming. In order to improve upon this situation, more sophisticated\ncomputer vision as well as machine learning approaches are actively developed.\nSince most of such approaches heavily rely on sample data, which especially in\nthe medical field is only sparsely available, with this work we publish the\nGynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset\ncontaining region-based annotations of a common medical condition named\nendometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the\nfirst of its kind and it has been created in collaboration with leading medical\nexperts in the field."}
{"id": "2508.21807", "pdf": "https://arxiv.org/pdf/2508.21807", "abs": "https://arxiv.org/abs/2508.21807", "authors": ["Maryanthe Malliaris", "Olga Medrano Mart√≠n del Campo", "Shay Moran"], "title": "Epsilon-saturation for stable graphs and Littlestone classes", "categories": ["math.LO", "cs.DM", "cs.LO", "math.CO"], "comment": "30 pages, 2 figures", "summary": "Any Littlestone class, or stable graph, has finite sets which function as\n``virtual elements'': these can be seen from the learning side as representing\nhypotheses which are expressible as weighted majority opinions of hypotheses in\nthe class, and from the model-theoretic side as an approximate finitary version\nof realizing types. We introduce and study the epsilon-saturation of a\nLittlestone class, or stable graph, which is essentially the closure of the\nclass under inductively adding all such virtual elements. We characterize this\nclosure and prove that under reasonable choices of parameters, it remains\nLittlestone (or stable), though not always of the same Littlestone dimension.\nThis highlights some surprising phenomena having to do with regimes of epsilon\nand the relation between Littlestone/stability and VC dimension."}
{"id": "2508.21209", "pdf": "https://arxiv.org/pdf/2508.21209", "abs": "https://arxiv.org/abs/2508.21209", "authors": ["Vanessa Figueiredo"], "title": "Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses", "categories": ["cs.HC", "cs.CL", "I.2.1; H.5.2"], "comment": null, "summary": "This paper presents two studies on how Brazilian children (ages 9--11) use\nconversational agents (CAs) for schoolwork, discovery, and entertainment, and\nhow structured scaffolds can enhance these interactions. In Study 1, a\nseven-week online investigation with 23 participants (children, parents,\nteachers) employed interviews, observations, and Cognitive Work Analysis to map\nchildren's information-processing flows, the role of more knowledgeable others,\nfunctional uses, contextual goals, and interaction patterns to inform\nconversation-tree design. We identified three CA functions: School, Discovery,\nEntertainment, and derived ``recipe'' scaffolds mirroring parent-child support.\nIn Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,\ncomparing conversation-tree recipes based on structured-prompting to an\nunstructured baseline. Quantitative evaluation of readability, question\ncount/depth/diversity, and coherence revealed gains for the recipe approach.\nBuilding on these findings, we offer design recommendations: scaffolded\nconversation-trees, child-dedicated profiles for personalized context, and\ncaregiver-curated content. Our contributions include the first CWA application\nwith Brazilian children, an empirical framework of child-CA information flows,\nand an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,\nscaffolded learning."}
{"id": "2508.21265", "pdf": "https://arxiv.org/pdf/2508.21265", "abs": "https://arxiv.org/abs/2508.21265", "authors": ["Sasan Razmkhah", "Mingye Li", "Zeming Cheng", "Robert S. Aviles", "Kyle Jackman", "Joey Delport", "Lieze Schindler", "Wenhui Luo", "Takuya Suzuki", "Mehdi Kamal", "Christopher L. Ayala", "Coenrad J. Fourie", "Nabuyuki Yoshikawa", "Peter A. Beerel", "Sandeep Gupta", "Massoud Pedram"], "title": "SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics", "categories": ["cs.AR", "cond-mat.supr-con", "cs.CR", "cs.ET"], "comment": "13 pages, 22 figures", "summary": "This research explores the use of superconductor electronics (SCE) for\naccelerating fully homomorphic encryption (FHE), focusing on the\nNumber-Theoretic Transform (NTT), a key computational bottleneck in FHE\nschemes. We present SCE-NTT, a dedicated hardware accelerator based on\nsuperconductive single flux quantum (SFQ) logic and memory, targeting high\nperformance and energy efficiency beyond the limits of conventional CMOS. To\naddress SFQ constraints such as limited dense RAM and restricted fanin/fanout,\nwe propose a deeply pipelined NTT-128 architecture using shift register memory\n(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7\nprocessing elements (PEs), each featuring a butterfly unit (BU), dual\ncoefficient memories operating in ping-pong mode via FIFO-based SRM queues, and\ntwiddle factor buffers. The BU integrates a Shoup modular multiplier optimized\nfor a small area, leveraging precomputed twiddle factors. A new RSFQ cell\nlibrary with over 50 parameterized cells, including compound logic units, was\ndeveloped for implementation. Functional and timing correctness were validated\nusing JoSIM analog simulations and Verilog models. A multiphase clocking scheme\nwas employed to enhance robustness and reduce path-balancing overhead,\nimproving circuit reliability. Fabricated results show the NTT-128 unit\nachieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art\nCMOS equivalents. We also project that the architecture can scale to larger\nsizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput\nis estimated at 1.63 million operations/sec, significantly exceeding existing\nhardware. These results demonstrate the strong potential of SCE-based\naccelerators for scalable, energy-efficient secure computation in the\npost-quantum era, with further gains anticipated through advances in\nfabrication."}
{"id": "2508.21682", "pdf": "https://arxiv.org/pdf/2508.21682", "abs": "https://arxiv.org/abs/2508.21682", "authors": ["Yasunobu Imamura", "Takeshi Shinohara", "Naoya Higuchi", "Kouichi Hirata", "Tetsuji Kuboyama"], "title": "Hilbert Forest in the SISAP 2025 Indexing Challenge", "categories": ["cs.DB", "cs.DS"], "comment": "7 pages", "summary": "We report our participation in the SISAP 2025 Indexing Challenge using a\nnovel indexing technique called the Hilbert forest. The method is based on the\nfast Hilbert sort algorithm, which efficiently orders high-dimensional points\nalong a Hilbert space-filling curve, and constructs multiple Hilbert trees to\nsupport approximate nearest neighbor search. We submitted implementations to\nboth Task 1 (approximate search on the PUBMED23 dataset) and Task 2 (k-nearest\nneighbor graph construction on the GOOAQ dataset) under the official resource\nconstraints of 16 GB RAM and 8 CPU cores. The Hilbert forest demonstrated\ncompetitive performance in Task 1 and achieved the fastest construction time in\nTask 2 while satisfying the required recall levels. These results highlight the\npractical effectiveness of Hilbert order-based indexing under strict memory\nlimitations."}
{"id": "2508.21267", "pdf": "https://arxiv.org/pdf/2508.21267", "abs": "https://arxiv.org/abs/2508.21267", "authors": ["Devon Lister", "Prabhu Vellaisamy", "John Paul Shen", "Di Wu"], "title": "Catwalk: Unary Top-K for Efficient Ramp-No-Leak Neuron Design for Temporal Neural Networks", "categories": ["cs.AR", "cs.NE"], "comment": "Amar Mukherjee Best Paper Award of ISVLSI 2025", "summary": "Temporal neural networks (TNNs) are neuromorphic neural networks that utilize\nbit-serial temporal coding. TNNs are composed of columns, which in turn employ\nneurons as their building blocks. Each neuron processes volleys of input\nspikes, modulated by associated synaptic weights, on its dendritic inputs.\nRecently proposed neuron implementation in CMOS employs a Spike Response Model\n(SRM) with a ramp-no-leak (RNL) response function and assumes all the inputs\ncan carry spikes. However, in actual spike volleys, only a small subset of the\ndendritic inputs actually carry spikes in each compute cycle. This form of\nsparsity can be exploited to achieve better hardware efficiency. In this paper,\nwe propose a Catwalk neuron implementation by relocating spikes in a spike\nvolley as a sorted subset cluster via unary top-k. Such relocation can\nsignificantly reduce the cost of the subsequent parallel counter (PC) for\naccumulating the response functions from the spiking inputs. This can lead to\nimprovements on area and power efficiency in RNL neuron implementation.\nPlace-and-route results show Catwalk is 1.39x and 1.86x better in area and\npower, respectively, as compared to existing SRM0-RNL neurons."}
{"id": "2508.21286", "pdf": "https://arxiv.org/pdf/2508.21286", "abs": "https://arxiv.org/abs/2508.21286", "authors": ["Changheng Wang", "Zhiqing Wei", "Lizhe Liu", "Qiao Deng", "Yingda Wu", "Yangyang Niu", "Yashan Pang", "Zhiyong Feng"], "title": "Decentralized Federated Averaging via Random Walk", "categories": ["cs.DC"], "comment": null, "summary": "Federated Learning (FL) is a communication-efficient distributed machine\nlearning method that allows multiple devices to collaboratively train models\nwithout sharing raw data. FL can be categorized into centralized and\ndecentralized paradigms. The centralized paradigm relies on a central server to\naggregate local models, potentially resulting in single points of failure,\ncommunication bottlenecks, and exposure of model parameters. In contrast, the\ndecentralized paradigm, which does not require a central server, provides\nimproved robustness and privacy. The essence of federated learning lies in\nleveraging multiple local updates for efficient communication. However, this\napproach may result in slower convergence or even convergence to suboptimal\nmodels in the presence of heterogeneous and imbalanced data. To address this\nchallenge, we study decentralized federated averaging via random walk (DFedRW),\nwhich replaces multiple local update steps on a single device with random walk\nupdates. Traditional Federated Averaging (FedAvg) and its decentralized\nversions commonly ignore stragglers, which reduces the amount of training data\nand introduces sampling bias. Therefore, we allow DFedRW to aggregate partial\nrandom walk updates, ensuring that each computation contributes to the model\nupdate. To further improve communication efficiency, we also propose a\nquantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves\nconvergence upper bound of order $\\mathcal{O}(\\frac{1}{k^{1-q}})$ under convex\nconditions. Furthermore, we propose a sufficient condition that reveals when\nquantization balances communication and convergence. Numerical analysis\nindicates that our proposed algorithms outperform (decentralized) FedAvg in\nboth convergence rate and accuracy, achieving a 38.3\\% and 37.5\\% increase in\ntest accuracy under high levels of heterogeneities."}
{"id": "2508.21553", "pdf": "https://arxiv.org/pdf/2508.21553", "abs": "https://arxiv.org/abs/2508.21553", "authors": ["J√∏rn Eirik Betten", "Quentin Mazouni", "Dennis Gross", "Pedro Lind", "Helge Spieker"], "title": "Reusable Test Suites for Reinforcement Learning", "categories": ["cs.SE"], "comment": null, "summary": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors."}
{"id": "2508.21256", "pdf": "https://arxiv.org/pdf/2508.21256", "abs": "https://arxiv.org/abs/2508.21256", "authors": ["Nripesh Niketan", "Vaatsalya Shrivastva"], "title": "CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation", "categories": ["cs.PL", "cs.CL", "cs.GR", "68N20, 68N15, 68W10", "D.3.4; D.3.2; D.1.3"], "comment": "15 Pages, 5 Figures, 1 Table. Introduces CrossTL, a universal\n  programming language translator enabling bidirectional translation between 8\n  programming languages (CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan\n  SPIR-V, Rust, Mojo) through a unified intermediate representation called\n  CrossGL. Includes comprehensive evaluation with complex real-world examples", "summary": "We present CrossTL, a universal programming language translator enabling\nbidirectional translation between multiple languages through a unified\nintermediate representation called CrossGL. Traditional approaches require\nseparate translators for each language pair, leading to exponential complexity\ngrowth. CrossTL uses a single universal IR to facilitate translations between\nCUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,\nwith Slang support in development. Our system consists of: language-specific\nlexers/parsers converting source code to ASTs, bidirectional CrossGL\ntranslation modules implementing ToCrossGLConverter classes for importing code\nand CodeGen classes for target generation, and comprehensive backend\nimplementations handling full translation pipelines. We demonstrate\neffectiveness through comprehensive evaluation across programming domains,\nachieving successful compilation and execution across all supported backends.\nThe universal IR design enables adding new languages with minimal effort,\nrequiring only language-specific frontend/backend components. Our contributions\ninclude: (1) a unified IR capturing semantics of multiple programming\nparadigms, (2) a modular architecture enabling extensibility, (3) a\ncomprehensive framework supporting GPU compute, graphics programming, and\nsystems languages, and (4) empirical validation demonstrating practical\nviability of universal code translation. CrossTL represents a significant step\ntoward language-agnostic programming, enabling write-once, deploy-everywhere\ndevelopment."}
{"id": "2508.21480", "pdf": "https://arxiv.org/pdf/2508.21480", "abs": "https://arxiv.org/abs/2508.21480", "authors": ["Narges Dadkhah", "Khan Reaz", "Gerhard Wunder"], "title": "Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "The increasing adoption of smart home devices and IoT-based security systems\npresents significant opportunities to enhance convenience, safety, and risk\nmanagement for homeowners and service providers. However, secure\nonboarding-provisioning credentials and establishing trust with cloud\nplatforms-remains a considerable challenge. Traditional onboarding methods\noften rely on centralized Public Key Infrastructure (PKI) models and\nmanufacturer-controlled keys, which introduce security risks and limit the\nuser's digital sovereignty. These limitations hinder the widespread deployment\nof scalable IoT solutions. This paper presents a novel onboarding framework\nthat builds upon existing network-layer onboarding techniques and extends them\nto the application layer to address these challenges. By integrating consortium\nblockchain technology, we propose a decentralized onboarding mechanism that\nenhances transparency, security, and monitoring for smart home architectures.\nThe architecture supports device registration, key revocation, access control\nmanagement, and risk detection through event-driven alerts across dedicated\nblockchain channels and smart contracts. To evaluate the framework, we formally\nmodel the protocol using the Tamarin Prover under the Dolev-Yao adversary\nmodel. The analysis focuses on authentication, token integrity, key\nconfidentiality, and resilience over public channels. A prototype\nimplementation demonstrates the system's viability in smart home settings, with\nverification completing in 0.34 seconds, highlighting its scalability and\nsuitability for constrained devices and diverse stakeholders. Additionally,\nperformance evaluation shows that the blockchain-based approach effectively\nhandles varying workloads, maintains high throughput and low latency, and\nsupports near real-time IoT data processing."}
{"id": "2508.21219", "pdf": "https://arxiv.org/pdf/2508.21219", "abs": "https://arxiv.org/abs/2508.21219", "authors": ["A H M Nazmus Sakib", "Mahsin Bin Akram", "Joseph Spracklen", "Sahan Kalutarage", "Raveen Wijewickrama", "Igor Bilogrevic", "Murtuza Jadliwala"], "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation", "categories": ["cs.CR", "cs.ET", "cs.PL"], "comment": null, "summary": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks."}
{"id": "2508.21399", "pdf": "https://arxiv.org/pdf/2508.21399", "abs": "https://arxiv.org/abs/2508.21399", "authors": ["Sabrina Kletz", "Klaus Schoeffmann", "Jenny Benois-Pineau", "Heinrich Husslein"], "title": "Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recorded videos from surgeries have become an increasingly important\ninformation source for the field of medical endoscopy, since the recorded\nfootage shows every single detail of the surgery. However, while video\nrecording is straightforward these days, automatic content indexing - the basis\nfor content-based search in a medical video archive - is still a great\nchallenge due to the very special video content. In this work, we investigate\nsegmentation and recognition of surgical instruments in videos recorded from\nlaparoscopic gynecology. More precisely, we evaluate the achievable performance\nof segmenting surgical instruments from their background by using a\nregion-based fully convolutional network for instance-aware (1) instrument\nsegmentation as well as (2) instrument recognition. While the first part\naddresses only binary segmentation of instances (i.e., distinguishing between\ninstrument or background) we also investigate multi-class instrument\nrecognition (i.e., identifying the type of instrument). Our evaluation results\nshow that even with a moderately low number of training examples, we are able\nto localize and segment instrument regions with a pretty high accuracy.\nHowever, the results also reveal that determining the particular instrument is\nstill very challenging, due to the inherently high similarity of surgical\ninstruments."}
{"id": "2508.21283", "pdf": "https://arxiv.org/pdf/2508.21283", "abs": "https://arxiv.org/abs/2508.21283", "authors": ["Jose Manuel Alcalde-Llergo", "Andrea Zingoni", "Pilar Aparicio-Martinez", "Sara Pinzi", "Enrique Yeguas-Bolivar"], "title": "Design and evaluation of a serious game in virtual reality to increase empathy towards students with phonological dyslexia", "categories": ["cs.HC"], "comment": "29 pages, 3 tables, 7 figures", "summary": "Dyslexia is a neurodevelopmental disorder estimated to strike approximately 5\nto 10 per cent of the population. In particular, phonological dyslexia causes\nproblems in connecting the sounds of words with their written forms.\nConsequently, affected individuals may encounter issues such as slow reading\nspeed, inaccurate reading, and difficulty decoding unfamiliar words. To address\nthese complexities, the use of compensatory tools and strategies is essential\nto ensure equitable opportunities for dyslexic students. However, the general\nunderestimation of the issue and lack of awareness regarding the significance\nof support methodologies pose significant obstacles. One of the ways to enhance\nconsciousness towards a certain issue is by stimulating empathy with whom is\naffected by it. In light of this, this study introduces a serious game in\nvirtual reality, targeted at educators, students, and, in general, at the\nnon-dyslexic community. The game seeks to enhance understanding of the\nchallenges that individuals with dyslexia experience daily, highlighting the\nrelevance of supportive measures. This approach encourages players to empathize\nwith the struggles of dyslexic individuals and to learn firsthand the\nimportance of supportive methodologies. The final version of the experience was\ntested by 101 participants and evaluated through a specific collection of\nquestionnaires validated in the literature. The results show that using the\nproposed virtual reality tool to promote empathy for individuals with\nphonological dyslexia is highly effective, leading to an average 20 per cent\nincrease in participants' empathy after playing the game."}
{"id": "2508.21493", "pdf": "https://arxiv.org/pdf/2508.21493", "abs": "https://arxiv.org/abs/2508.21493", "authors": ["Yaman Umuroglu", "Christoph Berganski", "Felix Jentzsch", "Michal Danilowicz", "Tomasz Kryjak", "Charalampos Bezaitis", "Magnus Sjalander", "Ian Colbert", "Thomas Preusser", "Jakoba Petri-Koenig", "Michaela Blott"], "title": "SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural Network Accelerators", "categories": ["cs.AR"], "comment": "Submitted to ACM TRETS Special Issue on Open-Source Tools for\n  Reconfigurable Devices and Systems", "summary": "While neural network quantization effectively reduces the cost of matrix\nmultiplications, aggressive quantization can expose non-matrix-multiply\noperations as significant performance and resource bottlenecks on embedded\nsystems. Addressing such bottlenecks requires a comprehensive approach to\ntailoring the precision across operations in the inference computation. To this\nend, we introduce scaled-integer range analysis (SIRA), a static analysis\ntechnique employing interval arithmetic to determine the range, scale, and bias\nfor tensors in quantized neural networks. We show how this information can be\nexploited to reduce the resource footprint of FPGA dataflow neural network\naccelerators via tailored bitwidth adaptation for accumulators and downstream\noperations, aggregation of scales and biases, and conversion of consecutive\nelementwise operations to thresholding operations. We integrate SIRA-driven\noptimizations into the open-source FINN framework, then evaluate their\neffectiveness across a range of quantized neural network workloads and compare\nimplementation alternatives for non-matrix-multiply operations. We demonstrate\nan average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator\nbitwidths with SIRA optimizations, providing detailed benchmark analysis and\nanalytical models to guide the implementation style for non-matrix layers.\nFinally, we open-source SIRA to facilitate community exploration of its\nbenefits across various applications and hardware platforms."}
{"id": "2508.21289", "pdf": "https://arxiv.org/pdf/2508.21289", "abs": "https://arxiv.org/abs/2508.21289", "authors": ["Val√©rie Hayot-Sasson", "Nathaniel Hudson", "Andr√© Bauer", "Maxime Gonthier", "Ian Foster", "Kyle Chard"], "title": "Addressing Reproducibility Challenges in HPC with Continuous Integration", "categories": ["cs.DC", "cs.SE"], "comment": null, "summary": "The high-performance computing (HPC) community has adopted incentive\nstructures to motivate reproducible research, with major conferences awarding\nbadges to papers that meet reproducibility requirements. Yet, many papers do\nnot meet such requirements. The uniqueness of HPC infrastructure and software,\ncoupled with strict access requirements, may limit opportunities for\nreproducibility. In the absence of resource access, we believe that regular\ndocumented testing, through continuous integration (CI), coupled with complete\nprovenance information, can be used as a substitute. Here, we argue that better\nHPC-compliant CI solutions will improve reproducibility of applications. We\npresent a survey of reproducibility initiatives and describe the barriers to\nreproducibility in HPC. To address existing limitations, we present a GitHub\nAction, CORRECT, that enables secure execution of tests on remote HPC\nresources. We evaluate CORRECT's usability across three different types of HPC\napplications, demonstrating the effectiveness of using CORRECT for automating\nand documenting reproducibility evaluations."}
{"id": "2508.21634", "pdf": "https://arxiv.org/pdf/2508.21634", "abs": "https://arxiv.org/abs/2508.21634", "authors": ["Domenico Cotroneo", "Cristina Improta", "Pietro Liguori"], "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity", "categories": ["cs.SE"], "comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)", "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming."}
{"id": "2508.21675", "pdf": "https://arxiv.org/pdf/2508.21675", "abs": "https://arxiv.org/abs/2508.21675", "authors": ["Jonathan Tonglet", "Jan Zimny", "Tinne Tuytelaars", "Iryna Gurevych"], "title": "Is this chart lying to me? Automating the detection of misleading visualizations", "categories": ["cs.CL", "cs.CV", "cs.GR"], "comment": "Preprint under review. Code and data available at:\n  https://github.com/UKPLab/arxiv2025-misviz", "summary": "Misleading visualizations are a potent driver of misinformation on social\nmedia and the web. By violating chart design principles, they distort data and\nlead readers to draw inaccurate conclusions. Prior work has shown that both\nhumans and multimodal large language models (MLLMs) are frequently deceived by\nsuch visualizations. Automatically detecting misleading visualizations and\nidentifying the specific design rules they violate could help protect readers\nand reduce the spread of misinformation. However, the training and evaluation\nof AI models has been limited by the absence of large, diverse, and openly\navailable datasets. In this work, we introduce Misviz, a benchmark of 2,604\nreal-world visualizations annotated with 12 types of misleaders. To support\nmodel training, we also release Misviz-synth, a synthetic dataset of 81,814\nvisualizations generated using Matplotlib and based on real-world data tables.\nWe perform a comprehensive evaluation on both datasets using state-of-the-art\nMLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that\nthe task remains highly challenging. We release Misviz, Misviz-synth, and the\naccompanying code."}
{"id": "2508.21558", "pdf": "https://arxiv.org/pdf/2508.21558", "abs": "https://arxiv.org/abs/2508.21558", "authors": ["Federica Bianchi", "Edoardo Di Paolo", "Angelo Spognardi"], "title": "Generalized Encrypted Traffic Classification Using Inter-Flow Signals", "categories": ["cs.CR", "cs.NI"], "comment": "Accepted manuscript at Availability, Reliability and Security (ARES\n  2025), published in Lecture Notes in Computer Science, vol. 15992, Springer,\n  Cham. DOI: https://doi.org/10.1007/978-3-032-00624-0_11", "summary": "In this paper, we present a novel encrypted traffic classification model that\noperates directly on raw PCAP data without requiring prior assumptions about\ntraffic type. Unlike existing methods, it is generalizable across multiple\nclassification tasks and leverages inter-flow signals - an innovative\nrepresentation that captures temporal correlations and packet volume\ndistributions across flows. Experimental results show that our model\noutperforms well-established methods in nearly every classification task and\nacross most datasets, achieving up to 99% accuracy in some cases, demonstrating\nits robustness and adaptability."}
{"id": "2508.21761", "pdf": "https://arxiv.org/pdf/2508.21761", "abs": "https://arxiv.org/abs/2508.21761", "authors": ["Xavier Juanola", "Giovana Morais", "Magdalena Fuentes", "Gloria Haro"], "title": "Learning from Silence and Noise for Visual Sound Source Localization", "categories": ["cs.CV", "cs.MM"], "comment": "10 pages, 2 figures, 4 tables + Supplementary Material", "summary": "Visual sound source localization is a fundamental perception task that aims\nto detect the location of sounding sources in a video given its audio. Despite\nrecent progress, we identify two shortcomings in current methods: 1) most\napproaches perform poorly in cases with low audio-visual semantic\ncorrespondence such as silence, noise, and offscreen sounds, i.e. in the\npresence of negative audio; and 2) most prior evaluations are limited to\npositive cases, where both datasets and metrics convey scenarios with a single\nvisible sound source in the scene. To address this, we introduce three key\ncontributions. First, we propose a new training strategy that incorporates\nsilence and noise, which improves performance in positive cases, while being\nmore robust against negative sounds. Our resulting self-supervised model,\nSSL-SaN, achieves state-of-the-art performance compared to other\nself-supervised models, both in sound localization and cross-modal retrieval.\nSecond, we propose a new metric that quantifies the trade-off between alignment\nand separability of auditory and visual features across positive and negative\naudio-visual pairs. Third, we present IS3+, an extended and improved version of\nthe IS3 synthetic dataset with negative audio.\n  Our data, metrics and code are available on the\nhttps://xavijuanola.github.io/SSL-SaN/."}
{"id": "2508.21308", "pdf": "https://arxiv.org/pdf/2508.21308", "abs": "https://arxiv.org/abs/2508.21308", "authors": ["Alekhya Gandu", "Aakash Gautam"], "title": "Conflict in Community-Based Design: A Case Study of a Relationship Breakdown", "categories": ["cs.HC", "cs.CY"], "comment": "23 pages", "summary": "Community-based design efforts rightly seek to reduce the power differences\nbetween researchers and community participants by aligning with community\nvalues and furthering their priorities. However, what should designers do when\nkey community members' practices seem to enact an oppressive and harmful\nstructure? We reflect on our two-year-long engagement with a non-profit\norganization in southern India that supports women subjected to domestic abuse\nor facing mental health crises. We highlight the organizational gaps in\nknowledge management and transfer, which became an avenue for our design\nintervention. During design, we encountered practices that upheld caste\nhierarchies. These practices were expected to be incorporated into our\ntechnology. Anticipating harms to indirect stakeholders, we resisted this\nincorporation. It led to a breakdown in our relationship with the partner\norganization. Reflecting on this experience, we outline pluralistic pathways\nthat community-based designers might inhabit when navigating value conflicts.\nThese include making space for reflection before and during engagements,\nstrategically repositioning through role reframing or appreciative inquiry, and\nexiting the engagement if necessary."}
{"id": "2508.21524", "pdf": "https://arxiv.org/pdf/2508.21524", "abs": "https://arxiv.org/abs/2508.21524", "authors": ["Wenyong Zhou", "Zhengwu Liu", "Yuan Ren", "Ngai Wong"], "title": "Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators", "categories": ["cs.AR", "cs.LG"], "comment": "5 pages, 6 figures", "summary": "Compute-in-memory (CIM) accelerators have emerged as a promising way for\nenhancing the energy efficiency of convolutional neural networks (CNNs).\nDeploying CNNs on CIM platforms generally requires quantization of network\nweights and activations to meet hardware constraints. However, existing\napproaches either prioritize hardware efficiency with binary weight and\nactivation quantization at the cost of accuracy, or utilize multi-bit weights\nand activations for greater accuracy but limited efficiency. In this paper, we\nintroduce a novel binary weight multi-bit activation (BWMA) method for CNNs on\nCIM-based accelerators. Our contributions include: deriving closed-form\nsolutions for weight quantization in each layer, significantly improving the\nrepresentational capabilities of binarized weights; and developing a\ndifferentiable function for activation quantization, approximating the ideal\nmulti-bit function while bypassing the extensive search for optimal settings.\nThrough comprehensive experiments on CIFAR-10 and ImageNet datasets, we show\nthat BWMA achieves notable accuracy improvements over existing methods,\nregistering gains of 1.44\\%-5.46\\% and 0.35\\%-5.37\\% on respective datasets.\nMoreover, hardware simulation results indicate that 4-bit activation\nquantization strikes the optimal balance between hardware cost and model\nperformance."}
{"id": "2508.21328", "pdf": "https://arxiv.org/pdf/2508.21328", "abs": "https://arxiv.org/abs/2508.21328", "authors": ["Zhiyu Wang", "Mohammad Goudarzi", "Mingming Gong", "Rajkumar Buyya"], "title": "A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling", "categories": ["cs.DC"], "comment": null, "summary": "The rapid proliferation of Internet of Things (IoT) applications across\nheterogeneous Cloud-Edge-IoT environments presents significant challenges in\ndistributed scheduling optimization. Existing approaches face issues, including\nfixed neural network architectures that are incompatible with computational\nheterogeneity, non-Independent and Identically Distributed (non-IID) data\ndistributions across IoT scheduling domains, and insufficient cross-domain\ncollaboration mechanisms. This paper proposes KD-AFRL, a Knowledge\nDistillation-empowered Adaptive Federated Reinforcement Learning framework that\naddresses multi-domain IoT application scheduling through three core\ninnovations. First, we develop a resource-aware hybrid architecture generation\nmechanism that creates dual-zone neural networks enabling heterogeneous devices\nto participate in collaborative learning while maintaining optimal resource\nutilization. Second, we propose a privacy-preserving environment-clustered\nfederated learning approach that utilizes differential privacy and K-means\nclustering to address non-IID challenges and facilitate effective collaboration\namong compatible domains. Third, we introduce an environment-oriented\ncross-architecture knowledge distillation mechanism that enables efficient\nknowledge transfer between heterogeneous models through temperature-regulated\nsoft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure\ndemonstrate KD-AFRL's effectiveness using diverse IoT applications. Results\nshow significant improvements over the best baseline, with 21% faster\nconvergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,\nenergy consumption, and weighted cost, respectively. Scalability experiments\nreveal that KD-AFRL achieves 3-5 times better performance retention compared to\nexisting solutions as the number of domains increases."}
{"id": "2508.21811", "pdf": "https://arxiv.org/pdf/2508.21811", "abs": "https://arxiv.org/abs/2508.21811", "authors": ["Ashley Hourigan", "Ridewaan Hanslo"], "title": "The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry", "categories": ["cs.SE", "68", "D.2.9"], "comment": "10 pages, 2 figures, conference", "summary": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives."}
{"id": "2508.21736", "pdf": "https://arxiv.org/pdf/2508.21736", "abs": "https://arxiv.org/abs/2508.21736", "authors": ["Simon Burbach", "Maria Maleshkova", "Florian Centler", "Tanja Joan Schmidt"], "title": "MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality", "categories": ["cs.HC", "cs.CE", "cs.GR", "q-bio.CB", "q-bio.MN"], "comment": null, "summary": "Microbiomes are a vital part of the human body, engaging in tasks like food\ndigestion and immune defense. Their structure and function must be understood\nin order to promote host health and facilitate swift recovery during disease.\nDue to the difficulties in experimentally studying these systems in situ, more\nresearch is being conducted in the field of mathematical modeling. Visualizing\nspatiotemporal data is challenging, and current tools that simulate microbial\ncommunities' spatial and temporal development often only provide limited\nfunctionalities, often requiring expert knowledge to generate useful results.\nTo overcome these limitations, we provide a user-friendly tool to interactively\nexplore spatiotemporal simulation data, called MicroLabVR, which transfers\nspatial data into virtual reality (VR) while following guidelines to enhance\nuser experience (UX). With MicroLabVR, users can import CSV datasets containing\npopulation growth, substance concentration development, and metabolic flux\ndistribution data. The implemented visualization methods allow users to\nevaluate the dataset in a VR environment interactively. MicroLabVR aims to\nimprove data analysis for the user by allowing the exploration of microbiome\ndata in their spatial context."}
{"id": "2508.21456", "pdf": "https://arxiv.org/pdf/2508.21456", "abs": "https://arxiv.org/abs/2508.21456", "authors": ["Yi-Hao Peng", "Dingzeyu Li", "Jeffrey P. Bigham", "Amy Pavel"], "title": "Morae: Proactively Pausing UI Agents for User Choices", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": "ACM UIST 2025", "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences."}
{"id": "2508.21739", "pdf": "https://arxiv.org/pdf/2508.21739", "abs": "https://arxiv.org/abs/2508.21739", "authors": ["Hamza Ezzaoui Rahali", "Abhilasha Dave", "Larry Ruckman", "Mohammad Mehdi Rahimifar", "Audrey C. Therrien", "James J. Russel", "Ryan T. Herbst"], "title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": null, "summary": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more."}
{"id": "2508.21473", "pdf": "https://arxiv.org/pdf/2508.21473", "abs": "https://arxiv.org/abs/2508.21473", "authors": ["Daniil Vostrikov", "Yash Madhwal", "Andrey Seoev", "Anastasiia Smirnova", "Yury Yanovich", "Alexey Smirnov", "Vladimir Gorgadze"], "title": "Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage", "categories": ["cs.DC"], "comment": null, "summary": "The evolution of blockchain technology, from its origins as a decentralized\nledger for cryptocurrencies to its broader applications in areas like\ndecentralized finance (DeFi), has significantly transformed financial\necosystems while introducing new challenges such as Maximum Extractable Value\n(MEV). This paper explores MEV on the Polygon blockchain, with a particular\nfocus on Atomic Arbitrage (AA) transactions. We establish criteria for\nidentifying AA transactions and analyze key factors such as searcher behavior,\nbidding dynamics, and token usage. Utilizing a dataset spanning 22 months and\ncovering 23 million blocks, we examine MEV dynamics with a focus on Spam-based\nand Auction-based backrunning strategies. Our findings reveal that while\nSpam-based transactions are more prevalent, Auction-based transactions\ndemonstrate greater profitability. Through detailed examples and analysis, we\ninvestigate the interactions between network architecture, transaction\nsequencing, and MEV extraction, offering comprehensive insights into the\nevolution and challenges of MEV in decentralized ecosystems. These results\nemphasize the need for robust transaction ordering mechanisms and highlight the\nimplications of emerging MEV strategies for blockchain networks."}
{"id": "2508.21289", "pdf": "https://arxiv.org/pdf/2508.21289", "abs": "https://arxiv.org/abs/2508.21289", "authors": ["Val√©rie Hayot-Sasson", "Nathaniel Hudson", "Andr√© Bauer", "Maxime Gonthier", "Ian Foster", "Kyle Chard"], "title": "Addressing Reproducibility Challenges in HPC with Continuous Integration", "categories": ["cs.DC", "cs.SE"], "comment": null, "summary": "The high-performance computing (HPC) community has adopted incentive\nstructures to motivate reproducible research, with major conferences awarding\nbadges to papers that meet reproducibility requirements. Yet, many papers do\nnot meet such requirements. The uniqueness of HPC infrastructure and software,\ncoupled with strict access requirements, may limit opportunities for\nreproducibility. In the absence of resource access, we believe that regular\ndocumented testing, through continuous integration (CI), coupled with complete\nprovenance information, can be used as a substitute. Here, we argue that better\nHPC-compliant CI solutions will improve reproducibility of applications. We\npresent a survey of reproducibility initiatives and describe the barriers to\nreproducibility in HPC. To address existing limitations, we present a GitHub\nAction, CORRECT, that enables secure execution of tests on remote HPC\nresources. We evaluate CORRECT's usability across three different types of HPC\napplications, demonstrating the effectiveness of using CORRECT for automating\nand documenting reproducibility evaluations."}
{"id": "2508.21666", "pdf": "https://arxiv.org/pdf/2508.21666", "abs": "https://arxiv.org/abs/2508.21666", "authors": ["Imran S. A. Khan", "Emmanuel G. Blanchard", "S√©bastien George"], "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG", "cs.SE"], "comment": null, "summary": "This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness."}
{"id": "2508.21613", "pdf": "https://arxiv.org/pdf/2508.21613", "abs": "https://arxiv.org/abs/2508.21613", "authors": ["Yuhang Zhou", "Zhibin Wang", "Peng Jiang", "Haoran Xia", "Junhe Lu", "Qianyu Jiang", "Rong Gu", "Hengxi Xu", "Xinjing Huang", "Guanghuan Fang", "Zhiheng Hu", "Jingyi Zhang", "Yongjin Cai", "Jian He", "Chen Tian"], "title": "Odyssey: Adaptive Policy Selection for Resilient Distributed Training", "categories": ["cs.DC"], "comment": null, "summary": "Training large language models faces frequent interruptions due to various\nfaults, demanding robust fault-tolerance. Existing backup-free methods, such as\nredundant computation, dynamic parallelism, and data rerouting, each incur\nperformance penalties, whether from ongoing overhead, lengthy reconfigurations,\nor post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant\nsystem that intelligently selects optimal recovery strategies when a failure\noccurs. Odyssey achieves this through a unified performance model, expedient\nexecution plan search, accurate performance estimation, and efficient\ncommunication optimizations. Experiments on a 32-card cluster show that Odyssey\nmaintains a performance gap of within 11.00% between post-recovery and\nfailure-free training, while preserving model convergence and efficient memory\nusage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and\n1.355x higher average throughput than Oobleck and Recycle, respectively."}
{"id": "2508.21302", "pdf": "https://arxiv.org/pdf/2508.21302", "abs": "https://arxiv.org/abs/2508.21302", "authors": ["Jie Zhu", "Chihao Shen", "Ziyang Li", "Jiahao Yu", "Yizheng Chen", "Kexin Pei"], "title": "Locus: Agentic Predicate Synthesis for Directed Fuzzing", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Directed fuzzing aims to find program inputs that lead to specified target\nprogram states. It has broad applications, such as debugging system crashes,\nconfirming reported bugs, and generating exploits for potential\nvulnerabilities. This task is inherently challenging because target states are\noften deeply nested in the program, while the search space manifested by\nnumerous possible program inputs is prohibitively large. Existing approaches\nrely on branch distances or manually-specified constraints to guide the search;\nhowever, the branches alone are often insufficient to precisely characterize\nprogress toward reaching the target states, while the manually specified\nconstraints are often tailored for specific bug types and thus difficult to\ngeneralize to diverse target states and programs.\n  We present Locus, a novel framework to improve the efficiency of directed\nfuzzing. Our key insight is to synthesize predicates to capture fuzzing\nprogress as semantically meaningful intermediate states, serving as milestones\ntowards reaching the target states. When used to instrument the program under\nfuzzing, they can reject executions unlikely to reach the target states, while\nproviding additional coverage guidance. To automate this task and generalize to\ndiverse programs, Locus features an agentic framework with program analysis\ntools to synthesize and iteratively refine the candidate predicates, while\nensuring the predicates strictly relax the target states to prevent false\nrejections via symbolic execution. Our evaluation shows that Locus\nsubstantially improves the efficiency of eight state-of-the-art fuzzers in\ndiscovering real-world vulnerabilities, achieving an average speedup of 41.6x.\nSo far, Locus has found eight previously unpatched bugs, with one already\nacknowledged with a draft patch."}
{"id": "2508.21733", "pdf": "https://arxiv.org/pdf/2508.21733", "abs": "https://arxiv.org/abs/2508.21733", "authors": ["Maya Guhan", "Meghan E. Hurley", "Eric A. Storch", "John Herrington", "Casey Zampella", "Julia Parish-Morris", "Gabriel L√°zaro-Mu√±oz", "Kristin Kostick-Quenet"], "title": "Developer Insights into Designing AI-Based Computer Perception Tools", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "15 pages", "summary": "Artificial intelligence (AI)-based computer perception (CP) technologies use\nmobile sensors to collect behavioral and physiological data for clinical\ndecision-making. These tools can reshape how clinical knowledge is generated\nand interpreted. However, effective integration of these tools into clinical\nworkflows depends on how developers balance clinical utility with user\nacceptability and trustworthiness. Our study presents findings from 20 in-depth\ninterviews with developers of AI-based CP tools. Interviews were transcribed\nand inductive, thematic analysis was performed to identify 4 key design\npriorities: 1) to account for context and ensure explainability for both\npatients and clinicians; 2) align tools with existing clinical workflows; 3)\nappropriately customize to relevant stakeholders for usability and\nacceptability; and 4) push the boundaries of innovation while aligning with\nestablished paradigms. Our findings highlight that developers view themselves\nas not merely technical architects but also ethical stewards, designing tools\nthat are both acceptable by users and epistemically responsible (prioritizing\nobjectivity and pushing clinical knowledge forward). We offer the following\nsuggestions to help achieve this balance: documenting how design choices around\ncustomization are made, defining limits for customization choices,\ntransparently conveying information about outputs, and investing in user\ntraining. Achieving these goals will require interdisciplinary collaboration\nbetween developers, clinicians, and ethicists."}
{"id": "2508.21706", "pdf": "https://arxiv.org/pdf/2508.21706", "abs": "https://arxiv.org/abs/2508.21706", "authors": ["Zhibin Wang", "Zhonghui Zhang", "Yuhang Zhou", "Zibo Wang", "Mo Zhou", "Peng Jiang", "Weilin Cai", "Chengying Huan", "Rong Gu", "Sheng Zhong", "Chen Tian"], "title": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding", "categories": ["cs.DC"], "comment": null, "summary": "Recent advancements in Mixture of Experts (MoE) models have significantly\nincreased their parameter scale as well as model performance. Extensive\noffloading techniques have been proposed to address the GPU memory limitations\nof MoE inference. However, due to the I/O bottleneck and sparse computation of\nMoE models, existing offloading techniques still suffer from low hardware\nutilization. To fully utilize the hardware resources, we propose SpecMoEOff,\nwhich employs the speculative decoding technique to enlarge the workload of\neach expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and\nempirical roofline analysis. In addition, we develop a dedicated CPU chunked\nattention verification kernel to fit the speculative decoding in offloading\nscenarios as well as minimizing the additional overhead led by draft models.\nSpecMoEOff further integrates an optimizer to automatically tune the\nhyperparameters of speculative decoding under given hardware and workload.\nExperimental results show that SpecMoEOff achieves up to 2.5x decode throughput\nimprovement over the state-of-the-art MoE offloading techniques."}
{"id": "2508.21386", "pdf": "https://arxiv.org/pdf/2508.21386", "abs": "https://arxiv.org/abs/2508.21386", "authors": ["Jukka Ruohonen", "Jesper L√∏ffler Nielsen", "Jakub Sk√≥rczynski"], "title": "Risks and Compliance with the EU's Core Cyber Security Legislation", "categories": ["cs.CR", "cs.CY", "cs.SE"], "comment": "Submitted to IST (VSI:RegCompliance in SE)", "summary": "The European Union (EU) has long favored a risk-based approach to regulation.\nSuch an approach is also used in recent cyber security legislation enacted in\nthe EU. Risks are also inherently related to compliance with the new\nlegislation. Objective: The paper investigates how risks are framed in the EU's\nfive core cyber security legislative acts, whether the framings indicate\nconvergence or divergence between the acts and their risk concepts, and what\nqualifying words and terms are used when describing the legal notions of risks.\nMethod : The paper's methodology is based on qualitative legal interpretation\nand taxonomy-building. Results: The five acts have an encompassing coverage of\ndifferent cyber security risks, including but not limited to risks related to\ntechnical, organizational, and human security as well as those not originating\nfrom man-made actions. Both technical aspects and assets are used to frame the\nlegal risk notions in many of the legislative acts. A threat-centric viewpoint\nis also present in one of the acts. Notable gaps are related to acceptable\nrisks, non-probabilistic risks, and residual risks. Conclusion: The EU's new\ncyber security legislation has significantly extended the risk-based approach\nto regulations. At the same time, complexity and compliance burden have\nincreased. With this point in mind, the paper concludes with a few practical\ntakeaways about means to deal with compliance and research it."}
{"id": "2508.21736", "pdf": "https://arxiv.org/pdf/2508.21736", "abs": "https://arxiv.org/abs/2508.21736", "authors": ["Simon Burbach", "Maria Maleshkova", "Florian Centler", "Tanja Joan Schmidt"], "title": "MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality", "categories": ["cs.HC", "cs.CE", "cs.GR", "q-bio.CB", "q-bio.MN"], "comment": null, "summary": "Microbiomes are a vital part of the human body, engaging in tasks like food\ndigestion and immune defense. Their structure and function must be understood\nin order to promote host health and facilitate swift recovery during disease.\nDue to the difficulties in experimentally studying these systems in situ, more\nresearch is being conducted in the field of mathematical modeling. Visualizing\nspatiotemporal data is challenging, and current tools that simulate microbial\ncommunities' spatial and temporal development often only provide limited\nfunctionalities, often requiring expert knowledge to generate useful results.\nTo overcome these limitations, we provide a user-friendly tool to interactively\nexplore spatiotemporal simulation data, called MicroLabVR, which transfers\nspatial data into virtual reality (VR) while following guidelines to enhance\nuser experience (UX). With MicroLabVR, users can import CSV datasets containing\npopulation growth, substance concentration development, and metabolic flux\ndistribution data. The implemented visualization methods allow users to\nevaluate the dataset in a VR environment interactively. MicroLabVR aims to\nimprove data analysis for the user by allowing the exploration of microbiome\ndata in their spatial context."}
{"id": "2508.21431", "pdf": "https://arxiv.org/pdf/2508.21431", "abs": "https://arxiv.org/abs/2508.21431", "authors": ["Yan Huang", "Jinming Xu", "Jiming Chen", "Karl Henrik Johansson"], "title": "An Optimistic Gradient Tracking Method for Distributed Minimax Optimization", "categories": ["math.OC", "cs.DC"], "comment": "This manuscript has been accepted for presentation at the 64th IEEE\n  Conference on Decision and Control", "summary": "This paper studies the distributed minimax optimization problem over\nnetworks. To enhance convergence performance, we propose a distributed\noptimistic gradient tracking method, termed DOGT, which solves a surrogate\nfunction that captures the similarity between local objective functions to\napproximate a centralized optimistic approach locally. Leveraging a\nLyapunov-based analysis, we prove that DOGT achieves linear convergence to the\noptimal solution for strongly convex-strongly concave objective functions while\nremaining robust to the heterogeneity among them. Moreover, by integrating an\naccelerated consensus protocol, the accelerated DOGT (ADOGT) algorithm achieves\nan optimal convergence rate of $\\mathcal{O} \\left( \\kappa \\log \\left( \\epsilon\n^{-1} \\right) \\right)$ and communication complexity of $\\mathcal{O} \\left(\n\\kappa \\log \\left( \\epsilon ^{-1} \\right) /\\sqrt{1-\\sqrt{\\rho _W}} \\right)$ for\na suboptimality level of $\\epsilon>0$, where $\\kappa$ is the condition number\nof the objective function and $\\rho_W$ is the spectrum gap of the network.\nNumerical experiments illustrate the effectiveness of the proposed algorithms."}
{"id": "2508.21417", "pdf": "https://arxiv.org/pdf/2508.21417", "abs": "https://arxiv.org/abs/2508.21417", "authors": ["Shuhan Liu", "Xing Hu", "Xin Xia", "David Lo", "Xiaohu Yang"], "title": "An Empirical Study of Vulnerable Package Dependencies in LLM Repositories", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have developed rapidly in recent years,\nrevolutionizing various fields. Despite their widespread success, LLMs heavily\nrely on external code dependencies from package management systems, creating a\ncomplex and interconnected LLM dependency supply chain. Vulnerabilities in\ndependencies can expose LLMs to security risks. While existing research\npredominantly focuses on model-level security threats, vulnerabilities within\nthe LLM dependency supply chain have been overlooked. To fill this gap, we\nconducted an empirical analysis of 52 open-source LLMs, examining their\nthird-party dependencies and associated vulnerabilities. We then explored\nactivities within the LLM repositories to understand how maintainers manage\nthird-party vulnerabilities in practice. Finally, we compared third-party\ndependency vulnerabilities in the LLM ecosystem to those in the Python\necosystem. Our results show that half of the vulnerabilities in the LLM\necosystem remain undisclosed for more than 56.2 months, significantly longer\nthan those in the Python ecosystem. Additionally, 75.8% of LLMs include\nvulnerable dependencies in their configuration files. This study advances the\nunderstanding of LLM supply chain risks, provides insights for practitioners,\nand highlights potential directions for improving the security of the LLM\nsupply chain."}
{"id": "2508.21248", "pdf": "https://arxiv.org/pdf/2508.21248", "abs": "https://arxiv.org/abs/2508.21248", "authors": ["Subham Kutum", "Abhijit Sinha", "Hemant Kumar Kathania", "Sudarsana Reddy Kadiri", "Mahesh Chandra Govil"], "title": "Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.SD", "eess.SP"], "comment": "Accepted", "summary": "Numerous methods have been proposed to enhance Keyword Spotting (KWS) in\nadult speech, but children's speech presents unique challenges for KWS systems\ndue to its distinct acoustic and linguistic characteristics. This paper\nintroduces a zero-shot KWS approach that leverages state-of-the-art\nself-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec.\nFeatures are extracted layer-wise from these SSL models and used to train a\nKaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for\ntraining, while the PFSTAR children's speech dataset was used for testing,\ndemonstrating the zero-shot capability of our method. Our approach achieved\nstate-of-the-art results across all keyword sets for children's speech.\nNotably, the Wav2Vec2 model, particularly layer 22, performed the best,\ndelivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of\nfalse alarm and probability of miss of 0.0164 and 0.0547 respectively, for a\nset of 30 keywords. Furthermore, age-specific performance evaluation confirmed\nthe system's effectiveness across different age groups of children. To assess\nthe system's robustness against noise, additional experiments were conducted\nusing the best-performing layer of the best-performing Wav2Vec2 model. The\nresults demonstrated a significant improvement over traditional MFCC-based\nbaseline, emphasizing the potential of SSL embeddings even in noisy conditions.\nTo further generalize the KWS framework, the experiments were repeated for an\nadditional CMU dataset. Overall the results highlight the significant\ncontribution of SSL features in enhancing Zero-Shot KWS performance for\nchildren's speech, effectively addressing the challenges associated with the\ndistinct characteristics of child speakers."}
{"id": "2508.21432", "pdf": "https://arxiv.org/pdf/2508.21432", "abs": "https://arxiv.org/abs/2508.21432", "authors": ["Wenjie Qu", "Yuguang Zhou", "Bo Wang", "Wengrui Zheng", "Yuexin Li", "Jinyuan Jia", "Jiaheng Zhang"], "title": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners."}
{"id": "2508.21628", "pdf": "https://arxiv.org/pdf/2508.21628", "abs": "https://arxiv.org/abs/2508.21628", "authors": ["Sarfaroz Yunusov", "Kaige Chen", "Kazi Nishat Anwar", "Ali Emami"], "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss."}
{"id": "2508.21636", "pdf": "https://arxiv.org/pdf/2508.21636", "abs": "https://arxiv.org/abs/2508.21636", "authors": ["Cristina Improta"], "title": "Detecting Stealthy Data Poisoning Attacks in AI Code Generators", "categories": ["cs.CR", "cs.SE"], "comment": "Accepted to the 3rd IEEE International Workshop on Reliable and\n  Secure AI for Software Engineering (ReSAISE, 2025), co-located with ISSRE\n  2025", "summary": "Deep learning (DL) models for natural language-to-code generation have become\nintegral to modern software development pipelines. However, their heavy\nreliance on large amounts of data, often collected from unsanitized online\nsources, exposes them to data poisoning attacks, where adversaries inject\nmalicious samples to subtly bias model behavior. Recent targeted attacks\nsilently replace secure code with semantically equivalent but vulnerable\nimplementations without relying on explicit triggers to launch the attack,\nmaking it especially hard for detection methods to distinguish clean from\npoisoned samples. We present a systematic study on the effectiveness of\nexisting poisoning detection methods under this stealthy threat model.\nSpecifically, we perform targeted poisoning on three DL models (CodeBERT,\nCodeT5+, AST-T5), and evaluate spectral signatures analysis, activation\nclustering, and static analysis as defenses. Our results show that all methods\nstruggle to detect triggerless poisoning, with representation-based approaches\nfailing to isolate poisoned samples and static analysis suffering false\npositives and false negatives, highlighting the need for more robust,\ntrigger-independent defenses for AI-assisted code generation."}
{"id": "2508.21666", "pdf": "https://arxiv.org/pdf/2508.21666", "abs": "https://arxiv.org/abs/2508.21666", "authors": ["Imran S. A. Khan", "Emmanuel G. Blanchard", "S√©bastien George"], "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG", "cs.SE"], "comment": null, "summary": "This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness."}
