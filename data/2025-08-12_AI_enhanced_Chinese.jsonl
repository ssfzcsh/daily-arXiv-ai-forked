{"id": "2508.06718", "pdf": "https://arxiv.org/pdf/2508.06718", "abs": "https://arxiv.org/abs/2508.06718", "authors": ["Daniel Ogenrwot", "John Businge"], "title": "Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks", "categories": ["cs.SE", "K.6.3; D.2.7"], "comment": "12 pages, 3 figures", "summary": "While most forks on platforms like GitHub are short-lived and used for social\ncollaboration, a smaller but impactful subset evolve into long-lived forks,\nreferred to here as variants, that maintain independent development\ntrajectories. Integrating bug-fix patches across such divergent variants poses\nchallenges due to structural drift, including refactorings that rename,\nrelocate, or reorganize code elements and obscure semantic correspondence. This\npaper presents an empirical study of patch integration failures in 14 divergent\npair of variants and introduces RePatch, a refactoring-aware integration system\nfor Java repositories. RePatch extends the RefMerge framework, originally\ndesigned for symmetric merges, by supporting asymmetric patch transfer. RePatch\ninverts refactorings in both the source and target to realign the patch\ncontext, applies the patch, and replays the transformations to preserve the\nintent of the variant. In our evaluation of 478 bug-fix pull requests, Git\ncherry-pick fails in 64.4% of cases due to structural misalignments, while\nRePatch successfully integrates 52.8% of the previously failing patches. These\nresults highlight the limitations of syntax-based tools and the need for\nsemantic reasoning in variant-aware patch propagation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8GitHub\u957f\u671f\u5206\u53c9\uff08\u53d8\u4f53\uff09\u7684\u4ee3\u7801\u8865\u4e01\u96c6\u6210\u95ee\u9898\uff0c\u63d0\u51faRePatch\u5de5\u5177\uff0c\u901a\u8fc7\u91cd\u6784\u611f\u77e5\u89e3\u51b3\u7ed3\u6784\u6f02\u79fb\u95ee\u9898\uff0c\u6210\u529f\u96c6\u621052.8%\u539f\u672c\u5931\u8d25\u7684\u8865\u4e01\u3002", "motivation": "GitHub\u4e0a\u957f\u671f\u5206\u53c9\uff08\u53d8\u4f53\uff09\u7684\u5f00\u53d1\u8f68\u8ff9\u72ec\u7acb\uff0c\u4ee3\u7801\u7ed3\u6784\u6f02\u79fb\u5bfc\u81f4\u8865\u4e01\u96c6\u6210\u56f0\u96be\uff0c\u9700\u8bed\u4e49\u63a8\u7406\u5de5\u5177\u652f\u6301\u3002", "method": "\u57fa\u4e8eRefMerge\u6846\u67b6\u6269\u5c55\u7684RePatch\u7cfb\u7edf\uff0c\u652f\u6301\u975e\u5bf9\u79f0\u8865\u4e01\u4f20\u8f93\uff0c\u901a\u8fc7\u9006\u5411\u91cd\u6784\u5bf9\u9f50\u8865\u4e01\u4e0a\u4e0b\u6587\u5e76\u4fdd\u7559\u53d8\u4f53\u610f\u56fe\u3002", "result": "\u5728478\u4e2a\u8865\u4e01\u8bf7\u6c42\u4e2d\uff0cGit cherry-pick\u5931\u8d25\u7387\u4e3a64.4%\uff0c\u800cRePatch\u6210\u529f\u96c6\u621052.8%\u7684\u5931\u8d25\u8865\u4e01\u3002", "conclusion": "\u8bed\u6cd5\u5de5\u5177\u5c40\u9650\u6027\u660e\u663e\uff0c\u8bed\u4e49\u63a8\u7406\u5728\u8865\u4e01\u4f20\u64ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0cRePatch\u4e3a\u53d8\u4f53\u5f00\u53d1\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2508.06879", "pdf": "https://arxiv.org/pdf/2508.06879", "abs": "https://arxiv.org/abs/2508.06879", "authors": ["Michael Dorner", "Andreas Bauer", "Darja \u0160mite", "Lukas Thode", "Daniel Mendez", "Ricardo Britto", "Stephan Lukasczyk", "Ehsan Zabardast", "Michael Kormann"], "title": "Quo Vadis, Code Review? Exploring the Future of Code Review", "categories": ["cs.SE"], "comment": null, "summary": "Code review has long been a core practice in collaborative software\nengineering. In this research, we explore how practitioners reflect on code\nreview today and what changes they anticipate in the near future. We then\ndiscuss the potential long-term risks of these anticipated changes for the\nevolution of code review and its role in collaborative software engineering.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ece\u4e1a\u8005\u5bf9\u5f53\u524d\u4ee3\u7801\u5ba1\u67e5\u7684\u53cd\u601d\u53ca\u5bf9\u672a\u6765\u53d8\u5316\u7684\u9884\u671f\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u53d8\u5316\u5bf9\u4ee3\u7801\u5ba1\u67e5\u6f14\u53d8\u7684\u6f5c\u5728\u957f\u671f\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u4ece\u4e1a\u8005\u5bf9\u4ee3\u7801\u5ba1\u67e5\u7684\u73b0\u72b6\u53cd\u601d\u548c\u672a\u6765\u9884\u671f\uff0c\u4ee5\u7406\u89e3\u5176\u5bf9\u534f\u4f5c\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u548c\u5206\u6790\u4ece\u4e1a\u8005\u7684\u89c2\u70b9\uff0c\u63a2\u8ba8\u4ee3\u7801\u5ba1\u67e5\u7684\u672a\u6765\u53d8\u5316\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u4ece\u4e1a\u8005\u5bf9\u4ee3\u7801\u5ba1\u67e5\u7684\u53cd\u601d\u548c\u9884\u671f\u53d8\u5316\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u53d8\u5316\u53ef\u80fd\u5e26\u6765\u7684\u957f\u671f\u98ce\u9669\u3002", "conclusion": "\u4ee3\u7801\u5ba1\u67e5\u7684\u672a\u6765\u53d8\u5316\u53ef\u80fd\u5bf9\u5176\u5728\u534f\u4f5c\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u89d2\u8272\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\uff0c\u9700\u8c28\u614e\u8bc4\u4f30\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2508.06888", "pdf": "https://arxiv.org/pdf/2508.06888", "abs": "https://arxiv.org/abs/2508.06888", "authors": ["Fanyu Wang", "Chetan Arora", "Yonghui Liu", "Kaicheng Huang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Dishan Sambathkumar", "David Lo"], "title": "Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Acceptance criteria (ACs) play a critical role in software development by\nclearly defining the conditions under which a software feature satisfies\nstakeholder expectations. However, manually creating accurate, comprehensive,\nand unambiguous acceptance criteria is challenging, particularly in user\ninterface-intensive applications, due to the reliance on domain-specific\nknowledge and visual context that is not always captured by textual\nrequirements alone. To address these challenges, we propose RAGcceptance M2RE,\na novel approach that leverages Retrieval-Augmented Generation (RAG) to\ngenerate acceptance criteria from multi-modal requirements data, including both\ntextual documentation and visual UI information. We systematically evaluated\nour approach in an industrial case study involving an education-focused\nsoftware system used by approximately 100,000 users. The results indicate that\nintegrating multi-modal information significantly enhances the relevance,\ncorrectness, and comprehensibility of the generated ACs. Moreover, practitioner\nevaluations confirm that our approach effectively reduces manual effort,\ncaptures nuanced stakeholder intent, and provides valuable criteria that domain\nexperts may overlook, demonstrating practical utility and significant potential\nfor industry adoption. This research underscores the potential of multi-modal\nRAG techniques in streamlining software validation processes and improving\ndevelopment efficiency. We also make our implementation and a dataset\navailable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff08RAGcceptance M2RE\uff09\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u548c\u89c6\u89c9UI\u6570\u636e\u4e2d\u81ea\u52a8\u751f\u6210\u51c6\u786e\u4e14\u5168\u9762\u7684\u9a8c\u6536\u6807\u51c6\uff08ACs\uff09\uff0c\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9e\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u624b\u52a8\u521b\u5efa\u9a8c\u6536\u6807\u51c6\uff08ACs\uff09\u5728\u6d89\u53ca\u7528\u6237\u754c\u9762\u7684\u5e94\u7528\u4e2d\u5c24\u5176\u56f0\u96be\uff0c\u56e0\u9700\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u4f46\u6587\u672c\u9700\u6c42\u5e38\u65e0\u6cd5\u5145\u5206\u6355\u83b7\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u9700\u6c42\u6570\u636e\uff08\u6587\u672c\u548cUI\u89c6\u89c9\u4fe1\u606f\uff09\u81ea\u52a8\u751f\u6210ACs\u3002", "result": "\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210ACs\u7684\u76f8\u5173\u6027\u3001\u6b63\u786e\u6027\u548c\u53ef\u7406\u89e3\u6027\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u52aa\u529b\u3002", "conclusion": "\u591a\u6a21\u6001RAG\u6280\u672f\u5728\u4f18\u5316\u8f6f\u4ef6\u9a8c\u8bc1\u6d41\u7a0b\u548c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5177\u5907\u884c\u4e1a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.06926", "pdf": "https://arxiv.org/pdf/2508.06926", "abs": "https://arxiv.org/abs/2508.06926", "authors": ["Feng Luo", "Kexing Ji", "Cuiyun Gao", "Shuzheng Gao", "Jia Feng", "Kui Liu", "Xin Xia", "Michael R. Lyu"], "title": "Integrating Rules and Semantics for LLM-Based C-to-Rust Translation", "categories": ["cs.SE"], "comment": "Accepted in ICSME 25 Industry Track", "summary": "Automated translation of legacy C code into Rust aims to ensure memory safety\nwhile reducing the burden of manual migration. Early approaches in code\ntranslation rely on static rule-based methods, but they suffer from limited\ncoverage due to dependence on predefined rule patterns. Recent works regard the\ntask as a sequence-to-sequence problem by leveraging large language models\n(LLMs). Although these LLM-based methods are capable of reducing unsafe code\nblocks, the translated code often exhibits issues in following Rust rules and\nmaintaining semantic consistency. On one hand, existing methods adopt a direct\nprompting strategy to translate the C code, which struggles to accommodate the\nsyntactic rules between C and Rust. On the other hand, this strategy makes it\ndifficult for LLMs to accurately capture the semantics of complex code. To\naddress these challenges, we propose IRENE, an LLM-based framework that\nIntegrates RulEs aNd sEmantics to enhance translation. IRENE consists of three\nmodules: 1) a rule-augmented retrieval module that selects relevant translation\nexamples based on rules generated from a static analyzer developed by us,\nthereby improving the handling of Rust rules; 2) a structured summarization\nmodule that produces a structured summary for guiding LLMs to enhance the\nsemantic understanding of C code; 3) an error-driven translation module that\nleverages compiler diagnostics to iteratively refine translations. We evaluate\nIRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial\ndataset provided by Huawei) and eight LLMs, focusing on translation accuracy\nand safety.", "AI": {"tldr": "IRENE\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c4\u5219\u548c\u8bed\u4e49\u63d0\u5347C\u4ee3\u7801\u5230Rust\u7684\u81ea\u52a8\u7ffb\u8bd1\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u6cd5\u89c4\u5219\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728C\u4ee3\u7801\u5230Rust\u7684\u7ffb\u8bd1\u4e2d\u5b58\u5728\u8bed\u6cd5\u89c4\u5219\u9002\u5e94\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0cIRENE\u65e8\u5728\u901a\u8fc7\u89c4\u5219\u589e\u5f3a\u3001\u8bed\u4e49\u603b\u7ed3\u548c\u9519\u8bef\u9a71\u52a8\u7ffb\u8bd1\u6765\u6539\u8fdb\u8fd9\u4e9b\u65b9\u9762\u3002", "method": "IRENE\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u89c4\u5219\u589e\u5f3a\u68c0\u7d22\u6a21\u5757\u3001\u7ed3\u6784\u5316\u603b\u7ed3\u6a21\u5757\u548c\u9519\u8bef\u9a71\u52a8\u7ffb\u8bd1\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u63d0\u5347Rust\u89c4\u5219\u5904\u7406\u3001\u589e\u5f3aC\u4ee3\u7801\u8bed\u4e49\u7406\u89e3\u548c\u8fed\u4ee3\u4f18\u5316\u7ffb\u8bd1\u7ed3\u679c\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08xCodeEval\u548cHW-Bench\uff09\u548c\u591a\u4e2aLLM\u4e0a\u8bc4\u4f30\uff0cIRENE\u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "IRENE\u901a\u8fc7\u6574\u5408\u89c4\u5219\u548c\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u4e86C\u5230Rust\u4ee3\u7801\u7ffb\u8bd1\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u52a8\u8fc1\u79fb\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06512", "pdf": "https://arxiv.org/pdf/2508.06512", "abs": "https://arxiv.org/abs/2508.06512", "authors": ["Alina Karakanta"], "title": "Accessibility Literacy: Increasing accessibility awareness among young content creators", "categories": ["cs.HC", "cs.CY"], "comment": "Master thesis: MASTER OF ARTS IN ACCESSIBILITY TO MEDIA, ARTS AND\n  CULTURE", "summary": "The proliferation of audiovisual and web content has created an increasing\nneed for media accessibility education in various fields. However,\naccessibility remains a low priority in university curricula. This project\nexplores the feasibility of an alternative learning experience aimed at\nincreasing the accessibility literacy of young content creators, taking web\naccessibility as a case study. We propose a mini module that uses simple,\neasy-to-use training materials, such as infographics and short quizzes, and can\nbe easily incorporated in educational programmes along existing courses. A\nsurvey was conducted to investigate the participants' accessibility literacy\nbefore and after training. The findings show that young content creators\ngenerally have limited accessibility literacy but even brief exposure to\naccessibility materials contributed to a shift in perceptions. After training,\nparticipants expressed more willingness to implement accessibility tools in\ntheir content, with ways varying depending on content type and purpose. This\nsuggests that small, yet targeted interventions could be an alternative for\nintegrating accessibility training into formal education across various\ndisciplines. While some responses reflected traces of the medical model of\ndisability and a particularlist view of accessibility, accessibility was\nrecognised as important for increasing inclusion, improving content, and\nshaping a fairer society.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u7b80\u77ed\u7684\u6a21\u5757\u5316\u57f9\u8bad\u63d0\u5347\u5e74\u8f7b\u5185\u5bb9\u521b\u4f5c\u8005\u7684\u65e0\u969c\u788d\u7d20\u517b\uff0c\u7ed3\u679c\u8868\u660e\u5373\u4f7f\u662f\u77ed\u6682\u63a5\u89e6\u65e0\u969c\u788d\u6750\u6599\u4e5f\u80fd\u663e\u8457\u6539\u53d8\u4ed6\u4eec\u7684\u8ba4\u77e5\u4e0e\u884c\u4e3a\u610f\u613f\u3002", "motivation": "\u968f\u7740\u89c6\u542c\u548c\u7f51\u7edc\u5185\u5bb9\u7684\u6fc0\u589e\uff0c\u5a92\u4f53\u65e0\u969c\u788d\u6559\u80b2\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u5728\u5927\u5b66\u8bfe\u7a0b\u4e2d\u4ecd\u88ab\u5ffd\u89c6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u7b80\u6613\u57f9\u8bad\u6750\u6599\uff08\u5982\u56fe\u8868\u548c\u7b80\u77ed\u6d4b\u9a8c\uff09\u7684\u8ff7\u4f60\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u524d\u540e\u8c03\u67e5\u8bc4\u4f30\u53c2\u4e0e\u8005\u7684\u65e0\u969c\u788d\u7d20\u517b\u53d8\u5316\u3002", "result": "\u57f9\u8bad\u540e\uff0c\u53c2\u4e0e\u8005\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u65e0\u969c\u788d\u5de5\u5177\u4f7f\u7528\u610f\u613f\uff0c\u4f46\u5177\u4f53\u65b9\u5f0f\u56e0\u5185\u5bb9\u7c7b\u578b\u548c\u76ee\u7684\u800c\u5f02\u3002", "conclusion": "\u5c0f\u89c4\u6a21\u3001\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u53ef\u4ee5\u4f5c\u4e3a\u5c06\u65e0\u969c\u788d\u57f9\u8bad\u878d\u5165\u6b63\u89c4\u6559\u80b2\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.07010", "pdf": "https://arxiv.org/pdf/2508.07010", "abs": "https://arxiv.org/abs/2508.07010", "authors": ["Roberto Balestri", "Guglielmo Pescatore"], "title": "Narrative Memory in Machines: Multi-Agent Arc Extraction in Serialized TV", "categories": ["cs.MM", "cs.HC", "cs.MA"], "comment": null, "summary": "Serialized television narratives present significant analytical challenges\ndue to their complex, temporally distributed storylines that necessitate\nsophisticated information management. This paper introduces a multi-agent\nsystem (MAS) designed to extract and analyze narrative arcs by implementing\nprinciples of computational memory architectures. The system conceptualizes\nnarrative understanding through analogues of human memory: Large Language\nModels (LLMs) provide a form of semantic memory for general narrative patterns,\nwhile a vector database stores specific arc progressions as episodic memories.\nA multi-agent workflow simulates working memory processes to integrate these\ninformation types. Tested on the first season of Grey's Anatomy (ABC 2005-),\nthe MAS identifies three arc types: Anthology (self-contained), Soap\n(relationship-focused), and Genre-Specific. These arcs and their episodic\ndevelopments are stored in a vector database, facilitating structured analysis\nand semantic comparison. To bridge automation with critical interpretation, a\ngraphical interface enables human oversight and refinement of the system's\nnarrative memory. While demonstrating strong performance in identifying\nAnthology Arcs and character entities, the system's reliance on textual\nparatexts (episode summaries) revealed limitations in discerning overlapping\narcs and opaque dynamics, underscoring the challenges in computational memory\nconsolidation versus human holistic understanding. This memory-centric approach\nhighlights the potential of combining AI-driven memory processing with human\nexpertise. Beyond television, it offers promise for serialized written formats\nwhere narrative is entirely text-based. Future work will focus on integrating\nmultimodal inputs to enrich episodic memory, refining memory integration\nmechanisms within the MAS, and expanding testing across diverse genres.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u7528\u4e8e\u901a\u8fc7\u8ba1\u7b97\u8bb0\u5fc6\u67b6\u6784\u63d0\u53d6\u548c\u5206\u6790\u7535\u89c6\u53d9\u4e8b\u7684\u590d\u6742\u6545\u4e8b\u7ebf\uff0c\u7ed3\u5408\u4eba\u7c7b\u8bb0\u5fc6\u7684\u6a21\u62df\uff08\u8bed\u4e49\u548c\u60c5\u666f\u8bb0\u5fc6\uff09\u6765\u5b9e\u73b0\u53d9\u4e8b\u7406\u89e3\uff0c\u5e76\u5728\u300a\u5b9e\u4e60\u533b\u751f\u683c\u857e\u300b\u7b2c\u4e00\u5b63\u4e2d\u6d4b\u8bd5\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4e14\u65f6\u95f4\u5206\u5e03\u5e7f\u6cdb\u7684\u7535\u89c6\u53d9\u4e8b\u5206\u6790\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u53d9\u4e8b\u7406\u89e3\u548c\u7ba1\u7406\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8bed\u4e49\u8bb0\u5fc6\u548c\u5411\u91cf\u6570\u636e\u5e93\u4f5c\u4e3a\u60c5\u666f\u8bb0\u5fc6\uff0c\u901a\u8fc7\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u62df\u6574\u5408\u4fe1\u606f\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u8bc6\u522b\u4e86\u4e09\u79cd\u53d9\u4e8b\u5f27\u7c7b\u578b\uff0c\u4f46\u4f9d\u8d56\u6587\u672c\u6458\u8981\u9650\u5236\u4e86\u5176\u5bf9\u91cd\u53e0\u5f27\u548c\u4e0d\u900f\u660e\u52a8\u6001\u7684\u5206\u6790\u80fd\u529b\uff0c\u663e\u793a\u4e86\u8ba1\u7b97\u8bb0\u5fc6\u4e0e\u4eba\u7c7b\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7684\u8bb0\u5fc6\u4e2d\u5fc3\u65b9\u6cd5\u4e3a\u7ed3\u5408AI\u8bb0\u5fc6\u5904\u7406\u548c\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u591a\u6a21\u6001\u8f93\u5165\u5e76\u4f18\u5316\u8bb0\u5fc6\u6574\u5408\u673a\u5236\u3002"}}
{"id": "2508.07855", "pdf": "https://arxiv.org/pdf/2508.07855", "abs": "https://arxiv.org/abs/2508.07855", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "R. Govind", "Samuel Grahn", "Ramanathan S. Thinniyam"], "title": "Checking Consistency of Event-driven Traces", "categories": ["cs.PL"], "comment": null, "summary": "Event-driven programming is a popular paradigm where the flow of execution is\ncontrolled by two features: (1) shared memory and (2) sending and receiving of\nmessages between multiple handler threads (just called handler). Each handler\nhas a mailbox (modelled as a queue) for receiving messages, with the constraint\nthat the handler processes its messages sequentially. Executions of messages by\ndifferent handlers may be interleaved. A central problem in this setting is\nchecking whether a candidate execution is consistent with the semantics of\nevent-driven programs. In this paper, we propose an axiomatic semantics for\neventdriven programs based on the standard notion of traces (also known as\nexecution graphs). We prove the equivalence of axiomatic and operational\nsemantics. This allows us to rephrase the consistency problem axiomatically,\nresulting in the event-driven consistency problem: checking whether a given\ntrace is consistent. We analyze the computational complexity of this problem\nand show that it is NP-complete, even when the number of handler threads is\nbounded. We then identify a tractable fragment: in the absence of nested\nposting, where handlers do not post new messages while processing a message,\nconsistency checking can be performed in polynomial time. Finally, we implement\nour approach in a prototype tool and report on experimental results on a wide\nrange of benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u8ddf\u8e2a\u7684event-driven\u7a0b\u5e8f\u7684\u516c\u7406\u8bed\u4e49\uff0c\u5e76\u8bc1\u660e\u5176\u4e0e\u64cd\u4f5c\u8bed\u4e49\u7684\u7b49\u4ef7\u6027\uff0c\u5c06\u4e00\u81f4\u6027\u95ee\u9898\u8f6c\u5316\u4e3aNP\u5b8c\u5168\u95ee\u9898\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u65e0\u5d4c\u5957\u53d1\u5e03\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u7247\u6bb5\u3002", "motivation": "\u89e3\u51b3event-driven\u7a0b\u5e8f\u4e2d\u5019\u9009\u6267\u884c\u662f\u5426\u7b26\u5408\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u68c0\u67e5\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8ddf\u8e2a\u7684\u516c\u7406\u8bed\u4e49\uff0c\u8bc1\u660e\u5176\u4e0e\u64cd\u4f5c\u8bed\u4e49\u7684\u7b49\u4ef7\u6027\uff0c\u5206\u6790\u95ee\u9898\u590d\u6742\u5ea6\u5e76\u8bc6\u522b\u53ef\u89e3\u7247\u6bb5\u3002", "result": "\u4e00\u81f4\u6027\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\uff0c\u4f46\u5728\u65e0\u5d4c\u5957\u53d1\u5e03\u60c5\u51b5\u4e0b\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u89e3\u51b3\u3002", "conclusion": "\u901a\u8fc7\u5de5\u5177\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2508.07240", "pdf": "https://arxiv.org/pdf/2508.07240", "abs": "https://arxiv.org/abs/2508.07240", "authors": ["Zixuan Li", "Zixiong Wang", "Jian Yang", "Milos Hasan", "Beibei Wang"], "title": "PureSample: Neural Materials Learned by Sampling Microgeometry", "categories": ["cs.GR"], "comment": null, "summary": "Traditional physically-based material models rely on analytically derived\nbidirectional reflectance distribution functions (BRDFs), typically by\nconsidering statistics of micro-primitives such as facets, flakes, or spheres,\nsometimes combined with multi-bounce interactions such as layering and multiple\nscattering. These derivations are often complex and model-specific, and\ntypically consider a statistical aggregate of a large surface area, ignoring\nspatial variation. Once an analytic BRDF's evaluation is defined, one still\nneeds to design an importance sampling method for it, and a way to evaluate the\npdf of that sampling distribution, requiring further model-specific\nderivations.\n  We present PureSample: a novel neural BRDF representation that allows\nlearning a material's behavior purely by sampling forward random walks on the\nmicrogeometry, which is usually straightforward to implement. Our\nrepresentation allows for efficient importance sampling, pdf evaluation, and\nBRDF evaluation, for homogeneous as well as spatially varying materials.\n  We achieve this by two learnable components: first, the sampling distribution\nis modeled using a flow matching neural network, which allows both importance\nsampling and pdf evaluation; second, we introduce a view-dependent albedo term,\ncaptured by a lightweight neural network, which allows for converting a scalar\npdf value to a colored BRDF value for any pair of view and light directions.\n  We demonstrate PureSample on challenging materials, including multi-layered\nmaterials, multiple-scattering microfacet materials, and various other\nmicrostructures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PureSample \u7684\u65b0\u578b\u795e\u7ecf BRDF \u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u5fae\u51e0\u4f55\u4e0a\u7684\u968f\u673a\u884c\u8d70\u6765\u5b66\u4e60\u6750\u6599\u884c\u4e3a\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7269\u7406\u6a21\u578b\u590d\u6742\u7684\u5206\u6790\u548c\u63a8\u5bfc\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6750\u8d28\u6a21\u578b\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u5206\u6790\u63a8\u5bfc BRDF \u65b9\u6cd5\uff0c\u4e14\u901a\u5e38\u5ffd\u7565\u7a7a\u95f4\u53d8\u5316\uff0c\u8bbe\u8ba1\u548c\u5b9e\u73b0\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u56f0\u96be\u3002", "method": "PureSample \u901a\u8fc7\u4e24\u4e2a\u53ef\u5b66\u4e60\u7ec4\u4ef6\u5b9e\u73b0\uff1a1\uff09\u57fa\u4e8e\u6d41\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u91c7\u6837\u5206\u5e03\uff1b2\uff09\u5f15\u5165\u89c6\u56fe\u76f8\u5173\u7684\u53cd\u7167\u7387\u9879\u3002", "result": "PureSample \u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u91cd\u8981\u6027\u91c7\u6837\u3001PDF \u8bc4\u4f30\u548c BRDF \u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u591a\u5c42\u6750\u6599\u548c\u591a\u6b21\u6563\u5c04\u5fae\u8868\u9762\u6750\u6599\u5728\u5185\u7684\u591a\u79cd\u590d\u6742\u6750\u8d28\u3002", "conclusion": "PureSample \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u901a\u7528\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u548c\u8868\u793a BRDF\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.07174", "pdf": "https://arxiv.org/pdf/2508.07174", "abs": "https://arxiv.org/abs/2508.07174", "authors": ["Rongshuan Geng", "Wantao Ning"], "title": "On the fault diameter and wide diameter of the exchanged 3-ary $n$-cube", "categories": ["cs.LO"], "comment": null, "summary": "Fault diameter and wide diameter are two critical parameters for evaluating\ncommunication performance in interconnection networks. They measure the fault\ntolerance and transmission efficiency of networks. The exchanged 3-ary $n$-cube\nis a recently proposed variant of the hypercube, denoted by $E3C(r, s, t)$. In\nthis work, we obtain that the $(2r + 1)$-fault diameter and $(2r + 2)$-wide\ndiameter of $E3C(r, s, t)$ are bounded between $n + 3$ and $n + 5$ for $1 \\leq\nr \\leq s \\leq t$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ea4\u6362\u4e09\u8fdb\u5236$n$-\u7acb\u65b9\u4f53\u7684\u6545\u969c\u76f4\u5f84\u548c\u5bbd\u76f4\u5f84\uff0c\u7ed9\u51fa\u4e86\u5b83\u4eec\u7684\u4e0a\u4e0b\u754c\uff0c\u8303\u56f4\u5728$n+3$\u5230$n+5$\u4e4b\u95f4\u3002", "motivation": "\u8bc4\u4f30\u4ea4\u6362\u4e09\u8fdb\u5236$n$-\u7acb\u65b9\u4f53\u7f51\u7edc\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u901a\u8fc7\u6545\u969c\u76f4\u5f84\u548c\u5bbd\u76f4\u5f84\u8861\u91cf\u5176\u5bb9\u9519\u6027\u548c\u4f20\u8f93\u6548\u7387\u3002", "method": "\u5206\u6790\u4e86\u4ea4\u6362\u4e09\u8fdb\u5236$n$-\u7acb\u65b9\u4f53$E3C(r, s, t)$\u7684\u7ed3\u6784\uff0c\u63a8\u5bfc\u4e86\u5176\u5728$1 \\leq r \\leq s \\leq t$\u6761\u4ef6\u4e0b\u7684\u6545\u969c\u76f4\u5f84$(2r + 1)$\u548c\u5bbd\u76f4\u5f84$(2r + 2)$\u3002", "result": "\u5f97\u51fa\u6545\u969c\u76f4\u5f84\u548c\u5bbd\u76f4\u5f84\u7684\u4e0a\u4e0b\u754c\u5206\u522b\u4e3a$n + 3$\u548c$n + 5$\u3002", "conclusion": "\u4ea4\u6362\u4e09\u8fdb\u5236$n$-\u7acb\u65b9\u4f53\u5728\u6545\u969c\u5bb9\u5fcd\u548c\u4f20\u8f93\u6548\u7387\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06584", "pdf": "https://arxiv.org/pdf/2508.06584", "abs": "https://arxiv.org/abs/2508.06584", "authors": ["Kalana Wijegunarathna", "Kristin Stock", "Christopher B. Jones"], "title": "Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "The development, integration, and maintenance of geospatial databases rely\nheavily on efficient and accurate matching procedures of Geospatial Entity\nResolution (ER). While resolution of points-of-interest (POIs) has been widely\naddressed, resolution of entities with diverse geometries has been largely\noverlooked. This is partly due to the lack of a uniform technique for embedding\nheterogeneous geometries seamlessly into a neural network framework. Existing\nneural approaches simplify complex geometries to a single point, resulting in\nsignificant loss of spatial information. To address this limitation, we propose\nOmni, a geospatial ER model featuring an omni-geometry encoder. This encoder is\ncapable of embedding point, line, polyline, polygon, and multi-polygon\ngeometries, enabling the model to capture the complex geospatial intricacies of\nthe places being compared. Furthermore, Omni leverages transformer-based\npre-trained language models over individual textual attributes of place records\nin an Attribute Affinity mechanism. The model is rigorously tested on existing\npoint-only datasets and a new diverse-geometry geospatial ER dataset. Omni\nproduces up to 12% (F1) improvement over existing methods.\n  Furthermore, we test the potential of Large Language Models (LLMs) to conduct\ngeospatial ER, experimenting with prompting strategies and learning scenarios,\ncomparing the results of pre-trained language model-based methods with LLMs.\nResults indicate that LLMs show competitive results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmni\u7684\u5730\u7406\u7a7a\u95f4\u5b9e\u4f53\u89e3\u6790\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u51e0\u4f55\u4f53\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6837\u51e0\u4f55\u4f53\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\u3002\u540c\u65f6\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5730\u7406\u7a7a\u95f4\u5b9e\u4f53\u89e3\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u7a7a\u95f4\u5b9e\u4f53\u89e3\u6790\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6837\u51e0\u4f55\u4f53\u65f6\u5b58\u5728\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u6280\u672f\u6709\u6548\u5d4c\u5165\u5f02\u6784\u51e0\u4f55\u4f53\u3002", "method": "\u63d0\u51faOmni\u6a21\u578b\uff0c\u91c7\u7528\u5168\u51e0\u4f55\u7f16\u7801\u5668\u5904\u7406\u70b9\u3001\u7ebf\u3001\u591a\u8fb9\u5f62\u7b49\u51e0\u4f55\u4f53\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6587\u672c\u5c5e\u6027\u3002\u8fd8\u6d4b\u8bd5\u4e86LLMs\u7684\u8868\u73b0\u3002", "result": "Omni\u5728\u70b9\u6570\u636e\u96c6\u548c\u65b0\u591a\u6837\u51e0\u4f55\u4f53\u6570\u636e\u96c6\u4e0aF1\u63d0\u5347\u8fbe12%\u3002LLMs\u4e5f\u663e\u793a\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "Omni\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u7a7a\u95f4\u5b9e\u4f53\u89e3\u6790\u7684\u6027\u80fd\uff0c\u540c\u65f6LLMs\u5c55\u73b0\u4e86\u6f5c\u5728\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.06978", "pdf": "https://arxiv.org/pdf/2508.06978", "abs": "https://arxiv.org/abs/2508.06978", "authors": ["Kwanhee Kyung", "Sungmin Yun", "Jung Ho Ahn"], "title": "SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency", "categories": ["cs.AR"], "comment": "4 pages, 6 figures, accepted at IEEE Computer Architecture Letters", "summary": "Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to\ntrillions of parameters but require vast memory, motivating a line of research\nto offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.\nWhile SSDs provide cost-effective capacity, their read energy per bit is\nsubstantially higher than that of DRAM. This paper quantitatively analyzes the\nenergy implications of offloading MoE expert weights to SSDs during the\ncritical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory\n(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that\noffloading MoE weights to current SSDs drastically increases\nper-token-generation energy consumption (e.g., by up to ~12x compared to the\nHBM baseline), dominating the total inference energy budget. Although\ntechniques like prefetching effectively hide access latency, they cannot\nmitigate this fundamental energy penalty. We further explore future\ntechnological scaling, finding that the inherent sparsity of MoE models could\npotentially make SSDs energy-viable if Flash read energy improves\nsignificantly, roughly by an order of magnitude.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5c06MoE\u4e13\u5bb6\u6743\u91cd\u4eceHBM\u5378\u8f7d\u5230SSD\u5bf9LLM\u63a8\u7406\u89e3\u7801\u9636\u6bb5\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f53\u524dSSD\u663e\u8457\u589e\u52a0\u80fd\u8017\uff0c\u672a\u6765\u9700Flash\u8bfb\u53d6\u80fd\u8017\u5927\u5e45\u6539\u5584\u624d\u80fd\u4f7f\u5176\u53ef\u884c\u3002", "motivation": "MoE\u6a21\u578b\u7684\u53c2\u6570\u89c4\u6a21\u5e9e\u5927\uff0c\u9700\u8981\u4eceHBM\u5378\u8f7d\u5230SSD\u4ee5\u8282\u7701\u5185\u5b58\uff0c\u4f46SSD\u7684\u9ad8\u8bfb\u53d6\u80fd\u8017\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\u6bd4\u8f83SSD\u3001DDR\u548cHBM\u5728\u4e0d\u540c\u5b58\u50a8\u573a\u666f\u4e0b\u7684\u80fd\u8017\uff0c\u8bc4\u4f30MoE\u6743\u91cd\u5378\u8f7d\u7684\u5f71\u54cd\u3002", "result": "\u5f53\u524dSSD\u5378\u8f7d\u4f1a\u4f7f\u6bcftoken\u751f\u6210\u7684\u80fd\u8017\u589e\u52a0\u9ad8\u8fbe12\u500d\uff0c\u9884\u53d6\u6280\u672f\u65e0\u6cd5\u89e3\u51b3\u80fd\u8017\u95ee\u9898\u3002\u672a\u6765Flash\u8bfb\u53d6\u80fd\u8017\u9700\u663e\u8457\u6539\u5584\uff08\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\uff09\u624d\u80fd\u4f7fSSD\u53ef\u884c\u3002", "conclusion": "\u867d\u7136SSD\u63d0\u4f9b\u4f4e\u6210\u672c\u5bb9\u91cf\uff0c\u4f46\u5176\u9ad8\u80fd\u8017\u9650\u5236\u4e86\u5f53\u524d\u9002\u7528\u6027\uff0c\u672a\u6765\u6280\u672f\u8fdb\u6b65\u53ef\u80fd\u6539\u53d8\u8fd9\u4e00\u5c40\u9762\u3002"}}
{"id": "2508.06526", "pdf": "https://arxiv.org/pdf/2508.06526", "abs": "https://arxiv.org/abs/2508.06526", "authors": ["Dong Liu", "Yanxuan Yu", "Ben Lengerich", "Ying Nian Wu", "Xuhong Wang"], "title": "PiKV: KV Cache Management System for Mixture of Experts", "categories": ["cs.DC", "cs.AI", "cs.AR"], "comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV", "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPiKV\u7684\u5e76\u884c\u5206\u5e03\u5f0fKV\u7f13\u5b58\u670d\u52a1\u6846\u67b6\uff0c\u4e13\u4e3aMoE\u67b6\u6784\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u591aGPU\u548c\u591a\u8282\u70b9\u63a8\u7406\u4e2d\u7684KV\u7f13\u5b58\u5b58\u50a8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591aGPU\u548c\u591a\u8282\u70b9\u63a8\u7406\u4e2dKV\u7f13\u5b58\u5b58\u50a8\u7684\u5185\u5b58\u548c\u901a\u4fe1\u6210\u672c\u74f6\u9888\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u5206\u7247KV\u5b58\u50a8\u3001PiKV\u8def\u7531\u51cf\u5c11\u8bbf\u95ee\u3001PiKV\u8c03\u5ea6\u81ea\u9002\u5e94\u4fdd\u7559\u67e5\u8be2\u76f8\u5173\u6761\u76ee\uff0c\u5e76\u96c6\u6210\u538b\u7f29\u6a21\u5757\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "result": "PiKV\u5df2\u5f00\u6e90\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u80fd\u6709\u6548\u7ba1\u7406KV\u7f13\u5b58\uff0c\u5e76\u4e0eNvidia kvpress\u96c6\u6210\u52a0\u901f\u3002", "conclusion": "PiKV\u65e8\u5728\u6210\u4e3aMoE\u67b6\u6784\u7684\u5168\u9762KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\u3002"}}
{"id": "2508.06615", "pdf": "https://arxiv.org/pdf/2508.06615", "abs": "https://arxiv.org/abs/2508.06615", "authors": ["Ryan Erik Landvater", "Navin Kathawa", "Mustafa Yousif MD", "Ulysses Balis MD"], "title": "Iris RESTful Server and IrisTileSource: An Iris implementation for existing OpenSeaDragon viewers", "categories": ["cs.NI"], "comment": "10 pages, 4 figures, 1 table", "summary": "The Iris File Extension (IFE) is a low overhead performance-oriented whole\nslide image (WSI) file format designed to improve the image rendering\nexperience for pathologists and simplify image management for system\nadministrators. However, static hypertext transfer protocol (HTTP) file servers\ncannot natively stream subregions of high-resolution image files, such as the\nIFE. The majority of contemporary WSI viewer systems are designed as\nbrowser-based web applications and leverage OpenSeaDragon as the tile-based\nrendering framework. These systems convert WSI files to Deep Zoom Images (DZI)\nfor compatibility with simple static HTTP file servers. In order to address\nthis limitation, we have developed the Iris RESTful Server, a low-overhead HTTP\nserver with a RESTful API that is natively compatible with the DICOMweb WADO-RS\nAPI. Written in C++ with Boost Beast HTTP and Asio networking libraries atop\nthe public IFE libraries, the server offers both security and high performance.\nTesting shows that a single instance can handle over 5000 tile requests per\nsecond with a median latency of 21 ms on a private network. We also developed\nand merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful\nAPI, into the next OpenSeaDragon release, enabling simple and immediate drop-in\nreplacement of DZI images within WSI viewer stacks. Designed as a secure\ncross-origin resource sharing microservice, this architecture includes detailed\ndeployment instructions for new or existing WSI workflows, and the public\nexamples.restful.irisdigitialpathology.org subdomain is provided as a\ndevelopment tool to accelerate WSI web viewer development.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Iris RESTful Server\uff0c\u4e00\u79cd\u4f4e\u5f00\u9500\u7684HTTP\u670d\u52a1\u5668\uff0c\u65e8\u5728\u6539\u8fdb\u9ad8\u5206\u8fa8\u7387WSI\u6587\u4ef6\u7684\u5b50\u533a\u57df\u6d41\u5f0f\u4f20\u8f93\uff0c\u5e76\u517c\u5bb9DICOMweb WADO-RS API\u3002", "motivation": "\u89e3\u51b3\u9759\u6001HTTP\u6587\u4ef6\u670d\u52a1\u5668\u65e0\u6cd5\u539f\u751f\u6d41\u5f0f\u4f20\u8f93\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5b50\u533a\u57df\u7684\u95ee\u9898\uff0c\u63d0\u5347\u75c5\u7406\u5b66\u5bb6\u7684\u5de5\u4f5c\u6548\u7387\u548c\u7cfb\u7edf\u7ba1\u7406\u5458\u7684\u56fe\u50cf\u7ba1\u7406\u4fbf\u6377\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eC++\u548cBoost Beast HTTP\u7684Iris RESTful Server\uff0c\u652f\u6301RESTful API\u5e76\u4e0eDICOMweb WADO-RS API\u517c\u5bb9\uff0c\u540c\u65f6\u4e3aOpenSeaDragon\u5f00\u53d1\u4e86\u65b0\u7684TileSource\u6a21\u5757\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u5355\u4e2a\u670d\u52a1\u5668\u5b9e\u4f8b\u6bcf\u79d2\u53ef\u5904\u7406\u8d85\u8fc75000\u4e2a\u74e6\u7247\u8bf7\u6c42\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u4e3a21\u6beb\u79d2\uff0c\u4e14\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709WSI\u5de5\u4f5c\u6d41\u4e2d\u3002", "conclusion": "Iris RESTful Server\u4e3aWSI\u56fe\u50cf\u7684\u9ad8\u6548\u6d41\u5f0f\u4f20\u8f93\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u548c\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7b80\u5316\u4e86\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2508.06617", "pdf": "https://arxiv.org/pdf/2508.06617", "abs": "https://arxiv.org/abs/2508.06617", "authors": ["Md Arafat Hossain", "Xingfu Wu", "Valerie Taylor", "Ali Jannesari"], "title": "Generalizing Scaling Laws for Dense and Sparse Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": "8 pages, 8 figures", "summary": "Over the past few years, the size of language models has grown exponentially,\nas has the computational cost to train these large models. This rapid growth\nhas motivated researchers to develop new techniques aimed at enhancing the\nefficiency of the training process. Despite these advancements, optimally\npredicting the model size or allocating optimal resources remains a challenge.\nSeveral efforts have addressed the challenge by proposing different scaling\nlaws, but almost all of them are architecture-specific (dense or sparse). In\nthis work we revisit existing scaling laws and propose a generalized scaling\nlaw to provide a unified framework that is applicable to both dense and sparse\nlarge language models. We evaluate and compare our proposed scaling law with\nexisting scaling laws to demonstrate its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5bc6\u96c6\u548c\u7a00\u758f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u7f29\u653e\u5b9a\u5f8b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u67b6\u6784\u7279\u5b9a\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u6025\u5267\u589e\u957f\uff0c\u9700\u8981\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u800c\u73b0\u6709\u7684\u7f29\u653e\u5b9a\u5f8b\u591a\u4e3a\u7279\u5b9a\u67b6\u6784\uff08\u5bc6\u96c6\u6216\u7a00\u758f\uff09\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7f29\u653e\u5b9a\u5f8b\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u9002\u7528\u4e8e\u5bc6\u96c6\u548c\u7a00\u758f\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u6bd4\u8f83\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u901a\u7528\u7f29\u653e\u5b9a\u5f8b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7f29\u653e\u5b9a\u5f8b\u6846\u67b6\uff0c\u4e3a\u4f18\u5316\u6a21\u578b\u89c4\u6a21\u548c\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06942", "pdf": "https://arxiv.org/pdf/2508.06942", "abs": "https://arxiv.org/abs/2508.06942", "authors": ["Zhenchang Xing", "Yang Liu", "Zhuo Cheng", "Qing Huang", "Dehai Zhao", "Daniel Sun", "Chenhua Liu"], "title": "When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust \"APIs'' for Human-AI Interaction", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "With the growing capabilities of large language models (LLMs), they are\nincreasingly applied in areas like intelligent customer service, code\ngeneration, and knowledge management. Natural language (NL) prompts act as the\n``APIs'' for human-LLM interaction. To improve prompt quality, best practices\nfor prompt engineering (PE) have been developed, including writing guidelines\nand templates. Building on this, we propose Controlled NL for Prompt (CNL-P),\nwhich not only incorporates PE best practices but also draws on key principles\nfrom software engineering (SE). CNL-P introduces precise grammar structures and\nstrict semantic norms, further eliminating NL's ambiguity, allowing for a\ndeclarative but structured and accurate expression of user intent. This helps\nLLMs better interpret and execute the prompts, leading to more consistent and\nhigher-quality outputs. We also introduce an NL2CNL-P conversion tool based on\nLLMs, enabling users to write prompts in NL, which are then transformed into\nCNL-P format, thus lowering the learning curve of CNL-P. In particular, we\ndevelop a linting tool that checks CNL-P prompts for syntactic and semantic\naccuracy, applying static analysis techniques to NL for the first time.\nExtensive experiments demonstrate that CNL-P enhances the quality of LLM\nresponses through the novel and organic synergy of PE and SE. We believe that\nCNL-P can bridge the gap between emerging PE and traditional SE, laying the\nfoundation for a new programming paradigm centered around NL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCNL-P\u7684\u63a7\u5236\u6027\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u5de5\u7a0b\u548c\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u4ee5\u51cf\u5c11\u81ea\u7136\u8bed\u8a00\u7684\u6b67\u4e49\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u5176\u5e94\u7528\u9886\u57df\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u6b67\u4e49\u6027\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u8f6f\u4ef6\u5de5\u7a0b\u7684\u539f\u5219\uff0c\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faCNL-P\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u8bed\u6cd5\u7ed3\u6784\u548c\u8bed\u4e49\u89c4\u8303\u6d88\u9664\u81ea\u7136\u8bed\u8a00\u7684\u6b67\u4e49\uff0c\u5e76\u5f00\u53d1\u4e86NL2CNL-P\u8f6c\u6362\u5de5\u5177\u53ca\u9759\u6001\u68c0\u67e5\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCNL-P\u663e\u8457\u63d0\u5347\u4e86LLM\u8f93\u51fa\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u63d0\u793a\u5de5\u7a0b\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6709\u673a\u7ed3\u5408\u3002", "conclusion": "CNL-P\u5f25\u8865\u4e86\u63d0\u793a\u5de5\u7a0b\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u4ee5\u81ea\u7136\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u65b0\u7f16\u7a0b\u8303\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06732", "pdf": "https://arxiv.org/pdf/2508.06732", "abs": "https://arxiv.org/abs/2508.06732", "authors": ["Yuya Kawakami", "Daniel Cayan", "Dongyu Liu", "Kwan-Liu Ma"], "title": "ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Ensemble datasets are ever more prevalent in various scientific domains. In\nclimate science, ensemble datasets are used to capture variability in\nprojections under plausible future conditions including greenhouse and aerosol\nemissions. Each ensemble model run produces projections that are fundamentally\nsimilar yet meaningfully distinct. Understanding this variability among\nensemble model runs and analyzing its magnitude and patterns is a vital task\nfor climate scientists. In this paper, we present ClimateSOM, a visual analysis\nworkflow that leverages a self-organizing map (SOM) and Large Language Models\n(LLMs) to support interactive exploration and interpretation of climate\nensemble datasets. The workflow abstracts climate ensemble model runs -\nspatiotemporal time series - into a distribution over a 2D space that captures\nthe variability among the ensemble model runs using a SOM. LLMs are integrated\nto assist in sensemaking of this SOM-defined 2D space, the basis for the visual\nanalysis tasks. In all, ClimateSOM enables users to explore the variability\namong ensemble model runs, identify patterns, compare and cluster the ensemble\nmodel runs. To demonstrate the utility of ClimateSOM, we apply the workflow to\nan ensemble dataset of precipitation projections over California and the\nNorthwestern United States. Furthermore, we conduct a short evaluation of our\nLLM integration, and conduct an expert review of the visual workflow and the\ninsights from the case studies with six domain experts to evaluate our approach\nand its utility.", "AI": {"tldr": "ClimateSOM\u662f\u4e00\u79cd\u53ef\u89c6\u5316\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u81ea\u7ec4\u7ec7\u6620\u5c04(SOM)\u548c\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\uff0c\u7528\u4e8e\u63a2\u7d22\u548c\u89e3\u91ca\u6c14\u5019\u96c6\u5408\u6570\u636e\u96c6\u7684\u53ef\u53d8\u6027\u3002", "motivation": "\u6c14\u5019\u79d1\u5b66\u4e2d\u96c6\u5408\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u62df\u672a\u6765\u6761\u4ef6\u4e0b\u7684\u53d8\u5f02\u6027\uff0c\u4f46\u5982\u4f55\u5206\u6790\u548c\u7406\u89e3\u8fd9\u4e9b\u53d8\u5f02\u6027\u7684\u6a21\u5f0f\u548c\u5e45\u5ea6\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528SOM\u5c06\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u62bd\u8c61\u4e3a2D\u7a7a\u95f4\u5206\u5e03\uff0c\u5e76\u96c6\u6210LLMs\u8f85\u52a9\u89e3\u91ca\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3001\u805a\u7c7b\u548c\u6bd4\u8f83\u3002", "result": "ClimateSOM\u6210\u529f\u5e94\u7528\u4e8e\u7f8e\u56fd\u897f\u5317\u90e8\u964d\u6c34\u9884\u6d4b\u7684\u96c6\u5408\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "ClimateSOM\u4e3a\u6c14\u5019\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u548c\u7406\u89e3\u96c6\u5408\u6a21\u578b\u7684\u53ef\u53d8\u6027\u3002"}}
{"id": "2508.07289", "pdf": "https://arxiv.org/pdf/2508.07289", "abs": "https://arxiv.org/abs/2508.07289", "authors": ["Ramadhan J. Mstafa"], "title": "Reversible Video Steganography Using Quick Response Codes and Modified ElGamal Cryptosystem", "categories": ["cs.MM", "cs.CR"], "comment": "20 Pages, 10 Figures, 3 Tables", "summary": "The rapid transmission of multimedia information has been achieved mainly by\nrecent advancements in the Internet's speed and information technology. In\nspite of this, advancements in technology have resulted in breaches of privacy\nand data security. When it comes to protecting private information in today's\nInternet era, digital steganography is vital. Many academics are interested in\ndigital video because it has a great capability for concealing important data.\nThere have been a vast number of video steganography solutions developed lately\nto guard against the theft of confidential data. The visual imperceptibility,\nrobustness, and embedding capacity of these approaches are all challenges that\nmust be addressed. In this paper, a novel solution to reversible video\nsteganography based on DWT and QR codes is proposed to address these concerns.\nIn order to increase the security level of the suggested method, an enhanced\nElGamal cryptosystem has also been proposed. Prior to the embedding stage, the\nsuggested method uses the modified ElGamal algorithm to encrypt secret QR\ncodes. Concurrently, it applies two-dimensional DWT on the Y-component of each\nvideo frame resulting in LL, LH, HL, and HH sub-bands. Then, the encrypted Low\n(L), Medium (M), Quantile (Q), and High (H) QR codes are embedded into the HL\nsub-band, HH sub-band, U-component, and V-component of video frames,\nrespectively, using the LSB technique. As a consequence of extensive testing of\nthe approach, it was shown to be very secure and highly invisible, as well as\nhighly resistant to attacks from Salt & Pepper, Gaussian, Poisson, and Speckle\nnoises, which has an average SSIM of more than 0.91. Aside from visual\nimperceptibility, the suggested method exceeds current methods in terms of PSNR\naverage of 52.143 dB, and embedding capacity 1 bpp.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDWT\u548cQR\u7801\u7684\u53ef\u9006\u89c6\u9891\u9690\u5199\u65b9\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u7684ElGamal\u52a0\u5bc6\u7cfb\u7edf\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6297\u653b\u51fb\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u5a92\u4f53\u4fe1\u606f\u4f20\u8f93\u8fc5\u901f\uff0c\u4f46\u9690\u79c1\u548c\u6570\u636e\u5b89\u5168\u95ee\u9898\u7a81\u51fa\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u9690\u5199\u6280\u672f\u6765\u4fdd\u62a4\u673a\u5bc6\u4fe1\u606f\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684ElGamal\u52a0\u5bc6QR\u7801\uff0c\u5e76\u901a\u8fc7DWT\u548cLSB\u6280\u672f\u5728\u89c6\u9891\u5e27\u7684\u591a\u4e2a\u5b50\u5e26\u548c\u5206\u91cf\u4e2d\u5d4c\u5165\u6570\u636e\u3002", "result": "\u65b9\u6cd5\u5177\u6709\u9ad8\u5b89\u5168\u6027\u3001\u9ad8\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u6027\uff0c\u6297\u591a\u79cd\u566a\u58f0\u653b\u51fb\uff0c\u5e73\u5747SSIM>0.91\uff0cPSNR\u5e73\u574752.143 dB\uff0c\u5d4c\u5165\u5bb9\u91cf1 bpp\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u6027\u3001\u6297\u653b\u51fb\u80fd\u529b\u548c\u5d4c\u5165\u5bb9\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u5408\u4fdd\u62a4\u673a\u5bc6\u6570\u636e\u3002"}}
{"id": "2508.07615", "pdf": "https://arxiv.org/pdf/2508.07615", "abs": "https://arxiv.org/abs/2508.07615", "authors": ["Chuanfu Hu", "Aimin Hou"], "title": "Verification Method for Graph Isomorphism Criteria", "categories": ["cs.GR"], "comment": "17 pages, 5 figures, 2 tables", "summary": "The criteria for determining graph isomorphism are crucial for solving graph\nisomorphism problems. The necessary condition is that two isomorphic graphs\npossess invariants, but their function can only be used to filtrate and\nsubdivide candidate spaces. The sufficient conditions are used to rebuild the\nisomorphic reconstruction of special graphs, but their drawback is that the\nisomorphic functions of subgraphs may not form part of the isomorphic functions\nof the parent graph. The use of sufficient or necessary conditions generally\nresults in backtracking to ensure the correctness of the decision algorithm.\nThe sufficient and necessary conditions can ensure that the determination of\ngraph isomorphism does not require backtracking, but the correctness of its\nproof process is difficult to guarantee. This article proposes a verification\nmethod that can correctly determine whether the judgment conditions proposed by\nprevious researchers are sufficient and necessary conditions. A subdivision\nmethod has also been proposed in this article, which can obtain more\nsubdivisions for necessary conditions and effectively reduce the size of\nbacktracking space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5224\u65ad\u524d\u4eba\u63d0\u51fa\u7684\u56fe\u540c\u6784\u5224\u5b9a\u6761\u4ef6\u662f\u5426\u5145\u5206\u4e14\u5fc5\u8981\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u5206\u65b9\u6cd5\u4ee5\u51cf\u5c11\u56de\u6eaf\u7a7a\u95f4\u3002", "motivation": "\u56fe\u540c\u6784\u5224\u5b9a\u7684\u6807\u51c6\u5bf9\u89e3\u51b3\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e2d\u5145\u5206\u548c\u5fc5\u8981\u6761\u4ef6\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u9700\u8981\u56de\u6eaf\u6216\u8bc1\u660e\u8fc7\u7a0b\u96be\u4ee5\u4fdd\u8bc1\u6b63\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u9a8c\u8bc1\u65b9\u6cd5\u548c\u7ec6\u5206\u65b9\u6cd5\uff0c\u524d\u8005\u7528\u4e8e\u786e\u8ba4\u6761\u4ef6\u7684\u5145\u5206\u5fc5\u8981\u6027\uff0c\u540e\u8005\u901a\u8fc7\u7ec6\u5206\u5fc5\u8981\u6761\u4ef6\u7684\u5019\u9009\u7a7a\u95f4\u51cf\u5c11\u56de\u6eaf\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5224\u65ad\u6761\u4ef6\u7684\u5145\u5206\u5fc5\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u7ec6\u5206\u51cf\u5c11\u56de\u6eaf\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u9a8c\u8bc1\u548c\u7ec6\u5206\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u9ad8\u4e86\u56fe\u540c\u6784\u5224\u5b9a\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07207", "pdf": "https://arxiv.org/pdf/2508.07207", "abs": "https://arxiv.org/abs/2508.07207", "authors": ["S. Akshay", "A. R. Balasubramanian", "Supratik Chakraborty", "Georg Zetzsche"], "title": "Presburger Functional Synthesis: Complexity and Tractable Normal Forms", "categories": ["cs.LO", "cs.AI"], "comment": "Full version of conference paper at KR 2025 (22nd International\n  Conference on Principles of Knowledge Representation and Reasoning)", "summary": "Given a relational specification between inputs and outputs as a logic\nformula, the problem of functional synthesis is to automatically synthesize a\nfunction from inputs to outputs satisfying the relation. Recently, a rich line\nof work has emerged tackling this problem for specifications in different\ntheories, from Boolean to general first-order logic. In this paper, we launch\nan investigation of this problem for the theory of Presburger Arithmetic, that\nwe call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved\nin EXPTIME and provide a matching exponential lower bound. This is unlike the\ncase for Boolean functional synthesis (BFnS), where only conditional\nexponential lower bounds are known. Further, we show that PFnS for one input\nand one output variable is as hard as BFnS in general. We then identify a\nspecial normal form, called PSyNF, for the specification formula that\nguarantees poly-time and poly-size solvability of PFnS. We prove several\nproperties of PSyNF, including how to check and compile to this form, and\nconditions under which any other form that guarantees poly-time solvability of\nPFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic\nnormal form that is easier to check but is exponentially less succinct than\nPSyNF.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Presburger\u7b97\u672f\u7406\u8bba\u4e2d\u7684\u529f\u80fd\u5408\u6210\u95ee\u9898\uff08PFnS\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728EXPTIME\u5185\u53ef\u89e3\u51b3\uff0c\u5e76\u63d0\u4f9b\u5339\u914d\u7684\u6307\u6570\u4e0b\u9650\u3002\u6b64\u5916\uff0c\u53d1\u73b0\u5355\u8f93\u5165\u5355\u53d8\u91cf\u7684PFnS\u4e0e\u4e00\u822c\u5e03\u5c14\u529f\u80fd\u5408\u6210\uff08BFnS\uff09\u96be\u5ea6\u76f8\u540c\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7279\u6b8a\u89c4\u8303\u5f62\u5f0fPSyNF\uff0c\u786e\u4fddPFnS\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u89e3\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u89e3\u51b3Presburger\u7b97\u672f\u7406\u8bba\u4e2d\u7684\u529f\u80fd\u5408\u6210\u95ee\u9898\uff0c\u586b\u8865\u4e86\u4ece\u5e03\u5c14\u5230\u4e00\u9636\u903b\u8f91\u7684\u5408\u6210\u95ee\u9898\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bc1\u660ePFnS\u5728EXPTIME\u5185\u53ef\u89e3\u51b3\uff0c\u5e76\u63d0\u4f9b\u6307\u6570\u4e0b\u9650\u3002\u63d0\u51faPSyNF\u89c4\u8303\u5f62\u5f0f\uff0c\u786e\u4fdd\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u7f16\u8bd1\u548c\u68c0\u67e5\u65b9\u6cd5\u3002", "result": "PFnS\u5728EXPTIME\u5185\u6709\u89e3\uff0c\u4e0e\u5e03\u5c14\u529f\u80fd\u5408\u6210\u76f8\u6bd4\u5177\u6709\u660e\u786e\u7684\u6307\u6570\u4e0b\u9650\u3002PSyNF\u5f62\u5f0f\u80fd\u9ad8\u6548\u89e3\u51b3PFnS\uff0c\u4e14\u5176\u4ed6\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u7684\u5f62\u5f0f\u5747\u53ef\u7f16\u8bd1\u4e3aPSyNF\u3002", "conclusion": "PFnS\u7684\u590d\u6742\u6027\u663e\u8457\u9ad8\u4e8eBFnS\uff0cPSyNF\u5f62\u5f0f\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2508.06814", "pdf": "https://arxiv.org/pdf/2508.06814", "abs": "https://arxiv.org/abs/2508.06814", "authors": ["Jinjin Zhao", "Sanjay Krishnan"], "title": "Metadata Management for AI-Augmented Data Workflows", "categories": ["cs.DB"], "comment": null, "summary": "AI-augmented data workflows introduce complex governance challenges, as both\nhuman and model-driven processes generate, transform, and consume data\nartifacts. These workflows blend heterogeneous tools, dynamic execution\npatterns, and opaque model decisions, making comprehensive metadata capture\ndifficult. In this work, we present TableVault, a metadata governance framework\ndesigned for human-AI collaborative data creation. TableVault records ingestion\nevents, traces operation status, links execution parameters to their data\norigins, and exposes a standardized metadata layer. By combining\ndatabase-inspired guarantees with AI-oriented design, such as declarative\noperation builders and lineage-aware references, TableVault supports\ntransparency and reproducibility across mixed human-model pipelines. Through a\ndocument classification case study, we demonstrate how TableVault preserves\ndetailed lineage and operational context, enabling robust metadata management,\neven in partially observable execution environments.", "AI": {"tldr": "TableVault\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u6570\u636e\u521b\u5efa\u7684\u5143\u6570\u636e\u6cbb\u7406\u6846\u67b6\uff0c\u89e3\u51b3AI\u589e\u5f3a\u6570\u636e\u5de5\u4f5c\u6d41\u4e2d\u7684\u6cbb\u7406\u96be\u9898\u3002", "motivation": "AI\u589e\u5f3a\u7684\u6570\u636e\u5de5\u4f5c\u6d41\u6d89\u53ca\u590d\u6742\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u52a8\u6001\u6267\u884c\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5143\u6570\u636e\u6355\u83b7\u56f0\u96be\uff0c\u9700\u8981\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684\u6cbb\u7406\u6846\u67b6\u3002", "method": "TableVault\u901a\u8fc7\u8bb0\u5f55\u4e8b\u4ef6\u3001\u8ffd\u8e2a\u64cd\u4f5c\u72b6\u6001\u3001\u94fe\u63a5\u6267\u884c\u53c2\u6570\u4e0e\u6570\u636e\u6765\u6e90\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316\u5143\u6570\u636e\u5c42\uff0c\u7ed3\u5408\u6570\u636e\u5e93\u4fdd\u8bc1\u548cAI\u5bfc\u5411\u8bbe\u8ba1\u3002", "result": "\u5728\u6587\u6863\u5206\u7c7b\u6848\u4f8b\u4e2d\uff0cTableVault\u6210\u529f\u4fdd\u7559\u4e86\u8be6\u7ec6\u7684\u8c31\u7cfb\u548c\u64cd\u4f5c\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u5143\u6570\u636e\u7ba1\u7406\u3002", "conclusion": "TableVault\u4e3a\u6df7\u5408\u4eba\u673a\u6570\u636e\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u5143\u6570\u636e\u6cbb\u7406\u65b9\u6848\u3002"}}
{"id": "2508.07110", "pdf": "https://arxiv.org/pdf/2508.07110", "abs": "https://arxiv.org/abs/2508.07110", "authors": ["Lorenzo Ruotolo", "Lara Orlandic", "Pengbo Yu", "Moritz Brunion", "Daniele Jahier Pagliari", "Dwaipayan Biswas", "Giovanni Ansaloni", "David Atienza", "Julien Ryckaert", "Francky Catthoor", "Yukai Chen"], "title": "Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes", "categories": ["cs.AR"], "comment": null, "summary": "This paper presents the physical design exploration of a domain-specific\nprocessor (DSIP) architecture targeted at machine learning (ML), addressing the\nchallenges of interconnect efficiency in advanced Angstrom-era technologies.\nThe design emphasizes reduced wire length and high core density by utilizing\nspecialized memory structures and SIMD (Single Instruction, Multiple Data)\nunits. Five configurations are synthesized and evaluated using the IMEC A10\nnanosheet node PDK. Key physical design metrics are compared across\nconfigurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline.\nResults show that our architecture achieves over 2x lower normalized wire\nlength and more than 3x higher density than the SoA, with low variability in\nthe metrics across all configurations, making it a promising solution for\nnext-generation DSIP designs. These improvements are achieved with minimal\nmanual layout intervention, demonstrating the architecture's intrinsic physical\nefficiency and potential for low-cost wire-friendly implementation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7684\u4e13\u7528\u5904\u7406\u5668\u67b6\u6784\uff08DSIP\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u7269\u7406\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5148\u8fdb\u6280\u672f\u8282\u70b9\u4e2d\u7684\u4e92\u8fde\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u9762\u4e34\u5148\u8fdb\u6280\u672f\u8282\u70b9\u4e2d\u4e92\u8fde\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u67b6\u6784\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u5bc6\u5ea6\u3002", "method": "\u5229\u7528\u4e13\u7528\u5185\u5b58\u7ed3\u6784\u548cSIMD\u5355\u5143\uff0c\u8bbe\u8ba1\u4e86\u4e94\u79cd\u914d\u7f6e\uff0c\u5e76\u5728IMEC A10\u7eb3\u7c73\u7247\u8282\u70b9\u4e0a\u8fdb\u884c\u5408\u6210\u4e0e\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u65b0\u67b6\u6784\u7684\u5f52\u4e00\u5316\u7ebf\u957f\u964d\u4f4e\u4e862\u500d\u4ee5\u4e0a\uff0c\u5bc6\u5ea6\u63d0\u9ad8\u4e863\u500d\u4ee5\u4e0a\uff0c\u4e14\u5728\u4e0d\u540c\u914d\u7f6e\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u67b6\u6784\u5177\u6709\u5185\u5728\u7684\u7269\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u4e0b\u4e00\u4ee3DSIP\u8bbe\u8ba1\u3002"}}
{"id": "2508.06948", "pdf": "https://arxiv.org/pdf/2508.06948", "abs": "https://arxiv.org/abs/2508.06948", "authors": ["Jinyuan Chen", "Jiuchen Shi", "Quan Chen", "Minyi Guo"], "title": "Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud", "categories": ["cs.DC"], "comment": null, "summary": "Multi-agent applications utilize the advanced capabilities of large language\nmodels (LLMs) for intricate task completion through agent collaboration in a\nworkflow. Under this situation, requests from different agents usually access\nthe same shared LLM to perform different kinds of tasks, forcing the shared LLM\nto suffer excessive loads. However, existing works have low serving performance\nfor these multi-agent applications, mainly due to the ignorance of inter-agent\nlatency and resource differences for request scheduling. We therefore propose\nKairos, a multi-agent orchestration system that optimizes end-to-end latency\nfor multi-agent applications. Kairos consists of a workflow orchestrator, a\nworkflow-aware priority scheduler, and a memory-aware dispatcher. The\norchestrator collects agent-specific information for online workflow analysis.\nThe scheduler decides the serving priority of the requests based on their\nlatency characteristics to reduce the overall queuing. The dispatcher\ndispatches the requests to different LLM instances based on their memory\ndemands to avoid GPU overloading. Experimental results show that Kairos reduces\nend-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.", "AI": {"tldr": "Kairos\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u8bf7\u6c42\u8c03\u5ea6\u6765\u964d\u4f4e\u591a\u667a\u80fd\u4f53\u5e94\u7528\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\uff0c\u5171\u4eab\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8d1f\u8f7d\u8fc7\u91cd\u548c\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u5bf9\u8de8\u667a\u80fd\u4f53\u5ef6\u8fdf\u548c\u8d44\u6e90\u5dee\u5f02\u7684\u5ffd\u89c6\uff0c\u5bfc\u81f4\u6027\u80fd\u4f4e\u4e0b\u3002", "method": "Kairos\u7531\u5de5\u4f5c\u6d41\u534f\u8c03\u5668\u3001\u5de5\u4f5c\u6d41\u611f\u77e5\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\u548c\u5185\u5b58\u611f\u77e5\u8c03\u5ea6\u5668\u7ec4\u6210\uff0c\u5206\u522b\u8d1f\u8d23\u6536\u96c6\u4fe1\u606f\u3001\u4f18\u5316\u8bf7\u6c42\u4f18\u5148\u7ea7\u548c\u5206\u914d\u8d44\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKairos\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u4e8617.8%\u81f328.4%\u3002", "conclusion": "Kairos\u6709\u6548\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5e94\u7528\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2508.06616", "pdf": "https://arxiv.org/pdf/2508.06616", "abs": "https://arxiv.org/abs/2508.06616", "authors": ["Md Arafat Habib", "Medhat Elsayed", "Yigit Ozcan", "Pedro Enrique Iturria-Rivera", "Majid Bavand", "Melike Erol-Kantarci"], "title": "Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "With the emergence of 6G, mobile networks are becoming increasingly\nheterogeneous and dynamic, necessitating advanced automation for efficient\nmanagement. Intent-Driven Networks (IDNs) address this by translating\nhigh-level intents into optimization policies. Large Language Models (LLMs) can\nenhance this process by understanding complex human instructions to enable\nadaptive, intelligent automation. Given the rapid advancements in Generative AI\n(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated\nRadio Access Network (RAN) environments is both timely and critical. This\narticle provides such a survey, along with a case study on a hierarchical\nlearning-enabled IDN architecture that integrates GenAI across three key\nstages: intent processing, intent validation, and intent execution. Unlike most\nexisting approaches that apply GenAI in the form of LLMs for intent processing\nonly, we propose a hierarchical framework that introduces GenAI across all\nthree stages of IDN. To demonstrate the effectiveness of the proposed IDN\nmanagement architecture, we present a case study based on the latest GenAI\narchitecture named Mamba. The case study shows how the proposed GenAI-driven\narchitecture enhances network performance through intelligent automation,\nsurpassing the performance of the conventional IDN architectures.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684IDN\u67b6\u6784\u57286G\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u667a\u80fd\u81ea\u52a8\u5316\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408GenAI\u7684\u5168\u9636\u6bb5\u5206\u5c42IDN\u6846\u67b6\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u7684\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u8981\u6c42\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u7ba1\u7406\uff0cIDN\u901a\u8fc7\u610f\u56fe\u9a71\u52a8\u4f18\u5316\uff0cLLM\u548cGenAI\u7684\u8fdb\u5c55\u4e3a\u667a\u80fd\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff0c\u5c06GenAI\u6574\u5408\u5230IDN\u7684\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff08\u610f\u56fe\u5904\u7406\u3001\u9a8c\u8bc1\u548c\u6267\u884c\uff09\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eMamba\u67b6\u6784\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u7684GenAI\u9a71\u52a8\u67b6\u6784\u5728\u667a\u80fd\u81ea\u52a8\u5316\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfIDN\u67b6\u6784\u3002", "conclusion": "GenAI\u5728IDN\u5168\u9636\u6bb5\u7684\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\uff0c\u4e3a6G\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06753", "pdf": "https://arxiv.org/pdf/2508.06753", "abs": "https://arxiv.org/abs/2508.06753", "authors": ["Evangelos Georganas", "Dhiraj Kalamkar", "Alexander Heinecke"], "title": "Pushing the Envelope of LLM Inference on AI-PC", "categories": ["cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\uff081/1.58/2-bit\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u95ee\u9898\uff0c\u63d0\u51fa\u4f18\u5316\u7684\u5fae\u5185\u6838\u548c\u8fd0\u884c\u65f6\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u8fd0\u884c\u65f6\u7cfb\u7edf\u7684\u8ba1\u7b97\u6548\u7387\u5c1a\u672a\u5145\u5206\u4f18\u5316\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u9488\u5bf9\u73b0\u4ee3CPU\u4f18\u5316\u76841-bit\u548c2-bit\u5fae\u5185\u6838\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\u4e2d\u3002", "result": "2-bit\u6a21\u578b\u63a8\u7406\u6027\u80fd\u6bd4\u5f53\u524d\u6700\u4f73\u8fd0\u884c\u65f6bitnet.cpp\u63d0\u53472.2\u500d\uff0c\u6bd416-bit\u6a21\u578b\u63a8\u7406\u5feb7\u500d\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u8fd0\u884c\u65f6\u4e3aAI PC\u548c\u8fb9\u7f18\u8bbe\u5907\u4e2d\u9ad8\u6548\u90e8\u7f72\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.06799", "pdf": "https://arxiv.org/pdf/2508.06799", "abs": "https://arxiv.org/abs/2508.06799", "authors": ["Naiyi Li", "Zihui Ma", "Runlong Yu", "Lingyao Li"], "title": "LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning", "categories": ["cs.ET", "cs.AI"], "comment": null, "summary": "Digital Twins (DTs) offer powerful tools for managing complex infrastructure\nsystems, but their effectiveness is often limited by challenges in integrating\nunstructured knowledge. Recent advances in Large Language Models (LLMs) bring\nnew potential to address this gap, with strong abilities in extracting and\norganizing diverse textual information. We therefore propose LSDTs\n(LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract\nplanning knowledge from unstructured documents like environmental regulations\nand technical guidelines, and organize it into a formal ontology. This ontology\nforms a semantic layer that powers a digital twin-a virtual model of the\nphysical system-allowing it to simulate realistic, regulation-aware planning\nscenarios. We evaluate LSDTs through a case study of offshore wind farm\nplanning in Maryland, including its application during Hurricane Sandy. Results\ndemonstrate that LSDTs support interpretable, regulation-aware layout\noptimization, enable high-fidelity simulation, and enhance adaptability in\ninfrastructure planning. This work shows the potential of combining generative\nAI with digital twins to support complex, knowledge-driven planning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LSDTs\u6846\u67b6\uff0c\u5229\u7528LLM\u4ece\u975e\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u63d0\u53d6\u89c4\u5212\u77e5\u8bc6\u5e76\u6784\u5efa\u672c\u4f53\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u8bed\u4e49\u5c42\uff0c\u652f\u6301\u6a21\u62df\u7b26\u5408\u6cd5\u89c4\u7684\u89c4\u5212\u573a\u666f\u3002\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u98ce\u529b\u53d1\u7535\u573a\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\uff08DTs\uff09\u5728\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u4e2d\u9762\u4e34\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u6574\u5408\u7684\u6311\u6218\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86LSDTs\u6846\u67b6\uff0c\u5229\u7528LLM\u63d0\u53d6\u548c\u6574\u7406\u975e\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u7684\u89c4\u5212\u77e5\u8bc6\uff0c\u6784\u5efa\u672c\u4f53\u5e76\u4f5c\u4e3a\u6570\u5b57\u5b6a\u751f\u7684\u8bed\u4e49\u5c42\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660eLSDTs\u80fd\u591f\u652f\u6301\u9ad8\u4fdd\u771f\u6a21\u62df\u3001\u6cd5\u89c4\u611f\u77e5\u7684\u5e03\u5c40\u4f18\u5316\uff0c\u5e76\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u5f0fAI\u4e0e\u6570\u5b57\u5b6a\u751f\u7684\u7ed3\u5408\u5728\u590d\u6742\u77e5\u8bc6\u9a71\u52a8\u89c4\u5212\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.07084", "pdf": "https://arxiv.org/pdf/2508.07084", "abs": "https://arxiv.org/abs/2508.07084", "authors": ["Kaveh Shahedi", "Nana Gyambrah", "Heng Li", "Maxime Lamothe", "Foutse Khomh"], "title": "An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects", "categories": ["cs.SE", "cs.PF"], "comment": null, "summary": "Performance is a critical quality attribute in software development, yet the\nimpact of method-level code changes on performance evolution remains poorly\nunderstood. While developers often make intuitive assumptions about which types\nof modifications are likely to cause performance regressions or improvements,\nthese beliefs lack empirical validation at a fine-grained level. We conducted a\nlarge-scale empirical study analyzing performance evolution in 15 mature\nopen-source Java projects hosted on GitHub. Our analysis encompassed 739\ncommits containing 1,499 method-level code changes, using Java Microbenchmark\nHarness (JMH) for precise performance measurement and rigorous statistical\nanalysis to quantify both the significance and magnitude of performance\nvariations. We employed bytecode instrumentation to capture method-specific\nexecution metrics and systematically analyzed four key aspects: temporal\nperformance patterns, code change type correlations, developer and complexity\nfactors, and domain-size interactions. Our findings reveal that 32.7% of\nmethod-level changes result in measurable performance impacts, with regressions\noccurring 1.3 times more frequently than improvements. Contrary to conventional\nwisdom, we found no significant differences in performance impact distributions\nacross code change categories, challenging risk-stratified development\nstrategies. Algorithmic changes demonstrate the highest improvement potential\nbut carry substantial regression risk. Senior developers produce more stable\nchanges with fewer extreme variations, while code complexity correlates with\nincreased regression likelihood. Domain-size interactions reveal significant\npatterns, with web server + small projects exhibiting the highest performance\ninstability. Our study provides empirical evidence for integrating automated\nperformance testing into continuous integration pipelines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u63a2\u8ba8\u4e86\u65b9\u6cd5\u7ea7\u4ee3\u7801\u53d8\u66f4\u5bf9\u6027\u80fd\u6f14\u5316\u7684\u5f71\u54cd\uff0c\u53d1\u73b032.7%\u7684\u53d8\u66f4\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5176\u4e2d\u6027\u80fd\u9000\u6b65\u6bd4\u6539\u8fdb\u66f4\u5e38\u89c1\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\u98ce\u9669\u5206\u5c42\u5f00\u53d1\u7b56\u7565\u3002", "motivation": "\u6027\u80fd\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u8d28\u91cf\u5c5e\u6027\uff0c\u4f46\u65b9\u6cd5\u7ea7\u4ee3\u7801\u53d8\u66f4\u5bf9\u6027\u80fd\u6f14\u5316\u7684\u5f71\u54cd\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5f00\u53d1\u8005\u7684\u76f4\u89c9\u5047\u8bbe\u5f80\u5f80\u672a\u7ecf\u8bc1\u5b9e\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8615\u4e2a\u6210\u719f\u7684\u5f00\u6e90Java\u9879\u76ee\u7684739\u6b21\u63d0\u4ea4\u4e2d\u76841,499\u4e2a\u65b9\u6cd5\u7ea7\u53d8\u66f4\uff0c\u4f7f\u7528JMH\u8fdb\u884c\u7cbe\u786e\u6027\u80fd\u6d4b\u91cf\uff0c\u901a\u8fc7\u5b57\u8282\u7801\u63d2\u6869\u6355\u83b7\u65b9\u6cd5\u6267\u884c\u6307\u6807\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u65f6\u95f4\u6a21\u5f0f\u3001\u53d8\u66f4\u7c7b\u578b\u3001\u5f00\u53d1\u8005\u4e0e\u590d\u6742\u6027\u56e0\u7d20\u4ee5\u53ca\u9886\u57df\u89c4\u6a21\u4ea4\u4e92\u3002", "result": "\u7814\u7a76\u53d1\u73b032.7%\u7684\u53d8\u66f4\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6027\u80fd\u9000\u6b65\u6bd4\u6539\u8fdb\u591a1.3\u500d\uff1b\u7b97\u6cd5\u53d8\u66f4\u6f5c\u529b\u5927\u4f46\u98ce\u9669\u9ad8\uff1b\u8d44\u6df1\u5f00\u53d1\u8005\u7684\u53d8\u66f4\u66f4\u7a33\u5b9a\uff1b\u4ee3\u7801\u590d\u6742\u6027\u4e0e\u9000\u6b65\u6982\u7387\u6b63\u76f8\u5173\uff1b\u5c0f\u578bWeb\u670d\u52a1\u5668\u9879\u76ee\u6027\u80fd\u6700\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u81ea\u52a8\u6027\u80fd\u6d4b\u8bd5\u5e94\u96c6\u6210\u5230\u6301\u7eed\u96c6\u6210\u6d41\u6c34\u7ebf\u4e2d\uff0c\u4ee5\u6355\u6349\u6027\u80fd\u53d8\u5316\u5e76\u6307\u5bfc\u5f00\u53d1\u5b9e\u8df5\u3002"}}
{"id": "2508.06751", "pdf": "https://arxiv.org/pdf/2508.06751", "abs": "https://arxiv.org/abs/2508.06751", "authors": ["Alex Kale"], "title": "Toward a Logic of Generalization about Visualization as a Decision Aid", "categories": ["cs.HC"], "comment": null, "summary": "Visualization as a discipline often grapples with generalization by reasoning\nabout how study results on the efficacy of a tool in one context might apply to\nanother context. This work offers an account of the logic of generalization in\nvisualization research and argues that it struggles in particular with\napplications of visualization as a decision aid. We use decision theory to\ndefine the dimensions on which decision problems can vary, and we present an\nanalysis of heterogeneity in scenarios where visualization supports\ndecision-making. Our findings identify utility as a focal and under-examined\nconcept in visualization research on decision-making, demonstrating how the\nvisualization community's logic of generalization might benefit from using\ndecision theory as a lens for understanding context variation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u6cdb\u5316\u903b\u8f91\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4f5c\u4e3a\u51b3\u7b56\u8f85\u52a9\u5de5\u5177\u65f6\u7684\u5e94\u7528\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u51b3\u7b56\u7406\u8bba\u4f5c\u4e3a\u7406\u89e3\u60c5\u5883\u53d8\u5316\u7684\u6846\u67b6\u3002", "motivation": "\u53ef\u89c6\u5316\u7814\u7a76\u5e38\u56e0\u60c5\u5883\u5dee\u5f02\u96be\u4ee5\u6cdb\u5316\u7814\u7a76\u7ed3\u679c\uff0c\u5c24\u5176\u5728\u51b3\u7b56\u8f85\u52a9\u5e94\u7528\u4e2d\u8868\u73b0\u660e\u663e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u8fd0\u7528\u51b3\u7b56\u7406\u8bba\u5b9a\u4e49\u4e86\u51b3\u7b56\u95ee\u9898\u7684\u7ef4\u5ea6\uff0c\u5e76\u5206\u6790\u4e86\u53ef\u89c6\u5316\u652f\u6301\u51b3\u7b56\u65f6\u7684\u5f02\u8d28\u6027\u60c5\u666f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6548\u7528\u662f\u53ef\u89c6\u5316\u51b3\u7b56\u7814\u7a76\u4e2d\u4e00\u4e2a\u6838\u5fc3\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u6982\u5ff5\uff0c\u51b3\u7b56\u7406\u8bba\u6709\u52a9\u4e8e\u7406\u89e3\u60c5\u5883\u53d8\u5316\u3002", "conclusion": "\u51b3\u7b56\u7406\u8bba\u53ef\u4f5c\u4e3a\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u7406\u89e3\u60c5\u5883\u53d8\u5316\u7684\u65b0\u89c6\u89d2\uff0c\u63d0\u5347\u6cdb\u5316\u903b\u8f91\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07554", "pdf": "https://arxiv.org/pdf/2508.07554", "abs": "https://arxiv.org/abs/2508.07554", "authors": ["Xusheng He", "Wei Liu", "Shanshan Ma", "Qian Liu", "Chenghao Ma", "Jianlong Wu"], "title": "FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video Understanding", "categories": ["cs.MM"], "comment": null, "summary": "Fine-grained analysis of complex and high-speed sports like badminton\npresents a significant challenge for Multimodal Large Language Models (MLLMs),\ndespite their notable advancements in general video understanding. This\ndifficulty arises primarily from the scarcity of datasets with sufficiently\nrich and domain-specific annotations. To bridge this gap, we introduce\nFineBadminton, a novel and large-scale dataset featuring a unique multi-level\nsemantic annotation hierarchy (Foundational Actions, Tactical Semantics, and\nDecision Evaluation) for comprehensive badminton understanding. The\nconstruction of FineBadminton is powered by an innovative annotation pipeline\nthat synergistically combines MLLM-generated proposals with human refinement.\nWe also present FBBench, a challenging benchmark derived from FineBadminton, to\nrigorously evaluate MLLMs on nuanced spatio-temporal reasoning and tactical\ncomprehension. Together, FineBadminton and FBBench provide a crucial ecosystem\nto catalyze research in fine-grained video understanding and advance the\ndevelopment of MLLMs in sports intelligence. Furthermore, we propose an\noptimized baseline approach incorporating Hit-Centric Keyframe Selection to\nfocus on pivotal moments and Coordinate-Guided Condensation to distill salient\nvisual information. The results on FBBench reveal that while current MLLMs\nstill face significant challenges in deep sports video analysis, our proposed\nstrategies nonetheless achieve substantial performance gains. The project\nhomepage is available at https://finebadminton.github.io/FineBadminton/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86FineBadminton\u6570\u636e\u96c6\u548cFBBench\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u7fbd\u6bdb\u7403\u8fd0\u52a8\u5206\u6790\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u4e30\u5bcc\u7684\u9886\u57df\u7279\u5b9a\u6ce8\u91ca\u6570\u636e\u96c6\uff0cMLLMs\u5728\u590d\u6742\u9ad8\u901f\u8fd0\u52a8\uff08\u5982\u7fbd\u6bdb\u7403\uff09\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86FineBadminton\u548cFBBench\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u521b\u65b0\u7684\u6807\u6ce8\u6d41\u7a0b\uff08\u7ed3\u5408MLLM\u751f\u6210\u5efa\u8bae\u548c\u4eba\u5de5\u7ec6\u5316\uff09\u6784\u5efaFineBadminton\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1FBBench\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86Hit-Centric\u5173\u952e\u5e27\u9009\u62e9\u548cCoordinate-Guided Condensation\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728FBBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524dMLLMs\u5728\u6df1\u5c42\u4f53\u80b2\u89c6\u9891\u5206\u6790\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f46\u63d0\u51fa\u7684\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "FineBadminton\u548cFBBench\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u548cMLLMs\u5728\u4f53\u80b2\u667a\u80fd\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2508.07852", "pdf": "https://arxiv.org/pdf/2508.07852", "abs": "https://arxiv.org/abs/2508.07852", "authors": ["Rui Su", "Honghao Dong", "Haojie Jin", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Vertex Features for Neural Global Illumination", "categories": ["cs.GR", "cs.AI"], "comment": "Accepted by ACM SIGGRAPH Asia'2025", "summary": "Recent research on learnable neural representations has been widely adopted\nin the field of 3D scene reconstruction and neural rendering applications.\nHowever, traditional feature grid representations often suffer from substantial\nmemory footprint, posing a significant bottleneck for modern parallel computing\nhardware. In this paper, we present neural vertex features, a generalized\nformulation of learnable representation for neural rendering tasks involving\nexplicit mesh surfaces. Instead of uniformly distributing neural features\nthroughout 3D space, our method stores learnable features directly at mesh\nvertices, leveraging the underlying geometry as a compact and structured\nrepresentation for neural processing. This not only optimizes memory\nefficiency, but also improves feature representation by aligning compactly with\nthe surface using task-specific geometric priors. We validate our neural\nrepresentation across diverse neural rendering tasks, with a specific emphasis\non neural radiosity. Experimental results demonstrate that our method reduces\nmemory consumption to only one-fifth (or even less) of grid-based\nrepresentations, while maintaining comparable rendering quality and lowering\ninference overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u9876\u70b9\u7684\u795e\u7ecf\u7279\u5f81\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u4e86\u6e32\u67d3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u7f51\u683c\u8868\u793a\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u9650\u5236\u4e86\u73b0\u4ee3\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\u7684\u6027\u80fd\u3002\u76ee\u6807\u662f\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5c06\u53ef\u5b66\u4e60\u7279\u5f81\u76f4\u63a5\u5b58\u50a8\u5728\u7f51\u683c\u9876\u70b9\uff0c\u5229\u7528\u51e0\u4f55\u7ed3\u6784\u4f5c\u4e3a\u7d27\u51d1\u8868\u793a\uff0c\u4f18\u5316\u5185\u5b58\u6548\u7387\u5e76\u63d0\u5347\u7279\u5f81\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5185\u5b58\u6d88\u8017\u4ec5\u4e3a\u4f20\u7edf\u7f51\u683c\u8868\u793a\u7684\u4e94\u5206\u4e4b\u4e00\u6216\u66f4\u4f4e\uff0c\u4e14\u6e32\u67d3\u8d28\u91cf\u76f8\u5f53\uff0c\u63a8\u7406\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "\u795e\u7ecf\u9876\u70b9\u7279\u5f81\u65b9\u6cd5\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u6e32\u67d3\u4efb\u52a1\u3002"}}
{"id": "2508.07304", "pdf": "https://arxiv.org/pdf/2508.07304", "abs": "https://arxiv.org/abs/2508.07304", "authors": ["Fabio Vitali"], "title": "From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "This paper introduces a new family of cognitive modal logics designed to\nformalize conjectural reasoning: a modal system in which cognitive contexts\nextend known facts with hypothetical assumptions to explore their consequences.\nUnlike traditional doxastic and epistemic systems, conjectural logics rely on a\nprinciple, called Axiom C ($\\varphi \\rightarrow \\Box\\varphi$), that ensures\nthat all established facts are preserved across hypothetical layers. While\nAxiom C was dismissed in the past due to its association with modal collapse,\nwe show that the collapse only arises under classical and bivalent assumptions,\nand specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a\nparacomplete semantic framework, grounded in Weak Kleene logic or Description\nLogic, where undefined propositions coexist with modal assertions. This\nprevents the modal collapse and guarantees a layering to distinguish between\nfactual and conjectural statements. Under this framework we define new modal\nsystems, e.g., KC and KDC, and show that they are complete, decidable, and\nrobust under partial knowledge. Finally, we introduce a dynamic operation,\n$\\mathsf{settle}(\\varphi)$, which formalizes the transition from conjecture to\naccepted fact, capturing the event of the update of a world's cognitive state\nthrough the resolution of uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u6a21\u6001\u903b\u8f91\u5bb6\u65cf\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u731c\u60f3\u63a8\u7406\uff0c\u901a\u8fc7\u907f\u514d\u7ecf\u5178\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u201c\u6a21\u6001\u574d\u584c\u201d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u731c\u60f3\u548c\u4e8b\u5b9e\u7684\u5206\u5c42\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6a21\u6001\u903b\u8f91\u5728\u731c\u60f3\u63a8\u7406\u4e2d\u56e0Axiom C\u548cAxiom T\u5bfc\u81f4\u7684\u6a21\u6001\u574d\u584c\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9002\u7528\u4e8e\u4e0d\u5b8c\u5168\u77e5\u8bc6\u7684\u903b\u8f91\u6846\u67b6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f31\u514b\u91cc\u5c3c\u903b\u8f91\u6216\u63cf\u8ff0\u903b\u8f91\u7684\u51c6\u5b8c\u5907\u8bed\u4e49\u6846\u67b6\uff0c\u907f\u514dAxiom T\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6a21\u6001\u7cfb\u7edf\uff08\u5982KC\u548cKDC\uff09\u3002", "result": "\u8bc1\u660e\u4e86\u65b0\u7cfb\u7edf\u7684\u5b8c\u5907\u6027\u3001\u53ef\u5224\u5b9a\u6027\uff0c\u5e76\u5728\u90e8\u5206\u77e5\u8bc6\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff1b\u5f15\u5165\u52a8\u6001\u64cd\u4f5csettle(\u03c6)\u4ee5\u4ece\u731c\u60f3\u8fc7\u6e21\u5230\u4e8b\u5b9e\u3002", "conclusion": "\u901a\u8fc7\u907f\u514d\u7ecf\u5178\u5047\u8bbe\uff0c\u65b0\u903b\u8f91\u6210\u529f\u5f62\u5f0f\u5316\u4e86\u731c\u60f3\u63a8\u7406\uff0c\u5e76\u5728\u8ba4\u77e5\u72b6\u6001\u66f4\u65b0\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2508.07044", "pdf": "https://arxiv.org/pdf/2508.07044", "abs": "https://arxiv.org/abs/2508.07044", "authors": ["William Zerong Wang", "Dongfang Zhao"], "title": "Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption", "categories": ["cs.DB", "cs.AI", "cs.CR"], "comment": null, "summary": "In the era of generative AI, ensuring the privacy of music data presents\nunique challenges: unlike static artworks such as images, music data is\ninherently temporal and multimodal, and it is sampled, transformed, and remixed\nat an unprecedented scale. These characteristics make its core vector\nembeddings, i.e, the numerical representations of the music, highly susceptible\nto being learned, misused, or even stolen by models without accessing the\noriginal audio files. Traditional methods like copyright licensing and digital\nwatermarking offer limited protection for these abstract mathematical\nrepresentations, thus necessitating a stronger, e.g., cryptographic, approach\nto safeguarding the embeddings themselves. Standard encryption schemes, such as\nAES, render data unintelligible for computation, making such searches\nimpossible. While Fully Homomorphic Encryption (FHE) provides a plausible\nsolution by allowing arbitrary computations on ciphertexts, its substantial\nperformance overhead remains impractical for large-scale vector similarity\nsearches. Given this trade-off, we propose a more practical approach using\nAdditive Homomorphic Encryption (AHE) for vector similarity search. The primary\ncontributions of this paper are threefold: we analyze threat models unique to\nmusic information retrieval systems; we provide a theoretical analysis and\npropose an efficient AHE-based solution through inner products of music\nembeddings to deliver privacy-preserving similarity search; and finally, we\ndemonstrate the efficiency and practicality of the proposed approach through\nempirical evaluation and comparison to FHE schemes on real-world MP3 files.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6cd5\u540c\u6001\u52a0\u5bc6\uff08AHE\uff09\u7684\u97f3\u4e50\u5d4c\u5165\u5411\u91cf\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u52a0\u5bc6\u548c\u5168\u540c\u6001\u52a0\u5bc6\uff08FHE\uff09\u5728\u5927\u89c4\u6a21\u97f3\u4e50\u6570\u636e\u76f8\u4f3c\u6027\u641c\u7d22\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5728\u751f\u6210\u5f0fAI\u65f6\u4ee3\uff0c\u97f3\u4e50\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u62bd\u8c61\u7684\u97f3\u4e50\u5411\u91cf\u8868\u793a\uff0c\u9700\u8981\u66f4\u5f3a\u52a0\u5bc6\u624b\u6bb5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAHE\u7684\u9690\u79c1\u4fdd\u62a4\u76f8\u4f3c\u6027\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u4e50\u5d4c\u5165\u5411\u91cf\u7684\u5185\u79ef\u5b9e\u73b0\u9ad8\u6548\u52a0\u5bc6\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8eFHE\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754cMP3\u6587\u4ef6\u3002", "conclusion": "AHE\u4e3a\u5927\u89c4\u6a21\u97f3\u4e50\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07227", "pdf": "https://arxiv.org/pdf/2508.07227", "abs": "https://arxiv.org/abs/2508.07227", "authors": ["Siyuan He", "Zhantong Zhu", "Yandong He", "Tianyu Jia"], "title": "LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization", "categories": ["cs.AR"], "comment": "Accepted by ICCAD'2025", "summary": "LLM inference on mobile devices faces extraneous challenges due to limited\nmemory bandwidth and computational resources. To address these issues,\nspeculative inference and processing-in-memory (PIM) techniques have been\nexplored at the algorithmic and hardware levels. However, speculative inference\nresults in more compute-intensive GEMM operations, creating new design\ntrade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there\nexists a significant amount of redundant draft tokens in tree-based speculative\ninference, necessitating efficient token management schemes to minimize energy\nconsumption. In this work, we present LP-Spec, an architecture-dataflow\nco-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with\ndraft token pruning and dynamic workload scheduling to accelerate LLM\nspeculative inference. A near-data memory controller is proposed to enable data\nreallocation between DRAM and PIM banks. Furthermore, a data allocation unit\nbased on the hardware-aware draft token pruner is developed to minimize energy\nconsumption and fully exploit parallel execution opportunities. Compared to\nend-to-end LLM inference on other mobile solutions such as mobile NPUs or\nGEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x\nimprovements in performance, energy efficiency, and energy-delay-product (EDP).\nCompared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and\n415.31x EDP reduction benefits.", "AI": {"tldr": "LP-Spec\u662f\u4e00\u79cd\u901a\u8fc7\u67b6\u6784-\u6570\u636e\u6d41\u534f\u540c\u8bbe\u8ba1\u548c\u8349\u7a3f\u4ee4\u724c\u526a\u679d\u4f18\u5316\u6765\u52a0\u901f\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LLM\u63a8\u7406\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u73b0\u6709\u6280\u672f\u5982\u63a8\u6d4b\u63a8\u7406\u548cPIM\u5bfc\u81f4\u9ad8\u80fd\u8017\u3002", "method": "\u63d0\u51faLP-Spec\uff0c\u7ed3\u5408LPDDR5 PIM\u67b6\u6784\u3001\u8349\u7a3f\u4ee4\u724c\u526a\u679d\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u3002", "result": "\u76f8\u6bd4\u5176\u4ed6\u65b9\u6848\uff0c\u6027\u80fd\u63d0\u534713.21\u500d\uff0c\u80fd\u6548\u63d0\u53477.56\u500d\uff0c\u80fd\u8017\u5ef6\u8fdf\u79ef\u964d\u4f4e99.87\u500d\u3002", "conclusion": "LP-Spec\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u548c\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.06949", "pdf": "https://arxiv.org/pdf/2508.06949", "abs": "https://arxiv.org/abs/2508.06949", "authors": ["Arya Tanmay Gupta"], "title": "Convergence Sans Synchronization", "categories": ["cs.DC", "cs.DM"], "comment": "PhD thesis", "summary": "We currently see a steady rise in the usage and size of multiprocessor\nsystems, and so the community is evermore interested in developing fast\nparallel processing algorithms. However, most algorithms require a\nsynchronization mechanism, which is costly in terms of computational resources\nand time. If an algorithm can be executed in asynchrony, then it can use all\nthe available computation power, and the nodes can execute without being\nscheduled or locked. However, to show that an algorithm guarantees convergence\nin asynchrony, we need to generate the entire global state transition graph and\ncheck for the absence of cycles. This takes time exponential in the size of the\nglobal state space. In this dissertation, we present a theory that explains the\nnecessary and sufficient properties of a multiprocessor algorithm that\nguarantees convergence even without synchronization. We develop algorithms for\nvarious problems that do not require synchronization. Additionally, we show for\nseveral existing algorithms that they can be executed without any\nsynchronization mechanism. A significant theoretical benefit of our work is in\nproving that an algorithm can converge even in asynchrony. Our theory implies\nthat we can make such conclusions about an algorithm, by only showing that the\nlocal state transition graph of a computing node forms a partial order, rather\nthan generating the entire global state space and determining the absence of\ncycles in it. Thus, the complexity of rendering such proofs, formal or social,\nis phenomenally reduced. Experiments show a significant reduction in time taken\nto converge, when we compare the execution time of algorithms in the literature\nversus the algorithms that we design. We get similar results when we run an\nalgorithm, that guarantees convergence in asynchrony, under a scheduler versus\nin asynchrony.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\uff0c\u7528\u4e8e\u8bbe\u8ba1\u65e0\u9700\u540c\u6b65\u5373\u53ef\u6536\u655b\u7684\u591a\u5904\u7406\u5668\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u72b6\u6001\u8f6c\u6362\u56fe\u7684\u504f\u5e8f\u6027\u8d28\u7b80\u5316\u8bc1\u660e\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u7b97\u6cd5\u6548\u7387\u3002", "motivation": "\u968f\u7740\u591a\u5904\u7406\u5668\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u5f00\u53d1\u5feb\u901f\u5e76\u884c\u5904\u7406\u7b97\u6cd5\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u540c\u6b65\u673a\u5236\u6210\u672c\u9ad8\u6602\u3002\u5f02\u6b65\u6267\u884c\u80fd\u5145\u5206\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u9a8c\u8bc1\u5176\u6536\u655b\u6027\u590d\u6742\u4e14\u8017\u65f6\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4ec5\u9700\u5c40\u90e8\u72b6\u6001\u8f6c\u6362\u56fe\u7684\u504f\u5e8f\u6027\u8d28\u5373\u53ef\u786e\u4fdd\u7b97\u6cd5\u5728\u5f02\u6b65\u73af\u5883\u4e0b\u7684\u6536\u655b\u6027\uff0c\u907f\u514d\u751f\u6210\u5168\u5c40\u72b6\u6001\u7a7a\u95f4\u56fe\u3002\u5f00\u53d1\u65e0\u540c\u6b65\u7b97\u6cd5\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u4e8e\u6587\u732e\u4e2d\u9700\u8981\u540c\u6b65\u7684\u7b97\u6cd5\uff0c\u6240\u8bbe\u8ba1\u7684\u65e0\u540c\u6b65\u7b97\u6cd5\u6536\u655b\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u5f02\u6b65\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bc1\u660e\u65b9\u6cd5\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u65e0\u540c\u6b65\u7b97\u6cd5\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.06975", "pdf": "https://arxiv.org/pdf/2508.06975", "abs": "https://arxiv.org/abs/2508.06975", "authors": ["Zhengying Lou", "Baha Eddine Youcef Belmekki", "Mohamed-Slim Alouini"], "title": "THz/RF Multi-Hop Routing Throughput: Performance, Optimization, and Application", "categories": ["cs.NI"], "comment": null, "summary": "Terahertz (THz) communication offers a promising solution for high-throughput\nwireless systems. However, the severe path loss of THz signals raises concerns\nabout its effectiveness compared to radio frequency (RF) communication. In this\narticle, we establish the first stochastic geometry (SG)-based analytical\nframework for routing in THz systems. We develop a stepwise optimization\napproach to maximize throughput, including power allocation, relay selection,\nand number of hops design. Analytical expressions for throughput and coverage\nprobability are derived under the SG framework, enabling low complexity and\nscalable performance evaluation. Numerical results show that the proposed\nstepwise-optimal routing strategies not only outperform existing SG-based\nmethods but also approach the ideal upper bound. Moreover, we compare the\nthroughput and coverage performance of THz and RF routing and demonstrate the\napplications of the proposed analytical framework and routing strategies in\nsystem parameter design and unmanned aerial vehicle networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\uff08SG\uff09\u7684\u592a\u8d6b\u5179\uff08THz\uff09\u901a\u4fe1\u8def\u7531\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u6b65\u4f18\u5316\u65b9\u6cd5\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u592a\u8d6b\u5179\u901a\u4fe1\u56e0\u5176\u9ad8\u541e\u5410\u91cf\u6f5c\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u4fe1\u53f7\u8def\u5f84\u635f\u8017\u4e25\u91cd\uff0c\u9700\u8981\u89e3\u51b3\u5176\u8def\u7531\u95ee\u9898\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u968f\u673a\u51e0\u4f55\u6846\u67b6\uff0c\u5f00\u53d1\u9010\u6b65\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u529f\u7387\u5206\u914d\u3001\u4e2d\u7ee7\u9009\u62e9\u548c\u8df3\u6570\u8bbe\u8ba1\u3002", "result": "\u63d0\u51fa\u7684\u8def\u7531\u7b56\u7565\u5728\u541e\u5410\u91cf\u548c\u8986\u76d6\u6982\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u7406\u8bba\u4e0a\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u7b56\u7565\u4e3a\u592a\u8d6b\u5179\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u65e0\u4eba\u673a\u7f51\u7edc\u7b49\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.07573", "pdf": "https://arxiv.org/pdf/2508.07573", "abs": "https://arxiv.org/abs/2508.07573", "authors": ["Binquan Guo", "Wanting Yang", "Zehui Xiong", "Zhou Zhang", "Baosheng Li", "Zhu Han", "Rahim Tafazolli", "Tony Q. S. Quek"], "title": "Enhancing Mega-Satellite Networks with Generative Semantic Communication: A Networking Perspective", "categories": ["cs.ET", "cs.NI"], "comment": "Accepted paper to be published in IEEE", "summary": "The advance of direct satellite-to-device communication has positioned\nmega-satellite constellations as a cornerstone of 6G wireless communication,\nenabling seamless global connectivity even in remote and underserved areas.\nHowever, spectrum scarcity and capacity constraints imposed by the Shannon's\nclassical information theory remain significant challenges for supporting the\nmassive data demands of multimedia-rich wireless applications. Generative\nSemantic Communication (GSC), powered by artificial intelligence-based\ngenerative foundation models, represents a paradigm shift from transmitting raw\ndata to exchanging semantic meaning. GSC can not only reduce bandwidth\nconsumption, but also enhance key semantic features in multimedia content,\nthereby offering a promising solution to overcome the limitations of\ntraditional satellite communication systems. This article investigates the\nintegration of GSC into mega-satellite constellations from a networking\nperspective. We propose a GSC-empowered satellite networking architecture and\nidentify key enabling technologies, focusing on GSC-empowered network modeling\nand GSC-aware networking strategies. We construct a discrete temporal graph to\nmodel semantic encoders and decoders, distinct knowledge bases, and resource\nvariations in mega-satellite networks. Based on this framework, we develop\nmodel deployment for semantic encoders and decoders and GSC-compatible routing\nschemes, and then present performance evaluations. Finally, we outline future\nresearch directions for advancing GSC-empowered satellite networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5229\u7528\u751f\u6210\u8bed\u4e49\u901a\u4fe1\uff08GSC\uff09\u63d0\u5347\u5927\u578b\u536b\u661f\u661f\u5ea7\u7f51\u7edc\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u9891\u8c31\u7a00\u7f3a\u548c\u5bb9\u91cf\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u89e3\u51b36G\u65e0\u7ebf\u901a\u4fe1\u4e2d\u536b\u661f\u76f4\u63a5\u901a\u4fe1\u9762\u4e34\u7684\u9891\u8c31\u7a00\u7f3a\u548c\u5bb9\u91cf\u9650\u5236\u95ee\u9898\uff0c\u63d0\u5347\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u4f20\u8f93\u6548\u7387\u3002", "method": "\u63d0\u51faGSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u62ec\u7f51\u7edc\u5efa\u6a21\u548cGSC\u611f\u77e5\u7684\u7f51\u7edc\u7b56\u7565\uff0c\u5229\u7528\u79bb\u6563\u65f6\u5e8f\u56fe\u5efa\u6a21\u8bed\u4e49\u7f16\u89e3\u7801\u5668\u548c\u77e5\u8bc6\u5e93\uff0c\u5f00\u53d1\u517c\u5bb9GSC\u7684\u8def\u7531\u65b9\u6848\u3002", "result": "\u6027\u80fd\u8bc4\u4f30\u663e\u793aGSC\u80fd\u6709\u6548\u51cf\u5c11\u5e26\u5bbd\u6d88\u8017\u5e76\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\uff0c\u4e3a\u536b\u661f\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GSC\u5728\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u5176\u6027\u80fd\u548c\u90e8\u7f72\u3002"}}
{"id": "2508.07169", "pdf": "https://arxiv.org/pdf/2508.07169", "abs": "https://arxiv.org/abs/2508.07169", "authors": ["Burak Yeti\u015ftiren", "Hong Jin Kang", "Miryung Kim"], "title": "From Noise to Knowledge: Interactive Summaries for Developer Alerts", "categories": ["cs.SE"], "comment": null, "summary": "Programmers using bug-finding tools often review their reported warnings one\nby one. Based on the insight that identifying recurring themes and\nrelationships can enhance the cognitive process of sensemaking, we propose\nCLARITY, which supports interpreting tool-generated warnings through\ninteractive inquiry. CLARITY derives summary rules for custom grouping of\nrelated warnings with active feedback. As users mark warnings as interesting or\nuninteresting, CLARITY's rule inference algorithm surfaces common symptoms,\nhighlighting structural similarities in containment, subtyping, invoked\nmethods, accessed fields, and expressions.\n  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java\nprojects. In a within-subject user study with 14 participants, users\narticulated root causes for similar uninteresting warnings faster and with more\nconfidence using CLARITY. We observed significant individual variation in\ndesired grouping, reinforcing the need for customizable sensemaking. Simulation\nshows that with rule-level feedback, only 11.8 interactions are needed on\naverage to align all inferred rules with a simulated user's labels (vs. 17.8\nwithout). Our evaluation suggests that CLARITY's active learning-based\nsummarization enhances interactive warning sensemaking.", "AI": {"tldr": "CLARITY \u662f\u4e00\u4e2a\u901a\u8fc7\u4ea4\u4e92\u5f0f\u67e5\u8be2\u5e2e\u52a9\u7a0b\u5e8f\u5458\u7406\u89e3 bug \u62a5\u544a\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u603b\u7ed3\u89c4\u5219\u5bf9\u76f8\u5173\u8b66\u544a\u8fdb\u884c\u5206\u7ec4\uff0c\u63d0\u5347\u95ee\u9898\u5b9a\u4f4d\u6548\u7387\u3002", "motivation": "\u7a0b\u5e8f\u5458\u5728\u5206\u6790 bug \u62a5\u544a\u65f6\u901a\u5e38\u9700\u8981\u9010\u6761\u68c0\u67e5\u8b66\u544a\uff0c\u800c CLARITY \u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u91cd\u590d\u6a21\u5f0f\u548c\u5173\u7cfb\uff0c\u63d0\u5347\u8ba4\u77e5\u6548\u7387\u3002", "method": "CLARITY \u901a\u8fc7\u7528\u6237\u53cd\u9988\u52a8\u6001\u751f\u6210\u603b\u7ed3\u89c4\u5219\uff0c\u5bf9\u8b66\u544a\u8fdb\u884c\u81ea\u5b9a\u4e49\u5206\u7ec4\uff0c\u5e76\u5229\u7528\u89c4\u5219\u63a8\u65ad\u7b97\u6cd5\u8bc6\u522b\u5e38\u89c1\u95ee\u9898\u3002", "result": "\u5728\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528 CLARITY \u7684\u7528\u6237\u80fd\u66f4\u5feb\u3001\u66f4\u6709\u4fe1\u5fc3\u5730\u5b9a\u4f4d\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0\u3002\u5e73\u5747\u4ec5\u9700 11.8 \u6b21\u4ea4\u4e92\u5373\u53ef\u5bf9\u9f50\u89c4\u5219\u4e0e\u7528\u6237\u6807\u7b7e\u3002", "conclusion": "CLARITY \u7684\u4e3b\u52a8\u5b66\u4e60\u603b\u7ed3\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u8b66\u544a\u5206\u6790\u7684\u6548\u7387\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5206\u7ec4\u9700\u6c42\u3002"}}
{"id": "2508.06772", "pdf": "https://arxiv.org/pdf/2508.06772", "abs": "https://arxiv.org/abs/2508.06772", "authors": ["Catherine Yeh", "Tara Menon", "Robin Singh Arya", "Helen He", "Moira Weigel", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "Story Ribbons: Reimagining Storyline Visualizations with Large Language Models", "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 (11 pages, 9 figures)", "summary": "Analyzing literature involves tracking interactions between characters,\nlocations, and themes. Visualization has the potential to facilitate the\nmapping and analysis of these complex relationships, but capturing structured\ninformation from unstructured story data remains a challenge. As large language\nmodels (LLMs) continue to advance, we see an opportunity to use their text\nprocessing and analysis capabilities to augment and reimagine existing\nstoryline visualization techniques. Toward this goal, we introduce an\nLLM-driven data parsing pipeline that automatically extracts relevant narrative\ninformation from novels and scripts. We then apply this pipeline to create\nStory Ribbons, an interactive visualization system that helps novice and expert\nliterary analysts explore detailed character and theme trajectories at multiple\nnarrative levels. Through pipeline evaluations and user studies with Story\nRibbons on 36 literary works, we demonstrate the potential of LLMs to\nstreamline narrative visualization creation and reveal new insights about\nfamiliar stories. We also describe current limitations of AI-based systems, and\ninteraction motifs designed to address these issues.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\u6570\u636e\u89e3\u6790\u6d41\u6c34\u7ebf\uff0c\u81ea\u52a8\u4ece\u5c0f\u8bf4\u548c\u5267\u672c\u4e2d\u63d0\u53d6\u53d9\u4e8b\u4fe1\u606f\uff0c\u5e76\u5f00\u53d1\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7cfb\u7edfStory Ribbons\uff0c\u5e2e\u52a9\u5206\u6790\u4eba\u7269\u548c\u4e3b\u9898\u8f68\u8ff9\u3002", "motivation": "\u5f53\u524d\u4ece\u975e\u7ed3\u6784\u5316\u6545\u4e8b\u6570\u636e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u6027\uff0cLLM\u7684\u6587\u672c\u5904\u7406\u80fd\u529b\u6709\u671b\u6539\u8fdb\u73b0\u6709\u60c5\u8282\u53ef\u89c6\u5316\u6280\u672f\u3002", "method": "\u5f00\u53d1LLM\u9a71\u52a8\u7684\u6570\u636e\u89e3\u6790\u6d41\u6c34\u7ebf\u63d0\u53d6\u53d9\u4e8b\u4fe1\u606f\uff0c\u5e76\u6784\u5efa\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7cfb\u7edfStory Ribbons\u3002", "result": "\u572836\u90e8\u6587\u5b66\u4f5c\u54c1\u4e0a\u7684\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7b80\u5316\u53d9\u4e8b\u53ef\u89c6\u5316\u5e76\u63ed\u793a\u65b0\u89c1\u89e3\uff0c\u4f46\u4e5f\u5b58\u5728AI\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LLM\u5728\u53d9\u4e8b\u53ef\u89c6\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u9488\u5bf9\u5176\u5c40\u9650\u6027\u8bbe\u8ba1\u4ea4\u4e92\u65b9\u5f0f\u3002"}}
{"id": "2508.07590", "pdf": "https://arxiv.org/pdf/2508.07590", "abs": "https://arxiv.org/abs/2508.07590", "authors": ["Xiongwei Xiao", "Baoying Chen", "Jishen Zeng", "Jianquan Yang"], "title": "MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Accurately assessing the perceptual quality of face images is crucial,\nespecially with the rapid progress in face restoration and generation.\nTraditional quality assessment methods often struggle with the unique\ncharacteristics of face images, limiting their generalizability. While\nlearning-based approaches demonstrate superior performance due to their strong\nfitting capabilities, their high complexity typically incurs significant\ncomputational and storage costs, hindering practical deployment. To address\nthis, we propose a lightweight face quality assessment network with Multi-Stage\nProgressive Training (MSPT). Our network employs a three-stage progressive\ntraining strategy that gradually introduces more diverse data samples and\nincreases input image resolution. This novel approach enables lightweight\nnetworks to achieve high performance by effectively learning complex quality\nfeatures while significantly mitigating catastrophic forgetting. Our MSPT\nachieved the second highest score on the VQualA 2025 face image quality\nassessment benchmark dataset, demonstrating that MSPT achieves comparable or\nbetter performance than state-of-the-art methods while maintaining efficient\ninference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9762\u90e8\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7f51\u7edcMSPT\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u9762\u90e8\u56fe\u50cf\u7684\u7279\u6027\uff0c\u800c\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u867d\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\u3002MSPT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u9010\u6b65\u5f15\u5165\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u6837\u672c\u5e76\u63d0\u9ad8\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u6709\u6548\u5b66\u4e60\u590d\u6742\u8d28\u91cf\u7279\u5f81\u5e76\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "MSPT\u5728VQualA 2025\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u7b2c\u4e8c\u9ad8\u5206\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MSPT\u662f\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u7ea7\u9762\u90e8\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2508.08198", "pdf": "https://arxiv.org/pdf/2508.08198", "abs": "https://arxiv.org/abs/2508.08198", "authors": ["Yupeng Zhang", "Adam Alon", "M. Khalid Jawed"], "title": "Emergent morphogenesis via planar fabrication enabled by a reduced model of composites", "categories": ["cs.GR", "cs.RO"], "comment": "GitHub repository:\n  https://github.com/StructuresComp/discrete-shells-shrinky-dink/", "summary": "The ability to engineer complex three-dimensional shapes from planar sheets\nwith precise, programmable control underpins emerging technologies in soft\nrobotics, reconfigurable devices, and functional materials. Here, we present a\nreduced-order numerical and experimental framework for a bilayer system\nconsisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to\na kirigami-patterned, inert plastic layer. Upon uniform heating, the active\nlayer contracts while the patterned layer constrains in-plane stretch but\nallows out-of-plane bending, yielding programmable 3D morphologies from simple\nplanar precursors. Our approach enables efficient computational design and\nscalable manufacturing of 3D forms with a single-layer reduced model that\ncaptures the coupled mechanics of stretching and bending. Unlike traditional\nbilayer modeling, our framework collapses the multilayer composite into a\nsingle layer of nodes and elements, reducing the degrees of freedom and\nenabling simulation on a 2D geometry. This is achieved by introducing a novel\nenergy formulation that captures the coupling between in-plane stretch mismatch\nand out-of-plane bending - extending beyond simple isotropic linear elastic\nmodels. Experimentally, we establish a fully planar, repeatable fabrication\nprotocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic\nlayer. The programmed strain mismatch drives an array of 3D morphologies, such\nas bowls, canoes, and flower petals, all verified by both simulation and\nphysical prototypes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u8ba1\u7b97\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u6750\u6599\u5b9e\u73b0\u53ef\u7f16\u7a0b3D\u5f62\u6001\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5e73\u9762\u6750\u6599\u5236\u9020\u590d\u67423D\u5f62\u72b6\uff0c\u652f\u6301\u8f6f\u673a\u5668\u4eba\u7b49\u529f\u80fd\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u6570\u503c\u5efa\u6a21\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u53cc\u5c42\u6750\u6599\u7cfb\u7edf\uff08\u523a\u6fc0\u54cd\u5e94\u5c42\u548ckirigami\u56fe\u6848\u5c42\uff09\u3002\u91c7\u7528\u5355\u5c42\u7b80\u5316\u6a21\u578b\uff0c\u51cf\u5c11\u81ea\u7531\u5ea6\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u79cd3D\u5f62\u6001\uff0c\u5982\u7897\u3001\u72ec\u6728\u821f\u548c\u82b1\u74e3\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u539f\u578b\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u591a\u5c42\u590d\u5408\u6750\u6599\u7684\u5efa\u6a21\u548c\u5236\u9020\uff0c\u4e3a\u529f\u80fd\u6750\u6599\u76843D\u5f62\u6001\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.07742", "pdf": "https://arxiv.org/pdf/2508.07742", "abs": "https://arxiv.org/abs/2508.07742", "authors": ["Meghyn Bienvenu", "Camille Bourgaux", "Katsumi Inoue", "Robin Jean"], "title": "A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases", "categories": ["cs.LO", "cs.AI", "cs.DB"], "comment": "This is an extended version of a paper appearing at the 22nd\n  International Conference on Principles of Knowledge Representation and\n  Reasoning (KR 2025). 24 pages", "summary": "Repair-based semantics have been extensively studied as a means of obtaining\nmeaningful answers to queries posed over inconsistent knowledge bases (KBs).\nWhile several works have considered how to exploit a priority relation between\nfacts to select optimal repairs, the question of how to specify such\npreferences remains largely unaddressed. This motivates us to introduce a\ndeclarative rule-based framework for specifying and computing a priority\nrelation between conflicting facts. As the expressed preferences may contain\nundesirable cycles, we consider the problem of determining when a set of\npreference rules always yields an acyclic relation, and we also explore a\npragmatic approach that extracts an acyclic relation by applying various cycle\nremoval techniques. Towards an end-to-end system for querying inconsistent KBs,\nwe present a preliminary implementation and experimental evaluation of the\nframework, which employs answer set programming to evaluate the preference\nrules, apply the desired cycle resolution techniques to obtain a priority\nrelation, and answer queries under prioritized-repair semantics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u660e\u7684\u89c4\u5219\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5b9a\u548c\u8ba1\u7b97\u51b2\u7a81\u4e8b\u5b9e\u4e4b\u95f4\u7684\u4f18\u5148\u7ea7\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u504f\u597d\u89c4\u5219\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u5faa\u73af\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u5982\u4f55\u6307\u5b9a\u4f18\u5148\u7ea7\u5173\u7cfb\u7684\u673a\u5236\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u58f0\u660e\u5f0f\u7684\u89c4\u5219\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u4e86\u6d88\u9664\u5faa\u73af\u504f\u597d\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u58f0\u660e\u5f0f\u7684\u89c4\u5219\u6846\u67b6\u6765\u5b9a\u4e49\u4f18\u5148\u7ea7\u5173\u7cfb\uff0c\u7814\u7a76\u4e86\u89c4\u5219\u662f\u5426\u4ea7\u751f\u65e0\u73af\u5173\u7cfb\u7684\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u79cd\u5faa\u73af\u6d88\u9664\u6280\u672f\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u4f18\u5148\u7ea7\u5173\u7cfb\u7684\u8ba1\u7b97\u3001\u5faa\u73af\u6d88\u9664\u548c\u67e5\u8be2\u56de\u7b54\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\u4e2d\u7684\u67e5\u8be2\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5148\u7ea7\u5173\u7cfb\u5b9a\u4e49\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07087", "pdf": "https://arxiv.org/pdf/2508.07087", "abs": "https://arxiv.org/abs/2508.07087", "authors": ["Mohammadreza Daviran", "Brian Lin", "Davood Rafiei"], "title": "SQL-Exchange: Transforming SQL Queries Across Domains", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SQL-Exchange, a framework for mapping SQL queries across\ndifferent database schemas by preserving the source query structure while\nadapting domain-specific elements to align with the target schema. We\ninvestigate the conditions under which such mappings are feasible and\nbeneficial, and examine their impact on enhancing the in-context learning\nperformance of text-to-SQL systems as a downstream task. Our comprehensive\nevaluation across multiple model families and benchmark datasets--assessing\nstructural alignment with source queries, execution validity on target\ndatabases, and semantic correctness--demonstrates that SQL-Exchange is\neffective across a wide range of schemas and query types. Our results further\nshow that using mapped queries as in-context examples consistently improves\ntext-to-SQL performance over using queries from the source schema.", "AI": {"tldr": "SQL-Exchange \u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u6570\u636e\u5e93\u6a21\u5f0f\u95f4\u6620\u5c04 SQL \u67e5\u8be2\uff0c\u901a\u8fc7\u4fdd\u7559\u6e90\u67e5\u8be2\u7ed3\u6784\u5e76\u9002\u5e94\u76ee\u6807\u6a21\u5f0f\u6765\u63d0\u5347\u6587\u672c\u5230 SQL \u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u4e0d\u540c\u6570\u636e\u5e93\u6a21\u5f0f\u95f4\u6620\u5c04 SQL \u67e5\u8be2\u7684\u53ef\u884c\u6027\u548c\u76ca\u5904\uff0c\u4ee5\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u901a\u8fc7 SQL-Exchange \u6846\u67b6\u6620\u5c04 SQL \u67e5\u8be2\uff0c\u4fdd\u7559\u6e90\u67e5\u8be2\u7ed3\u6784\u5e76\u9002\u5e94\u76ee\u6807\u6a21\u5f0f\uff0c\u8bc4\u4f30\u7ed3\u6784\u5bf9\u9f50\u3001\u6267\u884c\u6709\u6548\u6027\u548c\u8bed\u4e49\u6b63\u786e\u6027\u3002", "result": "SQL-Exchange \u5728\u591a\u79cd\u6a21\u5f0f\u548c\u67e5\u8be2\u7c7b\u578b\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u6620\u5c04\u67e5\u8be2\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\u80fd\u6301\u7eed\u63d0\u5347\u6587\u672c\u5230 SQL \u7684\u6027\u80fd\u3002", "conclusion": "SQL-Exchange \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u8de8\u6a21\u5f0f SQL \u67e5\u8be2\u6620\u5c04\uff0c\u4ece\u800c\u589e\u5f3a\u6587\u672c\u5230 SQL \u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07252", "pdf": "https://arxiv.org/pdf/2508.07252", "abs": "https://arxiv.org/abs/2508.07252", "authors": ["Siyuan He", "Peiran Yan", "Yandong He", "Youwei Zhuo", "Tianyu Jia"], "title": "Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference", "categories": ["cs.AR"], "comment": "Accepted by ICCAD'2025", "summary": "The autoregressive decoding in LLMs is the major inference bottleneck due to\nthe memory-intensive operations and limited hardware bandwidth. 3D-stacked\narchitecture is a promising solution with significantly improved memory\nbandwidth, which vertically stacked multi DRAM dies on top of logic die.\nHowever, our experiments also show the 3D-stacked architecture faces severer\nthermal issues compared to 2D architecture, in terms of thermal temperature,\ngradient and scalability. To better exploit the potential of 3D-stacked\narchitecture, we present Tasa, a heterogeneous architecture with cross-stack\nthermal optimizations to balance the temperature distribution and maximize the\nperformance under the thermal constraints. High-performance core is designed\nfor compute-intensive operations, while high-efficiency core is used for\nmemory-intensive operators, e.g. attention layers. Furthermore, we propose a\nbandwidth sharing scheduling to improve the bandwidth utilization in such\nheterogeneous architecture. Extensive thermal experiments show that our Tasa\narchitecture demonstrates greater scalability compared with the homogeneous\n3D-stacked architecture, i.e. up to 5.55 $\\tccentigrade$, 9.37 $\\tccentigrade$,\nand 7.91 $\\tccentigrade$ peak temperature reduction for 48, 60, and 72 core\nconfigurations. Our experimental for Llama-65B and GPT-3 66B inferences also\ndemonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and\nstate-of-the-art heterogeneous PIM-based LLM accelerator", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Tasa\u67b6\u6784\uff0c\u901a\u8fc7\u5f02\u6784\u8bbe\u8ba1\u548c\u70ed\u4f18\u5316\u89e3\u51b33D\u5806\u53e0\u67b6\u6784\u4e2d\u7684\u70ed\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u5806\u53e0\u67b6\u6784\u5728LLM\u63a8\u7406\u4e2d\u9762\u4e34\u7684\u70ed\u95ee\u9898\u548c\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faTasa\u5f02\u6784\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u6027\u80fd\u6838\u5fc3\u548c\u9ad8\u6548\u7387\u6838\u5fc3\uff0c\u5e76\u91c7\u7528\u5e26\u5bbd\u5171\u4eab\u8c03\u5ea6\u4f18\u5316\u3002", "result": "Tasa\u67b6\u6784\u572848\u300160\u548c72\u6838\u914d\u7f6e\u4e0b\u5206\u522b\u964d\u4f4e\u4e86\u5cf0\u503c\u6e29\u5ea6\uff0c\u5e76\u5728LLM\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u3002", "conclusion": "Tasa\u67b6\u6784\u901a\u8fc7\u70ed\u4f18\u5316\u548c\u5f02\u6784\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5806\u53e0\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.07071", "pdf": "https://arxiv.org/pdf/2508.07071", "abs": "https://arxiv.org/abs/2508.07071", "authors": ["Oscar Amoros", "Albert Andaluz", "Johnny Nunez", "Antonio J. Pena"], "title": "The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries", "categories": ["cs.DC"], "comment": "16 pages", "summary": "Existing GPU libraries often struggle to fully exploit the parallel resources\nand on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as\nindividual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion\n(HF) and Vertical Fusion (VF) can mitigate this, current library\nimplementations often require library developers to manually create fused\nkernels. Hence, library users rely on limited sets of pre-compiled or\ntemplate-based fused kernels. This limits the use cases that can benefit from\nHF and VF and increases development costs. In order to solve these issues, we\npresent a novel methodology for building GPU libraries that enables automatic\non-demand HF and VF for arbitrary combinations of GPU library functions. Our\nmethodology defines reusable, fusionable components that users combine via\nhigh-level programming interfaces. Leveraging C++17 metaprogramming features\navailable in compilers like nvcc, our methodology generates a single and\noptimized fused kernel tailored to the user's specific sequence of operations\nat compile time, without needing a custom compiler or manual development and\npre-compilation of kernel combinations. This approach abstracts low-level GPU\ncomplexities while maximizing GPU resource utilization and keeping intermediate\ndata in SRAM. We provide an open-source implementation demonstrating\nsignificant speedups compared to traditional libraries in various benchmarks,\nvalidating the effectiveness of this methodology for improving GPU performance\nin the range of 2x to more than 1000x, while preserving high-level\nprogrammability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u52a8\u878d\u5408GPU\u5185\u6838\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e93\u9700\u624b\u52a8\u878d\u5408\u7684\u9650\u5236\uff0c\u5229\u7528C++17\u5143\u7f16\u7a0b\u751f\u6210\u4f18\u5316\u5185\u6838\uff0c\u63d0\u5347\u6027\u80fd2x\u81f31000x\u3002", "motivation": "\u73b0\u6709GPU\u5e93\u96be\u4ee5\u5145\u5206\u5229\u7528\u8d44\u6e90\uff0c\u624b\u52a8\u5185\u6838\u878d\u5408\u53d7\u9650\u4e14\u5f00\u53d1\u6210\u672c\u9ad8\u3002", "method": "\u5b9a\u4e49\u53ef\u91cd\u7528\u3001\u53ef\u878d\u5408\u7ec4\u4ef6\uff0c\u5229\u7528C++17\u5143\u7f16\u7a0b\u81ea\u52a8\u751f\u6210\u4f18\u5316\u5185\u6838\u3002", "result": "\u5f00\u6e90\u5b9e\u73b0\u663e\u793a\u6027\u80fd\u63d0\u53472x\u81f31000x\uff0c\u4fdd\u6301\u4e86\u9ad8\u7ea7\u7f16\u7a0b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347GPU\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002"}}
{"id": "2508.07001", "pdf": "https://arxiv.org/pdf/2508.07001", "abs": "https://arxiv.org/abs/2508.07001", "authors": ["Myeung Suk Oh", "Zhiyao Zhang", "FNU Hairi", "Alvaro Velasquez", "Jia Liu"], "title": "Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "This paper has been accepted in ACM International Symposium on\n  Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and\n  Mobile Computing (MobiHoc) 2025", "summary": "With wireless devices increasingly forming a unified smart network for\nseamless, user-friendly operations, random access (RA) medium access control\n(MAC) design is considered a key solution for handling unpredictable data\ntraffic from multiple terminals. However, it remains challenging to design an\neffective RA-based MAC protocol to minimize collisions and ensure transmission\nfairness across the devices. While existing multi-agent reinforcement learning\n(MARL) approaches with centralized training and decentralized execution (CTDE)\nhave been proposed to optimize RA performance, their reliance on centralized\ntraining and the significant overhead required for information collection can\nmake real-world applications unrealistic. In this work, we adopt a fully\ndecentralized MARL architecture, where policy learning does not rely on\ncentralized tasks but leverages consensus-based information exchanges across\ndevices. We design our MARL algorithm over an actor-critic (AC) network and\npropose exchanging only local rewards to minimize communication overhead.\nFurthermore, we provide a theoretical proof of global convergence for our\napproach. Numerical experiments show that our proposed MARL algorithm can\nsignificantly improve RA network performance compared to other baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5206\u6563\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u672c\u5730\u5956\u52b1\u4ea4\u6362\u4f18\u5316\u968f\u673a\u63a5\u5165\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u65e0\u9700\u96c6\u4e2d\u8bad\u7ec3\u4e14\u901a\u4fe1\u5f00\u9500\u4f4e\u7684\u968f\u673a\u63a5\u5165\u591a\u5740\u534f\u8bae\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u57fa\u4e8e\u5171\u8bc6\u4fe1\u606f\u4ea4\u6362\u548c\u672c\u5730\u5956\u52b1\u5171\u4eab\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7f51\u7edc\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5168\u5c40\u6536\u655b\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u968f\u673a\u63a5\u5165\u7f51\u7edc\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07640", "pdf": "https://arxiv.org/pdf/2508.07640", "abs": "https://arxiv.org/abs/2508.07640", "authors": ["Chanh Nguyen", "Monowar Bhuyan", "Erik Elmroth"], "title": "Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control", "categories": ["cs.DC", "cs.PF"], "comment": "8 pages, 8 figures, preprint accepted at MASCOTS 2025", "summary": "Serverless computing has transformed cloud application deployment by\nintroducing a fine-grained, event-driven execution model that abstracts away\ninfrastructure management. Its on-demand nature makes it especially appealing\nfor latency-sensitive and bursty workloads. However, the cold start problem,\ni.e., where the platform incurs significant delay when provisioning new\ncontainers, remains the Achilles' heel of such platforms.\n  This paper presents a predictive serverless scheduling framework based on\nModel Predictive Control to proactively mitigate cold starts, thereby improving\nend-to-end response time. By forecasting future invocations, the controller\njointly optimizes container prewarming and request dispatching, improving\nlatency while minimizing resource overhead.\n  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based\ntestbed. Experimental results using real-world function traces and synthetic\nworkloads demonstrate that our method significantly outperforms\nstate-of-the-art baselines, achieving up to 85% lower tail latency and a 34%\nreduction in resource usage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u9884\u6d4b\u6027\u65e0\u670d\u52a1\u5668\u8c03\u5ea6\u6846\u67b6\uff0c\u65e8\u5728\u4e3b\u52a8\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u7aef\u5230\u7aef\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u51b7\u542f\u52a8\u95ee\u9898\u662f\u5176\u5e73\u53f0\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u5f71\u54cd\u4e86\u5ef6\u8fdf\u654f\u611f\u548c\u7a81\u53d1\u6027\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u8c03\u7528\uff0c\u8054\u5408\u4f18\u5316\u5bb9\u5668\u9884\u70ed\u548c\u8bf7\u6c42\u8c03\u5ea6\u3002", "result": "\u5728Apache OpenWhisk\u548cKubernetes\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e85%\uff0c\u8d44\u6e90\u4f7f\u7528\u51cf\u5c1134%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2508.07948", "pdf": "https://arxiv.org/pdf/2508.07948", "abs": "https://arxiv.org/abs/2508.07948", "authors": ["John D. Mayfield"], "title": "Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions", "categories": ["q-bio.OT", "cs.ET", "quant-ph", "81P68, 92C20, 42A38, 15A18, 81R05", "F.0; F.1.1; F.2.1; G.1.0; G.1.3; I.2; I.2.1; I.2.6; I.5; J.3"], "comment": "11 pages, 1 figure", "summary": "Progressive neurodegenerative diseases, including Alzheimer's disease (AD),\nmultiple sclerosis (MS), Parkinson's disease (PD), and amyotrophic lateral\nsclerosis (ALS), exhibit complex, nonlinear trajectories that challenge\ndeterministic modeling. Traditional time-domain analyses of multiomic and\nneuroimaging data often fail to capture hidden oscillatory patterns, limiting\npredictive accuracy. We propose a theoretical mathematical framework that\ntransforms time-series data into frequency or s-domain using Fourier and\nLaplace transforms, models neuronal dynamics via Hamiltonian formulations, and\nemploys quantum-classical hybrid computing with variational quantum\neigensolvers (VQE) for enhanced pattern detection. This theoretical construct\nserves as a foundation for future empirical works in quantum-enhanced analysis\nof neurodegenerative diseases. We extend this to quaternionic representations\nwith three imaginary axes ($i, j, k$) to model multistate Hamiltonians in\nmultifaceted disorders, drawing from quantum neuromorphic computing to capture\nentangled neural dynamics \\citep{Pehle2020, Emani2019}. This approach leverages\nquantum advantages in handling high-dimensional amplitude-phase data, enabling\noutlier detection and frequency signature analysis. Potential clinical\napplications include identifying high-risk patients with rapid progression or\ntherapy resistance using s-domain biomarkers, supported by quantum machine\nlearning (QML) precedents achieving up to 99.89% accuracy in Alzheimer's\nclassification \\citep{Belay2024, Bhowmik2025}. This framework aims to lay the\ngroundwork for redefining precision medicine for neurodegenerative diseases\nthrough future validations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5085\u91cc\u53f6\u53d8\u6362\u3001\u62c9\u666e\u62c9\u65af\u53d8\u6362\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u975e\u7ebf\u6027\u52a8\u6001\u3002", "motivation": "\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u590d\u6742\u52a8\u6001\u96be\u4ee5\u901a\u8fc7\u4f20\u7edf\u65f6\u95f4\u57df\u5206\u6790\u6355\u6349\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6570\u5b66\u6a21\u578b\u548c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9891\u7387\u57df\u6216s\u57df\u8f6c\u6362\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5229\u7528\u54c8\u5bc6\u987f\u65b9\u7a0b\u5efa\u6a21\u795e\u7ecf\u5143\u52a8\u6001\uff0c\u5e76\u91c7\u7528\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u8ba1\u7b97\u548c\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u589e\u5f3a\u6a21\u5f0f\u68c0\u6d4b\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u7684\u5b9e\u8bc1\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u7c7b\u4e2d\u5c55\u793a\u4e86\u9ad8\u8fbe99.89%\u7684\u51c6\u786e\u6027\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u4e00\u7406\u8bba\u6846\u67b6\u6709\u671b\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u7cbe\u51c6\u533b\u5b66\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.07180", "pdf": "https://arxiv.org/pdf/2508.07180", "abs": "https://arxiv.org/abs/2508.07180", "authors": ["Zhe Zhang", "Runlin Liu", "Aishan Liu", "Xingyu Liu", "Xiang Gao", "Hailong Sun"], "title": "Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As large language models LLMs) become increasingly integrated into software\ndevelopment workflows, rigorously evaluating their performance on complex,\nreal-world code generation tasks has become essential. However, existing\nbenchmarks often suffer from data contamination and limited test rigor,\nconstraining their ability to reveal model failures effectively. To address\nthese, we present CODE2BENCH, a end-to-end pipeline for dynamically\nconstructing robust and contamination-resistant benchmarks from real-world\nGitHub repositories. Specifically, CODE2BENCH introduces three key innovations:\n(1) Automated Dynamism, achieved through periodic ingestion of recent code to\nminimize training data contamination; (2) Scope Graph-based dependency\nanalysis, which enables structured classification of functions into benchmark\ninstances with controlled dependency levels (distinguishing between\nSelf-Contained (SC) tasks for cross-language evaluation and Weakly\nSelf-Contained (WSC) tasks involving permitted library usage); and (3)\nProperty-Based Testing (PBT) for the automated synthesis of rigorous test\nsuites to enable thorough functional verification. Using this pipeline, we\nconstruct CODE2BENCH-2505, the first benchmark derived from 880 recent Python\nprojects spanning diverse domains, comprising 1,163 code generation tasks with\n100% average branch coverage on ground-truth implementations. Extensive\nevaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently\nstruggle with SC tasks requiring complex, non-standard logic and cross-language\ntransfer, while showing relatively stronger performance on WSC tasks in Python.\nOur work introduces a contamination-resistant, language-agnostic methodology\nfor dynamic benchmark construction, offering a principled foundation for the\ncomprehensive and realistic evaluation of LLMs on real-world software\ndevelopment tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCODE2BENCH\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u771f\u5b9e\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6c61\u67d3\u548c\u6d4b\u8bd5\u4e25\u8c28\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u6d4b\u8bd5\u4e0d\u4e25\u8c28\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5bf9LLMs\u6027\u80fd\u7684\u6709\u6548\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86CODE2BENCH\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u70b9\uff1a\u81ea\u52a8\u52a8\u6001\u66f4\u65b0\u4ee3\u7801\u5e93\u4ee5\u51cf\u5c11\u6570\u636e\u6c61\u67d3\u3001\u57fa\u4e8e\u4f9d\u8d56\u5206\u6790\u7684\u51fd\u6570\u5206\u7c7b\u3001\u4ee5\u53ca\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\u751f\u6210\u4e25\u8c28\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u6784\u5efa\u4e86CODE2BENCH-2505\u57fa\u51c6\uff0c\u5305\u542b1,163\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6880\u4e2aPython\u9879\u76ee\uff0c\u6d4b\u8bd5\u663e\u793aLLMs\u5728\u590d\u6742\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u5728Python\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u597d\u3002", "conclusion": "CODE2BENCH\u63d0\u4f9b\u4e86\u4e00\u79cd\u6297\u6c61\u67d3\u3001\u8bed\u8a00\u65e0\u5173\u7684\u52a8\u6001\u57fa\u51c6\u6784\u5efa\u65b9\u6cd5\uff0c\u4e3aLLMs\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06773", "pdf": "https://arxiv.org/pdf/2508.06773", "abs": "https://arxiv.org/abs/2508.06773", "authors": ["Alex Escalante Viteri", "Javier Gamboa Cruzado", "Leonidas Asto Huaman"], "title": "Methodology for Business Intelligence Solutions in Internet Banking Companies", "categories": ["cs.HC"], "comment": "9 pages 3 figures", "summary": "Business intelligence in the banking industry has been studied extensively in\nthe last decade; however, business executives still do not perceive efficiency\nin the decision-making process since the management and treatment of\ninformation are very timeconsuming for the deliverer, generating costs in the\nprocess. On the other hand, there is no formal methodology for developing\nbusiness intelligence solutions in this sector. This work aims to optimize\ndecision-making in a business unit that works with internet banking companies,\nreducing the time, the number of people, and the costs involved in\ndecision-making. To meet the objective, basic and applied research was\nconducted. The basic research allowed the construction of a new methodology\nfrom a study of critical success factors and approaches from the business\nintelligence literature. The applied research involved the implementation of a\nbusiness intelligence solution applying the new methodology in a\npre-experimental study. Thirty decision-making processes were analyzed using\npre-test and post-test data. Tools such as a stopwatch and observation were\nused to collect and record data on time spent, the number of people, and the\ndecision-making costs. This information was processed in the specialized\nMinitab18 statistical software, which allowed the observation and confirmation\nof relevant results regarding time reduction, the number of people, and the\ncosts generated. Therefore, it was concluded that the business intelligence\nsolution, applying the new methodology, optimized decision making in the\nbusiness unit that works with internet banking for companies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5546\u4e1a\u667a\u80fd\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u4f18\u5316\u94f6\u884c\u4e1a\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u65f6\u95f4\u3001\u4eba\u529b\u548c\u6210\u672c\u3002", "motivation": "\u94f6\u884c\u4e1a\u51b3\u7b56\u6548\u7387\u4f4e\u4e0b\u3001\u4fe1\u606f\u5904\u7406\u8017\u65f6\u4e14\u65e0\u6b63\u5f0f\u65b9\u6cd5\u8bba\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u7ed3\u5408\u57fa\u7840\u7814\u7a76\u4e0e\u5e94\u7528\u7814\u7a76\uff0c\u6784\u5efa\u65b0\u65b9\u6cd5\u8bba\u5e76\u5b9e\u65bd\u5546\u4e1a\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5206\u679030\u4e2a\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u65b0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u51b3\u7b56\u65f6\u95f4\u3001\u4eba\u529b\u548c\u6210\u672c\u3002", "conclusion": "\u65b0\u5546\u4e1a\u667a\u80fd\u65b9\u6cd5\u8bba\u6709\u6548\u4f18\u5316\u4e86\u94f6\u884c\u4f01\u4e1a\u4e92\u8054\u7f51\u4e1a\u52a1\u7684\u51b3\u7b56\u3002"}}
{"id": "2508.07608", "pdf": "https://arxiv.org/pdf/2508.07608", "abs": "https://arxiv.org/abs/2508.07608", "authors": ["Junxiao Xue", "Xiaozhen Liu", "Xuecheng Wu", "Xinyi Yin", "Danlei Huang", "Fei Yu"], "title": "AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted by the ACM MM 2025 Workshop on SVC", "summary": "Audio-visual speech recognition (AVSR) combines audio-visual modalities to\nimprove speech recognition, especially in noisy environments. However, most\nexisting methods deploy the unidirectional enhancement or symmetric fusion\nmanner, which limits their capability to capture heterogeneous and\ncomplementary correlations of audio-visual data-especially under asymmetric\ninformation conditions. To tackle these gaps, we introduce a new AVSR framework\ntermed AD-AVSR based on bidirectional modality enhancement. Specifically, we\nfirst introduce the audio dual-stream encoding strategy to enrich audio\nrepresentations from multiple perspectives and intentionally establish\nasymmetry to support subsequent cross-modal interactions. The enhancement\nprocess involves two key components, Audio-aware Visual Refinement Module for\nenhanced visual representations under audio guidance, and Cross-modal Noise\nSuppression Masking Module which refines audio representations using visual\ncues, collaboratively leading to the closed-loop and bidirectional information\nflow. To further enhance correlation robustness, we adopt a threshold-based\nselection mechanism to filter out irrelevant or weakly correlated audio-visual\npairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate\nthat our AD-AVSR consistently surpasses SOTA methods in both performance and\nnoise robustness, highlighting the effectiveness of our model design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411\u6a21\u6001\u589e\u5f3a\u7684\u65b0AVSR\u6846\u67b6AD-AVSR\uff0c\u901a\u8fc7\u97f3\u9891\u53cc\u6d41\u7f16\u7801\u548c\u8de8\u6a21\u6001\u566a\u58f0\u6291\u5236\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AVSR\u65b9\u6cd5\u5728\u975e\u5bf9\u79f0\u4fe1\u606f\u6761\u4ef6\u4e0b\u96be\u4ee5\u6355\u6349\u97f3\u9891-\u89c6\u89c9\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u4e92\u8865\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u53cc\u5411\u589e\u5f3a\u6846\u67b6\u3002", "method": "\u5f15\u5165\u97f3\u9891\u53cc\u6d41\u7f16\u7801\u7b56\u7565\u548c\u8de8\u6a21\u6001\u566a\u58f0\u6291\u5236\u63a9\u6a21\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u5411\u4fe1\u606f\u6d41\u548c\u9608\u503c\u9009\u62e9\u673a\u5236\u4f18\u5316\u97f3\u9891-\u89c6\u89c9\u76f8\u5173\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "result": "\u5728LRS2\u548cLRS3\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAD-AVSR\u5728\u6027\u80fd\u548c\u566a\u58f0\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "AD-AVSR\u901a\u8fc7\u53cc\u5411\u6a21\u6001\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86AVSR\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5c55\u73b0\u4e86\u5176\u8bbe\u8ba1\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.08228", "pdf": "https://arxiv.org/pdf/2508.08228", "abs": "https://arxiv.org/abs/2508.08228", "authors": ["Sining Lu", "Guan Chen", "Nam Anh Dinh", "Itai Lang", "Ari Holtzman", "Rana Hanocka"], "title": "LL3M: Large Language 3D Modelers", "categories": ["cs.GR", "cs.AI"], "comment": "Our project page is at https://threedle.github.io/ll3m", "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.", "AI": {"tldr": "LL3M\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684Blender Python\u4ee3\u7801\u6765\u521b\u5efa3D\u8d44\u4ea7\u3002\u5b83\u5c06\u5f62\u72b6\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ee3\u7801\u7f16\u5199\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u6a21\u5757\u5316\u3001\u53ef\u7f16\u8f91\u6027\u548c\u4e0e\u827a\u672f\u5bb6\u5de5\u4f5c\u6d41\u7684\u96c6\u6210\u3002", "motivation": "\u4f20\u7edf\u76843D\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e3D\u6570\u636e\u96c6\u5408\u7684\u5b66\u4e60\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u3002LL3M\u901a\u8fc7\u751f\u6210\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u3001\u53ef\u7f16\u8f91\u4e14\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LL3M\u5229\u7528\u4e00\u7ec4\u4e13\u4e1a\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u89c4\u5212\u3001\u68c0\u7d22\u3001\u7f16\u5199\u3001\u8c03\u8bd5\u548c\u4f18\u5316Blender\u811a\u672c\u6765\u751f\u6210\u548c\u7f16\u8f91\u51e0\u4f55\u548c\u5916\u89c2\u3002\u7cfb\u7edf\u8fd8\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u77e5\u8bc6\u5e93\uff08BlenderRAG\uff09\uff0c\u652f\u6301\u9ad8\u7ea7\u5efa\u6a21\u64cd\u4f5c\u548c\u4ee3\u7801\u6b63\u786e\u6027\u3002", "result": "LL3M\u5728\u591a\u6837\u5316\u7684\u5f62\u72b6\u7c7b\u522b\u3001\u98ce\u683c\u548c\u6750\u8d28\u7f16\u8f91\u4ee5\u53ca\u7528\u6237\u9a71\u52a8\u7684\u7ec6\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u548c\u53ef\u89e3\u91ca\u4ecb\u8d28\u7684\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "LL3M\u901a\u8fc7\u4ee3\u7801\u751f\u62103D\u8d44\u4ea7\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u53ef\u7f16\u8f91\u4e14\u4e0e\u827a\u672f\u5bb6\u5de5\u4f5c\u6d41\u517c\u5bb9\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a3D\u521b\u4f5c\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.07963", "pdf": "https://arxiv.org/pdf/2508.07963", "abs": "https://arxiv.org/abs/2508.07963", "authors": ["Javier Esparza", "Vincent Fischer"], "title": "Runtime Verification for LTL in Stochastic Systems", "categories": ["cs.LO"], "comment": null, "summary": "Runtime verification encompasses several lightweight techniques for checking\nwhether a system's current execution satisfies a given specification. We focus\non runtime verification for Linear Temporal Logic (LTL). Previous work\ndescribes monitors which produce, at every time step one of three outputs -\ntrue, false, or inconclusive - depending on whether the observed execution\nprefix definitively determines satisfaction of the formula. However, for many\nLTL formulas, such as liveness properties, satisfaction cannot be concluded\nfrom any finite prefix. For these properties traditional monitors will always\noutput inconclusive. In this work, we propose a novel monitoring approach that\nreplaces hard verdicts with probabilistic predictions and an associated\nconfidence score. Our method guarantees eventual correctness of the prediction\nand ensures that confidence increases without bound from that point on.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u884c\u65f6\u76d1\u63a7\u65b9\u6cd5\uff0c\u7528\u6982\u7387\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u66ff\u4ee3\u4f20\u7edf\u76d1\u63a7\u5668\u7684\u4e8c\u5143\u5224\u51b3\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u6700\u7ec8\u6b63\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u589e\u957f\u3002", "motivation": "\u4f20\u7edf\u76d1\u63a7\u5668\u5bf9LTL\u516c\u5f0f\uff08\u5c24\u5176\u662f\u6d3b\u6027\u6027\u8d28\uff09\u7684\u6709\u9650\u524d\u7f00\u53ea\u80fd\u8f93\u51fa\u2018\u4e0d\u786e\u5b9a\u2019\uff0c\u65e0\u6cd5\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u91c7\u7528\u6982\u7387\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u53d6\u4ee3\u4f20\u7edf\u76d1\u63a7\u5668\u7684\u4e8c\u5143\u5224\u51b3\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u6e10\u8fdb\u6b63\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u589e\u957f\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u591f\u5bf9\u65e0\u6cd5\u901a\u8fc7\u6709\u9650\u524d\u7f00\u5224\u65ad\u7684LTL\u516c\u5f0f\u63d0\u4f9b\u9884\u6d4b\uff0c\u5e76\u4fdd\u8bc1\u6700\u7ec8\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fd0\u884c\u65f6\u76d1\u63a7\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65e0\u6cd5\u901a\u8fc7\u4f20\u7edf\u65b9\u6cd5\u5224\u65ad\u7684LTL\u6027\u8d28\u3002"}}
{"id": "2508.07218", "pdf": "https://arxiv.org/pdf/2508.07218", "abs": "https://arxiv.org/abs/2508.07218", "authors": ["Yunjun Gao", "Ruijie Zhao", "Zhonggen Li", "Baihua Zheng", "Yifan Zhu", "Zhaoqing Chen"], "title": "Accelerating High-Dimensional Nearest Neighbor Search with Dynamic Query Preference", "categories": ["cs.DB"], "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) is a crucial operation in\ndatabases and artificial intelligence. Current graph-based ANNS methods, such\nas HNSW and NSG, have shown remarkable performance but are designed under the\nassumption of a uniform query distribution. However, in practical scenarios,\nuser preferences and query temporal dynamics lead to some queries being\nsearched for more frequently than others. To fully utilize these\ncharacteristics, we propose DQF, a novel Dual-Index Query Framework. This\nframework comprises a dual-layer index structure and a dynamic search strategy\nbased on a decision tree. The dual-layer index structure comprises a hot index\nfor high-frequency nodes and a full index for the entire dataset, allowing for\nthe separate management of hot and cold queries. Furthermore, we propose a\ndynamic search strategy that employs a decision tree to adapt to the specific\ncharacteristics of each query. The decision tree evaluates whether a query is\nof the high-frequency type to detect the opportunities for early termination on\nthe dual-layer, avoiding unnecessary searches in the full index. Experimental\nresults on four real-world datasets demonstrate that the Dual-Index Query\nFramework achieves a significant speedup of 2.0-5.7x over state-of-the-art\nalgorithms while maintaining a 95% recall rate. Importantly, it does not\nrequire full index reconstruction when query distributions change, underscoring\nits efficiency and practicality in dynamic query distribution scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u7d22\u5f15\u67e5\u8be2\u6846\u67b6\uff08DQF\uff09\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u7b56\u7565\u548c\u53cc\u5c42\u7d22\u5f15\u7ed3\u6784\u4f18\u5316\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684ANNS\u65b9\u6cd5\uff08\u5982HNSW\u548cNSG\uff09\u5047\u8bbe\u67e5\u8be2\u5206\u5e03\u5747\u5300\uff0c\u4f46\u5b9e\u9645\u4e2d\u67e5\u8be2\u9891\u7387\u5b58\u5728\u52a8\u6001\u53d8\u5316\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u5c42\u7d22\u5f15\u7ed3\u6784\uff08\u70ed\u7d22\u5f15\u548c\u5168\u7d22\u5f15\uff09\u548c\u57fa\u4e8e\u51b3\u7b56\u6811\u7684\u52a8\u6001\u641c\u7d22\u7b56\u7565\uff0c\u5206\u522b\u7ba1\u7406\u9ad8\u9891\u548c\u4f4e\u9891\u67e5\u8be2\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.0-5.7\u500d\u7684\u52a0\u901f\uff0c\u4e14\u4fdd\u630195%\u53ec\u56de\u7387\uff0c\u65e0\u9700\u968f\u67e5\u8be2\u5206\u5e03\u53d8\u5316\u91cd\u5efa\u7d22\u5f15\u3002", "conclusion": "DQF\u5728\u52a8\u6001\u67e5\u8be2\u5206\u5e03\u573a\u666f\u4e0b\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07457", "pdf": "https://arxiv.org/pdf/2508.07457", "abs": "https://arxiv.org/abs/2508.07457", "authors": ["Janith Petangoda", "Chatura Samarakoon", "James Meech", "Divya Thekke Kanapram", "Hamid Toshani", "Nathaniel Tye", "Vasileios Tsoutsouras", "Phillip Stanley-Marbell"], "title": "The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It", "categories": ["cs.AR", "A.1; C.1.1; G.3; I.6.1; I.6.8"], "comment": "15 pages, 4 figures (17 subfigures)", "summary": "Computing systems interacting with real-world processes must safely and\nreliably process uncertain data. The Monte Carlo method is a popular approach\nfor computing with such uncertain values. This article introduces a framework\nfor describing the Monte Carlo method and highlights two advances in the domain\nof physics-based non-uniform random variate generators (PPRVGs) to overcome\ncommon limitations of traditional Monte Carlo sampling. This article also\nhighlights recent advances in architectural techniques that eliminate the need\nto use the Monte Carlo method by leveraging distributional microarchitectural\nstate to natively compute on probability distributions. Unlike Monte Carlo\nmethods, uncertainty-tracking processor architectures can be said to be\nconvergence-oblivious.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u53ca\u5176\u5728\u7269\u7406\u57fa\u7840\u975e\u5747\u5300\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\u4e2d\u7684\u4e24\u9879\u6539\u8fdb\uff0c\u4ee5\u53ca\u65e0\u9700\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u7684\u65b0\u578b\u67b6\u6784\u6280\u672f\u3002", "motivation": "\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6570\u636e\u9700\u8981\u5b89\u5168\u53ef\u9760\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4f20\u7edf\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7269\u7406\u57fa\u7840\u975e\u5747\u5300\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6982\u7387\u5206\u5e03\u7684\u5fae\u67b6\u6784\u6280\u672f\u3002", "result": "\u65b0\u578b\u67b6\u6784\u6280\u672f\u907f\u514d\u4e86\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u53ef\u76f4\u63a5\u5904\u7406\u6982\u7387\u5206\u5e03\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u8ddf\u8e2a\u5904\u7406\u5668\u67b6\u6784\u4e3a\u5904\u7406\u4e0d\u786e\u5b9a\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07124", "pdf": "https://arxiv.org/pdf/2508.07124", "abs": "https://arxiv.org/abs/2508.07124", "authors": ["Shashwat Jaiswal", "Suman Raj", "Subhajit Sidhanta", "Yogesh Simmhan"], "title": "AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets", "categories": ["cs.DC", "cs.DB"], "comment": null, "summary": "Recent years have seen an unprecedented growth in research that leverages the\nnewest computing paradigm of Internet of Drones, comprising a fleet of\nconnected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such\nas monitoring and analytics in highly mobile and changing environments\ncharacteristic of disaster regions. Given that the typical data (i.e., videos\nand images) collected by the fleet of UAVs deployed in such scenarios can be\nconsiderably larger than what the onboard computers can process, the UAVs need\nto offload their data in real-time to the edge and the cloud for further\nprocessing. To that end, we present the design of AerialDB - a lightweight\ndecentralized data storage and query system that can store and process time\nseries data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs\nfitted with onboard computers, and B) ground-based edge servers connected\nthrough a cellular link. Leveraging lightweight techniques for content-based\nreplica placement and indexing of shards, AerialDB has been optimized for\nefficient processing of different possible combinations of typical spatial and\ntemporal queries performed by real-world disaster management applications.\nUsing containerized deployment spanning up to 400 drones and 80 edges, we\ndemonstrate that AerialDB is able to scale efficiently while providing near\nreal-time performance with different realistic workloads. Further, AerialDB\ncomprises a decentralized and locality-aware distributed execution engine which\nprovides graceful degradation of performance upon edge failures with relatively\nlow latency while processing large spatio-temporal data. AerialDB exhibits\ncomparable insertion performance and 100 times improvement in query performance\nagainst state-of-the-art baseline. Moreover, it exhibits a 10 times and 100\ntimes improvement with insertion and query workloads respectively over the\ncloud baseline.", "AI": {"tldr": "AerialDB\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u8f7b\u91cf\u7ea7\u5206\u5e03\u5f0f\u6570\u636e\u5b58\u50a8\u548c\u67e5\u8be2\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u707e\u5bb3\u7ba1\u7406\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u707e\u5bb3\u5730\u533a\u6536\u96c6\u7684\u5927\u89c4\u6a21\u6570\u636e\u65e0\u6cd5\u7531\u673a\u8f7d\u8ba1\u7b97\u673a\u5b9e\u65f6\u5904\u7406\u7684\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u9ad8\u6548\u7684\u7cfb\u7edf\u652f\u6301\u6570\u636e\u5378\u8f7d\u548c\u5b9e\u65f6\u67e5\u8be2\u3002", "method": "AerialDB\u91c7\u7528\u5206\u6563\u5f0f\u5b58\u50a8\u548c\u67e5\u8be2\u8bbe\u8ba1\uff0c\u7ed3\u5408\u57fa\u4e8e\u5185\u5bb9\u7684\u526f\u672c\u653e\u7f6e\u548c\u5206\u7247\u7d22\u5f15\u6280\u672f\uff0c\u4f18\u5316\u7a7a\u95f4\u548c\u65f6\u95f4\u67e5\u8be2\u3002", "result": "\u5728400\u67b6\u65e0\u4eba\u673a\u548c80\u4e2a\u8fb9\u7f18\u8282\u70b9\u7684\u6d4b\u8bd5\u4e2d\uff0cAerialDB\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u6269\u5c55\u6027\u548c\u8fd1\u5b9e\u65f6\u6027\u80fd\uff0c\u5728\u63d2\u5165\u548c\u67e5\u8be2\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "AerialDB\u4e3a\u707e\u5bb3\u7ba1\u7406\u4e2d\u7684\u65e0\u4eba\u673a\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.07194", "pdf": "https://arxiv.org/pdf/2508.07194", "abs": "https://arxiv.org/abs/2508.07194", "authors": ["Jack Wampler", "Hammas Bin Tanveer", "Rishab Nithyanand", "Eric Wustrow"], "title": "ProtoScan: Measuring censorship in IPv6", "categories": ["cs.NI"], "comment": "10 pages, 2 figures, 2 tables", "summary": "Internet censorship continues to impact billions of people worldwide, and\nmeasurement of it remains an important focus of research. However, most\nInternet censorship measurements have focused solely on the IPv4 Internet\ninfrastructure. Yet, more clients and servers are available over IPv6:\nAccording to Google, over a third of their users now have native IPv6 access.\nGiven the slow-but-steady rate of IPv6 adoption, it is important to understand\nits impact on censorship. In this paper, we measure and analyze how censorship\ndiffers over IPv6 compared to the well-studied IPv4 censorship systems in use\ntoday. We perform a comprehensive global study of censorship across an array of\ncommonly censored protocols, including HTTP, DNS, and TLS, on both IPv4 and\nIPv6, and compare the results. We find that there are several differences in\nhow countries censor IPv6 traffic, both in terms of IPv6 resources, and in\nwhere and what blocklists or technologies are deployed on IPv6 networks. Many\nof these differences are not all-or-nothing: we find that most censors have\nsome capacity to block in IPv6, but are less comprehensive or less reliable\ncompared to their IPv4 censorship systems. Our results suggest that IPv6 offers\nnew areas for censorship circumvention researchers to explore, providing\npotentially new ways to evade censors. As more users gain access to IPv6\naddresses and networks, there will be a need for tools that take advantage of\nIPv6 techniques and infrastructure to bypass censorship.", "AI": {"tldr": "\u7814\u7a76\u4e86IPv6\u4e0eIPv4\u5728\u4e92\u8054\u7f51\u5ba1\u67e5\u4e2d\u7684\u5dee\u5f02\uff0c\u53d1\u73b0IPv6\u5ba1\u67e5\u5728\u67d0\u4e9b\u56fd\u5bb6\u4e0d\u591f\u5168\u9762\u6216\u53ef\u9760\u3002", "motivation": "\u968f\u7740IPv6\u7684\u9010\u6b65\u63a8\u5e7f\uff0c\u4e86\u89e3\u5176\u5bf9\u4e92\u8054\u7f51\u5ba1\u67e5\u7684\u5f71\u54cd\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5168\u7403\u6027\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86IPv4\u548cIPv6\u5728HTTP\u3001DNS\u548cTLS\u7b49\u534f\u8bae\u4e2d\u7684\u5ba1\u67e5\u60c5\u51b5\u3002", "result": "\u53d1\u73b0IPv6\u5ba1\u67e5\u80fd\u529b\u8f83\u5f31\uff0c\u53ef\u80fd\u4e3a\u7ed5\u8fc7\u5ba1\u67e5\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "conclusion": "IPv6\u4e3a\u5ba1\u67e5\u7ed5\u8fc7\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u591a\u5de5\u5177\u5229\u7528IPv6\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2409.15173", "pdf": "https://arxiv.org/pdf/2409.15173", "abs": "https://arxiv.org/abs/2409.15173", "authors": ["Yashar Deldjoo", "Zhankui He", "Julian McAuley", "Anton Korikov", "Scott Sanner", "Arnau Ramisa", "Rene Vidal", "Maheswaran Sathiamoorthy", "Atoosa Kasrizadeh", "Silvia Milano", "Francesco Ricci"], "title": "Recommendation with Generative Models", "categories": ["cs.IR", "cs.AI", "cs.ET"], "comment": "This submission is a full-length book, expanding significantly on two\n  chapters previously submitted (arXiv:2409.10993v1, arXiv:2408.10946v1). It\n  includes additional chapters, context, analysis, and content, providing a\n  comprehensive presentation of the subject. We have ensured it is\n  appropriately presented as a new, distinct work. arXiv admin note:\n  substantial text overlap with arXiv:2409.10993", "summary": "Generative models are a class of AI models capable of creating new instances\nof data by learning and sampling from their statistical distributions. In\nrecent years, these models have gained prominence in machine learning due to\nthe development of approaches such as generative adversarial networks (GANs),\nvariational autoencoders (VAEs), and transformer-based architectures such as\nGPT. These models have applications across various domains, such as image\ngeneration, text synthesis, and music composition. In recommender systems,\ngenerative models, referred to as Gen-RecSys, improve the accuracy and\ndiversity of recommendations by generating structured outputs, text-based\ninteractions, and multimedia content. By leveraging these capabilities,\nGen-RecSys can produce more personalized, engaging, and dynamic user\nexperiences, expanding the role of AI in eCommerce, media, and beyond.\n  Our book goes beyond existing literature by offering a comprehensive\nunderstanding of generative models and their applications, with a special focus\non deep generative models (DGMs) and their classification. We introduce a\ntaxonomy that categorizes DGMs into three types: ID-driven models, large\nlanguage models (LLMs), and multimodal models. Each category addresses unique\ntechnical and architectural advancements within its respective research area.\nThis taxonomy allows researchers to easily navigate developments in Gen-RecSys\nacross domains such as conversational AI and multimodal content generation.\nAdditionally, we examine the impact and potential risks of generative models,\nemphasizing the importance of robust evaluation frameworks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u4e86\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u3001VAEs\u548cGPT\uff09\u5728\u63a8\u8350\u7cfb\u7edf\uff08Gen-RecSys\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08DGMs\uff09\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u6587\u732e\u5bf9\u5176\u5206\u7c7b\u548c\u5e94\u7528\u7684\u5168\u9762\u7406\u89e3\u4ecd\u9700\u5b8c\u5584\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5c06\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08DGMs\uff09\u5206\u4e3aID\u9a71\u52a8\u6a21\u578b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u6a21\u578b\u4e09\u7c7b\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u6cd5\uff0c\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u66f4\u8f7b\u677e\u5730\u8ddf\u8e2a\u751f\u6210\u6a21\u578b\u5728\u5bf9\u8bdd\u5f0fAI\u548c\u591a\u6a21\u6001\u5185\u5bb9\u751f\u6210\u7b49\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e5f\u9700\u8981\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u5e94\u5bf9\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2508.07198", "pdf": "https://arxiv.org/pdf/2508.07198", "abs": "https://arxiv.org/abs/2508.07198", "authors": ["Burak Yeti\u015ftiren", "Hong Jin Kang", "Miryung Kim"], "title": "TraceLens: Question-Driven Debugging for Taint Flow Understanding", "categories": ["cs.SE"], "comment": null, "summary": "Taint analysis is a security analysis technique used to track the flow of\npotentially dangerous data through an application and its dependent libraries.\nInvestigating why certain unexpected flows appear and why expected flows are\nmissing is an important sensemaking process during end-user taint analysis.\nExisting taint analysis tools often do not provide this end-user debugging\ncapability, where developers can ask why, why-not, and what-if questions about\ndataflows and reason about the impact of configuring sources and sinks, and\nmodels of 3rd-party libraries that abstract permissible and impermissible data\nflows. Furthermore, a tree-view or a list-view used in existing\ntaint-analyzer's visualization makes it difficult to reason about the global\nimpact on connectivity between multiple sources and sinks.\n  Inspired by the insight that sensemaking tool-generated results can be\nsignificantly improved by a QA inquiry process, we propose TraceLens, a first\nend-user question-answer style debugging interface for taint analysis. It\nenables a user to ask why, why-not, and what-if questions to investigate the\nexistence of suspicious flows, the non-existence of expected flows, and the\nglobal impact of third-party library models. TraceLens performs speculative\nwhat-if analysis, to help a user in debugging how different connectivity\nassumptions affect overall results. A user study with 12 participants shows\nthat participants using TraceLens achieved 21% higher accuracy on average,\ncompared to CodeQL. They also reported a 45% reduction in mental demand\n(NASA-TLX) and rated higher confidence in identifying relevant flows using\nTraceLens.", "AI": {"tldr": "TraceLens\u662f\u4e00\u4e2a\u57fa\u4e8e\u95ee\u7b54\u8c03\u8bd5\u7684\u6c61\u70b9\u5206\u6790\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u5206\u6790\u6570\u636e\u6d41\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6c61\u70b9\u5206\u6790\u5de5\u5177\u7f3a\u4e4f\u5bf9\u6570\u636e\u6d41\u7684\u8c03\u8bd5\u80fd\u529b\uff0c\u96be\u4ee5\u56de\u7b54'\u4e3a\u4ec0\u4e48'\u3001'\u4e3a\u4ec0\u4e48\u6ca1\u6709'\u548c'\u5047\u8bbe'\u7c7b\u95ee\u9898\u3002", "method": "\u63d0\u51faTraceLens\uff0c\u652f\u6301\u95ee\u7b54\u5f0f\u8c03\u8bd5\uff0c\u5305\u62ec\u5047\u8bbe\u5206\u6790\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u6570\u636e\u6d41\u7684\u5f71\u54cd\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cTraceLens\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4CodeQL\u9ad821%\uff0c\u4e14\u7528\u6237\u5fc3\u7406\u8d1f\u62c5\u51cf\u5c1145%\u3002", "conclusion": "TraceLens\u901a\u8fc7\u95ee\u7b54\u5f0f\u8c03\u8bd5\u663e\u8457\u63d0\u5347\u4e86\u6c61\u70b9\u5206\u6790\u7684\u53ef\u7528\u6027\u548c\u5206\u6790\u6548\u679c\u3002"}}
{"id": "2508.06775", "pdf": "https://arxiv.org/pdf/2508.06775", "abs": "https://arxiv.org/abs/2508.06775", "authors": ["Michelle Morgenstern", "Amy Rae Fox", "Graham M. Jones", "Arvind Satyanarayan"], "title": "Visualization Vibes: The Socio-Indexical Function of Visualization Design", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "In contemporary information ecologies saturated with misinformation,\ndisinformation, and a distrust of science itself, public data communication\nfaces significant hurdles. Although visualization research has broadened\ncriteria for effective design, governing paradigms privilege the accurate and\nefficient transmission of data. Drawing on theory from linguistic anthropology,\nwe argue that such approaches-focused on encoding and decoding propositional\ncontent-cannot fully account for how people engage with visualizations and why\nparticular visualizations might invite adversarial or receptive responses. In\nthis paper, we present evidence that data visualizations communicate not only\nsemantic, propositional meaning$\\unicode{x2013}$meaning about\ndata$\\unicode{x2013}$but also social, indexical meaning$\\unicode{x2013}$meaning\nbeyond data. From a series of ethnographically-informed interviews, we document\nhow readers make rich and varied assessments of a visualization's\n\"vibes\"$\\unicode{x2013}$inferences about the social provenance of a\nvisualization based on its design features. Furthermore, these social\nattributions have the power to influence reception, as readers' decisions about\nhow to engage with a visualization concern not only content, or even aesthetic\nappeal, but also their sense of alignment or disalignment with the entities\nthey imagine to be involved in its production and circulation. We argue these\ninferences hinge on a function of human sign systems that has thus far been\nlittle studied in data visualization: socio-indexicality, whereby the formal\nfeatures (rather than the content) of communication evoke social contexts,\nidentities, and characteristics. Demonstrating the presence and significance of\nthis socio-indexical function in visualization, this paper offers both a\nconceptual foundation and practical intervention for troubleshooting breakdowns\nin public data communication.", "AI": {"tldr": "\u53ef\u89c6\u5316\u7814\u7a76\u901a\u5e38\u5173\u6ce8\u6570\u636e\u7684\u51c6\u786e\u548c\u9ad8\u6548\u4f20\u8f93\uff0c\u4f46\u5ffd\u89c6\u4e86\u793e\u4f1a\u80cc\u666f\u5bf9\u53d7\u4f17\u63a5\u6536\u7684\u5f71\u54cd\u3002\u672c\u6587\u901a\u8fc7\u8bed\u8a00\u5b66\u4eba\u7c7b\u5b66\u7406\u8bba\uff0c\u63d0\u51fa\u53ef\u89c6\u5316\u4e0d\u4ec5\u4f20\u8fbe\u6570\u636e\u610f\u4e49\uff0c\u8fd8\u4f20\u9012\u793e\u4f1a\u610f\u4e49\uff0c\u5f71\u54cd\u53d7\u4f17\u7684\u63a5\u53d7\u5ea6\u3002", "motivation": "\u5728\u4fe1\u606f\u65f6\u4ee3\uff0c\u865a\u5047\u4fe1\u606f\u548c\u79d1\u5b66\u4e0d\u4fe1\u4efb\u5bfc\u81f4\u516c\u5171\u6570\u636e\u4f20\u64ad\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u5f3a\u8c03\u6570\u636e\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u8bbe\u8ba1\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u53d7\u4f17\u7684\u793e\u4f1a\u8ba4\u77e5\u3002", "method": "\u901a\u8fc7\u6c11\u65cf\u5fd7\u8bbf\u8c08\uff0c\u5206\u6790\u8bfb\u8005\u5982\u4f55\u57fa\u4e8e\u8bbe\u8ba1\u7279\u5f81\u63a8\u65ad\u53ef\u89c6\u5316\u7684\u201c\u6c1b\u56f4\u201d\uff08\u793e\u4f1a\u80cc\u666f\uff09\uff0c\u4ece\u800c\u5f71\u54cd\u5176\u63a5\u53d7\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53ef\u89c6\u5316\u7684\u5f62\u5f0f\u7279\u5f81\uff08\u975e\u5185\u5bb9\uff09\u80fd\u5524\u8d77\u793e\u4f1a\u80cc\u666f\u8ba4\u540c\uff0c\u663e\u8457\u5f71\u54cd\u53d7\u4f17\u7684\u63a5\u53d7\u4e0e\u4e92\u52a8\u65b9\u5f0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u201c\u793e\u4f1a\u7d22\u5f15\u6027\u201d\u6982\u5ff5\uff0c\u4e3a\u516c\u5171\u6570\u636e\u4f20\u64ad\u4e2d\u7684\u95ee\u9898\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u9700\u8003\u8651\u793e\u4f1a\u80cc\u666f\u5f71\u54cd\u3002"}}
{"id": "2508.07666", "pdf": "https://arxiv.org/pdf/2508.07666", "abs": "https://arxiv.org/abs/2508.07666", "authors": ["Xianbing Zhao", "Shengzun Yang", "Buzhou Tang", "Ronghuan Jiang"], "title": "Towards Multimodal Sentiment Analysis via Contrastive Cross-modal Retrieval Augmentation and Hierachical Prompts", "categories": ["cs.MM"], "comment": "Under review", "summary": "Multimodal sentiment analysis is a fundamental problem in the field of\naffective computing. Although significant progress has been made in cross-modal\ninteraction, it remains a challenge due to the insufficient reference context\nin cross-modal interactions. Current cross-modal approaches primarily focus on\nleveraging modality-level reference context within a individual sample for\ncross-modal feature enhancement, neglecting the potential cross-sample\nrelationships that can serve as sample-level reference context to enhance the\ncross-modal features. To address this issue, we propose a novel multimodal\nretrieval-augmented framework to simultaneously incorporate inter-sample\nmodality-level reference context and cross-sample sample-level reference\ncontext to enhance the multimodal features. In particular, we first design a\ncontrastive cross-modal retrieval module to retrieve semantic similar samples\nand enhance target modality. To endow the model to capture both inter-sample\nand intra-sample information, we integrate two different types of prompts,\nmodality-level prompts and sample-level prompts, to generate modality-level and\nsample-level reference contexts, respectively. Finally, we design a cross-modal\nretrieval-augmented encoder that simultaneously leverages modality-level and\nsample-level reference contexts to enhance the target modality. Extensive\nexperiments demonstrate the effectiveness and superiority of our model on two\npublicly available datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u6837\u672c\u95f4\u548c\u6837\u672c\u5185\u7684\u53c2\u8003\u4e0a\u4e0b\u6587\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3\u8de8\u6a21\u6001\u4ea4\u4e92\u4e2d\u53c2\u8003\u4e0a\u4e0b\u6587\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6837\u672c\u5185\u7684\u6a21\u6001\u7ea7\u53c2\u8003\u4e0a\u4e0b\u6587\uff0c\u5ffd\u7565\u4e86\u8de8\u6837\u672c\u5173\u7cfb\u7684\u6f5c\u5728\u4ef7\u503c\u3002", "method": "\u8bbe\u8ba1\u4e86\u5bf9\u6bd4\u8de8\u6a21\u6001\u68c0\u7d22\u6a21\u5757\u3001\u4e24\u79cd\u63d0\u793a\uff08\u6a21\u6001\u7ea7\u548c\u6837\u672c\u7ea7\uff09\u4ee5\u53ca\u8de8\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u7f16\u7801\u5668\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u540c\u65f6\u5229\u7528\u6a21\u6001\u7ea7\u548c\u6837\u672c\u7ea7\u53c2\u8003\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u7279\u5f81\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2508.06757", "pdf": "https://arxiv.org/pdf/2508.06757", "abs": "https://arxiv.org/abs/2508.06757", "authors": ["Yash Garg", "Saketh Bachu", "Arindam Dutta", "Rohit Lal", "Sarosij Bose", "Calvin-Khang Ta", "M. Salman Asif", "Amit Roy-Chowdhury"], "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Human pose and shape (HPS) estimation methods have been extensively studied,\nwith many demonstrating high zero-shot performance on in-the-wild images and\nvideos. However, these methods often struggle in challenging scenarios\ninvolving complex human poses or significant occlusions. Although some studies\naddress 3D human pose estimation under occlusion, they typically evaluate\nperformance on datasets that lack realistic or substantial occlusions, e.g.,\nmost existing datasets introduce occlusions with random patches over the human\nor clipart-style overlays, which may not reflect real-world challenges. To\nbridge this gap in realistic occlusion datasets, we introduce a novel benchmark\ndataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and\nshape annotations. Inspired by works such as AGORA and BEDLAM, we constructed\nthis dataset using advanced computer graphics rendering techniques,\nincorporating diverse real-world occlusion scenarios, clothing textures, and\nhuman motions. Additionally, we fine-tuned recent HPS methods, CLIFF and\nBEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and\nquantitative improvements across multiple public datasets, as well as on the\ntest split of our dataset, while comparing its performance with other\nstate-of-the-art methods. Furthermore, we leveraged our dataset to enhance\nhuman detection performance under occlusion by fine-tuning an existing object\ndetector, YOLO11, thus leading to a robust end-to-end HPS estimation system\nunder occlusions. Overall, this dataset serves as a valuable resource for\nfuture research aimed at benchmarking methods designed to handle occlusions,\noffering a more realistic alternative to existing occlusion datasets. See the\nProject page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVOccl3D\u7684\u65b0\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u67093D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u771f\u5b9e\u906e\u6321\u573a\u666f\u4e0b\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u73b0\u6709\u65b9\u6cd5\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u906e\u6321\u573a\u666f\u4e0b\u76843D\u4eba\u4f53\u59ff\u6001\u4e0e\u5f62\u72b6\u4f30\u8ba1\u7f3a\u4e4f\u771f\u5b9e\u6027\u4e0e\u591a\u6837\u6027\uff0c\u9700\u8981\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u906e\u6321\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u65b9\u6cd5\u6027\u80fd\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u673a\u56fe\u5f62\u6e32\u67d3\u6280\u672f\u6784\u5efaVOccl3D\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u906e\u6321\u573a\u666f\u548c\u4eba\u4f53\u52a8\u4f5c\uff0c\u5e76\u5bf9CLIFF\u548cBEDLAM-CLIFF\u7b49HPS\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u540e\u7684\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u53caVOccl3D\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u4e14\u901a\u8fc7\u5fae\u8c03YOLO11\u63d0\u9ad8\u4e86\u906e\u6321\u4e0b\u7684\u4eba\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "VOccl3D\u4e3a\u672a\u6765\u906e\u6321\u573a\u666f\u4e0b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06706", "pdf": "https://arxiv.org/pdf/2508.06706", "abs": "https://arxiv.org/abs/2508.06706", "authors": ["Jaikrishna Manojkumar Patil", "Nathaniel Lee", "Al Mehdi Saadat Chowdhury", "YooJung Choi", "Paulo Shakarian"], "title": "Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Rule-based methods for knowledge graph completion provide explainable results\nbut often require a significantly large number of rules to achieve competitive\nperformance. This can hinder explainability due to overwhelmingly large rule\nsets. We discover rule contexts (meaningful subsets of rules that work\ntogether) from training data and use learned probability distribution (i.e.\nprobabilistic circuits) over these rule contexts to more rapidly achieve\nperformance of the full rule set. Our approach achieves a 70-96% reduction in\nnumber of rules used while outperforming baseline by up to 31$\\times$ when\nusing equivalent minimal number of rules and preserves 91% of peak baseline\nperformance even when comparing our minimal rule sets against baseline's full\nrule sets. We show that our framework is grounded in well-known semantics of\nprobabilistic logic, does not require independence assumptions, and that our\ntractable inference procedure provides both approximate lower bounds and exact\nprobability of a given query. The efficacy of our method is validated by\nempirical studies on 8 standard benchmark datasets where we show competitive\nperformance by using only a fraction of the rules required by AnyBURL's\nstandard inference method, the current state-of-the-art for rule-based\nknowledge graph completion. This work may have further implications for general\nprobabilistic reasoning over learned sets of rules.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u4e0a\u4e0b\u6587\u548c\u6982\u7387\u5206\u5e03\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c4\u5219\u6570\u91cf\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u89c4\u5219\u624d\u80fd\u8fbe\u5230\u9ad8\u6027\u80fd\uff0c\u4f46\u8fc7\u591a\u7684\u89c4\u5219\u4f1a\u964d\u4f4e\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53d1\u73b0\u89c4\u5219\u4e0a\u4e0b\u6587\u5e76\u5229\u7528\u6982\u7387\u5206\u5e03\u6765\u4f18\u5316\u89c4\u5219\u4f7f\u7528\uff0c\u4ece\u800c\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u53d1\u73b0\u89c4\u5219\u4e0a\u4e0b\u6587\uff0c\u5e76\u5229\u7528\u6982\u7387\u7535\u8def\u5b66\u4e60\u8fd9\u4e9b\u89c4\u5219\u7684\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u89c4\u5219\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u51cf\u5c11\u4e8670-96%\u7684\u89c4\u5219\u6570\u91cf\uff0c\u6027\u80fd\u6700\u9ad8\u63d0\u534731\u500d\uff0c\u4e14\u4fdd\u7559\u4e86\u57fa\u7ebf\u6027\u80fd\u768491%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u89c4\u5219\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u57288\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07427", "pdf": "https://arxiv.org/pdf/2508.07427", "abs": "https://arxiv.org/abs/2508.07427", "authors": ["Emanuele Cavalleri", "Paolo Perlasca", "Marco Mesiti"], "title": "RNA-KG v2.0: An RNA-centered Knowledge Graph with Properties", "categories": ["cs.DB", "q-bio.QM"], "comment": null, "summary": "RNA-KG is a recently developed knowledge graph that integrates the\ninteractions involving coding and non-coding RNA molecules extracted from\npublic data sources. It can be used to support the classification of new\nmolecules, identify new interactions through the use of link prediction\nmethods, and reveal hidden patterns among the represented entities. In this\npaper, we propose RNA-KG v2.0, a new release of RNA-KG that integrates around\n100M manually curated interactions sourced from 91 linked open data\nrepositories and ontologies. Relationships are characterized by standardized\nproperties that capture the specific context (e.g., cell line, tissue,\npathological state) in which they have been identified. In addition, the nodes\nare enriched with detailed attributes, such as descriptions, synonyms, and\nmolecular sequences sourced from platforms such as OBO ontologies, NCBI\nrepositories, RNAcentral, and Ensembl. The enhanced repository enables the\nexpression of advanced queries that take into account the context in which the\nexperiments were conducted. It also supports downstream applications in RNA\nresearch, including \"context-aware\" link prediction techniques that combine\nboth topological and semantic information.", "AI": {"tldr": "RNA-KG v2.0\u662f\u4e00\u4e2a\u5347\u7ea7\u7248\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u6574\u5408\u4e86\u7ea61\u4ebf\u6761\u624b\u52a8\u6574\u7406\u7684RNA\u5206\u5b50\u4ea4\u4e92\u6570\u636e\uff0c\u652f\u6301\u5206\u7c7b\u3001\u94fe\u63a5\u9884\u6d4b\u548c\u6a21\u5f0f\u53d1\u73b0\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u66f4\u5168\u9762\u7684RNA\u5206\u5b50\u4ea4\u4e92\u6570\u636e\u5e76\u652f\u6301\u9ad8\u7ea7\u67e5\u8be2\u548c\u4e0b\u6e38\u5e94\u7528\uff0c\u5982\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u94fe\u63a5\u9884\u6d4b\u3002", "method": "\u6574\u5408\u4e86\u6765\u81ea91\u4e2a\u5f00\u653e\u6570\u636e\u4ed3\u5e93\u548c\u672c\u4f53\u7684100M\u624b\u52a8\u6574\u7406\u4ea4\u4e92\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u5316\u5c5e\u6027\u63cf\u8ff0\u5173\u7cfb\u4e0a\u4e0b\u6587\u548c\u8282\u70b9\u5c5e\u6027\u3002", "result": "\u589e\u5f3a\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u67e5\u8be2\u80fd\u529b\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5206\u6790\u3002", "conclusion": "RNA-KG v2.0\u4e3aRNA\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u652f\u6301\u548c\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.07541", "pdf": "https://arxiv.org/pdf/2508.07541", "abs": "https://arxiv.org/abs/2508.07541", "authors": ["Kittiphon Phalakarn", "Athasit Surarerks"], "title": "A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication", "categories": ["cs.AR"], "comment": null, "summary": "Normal basis is used in many applications because of the efficiency of the\nimplementation. However, most space complexity reduction techniques for binary\nfield multiplier are applicable for only optimal normal basis or Gaussian\nnormal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to\n1,000 that use odd-type Gaussian normal basis. This paper presents a method to\nreduce the space complexity of odd-type Gaussian normal basis multipliers over\nbinary field GF(2^k). The idea is adapted from the matrix decomposition method\nfor optimal normal basis. The result shows that our space complexity reduction\nmethod can reduce the number of XOR gates used in the implementation comparing\nto previous works with a small trade-off in critical path delay.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u4e8c\u8fdb\u5236\u57dfGF(2^k)\u4e0a\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u4e58\u6cd5\u5668\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86XOR\u95e8\u7684\u4f7f\u7528\u6570\u91cf\uff0c\u4f46\u7565\u5fae\u589e\u52a0\u4e86\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u3002", "motivation": "\u867d\u7136\u6b63\u89c4\u57fa\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u73b0\u9ad8\u6548\u7684\u4f18\u70b9\uff0c\u4f46\u73b0\u6709\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u6280\u672f\u4ec5\u9002\u7528\u4e8e\u6700\u4f18\u6b63\u89c4\u57fa\u6216\u5076\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u3002\u5bf9\u4e8e187\u4e2a\u4e8c\u8fdb\u5236\u57dfGF(2^k)\uff08k\u4ece2\u52301,000\uff09\u4e2d\u4f7f\u7528\u7684\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u501f\u9274\u4e86\u6700\u4f18\u6b63\u89c4\u57fa\u7684\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86XOR\u95e8\u7684\u6570\u91cf\uff0c\u4f46\u7565\u5fae\u589e\u52a0\u4e86\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u4e58\u6cd5\u5668\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u4f18\u5316\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u7684\u4e8c\u8fdb\u5236\u57df\u3002"}}
{"id": "2508.07193", "pdf": "https://arxiv.org/pdf/2508.07193", "abs": "https://arxiv.org/abs/2508.07193", "authors": ["Haoyuan Zhang", "Yaqian Gao", "Xinxin Zhang", "Jialin Li", "Runfeng Jin", "Yidong Chen", "Feng Zhang", "Wu Yuan", "Wenpeng Ma", "Shan Liang", "Jian Zhang", "Zhonghua Lu"], "title": "FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs", "categories": ["cs.DC"], "comment": null, "summary": "Efficiently solving large-scale linear systems is a critical challenge in\nelectromagnetic simulations, particularly when using the Crank-Nicolson\nFinite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are\ncommonly employed to handle the resulting sparse systems but suffer from slow\nconvergence due to the ill-conditioned nature of the double-curl operator.\nApproximate preconditioners, like Successive Over-Relaxation (SOR) and\nIncomplete LU decomposition (ILU), provide insufficient convergence, while\ndirect solvers are impractical due to excessive memory requirements. To address\nthis, we propose FlashMP, a novel preconditioning system that designs a\nsubdomain exact solver based on discrete transforms. FlashMP provides an\nefficient GPU implementation that achieves multi-GPU scalability through domain\ndecomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that\nFlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to\n4.9x compared to baseline implementations in state-of-the-art libraries such as\nHypre. Weak scalability tests show parallel efficiencies up to 84.1%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FlashMP\uff0c\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u53d8\u6362\u7684\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u9884\u5904\u7406\u5668\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u78c1\u6a21\u62df\u4e2d\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u7684\u6c42\u89e3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u6c42\u89e3\u5668\u5728\u89e3\u51b3Crank-Nicolson FDTD\u65b9\u6cd5\u4ea7\u751f\u7684\u7a00\u758f\u7cfb\u7edf\u65f6\u6536\u655b\u6162\uff0c\u4f20\u7edf\u9884\u5904\u7406\u5668\u6548\u679c\u6709\u9650\uff0c\u76f4\u63a5\u6c42\u89e3\u5668\u5185\u5b58\u9700\u6c42\u8fc7\u5927\u3002", "method": "\u63d0\u51faFlashMP\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u79bb\u6563\u53d8\u6362\u7684\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u9884\u5904\u7406\u5668\uff0c\u5e76\u901a\u8fc7\u591aGPU\u57df\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\u3002", "result": "\u5728AMD MI60 GPU\u96c6\u7fa4\u4e0a\u6d4b\u8bd5\uff0cFlashMP\u5c06\u8fed\u4ee3\u6b21\u6570\u51cf\u5c1116\u500d\uff0c\u901f\u5ea6\u63d0\u53472.5-4.9\u500d\uff0c\u5e76\u884c\u6548\u7387\u8fbe84.1%\u3002", "conclusion": "FlashMP\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u7684\u6c42\u89e3\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.07197", "pdf": "https://arxiv.org/pdf/2508.07197", "abs": "https://arxiv.org/abs/2508.07197", "authors": ["Ian Martiny", "Hammas Bin Tanveer", "Jack Wampler", "Rishab Nithyanand", "Eric Wustrow"], "title": "Mind the IP Gap: Measuring the impact of IPv6 on DNS censorship", "categories": ["cs.NI"], "comment": "19 pages, 6 tables", "summary": "Internet censorship impacts large segments of the Internet, but so far, prior\nwork has focused almost exclusively on performing measurements using IPv4. As\nthe Internet grows, and more users connect, IPv6 is increasingly supported and\navailable to users and servers alike. But despite this steady growth, it\nremains unclear if the information control systems that implement censorship\n(firewalls, deep packet inspection, DNS injection, etc) are as effective with\nIPv6 traffic as they are with IPv4. In this paper, we perform the first global\nmeasurement of DNS censorship on the IPv6 Internet. Leveraging a recent\ntechnique that allows us to discover IPv6-capable open resolvers (along with\ntheir corresponding IPv4 address), we send over 20 million A and AAAA DNS\nrequests to DNS resolvers worldwide, and measure the rate at which they block,\nat the resolver, network, and country level as well examine the characteristics\nof blocked domains. We observe that while nearly all censors support blocking\nIPv6, their policies are inconsistent with and frequently less effective than\ntheir IPv4 censorship infrastructure. Our results suggest that supporting IPv6\ncensorship is not all-or-nothing: many censors support it, but poorly. As a\nresult, these censors may have to expend additional resources to bring IPv6\ncensorship up to parity with IPv4. In the meantime, this affords censorship\ncircumvention researchers a new opportunity to exploit these differences to\nevade detection and blocking.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9IPv6\u4e92\u8054\u7f51\u4e0a\u7684DNS\u5ba1\u67e5\u8fdb\u884c\u4e86\u5168\u7403\u6d4b\u91cf\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5927\u591a\u6570\u5ba1\u67e5\u673a\u6784\u652f\u6301IPv6\u5ba1\u67e5\uff0c\u4f46\u5176\u6548\u679c\u4e0d\u4e00\u81f4\u4e14\u5e38\u4e0d\u5982IPv4\u6709\u6548\u3002", "motivation": "\u7814\u7a76IPv6\u6d41\u91cf\u4e0b\u7684\u4fe1\u606f\u63a7\u5236\u7cfb\u7edf\uff08\u5982\u9632\u706b\u5899\u3001\u6df1\u5ea6\u5305\u68c0\u6d4b\u7b49\uff09\u662f\u5426\u4e0eIPv4\u4e00\u6837\u6709\u6548\uff0c\u586b\u8865\u6b64\u524d\u4e3b\u8981\u96c6\u4e2d\u5728IPv4\u6d4b\u91cf\u4e0a\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5229\u7528\u65b0\u6280\u672f\u53d1\u73b0\u652f\u6301IPv6\u7684\u5f00\u653e\u89e3\u6790\u5668\uff0c\u5e76\u5411\u5168\u7403DNS\u89e3\u6790\u5668\u53d1\u9001\u8d85\u8fc72000\u4e07\u6b21A\u548cAAAA\u8bf7\u6c42\uff0c\u6d4b\u91cf\u5176\u5728\u89e3\u6790\u5668\u3001\u7f51\u7edc\u548c\u56fd\u5bb6\u5c42\u9762\u7684\u5c4f\u853d\u7387\u53ca\u88ab\u5c4f\u853d\u57df\u540d\u7684\u7279\u5f81\u3002", "result": "\u51e0\u4e4e\u6240\u6709\u5ba1\u67e5\u673a\u6784\u90fd\u652f\u6301IPv6\u5c4f\u853d\uff0c\u4f46\u7b56\u7565\u4e0d\u4e00\u81f4\u4e14\u6548\u679c\u5e38\u4e0d\u5982IPv4\u3002", "conclusion": "IPv6\u5ba1\u67e5\u5e76\u975e\u5168\u6709\u6216\u5168\u65e0\uff0c\u8bb8\u591a\u5ba1\u67e5\u673a\u6784\u7684IPv6\u5ba1\u67e5\u6548\u679c\u4e0d\u4f73\uff0c\u8fd9\u4e3a\u89c4\u907f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2508.07057", "pdf": "https://arxiv.org/pdf/2508.07057", "abs": "https://arxiv.org/abs/2508.07057", "authors": ["Syed Ibrahim Mustafa Shah Bukhari", "Maha Sajid", "Bo Ji", "Brendan David-John"], "title": "Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders", "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": null, "summary": "As Extended Reality (XR) devices become increasingly prevalent in everyday\nsettings, they raise significant privacy concerns for bystanders: individuals\nin the vicinity of an XR device during its use, whom the device sensors may\naccidentally capture. Current privacy indicators, such as small LEDs, often\npresume that bystanders are attentive enough to interpret the privacy signals.\nHowever, these cues can be easily overlooked when bystanders are distracted or\nhave limited vision. We define such individuals as situationally impaired\nbystanders. This study explores XR privacy indicator designs that are effective\nfor situationally impaired bystanders. A focus group with eight participants\nwas conducted to design five novel privacy indicators. We evaluated these\ndesigns through a user study with seven additional participants. Our results\nshow that visual-only indicators, typical in commercial XR devices, received\nlow ratings for perceived usefulness in impairment scenarios. In contrast,\nmultimodal indicators were preferred in privacy-sensitive scenarios with\nsituationally impaired bystanders. Ultimately, our results highlight the need\nto move toward adaptable, multimodal, and situationally aware designs that\neffectively support bystander privacy in everyday XR environments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9488\u5bf9XR\u8bbe\u5907\u60c5\u5883\u6027\u53d7\u635f\u65c1\u89c2\u8005\u7684\u9690\u79c1\u6307\u793a\u5668\u8bbe\u8ba1\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u6307\u793a\u5668\u6bd4\u89c6\u89c9\u6307\u793a\u5668\u66f4\u6709\u6548\u3002", "motivation": "\u968f\u7740XR\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u65c1\u89c2\u8005\u7684\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u73b0\u6709\u89c6\u89c9\u9690\u79c1\u6307\u793a\u5668\u5728\u60c5\u5883\u6027\u53d7\u635f\u65c1\u89c2\u8005\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u7126\u70b9\u5c0f\u7ec4\u8bbe\u8ba1\u4e94\u79cd\u65b0\u578b\u9690\u79c1\u6307\u793a\u5668\uff0c\u5e76\u5bf9\u4e03\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u3002", "result": "\u89c6\u89c9\u6307\u793a\u5668\u5728\u60c5\u5883\u6027\u53d7\u635f\u573a\u666f\u4e2d\u8bc4\u4ef7\u8f83\u4f4e\uff0c\u591a\u6a21\u6001\u6307\u793a\u5668\u66f4\u53d7\u9752\u7750\u3002", "conclusion": "\u9700\u91c7\u7528\u9002\u5e94\u6027\u3001\u591a\u6a21\u6001\u7684\u60c5\u5883\u611f\u77e5\u8bbe\u8ba1\u4ee5\u63d0\u5347XR\u73af\u5883\u4e2d\u65c1\u89c2\u8005\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2508.07371", "pdf": "https://arxiv.org/pdf/2508.07371", "abs": "https://arxiv.org/abs/2508.07371", "authors": ["Yi Zhong", "Hongchao Liu", "Di ZHao"], "title": "AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation", "categories": ["cs.SE", "cs.AI"], "comment": "16pages,6figures", "summary": "As the complexity of software systems continues to increase, the demand for\nautomated testing and maintenance tools is growing exponentially. To meet this\nurgent need, we propose a new assertion generation method based on Hardware\nDescription Language (HDL). This method combines a lightweight,\nparameter-adjustable large language model (LLM) with the Unsloth platform to\nautomatically generate test cases, thereby significantly reducing training\ncosts without sacrificing accuracy or generalization performance. Empirical\nevaluation shows that our method can efficiently generate assertions that\nstrictly conform to the hardware logic. This framework provides a robust and\nflexible solution to modern software testing and maintenance challenges.\nhttps://github.com/liusu-orange/AutoAssert-1 and\nhttps://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08HDL\uff09\u7684\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u53c2\u6570\u53ef\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0eUnsloth\u5e73\u53f0\uff0c\u9ad8\u6548\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\uff0c\u5bf9\u81ea\u52a8\u5316\u6d4b\u8bd5\u4e0e\u7ef4\u62a4\u5de5\u5177\u7684\u9700\u6c42\u6025\u5267\u4e0a\u5347\u3002", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u53c2\u6570\u53ef\u8c03\u7684LLM\u4e0eUnsloth\u5e73\u53f0\uff0c\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u4e25\u683c\u7b26\u5408\u786c\u4ef6\u903b\u8f91\u7684\u65ad\u8a00\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u4ee3\u8f6f\u4ef6\u6d4b\u8bd5\u4e0e\u7ef4\u62a4\u63d0\u4f9b\u4e86\u7a33\u5065\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06778", "pdf": "https://arxiv.org/pdf/2508.06778", "abs": "https://arxiv.org/abs/2508.06778", "authors": ["Pyeonghwa Kim", "Steve Sawyer", "Michael Dunn"], "title": "Gender and Careers in Platform-Mediated Work: A Longitudinal Study of Online Freelancers", "categories": ["cs.HC"], "comment": "Accepted to CSCW 2025", "summary": "We advance gender-inclusive research within the CSCW field by investigating\nthe long-term gendered experiences of online freelancers on digital labor\nplatforms. The prevalence of gender-based inequalities has attracted\nsignificant attention within the CSCW community. Yet, insights remain limited\non how these inequalities shape workers' long-term experiences on digital labor\nplatforms. Through a five-year longitudinal study of 105 freelancers on Upwork,\nwe reveal persistent gender disparities that influence workers' long-term work\nand career trajectories, raising concerns about the sustainability of\nplatform-mediated work. We advance the ongoing dialogue on gender inclusivity\nin the community by introducing the concepts of career disempowerment and\nplatform-mediated motherhood penalty and by offering research and design\nimplications for CSCW to foster more sustainable, equitable platform work\nenvironments for all genders.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e00\u9879\u4e3a\u671f\u4e94\u5e74\u7684\u7eb5\u5411\u8c03\u67e5\uff0c\u63ed\u793a\u4e86\u6570\u5b57\u52b3\u52a8\u5e73\u53f0\u4e0a\u7684\u6027\u522b\u4e0d\u5e73\u7b49\u5bf9\u81ea\u7531\u804c\u4e1a\u8005\u957f\u671f\u804c\u4e1a\u53d1\u5c55\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u6570\u5b57\u52b3\u52a8\u5e73\u53f0\u4e2d\u6027\u522b\u4e0d\u5e73\u7b49\u5bf9\u81ea\u7531\u804c\u4e1a\u8005\u957f\u671f\u804c\u4e1a\u53d1\u5c55\u7684\u5f71\u54cd\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u5bf9105\u540dUpwork\u81ea\u7531\u804c\u4e1a\u8005\u8fdb\u884c\u4e86\u4e94\u5e74\u7684\u7eb5\u5411\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6027\u522b\u5dee\u5f02\u4f1a\u957f\u671f\u5f71\u54cd\u81ea\u7531\u804c\u4e1a\u8005\u7684\u804c\u4e1a\u8f68\u8ff9\uff0c\u5e76\u63d0\u51fa\u4e86\u2018\u804c\u4e1a\u8d4b\u6743\u7f3a\u5931\u2019\u548c\u2018\u5e73\u53f0\u4e2d\u4ecb\u7684\u6bcd\u804c\u60e9\u7f5a\u2019\u7b49\u65b0\u6982\u5ff5\u3002", "conclusion": "\u7814\u7a76\u547c\u5401CSCW\u793e\u533a\u901a\u8fc7\u8bbe\u8ba1\u4e0e\u7814\u7a76\u6539\u8fdb\uff0c\u4e3a\u6240\u6709\u6027\u522b\u521b\u9020\u66f4\u53ef\u6301\u7eed\u3001\u516c\u5e73\u7684\u5e73\u53f0\u5de5\u4f5c\u73af\u5883\u3002"}}
{"id": "2508.07992", "pdf": "https://arxiv.org/pdf/2508.07992", "abs": "https://arxiv.org/abs/2508.07992", "authors": ["Haisong Gong", "Bolan Su", "Xinrong Zhang", "Jing Li", "Qiang Liu", "Shu Wu", "Liang Wang"], "title": "Mining the Social Fabric: Unveiling Communities for Fake News Detection in Short Videos", "categories": ["cs.MM"], "comment": null, "summary": "Short video platforms have become a major medium for information sharing, but\ntheir rapid content generation and algorithmic amplification also enable the\nwidespread dissemination of fake news. Detecting misinformation in short videos\nis challenging due to their multi-modal nature and the limited context of\nindividual videos. While recent methods focus on analyzing content\nsignals-visual, textual, and audio-they often overlook implicit relationships\namong videos, uploaders, and events. To address this gap, we propose DugFND\n(Dual-community graph for fake news detection), a novel method that enhances\nexisting video classifiers by modeling two key community patterns: (1) uploader\ncommunities, where uploaders with shared interests or similar content creation\npatterns group together, and (2) event-driven communities, where videos related\nto the same or semantically similar public events form localized clusters. We\nconstruct a heterogeneous graph connecting uploader, video, and event nodes,\nand design a time-aware heterogeneous graph attention network to enable\neffective message passing. A reconstruction-based pretraining phase further\nimproves node representation learning. DugFND can be applied to any pre-trained\nclassifier. Experiments on public datasets show that our method achieves\nsignificant performance gains, demonstrating the value of dual-community\nmodeling for fake news detection in short videos.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDugFND\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4e0a\u4f20\u8005\u793e\u533a\u548c\u4e8b\u4ef6\u9a71\u52a8\u793e\u533a\u7684\u53cc\u91cd\u6a21\u5f0f\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77ed\u89c6\u9891\u4e2d\u865a\u5047\u65b0\u95fb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u56e0\u5185\u5bb9\u751f\u6210\u5feb\u548c\u7b97\u6cd5\u653e\u5927\u6548\u5e94\u5bfc\u81f4\u865a\u5047\u65b0\u95fb\u6cdb\u6ee5\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u9891\u3001\u4e0a\u4f20\u8005\u548c\u4e8b\u4ef6\u4e4b\u95f4\u7684\u9690\u542b\u5173\u7cfb\u3002", "method": "\u63d0\u51faDugFND\u65b9\u6cd5\uff0c\u6784\u5efa\u4e0a\u4f20\u8005\u3001\u89c6\u9891\u548c\u4e8b\u4ef6\u7684\u5f02\u6784\u56fe\uff0c\u8bbe\u8ba1\u65f6\u95f4\u611f\u77e5\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u5e76\u91c7\u7528\u91cd\u6784\u9884\u8bad\u7ec3\u4f18\u5316\u8282\u70b9\u8868\u793a\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDugFND\u663e\u8457\u63d0\u5347\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u53cc\u91cd\u793e\u533a\u5efa\u6a21\u5bf9\u77ed\u89c6\u9891\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.06768", "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "categories": ["cs.CV", "cs.GR"], "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous\nsurgical procedures, but its interpretation is complicated by noise, artifacts,\nand poor alignment with high-resolution preoperative MRI/CT scans. To bridge\nthe gap between reoperative planning and intraoperative guidance, we present\nDiffUS, a physics-based, differentiable ultrasound renderer that synthesizes\nrealistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D\nscans into acoustic impedance volumes using a machine learning approach. Next,\nwe simulate ultrasound beam propagation using ray tracing with coupled\nreflection-transmission equations. DiffUS formulates wave propagation as a\nsparse linear system that captures multiple internal reflections. Finally, we\nreconstruct B-mode images via depth-resolved echo extraction across fan-shaped\nacquisition geometry, incorporating realistic artifacts including speckle noise\nand depth-dependent degradation. DiffUS is entirely implemented as\ndifferentiable tensor operations in PyTorch, enabling gradient-based\noptimization for downstream applications such as slice-to-volume registration\nand volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates\nDiffUS's ability to generate anatomically accurate ultrasound images from brain\nMRI data.", "AI": {"tldr": "DiffUS\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u8d85\u58f0\u6e32\u67d3\u5668\uff0c\u80fd\u4eceMRI 3D\u626b\u63cf\u751f\u6210\u903c\u771f\u7684B\u6a21\u5f0f\u8d85\u58f0\u56fe\u50cf\uff0c\u7528\u4e8e\u89e3\u51b3\u672f\u4e2d\u8d85\u58f0\u56fe\u50cf\u4e0e\u672f\u524dMRI/CT\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u672f\u4e2d\u8d85\u58f0\u6210\u50cf\u56e0\u566a\u58f0\u3001\u4f2a\u5f71\u53ca\u4e0e\u672f\u524d\u9ad8\u5206\u8fa8\u7387MRI/CT\u5bf9\u9f50\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002DiffUS\u65e8\u5728\u586b\u8865\u672f\u524d\u8ba1\u5212\u4e0e\u672f\u4e2d\u5f15\u5bfc\u95f4\u7684\u5dee\u8ddd\u3002", "method": "DiffUS\u9996\u5148\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5c06MRI 3D\u626b\u63cf\u8f6c\u6362\u4e3a\u58f0\u963b\u6297\u4f53\u79ef\uff0c\u518d\u7528\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u8d85\u58f0\u675f\u4f20\u64ad\uff0c\u7ed3\u5408\u53cd\u5c04-\u900f\u5c04\u65b9\u7a0b\uff0c\u6700\u540e\u901a\u8fc7\u6df1\u5ea6\u89e3\u6790\u56de\u6ce2\u63d0\u53d6\u91cd\u5efaB\u6a21\u5f0f\u56fe\u50cf\u3002", "result": "\u5728ReMIND\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDiffUS\u80fd\u4ece\u8111\u90e8MRI\u6570\u636e\u751f\u6210\u89e3\u5256\u51c6\u786e\u7684\u8d85\u58f0\u56fe\u50cf\u3002", "conclusion": "DiffUS\u4f5c\u4e3a\u4e00\u79cd\u53ef\u5fae\u5206\u8d85\u58f0\u6e32\u67d3\u5668\uff0c\u4e3a\u672f\u4e2d\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u5e76\u53ef\u652f\u6301\u5207\u7247-\u4f53\u79ef\u914d\u51c6\u7b49\u4e0b\u6e38\u4f18\u5316\u4efb\u52a1\u3002"}}
{"id": "2508.06716", "pdf": "https://arxiv.org/pdf/2508.06716", "abs": "https://arxiv.org/abs/2508.06716", "authors": ["Blair Johnson", "Clayton Kerce", "Faramarz Fekri"], "title": "GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Differentiable inductive logic programming (ILP) techniques have proven\neffective at finding approximate rule-based solutions to link prediction and\nnode classification problems on knowledge graphs; however, the common\nassumption of chain-like rule structure can hamper the performance and\ninterpretability of existing approaches. We introduce GLIDR, a differentiable\nrule learning method that models the inference of logic rules with more\nexpressive syntax than previous methods. GLIDR uses a differentiable message\npassing inference algorithm that generalizes previous chain-like rule learning\nmethods to allow rules with features like branches and cycles. GLIDR has a\nsimple and expressive rule search space which is parameterized by a limit on\nthe maximum number of free variables that may be included in a rule. Explicit\nlogic rules can be extracted from the weights of a GLIDR model for use with\nsymbolic solvers. We demonstrate that GLIDR can significantly outperform\nexisting rule learning methods on knowledge graph completion tasks and even\ncompete with embedding methods despite the inherent disadvantage of being a\nstructure-only prediction method. We show that rules extracted from GLIDR\nretain significant predictive performance, and that GLIDR is highly robust to\ntraining data noise. Finally, we demonstrate that GLIDR can be chained with\ndeep neural networks and optimized end-to-end for rule learning on arbitrary\ndata modalities.", "AI": {"tldr": "GLIDR\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u7075\u6d3b\u7684\u8bed\u6cd5\u548c\u63a8\u7406\u7b97\u6cd5\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u5fae\u5206\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b(ILP)\u65b9\u6cd5\u5728\u94fe\u5f0f\u89c4\u5219\u7ed3\u6784\u5047\u8bbe\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u5f71\u54cd\u4e86\u7ed3\u679c\u7684\u8868\u73b0\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "GLIDR\u5f15\u5165\u4e86\u652f\u6301\u5206\u652f\u548c\u5faa\u73af\u7684\u7075\u6d3b\u89c4\u5219\u8bed\u6cd5\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u6d88\u606f\u4f20\u9012\u63a8\u7406\u7b97\u6cd5\uff0c\u5e76\u80fd\u4ece\u6a21\u578b\u6743\u91cd\u4e2d\u63d0\u53d6\u663e\u5f0f\u903b\u8f91\u89c4\u5219\u3002", "result": "\u5728\u77e5\u8bc6\u56fe\u8c31\u5b8c\u6210\u4efb\u52a1\u4e0a\uff0cGLIDR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u53ef\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ade\u4e89\uff0c\u4e14\u5bf9\u8bad\u7ec3\u6570\u636e\u566a\u58f0\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "GLIDR\u4e0d\u4ec5\u63d0\u5347\u4e86\u89c4\u5219\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.07551", "pdf": "https://arxiv.org/pdf/2508.07551", "abs": "https://arxiv.org/abs/2508.07551", "authors": ["Danushka Liyanage", "Shubham Pandey", "Joshua Goldstein", "Michael Cahill", "Akon Dey", "Alan Fekete", "Uwe R\u00f6hm"], "title": "A Benchmark for Databases with Varying Value Lengths", "categories": ["cs.DB", "E.2"], "comment": "Seventeenth TPC Technology Conference on Performance Evaluation &\n  Benchmarking (TPCTC 2025) Keywords: Key-value stores, Benchmarking,\n  Throughput, Latency", "summary": "The performance of database management systems (DBMS) is traditionally\nevaluated using benchmarks that focus on workloads with (almost) fixed record\nlengths. However, some real-world workloads in key/value stores, document\ndatabases, and graph databases exhibit significant variability in value\nlengths, which can lead to performance anomalies, particularly when popular\nrecords grow disproportionately large. Existing benchmarks fail to account for\nthis variability, leaving an important aspect of DBMS behavior underexplored.\n  In this paper, we address this gap by extending the Yahoo! Cloud Serving\nBenchmark (YCSB) to include an \"extend\" operation, which appends data to record\nfields, simulating the growth of values over time. Using this modified\nbenchmark, we have measured the performance of three popular DBMS backends:\nMongoDB, MariaDB with the InnoDB storage engine, and MariaDB with the MyRocks\nstorage engine. Our experiments alternate between extending values and\nexecuting query workloads, revealing significant performance differences driven\nby storage engine design and their handling of variable-sized values.\n  Our key contribution is the introduction of a novel benchmarking approach to\nevaluate the impact of growing value sizes and isolate the effect of querying\ndata with a distribution of data sizes from any cost associated with accessing\ndata after a history of updates. This highlights the need for more\nrepresentative benchmarks that capture the dynamic nature of real-world\nworkloads, providing valuable guidance for both practitioners and researchers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55YCSB\u57fa\u51c6\u6d4b\u8bd5\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0'extend'\u64cd\u4f5c\u6765\u6a21\u62df\u73b0\u5b9e\u4e2d\u6570\u636e\u8bb0\u5f55\u52a8\u6001\u589e\u957f\u7684\u60c5\u51b5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u5728\u5904\u7406\u53d8\u957f\u6570\u636e\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u5e93\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u4e2d\u6570\u636e\u8bb0\u5f55\u957f\u5ea6\u52a8\u6001\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5bfc\u81f4\u6027\u80fd\u5f02\u5e38\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u6269\u5c55YCSB\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u5305\u542b'extend'\u64cd\u4f5c\uff0c\u6a21\u62df\u6570\u636e\u8bb0\u5f55\u7684\u589e\u957f\uff0c\u5e76\u5728MongoDB\u548cMariaDB\uff08InnoDB\u548cMyRocks\u5b58\u50a8\u5f15\u64ce\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5b58\u50a8\u5f15\u64ce\u8bbe\u8ba1\u53ca\u5176\u5bf9\u53d8\u957f\u6570\u636e\u5904\u7406\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u66f4\u5177\u4ee3\u8868\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3a\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u51f8\u663e\u4e86\u6355\u6349\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u9700\u6c42\u3002"}}
{"id": "2508.07725", "pdf": "https://arxiv.org/pdf/2508.07725", "abs": "https://arxiv.org/abs/2508.07725", "authors": ["Andreas Hager-Clukas", "Philipp van Kempen", "Stefan Wallentowitz"], "title": "ARISE: Automating RISC-V Instruction Set Extension", "categories": ["cs.AR"], "comment": null, "summary": "RISC-V is an extendable Instruction Set Architecture, growing in popularity\nfor embedded systems. However, optimizing it to specific requirements, imposes\na great deal of manual effort. To bridge the gap between software and ISA, the\ntool ARISE is presented. It automates the generation of RISC-V instructions\nbased on assembly patterns, which are selected by an extendable set of metrics.\nThese metrics implement the optimization goals of code size and instruction\ncount reduction, both statically and dynamically. The instruction set\nextensions are generated using the ISA description language CoreDSL. Allowing\nseamless embedding in advanced tools such as the retargeting compiler Seal5 or\nthe instruction set simulator ETISS. ARISE improves the static code size by\n1.48% and the dynamic code size by 3.84%, as well as the number of instructions\nto be executed by 7.39% on average for Embench-Iot.", "AI": {"tldr": "ARISE\u5de5\u5177\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u57fa\u4e8e\u6c47\u7f16\u6a21\u5f0f\u7684RISC-V\u6307\u4ee4\uff0c\u4f18\u5316\u4ee3\u7801\u5927\u5c0f\u548c\u6307\u4ee4\u6570\uff0c\u63d0\u5347\u5d4c\u5165\u5f0f\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4e3a\u51cf\u5c11RISC-V\u67b6\u6784\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u624b\u52a8\u4f18\u5316\u7684\u7e41\u7410\u5de5\u4f5c\uff0c\u5f00\u53d1\u4e86ARISE\u5de5\u5177\uff0c\u4ee5\u81ea\u52a8\u5316\u65b9\u5f0f\u4f18\u5316\u6307\u4ee4\u96c6\u3002", "method": "\u5229\u7528CoreDSL\u8bed\u8a00\u751f\u6210\u6307\u4ee4\u96c6\u6269\u5c55\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u6307\u6807\u9009\u62e9\u6c47\u7f16\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9759\u6001\u548c\u52a8\u6001\u7684\u4ee3\u7801\u4f18\u5316\u3002", "result": "\u5728Embench-Iot\u6d4b\u8bd5\u4e2d\uff0c\u9759\u6001\u4ee3\u7801\u5927\u5c0f\u51cf\u5c111.48%\uff0c\u52a8\u6001\u4ee3\u7801\u5927\u5c0f\u51cf\u5c113.84%\uff0c\u6307\u4ee4\u6267\u884c\u6570\u51cf\u5c117.39%\u3002", "conclusion": "ARISE\u6210\u529f\u81ea\u52a8\u5316\u4e86RISC-V\u6307\u4ee4\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.07317", "pdf": "https://arxiv.org/pdf/2508.07317", "abs": "https://arxiv.org/abs/2508.07317", "authors": ["Pedro Carrinho", "Hamid Moghadaspour", "Oscar Ferraz", "Jo\u00e3o Dinis Ferreira", "Yann Falevoz", "Vitor Silva", "Gabriel Falcao"], "title": "An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons", "categories": ["cs.DC", "eess.SP"], "comment": "19 pages, 1 figures, and 2 tables", "summary": "In modern computer architectures, the performance of many memory-bound\nworkloads (e.g., machine learning, graph processing, databases) is limited by\nthe data movement bottleneck that emerges when transferring large amounts of\ndata between the main memory and the central processing unit (CPU).\nProcessing-in-memory is an emerging computing paradigm that aims to alleviate\nthis data movement bottleneck by performing computation close to or within the\nmemory units, where data resides. One example of a prevalent workload whose\nperformance is bound by the data movement bottleneck is the training and\ninference process of artificial neural networks. In this work, we analyze the\npotential of modern general-purpose PiM architectures to accelerate neural\nnetworks. To this end, we selected the UPMEM PiM system, the first commercially\navailable real-world general-purpose PiM architecture. We compared the\nimplementation of multilayer perceptrons (MLPs) in PiM with a sequential\nbaseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to\n$259\\times$ better performance for inference of large batch sizes when compared\nagainst the CPU that exploits the size of the available PiM memory.\nAdditionally, two smaller MLPs were implemented using UPMEM's working SRAM\n(WRAM), a scratchpad memory, to evaluate their performance against a low-power\nNvidia Jetson graphics processing unit (GPU), providing further insights into\nthe efficiency of UPMEM's PiM for neural network inference. Results show that\nusing WRAM achieves kernel execution times for MLP inference of under $3$ ms,\nwhich is within the same order of magnitude as low-power GPUs.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u73b0\u4ee3\u901a\u7528PiM\u67b6\u6784\uff08\u5982UPMEM\uff09\u5728\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4e0eCPU\u548c\u4f4e\u529f\u8017GPU\u7684\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86PiM\u5728\u6570\u636e\u79fb\u52a8\u74f6\u9888\u4e2d\u7684\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u673a\u67b6\u6784\u4e2d\uff0c\u6570\u636e\u79fb\u52a8\u74f6\u9888\u9650\u5236\u4e86\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u6027\u80fd\u3002\u5904\u7406\u5185\u5b58\uff08PiM\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u5c31\u8fd1\u6216\u5728\u5185\u5b58\u5355\u5143\u4e2d\u6267\u884c\u8ba1\u7b97\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9009\u62e9\u4e86UPMEM PiM\u7cfb\u7edf\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61\uff0c\u5b9e\u73b0\u4e86\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u7684PiM\u7248\u672c\uff0c\u5e76\u4e0eIntel Xeon CPU\u548c\u4f4e\u529f\u8017Nvidia Jetson GPU\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "UPMEM PiM\u5728\u5927\u578b\u6279\u91cf\u63a8\u7406\u4e2d\u6bd4CPU\u5feb259\u500d\uff1b\u4f7f\u7528UPMEM\u7684WRAM\u65f6\uff0cMLP\u63a8\u7406\u7684\u6838\u6267\u884c\u65f6\u95f4\u4f4e\u4e8e3\u6beb\u79d2\uff0c\u4e0e\u4f4e\u529f\u8017GPU\u5904\u4e8e\u540c\u4e00\u6570\u91cf\u7ea7\u3002", "conclusion": "PiM\u67b6\u6784\u5728\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u79fb\u52a8\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.07394", "pdf": "https://arxiv.org/pdf/2508.07394", "abs": "https://arxiv.org/abs/2508.07394", "authors": ["Luca Lusvarghi", "Javier Gozalvez", "Baldomero Coll-Perales", "Mohammad Irfan Khan", "Miguel Sepulcre", "Seyhan Ucar", "Onur Altintas"], "title": "The Search for Relevance: A Context-Aware Paradigm Shift in Semantic and Task-Oriented V2X Communications", "categories": ["cs.NI"], "comment": null, "summary": "The design of communication systems has traditionally focused on the reliable\nand timely delivery of data. However, the scalability challenges faced by the\nevolution to a 6G-driven society demand new communication paradigms that\ncarefully curate the content being transmitted. This paper envisions a joint\nsemantic and task-oriented communication paradigm where Connected and\nAutonomous Vehicles (CAVs) transmit only the information necessary to convey\nthe desired meaning that is relevant to the intended receivers based on the\ncommunication context. The V2X domain offers a unique environment for the\ndevelopment of the envisioned semantic and task-oriented communications\nparadigm, as CAVs are native semantic devices, and the V2X domain is rich in\ncontextual information. This contextual information can be leveraged to\nestimate the relevance that information may have for the intended receivers. We\nillustrate and quantitatively evaluate the potential benefits of semantic and\ntask-oriented V2X communications. Numerical results show that by focusing on\nthe transmission of the most relevant information for the intended receivers,\nsemantic and task-oriented V2X communications can achieve a two-fold\nimprovement in communication efficiency, which will significantly benefit the\nscalability of V2X networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8bed\u4e49\u548c\u4efb\u52a1\u7684V2X\u901a\u4fe1\u8303\u5f0f\uff0c\u901a\u8fc7\u4f20\u8f93\u6700\u76f8\u5173\u4fe1\u606f\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u6ce8\u91cd\u6570\u636e\u7684\u53ef\u9760\u53ca\u65f6\u4f20\u8f93\uff0c\u4f466G\u65f6\u4ee3\u9700\u8981\u65b0\u8303\u5f0f\u4ee5\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8054\u5408\u8bed\u4e49\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u901a\u4fe1\u6a21\u5f0f\uff0c\u5229\u7528V2X\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u8bc4\u4f30\u4fe1\u606f\u76f8\u5173\u6027\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u63d0\u5347\u901a\u4fe1\u6548\u7387\u4e24\u500d\uff0c\u663e\u8457\u589e\u5f3aV2X\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8bed\u4e49\u548c\u4efb\u52a1\u5bfc\u5411\u7684V2X\u901a\u4fe1\u80fd\u6709\u6548\u63d0\u5347\u6548\u7387\uff0c\u9002\u7528\u4e8e6G\u65f6\u4ee3\u7684\u9700\u6c42\u3002"}}
{"id": "2508.07190", "pdf": "https://arxiv.org/pdf/2508.07190", "abs": "https://arxiv.org/abs/2508.07190", "authors": ["Minfeng Qi", "Qin Wang", "Guangsheng Yu", "Ruiqiang Li", "Victor Zhou", "Shiping Chen"], "title": "Understanding NFTs from EIP Standards", "categories": ["cs.CR", "cs.ET"], "comment": null, "summary": "We argue that the technical foundations of non-fungible tokens (NFTs) remain\ninadequately understood. Prior research has focused on market dynamics, user\nbehavior, and isolated security incidents, yet systematic analysis of the\nstandards underpinning NFT functionality is largely absent.\n  We present the first study of NFTs through the lens of Ethereum Improvement\nProposals (EIPs). We conduct a large-scale empirical analysis of 191\nNFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July, 2025). We\nintegrate multi-dimensional analyses including the automated parsing of\nSolidity interfaces, graph-based modeling of inheritance structures,\ncontributor profiling, and mining of community discussion data. We distinguish\nfoundational from emerging standards, expose poor cross-version\ninteroperability, and show that growing functional complexity heightens\nsecurity risks.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u901a\u8fc7\u4ee5\u592a\u574a\u6539\u8fdb\u63d0\u6848\uff08EIPs\uff09\u7684\u7cfb\u7edf\u89c6\u89d2\u7814\u7a76NFT\u6280\u672f\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u8bbe\u8ba1\u7684\u4e0d\u8db3\u548c\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u5bf9NFT\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5e02\u573a\u52a8\u6001\u548c\u7528\u6237\u884c\u4e3a\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5e95\u5c42\u6280\u672f\u6807\u51c6\uff08\u5982EIPs\uff09\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec191\u4e2aNFT\u76f8\u5173EIPs\u548c1\u4e07+\u793e\u533a\u8ba8\u8bba\u6570\u636e\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u63a5\u53e3\u89e3\u6790\u3001\u56fe\u6a21\u578b\u7ee7\u627f\u7ed3\u6784\u5206\u6790\u7b49\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u8de8\u7248\u672c\u4e92\u64cd\u4f5c\u6027\u5dee\u3001\u529f\u80fd\u590d\u6742\u6027\u52a0\u5267\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u533a\u5206\u4e86\u57fa\u7840\u4e0e\u65b0\u5174\u6807\u51c6\u3002", "conclusion": "NFT\u6280\u672f\u6807\u51c6\u4e9f\u9700\u7cfb\u7edf\u6027\u4f18\u5316\u4ee5\u907f\u514d\u5b89\u5168\u6f0f\u6d1e\u548c\u529f\u80fd\u6df7\u4e71\u3002"}}
{"id": "2508.07486", "pdf": "https://arxiv.org/pdf/2508.07486", "abs": "https://arxiv.org/abs/2508.07486", "authors": ["Morteza Ziabakhsh", "Kiyan Rezaee", "Sadegh Eskandari", "Seyed Amir Hossein Tabatabaei", "Mohammad M. Ghassemi"], "title": "Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering", "categories": ["cs.SE", "cs.AI", "cs.CV"], "comment": null, "summary": "Modern software systems are increasingly shifting from monolithic\narchitectures to microservices to enhance scalability, maintainability, and\ndeployment flexibility. Existing microservice extraction methods typically rely\non hard clustering, assigning each software component to a single microservice.\nThis approach often increases inter-service coupling and reduces intra-service\ncohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a\nframework that formulates microservice extraction as a soft clustering problem,\nallowing components to belong probabilistically to multiple microservices. This\napproach is inspired by expert-driven decompositions, where practitioners\nintentionally replicate certain software components across services to reduce\ncommunication overhead. Mo2oM combines deep semantic embeddings with structural\ndependencies extracted from methodcall graphs to capture both functional and\narchitectural relationships. A graph neural network-based soft clustering\nalgorithm then generates the final set of microservices. We evaluate Mo2oM on\nfour open-source monolithic benchmarks and compare it against eight\nstate-of-the-art baselines. Our results demonstrate that Mo2oM achieves\nimprovements of up to 40.97% in structural modularity (balancing cohesion and\ncoupling), 58% in inter-service call percentage (communication overhead),\n26.16% in interface number (modularity and decoupling), and 38.96% in\nnon-extreme distribution (service size balance) across all benchmarks.", "AI": {"tldr": "Mo2oM\u662f\u4e00\u79cd\u5c06\u5355\u4f53\u67b6\u6784\u8f6c\u6362\u4e3a\u5fae\u670d\u52a1\u7684\u8f6f\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5141\u8bb8\u7ec4\u4ef6\u6982\u7387\u6027\u5c5e\u4e8e\u591a\u4e2a\u5fae\u670d\u52a1\uff0c\u663e\u8457\u63d0\u5347\u7ed3\u6784\u6a21\u5757\u5316\u548c\u670d\u52a1\u95f4\u8c03\u7528\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u786c\u805a\u7c7b\u65b9\u6cd5\u5728\u5fae\u670d\u52a1\u63d0\u53d6\u4e2d\u589e\u52a0\u4e86\u670d\u52a1\u95f4\u8026\u5408\uff0c\u964d\u4f4e\u4e86\u670d\u52a1\u5185\u5185\u805a\u3002Mo2oM\u65e8\u5728\u901a\u8fc7\u8f6f\u805a\u7c7b\u548c\u7ec4\u4ef6\u590d\u5236\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u5d4c\u5165\u548c\u65b9\u6cd5\u8c03\u7528\u56fe\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8f6f\u805a\u7c7b\u751f\u6210\u5fae\u670d\u52a1\u3002", "result": "\u5728\u56db\u79cd\u5f00\u6e90\u5355\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMo2oM\u5728\u7ed3\u6784\u6a21\u5757\u5316\u3001\u901a\u4fe1\u5f00\u9500\u7b49\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Mo2oM\u4e3a\u5fae\u670d\u52a1\u63d0\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u51cf\u5c11\u670d\u52a1\u95f4\u901a\u4fe1\u7684\u573a\u666f\u3002"}}
{"id": "2508.06786", "pdf": "https://arxiv.org/pdf/2508.06786", "abs": "https://arxiv.org/abs/2508.06786", "authors": ["Amy Rae Fox", "Michelle Morgenstern", "Graham M. Jones", "Arvind Satyanarayan"], "title": "Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "What impressions might readers form with visualizations that go beyond the\ndata they encode? In this paper, we build on recent work that demonstrates the\nsocio-indexical function of visualization, showing that visualizations\ncommunicate more than the data they explicitly encode. Bridging this with prior\nwork examining public discourse about visualizations, we contribute an analytic\nframework for describing inferences about an artifact's social provenance. Via\na series of attribution-elicitation surveys, we offer descriptive evidence that\nthese social inferences: (1) can be studied asynchronously, (2) are not unique\nto a particular sociocultural group or a function of limited data literacy, and\n(3) may influence assessments of trust. Further, we demonstrate (4) how design\nfeatures act in concert with the topic and underlying messages of an artifact's\ndata to give rise to such 'beyond-data' readings. We conclude by discussing the\ndesign and research implications of inferences about social provenance, and why\nwe believe broadening the scope of research on human factors in visualization\nto include sociocultural phenomena can yield actionable design recommendations\nto address urgent challenges in public data communication.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u89c6\u5316\u5982\u4f55\u4f20\u8fbe\u8d85\u51fa\u5176\u663e\u5f0f\u7f16\u7801\u6570\u636e\u7684\u793e\u4f1a\u4fe1\u606f\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8c03\u67e5\u8bc1\u660e\u793e\u4f1a\u63a8\u65ad\u7684\u666e\u904d\u6027\u548c\u5bf9\u4fe1\u4efb\u8bc4\u4f30\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u53ef\u89c6\u5316\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u4f20\u8fbe\u793e\u4f1a\u4fe1\u606f\uff0c\u4ee5\u5e94\u5bf9\u516c\u5171\u6570\u636e\u4f20\u64ad\u4e2d\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5f52\u56e0-\u5f15\u53d1\u8c03\u67e5\u548c\u63cf\u8ff0\u6027\u8bc1\u636e\uff0c\u5206\u6790\u793e\u4f1a\u63a8\u65ad\u7684\u666e\u904d\u6027\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u793e\u4f1a\u63a8\u65ad\u53ef\u5f02\u6b65\u7814\u7a76\u3001\u4e0d\u9650\u4e8e\u7279\u5b9a\u7fa4\u4f53\u6216\u6570\u636e\u7d20\u517b\uff0c\u4e14\u8bbe\u8ba1\u7279\u5f81\u4e0e\u4e3b\u9898\u5171\u540c\u5f71\u54cd\u8fd9\u4e9b\u63a8\u65ad\u3002", "conclusion": "\u547c\u5401\u5728\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u7eb3\u5165\u793e\u4f1a\u6587\u5316\u56e0\u7d20\uff0c\u4ee5\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2508.08237", "pdf": "https://arxiv.org/pdf/2508.08237", "abs": "https://arxiv.org/abs/2508.08237", "authors": ["Daniil Zverev", "Thadd\u00e4us Wiedemer", "Ameya Prabhu", "Matthias Bethge", "Wieland Brendel", "A. Sophia Koepke"], "title": "VGGSounder: Audio-Visual Evaluations for Foundation Models", "categories": ["cs.MM", "cs.AI", "cs.SD"], "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision (ICCV) 2025", "summary": "The emergence of audio-visual foundation models underscores the importance of\nreliably assessing their multi-modal understanding. The VGGSounder dataset is\ncommonly used as a benchmark for evaluation audio-visual classification.\nHowever, our analysis identifies several limitations of VGGSounder, including\nincomplete labelling, partially overlapping classes, and misaligned modalities.\nThese lead to distorted evaluations of auditory and visual capabilities. To\naddress these limitations, we introduce VGGSounder, a comprehensively\nre-annotated, multi-label test set that extends VGGSound and is specifically\ndesigned to evaluate audio-visual foundation models. VGGSounder features\ndetailed modality annotations, enabling precise analyses of modality-specific\nperformance. Furthermore, we reveal model limitations by analysing performance\ndegradation when adding another input modality with our new modality confusion\nmetric.", "AI": {"tldr": "VGGSounder\u6570\u636e\u96c6\u5b58\u5728\u6807\u6ce8\u4e0d\u5168\u3001\u7c7b\u522b\u91cd\u53e0\u548c\u6a21\u6001\u4e0d\u5bf9\u9f50\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u8bc4\u4f30\u6548\u679c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u65b0\u6807\u6ce8\u7684\u591a\u6807\u7b7e\u6d4b\u8bd5\u96c6VGGSounder\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u97f3\u9891\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u73b0\u6709VGGSounder\u6570\u636e\u96c6\u5728\u8bc4\u4f30\u97f3\u9891\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u5931\u771f\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91cd\u65b0\u6807\u6ce8VGGSounder\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u6807\u7b7e\u6d4b\u8bd5\u96c6\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6a21\u6001\u6df7\u6dc6\u5ea6\u91cf\u6765\u5206\u6790\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "result": "VGGSounder\u63d0\u4f9b\u4e86\u66f4\u8be6\u7ec6\u7684\u6a21\u6001\u6807\u6ce8\uff0c\u80fd\u591f\u7cbe\u786e\u5206\u6790\u6a21\u578b\u5728\u7279\u5b9a\u6a21\u6001\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u5728\u591a\u6a21\u6001\u8f93\u5165\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u65b0VGGSounder\u6d4b\u8bd5\u96c6\u80fd\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u97f3\u9891\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u539f\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.06918", "pdf": "https://arxiv.org/pdf/2508.06918", "abs": "https://arxiv.org/abs/2508.06918", "authors": ["Stefano Fioravanti", "Michael Kompatscher", "Bernardo Rossi", "Albert Vucaj"], "title": "Mal'cev clones over a three-element set up to minor-equivalence", "categories": ["math.RA", "cs.LO", "03B50, 08A70, 08B05"], "comment": "33 pages, 4 figures; appendix contains a github link with additional\n  material", "summary": "We classify all Mal'cev clones over a three-element set up to minion\nhomomorphisms. This is another step toward the complete classification of\nthree-element relational structures up to pp-constructability. We furthermore\nprovide an alternative proof of Bulatov's result that all Mal'cev clones over a\nthree-element set have an at most 4-ary relational basis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u4e09\u5143\u7d20\u96c6\u5408\u4e0a\u7684\u6240\u6709Mal'cev\u514b\u9686\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7minion\u540c\u6001\u9a8c\u8bc1\uff0c\u8fdb\u4e00\u6b65\u63a8\u8fdb\u4e86\u4e09\u5143\u7d20\u5173\u7cfb\u7ed3\u6784\u7684\u5206\u7c7b\u7814\u7a76\u3002", "motivation": "\u76ee\u6807\u662f\u5b8c\u6210\u4e09\u5143\u7d20\u5173\u7cfb\u7ed3\u6784\u5728pp-\u53ef\u6784\u9020\u6027\u4e0b\u7684\u5b8c\u6574\u5206\u7c7b\uff0c\u5e76\u9a8c\u8bc1Bulatov\u5173\u4e8eMal'cev\u514b\u9686\u5173\u7cfb\u57fa\u7840\u7684\u7ed3\u679c\u3002", "method": "\u4f7f\u7528minion\u540c\u6001\u5bf9Mal'cev\u514b\u9686\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9bBulatov\u7ed3\u679c\u7684\u66ff\u4ee3\u8bc1\u660e\u65b9\u6cd5\u3002", "result": "\u6210\u529f\u5206\u7c7b\u4e86\u4e09\u5143\u7d20\u96c6\u5408\u4e0a\u7684\u6240\u6709Mal'cev\u514b\u9686\uff0c\u5e76\u786e\u8ba4\u5176\u5173\u7cfb\u57fa\u7840\u6700\u591a\u4e3a4\u5143\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e09\u5143\u7d20\u5173\u7cfb\u7ed3\u6784\u7684\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u8fdb\u5c55\uff0c\u5e76\u9a8c\u8bc1\u4e86Bulatov\u7684\u7406\u8bba\u3002"}}
{"id": "2508.07654", "pdf": "https://arxiv.org/pdf/2508.07654", "abs": "https://arxiv.org/abs/2508.07654", "authors": ["Fei Ye", "Jiapan Liu", "Yinan Jing", "Zhenying He", "Weirao Wang", "X. Sean Wang"], "title": "MLego: Interactive and Scalable Topic Exploration Through Model Reuse", "categories": ["cs.DB", "cs.IR"], "comment": "14 pages", "summary": "With massive texts on social media, users and analysts often rely on topic\nmodeling techniques to quickly extract key themes and gain insights.\nTraditional topic modeling techniques, such as Latent Dirichlet Allocation\n(LDA), provide valuable insights but are computationally expensive, making them\nimpractical for real-time data analysis. Although recent advances in\ndistributed training and fast sampling methods have improved efficiency,\nreal-time topic exploration remains a significant challenge. In this paper, we\npresent MLego, an interactive query framework designed to support real-time\ntopic modeling analysis by leveraging model materialization and reuse. Instead\nof retraining models from scratch, MLego efficiently merges materialized topic\nmodels to construct approximate results at interactive speeds. To further\nenhance efficiency, we introduce a hierarchical plan search strategy for single\nqueries and an optimized query reordering technique for batch queries. We\nintegrate MLego into a visual analytics prototype system, enabling users to\nexplore large-scale textual datasets through interactive queries. Extensive\nexperiments demonstrate that MLego significantly reduces computation costs\nwhile maintaining high-quality topic modeling results. MLego enhances existing\nvisual analytics approaches, which primarily focus on user-driven topic\nmodeling, by enabling real-time, query-driven exploration. This complements\ntraditional methods and bridges the gap between scalable topic modeling and\ninteractive data analysis.", "AI": {"tldr": "MLego\u662f\u4e00\u4e2a\u5b9e\u65f6\u4e3b\u9898\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u7269\u5316\u548c\u91cd\u7528\u652f\u6301\u4ea4\u4e92\u5f0f\u67e5\u8be2\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u5efa\u6a21\u6280\u672f\uff08\u5982LDA\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u5206\u6790\u3002\u5c3d\u7ba1\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u5feb\u901f\u91c7\u6837\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5b9e\u65f6\u4e3b\u9898\u63a2\u7d22\u4ecd\u662f\u6311\u6218\u3002", "method": "MLego\u5229\u7528\u6a21\u578b\u7269\u5316\u548c\u91cd\u7528\u7684\u65b9\u6cd5\uff0c\u9ad8\u6548\u5408\u5e76\u5df2\u7269\u5316\u7684\u4e3b\u9898\u6a21\u578b\u4ee5\u8fd1\u4f3c\u7ed3\u679c\u3002\u8fd8\u5f15\u5165\u4e86\u5206\u5c42\u8ba1\u5212\u641c\u7d22\u548c\u67e5\u8be2\u91cd\u6392\u5e8f\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMLego\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u4e3b\u9898\u5efa\u6a21\u7ed3\u679c\u3002", "conclusion": "MLego\u586b\u8865\u4e86\u53ef\u6269\u5c55\u4e3b\u9898\u5efa\u6a21\u4e0e\u4ea4\u4e92\u5f0f\u5206\u6790\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u53ef\u89c6\u5316\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2508.07796", "pdf": "https://arxiv.org/pdf/2508.07796", "abs": "https://arxiv.org/abs/2508.07796", "authors": ["Dengke Han", "Duo Wang", "Mingyu Yan", "Xiaochun Ye", "Dongrui Fan"], "title": "TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference", "categories": ["cs.AR"], "comment": "8 pages, 9 figures, accepted by ICCD 2025", "summary": "Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous\ngraph data and are widely applied in critical domains. In HGNN inference, the\nneighbor aggregation stage is the primary performance determinant, yet it\nsuffers from two major sources of memory inefficiency. First, the commonly\nadopted per-semantic execution paradigm stores intermediate aggregation results\nfor each semantic prior to semantic fusion, causing substantial memory\nexpansion. Second, the aggregation process incurs extensive redundant memory\naccesses, including repeated loading of target vertex features across semantics\nand repeated accesses to shared neighbors due to cross-semantic neighborhood\noverlap. These inefficiencies severely limit scalability and reduce HGNN\ninference performance.\n  In this work, we first propose a semantics-complete execution paradigm from a\nvertex perspective that eliminates per-semantic intermediate storage and\nredundant target vertex accesses. Building on this paradigm, we design\nTVL-HGNN, a reconfigurable hardware accelerator optimized for efficient\naggregation. In addition, we introduce a vertex grouping technique based on\ncross-semantic neighborhood overlap, with hardware implementation, to reduce\nredundant accesses to shared neighbors. Experimental results demonstrate that\nTVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU\nand the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing\nenergy consumption by 98.79% and 32.61%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u6267\u884c\u8303\u5f0fTVL-HGNN\uff0c\u901a\u8fc7\u6d88\u9664\u4e2d\u95f4\u5b58\u50a8\u548c\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "HGNN\u63a8\u7406\u4e2d\u7684\u90bb\u5c45\u805a\u5408\u9636\u6bb5\u5b58\u5728\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5305\u62ec\u4e2d\u95f4\u5b58\u50a8\u7684\u6269\u5c55\u548c\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9876\u70b9\u89c6\u89d2\u7684\u8bed\u4e49\u5b8c\u6574\u6267\u884c\u8303\u5f0f\uff0c\u6d88\u9664\u4e86\u4e2d\u95f4\u5b58\u50a8\u548c\u76ee\u6807\u9876\u70b9\u5197\u4f59\u8bbf\u95ee\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u786c\u4ef6\u52a0\u901f\u5668TVL-HGNN\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u8de8\u8bed\u4e49\u90bb\u5c45\u91cd\u53e0\u7684\u9876\u70b9\u5206\u7ec4\u6280\u672f\u3002", "result": "TVL-HGNN\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eNVIDIA A100 GPU\u548c\u73b0\u6709HGNN\u52a0\u901f\u5668HiHGNN\uff0c\u901f\u5ea6\u63d0\u5347\u5206\u522b\u4e3a7.85\u500d\u548c1.41\u500d\uff0c\u80fd\u8017\u5206\u522b\u964d\u4f4e98.79%\u548c32.61%\u3002", "conclusion": "TVL-HGNN\u901a\u8fc7\u4f18\u5316\u6267\u884c\u8303\u5f0f\u548c\u786c\u4ef6\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86HGNN\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2508.07472", "pdf": "https://arxiv.org/pdf/2508.07472", "abs": "https://arxiv.org/abs/2508.07472", "authors": ["Ramesh Adhikari", "Costas Busch", "Miroslav Popovic"], "title": "On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding", "categories": ["cs.DC"], "comment": "15 pages, 2 figures, accepted as a regular paper at 39th\n  International Symposium on Distributed Computing (DISC 2025)", "summary": "Sharding is a technique to speed up transaction processing in blockchains,\nwhere the $n$ processing nodes in the blockchain are divided into $s$ disjoint\ngroups (shards) that can process transactions in parallel. We study dynamic\nscheduling problems on a shard graph $G_s$ where transactions arrive online\nover time and are not known in advance. Each transaction may access at most $k$\nshards, and we denote by $d$ the worst distance between a transaction and its\naccessing (destination) shards (the parameter $d$ is unknown to the shards). To\nhandle different values of $d$, we assume a locality sensitive decomposition of\n$G_s$ into clusters of shards, where every cluster has a leader shard that\nschedules transactions for the cluster. We first examine the simpler case of\nthe stateless model, where leaders are not aware of the current state of the\ntransaction accounts, and we prove a $O(d \\log^2 s \\cdot \\min\\{k, \\sqrt{s}\\})$\ncompetitive ratio for latency. We then consider the stateful model, where\nleader shards gather the current state of accounts, and we prove a $O(\\log\ns\\cdot \\min\\{k, \\sqrt{s}\\}+\\log^2 s)$ competitive ratio for latency. Each\nleader calculates the schedule in polynomial time for each transaction that it\nprocesses. We show that for any $\\epsilon > 0$, approximating the optimal\nschedule within a $(\\min\\{k, \\sqrt{s}\\})^{1 -\\epsilon}$ factor is NP-hard.\nHence, our bound for the stateful model is within a poly-log factor from the\nbest possibly achievable. To the best of our knowledge, this is the first work\nto establish provably efficient dynamic scheduling algorithms for blockchain\nsharding systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u578b\uff08stateless\u548cstateful\uff09\uff0c\u5206\u522b\u8bc1\u660e\u4e86\u5ef6\u8fdf\u7684\u7ade\u4e89\u6bd4\uff0c\u5e76\u6307\u51fa\u67d0\u4e9b\u6761\u4ef6\u4e0b\u8fd1\u4f3c\u6700\u4f18\u8c03\u5ea6\u662fNP\u96be\u7684\u3002", "motivation": "\u533a\u5757\u94fe\u5206\u7247\u6280\u672f\u901a\u8fc7\u5e76\u884c\u5904\u7406\u4ea4\u6613\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\uff0c\u672c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5206\u7247\u56fe\u6a21\u578b\uff0c\u5206\u6790\u4e24\u79cd\u8c03\u5ea6\u6a21\u578b\uff08stateless\u548cstateful\uff09\uff0c\u5206\u522b\u8bbe\u8ba1\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u7ade\u4e89\u6bd4\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "stateless\u6a21\u578b\u5ef6\u8fdf\u7ade\u4e89\u6bd4\u4e3a$O(d \\log^2 s \\cdot \\min\\{k, \\sqrt{s}\\})$\uff0cstateful\u6a21\u578b\u4e3a$O(\\log s\\cdot \\min\\{k, \\sqrt{s}\\}+\\log^2 s)$\uff0c\u4e14\u8fd1\u4f3c\u6700\u4f18\u8c03\u5ea6\u662fNP\u96be\u7684\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u4e3a\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u9ad8\u6548\u7684\u52a8\u6001\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7406\u8bba\u7ed3\u679c\u63a5\u8fd1\u6700\u4f18\u3002"}}
{"id": "2508.07506", "pdf": "https://arxiv.org/pdf/2508.07506", "abs": "https://arxiv.org/abs/2508.07506", "authors": ["Hammas Bin Tanveer", "Wai Sun Chan", "Ricky K. P. Mok", "Sebastian Kappes", "Philipp Richter", "Oliver Gasser", "John Ronan", "Arthur Berger", "kc Claffy"], "title": "Unveiling IPv6 Scanning Dynamics: A Longitudinal Study Using Large Scale Proactive and Passive IPv6 Telescopes", "categories": ["cs.NI"], "comment": "24 pages, 16 figures, 8 tables, Accepted at ACM CoNEXT 2025 for\n  publication in the Proceedings of the ACM on Networking", "summary": "We introduce new tools and vantage points to develop and integrate proactive\ntechniques to attract IPv6 scan traffic, thus enabling its analysis. By\ndeploying the largest-ever IPv6 proactive telescope in a production ISP\nnetwork, we collected over 600M packets of unsolicited traffic from 1.9k\nAutonomous Systems in 10 months. We characterized the sources of unsolicited\ntraffic, evaluated the effectiveness of five major features across the network\nstack, and inferred scanners' sources of target addresses and their strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u5de5\u5177\u548c\u89c6\u89d2\uff0c\u901a\u8fc7\u90e8\u7f72IPv6\u4e3b\u52a8\u671b\u8fdc\u955c\uff0c\u5206\u6790\u672a\u8bf7\u6c42\u6d41\u91cf\u3002", "motivation": "\u5f00\u53d1\u4e3b\u52a8\u6280\u672f\u4ee5\u5438\u5f15\u548c\u5206\u6790IPv6\u626b\u63cf\u6d41\u91cf\u3002", "method": "\u5728\u751f\u4ea7ISP\u7f51\u7edc\u4e2d\u90e8\u7f72\u53f2\u4e0a\u6700\u5927IPv6\u4e3b\u52a8\u671b\u8fdc\u955c\uff0c\u6536\u96c6\u6570\u636e\u5e76\u5206\u6790\u3002", "result": "\u6536\u96c6\u4e866\u4ebf\u4e2a\u672a\u8bf7\u6c42\u6570\u636e\u5305\uff0c\u5206\u6790\u4e86\u6d41\u91cf\u6765\u6e90\u548c\u626b\u63cf\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9IPv6\u626b\u63cf\u884c\u4e3a\u7684\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2508.07356", "pdf": "https://arxiv.org/pdf/2508.07356", "abs": "https://arxiv.org/abs/2508.07356", "authors": ["Linyi Xu", "Zihao Li"], "title": "Enhancing Systematic Interoperability: Convergences and Mismatches between Web 3.0 and the EU Data Act", "categories": ["cs.CY", "cs.ET"], "comment": "Accepted for International Conference on Future Communications and\n  Networks (FCN) 2025", "summary": "Interoperability is increasingly recognised as a foundational principle for\nfostering innovation, competition, and user autonomy in the evolving digital\necosystem. Existing research on interoperability predominantly focuses either\non technological interoperability itself or on the legal regulations concerning\ninteroperability, with insufficient exploration of their interdisciplinary\nintersection. This paper compares the technological interoperability in Web 3.0\nwith the theoretical framework of legal interoperability established by the EU\nData Act, analysing the areas of convergence and mismatch. The goal is to align\ntechnical interoperability with legal concepts of interoperability, thereby\nenhancing the practical implementation of systematic interoperability in the\nnext generation of the Web. This study finds that, firstly, Web 3.0's concept\nof interoperability spans data, systems, and applications, while the Data Act\nfocuses solely on data. This narrow scope risks creating a fragmented\necosystem, where data exchange is possible, but full integration of systems and\napplications is hindered, leading to inefficiencies, and obstructing seamless\ndata flow across platforms. Secondly, while Web 3.0 technically seeks to\nachieve interoperability through the integration of entire systems and\ndecentralised applications, the compliance with Data Act might negatively limit\nsuch system and application interoperability through its data interoperability\nprovisions. This paper suggests interdisciplinary recommendations to enhance\nthe implementation and enforcement of interoperability. On one hand, the Data\nAct should broaden its concept of interoperability to encompass both the\nsystems and applications layers. On the other hand, it is advisable to\nintroduce provisions for standardised protocols through soft law mechanisms to\naddress legal shortcomings and keep pace with technological advancements.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Web 3.0\u6280\u672f\u4e92\u64cd\u4f5c\u6027\u4e0e\u6b27\u76df\u6570\u636e\u6cd5\u6848\u6cd5\u5f8b\u4e92\u64cd\u4f5c\u6027\u6846\u67b6\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u8de8\u5b66\u79d1\u5efa\u8bae\u4ee5\u4fc3\u8fdb\u7cfb\u7edf\u6027\u4e92\u64cd\u4f5c\u6027\u5b9e\u8df5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u6280\u672f\u4e92\u64cd\u4f5c\u6027\u4e0e\u6cd5\u5f8b\u4e92\u64cd\u4f5c\u6027\u7684\u4ea4\u53c9\u63a2\u7d22\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u7684\u7cfb\u7edf\u6027\u4e92\u64cd\u4f5c\u6027\u5b9e\u73b0\u3002", "method": "\u6bd4\u8f83\u5206\u6790Web 3.0\u7684\u6280\u672f\u4e92\u64cd\u4f5c\u6027\u4e0e\u6b27\u76df\u6570\u636e\u6cd5\u6848\u7684\u6cd5\u5f8b\u4e92\u64cd\u4f5c\u6027\u6846\u67b6\uff0c\u63ed\u793a\u5176\u8d8b\u540c\u4e0e\u4e0d\u5339\u914d\u4e4b\u5904\u3002", "result": "\u53d1\u73b0Web 3.0\u7684\u4e92\u64cd\u4f5c\u6027\u6db5\u76d6\u6570\u636e\u3001\u7cfb\u7edf\u548c\u5e94\u7528\uff0c\u800c\u6570\u636e\u6cd5\u6848\u4ec5\u5173\u6ce8\u6570\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\uff1b\u6570\u636e\u6cd5\u6848\u7684\u5408\u89c4\u8981\u6c42\u53ef\u80fd\u9650\u5236\u7cfb\u7edf\u548c\u5e94\u7528\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u5efa\u8bae\u6570\u636e\u6cd5\u6848\u6269\u5927\u4e92\u64cd\u4f5c\u6027\u6982\u5ff5\uff0c\u7eb3\u5165\u7cfb\u7edf\u548c\u5e94\u7528\u5c42\uff1b\u540c\u65f6\u901a\u8fc7\u8f6f\u6cd5\u673a\u5236\u5f15\u5165\u6807\u51c6\u5316\u534f\u8bae\uff0c\u4ee5\u5e94\u5bf9\u6cd5\u5f8b\u4e0d\u8db3\u5e76\u8ddf\u4e0a\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2508.07881", "pdf": "https://arxiv.org/pdf/2508.07881", "abs": "https://arxiv.org/abs/2508.07881", "authors": ["Henna Tammia", "Benjamin K\u00e4m\u00e4", "Ella Peltonen"], "title": "Adopting Road-Weather Open Data in Route Recommendation Engine", "categories": ["cs.SE"], "comment": null, "summary": "Digitraffic, Finland's open road data interface, provides access to\nnationwide road sensors with more than 2,300 real-time attributes from 1,814\nstations. However, efficiently utilizing such a versatile data API for a\npractical application requires a deeper understanding of the data qualities,\npreprocessing phases, and machine learning tools. This paper discusses the\nchallenges of large-scale road weather and traffic data. We go through the\nroad-weather-related attributes from DigiTraffic as a practical example of\nprocesses required to work with such a dataset. In addition, we provide a\nmethodology for efficient data utilization for the target application, a\npersonalized road recommendation engine based on a simple routing application.\nWe validate our solution based on real-world data, showing we can efficiently\nidentify and recommend personalized routes for three different driver profiles.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u9ad8\u6548\u5229\u7528\u82ac\u5170\u7684\u5f00\u653e\u9053\u8def\u6570\u636e\u63a5\u53e3DigiTraffic\uff0c\u63d0\u51fa\u4e86\u5904\u7406\u5927\u89c4\u6a21\u9053\u8def\u5929\u6c14\u548c\u4ea4\u901a\u6570\u636e\u7684\u6311\u6218\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b80\u5355\u8def\u7531\u5e94\u7528\u7684\u4e2a\u6027\u5316\u9053\u8def\u63a8\u8350\u65b9\u6cd5\u3002", "motivation": "\u82ac\u5170\u7684DigiTraffic\u63a5\u53e3\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5b9e\u65f6\u9053\u8def\u6570\u636e\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u4e3a\u5b9e\u9645\u5e94\u7528\u670d\u52a1\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86DigiTraffic\u4e2d\u7684\u9053\u8def\u5929\u6c14\u76f8\u5173\u5c5e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e2a\u6027\u5316\u9053\u8def\u63a8\u8350\u5f15\u64ce\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u4e3a\u4e09\u79cd\u4e0d\u540c\u9a7e\u9a76\u8005\u63a8\u8350\u4e86\u4e2a\u6027\u5316\u8def\u7ebf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5408\u7406\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u65b9\u6cd5\u8bbe\u8ba1\u53ef\u4ee5\u9ad8\u6548\u5229\u7528\u5f00\u653e\u9053\u8def\u6570\u636e\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u63a8\u8350\u3002"}}
{"id": "2508.06791", "pdf": "https://arxiv.org/pdf/2508.06791", "abs": "https://arxiv.org/abs/2508.06791", "authors": ["De\u00f3genes P. da Silva Junior", "Jonas Lopes Guerra", "Krissia Menezes", "Marisa Sel Franco", "Roberto Pereira"], "title": "Entendimento de Campanhas no Contexto da Aten\u00e7\u00e3o Prim\u00e1ria \u00e0 Sa\u00fade: Um Processo de Design Socialmente Consciente", "categories": ["cs.HC"], "comment": "62 pages, in Portuguese language, 8 figures", "summary": "This report presents the results of an exploratory analysis of the work\ncontext of Community Health Agents and Endemic Disease Control Agents in\nPrimary Health Care (PHC), with a particular focus on Health Campaigns. To\nunderstand this context, the study adopted the Socially Aware Design framework,\nwhich employs artifacts and techniques to examine problem domains in a\ncomprehensive and sociotechnical manner. Methods such as the Stakeholder\nIdentification Diagram, Evaluation Frame, and Semiotic Framework were applied\nto identify stakeholders, anticipate challenges, and elicit social and\ntechnical requirements for the solution. Personas and Scenarios were also used\nto illustrate the potential impacts of a solution on various stakeholders and\ntheir life contexts within health campaigns. This report presents the analysis\nmethod, its application, and results, discussing the study's findings to inform\nthe development of medium-fidelity prototypes for a PHC health campaign\nmanagement solution.", "AI": {"tldr": "\u8be5\u62a5\u544a\u5206\u6790\u4e86\u793e\u533a\u536b\u751f\u5de5\u4f5c\u8005\u548c\u5730\u65b9\u75c5\u63a7\u5236\u5de5\u4f5c\u8005\u5728\u521d\u7ea7\u536b\u751f\u4fdd\u5065\u4e2d\u7684\u5de5\u4f5c\u80cc\u666f\uff0c\u7279\u522b\u662f\u5065\u5eb7\u5ba3\u4f20\u6d3b\u52a8\uff0c\u91c7\u7528\u4e86\u793e\u4f1a\u610f\u8bc6\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u8bc6\u522b\u5229\u76ca\u76f8\u5173\u8005\u5e76\u6536\u96c6\u9700\u6c42\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6df1\u5165\u4e86\u89e3\u521d\u7ea7\u536b\u751f\u4fdd\u5065\u4e2d\u5065\u5eb7\u5ba3\u4f20\u6d3b\u52a8\u7684\u793e\u4f1a\u6280\u672f\u80cc\u666f\uff0c\u4ee5\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e86\u793e\u4f1a\u610f\u8bc6\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u5229\u76ca\u76f8\u5173\u8005\u8bc6\u522b\u56fe\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u7b26\u53f7\u6846\u67b6\u7b49\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4eba\u7269\u89d2\u8272\u548c\u573a\u666f\u5206\u6790\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86\u5229\u76ca\u76f8\u5173\u8005\u3001\u9700\u6c42\u53ca\u6f5c\u5728\u6311\u6218\uff0c\u4e3a\u5f00\u53d1\u5065\u5eb7\u5ba3\u4f20\u6d3b\u52a8\u7ba1\u7406\u7684\u4e2d\u4fdd\u771f\u539f\u578b\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u5206\u6790\u7ed3\u679c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u9488\u5bf9\u521d\u7ea7\u536b\u751f\u4fdd\u5065\u7684\u5065\u5eb7\u5ba3\u4f20\u6d3b\u52a8\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u793e\u4f1a\u6280\u672f\u89c6\u89d2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07022", "pdf": "https://arxiv.org/pdf/2508.07022", "abs": "https://arxiv.org/abs/2508.07022", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MultiMedEdit\uff0c\u9996\u4e2a\u9488\u5bf9\u4e34\u5e8a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u901a\u7528\u6027\u548c\u957f\u5c3e\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u4e34\u5e8a\u7a33\u5065\u7684KE\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u77e5\u8bc6\u7f16\u8f91\u5728\u901a\u7528\u9886\u57df\u548c\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u6a21\u6001\u533b\u5b66\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4e9f\u9700\u652f\u6301\u89c6\u89c9\u63a8\u7406\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86MultiMedEdit\u6846\u67b6\uff0c\u6db5\u76d6\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u7c7b\u578b\uff0c\u5b9a\u4e49\u4e86\u4e09\u7ef4\u5ea6\u6307\u6807\uff08\u53ef\u9760\u6027\u3001\u901a\u7528\u6027\u548c\u5c40\u90e8\u6027\uff09\uff0c\u5e76\u652f\u6301\u8de8\u8303\u5f0f\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u901a\u7528\u6027\u3001\u957f\u5c3e\u63a8\u7406\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540cKE\u8303\u5f0f\u5728\u5b9e\u7528\u90e8\u7f72\u4e2d\u7684\u6743\u8861\u3002", "conclusion": "MultiMedEdit\u4e0d\u4ec5\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u4e3a\u672a\u6765\u5f00\u53d1\u4e34\u5e8a\u7a33\u5065\u7684KE\u6280\u672f\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07786", "pdf": "https://arxiv.org/pdf/2508.07786", "abs": "https://arxiv.org/abs/2508.07786", "authors": ["Alexander V. Gheorghiu", "David J. Pym"], "title": "Proof-theoretic Semantics for Second-order Logic", "categories": ["math.LO", "cs.LO"], "comment": null, "summary": "We develop a proof-theoretic semantics (P-tS) for second-order logic (S-oL),\nproviding an inferentialist alternative to both full and Henkin model-theoretic\ninterpretations. Our approach is grounded in base-extension semantics (B-eS), a\nframework in which meaning is determined by inferential roles relative to\natomic systems -- collections of rules that encode an agent's pre-logical\ninferential commitments. We show how both classical and intuitionistic versions\nof S-oL emerge from this set-up by varying the class of atomic systems. These\nsystems yield modular soundness and completeness results for corresponding\nHilbert-style calculi, which we prove equivalent to Henkin's account of S-oL.\nIn doing so, we reframe second-order quantification as systematic substitution\nrather than set-theoretic commitment, thereby offering a philosophically\nlightweight yet expressive semantics for higher-order logic. This work\ncontributes to the broader programme of grounding logical meaning in use rather\nthan reference and offers a new lens on the foundations of logic and\nmathematics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc1\u660e\u8bba\u8bed\u4e49\u5b66\u7684\u4e8c\u9636\u903b\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u6269\u5c55\u8bed\u4e49\u5b66\u6846\u67b6\uff0c\u4ee5\u63a8\u7406\u89d2\u8272\u4e3a\u57fa\u7840\u5b9a\u4e49\u903b\u8f91\u542b\u4e49\uff0c\u63d0\u4f9b\u4e86\u5bf9\u7ecf\u5178\u548c\u76f4\u89c9\u4e3b\u4e49\u4e8c\u9636\u903b\u8f91\u7684\u7edf\u4e00\u89e3\u91ca\u3002", "motivation": "\u65e8\u5728\u4e3a\u4e8c\u9636\u903b\u8f91\u63d0\u4f9b\u4e00\u4e2a\u4e0d\u4f9d\u8d56\u4e8e\u6a21\u578b\u8bba\u7684\u66ff\u4ee3\u8bed\u4e49\u5b66\uff0c\u5f3a\u8c03\u903b\u8f91\u542b\u4e49\u5e94\u57fa\u4e8e\u63a8\u7406\u548c\u4f7f\u7528\u800c\u975e\u96c6\u5408\u8bba\u627f\u8bfa\u3002", "method": "\u91c7\u7528\u57fa\u6269\u5c55\u8bed\u4e49\u5b66\uff08B-eS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u539f\u5b50\u7cfb\u7edf\u7684\u7c7b\u522b\u6765\u533a\u5206\u7ecf\u5178\u548c\u76f4\u89c9\u4e3b\u4e49\u4e8c\u9636\u903b\u8f91\uff0c\u5e76\u8bc1\u660e\u4e86\u4e0eHenkin\u6a21\u578b\u8bba\u7684\u7b49\u4ef7\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u5b66\u5bf9Hilbert-style\u6f14\u7b97\u7684\u6a21\u5757\u5316\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u5c06\u4e8c\u9636\u91cf\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7cfb\u7edf\u66ff\u6362\u800c\u975e\u96c6\u5408\u8bba\u627f\u8bfa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u9636\u903b\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u54f2\u5b66\u4e0a\u66f4\u8f7b\u91cf\u7ea7\u7684\u8bed\u4e49\u5b66\uff0c\u652f\u6301\u903b\u8f91\u542b\u4e49\u5e94\u57fa\u4e8e\u63a8\u7406\u4f7f\u7528\u7684\u89c2\u70b9\uff0c\u5e76\u5bf9\u903b\u8f91\u548c\u6570\u5b66\u57fa\u7840\u63d0\u51fa\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.08054", "pdf": "https://arxiv.org/pdf/2508.08054", "abs": "https://arxiv.org/abs/2508.08054", "authors": ["Andrew Kang", "Sainyam Galhotra"], "title": "TQL: Towards Type-Driven Data Discovery", "categories": ["cs.DB", "cs.PL"], "comment": "2024 IEEE BigData paper", "summary": "Existing query languages for data discovery exhibit system-driven designs\nthat emphasize database features and functionality over user needs. We propose\na re-prioritization of the client through an introduction of a language-driven\napproach to data discovery systems that can leverage powerful results from\nprogramming languages research. In this paper, we describe TQL, a flexible and\npractical query language which incorporates a type-like system to encompass\ndownstream transformation-context in its discovery queries. The syntax and\nsemantics of TQL (including the underlying evaluation model), are formally\ndefined, and a sketch of its implementation is also provided. Additionally, we\nprovide comparisons to existing languages for data retrieval and data discovery\nto examine the advantages of TQL's expanded expressive power in real-life\nsettings.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTQL\u7684\u65b0\u578b\u67e5\u8be2\u8bed\u8a00\uff0c\u5176\u8bbe\u8ba1\u4ee5\u7528\u6237\u9700\u6c42\u4e3a\u6838\u5fc3\uff0c\u800c\u975e\u4f20\u7edf\u7cfb\u7edf\u529f\u80fd\u9a71\u52a8\u7684\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u53d1\u73b0\u67e5\u8be2\u8bed\u8a00\u591a\u4e3a\u7cfb\u7edf\u9a71\u52a8\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "TQL\u5f15\u5165\u4e86\u4e00\u79cd\u8bed\u8a00\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u4e86\u7c7b\u4f3c\u7c7b\u578b\u7cfb\u7edf\u7684\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u4e86\u7f16\u7a0b\u8bed\u8a00\u7814\u7a76\u7684\u6210\u679c\u3002\u8bba\u6587\u8be6\u7ec6\u5b9a\u4e49\u4e86TQL\u7684\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u73b0\u6846\u67b6\u3002", "result": "TQL\u5728\u8868\u8fbe\u80fd\u529b\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u68c0\u7d22\u548c\u53d1\u73b0\u8bed\u8a00\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "TQL\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u8bbe\u8ba1\u63d0\u5347\u4e86\u6570\u636e\u53d1\u73b0\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07605", "pdf": "https://arxiv.org/pdf/2508.07605", "abs": "https://arxiv.org/abs/2508.07605", "authors": ["Zhong Zheng", "Michael E. Papka", "Zhiling Lan"], "title": "Coordinated Power Management on Heterogeneous Systems", "categories": ["cs.DC"], "comment": null, "summary": "Performance prediction is essential for energy-efficient computing in\nheterogeneous computing systems that integrate CPUs and GPUs. However,\ntraditional performance modeling methods often rely on exhaustive offline\nprofiling, which becomes impractical due to the large setting space and the\nhigh cost of profiling large-scale applications. In this paper, we present\nOPEN, a framework consists of offline and online phases. The offline phase\ninvolves building a performance predictor and constructing an initial dense\nmatrix. In the online phase, OPEN performs lightweight online profiling, and\nleverages the performance predictor with collaborative filtering to make\nperformance prediction. We evaluate OPEN on multiple heterogeneous systems,\nincluding those equipped with A100 and A30 GPUs. Results show that OPEN\nachieves prediction accuracy up to 98.29\\%. This demonstrates that OPEN\neffectively reduces profiling cost while maintaining high accuracy, making it\npractical for power-aware performance modeling in modern HPC environments.\nOverall, OPEN provides a lightweight solution for performance prediction under\npower constraints, enabling better runtime decisions in power-aware computing\nenvironments.", "AI": {"tldr": "OPEN\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u6027\u80fd\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u4e0e\u5728\u7ebf\u9636\u6bb5\uff0c\u964d\u4f4e\u5206\u6790\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u79bb\u7ebf\u5206\u6790\uff0c\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u3002OPEN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "OPEN\u5305\u62ec\u79bb\u7ebf\u9636\u6bb5\uff08\u6784\u5efa\u6027\u80fd\u9884\u6d4b\u5668\u548c\u521d\u59cb\u77e9\u9635\uff09\u548c\u5728\u7ebf\u9636\u6bb5\uff08\u8f7b\u91cf\u7ea7\u5206\u6790\u548c\u534f\u4f5c\u8fc7\u6ee4\uff09\u3002", "result": "\u5728\u591a\u79cd\u5f02\u6784\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\uff0cOPEN\u7684\u9884\u6d4b\u7cbe\u5ea6\u9ad8\u8fbe98.29%\u3002", "conclusion": "OPEN\u4e3a\u529f\u8017\u611f\u77e5\u8ba1\u7b97\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u66f4\u4f18\u7684\u8fd0\u884c\u65f6\u51b3\u7b56\u3002"}}
{"id": "2508.07578", "pdf": "https://arxiv.org/pdf/2508.07578", "abs": "https://arxiv.org/abs/2508.07578", "authors": ["Yu Gou", "Tong Zhang", "Jun Liu", "Tingting Yang", "Shanshan Song", "Jun-Hong Cui"], "title": "Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach", "categories": ["cs.NI"], "comment": null, "summary": "This paper investigates the fair-effective communication and robustness in\nimperfect and energy-constrained underwater acoustic sensor networks\n(IC-UASNs). Specifically, we investigate the impact of unexpected node\nmalfunctions on the network performance under the time-varying acoustic\nchannels. Each node is expected to satisfy Quality of Service (QoS)\nrequirements. However, achieving individual QoS requirements may interfere with\nother concurrent communications. Underwater nodes rely excessively on the\nrationality of other underwater nodes when guided by fully cooperative\napproaches, making it difficult to seek a trade-off between individual QoS and\nglobal fair-effective communications under imperfect conditions. Therefore,\nthis paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that\nachieves fair-effective communication and robustness in IC-UASNs. The approach\nis distributed multi-agent reinforcement learning (MARL)-based, and the\nobjectives are twofold. On the one hand, each intelligent node individually\ndecides the transmission power to simultaneously optimize individual and global\nperformance. On the other hand, advanced training algorithms are developed to\nprovide imperfect environments for training robust models that can adapt to the\ntime-varying acoustic channels and handle unexpected node failures in the\nnetwork. Numerical results are presented to validate our proposed approach.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u5b8c\u7f8e\u4e14\u80fd\u91cf\u53d7\u9650\u7684\u6c34\u4e0b\u58f0\u5b66\u4f20\u611f\u5668\u7f51\u7edc\uff08IC-UASNs\uff09\u4e2d\u7684\u516c\u5e73\u6709\u6548\u901a\u4fe1\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u6c34\u4e0b\u58f0\u5b66\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\uff0c\u8282\u70b9\u6545\u969c\u548c\u65f6\u53d8\u4fe1\u9053\u5bf9\u6027\u80fd\u4ea7\u751f\u5f71\u54cd\uff0c\u5bfc\u81f4\u5b8c\u5168\u534f\u4f5c\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u4e2a\u4f53QoS\u9700\u6c42\u4e0e\u5168\u5c40\u516c\u5e73\u6709\u6548\u901a\u4fe1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u534a\u534f\u4f5c\u529f\u7387\u5206\u914d\u65b9\u6cd5\uff08SECOPA\uff09\uff0c\u667a\u80fd\u8282\u70b9\u81ea\u4e3b\u51b3\u7b56\u4f20\u8f93\u529f\u7387\u4ee5\u4f18\u5316\u4e2a\u4f53\u548c\u5168\u5c40\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u65f6\u53d8\u4fe1\u9053\u5e76\u5904\u7406\u8282\u70b9\u6545\u969c\uff0c\u5b9e\u73b0\u516c\u5e73\u6709\u6548\u901a\u4fe1\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SECOPA\u65b9\u6cd5\u5728IC-UASNs\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u96be\u9898\u3002"}}
{"id": "2508.07714", "pdf": "https://arxiv.org/pdf/2508.07714", "abs": "https://arxiv.org/abs/2508.07714", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "Accurate detection and classification of diverse door types in floor plans\ndrawings is critical for multiple applications, such as building compliance\nchecking, and indoor scene understanding. Despite their importance, publicly\navailable datasets specifically designed for fine-grained multi-class door\ndetection remain scarce. In this work, we present a semi-automated pipeline\nthat leverages a state-of-the-art object detector and a large language model\n(LLM) to construct a multi-class door detection dataset with minimal manual\neffort. Doors are first detected as a unified category using a deep object\ndetection model. Next, an LLM classifies each detected instance based on its\nvisual and contextual features. Finally, a human-in-the-loop stage ensures\nhigh-quality labels and bounding boxes. Our method significantly reduces\nannotation cost while producing a dataset suitable for benchmarking neural\nmodels in floor plan analysis. This work demonstrates the potential of\ncombining deep learning and multimodal reasoning for efficient dataset\nconstruction in complex real-world domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u8f83\u4f4e\u6210\u672c\u6784\u5efa\u7ec6\u7c92\u5ea6\u7684\u591a\u7c7b\u522b\u95e8\u68c0\u6d4b\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8e\u516c\u5f00\u7684\u591a\u7c7b\u522b\u95e8\u68c0\u6d4b\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u6784\u5efa\u6b64\u7c7b\u6570\u636e\u96c6\u4ee5\u652f\u6301\u5efa\u7b51\u5408\u89c4\u68c0\u67e5\u548c\u5ba4\u5185\u573a\u666f\u7406\u89e3\u7b49\u5e94\u7528\u3002", "method": "1. \u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7edf\u4e00\u68c0\u6d4b\u95e8\uff1b2. \u7528LLM\u6839\u636e\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u5206\u7c7b\uff1b3. \u4eba\u5de5\u6821\u9a8c\u786e\u4fdd\u6807\u7b7e\u8d28\u91cf\u3002", "result": "\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u751f\u6210\u9002\u7528\u4e8e\u5e73\u9762\u56fe\u5206\u6790\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u590d\u6742\u9886\u57df\u7684\u6570\u636e\u96c6\u6784\u5efa\u63d0\u4f9b\u9ad8\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.07935", "pdf": "https://arxiv.org/pdf/2508.07935", "abs": "https://arxiv.org/abs/2508.07935", "authors": ["Jingwen Zhou", "Jieshan Chen", "Qinghua Lu", "Dehai Zhao", "Liming Zhu"], "title": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Model (LLM) agentic systems are software systems powered by\nLLMs that autonomously reason, plan, and execute multi-step workflows to\nachieve human goals, rather than merely executing predefined steps. During\nexecution, these workflows frequently encounter exceptions. Existing exception\nhandling solutions often treat exceptions superficially, failing to trace\nexecution-phase exceptions to their reasoning-phase root causes. Furthermore,\ntheir recovery logic is brittle, lacking structured escalation pathways when\ninitial attempts fail. To tackle these challenges, we first present a\ncomprehensive taxonomy of 36 exception types across 12 agent artifacts.\nBuilding on this, we propose SHIELDA (Structured Handling of Exceptions in\nLLM-Driven Agentic Workflows), a modular runtime exception handling framework\nfor LLM agentic workflows. SHIELDA uses an exception classifier to select a\npredefined exception handling pattern from a handling pattern registry. These\npatterns are then executed via a structured handling executor, comprising local\nhandling, flow control, and state recovery, to enable phase-aware recovery by\nlinking exceptions to their root causes and facilitating composable strategies.\nWe validate SHIELDA's effectiveness through a case study on the AutoPR agent,\ndemonstrating effective, cross-phase recovery from a reasoning-induced\nexception.", "AI": {"tldr": "SHIELDA\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u5f02\u5e38\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u7c7b\u548c\u6267\u884c\u9884\u5b9a\u4e49\u7684\u6a21\u5f0f\u5b9e\u73b0\u6709\u6548\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u5f02\u5e38\u5904\u7406\u8f83\u8868\u9762\uff0c\u65e0\u6cd5\u8ffd\u6eaf\u5230\u6839\u672c\u539f\u56e0\uff0c\u6062\u590d\u903b\u8f91\u8106\u5f31\u3002SHIELDA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa36\u79cd\u5f02\u5e38\u7c7b\u578b\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bbe\u8ba1SHIELDA\u6846\u67b6\uff0c\u5305\u62ec\u5f02\u5e38\u5206\u7c7b\u5668\u3001\u5904\u7406\u6a21\u5f0f\u6ce8\u518c\u8868\u548c\u7ed3\u6784\u5316\u6267\u884c\u5668\u3002", "result": "\u901a\u8fc7AutoPR\u4ee3\u7406\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86SHIELDA\u5728\u8de8\u9636\u6bb5\u5f02\u5e38\u6062\u590d\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SHIELDA\u80fd\u591f\u6709\u6548\u5904\u7406LLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u5f02\u5e38\uff0c\u5e76\u652f\u6301\u6a21\u5757\u5316\u6062\u590d\u7b56\u7565\u3002"}}
{"id": "2508.06801", "pdf": "https://arxiv.org/pdf/2508.06801", "abs": "https://arxiv.org/abs/2508.06801", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Callum Parker", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "title": "Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning", "categories": ["cs.HC"], "comment": null, "summary": "Pedestrian gestures play an important role in traffic communication,\nparticularly in interactions with autonomous vehicles (AVs), yet their subtle,\nambiguous, and context-dependent nature poses persistent challenges for machine\ninterpretation. This study investigates these challenges by using GPT-4V, a\nvision-language model, not as a performance benchmark but as a diagnostic tool\nto reveal patterns and causes of gesture misrecognition. We analysed a public\ndataset of pedestrian-vehicle interactions, combining manual video review with\nthematic analysis of the model's qualitative reasoning. This dual approach\nsurfaced recurring factors influencing misrecognition, including gesture\nvisibility, pedestrian behaviour, interaction context, and environmental\nconditions. The findings suggest practical considerations for gesture design,\nincluding the value of salience and contextual redundancy, and highlight\nopportunities to improve AV recognition systems through richer context\nmodelling and uncertainty-aware interpretations. While centred on AV-pedestrian\ninteraction, the method and insights are applicable to other domains where\nmachines interpret human gestures, such as wearable AR and assistive\ntechnologies.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u884c\u4eba\u624b\u52bf\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\u53ca\u5176\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5229\u7528GPT-4V\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u5206\u6790\u8bef\u8bc6\u522b\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u884c\u4eba\u624b\u52bf\u5728\u4ea4\u901a\u4ea4\u6d41\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u7ec6\u5fae\u3001\u6a21\u7cca\u548c\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u7279\u70b9\u4f7f\u673a\u5668\u8bc6\u522b\u6210\u4e3a\u96be\u9898\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u624b\u52bf\u8bef\u8bc6\u522b\u7684\u6a21\u5f0f\u548c\u539f\u56e0\u3002", "method": "\u4f7f\u7528GPT-4V\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\uff0c\u7ed3\u5408\u4eba\u5de5\u89c6\u9891\u5ba1\u67e5\u548c\u4e3b\u9898\u5206\u6790\uff0c\u5206\u6790\u516c\u5171\u6570\u636e\u96c6\u4e2d\u7684\u884c\u4eba-\u8f66\u8f86\u4ea4\u4e92\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u624b\u52bf\u53ef\u89c1\u6027\u3001\u884c\u4eba\u884c\u4e3a\u3001\u4ea4\u4e92\u4e0a\u4e0b\u6587\u53ca\u73af\u5883\u6761\u4ef6\u662f\u5f71\u54cd\u8bef\u8bc6\u522b\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u624b\u52bf\u8bbe\u8ba1\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdbAV\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u91cd\u8981\u6027\uff0c\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2508.07183", "pdf": "https://arxiv.org/pdf/2508.07183", "abs": "https://arxiv.org/abs/2508.07183", "authors": ["Ahmed M. Abuzuraiq", "Philippe Pasquier"], "title": "Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MM", "I.2; J.5"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "Explainable AI (XAI) in creative contexts can go beyond transparency to\nsupport artistic engagement, modifiability, and sustained practice. While\ncurated datasets and training human-scale models can offer artists greater\nagency and control, large-scale generative models like text-to-image diffusion\nsystems often obscure these possibilities. We suggest that even large models\ncan be treated as creative materials if their internal structure is exposed and\nmanipulable. We propose a craft-based approach to explainability rooted in\nlong-term, hands-on engagement akin to Sch\\\"on's \"reflection-in-action\" and\ndemonstrate its application through a model-bending and inspection plugin\nintegrated into the node-based interface of ComfyUI. We demonstrate that by\ninteractively manipulating different parts of a generative model, artists can\ndevelop an intuition about how each component influences the output.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5de5\u827a\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u901a\u8fc7\u957f\u671f\u5b9e\u8df5\u548c\u4ea4\u4e92\u5f0f\u64cd\u4f5c\u5e2e\u52a9\u827a\u672f\u5bb6\u7406\u89e3\u751f\u6210\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u3002", "motivation": "\u5728\u521b\u610f\u80cc\u666f\u4e0b\uff0c\u73b0\u6709\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u5f80\u5f80\u5c40\u9650\u4e8e\u900f\u660e\u5ea6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u827a\u672f\u5bb6\u7684\u9700\u6c42\u3002\u5927\u578b\u751f\u6210\u6a21\u578b\uff08\u5982\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u7cfb\u7edf\uff09\u901a\u5e38\u9690\u85cf\u4e86\u5176\u5185\u90e8\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u827a\u672f\u5bb6\u7684\u53c2\u4e0e\u548c\u63a7\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5de5\u827a\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u957f\u671f\u5b9e\u8df5\uff08\u7c7b\u4f3cSch\u00f6n\u7684\u201c\u53cd\u601d\u6027\u5b9e\u8df5\u201d\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\u5f2f\u66f2\u548c\u68c0\u67e5\u63d2\u4ef6\uff0c\u96c6\u6210\u5230ComfyUI\u7684\u8282\u70b9\u754c\u9762\u4e2d\u3002", "result": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u64cd\u4f5c\u6a21\u578b\u7684\u5404\u90e8\u5206\uff0c\u827a\u672f\u5bb6\u80fd\u591f\u76f4\u89c2\u5730\u7406\u89e3\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\u3002", "conclusion": "\u66b4\u9732\u548c\u53ef\u64cd\u4f5c\u7684\u5927\u578b\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u521b\u610f\u6750\u6599\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u66f4\u591a\u53c2\u4e0e\u548c\u63a7\u5236\u7684\u673a\u4f1a\u3002"}}
{"id": "2508.06968", "pdf": "https://arxiv.org/pdf/2508.06968", "abs": "https://arxiv.org/abs/2508.06968", "authors": ["Ulas Gunes", "Matias Turkulainen", "Juho Kannala", "Esa Rahtu"], "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present the first evaluation of fisheye-based 3D Gaussian Splatting\nmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180\ndegree. Our study covers both indoor and outdoor scenes captured with 200\ndegree fisheye cameras and analyzes how each method handles extreme distortion\nin real world settings. We evaluate performance under varying fields of view\n(200 degree, 160 degree, and 120 degree) to study the tradeoff between\nperipheral distortion and spatial coverage. Fisheye-GS benefits from field of\nview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable\nacross all settings and maintains high perceptual quality at the full 200\ndegree view. To address the limitations of SfM-based initialization, which\noften fails under strong distortion, we also propose a depth-based strategy\nusing UniK3D predictions from only 2-3 fisheye images per scene. Although\nUniK3D is not trained on real fisheye data, it produces dense point clouds that\nenable reconstruction quality on par with SfM, even in difficult scenes with\nfog, glare, or sky. Our results highlight the practical viability of\nfisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and\ndistortion-heavy image inputs.", "AI": {"tldr": "\u9996\u6b21\u8bc4\u4f30\u9c7c\u773c\u955c\u5934\u4e0b\u76843D\u9ad8\u65af\u6563\u5c04\u65b9\u6cd5\uff08Fisheye-GS\u548c3DGUT\uff09\uff0c\u8986\u76d6\u8d85180\u5ea6\u89c6\u91ce\u7684\u771f\u5b9e\u56fe\u50cf\uff0c\u5206\u6790\u6781\u7aef\u7578\u53d8\u7684\u5904\u7406\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u9c7c\u773c\u955c\u5934\u4e0b3D\u9ad8\u65af\u6563\u5c04\u65b9\u6cd5\u5728\u6781\u7aef\u7578\u53d8\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4f20\u7edfSfM\u521d\u59cb\u5316\u5728\u5f3a\u7578\u53d8\u4e0b\u7684\u5931\u6548\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83Fisheye-GS\u548c3DGUT\u5728\u4e0d\u540c\u89c6\u91ce\uff08200\u5ea6\u3001160\u5ea6\u3001120\u5ea6\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eUniK3D\u9884\u6d4b\u7684\u6df1\u5ea6\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "Fisheye-GS\u5728160\u5ea6\u89c6\u91ce\u8868\u73b0\u6700\u4f73\uff0c\u800c3DGUT\u5728\u5168200\u5ea6\u89c6\u91ce\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002UniK3D\u7b56\u7565\u4e0eSfM\u6548\u679c\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u96fe\u3001\u7729\u5149\u7b49\u590d\u6742\u573a\u666f\u3002", "conclusion": "\u9c7c\u773c\u955c\u5934\u4e0b\u76843D\u9ad8\u65af\u6563\u5c04\u65b9\u6cd5\u5728\u5bbd\u89d2\u5ea63D\u91cd\u5efa\u4e2d\u5177\u6709\u5b9e\u7528\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u4e14\u7578\u53d8\u4e25\u91cd\u7684\u56fe\u50cf\u8f93\u5165\u3002"}}
{"id": "2508.07790", "pdf": "https://arxiv.org/pdf/2508.07790", "abs": "https://arxiv.org/abs/2508.07790", "authors": ["Alessandro Abate", "Thom Badings", "Giuseppe De Giacomo", "Francesco Fabiano"], "title": "Best-Effort Policies for Robust Markov Decision Processes", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RMDPs\uff09\u7684\u901a\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u6700\u4f18\u9c81\u68d2\u6700\u4f73\u52aa\u529b\uff08ORBE\uff09\u7b56\u7565\uff0c\u4ee5\u5728\u591a\u6700\u4f18\u7b56\u7565\u4e2d\u9009\u62e9\u8868\u73b0\u66f4\u597d\u7684\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684RMDPs\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u8f6c\u79fb\u6982\u7387\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5bfc\u81f4\u7b56\u7565\u9009\u62e9\u7f3a\u4e4f\u533a\u5206\u5ea6\u3002", "method": "\u7ed3\u5408\u535a\u5f08\u8bba\u4e2d\u7684\u4f18\u52bf\u4e0e\u6700\u4f73\u52aa\u529b\u6982\u5ff5\uff0c\u63d0\u51faORBE\u7b56\u7565\uff0c\u8981\u6c42\u7b56\u7565\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f18\u4e14\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u5c3d\u53ef\u80fd\u597d\u3002", "result": "\u8bc1\u660e\u4e86ORBE\u7b56\u7565\u7684\u5b58\u5728\u6027\uff0c\u63cf\u8ff0\u4e86\u5176\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "ORBE\u7b56\u7565\u4e3a\u591a\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u9009\u62e9\u6807\u51c6\uff0c\u8865\u5145\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.08074", "pdf": "https://arxiv.org/pdf/2508.08074", "abs": "https://arxiv.org/abs/2508.08074", "authors": ["Andrew Kang", "Yashnil Saha", "Sainyam Galhotra"], "title": "Towards General-Purpose Data Discovery: A Programming Languages Approach", "categories": ["cs.DB", "cs.PL"], "comment": null, "summary": "Efficient and effective data discovery is critical for many modern\napplications in machine learning and data science. One major bottleneck to the\ndevelopment of a general-purpose data discovery tool is the absence of an\nexpressive formal language, and corresponding implementation, for\ncharacterizing and solving generic discovery queries. To this end, we present\nTQL, a domain-specific language for data discovery well-designed to leverage\nand exploit the results of programming languages research in both its syntax\nand semantics. In this paper, we fully and formally characterize the core\nlanguage through an algebraic model, Imperative Relational Algebra with Types\n(ImpRAT), and implement a modular proof-of-concept system prototype.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTQL\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u7528\u4e8e\u9ad8\u6548\u6570\u636e\u53d1\u73b0\uff0c\u5e76\u57fa\u4e8eImpRAT\u4ee3\u6570\u6a21\u578b\u8fdb\u884c\u5f62\u5f0f\u5316\u63cf\u8ff0\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u79d1\u5b66\u5e94\u7528\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u53d1\u73b0\u5de5\u5177\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u7684\u5f62\u5f0f\u5316\u8bed\u8a00\u963b\u788d\u4e86\u8fd9\u4e00\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86TQL\u8bed\u8a00\uff0c\u5e76\u7ed3\u5408\u7f16\u7a0b\u8bed\u8a00\u7814\u7a76\u7684\u6210\u679c\uff0c\u63d0\u51faImpRAT\u4ee3\u6570\u6a21\u578b\u8fdb\u884c\u5f62\u5f0f\u5316\u63cf\u8ff0\u3002", "result": "\u5b9e\u73b0\u4e86\u6a21\u5757\u5316\u7684\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\u539f\u578b\uff0c\u5c55\u793a\u4e86TQL\u7684\u53ef\u884c\u6027\u3002", "conclusion": "TQL\u548cImpRAT\u4e3a\u901a\u7528\u6570\u636e\u53d1\u73b0\u5de5\u5177\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u652f\u6301\u3002"}}
{"id": "2508.08073", "pdf": "https://arxiv.org/pdf/2508.08073", "abs": "https://arxiv.org/abs/2508.08073", "authors": ["Dimitris Tsaras", "Xing Li", "Lei Chen", "Zhiyao Xie", "Mingxuan Yuan"], "title": "ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": "Accepted to DAC 2025", "summary": "In electronic design automation, logic optimization operators play a crucial\nrole in minimizing the gate count of logic circuits. However, their computation\ndemands are high. Operators such as refactor conventionally form iterative cuts\nfor each node, striving for a more compact representation - a task which often\nfails 98% on average. Prior research has sought to mitigate computational cost\nthrough parallelization. In contrast, our approach leverages a classifier to\nprune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis\noperations. Experiments on the refactor operator using the EPFL benchmark suite\nand 10 large industrial designs demonstrate that this technique can speedup\nlogic optimization by 3.9x on average compared with the state-of-the-art ABC\nimplementation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u903b\u8f91\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u526a\u679d\u65e0\u6548\u5207\u5272\uff0c\u663e\u8457\u63d0\u9ad8\u4f18\u5316\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u903b\u8f91\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0898% \u5931\u8d25\u7387\uff09\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5206\u7c7b\u5668\u9884\u5224\u5e76\u526a\u679d\u65e0\u6548\u5207\u5272\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u91cd\u65b0\u7efc\u5408\u64cd\u4f5c\u3002", "result": "\u5728 EPFL \u57fa\u51c6\u6d4b\u8bd5\u548c\u5de5\u4e1a\u8bbe\u8ba1\u4e2d\uff0c\u4f18\u5316\u901f\u5ea6\u63d0\u5347 3.9 \u500d\uff08\u4e0e ABC \u5b9e\u73b0\u76f8\u6bd4\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u526a\u679d\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u4e86\u903b\u8f91\u4f18\u5316\u6548\u7387\u3002"}}
{"id": "2508.07604", "pdf": "https://arxiv.org/pdf/2508.07604", "abs": "https://arxiv.org/abs/2508.07604", "authors": ["Maryam Abbasalizadeh", "Sashank Narain"], "title": "Joint Scheduling and Resource Allocation in mmWave IAB Networks Using Deep RL", "categories": ["cs.NI"], "comment": "Accepted at MILCOM 2025 (IEEE Military Communications Conference)", "summary": "Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond\ndeployments, especially in mmWave bands where fiber backhaul is infeasible. We\npropose a novel Deep Reinforcement Learning (DRL) framework for joint link\nscheduling and resource slicing in dynamic, interference-prone IAB networks.\nOur method integrates a greedy Double Deep Q-Network (DDQN) scheduler to\nactivate access and backhaul links based on traffic and topology, with a\nmulti-agent DDQN allocator for bandwidth and antenna assignment across network\nslices. This decentralized approach respects strict antenna constraints and\nsupports concurrent scheduling across heterogeneous links. Evaluations across\n96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent\nthroughput improvement over baselines. The framework's efficient operation and\nadaptability make it suitable for dynamic and resource-constrained deployments,\nwhere fast link scheduling and autonomous backhaul coordination are vital.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u3001\u6613\u53d7\u5e72\u6270\u7684IAB\u7f51\u7edc\u4e2d\u8054\u5408\u94fe\u8def\u8c03\u5ea6\u548c\u8d44\u6e90\u5207\u7247\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8c03\u5ea6\u51c6\u786e\u6027\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u57285G\u53ca\u66f4\u9ad8\u9891\u6bb5\u90e8\u7f72\u4e2d\uff0cIAB\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728mmWave\u9891\u6bb5\u5149\u7ea4\u56de\u4f20\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u9ad8\u6548\u7684\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u8d2a\u5fc3Double Deep Q-Network\u8c03\u5ea6\u5668\u548c\u591a\u4ee3\u7406DDQN\u5206\u914d\u5668\uff0c\u5b9e\u73b0\u5f02\u6784\u94fe\u8def\u4e0a\u7684\u8054\u5408\u8c03\u5ea6\u548c\u8d44\u6e90\u5206\u914d\uff0c\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u64cd\u4f5c\u3002", "result": "\u572896\u79cd\u52a8\u6001\u62d3\u6251\u4e2d\u6d4b\u8bd5\uff0c\u8c03\u5ea6\u7cbe\u5ea6\u8fbe99.84%\uff0c\u541e\u5410\u91cf\u63d0\u534720.90%\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u52a8\u6001\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2508.07834", "pdf": "https://arxiv.org/pdf/2508.07834", "abs": "https://arxiv.org/abs/2508.07834", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Lisa Bender", "Christian Weber", "Madjid Fathi"], "title": "KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations", "categories": ["cs.AI", "cs.ET"], "comment": "LWDA'23, KIRETT project, University of Siegen, Germany", "summary": "Over the years, the need for rescue operations throughout the world has\nincreased rapidly. Demographic changes and the resulting risk of injury or\nhealth disorders form the basis for emergency calls. In such scenarios, first\nresponders are in a rush to reach the patient in need, provide first aid, and\nsave lives. In these situations, they must be able to provide personalized and\noptimized healthcare in the shortest possible time and estimate the patients\ncondition with the help of freshly recorded vital data in an emergency\nsituation. However, in such a timedependent situation, first responders and\nmedical experts cannot fully grasp their knowledge and need assistance and\nrecommendation for further medical treatments. To achieve this, on the spot\ncalculated, evaluated, and processed knowledge must be made available to\nimprove treatments by first responders. The Knowledge Graph presented in this\narticle as a central knowledge representation provides first responders with an\ninnovative knowledge management that enables intelligent treatment\nrecommendations with an artificial intelligence-based pre-recognition of the\nsituation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548c\u4eba\u5de5\u667a\u80fd\u7684\u521b\u65b0\u77e5\u8bc6\u7ba1\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u652f\u6301\u6025\u6551\u4eba\u5458\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5feb\u901f\u63d0\u4f9b\u4e2a\u6027\u5316\u533b\u7597\u5efa\u8bae\u3002", "motivation": "\u5168\u7403\u6551\u63f4\u9700\u6c42\u7684\u589e\u52a0\u4ee5\u53ca\u6025\u6551\u4eba\u5458\u5728\u65f6\u95f4\u7d27\u8feb\u60c5\u51b5\u4e0b\u96be\u4ee5\u5145\u5206\u5229\u7528\u4e13\u4e1a\u77e5\u8bc6\u7684\u9700\u6c42\uff0c\u63a8\u52a8\u4e86\u672c\u6587\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u4e2d\u592e\u77e5\u8bc6\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5bf9\u6025\u6551\u573a\u666f\u8fdb\u884c\u9884\u8bc6\u522b\uff0c\u4ee5\u63d0\u4f9b\u667a\u80fd\u6cbb\u7597\u5efa\u8bae\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u5b9e\u65f6\u3001\u7ecf\u8fc7\u8ba1\u7b97\u548c\u8bc4\u4f30\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u4f18\u5316\u7d27\u6025\u533b\u7597\u5904\u7406\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u6025\u6551\u6548\u7387\u548c\u6cbb\u7597\u8d28\u91cf\u3002"}}
{"id": "2508.07966", "pdf": "https://arxiv.org/pdf/2508.07966", "abs": "https://arxiv.org/abs/2508.07966", "authors": ["Philipp Eibl", "Sadra Sabouri", "Souti Chattopadhyay"], "title": "Exploring the Challenges and Opportunities of AI-assisted Codebase Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Recent AI code assistants have significantly improved their ability to\nprocess more complex contexts and generate entire codebases based on a textual\ndescription, compared to the popular snippet-level generation. These codebase\nAI assistants (CBAs) can also extend or adapt codebases, allowing users to\nfocus on higher-level design and deployment decisions. While prior work has\nextensively studied the impact of snippet-level code generation, this new class\nof codebase generation models is relatively unexplored. Despite initial\nanecdotal reports of excitement about these agents, they remain less frequently\nadopted compared to snippet-level code assistants. To utilize CBAs better, we\nneed to understand how developers interact with CBAs, and how and why CBAs fall\nshort of developers' needs. In this paper, we explored these gaps through a\ncounterbalanced user study and interview with (n = 16) students and developers\nworking on coding tasks with CBAs. We found that participants varied the\ninformation in their prompts, like problem description (48% of prompts),\nrequired functionality (98% of prompts), code structure (48% of prompts), and\ntheir prompt writing process. Despite various strategies, the overall\nsatisfaction score with generated codebases remained low (mean = 2.8, median =\n3, on a scale of one to five). Participants mentioned functionality as the most\ncommon factor for dissatisfaction (77% of instances), alongside poor code\nquality (42% of instances) and communication issues (25% of instances). We\ndelve deeper into participants' dissatisfaction to identify six underlying\nchallenges that participants faced when using CBAs, and extracted five barriers\nto incorporating CBAs into their workflows. Finally, we surveyed 21 commercial\nCBAs to compare their capabilities with participant challenges and present\ndesign opportunities for more efficient and useful CBAs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7801\u5e93AI\u52a9\u624b\uff08CBAs\uff09\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5176\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5f00\u53d1\u8005\u7684\u6ee1\u610f\u5ea6\u8f83\u4f4e\uff0c\u4e3b\u8981\u539f\u56e0\u5305\u62ec\u529f\u80fd\u4e0d\u8db3\u3001\u4ee3\u7801\u8d28\u91cf\u5dee\u548c\u6c9f\u901a\u95ee\u9898\u3002", "motivation": "\u7814\u7a76CBAs\u5982\u4f55\u6ee1\u8db3\u5f00\u53d1\u8005\u9700\u6c42\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u4ee3\u7801\u5e93\u7ea7\u751f\u6210\u6a21\u578b\u5f71\u54cd\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u53cd\u5e73\u8861\u7528\u6237\u7814\u7a76\u548c\u8bbf\u8c08\uff08n = 16\uff09\uff0c\u5206\u6790\u5f00\u53d1\u8005\u4e0eCBAs\u7684\u4e92\u52a8\u65b9\u5f0f\u53ca\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u8005\u5bf9\u751f\u6210\u7684\u4ee3\u7801\u5e93\u6ee1\u610f\u5ea6\u4f4e\uff08\u5747\u52062.8/5\uff09\uff0c\u529f\u80fd\u4e0d\u8db3\u662f\u6700\u5e38\u89c1\u7684\u4e0d\u6ee1\u539f\u56e0\uff0c\u8fd8\u53d1\u73b0\u4e86\u516d\u7c7b\u6311\u6218\u548c\u4e94\u7c7b\u4f7f\u7528\u969c\u788d\u3002", "conclusion": "\u63d0\u51fa\u9700\u8981\u6539\u8fdbCBAs\u7684\u8bbe\u8ba1\u4ee5\u63d0\u5347\u5176\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u6bd4\u8f83\u4e8621\u6b3e\u5546\u4e1aCBAs\u7684\u80fd\u529b\u4e0e\u7528\u6237\u6311\u6218\u3002"}}
{"id": "2508.06826", "pdf": "https://arxiv.org/pdf/2508.06826", "abs": "https://arxiv.org/abs/2508.06826", "authors": ["Nels Numan", "Jessica Van Brummelen", "Ziwen Lu", "Anthony Steed"], "title": "AdjustAR: AI-Driven In-Situ Adjustment of Site-Specific Augmented Reality Content", "categories": ["cs.HC"], "comment": "4 pages, 1 figure, ACM UIST 2025 Poster", "summary": "Site-specific outdoor AR experiences are typically authored using static 3D\nmodels, but are deployed in physical environments that change over time. As a\nresult, virtual content may become misaligned with its intended real-world\nreferents, degrading user experience and compromising contextual\ninterpretation. We present AdjustAR, a system that supports in-situ correction\nof AR content in dynamic environments using multimodal large language models\n(MLLMs). Given a composite image comprising the originally authored view and\nthe current live user view from the same perspective, an MLLM detects\ncontextual misalignments and proposes revised 2D placements for affected AR\nelements. These corrections are backprojected into 3D space to update the scene\nat runtime. By leveraging MLLMs for visual-semantic reasoning, this approach\nenables automated runtime corrections to maintain alignment with the authored\nintent as real-world target environments evolve.", "AI": {"tldr": "AdjustAR\u7cfb\u7edf\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u52a8\u6001\u6821\u6b63AR\u5185\u5bb9\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u4ee5\u9002\u5e94\u5f53\u524d\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u9759\u60013D\u6a21\u578b\u7684AR\u5185\u5bb9\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7269\u7406\u73af\u5883\uff0c\u5bfc\u81f4\u865a\u62df\u5185\u5bb9\u4e0e\u5b9e\u9645\u573a\u666f\u9519\u4f4d\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "\u5229\u7528MLLM\u5206\u6790\u5408\u6210\u56fe\u50cf\u4e2d\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u63d0\u51fa2D\u6821\u6b63\u5efa\u8bae\uff0c\u5e76\u5c06\u5176\u53cd\u5411\u6295\u5f71\u52303D\u7a7a\u95f4\u4e2d\u5b9e\u65f6\u66f4\u65b0\u573a\u666f\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u5e76\u6821\u6b63AR\u5185\u5bb9\u7684\u9519\u4f4d\uff0c\u4fdd\u6301\u5185\u5bb9\u4e0e\u9884\u671f\u573a\u666f\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "AdjustAR\u8bc1\u660e\u4e86MLLM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7ef4\u62a4AR\u5185\u5bb9\u5bf9\u9f50\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.07817", "pdf": "https://arxiv.org/pdf/2508.07817", "abs": "https://arxiv.org/abs/2508.07817", "authors": ["Tao Tang", "Chengxu Yang"], "title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "6 pages, 6 figures", "summary": "The core role of medical images in disease diagnosis makes their quality\ndirectly affect the accuracy of clinical judgment. However, due to factors such\nas low-dose scanning, equipment limitations and imaging artifacts, medical\nimages are often accompanied by non-uniform noise interference, which seriously\naffects structure recognition and lesion detection. This paper proposes a\nmedical image adaptive denoising model (MI-ND) that integrates multi-scale\nconvolutional and Transformer architecture, introduces a noise level estimator\n(NLE) and a noise adaptive attention module (NAAB), and realizes\nchannel-spatial attention regulation and cross-modal feature fusion driven by\nnoise perception. Systematic testing is carried out on multimodal public\ndatasets. Experiments show that this method significantly outperforms the\ncomparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,\nand improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing\nstrong prac-tical value and promotional potential. The model has outstanding\nbenefits in structural recovery, diagnostic sensitivity, and cross-modal\nrobustness, and provides an effective solution for medical image enhancement\nand AI-assisted diagnosis and treatment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\u67b6\u6784\u7684\u533b\u5b66\u56fe\u50cf\u81ea\u9002\u5e94\u53bb\u566a\u6a21\u578b\uff08MI-ND\uff09\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u9a71\u52a8\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u8c03\u8282\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u4f4e\u5242\u91cf\u626b\u63cf\u3001\u8bbe\u5907\u9650\u5236\u548c\u6210\u50cf\u4f2a\u5f71\u7b49\u56e0\u7d20\uff0c\u56fe\u50cf\u5e38\u53d7\u975e\u5747\u5300\u566a\u58f0\u5e72\u6270\uff0c\u5f71\u54cd\u7ed3\u6784\u8bc6\u522b\u548c\u75c5\u53d8\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86MI-ND\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\uff0c\u5f15\u5165\u566a\u58f0\u6c34\u5e73\u4f30\u8ba1\u5668\uff08NLE\uff09\u548c\u566a\u58f0\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\uff08NAAB\uff09\uff0c\u5b9e\u73b0\u566a\u58f0\u611f\u77e5\u9a71\u52a8\u7684\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u8c03\u8282\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u6a21\u6001\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728PSNR\u3001SSIM\u548cLPIPS\u7b49\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u7684F1\u5206\u6570\u548cROC-AUC\u503c\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7ed3\u6784\u6062\u590d\u3001\u8bca\u65ad\u654f\u611f\u6027\u548c\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548cAI\u8f85\u52a9\u8bca\u7597\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u548c\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2508.07011", "pdf": "https://arxiv.org/pdf/2508.07011", "abs": "https://arxiv.org/abs/2508.07011", "authors": ["Zixiong Wang", "Jian Yang", "Yiwei Hu", "Milos Hasan", "Beibei Wang"], "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Creating highly detailed SVBRDFs is essential for 3D content creation. The\nrise of high-resolution text-to-image generative models, based on diffusion\ntransformers (DiT), suggests an opportunity to finetune them for this task.\nHowever, retargeting the models to produce multiple aligned SVBRDF maps instead\nof just RGB images, while achieving high efficiency and ensuring consistency\nacross different maps, remains a challenge. In this paper, we introduce HiMat:\na memory- and computation-efficient diffusion-based framework capable of\ngenerating native 4K-resolution SVBRDFs. A key challenge we address is\nmaintaining consistency across different maps in a lightweight manner, without\nrelying on training new VAEs or significantly altering the DiT backbone (which\nwould damage its prior capabilities). To tackle this, we introduce the\nCrossStitch module, a lightweight convolutional module that captures inter-map\ndependencies through localized operations. Its weights are initialized such\nthat the DiT backbone operation is unchanged before finetuning starts. HiMat\nenables generation with strong structural coherence and high-frequency details.\nResults with a large set of text prompts demonstrate the effectiveness of our\napproach for 4K SVBRDF generation. Further experiments suggest generalization\nto tasks such as intrinsic decomposition.", "AI": {"tldr": "HiMat\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7CrossStitch\u6a21\u5757\uff0c\u9ad8\u6548\u751f\u62104K\u5206\u8fa8\u7387SVBRDF\uff0c\u4fdd\u6301\u8de8\u56fe\u4e00\u81f4\u6027\uff0c\u4e0d\u635f\u574fDiT\u4e3b\u5e72\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9ad8\u5206\u8fa8\u7387\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\uff08\u5982\u57fa\u4e8eDiT\uff09\u96be\u4ee5\u9ad8\u6548\u751f\u6210\u591a\u5bf9\u9f50SVBRDF\u56fe\u4e14\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u9700\u89e3\u51b3\u6b64\u6311\u6218\u3002", "method": "\u63d0\u51faHiMat\u6846\u67b6\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7CrossStitch\u6a21\u5757\u6355\u83b7\u8de8\u56fe\u4f9d\u8d56\uff0c\u4fdd\u6301DiT\u4e3b\u5e72\u4e0d\u53d8\uff0c\u5b9e\u73b0\u9ad8\u65484K SVBRDF\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHiMat\u80fd\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u3001\u9ad8\u9891\u7ec6\u8282\u4e30\u5bcc\u76844K SVBRDF\uff0c\u6cdb\u5316\u80fd\u529b\u9002\u7528\u4e8e\u672c\u5f81\u5206\u89e3\u7b49\u4efb\u52a1\u3002", "conclusion": "HiMat\u4e3a\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cfSVBRDF\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86DiT\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2508.08172", "pdf": "https://arxiv.org/pdf/2508.08172", "abs": "https://arxiv.org/abs/2508.08172", "authors": ["Vincent Perreault", "Katsumi Inoue", "Richard Labib", "Alain Hertz"], "title": "Neural Logic Networks for Interpretable Classification", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "21 pages, 6 figures, pre-print", "summary": "Traditional neural networks have an impressive classification performance,\nbut what they learn cannot be inspected, verified or extracted. Neural Logic\nNetworks on the other hand have an interpretable structure that enables them to\nlearn a logical mechanism relating the inputs and outputs with AND and OR\noperations. We generalize these networks with NOT operations and biases that\ntake into account unobserved data and develop a rigorous logical and\nprobabilistic modeling in terms of concept combinations to motivate their use.\nWe also propose a novel factorized IF-THEN rule structure for the model as well\nas a modified learning algorithm. Our method improves the state-of-the-art in\nBoolean networks discovery and is able to learn relevant, interpretable rules\nin tabular classification, notably on an example from the medical field where\ninterpretability has tangible value.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u903b\u8f91\u7f51\u7edc\uff0c\u901a\u8fc7\u5f15\u5165NOT\u64cd\u4f5c\u548c\u504f\u5dee\u9879\u6765\u6539\u8fdb\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u56e0\u5b50\u5316IF-THEN\u89c4\u5219\u7ed3\u6784\u548c\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u5e03\u5c14\u7f51\u7edc\u53d1\u73b0\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5206\u7c7b\u6027\u80fd\u5f3a\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u795e\u7ecf\u903b\u8f91\u7f51\u7edc\u901a\u8fc7\u5b66\u4e60\u903b\u8f91\u673a\u5236\uff08AND/OR\u64cd\u4f5c\uff09\u63d0\u4f9b\u4e86\u89e3\u91ca\u6027\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u5904\u7406\u672a\u89c2\u5bdf\u6570\u636e\u548c\u66f4\u590d\u6742\u7684\u903b\u8f91\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5f15\u5165NOT\u64cd\u4f5c\u548c\u504f\u5dee\u9879\u6765\u6269\u5c55\u795e\u7ecf\u903b\u8f91\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86\u56e0\u5b50\u5316\u7684IF-THEN\u89c4\u5219\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6539\u8fdb\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u65b9\u6cd5\u5728\u5e03\u5c14\u7f51\u7edc\u53d1\u73b0\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8868\u683c\u5206\u7c7b\uff08\u5c24\u5176\u662f\u533b\u7597\u9886\u57df\uff09\u4e2d\u5b66\u4e60\u5230\u4e86\u76f8\u5173\u4e14\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u9700\u8981\u9ad8\u89e3\u91ca\u6027\u7684\u5e94\u7528\u573a\u666f\uff08\u5982\u533b\u7597\u9886\u57df\uff09\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2508.08076", "pdf": "https://arxiv.org/pdf/2508.08076", "abs": "https://arxiv.org/abs/2508.08076", "authors": ["Mohammad Hossein Moslemi", "Amir Mousavi", "Behshid Behkamal", "Mostafa Milani"], "title": "Heterogeneity in Entity Matching: A Survey and Experimental Analysis", "categories": ["cs.DB", "68P20 68P20 68P20", "H.2.8; H.2.4; I.2.7"], "comment": "Survey and experimental analysis on heterogeneous entity matching", "summary": "Entity matching (EM) is a fundamental task in data integration and analytics,\nessential for identifying records that refer to the same real-world entity\nacross diverse sources. In practice, datasets often differ widely in structure,\nformat, schema, and semantics, creating substantial challenges for EM. We refer\nto this setting as Heterogeneous EM (HEM). This survey offers a unified\nperspective on HEM by introducing a taxonomy, grounded in prior work, that\ndistinguishes two primary categories -- representation and semantic\nheterogeneity -- and their subtypes. The taxonomy provides a systematic lens\nfor understanding how variations in data form and meaning shape the complexity\nof matching tasks. We then connect this framework to the FAIR principles --\nFindability, Accessibility, Interoperability, and Reusability -- demonstrating\nhow they both reveal the challenges of HEM and suggest strategies for\nmitigating them. Building on this foundation, we critically review recent EM\nmethods, examining their ability to address different heterogeneity types, and\nconduct targeted experiments on state-of-the-art models to evaluate their\nrobustness and adaptability under semantic heterogeneity. Our analysis uncovers\npersistent limitations in current approaches and points to promising directions\nfor future research, including multimodal matching, human-in-the-loop\nworkflows, deeper integration with large language models and knowledge graphs,\nand fairness-aware evaluation in heterogeneous settings.", "AI": {"tldr": "\u5b9e\u4f53\u5339\u914d\uff08EM\uff09\u662f\u6570\u636e\u96c6\u6210\u548c\u5206\u6790\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u4f46\u5728\u5f02\u6784\u6570\u636e\uff08HEM\uff09\u73af\u5883\u4e0b\u5b58\u5728\u6311\u6218\u3002\u672c\u6587\u901a\u8fc7\u5206\u7c7b\u6cd5\u7cfb\u7edf\u5206\u6790\u4e86HEM\u7684\u4e24\u5927\u7c7b\u522b\u2014\u2014\u8868\u793a\u548c\u8bed\u4e49\u5f02\u8d28\u6027\uff0c\u5e76\u8fde\u63a5FAIR\u539f\u5219\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5f02\u6784\u6570\u636e\u96c6\u5728\u7ed3\u6784\u3001\u683c\u5f0f\u3001\u6a21\u5f0f\u548c\u8bed\u4e49\u4e0a\u7684\u5de8\u5927\u5dee\u5f02\u7ed9\u5b9e\u4f53\u5339\u914d\uff08EM\uff09\u5e26\u6765\u4e86\u663e\u8457\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u548c\u7b56\u7565\u5206\u6790\uff0c\u4e3aHEM\u63d0\u4f9b\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u5e76\u63a8\u52a8\u672a\u6765\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8868\u793a\u548c\u8bed\u4e49\u5f02\u8d28\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u5176\u4e0eFAIR\u539f\u5219\u5173\u8054\u4ee5\u63ed\u793a\u6311\u6218\u548c\u7b56\u7565\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u6700\u65b0EM\u65b9\u6cd5\u5728\u8bed\u4e49\u5f02\u8d28\u6027\u4e0b\u7684\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5e94\u5bf9HEM\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u591a\u6a21\u6001\u5339\u914d\u3001\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u6d41\u3001\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u4ee5\u53ca\u516c\u5e73\u6027\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f02\u6784\u5b9e\u4f53\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u591a\u6a21\u6001\u6574\u5408\u548c\u516c\u5e73\u6027\u4f18\u5316\u7b49\u65b9\u5411\uff0c\u4ee5\u63d0\u5347\u65b9\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2508.07703", "pdf": "https://arxiv.org/pdf/2508.07703", "abs": "https://arxiv.org/abs/2508.07703", "authors": ["Adri Bhattacharya", "Pritam Goswami", "Evangelos Bampas", "Partha Sarathi Mandal"], "title": "Perpetual exploration in anonymous synchronous networks with a Byzantine black hole", "categories": ["cs.DC", "cs.MA"], "comment": "This paper has been accepted at DISC 2025", "summary": "In this paper, we investigate: ``How can a group of initially co-located\nmobile agents perpetually explore an unknown graph, when one stationary node\noccasionally behaves maliciously, under an adversary's control?'' We call this\nnode a ``Byzantine black hole (BBH)'' and at any given round it may choose to\ndestroy all visiting agents, or none. This subtle power can drastically\nundermine classical exploration strategies designed for an always active black\nhole. We study this perpetual exploration problem in the presence of at most\none BBH, without initial knowledge of the network size. Since the underlying\ngraph may be 1-connected, perpetual exploration of the entire graph may be\ninfeasible. We thus define two variants: \\pbmPerpExpl\\ and \\pbmPerpExplHome. In\nthe former, the agents are tasked to perform perpetual exploration of at least\none component, obtained after the exclusion of the BBH. In the latter, the\nagents are tasked to perform perpetual exploration of the component which\ncontains the \\emph{home} node, where agents are initially co-located.\nNaturally, \\pbmPerpExplHome\\ is a special case of \\pbmPerpExpl. Agents operate\nunder a synchronous scheduler and communicate in a face-to-face model. Our goal\nis to determine the minimum number of agents necessary and sufficient to solve\nthese problems. In acyclic networks, we obtain optimal algorithms that solve\n\\pbmPerpExpl\\ with $4$ agents, and \\pbmPerpExplHome\\ with $6$ agents in trees.\nThe lower bounds hold even in path graphs. In general graphs, we give a\nnon-trivial lower bound of $2\\Delta-1$ agents for \\pbmPerpExpl, and an upper\nbound of $3\\Delta+3$ agents for \\pbmPerpExplHome. To our knowledge, this is the\nfirst study of a black-hole variant in arbitrary networks without initial\ntopological knowledge.", "AI": {"tldr": "\u7814\u7a76\u5728\u672a\u77e5\u56fe\u4e2d\uff0c\u4e00\u7ec4\u521d\u59cb\u5171\u4f4d\u7684\u79fb\u52a8\u4ee3\u7406\u5982\u4f55\u5728\u5b58\u5728\u4e00\u4e2a\u53ef\u80fd\u6076\u610f\u7684\u9759\u6b62\u8282\u70b9\uff08\u79f0\u4e3a\u300e\u62dc\u5360\u5ead\u9ed1\u6d1e\u300f\uff09\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u63a2\u7d22\u56fe\u3002\u5b9a\u4e49\u4e86\u4e24\u79cd\u63a2\u7d22\u53d8\u4f53\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u6811\u5f62\u548c\u4e00\u822c\u56fe\u4e2d\u89e3\u51b3\u95ee\u9898\u6240\u9700\u7684\u6700\u5c0f\u4ee3\u7406\u6570\u91cf\u3002", "motivation": "\u63a2\u7d22\u5728\u6076\u610f\u8282\u70b9\u5b58\u5728\u4e0b\u7684\u6301\u7eed\u56fe\u63a2\u7d22\u95ee\u9898\uff0c\u586b\u8865\u4e86\u7f3a\u4e4f\u521d\u59cb\u62d3\u6251\u77e5\u8bc6\u7684\u4efb\u610f\u7f51\u7edc\u4e2d\u9ed1\u6d1e\u53d8\u4f53\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u4e86\u4e24\u79cd\u63a2\u7d22\u53d8\u4f53\uff08PerpExpl\u548cPerpExplHome\uff09\uff0c\u7814\u7a76\u4e86\u5728\u540c\u6b65\u8c03\u5ea6\u5668\u548c\u9762\u5bf9\u9762\u901a\u4fe1\u6a21\u578b\u4e0b\u7684\u4ee3\u7406\u6570\u91cf\u9700\u6c42\u3002", "result": "\u5728\u6811\u5f62\u7f51\u7edc\u4e2d\uff0c\u6700\u4f18\u4ee3\u7406\u6570\u4e3a4\uff08PerpExpl\uff09\u548c6\uff08PerpExplHome\uff09\uff1b\u5728\u4e00\u822c\u56fe\u4e2d\uff0c\u4e0b\u754c\u4e3a2\u0394-1\uff0c\u4e0a\u754c\u4e3a3\u0394+3\u3002", "conclusion": "\u89e3\u51b3\u4e86\u521d\u59cb\u62d3\u6251\u77e5\u8bc6\u7f3a\u5931\u60c5\u51b5\u4e0b\u62dc\u5360\u5ead\u9ed1\u6d1e\u56fe\u7684\u6301\u7eed\u63a2\u7d22\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u4ee3\u7406\u6570\u91cf\u754c\u9650\u3002"}}
{"id": "2508.07679", "pdf": "https://arxiv.org/pdf/2508.07679", "abs": "https://arxiv.org/abs/2508.07679", "authors": ["Tong Zhang", "Yu Gou", "Jun Liu", "Shanshan Song", "Tingting Yang", "Jun-Hong Cui"], "title": "Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks", "categories": ["cs.NI"], "comment": "Accepted by IEEE Transactions on Mobile Computing", "summary": "Underwater wireless sensor networks (UWSNs) stand as promising technologies\nfacilitating diverse underwater applications. However, the major design issues\nof the considered system are the severely limited energy supply and unexpected\nnode malfunctions. This paper aims to provide fair, efficient, and reliable\n(FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs).\nTherefore, we formulate a FER-communication optimization problem (FERCOP) and\npropose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep\nmulti-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through\njoint link scheduling and power allocation, which automatically learns\nscheduling algorithms without human intervention. However, conventional RL\nmethods are unable to address the challenges posed by underwater environments\nand IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs\nand propose an advanced training mechanism to deal with complex acoustic\nchannels, limited energy supplies, and unexpected node malfunctions. Simulation\nresults demonstrate the superiority of the proposed ICRL-JSA scheme with an\nadvanced training mechanism compared to various benchmark algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICRL-JSA\u7684\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5668\uff0c\u65e8\u5728\u4e3a\u80fd\u91cf\u53d7\u9650\u548c\u8282\u70b9\u6545\u969c\u9891\u7e41\u7684\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08IC-UWSNs\uff09\u63d0\u4f9b\u516c\u5e73\u3001\u9ad8\u6548\u548c\u53ef\u9760\u7684\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u94fe\u8def\u8c03\u5ea6\u548c\u529f\u7387\u5206\u914d\u5b9e\u73b0\u76ee\u6807\u3002", "motivation": "\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08UWSNs\uff09\u5728\u80fd\u6e90\u4f9b\u5e94\u53d7\u9650\u548c\u8282\u70b9\u610f\u5916\u6545\u969c\u65b9\u9762\u5b58\u5728\u91cd\u5927\u8bbe\u8ba1\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u7684\u901a\u4fe1\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u4f18\u5316\u5668ICRL-JSA\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6Q\u7f51\u7edc\u548c\u5148\u8fdb\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u8054\u5408\u94fe\u8def\u8c03\u5ea6\u548c\u529f\u7387\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cICRL-JSA\u65b9\u6848\u5728\u5404\u79cd\u57fa\u51c6\u7b97\u6cd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "ICRL-JSA\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u8c03\u5ea6\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6c34\u4e0b\u73af\u5883\u548cIC-UWSNs\u5e26\u6765\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2508.08029", "pdf": "https://arxiv.org/pdf/2508.08029", "abs": "https://arxiv.org/abs/2508.08029", "authors": ["Thusitha Dayaratne", "Ngoc Duy Pham", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks", "categories": ["cs.CR", "cs.ET", "cs.LG"], "comment": null, "summary": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e865G\u548cO-RAN\u67b6\u6784\u4e2d\u7684\u65b0\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u6076\u610fxApp\u901a\u8fc7Unicode\u653b\u51fb\uff08\u4f4e\u8840\u7cd6\uff09\u7ed5\u8fc7\u4f20\u7edfML\u5f02\u5e38\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528LLM\u63d0\u5347\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "O-RAN\u67b6\u6784\u7684\u5f00\u653e\u6027\u548c\u590d\u6742\u6027\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6311\u6218\uff0c\u5982\u6076\u610fxApp\u5229\u7528Unicode\u653b\u51fb\u7ed5\u8fc7\u4f20\u7edfML\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684xApp\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u6d4b\u8bd5\u5176\u5bf9Unicode\u653b\u51fb\uff08\u4f4e\u8840\u7cd6\uff09\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "result": "LLM\u80fd\u5904\u7406\u88ab\u653b\u51fb\u7684\u6570\u636e\u4e14\u4e0d\u5d29\u6e83\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\uff08<0.07\u79d2\uff09\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\uff0c\u4f46\u521d\u59cb\u68c0\u6d4b\u7cbe\u5ea6\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "LLM\u5728\u5bf9\u6297Unicode\u653b\u51fb\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.08171", "pdf": "https://arxiv.org/pdf/2508.08171", "abs": "https://arxiv.org/abs/2508.08171", "authors": ["Pedro Orvalho", "Marta Kwiatkowska"], "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages, 6 tables, 1 figure", "summary": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PyVeritas\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06Python\u4ee3\u7801\u8f6c\u8bd1\u4e3aC\u4ee3\u7801\uff0c\u7136\u540e\u7ed3\u5408\u6a21\u578b\u68c0\u67e5\u5de5\u5177\u8fdb\u884c\u9a8c\u8bc1\u548c\u6545\u969c\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86Python\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8ePython\u7f3a\u4e4f\u6210\u719f\u7684\u6a21\u578b\u68c0\u67e5\u5de5\u5177\uff0c\u800cC\u8bed\u8a00\u5219\u6709\u6210\u719f\u7684\u5de5\u5177\uff08\u5982CBMC\uff09\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7LLM\u5c06Python\u4ee3\u7801\u8f6c\u8bd1\u4e3aC\u4ee3\u7801\uff0c\u4ece\u800c\u5229\u7528C\u7684\u5de5\u5177\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "method": "PyVeritas\u6846\u67b6\u901a\u8fc7LLMs\u5c06Python\u4ee3\u7801\u8f6c\u8bd1\u4e3aC\u4ee3\u7801\uff0c\u968f\u540e\u4f7f\u7528\u6709\u754c\u6a21\u578b\u68c0\u67e5\u548cMaxSAT\u6280\u672f\u8fdb\u884c\u6545\u969c\u5b9a\u4f4d\u3002", "result": "\u5728\u4e24\u4e2aPython\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u8f6c\u8bd1\u7684\u51c6\u786e\u7387\u53ef\u8fbe80-90%\uff0c\u652f\u6301\u65ad\u8a00\u9a8c\u8bc1\u548c\u6545\u969c\u8bca\u65ad\u3002", "conclusion": "PyVeritas\u4e3aPython\u4ee3\u7801\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u6545\u969c\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5c0f\u89c4\u6a21\u4f46\u975e\u5e73\u51e1\u7684Python\u7a0b\u5e8f\u3002"}}
{"id": "2508.06846", "pdf": "https://arxiv.org/pdf/2508.06846", "abs": "https://arxiv.org/abs/2508.06846", "authors": ["Hyo Jin Do", "Rachel Ostrand", "Werner Geyer", "Keerthiram Murugesan", "Dennis Wei", "Justin Weisz"], "title": "Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators", "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 8 figures, To be published in Proceedings of the 8th\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Large language models (LLMs) are susceptible to generating inaccurate or\nfalse information, often referred to as \"hallucinations\" or \"confabulations.\"\nWhile several technical advancements have been made to detect hallucinated\ncontent by assessing the factuality of the model's responses, there is still\nlimited research on how to effectively communicate this information to users.\nTo address this gap, we conducted two scenario-based experiments with a total\nof 208 participants to systematically compare the effects of various design\nstrategies for communicating factuality scores by assessing participants'\nratings of trust, ease in validating response accuracy, and preference. Our\nfindings reveal that participants preferred and trusted a design in which all\nphrases within a response were color-coded based on factuality scores.\nParticipants also found it easier to validate accuracy of the response in this\nstyle compared to a baseline with no style applied. Our study offers practical\ndesign guidelines for LLM application developers and designers, aimed at\ncalibrating user trust, aligning with user preferences, and enhancing users'\nability to scrutinize LLM outputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u6237\u66f4\u559c\u6b22\u901a\u8fc7\u989c\u8272\u7f16\u7801\u663e\u793a\u4e8b\u5b9e\u6027\u7684\u8bbe\u8ba1\uff0c\u8fd9\u63d0\u9ad8\u4e86\u4fe1\u4efb\u5ea6\u548c\u51c6\u786e\u6027\u9a8c\u8bc1\u7684\u4fbf\u6377\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u5bb9\u6613\u751f\u6210\u4e0d\u51c6\u786e\u4fe1\u606f\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u5411\u7528\u6237\u4f20\u8fbe\u4fe1\u606f\u771f\u5b9e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u573a\u666f\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bbe\u8ba1\u7b56\u7565\u5bf9\u7528\u6237\u4fe1\u4efb\u3001\u9a8c\u8bc1\u4fbf\u6377\u6027\u548c\u504f\u597d\u7684\u5f71\u54cd\u3002", "result": "\u989c\u8272\u7f16\u7801\u7684\u8bbe\u8ba1\u6700\u53d7\u7528\u6237\u4fe1\u4efb\u548c\u559c\u7231\uff0c\u4e14\u9a8c\u8bc1\u51c6\u786e\u6027\u66f4\u4fbf\u6377\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u6307\u5357\uff0c\u4ee5\u6821\u51c6\u7528\u6237\u4fe1\u4efb\u5e76\u589e\u5f3a\u5bf9\u8f93\u51fa\u7684\u5ba1\u67e5\u80fd\u529b\u3002"}}
{"id": "2508.08039", "pdf": "https://arxiv.org/pdf/2508.08039", "abs": "https://arxiv.org/abs/2508.08039", "authors": ["Shu Wu", "Chenxing Li", "Wenfu Wang", "Hao Zhang", "Hualei Wang", "Meng Yu", "Dong Yu"], "title": "Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "preprint", "summary": "Recent advancements in large language models, multimodal large language\nmodels, and large audio language models (LALMs) have significantly improved\ntheir reasoning capabilities through reinforcement learning with rule-based\nrewards. However, the explicit reasoning process has yet to show significant\nbenefits for audio question answering, and effectively leveraging deep\nreasoning remains an open challenge, with LALMs still falling short of\nhuman-level auditory-language reasoning. To address these limitations, we\npropose Audio-Thinker, a reinforcement learning framework designed to enhance\nthe reasoning capabilities of LALMs, with a focus on improving adaptability,\nconsistency, and effectiveness. Our approach introduces an adaptive think\naccuracy reward, enabling the model to adjust its reasoning strategies based on\ntask complexity dynamically. Furthermore, we incorporate an external reward\nmodel to evaluate the overall consistency and quality of the reasoning process,\ncomplemented by think-based rewards that help the model distinguish between\nvalid and flawed reasoning paths during training. Experimental results\ndemonstrate that our Audio-Thinker model outperforms existing\nreasoning-oriented LALMs across various benchmark tasks, exhibiting superior\nreasoning and generalization capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAudio-Thinker\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7b56\u7565\u548c\u5916\u90e8\u5956\u52b1\u6a21\u578b\u6765\u63d0\u9ad8\u5176\u9002\u5e94\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u5c1a\u4e0d\u663e\u8457\uff0c\u4e14\u4e0e\u4eba\u7c7b\u542c\u89c9-\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u5b58\u5728\u5dee\u8ddd\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faAudio-Thinker\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u601d\u7ef4\u51c6\u786e\u6027\u5956\u52b1\u548c\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u52a8\u6001\u8c03\u6574\u63a8\u7406\u7b56\u7565\u5e76\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAudio-Thinker\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u63a8\u7406\u5bfc\u5411\u7684LALMs\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Audio-Thinker\u901a\u8fc7\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LALMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u542c\u89c9-\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07760", "pdf": "https://arxiv.org/pdf/2508.07760", "abs": "https://arxiv.org/abs/2508.07760", "authors": ["Maximilian Kromer", "Panagiotis Agrafiotis", "Beg\u00fcm Demir"], "title": "Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "Under review in IEEE Geoscience and Remote Sensing Letters", "summary": "Accurate image-based bathymetric mapping in shallow waters remains\nchallenging due to the complex optical distortions such as wave induced\npatterns, scattering and sunglint, introduced by the dynamic water surface, the\nwater column properties, and solar illumination. In this work, we introduce\nSea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512\nthrough-water scenes rendered in Blender. Each pair comprises a distortion-free\nand a distorted view, featuring realistic water effects such as sun glint,\nwaves, and scattering over diverse seabeds. Accompanied by per-image metadata\nsuch as camera parameters, sun position, and average depth, Sea-Undistort\nenables supervised training that is otherwise infeasible in real environments.\nWe use Sea-Undistort to benchmark two state-of-the-art image restoration\nmethods alongside an enhanced lightweight diffusion-based framework with an\nearly-fusion sun-glint mask. When applied to real aerial data, the enhanced\ndiffusion model delivers more complete Digital Surface Models (DSMs) of the\nseabed, especially in deeper areas, reduces bathymetric errors, suppresses\nglint and scattering, and crisply restores fine seabed details. Dataset,\nweights, and code are publicly available at\nhttps://www.magicbathy.eu/Sea-Undistort.html.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aSea-Undistort\u7684\u7efc\u5408\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u6d45\u6c34\u533a\u56fe\u50cf\u6d4b\u6df1\u4e2d\u7684\u5149\u5b66\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u6d45\u6c34\u533a\u56fe\u50cf\u6d4b\u6df1\u56e0\u52a8\u6001\u6c34\u9762\u548c\u6c34\u67f1\u6027\u8d28\u5f15\u8d77\u7684\u5149\u5b66\u5931\u771f\uff08\u5982\u6ce2\u6d6a\u3001\u6563\u5c04\u548c\u592a\u9633\u5149\u6655\uff09\u800c\u56f0\u96be\u3002", "method": "\u4f7f\u7528Blender\u751f\u62101200\u5bf9512x512\u7684\u6c34\u4e0b\u573a\u666f\u56fe\u50cf\uff08\u5931\u771f\u548c\u672a\u5931\u771f\uff09\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6062\u590d\u3002", "result": "\u6a21\u578b\u5728\u771f\u5b9e\u822a\u7a7a\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u9ad8\u4e86\u6d77\u5e95\u6570\u5b57\u8868\u9762\u6a21\u578b\u7684\u5b8c\u6574\u6027\uff0c\u51cf\u5c11\u4e86\u6d4b\u6df1\u8bef\u5dee\u3002", "conclusion": "Sea-Undistort\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u6d45\u6c34\u533a\u56fe\u50cf\u6d4b\u6df1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07744", "pdf": "https://arxiv.org/pdf/2508.07744", "abs": "https://arxiv.org/abs/2508.07744", "authors": ["Ingo Friese", "Jochen Klaffer", "Mandy Galkow-Schneider", "Sergiy Melnyk", "Qiuheng Zhou", "Hans Dieter Schotten"], "title": "Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure", "categories": ["cs.DC", "cs.NI", "eess.SP"], "comment": null, "summary": "6G network architectures will usher in a wave of innovative services and\ncapabilities, introducing concepts like split computing and dynamic processing\nnodes. This implicates a paradigm where accessing resources seamlessly aligns\nwith diverse processing node characteristics, ensuring a uniform interface. In\nthis landscape, the identity of the operator becomes inconsequential, paving\nthe way for a collaborative ecosystem where multiple providers contribute to a\nshared pool of resources. At the core of this vision is the guarantee of\nspecific performance parameters, precisely tailored to the location and service\nrequirements. A consistent layer, as the abstraction of the complexities of\ndifferent infrastructure providers, is needed to simplify service deployment.\nOne promising approach is the introduction of an over-the-top broker for\nresource allocation, which streamlines the integration of these services into\nthe network and cloud infrastructure of the future. This paper explores the\nrole of the broker in two split computing scenarios. By abstracting the\ncomplexities of various infrastructures, the broker proves to be a versatile\nsolution applicable not only to cloud environments but also to networks and\nbeyond. Additionally, a detailed discussion of a proof-of-concept\nimplementation provides insights into the broker's actual architectural\nframework.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e866G\u7f51\u7edc\u4e2d\u901a\u8fc7\u5f15\u5165\u8d44\u6e90\u5206\u914d\u4e2d\u95f4\u4ee3\u7406\uff08broker\uff09\u6765\u7b80\u5316\u670d\u52a1\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5206\u7247\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "6G\u7f51\u7edc\u5c06\u5e26\u6765\u521b\u65b0\u7684\u670d\u52a1\u548c\u80fd\u529b\uff0c\u4f46\u9700\u8981\u7edf\u4e00\u7684\u63a5\u53e3\u548c\u8d44\u6e90\u5171\u4eab\u673a\u5236\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u62bd\u8c61\u5316\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u8d44\u6e90\u8bbf\u95ee\u6210\u4e3a\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2d\u95f4\u4ee3\u7406\uff08broker\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u62bd\u8c61\u5316\u4e0d\u540c\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u6027\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e2d\u95f4\u4ee3\u7406\u5728\u5206\u7247\u8ba1\u7b97\u7b49\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u6574\u5408\u672a\u6765\u7f51\u7edc\u548c\u4e91\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u4e2d\u95f4\u4ee3\u7406\u662f6G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u8d44\u6e90\u5171\u4eab\u548c\u670d\u52a1\u90e8\u7f72\u7b80\u5316\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07778", "pdf": "https://arxiv.org/pdf/2508.07778", "abs": "https://arxiv.org/abs/2508.07778", "authors": ["Farhad Rezazadeh", "Raymond Zhao", "Jiongyu Dai", "Amir Ashtari Gargari", "Hatim Chergui", "Lingjia Liu"], "title": "An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study", "categories": ["cs.NI"], "comment": "5 pages, 2 figures", "summary": "Next-generation open radio access networks (O-RAN) continuously stream tens\nof key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)\nsamples, yielding ultra-high-dimensional, non-stationary time series that\noverwhelm conventional transformer architectures. We introduce a\nreservoir-augmented masked autoencoding transformer (RA-MAT). This time series\nfoundation model employs echo state network (ESN) computing with masked\nautoencoding to satisfy the stringent latency, energy efficiency, and\nscalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESN\nrapidly projects each temporal patch into a rich dynamical embedding without\nbackpropagation through time, converting the quadratic self-attention\nbottleneck into a lightweight linear operation. These embeddings drive a\npatch-wise masked autoencoder that reconstructs 30% randomly masked patches,\ncompelling the encoder to capture both local dynamics and long-range structure\nfrom unlabeled data. After self-supervised pre-training, RA-MAT is fine-tuned\nwith a shallow task head while keeping the reservoir and most transformer\nlayers frozen, enabling low-footprint adaptation to diverse downstream tasks\nsuch as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MAT\nachieved sub-0.06 mean squared error (MSE) on several continuous and discrete\nKPIs. This work positions RA-MAT as a practical pathway toward real-time,\nfoundation-level analytics in future 6G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e6G O-RAN\u6d4b\u8bd5\u7684\u65b0\u578b\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578bRA-MAT\uff0c\u7ed3\u5408\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u548c\u81ea\u7f16\u7801\u5668\uff0c\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u7387\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u53d8\u6362\u5668\u5728\u5904\u7406O-RAN\u8d85\u9ad8\u7ef4\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u968f\u673a\u521d\u59cb\u5316\u7684ESN\u5feb\u901f\u6295\u5f71\u65f6\u95f4\u7247\uff0c\u7ed3\u5408\u63a9\u7801\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u540e\u901a\u8fc7\u5c11\u91cf\u8c03\u9002\u5b8c\u6210\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5728O-RAN KPI\u9884\u6d4b\u4e2d\u8fbe\u5230\u4f4e\u4e8e0.06\u7684MSE\u3002", "conclusion": "RA-MAT\u4e3a6G\u7f51\u7edc\u7684\u5b9e\u65f6\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.06668", "pdf": "https://arxiv.org/pdf/2508.06668", "abs": "https://arxiv.org/abs/2508.06668", "authors": ["Jessie Galasso"], "title": "Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis", "categories": ["cs.AI", "cs.IR", "cs.SE"], "comment": null, "summary": "Formal Concept Analysis (FCA) is a mathematical framework for knowledge\nrepresentation and discovery. It performs a hierarchical clustering over a set\nof objects described by attributes, resulting in conceptual structures in which\nobjects are organized depending on the attributes they share. These conceptual\nstructures naturally highlight commonalities and variabilities among similar\nobjects by categorizing them into groups which are then arranged by similarity,\nmaking it particularly appropriate for variability extraction and analysis.\nDespite the potential of FCA, determining which of its properties can be\nleveraged for variability-related tasks (and how) is not always\nstraightforward, partly due to the mathematical orientation of its foundational\nliterature. This paper attempts to bridge part of this gap by gathering a\nselection of properties of the framework which are essential to variability\nanalysis, and how they can be used to interpret diverse variability information\nwithin the resulting conceptual structures.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u5728\u53d8\u5f02\u6027\u5206\u6790\u4e2d\u7684\u5173\u952e\u6027\u8d28\u53ca\u5176\u5e94\u7528\u65b9\u6cd5\u3002", "motivation": "FCA\u867d\u7136\u9002\u5408\u53d8\u5f02\u6027\u63d0\u53d6\u548c\u5206\u6790\uff0c\u4f46\u7531\u4e8e\u5176\u6570\u5b66\u57fa\u7840\u6587\u732e\u7684\u590d\u6742\u6027\uff0c\u5982\u4f55\u5229\u7528\u5176\u6027\u8d28\u8fdb\u884c\u53d8\u5f02\u6027\u76f8\u5173\u4efb\u52a1\u5e76\u4e0d\u76f4\u89c2\u3002", "method": "\u6536\u96c6\u5e76\u6574\u7406\u4e86FCA\u6846\u67b6\u4e2d\u4e0e\u53d8\u5f02\u6027\u5206\u6790\u76f8\u5173\u7684\u5173\u952e\u6027\u8d28\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u6027\u8d28\u89e3\u91ca\u6982\u5ff5\u7ed3\u6784\u4e2d\u7684\u53d8\u5f02\u6027\u4fe1\u606f\u3002", "result": "\u660e\u786e\u4e86FCA\u5728\u53d8\u5f02\u6027\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u53ca\u5176\u6027\u8d28\u7684\u5e94\u7528\u65b9\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u603b\u7ed3FCA\u7684\u6027\u8d28\uff0c\u4e3a\u53d8\u5f02\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u548c\u5de5\u5177\u3002"}}
{"id": "2508.06872", "pdf": "https://arxiv.org/pdf/2508.06872", "abs": "https://arxiv.org/abs/2508.06872", "authors": ["Danyang Fan", "Walker Smith", "Takako Fujioka", "Chris Chage", "Sile O'Modhrain", "Diana Deutsch", "Sean Follmer"], "title": "Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions", "categories": ["cs.HC"], "comment": null, "summary": "Sonification offers a non-visual way to understand data, with pitch-based\nencodings being the most common. Yet, how well people perceive slope and\nacceleration-key features of data trends-remains poorly understood. Drawing on\npeople's natural abilities to perceive tempo, we introduce a novel sampling\nmethod for pitch-based sonification to enhance the perception of slope and\nacceleration in univariate functions. While traditional sonification methods\noften sample data at uniform x-spacing, yielding notes played at a fixed tempo\nwith variable pitch intervals (Variable Pitch Interval), our approach samples\nat uniform y-spacing, producing notes with consistent pitch intervals but\nvariable tempo (Variable Tempo). We conducted psychoacoustic experiments to\nunderstand slope and acceleration perception across three sampling methods:\nVariable Pitch Interval, Variable Tempo, and a Continuous (no sampling)\nbaseline. In slope comparison tasks, Variable Tempo was more accurate than the\nother methods when modulated by the magnitude ratio between slopes. For\nacceleration perception, just-noticeable differences under Variable Tempo were\nover 13 times finer than with other methods. Participants also commonly\nreported higher confidence, lower mental effort, and a stronger preference for\nVariable Tempo compared to other methods. This work contributes models of slope\nand acceleration perception across pitch-based sonification techniques,\nintroduces Variable Tempo as a novel and preferred sampling method, and\nprovides promising initial evidence that leveraging timing can lead to more\nsensitive, accurate, and precise interpretation of derivative-based data\nfeatures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5747\u5300y\u95f4\u8ddd\u91c7\u6837\u7684\u65b0\u578b\u58f0\u5316\u65b9\u6cd5\uff08Variable Tempo\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u611f\u77e5\u6570\u636e\u8d8b\u52bf\u7684\u659c\u7387\u548c\u52a0\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u5747\u5300x\u95f4\u8ddd\u91c7\u6837\u65b9\u6cd5\uff08Variable Pitch Interval\uff09\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u901a\u8fc7\u58f0\u5316\u6280\u672f\u611f\u77e5\u6570\u636e\u8d8b\u52bf\u7684\u5173\u952e\u7279\u5f81\uff08\u5982\u659c\u7387\u548c\u52a0\u901f\u5ea6\uff09\uff0c\u5229\u7528\u4eba\u4eec\u5bf9\u8282\u594f\u7684\u81ea\u7136\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u91c7\u6837\u65b9\u6cd5\uff08Variable Tempo\uff09\uff0c\u901a\u8fc7\u5747\u5300y\u95f4\u8ddd\u91c7\u6837\u4ea7\u751f\u4e00\u81f4\u7684\u97f3\u9ad8\u95f4\u9694\u4f46\u53ef\u53d8\u7684\u8282\u594f\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u5747\u5300x\u95f4\u8ddd\u91c7\u6837\u65b9\u6cd5\uff08Variable Pitch Interval\uff09\u548c\u65e0\u91c7\u6837\u57fa\u7ebf\uff08Continuous\uff09\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVariable Tempo\u5728\u659c\u7387\u6bd4\u8f83\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u51c6\u786e\uff0c\u52a0\u901f\u5ea6\u611f\u77e5\u7684\u6700\u5c0f\u53ef\u89c9\u5dee\u6bd4\u5176\u4ed6\u65b9\u6cd5\u7cbe\u7ec613\u500d\u4ee5\u4e0a\uff0c\u53c2\u4e0e\u8005\u5bf9\u5176\u4fe1\u5fc3\u66f4\u9ad8\u3001\u8ba4\u77e5\u8d1f\u8377\u66f4\u4f4e\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u8282\u594f\u7684Variable Tempo\u65b9\u6cd5\u5728\u611f\u77e5\u6570\u636e\u8d8b\u52bf\u7684\u659c\u7387\u548c\u52a0\u901f\u5ea6\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u58f0\u5316\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u654f\u611f\u3001\u51c6\u786e\u548c\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08179", "pdf": "https://arxiv.org/pdf/2508.08179", "abs": "https://arxiv.org/abs/2508.08179", "authors": ["Sihan Zhao", "Zixuan Wang", "Tianyu Luan", "Jia Jia", "Wentao Zhu", "Jiebo Luo", "Junsong Yuan", "Nan Xi"], "title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ACM Multimedia 2025", "summary": "Human motion generation has found widespread applications in AR/VR, film,\nsports, and medical rehabilitation, offering a cost-effective alternative to\ntraditional motion capture systems. However, evaluating the fidelity of such\ngenerated motions is a crucial, multifaceted task. Although previous approaches\nhave attempted at motion fidelity evaluation using human perception or physical\nconstraints, there remains an inherent gap between human-perceived fidelity and\nphysical feasibility. Moreover, the subjective and coarse binary labeling of\nhuman perception further undermines the development of a robust data-driven\nmetric. We address these issues by introducing a physical labeling method. This\nmethod evaluates motion fidelity by calculating the minimum modifications\nneeded for a motion to align with physical laws. With this approach, we are\nable to produce fine-grained, continuous physical alignment annotations that\nserve as objective ground truth. With these annotations, we propose PP-Motion,\na novel data-driven metric to evaluate both physical and perceptual fidelity of\nhuman motion. To effectively capture underlying physical priors, we employ\nPearson's correlation loss for the training of our metric. Additionally, by\nincorporating a human-based perceptual fidelity loss, our metric can capture\nfidelity that simultaneously considers both human perception and physical\nalignment. Experimental results demonstrate that our metric, PP-Motion, not\nonly aligns with physical laws but also aligns better with human perception of\nmotion fidelity than previous work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPP-Motion\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u6807\u8bb0\u548c\u611f\u77e5\u635f\u5931\u7ed3\u5408\uff0c\u8bc4\u4f30\u4eba\u7c7b\u8fd0\u52a8\u751f\u6210\u7684\u903c\u771f\u5ea6\uff0c\u5728\u7269\u7406\u53ef\u884c\u6027\u548c\u4eba\u7c7b\u611f\u77e5\u4e24\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u8fd0\u52a8\u751f\u6210\u903c\u771f\u5ea6\u65f6\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u7269\u7406\u53ef\u884c\u6027\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u5ba2\u89c2\u3001\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u63d0\u51fa\u7269\u7406\u6807\u8bb0\u65b9\u6cd5\uff0c\u8ba1\u7b97\u8fd0\u52a8\u5bf9\u9f50\u7269\u7406\u6cd5\u5219\u6240\u9700\u7684\u6700\u5c0f\u4fee\u6539\uff1b\u7ed3\u5408Pearson\u76f8\u5173\u6027\u635f\u5931\u548c\u4eba\u7c7b\u611f\u77e5\u635f\u5931\uff0c\u8bad\u7ec3PP-Motion\u6a21\u578b\u3002", "result": "PP-Motion\u5728\u7269\u7406\u548c\u611f\u77e5\u903c\u771f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "PP-Motion\u4e3a\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u5ba2\u89c2\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7ed3\u5408\u4e86\u7269\u7406\u53ef\u884c\u6027\u548c\u4eba\u7c7b\u611f\u77e5\u7684\u903c\u771f\u5ea6\u3002"}}
{"id": "2508.08086", "pdf": "https://arxiv.org/pdf/2508.08086", "abs": "https://arxiv.org/abs/2508.08086", "authors": ["Zhongqi Yang", "Wenhang Ge", "Yuqi Li", "Jiaqi Chen", "Haoyuan Li", "Mengyin An", "Fei Kang", "Hua Xue", "Baixin Xu", "Yuyang Yin", "Eric Li", "Yang Liu", "Yikai Wang", "Hao-Xiang Guo", "Yahui Zhou"], "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "categories": ["cs.CV", "cs.GR"], "comment": "Technical Report", "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.", "AI": {"tldr": "Matrix-3D \u662f\u4e00\u4e2a\u901a\u8fc7\u5168\u666f\u8868\u793a\u751f\u6210\u53ef\u63a2\u7d22 3D \u4e16\u754c\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6761\u4ef6\u89c6\u9891\u751f\u6210\u548c\u5168\u666f 3D \u91cd\u5efa\uff0c\u5e76\u5728\u6027\u80fd\u548c\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u573a\u666f\u8303\u56f4\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u53ef\u63a2\u7d22 3D \u4e16\u754c\u7684\u5e7f\u5ea6\u548c\u8d28\u91cf\u3002", "method": "1. \u8bad\u7ec3\u8f68\u8ff9\u5f15\u5bfc\u7684\u5168\u666f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff1b2. \u63d0\u51fa\u4e24\u79cd 3D \u91cd\u5efa\u65b9\u6cd5\uff1a\u524d\u9988\u5927\u8303\u56f4\u91cd\u5efa\u548c\u4f18\u5316\u7ec6\u8282\u91cd\u5efa\uff1b3. \u5f15\u5165\u6570\u636e\u96c6 Matrix-Pano\u3002", "result": "\u5728\u5168\u666f\u89c6\u9891\u751f\u6210\u548c 3D \u4e16\u754c\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "Matrix-3D \u6846\u67b6\u901a\u8fc7\u5168\u666f\u8868\u793a\u548c\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86 3D \u4e16\u754c\u751f\u6210\u7684\u8986\u76d6\u8303\u56f4\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.07179", "pdf": "https://arxiv.org/pdf/2508.07179", "abs": "https://arxiv.org/abs/2508.07179", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u63d0\u53d6\u591a\u8bed\u8a00\u4f01\u4e1a\u7ba1\u9053\u811a\u672c\u4e2d\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u8c31\u7cfb\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u63d0\u793a\u6280\u672f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f01\u4e1a\u6570\u636e\u7ba1\u9053\u4e2d\u590d\u6742\u7684\u591a\u8bed\u8a00\u8f6c\u6362\u5bfc\u81f4\u8bed\u4e49\u6f02\u79fb\uff0c\u5f71\u54cd\u6570\u636e\u53ef\u91cd\u73b0\u6027\u548c\u6cbb\u7406\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u53d6\u6a21\u5f0f\u8c31\u7cfb\u7684\u6846\u67b6\uff0c\u5305\u62ec\u6e90\u6a21\u5f0f\u3001\u6e90\u8868\u3001\u8f6c\u6362\u903b\u8f91\u548c\u805a\u5408\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165SLiCE\u8bc4\u4f30\u6307\u6807\u548c\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u5f0f\u8c31\u7cfb\u63d0\u53d6\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u548c\u63d0\u793a\u6280\u672f\u7684\u63d0\u5347\u800c\u63d0\u9ad8\uff0c32B\u5f00\u6e90\u6a21\u578b\u5728\u5355\u63a8\u7406\u8f68\u8ff9\u4e0b\u6027\u80fd\u63a5\u8fd1GPT\u7cfb\u5217\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u6a21\u5f0f\u611f\u77e5\u4ee3\u7406\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07756", "pdf": "https://arxiv.org/pdf/2508.07756", "abs": "https://arxiv.org/abs/2508.07756", "authors": ["Hanze Zhang", "Rong Chen", "Haibo Chen"], "title": "Towards Lock Modularization for Heterogeneous Environments", "categories": ["cs.DC"], "comment": null, "summary": "Modern hardware environments are becoming increasingly heterogeneous, leading\nto the emergence of applications specifically designed to exploit this\nheterogeneity. Efficiently adopting locks in these applications poses distinct\nchallenges. The uneven distribution of resources in such environments can\ncreate bottlenecks for lock operations, severely hindering application\nperformance. Existing solutions are often tailored to specific types of\nhardware, which underutilizes resources on other components within\nheterogeneous environments.\n  This paper introduces a new design principle: decomposing locks across\nhardware components to fully utilize unevenly distributed resources in\nheterogeneous environments. Following this principle, we propose lock\nmodularization, a systematic approach that decomposes a lock into independent\nmodules and assigns them to appropriate hardware components. This approach\naligns the resource requirements of lock modules with the attributes of\nspecific hardware components, maximizing strengths while minimizing weaknesses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9501\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7\u5c06\u9501\u6a21\u5757\u5316\u5e76\u5206\u914d\u5230\u5f02\u6784\u786c\u4ef6\u7684\u4e0d\u540c\u7ec4\u4ef6\u4e0a\uff0c\u4ee5\u5145\u5206\u5229\u7528\u8d44\u6e90\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f02\u6784\u786c\u4ef6\u73af\u5883\u4e2d\u8d44\u6e90\u5206\u5e03\u4e0d\u5747\uff0c\u4f20\u7edf\u9501\u8bbe\u8ba1\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8d44\u6e90\uff0c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u9501\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u5c06\u9501\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\u5e76\u5206\u914d\u5230\u9002\u5408\u7684\u786c\u4ef6\u7ec4\u4ef6\u4e0a\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5339\u914d\u786c\u4ef6\u7279\u6027\uff0c\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u9501\u6a21\u5757\u5316\u662f\u89e3\u51b3\u5f02\u6784\u786c\u4ef6\u73af\u5883\u4e2d\u9501\u6027\u80fd\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.07882", "pdf": "https://arxiv.org/pdf/2508.07882", "abs": "https://arxiv.org/abs/2508.07882", "authors": ["Conor Muldoon"], "title": "Scalable and Energy-Efficient Predictive Data Collection in Wireless Sensor Networks with Constructive Interference", "categories": ["cs.NI"], "comment": null, "summary": "A new class of Wireless Sensor Network has emerged whereby multiple nodes\ntransmit data simultaneously, exploiting constructive interference to enable\ndata collection frameworks with low energy usage and latency. This paper\npresents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a\nscalable, resilient framework for Wireless Sensor Networks that leverages\nconstructive interference and operates effectively under stringent resource\nconstraints. Using constructive interference requires all nodes to transmit the\nsame packet at the same time, thus, only one source node can send data per time\nslot. STAIR uses coarse-grained topology information to flood a selected subset\nof the network, relaying sensor readings from individual nodes during their\nallocated time slots. A submodular optimisation algorithm with proven quality\nbounds determines near-optimal sensor activation locations and times, aiming to\nminimise the sum of mean squared prediction errors from a multiple multivariate\nlinear regression model, which is used to estimate values at unselected\nlocations and times. This framework has been extensively validated on a\nreal-world testbed deployment.", "AI": {"tldr": "STAIR\u662f\u4e00\u79cd\u5229\u7528\u5efa\u8bbe\u6027\u5e72\u6270\u7684\u4f4e\u80fd\u8017\u3001\u4f4e\u5ef6\u8fdf\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u6a21\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u4f20\u611f\u5668\u6fc0\u6d3b\u4f4d\u7f6e\u548c\u65f6\u95f4\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u591a\u8282\u70b9\u540c\u65f6\u4f20\u8f93\u6570\u636e\u7684\u80fd\u91cf\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u5efa\u8bbe\u6027\u5e72\u6270\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7c97\u7cd9\u7684\u62d3\u6251\u4fe1\u606f\u9009\u62e9\u5b50\u7f51\uff0c\u901a\u8fc7\u5b50\u6a21\u4f18\u5316\u7b97\u6cd5\u786e\u5b9a\u4f20\u611f\u5668\u6fc0\u6d3b\u4f4d\u7f6e\u548c\u65f6\u95f4\uff0c\u4ee5\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\u3002", "result": "\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86STAIR\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "STAIR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u7684\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u6846\u67b6\u3002"}}
{"id": "2508.07203", "pdf": "https://arxiv.org/pdf/2508.07203", "abs": "https://arxiv.org/abs/2508.07203", "authors": ["Prashant Sharma"], "title": "Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools", "categories": ["cs.HC", "cs.CR", "cs.SE"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "Current digital government literature focuses on professional in-house IT\nteams, specialized digital service teams, vendor-developed systems, or\nproprietary low-code/no-code tools. Almost no scholarship addresses a growing\nmiddle ground: technically skilled civil servants outside formal IT roles who\ncan write real code but lack a sanctioned, secure path to deploy their work.\nThis paper introduces a limits-aware, open-source and replicable platform that\nenables such public servants to develop, peer review, and deploy small-scale,\ndomain-specific applications within government networks via a sandboxed,\nauditable workflow. By combining Jupyter Notebooks, preapproved open-source\nlibraries, and lightweight governance, the platform works within institutional\nconstraints such as procurement rules and IT security policies while avoiding\nvendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil\nservants' programming skills, keeping them technically competitive with their\nprivate-sector peers. This contribution fills a critical gap, offering a\nreplicable model for public-sector skill retention, resilience, and bottom-up\ndigital transformation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u7684\u3001\u53ef\u590d\u5236\u7684\u5e73\u53f0\uff0c\u5141\u8bb8\u975eIT\u90e8\u95e8\u7684\u516c\u52a1\u5458\u5728\u653f\u5e9c\u7f51\u7edc\u4e2d\u5f00\u53d1\u548c\u90e8\u7f72\u5c0f\u89c4\u6a21\u5e94\u7528\u7a0b\u5e8f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u5b57\u653f\u5e9c\u6587\u732e\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u653f\u5e9c\u7814\u7a76\u96c6\u4e2d\u5728\u4e13\u4e1aIT\u56e2\u961f\u6216\u4f9b\u5e94\u5546\u5f00\u53d1\u7684\u7cfb\u7edf\u4e0a\uff0c\u5ffd\u89c6\u4e86\u5177\u5907\u7f16\u7801\u80fd\u529b\u4f46\u7f3a\u4e4f\u6b63\u5f0f\u90e8\u7f72\u9014\u5f84\u7684\u516c\u52a1\u5458\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408Jupyter Notebooks\u3001\u9884\u6279\u51c6\u7684\u5f00\u6e90\u5e93\u548c\u8f7b\u91cf\u7ea7\u6cbb\u7406\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6c99\u76d2\u5316\u3001\u53ef\u5ba1\u8ba1\u7684\u5de5\u4f5c\u6d41\u7a0b\u5e73\u53f0\u3002", "result": "\u8be5\u5e73\u53f0\u5728\u9075\u5b88\u673a\u6784\u7ea6\u675f\uff08\u5982\u91c7\u8d2d\u89c4\u5219\u548c\u5b89\u5168\u653f\u7b56\uff09\u7684\u540c\u65f6\uff0c\u907f\u514d\u4e86\u4f9b\u5e94\u5546\u9501\u5b9a\uff0c\u5e76\u4fdd\u7559\u4e86\u516c\u52a1\u5458\u7684\u7f16\u7a0b\u6280\u80fd\u3002", "conclusion": "\u8fd9\u4e00\u5e73\u53f0\u4e3a\u516c\u5171\u90e8\u95e8\u7684\u6280\u80fd\u4fdd\u7559\u3001\u97e7\u6027\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u6570\u5b57\u5316\u8f6c\u578b\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6a21\u578b\u3002"}}
{"id": "2508.06889", "pdf": "https://arxiv.org/pdf/2508.06889", "abs": "https://arxiv.org/abs/2508.06889", "authors": ["Dooyoung Kim", "Jinseok Hong", "Heejeong Ko", "Woontack Woo"], "title": "Viewpoint-Tolerant Depth Perception for Shared Extended Space Experience on Wall-Sized Display", "categories": ["cs.HC"], "comment": "11 pages, 5 figures, 3 tables, Accepted in TVCG Special Issue on the\n  2025 IEEE Symposium on Mixed and Augmented Reality (IEEE ISMAR)", "summary": "We proposed viewpoint-tolerant shared depth perception without individual\ntracking by leveraging human cognitive compensation in universally 3D rendered\nimages on a wall-sized display. While traditional 3D perception-enabled display\nsystems have primarily focused on single-user scenarios-adapting rendering\nbased on head and eye tracking the use of wall-sized displays to extend spatial\nexperiences and support perceptually coherent multi-user interactions remains\nunderexplored. We investigated the effects of virtual depths (dv) and absolute\nviewing distance (da) on human cognitive compensation factors (perceived\ndistance difference, viewing angle threshold, and perceived presence) to\nconstruct the wall display-based eXtended Reality (XR) space. Results show that\nparticipants experienced a compelling depth perception even from off-center\nangles of 23 to 37 degrees, and largely increasing virtual depth worsens depth\nperception and presence factors, highlighting the importance of balancing\nextended depth of virtual space and viewing distance from the wall-sized\ndisplay. Drawing on these findings, wall-sized displays in venues such as\nmuseums, galleries, and classrooms can evolve beyond 2D information sharing to\noffer immersive, spatially extended group experiences without individualized\ntracking or wearables.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5899\u5f0f\u663e\u793a\u5c4f\u7684XR\u7a7a\u95f4\uff0c\u901a\u8fc7\u4eba\u7c7b\u8ba4\u77e5\u8865\u507f\u5b9e\u73b0\u591a\u7528\u6237\u5171\u4eab\u6df1\u5ea6\u611f\u77e5\uff0c\u65e0\u9700\u4e2a\u4f53\u8ffd\u8e2a\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u865a\u62df\u6df1\u5ea6\u4e0e\u89c2\u770b\u8ddd\u79bb\u7684\u5e73\u8861\u5bf9\u6df1\u5ea6\u611f\u77e5\u548c\u4e34\u573a\u611f\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u4f20\u7edf3D\u663e\u793a\u7cfb\u7edf\u591a\u9488\u5bf9\u5355\u7528\u6237\uff0c\u4f9d\u8d56\u5934\u90e8\u548c\u773c\u7403\u8ffd\u8e2a\u3002\u5899\u5f0f\u663e\u793a\u5c4f\u5728\u591a\u7528\u6237\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4e86\u865a\u62df\u6df1\u5ea6(dv)\u548c\u7edd\u5bf9\u89c2\u770b\u8ddd\u79bb(da)\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u8865\u507f\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u611f\u77e5\u8ddd\u79bb\u5dee\u5f02\u3001\u89c6\u89d2\u9608\u503c\u548c\u4e34\u573a\u611f\u3002", "result": "\u7528\u6237\u53ef\u572823\u81f337\u5ea6\u7684\u504f\u79bb\u4e2d\u5fc3\u89d2\u5ea6\u4f53\u9a8c\u6df1\u5ea6\u611f\u77e5\uff0c\u4f46\u865a\u62df\u6df1\u5ea6\u8fc7\u5927\u65f6\u4f1a\u964d\u4f4e\u611f\u77e5\u6548\u679c\u548c\u4e34\u573a\u611f\u3002", "conclusion": "\u5899\u5f0f\u663e\u793a\u5c4f\u53ef\u5728\u535a\u7269\u9986\u3001\u753b\u5eca\u548c\u6559\u5ba4\u7b49\u573a\u6240\u63d0\u4f9b\u6c89\u6d78\u5f0f\u591a\u7528\u6237\u4f53\u9a8c\uff0c\u65e0\u9700\u4e2a\u4f53\u8ffd\u8e2a\u6216\u7a7f\u6234\u8bbe\u5907\u3002"}}
{"id": "2508.07934", "pdf": "https://arxiv.org/pdf/2508.07934", "abs": "https://arxiv.org/abs/2508.07934", "authors": ["Lorenzo La Corte", "Syed Aftab Rashid", "Andrei-Marian Dan"], "title": "Performance Evaluation of Brokerless Messaging Libraries", "categories": ["cs.DC", "cs.NI"], "comment": "11 pages, 9 figures", "summary": "Messaging systems are essential for efficiently transferring large volumes of\ndata, ensuring rapid response times and high-throughput communication. The\nstate-of-the-art on messaging systems mainly focuses on the performance\nevaluation of brokered messaging systems, which use an intermediate broker to\nguarantee reliability and quality of service. However, over the past decade,\nbrokerless messaging systems have emerged, eliminating the single point of\nfailure and trading off reliability guarantees for higher performance. Still,\nthe state-of-the-art on evaluating the performance of brokerless systems is\nscarce. In this work, we solely focus on brokerless messaging systems. First,\nwe perform a qualitative analysis of several possible candidates, to find the\nmost promising ones. We then design and implement an extensive open-source\nbenchmarking suite to systematically and fairly evaluate the performance of the\nchosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).\nWe evaluate these libraries considering different metrics and workload\nconditions, and provide useful insights into their limitations. Our analysis\nenables practitioners to select the most suitable library for their\nrequirements.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u6765\u8bc4\u4f30ZeroMQ\u3001NanoMsg\u548cNNG\u7b49\u5e93\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u9009\u62e9\u4f9d\u636e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6709\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u800c\u7f3a\u4e4f\u5bf9\u65e0\u4ee3\u7406\u7cfb\u7edf\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u7b5b\u9009\u51fa\u6709\u524d\u666f\u7684\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u5e93\uff0c\u7136\u540e\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30ZeroMQ\u3001NanoMsg\u548cNNG\u5728\u4e0d\u540c\u6307\u6807\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e9b\u5e93\u6027\u80fd\u7684\u8be6\u7ec6\u8bc4\u4f30\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u5c40\u9650\u6027\uff0c\u5e2e\u52a9\u5b9e\u8df5\u8005\u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u5e93\u3002", "conclusion": "\u672c\u6587\u4e3a\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u5de5\u5177\u548c\u89c1\u89e3\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.07978", "pdf": "https://arxiv.org/pdf/2508.07978", "abs": "https://arxiv.org/abs/2508.07978", "authors": ["Hamidreza Mazandarani", "Mohammad Farhoudi", "Masoud Shokrnezhad", "Tarik Taleb"], "title": "Adaptive Multiple Access and Service Placement for Generative Diffusion Models", "categories": ["cs.NI"], "comment": "This manuscript has been accepted for presentation at IEEE GLOBECOM\n  2025. You can use this material personally. Reprinting or republishing this\n  material for the purpose of advertising or promotion, etc., must adhere to\n  IEEE policy. The DOI will be supplied as soon as it becomes available", "summary": "Generative Diffusion Models (GDMs) have emerged as key components of\nGenerative Artificial Intelligence (GenAI), offering unparalleled\nexpressiveness and controllability for complex data generation tasks. However,\ntheir deployment in real-time and mobile environments remains challenging due\nto the iterative and resource-intensive nature of the inference process.\nAddressing these challenges, this paper introduces a unified optimization\nframework that jointly tackles service placement and multiple access control\nfor GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement\nLearning-based algorithm that dynamically partitions denoising blocks across\nheterogeneous edge nodes, while accounting for latent transmission costs and\nenabling adaptive reduction of inference steps. Our approach integrates a\ngreedy multiple access scheme with a Double and Dueling Deep Q-Learning\n(D3QL)-based service placement, allowing for scalable, adaptable, and\nresource-efficient operation under stringent quality of service requirements.\nSimulations demonstrate the superior performance of the proposed framework in\nterms of scalability and latency resilience compared to conventional monolithic\nand fixed chain-length placement strategies. This work advances the state of\nthe art in edge-enabled GenAI by offering an adaptable solution for GDM\nservices orchestration, paving the way for future extensions toward semantic\nnetworking and co-inference across distributed environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEARN-GDM\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u5206\u914d\u53bb\u566a\u4efb\u52a1\uff0c\u89e3\u51b3\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u79fb\u52a8\u8fb9\u7f18\u7f51\u7edc\u4e2d\u7684\u90e8\u7f72\u6311\u6218\u3002", "motivation": "\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u5b9e\u65f6\u548c\u79fb\u52a8\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u8fed\u4ee3\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faLEARN-GDM\u7b97\u6cd5\uff0c\u7ed3\u5408\u8d2a\u5fc3\u591a\u5740\u63a5\u5165\u548cD3QL-based\u670d\u52a1\u653e\u7f6e\uff0c\u52a8\u6001\u5206\u914d\u53bb\u566a\u4efb\u52a1\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u5f39\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8fb9\u7f18\u751f\u6210AI\u63d0\u4f9b\u4e86\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8bed\u4e49\u7f51\u7edc\u548c\u534f\u540c\u63a8\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07468", "pdf": "https://arxiv.org/pdf/2508.07468", "abs": "https://arxiv.org/abs/2508.07468", "authors": ["Stefan Szeider"], "title": "CP-Agent: Agentic Constraint Programming", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u56fa\u5b9a\u6d41\u7a0b\u7684\u7eaf\u4ee3\u7406\u7b56\u7565\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u8f6c\u5316\u4e3a\u7ea6\u675f\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u7f16\u7801\u5de5\u5177\u548c\u9886\u57df\u77e5\u8bc6\u63d0\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6240\u6709101\u4e2a\u57fa\u51c6\u95ee\u9898\u3002", "motivation": "\u7ea6\u675f\u7f16\u7a0b\u4e2d\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u8f6c\u6362\u4e3a\u5f62\u5f0f\u5316\u7ea6\u675f\u6a21\u578b\u9700\u8981\u6df1\u539a\u7684\u9886\u57df\u548c\u5efa\u6a21\u77e5\u8bc6\uff0c\u73b0\u6709\u56fa\u5b9a\u6d41\u7a0b\u65b9\u6cd5\u5728\u8bb8\u591a\u57fa\u51c6\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eReAct\u539f\u5219\u7684\u901a\u7528Python\u7f16\u7801\u4ee3\u7406\uff0c\u5229\u7528\u6301\u4e45IPython\u5185\u6838\u8fdb\u884c\u72b6\u6001\u5316\u4ee3\u7801\u6267\u884c\u548c\u8fed\u4ee3\u5f00\u53d1\uff0c\u901a\u8fc7\u9879\u76ee\u63d0\u793a\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u800c\u975e\u5d4c\u5165\u4ee3\u7406\u67b6\u6784\u3002", "result": "\u8be5\u67b6\u6784\u5728\u4ec5\u51e0\u767e\u884c\u4ee3\u7801\u4e0b\uff0c\u6210\u529f\u89e3\u51b3\u4e86CP-Bench\u57fa\u51c6\u96c6\u4e2d\u7684\u6240\u6709101\u4e2a\u95ee\u9898\u3002", "conclusion": "\u7ea6\u675f\u5efa\u6a21\u4efb\u52a1\u4f9d\u8d56\u901a\u7528\u7f16\u7801\u5de5\u5177\u548c\u63d0\u793a\u4e2d\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u800c\u975e\u4e13\u7528\u4ee3\u7406\u67b6\u6784\u6216\u9884\u5b9a\u4e49\u6d41\u7a0b\u3002"}}
{"id": "2508.06955", "pdf": "https://arxiv.org/pdf/2508.06955", "abs": "https://arxiv.org/abs/2508.06955", "authors": ["Kyuwon Kim", "Jaeryeong Hwang", "Younseo Lee", "Jeanhee Lee", "Sung-Eun Kimm", "Hyo-Jeong So"], "title": "Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent", "categories": ["cs.HC"], "comment": null, "summary": "As complex societal issues continue to emerge, fostering democratic skills\nlike valuing diverse perspectives and collaborative decision-making is\nincreasingly vital in education. In this paper, we propose a Peer Agent (PA)\nsystem designed to simulate a deliberative conversational partner that induces\nsocio-cognitive conflict within dilemma-based game play. Drawing on by the\nInner Thoughts framework and grounded in value-sensitive discourse analysis,\nthe PA actively participates in voice-based multi-party deliberation with human\nplayers. The system architecture consists of five core modules: Context\nInterpreter, Agent State Manager, Thought Generator, Thought Evaluator, and\nThought Articulator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdPeer Agent\uff08PA\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u5bf9\u8bdd\u4f19\u4f34\u5728\u6e38\u620f\u60c5\u5883\u4e2d\u8bf1\u5bfc\u793e\u4f1a\u8ba4\u77e5\u51b2\u7a81\uff0c\u4ee5\u57f9\u517b\u6c11\u4e3b\u6280\u80fd\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u793e\u4f1a\u95ee\u9898\uff0c\u9700\u8981\u57f9\u517b\u6559\u80b2\u7684\u6c11\u4e3b\u6280\u80fd\uff0c\u5982\u5c0a\u91cd\u591a\u5143\u89c2\u70b9\u548c\u534f\u4f5c\u51b3\u7b56\u3002", "method": "\u57fa\u4e8eInner Thoughts\u6846\u67b6\u548c\u4ef7\u503c\u89c2\u654f\u611f\u7684\u8bdd\u8bed\u5206\u6790\uff0c\u8bbe\u8ba1\u4e86PA\u7cfb\u7edf\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u6a21\u5757\u3002", "result": "PA\u7cfb\u7edf\u80fd\u6709\u6548\u53c2\u4e0e\u591a\u4eba\u8bed\u97f3\u5ba1\u8bae\uff0c\u8bf1\u5bfc\u793e\u4f1a\u8ba4\u77e5\u51b2\u7a81\u3002", "conclusion": "PA\u7cfb\u7edf\u4e3a\u57f9\u517b\u6c11\u4e3b\u6280\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.08061", "pdf": "https://arxiv.org/pdf/2508.08061", "abs": "https://arxiv.org/abs/2508.08061", "authors": ["Sven Weinzierl", "Sandra Zilker", "Annina Liessmann", "Martin K\u00e4ppel", "Weixin Wang", "Martin Matzner"], "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations", "categories": ["cs.LG", "cs.CL", "cs.DB"], "comment": null, "summary": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\uff08PPM\uff09\u6280\u672f\uff0c\u4f7f\u7f3a\u4e4f\u8db3\u591f\u4e8b\u4ef6\u6570\u636e\u6216\u5176\u4ed6\u8d44\u6e90\u7684\u7ec4\u7ec7\u4e5f\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002\u901a\u8fc7\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u9a8c\u8bc1\u4e86\u6280\u672f\u5728\u7ec4\u7ec7\u5185\u548c\u7ec4\u7ec7\u95f4\u8fc1\u79fb\u77e5\u8bc6\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709PPM\u6280\u672f\u9700\u8981\u5927\u91cf\u4e8b\u4ef6\u6570\u636e\u6216\u5176\u4ed6\u8d44\u6e90\uff0c\u4e00\u4e9b\u7ec4\u7ec7\u56e0\u8d44\u6e90\u4e0d\u8db3\u65e0\u6cd5\u4f7f\u7528\u8be5\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684PPM\u6280\u672f\uff0c\u5229\u7528\u6e90\u8fc7\u7a0b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e3a\u76ee\u6807\u8fc7\u7a0b\u63d0\u4f9b\u652f\u6301\u3002\u901a\u8fc7\u4e24\u4e2a\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7ec4\u7ec7\u5185\u548c\u7ec4\u7ec7\u95f4\uff0c\u6e90\u8fc7\u7a0b\u7684\u77e5\u8bc6\u53ef\u4ee5\u8fc1\u79fb\u5230\u7c7b\u4f3c\u7684\u76ee\u6807\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u6709\u6548\u9884\u6d4b\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u7ec4\u7ec7\u63d0\u4f9b\u4e86PPM\u7684\u53ef\u80fd\u6027\uff0c\u62d3\u5bbd\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2508.08022", "pdf": "https://arxiv.org/pdf/2508.08022", "abs": "https://arxiv.org/abs/2508.08022", "authors": ["Roopkatha Banerjee", "Sampath Koti", "Gyanendra Singh", "Anirban Chakraborty", "Gurunath Gurrala", "Bhushan Jagyasi", "Yogesh Simmhan"], "title": "Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Real-time monitoring of power consumption in cities and micro-grids through\nthe Internet of Things (IoT) can help forecast future demand and optimize grid\noperations. But moving all consumer-level usage data to the cloud for\npredictions and analysis at fine time scales can expose activity patterns.\nFederated Learning~(FL) is a privacy-sensitive collaborative DNN training\napproach that retains data on edge devices, trains the models on private data\nlocally, and aggregates the local models in the cloud. But key challenges\nexist: (i) clients can have non-independently identically distributed~(non-IID)\ndata, and (ii) the learning should be computationally cheap while scaling to\n1000s of (unseen) clients. In this paper, we develop and evaluate several\noptimizations to FL training across edge and cloud for time-series demand\nforecasting in micro-grids and city-scale utilities using DNNs to achieve a\nhigh prediction accuracy while minimizing the training cost. We showcase the\nbenefit of using exponentially weighted loss while training and show that it\nfurther improves the prediction of the final model. Finally, we evaluate these\nstrategies by validating over 1000s of clients for three states in the US from\nthe OpenEIA corpus, and performing FL both in a pseudo-distributed setting and\na Pi edge cluster. The results highlight the benefits of the proposed methods\nover baselines like ARIMA and DNNs trained for individual consumers, which are\nnot scalable.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u7269\u8054\u7f51\uff08IoT\uff09\u548c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5b9e\u65f6\u76d1\u6d4b\u57ce\u5e02\u548c\u5fae\u7535\u7f51\u7684\u7535\u529b\u6d88\u8017\uff0c\u63d0\u51fa\u4f18\u5316FL\u8bad\u7ec3\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u901a\u8fc7IoT\u5b9e\u65f6\u76d1\u6d4b\u7535\u529b\u6d88\u8017\u9700\u8981\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0cFL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u654f\u611f\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u51e0\u79cdFL\u8bad\u7ec3\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528\u6307\u6570\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u8fb9\u7f18\u8bbe\u5907\u548c\u4e91\u7aef\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u7684FL\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982ARIMA\u548c\u5355\u4e2a\u6d88\u8d39\u8005\u7684DNN\uff09\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f18\u5316\u7684FL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7535\u529b\u9700\u6c42\u9884\u6d4b\u4e2d\u7684\u9690\u79c1\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u3002"}}
{"id": "2508.08225", "pdf": "https://arxiv.org/pdf/2508.08225", "abs": "https://arxiv.org/abs/2508.08225", "authors": ["Mansoor Shafi", "Erik G. Larsson", "Xingqin Lin", "Dorin Panaitopol", "Stefan Parkvall", "Flavien Ronteix-Jacquet", "Antti Toskala"], "title": "Industrial Viewpoints on RAN Technologies for 6G", "categories": ["cs.NI", "cs.IT", "eess.SP", "math.IT"], "comment": "submitted to the Proceedings of the IEEE", "summary": "6G standardization is to start imminently, with commercial deployments\nexpected before 2030. Its technical components and performance requirements are\nthe focus of this article. Our emphasis is on the 6G radio access, especially\nMIMO, AI, waveforms, coding, signal constellations and integration with\nnon-terrestrial networks. Whilst standardization has not yet formally started,\nthe scope of the 6G study items has been defined. Our predictions in this paper\nare speculative as there are no results of the study yet, but our views are\nguided by implementation and deployment aspects. We expect that the views here\nwill guide researchers and industry practitioners.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e866G\u6807\u51c6\u5316\u7684\u6280\u672f\u7ec4\u4ef6\u548c\u6027\u80fd\u8981\u6c42\uff0c\u4fa7\u91cd\u4e8eMIMO\u3001AI\u3001\u6ce2\u5f62\u3001\u7f16\u7801\u3001\u4fe1\u53f7\u661f\u5ea7\u53ca\u4e0e\u975e\u5730\u9762\u7f51\u7edc\u7684\u96c6\u6210\u3002", "motivation": "\u4e3a\u5373\u5c06\u5f00\u59cb\u76846G\u6807\u51c6\u5316\u63d0\u4f9b\u6280\u672f\u6307\u5bfc\uff0c\u5e2e\u52a9\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u4e86\u89e3\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "method": "\u57fa\u4e8e\u5b9e\u65bd\u548c\u90e8\u7f72\u65b9\u9762\u7684\u7ecf\u9a8c\uff0c\u5bf96G\u6280\u672f\u7ec4\u4ef6\u548c\u6027\u80fd\u8981\u6c42\u8fdb\u884c\u9884\u6d4b\u6027\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e866G\u6807\u51c6\u5316\u53ef\u80fd\u7684\u6280\u672f\u65b9\u5411\u548c\u6027\u80fd\u8981\u6c42\uff0c\u5c3d\u7ba1\u5c1a\u65e0\u5b9e\u9645\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u7684\u89c2\u70b9\u5c06\u6709\u52a9\u4e8e\u5f15\u5bfc\u7814\u7a76\u4eba\u5458\u548c\u884c\u4e1a\u4ece\u4e1a\u8005\u57286G\u6807\u51c6\u5316\u4e2d\u7684\u5de5\u4f5c\u3002"}}
{"id": "2508.07745", "pdf": "https://arxiv.org/pdf/2508.07745", "abs": "https://arxiv.org/abs/2508.07745", "authors": ["Jiongchi Yu", "Xiaofei Xie", "Qiang Hu", "Yuhan Ma", "Ziming Zhao"], "title": "Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "23 pages", "summary": "Insider threats, which can lead to severe losses, remain a major security\nconcern. While machine learning-based insider threat detection (ITD) methods\nhave shown promising results, their progress is hindered by the scarcity of\nhigh-quality data. Enterprise data is sensitive and rarely accessible, while\npublicly available datasets, when limited in scale due to cost, lack sufficient\nreal-world coverage; and when purely synthetic, they fail to capture rich\nsemantics and realistic user behavior. To address this, we propose Chimera, the\nfirst large language model (LLM)-based multi-agent framework that automatically\nsimulates both benign and malicious insider activities and collects diverse\nlogs across diverse enterprise environments. Chimera models each employee with\nagents that have role-specific behavior and integrates modules for group\nmeetings, pairwise interactions, and autonomous scheduling, capturing realistic\norganizational dynamics. It incorporates 15 types of insider attacks (e.g., IP\ntheft, system sabotage) and has been deployed to simulate activities in three\nsensitive domains: technology company, finance corporation, and medical\ninstitution, producing a new dataset, ChimeraLog. We assess ChimeraLog via\nhuman studies and quantitative analysis, confirming its diversity, realism, and\npresence of explainable threat patterns. Evaluations of existing ITD methods\nshow an average F1-score of 0.83, which is significantly lower than 0.99 on the\nCERT dataset, demonstrating ChimeraLog's higher difficulty and utility for\nadvancing ITD research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6Chimera\uff0c\u7528\u4e8e\u6a21\u62df\u4f01\u4e1a\u5185\u90e8\u5458\u5de5\u884c\u4e3a\u5e76\u751f\u6210\u591a\u6837\u5316\u7684\u65e5\u5fd7\u6570\u636e\uff0c\u89e3\u51b3\u4e86ITD\u7814\u7a76\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\uff08ITD\uff09\u7814\u7a76\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u6216\u5408\u6210\u6570\u636e\u800c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u5f00\u53d1\u4e86Chimera\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u62df\u5458\u5de5\u884c\u4e3a\uff0c\u5305\u62ec\u5408\u6cd5\u548c\u6076\u610f\u6d3b\u52a8\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u65e5\u5fd7\u6570\u636e\u96c6ChimeraLog\u3002", "result": "ChimeraLog\u5728\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u73b0\u6709ITD\u65b9\u6cd5\u5728\u5176\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8eCERT\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u5176\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Chimera\u4e3aITD\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.08064", "pdf": "https://arxiv.org/pdf/2508.08064", "abs": "https://arxiv.org/abs/2508.08064", "authors": ["Marco Bernardo", "Federico Calandra", "Andrea Esposito", "Francesco Fabris"], "title": "On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments", "categories": ["cs.DC"], "comment": null, "summary": "Information and communication technologies are by now employed in most\nactivities, including economics and finance. Despite the extraordinary power of\nmodern computers and the vast amount of memory, some results of theoretical\ncomputer science imply the impossibility of certifying software quality in\ngeneral. With the exception of safety-critical systems, this has primarily\nconcerned the information processed by confined systems, with limited\nsocio-economic consequences. In the emerging era of technologies for exchanging\ndigital money and tokenized assets over the Internet - such as central bank\ndigital currencies (CBDCs) - even a minor bug could trigger a financial\ncollapse. Although the aforementioned impossibility results cannot be overcome\nin an absolute sense, there exist formal methods that can provide assertions of\ncomputing systems correctness. We advocate their use to validate the\noperational resilience of software infrastructures enabling CBDCs, with special\nemphasis on offline payments as they constitute a very critical issue.", "AI": {"tldr": "\u73b0\u4ee3\u8ba1\u7b97\u673a\u6280\u672f\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5728\u4fdd\u969c\u8f6f\u4ef6\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u7406\u8bba\u4e0a\u7684\u4e0d\u53ef\u80fd\u6027\u3002\u5728\u6570\u5b57\u8d27\u5e01\u7b49\u91d1\u878d\u6280\u672f\u4e2d\uff0c\u5c0f\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u5efa\u8bae\u91c7\u7528\u5f62\u5f0f\u5316\u65b9\u6cd5\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u8d27\u5e01\u548c\u8d44\u4ea7\u4ee3\u5e01\u5316\u6280\u672f\u7684\u5174\u8d77\uff0c\u5373\u4f7f\u662f\u5c0f\u89c4\u6a21\u7684\u8f6f\u4ef6\u7f3a\u9677\u4e5f\u53ef\u80fd\u5f15\u53d1\u91d1\u878d\u5d29\u6e83\uff0c\u56e0\u6b64\u9700\u8981\u786e\u4fdd\u76f8\u5173\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd\u7684\u64cd\u4f5c\u5f39\u6027\u3002", "method": "\u91c7\u7528\u5f62\u5f0f\u5316\u65b9\u6cd5\uff08formal methods\uff09\u6765\u9a8c\u8bc1\u4e2d\u592e\u94f6\u884c\u6570\u5b57\u8d27\u5e01\uff08CBDCs\uff09\u8f6f\u4ef6\u7684\u6b63\u786e\u6027\uff0c\u7279\u522b\u662f\u79bb\u7ebf\u652f\u4ed8\u7b49\u5173\u952e\u73af\u8282\u3002", "result": "\u5f62\u5f0f\u5316\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u8ba1\u7b97\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u65ad\u8a00\uff0c\u6709\u52a9\u4e8e\u589e\u5f3aCBDCs\u8f6f\u4ef6\u7684\u64cd\u4f5c\u5f39\u6027\u3002", "conclusion": "\u5728\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u8f6f\u4ef6\u7f3a\u9677\u7684\u60c5\u51b5\u4e0b\uff0c\u5f62\u5f0f\u5316\u65b9\u6cd5\u662f\u9a8c\u8bc1CBDCs\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd\u53ef\u9760\u6027\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2508.07555", "pdf": "https://arxiv.org/pdf/2508.07555", "abs": "https://arxiv.org/abs/2508.07555", "authors": ["Keyuan Zhang", "Yin Sun", "Bo Ji"], "title": "Multimodal Remote Inference", "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "comment": "Accepted by The 22nd IEEE International Conference on Mobile Ad-Hoc\n  and Smart Systems (MASS 2025)", "summary": "We consider a remote inference system with multiple modalities, where a\nmultimodal machine learning (ML) model performs real-time inference using\nfeatures collected from remote sensors. As sensor observations may change\ndynamically over time, fresh features are critical for inference tasks.\nHowever, timely delivering features from all modalities is often infeasible due\nto limited network resources. To this end, we study a two-modality scheduling\nproblem to minimize the ML model's inference error, which is expressed as a\npenalty function of AoI for both modalities. We develop an index-based\nthreshold policy and prove its optimality. Specifically, the scheduler switches\nmodalities when the current modality's index function exceeds a threshold. We\nshow that the two modalities share the same threshold, and both the index\nfunctions and the threshold can be computed efficiently. The optimality of our\npolicy holds for (i) general AoI functions that are \\emph{non-monotonic} and\n\\emph{non-additive} and (ii) \\emph{heterogeneous} transmission times. Numerical\nresults show that our policy reduces inference error by up to 55% compared to\nround-robin and uniform random policies, which are oblivious to the AoI-based\ninference error function. Our results shed light on how to improve remote\ninference accuracy by optimizing task-oriented AoI functions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u4f20\u8f93\u65f6\u5ef6\uff08AoI\uff09\u6765\u6700\u5c0f\u5316\u63a8\u7406\u8bef\u5dee\uff0c\u5176\u57fa\u4e8e\u7d22\u5f15\u7684\u9608\u503c\u7b56\u7565\u5728\u975e\u5355\u8c03\u3001\u975e\u7d2f\u52a0\u7684AoI\u51fd\u6570\u548c\u5f02\u6784\u4f20\u8f93\u65f6\u95f4\u4e0b\u5747\u6700\u4f18\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u4e2d\uff0c\u4f20\u611f\u5668\u6570\u636e\u7684\u52a8\u6001\u53d8\u5316\u8981\u6c42\u7279\u5f81\u4f20\u8f93\u5177\u5907\u9ad8\u65f6\u6548\u6027\uff0c\u4f46\u7f51\u7edc\u8d44\u6e90\u6709\u9650\u5bfc\u81f4\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u6a21\u6001\u7684\u4f20\u8f93\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8bbe\u8ba1\u4e00\u79cd\u8c03\u5ea6\u7b56\u7565\u4ee5\u6700\u5c0f\u5316AoI\u5bf9\u63a8\u7406\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d22\u5f15\u7684\u9608\u503c\u7b56\u7565\uff0c\u8c03\u5ea6\u5668\u5728\u6a21\u6001\u7d22\u5f15\u51fd\u6570\u8d85\u8fc7\u9608\u503c\u65f6\u5207\u6362\u6a21\u6001\u3002\u8be5\u7b56\u7565\u5728\u975e\u5355\u8c03\u3001\u975e\u7d2f\u52a0\u7684AoI\u51fd\u6570\u548c\u5f02\u6784\u4f20\u8f93\u65f6\u95f4\u4e0b\u5747\u80fd\u9ad8\u6548\u8ba1\u7b97\u9608\u503c\u548c\u7d22\u5f15\u51fd\u6570\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u8f6e\u8be2\u548c\u968f\u673a\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u80fd\u51cf\u5c11\u9ad8\u8fbe55%\u7684\u63a8\u7406\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u9762\u5411\u4efb\u52a1\u7684AoI\u51fd\u6570\uff0c\u4e3a\u63d0\u5347\u8fdc\u7a0b\u63a8\u7406\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5176\u7b56\u7565\u5728\u5e7f\u6cdb\u6761\u4ef6\u4e0b\u5177\u6709\u6700\u4f18\u6027\u3002"}}
{"id": "2508.08101", "pdf": "https://arxiv.org/pdf/2508.08101", "abs": "https://arxiv.org/abs/2508.08101", "authors": ["Yeana Lee Bond", "Mungyeong Choe", "Baker Kasim Hasan", "Arsh Siddiqui", "Myounghoon Jeon"], "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": "Submitted to International Journal of Human-Computer Studies. Bond\n  and Choe: Drafting, Review, Editing, Validation, Software, Methodology,\n  Investigation, Data Analysis, Conceptualization, Experiment training. Hasan\n  and Siddiqui: Experimental and Data Analysis Support. Jeon: Supervision,\n  Review, Resources, Project Admin, Methodology, Conceptualization. Total 34\n  pages", "summary": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eChatGPT\u7684\u8f66\u8f7d\u5bf9\u8bdd\u4ee3\u7406\u5728\u63d0\u5347\u9a7e\u9a76\u5b89\u5168\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u9a7e\u9a76\u6027\u80fd\u548c\u7528\u6237\u8bc4\u4ef7\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9884\u7f16\u5199\u4ee3\u7406\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f7d\u5bf9\u8bdd\u4ee3\u7406\u4f9d\u8d56\u9884\u8bbe\u811a\u672c\u6216\u6709\u9650\u8bed\u97f3\u547d\u4ee4\uff0c\u9650\u5236\u4e86\u81ea\u7136\u4ea4\u4e92\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u4e8eChatGPT\u7684\u4ee3\u7406\u662f\u5426\u80fd\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u9a7e\u9a76\u4f53\u9a8c\u3002", "method": "40\u540d\u9a7e\u9a76\u5458\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u6d4b\u8bd5\u4e09\u79cd\u6761\u4ef6\uff08\u65e0\u4ee3\u7406\u3001\u9884\u8bbe\u4ee3\u7406\u3001ChatGPT\u4ee3\u7406\uff09\uff0c\u5e76\u6bd4\u8f83\u9a7e\u9a76\u6027\u80fd\u548c\u4e3b\u89c2\u8bc4\u4ef7\u3002", "result": "ChatGPT\u4ee3\u7406\u5728\u591a\u6307\u6807\u9a7e\u9a76\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u7528\u6237\u4e3b\u89c2\u8bc4\u4ef7\u66f4\u9ad8\uff0c\u4e14\u4ea4\u4e92\u4e3b\u9898\u66f4\u4e30\u5bcc\u591a\u6837\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f66\u8f7d\u4ee3\u7406\u80fd\u901a\u8fc7\u81ea\u7136\u4ea4\u4e92\u63d0\u5347\u9a7e\u9a76\u5b89\u5168\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.07058", "pdf": "https://arxiv.org/pdf/2508.07058", "abs": "https://arxiv.org/abs/2508.07058", "authors": ["Paul C. Parsons", "Prakash Chandra Shukla"], "title": "Beyond Problem Solving: Framing and Problem-Solution Co-Evolution in Data Visualization Design", "categories": ["cs.HC"], "comment": "Author version; article accepted to IEEE VIS 2025; will be published\n  in IEEE TVCG in 2026", "summary": "Visualization design is often described as the process of solving a\nwell-defined problem by navigating a design space. While existing visualization\ndesign models have provided valuable structure and guidance, they tend to\nforeground technical problem-solving and underemphasize the interpretive,\njudgment-based aspects of design. In contrast, research in other design\ndisciplines has emphasized the importance of framing--how designers define and\nredefine what the problem is--and the co-evolution of problem and solution\nspaces through reflective practice. These dimensions remain underexplored in\nvisualization research, particularly from the perspective of expert\npractitioners. This paper investigates how visualization designers frame\nproblems and navigate the dynamic interplay between problem understanding and\nsolution development. We conducted a mixed-methods study with 11 expert\npractitioners using design challenges, diary entries, and semi-structured\ninterviews. Through reflexive thematic analysis, we identified key strategies\nthat participants used to frame problems, reframe them in response to evolving\nconstraints or insights, and build bridges between problem and solution spaces.\nThese included using metaphors, heuristics, sketching, primary generators, and\nreflective evaluation of failed or incomplete ideas. Our findings contribute an\nempirically grounded account of visualization design as a reflective,\nco-evolutionary practice, where framing is not a preliminary step but a\ncontinuous activity embedded in design. Participants often reshaped their\nunderstanding of the problem based on solution attempts, tool feedback, and\nethical or narrative concerns. These insights extend current visualization\ndesign models and highlight the need for frameworks that better account for\nframing and interpretive judgment. (See paper for full abstract.)", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e2d\u95ee\u9898\u6846\u67b6\u7684\u8fde\u8d2f\u6027\u4e0e\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u662f\u4e00\u4e2a\u53cd\u601d\u6027\u7684\u8fc7\u7a0b\uff0c\u800c\u975e\u7ebf\u6027\u6280\u672f\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u73b0\u6709\u53ef\u89c6\u5316\u8bbe\u8ba1\u6a21\u578b\u8fc7\u4e8e\u4fa7\u91cd\u6280\u672f\u95ee\u9898\u89e3\u51b3\uff0c\u5ffd\u89c6\u4e86\u8bbe\u8ba1\u7684\u89e3\u91ca\u6027\u548c\u5224\u65ad\u6027\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5173\u6ce8\u4e13\u5bb6\u8bbe\u8ba1\u5e08\u5982\u4f55\u901a\u8fc7\u95ee\u9898\u6846\u67b6\u548c\u53cd\u601d\u5b9e\u8df5\u5bfc\u822a\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u6311\u6218\u3001\u65e5\u8bb0\u8bb0\u5f55\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u5bf911\u540d\u4e13\u5bb6\u8fdb\u884c\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5e76\u91c7\u7528\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u8bc6\u522b\u5173\u952e\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bbe\u8ba1\u5e08\u4f7f\u7528\u9690\u55bb\u3001\u542f\u53d1\u5f0f\u3001\u8349\u56fe\u7b49\u7b56\u7565\u91cd\u6784\u95ee\u9898\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u8bbe\u8ba1\u5982\u4f55\u901a\u8fc7\u53cd\u9988\u548c\u4f26\u7406\u8003\u91cf\u52a8\u6001\u8c03\u6574\u3002", "conclusion": "\u53ef\u89c6\u5316\u8bbe\u8ba1\u662f\u4e00\u4e2a\u53cd\u601d\u6027\u7684\u5171\u540c\u8fdb\u5316\u8fc7\u7a0b\uff0c\u95ee\u9898\u6846\u67b6\u662f\u6301\u7eed\u7684\u6d3b\u52a8\uff0c\u800c\u975e\u521d\u6b65\u6b65\u9aa4\u3002\u7814\u7a76\u547c\u5401\u672a\u6765\u6846\u67b6\u66f4\u5173\u6ce8\u8bbe\u8ba1\u4e2d\u7684\u89e3\u91ca\u6027\u5224\u65ad\u3002"}}
{"id": "2508.06524", "pdf": "https://arxiv.org/pdf/2508.06524", "abs": "https://arxiv.org/abs/2508.06524", "authors": ["Lei Jiang", "Fan Chen"], "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CarbonScaling\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u6269\u5c55\u5b9a\u5f8b\u6269\u5c55\u5230\u5305\u542bLLM\u8bad\u7ec3\u4e2d\u7684\u78b3\u6392\u653e\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u78b3\u8db3\u8ff9\u4e4b\u95f4\u7684\u5e42\u5f8b\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6269\u5c55\u5b9a\u5f8b\u5ffd\u89c6\u4e86LLM\u89c4\u6a21\u6269\u5927\u5e26\u6765\u7684\u78b3\u6392\u653e\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u91cf\u5316\u78b3\u6392\u653e\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u6269\u5c55\u6a21\u578b\u3001GPU\u786c\u4ef6\u6f14\u5316\u3001\u5e76\u884c\u4f18\u5316\u548c\u78b3\u4f30\u8ba1\u6a21\u578b\uff0c\u6784\u5efa\u4e86CarbonScaling\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u663e\u793a\u51c6\u786e\u6027\u4e0e\u78b3\u8db3\u8ff9\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u4f4e\u6548\u6027\u663e\u8457\u589e\u52a0\u4e86\u6269\u5c55\u56e0\u5b50\uff1b\u786c\u4ef6\u6280\u672f\u6539\u8fdb\u5bf9\u5c0f\u578b\u81f3\u4e2d\u578b\u6a21\u578b\u6709\u6548\uff0c\u4f46\u5bf9\u8d85\u5927LLM\u6548\u679c\u6709\u9650\u3002", "conclusion": "CarbonScaling\u4e3a\u8bad\u7ec3\u66f4\u5177\u53ef\u6301\u7eed\u6027\u548c\u78b3\u6548\u7387\u7684LLMs\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u5173\u952e\u6279\u91cf\u5927\u5c0f\u6269\u5c55\uff09\u3002"}}
{"id": "2508.07586", "pdf": "https://arxiv.org/pdf/2508.07586", "abs": "https://arxiv.org/abs/2508.07586", "authors": ["Wenjing Zhang", "Ye Hu", "Tao Luo", "Zhilong Zhang", "Mingzhe Chen"], "title": "Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method", "categories": ["cs.AI", "cs.NI"], "comment": null, "summary": "In this paper, a novel covert semantic communication framework is\ninvestigated. Within this framework, a server extracts and transmits the\nsemantic information, i.e., the meaning of image data, to a user over several\ntime slots. An attacker seeks to detect and eavesdrop the semantic transmission\nto acquire details of the original image. To avoid data meaning being\neavesdropped by an attacker, a friendly jammer is deployed to transmit jamming\nsignals to interfere the attacker so as to hide the transmitted semantic\ninformation. Meanwhile, the server will strategically select time slots for\nsemantic information transmission. Due to limited energy, the jammer will not\ncommunicate with the server and hence the server does not know the transmit\npower of the jammer. Therefore, the server must jointly optimize the semantic\ninformation transmitted at each time slot and the corresponding transmit power\nto maximize the privacy and the semantic information transmission quality of\nthe user. To solve this problem, we propose a prioritised sampling assisted\ntwin delayed deep deterministic policy gradient algorithm to jointly determine\nthe transmitted semantic information and the transmit power per time slot\nwithout the communications between the server and the jammer. Compared to\nstandard reinforcement learning methods, the propose method uses an additional\nQ network to estimate Q values such that the agent can select the action with a\nlower Q value from the two Q networks thus avoiding local optimal action\nselection and estimation bias of Q values. Simulation results show that the\nproposed algorithm can improve the privacy and the semantic information\ntransmission quality by up to 77.8% and 14.3% compared to the traditional\nreinforcement learning methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u548c\u5e72\u6270\u7b56\u7565\u63d0\u5347\u9690\u79c1\u548c\u4f20\u8f93\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5b58\u5728\u653b\u51fb\u8005\u7684\u73af\u5883\u4e2d\uff0c\u4fdd\u62a4\u8bed\u4e49\u901a\u4fe1\u7684\u9690\u79c1\u5e76\u63d0\u9ad8\u4f20\u8f93\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4f18\u5148\u91c7\u6837\u8f85\u52a9\u7684\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u4f18\u5316\u8bed\u4e49\u4fe1\u606f\u548c\u53d1\u5c04\u529f\u7387\u7684\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u9690\u79c1\u548c\u4f20\u8f93\u8d28\u91cf\u5206\u522b\u63d0\u534777.8%\u548c14.3%\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.08151", "pdf": "https://arxiv.org/pdf/2508.08151", "abs": "https://arxiv.org/abs/2508.08151", "authors": ["Moses Openja", "Paolo Arcaini", "Foutse Khomh", "Fuyuki Ishikawa"], "title": "FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "Deep neural networks (DNNs) are being utilized in various aspects of our\ndaily lives, including high-stakes decision-making applications that impact\nindividuals. However, these systems reflect and amplify bias from the data used\nduring training and testing, potentially resulting in biased behavior and\ninaccurate decisions. For instance, having different misclassification rates\nbetween white and black sub-populations. However, effectively and efficiently\nidentifying and correcting biased behavior in DNNs is a challenge. This paper\nintroduces FairFLRep, an automated fairness-aware fault localization and repair\ntechnique that identifies and corrects potentially bias-inducing neurons in DNN\nclassifiers. FairFLRep focuses on adjusting neuron weights associated with\nsensitive attributes, such as race or gender, that contribute to unfair\ndecisions. By analyzing the input-output relationships within the network,\nFairFLRep corrects neurons responsible for disparities in predictive quality\nparity. We evaluate FairFLRep on four image classification datasets using two\nDNN classifiers, and four tabular datasets with a DNN model. The results show\nthat FairFLRep consistently outperforms existing methods in improving fairness\nwhile preserving accuracy. An ablation study confirms the importance of\nconsidering fairness during both fault localization and repair stages. Our\nfindings also show that FairFLRep is more efficient than the baseline\napproaches in repairing the network.", "AI": {"tldr": "FairFLRep\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u516c\u5e73\u6027\u611f\u77e5\u7684\u6545\u969c\u5b9a\u4f4d\u4e0e\u4fee\u590d\u6280\u672f\uff0c\u7528\u4e8e\u51cf\u5c11DNN\u4e2d\u7684\u504f\u89c1\u884c\u4e3a\u3002", "motivation": "DNN\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u53ef\u80fd\u653e\u5927\u6570\u636e\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u65b9\u6cd5\u8bc6\u522b\u548c\u4fee\u590d\u504f\u89c1\u795e\u7ecf\u5143\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u4e0e\u654f\u611f\u5c5e\u6027\u76f8\u5173\u7684\u795e\u7ecf\u5143\u6743\u91cd\uff0c\u5206\u6790\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u6765\u7ea0\u6b63\u9884\u6d4b\u8d28\u91cf\u5dee\u5f02\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\uff0cFairFLRep\u5728\u63d0\u5347\u516c\u5e73\u6027\u548c\u4fdd\u6301\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cFairFLRep\u5728\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u9636\u6bb5\u8003\u8651\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u80fd\u9ad8\u6548\u4fee\u590d\u7f51\u7edc\u3002"}}
{"id": "2508.07095", "pdf": "https://arxiv.org/pdf/2508.07095", "abs": "https://arxiv.org/abs/2508.07095", "authors": ["Hyo Jin Do", "Werner Geyer"], "title": "Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust", "categories": ["cs.HC", "cs.AI"], "comment": "17 pages, 3 figures, To be published in Proceedings of the 8th\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Large language models are known to produce outputs that are plausible but\nfactually incorrect. To prevent people from making erroneous decisions by\nblindly trusting AI, researchers have explored various ways of communicating\nfactuality estimates in AI-generated outputs to end-users. However, little is\nknown about whether revealing content estimated to be factually incorrect\ninfluences users' trust when compared to hiding it altogether. We tested four\ndifferent ways of disclosing an AI-generated output with factuality\nassessments: transparent (highlights less factual content), attention\n(highlights factual content), opaque (removes less factual content), ambiguity\n(makes less factual content vague), and compared them with a baseline response\nwithout factuality information. We conducted a human subjects research (N =\n148) using the strategies in question-answering scenarios. We found that the\nopaque and ambiguity strategies led to higher trust while maintaining perceived\nanswer quality, compared to the other strategies. We discuss the efficacy of\nhiding presumably less factual content to build end-user trust.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56db\u79cd\u5c55\u793aAI\u751f\u6210\u5185\u5bb9\u4e8b\u5b9e\u6027\u8bc4\u4f30\u7684\u65b9\u5f0f\u5bf9\u7528\u6237\u4fe1\u4efb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9690\u85cf\u6216\u6a21\u7cca\u4e0d\u5b9e\u5185\u5bb9\u80fd\u63d0\u9ad8\u4fe1\u4efb\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u4e3a\u907f\u514d\u76f2\u76ee\u4fe1\u4efbAI\u5bfc\u81f4\u9519\u8bef\u51b3\u7b56\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u5411\u7528\u6237\u4f20\u8fbe\u5185\u5bb9\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\u3002", "method": "\u6d4b\u8bd5\u4e86\u56db\u79cd\u7b56\u7565\uff08\u900f\u660e\u3001\u6ce8\u610f\u3001\u4e0d\u900f\u660e\u3001\u6a21\u7cca\uff09\u4e0e\u65e0\u4e8b\u5b9e\u6027\u4fe1\u606f\u7684\u57fa\u7ebf\u7b54\u6848\uff0c\u901a\u8fc7148\u4eba\u5b9e\u9a8c\u6bd4\u8f83\u6548\u679c\u3002", "result": "\u4e0d\u900f\u660e\u548c\u6a21\u7cca\u7b56\u7565\u5728\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u4fe1\u4efb\u3002", "conclusion": "\u9690\u85cf\u6216\u6a21\u7cca\u4e0d\u5b9e\u5185\u5bb9\u53ef\u6709\u6548\u63d0\u5347\u7528\u6237\u5bf9AI\u7684\u4fe1\u4efb\u3002"}}
{"id": "2508.06767", "pdf": "https://arxiv.org/pdf/2508.06767", "abs": "https://arxiv.org/abs/2508.06767", "authors": ["Arman Dogru", "R. Irem Bor-Yaliniz", "Nimal Gamini Senarath"], "title": "PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA", "cs.RO"], "comment": null, "summary": "Digital Twins (DTs) are transforming industries through advanced data\nprocessing and analysis, positioning the world of DTs, Digital World, as a\ncornerstone of nextgeneration technologies including embodied AI. As robotics\nand automated systems scale, efficient data-sharing frameworks and robust\nalgorithms become critical. We explore the pivotal role of data handling in\nnext-gen networks, focusing on dynamics between application and network\nproviders (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with\nPriority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)\nbased multi-agent path finding (MAPF). By adopting a Centralized Training with\nDecentralized Execution (CTDE) framework and asynchronous actor-learner\narchitectures, PANAMA accelerates training while enabling autonomous task\nexecution by embodied AI. Our approach demonstrates superior pathfinding\nperformance in accuracy, speed, and scalability compared to existing\nbenchmarks. Through simulations, we highlight optimized data-sharing strategies\nfor scalable, automated systems, ensuring resilience in complex, real-world\nenvironments. PANAMA bridges the gap between network-aware decision-making and\nrobust multi-agent coordination, advancing the synergy between DTs, wireless\nnetworks, and AI-driven automation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPANAMA\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5148\u4e0d\u5bf9\u79f0\u7684\u7f51\u7edc\u611f\u77e5\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\uff0c\u5728\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5316\u5b6a\u751f\uff08DTs\uff09\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\uff0c\u9ad8\u6548\u7684\u6570\u636e\u5171\u4eab\u6846\u67b6\u548c\u7b97\u6cd5\u6210\u4e3a\u5173\u952e\u3002\u7814\u7a76\u805a\u7126\u4e8e\u5e94\u7528\u4e0e\u7f51\u7edc\u63d0\u4f9b\u5546\uff08AP/NP\uff09\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u63d0\u51faPANAMA\u7b97\u6cd5\u4ee5\u4f18\u5316\u6570\u636e\u5171\u4eab\u548c\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u4e0e\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u6846\u67b6\u53ca\u5f02\u6b65\u52a8\u4f5c-\u5b66\u4e60\u67b6\u6784\uff0cPANAMA\u7ed3\u5408\u7f51\u7edc\u611f\u77e5\u7684\u4f18\u5148\u4e0d\u5bf9\u79f0\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u3002", "result": "\u6a21\u62df\u663e\u793aPANAMA\u5728\u8def\u5f84\u89c4\u5212\u7684\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u6570\u636e\u5171\u4eab\u7b56\u7565\uff0c\u589e\u5f3a\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u7cfb\u7edf\u97e7\u6027\u3002", "conclusion": "PANAMA\u586b\u8865\u4e86\u7f51\u7edc\u611f\u77e5\u51b3\u7b56\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u6570\u5b57\u5316\u5b6a\u751f\u3001\u65e0\u7ebf\u7f51\u7edc\u4e0eAI\u9a71\u52a8\u81ea\u52a8\u5316\u7684\u534f\u540c\u53d1\u5c55\u3002"}}
{"id": "2508.07129", "pdf": "https://arxiv.org/pdf/2508.07129", "abs": "https://arxiv.org/abs/2508.07129", "authors": ["Caroline M. Johnston", "Olga Koumoundouros", "Angel Hsing-Chi Hwang", "Laura Onasch-Vera", "Eric Rice", "Phebe Vayanos"], "title": "Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers", "categories": ["cs.HC", "cs.AI"], "comment": "21 pages, 1 figure, 2 tables", "summary": "Artificial intelligence researchers have proposed various data-driven\nalgorithms to improve the processes that match individuals experiencing\nhomelessness to scarce housing resources. It remains unclear whether and how\nthese algorithms are received or adopted by practitioners and what their\ncorresponding consequences are. Through semi-structured interviews with 13\npolicymakers in homeless services in Los Angeles, we investigate whether such\nchange-makers are open to the idea of integrating AI into the housing resource\nmatching process, identifying where they see potential gains and drawbacks from\nsuch a system in issues of efficiency, fairness, and transparency. Our\nqualitative analysis indicates that, even when aware of various complicating\nfactors, policymakers welcome the idea of an AI matching tool if thoughtfully\ndesigned and used in tandem with human decision-makers. Though there is no\nconsensus as to the exact design of such an AI system, insights from\npolicymakers raise open questions and design considerations that can be\nenlightening for future researchers and practitioners who aim to build\nresponsible algorithmic systems to support decision-making in low-resource\nscenarios.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u4f4f\u623f\u8d44\u6e90\u5339\u914d\u4e2d\u7684\u63a5\u53d7\u5ea6\u4e0e\u6f5c\u5728\u5f71\u54cd\uff0c\u901a\u8fc7\u8bbf\u8c08\u53d1\u73b0\u653f\u7b56\u5236\u5b9a\u8005\u5bf9AI\u5de5\u5177\u6301\u5f00\u653e\u6001\u5ea6\uff0c\u4f46\u9700\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7ed3\u5408\u3002", "motivation": "\u4e86\u89e3AI\u5728\u65e0\u5bb6\u53ef\u5f52\u8005\u4f4f\u623f\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u63a5\u53d7\u5ea6\u53ca\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u5bf9\u6d1b\u6749\u77f613\u540d\u653f\u7b56\u5236\u5b9a\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u5206\u6790\u5176\u5bf9AI\u5de5\u5177\u7684\u63a5\u53d7\u5ea6\u4e0e\u770b\u6cd5\u3002", "result": "\u653f\u7b56\u5236\u5b9a\u8005\u6b22\u8fceAI\u5de5\u5177\uff0c\u4f46\u5f3a\u8c03\u9700\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7ed3\u5408\u5e76\u89e3\u51b3\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u8bbe\u8ba1\u8d1f\u8d23\u4efbAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f00\u653e\u6027\u95ee\u9898\u4e0e\u8bbe\u8ba1\u8003\u91cf\u3002"}}
{"id": "2508.06771", "pdf": "https://arxiv.org/pdf/2508.06771", "abs": "https://arxiv.org/abs/2508.06771", "authors": ["James Almgren-Bell", "Nader Al Awar", "Dilip S Geethakrishnan", "Milos Gligoric", "George Biros"], "title": "A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions", "categories": ["cs.CE", "cs.DC"], "comment": null, "summary": "We study parallel particle-in-cell (PIC) methods for low-temperature plasmas\n(LTPs), which discretize kinetic formulations that capture the time evolution\nof the probability density function of particles as a function of position and\nvelocity. We use a kinetic description for electrons and a fluid approximation\nfor heavy species. In this paper, we focus on GPU acceleration of algorithms\nfor velocity-space interactions and in particular, collisions of electrons with\nneutrals, ions, and electrons. Our work has two thrusts. The first is\nalgorithmic exploration and analysis. The second is examining the viability of\nrapid-prototyping implementations using Python-based HPC tools, in particular\nPyKokkos. We discuss several common PIC kernels and present performance results\non NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly\nfaster for most kernels but shows more sensitivity to register pressure. We\nalso report scaling results for a distributed memory implementation on up to 16\nMPI ranks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7528\u4e8e\u4f4e\u6e29\u7b49\u79bb\u5b50\u4f53\u7684\u5e76\u884c\u7c92\u5b50\u5355\u5143\u65b9\u6cd5\uff0c\u91cd\u70b9\u63a2\u7d22\u4e86GPU\u52a0\u901f\u7b97\u6cd5\u53caPython\u5de5\u5177\u7684\u4f7f\u7528\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u4f4e\u6e29\u7b49\u79bb\u5b50\u4f53\u6a21\u62df\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u7535\u5b50\u78b0\u649e\u7684\u8ba1\u7b97\u901f\u5ea6\uff0c\u5e76\u6d4b\u8bd5\u5feb\u901f\u539f\u578b\u5f00\u53d1\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u7535\u5b50\u52a8\u529b\u5b66\u63cf\u8ff0\u548c\u6d41\u4f53\u8fd1\u4f3c\u6cd5\uff0c\u7ed3\u5408GPU\u52a0\u901f\u7b97\u6cd5\uff0c\u5e76\u4f7f\u7528Python\u5de5\u5177PyKokkos\u8fdb\u884c\u5b9e\u73b0\u3002", "result": "\u5728NVIDIA V100\u548cAMD MI250X GPU\u4e0a\u6d4b\u8bd5\uff0cMI250X\u6027\u80fd\u7565\u4f18\u4f46\u5bf9\u5bc4\u5b58\u5668\u538b\u529b\u66f4\u654f\u611f\uff1b\u5206\u5e03\u5f0f\u5185\u5b58\u5b9e\u73b0\u6269\u5c55\u81f316 MPI\u79e9\u3002", "conclusion": "GPU\u52a0\u901f\u6709\u6548\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0cPython\u5de5\u5177\u53ef\u7528\u4e8e\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0cAMD GPU\u5728\u5c0f\u90e8\u5206\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2508.07135", "pdf": "https://arxiv.org/pdf/2508.07135", "abs": "https://arxiv.org/abs/2508.07135", "authors": ["Runlin Duan", "Yuzhao Chen", "Rahul Jain", "Yichen Hu", "Jingyu Shi", "Karthik Ramani"], "title": "Canvas3D: Empowering Precise Spatial Control for Image Generation with Constraints from a 3D Virtual Canvas", "categories": ["cs.HC"], "comment": null, "summary": "Generative AI (GenAI) has significantly advanced the ease and flexibility of\nimage creation. However, it remains a challenge to precisely control spatial\ncompositions, including object arrangement and scene conditions. To bridge this\ngap, we propose Canvas3D, an interactive system leveraging a 3D engine to\nenable precise spatial manipulation for image generation. Upon user prompt,\nCanvas3D automatically converts textual descriptions into interactive objects\nwithin a 3D engine-driven virtual canvas, empowering direct and precise spatial\nconfiguration. These user-defined arrangements generate explicit spatial\nconstraints that guide generative models in accurately reflecting user\nintentions in the resulting images. We conducted a closed-end comparative study\nbetween Canvas3D and a baseline system. And an open-ended study to evaluate our\nsystem \"in the wild\". The result indicates that Canvas3D outperforms the\nbaseline on spatial control, interactivity, and overall user experience.", "AI": {"tldr": "Canvas3D\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u5229\u75283D\u5f15\u64ce\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7cbe\u786e\u7a7a\u95f4\u63a7\u5236\uff0c\u63d0\u5347\u7528\u6237\u5bf9\u5bf9\u8c61\u6392\u5217\u548c\u573a\u666f\u6761\u4ef6\u7684\u64cd\u63a7\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u751f\u6210AI\u5728\u7a7a\u95f4\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u7cbe\u786e\u6ee1\u8db3\u7528\u6237\u5bf9\u5bf9\u8c61\u6392\u5217\u548c\u573a\u666f\u6761\u4ef6\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc73D\u5f15\u64ce\u5c06\u6587\u672c\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u4ea4\u4e92\u5bf9\u8c61\uff0c\u7528\u6237\u53ef\u76f4\u63a5\u5728\u865a\u62df\u753b\u5e03\u4e2d\u7cbe\u786e\u914d\u7f6e\u7a7a\u95f4\u5e03\u5c40\uff0c\u751f\u6210\u7a7a\u95f4\u7ea6\u675f\u6307\u5bfc\u56fe\u50cf\u751f\u6210\u3002", "result": "Canvas3D\u5728\u7a7a\u95f4\u63a7\u5236\u3001\u4ea4\u4e92\u6027\u548c\u7528\u6237\u4f53\u9a8c\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "Canvas3D\u6709\u6548\u5f25\u8865\u4e86\u751f\u6210AI\u5728\u7a7a\u95f4\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u7528\u6237\u5bf9\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u786e\u64cd\u63a7\u80fd\u529b\u3002"}}
{"id": "2508.06972", "pdf": "https://arxiv.org/pdf/2508.06972", "abs": "https://arxiv.org/abs/2508.06972", "authors": ["Dan Ivanov", "Tristan Freiberg", "Haruna Isah"], "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.LG"], "comment": "12 pages, 8 figures, and 10 tables", "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.", "AI": {"tldr": "DSperse \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u6218\u7565\u6027\u7684\u52a0\u5bc6\u9a8c\u8bc1\u5b9e\u73b0\u4fe1\u4efb\u6700\u5c0f\u5316\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5168\u6a21\u578b\u7535\u8def\u5316\u7684\u9ad8\u6210\u672c\u548c\u50f5\u5316\u95ee\u9898\uff0cDSperse \u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u9009\u62e9\u6027\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4ec5\u9a8c\u8bc1\u5173\u952e\u5b50\u8ba1\u7b97\u90e8\u5206\uff08\u79f0\u4e3a\u201c\u5207\u7247\u201d\uff09\u3002", "method": "DSperse \u5141\u8bb8\u9488\u5bf9\u6027\u5730\u9a8c\u8bc1\u6a21\u578b\u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u7684\u90e8\u5206\u6216\u5168\u90e8\u5b50\u8ba1\u7b97\uff0c\u5229\u7528\u5ba1\u8ba1\u3001\u590d\u5236\u6216\u7ecf\u6d4e\u6fc0\u52b1\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDSperse \u5728\u5185\u5b58\u4f7f\u7528\u3001\u8fd0\u884c\u65f6\u548c\u7535\u8def\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u7075\u6d3b\u7684\u9a8c\u8bc1\u7b56\u7565\u3002", "conclusion": "DSperse \u901a\u8fc7\u7075\u6d3b\u7684\u9a8c\u8bc1\u8fb9\u754c\u8bbe\u8ba1\uff0c\u4e3a\u591a\u6837\u5316\u7684\u90e8\u7f72\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u3001\u9488\u5bf9\u6027\u7684\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07141", "pdf": "https://arxiv.org/pdf/2508.07141", "abs": "https://arxiv.org/abs/2508.07141", "authors": ["Runlin Duan", "Chenfei Zhu", "Yuzhao Chen", "Dizhi Ma", "Jingyu Shi", "Ziyi Liu", "Karthik Ramani"], "title": "SketchConcept: Sketching-based Concept Recomposition for Product Design using Generative AI", "categories": ["cs.HC"], "comment": null, "summary": "Conceptual product design requires designers to explore the design space of\nvisual and functional concepts simultaneously. Sketching has long been adopted\nto empower concept exploration. However, current sketch-based design tools\nmostly emphasize visual design using emerging techniques. We present\nSketchConcept, a design support tool that decomposes design concepts into\nvisual representations and functionality of concepts using sketches and textual\ndescriptions. We propose a function-to-visual mapping workflow that maps the\nfunction descriptions generated by a Large Language Model to a component of the\nconcept produced by image Generative Artificial Intelligence(GenAI). The\nfunction-to-visual mapping allows our system to leverage multimodal GenAI to\ndecompose, generate, and edit the design concept to satisfy the overall\nfunction and behavior. We present multiple use cases enabled by SketchConcept\nto validate the workflow. Finally, we evaluated the efficacy and usability of\nour system with a two-session user study.", "AI": {"tldr": "SketchConcept\u662f\u4e00\u4e2a\u8bbe\u8ba1\u652f\u6301\u5de5\u5177\uff0c\u5229\u7528\u8349\u56fe\u548c\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5c06\u8bbe\u8ba1\u6982\u5ff5\u5206\u89e3\u4e3a\u89c6\u89c9\u548c\u529f\u80fd\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8349\u56fe\u8bbe\u8ba1\u5de5\u5177\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8bbe\u8ba1\uff0c\u672a\u80fd\u540c\u65f6\u63a2\u7d22\u89c6\u89c9\u4e0e\u529f\u80fd\u6982\u5ff5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u529f\u80fd\u5230\u89c6\u89c9\u7684\u6620\u5c04\u5de5\u4f5c\u6d41\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u529f\u80fd\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u56fe\u50cf\u751f\u6210\u5f0fAI\u751f\u6210\u6982\u5ff5\u7ec4\u4ef6\u3002", "result": "SketchConcept\u652f\u6301\u591a\u6a21\u6001\u751f\u6210\u5f0fAI\u5206\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u8bbe\u8ba1\u6982\u5ff5\uff0c\u6ee1\u8db3\u4e86\u6574\u4f53\u529f\u80fd\u548c\u884c\u4e3a\u7684\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u53ef\u7528\u6027\u3002", "conclusion": "SketchConcept\u4e3a\u6982\u5ff5\u4ea7\u54c1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u5177\uff0c\u80fd\u591f\u540c\u65f6\u63a2\u7d22\u89c6\u89c9\u4e0e\u529f\u80fd\u8bbe\u8ba1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07423", "pdf": "https://arxiv.org/pdf/2508.07423", "abs": "https://arxiv.org/abs/2508.07423", "authors": ["Fotis I. Giasemis"], "title": "Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures", "categories": ["hep-ex", "cs.AI", "cs.DC", "cs.LG", "physics.data-an"], "comment": "PhD thesis, Chapters 8 and 9 include results from work performed in\n  collaboration with Anthony Correia", "summary": "As the particle physics community needs higher and higher precisions in order\nto test our current model of the subatomic world, larger and larger datasets\nare necessary. With upgrades scheduled for the detectors of colliding-beam\nexperiments around the world, and specifically at the Large Hadron Collider at\nCERN, more collisions and more complex interactions are expected. This directly\nimplies an increase in data produced and consequently in the computational\nresources needed to process them. At CERN, the amount of data produced is\ngargantuan. This is why the data have to be heavily filtered and selected in\nreal time before being permanently stored. This data can then be used to\nperform physics analyses, in order to expand our current understanding of the\nuniverse and improve the Standard Model of physics. This real-time filtering,\nknown as triggering, involves complex processing happening often at frequencies\nas high as 40 MHz. This thesis contributes to understanding how machine\nlearning models can be efficiently deployed in such environments, in order to\nmaximize throughput and minimize energy consumption. Inevitably, modern\nhardware designed for such tasks and contemporary algorithms are needed in\norder to meet the challenges posed by the stringent, high-frequency data rates.\nIn this work, I present our graph neural network-based pipeline, developed for\ncharged particle track reconstruction at the LHCb experiment at CERN. The\npipeline was implemented end-to-end inside LHCb's first-level trigger, entirely\non GPUs. Its performance was compared against the classical tracking algorithms\ncurrently in production at LHCb. The pipeline was also accelerated on the FPGA\narchitecture, and its performance in terms of power consumption and processing\nspeed was compared against the GPU implementation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7c92\u5b50\u7269\u7406\u5b9e\u9a8c\u4e2d\u5982\u4f55\u9ad8\u6548\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u9891\u7387\u6570\u636e\u5904\u7406\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7ba1\u9053\uff0c\u5e76\u5728LHCb\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740\u7c92\u5b50\u7269\u7406\u5b9e\u9a8c\u6570\u636e\u91cf\u7684\u6025\u5267\u589e\u52a0\uff0c\u4f20\u7edf\u5904\u7406\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u8fc7\u6ee4\u548c\u9ad8\u9891\u6570\u636e\u5904\u7406\u9700\u6c42\uff0c\u6025\u9700\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u548c\u786c\u4ef6\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5e26\u7535\u7c92\u5b50\u8f68\u8ff9\u91cd\u5efa\uff0c\u5e76\u5728GPU\u548cFPGA\u4e0a\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u90e8\u7f72\uff0c\u4e0e\u4f20\u7edf\u7684\u8ddf\u8e2a\u7b97\u6cd5\u8fdb\u884c\u4e86\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u8be5\u7ba1\u9053\u5728LHCb\u7684\u7b2c\u4e00\u7ea7\u89e6\u53d1\u5668\u4e2d\u8868\u73b0\u51fa\u8272\uff0cGPU\u548cFPGA\u5b9e\u73b0\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u901f\u5ea6\u548c\u80fd\u8017\u65b9\u9762\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u4ee5\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u548c\u7b97\u6cd5\u6539\u8fdb\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.07505", "pdf": "https://arxiv.org/pdf/2508.07505", "abs": "https://arxiv.org/abs/2508.07505", "authors": ["Yueyang Quan", "Chang Wang", "Shengjie Zhai", "Minghong Fang", "Zhuqing Liu"], "title": "Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "To appear in ACM MobiHoc 2025", "summary": "Decentralized min-max optimization allows multi-agent systems to\ncollaboratively solve global min-max optimization problems by facilitating the\nexchange of model updates among neighboring agents, eliminating the need for a\ncentral server. However, sharing model updates in such systems carry a risk of\nexposing sensitive data to inference attacks, raising significant privacy\nconcerns. To mitigate these privacy risks, differential privacy (DP) has become\na widely adopted technique for safeguarding individual data. Despite its\nadvantages, implementing DP in decentralized min-max optimization poses\nchallenges, as the added noise can hinder convergence, particularly in\nnon-convex scenarios with complex agent interactions in min-max optimization\nproblems. In this work, we propose an algorithm called DPMixSGD (Differential\nPrivate Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving\nalgorithm specifically designed for non-convex decentralized min-max\noptimization. Our method builds on the state-of-the-art STORM-based algorithm,\none of the fastest decentralized min-max solutions. We rigorously prove that\nthe noise added to local gradients does not significantly compromise\nconvergence performance, and we provide theoretical bounds to ensure privacy\nguarantees. To validate our theoretical findings, we conduct extensive\nexperiments across various tasks and models, demonstrating the effectiveness of\nour approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPMixSGD\u7684\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u7528\u4e8e\u975e\u51f8\u5206\u6563\u5f0f\u6700\u5c0f\u6700\u5927\u4f18\u5316\uff0c\u7ed3\u5408\u4e86\u5dee\u5206\u9690\u79c1\u548cSTORM\u7b97\u6cd5\uff0c\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u4e0d\u5f71\u54cd\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u5206\u6563\u5f0f\u6700\u5c0f\u6700\u5927\u4f18\u5316\u4e2d\u6a21\u578b\u66f4\u65b0\u7684\u5171\u4eab\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u6570\u636e\u6cc4\u9732\uff0c\u5dee\u5206\u9690\u79c1\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u566a\u58f0\u53ef\u80fd\u5f71\u54cd\u6536\u655b\uff0c\u5c24\u5176\u5728\u975e\u51f8\u573a\u666f\u4e2d\u3002", "method": "\u57fa\u4e8eSTORM\u7b97\u6cd5\u6539\u8fdb\uff0c\u63d0\u51faDPMixSGD\u7b97\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u566a\u58f0\u4e0d\u5f71\u54cd\u6536\u655b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cDPMixSGD\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\u3002", "conclusion": "DPMixSGD\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u51f8\u5206\u6563\u5f0f\u6700\u5c0f\u6700\u5927\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2508.07676", "pdf": "https://arxiv.org/pdf/2508.07676", "abs": "https://arxiv.org/abs/2508.07676", "authors": ["Chenchen Lin", "Xuehe Wang"], "title": "Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks", "categories": ["cs.LG", "cs.DC", "cs.GT"], "comment": "Accepted by ECAI25", "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, thereby enhancing privacy and\nfacilitating collaboration among clients connected via social networks.\nHowever, these social connections introduce privacy externalities: a client's\nprivacy loss depends not only on its privacy protection strategy but also on\nthe privacy decisions of others, propagated through the network via multi-hop\ninteractions. In this work, we propose a socially-aware privacy-preserving FL\nmechanism that systematically quantifies indirect privacy leakage through a\nmulti-hop propagation model. We formulate the server-client interaction as a\ntwo-stage Stackelberg game, where the server, as the leader, optimizes\nincentive policies, and clients, as followers, strategically select their\nprivacy budgets, which determine their privacy-preserving levels by controlling\nthe magnitude of added noise. To mitigate information asymmetry in networked\nprivacy estimation, we introduce a mean-field estimator to approximate the\naverage external privacy risk. We theoretically prove the existence and\nconvergence of the fixed point of the mean-field estimator and derive\nclosed-form expressions for the Stackelberg Nash Equilibrium. Despite being\ndesigned from a client-centric incentive perspective, our mechanism achieves\napproximately-optimal social welfare, as revealed by Price of Anarchy (PoA)\nanalysis. Experiments on diverse datasets demonstrate that our approach\nsignificantly improves client utilities and reduces server costs while\nmaintaining model performance, outperforming both Social-Agnostic (SA)\nbaselines and methods that account for social externalities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u4ea4\u611f\u77e5\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8df3\u4f20\u64ad\u6a21\u578b\u91cf\u5316\u95f4\u63a5\u9690\u79c1\u6cc4\u9732\uff0c\u5e76\u91c7\u7528Stackelberg\u535a\u5f08\u4f18\u5316\u6fc0\u52b1\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba2\u6237\u7aef\u6548\u7528\u5e76\u964d\u4f4e\u4e86\u670d\u52a1\u5668\u6210\u672c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u793e\u4ea4\u7f51\u7edc\u7684\u8fde\u63a5\u5f15\u5165\u4e86\u9690\u79c1\u5916\u90e8\u6027\u95ee\u9898\uff0c\u5373\u5ba2\u6237\u7aef\u7684\u9690\u79c1\u635f\u5931\u4e0d\u4ec5\u53d7\u81ea\u8eab\u4fdd\u62a4\u7b56\u7565\u5f71\u54cd\uff0c\u8fd8\u53d7\u7f51\u7edc\u4e2d\u5176\u4ed6\u8282\u70b9\u7684\u9690\u79c1\u51b3\u7b56\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u793e\u4ea4\u611f\u77e5\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8df3\u4f20\u64ad\u6a21\u578b\u91cf\u5316\u95f4\u63a5\u9690\u79c1\u6cc4\u9732\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\u4f18\u5316\u670d\u52a1\u5668\u6fc0\u52b1\u7b56\u7565\u548c\u5ba2\u6237\u7aef\u7684\u9690\u79c1\u9884\u7b97\u9009\u62e9\u3002\u5f15\u5165\u5747\u503c\u573a\u4f30\u8ba1\u5668\u4ee5\u51cf\u5c11\u9690\u79c1\u4f30\u8ba1\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u5747\u503c\u573a\u4f30\u8ba1\u5668\u7684\u5b58\u5728\u6027\u548c\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86Stackelberg\u7eb3\u4ec0\u5747\u8861\u7684\u95ed\u5f0f\u8868\u8fbe\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5ba2\u6237\u7aef\u6548\u7528\u5e76\u964d\u4f4e\u4e86\u670d\u52a1\u5668\u6210\u672c\u3002", "conclusion": "\u8be5\u673a\u5236\u5728\u5ba2\u6237\u7aef\u6548\u7528\u4e0e\u670d\u52a1\u5668\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4f18\u4e8e\u793e\u4ea4\u65e0\u5173\u57fa\u51c6\u548c\u5176\u4ed6\u8003\u8651\u793e\u4ea4\u5916\u90e8\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07256", "pdf": "https://arxiv.org/pdf/2508.07256", "abs": "https://arxiv.org/abs/2508.07256", "authors": ["Wei Xiang", "Chuyue Zhang", "Jie Yan"], "title": "Exploring Micro Accidents and Driver Responses in Automated Driving: Insights from Real-world Videos", "categories": ["cs.HC"], "comment": "31 pages, 5 figures, under review", "summary": "Automated driving in level 3 autonomy has been adopted by multiple companies\nsuch as Tesla and BMW, alleviating the burden on drivers while unveiling new\ncomplexities. This article focused on the under-explored territory of micro\naccidents during automated driving, characterized as not fatal but abnormal\naberrations such as abrupt deceleration and snake driving. These micro\naccidents are basic yet pervasive events that might results in more severe\naccidents. Through collecting a comprehensive dataset of user generated video\nrecording such micro accidents in natural driving scenarios, this article\nlocates key variables pertaining to environments and autonomous agents using\nmachine learning methods. Subsequently, crowdsourcing method provides insights\ninto human risk perceptions and reactions to these micro accidents. This\narticle thus describes features of safety critical scenarios other than crashes\nand fatal accidents, informing and potentially advancing the design of\nautomated driving systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86L3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5fae\u89c2\u4e8b\u6545\uff08\u5982\u7a81\u7136\u51cf\u901f\u548c\u86c7\u5f62\u9a7e\u9a76\uff09\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u6790\u73af\u5883\u4e0e\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u7684\u5173\u952e\u53d8\u91cf\uff0c\u5e76\u7ed3\u5408\u4f17\u5305\u65b9\u6cd5\u7814\u7a76\u4eba\u7c7b\u98ce\u9669\u611f\u77e5\uff0c\u4ee5\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76L3\u867d\u51cf\u8f7b\u53f8\u673a\u8d1f\u62c5\uff0c\u4f46\u5fae\u89c2\u4e8b\u6545\uff08\u975e\u81f4\u547d\u4f46\u5f02\u5e38\u4e8b\u4ef6\uff09\u666e\u904d\u4e14\u53ef\u80fd\u5f15\u53d1\u66f4\u4e25\u91cd\u4e8b\u6545\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\uff0c\u9700\u6df1\u5165\u4e86\u89e3\u5176\u7279\u6027\u548c\u4eba\u7c7b\u53cd\u5e94\u3002", "method": "\u6536\u96c6\u7528\u6237\u751f\u6210\u7684\u5fae\u89c2\u4e8b\u6545\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5b9a\u4f4d\u5173\u952e\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u4f17\u5305\u83b7\u53d6\u4eba\u7c7b\u98ce\u9669\u611f\u77e5\u6570\u636e\u3002", "result": "\u8bc6\u522b\u4e86\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u7279\u5f81\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u5fae\u89c2\u4e8b\u6545\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u4f18\u5316\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u548c\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2508.07873", "pdf": "https://arxiv.org/pdf/2508.07873", "abs": "https://arxiv.org/abs/2508.07873", "authors": ["Samaneh Mohammadi", "Vasileios Tsouvalas", "Iraklis Symeonidis", "Ali Balador", "Tanir Ozcelebi", "Francesco Flammini", "Nirvana Meratnia"], "title": "EFU: Enforcing Federated Unlearning via Functional Encryption", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": null, "summary": "Federated unlearning (FU) algorithms allow clients in federated settings to\nexercise their ''right to be forgotten'' by removing the influence of their\ndata from a collaboratively trained model. Existing FU methods maintain data\nprivacy by performing unlearning locally on the client-side and sending\ntargeted updates to the server without exposing forgotten data; yet they often\nrely on server-side cooperation, revealing the client's intent and identity\nwithout enforcement guarantees - compromising autonomy and unlearning privacy.\nIn this work, we propose EFU (Enforced Federated Unlearning), a\ncryptographically enforced FU framework that enables clients to initiate\nunlearning while concealing its occurrence from the server. Specifically, EFU\nleverages functional encryption to bind encrypted updates to specific\naggregation functions, ensuring the server can neither perform unauthorized\ncomputations nor detect or skip unlearning requests. To further mask behavioral\nand parameter shifts in the aggregated model, we incorporate auxiliary\nunlearning losses based on adversarial examples and parameter importance\nregularization. Extensive experiments show that EFU achieves near-random\naccuracy on forgotten data while maintaining performance comparable to full\nretraining across datasets and neural architectures - all while concealing\nunlearning intent from the server. Furthermore, we demonstrate that EFU is\nagnostic to the underlying unlearning algorithm, enabling secure,\nfunction-hiding, and verifiable unlearning for any client-side FU mechanism\nthat issues targeted updates.", "AI": {"tldr": "EFU\u662f\u4e00\u79cd\u52a0\u5bc6\u5f3a\u5236\u7684\u8054\u90a6\u9057\u5fd8\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u62a4\u5ba2\u6237\u7aef\u7684\u9057\u5fd8\u610f\u56fe\u548c\u8eab\u4efd\u9690\u79c1\uff0c\u540c\u65f6\u786e\u4fdd\u670d\u52a1\u5668\u65e0\u6cd5\u68c0\u6d4b\u6216\u8df3\u8fc7\u9057\u5fd8\u8bf7\u6c42\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u4f9d\u8d56\u670d\u52a1\u5668\u5408\u4f5c\uff0c\u53ef\u80fd\u6cc4\u9732\u5ba2\u6237\u7684\u9057\u5fd8\u610f\u56fe\u548c\u8eab\u4efd\uff0c\u635f\u5bb3\u5176\u9690\u79c1\u548c\u81ea\u4e3b\u6027\u3002", "method": "EFU\u5229\u7528\u529f\u80fd\u52a0\u5bc6\u7ed1\u5b9a\u52a0\u5bc6\u66f4\u65b0\u5230\u7279\u5b9a\u805a\u5408\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u6027\u793a\u4f8b\u548c\u53c2\u6570\u91cd\u8981\u6027\u6b63\u5219\u5316\u7684\u8f85\u52a9\u635f\u5931\u3002", "result": "EFU\u5728\u9057\u5fd8\u6570\u636e\u4e0a\u63a5\u8fd1\u968f\u673a\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u91cd\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u4e14\u80fd\u9690\u85cf\u9057\u5fd8\u610f\u56fe\u3002", "conclusion": "EFU\u662f\u4e00\u79cd\u901a\u7528\u3001\u5b89\u5168\u548c\u53ef\u9a8c\u8bc1\u7684\u8054\u90a6\u9057\u5fd8\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u5ba2\u6237\u7aef\u9057\u5fd8\u673a\u5236\u3002"}}
{"id": "2508.07283", "pdf": "https://arxiv.org/pdf/2508.07283", "abs": "https://arxiv.org/abs/2508.07283", "authors": ["Bujar Raufi"], "title": "Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment", "categories": ["cs.HC", "cs.AI", "eess.SP", "q-bio.NC", "97R40", "I.2"], "comment": "17 Pages, 7 figures, 3 tables and one prompt template", "summary": "This study explores the intersection of electroencephalography (EEG)\nmicrostates and Large Language Models (LLMs) to enhance the assessment of\ncognitive load states. By utilizing EEG microstate features, the research aims\nto fine-tune LLMs for improved predictions of distinct cognitive states,\nspecifically 'Rest' and 'Load'. The experimental design is delineated in four\ncomprehensive stages: dataset collection and preprocessing, microstate\nsegmentation and EEG backfitting, feature extraction paired with prompt\nengineering, and meticulous LLM model selection and refinement. Employing a\nsupervised learning paradigm, the LLM is trained to identify cognitive load\nstates based on EEG microstate features integrated into prompts, producing\naccurate discrimination of cognitive load. A curated dataset, linking EEG\nfeatures to specified cognitive load conditions, underpins the experimental\nframework. The results indicate a significant improvement in model performance\nfollowing the proposed fine-tuning, showcasing the potential of EEG-informed\nLLMs in cognitive neuroscience and cognitive AI applications. This approach not\nonly contributes to the understanding of brain dynamics but also paves the way\nfor advancements in machine learning techniques applicable to cognitive load\nand cognitive AI research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86EEG\u5fae\u72b6\u6001\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7ed3\u5408\uff0c\u4ee5\u4f18\u5316\u8ba4\u77e5\u8d1f\u8377\u72b6\u6001\u7684\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5fae\u8c03LLMs\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u5bf9\u8ba4\u77e5\u72b6\u6001\u7684\u9884\u6d4b\u51c6\u786e\u5ea6\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408EEG\u5fae\u72b6\u6001\u548cLLMs\uff0c\u63d0\u5347\u5bf9\u8ba4\u77e5\u8d1f\u8377\u72b6\u6001\uff08\u5982'\u4f11\u606f'\u548c'\u8d1f\u8377'\uff09\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u63a8\u52a8\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548c\u8ba4\u77e5AI\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u6536\u96c6\u4e0e\u9884\u5904\u7406\u3001\u5fae\u72b6\u6001\u5206\u5272\u4e0eEEG\u56de\u62df\u5408\u3001\u7279\u5f81\u63d0\u53d6\u4e0e\u63d0\u793a\u5de5\u7a0b\u3001LLM\u6a21\u578b\u9009\u62e9\u4e0e\u4f18\u5316\u3002\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u8bad\u7ec3LLMs\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u53ef\u51c6\u786e\u533a\u5206\u8ba4\u77e5\u8d1f\u8377\u72b6\u6001\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7406\u89e3\u5927\u8111\u52a8\u6001\uff0c\u8fd8\u4e3a\u8ba4\u77e5\u8d1f\u8377\u548c\u8ba4\u77e5AI\u7814\u7a76\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.07879", "pdf": "https://arxiv.org/pdf/2508.07879", "abs": "https://arxiv.org/abs/2508.07879", "authors": ["Oscar Ferraz", "Bruno Coutinho", "Gabriel Falcao", "Marco Gomes", "Francisco A. Monteiro", "Vitor Silva"], "title": "GPU-Accelerated Syndrome Decoding for Quantum LDPC Codes below the 63 $\u03bc$s Latency Threshold", "categories": ["quant-ph", "cs.DC"], "comment": "7 pages, 3 figures, 1 table", "summary": "This paper presents a GPU-accelerated decoder for quantum low-density\nparity-check (QLDPC) codes that achieves sub-$63$ $\\mu$s latency, below the\nsurface code decoder's real-time threshold demonstrated on Google's Willow\nquantum processor. While surface codes have demonstrated below-threshold\nperformance, the encoding rates approach zero as code distances increase,\nposing challenges for scalability. Recently proposed QLDPC codes, such as those\nby Panteleev and Kalachev, offer constant-rate encoding and asymptotic goodness\nbut introduce higher decoding complexity. To address such limitation, this work\npresents a parallelized belief propagation decoder leveraging syndrome\ninformation on commodity GPU hardware. Parallelism was exploited to maximize\nperformance within the limits of target latency, allowing decoding latencies\nunder $50$ $\\mu$s for [[$784$, $24$, $24$]] codes and as low as $23.3$ $\\mu$s\nfor smaller codes, meeting the tight timing constraints of superconducting\nqubit cycles. These results show that real-time, scalable decoding of\nasymptotically good quantum codes is achievable using widely available\ncommodity hardware, advancing the feasibility of fault-tolerant quantum\ncomputation beyond surface codes.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c(QLDPC)\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u4f4e\u4e8e63\u5fae\u79d2\u7684\u5ef6\u8fdf\uff0c\u4f18\u4e8eGoogle Willow\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u8868\u9762\u7801\u89e3\u7801\u5668\u7684\u5b9e\u65f6\u9608\u503c\u3002QLDPC\u7f16\u7801\u5177\u6709\u6052\u5b9a\u901f\u7387\u548c\u6e10\u8fdb\u4f18\u6027\uff0c\u4f46\u89e3\u7801\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u901a\u8fc7\u5e76\u884c\u5316\u7f6e\u4fe1\u4f20\u64ad\u89e3\u7801\u5668\uff0c\u5229\u7528GPU\u786c\u4ef6\u5b9e\u73b0\u4e86\u4f4e\u4e8e50\u5fae\u79d2\u7684\u89e3\u7801\u5ef6\u8fdf\uff0c\u5c55\u793a\u4e86\u4f7f\u7528\u5e7f\u6cdb\u5546\u7528\u786c\u4ef6\u5b9e\u73b0\u5b9e\u65f6\u53ef\u6269\u5c55\u91cf\u5b50\u89e3\u7801\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u8868\u9762\u7801\u867d\u5df2\u5b9e\u73b0\u4f4e\u4e8e\u9608\u503c\u7684\u6027\u80fd\uff0c\u4f46\u5176\u7f16\u7801\u901f\u7387\u968f\u7801\u8ddd\u589e\u52a0\u8d8b\u8fd1\u4e8e\u96f6\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u6700\u8fd1\u63d0\u51fa\u7684QLDPC\u7801\u5177\u6709\u6052\u5b9a\u901f\u7387\u548c\u6e10\u8fdb\u4f18\u6027\uff0c\u4f46\u89e3\u7801\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u89e3\u51b3QLDPC\u7801\u7684\u89e3\u7801\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u5b9e\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e76\u884c\u5316\u7684\u7f6e\u4fe1\u4f20\u64ad\u89e3\u7801\u5668\uff0c\u5229\u7528GPU\u786c\u4ef6\u52a0\u901f\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u5e76\u884c\u6027\u4ee5\u5728\u76ee\u6807\u5ef6\u8fdf\u8303\u56f4\u5185\u6700\u5927\u5316\u6027\u80fd\u3002\u9488\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684QLDPC\u7801\uff08\u5982[[784, 24, 24]]\u7801\uff09\u8fdb\u884c\u4e86\u5b9e\u73b0\u548c\u6d4b\u8bd5\u3002", "result": "\u5728\u5546\u7528GPU\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u4e8e50\u5fae\u79d2\u7684\u89e3\u7801\u5ef6\u8fdf\uff08\u5bf9[[784, 24, 24]]\u7801\uff09\u548c\u4f4e\u81f323.3\u5fae\u79d2\u7684\u89e3\u7801\u5ef6\u8fdf\uff08\u5bf9\u5c0f\u89c4\u6a21\u7801\uff09\uff0c\u6ee1\u8db3\u4e86\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u5468\u671f\u7684\u4e25\u683c\u65f6\u95f4\u7ea6\u675f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u5e7f\u6cdb\u53ef\u5f97\u7684\u5546\u7528\u786c\u4ef6\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u4e14\u53ef\u6269\u5c55\u7684\u6e10\u8fdb\u4f18\u6027\u91cf\u5b50\u7801\u89e3\u7801\uff0c\u4e3a\u8d85\u8d8a\u8868\u9762\u7801\u7684\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07301", "pdf": "https://arxiv.org/pdf/2508.07301", "abs": "https://arxiv.org/abs/2508.07301", "authors": ["Abasi-amefon Obot Affia-Jomants", "Alexander Serebrenik", "James D. Herbsleb", "Alexander Nolte"], "title": "In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons", "categories": ["cs.HC", "cs.CY"], "comment": "Accepted in Proceedings of the ACM on Human Computer Interaction\n  (CSCW'25)", "summary": "Hybrid hackathons, which combine in-person and online participation, present\nunique challenges for organizers and participants. Although such events are\nincreasingly conducted, research on them remains fragmented, with limited\nintegration between hackathon studies and hybrid collaboration. Existing\nstrategies for in-person or online-only events often fail to address the unique\nchallenges of hybrid formats, such as managing communication across physical\nand virtual spaces. Our work addresses this gap by examining how hybrid\nhackathons function, analyzing how organizers structure these events and how\nparticipants navigate hybrid-specific challenges. Drawing on established\ntheories of hybrid collaboration, we examine key dimensions - synchronicity,\nphysical distribution, dynamic transitions, and technological infrastructure -\nthat shape collaboration in hybrid events. Through an exploratory case study of\nthree hackathon events, we analyze how these dimensions are implemented and\ntheir effects on participant experiences. Our findings reveal differing\norganizer considerations of the hybrid dimensions in the hackathon design,\nleading to distinct experiences for participants. Implementation styles -\nfavoring in-person, online, or balanced participation - led to varied\nparticipant experiences, affecting access to resources, communication, and team\ncoordination. Organizers in our study also relied on technology to bridge\nhybrid interactions, but overlooked critical aspects like time-zone management,\ndynamic transitions, and targeted support for hybrid teams. Additionally,\nparticipants in their teams responded to gaps in event scaffolding by adapting\ncollaboration strategies, revealing gaps in organizers' preparedness for hybrid\nevents. Learning from our findings, we offer practical recommendations when\norganizing hybrid hackathon events and recommendations to participants when\nattending them.", "AI": {"tldr": "\u603b\u7ed3\u4e86\u6df7\u5408\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7684\u72ec\u7279\u6311\u6218\u3001\u7ec4\u7ec7\u8005\u4e0e\u53c2\u4e0e\u8005\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u6df7\u5408\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7684\u72ec\u7279\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u6df7\u5408\u534f\u4f5c\u4e0e\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u6027\u6848\u4f8b\u7814\u7a76\u5206\u6790\u4e86\u4e09\u4e2a\u9ed1\u5ba2\u9a6c\u62c9\u677e\u4e8b\u4ef6\uff0c\u8003\u5bdf\u4e86\u540c\u6b65\u6027\u3001\u7269\u7406\u5206\u5e03\u3001\u52a8\u6001\u8fc7\u6e21\u548c\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u7b49\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0\u7ec4\u7ec7\u8005\u5bf9\u6df7\u5408\u7ef4\u5ea6\u7684\u4e0d\u540c\u8003\u8651\u5bfc\u81f4\u53c2\u4e0e\u8005\u4f53\u9a8c\u5dee\u5f02\uff0c\u6280\u672f\u8fd0\u7528\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u53c2\u4e0e\u8005\u9700\u81ea\u6211\u8c03\u6574\u534f\u4f5c\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7ec4\u7ec7\u6df7\u5408\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u8005\u548c\u53c2\u4e0e\u8005\u66f4\u597d\u5730\u5e94\u5bf9\u6df7\u5408\u5f62\u5f0f\u7684\u6311\u6218\u3002"}}
{"id": "2508.08068", "pdf": "https://arxiv.org/pdf/2508.08068", "abs": "https://arxiv.org/abs/2508.08068", "authors": ["Yuval Efron", "Joachim Neu", "Toniann Pitassi"], "title": "Fully-Fluctuating Participation in Sleepy Consensus", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations\nin participation of miners throughout time, so long as, at any point in time, a\nmajority of hash power is honest. In recent years, however, the pendulum has\nshifted in favor of proof-of-stake-based consensus protocols. There, the sleepy\nmodel is the most prominent model for handling fluctuating participation of\nnodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its\nrobustness to drastic fluctuations in participation levels, with\nstate-of-the-art protocols making various restrictive assumptions. In this\nwork, we present a new adversary model, called external adversary. Intuitively,\nin our model, corrupt nodes do not divulge information about their secret keys.\nIn this model, we show that protocols in the sleepy model can meaningfully\nclaim to remain secure against fully fluctuating participation, without\ncompromising efficiency or corruption resilience. Our adversary model is quite\nnatural, and arguably naturally captures the process via which malicious\nbehavior arises in protocols, as opposed to traditional worst-case modeling. On\ntop of which, the model is also theoretically appealing, circumventing a\nbarrier established in a recent work of Malkhi, Momose, and Ren.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u654c\u624b\u6a21\u578b\u2014\u2014\u5916\u90e8\u654c\u624b\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u534f\u8bae\u5728\u53c2\u4e0e\u5ea6\u5927\u5e45\u6ce2\u52a8\u65f6\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6743\u76ca\u8bc1\u660e\uff08PoS\uff09\u7684\u5171\u8bc6\u534f\u8bae\u5728\u53c2\u4e0e\u5ea6\u6ce2\u52a8\u65f6\u9650\u5236\u8f83\u591a\uff0c\u65e0\u6cd5\u50cf\u6bd4\u7279\u5e01\u5de5\u4f5c\u91cf\u8bc1\u660e\uff08PoW\uff09\u90a3\u6837\u5177\u5907\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u800c\u7761\u7720\u6a21\u578b\u4e2d\u7684\u534f\u8bae\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5916\u90e8\u654c\u624b\u6a21\u578b\u7684\u65b0\u6a21\u5f0f\uff0c\u5047\u8bbe\u8150\u8d25\u8282\u70b9\u4e0d\u4f1a\u6cc4\u9732\u5176\u79c1\u94a5\u4fe1\u606f\u3002", "result": "\u5728\u8be5\u6a21\u578b\u4e0b\uff0c\u7761\u7720\u6a21\u578b\u4e2d\u7684\u534f\u8bae\u80fd\u591f\u5728\u4fdd\u8bc1\u6548\u7387\u548c\u6297\u8150\u8d25\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u5e94\u5bf9\u53c2\u4e0e\u5ea6\u7684\u5b8c\u5168\u6ce2\u52a8\u3002", "conclusion": "\u5916\u90e8\u654c\u624b\u6a21\u578b\u81ea\u7136\u4e14\u5b9e\u7528\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6700\u574f\u60c5\u51b5\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u6709\u5438\u5f15\u529b\u3002"}}
{"id": "2508.07390", "pdf": "https://arxiv.org/pdf/2508.07390", "abs": "https://arxiv.org/abs/2508.07390", "authors": ["Gustavo Moreira", "Leonardo Ferreira", "Carolina Veiga", "Maryam Hosseini", "Fabio Miranda"], "title": "Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at IEEE VIS 2025. Urbanite is available at\n  https://urbantk.org/urbanite", "summary": "With the growing availability of urban data and the increasing complexity of\nsocietal challenges, visual analytics has become essential for deriving\ninsights into pressing real-world problems. However, analyzing such data is\ninherently complex and iterative, requiring expertise across multiple domains.\nThe need to manage diverse datasets, distill intricate workflows, and integrate\nvarious analytical methods presents a high barrier to entry, especially for\nresearchers and urban experts who lack proficiency in data management, machine\nlearning, and visualization. Advancements in large language models offer a\npromising solution to lower the barriers to the construction of analytics\nsystems by enabling users to specify intent rather than define precise\ncomputational operations. However, this shift from explicit operations to\nintent-based interaction introduces challenges in ensuring alignment throughout\nthe design and development process. Without proper mechanisms, gaps can emerge\nbetween user intent, system behavior, and analytical outcomes. To address these\nchallenges, we propose Urbanite, a framework for human-AI collaboration in\nurban visual analytics. Urbanite leverages a dataflow-based model that allows\nusers to specify intent at multiple scopes, enabling interactive alignment\nacross the specification, process, and evaluation stages of urban analytics.\nBased on findings from a survey to uncover challenges, Urbanite incorporates\nfeatures to facilitate explainability, multi-resolution definition of tasks\nacross dataflows, nodes, and parameters, while supporting the provenance of\ninteractions. We demonstrate Urbanite's effectiveness through usage scenarios\ncreated in collaboration with urban experts. Urbanite is available at\nhttps://urbantk.org/urbanite.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Urbanite\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u964d\u4f4e\u57ce\u5e02\u53ef\u89c6\u5316\u5206\u6790\u7684\u590d\u6742\u5ea6\uff0c\u5229\u7528\u610f\u56fe\u4ea4\u4e92\u548c\u6570\u636e\u6d41\u6a21\u578b\u5b9e\u73b0\u591a\u9636\u6bb5\u5bf9\u9f50\u3002", "motivation": "\u57ce\u5e02\u6570\u636e\u5206\u6790\u590d\u6742\u4e14\u8de8\u9886\u57df\uff0c\u4f20\u7edf\u65b9\u6cd5\u95e8\u69db\u9ad8\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u964d\u4f4e\u95e8\u69db\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u610f\u56fe\u4e0e\u7ed3\u679c\u5bf9\u2eec\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faUrbanite\u6846\u67b6\uff0c\u91c7\u7528\u6570\u636e\u6d41\u6a21\u578b\u652f\u6301\u591a\u5c42\u6b21\u610f\u56fe\u6307\u5b9a\uff0c\u5e76\u5f15\u5165\u53ef\u89e3\u91ca\u6027\u548c\u4efb\u52a1\u591a\u5206\u8fa8\u7387\u5b9a\u4e49\u7b49\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u4e0e\u57ce\u5e02\u4e13\u5bb6\u5408\u4f5c\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86Urbanite\u7684\u6709\u6548\u6027\uff0c\u6846\u67b6\u53ef\u8bbf\u95ee\u4e8ehttps://urbantk.org/urbanite\u3002", "conclusion": "Urbanite\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u548c\u6570\u636e\u6d41\u6a21\u578b\uff0c\u6210\u529f\u964d\u4f4e\u4e86\u57ce\u5e02\u53ef\u89c6\u5316\u5206\u6790\u7684\u95e8\u69db\uff0c\u5e76\u89e3\u51b3\u4e86\u610f\u56fe\u5bf9\u2eec\u95ee\u9898\u3002"}}
{"id": "2508.07496", "pdf": "https://arxiv.org/pdf/2508.07496", "abs": "https://arxiv.org/abs/2508.07496", "authors": ["Sanjana Srabanti", "G. Elisabeta Marai", "Fabio Miranda"], "title": "StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data", "categories": ["cs.HC", "cs.CY"], "comment": "Accepted at IEEE VIS 2025. StreetWeave is available at\n  https://urbantk.org/streetweave", "summary": "The visualization and analysis of street and pedestrian networks are\nimportant to various domain experts, including urban planners, climate\nresearchers, and health experts. This has led to the development of new\ntechniques for street and pedestrian network visualization, expanding how data\ncan be shown and understood more effectively. Despite their increasing\nadoption, there is no established design framework to guide the creation of\nthese visualizations while addressing the diverse requirements of various\ndomains. When exploring a feature of interest, domain experts often need to\ntransform, integrate, and visualize a combination of thematic data (e.g.,\ndemographic, socioeconomic, pollution) and physical data (e.g., zip codes,\nstreet networks), often spanning multiple spatial and temporal scales. This not\nonly complicates the process of visual data exploration and system\nimplementation for developers but also creates significant entry barriers for\nexperts who lack a background in programming. With this in mind, in this paper,\nwe reviewed 45 studies utilizing street-overlaid visualizations to understand\nhow they are used. Through qualitative coding of these visualizations, we\nanalyzed three key aspects of street and pedestrian network visualization\nusage: the analytical purpose they serve, the visualization approaches\nemployed, and the data sources used in their creation. Building on this design\nspace, we introduce StreetWeave, a declarative grammar for designing custom\nvisualizations of multivariate spatial network data across multiple\nresolutions. We demonstrate how StreetWeave can be used to create various\nstreet-overlaid visualizations, enabling effective exploration and analysis of\nspatial data. StreetWeave is available at https://urbantk.org/streetweave.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStreetWeave\u7684\u58f0\u660e\u5f0f\u8bed\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u8de8\u591a\u5206\u8fa8\u7387\u7684\u591a\u5143\u7a7a\u95f4\u7f51\u7edc\u6570\u636e\u53ef\u89c6\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8857\u9053\u548c\u884c\u4eba\u7f51\u7edc\u53ef\u89c6\u5316\u7f3a\u4e4f\u7edf\u4e00\u8bbe\u8ba1\u6846\u67b6\u7684\u95ee\u9898\u3002", "motivation": "\u8857\u9053\u548c\u884c\u4eba\u7f51\u7edc\u7684\u53ef\u89c6\u5316\u5bf9\u57ce\u5e02\u89c4\u5212\u3001\u6c14\u5019\u7814\u7a76\u548c\u5065\u5eb7\u4e13\u5bb6\u7b49\u9886\u57df\u4e13\u5bb6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bbe\u8ba1\u6846\u67b6\u6765\u6ee1\u8db3\u4e0d\u540c\u9886\u57df\u7684\u9700\u6c42\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5bf9\u975e\u7f16\u7a0b\u80cc\u666f\u7684\u4e13\u5bb6\u5b58\u5728\u9ad8\u95e8\u69db\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7f16\u7801\u5206\u6790\u4e8645\u9879\u7814\u7a76\u4e2d\u7684\u8857\u9053\u8986\u76d6\u53ef\u89c6\u5316\uff0c\u63a2\u8ba8\u4e86\u5176\u5206\u6790\u76ee\u7684\u3001\u53ef\u89c6\u5316\u65b9\u6cd5\u548c\u6570\u636e\u6765\u6e90\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86StreetWeave\u58f0\u660e\u5f0f\u8bed\u6cd5\u3002", "result": "StreetWeave\u80fd\u591f\u521b\u5efa\u591a\u79cd\u8857\u9053\u8986\u76d6\u53ef\u89c6\u5316\uff0c\u652f\u6301\u7a7a\u95f4\u6570\u636e\u7684\u6709\u6548\u63a2\u7d22\u548c\u5206\u6790\u3002", "conclusion": "StreetWeave\u4e3a\u8857\u9053\u548c\u884c\u4eba\u7f51\u7edc\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6613\u7528\u7684\u8bbe\u8ba1\u5de5\u5177\uff0c\u964d\u4f4e\u4e86\u975e\u7f16\u7a0b\u4e13\u5bb6\u7684\u4f7f\u7528\u95e8\u69db\u3002"}}
{"id": "2508.07497", "pdf": "https://arxiv.org/pdf/2508.07497", "abs": "https://arxiv.org/abs/2508.07497", "authors": ["Leonardo Ferreira", "Gustavo Moreira", "Fabio Miranda"], "title": "VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at IEEE VIS 2025. VA-Blueprint is available at\n  https://urbantk.org/va-blueprint", "summary": "Designing and building visual analytics (VA) systems is a complex, iterative\nprocess that requires the seamless integration of data processing, analytics\ncapabilities, and visualization techniques. While prior research has\nextensively examined the social and collaborative aspects of VA system\nauthoring, the practical challenges of developing these systems remain\nunderexplored. As a result, despite the growing number of VA systems, there are\nonly a few structured knowledge bases to guide their design and development. To\ntackle this gap, we propose VA-Blueprint, a methodology and knowledge base that\nsystematically reviews and categorizes the fundamental building blocks of urban\nVA systems, a domain particularly rich and representative due to its intricate\ndata and unique problem sets. Applying this methodology to an initial set of 20\nsystems, we identify and organize their core components into a multi-level\nstructure, forming an initial knowledge base with a structured blueprint for VA\nsystem development. To scale this effort, we leverage a large language model to\nautomate the extraction of these components for other 81 papers (completing a\ncorpus of 101 papers), assessing its effectiveness in scaling knowledge base\nconstruction. We evaluate our method through interviews with experts and a\nquantitative analysis of annotation metrics. Our contributions provide a deeper\nunderstanding of VA systems' composition and establish a practical foundation\nto support more structured, reproducible, and efficient system development.\nVA-Blueprint is available at https://urbantk.org/va-blueprint.", "AI": {"tldr": "\u63d0\u51fa\u4e86VA-Blueprint\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5316\u68b3\u7406\u4e86\u57ce\u5e02\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u77e5\u8bc6\u5e93\u5efa\u8bbe\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u77e5\u8bc6\u5e93\u6765\u6307\u5bfcVA\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u5f00\u53d1\uff0c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faVA-Blueprint\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5316\u5206\u7c7b\u57ce\u5e02VA\u7cfb\u7edf\u7684\u6784\u5efa\u5757\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u6269\u5c55\u77e5\u8bc6\u5e93\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b101\u7bc7\u8bba\u6587\u7684\u521d\u59cb\u77e5\u8bc6\u5e93\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u548c\u5b9a\u91cf\u5206\u6790\u8bc4\u4f30\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "VA-Blueprint\u4e3aVA\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\u652f\u6301\u3002"}}
{"id": "2508.07520", "pdf": "https://arxiv.org/pdf/2508.07520", "abs": "https://arxiv.org/abs/2508.07520", "authors": ["Baihan Lin"], "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds.", "AI": {"tldr": "Conversational DNA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89c6\u5316\u8bed\u8a00\uff0c\u901a\u8fc7\u751f\u7269\u9690\u55bb\u63ed\u793a\u5bf9\u8bdd\u7684\u65f6\u5e8f\u7ed3\u6784\uff0c\u7528\u4e8e\u5206\u6790\u4eba\u7c7b\u3001\u4eba\u673a\u6216\u7fa4\u4f53\u5bf9\u8bdd\u7684\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u7684\u5bf9\u8bdd\u5206\u6790\u901a\u5e38\u5c06\u4e30\u5bcc\u7684\u4e92\u52a8\u7b80\u5316\u4e3a\u7edf\u8ba1\u6458\u8981\uff0c\u800c\u8be5\u65b9\u6cd5\u65e8\u5728\u66f4\u76f4\u89c2\u5730\u63ed\u793a\u5bf9\u8bdd\u4e2d\u7684\u9690\u542b\u7ed3\u6784\u548c\u4e92\u52a8\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u751f\u7269\u9690\u55bb\uff08\u5982\u94fe\u539a\u5ea6\u8868\u793a\u8bed\u8a00\u590d\u6742\u6027\uff0c\u989c\u8272\u6e10\u53d8\u8868\u793a\u60c5\u611f\u8f68\u8ff9\uff09\u8bbe\u8ba1\u53ef\u89c6\u5316\u8bed\u8a00\uff0c\u5e76\u5e94\u7528\u4e8e\u6cbb\u7597\u5bf9\u8bdd\u548c\u91cd\u8981\u4eba\u673a\u5bf9\u8bdd\u7684\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u80fd\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u4e92\u52a8\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3\u5bf9\u8bdd\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u5408\u6570\u636e\u53ef\u89c6\u5316\u548c\u4eba\u673a\u4ea4\u4e92\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u521b\u610f\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u5bf9\u8bdd\u7684\u610f\u4e49\u3002"}}
{"id": "2508.07576", "pdf": "https://arxiv.org/pdf/2508.07576", "abs": "https://arxiv.org/abs/2508.07576", "authors": ["Kenneth Ge", "Ryan Paul", "Priscilla Zhang", "JooYoung Seo"], "title": "Phoenix: A Novel Context-Aware Voice-Powered Math Equation Workspace and Editor", "categories": ["cs.HC"], "comment": "Published at ASSETS '25", "summary": "Writing mathematical notation requires substantial effort, diverting\ncognitive resources from conceptual understanding to documentation mechanics,\nsignificantly impacting individuals with fine motor disabilities (FMDs).\nCurrent limits of speech-based math technologies rely on precise dictation of\nmath symbols and unintuitive command-based interfaces. We present a novel\nvoice-powered math workspace, applying neuroscience insights to create an\nintuitive problem-solving environment. To minimize cognitive load, we leverage\nlarge language models with our novel context engine to support natural language\ninteraction. Ultimately, we enable fluid mathematical engagement for\nindividuals with FMDs -- freed from mechanical constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8bed\u97f3\u9a71\u52a8\u7684\u6570\u5b66\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5229\u7528\u795e\u7ecf\u79d1\u5b66\u6d1e\u5bdf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e2e\u52a9\u6709\u7cbe\u7ec6\u52a8\u4f5c\u969c\u788d\u7684\u4eba\u66f4\u81ea\u7136\u5730\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u97f3\u6570\u5b66\u6280\u672f\u4f9d\u8d56\u7cbe\u786e\u7684\u7b26\u53f7\u542c\u5199\u548c\u547d\u4ee4\u5f0f\u754c\u9762\uff0c\u5bf9\u7cbe\u7ec6\u52a8\u4f5c\u969c\u788d\u8005\u4e0d\u53cb\u597d\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u7406\u5ff5\u548c\u4e0a\u4e0b\u6587\u5f15\u64ce\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u76f4\u89c2\u7684\u95ee\u9898\u89e3\u51b3\u73af\u5883\uff0c\u4f7f\u7cbe\u7ec6\u52a8\u4f5c\u969c\u788d\u8005\u80fd\u591f\u6d41\u7545\u5730\u53c2\u4e0e\u6570\u5b66\u6d3b\u52a8\u3002", "conclusion": "\u8be5\u6280\u672f\u6210\u529f\u89e3\u653e\u4e86\u7cbe\u7ec6\u52a8\u4f5c\u969c\u788d\u8005\u5728\u6570\u5b66\u8868\u8fbe\u4e2d\u7684\u673a\u68b0\u7ea6\u675f\uff0c\u63d0\u5347\u4e86\u4f7f\u7528\u4f53\u9a8c\u3002"}}
{"id": "2508.07617", "pdf": "https://arxiv.org/pdf/2508.07617", "abs": "https://arxiv.org/abs/2508.07617", "authors": ["Sarah Jabbour", "David Fouhey", "Nikola Banovic", "Stephanie D. Shepard", "Ella Kazerooni", "Michael W. Sjoding", "Jenna Wiens"], "title": "On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making", "categories": ["cs.HC", "cs.AI"], "comment": "14 pages, 10 figures, 5 tables", "summary": "AI has the potential to augment human decision making. However, even\nhigh-performing models can produce inaccurate predictions when deployed. These\ninaccuracies, combined with automation bias, where humans overrely on AI\npredictions, can result in worse decisions. Selective prediction, in which\npotentially unreliable model predictions are hidden from users, has been\nproposed as a solution. This approach assumes that when AI abstains and informs\nthe user so, humans make decisions as they would without AI involvement. To\ntest this assumption, we study the effects of selective prediction on human\ndecisions in a clinical context. We conducted a user study of 259 clinicians\ntasked with diagnosing and treating hospitalized patients. We compared their\nbaseline performance without any AI involvement to their AI-assisted accuracy\nwith and without selective prediction. Our findings indicate that selective\nprediction mitigates the negative effects of inaccurate AI in terms of decision\naccuracy. Compared to no AI assistance, clinician accuracy declined when shown\ninaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),\nbut recovered under selective prediction (64% [95% CI: 54%-73%]). However,\nwhile selective prediction nearly maintains overall accuracy, our results\nsuggest that it alters patterns of mistakes: when informed the AI abstains,\nclinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%\nincrease in missed treatments) compared to no AI input at all. Our findings\nunderscore the importance of empirically validating assumptions about how\nhumans engage with AI within human-AI systems.", "AI": {"tldr": "\u9009\u62e9\u6027\u9884\u6d4b\u53ef\u4ee5\u51cf\u8f7b\u4e0d\u51c6\u786eAI\u5bf9\u51b3\u7b56\u51c6\u786e\u6027\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u4f1a\u6539\u53d8\u9519\u8bef\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6f0f\u8bca\u548c\u6f0f\u6cbb\u589e\u52a0\u3002", "motivation": "\u7814\u7a76\u9009\u62e9\u6027\u9884\u6d4b\u5bf9\u4e34\u5e8a\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u51cf\u5c11\u4e0d\u51c6\u786eAI\u9884\u6d4b\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9259\u540d\u4e34\u5e8a\u533b\u751f\u7684\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u57fa\u7ebf\u8868\u73b0\u3001AI\u8f85\u52a9\u53ca\u9009\u62e9\u6027\u9884\u6d4b\u8f85\u52a9\u7684\u51b3\u7b56\u51c6\u786e\u6027\u3002", "result": "\u9009\u62e9\u6027\u9884\u6d4b\u7ef4\u6301\u4e86\u603b\u4f53\u51c6\u786e\u6027\uff0864% vs 56%\uff09\uff0c\u4f46\u589e\u52a0\u4e86\u6f0f\u8bca\uff0818%\uff09\u548c\u6f0f\u6cbb\uff0835%\uff09\u3002", "conclusion": "\u9700\u5b9e\u8bc1\u9a8c\u8bc1\u4eba\u673a\u4ea4\u4e92\u5047\u8bbe\uff0c\u9009\u62e9\u6027\u9884\u6d4b\u867d\u6709\u6548\u4f46\u9700\u6ce8\u610f\u5176\u526f\u4f5c\u7528\u3002"}}
{"id": "2508.07620", "pdf": "https://arxiv.org/pdf/2508.07620", "abs": "https://arxiv.org/abs/2508.07620", "authors": ["Andr\u00e9s Eduardo Fuentes-Cort\u00e1zar", "Alejandra Rivera-Hern\u00e1ndez", "Jos\u00e9 Rafael Rojano-C\u00e1ceres"], "title": "Are UX evaluation methods truly accessible", "categories": ["cs.HC", "H.5.2"], "comment": "24 pages, 8 figures, 8 tables, submitted to TecnoL\\'ogicas ISNN\n  0123-7799", "summary": "Providing an equitable and inclusive user experience (UX) for people with\ndisabilities (PWD) is a central goal of accessible design. In the specific case\nof Deaf users, whose hearing impairments impact language development and\ncommunication, it is essential to consider their specific needs during software\nevaluation processes. This study aimed to analyze a set of UX evaluation\nmethods suggested in the literature as suitable for Deaf individuals, with the\ngoal of validating their level of accessibility in real-world contexts. The\nresearch was based on a critical review and practical application of these\nmethods, identifying their strengths and limitations in relation to the\ninteraction, perception, and comprehension of Deaf users. Traditional\nevaluation instruments, commonly designed for hearing individuals, pose\nsignificant barriers when applied to Deaf users due to their re-liance on\nauditory and cognitive abilities, as well as the lack of consideration for\ncommu-nicational accessibility. The results show that although these methods\nare frequently rec-ommended, they exhibit critical shortcomings that hinder the\ncollection of accurate and representative data. It is concluded that it is\nessential to adapt UX evaluation methods to ensure genuinely accessible\nprocesses that address the communicative and cognitive needs of the Deaf\ncommunity and accurately reflect their user experience.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709\u7684\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\u8bc4\u4f30\u65b9\u6cd5\u867d\u5e38\u88ab\u63a8\u8350\u7528\u4e8e\u804b\u4eba\u7528\u6237\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u969c\u788d\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u9002\u5e94\u804b\u4eba\u7fa4\u4f53\u7684\u6c9f\u901a\u4e0e\u8ba4\u77e5\u9700\u6c42\u3002", "motivation": "\u4e3a\u786e\u4fdd\u804b\u4eba\u7528\u6237\u5728\u8f6f\u4ef6\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u9700\u6c42\u5f97\u5230\u5145\u5206\u8003\u8651\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u6587\u732e\u4e2d\u63a8\u8350\u7684UX\u8bc4\u4f30\u65b9\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u6587\u732e\u7efc\u8ff0\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5206\u6790\u4e86\u9002\u5408\u804b\u4eba\u7684UX\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u5176\u4ea4\u4e92\u3001\u611f\u77e5\u548c\u7406\u89e3\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u8bc4\u4f30\u5de5\u5177\u56e0\u4f9d\u8d56\u542c\u89c9\u548c\u8ba4\u77e5\u80fd\u529b\u4e14\u7f3a\u4e4f\u6c9f\u901a\u53ef\u8bbf\u95ee\u6027\uff0c\u5bf9\u804b\u4eba\u7528\u6237\u5b58\u5728\u91cd\u5927\u969c\u788d\uff0c\u5bfc\u81f4\u6570\u636e\u6536\u96c6\u4e0d\u51c6\u786e\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u9700\u6539\u8fdbUX\u8bc4\u4f30\u65b9\u6cd5\uff0c\u786e\u4fdd\u5176\u771f\u6b63\u6ee1\u8db3\u804b\u4eba\u7fa4\u4f53\u7684\u9700\u6c42\uff0c\u51c6\u786e\u53cd\u6620\u5176\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.07658", "pdf": "https://arxiv.org/pdf/2508.07658", "abs": "https://arxiv.org/abs/2508.07658", "authors": ["Shuning Zhang", "Gengrui Zhang", "Yibo Meng", "Ziyi Zhang", "Hantao Zhao", "Xin Yi", "Hewu Li"], "title": "Through Their Eyes: User Perceptions on Sensitive Attribute Inference of Social Media Videos by Visual Language Models", "categories": ["cs.HC"], "comment": null, "summary": "The rapid advancement of Visual Language Models (VLMs) has enabled\nsophisticated analysis of visual content, leading to concerns about the\ninference of sensitive user attributes and subsequent privacy risks. While\ntechnical capabilities of VLMs are increasingly studied, users' understanding,\nperceptions, and reactions to these inferences remain less explored, especially\nconcerning videos uploaded on the social media. This paper addresses this gap\nthrough a semi-structured interview (N=17), investigating user perspectives on\nVLM-driven sensitive attribute inference from their visual data. Findings\nreveal that users perceive VLMs as capable of inferring a range of attributes,\nincluding location, demographics, and socioeconomic indicators, often with\nunsettling accuracy. Key concerns include unauthorized identification, misuse\nof personal information, pervasive surveillance, and harm from inaccurate\ninferences. Participants reported employing various mitigation strategies,\nthough with skepticism about their ultimate effectiveness against advanced AI.\nUsers also articulate clear expectations for platforms and regulators,\nemphasizing the need for enhanced transparency, user control, and proactive\nprivacy safeguards. These insights are crucial for guiding the development of\nresponsible AI systems, effective privacy-enhancing technologies, and informed\npolicymaking that aligns with user expectations and societal values.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u7528\u6237\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ece\u89c6\u89c9\u6570\u636e\u63a8\u65ad\u654f\u611f\u5c5e\u6027\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u7528\u6237\u5bf9\u672a\u7ecf\u6388\u6743\u7684\u8bc6\u522b\u548c\u4e2a\u4eba\u4fe1\u606f\u6ee5\u7528\u6df1\u611f\u62c5\u5fe7\uff0c\u63d0\u51fa\u5bf9\u5e73\u53f0\u900f\u660e\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u9700\u6c42\u3002", "motivation": "\u586b\u8865\u5173\u4e8e\u7528\u6237\u5bf9VLM\u63a8\u65ad\u654f\u611f\u5c5e\u6027\u7684\u7406\u89e3\u3001\u611f\u77e5\u53ca\u53cd\u5e94\u7684\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u89c6\u9891\u9886\u57df\u3002", "method": "\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff08N=17\uff09\uff0c\u8c03\u67e5\u7528\u6237\u5bf9VLM\u4ece\u5176\u89c6\u89c9\u6570\u636e\u4e2d\u63a8\u65ad\u654f\u611f\u5c5e\u6027\u7684\u89c2\u70b9\u3002", "result": "\u7528\u6237\u8ba4\u4e3aVLM\u80fd\u9ad8\u7cbe\u5ea6\u63a8\u65ad\u591a\u79cd\u654f\u611f\u5c5e\u6027\uff0c\u4e3b\u8981\u62c5\u5fe7\u5305\u62ec\u672a\u7ecf\u6388\u6743\u7684\u8bc6\u522b\u548c\u4e2a\u4eba\u4fe1\u606f\u6ee5\u7528\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u7f13\u89e3\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u5f00\u53d1\u8d1f\u8d23\u4efbAI\u7cfb\u7edf\u3001\u9690\u79c1\u589e\u5f3a\u6280\u672f\u53ca\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u653f\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.07664", "pdf": "https://arxiv.org/pdf/2508.07664", "abs": "https://arxiv.org/abs/2508.07664", "authors": ["Shuning Zhang", "Rongjun Ma", "Ying Ma", "Shixuan Li", "Yiqun Xu", "Xin Yi", "Hewu Li"], "title": "Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory", "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrating memory\nfunctionalities to provide personalized and context-aware interactions.\nHowever, user understanding, practices and expectations regarding these memory\nsystems are not yet well understood. This paper presents a thematic analysis of\nsemi-structured interviews with 18 users to explore their mental models of\nLLM's Retrieval Augmented Generation (RAG)-based memory, current usage\npractices, perceived benefits and drawbacks, privacy concerns and expectations\nfor future memory systems. Our findings reveal diverse and often incomplete\nmental models of how memory operates. While users appreciate the potential for\nenhanced personalization and efficiency, significant concerns exist regarding\nprivacy, control and the accuracy of remembered information. Users express a\ndesire for granular control over memory generation, management, usage and\nupdating, including clear mechanisms for reviewing, editing, deleting and\ncategorizing memories, as well as transparent insight into how memories and\ninferred information are used. We discuss design implications for creating more\nuser-centric, transparent, and trustworthy LLM memory systems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bbf\u8c0818\u540d\u7528\u6237\uff0c\u63a2\u8ba8\u4e86\u4ed6\u4eec\u5bf9LLM\u4e2d\u57fa\u4e8eRAG\u7684\u8bb0\u5fc6\u7cfb\u7edf\u7684\u7406\u89e3\u3001\u4f7f\u7528\u5b9e\u8df5\u548c\u671f\u671b\uff0c\u63ed\u793a\u4e86\u7528\u6237\u5bf9\u8bb0\u5fc6\u7cfb\u7edf\u7684\u591a\u6837\u5316\u4f46\u4e0d\u5b8c\u6574\u7684\u8ba4\u77e5\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u66f4\u900f\u660e\u3001\u53ef\u4fe1\u8d56\u7cfb\u7edf\u7684\u5efa\u8bae\u3002", "motivation": "\u4e86\u89e3\u7528\u6237\u5bf9LLM\u8bb0\u5fc6\u7cfb\u7edf\u7684\u7406\u89e3\u3001\u5b9e\u8df5\u548c\u671f\u671b\uff0c\u4ee5\u6539\u8fdb\u7cfb\u7edf\u7684\u7528\u6237\u4e2d\u5fc3\u548c\u900f\u660e\u5ea6\u3002", "method": "\u5bf918\u540d\u7528\u6237\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u5e76\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u7528\u6237\u5bf9\u8bb0\u5fc6\u7cfb\u7edf\u7684\u8ba4\u77e5\u591a\u6837\u5316\u4f46\u4e0d\u5b8c\u6574\uff0c\u5173\u6ce8\u9690\u79c1\u3001\u63a7\u5236\u548c\u51c6\u786e\u6027\uff0c\u5e0c\u671b\u6709\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u8bbe\u8ba1LLM\u8bb0\u5fc6\u7cfb\u7edf\u65f6\u9700\u66f4\u6ce8\u91cd\u7528\u6237\u4e2d\u5fc3\u3001\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u4ee5\u63d0\u5347\u4fe1\u4efb\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.07672", "pdf": "https://arxiv.org/pdf/2508.07672", "abs": "https://arxiv.org/abs/2508.07672", "authors": ["Shuning Zhang", "Ying Ma", "Jingruo Chen", "Simin Li", "Xin Yi", "Hewu Li"], "title": "Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences", "categories": ["cs.HC"], "comment": null, "summary": "The proliferation of AI agents, with their complex and context-dependent\nactions, renders conventional privacy paradigms obsolete. This position paper\nargues that the current model of privacy management, rooted in a user's\nunilateral control over a passive tool, is inherently mismatched with the\ndynamic and interactive nature of AI agents. We contend that ensuring effective\nprivacy protection necessitates that the agents proactively align with users'\nprivacy preferences instead of passively waiting for the user to control. To\nground this shift, and using personalized conversational recommendation agents\nas a case, we propose a conceptual framework built on Contextual Integrity (CI)\ntheory and Privacy Calculus theory. This synthesis first reframes automatically\ncontrolling users' privacy as an alignment problem, where AI agents initially\ndid not know users' preferences, and would learn their privacy preferences\nthrough implicit or explicit feedback. Upon receiving the preference feedback,\nthe agents used alignment and Pareto optimization for aligning preferences and\nbalancing privacy and utility. We introduced formulations and instantiations,\npotential applications, as well as five challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u4ee3\u7406\u7684\u52a8\u6001\u884c\u4e3a\u4f7f\u4f20\u7edf\u9690\u79c1\u7ba1\u7406\u8fc7\u65f6\uff0c\u4e3b\u5f20\u4ee3\u7406\u5e94\u4e3b\u52a8\u9002\u5e94\u7528\u6237\u9690\u79c1\u504f\u597d\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u60c5\u5883\u5b8c\u6574\u6027\u548c\u9690\u79c1\u8ba1\u7b97\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u9690\u79c1\u7ba1\u7406\u6a21\u5f0f\u4f9d\u8d56\u7528\u6237\u5355\u5411\u63a7\u5236\uff0c\u800cAI\u4ee3\u7406\u7684\u52a8\u6001\u4ea4\u4e92\u7279\u6027\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u60c5\u5883\u5b8c\u6574\u6027\u7406\u8bba\u548c\u9690\u79c1\u8ba1\u7b97\u7406\u8bba\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u9988\u5b66\u4e60\u7528\u6237\u9690\u79c1\u504f\u597d\uff0c\u5e76\u5229\u7528\u5bf9\u9f50\u548c\u5e15\u7d2f\u6258\u4f18\u5316\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\u3002", "result": "\u6846\u67b6\u5c06\u9690\u79c1\u7ba1\u7406\u91cd\u6784\u4e3a\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u5f62\u5f0f\u5316\u65b9\u6cd5\u3001\u5e94\u7528\u573a\u666f\u53ca\u4e94\u5927\u6311\u6218\u3002", "conclusion": "AI\u4ee3\u7406\u9700\u4e3b\u52a8\u9002\u5e94\u7528\u6237\u9690\u79c1\u504f\u597d\uff0c\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2508.07677", "pdf": "https://arxiv.org/pdf/2508.07677", "abs": "https://arxiv.org/abs/2508.07677", "authors": ["Parth G. Dangi", "Yogesh Kumar Meena"], "title": "Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration", "categories": ["cs.HC"], "comment": "7 pages, 5 figures, 2 tables, selected for presentation in special\n  session of System, Man, Cybernetics (SMC) conference, 2025. The codes\n  developed for this paper is available in the GitHub repository given in the\n  conclusion section of this paper", "summary": "Brain-machine interfaces (BMIs) have significantly advanced\nneuro-rehabilitation by enhancing motor control. However, accurately decoding\ncontinuous grasp force remains a challenge, limiting the effectiveness of BMI\napplications for fine motor tasks. Current models tend to prioritise\nalgorithmic complexity rather than incorporating neurophysiological insights\ninto force control, which is essential for developing effective neural\nengineering solutions. To address this, we propose EEGForceMap, an EEG-based\nmethodology that isolates signals from the premotor-parietal region and\nextracts task-specific components. We construct three distinct time-frequency\nfeature sets, which are validated by comparing them with prior studies, and use\nthem for force prediction with linear, non-linear, and deep learning-based\nregressors. The performance of these regressors was evaluated on the\nWAY-EEG-GAL dataset that includes 12 subjects. Our results show that\nintegrating EEGForceMap approach with regressor models yields a 61.7%\nimprovement in subject-specific conditions (R-squared = 0.815) and a 55.7%\nimprovement in subject-independent conditions (R-squared = 0.785) over the\nstate-of-the-art kinematic decoder models. Furthermore, an ablation study\nconfirms that each preprocessing step significantly enhances decoding accuracy.\nThis work contributes to the advancement of responsive BMIs for stroke\nrehabilitation and assistive robotics by improving EEG-based decoding of\ndynamic grasp force.", "AI": {"tldr": "EEGForceMap\u65b9\u6cd5\u901a\u8fc7\u63d0\u53d6\u8fd0\u52a8\u76f8\u5173\u8111\u533a\u4fe1\u53f7\u548c\u6539\u8fdb\u7279\u5f81\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u63e1\u529b\u89e3\u7801\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8111\u673a\u63a5\u53e3\u5728\u89e3\u7801\u8fde\u7eed\u63e1\u529b\u65f6\u6548\u679c\u6709\u9650\uff0c\u7f3a\u4e4f\u795e\u7ecf\u751f\u7406\u5b66\u6d1e\u5bdf\uff0cEEGForceMap\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eEEG\u4fe1\u53f7\uff0c\u4ece\u8fd0\u52a8\u524d\u533a-\u9876\u53f6\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u6210\u5206\uff0c\u6784\u5efa\u4e09\u79cd\u65f6\u9891\u7279\u5f81\u96c6\uff0c\u5e76\u7528\u591a\u79cd\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u7279\u5b9a\u548c\u72ec\u7acb\u88ab\u8bd5\u6761\u4ef6\u4e0b\uff0c\u6027\u80fd\u5206\u522b\u63d0\u534761.7%\u548c55.7%\uff0cR\u5e73\u65b9\u8fbe0.815\u548c0.785\u3002", "conclusion": "EEGForceMap\u4e3a\u4e2d\u98ce\u5eb7\u590d\u548c\u8f85\u52a9\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u52a8\u6001\u63e1\u529b\u89e3\u7801\u65b9\u6cd5\u3002"}}
{"id": "2508.07730", "pdf": "https://arxiv.org/pdf/2508.07730", "abs": "https://arxiv.org/abs/2508.07730", "authors": ["Mingyang Su", "Chao Liu", "Jingling Zhang", "WU Shuang", "Mingming Fan"], "title": "SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums", "categories": ["cs.HC"], "comment": null, "summary": "Offering diverse perspectives on a museum artifact can deepen visitors'\nunderstanding and help avoid the cognitive limitations of a single narrative,\nultimately enhancing their overall experience. Physical museums promote\ndiversity through visitor interactions. However, it remains a challenge to\npresent multiple voices appropriately while attracting and sustaining a\nvisitor's attention in the virtual museum. Inspired by recent studies that show\nthe effectiveness of LLM-powered multi-agents in presenting different opinions\nabout an event, we propose SimViews, an interactive multi-agent system that\nsimulates visitor-to-visitor conversational patterns to promote the\npresentation of diverse perspectives. The system employs LLM-powered\nmulti-agents that simulate virtual visitors with different professional\nidentities, providing diverse interpretations of artifacts. Additionally, we\nconstructed 4 conversational patterns between users and agents to simulate\nvisitor interactions. We conducted a within-subject study with 20 participants,\ncomparing SimViews to a traditional single-agent condition. Our results show\nthat SimViews effectively facilitates the presentation of diverse perspectives\nthrough conversations, enhancing participants' understanding of viewpoints and\nengagement within the virtual museum.", "AI": {"tldr": "SimViews\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u591a\u4ee3\u7406\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u6e38\u5ba2\u95f4\u7684\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u5c55\u793a\u535a\u7269\u9986\u5c55\u54c1\u7684\u591a\u5143\u89c6\u89d2\uff0c\u63d0\u9ad8\u6e38\u5ba2\u7406\u89e3\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u4e3a\u865a\u62df\u535a\u7269\u9986\u4e2d\u5c55\u793a\u591a\u5143\u89c6\u89d2\u4ee5\u907f\u514d\u5355\u4e00\u53d9\u4e8b\u7684\u8ba4\u77e5\u5c40\u9650\uff0c\u63d0\u5347\u6e38\u5ba2\u4f53\u9a8c\u3002", "method": "\u5229\u7528LLM\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u4eba\u6a21\u62df\u4e0d\u540c\u4e13\u4e1a\u8eab\u4efd\u7684\u865a\u62df\u6e38\u5ba2\uff0c\u6784\u5efa4\u79cd\u5bf9\u8bdd\u6a21\u5f0f\u6a21\u62df\u6e38\u5ba2\u4e92\u52a8\u3002", "result": "SimViews\u80fd\u6709\u6548\u901a\u8fc7\u5bf9\u8bdd\u5c55\u793a\u591a\u5143\u89c6\u89d2\uff0c\u63d0\u5347\u53c2\u4e0e\u8005\u5bf9\u89c2\u70b9\u7684\u7406\u89e3\u548c\u53c2\u4e0e\u5ea6\u3002", "conclusion": "SimViews\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u865a\u62df\u535a\u7269\u9986\u4e2d\u5c55\u793a\u591a\u5143\u89c6\u89d2\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6e38\u5ba2\u4f53\u9a8c\u3002"}}
{"id": "2508.07731", "pdf": "https://arxiv.org/pdf/2508.07731", "abs": "https://arxiv.org/abs/2508.07731", "authors": ["Abdul Basit", "Maha Nawaz", "Saim Rehman", "Muhammad Shafique"], "title": "CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning", "categories": ["cs.HC", "cs.AI", "68T50, 68T40, 68T07, 92C55", "I.2.7; I.2.9"], "comment": "7 pages, 12 figures, Accepted to 62nd DAC 2025", "summary": "Efficient control of prosthetic limbs via non-invasive brain-computer\ninterfaces (BCIs) requires advanced EEG processing, including pre-filtering,\nfeature extraction, and action prediction, performed in real time on edge AI\nhardware. Achieving this on resource-constrained devices presents challenges in\nbalancing model complexity, computational efficiency, and latency. We present\nCognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on\nembedded AI hardware, achieving real-time operation without compromising\naccuracy. The system integrates BrainFlow, an open-source library for EEG data\nacquisition and streaming, with optimized deep learning (DL) models for precise\nbrain signal classification. Using evolutionary search, we identify\nPareto-optimal DL configurations through hyperparameter tuning, optimizer\nanalysis, and window selection, analyzed individually and in ensemble\nconfigurations. We apply model compression techniques such as pruning and\nquantization to optimize models for embedded deployment, balancing efficiency\nand accuracy. We collected an EEG dataset and designed an annotation pipeline\nenabling precise labeling of brain signals corresponding to specific intended\nactions, forming the basis for training our optimized DL models. CognitiveArm\nalso supports voice commands for seamless mode switching, enabling control of\nthe prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded\nhardware, it ensures low latency and real-time responsiveness. A full-scale\nprototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,\nachieved up to 90% accuracy in classifying three core actions (left, right,\nidle). Voice integration enables multiplexed, variable movement for everyday\ntasks (e.g., handshake, cup picking), enhancing real-world performance and\ndemonstrating CognitiveArm's potential for advanced prosthetic control.", "AI": {"tldr": "CognitiveArm\u662f\u4e00\u79cd\u57fa\u4e8e\u8111\u7535\u56fe\u7684\u5b9e\u65f6\u8111\u63a7\u5047\u80a2\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u5728\u5d4c\u5165\u5f0fAI\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5047\u80a2\u63a7\u5236\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u4fb5\u5165\u7684\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u5047\u80a2\uff0c\u540c\u65f6\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5e73\u8861\u6a21\u578b\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u7ed3\u5408BrainFlow\u5e93\u4e0e\u4f18\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u627e\u5230Pareto\u6700\u4f18\u914d\u7f6e\uff0c\u5e76\u5e94\u7528\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08\u5982\u526a\u679d\u548c\u91cf\u5316\uff09\u3002\u8bbe\u8ba1\u4e86EEG\u6570\u636e\u96c6\u548c\u6807\u6ce8\u6d41\u7a0b\uff0c\u652f\u6301\u8bed\u97f3\u547d\u4ee4\u5207\u6362\u6a21\u5f0f\u3002", "result": "\u5728OpenBCI UltraCortex Mark IV EEG\u5934\u6234\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe90%\u7684\u51c6\u786e\u7387\uff0c\u652f\u6301\u4e09\u79cd\u6838\u5fc3\u52a8\u4f5c\u5206\u7c7b\uff08\u5de6\u3001\u53f3\u3001\u7a7a\u95f2\uff09\u548c\u591a\u81ea\u7531\u5ea6\u63a7\u5236\u3002", "conclusion": "CognitiveArm\u5c55\u793a\u4e86\u975e\u4fb5\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u5728\u5b9e\u65f6\u5047\u80a2\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u65e5\u5e38\u4efb\u52a1\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.07854", "pdf": "https://arxiv.org/pdf/2508.07854", "abs": "https://arxiv.org/abs/2508.07854", "authors": ["Valerie Tan", "Jens Gerken"], "title": "Challenges in Mixed Reality in Assisting Adults with ADHD Symptoms", "categories": ["cs.HC"], "comment": "3 pages, Submitted as a position paper to a workshop (\"Envisioning\n  the Future of Accessible Immersive Technology\") at the Mensch und Computer\n  2024 conference", "summary": "In this position paper, we discuss symptoms of attention deficit\nhyperactivity disorder (ADHD) in adults, as well as available forms of\ntreatment or assistance in the context of mixed reality. Mixed reality offers\nmany potentials for assisting adults with symptoms commonly found in (but not\nlimited to) ADHD, but the availability of mixed reality solutions is not only\nlimited commercially, but also limited in terms of proof-of-concept prototypes.\nWe discuss two major challenges with attention assistance using mixed reality\nsolutions: the limited availability of adult-specific prototypes and studies,\nas well as the limited number of solutions that offer continuous intervention\nof ADHD-like symptoms that users can employ in their daily life.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u6210\u4ebaADHD\u75c7\u72b6\u53ca\u6df7\u5408\u73b0\u5b9e\u5728\u6cbb\u7597\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u6709\u9650\u4e14\u7f3a\u4e4f\u6210\u4eba\u7279\u5f02\u6027\u7814\u7a76\u3002", "motivation": "\u63a2\u8ba8\u6df7\u5408\u73b0\u5b9e\u6280\u672f\u5728\u6210\u4ebaADHD\u75c7\u72b6\u8f85\u52a9\u6cbb\u7597\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5206\u6790\u73b0\u6709\u6df7\u5408\u73b0\u5b9e\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\u3002", "result": "\u53d1\u73b0\u7f3a\u4e4f\u9488\u5bf9\u6210\u4ebaADHD\u7684\u6df7\u5408\u73b0\u5b9e\u539f\u578b\u548c\u6301\u7eed\u5e72\u9884\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6df7\u5408\u73b0\u5b9e\u6280\u672f\u5728\u6210\u4ebaADHD\u6cbb\u7597\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u591a\u7814\u7a76\u548c\u5f00\u53d1\u3002"}}
{"id": "2508.07980", "pdf": "https://arxiv.org/pdf/2508.07980", "abs": "https://arxiv.org/abs/2508.07980", "authors": ["Alan Said"], "title": "Early Explorations of Recommender Systems for Physical Activity and Well-being", "categories": ["cs.HC", "cs.IR"], "comment": "Second International Workshop on Recommender Systems for\n  Sustainability and Social Good (RecSoGood) in conjunction with ACM RecSys\n  2025", "summary": "As recommender systems increasingly guide physical actions, often through\nwearables and coaching tools, new challenges arise around how users interpret,\ntrust, and respond to this advice. This paper introduces a conceptual framework\nfor tangible recommendations that influence users' bodies, routines, and\nwell-being. We describe three design dimensions: trust and interpretation,\nintent alignment, and consequence awareness. These highlight key limitations in\napplying conventional recommender logic to embodied settings. Through examples\nand design reflections, we outline how future systems can support long-term\nwell-being, behavioral alignment, and socially responsible personalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u6709\u5f62\u63a8\u8350\u7684\u6846\u67b6\uff0c\u5173\u6ce8\u7528\u6237\u5bf9\u8eab\u4f53\u3001\u65e5\u5e38\u548c\u5065\u5eb7\u7684\u4fe1\u4efb\u4e0e\u89e3\u91ca\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u540e\u679c\u610f\u8bc6\u3002", "motivation": "\u968f\u7740\u63a8\u8350\u7cfb\u7edf\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\u5de5\u5177\u5f15\u5bfc\u7528\u6237\u884c\u4e3a\uff0c\u5982\u4f55\u8ba9\u7528\u6237\u7406\u89e3\u3001\u4fe1\u4efb\u5e76\u54cd\u5e94\u8fd9\u4e9b\u5efa\u8bae\u6210\u4e3a\u65b0\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u6846\u67b6\uff0c\u754c\u5b9a\u4fe1\u4efb\u4e0e\u89e3\u91ca\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u540e\u679c\u610f\u8bc6\u4e09\u4e2a\u8bbe\u8ba1\u7ef4\u5ea6\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86\u4f20\u7edf\u63a8\u8350\u903b\u8f91\u5728\u5b9e\u4f53\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7cfb\u7edf\u8bbe\u8ba1\u3002", "conclusion": "\u672a\u6765\u7cfb\u7edf\u5e94\u652f\u6301\u957f\u671f\u5065\u5eb7\u3001\u884c\u4e3a\u5bf9\u9f50\u548c\u793e\u4f1a\u8d23\u4efb\u4e2a\u6027\u5316\u3002"}}
{"id": "2508.08020", "pdf": "https://arxiv.org/pdf/2508.08020", "abs": "https://arxiv.org/abs/2508.08020", "authors": ["Zeyu Yang", "Zheng Wei", "Yang Zhang", "Xian Xu", "Changyang He", "Muzhi Zhou", "Pan Hui"], "title": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH Community", "categories": ["cs.HC"], "comment": "Paper for CSCW 2025", "summary": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users.", "AI": {"tldr": "\u6458\u8981\u6982\u8ff0\u4e86\u4e00\u4e2a\u540d\u4e3aEchoAid\u7684\u79fb\u52a8\u5e94\u7528\uff0c\u65e8\u5728\u6539\u5584\u804b\u54d1\u53ca\u542c\u529b\u969c\u788d\u7fa4\u4f53\u5728\u76f4\u64ad\u8d2d\u7269\u4e2d\u7684\u4f53\u9a8c\u3002", "motivation": "\u76f4\u64ad\u8d2d\u7269\u5e73\u53f0\u5f80\u5f80\u5ffd\u89c6\u4e86\u804b\u54d1\u53ca\u542c\u529b\u969c\u788d\u8005\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u4fe1\u606f\u83b7\u53d6\u969c\u788d\u548c\u8ba4\u77e5\u8fc7\u8f7d\u3002", "method": "\u5f00\u53d1\u4e86EchoAid\u5e94\u7528\uff0c\u7ed3\u5408\u8bed\u97f3\u8f6c\u6587\u672c\u6280\u672f\u3001RSVP\u6280\u672f\u548cLLM\u6a21\u578b\uff0c\u7b80\u5316\u76f4\u64ad\u9500\u552e\u4e2d\u7684\u4fe1\u606f\u6d41\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u8bbe\u8ba1\u548c\u7528\u6237\u7814\u7a76\u4f18\u5316\u3002", "result": "\u572838\u540d\u7528\u6237\u7684\u6d4b\u8bd5\u4e2d\uff0cEchoAid\u663e\u8457\u63d0\u5347\u4e86\u4ea7\u54c1\u4fe1\u606f\u63d0\u53d6\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u4e2a\u6027\u5316\u7684\u8d2d\u7269\u4f53\u9a8c\u3002", "conclusion": "EchoAid\u6210\u529f\u9a8c\u8bc1\u4e86\u5176\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e86\u4e3a\u804b\u54d1\u53ca\u542c\u529b\u969c\u788d\u7528\u6237\u63d0\u4f9b\u66f4\u597d\u76f4\u64ad\u8d2d\u7269\u4f53\u9a8c\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08128", "pdf": "https://arxiv.org/pdf/2508.08128", "abs": "https://arxiv.org/abs/2508.08128", "authors": ["Vladimir Zhurov", "John Kausch", "Kamran Sedig", "Mostafa Milani"], "title": "Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration", "categories": ["cs.HC"], "comment": "Journal submission", "summary": "Ontologies play a central role in structuring knowledge across domains,\nsupporting tasks such as reasoning, data integration, and semantic search.\nHowever, their large size and complexity, particularly in fields such as\nbiomedicine, computational biology, law, and engineering, make them difficult\nfor non-experts to navigate. Formal query languages such as SPARQL offer\nexpressive access but require users to understand the ontology's structure and\nsyntax. In contrast, visual exploration tools and basic keyword-based search\ninterfaces are easier to use but often lack flexibility and expressiveness. We\nintroduce FuzzyVis, a proof-of-concept system that enables intuitive and\nexpressive exploration of complex ontologies. FuzzyVis integrates two key\ncomponents: a fuzzy logic-based querying model built on fuzzy ontology\nembeddings, and an interactive visual interface for building and interpreting\nqueries. Users can construct new composite concepts by selecting and combining\nexisting ontology concepts using logical operators such as conjunction,\ndisjunction, and negation. These composite concepts are matched against the\nontology using fuzzy membership-based embeddings, which capture degrees of\nmembership and support approximate, concept-level similarity search. The visual\ninterface supports browsing, query composition, and partial search without\nrequiring formal syntax. By combining fuzzy semantics with embedding-based\nreasoning, FuzzyVis enables flexible interpretation, efficient computation, and\nexploratory learning. Case studies demonstrate how FuzzyVis supports subtle\ninformation needs and helps users uncover relevant concepts in large, complex\nontologies.", "AI": {"tldr": "FuzzyVis\u662f\u4e00\u79cd\u76f4\u89c2\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u63a2\u7d22\u590d\u6742\u672c\u4f53\uff0c\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u548c\u53ef\u89c6\u5316\u754c\u9762\uff0c\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u8f7b\u677e\u5bfc\u822a\u548c\u67e5\u8be2\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u672c\u4f53\uff08\u5982\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\u9886\u57df\uff09\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u96be\u4ee5\u5bfc\u822a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865\u73b0\u6709\u67e5\u8be2\u5de5\u5177\u5728\u7075\u6d3b\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "FuzzyVis\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u67e5\u8be2\u6a21\u578b\uff08\u57fa\u4e8e\u6a21\u7cca\u672c\u4f53\u5d4c\u5165\uff09\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u903b\u8f91\u64cd\u4f5c\u7b26\u6784\u5efa\u590d\u5408\u6982\u5ff5\uff0c\u5e76\u8fdb\u884c\u8fd1\u4f3c\u76f8\u4f3c\u6027\u641c\u7d22\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cFuzzyVis\u80fd\u591f\u652f\u6301\u590d\u6742\u4fe1\u606f\u9700\u6c42\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u5927\u578b\u672c\u4f53\u4e2d\u53d1\u73b0\u76f8\u5173\u6982\u5ff5\u3002", "conclusion": "FuzzyVis\u901a\u8fc7\u6a21\u7cca\u8bed\u4e49\u548c\u5d4c\u5165\u63a8\u7406\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u89e3\u91ca\u3001\u9ad8\u6548\u7684\u8ba1\u7b97\u548c\u63a2\u7d22\u6027\u5b66\u4e60\u3002"}}
{"id": "2508.08158", "pdf": "https://arxiv.org/pdf/2508.08158", "abs": "https://arxiv.org/abs/2508.08158", "authors": ["Laura Spillner", "Rachel Ringe", "Robert Porzel", "Rainer Malaka"], "title": "Can AI Explanations Make You Change Your Mind?", "categories": ["cs.HC", "cs.AI"], "comment": "This paper was presented at the Explainable AI workshop at IJCAI\n  2025: https://sites.google.com/view/xai2025/proceedings", "summary": "In the context of AI-based decision support systems, explanations can help\nusers to judge when to trust the AI's suggestion, and when to question it. In\nthis way, human oversight can prevent AI errors and biased decision-making.\nHowever, this rests on the assumption that users will consider explanations in\nenough detail to be able to catch such errors. We conducted an online study on\ntrust in explainable DSS, and were surprised to find that in many cases,\nparticipants spent little time on the explanation and did not always consider\nit in detail. We present an exploratory analysis of this data, investigating\nwhat factors impact how carefully study participants consider AI explanations,\nand how this in turn impacts whether they are open to changing their mind based\non what the AI suggests.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u53ef\u89e3\u91ca\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08DSS\uff09\u4e2d\u7684\u89e3\u91ca\u6709\u52a9\u4e8e\u7528\u6237\u5224\u65adAI\u5efa\u8bae\u7684\u53ef\u9760\u6027\uff0c\u4f46\u7528\u6237\u5e38\u5e38\u5ffd\u7565\u8be6\u7ec6\u9605\u8bfb\u89e3\u91ca\uff0c\u5f71\u54cd\u5176\u5bf9AI\u5efa\u8bae\u7684\u63a5\u53d7\u5ea6\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u662f\u5426\u5145\u5206\u5173\u6ce8AI\u89e3\u91ca\u53ca\u5176\u5bf9\u4fe1\u4efb\u548c\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\u5206\u6790\u7528\u6237\u5bf9\u53ef\u89e3\u91caDSS\u7684\u4fe1\u4efb\u7a0b\u5ea6\u53ca\u5176\u89e3\u91ca\u4f7f\u7528\u884c\u4e3a\u3002", "result": "\u8bb8\u591a\u53c2\u4e0e\u8005\u672a\u8be6\u7ec6\u9605\u8bfb\u89e3\u91ca\uff0c\u4e14\u89e3\u91ca\u7684\u7ec6\u8282\u8003\u8651\u7a0b\u5ea6\u5f71\u54cd\u5176\u5bf9AI\u5efa\u8bae\u7684\u63a5\u53d7\u5ea6\u3002", "conclusion": "\u9700\u4f18\u5316\u89e3\u91ca\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u7528\u6237\u5173\u6ce8\u5ea6\u548c\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2508.08242", "pdf": "https://arxiv.org/pdf/2508.08242", "abs": "https://arxiv.org/abs/2508.08242", "authors": ["Mohammed Alsobay", "David M. Rothschild", "Jake M. Hofman", "Daniel G. Goldstein"], "title": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated Group Decision Making", "categories": ["cs.HC"], "comment": null, "summary": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7fa4\u4f53\u51b3\u7b56\u4e2d\u4f5c\u4e3a\u52a9\u624b\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u80fd\u4fc3\u8fdb\u4fe1\u606f\u5171\u4eab\u4f46\u672a\u663e\u8457\u5f71\u54cd\u6700\u7ec8\u51b3\u7b56\u7ed3\u679c\u3002", "motivation": "\u7fa4\u4f53\u51b3\u7b56\u5e38\u56e0\u4fe1\u606f\u5171\u4eab\u4e0d\u5747\u800c\u5f71\u54cd\u8d28\u91cf\uff0cLLM\u5728\u6b64\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u9884\u6ce8\u518c\u968f\u673a\u5b9e\u9a8c\uff0c\u6d4b\u8bd5LLM\uff08GPT-4o\uff09\u4e0e\u5176\u4ed6\u4e09\u79cd\u8f85\u52a9\u65b9\u5f0f\u5728\u7fa4\u4f53\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u63d0\u9ad8\u4e86\u4fe1\u606f\u5171\u4eab\uff0c\u4f46\u672a\u663e\u8457\u6539\u53d8\u51b3\u7b56\u7ed3\u679c\uff1b\u4fe1\u606f\u5171\u4eab\u7684\u589e\u52a0\u4e0d\u8db3\u4ee5\u514b\u670d\u9690\u85cf\u8f6e\u5ed3\u6548\u5e94\u3002", "conclusion": "LLM\u53ef\u7528\u4e8e\u63d0\u5347\u7fa4\u4f53\u4fe1\u606f\u5171\u4eab\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5bf9\u51b3\u7b56\u7ed3\u679c\u7684\u5f71\u54cd\uff1b\u5b9e\u9a8c\u5de5\u5177GRAIL\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06849", "pdf": "https://arxiv.org/pdf/2508.06849", "abs": "https://arxiv.org/abs/2508.06849", "authors": ["Sanjana Gautam", "Mohit Chandra", "Ankolika De", "Tatiana Chakravorti", "Girik Malik", "Munmun De Choudhury"], "title": "Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Lived experiences fundamentally shape how individuals interact with AI\nsystems, influencing perceptions of safety, trust, and usability. While prior\nresearch has focused on developing techniques to emulate human preferences, and\nproposed taxonomies to categorize risks (such as psychological harms and\nalgorithmic biases), these efforts have provided limited systematic\nunderstanding of lived human experiences or actionable strategies for embedding\nthem meaningfully into the AI development lifecycle. This work proposes a\nframework for meaningfully integrating lived experience into the design and\nevaluation of AI systems. We synthesize interdisciplinary literature across\nlived experience philosophy, human-centered design, and human-AI interaction,\narguing that centering lived experience can lead to models that more accurately\nreflect the retrospective, emotional, and contextual dimensions of human\ncognition. Drawing from a wide body of work across psychology, education,\nhealthcare, and social policy, we present a targeted taxonomy of lived\nexperiences with specific applicability to AI systems. To ground our framework,\nwe examine three application domains (i) education, (ii) healthcare, and (iii)\ncultural alignment, illustrating how lived experience informs user goals,\nsystem expectations, and ethical considerations in each context. We further\nincorporate insights from AI system operators and human-AI partnerships to\nhighlight challenges in responsibility allocation, mental model calibration,\nand long-term system adaptation. We conclude with actionable recommendations\nfor developing experience-centered AI systems that are not only technically\nrobust but also empathetic, context-aware, and aligned with human realities.\nThis work offers a foundation for future research that bridges technical\ndevelopment with the lived experiences of those impacted by AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u751f\u6d3b\u4f53\u9a8c\u7eb3\u5165AI\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u4e2d\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4eba\u6027\u5316\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u4eba\u7c7b\u751f\u6d3b\u4f53\u9a8c\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u6709\u9650\uff0c\u7f3a\u4e4f\u5c06\u5176\u878d\u5165AI\u5f00\u53d1\u7684\u5177\u4f53\u7b56\u7565\u3002", "method": "\u7efc\u5408\u8de8\u5b66\u79d1\u6587\u732e\uff0c\u63d0\u51fa\u751f\u6d3b\u4f53\u9a8c\u5206\u7c7b\u6cd5\uff0c\u5e76\u5728\u6559\u80b2\u3001\u533b\u7597\u548c\u6587\u5316\u5bf9\u9f50\u4e09\u4e2a\u9886\u57df\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "\u6846\u67b6\u5c55\u793a\u4e86\u751f\u6d3b\u4f53\u9a8c\u5982\u4f55\u5f71\u54cd\u7528\u6237\u76ee\u6807\u3001\u7cfb\u7edf\u671f\u671b\u548c\u4f26\u7406\u8003\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u751f\u6d3b\u4f53\u9a8c\uff0cAI\u7cfb\u7edf\u53ef\u4ee5\u66f4\u8d34\u8fd1\u4eba\u7c7b\u73b0\u5b9e\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.06997", "pdf": "https://arxiv.org/pdf/2508.06997", "abs": "https://arxiv.org/abs/2508.06997", "authors": ["Helbert Paat", "Guohao Shen"], "title": "Conformal Set-based Human-AI Complementarity with Multiple Experts", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "comment": "Accepted at AAMAS 2025. Code available at:\n  https://github.com/paathelb/conformal_hai_multiple", "summary": "Decision support systems are designed to assist human experts in\nclassification tasks by providing conformal prediction sets derived from a\npre-trained model. This human-AI collaboration has demonstrated enhanced\nclassification performance compared to using either the model or the expert\nindependently. In this study, we focus on the selection of instance-specific\nexperts from a pool of multiple human experts, contrasting it with existing\nresearch that typically focuses on single-expert scenarios. We characterize the\nconditions under which multiple experts can benefit from the conformal sets.\nWith the insight that only certain experts may be relevant for each instance,\nwe explore the problem of subset selection and introduce a greedy algorithm\nthat utilizes conformal sets to identify the subset of expert predictions that\nwill be used in classifying an instance. This approach is shown to yield better\nperformance compared to naive methods for human subset selection. Based on real\nexpert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation\nstudy indicates that our proposed greedy algorithm achieves near-optimal\nsubsets, resulting in improved classification performance among multiple\nexperts.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u7b26\u5408\u9884\u6d4b\u96c6\u4ece\u591a\u4e13\u5bb6\u6c60\u4e2d\u9009\u62e9\u5b9e\u4f8b\u76f8\u5173\u4e13\u5bb6\u7684\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u4e13\u5bb6\u573a\u666f\u4e0b\uff0c\u63a2\u7d22\u5982\u4f55\u9009\u62e9\u5b9e\u4f8b\u76f8\u5173\u7684\u4e13\u5bb6\u5b50\u96c6\uff0c\u4ee5\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7b26\u5408\u9884\u6d4b\u96c6\u7684\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5b50\u96c6\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728CIFAR-10H\u548cImageNet-16H\u6570\u636e\u96c6\u4e0a\uff0c\u7b97\u6cd5\u8868\u73b0\u4f18\u4e8e\u7b80\u5355\u65b9\u6cd5\uff0c\u63a5\u8fd1\u6700\u4f18\u5b50\u96c6\u3002", "conclusion": "\u591a\u4e13\u5bb6\u52a8\u6001\u9009\u62e9\u80fd\u6709\u6548\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u8d2a\u5fc3\u7b97\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.07230", "pdf": "https://arxiv.org/pdf/2508.07230", "abs": "https://arxiv.org/abs/2508.07230", "authors": ["Mallory Knodel", "Mallika Balakrishnan", "Lauren M. Chambers"], "title": "Shaping a Profession, Building a Community: A Practitioner-Led Investigation of Public Interest Technologists in Civil Society", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "The label `public interest technology' (PIT) is growing in popularity among\nthose seeking to use `tech for good' - especially among technical practitioners\nworking in civil society and nonprofit organizations. PIT encompasses a broad\nrange of sociotechnical work across professional domains and sectors; however,\nthe trend remains understudied within sociotechnical research. This paper\ndescribes a mixed-methods study, designed and conducted by PIT practitioners at\nthe Center for Democracy and Technology, that characterizes technologists\nwithin the specific context of civil society, civil rights, and advocacy\norganizations in North America and Western Europe. We conducted interviews with\ncivil society leaders to investigate how PIT practitioners position the field\nand themselves, and we held a roundtable discussion bringing diverse voices\ntogether to make meaning of this growing phenomenon. Ultimately, we find that\nPIT remains both defined and plagued by its expansiveness, and that today's\ncivil society public interest technologists see a need for both (a) more robust\nprofessionalization infrastructures, including philanthropic attention, and (b)\nmore engaged, coherent community. This study illuminates a nascent intersection\nof technology and policy on-the-ground that is of growing relevance to critical\nsociotechnical research on the shifting relationship between computing and\nsociety.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u201c\u516c\u5171\u5229\u76ca\u6280\u672f\u201d\uff08PIT\uff09\u5728\u6c11\u95f4\u793e\u4f1a\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u53d1\u73b0\u76ee\u524dPIT\u9886\u57df\u7f3a\u4e4f\u4e13\u4e1a\u5316\u548c\u793e\u533a\u652f\u6301\u3002", "motivation": "\u63a2\u8ba8PIT\u5728\u6c11\u95f4\u793e\u4f1a\u548c\u975e\u8425\u5229\u7ec4\u7ec7\u4e2d\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u586b\u8865\u4e86\u7814\u7a76\u4e2d\u5bf9\u8fd9\u4e00\u8d8b\u52bf\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5305\u62ec\u8bbf\u8c08\u548c\u5706\u684c\u8ba8\u8bba\uff0c\u5206\u6790\u4e86\u5317\u7f8e\u548c\u897f\u6b27\u7684PIT\u5b9e\u8df5\u8005\u3002", "result": "\u7814\u7a76\u53d1\u73b0PIT\u9886\u57df\u7f3a\u4e4f\u4e13\u4e1a\u5316\u57fa\u7840\u8bbe\u65bd\u548c\u793e\u533a\u652f\u6301\uff0c\u9700\u8981\u66f4\u591a\u5173\u6ce8\u3002", "conclusion": "PIT\u7684\u53d1\u5c55\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u8d44\u6e90\u548c\u793e\u533a\u652f\u6301\uff0c\u8fd9\u5bf9\u6280\u672f\u4e0e\u793e\u4f1a\u7684\u4e92\u52a8\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.07501", "pdf": "https://arxiv.org/pdf/2508.07501", "abs": "https://arxiv.org/abs/2508.07501", "authors": ["Xiaoye Zuo", "Nikos Athanasiou", "Ginger Delmas", "Yiming Huang", "Xingyu Fu", "Lingjie Liu"], "title": "FormCoach: Lift Smarter, Not Harder", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI.", "AI": {"tldr": "FormCoach\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5c06\u666e\u901a\u6444\u50cf\u8bbe\u5907\u8f6c\u5316\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u5f0fAI\u8bad\u7ec3\u4f19\u4f34\uff0c\u63d0\u4f9b\u59ff\u52bf\u7ea0\u6b63\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u9488\u5bf9\u5c45\u5bb6\u5065\u8eab\u7231\u597d\u8005\u7f3a\u4e4f\u4e13\u4e1a\u53cd\u9988\u7684\u95ee\u9898\uff0cFormCoach\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eVLM\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6444\u50cf\u5934\u5b9e\u65f6\u68c0\u6d4b\u5e76\u7ea0\u6b63\u52a8\u4f5c\u9519\u8bef\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b1700\u7ec4\u4e13\u5bb6\u6807\u6ce8\u89c6\u9891\u7684\u6570\u636e\u96c6\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5f53\u524dVLM\u4e0e\u4eba\u7c7b\u6559\u7ec3\u6c34\u5e73\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\uff0c\u4f46\u5c55\u793a\u4e86AI\u5728\u8fd0\u52a8\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "FormCoach\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u7684\u521b\u65b0\u65b9\u5f0f\uff0c\u4e3a\u5177\u8eabAI\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517", "abs": "https://arxiv.org/abs/2508.07517", "authors": ["Joseph T. Colonel", "Baihan Lin"], "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "ThemeClouds\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u751f\u6210\u4e3b\u9898\u8bcd\u4e91\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9891\u7387\u6cd5\u5728\u5bf9\u8bdd\u8bed\u5883\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u5206\u6790\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u8bcd\u9891\u65b9\u6cd5\u5728\u5bf9\u8bdd\u8bed\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8bed\u4e49\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u9650\u5236\u4e86\u65e9\u671f\u7814\u7a76\u7684\u5feb\u901f\u5206\u6790\u9700\u6c42\u3002", "method": "ThemeClouds\u5229\u7528LLM\u8bc6\u522b\u8bed\u6599\u5e93\u4e2d\u7684\u6982\u5ff5\u7ea7\u4e3b\u9898\uff0c\u5e76\u6839\u636e\u53c2\u4e0e\u8005\u63d0\u53ca\u6b21\u6570\u751f\u6210\u8bcd\u4e91\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u63d0\u793a\u548c\u53ef\u89c6\u5316\u53c2\u6570\u3002", "result": "\u5728\u7528\u6237\u7814\u7a76\u4e2d\uff0cThemeClouds\u6bd4\u4f20\u7edf\u8bcd\u9891\u548c\u4e3b\u9898\u6a21\u578b\uff08\u5982LDA\u3001BERTopic\uff09\u66f4\u80fd\u63d0\u53d6\u5b9e\u7528\u7684\u8bbe\u5907\u5173\u6ce8\u70b9\u3002", "conclusion": "ThemeClouds\u7ed3\u5408LLM\u8f85\u52a9\u5b9a\u6027\u5206\u6790\uff0c\u63d0\u5347\u4e86\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7814\u7a76\u8005\u7684\u63a7\u5236\u529b\uff0c\u540c\u65f6\u4e3a\u4ea4\u4e92\u5f0f\u5206\u6790\uff08\u5982\u5dee\u5f02\u5bf9\u6bd4\uff09\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2508.07579", "pdf": "https://arxiv.org/pdf/2508.07579", "abs": "https://arxiv.org/abs/2508.07579", "authors": ["Ziqi Pan", "Runhua Zhang", "Jiehui Luo", "Yuanhao Zhang", "Yue Deng", "Xiaojuan Ma"], "title": "From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women", "categories": ["cs.SI", "cs.CY", "cs.HC"], "comment": null, "summary": "Hashtags serve as identity markers and connection tools in online queer\ncommunities. Recently, the Western-origin #wlw (women-loving-women) hashtag has\nrisen in the Chinese lesbian community on RedNote, coinciding with user\nmigration triggered by the temporary US TikTok ban. This event provides a\nunique lens to study cross-cultural hashtag ingress and diffusion through the\npopulations' responsive behaviors in cyber-migration. In this paper, we\nconducted a two-phase content analysis of 418 #wlw posts from January and\nApril, examining different usage patterns during the hashtag's ingress and\ndiffusion. Results indicate that the successful introduction of #wlw was\nfacilitated by TikTok immigrants' bold importation, both populations' mutual\ninterpretation, and RedNote natives' discussions. In current manifestation of\ndiffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer\nlife, and semantically expands to support feminism discourse. Our findings\nprovide empirical insights for enhancing the marginalized communities'\ncross-cultural communication.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u897f\u65b9\u8d77\u6e90\u7684#wlw\u6807\u7b7e\u5728\u4e2d\u56fd\u5973\u540c\u6027\u604b\u793e\u533a\u4e2d\u7684\u4f20\u64ad\u4e0e\u6269\u6563\uff0c\u5206\u6790\u4e86\u7528\u6237\u8fc1\u79fb\u548c\u6587\u5316\u9002\u5e94\u5982\u4f55\u4fc3\u8fdb\u5176\u6210\u529f\u5f15\u5165\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u8de8\u6587\u5316\u6807\u7b7e\u5728\u6570\u5b57\u8fc1\u79fb\u4e2d\u7684\u4f20\u64ad\u673a\u5236\uff0c\u7279\u522b\u5173\u6ce8\u8fb9\u7f18\u5316\u793e\u533a\u7684\u54cd\u5e94\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5185\u5bb9\u5206\u6790\u6cd5\uff0c\u5206\u6790418\u6761#wlw\u5e16\u5b50\uff0c\u6bd4\u8f83\u6807\u7b7e\u8fdb\u5165\u548c\u6269\u6563\u9636\u6bb5\u7684\u4e0d\u540c\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c#wlw\u6807\u7b7e\u7684\u6210\u529f\u5f15\u5165\u5f97\u76ca\u4e8eTikTok\u79fb\u6c11\u7684\u5927\u80c6\u5bfc\u5165\u3001\u53cc\u65b9\u7684\u76f8\u4e92\u89e3\u8bfb\u53ca\u672c\u571f\u7528\u6237\u7684\u8ba8\u8bba\uff0c\u6807\u7b7e\u8bed\u4e49\u8fd8\u6269\u5c55\u81f3\u5973\u6743\u4e3b\u4e49\u8bae\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8fb9\u7f18\u5316\u793e\u533a\u8de8\u6587\u5316\u4f20\u64ad\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5f3a\u8c03\u4e86\u7528\u6237\u884c\u4e3a\u7684\u9002\u5e94\u6027\u5728\u6807\u7b7e\u6269\u6563\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2508.07671", "pdf": "https://arxiv.org/pdf/2508.07671", "abs": "https://arxiv.org/abs/2508.07671", "authors": ["Mohamed Rayan Barhdadi", "Mehmet Tuncel", "Erchin Serpedin", "Hasan Kurban"], "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "stat.AP", "68T07, 68T42, 68T50, 91F20, 62P25", "I.2.11; I.2.1; H.1.2; J.4; K.4.2"], "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1\n  algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity", "summary": "Current AI approaches to refugee integration optimize narrow objectives such\nas employment and fail to capture the cultural, emotional, and ethical\ndimensions critical for long-term success. We introduce EMPATHIA (Enriched\nMultimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),\na multi-agent framework addressing the central Creative AI question: how do we\npreserve human dignity when machines participate in life-altering decisions?\nGrounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes\nintegration into three modules: SEED (Socio-cultural Entry and Embedding\nDecision) for initial placement, RISE (Rapid Integration and Self-sufficiency\nEngine) for early independence, and THRIVE (Transcultural Harmony and\nResilience through Integrated Values and Engagement) for sustained outcomes.\nSEED employs a selector-validator architecture with three specialized agents -\nemotional, cultural, and ethical - that deliberate transparently to produce\ninterpretable recommendations. Experiments on the UN Kakuma dataset (15,026\nindividuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and\nimplementation on 6,359 working-age refugees (15+) with 150+ socioeconomic\nvariables achieved 87.4% validation convergence and explainable assessments\nacross five host countries. EMPATHIA's weighted integration of cultural,\nemotional, and ethical factors balances competing value systems while\nsupporting practitioner-AI collaboration. By augmenting rather than replacing\nhuman expertise, EMPATHIA provides a generalizable framework for AI-driven\nallocation tasks where multiple values must be reconciled.", "AI": {"tldr": "EMPATHIA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u96be\u6c11\u6574\u5408\u7684\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u7ef4\u5ea6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\uff08SEED\u3001RISE\u3001THRIVE\uff09\u5b9e\u73b0\u900f\u660e\u51b3\u7b56\u548c\u53ef\u89e3\u91ca\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u5728\u96be\u6c11\u6574\u5408\u4e2d\u8fc7\u4e8e\u5173\u6ce8\u5c31\u4e1a\u7b49\u72ed\u9698\u76ee\u6807\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u7b49\u957f\u671f\u6210\u529f\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "method": "\u57fa\u4e8eKegan\u7684\u7406\u8bba\uff0cEMPATHIA\u5206\u89e3\u6574\u5408\u8fc7\u7a0b\u4e3aSEED\u3001RISE\u548cTHRIVE\u4e09\u4e2a\u6a21\u5757\uff0c\u5176\u4e2dSEED\u901a\u8fc7\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\uff08\u60c5\u611f\u3001\u6587\u5316\u3001\u4f26\u7406\uff09\u8fdb\u884c\u900f\u660e\u51b3\u7b56\u3002", "result": "\u5728UN Kakuma\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e8687.4%\u7684\u6536\u655b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u4e94\u4e2a\u4e1c\u9053\u56fd\u76846,359\u540d\u96be\u6c11\u3002", "conclusion": "EMPATHIA\u901a\u8fc7\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e3a\u591a\u4ef7\u503c\u534f\u8c03\u7684AI\u9a71\u52a8\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2508.07872", "pdf": "https://arxiv.org/pdf/2508.07872", "abs": "https://arxiv.org/abs/2508.07872", "authors": ["Holli Sargeant", "Mackenzie Jorgensen", "Arina Shah", "Adrian Weller", "Umang Bhatt"], "title": "Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI", "categories": ["cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "Uncertainty in artificial intelligence (AI) predictions poses urgent legal\nand ethical challenges for AI-assisted decision-making. We examine two\nalgorithmic interventions that act as guardrails for human-AI collaboration:\nselective abstention, which withholds high-uncertainty predictions from human\ndecision-makers, and selective friction, which delivers those predictions\ntogether with salient warnings or disclosures that slow the decision process.\nResearch has shown that selective abstention based on uncertainty can\ninadvertently exacerbate disparities and disadvantage under-represented groups\nthat disproportionately receive uncertain predictions. In this paper, we\nprovide the first integrated socio-technical and legal analysis of\nuncertainty-based algorithmic interventions. Through two case studies,\nAI-assisted consumer credit decisions and AI-assisted content moderation, we\ndemonstrate how the seemingly neutral use of uncertainty thresholds can trigger\ndiscriminatory impacts. We argue that, although both interventions pose risks\nof unlawful discrimination under UK law, selective frictions offer a promising\npathway toward fairer and more accountable AI-assisted decision-making by\npreserving transparency and encouraging more cautious human judgment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684AI\u5e72\u9884\u63aa\u65bd\uff08\u9009\u62e9\u6027\u4fdd\u7559\u548c\u9009\u62e9\u6027\u6469\u64e6\uff09\u5728\u6cd5\u5f8b\u548c\u4f26\u7406\u4e0a\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u9009\u62e9\u6027\u6469\u64e6\u5728\u4fc3\u8fdb\u516c\u5e73\u548c\u900f\u660e\u5ea6\u7684\u6f5c\u529b\u3002", "motivation": "AI\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u53d1\u4e86\u6cd5\u5f8b\u548c\u4f26\u7406\u6311\u6218\uff0c\u9700\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5e72\u9884\u63aa\u65bd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u4fe1\u8d37\u51b3\u7b56\u548c\u5185\u5bb9\u5ba1\u6838\uff09\uff0c\u5206\u6790\u4e86\u9009\u62e9\u6027\u4fdd\u7559\u548c\u9009\u62e9\u6027\u6469\u64e6\u7684\u6f5c\u5728\u6b67\u89c6\u5f71\u54cd\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u9608\u503c\u53ef\u80fd\u5bfc\u81f4\u6b67\u89c6\uff0c\u9009\u62e9\u6027\u6469\u64e6\u80fd\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u51b3\u7b56\u3002", "conclusion": "\u9009\u62e9\u6027\u6469\u64e6\u662f\u66f4\u516c\u5e73\u548c\u900f\u660e\u7684AI\u8f85\u52a9\u51b3\u7b56\u8def\u5f84\uff0c\u4f46\u4ecd\u9700\u6cd5\u5f8b\u5ba1\u67e5\u3002"}}
{"id": "2508.07875", "pdf": "https://arxiv.org/pdf/2508.07875", "abs": "https://arxiv.org/abs/2508.07875", "authors": ["Shuo Han", "Ahmed Karam Eldaly", "Solomon Sunday Oyelere"], "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,\nand early, accurate diagnosis is critical to improving patient survival rates\nby guiding treatment decisions. Combining medical expertise with artificial\nintelligence (AI) holds significant promise for enhancing the precision and\nefficiency of IDC detection. In this work, we propose a human-in-the-loop\n(HITL) deep learning system designed to detect IDC in histopathology images.\nThe system begins with an initial diagnosis provided by a high-performance\nEfficientNetV2S model, offering feedback from AI to the human expert. Medical\nprofessionals then review the AI-generated results, correct any misclassified\nimages, and integrate the revised labels into the training dataset, forming a\nfeedback loop from the human back to the AI. This iterative process refines the\nmodel's performance over time. The EfficientNetV2S model itself achieves\nstate-of-the-art performance compared to existing methods in the literature,\nwith an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system\nfurther improves the model's accuracy using four experimental groups with\nmisclassified images. These results demonstrate the potential of this\ncollaborative approach to enhance AI performance in diagnostic systems. This\nwork contributes to advancing automated, efficient, and highly accurate methods\nfor IDC detection through human-AI collaboration, offering a promising\ndirection for future AI-assisted medical diagnostics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u540c\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e73\u817a\u764c\u7684\u8bca\u65ad\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u6d78\u6da6\u6027\u5bfc\u7ba1\u764c\uff08IDC\uff09\u65e9\u671f\u8bca\u65ad\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\uff0c\u7ed3\u5408\u533b\u751f\u4e13\u4e1a\u77e5\u8bc6\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u3002", "method": "\u91c7\u7528EfficientNetV2S\u6a21\u578b\u8fdb\u884c\u521d\u6b65\u8bca\u65ad\uff0c\u533b\u751f\u4fee\u6b63\u8bef\u5206\u7c7b\u56fe\u50cf\u5e76\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u4f18\u5316\u6a21\u578b\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe93.65%\uff0c\u4eba\u673a\u534f\u540c\u7cfb\u7edf\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u8bca\u65ad\u7cbe\u5ea6\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u4e3aAI\u8f85\u52a9\u533b\u7597\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07923", "pdf": "https://arxiv.org/pdf/2508.07923", "abs": "https://arxiv.org/abs/2508.07923", "authors": ["Jakub Binda", "Valentina Paneta", "Vasileios Eleftheriadis", "Hongkyou Chung", "Panagiotis Papadimitroulas", "Neo Christopher Chung"], "title": "Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Generative AI holds great potentials to automate and enhance data synthesis\nin nuclear medicine. However, the high-stakes nature of biomedical imaging\nnecessitates robust mechanisms to detect and manage unexpected or erroneous\nmodel behavior. We introduce development and implementation of a hybrid anomaly\ndetection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.\nTwo applications are demonstrated: Pose2Xray, which generates synthetic X-rays\nfrom photographic mouse images, and DosimetrEYE, which estimates 3D radiation\ndose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)\nenhances reliability, reduces manual oversight, and supports real-time quality\ncontrol. This approach strengthens the industrial viability of GenAI in\npreclinical settings by increasing robustness, scalability, and regulatory\ncompliance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u6838\u533b\u5b66\u4e2d\u751f\u6210\u5f0fAI\u7684\u53ef\u9760\u6027\u548c\u5b9e\u65f6\u8d28\u91cf\u63a7\u5236\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u751f\u7269\u533b\u5b66\u5f71\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f00\u53d1\u5e76\u5b9e\u65bd\u4e86\u4e00\u79cd\u6df7\u5408\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5e94\u7528\u4e8ePose2Xray\u548cDosimetrEYE\u4e24\u4e2a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u8d28\u91cf\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5408\u89c4\u6027\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u751f\u6210\u5f0fAI\u5728\u4e34\u5e8a\u524d\u73af\u5883\u4e2d\u7684\u5de5\u4e1a\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07989", "pdf": "https://arxiv.org/pdf/2508.07989", "abs": "https://arxiv.org/abs/2508.07989", "authors": ["Xiantao Zhang"], "title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility", "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5e2e\u52a9\u76f2\u4eba\u548c\u89c6\u969c\u7fa4\u4f53\uff08BVI\uff09\u65f6\u7684\u5173\u952e\u95ee\u9898\u2014\u2014\u2018Escalator Problem\u2019\uff0c\u5373\u65e0\u6cd5\u611f\u77e5\u7535\u68af\u8fd0\u52a8\u65b9\u5411\u7684\u7f3a\u9677\u3002", "motivation": "MLLMs\u4f5c\u4e3a\u8f85\u52a9\u6280\u672f\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u2018Implicit Motion Blindness\u2019\u95ee\u9898\uff0c\u5bfc\u81f4\u7528\u6237\u4fe1\u4efb\u5ea6\u964d\u4f4e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u89c6\u9891\u7406\u89e3\u7684\u5e27\u91c7\u6837\u8303\u5f0f\uff0c\u6307\u51fa\u5176\u65e0\u6cd5\u5904\u7406\u8fde\u7eed\u4f4e\u901f\u8fd0\u52a8\u7684\u95ee\u9898\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u8fd9\u4e00\u7f3a\u9677\u7684\u6df1\u5c42\u539f\u56e0\u53ca\u5176\u5bf9\u7528\u6237\u4fe1\u4efb\u7684\u5f71\u54cd\u3002", "conclusion": "\u547c\u5401\u4ece\u8bed\u4e49\u8bc6\u522b\u8f6c\u5411\u7269\u7406\u611f\u77e5\uff0c\u5e76\u5f00\u53d1\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2508.08043", "pdf": "https://arxiv.org/pdf/2508.08043", "abs": "https://arxiv.org/abs/2508.08043", "authors": ["Yancheng Jiang", "Yan Jiang", "Ruochen Zhou", "Yi-Chao Chen", "Xiaoyu Ji", "Wenyuan Xu"], "title": "False Reality: Uncovering Sensor-induced Human-VR Interaction Vulnerability", "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Virtual Reality (VR) techniques, serving as the bridge between the real and\nvirtual worlds, have boomed and are widely used in manufacturing, remote\nhealthcare, gaming, etc. Specifically, VR systems offer users immersive\nexperiences that include both perceptions and actions. Various studies have\ndemonstrated that attackers can manipulate VR software to influence users'\ninteractions, including perception and actions. However, such attacks typically\nrequire strong access and specialized expertise. In this paper, we are the\nfirst to present a systematic analysis of physical attacks against VR systems\nand introduce False Reality, a new attack threat to VR devices without\nrequiring access to or modification of their software. False Reality disturbs\nVR system services by tampering with sensor measurements, and further spoofing\nusers' perception even inducing harmful actions, e.g., inducing dizziness or\ncausing users to crash into obstacles, by exploiting perceptual and\npsychological effects. We formalize these threats through an attack pathway\nframework and validate three representative pathways via physical experiments\nand user studies on five commercial VR devices. Finally, we further propose a\ndefense prototype to mitigate such threats. Our findings shall provide valuable\ninsights for enhancing the security and resilience of future VR systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u9488\u5bf9VR\u7cfb\u7edf\u7684\u7269\u7406\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u4fee\u6539\u8f6f\u4ef6\u7684False Reality\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5a01\u80c1\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "VR\u6280\u672f\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u4f7f\u5176\u5b89\u5168\u6027\u53d8\u5f97\u91cd\u8981\uff0c\u73b0\u6709\u653b\u51fb\u9700\u9ad8\u6743\u9650\uff0c\u800cFalse Reality\u653b\u51fb\u65e0\u9700\u8f6f\u4ef6\u4fee\u6539\u5373\u53ef\u5a01\u80c1\u7528\u6237\u3002", "method": "\u901a\u8fc7\u7be1\u6539\u4f20\u611f\u5668\u6570\u636e\uff0c\u5229\u7528\u611f\u77e5\u548c\u5fc3\u7406\u6548\u5e94\u8bf1\u5bfc\u7528\u6237\u884c\u4e3a\uff0c\u6784\u5efa\u653b\u51fb\u8def\u5f84\u6846\u67b6\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e94\u6b3e\u5546\u7528VR\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u4e09\u79cd\u653b\u51fb\u8def\u5f84\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u9632\u5fa1\u539f\u578b\u3002", "conclusion": "False Reality\u653b\u51fb\u63ed\u793a\u4e86VR\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6848\u5bf9\u672a\u6765VR\u7cfb\u7edf\u5b89\u5168\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002"}}
