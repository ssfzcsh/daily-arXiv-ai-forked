{"id": "2507.00347", "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis."}
{"id": "2507.00352", "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity."}
{"id": "2507.00378", "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times."}
{"id": "2507.00413", "pdf": "https://arxiv.org/pdf/2507.00413", "abs": "https://arxiv.org/abs/2507.00413", "authors": ["Taiming Wang", "Hui Liu", "Yuxia Zhang", "Yanjie Jiang"], "title": "Recommending Variable Names for Extract Local Variable Refactorings", "categories": ["cs.SE", "D.2.7"], "comment": "Accepted by TOSEM", "summary": "Extract local variable is one of the most popular refactorings, and most IDEs\nand refactoring tools provide automated support for this refactoring. However,\nwe find approximately 70% of the names recommended by these IDEs are different\nfrom what developers manually constructed, adding additional renaming burdens\nto developers and providing limited assistance. In this paper, we introduce\nVarNamer, an automated approach designed to recommend variable names for\nextract local variable refactorings. Through a large-scale empirical study, we\nidentify key contexts that are useful for composing variable names. Leveraging\nthese insights, we developed a set of heuristic rules through program static\nanalysis techniques and employ data mining techniques to recommend variable\nnames effectively. Notably, some of our heuristic rules have been successfully\nintegrated into Eclipse, where they are now distributed with the latest\nreleases of the IDE. Evaluation demonstrates its superiority over\nstate-of-the-art IDEs. Specifically, VarNamer significantly increases the\nchance of exact match by 52.6% compared to Eclipse and 40.7% compared to\nIntelliJ IDEA. We also evaluated the proposed approach with real-world extract\nlocal variable refactorings conducted in C++ projects, and the results suggest\nthat the approach can achieve comparable performance on programming languages\nbesides Java. It may suggest the generalizability of VarNamer. Finally, we\ndesigned and conducted a user study and the results of the user study suggest\nthat our approach can speed up the refactoring by 27.8% and reduce 49.3% edits\non the recommended variable names."}
{"id": "2507.00006", "pdf": "https://arxiv.org/pdf/2507.00006", "abs": "https://arxiv.org/abs/2507.00006", "authors": ["Xianghui Xie", "Chuhang Zou", "Meher Gitika Karumuri", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MVGBench: Comprehensive Benchmark for Multi-view Generation Models", "categories": ["cs.GR", "cs.LG", "eess.IV"], "comment": "17 pages, 11 figures, 9 tables, project page:\n  https://virtualhumans.mpi-inf.mpg.de/MVGBench/", "summary": "We propose MVGBench, a comprehensive benchmark for multi-view image\ngeneration models (MVGs) that evaluates 3D consistency in geometry and texture,\nimage quality, and semantics (using vision language models). Recently, MVGs\nhave been the main driving force in 3D object creation. However, existing\nmetrics compare generated images against ground truth target views, which is\nnot suitable for generative tasks where multiple solutions exist while\ndiffering from ground truth. Furthermore, different MVGs are trained on\ndifferent view angles, synthetic data and specific lightings -- robustness to\nthese factors and generalization to real data are rarely evaluated thoroughly.\nWithout a rigorous evaluation protocol, it is also unclear what design choices\ncontribute to the progress of MVGs. MVGBench evaluates three different aspects:\nbest setup performance, generalization to real data and robustness. Instead of\ncomparing against ground truth, we introduce a novel 3D self-consistency metric\nwhich compares 3D reconstructions from disjoint generated multi-views. We\nsystematically compare 12 existing MVGs on 4 different curated real and\nsynthetic datasets. With our analysis, we identify important limitations of\nexisting methods specially in terms of robustness and generalization, and we\nfind the most critical design choices. Using the discovered best practices, we\npropose ViFiGen, a method that outperforms all evaluated MVGs on 3D\nconsistency. Our code, model, and benchmark suite will be publicly released."}
{"id": "2507.00189", "pdf": "https://arxiv.org/pdf/2507.00189", "abs": "https://arxiv.org/abs/2507.00189", "authors": ["Alessio Di Santo"], "title": "Plug. Play. Persist. Inside a Ready-to-Go Havoc C2 Infrastructure", "categories": ["cs.CR", "cs.OS"], "comment": null, "summary": "This analysis focuses on a single Azure-hosted Virtual Machine at\n52.230.23.114 that the adversary converted into an all-in-one delivery, staging\nand Command-and-Control node. The host advertises an out-of-date Apache 2.4.52\ninstance whose open directory exposes phishing lures, PowerShell loaders,\nReflective Shell-Code, compiled Havoc Demon implants and a toolbox of\nlateral-movement binaries; the same server also answers on 8443/80 for\nencrypted beacon traffic. The web tier is riddled with publicly documented\ncritical vulnerabilities, that would have allowed initial code-execution had\nthe attackers not already owned the device.\n  Initial access is delivered through an HTML file that, once de-obfuscated,\nperfectly mimics Google Unusual sign-in attempt notification and funnels\nvictims toward credential collection. A PowerShell command follows: it disables\nAMSI in-memory, downloads a Base64-encoded stub, allocates RWX pages and starts\nthe shell-code without ever touching disk. That stub reconstructs a DLL in\nmemory using the Reflective-Loader technique and hands control to Havoc Demon\nimplant. Every Demon variant-32- and 64-bit alike-talks to the same backend,\nresolves Windows APIs with hashed look-ups, and hides its activity behind\nindirect syscalls.\n  Runtime telemetry shows interests in registry under Image File Execution\nOptions, deliberate queries to Software Restriction Policy keys, and heavy use\nof Crypto DLLs to protect payloads and C2 traffic. The attacker toolkit further\ncontains Chisel, PsExec, Doppelganger and Whisker, some of them re-compiled\nunder user directories that leak the developer personas tonzking123 and thobt.\nCollectively the findings paint a picture of a technically adept actor who\nvalues rapid re-tooling over deep operational security, leaning on Havoc\nmodularity and on legitimate cloud services to blend malicious flows into\nordinary enterprise traffic."}
{"id": "2507.00057", "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel Böhme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence."}
{"id": "2507.00465", "pdf": "https://arxiv.org/pdf/2507.00465", "abs": "https://arxiv.org/abs/2507.00465", "authors": ["Sohei Ito", "Makoto Tatsuta"], "title": "Encoding Peano Arithmetic in a Minimal Fragment of Separation Logic", "categories": ["cs.LO"], "comment": null, "summary": "This paper investigates the expressive power of a minimal fragment of\nseparation logic extended with natural numbers. Specifically, it demonstrates\nthat the fragment consisting solely of the intuitionistic points-to predicate,\nthe constant 0, and the successor function is sufficient to encode all\n$\\Pi^0_1$ formulas of Peano Arithmetic (PA). The authors construct a\ntranslation from PA into this fragment, showing that a $\\Pi^0_1$ formula is\nvalid in the standard model of arithmetic if and only if its translation is\nvalid in the standard interpretation of the separation logic fragment. This\nresult implies the undecidability of validity in the fragment, despite its\nsyntactic simplicity. The translation leverages a heap-based encoding of\narithmetic operations - addition, multiplication, and inequality - using\nstructured memory cells. The paper also explores the boundaries of this\nencoding, showing that the translation does not preserve validity for\n$\\Sigma^0_1$ formulas. Additionally, an alternative undecidability proof is\npresented via a reduction from finite model theory. Finally, the paper\nestablishes that the validity problem for this fragment is $\\Pi^0_1$-complete,\nhighlighting its theoretical significance in the landscape of logic and program\nverification."}
{"id": "2507.00004", "pdf": "https://arxiv.org/pdf/2507.00004", "abs": "https://arxiv.org/abs/2507.00004", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation."}
{"id": "2507.00066", "pdf": "https://arxiv.org/pdf/2507.00066", "abs": "https://arxiv.org/abs/2507.00066", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies."}
{"id": "2507.00926", "pdf": "https://arxiv.org/pdf/2507.00926", "abs": "https://arxiv.org/abs/2507.00926", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction", "categories": ["cs.MM", "cs.LG"], "comment": null, "summary": "Social media popularity prediction plays a crucial role in content\noptimization, marketing strategies, and user engagement enhancement across\ndigital platforms. However, predicting post popularity remains challenging due\nto the complex interplay between visual, textual, temporal, and user behavioral\nfactors. This paper presents HyperFusion, a hierarchical multimodal ensemble\nlearning framework for social media popularity prediction. Our approach employs\na three-tier fusion architecture that progressively integrates features across\nabstraction levels: visual representations from CLIP encoders, textual\nembeddings from transformer models, and temporal-spatial metadata with user\ncharacteristics. The framework implements a hierarchical ensemble strategy\ncombining CatBoost, TabNet, and custom multi-layer perceptrons. To address\nlimited labeled data, we propose a two-stage training methodology with\npseudo-labeling and iterative refinement. We introduce novel cross-modal\nsimilarity measures and hierarchical clustering features that capture\ninter-modal dependencies. Experimental results demonstrate that HyperFusion\nachieves competitive performance on the SMP challenge dataset. Our team\nachieved third place in the SMP Challenge 2025 (Image Track). The source code\nis available at https://anonymous.4open.science/r/SMPDImage."}
{"id": "2507.00306", "pdf": "https://arxiv.org/pdf/2507.00306", "abs": "https://arxiv.org/abs/2507.00306", "authors": ["Chao Zhang", "Neha Arora", "Christopher Bian", "Yechen Li", "Willa Ng", "Andrew Tomkins", "Bin Yan", "Janny Zhang", "Carolina Osorio"], "title": "Origin-Destination Travel Demand Estimation: An Approach That Scales Worldwide, and Its Application to Five Metropolitan Highway Networks", "categories": ["cs.ET"], "comment": null, "summary": "Estimating Origin-Destination (OD) travel demand is vital for effective urban\nplanning and traffic management. Developing universally applicable OD\nestimation methodologies is significantly challenged by the pervasive scarcity\nof high-fidelity traffic data and the difficulty in obtaining city-specific\nprior OD estimates (or seed ODs), which are often prerequisite for traditional\napproaches. Our proposed method directly estimates OD travel demand by\nsystematically leveraging aggregated, anonymized statistics from Google Maps\nTraffic Trends, obviating the need for conventional census or city-provided OD\ndata. The OD demand is estimated by formulating a single-level,\none-dimensional, continuous nonlinear optimization problem with nonlinear\nequality and bound constraints to replicate highway path travel times. The\nmethod achieves efficiency and scalability by employing a differentiable\nanalytical macroscopic network model. This model by design is computationally\nlightweight, distinguished by its parsimonious parameterization that requires\nminimal calibration effort and its capacity for instantaneous evaluation. These\nattributes ensure the method's broad applicability and practical utility across\ndiverse cities globally. Using segment sensor counts from Los Angeles and San\nDiego highway networks, we validate our proposed approach, demonstrating a\ntwo-thirds to three-quarters improvement in the fit to segment count data over\na baseline. Beyond validation, we establish the method's scalability and robust\nperformance in replicating path travel times across diverse highway networks,\nincluding Seattle, Orlando, Denver, Philadelphia, and Boston. In these expanded\nevaluations, our method not only aligns with simulation-based benchmarks but\nalso achieves an average 13% improvement in it's ability to fit travel time\ndata compared to the baseline during afternoon peak hours."}
{"id": "2507.00217", "pdf": "https://arxiv.org/pdf/2507.00217", "abs": "https://arxiv.org/abs/2507.00217", "authors": ["Tiancheng Chen", "Ales Kubicek", "Langwen Huang", "Torsten Hoefler"], "title": "CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training", "categories": ["cs.DC"], "comment": "USENIX ATC '25", "summary": "Training large language models (LLMs) now requires resources that exceed a\nsingle datacenter, making cross-datacenter strategies increasingly crucial. We\npresent CrossPipe, a framework designed to optimize model training across\ngeographically distributed datacenters by explicitly modeling and mitigating\nthe impact of network latency and limited bandwidth. It enables unified\nanalysis and optimization incorporating both pipeline parallelism (PP) and\nopportunities for overlapping data parallelism (DP) communication. CrossPipe\ngenerates optimized pipeline schedules using either solver-based optimal or\nfast near-optimal greedy algorithms, built upon a flexible execution engine\nthat separates scheduling logic from communication details. Our evaluation\nshows that CrossPipe reduces training time by up to 33.6\\% compared to\ntraditional pipeline schedules under identical memory constraints. When memory\nconstraints are relaxed, CrossPipe maintains strong performance despite\ncommunication delays, approaching the efficiency of idealized schedules without\ndelays. CrossPipe offers improved scalability and resource utilization,\nparticularly in environments with high network latency or limited bandwidth."}
{"id": "2507.00094", "pdf": "https://arxiv.org/pdf/2507.00094", "abs": "https://arxiv.org/abs/2507.00094", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications."}
{"id": "2507.00237", "pdf": "https://arxiv.org/pdf/2507.00237", "abs": "https://arxiv.org/abs/2507.00237", "authors": ["Oleg Kolosov", "David Breitgand", "Dean H. Lorenz", "Gala Yadgar"], "title": "Plan-Based Scalable Online Virtual Network Embedding", "categories": ["cs.NI"], "comment": "Accepted to IEEE ICDCS 2025", "summary": "Network virtualization allows hosting applications with diverse computation\nand communication requirements on shared edge infrastructure. Given a set of\nrequests for deploying virtualized applications, the edge provider has to\ndeploy a maximum number of them to the underlying physical network, subject to\ncapacity constraints. This challenge is known as the virtual network embedding\n(VNE) problem: it models applications as virtual networks, where virtual nodes\nrepresent functions and virtual links represent communication between the\nvirtual nodes.\n  All variants of VNE are known to be strongly NP-hard. Because of its\ncentrality to network virtualization, VNE has been extensively studied. We\nfocus on the online variant of VNE, in which deployment requests are not known\nin advance. This reflects the highly skewed and unpredictable demand intrinsic\nto the edge. Unfortunately, existing solutions to online VNE do not scale well\nwith the number of requests per second and the physical topology size.\n  We propose a novel approach in which our new online algorithm, OLIVE,\nleverages a nearly optimal embedding for an aggregated expected demand. This\nembedding is computed offline. It serves as a plan that OLIVE uses as a guide\nfor handling actual individual requests while dynamically compensating for\ndeviations from the plan. We demonstrate that our solution can handle a number\nof requests per second greater by two orders of magnitude than the best results\nreported in the literature. Thus, it is particularly suitable for realistic\nedge environments."}
{"id": "2507.00367", "pdf": "https://arxiv.org/pdf/2507.00367", "abs": "https://arxiv.org/abs/2507.00367", "authors": ["Yeonsoo Jeon", "Mattan Erez", "Michael Orshansky"], "title": "Presto: Hardware Acceleration of Ciphers for Hybrid Homomorphic Encryption", "categories": ["cs.AR", "cs.CR"], "comment": null, "summary": "Hybrid Homomorphic Encryption (HHE) combines symmetric key and homomorphic\nencryption to reduce ciphertext expansion crucial in client-server deployments\nof HE. Special symmetric ciphers, amenable to efficient HE evaluation, have\nbeen developed. Their client-side deployment calls for performant and\nenergy-efficient implementation, and in this paper we develop and evaluate\nhardware accelerators for the two known CKKS-targeting HHE ciphers, HERA and\nRubato.\n  We design vectorized and overlapped functional modules. The design exploits\ntransposition-invariance property of the MixColumns and MixRows function and\nalternates the order of intermediate state to eliminate bubbles in stream key\ngeneration, improving latency and throughput. We decouple the RNG and key\ncomputation phases to hide the latency of RNG and to reduce the critical path\nin FIFOs, achieving higher operating frequency.\n  We implement the accelerator on an AMD Virtex UltraScale+ FPGA. Both Rubato\nand HERA achieve a 6x improvement in throughput compared to the software\nimplementation. In terms of latency, Rubato achieves a 5x reduction, while HERA\nachieves a 3x reduction. Additionally, our hardware implementations reduce\nenergy consumption by 75x for Rubato and 47x for HERA compared to their\nsoftware implementation."}
{"id": "2507.00421", "pdf": "https://arxiv.org/pdf/2507.00421", "abs": "https://arxiv.org/abs/2507.00421", "authors": ["Parthiv Katapara", "Anand Sharma"], "title": "Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development", "categories": ["cs.SE"], "comment": "This paper present survey on DevOps practices which exists in\n  Embedded Software development", "summary": "The adoption of DevOps practices in embedded systems and firmware development\nis emerging as a response to the growing complexity of modern\nhardware--software co-designed products. Unlike cloud-native applications,\nembedded systems introduce challenges such as hardware dependency, real-time\nconstraints, and safety-critical requirements. This literature review\nsynthesizes findings from 20 academic and industrial sources to examine how\nDevOps principles--particularly continuous integration, continuous delivery,\nand automated testing--are adapted to embedded contexts. We categorize efforts\nacross tooling, testing strategies, pipeline automation, and security\npractices. The review highlights current limitations in deployment workflows\nand observability, proposing a roadmap for future research. This work offers\nresearchers and practitioners a consolidated understanding of Embedded DevOps,\nbridging fragmented literature with a structured perspective."}
{"id": "2507.00412", "pdf": "https://arxiv.org/pdf/2507.00412", "abs": "https://arxiv.org/abs/2507.00412", "authors": ["Meenakshi Krishnan", "Ramani Duraiswami"], "title": "ViscoReg: Neural Signed Distance Functions via Viscosity Solutions", "categories": ["cs.GR"], "comment": "14 pages, 6 figures", "summary": "Implicit Neural Representations (INRs) that learn a Signed Distance Function\n(SDF) are a powerful tool for continuous 3D scene reconstruction. These models\nare trained by enforcing the Eikonal equation. We demonstrate theoretically\nthat despite the ill-posedness of the Eikonal equation, generalization error\nestimates may be obtained for Neural SDFs in terms of the training error.\nHowever, training with the Eikonal loss can lead to unstable gradient flows,\nnecessitating alternate stabilization techniques. Traditional numerical solvers\nfor the equation have relied on viscosity approaches for regularization. We\nenhance Neural SDF training using this well-developed theory, and introduce a\nnew loss formulation we call ViscoReg. We theoretically demonstrate the\nstability of the gradient flow equation of our proposed loss term. Empirically,\nViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik\nwithout adding significant computational cost."}
{"id": "2507.00264", "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility."}
{"id": "2507.00557", "pdf": "https://arxiv.org/pdf/2507.00557", "abs": "https://arxiv.org/abs/2507.00557", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "categories": ["cs.AI", "cs.LO", "cs.SC"], "comment": null, "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods."}
{"id": "2507.00264", "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility."}
{"id": "2507.00161", "pdf": "https://arxiv.org/pdf/2507.00161", "abs": "https://arxiv.org/abs/2507.00161", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments."}
{"id": "2507.00055", "pdf": "https://arxiv.org/pdf/2507.00055", "abs": "https://arxiv.org/abs/2507.00055", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks."}
{"id": "2507.00444", "pdf": "https://arxiv.org/pdf/2507.00444", "abs": "https://arxiv.org/abs/2507.00444", "authors": ["Chengjie Liu", "Jiajia Li", "Yabing Feng", "Wenhao Huang", "Weiyu Chen", "Yuan Du", "Jun Yang", "Li Du"], "title": "DiffCkt: A Diffusion Model-Based Hybrid Neural Network Framework for Automatic Transistor-Level Generation of Analog Circuits", "categories": ["cs.ET"], "comment": "Accepted by ICCAD2025", "summary": "Analog circuit design consists of the pre-layout and layout phases. Among\nthem, the pre-layout phase directly decides the final circuit performance, but\nheavily depends on experienced engineers to do manual design according to\nspecific application scenarios. To overcome these challenges and automate the\nanalog circuit pre-layout design phase, we introduce DiffCkt: a diffusion\nmodel-based hybrid neural network framework for the automatic transistor-level\ngeneration of analog circuits, which can directly generate corresponding\ncircuit structures and device parameters tailored to specific performance\nrequirements. To more accurately quantify the efficiency of circuits generated\nby DiffCkt, we introduce the Circuit Generation Efficiency Index (CGEI), which\nis determined by both the figure of merit (FOM) of a single generated circuit\nand the time consumed. Compared with relative research, DiffCkt has improved\nCGEI by a factor of $2.21 \\sim 8365\\times$, reaching a state-of-the-art (SOTA)\nlevel. In conclusion, this work shows that the diffusion model has the\nremarkable ability to learn and generate analog circuit structures and device\nparameters, providing a revolutionary method for automating the pre-layout\ndesign of analog circuits. The circuit dataset will be open source, its preview\nversion is available at https://github.com/CjLiu-NJU/DiffCkt."}
{"id": "2507.00418", "pdf": "https://arxiv.org/pdf/2507.00418", "abs": "https://arxiv.org/abs/2507.00418", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank Würthwein"], "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "categories": ["cs.DC", "cs.AI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP)."}
{"id": "2507.00188", "pdf": "https://arxiv.org/pdf/2507.00188", "abs": "https://arxiv.org/abs/2507.00188", "authors": ["Qihan Zhang", "Shaolin Xie", "Ibrahim Sabek"], "title": "LIMAO: A Framework for Lifelong Modular Learned Query Optimization", "categories": ["cs.DB"], "comment": "To appear at VLDB 2025 (https://vldb.org/2025/)", "summary": "Query optimizers are crucial for the performance of database systems.\nRecently, many learned query optimizers (LQOs) have demonstrated significant\nperformance improvements over traditional optimizers. However, most of them\noperate under a limited assumption: a static query environment. This limitation\nprevents them from effectively handling complex, dynamic query environments in\nreal-world scenarios. Extensive retraining can lead to the well-known\ncatastrophic forgetting problem, which reduces the LQO generalizability over\ntime. In this paper, we address this limitation and introduce LIMAO (Lifelong\nModular Learned Query Optimizer), a framework for lifelong learning of plan\ncost prediction that can be seamlessly integrated into existing LQOs. LIMAO\nleverages a modular lifelong learning technique, an attention-based neural\nnetwork composition architecture, and an efficient training paradigm designed\nto retain prior knowledge while continuously adapting to new environments. We\nimplement LIMAO in two LQOs, showing that our approach is agnostic to\nunderlying engines. Experimental results show that LIMAO significantly enhances\nthe performance of LQOs, achieving up to a 40% improvement in query execution\ntime and reducing the variance of execution time by up to 60% under dynamic\nworkloads. By leveraging a precise and self-consistent design, LIMAO\neffectively mitigates catastrophic forgetting, ensuring stable and reliable\nplan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup\non selected benchmarks, highlighting its practical advantages in real-world\nquery optimization."}
{"id": "2507.00337", "pdf": "https://arxiv.org/pdf/2507.00337", "abs": "https://arxiv.org/abs/2507.00337", "authors": ["Yuxin Liu", "Tianyang Zhang", "Qiang Wu", "Ju Ren", "Kyle Jamieson", "Yaxiong Xie"], "title": "Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols", "categories": ["cs.NI"], "comment": null, "summary": "Delay-based protocols rely on end-to-end delay measurements to detect network\ncongestion. However, in cellular networks, Radio Access Network (RAN) buffers\nintroduce significant delays unrelated to congestion, fundamentally challenging\nthese protocols' assumptions. We identify two major types of RAN buffers -\nretransmission buffers and uplink scheduling buffers - that can introduce\ndelays comparable to congestion-induced delays, severely degrading protocol\nperformance. We present CellNinjia, a software-based system providing real-time\nvisibility into RAN operations, and Gandalf, which leverages this visibility to\nsystematically handle RAN-induced delays. Unlike existing approaches that treat\nthese delays as random noise, Gandalf identifies specific RAN operations and\ncompensates for their effects. Our evaluation in commercial 4G LTE and 5G\nnetworks shows that Gandalf enables substantial performance improvements - up\nto 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols'\ncore algorithms, demonstrating that delay-based protocols can realize their\nfull potential in cellular networks."}
{"id": "2507.00642", "pdf": "https://arxiv.org/pdf/2507.00642", "abs": "https://arxiv.org/abs/2507.00642", "authors": ["Runkai Li", "Jia Xiong", "Xiuyuan He", "Jieru Zhao", "Qiang Xu", "Xi Wang"], "title": "ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis", "categories": ["cs.AR"], "comment": null, "summary": "The increasing complexity of computational demands has accelerated the\nadoption of domain-specific accelerators, yet traditional hardware design\nmethodologies remain constrained by prolonged development and verification\ncycles. High-Level Synthesis (HLS) bridges the gap between software and\nhardware by enabling hardware design from high-level programming languages.\nHowever, its widespread adoption is hindered by strict coding constraints and\nintricate hardware-specific optimizations, creating significant obstacles for\ndevelopers. Recent advancements in Large Language Models (LLMs) demonstrate\nsubstantial potential in hardware design automation. However, their\neffectiveness is limited by the scarcity of high-quality datasets, particularly\nin the context of HLS. To address these challenges, we introduce ChatHLS, an\nagile HLS design automation and optimization workflow that leverages fine-tuned\nLLMs integrated within a multi-agent framework for error correction and design\noptimization. Our extensive evaluations reveal that ChatHLS achieves an average\nrepair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and\nLlama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers\nperformance enhancements ranging from 1.9$\\times$ to 14.8$\\times$ upon\nresource-constrained kernels. By enabling sophisticated optimization reasoning\nwithin practical computational budgets, ChatHLS attains a 4.9$\\times$ geometric\nmean speedup compared to state-of-the-art DSL-based approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while maintaining rigorous standards of design reliability\nand optimization quality."}
{"id": "2507.00481", "pdf": "https://arxiv.org/pdf/2507.00481", "abs": "https://arxiv.org/abs/2507.00481", "authors": ["Philipp M. Zähl", "Sabine Theis", "Martin R. Wolf"], "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research."}
{"id": "2507.00476", "pdf": "https://arxiv.org/pdf/2507.00476", "abs": "https://arxiv.org/abs/2507.00476", "authors": ["Chenliang Zhou", "Zheyuan Hu", "Cengiz Oztireli"], "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications."}
{"id": "2507.00488", "pdf": "https://arxiv.org/pdf/2507.00488", "abs": "https://arxiv.org/abs/2507.00488", "authors": ["Lloyd Allison"], "title": "Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?", "categories": ["cs.PL", "D.3.3; D.2"], "comment": null, "summary": "Compared to functions in mathematics, functions in programming languages seem\nto be under classified. Functional programming languages based on the lambda\ncalculus famously treat functions as first-class values. Object-oriented\nlanguages have adopted ``lambdas'', notably for call-back routines in\nevent-based programming. Typically a programming language has functions, a\nfunction has a type, and some functions act on other functions and/or return\nfunctions but there is generally a lack of (i) ``class Function'' in the OO\nsense of the word class and particularly (ii) subclasses of Function for\nfunctions having specific properties. Some such classes are presented here and\nprogrammed in some popular programming languages as an experimental\ninvestigation into OO languages missing this opportunity."}
{"id": "2507.00491", "pdf": "https://arxiv.org/pdf/2507.00491", "abs": "https://arxiv.org/abs/2507.00491", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets."}
{"id": "2507.00198", "pdf": "https://arxiv.org/pdf/2507.00198", "abs": "https://arxiv.org/abs/2507.00198", "authors": ["Ji Hwan Park", "Braden Roper", "Amirhossein Arezoumand", "Tien Tran"], "title": "Exploring AR Label Placements in Visually Cluttered Scenarios", "categories": ["cs.HC"], "comment": null, "summary": "We investigate methods for placing labels in AR environments that have\nvisually cluttered scenes. As the number of items increases in a scene within\nthe user' FOV, it is challenging to effectively place labels based on existing\nlabel placement guidelines. To address this issue, we implemented three label\nplacement techniques for in-view objects for AR applications. We specifically\ntarget a scenario, where various items of different types are scattered within\nthe user's field of view, and multiple items of the same type are situated\nclose together. We evaluate three placement techniques for three target tasks.\nOur study shows that using a label to spatially group the same types of items\nis beneficial for identifying, comparing, and summarizing data."}
{"id": "2507.00466", "pdf": "https://arxiv.org/pdf/2507.00466", "abs": "https://arxiv.org/abs/2507.00466", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation."}
{"id": "2507.00710", "pdf": "https://arxiv.org/pdf/2507.00710", "abs": "https://arxiv.org/abs/2507.00710", "authors": ["Can Cui", "ZIye Jia", "Chao Dong", "Qihui Wu"], "title": "Robust Task Offloading for UAV-enabled Secure MEC Against Aerial Eavesdropper", "categories": ["cs.ET"], "comment": null, "summary": "Unmanned aerial vehicles (UAVs) are recognized as a promising candidate for\nthe multi-access edge computing (MEC) in the future sixth generation\ncommunication networks. However, the aerial eavesdropping UAVs (EUAVs) pose a\nsignificant security threat to the data offloading. In this paper, we\ninvestigate a robust MEC scenario with multiple service UAVs (SUAVs) towards\nthe potential eavesdropping from the EUAV, in which the random parameters such\nas task complexities are considered in the practical applications. In detail,\nthe problem is formulated to optimize the deployment positions of SUAVs, the\nconnection relationships between GUs and SUAVs, and the offloading ratios. With\nthe uncertain task complexities, the corresponding chance constraints are\nconstructed under the uncertainty set, which is tricky to deal with. Therefore,\nwe first optimize the pre-deployment of SUAVs by the K-means algorithm. Then,\nthe distributionally robust optimization method is employed, and the\nconditional value at risk is utilized to transform the chance constraints into\nconvex forms, which can be solved via convex toolkits. Finally, the simulation\nresults show that with the consideration of uncertainties, just 5% more energy\nis consumed compared with the ideal circumstance, which verifies the robustness\nof the proposed algorithms."}
{"id": "2507.00428", "pdf": "https://arxiv.org/pdf/2507.00428", "abs": "https://arxiv.org/abs/2507.00428", "authors": ["Mohammad Firas Sada", "John J. Graham", "Mahidhar Tatineni", "Dmitry Mishin", "Thomas A. DeFanti", "Frank Würthwein"], "title": "Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor", "categories": ["cs.DC", "cs.NI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC25)", "summary": "As machine learning (ML) applications become integral to modern network\noperations, there is an increasing demand for network programmability that\nenables low-latency ML inference for tasks such as Quality of Service (QoS)\nprediction and anomaly detection in cybersecurity. ML models provide\nadaptability through dynamic weight adjustments, making Programming\nProtocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an\nideal platform for investigating In-Network Machine Learning (INML). These\ndevices offer high-throughput, low-latency packet processing and can be\ndynamically reconfigured via the control plane, allowing for flexible\nintegration of ML models directly at the network edge. This paper explores the\napplication of the P4 programming paradigm to neural networks and regression\nmodels, where weights and biases are stored in control plane table lookups.\nThis approach enables flexible programmability and efficient deployment of\nretrainable ML models at the network edge, independent of core infrastructure\nat the switch level."}
{"id": "2507.00343", "pdf": "https://arxiv.org/pdf/2507.00343", "abs": "https://arxiv.org/abs/2507.00343", "authors": ["Vishal Chakraborty", "Youri Kaminsky", "Sharad Mehrotra", "Felix Naumann", "Faisal Nawab", "Primal Pappachan", "Mohammad Sadoghi", "Nalini Venkatasubramanian"], "title": "Meaningful Data Erasure in the Presence of Dependencies", "categories": ["cs.DB"], "comment": "VLDB 2025 Preprint", "summary": "Data regulations like GDPR require systems to support data erasure but leave\nthe definition of \"erasure\" open to interpretation. This ambiguity makes\ncompliance challenging, especially in databases where data dependencies can\nlead to erased data being inferred from remaining data. We formally define a\nprecise notion of data erasure that ensures any inference about deleted data,\nthrough dependencies, remains bounded to what could have been inferred before\nits insertion. We design erasure mechanisms that enforce this guarantee at\nminimal cost. Additionally, we explore strategies to balance cost and\nthroughput, batch multiple erasures, and proactively compute data retention\ntimes when possible. We demonstrate the practicality and scalability of our\nalgorithms using both real and synthetic datasets."}
{"id": "2507.00623", "pdf": "https://arxiv.org/pdf/2507.00623", "abs": "https://arxiv.org/abs/2507.00623", "authors": ["Daniel Mejías", "Inhar Yeregui", "Roberto Viola", "Miguel Fernández", "Mario Montagud"], "title": "Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols", "categories": ["cs.NI"], "comment": null, "summary": "The increasing complexity of Extended Reality (XR) applications demands\nsubstantial processing power and high bandwidth communications, often\nunavailable on lightweight devices. Remote rendering consists of offloading\nprocessing tasks to a remote node with a powerful GPU, delivering the rendered\ncontent to the end device. The delivery is usually performed through popular\nstreaming protocols such as Web Real-Time Communications (WebRTC), offering a\ndata channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH),\nbetter suitable for scalability. Moreover, new streaming protocols based on\nQUIC are emerging as potential replacements for WebRTC and DASH and offer\nbenefits like connection migration, stream multiplexing and multipath delivery.\nThis work describes the integration of the two most popular multimedia\nframeworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote\nRenderer, and analyzes their performance when offering different protocols for\ndelivering the rendered content to the end device over WIFI or 5G. This\nsolution constitutes a beyond state-of-the-art testbed to conduct cutting-edge\nresearch in the XR field."}
{"id": "2507.00797", "pdf": "https://arxiv.org/pdf/2507.00797", "abs": "https://arxiv.org/abs/2507.00797", "authors": ["Zhican Wang", "Hongxiang Fan", "Haroon Waris", "Gang Wang", "Zhenyu Li", "Jianfei Jiang", "Yanan Sun", "Guanghui He"], "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction and Dataflow-flexible Accelerator", "categories": ["cs.AR"], "comment": "DAC 2025", "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."}
{"id": "2507.00496", "pdf": "https://arxiv.org/pdf/2507.00496", "abs": "https://arxiv.org/abs/2507.00496", "authors": ["Hongjing Guo", "Chuanqi Tao", "Zhiqiu Huang", "Weiqin Zou"], "title": "Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey", "categories": ["cs.SE"], "comment": null, "summary": "As Deep Learning (DL) models are increasingly applied in safety-critical\ndomains, ensuring their quality has emerged as a pressing challenge in modern\nsoftware engineering. Among emerging validation paradigms, coverage-guided\ntesting (CGT) has gained prominence as a systematic framework for identifying\nerroneous or unexpected model behaviors. Despite growing research attention,\nexisting CGT studies remain methodologically fragmented, limiting the\nunderstanding of current advances and emerging trends. This work addresses that\ngap through a comprehensive review of state-of-the-art CGT methods for DL\nmodels, including test coverage analysis, coverage-guided test input\ngeneration, and coverage-guided test input optimization. This work provides\ndetailed taxonomies to organize these methods based on methodological\ncharacteristics and application scenarios. We also investigate evaluation\npractices adopted in existing studies, including the use of benchmark datasets,\nmodel architectures, and evaluation aspects. Finally, open challenges and\nfuture directions are highlighted in terms of the correlation between\nstructural coverage and testing objectives, method generalizability across\ntasks and models, practical deployment concerns, and the need for standardized\nevaluation and tool support. This work aims to provide a roadmap for future\nacademic research and engineering practice in DL model quality assurance."}
{"id": "2507.00725", "pdf": "https://arxiv.org/pdf/2507.00725", "abs": "https://arxiv.org/abs/2507.00725", "authors": ["Amritendu Dhar", "Apratim Chakraborty", "Vijay Natarajan"], "title": "Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory", "categories": ["cs.GR", "cs.CG", "I.3.5"], "comment": null, "summary": "Morse-Cerf theory considers a one-parameter family of smooth functions\ndefined on a manifold and studies the evolution of their critical points with\nthe parameter. This paper presents an adaptation of Morse-Cerf theory to a\nfamily of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram\nare introduced as representations of the evolution of critical points of the PL\nfunction. The characterization of a crossing in the vertex diagram based on the\nhomology of the lower links of vertices leads to the definition of a\ntopological descriptor for time-varying scalar fields. An algorithm for\ncomputing the Cerf diagram and a measure for comparing two Cerf diagrams are\nalso described together with experimental results on time-varying scalar\nfields."}
{"id": "2507.00824", "pdf": "https://arxiv.org/pdf/2507.00824", "abs": "https://arxiv.org/abs/2507.00824", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Michał Król", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivière"], "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "categories": ["cs.DC", "cs.NI", "cs.PF"], "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline."}
{"id": "2507.00202", "pdf": "https://arxiv.org/pdf/2507.00202", "abs": "https://arxiv.org/abs/2507.00202", "authors": ["Blade Frisch", "Betts Peters", "Keith Vertanen"], "title": "Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group", "categories": ["cs.HC"], "comment": null, "summary": "Purpose: Little research has explored the communication needs of autistic\nadults and how their needs differ from those of other disabled populations.\nAugmentative and Alternative Communication (AAC) can support these\ncommunication needs, but more guidance is needed on how to design AAC to\nsupport this population.\n  Materials and Methods: We conducted an online, asynchronous, text-based focus\ngroup with five autistic adults to explore their social communication and\ncommunity engagement and how AAC can help support them.\n  Results and Conclusion: Our analysis of the participant responses found that\n1) participants' emotional experiences impacted the communication methods they\nused, 2) speaking autistic adults can benefit from AAC use, and 3) autistic\nshutdown creates dynamic communication needs. We present implications for\nfuture AAC design: supporting communication in times of shutdown, indicating\ncommunication ability to communication partners, and a need to better\nunderstand the fear of using AAC. These implications can inform the design for\nfuture AAC systems. We also provide themes for future autism research:\nexploring the impact of a late diagnosis, gaining a better understanding of the\ncommunication needs during autistic shutdown, and expanding research to include\nthe social and environmental factors that impact communication. Finally, we\nprovide guidance on how future online focus groups can be run in an accessible\nmanner."}
{"id": "2507.00498", "pdf": "https://arxiv.org/pdf/2507.00498", "abs": "https://arxiv.org/abs/2507.00498", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "title": "MuteSwap: Silent Face-based Voice Conversion", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "comment": null, "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC."}
{"id": "2507.00081", "pdf": "https://arxiv.org/pdf/2507.00081", "abs": "https://arxiv.org/abs/2507.00081", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments."}
{"id": "2507.00507", "pdf": "https://arxiv.org/pdf/2507.00507", "abs": "https://arxiv.org/abs/2507.00507", "authors": ["Chuhao Xu", "Zijun Li", "Quan Chen", "Han Zhao", "Minyi Guo"], "title": "LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference", "categories": ["cs.DC"], "comment": null, "summary": "The rise of LLMs has driven demand for private serverless deployments,\ncharacterized by moderate-scale models and infrequent requests. While existing\nsolutions follow exclusive GPU deployment, we take a step back to explore\nmodern platforms and find that: Emerging CPU architectures with built-in\naccelerators are capable of serving LLMs but remain underutilized, and both\nCPUs and GPUs can accommodate multiple LLMs simultaneously.\n  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized\nLLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh\ntackles three fundamental challenges: (1) precise, fine-grained compute\nresource allocation at token-level to handle fluctuating computational demands;\n(2) a coordinated and forward-looking memory scaling mechanism to detect\nout-of-memory hazards and reduce operational overhead; and (3) a dual approach\nthat reduces resource fragmentation through proactive preemption and reactive\nbin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that\nLLM-Meshimproves service capacity by 44% - 63% through sharing, while further\nleveraging CPUs boosts this to 91% - 159%."}
{"id": "2507.00379", "pdf": "https://arxiv.org/pdf/2507.00379", "abs": "https://arxiv.org/abs/2507.00379", "authors": ["Zikai Wang", "Qianxi Zhang", "Baotong Lu", "Qi Chen", "Cheng Tan"], "title": "Towards Robustness: A Critique of Current Vector Database Assessments", "categories": ["cs.DB"], "comment": null, "summary": "Vector databases are critical infrastructure in AI systems, and average\nrecall is the dominant metric for their evaluation. Both users and researchers\nrely on it to choose and optimize their systems. We show that relying on\naverage recall is problematic. It hides variability across queries, allowing\nsystems with strong mean performance to underperform significantly on hard\nqueries. These tail cases confuse users and can lead to failure in downstream\napplications such as RAG. We argue that robustness consistently achieving\nacceptable recall across queries is crucial to vector database evaluation. We\npropose Robustness-$\\delta$@K, a new metric that captures the fraction of\nqueries with recall above a threshold $\\delta$. This metric offers a deeper\nview of recall distribution, helps vector index selection regarding application\nneeds, and guides the optimization of tail performance. We integrate\nRobustness-$\\delta$@K into existing benchmarks and evaluate mainstream vector\nindexes, revealing significant robustness differences. More robust vector\nindexes yield better application performance, even with the same average\nrecall. We also identify design factors that influence robustness, providing\nguidance for improving real-world performance."}
{"id": "2507.00672", "pdf": "https://arxiv.org/pdf/2507.00672", "abs": "https://arxiv.org/abs/2507.00672", "authors": ["Haoxiang Luo", "Yinqiu Liu", "Ruichen Zhang", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Zehui Xiong", "Xianbin Wang", "Xuemin Shen"], "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications."}
{"id": "2507.00855", "pdf": "https://arxiv.org/pdf/2507.00855", "abs": "https://arxiv.org/abs/2507.00855", "authors": ["Marta Navarro", "Josué Feliu", "Salvador Petit", "María E. Gómez", "Julio Sahuquillo"], "title": "A New Family of Thread to Core Allocation Policies for an SMT ARM Processor", "categories": ["cs.DC", "cs.AR"], "comment": "13 pages", "summary": "Modern high-performance servers commonly integrate Simultaneous\nMultithreading (SMT) processors, which efficiently boosts throughput over\nsingle-threaded cores. Optimizing performance in SMT processors faces\nchallenges due to the inter-application interference within each SMT core. To\nmitigate the interference, thread-to-core (T2C) allocation policies play a\npivotal role. State-of-the-art T2C policies work in two steps: i) building a\nper-application performance stack using performance counters and ii) building\nperformance prediction models to identify the best pairs of applications to run\non each core.\n  This paper explores distinct ways to build the performance stack in ARM\nprocessors and introduces the Instructions and Stalls Cycles (ISC) stack, a\nnovel approach to overcome ARM PMU limitations. The ISC stacks are used as\ninputs for a performance prediction model to estimate the applications'\nperformance considering the inter-application interference. The accuracy of the\nprediction model (second step) depends on the accuracy of the performance stack\n(first step); thus, the higher the accuracy of the performance stack, the\nhigher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies.\nExperimental results show that $SYNPA4$, the best-performing SYNPA variant,\noutperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the\ngains achieved by the state-of-the-art policies for ARM processors.\nFurthermore, the multiple discussions and refinements presented throughout this\npaper can be applied to other SMT processors from distinct vendors and are\naimed at helping performance analysts build performance stacks for accurate\nperformance estimates in real processors."}
{"id": "2507.00686", "pdf": "https://arxiv.org/pdf/2507.00686", "abs": "https://arxiv.org/abs/2507.00686", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "categories": ["cs.SE", "cs.ET"], "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement."}
{"id": "2507.00261", "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer."}
{"id": "2507.00909", "pdf": "https://arxiv.org/pdf/2507.00909", "abs": "https://arxiv.org/abs/2507.00909", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "comment": "10 pages, 6 figures, 1 table", "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development."}
{"id": "2507.00271", "pdf": "https://arxiv.org/pdf/2507.00271", "abs": "https://arxiv.org/abs/2507.00271", "authors": ["Zhuochao Peng", "Jiaxin Xu", "Jun Hu", "Haian Xue", "Laurens A. G. Kolks", "Pieter M. A. Desmet"], "title": "User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted to International Conference on Social Robotics + AI (ICSR\n  2025)", "summary": "While recent research highlights the potential of social robots to support\nmood regulation, little is known about how prospective users view their\nintegration into everyday life. To explore this, we conducted an exploratory\ncase study that used a speculative robot concept \"Mora\" to provoke reflection\nand facilitate meaningful discussion about using social robots to manage\nsubtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a\ncommon dip in mood that occurs at the end of the weekend, as a relatable\ncontext in which to explore individuals' insights. Using a video prototype and\na co-constructing stories method, we engaged 15 participants in imagining\ninteractions with Mora and discussing their expectations, doubts, and concerns.\nThe study surfaced a range of nuanced reflections around the attributes of\nsocial robots like empathy, intervention effectiveness, and ethical boundaries,\nwhich we translated into design considerations for future research and\ndevelopment in human-robot interaction."}
{"id": "2507.00950", "pdf": "https://arxiv.org/pdf/2507.00950", "abs": "https://arxiv.org/abs/2507.00950", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "MVP: Winning Solution to SMP Challenge 2025 Video Track", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Social media platforms serve as central hubs for content dissemination,\nopinion expression, and public engagement across diverse modalities. Accurately\npredicting the popularity of social media videos enables valuable applications\nin content recommendation, trend detection, and audience engagement. In this\npaper, we present Multimodal Video Predictor (MVP), our winning solution to the\nVideo Track of the SMP Challenge 2025. MVP constructs expressive post\nrepresentations by integrating deep video features extracted from pretrained\nmodels with user metadata and contextual information. The framework applies\nsystematic preprocessing techniques, including log-transformations and outlier\nremoval, to improve model robustness. A gradient-boosted regression model is\ntrained to capture complex patterns across modalities. Our approach ranked\nfirst in the official evaluation of the Video Track, demonstrating its\neffectiveness and reliability for multimodal video popularity prediction on\nsocial platforms. The source code is available at\nhttps://anonymous.4open.science/r/SMPDVideo."}
{"id": "2507.00108", "pdf": "https://arxiv.org/pdf/2507.00108", "abs": "https://arxiv.org/abs/2507.00108", "authors": ["Clemente Rubio-Manzano", "Jazna Meza", "Rodolfo Fernandez-Santibanez", "Christian Vidal-Castro"], "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.PL"], "comment": null, "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process."}
{"id": "2507.00550", "pdf": "https://arxiv.org/pdf/2507.00550", "abs": "https://arxiv.org/abs/2507.00550", "authors": ["Bruce Fang", "Danyi Gao"], "title": "Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling", "categories": ["cs.DC"], "comment": null, "summary": "This paper addresses the challenges of rapid resource variation and highly\nuncertain task loads in cloud computing environments. It proposes an\noptimization method for elastic cloud resource scaling based on a multi-agent\nsystem. The method deploys multiple autonomous agents to perceive resource\nstates in parallel and make local decisions. While maintaining the distributed\nnature of the system, it introduces a collaborative value function to achieve\nglobal coordination. This improves the responsiveness of resource scheduling\nand enhances overall system performance. To strengthen system foresight, a\nlightweight state prediction model is designed. It assists agents in\nidentifying future workload trends and optimizes the selection of scaling\nactions. For policy training, the method adopts a centralized training and\ndecentralized execution reinforcement learning framework. This enables agents\nto learn effectively and coordinate strategies under conditions of incomplete\ninformation. The paper also constructs typical cloud scenarios, including\nmulti-tenancy and burst traffic, to evaluate the proposed method. The\nevaluation focuses on resource isolation, service quality assurance, and\nrobustness. Experimental results show that the proposed multi-agent scaling\nstrategy outperforms existing methods in resource utilization, SLA violation\ncontrol, and scheduling latency. The results demonstrate strong adaptability\nand intelligent regulation. This provides an efficient and reliable new\napproach to solving the problem of elastic resource scaling in complex cloud\nplatforms."}
{"id": "2507.00427", "pdf": "https://arxiv.org/pdf/2507.00427", "abs": "https://arxiv.org/abs/2507.00427", "authors": ["Hao Wu", "Changzheng Wei", "Yanhao Wang", "Li Lin", "Yilong Leng", "Shiyu He", "Minghao Zhao", "Hanghang Wu", "Ying Yan", "Aoying Zhou"], "title": "Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition", "categories": ["cs.DB"], "comment": null, "summary": "This paper investigates the feasibility of achieving zero-knowledge\nverifiability for graph databases, enabling database owners to\ncryptographically prove the query execution correctness without disclosing the\nunderlying data. Although similar capabilities have been explored for\nrelational databases, their implementation for graph databases presents unique\nchallenges. This is mainly attributed to the relatively large complexity of\nqueries in graph databases. When translating graph queries into arithmetic\ncircuits, the circuit scale can be too large to be practically evaluated. To\naddress this issue, we propose to break down graph queries into more\nfine-grained, primitive operators, enabling a step-by-step evaluation through\nsmaller-scale circuits. Accordingly, the verification with ZKP circuits of\ncomplex graph queries can be decomposed into a series of composable\ncryptographic primitives, each designed to verify a fundamental structural\nproperty such as path ordering or edge directionality. Especially, having\nnoticed that the graph expansion (i.e., traversing from nodes to their\nneighbors along edges) operation serves as the backbone of graph query\nevaluation, we design the expansion centric operator decomposition. In addition\nto constructing circuits for the expansion primitives, we also design\nspecialized ZKP circuits for the various attributes that augment this\ntraversal. The circuits are meticulously designed to take advantage of PLONKish\narithmetization. By integrating these optimized circuits, we implement ZKGraph,\na system that provides verifiable query processing while preserving data\nprivacy. Performance evaluation indicates that ZKGraph significantly\noutperforms naive in circuit implementations of graph operators, achieving\nsubstantial improvements in both runtime and memory consumption."}
{"id": "2507.00856", "pdf": "https://arxiv.org/pdf/2507.00856", "abs": "https://arxiv.org/abs/2507.00856", "authors": ["Beining Wu", "Jun Huang", "Qiang Duan", "Liang Dong", "Zhipeng Cai"], "title": "Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework", "categories": ["cs.NI", "eess.SP"], "comment": "Under review at IEEE Transactions on Networking", "summary": "This paper aims to enhance the performance of Vehicular Platooning (VP)\nsystems integrated with Wireless Federated Learning (WFL). In highly dynamic\nenvironments, vehicular platoons experience frequent communication changes and\nresource constraints, which significantly affect information exchange and\nlearning model synchronization. To address these challenges, we first formulate\nWFL in VP as a joint optimization problem that simultaneously considers Age of\nInformation (AoI) and Federated Learning Model Drift (FLMD) to ensure timely\nand accurate control. Through theoretical analysis, we examine the impact of\nFLMD on convergence performance and develop a two-stage Resource-Aware Control\nframework (RACE). The first stage employs a Lagrangian dual decomposition\nmethod for resource configuration, while the second stage implements a\nmulti-agent deep reinforcement learning approach for vehicle selection. The\napproach integrates Multi-Head Self-Attention and Long Short-Term Memory\nnetworks to capture spatiotemporal correlations in communication states.\nExperimental results demonstrate that, compared to baseline methods, the\nproposed framework improves AoI optimization by up to 45%, accelerates learning\nconvergence, and adapts more effectively to dynamic VP environments on the\nAI4MARS dataset."}
{"id": "2507.00937", "pdf": "https://arxiv.org/pdf/2507.00937", "abs": "https://arxiv.org/abs/2507.00937", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "comment": "8 pages, accepted by IROS 2025", "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots."}
{"id": "2507.00699", "pdf": "https://arxiv.org/pdf/2507.00699", "abs": "https://arxiv.org/abs/2507.00699", "authors": ["Guoliang Duan", "Mingwei Liu", "Yanlin Wang", "Chong Wang", "Xin Peng", "Zibin Zheng"], "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF."}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333", "abs": "https://arxiv.org/abs/2507.00333", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports."}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286", "abs": "https://arxiv.org/abs/2507.00286", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data."}
{"id": "2507.00145", "pdf": "https://arxiv.org/pdf/2507.00145", "abs": "https://arxiv.org/abs/2507.00145", "authors": ["Hasan Yiğit"], "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness."}
{"id": "2507.00576", "pdf": "https://arxiv.org/pdf/2507.00576", "abs": "https://arxiv.org/abs/2507.00576", "authors": ["Dante D. Sanchez-Gallegos", "J. L. Gonzalez-Compean", "Maxime Gonthier", "Valerie Hayot-Sasson", "J. Gregory Pauloski", "Haochen Pan", "Kyle Chard", "Jesus Carretero", "Ian Foster"], "title": "DynoStore: A wide-area distribution system for the management of data over heterogeneous storage", "categories": ["cs.DC"], "comment": "10 pages. Conference: The 25th IEEE International Symposium on\n  Cluster, Cloud, and Internet Computing", "summary": "Data distribution across different facilities offers benefits such as\nenhanced resource utilization, increased resilience through replication, and\nimproved performance by processing data near its source. However, managing such\ndata is challenging due to heterogeneous access protocols, disparate\nauthentication models, and the lack of a unified coordination framework. This\npaper presents DynoStore, a system that manages data across heterogeneous\nstorage systems. At the core of DynoStore are data containers, an abstraction\nthat provides standardized interfaces for seamless data management,\nirrespective of the underlying storage systems. Multiple data container\nconnections create a cohesive wide-area storage network, ensuring resilience\nusing erasure coding policies. Furthermore, a load-balancing algorithm ensures\nequitable and efficient utilization of storage resources. We evaluate DynoStore\nusing benchmarks and real-world case studies, including the management of\nmedical and satellite data across geographically distributed environments. Our\nresults demonstrate a 10\\% performance improvement compared to centralized\ncloud-hosted systems while maintaining competitive performance with\nstate-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits\nsuperior fault tolerance, withstanding more failures than traditional systems."}
{"id": "2507.00489", "pdf": "https://arxiv.org/pdf/2507.00489", "abs": "https://arxiv.org/abs/2507.00489", "authors": ["Pengyu Chen", "Zizheng Guo", "Jianwei Yang", "Dongjing Miao"], "title": "Towards Efficient Random-Order Enumeration for Join Queries", "categories": ["cs.DB"], "comment": null, "summary": "In many data analysis pipelines, a basic and time-consuming process is to\nproduce join results and feed them into downstream tasks. Numerous enumeration\nalgorithms have been developed for this purpose. To be a statistically\nmeaningful representation of the whole join result, the result tuples are\nrequired to be enumerated in uniformly random order. However, existing studies\nlack an efficient random-order enumeration algorithm with a worst-case runtime\nguarantee for (cyclic) join queries. In this paper, we study the problem of\nenumerating the results of a join query in random order. We develop an\nefficient random-order enumeration algorithm for join queries with no large\nhidden constants in its complexity, achieving expected\n$O(\\frac{\\mathrm{AGM}(Q)}{|Res(Q)|}\\log^2|Q|)$ delay,\n$O(\\mathrm{AGM}(Q)\\log|Q|)$ total running time after $O(|Q|\\log|Q|)$-time index\nconstruction, where $|Q|$ is the size of input, $\\mathrm{AGM}(Q)$ is the AGM\nbound, and $|Res(Q)|$ is the size of the join result. We prove that our\nalgorithm is near-optimal in the worst case, under the combinatorial $k$-clique\nhypothesis. Our algorithm requires no query-specific preprocessing and can be\nflexibly adapted to many common database indexes with only minor modifications.\nWe also devise two non-trivial techniques to speed up the enumeration, and\nprovide an experimental study on our enumeration algorithm along with the\nspeed-up techniques. The experimental results show that our algorithm, enhanced\nwith the proposed techniques, significantly outperforms existing\nstate-of-the-art methods."}
{"id": "2507.00896", "pdf": "https://arxiv.org/pdf/2507.00896", "abs": "https://arxiv.org/abs/2507.00896", "authors": ["Saverio Mascolo", "Andrea Vittorio Balillo", "Gioacchino Manfredi", "Davide D'Agostino", "Luca De Cicco"], "title": "QUIC Delay Control: an implementation of congestion and delay control", "categories": ["cs.NI", "C.2.2; C.2.1"], "comment": "8 pages, 9 figures", "summary": "A new congestion and delay control algorithm named QUIC Delay Control\n(QUIC-DC) is proposed for controlling not only congestion but also the queuing\ndelay encountered along the forward communication path. The core idea is to\nestimate the one-way queuing delay of a connection to trigger an early reaction\nto congestion. This idea, along with the TCP Westwood+ congestion control\nalgorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2,\nNewReno, Westwood+. The results obtained in both emulated and real network\nconnections show that QUIC-DC can significantly reduce packet losses along with\nend-to-end communication delays, while preserving network utilization, features\nthat are both very useful for real-time applications."}
{"id": "2507.00949", "pdf": "https://arxiv.org/pdf/2507.00949", "abs": "https://arxiv.org/abs/2507.00949", "authors": ["Yuqing Wang", "Charles Colley", "Brian Wheatman", "Jiya Su", "David F. Gleich", "Andrew A. Chien"], "title": "How Fast Can Graph Computations Go on Fine-grained Parallel Architectures", "categories": ["cs.DC", "cs.AR"], "comment": "13 pages, 11 figures, 6 tables", "summary": "Large-scale graph problems are of critical and growing importance and\nhistorically parallel architectures have provided little support. In the spirit\nof co-design, we explore the question, How fast can graph computing go on a\nfine-grained architecture? We explore the possibilities of an architecture\noptimized for fine-grained parallelism, natural programming, and the\nirregularity and skew found in real-world graphs. Using two graph benchmarks,\nPageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph\narchitecture, UpDown, to explore what performance codesign can achieve. To\ndemonstrate programmability, we wrote five variants of these algorithms.\nSimulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes\n(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS\non RMAT, exceeding the best prior results by 5x and 100x respectively."}
{"id": "2507.00786", "pdf": "https://arxiv.org/pdf/2507.00786", "abs": "https://arxiv.org/abs/2507.00786", "authors": ["Jukka Ruohonen", "Qusai Ramadan"], "title": "Snaps: Bloated and Outdated?", "categories": ["cs.SE"], "comment": "Submitted as a \"poster paper\" to APSEC", "summary": "Snap is an alternative software packaging system developed by Canonical and\nprovided by default in the Ubuntu Linux distribution. Given the heterogeneity\nof various Linux distributions and their various releases, Snap allows an\ninteroperable delivery of software directly to users. However, concerns and\ncriticism have also been frequently expressed. Regarding this criticism, the\npaper shows that currently distributed snap packages are indeed on average\nbloated in terms of their sizes and outdated in terms updating frequencies.\nWith these empirical observations, this short paper contributes to the research\ndomain of software packaging, software packages, and package managers."}
{"id": "2507.00299", "pdf": "https://arxiv.org/pdf/2507.00299", "abs": "https://arxiv.org/abs/2507.00299", "authors": ["Olivia Figueira", "Pranathi Chamarthi", "Tu Le", "Athina Markopoulou"], "title": "When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"", "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "TikTok, the social media platform that is popular among children and\nadolescents, offers a more restrictive \"Under 13 Experience\" exclusively for\nyoung users in the US, also known as TikTok's \"Kids Mode\". While prior research\nhas studied various aspects of TikTok's regular mode, including privacy and\npersonalization, TikTok's Kids Mode remains understudied, and there is a lack\nof transparency regarding its content curation and its safety and privacy\nprotections for children. In this paper, (i) we propose an auditing methodology\nto comprehensively investigate TikTok's Kids Mode and (ii) we apply it to\ncharacterize the platform's content curation and determine the prevalence of\nchild-directed content, based on regulations in the Children's Online Privacy\nProtection Act (COPPA). We find that 83% of videos observed on the \"For You\"\npage in Kids Mode are actually not child-directed, and even inappropriate\ncontent was found. The platform also lacks critical features, namely parental\ncontrols and accessibility settings. Our findings have important design and\nregulatory implications, as children may be incentivized to use TikTok's\nregular mode instead of Kids Mode, where they are known to be exposed to\nfurther safety and privacy risks."}
{"id": "2507.00172", "pdf": "https://arxiv.org/pdf/2507.00172", "abs": "https://arxiv.org/abs/2507.00172", "authors": ["Pranav Darshan", "Rohan J S", "Raghuveer Rajesh", "Ruchitha M", "Sanika Kamath", "Manas M N"], "title": "Intellectual Property Rights and Entrepreneurship in the NFT Ecosystem: Legal Frameworks, Business Models, and Innovation Opportunities", "categories": ["cs.CY", "cs.ET"], "comment": "11 pages", "summary": "Non Fungible Tokens have changed digital ownership and how creators earn\nmoney. Between 2021 and 2024, the market value exceeded 40 billion. However,\nthe fast growth of the NFT ecosystem has revealed serious issues in managing\nintellectual property rights. There is a lot of confusion about the difference\nbetween owning an NFT and owning the copyright for the underlying content. This\nresearch looks at the gap between traditional copyright laws and\nblockchain-based transactions. We use a mixed methods approach to analyze this\ndisconnect. We create a new IP rights matrix that clearly shows how copyright\nlaw relates to NFT ownership structures. Additionally, we include a business\nmodel taxonomy that sorts new commercial applications by their IP risk and\nsustainability factors. By examining important legal cases, smart contracts,\nand interviews with stakeholders, we find key problems in enforcing laws across\ndifferent regions, standardizing licenses, and assessing business\nopportunities."}
{"id": "2507.00716", "pdf": "https://arxiv.org/pdf/2507.00716", "abs": "https://arxiv.org/abs/2507.00716", "authors": ["Mohsen Koohi Esfahani"], "title": "Accelerating Loading WebGraphs in ParaGrapher", "categories": ["cs.DC"], "comment": null, "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."}
{"id": "2507.00839", "pdf": "https://arxiv.org/pdf/2507.00839", "abs": "https://arxiv.org/abs/2507.00839", "authors": ["Chiyu Hao", "Jixian Su", "Shixuan Sun", "Hao Zhang", "Sen Gao", "Jianwen Zhao", "Chenyi Zhang", "Jieru Zhao", "Chen Chen", "Minyi Guo"], "title": "RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries", "categories": ["cs.DB"], "comment": "17 pages, 18 figures", "summary": "Dynamic graph storage systems are essential for real-time applications such\nas social networks and recommendation, where graph data continuously evolves.\nHowever, they face significant challenges in efficiently handling concurrent\nread and write operations. We find that existing methods suffer from write\nqueries interfering with read efficiency, substantial time and space overhead\ndue to per-edge versioning, and an inability to balance performance, such as\nslow searches under concurrent workloads. To address these issues, we propose\nRapidStore, a holistic approach for efficient in-memory dynamic graph storage\ndesigned for read-intensive workloads. Our key idea is to exploit the\ncharacteristics of graph queries through a decoupled system design that\nseparates the management of read and write queries and decouples version data\nfrom graph data. Particularly, we design an efficient dynamic graph store to\ncooperate with the graph concurrency control mechanism. Experimental results\ndemonstrate that RapidStore enables fast and scalable concurrent graph queries,\neffectively balancing the performance of inserts, searches, and scans, and\nsignificantly improving efficiency in dynamic graph storage systems."}
{"id": "2507.00003", "pdf": "https://arxiv.org/pdf/2507.00003", "abs": "https://arxiv.org/abs/2507.00003", "authors": ["Eyhab Al-Masri"], "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "comment": null, "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems."}
{"id": "2507.00788", "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma Söderberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation."}
{"id": "2507.00305", "pdf": "https://arxiv.org/pdf/2507.00305", "abs": "https://arxiv.org/abs/2507.00305", "authors": ["Deland Liu", "Frigyes Samuel Racz", "Zoe Lalji", "Jose del R. Millan"], "title": "EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in\nstate (CLIS) can lose all reliable motor control and are left without any means\nof communication. It remains unknown whether non-invasive electroencephalogram\n(EEG) based brain-computer interfaces (BCIs) can support volitional\ncommunication in CLIS. Here, we show that a CLIS patient was able to operate an\nEEG-based BCI across multiple online sessions to respond to both general\nknowledge and personally relevant assistive questions. The patient delivered\n\"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at\ndifferent channels, guided by real-time auditory feedback from the BCI. The\npatient communicated assistive needs above chance in all sessions, achieving a\nperfect score in the final session. Performance on general knowledge questions\nvaried across sessions, with two sessions showing accurate and above-chance\nresponses, while the first and last sessions remained at chance level. The\npatient also showed consistent modulation patterns over time. These findings\nsuggest that non-invasive BCIs may offer a potential pathway for restoring\nbasic communication in CLIS."}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286", "abs": "https://arxiv.org/abs/2507.00286", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data."}
{"id": "2507.00824", "pdf": "https://arxiv.org/pdf/2507.00824", "abs": "https://arxiv.org/abs/2507.00824", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Michał Król", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivière"], "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "categories": ["cs.DC", "cs.NI", "cs.PF"], "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline."}
{"id": "2507.00304", "pdf": "https://arxiv.org/pdf/2507.00304", "abs": "https://arxiv.org/abs/2507.00304", "authors": ["Yujun Zhang", "Runlong Li", "Xiaoxiang Liang", "Xinhao Yang", "Tian Su", "Bo Liu", "Yan Zhou"], "title": "MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic", "categories": ["cs.LG", "cs.NI"], "comment": "16 pages", "summary": "The abnormal fluctuations in network traffic may indicate potential security\nthreats or system failures. Therefore, efficient network traffic prediction and\nanomaly detection methods are crucial for network security and traffic\nmanagement. This paper proposes a novel network traffic prediction and anomaly\ndetection model, MamNet, which integrates time-domain modeling and\nfrequency-domain feature extraction. The model first captures the long-term\ndependencies of network traffic through the Mamba module (time-domain\nmodeling), and then identifies periodic fluctuations in the traffic using\nFourier Transform (frequency-domain feature extraction). In the feature fusion\nlayer, multi-scale information is integrated to enhance the model's ability to\ndetect network traffic anomalies. Experiments conducted on the UNSW-NB15 and\nCAIDA datasets demonstrate that MamNet outperforms several recent mainstream\nmodels in terms of accuracy, recall, and F1-Score. Specifically, it achieves an\nimprovement of approximately 2% to 4% in detection performance for complex\ntraffic patterns and long-term trend detection. The results indicate that\nMamNet effectively captures anomalies in network traffic across different time\nscales and is suitable for anomaly detection tasks in network security and\ntraffic management. Future work could further optimize the model structure by\nincorporating external network event information, thereby improving the model's\nadaptability and stability in complex network environments."}
{"id": "2507.00803", "pdf": "https://arxiv.org/pdf/2507.00803", "abs": "https://arxiv.org/abs/2507.00803", "authors": ["Gillian Daniel", "Chris Hall", "Per Hammer", "Alec-Angus Macdonald", "Hollie Marwick-Best", "Emma McKenzie", "George Popa", "Derek Somerville", "Tim Storer"], "title": "Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses", "categories": ["cs.SE"], "comment": null, "summary": "Over more than two decades, The University of Glasgow has co-designed and\ndelivered numerous software engineering focused courses with industry partners,\ncovering both technical and discipline specific professional skills. Such\ncollaborations are not unique and many of the benefits are well recognised in\nthe literature. These include enhancing the real-world relevance of curricula,\ndeveloping student professional networks ahead of graduation and easing\nrecruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of\nindustry practitioners who participate in course design and delivery. This gap\nis significant, since the effort invested by practitioners is often substantial\nand may require ongoing support from both the industry partner and academic\ninstitution. Understanding the motivations, expectations and experiences of\npractitioners who engage in course delivery can guide the formation of future\npartnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective\nconducted amongst the practitioner coauthors of this paper, with the academic\ncoauthors acting as facilitators. All coauthors have participated in the recent\nco-design and delivery of software engineering courses, but we choose to focus\nexplicitly on the perspectives of the practitioners. We report on the themes\nthat emerged from the discussions and our resulting recommendations for future\ncollaborations."}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333", "abs": "https://arxiv.org/abs/2507.00333", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports."}
{"id": "2507.00288", "pdf": "https://arxiv.org/pdf/2507.00288", "abs": "https://arxiv.org/abs/2507.00288", "authors": ["Claire Li", "David Freeborn"], "title": "Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context", "categories": ["econ.TH", "cs.AI", "cs.ET"], "comment": "22 pages", "summary": "This study explores how AI-powered digital innovations are reshaping\norganisational accountability in a transnational governance context. As AI\nsystems increasingly mediate decision-making in domains such as auditing and\nfinancial reporting, traditional mechanisms of accountability, based on\ncontrol, transparency, and auditability, are being destabilised. We integrate\nthe Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and\ninstitutional theory to examine how organisations adopt AI technologies in\nresponse to regulatory, ethical, and cultural pressures that transcend national\nboundaries. We argue that accountability is co-constructed within global\nsocio-technical networks, shaped not only by user perceptions but also by\ngovernance logics and normative expectations. Extending TAM, we incorporate\ncompliance and legitimacy as key factors in perceived usefulness and usability.\nDrawing on ANT, we reconceptualise accountability as a relational and emergent\nproperty of networked assemblages. We propose two organisational strategies\nincluding internal governance reconfiguration and external actor-network\nengagement to foster responsible, legitimate, and globally accepted AI adoption\nin the accounting domain."}
{"id": "2507.00855", "pdf": "https://arxiv.org/pdf/2507.00855", "abs": "https://arxiv.org/abs/2507.00855", "authors": ["Marta Navarro", "Josué Feliu", "Salvador Petit", "María E. Gómez", "Julio Sahuquillo"], "title": "A New Family of Thread to Core Allocation Policies for an SMT ARM Processor", "categories": ["cs.DC", "cs.AR"], "comment": "13 pages", "summary": "Modern high-performance servers commonly integrate Simultaneous\nMultithreading (SMT) processors, which efficiently boosts throughput over\nsingle-threaded cores. Optimizing performance in SMT processors faces\nchallenges due to the inter-application interference within each SMT core. To\nmitigate the interference, thread-to-core (T2C) allocation policies play a\npivotal role. State-of-the-art T2C policies work in two steps: i) building a\nper-application performance stack using performance counters and ii) building\nperformance prediction models to identify the best pairs of applications to run\non each core.\n  This paper explores distinct ways to build the performance stack in ARM\nprocessors and introduces the Instructions and Stalls Cycles (ISC) stack, a\nnovel approach to overcome ARM PMU limitations. The ISC stacks are used as\ninputs for a performance prediction model to estimate the applications'\nperformance considering the inter-application interference. The accuracy of the\nprediction model (second step) depends on the accuracy of the performance stack\n(first step); thus, the higher the accuracy of the performance stack, the\nhigher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies.\nExperimental results show that $SYNPA4$, the best-performing SYNPA variant,\noutperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the\ngains achieved by the state-of-the-art policies for ARM processors.\nFurthermore, the multiple discussions and refinements presented throughout this\npaper can be applied to other SMT processors from distinct vendors and are\naimed at helping performance analysts build performance stacks for accurate\nperformance estimates in real processors."}
{"id": "2507.00428", "pdf": "https://arxiv.org/pdf/2507.00428", "abs": "https://arxiv.org/abs/2507.00428", "authors": ["Mohammad Firas Sada", "John J. Graham", "Mahidhar Tatineni", "Dmitry Mishin", "Thomas A. DeFanti", "Frank Würthwein"], "title": "Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor", "categories": ["cs.DC", "cs.NI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC25)", "summary": "As machine learning (ML) applications become integral to modern network\noperations, there is an increasing demand for network programmability that\nenables low-latency ML inference for tasks such as Quality of Service (QoS)\nprediction and anomaly detection in cybersecurity. ML models provide\nadaptability through dynamic weight adjustments, making Programming\nProtocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an\nideal platform for investigating In-Network Machine Learning (INML). These\ndevices offer high-throughput, low-latency packet processing and can be\ndynamically reconfigured via the control plane, allowing for flexible\nintegration of ML models directly at the network edge. This paper explores the\napplication of the P4 programming paradigm to neural networks and regression\nmodels, where weights and biases are stored in control plane table lookups.\nThis approach enables flexible programmability and efficient deployment of\nretrainable ML models at the network edge, independent of core infrastructure\nat the switch level."}
{"id": "2507.00014", "pdf": "https://arxiv.org/pdf/2507.00014", "abs": "https://arxiv.org/abs/2507.00014", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering."}
{"id": "2507.00513", "pdf": "https://arxiv.org/pdf/2507.00513", "abs": "https://arxiv.org/abs/2507.00513", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "ACM CSCW Poster 2025", "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system."}
{"id": "2507.00352", "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity."}
{"id": "2507.00909", "pdf": "https://arxiv.org/pdf/2507.00909", "abs": "https://arxiv.org/abs/2507.00909", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "comment": "10 pages, 6 figures, 1 table", "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development."}
{"id": "2507.00523", "pdf": "https://arxiv.org/pdf/2507.00523", "abs": "https://arxiv.org/abs/2507.00523", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "title": "Edge Computing and its Application in Robotics: A Survey", "categories": ["cs.RO", "cs.DC", "cs.NI"], "comment": null, "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics."}
{"id": "2507.00057", "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel Böhme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence."}
{"id": "2507.00596", "pdf": "https://arxiv.org/pdf/2507.00596", "abs": "https://arxiv.org/abs/2507.00596", "authors": ["Mayar Elfares", "Pascal Reisert", "Ralf Küsters", "Andreas Bulling"], "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy", "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations."}
{"id": "2507.00686", "pdf": "https://arxiv.org/pdf/2507.00686", "abs": "https://arxiv.org/abs/2507.00686", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "categories": ["cs.SE", "cs.ET"], "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement."}
{"id": "2507.00949", "pdf": "https://arxiv.org/pdf/2507.00949", "abs": "https://arxiv.org/abs/2507.00949", "authors": ["Yuqing Wang", "Charles Colley", "Brian Wheatman", "Jiya Su", "David F. Gleich", "Andrew A. Chien"], "title": "How Fast Can Graph Computations Go on Fine-grained Parallel Architectures", "categories": ["cs.DC", "cs.AR"], "comment": "13 pages, 11 figures, 6 tables", "summary": "Large-scale graph problems are of critical and growing importance and\nhistorically parallel architectures have provided little support. In the spirit\nof co-design, we explore the question, How fast can graph computing go on a\nfine-grained architecture? We explore the possibilities of an architecture\noptimized for fine-grained parallelism, natural programming, and the\nirregularity and skew found in real-world graphs. Using two graph benchmarks,\nPageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph\narchitecture, UpDown, to explore what performance codesign can achieve. To\ndemonstrate programmability, we wrote five variants of these algorithms.\nSimulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes\n(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS\non RMAT, exceeding the best prior results by 5x and 100x respectively."}
{"id": "2507.00824", "pdf": "https://arxiv.org/pdf/2507.00824", "abs": "https://arxiv.org/abs/2507.00824", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Michał Król", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivière"], "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "categories": ["cs.DC", "cs.NI", "cs.PF"], "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline."}
{"id": "2507.00264", "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility."}
{"id": "2507.00657", "pdf": "https://arxiv.org/pdf/2507.00657", "abs": "https://arxiv.org/abs/2507.00657", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": null, "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling."}
{"id": "2507.00264", "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility."}
{"id": "2507.00847", "pdf": "https://arxiv.org/pdf/2507.00847", "abs": "https://arxiv.org/abs/2507.00847", "authors": ["Keiichiro Kimura", "Hiroki Kuzuno", "Yoshiaki Shiraishi", "Masakatu Morii"], "title": "Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing", "categories": ["cs.CR", "cs.NI"], "comment": "13 pages, 6 figures. We plan to extend our evaluation to additional\n  device categories. Responsible disclosure completed", "summary": "Bluetooth is a pervasive wireless communication technology used by billions\nof devices for short-range connectivity. The security of Bluetooth relies on\nthe pairing process, where devices establish shared long-term keys for secure\ncommunications. However, many commercial Bluetooth devices implement automatic\npairing functions to improve user convenience, creating a previously unexplored\nattack surface.\n  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in\nthe automatic pairing functions in commercial Bluetooth devices to achieve\ncompletely silent device link key overwriting. The Stealtooth attack leverages\nthe fact that Bluetooth audio devices automatically transition to pairing mode\nunder specific conditions, enabling attackers to hijack pairing processes\nwithout user awareness or specialized tools. We also extend the attack into the\nMitM Stealtooth attack, combining automatic pairing abuse with power-saving\nmode techniques to enable man-in-the-middle attacks.\n  We evaluate the attacks against 10 commercial Bluetooth devices from major\nmanufacturers, demonstrating widespread vulnerabilities across diverse device\ntypes and manufacturers. Our practical implementation requires only commodity\nhardware and open-source software, highlighting the low barrier to entry for\nattackers.\n  We propose defenses both device and protocol levels, including enhanced user\nnotifications and standardized automatic pairing guidelines. Our findings\nreveal a critical tension between security and usability, showing that current\nautomatic pairing implementations create systematic vulnerabilities. We\nresponsibly disclosed our findings to affected vendors, with several already\nreleasing patches."}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%."}
{"id": "2507.00775", "pdf": "https://arxiv.org/pdf/2507.00775", "abs": "https://arxiv.org/abs/2507.00775", "authors": ["Haonan Yao", "Lingyun Yu", "Lijie Yao"], "title": "Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review", "categories": ["cs.HC"], "comment": null, "summary": "We present a systematic review on tasks, interactions, and visualization\nwidgets (refer to tangible entities that are used to accomplish data\nexploration tasks through specific interactions) in the context of tangible\ndata exploration. Tangible widgets have been shown to reduce cognitive load,\nenable more natural interactions, and support the completion of complex data\nexploration tasks. Yet, the field lacks a structured understanding of how task\ntypes, interaction methods, and widget designs are coordinated, limiting the\nability to identify recurring design patterns and opportunities for innovation.\nTo address this gap, we conduct a systematic review to analyze existing work\nand characterize the current design of data exploration tasks, interactions,\nand tangible visualization widgets. We next reflect based on our findings and\npropose a research agenda to inform the development of a future widget design\ntoolkit for tangible data exploration. Our systematic review and supplemental\nmaterials are available at physicalviswidget.github.io and osf.io/vjw5e."}
{"id": "2507.00394", "pdf": "https://arxiv.org/pdf/2507.00394", "abs": "https://arxiv.org/abs/2507.00394", "authors": ["Geng Zhang", "Shenggan Cheng", "Xuanlei Zhao", "Ziming Liu", "Yang You"], "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "As transformer sequence lengths grow, existing pipeline parallelisms incur\nsuboptimal performance due to the quadratic attention computation and the\nsubstantial memory overhead. To relieve these challenges, we propose HelixPipe,\na novel pipeline parallelism for long sequence transformer training. First,\nHelixPipe introduces attention parallel partition, which schedules attention\ncomputations of different micro batches across different pipeline stages in\nparallel, reducing pipeline bubbles. Second, it employs a two-fold\nfirst-in-last-out micro batch schedule to balance memory usage and overlap\ncommunication with computation. Additionally, HelixPipe utilizes recomputation\nwithout attention and chunked MLP to mitigate fragmentation and enable longer\nsequences. Experiments demonstrate that HelixPipe gains increasing advantages\nwith longer sequence lengths, and outperforms existing methods in throughput\nand scalability across varying pipeline sizes, model sizes, and cluster\nconfigurations. Notably, it achieves a 26\\% speedup over baseline methods when\ntraining a 7B model with 128k sequence length on 64 H20 GPUs. Code is available\nat https://github.com/code-tunnel/Megatron-LM/tree/dev."}
{"id": "2507.00595", "pdf": "https://arxiv.org/pdf/2507.00595", "abs": "https://arxiv.org/abs/2507.00595", "authors": ["Linard Arquint", "Samarth Kishor", "Jason R. Koenig", "Joey Dodds", "Daniel Kroening", "Peter Müller"], "title": "The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)", "categories": ["cs.CR", "cs.PL", "cs.SE"], "comment": null, "summary": "Existing program verifiers can prove advanced properties about security\nprotocol implementations, but are difficult to scale to large codebases because\nof the manual effort required. We develop a novel methodology called *Diodon*\nthat addresses this challenge by splitting the codebase into the protocol\nimplementation (the *Core*) and the remainder (the *Application*). This split\nallows us to apply powerful semi-automated verification techniques to the\nsecurity-critical Core, while fully-automatic static analyses scale the\nverification to the entire codebase by ensuring that the Application cannot\ninvalidate the security properties proved for the Core. The static analyses\nachieve that by proving *I/O independence*, i.e., that the I/O operations\nwithin the Application are independent of the Core's security-relevant data\n(such as keys), and that the Application meets the Core's requirements. We have\nproved Diodon sound by first showing that we can safely allow the Application\nto perform I/O independent of the security protocol, and second that manual\nverification and static analyses soundly compose. We evaluate Diodon on two\ncase studies: an implementation of the signed Diffie-Hellman key exchange and a\nlarge (100k+ LoC) production Go codebase implementing a key exchange protocol\nfor which we obtained secrecy and injective agreement guarantees by verifying a\nCore of about 1% of the code with the auto-active program verifier Gobra in\nless than three person months."}
{"id": "2507.00821", "pdf": "https://arxiv.org/pdf/2507.00821", "abs": "https://arxiv.org/abs/2507.00821", "authors": ["Mihnea Stefan Calota", "Wessel Nieuwenhuys", "Janet Yi-Ching Huang", "Lin-Lin Chen", "Mathias Funk"], "title": "Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures", "categories": ["cs.HC"], "comment": null, "summary": "Designers have ample opportunities to impact the healthcare domain. However,\nhospitals are often closed ecosystems that pose challenges in engaging clinical\nstakeholders, developing domain knowledge, and accessing relevant systems and\ndata. In this paper, we introduce a making-oriented approach to help designers\nunderstand the intricacies of their target healthcare context. Using Remote\nPatient Monitoring (RPM) as a case study, we explore how manually crafting\nsynthetic datasets based on real-world observations enables designers to learn\nabout complex data-driven healthcare systems. Our process involves observing\nand modeling the real-world RPM context, crafting synthetic datasets, and\niteratively prototyping a simplified RPM system that balances contextual\nrichness and intentional abstraction. Through this iterative process of\nsensemaking through making, designers can still develop context familiarity\nwhen direct access to the actual healthcare system is limited. Our approach\nemphasizes the value of hands-on interaction with data structures to support\ndesigners in understanding opaque healthcare systems."}
{"id": "2507.00423", "pdf": "https://arxiv.org/pdf/2507.00423", "abs": "https://arxiv.org/abs/2507.00423", "authors": ["Wenjin Mo", "Zhiyuan Li", "Minghong Fang", "Mingwei Fang"], "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "To appear in ICCV 2025", "summary": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree."}
{"id": "2507.00881", "pdf": "https://arxiv.org/pdf/2507.00881", "abs": "https://arxiv.org/abs/2507.00881", "authors": ["Linhao Meng", "Stef van den Elzen", "Anna Vilanova"], "title": "Towards Difficulty-Aware Analysis of Deep Neural Networks", "categories": ["cs.HC"], "comment": null, "summary": "Traditional instance-based model analysis focuses mainly on misclassified\ninstances. However, this approach overlooks the varying difficulty associated\nwith different instances. Ideally, a robust model should recognize and reflect\nthe challenges presented by intrinsically difficult instances. It is also\nvaluable to investigate whether the difficulty perceived by the model aligns\nwith that perceived by humans. To address this, we propose incorporating\ninstance difficulty into the deep neural network evaluation process,\nspecifically for supervised classification tasks on image data. Specifically,\nwe consider difficulty measures from three perspectives -- data, model, and\nhuman -- to facilitate comprehensive evaluation and comparison. Additionally,\nwe develop an interactive visual tool, DifficultyEyes, to support the\nidentification of instances of interest based on various difficulty patterns\nand to aid in analyzing potential data or model issues. Case studies\ndemonstrate the effectiveness of our approach."}
{"id": "2507.00523", "pdf": "https://arxiv.org/pdf/2507.00523", "abs": "https://arxiv.org/abs/2507.00523", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "title": "Edge Computing and its Application in Robotics: A Survey", "categories": ["cs.RO", "cs.DC", "cs.NI"], "comment": null, "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics."}
{"id": "2507.00963", "pdf": "https://arxiv.org/pdf/2507.00963", "abs": "https://arxiv.org/abs/2507.00963", "authors": ["Fan Wang", "Giulia Perugia", "Yuan Feng", "Wijnand IJsselsteijn"], "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception", "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": null, "summary": "As social robots increasingly enter dementia care, concerns about deception,\nintentional or not, are gaining attention. Yet, how robotic design cues might\nelicit misleading perceptions in people with dementia, and how these\nperceptions arise, remains insufficiently understood. In this scoping review,\nwe examined 26 empirical studies on interactions between people with dementia\nand physical social robots. We identify four key design cue categories that may\ninfluence deceptive impressions: cues resembling physiological signs (e.g.,\nsimulated breathing), social intentions (e.g., playful movement), familiar\nbeings (e.g., animal-like form and sound), and, to a lesser extent, cues that\nreveal artificiality. Thematic analysis of user responses reveals that people\nwith dementia often attribute biological, social, and mental capacities to\nrobots, dynamically shifting between awareness and illusion. These findings\nunderscore the fluctuating nature of ontological perception in dementia\ncontexts. Existing definitions of robotic deception often rest on philosophical\nor behaviorist premises, but rarely engage with the cognitive mechanisms\ninvolved. We propose an empirically grounded definition: robotic deception\noccurs when Type 1 (automatic, heuristic) processing dominates over Type 2\n(deliberative, analytic) reasoning, leading to misinterpretation of a robot's\nartificial nature. This dual-process perspective highlights the ethical\ncomplexity of social robots in dementia care and calls for design approaches\nthat are not only engaging, but also epistemically respectful."}
{"id": "2507.00672", "pdf": "https://arxiv.org/pdf/2507.00672", "abs": "https://arxiv.org/abs/2507.00672", "authors": ["Haoxiang Luo", "Yinqiu Liu", "Ruichen Zhang", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Zehui Xiong", "Xianbin Wang", "Xuemin Shen"], "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications."}
{"id": "2507.01017", "pdf": "https://arxiv.org/pdf/2507.01017", "abs": "https://arxiv.org/abs/2507.01017", "authors": ["Xingyu Xiao", "Hongxu Zhu", "Jingang Liang", "Jiejuan Tong", "Haitao Wang"], "title": "A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models", "categories": ["cs.HC"], "comment": null, "summary": "Human error remains a dominant risk driver in safety-critical sectors such as\nnuclear power, aviation, and healthcare, where seemingly minor mistakes can\ncascade into catastrophic outcomes. Although decades of research have produced\na rich repertoire of mitigation techniques, persistent limitations: scarce\nhigh-quality data, algorithmic opacity, and residual reliance on expert\njudgment, continue to constrain progress. This review synthesizes recent\nadvances at the intersection of risk-informed decision making, human\nreliability assessment (HRA), artificial intelligence (AI), and cognitive\nscience to clarify how their convergence can curb human-error risk. We first\ncategorize the principal forms of human error observed in complex\nsociotechnical environments and outline their quantitative impact on system\nreliability. Next, we examine risk-informed frameworks that embed HRA within\nprobabilistic and data-driven methodologies, highlighting successes and gaps.\nWe then survey cognitive and human-performance models, detailing how\nmechanistic accounts of perception, memory, and decision-making enrich error\nprediction and complement HRA metrics. Building on these foundations, we\ncritically assess AI-enabled techniques for real-time error detection,\noperator-state estimation, and AI-augmented HRA workflows. Across these\nstrands, a recurring insight emerges: integrating cognitive models with\nAI-based analytics inside risk-informed HRA pipelines markedly enhances\npredictive fidelity, yet doing so demands richer datasets, transparent\nalgorithms, and rigorous validation. Finally, we identify promising research\ndirections, coupling resilience engineering concepts with grounded theory,\noperationalizing the iceberg model of incident causation, and establishing\ncross-domain data consortia, to foster a multidisciplinary paradigm that\nelevates human reliability in high-stakes systems."}
{"id": "2507.00740", "pdf": "https://arxiv.org/pdf/2507.00740", "abs": "https://arxiv.org/abs/2507.00740", "authors": ["Craig S Wright"], "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "comment": "56 pages 5 images", "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients."}
{"id": "2507.00008", "pdf": "https://arxiv.org/pdf/2507.00008", "abs": "https://arxiv.org/abs/2507.00008", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning."}
{"id": "2507.00050", "pdf": "https://arxiv.org/pdf/2507.00050", "abs": "https://arxiv.org/abs/2507.00050", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets."}
{"id": "2507.00055", "pdf": "https://arxiv.org/pdf/2507.00055", "abs": "https://arxiv.org/abs/2507.00055", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks."}
{"id": "2507.00224", "pdf": "https://arxiv.org/pdf/2507.00224", "abs": "https://arxiv.org/abs/2507.00224", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts."}
{"id": "2507.00253", "pdf": "https://arxiv.org/pdf/2507.00253", "abs": "https://arxiv.org/abs/2507.00253", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet."}
{"id": "2507.00456", "pdf": "https://arxiv.org/pdf/2507.00456", "abs": "https://arxiv.org/abs/2507.00456", "authors": ["Deepak Varuvel Dennison", "Bakhtawar Ahtisham", "Kavyansh Chourasia", "Nirmit Arora", "Rahul Singh", "Rene F. Kizilcec", "Akshay Nambi", "Tanuja Ganu", "Aditya Vashistha"], "title": "Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "This study investigates Shiksha copilot, an AI-assisted lesson planning tool\ndeployed in government schools across Karnataka, India. The system combined\nLLMs and human expertise through a structured process in which English and\nKannada lesson plans were co-created by curators and AI; teachers then further\ncustomized these curated plans for their classrooms using their own expertise\nalongside AI support. Drawing on a large-scale mixed-methods study involving\n1,043 teachers and 23 curators, we examine how educators collaborate with AI to\ngenerate context-sensitive lesson plans, assess the quality of AI-generated\ncontent, and analyze shifts in teaching practices within multilingual,\nlow-resource environments. Our findings show that teachers used Shiksha copilot\nboth to meet administrative documentation needs and to support their teaching.\nThe tool eased bureaucratic workload, reduced lesson planning time, and lowered\nteaching-related stress, while promoting a shift toward activity-based\npedagogy. However, systemic challenges such as staffing shortages and\nadministrative demands constrained broader pedagogical change. We frame these\nfindings through the lenses of teacher-AI collaboration and communities of\npractice to examine the effective integration of AI tools in teaching. Finally,\nwe propose design directions for future teacher-centered EdTech, particularly\nin multilingual and Global South contexts."}
{"id": "2507.00481", "pdf": "https://arxiv.org/pdf/2507.00481", "abs": "https://arxiv.org/abs/2507.00481", "authors": ["Philipp M. Zähl", "Sabine Theis", "Martin R. Wolf"], "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research."}
{"id": "2507.00543", "pdf": "https://arxiv.org/pdf/2507.00543", "abs": "https://arxiv.org/abs/2507.00543", "authors": ["Leila Tavakoli", "Hamed Zamani"], "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications", "categories": ["cs.IR", "cs.HC"], "comment": "9 pages,5 figures", "summary": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings."}
{"id": "2507.00635", "pdf": "https://arxiv.org/pdf/2507.00635", "abs": "https://arxiv.org/abs/2507.00635", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted by ICRA 2025", "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation."}
{"id": "2507.00792", "pdf": "https://arxiv.org/pdf/2507.00792", "abs": "https://arxiv.org/abs/2507.00792", "authors": ["Hendric Voss", "Stefan Kopp"], "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK"}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875", "abs": "https://arxiv.org/abs/2507.00875", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face."}
