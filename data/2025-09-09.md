<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 7]
- [cs.PF](#cs.PF) [Total: 4]
- [cs.NI](#cs.NI) [Total: 17]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 17]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.SD](#cs.SD) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: 本文研究了基于大型语言模型（LLM）的自动程序修复（APR）系统在面对敌对错误报告时的安全风险，发现当前防御措施不足，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM-based APR系统在软件开发中的广泛应用，其对用户输入的依赖性引入了新的攻击面，尤其是敌对错误报告可能导致不安全的代码变更。

Method: 研究通过51种敌对错误报告策略（从手动生成到全自动流程）测试主流APR系统，评估预修复防御（如LlamaGuard、PromptGuard等）和后修复检测（如GitHub Copilot、CodeQL）的效果。

Result: 结果表明，90%的敌对报告成功触发攻击者期望的补丁，最佳预修复过滤仅拦截47%，后修复检测的有效性仅为58%。

Conclusion: 研究揭示了敌对报告生成的低成本与检测高成本之间的不对称性，提出了提升APR系统鲁棒性的实用建议，并指出了可信自动修复的未来研究方向。

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [2] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: 该论文研究了将用户界面设计自动转换为代码的问题，提出使用矢量图像作为输入，并引入一种新的多尺度评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的图像到代码的解决方案无法高度还原原始设计，因此作者希望通过使用矢量图像和改进评估方法来提高保真度。

Method: 作者利用矢量图像作为输入，创建了多个大型数据集，评估了现有的图像质量评估算法，并引入了一种新的多尺度指标。

Result: 论文训练了一个大型开放权重模型，并讨论了其局限性。

Conclusion: 通过使用矢量图像和多尺度评估指标，该方法在提高设计保真度方面展示了潜力，但仍存在一些限制。

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [3] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: RestTSLLM利用TSL和LLMs自动生成REST API测试用例，解决测试场景和输入数据定义的挑战。最佳模型Claude 3.5 Sonnet表现最优。


<details>
  <summary>Details</summary>
Motivation: REST API测试执行复杂且耗时，现有方法难以覆盖所有输入组合，导致测试覆盖率低。

Method: 结合TSL和LLMs，通过提示工程和自动化流程从OpenAPI规范生成测试。

Result: Claude 3.5 Sonnet在所有评估指标中表现最佳，能生成鲁棒且上下文一致的测试用例。

Conclusion: LLMs可有效自动化基于API规范的测试生成，Claude 3.5 Sonnet是此任务的最佳选择。

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [4] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: 论文提出了一种结合多种领域特定辅助策略的方法，用于改进软件跟踪链接恢复（TLR）任务中的NL-PL场景，基于HGT和Gemini 2.5 Pro模型，实验结果显示效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在NL-PL场景中，仅依赖文本相似性存在语义鸿沟问题，需要更有效的策略来提升TLR任务的性能。

Method: 通过实证分析，提出将多种辅助策略集成到HGT和Gemini 2.5 Pro模型中，并在需求到代码的TLR任务中验证其有效性。

Result: 多策略HGT和Gemini 2.5 Pro模型在12个开源项目中平均F1分数分别提高了3.68%和8.84%，显著优于现有方法HGNNLink。

Conclusion: 多策略集成是提升NL-PL场景TLR任务性能的有效方法，为未来研究提供了新方向。

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [5] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: 本文提出了一种基于验证的方法，通过将新旧版本的顺序功能图（SFC）转换为Petri网模型，并使用符号路径等价的新算法检查模型包含关系，以确保PLC软件升级后的功能正确性。实验证明了该方法的可扩展性和高效性。


<details>
  <summary>Details</summary>
Motivation: 由于工业需求不断变化，PLC软件升级频繁，而验证升级后的功能正确性是一个重要挑战。因此，需要一种有效的方法来确保升级不会影响现有功能。

Method: 将新旧版本的SFC转换为Petri网模型，利用基于符号路径等价的新算法检查模型之间的包含关系，并开发了一个Petri网包含检查工具。

Result: 在80个真实世界的基准测试中，该方法展现了良好的可扩展性和有效性。与现有工具verifAPS相比，性能提升了近4倍。

Conclusion: 提出的验证方法能够高效且可靠地确保PLC软件升级后的功能正确性，适用于工业应用。

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [6] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: 该论文提出了一种自动化方法，通过BERTopic分析Stack Overflow帖子并结合提取式摘要技术，生成简洁的API文档摘要，提升了开发者工作效率。


<details>
  <summary>Details</summary>
Motivation: 开发者依赖API文档，但官方文档通常冗长复杂或不完整，社区驱动的论坛如Stack Overflow提供了实用见解，因此需要将两者结合。

Method: 使用BERTopic从360万条Stack Overflow帖子中提取主题，并应用提取式摘要技术生成包含代码片段的简洁摘要。

Result: 30名Android开发者的用户研究表明，生成的摘要在连贯性、相关性、信息量和满意度上表现良好，提升了工作效率。

Conclusion: 结合正式API知识与社区生成内容，可以增强文档的可访问性和实用性。

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [7] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: IoT Miner是一个自动从工业传感器数据生成高级事件日志的框架，填补了传感器数据缺乏结构和语义的空白。


<details>
  <summary>Details</summary>
Motivation: 在采矿或制造等实际场景中，标准事件日志不可用，传感器数据缺乏分析所需的结构和语义，IoT Miner的目标是解决这一问题。

Method: 采用四阶段流程：数据预处理、无监督聚类、基于大语言模型（LLM）的标签生成和事件日志构建，关键创新是利用LLM生成有意义的活动标签。

Result: 使用LHD采矿机的传感器数据进行评估，结果表明更丰富的提示能生成更准确和一致的标签。

Conclusion: 结合AI和领域感知数据处理，IoT Miner提供了一种可扩展且可解释的方法，适用于传统日志缺失的场景。

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [8] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: GeoAnalystBench是一个用于评估大语言模型（LLMs）在GIS任务中表现的基准测试，揭示了专有模型（如ChatGPT-4o-mini）和开源模型（如DeepSeek-R1-7B）之间的性能差距，尤其是在空间推理任务中。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在GIS自动化中的实际能力，为GeoAI研究提供严谨的评估框架。

Method: 设计了GeoAnalystBench基准测试，包含50个Python任务，并通过多种指标（如CodeBLEU）进行评估。

Result: 专有模型表现优于开源模型，但在需要空间推理的任务中仍有挑战。

Conclusion: LLMs在GIS自动化中展现了潜力但仍有局限，需要结合人类参与的研究框架。

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [9] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: 论文提出了Code2MCP框架，自动化将GitHub仓库转换为MCP兼容服务，解决LLM生态中的集成碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）生态中存在N×M集成问题，现有软件转换为MCP服务需要大量手动工作，阻碍了创新和发展。

Method: Code2MCP采用多阶段工作流和LLM驱动的'Run--Review--Fix'循环，自动化完成代码分析、环境配置、服务生成和部署。

Result: 框架能高效生成可部署的MCP服务和技术文档，显著降低集成成本。

Conclusion: Code2MCP通过自动化解决了LLM生态中的集成难题，推动了MCP生态的发展。

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [10] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: GRACE通过构建多级多语义代码图和混合图检索器，解决了大语言模型在仓库级代码任务中的语义和结构依赖问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在仓库级代码任务中因上下文窗口有限和复杂的代码依赖关系表现不佳，现有方法过于依赖文本相似性且忽略了代码的结构信息。

Method: GRACE构建了统一静态和动态代码语义的多级多语义代码图，并设计了混合图检索器和结构融合机制来增强上下文理解。

Result: GRACE在公共仓库级基准测试中全面超越现有方法，使用DeepSeek-V3作为主干模型时，EM和ES指标分别提升8.19%和7.51%。

Conclusion: GRACE通过结合代码结构和语义的多层次表示，显著提升了仓库级代码任务的性能，为未来研究方向提供了新思路。

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [11] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLM）在需求工程（RE）教育中的应用对学生学习体验的影响，发现LLM有助于理解RE概念，但也带来学术诚信等问题。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在RE教育中的集成如何提升学生参与度和职业能力。

Method: 通过调查179名学生，比较LLM在不同教学形式（个人作业与团队项目）中的应用效果。

Result: LLM提升了RE概念的理解，但学生对学术诚信和AI过度依赖表示担忧。个人作业受益更多。

Conclusion: 需平衡AI辅助学习与批判性思维团队合作，未来研究应优化LLM集成方式。

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [12] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: 提出了一种名为BISection Sampling (BISS)的新方法，通过优化测试套件减少基准测试的计算成本，同时保持变体排名的稳定性。


<details>
  <summary>Details</summary>
Motivation: 基准测试在软件工程中非常耗时和资源密集，需要一种新方法来减少测试实例同时不影响排名结果。

Method: BISS方法通过战略性地保留关键测试并采用分治法高效采样剩余测试。

Result: 在多个数据集上的实验表明，BISS平均将计算成本降低至44%，部分情况下可减少高达99%。

Conclusion: BISS是一种高效且稳定的基准测试优化方法，能够显著降低计算成本。

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [13] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: 本文通过快速综述探讨了需求工程（RE）研究中法律要求（LRs）的概念，指出了现有文献中的定义模糊和概念操作性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 作者因个人困惑、同行评审意见及现有文献中的明显混淆，决定对法律要求在需求工程中的概念进行快速综述。

Method: 通过文献综述，分析了法律要求在需求工程中的定义、分类及其它特性。

Result: 文献表明，法律要求常被视为功能或非功能需求，具复杂性且易变，缺乏实证支持。

Conclusion: 研究揭示了关于法律要求的知识空白和概念混淆，呼吁进一步实证研究。

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [14] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: 论文探讨了利用代码大型语言模型（LLM）检测二进制安全补丁的潜力，并通过微调策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型安全补丁检测方法主要针对源码，无法应用于闭源软件，而代码LLM在二进制分析任务中表现出色，但其在二进制安全补丁检测中的潜力尚未探索。

Method: 构建了包含19,448个样本的二进制补丁数据集，包含汇编代码和伪代码两种表示形式，并评估了19种不同规模的代码LLM。通过微调策略向LLM注入二进制安全补丁检测的领域知识。

Result: 直接使用原始代码LLM难以准确识别二进制安全补丁，而经过微调的LLM在伪代码表示下表现最佳。

Conclusion: 研究填补了代码LLM在二进制安全补丁检测领域的空白，证明了微调策略的有效性，尤其是在伪代码表示中的应用前景广阔。

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [15] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: 论文研究了预训练模型（PTMs）作为新型软件依赖（Software Dependencies 2.0）在开源项目中的应用及管理方式。


<details>
  <summary>Details</summary>
Motivation: PTMs的广泛应用引入了新型软件依赖，但其维护性和可靠性尚不明确，可能威胁现代软件系统的稳定性。

Method: 采用混合方法分析401个GitHub仓库，定量分析PTM重用模式，定性研究开发者如何实际集成与管理这些模型。

Result: 研究发现PTMs在开源项目中的结构和文档方式、重用管道的阶段与组织模式，以及PTMs与其他学习组件的交互。

Conclusion: 研究揭示了PTMs作为软件依赖的实际集成和管理问题，为未来改进提供了基础。

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [16] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: 论文提出了一种新型的Agentic软件工程（SE 3.0），通过双模态（SE for Humans和SE for Agents）重新定义软件工程的基础架构，并设计了ACE和AEE两个工作台支持这一愿景，推动人-AI协作的范式升级。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理在复杂软件工程任务中的应用，现有软件工程范式无法满足其需求，需要重新思考其基础架构以确保可信度。

Method: 提出双模态架构（SE for Humans和SE for Agents）和支持工具（ACE和AEE），通过新的结构化工程活动（如MRPs和CRPs）实现人-AI协作。

Result: 构建了Structured Agentic Software Engineering（SASE）框架，并提出未来SE的研究路线图。

Conclusion: 论文提供了一个概念性框架和结构化词汇，旨在推动社区讨论，迈向可信、可扩展的Agentic软件工程未来。

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [17] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: 论文研究了软件工程师如何从失败中学习，发现学习过程非正式且缺乏结构化，导致重复失败。研究通过案例访谈揭示时间限制、知识流失等挑战，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 软件失效可能导致严重后果，但缺乏对工程师如何从失败中学习的深入理解。行业亟需改进失效管理实践。

Method: 通过10个深度访谈研究国家空间研究中心的研究软件工程师，并补充5个其他高可靠性组织的访谈数据以评估普适性。

Result: 研究发现失败学习多是非正式和临时性的，缺乏结构化流程，且时间限制、人员流动和文档分散等挑战阻碍系统性学习。

Conclusion: 研究结果加深了对软件工程师如何从失败中学习的理解，并为改进失效管理实践提供了指导。

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [18] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: PyMOP是一个高效、通用的Python运行时验证（RV）系统，支持多种逻辑和监控算法，能显著提升Python生态系统的RV能力。


<details>
  <summary>Details</summary>
Motivation: 目前Python的RV系统受限于特定领域或性能低下，PyMOP旨在提供通用、高效且可扩展的解决方案。

Method: PyMOP支持五种逻辑和现有监控算法，提供大量API规范，并支持三种插桩策略，用户可轻松扩展。

Result: PyMOP在290,133个单元测试中表现出色：监控算法速度显著优于Java默认算法；比最新动态分析系统快1,168.3倍；已帮助修复44个漏洞。

Conclusion: PyMOP的通用性和高效性使其成为Python RV领域的重要平台。

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [19] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: 研究发现，在代码修复任务中，大型语言模型（LLMs）的输出会随温度参数变化而产生显著不稳定性，高温下功能失败率更高。


<details>
  <summary>Details</summary>
Motivation: 探讨ChatGPT等LLM在代码修复任务中的不稳定性，尤其是不同温度设置对输出一致性的影响。

Method: 通过在不同温度值（0、0.5、1）下生成多个修复建议，使用语法相似性和输出等价率（OER）评估结构及功能一致性。

Result: 高温导致输出更不稳定且功能失败率高；低温下结构相似性较高。

Conclusion: 研究为LLM在软件开发中的一致性应用提供了方法学见解，同时对其可靠性提出了质疑。

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [20] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: 提出了一种名为“Design Multiverse”的概念，旨在通过整合设计状态的多重快照，帮助团队追踪和管理复杂系统中的设计决策和变体。


<details>
  <summary>Details</summary>
Motivation: 现实设计过程中，多团队或多利益相关者的并发操作会导致设计路径的分歧和演化，现有建模工具无法直接管理这种时空上的变异性。

Method: 引入Design Multiverse概念，通过模型联邦范式实现设计状态的快照管理和多版本整合。

Result: 支持设计决策追踪、系统变体分析及其相互依赖管理，适用于模型产品线和模型/元模型协同演化等场景。

Conclusion: Design Multiverse为复杂系统设计中的变体管理提供了新的解决方案，提升了设计过程的灵活性和可追溯性。

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [21] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: 介绍了一种名为Bmod的DSL，用于建模疏散场景，基于Eclipse EMF/GMF框架，并与其他建模工具进行了比较。


<details>
  <summary>Details</summary>
Motivation: 解决企业内部的依赖关系并提升业务管理流程，缩短新手用户的使用门槛。

Method: 使用Eclipse EMF和GMF构建Bmod语言，支持图形界面和分层结构。

Result: 展示了Bmod的功能，并与其他工具（如AToMPM、metaDepth、Sirius）在表达能力、学习曲线和性能方面的比较。

Conclusion: Bmod是一种有效的DSL工具，适用于建模疏散场景，且在多个方面优于其他工具。

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [22] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: 论文介绍了OpenCoderRank平台，旨在模拟技术评估，平衡出题者与答题者的需求。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）对技术评估完整性带来的挑战，同时为资源受限环境提供低成本、可定制的评估工具。

Method: 开发OpenCoderRank平台，支持出题者自托管评估任务，帮助答题者适应时间限制和陌生问题。

Result: 提供了一个易用平台，通过模拟技术评估增强了评估的公平性和准备效果。

Conclusion: OpenCoderRank通过平衡出题与答题需求，为技术评估提供了一个有效的解决方案。

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [23] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: HyGLAD是一种新型算法，通过构建可解释的模式来检测事件数据中的异常行为，优于现有深度学习方法且效率更高。


<details>
  <summary>Details</summary>
Motivation: 在静态系统中，检测事件数据的异常行为对识别恶意活动至关重要，现有深度学习方法缺乏可解释性。

Method: HyGLAD通过推断行为相似的实体等价类并构建其正则表达式，生成直接可解释的模式。

Result: 实验显示HyGLAD在5个真实系统数据集上平均优于7种无监督异常检测方法，训练和推理效率高一个数量级。

Conclusion: HyGLAD在性能和可解释性上均优于深度学习方法，适用于高效且透明的异常检测。

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [24] [Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504)
*Karl Aaron Rudkowski,Sallar Ahmadi-Pour,Rolf Drechsler*

Main category: cs.PL

TL;DR: 本文提出了两种虚拟原型（VP）验证方法CrosSym和SEFOS，分别通过修改SystemC内核和符号执行引擎实现对复杂外设的跨层次验证。


<details>
  <summary>Details</summary>
Motivation: 虚拟原型在复杂硬件设计中至关重要，但现有验证方法需要修改SystemC内核或无法实现跨层次验证。

Method: CrosSym修改SystemC内核，SEFOS修改符号执行引擎，两种方法均支持外设的符号执行验证。

Result: 相比现有方法（限于TLM），两种工具在运行时性能相当的同时支持跨层次验证，SEFOS保持内核和外设不变，CrosSym在运行时和内存占用上略优。

Conclusion: CrosSym和SEFOS为外设验证提供了灵活选择，SEFOS更适合保持原有设计完整性的场景，CrosSym则在性能上更具优势。

Abstract: Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.

</details>


### [25] [Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types](https://arxiv.org/abs/2509.05586)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 证明了在有限并发条件下，监控栈、队列和AADT的线性化可验证性是固定参数可处理的。


<details>
  <summary>Details</summary>
Motivation: 并发数据结构的线性化验证通常是NP难的，即使对于简单类型。本文旨在提供高效算法，解决这一问题。

Method: 利用前沿图和分区状态限制搜索空间。对于AADT，利用线性化等价性实现对数线性时间监控；对于栈，引入基于文法的方法并降为矩阵乘法；对于队列，采用分裂序列转移系统支持高效动态规划。

Result: 为栈、队列和AADT在有限并发条件下提供了统一的可处理性保证。

Conclusion: 本文方法为并发数据结构的线性化验证提供了高效算法，适用于有限并发场景。

Abstract: Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.

</details>


### [26] [Pacing Types: Safe Monitoring of Asynchronous Streams](https://arxiv.org/abs/2509.06724)
*Florian Kohn,Arthur Correnson,Jan Baumeister,Bernd Finkbeiner*

Main category: cs.PL

TL;DR: 本文介绍了RTLola框架中的一种新型类型系统——pacing types，用于确保异步数据流的监控器在运行时行为良好。


<details>
  <summary>Details</summary>
Motivation: 在复杂的网络物理系统（如无人机）中，基于流的监控器是实时安全保障的关键组件。但由于数据流的异步性，监控器的设计和实现容易引入运行时错误。

Method: 作者提出了pacing types类型系统，并在RTLola的核心片段中形式化其核心概念，同时通过新的逻辑关系证明了该系统的可靠性。

Result: pacing types可以有效防止因异步数据流导致的运行时错误，提升监控器的可靠性。

Conclusion: pacing types为设计可靠的异步流监控器提供了理论支持，并通过RTLola框架实现了实际应用。

Abstract: Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.

</details>


### [27] [Termination Analysis of Linear-Constraint Programs](https://arxiv.org/abs/2509.06752)
*Amir M. Ben-Amram,Samir Genaim,Joël Ouaknine,James Worrell*

Main category: cs.PL

TL;DR: 该综述总结了针对数值变量和线性约束转换程序的终止分析技术，探讨了不可判定问题的挑战及解决方法，未涉及实际编程语言或更复杂的抽象模型。


<details>
  <summary>Details</summary>
Motivation: 由于终止分析中存在不可判定问题，需系统性探索缓解方法。

Method: 综述了基础可判定性结果、排名函数、分离良基转换不变量及非终止见证等技术。

Result: 分析了不同方法在表达能力和计算复杂性之间的权衡。

Conclusion: 该综述聚焦于线性约束程序终止分析的核心技术，未扩展至非线性或实际语言场景。

Abstract: This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.

</details>


### [28] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: Dato是一种基于任务的编程模型，通过提升数据通信和分片为一类类型构造，显著降低了编写优化代码的负担，并在AMD和FPGA设备上实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据流加速器虽有片上流处理能力，但现有编程模型难以有效利用这些功能，低层次接口开发效率低，高层次语言则限制了优化空间。

Method: Dato采用Python嵌入式任务编程模型，开发者通过显式流类型连接任务，并通过布局类型指定分片输入，任务虚拟映射到加速器空间结构后由编译器生成物理映射。

Result: 在AMD NPU上，Dato实现84%的硬件利用率，注意力核速度提升2.81倍；在FPGA上，性能超越主流框架，达到理论峰值性能的98%。

Conclusion: Dato在高效利用数据流加速器能力的同时，显著提升了开发效率和性能。

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


### [29] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: 论文提出了一种新型的多重宇宙调试方法MIO，能够处理广泛的输入/输出操作，避免探索无法访问的程序状态。


<details>
  <summary>Details</summary>
Motivation: 现有的多重宇宙调试器在处理输入/输出操作时会暴露无法访问的程序状态，严重影响调试效率。

Method: 提出了一种支持多样化输入/输出操作的多重宇宙调试方法，并通过WARDuino WebAssembly虚拟机实现了原型MIO。

Result: MIO能够确保仅探索常规执行中可到达的程序状态，提升了调试效率和准确性。

Conclusion: MIO为微控制器上的非确定性程序调试提供了一种高效且可靠的方法。

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [30] [Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs](https://arxiv.org/abs/2509.06872)
*Zachary Kent,Ugur Y. Yavuz,Siddhartha Jayanti,Stephanie Balzer,Guy Blelloch*

Main category: cs.PL

TL;DR: 该论文总结了并发数据结构线性化验证的最新进展，并提出了一种基于前向推理的完整验证方法，通过Rocq实现了理论和实践的验证。


<details>
  <summary>Details</summary>
Motivation: 为了减少可信计算基础的规模，作者希望形式化并发数据结构的前向推理技术，并提供完整且严格的验证方法。

Method: 作者在Rocq中形式化并发数据结构和前向推理技术，并通过案例研究验证了一个简单并发寄存器的线性化性。

Result: 论文展示了如何在Rocq中实现前向推理技术的理论验证，并通过案例研究证明了方法的有效性。

Conclusion: 该研究为并发数据结构的线性化性验证提供了一个完整的理论框架和实践工具，推动了相关领域的发展。

Abstract: In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [31] [Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs](https://arxiv.org/abs/2509.05365)
*Dingcui Yu,Yunpeng Song,Yiyang Huang,Yumiao Zhao,Yina Lv,Chundong Wang,Youtao Zhang,Liang Shi*

Main category: cs.PF

TL;DR: 论文分析了主机端和设备端数据压缩对SSD温度和性能的影响，提出了一种温度感知的协同压缩方法Waltz，优化性能和空间利用率。


<details>
  <summary>Details</summary>
Motivation: 解决SSD在数据压缩中因设备端压缩导致的高温和主机端压缩性能下降的问题。

Method: 提出Waltz方法，通过监测设备温度调度主机和设备端的（解）压缩任务，并优化了空间和性能的两种变体Waltzs和Waltzp。

Result: Waltz在F2FS中实现，提升了性能，延长SSD寿命并防止过热导致的关机。

Conclusion: Waltz通过协同调度和温度监测，平衡了SSD压缩的性能和温度问题。

Abstract: Data compression is widely adopted for modern solid-state drives (SSDs) to
mitigate both storage capacity and SSD lifetime issues. Researchers have
proposed compression schemes at different system layers, including device-side
solutions like CCSDs ( c ompression-based c omputational SSDs) and compression
supported by host-side, like F2FS (flash-friendly file system). We conduct
quantitative studies to understand how host-side and device-side compression
schemes affect the temperature and performance of SSD-based storage systems.
From our experiments, device-side compression, facilitated by a hardware
compression engine, can raise the temperature of CCSDs to intolerable levels,
resulting in throttling and service shutdown. In contrast, host-side
compression causes software-stack overhead, which often results in large
performance degradation and resource consumption. To ensure efficient data
compression with high performance and better temperature control, we propose
Waltz, a temperature-aware cooperative compression method that schedules
(de)compression tasks at the host and device sides by monitoring device
temperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for
space and performance optimization, respectively. Waltz is implemented within
F2FS, achieving high performance while extending SSD lifetime and preventing
overheating-induced in-flight shutdowns.

</details>


### [32] [Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology](https://arxiv.org/abs/2509.05511)
*Dhanya R Mathews,Mudit Verma,Pooja Aggarwal,J. Lakshmi*

Main category: cs.PF

TL;DR: 该论文提出了一种名为TA-RCD的新方法，通过结合云服务端到端拓扑结构来提升异常根因检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于微服务架构的广泛采用，云应用的故障管理变得更加复杂，需要自主且高效的策略来提升服务弹性。

Method: 提出了一种新颖的方法，利用应用服务拓扑结构从云堆栈中选择最具信息性的指标，并结合TA-RCD算法改进根因检测。

Result: 实验表明，TA-RCD在Top-3和Top-5召回率上平均比现有RCD算法提升至少2倍。

Conclusion: 该方法通过拓扑感知显著提升了根因检测的性能，对云服务弹性的提升具有重要意义。

Abstract: Cloud application services are distributed in nature and have components
across the stack working together to deliver the experience to end users. The
wide adoption of microservice architecture exacerbates failure management due
to increased service components. To be effective, the strategies to enhance the
application service resilience need to be autonomous and developed at the
service's granularity, considering its end-to-end components. However, the
massive amount of observability data generated by all these components across
the service stack poses a significant challenge in reacting to anomalies and
restoring the service quality in real time. Identifying the most informative
observability data from across the cloud service stack and timely localization
of root causes of anomalies thus becomes crucial to ensure service resilience.
This article presents a novel approach that considers the application service
topology to select the most informative metrics across the cloud stack to
support efficient, explainable, and accurate root cause identifications in case
of performance anomalies. The usefulness of the selected metrics is then
evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for
localizing the root cause of performance anomalies. As a step towards improving
the accuracy and efficiency of RCD, this article then proposes the
Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application
service topology in RCD. The evaluation of the failure injection studies shows
that the proposed approach performs at least 2X times better on average than
the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.

</details>


### [33] [Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach](https://arxiv.org/abs/2509.05790)
*Hai Dinh-Tuan,Franz Florian Six*

Main category: cs.PF

TL;DR: 论文提出了一种基于服务亲和力图的微服务优化方法SAGA，通过图模型和近似算法解决服务放置问题，实验验证平均延迟降低了23.40%。


<details>
  <summary>Details</summary>
Motivation: 现代云原生微服务架构虽高效，但在动态分布式环境下难以保障端到端服务质量，需要一种新方法优化服务交互。

Method: 采用图模型建模微服务交互，将服务放置问题转化为最小权重k割问题，并使用近似算法进行服务聚类。

Result: 实验结果表明，SAGA框架在Kubernetes集群上实现了23.40%的平均延迟改善。

Conclusion: SAGA框架在性能提升、数据隐私和成本优化方面表现出色，但也面临复杂性和实施挑战，需进一步探讨。

Abstract: Modern software architectures are characterized by their cloud-native,
modular, and microservice-based designs. While these systems are known for
their efficiency, they also face complex challenges in service optimization,
especially in maintaining end-to-end quality of service across dynamically
distributed services. This paper introduces a novel approach using the concept
of Service Affinity to address this challenge. The proposed method, termed
Service Affinity Graph-based Approach, employs a graph-based model to model the
interactions among microservices. It formulates the service placement as a
minimum-weight k-cut problem and utilizes an approximation algorithm for
service clustering. This approach is realized through a conceptual framework
that takes into account a wide range of optimization objectives, ranging from
enhancing application performance and enforcing data privacy to optimizing
operational costs. In addition to presenting the SAGA framework in details,
this paper conducts an in-depth empirical evaluation using a prototype deployed
on a Kubernetes cluster. The results demonstrate a mean latency improvement of
23.40%, validating the effectiveness of our approach. Finally, the paper
comprehensively discusses various aspects of the proposed methods, including
their implications, challenges, and benefits, providing a thorough analysis of
the approach's impact.

</details>


### [34] [Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing](https://arxiv.org/abs/2509.05794)
*Hai Dinh-Tuan,Jialun Jiang*

Main category: cs.PF

TL;DR: 论文提出了一种优化的Kubernetes状态服务迁移方案，结合MS2M框架和FCC特性，显著降低了停机时间。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统广泛采用微服务架构，但状态服务的迁移因需要保留内存状态而复杂，现有容器编排器缺乏原生支持。

Method: 整合MS2M框架与Kubernetes的FCC特性，支持StatefulSet管理的Pod迁移，并引入阈值截止机制处理高消息率。

Result: MS2M方案将单Pod停机时间降低96.986%，StatefulSet方法提升了状态服务的灵活性。

Conclusion: 方案为云原生环境中状态服务迁移提供了优化策略。

Abstract: The widespread adoption of microservices architecture in modern software
systems has emphasized the need for efficient management of distributed
services. While stateless microservices enable straightforward migration,
stateful microservices introduce added complexity due to the need to preserve
in-memory state during migration. However, most container orchestrators,
including Kubernetes, lack native support for live stateful service migration.
This paper proposes an optimized migration scheme for stateful services in
Kubernetes by integrating the Message-based Stateful Microservice Migration
(MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC)
feature. Key enhancements include support for migrating StatefulSet-managed
Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high
incoming message rates. Evaluation results demonstrate that MS2M for individual
Pods reduces downtime by 96.986% compared to cold migration methods, while the
StatefulSet approach provides greater flexibility in managing stateful
services. These insights provide practical strategies for optimizing stateful
microservice migration in cloud-native environments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [35] [Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)](https://arxiv.org/abs/2509.05447)
*Zhongyuan Zhao,Gunjan Verma,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: 提出了一种基于图神经网络的分布式链路稀疏化方案，以减少密集无线网络中调度开销。


<details>
  <summary>Details</summary>
Motivation: 在密集无线网络中，分布式链路调度算法的高信令开销会导致拥塞、能耗和无线电占用增加。

Method: 通过图神经网络调整链路竞争阈值，利用离线无监督学习平衡调度开销最小化和总效用达标。

Result: 在500链路的模拟网络中，该技术有效减轻了拥塞，减少了无线电占用。

Conclusion: 方法成功降低了调度开销，同时保持了网络容量，适用于多种调度协议。

Abstract: In wireless networks characterized by dense connectivity, the significant
signaling overhead generated by distributed link scheduling algorithms can
exacerbate issues like congestion, energy consumption, and radio footprint
expansion. To mitigate these challenges, we propose a distributed link
sparsification scheme employing graph neural networks (GNNs) to reduce
scheduling overhead for delay-tolerant traffic while maintaining network
capacity. A GNN module is trained to adjust contention thresholds for
individual links based on traffic statistics and network topology, enabling
links to withdraw from scheduling contention when they are unlikely to succeed.
Our approach is facilitated by a novel offline constrained {unsupervised}
learning algorithm capable of balancing two competing objectives: minimizing
scheduling overhead while ensuring that total utility meets the required level.
In simulated wireless multi-hop networks with up to 500 links, our link
sparsification technique effectively alleviates network congestion and reduces
radio footprints across four distinct distributed link scheduling protocols.

</details>


### [36] [Joint Routing, Resource Allocation, and Energy Optimization for Integrated Access and Backhaul with Open RAN](https://arxiv.org/abs/2509.05467)
*Reshma Prasad,Maxime Elkael,Gabriele Gemmi,Osama M. Bushnaq,Debashisha Mishra,Prasanna Raut,Jennifer Simonjan,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 该论文研究了6G网络中集成接入与回传（IAB）网络的联合优化问题，提出了节能和吞吐量最大化的两种目标，并通过大规模方法解决了路由和资源分配的优化问题。


<details>
  <summary>Details</summary>
Motivation: 随着网络向6G发展，移动网络运营商需要平衡多样化的需求和不断增长的能耗问题，尤其是在多跳无线回传的IAB网络中。

Method: 论文提出了一个新颖的容量模型，将功率水平与可实现的数据速率联系起来，并开发了两种大规模优化方法，利用O-RAN架构的闭环控制框架进行集成。

Result: 实验结果表明，提出的方法有效减少了激活节点数量以节能，并在高峰时段实现了每用户设备约100 Mbps的最低数据速率。

Conclusion: 论文的研究成果验证了其框架在下一代IAB网络部署和优化中的实际应用价值。

Abstract: As networks evolve towards 6G, Mobile Network Operators (MNOs) must
accommodate diverse requirements and at the same time manage rising energy
consumption. Integrated Access and Backhaul (IAB) networks facilitate dense
cellular deployments with reduced infrastructure complexity. However, the
multi-hop wireless backhauling in IAB networks necessitates proper routing and
resource allocation decisions to meet the performance requirements. At the same
time, cell densification makes energy optimization crucial. This paper
addresses the joint optimization of routing and resource allocation in IAB
networks through two distinct objectives: energy minimization and throughput
maximization. We develop a novel capacity model that links power levels to
achievable data rates. We propose two practical large-scale approaches to solve
the optimization problems and leverage the closed-loop control framework
introduced by the Open Radio Access Network (O-RAN) architecture to integrate
the solutions. The approaches are evaluated on diverse scenarios built upon
open data of two months of traffic collected by network operators in the city
of Milan, Italy. Results show that the proposed approaches effectively reduces
number of activated nodes to save energy and achieves approximately 100 Mbps of
minimum data rate per User Equipment (UE) during peak hours of the day using
spectrum within the Frequency Range (FR) 3, or upper midband. The results
validate the practical applicability of our framework for next-generation IAB
network deployment and optimization.

</details>


### [37] [Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]](https://arxiv.org/abs/2509.05759)
*Jinkun Geng,Shuai Mu,Anirudh Sivaraman,Balaji Prabhakar*

Main category: cs.NI

TL;DR: Tiga是一种新型的地理复制和可扩展事务数据库设计，能够在单次广域往返时间（1 WRTT）内提交事务，同时保持高吞吐量和低计算开销。


<details>
  <summary>Details</summary>
Motivation: Tiga旨在为广泛的应用场景提供高性能的事务处理，目标是减少延迟并提高吞吐量。

Method: Tiga通过合并并发控制和共识机制，在单轮中完成严格序列化执行和一致性复制，并使用同步时钟主动为事务分配未来时间戳以实现排序。

Result: Tiga在大多数情况下能在1 WRTT内提交事务，性能优于现有解决方案，吞吐量提高了1.3--7.2倍，延迟降低了1.4--4.6倍。

Conclusion: Tiga是一种高效的地理复制事务数据库设计，适用于需要低延迟和高吞吐量的场景。

Abstract: This paper presents Tiga, a new design for geo-replicated and scalable
transactional databases such as Google Spanner. Tiga aims to commit
transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of
scenarios, while maintaining high throughput with minimal computational
overhead. Tiga consolidates concurrency control and consensus, completing both
strictly serializable execution and consistent replication in a single round.
It uses synchronized clocks to proactively order transactions by assigning each
a future timestamp at submission. In most cases, transactions arrive at servers
before their future timestamps and are serialized according to the designated
timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed
and proactive ordering fails, in which case Tiga falls back to a slow path,
committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can
commit more transactions at 1-WRTT latency, and incurs much less throughput
overhead. Evaluation results show that Tiga outperforms all baselines,
achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower
latency. Tiga is open-sourced at
https://github.com/New-Consensus-Concurrency-Control/Tiga.

</details>


### [38] [On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks with Reduced Latency and Task Loss](https://arxiv.org/abs/2509.05889)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 本文提出了一种在线动态成本驱动算法（On-Dyn-CDA），用于车载网络中的实时任务处理，显著降低了任务丢弃率和延迟。


<details>
  <summary>Details</summary>
Motivation: 车载网络中实时任务处理的低延迟和高完成率是关键挑战，需优化算法以解决这一问题。

Method: 研究静态和动态优化算法，区分在线和离线情况，并提出On-Dyn-CDA算法。

Result: On-Dyn-CDA在复杂场景中仅需0.05秒完成，任务丢弃率降低3.42%，平均延迟减少29.22%。

Conclusion: On-Dyn-CDA无需训练且复杂度低，适用于动态环境。

Abstract: Real-time task processing is a critical challenge in vehicular networks,
where achieving low latency and minimizing dropped task ratio depend on
efficient task execution. Our primary objective is to maximize the number of
completed tasks while minimizing overall latency, with a particular focus on
reducing number of dropped tasks. To this end, we investigate both static and
dynamic versions of an optimization algorithm. The static version assumes full
task availability, while the dynamic version manages tasks as they arrive. We
also distinguish between online and offline cases: the online version
incorporates execution time into the offloading decision process, whereas the
offline version excludes it, serving as a theoretical benchmark for optimal
performance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm
(On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm
Optimization (PSO) baseline assumes all tasks are transferred to the RSU and
processed by the MEC, and its offline version disregards execution time, making
it infeasible for real-time applications despite its optimal performance in
theory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the
most complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It
also outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22%
reduction in average latency in complex scenarios. Furthermore, it requires
neither a dataset nor a training phase, and its low computational complexity
ensures efficiency and scalability in dynamic environments.

</details>


### [39] [ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection](https://arxiv.org/abs/2509.05936)
*Xuanhao Luo,Shivesh Madan Nath Jha,Akruti Sinha,Zhizhen Li,Yuchen Liu*

Main category: cs.NI

TL;DR: 提出了ALPHA，一种无需人工的主动学习日志分析管道，结合语义嵌入、聚类采样和LLM辅助标注，实现高效自动化异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析方法依赖专家知识或监督学习，需要大量标注数据和人工努力，ALPHA旨在解决这些问题。

Method: ALPHA整合语义嵌入、基于聚类的代表性采样和LLM辅助少样本标注，并通过两步细化策略提升准确性。

Result: 实验表明ALPHA的检测精度接近全监督方法，同时显著减少人工干预，并提供可解释的根因分析。

Conclusion: ALPHA是一种可扩展、低成本的日志异常检测自动化解决方案。

Abstract: Network log data analysis plays a critical role in detecting security threats
and operational anomalies. Traditional log analysis methods for anomaly
detection and root cause analysis rely heavily on expert knowledge or fully
supervised learning models, both of which require extensive labeled data and
significant human effort. To address these challenges, we propose ALPHA, the
first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates
semantic embedding, clustering-based representative sampling, and large
language model (LLM)-assisted few-shot annotation to automate the anomaly
detection process. The LLM annotated labels are propagated across clusters,
enabling large-scale training of an anomaly detector with minimal supervision.
To enhance the annotation accuracy, we propose a two-step few-shot refinement
strategy that adaptively selects informative prompts based on the LLM's
observed error patterns. Extensive experiments on real-world log datasets
demonstrate that ALPHA achieves detection accuracy comparable to fully
supervised methods while mitigating human efforts in the loop. ALPHA also
supports interpretable analysis through LLM-driven root cause explanations in
the post-detection stage. These capabilities make ALPHA a scalable and
cost-efficient solution for truly automated log-based anomaly detection.

</details>


### [40] [Understanding BBRv3 Performance in AQM-Enabled WiFi Networks](https://arxiv.org/abs/2509.06245)
*Shyam Kumar Shrestha,Jonathan Kua,Shiva Raj Pokhrel*

Main category: cs.NI

TL;DR: 介绍了一个模块化实验平台和轻量级可视化工具，用于评估无线网络中TCP拥塞控制性能，比较了BBRv3和CUBIC在不同AQM方案下的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线网络中TCP拥塞控制的性能评估问题，尤其是在AQM方案下的表现差异。

Method: 使用模块化实验平台和可视化工具，在Wi-Fi链路上比较BBRv3和CUBIC在不同AQM方案（PFIFO、FQ-CoDel、CAKE）下的性能。

Result: BBRv3在AQM条件下显著提高了公平性和收敛性，尤其是在FQ-CoDel方案下。

Conclusion: 该可视化工具和测试平台为未来TCP变体在现实无线网络中的评估提供了实用基础。

Abstract: We present a modular experimental testbed and lightweight visualization tool
for evaluating TCP congestion control performance in wireless networks. We
compare Google's latest Bottleneck Bandwidth and Round-trip time version 3
(BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management
(AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a
commercial MikroTik router. Our real-time dashboard visualizes metrics such as
throughput, latency, and fairness across competing flows. Results show that
BBRv3 significantly improves fairness and convergence under AQM, especially
with FQ-CoDel. Our visualization tool and modular testbed provide a practical
foundation for evaluating next-generation TCP variants in real-world
AQM-enabled home wireless networks.

</details>


### [41] [An Axiomatic Analysis of Path Selection Strategies for Multipath Transport in Path-Aware Networks](https://arxiv.org/abs/2509.05938)
*Alissa Baumeister,Sina Keshvadi*

Main category: cs.NI

TL;DR: 该论文通过严格的公理分析量化了路径感知网络中个体性能优化与网络稳定性之间的权衡，发现混合策略（如Epsilon-Greedy）能在高竞争场景下兼顾效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究路径感知网络（如SCION）与多路径传输协议（如MPTCP/MPQUIC）结合时的性能与稳定性权衡问题。

Method: 通过模拟环境测试贪婪（Min-RTT）、协作（Round-Robin）及混合（Epsilon-Greedy）算法，结合效率、损失避免、稳定性和公平性公理进行分析。

Result: 贪婪策略在低竞争下高效但会导致严重网络不稳定；协作策略稳定但牺牲高性能路径；混合策略（如Epsilon-Greedy）能平衡两者。

Conclusion: 可调节的混合算法是设计下一代网络路径选择机制的关键，能在高效和稳定性之间取得最佳平衡。

Abstract: Path-aware networking architectures like SCION provide end-hosts with
explicit control over inter-domain routing, while multipath transport protocols
like MPTCP and MPQUIC enable the concurrent use of multiple paths. This
combination promises significant gains in performance and policy enforcement,
but it also creates a stark trade-off between individual performance
optimization and overall network stability. This paper quantifies this
trade-off through a rigorous axiomatic analysis. We evaluate a spectrum of
algorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid
approaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance,
Stability, and Fairness in a simulated path-aware environment.
  Our simulations reveal that purely greedy strategies, while efficient under
low contention, induce catastrophic packet loss, increasing by over >18,000% as
the number of competing agents grow, due to herd effects that cause severe
network instability. Conversely, cooperative strategies ensure fairness and
stability but at the cost of underutilizing high-capacity paths. Crucially, we
demonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy
algorithm, for instance, achieves the highest efficiency of all tested
strategies in high-contention scenarios while mitigating the instability
inherent to the greedy approach. Our axiomatic analysis suggests that tunable,
hybrid algorithms are essential for designing robust and high-performance path
selection mechanisms for next-generation networks.

</details>


### [42] [Large Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial](https://arxiv.org/abs/2509.05946)
*Bisheng Wei,Ruihong Jiang,Ruichen Zhang,Yinqiu Liu,Dusit Niyato,Yaohua Sun,Yang Lu,Yonghui Li,Shiwen Mao,Chau Yuen,Marco Di Renzo,Mugen Peng*

Main category: cs.NI

TL;DR: 本文综述了基于大语言模型（LLM）的优化框架在6G无线网络中的应用，探讨了LLM在问题建模、求解和验证中的潜力，并分析了实际案例与研究挑战。


<details>
  <summary>Details</summary>
Motivation: 6G无线网络的快速发展使得资源分配和轨迹设计等优化问题变得极其复杂，传统优化方法难以满足实时性和动态性需求，LLM因其自然语言处理和推理能力成为潜在的解决方案。

Method: 介绍了LLM优化框架的设计概念，分析了自然语言建模、求解器协作和解决方案验证等关键方法，并通过案例研究展示了实际应用潜力。

Result: 研究表明，LLM在无线网络优化中具有显著潜力，尤其在问题的自然语言建模和自适应求解方面表现出色。

Conclusion: LLM为6G无线网络优化提供了新方向，但仍需解决研究挑战，未来需开发更鲁棒、可扩展的解决方案。

Abstract: The rapid advancement toward sixth-generation (6G) wireless networks has
significantly intensified the complexity and scale of optimization problems,
including resource allocation and trajectory design, often formulated as
combinatorial problems in large discrete decision spaces. However, traditional
optimization methods, such as heuristics and deep reinforcement learning (DRL),
struggle to meet the demanding requirements of real-time adaptability,
scalability, and dynamic handling of user intents in increasingly heterogeneous
and resource-constrained network environments. Large language models (LLMs)
present a transformative paradigm by enabling natural language-driven problem
formulation, context-aware reasoning, and adaptive solution refinement through
advanced semantic understanding and structured reasoning capabilities. This
paper provides a systematic and comprehensive survey of LLM-enabled
optimization frameworks tailored for wireless networks. We first introduce
foundational design concepts and distinguish LLM-enabled methods from
conventional optimization paradigms. Subsequently, we critically analyze key
enabling methodologies, including natural language modeling, solver
collaboration, and solution verification processes. Moreover, we explore
representative case studies to demonstrate LLMs' transformative potential in
practical scenarios such as optimization formulation, low-altitude economy
networking, and intent networking. Finally, we discuss current research
challenges, examine prominent open-source frameworks and datasets, and identify
promising future directions to facilitate robust, scalable, and trustworthy
LLM-enabled optimization solutions for next-generation wireless networks.

</details>


### [43] [Optimized Split Computing Framework for Edge and Core Devices](https://arxiv.org/abs/2509.06049)
*Andrea Tassi,Oluwatayo Yetunde Kolawole,Joan Pujol Roig,Daniel Warren*

Main category: cs.NI

TL;DR: 该研究提出了一种针对移动网络中严格服务需求的优化框架，通过分割FFNN模型并在UE、边缘节点和核心节点上执行，以减少UE的计算资源占用并保持推理时间。


<details>
  <summary>Details</summary>
Motivation: 移动网络需支持高要求的服务，但UE资源有限，直接运行FFNN模型面临挑战。

Method: 提出一个分割FFNN模型的优化框架，并开发了高效的启发式策略以解决优化问题。

Result: 框架在异构环境中表现鲁棒，无需重新训练，UE内存占用减少33.6%，CPU占用减少60%。

Conclusion: 该框架有效解决了UE资源有限的问题，显著降低了计算资源需求。

Abstract: With mobile networks expected to support services with stringent requirements
that ensure high-quality user experience, the ability to apply Feed-Forward
Neural Network (FFNN) models to User Equipment (UE) use cases has become
critical. Given that UEs have limited resources, running FFNNs directly on UEs
is an intrinsically challenging problem. This letter proposes an optimization
framework for split computing applications where an FFNN model is partitioned
into multiple sections, and executed by UEs, edge- and core-located nodes to
reduce the required UE computational footprint while containing the inference
time. An efficient heuristic strategy for solving the optimization problem is
also provided. The proposed framework is shown to be robust in heterogeneous
settings, eliminating the need for retraining and reducing the UE's memory
(CPU) footprint by over 33.6% (60%).

</details>


### [44] [Network-Aware Control of AGVs in an Industrial Scenario: A Simulation Study Based on ROS 2 and Gazebo](https://arxiv.org/abs/2509.06451)
*Filippo Bragato,Tullia Fontana,Marco Giordani,Malte Schellmann,Josef Eichinger,Michele Zorzi*

Main category: cs.NI

TL;DR: 这篇论文提出了一个基于Gazebo和ROS 2的模拟框架，用于研究工业环境中AGV的控制与通信网络的交互关系，特别是网络性能对控制精度的影响。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，AGV的控制与底层通信网络的性能密切相关，网络延迟或错误可能导致AGV偏离预规划路径。因此，研究两者的联合关系对提高AGV的效率和准确性至关重要。

Method: 作者开发了一个基于Gazebo和ROS 2的模拟框架，该框架结合了通信指标（如延迟和数据包丢失）和控制指标（如路径跟踪的MSE），以模拟和可视化AGV控制与通信网络的交互。

Result: 研究结果揭示了网络性能（特别是PRR）与控制精度之间的相关性，为优化AGV的控制系统提供了依据。

Conclusion: 通过模拟框架，论文证明了通信网络性能对AGV控制的重要性，为未来工业环境中AGV系统的设计和优化提供了有价值的参考。

Abstract: Networked Control System (NCS) is a paradigm where sensors, controllers, and
actuators communicate over a shared network. One promising application of NCS
is the control of Automated Guided Vehicles (AGVs) in the industrial
environment, for example to transport goods efficiently and to autonomously
follow predefined paths or routes. In this context, communication and control
are tightly correlated, a paradigm referred to as Joint Communication and
Control (JCC), since network issues such as delays or errors can lead to
significant deviations of the AGVs from the planned trajectory. In this paper,
we present a simulation framework based on Gazebo and Robot Operating System 2
(ROS 2) to simulate and visualize, respectively, the complex interaction
between the control of AGVs and the underlying communication network. This
framework explicitly incorporates communication metrics, such as delay and
packet loss, and control metrics, especially the Mean Squared Error (MSE)
between the optimal/desired and actual path of the AGV in response to driving
commands. Our results shed light into the correlation between the network
performance, particularly Packet Reception Ratio (PRR), and accuracy of
control.

</details>


### [45] [Empirical Evaluation of a 5G Transparent Clock for Time Synchronization in a TSN-5G Network](https://arxiv.org/abs/2509.06454)
*Julia Caleya-Sanchez,Pablo Muñoz,Jorge Sánchez-Garrido,Emilio Florentín,Felix Delgado-Ferro,Pablo Rodriguez-Martin,Pablo Ameigeiras*

Main category: cs.NI

TL;DR: 本文实证评估了TSN-5G网络中端到端透明时钟（TC）的性能，结果显示其同步精度满足工业需求。


<details>
  <summary>Details</summary>
Motivation: 工业物联网和工业4.0/5.0应用中，时间同步至关重要，但在TSN-5G网络中实现高精度同步面临抖动和不对称延迟的挑战。

Method: 使用商用TSN交换机和单个时钟构建TSN-5G测试床，通过修改PTP消息传输速率评估TC性能。

Result: 实验结果表明，峰值同步精度为500纳秒，满足工业需求（<1微秒）。

Conclusion: TC在TSN-5G网络中表现出色，为工业应用提供了可行的同步解决方案。

Abstract: Time synchronization is essential for industrial IoT and Industry 4.0/5.0
applications, but achieving high synchronization accuracy in Time-Sensitive
Networking (TSN)-5G networks is challenging due to jitter and asymmetric
delays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware
system, boundary clock (BC), and transparent clock (TC), where TC offers a
promising solution. However, to the best of our knowledge, there is no
empirical evaluation of TC in a TSN-5G network. This paper empirically
evaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial
TSN switches with a single clock. For TC development, we compute the residence
time in 5G and recover the clock domain at the slave node. We deploy a TSN-5G
testbed with commercial equipment for synchronization evaluation by modifying
the Precision Timing Protocol (PTP) message transmission rates. Experimental
results show a peak-to-peak synchronization of 500 ns, meeting the industrial
requirement of < 1 us, with minimal synchronization offsets for specific PTP
message transmission rates.

</details>


### [46] [Five Blind Men and the Internet: Towards an Understanding of Internet Traffic](https://arxiv.org/abs/2509.06515)
*Ege Cem Kirci,Ayush Mishra,Laurent Vanbever*

Main category: cs.NI

TL;DR: 该研究通过分析全球472个互联网交换点的公开流量数据，提供了互联网流量增长、区域模式和基础设施扩展的全面视图。


<details>
  <summary>Details</summary>
Motivation: 互联网作为全球最大网络，其流量模式和增长趋势缺乏透明度和细粒度观察，影响了对其动态的理解。

Method: 利用公开的互联网交换点流量统计数据，对2023-2024年间的流量进行综合分析。

Result: 研究发现全球流量增长49.2%（年化24.5%），揭示了区域性的昼夜模式和事件驱动的异常。

Conclusion: 互联网交换点流量可作为互联网整体增长和行为的可靠代理，为长期监测提供了可验证的数据基础。

Abstract: The Internet, the world's largest and most pervasive network, lacks a
transparent, granular view of its traffic patterns, volumes, and growth trends,
hindering the networking community's understanding of its dynamics. This paper
leverages publicly available Internet Exchange Point traffic statistics to
address this gap, presenting a comprehensive two-year study (2023-2024) from
472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate
traffic by late 2024. Our analysis reveals a 49.2% global traffic increase
(24.5% annualized), uncovers regionally distinct diurnal patterns and
event-driven anomalies, and demonstrates stable utilization rates, reflecting
predictable infrastructure scaling. By analyzing biases and confirming high
self-similarity, we establish IXP traffic as a robust proxy for overall
Internet growth and usage behavior. With transparent, replicable data--covering
87% of the worldwide IXP port capacity--and plans to release our dataset, this
study offers a verifiable foundation for long-term Internet traffic monitoring.
In particular, our findings shed light on the interplay between network design
and function, providing an accessible framework for researchers and operators
to explore the Internet's evolving ecosystem.

</details>


### [47] [Ghost Points Matter: Far-Range Vehicle Detection with a Single mmWave Radar in Tunnel](https://arxiv.org/abs/2509.06639)
*Chenming He,Rui Xia,Chengzhen Meng,Xiaoran Fan,Dequan Wang,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.NI

TL;DR: mmTunnel是一种毫米波雷达系统，用于隧道中的远距离车辆检测，通过纠正多径反射产生的幽灵点，提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决隧道中车辆检测的挑战，特别是多径反射导致的幽灵点和定位误差。

Method: 提出多径光线追踪算法和曲线到平面分割方法，优化隧道表面建模和实时处理。

Result: 在测试隧道中，平均F1分数达到93.7%，在遮挡场景下仍高于91%。

Conclusion: mmTunnel在复杂隧道环境中表现出色，具有实时性和高准确性。

Abstract: Vehicle detection in tunnels is crucial for traffic monitoring and accident
response, yet remains underexplored. In this paper, we develop mmTunnel, a
millimeter-wave radar system that achieves far-range vehicle detection in
tunnels. The main challenge here is coping with ghost points caused by
multi-path reflections, which lead to severe localization errors and false
alarms. Instead of merely removing ghost points, we propose correcting them to
true vehicle positions by recovering their signal reflection paths, thus
reserving more data points and improving detection performance, even in
occlusion scenarios. However, recovering complex 3D reflection paths from
limited 2D radar points is highly challenging. To address this problem, we
develop a multi-path ray tracing algorithm that leverages the ground plane
constraint and identifies the most probable reflection path based on signal
path loss and spatial distance. We also introduce a curve-to-plane segmentation
method to simplify tunnel surface modeling such that we can significantly
reduce the computational delay and achieve real-time processing.
  We have evaluated mmTunnel with comprehensive experiments. In two test
tunnels, we conducted controlled experiments in various scenarios with cars and
trucks. Our system achieves an average F1 score of 93.7% for vehicle detection
while maintaining real-time processing. Even in the challenging occlusion
scenarios, the F1 score remains above 91%. Moreover, we collected extensive
data from a public tunnel with heavy traffic at times and show our method could
achieve an F1 score of 91.5% in real-world traffic conditions.

</details>


### [48] [Sovereign AI for 6G: Towards the Future of AI-Native Networks](https://arxiv.org/abs/2509.06700)
*Swarna Bindu Chetty,David Grace,Simon Saunders,Paul Harris,Eirini Eleni Tsiropoulou,Tony Quek,Hamed Ahmadi*

Main category: cs.NI

TL;DR: 论文提出‘主权AI’作为6G的战略核心，通过架构、运营和治理框架确保国家对AI开发与部署的控制，尤其关注O-RAN架构中的xApps和rApps部署。


<details>
  <summary>Details</summary>
Motivation: 6G时代AI原生架构带来实时自动化等能力，但外部训练的AI模型引发数据主权和安全风险，需建立可控的AI治理框架。

Method: 提出主权AI概念，设计架构与治理框架，在O-RAN中部署xApps和rApps，支持安全的模型更新和联邦学习。

Result: 主权AI不仅是法规要求，更是6G网络安全、弹性与伦理对齐的基石。

Conclusion: 主权AI为6G提供必要的安全保障，需全球协作应对技术与管理挑战。

Abstract: The advent of Generative Artificial Intelligence (GenAI), Large Language
Models (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile
networks, especially as the telecom industry transitions from 5G's
cloud-centric to AI-native 6G architectures. This transition unlocks
unprecedented capabilities in real-time automation, semantic networking, and
autonomous service orchestration. However, it introduces critical risks related
to data sovereignty, security, explainability, and regulatory compliance
especially when AI models are trained, deployed, or governed externally. This
paper introduces the concept of `Sovereign AI' as a strategic imperative for
6G, proposing architectural, operational, and governance frameworks that enable
national or operator-level control over AI development, deployment, and
life-cycle management. Focusing on O-RAN architecture, we explore how sovereign
AI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure
policy-aligned control, secure model updates, and federated learning across
trusted infrastructure. We analyse global strategies, technical enablers, and
challenges across safety, talent, and model governance. Our findings underscore
that Sovereign AI is not just a regulatory necessity but a foundational pillar
for secure, resilient, and ethically-aligned 6G networks.

</details>


### [49] [VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning](https://arxiv.org/abs/2509.06763)
*Huijun Tang,Wang Zeng,Ming Du,Pinlong Zhao,Pengfei Jiao,Huaming Wu,Hongjian Sun*

Main category: cs.NI

TL;DR: 该论文提出了一种名为VariSAC的GNN增强深度强化学习框架，用于RIS辅助的ISAC车载网络中确保连续连接性。通过统一的CCR指标和SAC代理优化资源分配，实验证明VariSAC在动态车载环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 车载网络中V2I和V2V连接的多样性与动态拓扑结构带来了可靠性建模和资源优化的挑战。

Method: 采用GNN编码高维系统状态，并结合SAC代理优化信道分配、功率控制和RIS配置。

Result: VariSAC在真实数据集上显著提升了V2I和V2V连接的可靠性和连续性。

Conclusion: VariSAC在动态车载环境中实现了高效的资源管理和持续的连接性。

Abstract: The integration of Reconfigurable Intelligent Surfaces (RIS) and Integrated
Sensing and Communication (ISAC) in vehicular networks enables dynamic spatial
resource management and real-time adaptation to environmental changes. However,
the coexistence of distinct vehicle-to-infrastructure (V2I) and
vehicle-to-vehicle (V2V) connectivity requirements, together with highly
dynamic and heterogeneous network topologies, presents significant challenges
for unified reliability modeling and resource optimization. To address these
issues, we propose VariSAC, a graph neural network (GNN)-augmented deep
reinforcement learning framework for assured, time-continuous connectivity in
RIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,
we introduce the Continuous Connectivity Ratio (CCR), a unified metric that
characterizes the sustained temporal reliability of V2I connections and the
probabilistic delivery guarantees of V2V links, thus unifying their continuous
reliability semantics. Next, we employ a GNN with residual adapters to encode
complex, high-dimensional system states, capturing spatial dependencies among
vehicles, base stations (BS), and RIS nodes. These representations are then
processed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channel
allocation, power control, and RIS configurations to maximize CCR-driven
long-term rewards. Extensive experiments on real-world urban datasets
demonstrate that VariSAC consistently outperforms existing baselines in terms
of continuous V2I ISAC connectivity and V2V delivery reliability, enabling
persistent connectivity in highly dynamic vehicular environments.

</details>


### [50] [Resilience of Mega-Satellite Constellations: How Node Failures Impact Inter-Satellite Networking Over Time?](https://arxiv.org/abs/2509.06766)
*Binquan Guo,Zehui Xiong,Zhou Zhang,Baosheng Li,Dusit Niyato,Chau Yuen,Zhu Han*

Main category: cs.NI

TL;DR: 研究探讨了大卫星星座中单个节点的重要性及其对节点故障的韧性，提出了服务感知时间介数度量和一个分析框架来评估节点故障的影响，并在Starlink星座中验证了卫星网络的韧性。


<details>
  <summary>Details</summary>
Motivation: 大卫星星座通过星间链路提供低延迟全球通信服务，但恶劣的太空环境导致卫星易故障，影响网络连通性，因此需研究节点故障的影响。

Method: 将大卫星星座建模为离散时间图，提出服务感知时间介数度量来衡量节点重要性，开发分析框架评估当前及后续时间窗口内节点故障的影响。

Result: 模拟显示卫星网络对节点故障有韧性，动态拓扑部分恢复连接并减少长期影响；重路由机制对快速恢复网络至关重要。

Conclusion: 卫星网络在节点故障下展现韧性，动态拓扑和重路由机制是关键，未来需优化恢复策略以提升网络可靠性。

Abstract: Mega-satellite constellations have the potential to leverage inter-satellite
links to deliver low-latency end-to-end communication services globally,
thereby extending connectivity to underserved regions. However, harsh space
environments make satellites vulnerable to failures, leading to node removals
that disrupt inter-satellite networking. With the high risk of satellite node
failures, understanding their impact on end-to-end services is essential. This
study investigates the importance of individual nodes on inter-satellite
networking and the resilience of mega satellite constellations against node
failures. We represent the mega-satellite constellation as discrete temporal
graphs and model node failure events accordingly. To quantify node importance
for targeted services over time, we propose a service-aware temporal
betweenness metric. Leveraging this metric, we develop an analytical framework
to identify critical nodes and assess the impact of node failures. The
framework takes node failure events as input and efficiently evaluates their
impacts across current and subsequent time windows. Simulations on the Starlink
constellation setting reveal that satellite networks inherently exhibit
resilience to node failures, as their dynamic topology partially restore
connectivity and mitigate the long-term impact. Furthermore, we find that the
integration of rerouting mechanisms is crucial for unleashing the full
resilience potential to ensure rapid recovery of inter-satellite networking.

</details>


### [51] [BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with Zero-Shot Template Generation](https://arxiv.org/abs/2509.06898)
*Zhihui Gao,Zhecun Liu,Tingjun Chen*

Main category: cs.NI

TL;DR: BatStation是一种轻量级雷达感知框架，利用5G基站的上行资源网格提取雷达信号，解决了雷达信号与5G信号共存的问题。


<details>
  <summary>Details</summary>
Motivation: 现有雷达信号与商用5G信号的共存需要高效和自适应的频谱共享，因此提出了一种利用5G基站进行雷达感知的解决方案。

Method: BatStation通过雷达信号分离、资源网格重塑和零样本模板关联三个关键组件，从5G上行传输中提取雷达信号。

Result: 在真实5G流量下测试，BatStation表现出色，检测概率最高达97.02%，分类准确率97%，运行时延低至0.11ms。

Conclusion: BatStation为5G网络中的雷达感知提供了一种实时、高效的解决方案。

Abstract: The coexistence between incumbent radar signals and commercial 5G signals
necessitates a versatile and ubiquitous radar sensing for efficient and
adaptive spectrum sharing. In this context, leveraging the densely deployed 5G
base stations (BS) for radar sensing is particularly promising, offering both
wide coverage and immediate feedback to 5G scheduling. However, the targeting
radar signals are superimposed with concurrent 5G uplink transmissions received
by the BS, and practical deployment also demands a lightweight, portable radar
sensing model. This paper presents BatStation, a lightweight, in-situ radar
sensing framework seamlessly integrated into 5G BSs. BatStation leverages
uplink resource grids to extract radar signals through three key components:
(i) radar signal separation to cancel concurrent 5G transmissions and reveal
the radar signals, (ii) resource grid reshaping to align time-frequency
resolution with radar pulse characteristics, and (iii) zero-shot template
correlation based on a portable model trained purely on synthetic data that
supports detection, classification, and localization of radar pulses without
fine-tuning using experimental data. We implement BatStation on a
software-defined radio (SDR) testbed and evaluate its performance with real 5G
traffic in the CBRS band. Results show robust performance across diverse radar
types, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),
classification accuracy up to 97.00%, and median localization errors of
2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,
BatStation achieves this performance with a runtime latency of only 0.11/0.94
ms on GPU/CPU, meeting the real-time requirement of 5G networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [52] [Effectively obtaining acoustic, visual and textual data from videos](https://arxiv.org/abs/2509.05786)
*Jorge E. León,Miguel Carrasco*

Main category: cs.MM

TL;DR: 论文提出一种从视频中提取音频-图像-文本数据的方法，填补多模态数据集不足的空白。


<details>
  <summary>Details</summary>
Motivation: 因高质量多模态数据集（尤其是结合声音、图像和文本的数据）稀缺，本文旨在解决这一问题。

Method: 通过从视频中选择合适内容、提取相关数据对，并利用图像到文本模型生成描述性文本。

Result: 生成了公开可用的数据集，增强了多模态数据在机器学习和数据分析中的应用。

Conclusion: 该方法提升了数据集的语义连接和质量，支持多模态研究的进一步发展。

Abstract: The increasing use of machine learning models has amplified the demand for
high-quality, large-scale multimodal datasets. However, the availability of
such datasets, especially those combining acoustic, visual and textual data,
remains limited. This paper addresses this gap by proposing a method to extract
related audio-image-text observations from videos. We detail the process of
selecting suitable videos, extracting relevant data pairs, and generating
descriptive texts using image-to-text models. Our approach ensures a robust
semantic connection between modalities, enhancing the utility of the created
datasets for various applications. We also discuss the challenges encountered
and propose solutions to improve data quality. The resulting datasets, publicly
available, aim to support and advance research in multimodal data analysis and
machine learning.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [53] [Compositional Inductive Invariant Inference via Assume-Guarantee Reasoning](https://arxiv.org/abs/2509.06250)
*Ian Dardik,Eunsuk Kang*

Main category: cs.LO

TL;DR: 论文提出了一种分解系统并推断局部归纳不变量的新方法，以简化复杂系统中的安全验证。


<details>
  <summary>Details</summary>
Motivation: 归纳不变量因其复杂性而难以推断，尤其是需要覆盖整个系统的过渡关系。

Method: 将系统分解为组件，为每个组件分配假设-保证合同，并通过局部归纳不变量验证合同。

Result: 案例研究表明，该方法比全局技术更高效，并提供了模块化的洞察。

Conclusion: 局部归纳不变量为系统验证提供了更高效和模块化的解决方案。

Abstract: A common technique for verifying the safety of complex systems is the
inductive invariant method. Inductive invariants are inductive formulas that
overapproximate the reachable states of a system and imply a desired safety
property. However, inductive invariants are notoriously complex, which makes
inductive invariant inference a challenging problem. In this work, we observe
that inductive invariant formulas are complex primarily because they must be
closed over the transition relation of an entire system. Therefore, we propose
a new approach in which we decompose a system into components, assign an
assume-guarantee contract to each component, and prove that each component
fulfills its contract by inferring a local inductive invariant. The key
advantage of local inductive invariant inference is that the local invariant
need only be closed under the transition relation for the component, which is
simpler than the transition relation for the entire system. Once local
invariant inference is complete, system-wide safety follows by construction
because the conjunction of all local invariants becomes an inductive invariant
for the entire system. We apply our compositional inductive invariant inference
technique to two case studies, in which we provide evidence that our framework
can infer invariants more efficiently than the global technique. Our case
studies also show that local inductive invariants provide modular insights
about a specification that are not offered by global invariants.

</details>


### [54] [Verifying Sampling Algorithms via Distributional Invariants](https://arxiv.org/abs/2509.06410)
*Kevin Batz,Joost-Pieter Katoen,Tobias Winkler,Daniel Zilken*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper develops a verification framework aimed at establishing the
correctness of discrete sampling algorithms. We do so by considering
probabilistic programs as distribution transformers. Inspired by recent work on
distributional verification of Markov models, we introduce the notion of
(inductive) distributional loop invariants for discrete probabilistic programs.
These invariants are embedded in a Hoare-like verification framework that
includes proof rules for total and partial correctness. To illustrate the
applicability of our framework, we prove the correctness of two discrete
sampling algorithms: the Fast Dice Roller and the Fast Loaded Dice Roller.

</details>


### [55] [Tabular intermediate logics comparison](https://arxiv.org/abs/2509.06841)
*Paweł Rzążewski,Michał Stronkowski*

Main category: cs.LO

TL;DR: 研究了表格中阶逻辑的决策问题LogContain，探讨了其与SPMorph问题的关系，提出了图到偏序集的构造方法，证明了部分受限问题的NP完备性，并针对树结构设计了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 旨在分析表格中阶逻辑的包含关系问题LogContain的复杂性，并将其与图论中的问题关联，以揭示逻辑问题在图论中的对应关系。

Method: 提出了从图构造偏序集的方法，证明了图论中的局部满射同态与偏序集中的p-满射同态的等价性，进而通过图论中的硬度结果证明LogContain和SPMorph问题的受限版本的NP完备性。

Result: 发现某些受限版本的LogContain和SPMorph是NP完备的，特别是展示了一个18元的偏序集使得对应问题为NP完备；同时为树结构设计了一个多项式时间算法。

Conclusion: 表格中阶逻辑的包含关系问题在图论中有对应表达，部分问题为NP完备，但对于树结构可高效解决。

Abstract: Tabular intermediate logics are intermediate logics characterized by finite
posets treated as Kripke frames. For a poset $\mathbb{P}$, let $L(\mathbb{P})$
denote the corresponding tabular intermediate logic. We investigate the
complexity of the following decision problem $\mathsf{LogContain}$: given two
finite posets $\mathbb P$ and $\mathbb Q$, decide whether $L(\mathbb P)
\subseteq L(\mathbb Q)$.
  By Jankov's and de Jongh's theorem, the problem $\mathsf{LogContain}$ is
related to the problem $\mathsf{SPMorph}$: given two finite posets $\mathbb P$
and $\mathbb Q$, decide whether there exists a surjective $p$-morphism from
$\mathbb P$ onto $\mathbb Q$. Both problems belong to the complexity class NP.
  We present two contributions. First, we describe a construction which,
starting with a graph $\mathbb{G}$, gives a poset $\mathsf{Pos}(\mathbb{G})$
such that there is a surjective locally surjective homomorphism (the
graph-theoretic analog of a $p$-morphism) from $\mathbb{G}$ onto $\mathbb{H}$
if and only if there is a surjective $p$-morphism from
$\mathsf{Pos}(\mathbb{G})$ onto $\mathsf{Pos}(\mathbb{H})$. This allows us to
translate some hardness results from graph theory and obtain that several
restricted versions of the problems $\mathsf{LogContain}$ and
$\mathsf{SPMorph}$ are NP-complete. Among other results, we present a
18-element poset $\mathbb{Q}$ such that the problem to decide, for a given
poset $\mathbb{P}$, whether $L(\mathbb{P})\subseteq L(\mathbb{Q})$ is
NP-complete.
  Second, we describe a polynomial-time algorithm that decides
$\mathsf{LogContain}$ and $\mathsf{SPMorph}$ for posets $\mathbb{T}$ and
$\mathbb{Q}$, when $\mathbb{T}$ is a tree.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [56] [Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression](https://arxiv.org/abs/2509.05298)
*Rui Xi,Xianghan Wang*

Main category: cs.HC

TL;DR: Livia是一款基于AR的情感感知AI伴侣应用，通过模块化AI和多模态情感计算提供个性化情感支持，显著降低用户孤独感。


<details>
  <summary>Details</summary>
Motivation: 孤独感和社交隔离对情感和健康带来挑战，需要技术解决方案提供陪伴和支持。

Method: 结合模块化AI（情绪分析、对话生成等）、TBC和DIMF算法管理记忆，多模态情感检测提升交互准确性。

Result: 用户测试显示情感连接增强，满意度提升，孤独感显著减少。

Conclusion: Livia在情感支持方面效果显著，未来可扩展交互方式和多用户体验。

Abstract: Loneliness and social isolation pose significant emotional and health
challenges, prompting the development of technology-based solutions for
companionship and emotional support. This paper introduces Livia, an
emotion-aware augmented reality (AR) companion app designed to provide
personalized emotional support by combining modular artificial intelligence
(AI) agents, multimodal affective computing, progressive memory compression,
and AR driven embodied interaction. Livia employs a modular AI architecture
with specialized agents responsible for emotion analysis, dialogue generation,
memory management, and behavioral orchestration, ensuring robust and adaptive
interactions. Two novel algorithms-Temporal Binary Compression (TBC) and
Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize
long-term memory, significantly reducing storage requirements while retaining
critical context. Our multimodal emotion detection approach achieves high
accuracy, enhancing proactive and empathetic engagement. User evaluations
demonstrated increased emotional bonds, improved satisfaction, and
statistically significant reductions in loneliness. Users particularly valued
Livia's adaptive personality evolution and realistic AR embodiment. Future
research directions include expanding gesture and tactile interactions,
supporting multi-user experiences, and exploring customized hardware
implementations.

</details>


### [57] [Hybrid User Interfaces: Past, Present, and Future of Complementary Cross-Device Interaction in Mixed Reality](https://arxiv.org/abs/2509.05491)
*Sebastian Hubenschmid,Marc Satkowski,Johannes Zagermann,Julián Méndez,Niklas Elmqvist,Steven Feiner,Tiara Feuchtner,Jens Emil Grønbæk,Benjamin Lee,Dieter Schmalstieg,Raimund Dachselt,Harald Reiterer*

Main category: cs.HC

TL;DR: 本文研究了混合用户界面（HUIs），旨在为这一新兴研究领域建立统一的理解和一致的术语。HUIs通过结合异质设备的互补优势，特别是在2D设备和混合现实环境之间的跨设备交互方面，探索了独特的设计可能性和挑战。


<details>
  <summary>Details</summary>
Motivation: 混合用户界面（HUIs）结合了不同设备的优势，尤其是在2D设备和混合现实环境之间的交互方面具有潜力。但目前缺乏对这一领域的统一理解和术语，导致研究碎片化。

Method: 通过系统调查，提出了结合传统显示技术和混合现实环境的HUIs分类法。

Result: 提出了HUIs的分类法，并讨论了过去的挑战、定义的演变以及未来的机会。

Conclusion: 本文为HUIs研究提供了统一的框架，连接了过去30年的研究成果并展望了未来的发展方向。

Abstract: We investigate hybrid user interfaces (HUIs), aiming to establish a cohesive
understanding and adopt consistent terminology for this nascent research area.
HUIs combine heterogeneous devices in complementary roles, leveraging the
distinct benefits of each. Our work focuses on cross-device interaction between
2D devices and mixed reality environments, which are particularly compelling,
leveraging the familiarity of traditional 2D platforms while providing spatial
awareness and immersion. Although such HUIs have been prominently explored in
the context of mixed reality by prior work, we still lack a cohesive
understanding of the unique design possibilities and challenges of such
combinations, resulting in a fragmented research landscape. We conducted a
systematic survey and present a taxonomy of HUIs that combine conventional
display technology and mixed reality environments. Based on this, we discuss
past and current challenges, the evolution of definitions, and prospective
opportunities to tie together the past 30 years of research with our vision of
future HUIs.

</details>


### [58] [GestoBrush: Facilitating Graffiti Artists' Digital Creation Experiences through Embodied AR Interactions](https://arxiv.org/abs/2509.05619)
*Ruiqi Chen,Qingyang He,Hanxi Bao,Jung Choi,Xin Tong*

Main category: cs.HC

TL;DR: 论文提出了GestoBrush，一个基于AR的移动应用，通过智能手机模拟喷漆，支持涂鸦艺术家的创意过程，研究发现其能增强直觉性、沉浸感和表现力。


<details>
  <summary>Details</summary>
Motivation: 研究探讨如何通过数字工具如AR技术，解决涂鸦艺术家在全球法规限制下面临的创意自由问题。

Method: 开发了GestoBrush原型，通过智能手机模拟喷漆，并组织涂鸦艺术家参与共同设计工作坊。

Result: GestoBrush支持涂鸦艺术家超越现实限制，探索新艺术可能，提高直觉性、沉浸感和表现力。

Conclusion: 研究展示了AR工具如何弥合实体涂鸦与数字表达间的差距，为设计尊重街头艺术文化的沉浸式创意系统提供路径。

Abstract: Graffiti has long documented the socio-cultural landscapes of urban spaces,
yet increasing global regulations have constrained artists' creative freedom,
prompting exploration of digital alternatives. Augmented Reality (AR) offers
opportunities to extend graffiti into digital environments while retaining
spatial and cultural significance, but prior research has largely centered on
audience engagement rather than the embodied creative processes of graffiti
artists. To address this, we developed GestoBrush, a mobile AR prototype that
turns smartphones into virtual spray cans, enabling graffiti creation through
embodied gestures. A co-design workshop underscored the role of
embodiment-physical engagement with surroundings and body-driven creative
processes-in digital workflows. We evaluated GestoBrush with six graffiti
artists and findings suggested that embodied AR interactions supporting artists
bypass real-world constraints and explore new artistic possibilities, whose AR
artworks created enhanced senses of intuitiveness, immersion, and
expressiveness. This work highlight how embodied AR tools can bridge the gap
between physical graffiti practice and digital expression, suggesting pathways
for designing immersive creative systems that respect the cultural ethos of
street art while expanding its possibilities in virtual spaces.

</details>


### [59] [Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization](https://arxiv.org/abs/2509.05718)
*Péter Ferenc Gyarmati,Manfred Klaffenböck,Laura Koesten,Torsten Möller*

Main category: cs.HC

TL;DR: 研究探讨了视觉语言模型（VLMs）在分类科学可视化图表时的能力，发现其在识别目的和维度上表现较好，但在特定编码类型上存在困难，提示需要人类监督以确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 验证VLMs是否能像人类一样识别图表的核心视觉属性，以促进可视化工具中有效的人机协作。

Method: 通过13种不同的VLMs，仅基于视觉刺激对科学可视化图表进行分类，评估其在目的、编码和维度上的准确性。

Result: VLMs在目的和维度分类上表现良好，但在特定编码类型上表现不佳；大型模型不一定表现更优。

Conclusion: VLMs在可视化任务中的应用需谨慎，建议结合人类监督以提高可靠性。

Abstract: Vision-language models (VLMs) hold promise for enhancing visualization tools,
but effective human-AI collaboration hinges on a shared perceptual
understanding of visual content. Prior studies assessed VLM visualization
literacy through interpretive tasks, revealing an over-reliance on textual cues
rather than genuine visual analysis. Our study investigates a more foundational
skill underpinning such literacy: the ability of VLMs to recognize a chart's
core visual properties as humans do. We task 13 diverse VLMs with classifying
scientific visualizations based solely on visual stimuli, according to three
criteria: purpose (e.g., schematic, GUI, visualization), encoding (e.g., bar,
point, node-link), and dimensionality (e.g., 2D, 3D). Using expert labels from
the human-centric VisType typology as ground truth, we find that VLMs often
identify purpose and dimensionality accurately but struggle with specific
encoding types. Our preliminary results show that larger models do not always
equate to superior performance and highlight the need for careful integration
of VLMs in visualization tasks, with human supervision to ensure reliable
outcomes.

</details>


### [60] [A Composable Agentic System for Automated Visual Data Reporting](https://arxiv.org/abs/2509.05721)
*Péter Ferenc Gyarmati,Dominik Moritz,Torsten Möller,Laura Koesten*

Main category: cs.HC

TL;DR: 论文探讨了通过多智能体架构实现人机合作的自动化视觉数据报告系统，结合了确定性模块和LLM，提供了交互式报告和可执行笔记本的双重输出。


<details>
  <summary>Details</summary>
Motivation: 解决单体AI智能体的脆弱性问题，探索更可靠的人机合作模式。

Method: 提出混合多智能体架构，利用Draco规则系统进行可视化设计，生成交互式报告和可执行笔记本。

Result: 实现了全自动、可审计且可调控的系统，增强了人机协同。

Conclusion: 该系统为人类专家与AI的协同合作提供了一条可行路径，代码和示例已开源。

Abstract: To address the brittleness of monolithic AI agents, our prototype for
automated visual data reporting explores a Human-AI Partnership model. Its
hybrid, multi-agent architecture strategically externalizes logic from LLMs to
deterministic modules, leveraging the rule-based system Draco for principled
visualization design. The system delivers a dual-output: an interactive
Observable report with Mosaic for reader exploration, and executable Marimo
notebooks for deep, analyst-facing traceability. This granular architecture
yields a fully automatic yet auditable and steerable system, charting a path
toward a more synergistic partnership between human experts and AI. For
reproducibility, our implementation and examples are available at
https://peter-gy.github.io/VISxGenAI-2025/.

</details>


### [61] [Augmenting Human-Centered Racial Covenant Detection and Georeferencing with Plug-and-Play NLP Pipelines](https://arxiv.org/abs/2509.05829)
*Jiyoon Pyo,Yuankun Jiao,Yao-Yi Chiang,Michael Corey*

Main category: cs.HC

TL;DR: 该研究提出了一种人机协作方法，通过两个NLP流水线（文本标记和地理定位）高效识别和定位历史地契中的种族限制条款，提高了准确性和志愿者参与效率。


<details>
  <summary>Details</summary>
Motivation: 探讨历史地契中种族限制条款对现代社会不平等的影响，并寻找高效识别和定位这些条款的方法，以推动社会正义和公共参与。

Method: 采用人机协作方式，开发了两个NLP流水线：上下文感知文本标记模型和高精度地理定位模块，用于识别种族限制语言并关联现实世界位置。

Result: 在明尼苏达和威斯康星六个县的历史地契上测试，系统减少了25.96%的误报，召回率达91.73%，地理定位精度为85.58%。

Conclusion: 该方法显著提升了地契筛选和标注效率，促进了公众参与，为社会不平等研究提供了有力工具。

Abstract: Though no longer legally enforceable, racial covenants in twentieth-century
property deeds continue to shape spatial and socioeconomic inequalities.
Understanding this legacy requires identifying racially restrictive language
and geolocating affected properties. The Mapping Prejudice project addresses
this by engaging volunteers on the Zooniverse crowdsourcing platform to
transcribe covenants from scanned deeds and link them to modern parcel maps
using transcribed legal descriptions. While the project has explored
automation, it values crowdsourcing for its social impact and technical
advantages. Historically, Mapping Prejudice relied on lexicon-based searching
and, more recently, fuzzy matching to flag suspected covenants. However, fuzzy
matching has increased false positives, burdening volunteers and raising
scalability concerns. Additionally, while many properties can be mapped
automatically, others still require time-intensive manual geolocation.
  We present a human-centered computing approach with two plug-and-play NLP
pipelines: (1) a context-aware text labeling model that flags racially
restrictive language with high precision and (2) a georeferencing module that
extracts geographic descriptions from deeds and resolves them to real-world
locations. Evaluated on historical deed documents from six counties in
Minnesota and Wisconsin, our system reduces false positives in racial term
detection by 25.96% while maintaining 91.73% recall and achieves 85.58%
georeferencing accuracy within 1x1 square-mile ranges. These tools enhance
document filtering and enrich spatial annotations, accelerating volunteer
participation and reducing manual cleanup while strengthening public
engagement.

</details>


### [62] [Attention, Action, and Memory: How Multi-modal Interfaces and Cognitive Load Alter Information Retention](https://arxiv.org/abs/2509.05898)
*Omar Elgohary,Zhu-Tien*

Main category: cs.HC

TL;DR: 比较多模态交互与传统触控板对信息记忆的影响，发现多模态交互在短期记忆效果相似，但认知负荷更高且长期记忆效果较差。


<details>
  <summary>Details</summary>
Motivation: 探索多模态系统（如眼动与手势控制）对学习和记忆保留的影响，填补现有研究的空白。

Method: 12名参与者通过触控板和多模态界面阅读文章，分别在即时和24小时后测试记忆，并评估认知负荷和系统可用性。

Result: 多模态交互在信息保留上与触控板效果相似，但认知负荷更高，且长期记忆效果下降。

Conclusion: 多模态系统在教育和技术设计中需权衡认知负荷与记忆效果，为未来设计提供建议。

Abstract: Each year, multi-modal interaction continues to grow within both industry and
academia. However, researchers have yet to fully explore the impact of
multi-modal systems on learning and memory retention. This research
investigates how combining gaze-based controls with gesture navigation affects
information retention when compared to standard track-pad usage. A total of
twelve participants read four textual articles through two different user
interfaces which included a track-pad and a multi-modal interface that tracked
eye movements and hand gestures for scrolling, zooming, and revealing content.
Participants underwent two assessment sessions that measured their information
retention immediately and after a twenty-four hour period along with the
NASA-TLX workload evaluation and the System Usability Scale assessment. The
initial analysis indicates that multi-modal interaction produces similar
targeted information retention to traditional track-pad usage, but this neutral
effect comes with higher cognitive workload demands and seems to deteriorate
with long-term retention. The research results provide new knowledge about how
multi-modal systems affect cognitive engagement while providing design
recommendations for future educational and assistive technologies that require
effective memory performance.

</details>


### [63] [DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification](https://arxiv.org/abs/2509.05943)
*Yi Wang,Haodong Zhang,Hongqi Li*

Main category: cs.HC

TL;DR: 介绍了一种名为DRDCAE-STGNN的端到端深度学习框架，结合残差密集卷积自编码器和时空图神经网络，显著提升了运动想象脑电信号的解码性能，实现了高准确率和实时性。


<details>
  <summary>Details</summary>
Motivation: 运动想象（MI）由于其非平稳性和低信噪比，解码难度大，限制了其在辅助技术和神经康复中的应用潜力。

Method: 提出DRDCAE-STGNN框架：DRDCAE模块通过残差密集连接学习区分性特征；STGNN模块利用动态空间依赖性和双向LSTM建模时空特征。

Result: 在多个数据集上实现了95.42%、97.51%和90.15%的平均准确率，参数量和推理时间（0.32ms/样本）满足实时性需求。

Conclusion: 该方法为MI-EEG解码提供了高鲁棒性、高准确性和可解释性的解决方案，适用于跨被试和任务的通用场景。

Abstract: Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant
potential for assistive technologies and neurorehabilitation. However, the
precise and efficient decoding of MI remains challenging due to their
non-stationary nature and low signal-to-noise ratio. This paper introduces a
novel end-to-end deep learning framework of Discriminative Residual Dense
Convolutional Autoencoder with Spatio-Temporal Graph Neural Network
(DRDCAE-STGNN) to enhance the MI feature learning and classification.
Specifically, the DRDCAE module leverages residual-dense connections to learn
discriminative latent representations through joint reconstruction and
classifica-tion, while the STGNN module captures dynamic spatial dependencies
via a learnable graph adjacency matrix and models temporal dynamics using
bidirectional long short-term memory (LSTM). Extensive evaluations on BCI
Competition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art
performance, with average accuracies of 95.42%, 97.51%, and 90.15%,
respectively. Ablation studies confirm the contribution of each component, and
interpreta-bility analysis reveals neurophysiologically meaningful connectivity
patterns. Moreover, despite its complexity, the model maintains a feasible
parameter count and an inference time of 0.32 ms per sample. These results
indicate that our method offers a robust, accurate, and interpretable solution
for MI-EEG decoding, with strong generalizability across subjects and tasks and
meeting the requirements for potential real-time BCI applications.

</details>


### [64] [Hue4U: Real-Time Personalized Color Correction in Augmented Reality](https://arxiv.org/abs/2509.06776)
*Jingwen Qin,Semen Checherin,Yue Li,Berend-Jan van der Zwaag,Özlem Durmaz-Incel*

Main category: cs.HC

TL;DR: Hue4U是一种基于增强现实的个性化实时色彩校正系统，无需临床诊断且能实时适应用户需求，显著改善了色觉缺陷用户的辨色能力。


<details>
  <summary>Details</summary>
Motivation: 现有的色彩校正方法依赖临床诊断和静态过滤，对轻中度色觉缺陷用户效果不佳，需要更灵活的解决方案。

Method: 开发了Hue4U系统，利用消费级Meta Quest头显实现实时个性化色彩校正，无需临床诊断。

Result: 用户研究表明，10名参与者辨色能力显著提升，效应量大（Cohen's d > 1.4）。

Conclusion: 个性化增强现实干预有望改善色觉缺陷用户的视觉可及性和生活质量。

Abstract: Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent
of women worldwide. Existing color-correction methods often rely on prior
clinical diagnosis and static filtering, making them less effective for users
with mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,
real-time color-correction system in augmented reality using consumer-grade
Meta Quest headsets. Unlike previous methods, Hue4U requires no prior medical
diagnosis and adapts to the user in real time. A user study with 10
participants showed notable improvements in their ability to distinguish
colors. The results demonstrated large effect sizes (Cohen's d > 1.4),
suggesting clinically meaningful gains for individuals with CVD. These findings
highlight the potential of personalized AR interventions to improve visual
accessibility and quality of life for people affected by CVD.

</details>


### [65] [A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners](https://arxiv.org/abs/2509.05961)
*Evgeny V. Votyakov,Marios Constantinides,Fotis Liarokapis*

Main category: cs.HC

TL;DR: 业余跑者常用穿戴设备追踪训练数据，但心率与配速等单一指标缺乏可解释性。本文提出Fitplotter可视化工具并重新定义心率效率（HRE），通过多跑者数据验证其稳定性与实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决业余跑者依赖单一指标（心率、配速）分析训练效果时缺乏可解释性的问题。

Method: 开发Fitplotter客户端应用，提出并验证HRE（心率与配速乘积）作为综合性指标，分析十年个人数据及12名跑者公开日志。

Result: HRE比单一指标更稳定，能反映训练量、季节进展，并在长跑中保持稳定。

Conclusion: HRE为业余跑者提供直观且可解释的指标，支持训练决策，提升用户体验，助力自主数据管理。

Abstract: Amateur runners are increasingly using wearable devices to track their
training, and often do so through simple metrics such as heart rate and pace.
However, these metrics are typically analyzed in isolation and lack the
explainability needed for long-term self-monitoring. In this paper, we first
present Fitplotter, which is a client-side web application designed for the
visualization and analysis of data associated with fitness and activity
tracking devices. Next, we revisited and formalized Heart Rate Efficiency
(HRE), defined as the product of pace and heart rate, as a practical and
explainable metric to track aerobic fitness in everyday running. Drawing on
more than a decade of training data from one athlete, and supplemented by
publicly available logs from twelve runners, we showed that HRE provides more
stable and meaningful feedback on aerobic development than heart rate or pace
alone. We showed that HRE correlates with training volume, reflects seasonal
progress, and remains stable during long runs in well-trained individuals. We
also discuss how HRE can support everyday training decisions, improve the user
experience in fitness tracking, and serve as an explainable metric to
proprietary ones of commercial platforms. Our findings have implications for
designing user-centered fitness tools that empower amateur athletes to
understand and manage their own performance data.

</details>


### [66] [The Reel Deal: Designing and Evaluating LLM-Generated Short-Form Educational Videos](https://arxiv.org/abs/2509.05962)
*Lazaros Stavrinou,Argyris Constantinides,Marios Belk,Vasos Vassiliou,Fotis Liarokapis,Marios Constantinides*

Main category: cs.HC

TL;DR: ReelsEd系统利用大语言模型自动从长视频生成短视频，提高了学习效果和用户体验，且用户信任度高。


<details>
  <summary>Details</summary>
Motivation: 短视频因其简洁易学在教育中受欢迎，但自动生成视频对学习效果、用户体验和信任的影响尚不清楚。

Method: 开发ReelsEd系统，使用大语言模型将长视频转为结构化短视频，并在62名学生中开展对比研究。

Result: ReelsEd在参与度、测试表现和任务效率上优于长视频，且认知负荷未增加，用户对其信任度高。

Conclusion: 生成式AI可有效整合到教育工具中，需注重可用性、学习者自主性和教学一致性。

Abstract: Short-form videos are gaining popularity in education due to their concise
and accessible format that enables microlearning. Yet, most of these videos are
manually created. Even for those automatically generated using artificial
intelligence (AI), it is not well understood whether or how they affect
learning outcomes, user experience, and trust. To address this gap, we
developed ReelsEd, which is a web-based system that uses large language models
(LLMs) to automatically generate structured short-form video (i.e., reels) from
lecture long-form videos while preserving instructor-authored material. In a
between-subject user study with 62 university students, we evaluated ReelsEd
and demonstrated that it outperformed traditional long-form videos in
engagement, quiz performance, and task efficiency without increasing cognitive
load. Learners expressed high trust in our system and valued its clarity,
usefulness, and ease of navigation. Our findings point to new design
opportunities for integrating generative AI into educational tools that
prioritize usability, learner agency, and pedagogical alignment.

</details>


### [67] [Material Experience: An Evaluation Model for Creative Materials Based on Visual-Tactile Sensory Properties](https://arxiv.org/abs/2509.06114)
*Yuxin Zhang,Fan Zhang,Jinjun Xia,Chao Zhao*

Main category: cs.HC

TL;DR: 研究采用设计导向方法，将传统编织与常用基质材料结合，通过改变材料类型和编织模式开发具有不同感官特性的创意材料。提出定量结构化模型，分析材料属性与感官感知关系，指导材料选择。结果显示创意材料显著影响印象评价，其吸引力由内在、审美和物理属性共同决定。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过结合传统编织和现代基质材料，开发更具感官吸引力的创意材料，并为设计师提供量化工具以优化材料选择。

Method: 使用7点语义差异量表评估材料在视觉-触觉条件下的感官感知，通过相关分析、双向方差分析和结构方程模型分析数据。

Result: 创意材料显著改变了印象评价，其吸引力受内在（0.486）、审美（0.650）和物理属性（0.103）共同影响。

Conclusion: 设计师应利用材料属性间的关系增强感官体验，同时平衡技术与体验，遵循“形式追随功能”原则。

Abstract: This study adopts a design-oriented approach to integrate traditional braids
with commonly used matrix materials, developing creative materials with
different sensory properties by altering matrix material types and braid
patterns. Based on these creative materials, a quantitative and structured
model is proposed to assist designers understanding the material experience
process and guide material selection by analyzing the relationship between
material properties and sensory perception. Specifically, participants
evaluated the creative materials under visual-tactile conditions using a
7-point semantic differential (SD) scale. Correlation analysis was performed to
explore the data. The main and interaction effects of matrix materials and
braid patterns on impression evaluation were analyzed using two-way analysis of
variance (ANOVA). A structural equation model (SEM) was constructed based on
exploratory factor analysis (EFA), and path coefficients were computed to
assess the relative importance of material properties in determining material
attractiveness. The results show that, compared to braids, the creative
materials resulted in significant changes in impression evaluation.
Furthermore, the creative materials can be understood through intrinsic,
aesthetic, and physical properties, with their standardized regression
coefficients for material attractiveness of 0.486, 0.650, and 0.103,
respectively. These properties are interrelated and under their combined
influence affect the attractiveness of the material. Therefore, designers
should consider utilizing these relationships to enhance sensory experience in
order to achieve design objectives. Moreover, designers should also consider
balancing technology and experience, using materials according to the principle
of "form follows function".

</details>


### [68] [Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn Multimodal LLM Conversation](https://arxiv.org/abs/2509.06382)
*Yingke Ding,Zeyu Wang,Xiyuxing Zhang,Hongbin Chen,Zhenan Xu*

Main category: cs.HC

TL;DR: 提出了名为CAFA的上下文自适应助听器调整系统。


<details>
  <summary>Details</summary>
Motivation: 传统助听器静态调整无法适应动态环境。

Method: 结合多模态数据和多智能体LLM工作流。

Result: 实时声音分类准确率为91.2%，提升了对话效率。

Conclusion: 展示了智能、用户为中心的辅助技术潜力。

Abstract: Traditional hearing aids often rely on static fittings that fail to adapt to
their dynamic acoustic environments. We propose CAFA, a Context-Adaptive
Fitting Advisor that provides personalized, real-time hearing aid adjustments
through a multi-agent Large Language Model (LLM) workflow. CAFA combines live
ambient audio, audiograms, and user feedback in a multi-turn conversational
system. Ambient sound is classified into conversation, noise, or quiet with
91.2\% accuracy using a lightweight neural network based on YAMNet embeddings.
This system utilizes a modular LLM workflow, comprising context acquisition,
subproblem classification, strategy provision, and ethical regulation, and is
overseen by an LLM Judge. The workflow translates context and feedback into
precise, safe tuning commands. Evaluation confirms that real-time sound
classification enhances conversational efficiency. CAFA exemplifies how
agentic, multimodal AI can enable intelligent, user-centric assistive
technologies.

</details>


### [69] [Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support](https://arxiv.org/abs/2509.06393)
*Mehrnoosh Sadat Shirvani,Jackie Liu,Thomas Chao,Suky Martinez,Laura Brandt,Ig-Jae Kim,Dongwook Yoon*

Main category: cs.HC

TL;DR: 该论文研究了利用AI驱动的自我克隆聊天机器人提高心理健康干预的用户参与度，相较传统通用咨询机器人，显著提升了情感与认知参与度。


<details>
  <summary>Details</summary>
Motivation: 解决心理健康对话机器人用户参与度低的问题，借鉴用户内部对话的方式，提出外部化自我对话的新方法。

Method: 设计了自我克隆聊天机器人，模仿用户的支持策略与对话模式，并通过半控制实验（N=180）验证其效果。

Result: 实验显示，自我克隆聊天机器人比通用咨询机器人显著提高了情感与认知参与度。

Conclusion: 自我克隆聊天机器人在心理健康干预中具有潜力，关键是平衡真实自我表达与积极互动。

Abstract: Mental health conversational agents have the potential to deliver valuable
therapeutic impact, but low user engagement remains a critical barrier
hindering their efficacy. Existing therapeutic approaches have leveraged
clients' internal dialogues (e.g., journaling, talking to an empty chair) to
enhance engagement through accountable, self-sourced support. Inspired by
these, we designed novel AI-driven self-clone chatbots that replicate users'
support strategies and conversational patterns to improve therapeutic
engagement through externalized meaningful self-conversation. Validated through
a semi-controlled experiment (N=180), significantly higher emotional and
cognitive engagement was demonstrated with self-clone chatbots than a chatbot
with a generic counselor persona. Our findings highlight self-clone
believability as a mediator and emphasize the balance required in maintaining
convincing self-representation while creating positive interactions. This study
contributes to AI-based mental health interventions by introducing and
evaluating self-clones as a promising approach to increasing user engagement,
while exploring implications for their application in mental health care.

</details>


### [70] [Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems](https://arxiv.org/abs/2509.06475)
*Yannick Kalff,Katharina Simbeck*

Main category: cs.HC

TL;DR: 研究探讨了HR管理者的AI素养如何影响他们对可解释AI（XAI）的主观感知与客观理解，发现XAI功能对高素养用户有帮助，但需定制化策略及培训。


<details>
  <summary>Details</summary>
Motivation: 随着AI推荐系统在招聘中的作用增强，研究透明度和负责任的使用在HRM中的重要性。

Method: 通过在线实验，410名德国HR管理者对比了基础版与三种XAI风格的推荐仪表盘。

Result: XAI改进主观感知（帮助与信任），但未提升客观理解；高素养用户仅从重要特征覆盖中受益。

Conclusion: XAI的益处取决于用户AI素养，需定制化解释策略和培训以实现公平透明。

Abstract: AI-based recommender systems increasingly influence recruitment decisions.
Thus, transparency and responsible adoption in Human Resource Management (HRM)
are critical. This study examines how HR managers' AI literacy influences their
subjective perception and objective understanding of explainable AI (XAI)
elements in recruiting recommender dashboards. In an online experiment, 410
German-based HR managers compared baseline dashboards to versions enriched with
three XAI styles: important features, counterfactuals, and model criteria. Our
results show that the dashboards used in practice do not explain AI results and
even keep AI elements opaque. However, while adding XAI features improves
subjective perceptions of helpfulness and trust among users with moderate or
high AI literacy, it does not increase their objective understanding. It may
even reduce accurate understanding, especially with complex explanations. Only
overlays of important features significantly aided the interpretations of
high-literacy users. Our findings highlight that the benefits of XAI in
recruitment depend on users' AI literacy, emphasizing the need for tailored
explanation strategies and targeted literacy training in HRM to ensure fair,
transparent, and effective adoption of AI.

</details>


### [71] [Mapping Community Appeals Systems: Lessons for Community-led Moderation in Multi-Level Governance](https://arxiv.org/abs/2509.06557)
*Juhoon Lee,Bich Ngoc Doan,Jonghyun Jee,Joseph Seering*

Main category: cs.HC

TL;DR: 研究探讨了Discord社区如何通过申诉系统实现社区主导的治理，平衡规模化和公平性，同时维护社区核心价值。


<details>
  <summary>Details</summary>
Motivation: 平台普遍采用工业化审核模式，忽视了用户需求和上下文敏感性，因此研究社区申诉系统以探索更好的治理方式。

Method: 通过焦点小组和个人访谈，对17位社区版主进行定性研究，分析Discord申诉系统的运作机制。

Result: 研究发现，社区申诉系统在规模化、公平性和问责制之间取得平衡，同时支持用户成长与康复。

Conclusion: 社区主导的治理模式可提供实用经验，帮助平台设计兼顾多方利益的治理结构。

Abstract: Platforms are increasingly adopting industrial models of moderation that
prioritize scalability and consistency, frequently at the expense of
context-sensitive and user-centered values. Building on the multi-level
governance framework that examines the interdependent relationship between
platforms and middle-level communities, we investigate community appeals
systems on Discord as a model for successful community-led governance. We
investigate how Discord servers operationalize appeal systems through a
qualitative interview study with focus groups and individual interviews with 17
community moderators. Our findings reveal a structured appeals process that
balances scalability, fairness, and accountability while upholding
community-centered values of growth and rehabilitation. Communities design
these processes to empower users, ensuring their voices are heard in moderation
decisions and fostering a sense of belonging. This research provides insights
into the practical implementation of community-led governance in a multi-level
governance framework, illustrating how communities can maintain their core
principles while integrating procedural fairness and tool-based design. We
discuss how platforms can gain insights from community-led moderation work to
motivate governance structures that effectively balance and align the interests
of multiple stakeholders.

</details>


### [72] ["It was Tragic": Exploring the Impact of a Robot's Shutdown](https://arxiv.org/abs/2509.06934)
*Agam Oberlender,Hadas Erel*

Main category: cs.HC

TL;DR: 人们倾向于将机器人视为社交实体，即使它们并非为社交互动设计。实验显示，机器人的关闭动作会影响用户对其的感知，设计自然的关闭动作（如“入睡”）比突然关闭（“死亡”）更能提升好感度和智能感知。


<details>
  <summary>Details</summary>
Motivation: 探讨机器人关闭时的社交化动作如何影响用户对机器人的感知，尤其是当人们倾向于将机器人视为社交实体时。

Method: 参与者与机器人进行中性互动后，在两种条件下关闭机器人：非设计条件（突然关闭）和设计条件（渐近关闭如“入睡”）。

Result: 设计条件下的关闭动作被更中性解读（“入睡”），提升了好感度、智能感知和活力；非设计条件下解读为负面（“死亡”）。

Conclusion: 机器人关闭动作的设计应考虑人们的社交化倾向，以提升用户体验和机器人感知。

Abstract: It is well established that people perceive robots as social entities, even
when they are not designed for social interaction. We evaluated whether the
social interpretation of robotic gestures should also be considered when
turning off a robot. In the experiment, participants engaged in a brief
preliminary neutral interaction while a robotic arm showed interest in their
actions. At the end of the task, participants were asked to turn off the
robotic arm under two conditions: (1) a Non-designed condition, where all of
the robot's engines were immediately and simultaneously turned off, as robots
typically shut down; (2) a Designed condition, where the robot's engines
gradually folded inward in a motion resembling "falling asleep." Our findings
revealed that all participants anthropomorphized the robot's movement when it
was turned off. In the Non-designed condition, most participants interpreted
the robot's turn-off movement negatively, as if the robot had "died." In the
Designed condition, most participants interpreted it more neutrally, stating
that the robot "went to sleep." The robot's turn-off movement also impacted its
perception, leading to higher likeability, perceived intelligence, and animacy
in the Designed condition. We conclude that the impact of common edge
interactions, such as turning off a robot, should be carefully designed while
considering people's automatic tendency to perceive robots as social entities.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [73] [PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly Modeling on the GPU](https://arxiv.org/abs/2509.05595)
*Seonghun Oh,Xiaodi Yuan,Xinyue Wei,Ruoxi Shi,Fanbo Xiang,Minghua Liu,Hao Su*

Main category: cs.GR

TL;DR: 提出了一种基于GPU的网格优化方法，解决现有网格简化技术中的自交、表面偏移和效率问题，实现高速且高质量的网格简化。


<details>
  <summary>Details</summary>
Motivation: 现有网格简化方法存在自交、表面偏移和低效问题，亟需一种高效且高质量的解决方案。

Method: 采用并行重网格化算法、并行简化算法和基于优化的安全投影算法，确保无自交并恢复原始特征。

Result: 在RTX4090上，2百万面的网格在3秒内简化为2万面，性能优于现有方法。

Conclusion: 该方法在几何保留和速度方面表现优异，适用于大规模网格处理。

Abstract: Reducing the triangle count in complex 3D models is a basic geometry
preprocessing step in graphics pipelines such as efficient rendering and
interactive editing. However, most existing mesh simplification methods exhibit
a few issues. Firstly, they often lead to self-intersections during decimation,
a major issue for applications such as 3D printing and soft-body simulation.
Second, to perform simplification on a mesh in the wild, one would first need
to perform re-meshing, which often suffers from surface shifts and losses of
sharp features. Finally, existing re-meshing and simplification methods can
take minutes when processing large-scale meshes, limiting their applications in
practice. To address the challenges, we introduce a novel GPU-based mesh
optimization approach containing three key components: (1) a parallel
re-meshing algorithm to turn meshes in the wild into watertight, manifold, and
intersection-free ones, and reduce the prevalence of poorly shaped triangles;
(2) a robust parallel simplification algorithm with intersection-free
guarantees; (3) an optimization-based safe projection algorithm to realign the
simplified mesh with the input, eliminating the surface shift introduced by
re-meshing and recovering the original sharp features. The algorithm
demonstrates remarkable efficiency, simplifying a 2-million-face mesh to 20k
triangles in 3 seconds on RTX4090. We evaluated the approach on the Thingi10K
dataset and showcased its exceptional performance in geometry preservation and
speed.

</details>


### [74] [Programming tension in 3D printed networks inspired by spiderwebs](https://arxiv.org/abs/2509.05855)
*Thijs Masmeijer,Caleb Swain,Jeff Hill,Ed Habtour*

Main category: cs.GR

TL;DR: 提出了一个直接3D打印张力网络结构的算法，通过编程张力梯度克服制造过程中的不准确性，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究解决了张力结构网络在制造和组装过程中因扁平化而导致的张力梯度不准确性问题。

Method: 使用力密度法定义网络和张力梯度，优化顶点位置和元素形状，分解为可打印路径，处理扁平化和交叉问题。

Result: 实验显示张力梯度准确（应变误差<1.0%），适用于5.8毫米最小长度和7.3兆帕最大应力的网络。

Conclusion: 该算法可制造复杂张力结构，用于医疗支架等新应用。

Abstract: Each element in tensioned structural networks -- such as tensegrity,
architectural fabrics, or medical braces/meshes -- requires a specific tension
level to achieve and maintain the desired shape, stability, and compliance.
These structures are challenging to manufacture, 3D print, or assemble because
flattening the network during fabrication introduces multiplicative
inaccuracies in the network's final tension gradients. This study overcomes
this challenge by offering a fabrication algorithm for direct 3D printing of
such networks with programmed tension gradients, an approach analogous to the
spinning of spiderwebs. The algorithm: (i) defines the desired network and
prescribes its tension gradients using the force density method; (ii) converts
the network into an unstretched counterpart by numerically optimizing vertex
locations toward target element lengths and converting straight elements into
arcs to resolve any remaining error; and (iii) decomposes the network into
printable toolpaths; Optional additional steps are: (iv) flattening curved 2D
networks or 3D networks to ensure 3D printing compatibility; and (v)
automatically resolving any unwanted crossings introduced by the flattening
process. The proposed method is experimentally validated using 2D unit cells of
viscoelastic filaments, where accurate tension gradients are achieved with an
average element strain error of less than 1.0\%. The method remains effective
for networks with element minimum length and maximum stress of 5.8 mm and 7.3
MPa, respectively. The method is used to demonstrate the fabrication of three
complex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The
programmable tension gradient algorithm can be utilized to produce compact,
integrated cable networks, enabling novel applications such as moment-exerting
structures in medical braces and splints.

</details>


### [75] [From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters](https://arxiv.org/abs/2509.06573)
*Jie Zhou,Linzi Qu,Miu-Ling Lam,Hongbo Fu*

Main category: cs.GR

TL;DR: 本文提出了一种结合骨骼动画和视频扩散的混合动画系统，解决了手绘角色动画中几何一致性与动态表现力的矛盾，通过二次动态注入和头发分层建模技术显著提升了动画质量。


<details>
  <summary>Details</summary>
Motivation: 传统骨骼动画难以处理复杂非刚性元素（如头发和裙子），而视频扩散模型会导致几何失真。本文旨在结合两者优势，实现高质量的角色动画。

Method: 系统首先生成骨骼动画的粗略图像，再使用视频扩散模型进行纹理增强和二次动态处理，并通过域适应扩散模型优化用户标记区域。此外，引入了二次动态注入策略和头发分层建模技术。

Result: 实验表明，该系统在定量和定性评估中均优于现有方法，显著提升了动态真实感和几何一致性。

Conclusion: 该混合系统成功结合了骨骼动画与视频扩散的优势，为手绘角色动画提供了高质量的解决方案。

Abstract: Hand-drawn character animation is a vibrant field in computer graphics,
presenting challenges in achieving geometric consistency while conveying
expressive motion. Traditional skeletal animation methods maintain geometric
consistency but struggle with complex non-rigid elements like flowing hair and
skirts, leading to unnatural deformation. Conversely, video diffusion models
synthesize realistic dynamics but often create geometric distortions in
stylized drawings due to domain gaps. This work proposes a hybrid animation
system that combines skeletal animation and video diffusion. Initially, coarse
images are generated from characters retargeted with skeletal animations for
geometric guidance. These images are then enhanced in texture and secondary
dynamics using video diffusion priors, framing this enhancement as an
inpainting task. A domain-adapted diffusion model refines user-masked regions
needing improvement, especially for secondary dynamics. To enhance motion
realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in
the denoising process, incorporating features from a pre-trained diffusion
model enriched with human motion priors. Additionally, to tackle unnatural
deformations from low-poly single-mesh character modeling, we present a Hair
Layering Modeling (HLM) technique that uses segmentation maps to separate hair
from the body, allowing for more natural animation of long-haired characters.
Extensive experiments show that our system outperforms state-of-the-art methods
in both quantitative and qualitative evaluations.

</details>


### [76] [From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans](https://arxiv.org/abs/2509.06607)
*Marilyn Keller,Keenon Werling,Soyong Shin,Scott Delp,Sergi Pujades,C. Karen Liu,Michael J. Black*

Main category: cs.GR

TL;DR: SKEL模型通过优化SMPL网格内的生物力学骨骼，提供了一个更精确且易于操作的参数化人体模型。


<details>
  <summary>Details</summary>
Motivation: 现有的人体模型（如SMPL）的关节结构过于简化，无法满足生物力学需求，而精确的骨骼运动估计方法又过于复杂。SKEL旨在填补这一空白。

Method: 通过优化AMASS序列中的骨骼数据，训练从SMPL顶点到关节位置和骨骼旋转的回归器，并重新参数化SMPL网格。

Result: SKEL模型的关节位置更符合生物力学要求，骨骼与身体表面的贴合度优于以往方法。

Conclusion: SKEL为生物力学研究提供了新工具，同时也为视觉和图形学研究者提供了一个更真实的人体关节模型。

Abstract: Great progress has been made in estimating 3D human pose and shape from
images and video by training neural networks to directly regress the parameters
of parametric human models like SMPL. However, existing body models have
simplified kinematic structures that do not correspond to the true joint
locations and articulations in the human skeletal system, limiting their
potential use in biomechanics. On the other hand, methods for estimating
biomechanically accurate skeletal motion typically rely on complex motion
capture systems and expensive optimization methods. What is needed is a
parametric 3D human model with a biomechanically accurate skeletal structure
that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL
body model with a biomechanics skeleton. To enable this, we need training data
of skeletons inside SMPL meshes in diverse poses.
  We build such a dataset by optimizing biomechanically accurate skeletons
inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL
mesh vertices to the optimized joint locations and bone rotations. Finally, we
re-parametrize the SMPL mesh with the new kinematic parameters. The resulting
SKEL model is animatable like SMPL but with fewer, and
biomechanically-realistic, degrees of freedom. We show that SKEL has more
biomechanically accurate joint locations than SMPL, and the bones fit inside
the body surface better than previous methods. By fitting SKEL to SMPL meshes
we are able to "upgrade" existing human pose and shape datasets to include
biomechanical parameters. SKEL provides a new tool to enable biomechanics in
the wild, while also providing vision and graphics researchers with a better
constrained and more realistic model of human articulation. The model, code,
and data are available for research at https://skel.is.tue.mpg.de..

</details>


### [77] [Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data](https://arxiv.org/abs/2509.06950)
*Nithin Gopalakrishnan Nair,Srinivas Kaza,Xuan Luo,Vishal M. Patel,Stephen Lombardi,Jungyeon Park*

Main category: cs.GR

TL;DR: 通过结合扩散模型生成的合成数据和改进的变压器架构，提出了一种新的通用性视图合成方法，显著提升了重建质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模变压器模型在通用性视图合成中受限于公开场景数据集的有限多样性，难以处理现实世界中的分布外场景。

Method: 引入扩散模型生成的合成数据，并提出变压器架构中的令牌解耦过程，以提升特征分离和学习效果。

Result: 该方法在数据集内和跨数据集评估中均优于现有模型，显著降低计算成本。

Conclusion: 通过合成数据和架构改进，实现了高效的通用性视图合成，适用于多样化的场景。

Abstract: Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization. However, these
models are constrained by the limited diversity of publicly available scene
datasets, making most real-world (in-the-wild) scenes out-of-distribution. To
overcome this, we incorporate synthetic training data generated from diffusion
models, which improves generalization across unseen domains. While synthetic
data offers scalability, we identify artifacts introduced during data
generation as a key bottleneck affecting reconstruction quality. To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning. This refinement not only improves reconstruction quality over
standard transformers but also enables scalable training with synthetic data.
As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs. Project page:
https://scaling3dnvs.github.io/

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [78] [SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips](https://arxiv.org/abs/2509.05532)
*Changxu Song,Arda Caliskan,Beyza Zeynep Ucpinar,Yasemin Kopur,Mustafa Altay Karamuftuoglu,Sasan Razmkhah,Shahin Nazarian,Massoud Pedram*

Main category: cs.ET

TL;DR: SuperSNN 是一个针对超导神经网络（SNN）的框架，考虑了实际制造约束，实现了芯片上的高效实现，并在 MNIST 数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的超导神经网络设计大多忽视了实际制造约束，导致只能实现少量神经元或突触。SuperSNN 旨在解决这一限制。

Method: 框架包括硬件感知训练方法、高扇入神经元设计、定制超导单元和优化的时钟分配方案。

Result: 实现了 96.47% 的 MNIST 分类准确率，芯片分类准确率达 80.07%-86.2%，运行频率 3.02 GHz，功耗极低。

Conclusion: SuperSNN 成功在超导芯片上实现了高效、高性能的神经网络，为实际应用提供了可行方案。

Abstract: Despite numerous proposed designs for superconducting neural networks (SNNs),
most have overlooked practical fabrication constraints, leading to
implementations limited to only a few neurons or synapses. Current
superconducting technologies, such as MIT LL SFQ5ee, impose severe limitations
on chip area, routing, and input/output pin counts (e.g., 5x5 mm^2 chip with 40
pins), drastically restricting network size and complexity. These hardware
constraints necessitate a comprehensive framework to tailor network designs for
physical realizability while minimizing accuracy loss. This paper introduces
SuperSNN, a comprehensive framework for the implementation of full
superconducting SNNs on a chip within these constraints. The key technical
contributions include: (1) A hardware-aware training methodology for SNNs,
utilizing off-chip pruning and weight quantization for energy-efficient
superconducting implementations. (2) Design and layout of an inference SNN chip
that incorporates novel high fan-in neurons and custom superconducting cells.
(3) An optimized locally synchronous, globally synchronous (LAGS) clock
distribution scheme for robust circuit implementation and management of data
transfer delays in SFQ SNNs. The main results and findings demonstrate the
effectiveness of the framework: (1) The complete network achieved 96.47%
accuracy on the full MNIST dataset after quantization and pruning. (2) The
fabricated SuperSNN chip successfully classified a reduced set of digits (2, 3,
and 4) with 80.07% accuracy, reaching a maximum of 86.2% accuracy for digits 0,
1, and 2. (3) The chip operates at an ultra-high 3.02 GHz clock frequency. (4)
It occupies a compact area of 3.4 x 3.9 mm^2, incorporates 5,822 Josephson
Junctions, consumes 2.15 mW static power, and has an exceptionally low energy
cost of 6.55 fJ (or 1.31e-6 nJ) per inference.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [79] [Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats](https://arxiv.org/abs/2509.05303)
*Sam Davidson,Li Sun,Bhavana Bhasker,Laurent Callot,Anoop Deoras*

Main category: cs.DC

TL;DR: 论文提出了Multi-IaC-Bench，一个用于评估LLM在不同IaC格式（如AWS CloudFormation、Terraform和CDK）中生成和修改代码能力的基准数据集。尽管现代LLM在语法正确性上表现良好（成功率>95%），但在语义对齐和复杂模式处理上仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 不同云服务提供商使用多样化的IaC格式，缺乏标准化增加了云部署的复杂性，而现有的LLM研究缺乏全面的跨格式评估基准。

Method: 通过合成数据生成管道和严格验证，创建包含初始模板、修改请求和更新模板的三元组数据集Multi-IaC-Bench，并评估多个SOTA LLM。

Result: 现代LLM在生成语法正确的IaC代码方面表现出色，但在语义对齐和复杂模式处理上仍有显著挑战。提示工程和重试机制对成功生成IaC至关重要。

Conclusion: Multi-IaC-Bench的发布将促进AI辅助基础设施管理的研究，并为该领域建立标准化评估指标。

Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing,
enabling teams to define and manage infrastructure through machine-readable
configuration files. However, different cloud service providers utilize diverse
IaC formats. The lack of a standardized format requires cloud architects to be
proficient in multiple IaC languages, adding complexity to cloud deployment.
While Large Language Models (LLMs) show promise in automating IaC creation and
maintenance, progress has been limited by the lack of comprehensive benchmarks
across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark
dataset for evaluating LLM-based IaC generation and mutation across AWS
CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset
consists of triplets containing initial IaC templates, natural language
modification requests, and corresponding updated templates, created through a
synthetic data generation pipeline with rigorous validation. We evaluate
several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while
modern LLMs can achieve high success rates (>95%) in generating syntactically
valid IaC across formats, significant challenges remain in semantic alignment
and handling complex infrastructure patterns. Our ablation studies highlight
the importance of prompt engineering and retry mechanisms in successful IaC
generation. We release Multi-IaC-Bench to facilitate further research in
AI-assisted infrastructure management and establish standardized evaluation
metrics for this crucial domain.

</details>


### [80] [A Simple and Robust Protocol for Distributed Counting](https://arxiv.org/abs/2509.05870)
*Edith Cohen,Moshe Shechner,Uri Stemmer*

Main category: cs.DC

TL;DR: 该论文重新审视分布式计数问题，提出了一种新的简单且鲁棒的协议，解决了之前协议在适应性攻击下的不鲁棒问题，并达到了最优通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 分布式计数问题的通信复杂度在确定性协议中较高，随机化协议在非适应性设置下表现更好，但之前的鲁棒协议复杂度较高且复杂。论文旨在验证和解决这个问题。

Method: 论文首先分析了Huang等人（2012）协议在适应性攻击下的不鲁棒性，然后提出了一种新的简单协议，实现了最优通信复杂度。

Result: 新协议不仅在适应性攻击下保持鲁棒性，而且复杂度与最优的非适应性协议一致，甚至更简单。

Conclusion: 论文通过实验和理论分析，验证了新协议的高效性和鲁棒性，为分布式计数问题提供了更优的解决方案。

Abstract: We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.

</details>


### [81] [DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers](https://arxiv.org/abs/2509.06046)
*Philip Adams,Menghao Li,Shi Zhang,Li Tan,Qi Chen,Mingqin Li,Zengzhong Li,Knut Risvik,Harsha Vardhan Simhadri*

Main category: cs.DC

TL;DR: DISTRIBUTEDANN是一种分布式向量搜索服务，能在50亿向量的图谱索引上实现26毫秒的中位数查询延迟，每秒处理超过10万次查询，比现有方法高效6倍。


<details>
  <summary>Details</summary>
Motivation: 现有分区和路由策略在扩展向量搜索系统中效率不足，需要一种更高效的解决方案。

Method: 结合分布式键值存储器和内存中的近似最近邻（ANN）索引构建DISTRIBUTEDANN。

Result: 成功应用于Bing搜索引擎，取代传统扩展架构，显著提升查询效率和吞吐量。

Conclusion: DISTRIBUTEDANN通过分布式设计实现了高效的向量搜索，适用于大规模应用。

Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

</details>


### [82] [Gathering in Non-Vertex-Transitive Graphs Under Round Robin](https://arxiv.org/abs/2509.06064)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究了在非顶点传递图中机器人聚集问题，提出了一种解决算法并分析了其时间复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究在初始配置可能包含多重性且机器人无法检测多重性的‘敌对’条件下，如何在非顶点传递图中实现机器人聚集。

Method: 采用轮询调度的分布式算法，机器人一次激活一个，在非顶点传递图中移动。

Result: 提出了一种适用于任何机器人配置的解决算法，并证明了其正确性和时间复杂性。

Conclusion: 在非顶点传递图中，提出的算法能够有效解决机器人聚集问题。

Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm
that brings such entities to a common place, not known in advance. We consider
the well-known OBLOT model with robots constrained to move along the edges of a
graph, hence gathering in one vertex, eventually. Despite the classical setting
under which the problem has been usually approached, we consider the `hostile'
case where: i) the initial configuration may contain multiplicities, i.e. more
than one robot may occupy the same vertex; ii) robots cannot detect
multiplicities. As a scheduler for robot activation, we consider the
"favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the
broad context of non-vertex-transitive graphs, i.e., graphs where the vertices
are partitioned into at least two different classes of equivalence. We provide
a resolution algorithm for any configuration of robots moving on such graphs,
along with its correctness. Furthermore, we analyze its time complexity.

</details>


### [83] [20 Years in Life of a Smart Building: A retrospective](https://arxiv.org/abs/2509.06229)
*Karolina Skrivankova,Mark Handley,Stephen Hailes*

Main category: cs.DC

TL;DR: KaOS是一个分布式控制平台，旨在解决2025年智能建筑自动化系统中的硬件故障、供应商淘汰和安全威胁等问题。


<details>
  <summary>Details</summary>
Motivation: 工业建筑和家庭自动化行业未能全面应对硬件故障、供应商淘汰和不断演变的安全威胁等问题，限制了大规模智能自动化部署的可行性。

Method: 利用容器化和有管理的资源访问，KaOS支持控制应用和分布式系统操作，目标是实现灵活性、安全性和容错性，同时保持成本效益。

Result: 初步评估证实了该方法的实际可行性，展示了其在长期维护和逐步升级建筑控制功能方面的潜力。

Conclusion: KaOS为构建稳健且可演进的智能建筑自动化系统提供了实用的解决方案，能够在成本效益的前提下满足灵活性、安全性和容错性需求。

Abstract: Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.

</details>


### [84] [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)
*Kyungmin Bin,Seungbeom Choi,Jimyoung Son,Jieun Choi,Daseul Bae,Daehyeon Baek,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.DC

TL;DR: FineServe是一个针对混合精度大语言模型的推断服务框架，通过KV Slab内存管理和两级调度框架，显著提高了内存效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 量化后的LLMs虽然减少了内存使用并提高了GPU资源利用率，但由于KV块大小较小，容易导致内存碎片化，同时与非量化模型的资源使用模式不同，需要高效的调度策略来最大化吞吐量。

Method: FineServe提出KV Slab技术动态管理KV缓存以减少内存碎片化，并设计了一个两级调度框架，包括全局调度器和本地调度器，分别负责模型放置和批处理大小调整。

Result: 实验结果显示，FineServe相比现有GPU共享系统，SLO达成率提高了2.2倍，令牌生成吞吐量提高了1.8倍。

Conclusion: FineServe通过创新的内存管理和调度策略，有效解决了量化LLMs在推断服务中的内存碎片化和调度效率问题，显著提升了性能。

Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have
significantly increased demand for serving quantized large language models
(LLMs), enabling higher throughput and substantially reduced memory usage with
minimal accuracy loss. Quantized models address memory constraints in LLMs and
enhance GPU resource utilization through efficient GPU sharing. However,
quantized models have smaller KV block sizes than non-quantized models, causing
limited memory efficiency due to memory fragmentation. Also, distinct resource
usage patterns between quantized and non-quantized models require efficient
scheduling to maximize throughput. To address these challenges, we propose
FineServe, an inference serving framework for mixed-precision LLMs. FineServe's
key contributions include: (1) KV Slab, a precision-aware adaptive memory
management technique dynamically allocating KV cache based on model
quantization characteristics, significantly reducing GPU memory fragmentation,
and (2) a two-level scheduling framework comprising a global scheduler that
places models to GPUs based on request rates, latency SLOs, and memory
constraints and efficiency, and a local scheduler that adaptively adjusts batch
sizes according to real-time request fluctuations. Experimental results
demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x
higher token generation throughput compared to the state-of-the-art GPU sharing
systems.

</details>


### [85] [MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS](https://arxiv.org/abs/2509.06362)
*Mo Xuan,Zhang yue,Wu Weigang*

Main category: cs.DC

TL;DR: MaaSO是一个新型MaaS编排器，通过优化LLM实例配置和请求分发，显著提升SLO满意度和响应速度。


<details>
  <summary>Details</summary>
Motivation: MaaS平台面临多样化的SLO需求，而现有LLM推理系统缺乏利用异构配置的机制。

Method: 设计三个模块：性能分析器、异构实例配置优化器、SLO感知请求分发器。

Result: SLO满意度提升15-30%，响应延迟降低40-60%，编排开销显著减少。

Conclusion: MaaSO有效解决了MaaS平台中的SLO需求多样性和异构配置利用问题。

Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective
(SLO) requirements stemming from various large language model (LLM)
applications, manifested in contextual complexity, first-token latency, and
between-token latency. On the other hand, an LLM instance, when configured with
different parallelism strategies and inference batch sizes, exhibits distinct
performance characteristics and can thus be used to serve different SLO
requirements. However, current LLM inference systems typically deploy instances
of the same model with identical configurations, lacking mechanisms to leverage
such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS
Orchestrator, which comprises three modules: (1) a profiler characterizing
instance performance under diverse parallelism strategies and inference batch
sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a
distributor enabling SLO-aware request distribution and preventing cascaded
timeouts in continuous batching. Experiments show that MaaSO improves the SLO
satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%
compared to existing approaches, and significantly lowers overall orchestration
overhead.

</details>


### [86] [IM-PIR: In-Memory Private Information Retrieval](https://arxiv.org/abs/2509.06514)
*Mpoki Mwaisela,Peterson Yuhala,Pascal Felber,Valerio Schiavoni*

Main category: cs.DC

TL;DR: 该论文提出了基于内存计算（PIM）的多服务器私有信息检索（PIR）架构IM-PIR，显著提高了查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前PIR实现因计算成本高和内存带宽限制而性能受限，PIM架构因其高并行性和内存带宽优势有望解决这一问题。

Method: 设计并实现了基于UPMEM PIM架构的IM-PIR，利用PIM的并行性和高内存带宽优化多服务器PIR操作。

Result: 与传统的CPU-based PIR相比，IM-PIR的查询吞吐量提高了3.7倍以上。

Conclusion: PIM架构为PIR提供了有效的性能优化方案，IM-PIR展示了PIM在多服务器PIR中的潜力。

Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows
a client to securely query one or multiple servers without revealing their
specific interests. In spite of their strong security guarantees, current PIR
constructions are computationally costly. Specifically, most PIR
implementations are memory-bound due to the need to scan extensive databases
(in the order of GB), making them inherently constrained by the limited memory
bandwidth in traditional processor-centric computing
architectures.Processing-in-memory (PIM) is an emerging computing paradigm that
augments memory with compute capabilities, addressing the memory bandwidth
bottleneck while simultaneously providing extensive parallelism.Recent research
has demonstrated PIM's potential to significantly improve performance across a
range of data-intensive workloads, including graph processing, genome analysis,
and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server
PIR. We discuss the algorithmic foundations of the latter and show how its
operations align with the core strengths of PIM architectures: extensive
parallelism and high memory bandwidth. Based on this observation, we design and
implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,
the first openly commercialized PIM architecture. Our evaluation demonstrates
that a PIM-based multi-server PIR implementation significantly improves query
throughput by more than 3.7x when compared to a standard CPU-based PIR
approach.

</details>


### [87] [Mangrove: Fast and Parallelizable State Replication for Blockchains](https://arxiv.org/abs/2509.06616)
*Anton Paramonov,Yann Vonlanthen,Quentin Kniep,Jakub Sliwinski,Roger Wattenhofer*

Main category: cs.DC

TL;DR: Mangrove是一种新型的区块链扩展方法，支持并行智能合约。它通过为每个智能合约使用独立的共识实例，避免全局顺序，并提出Parallel Optimistic Agreement机制来防止冲突。


<details>
  <summary>Details</summary>
Motivation: 传统单片区块链的单一共识机制限制了交易顺序的灵活性，Mangrove旨在通过并行支持提高性能。

Method: 使用了Parallel Optimistic Agreement机制确保并行实例的安全性，并利用Byzantine Reliable Broadcast降低简单交易的延迟。

Result: 在乐观条件下（无恶意行为且网络同步），Mangrove实现了2步通信延迟的交易创建与执行。

Conclusion: Mangrove通过并行化和轻量级协议优化区块链性能，特别适合高吞吐场景。

Abstract: Mangrove is a novel scaling approach to building blockchains with parallel
smart contract support. Unlike in monolithic blockchains, where a single
consensus mechanism determines a strict total order over all transactions,
Mangrove uses separate consensus instances per smart contract, without a global
order. To allow multiple instances to run in parallel while ensuring that no
conflicting transactions are committed, we propose a mechanism called Parallel
Optimistic Agreement. Additionally, for simple transactions, we leverage a
lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove
is optimized for performance under optimistic conditions, where there is no
misbehavior and the network is synchronous. Under these conditions, our
protocol can achieve a latency of 2 communication steps between creating and
executing a transaction.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [88] [A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach](https://arxiv.org/abs/2509.06044)
*Lingxiao Kong,Apostolos Sarris,Miltiadis Polidorou,Victor Klingenberg,Vasilis Sevetlidis,Vasilis Arampatzakis,George Pavlidis,Cong Yang,Zeyd Boukhers*

Main category: cs.DB

TL;DR: 本文提出了一种用于文化遗产保护的数据历史性和迁移框架，通过标准化、丰富化和集成处理异构数据，提升数据可访问性和决策能力。


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护面临多源、多尺度数据管理的挑战，需要一种系统性方法处理复杂异构数据以支持监测和保护。

Method: 采用包含标准化、丰富化、集成、可视化和发布的系统化数据处理流程，结合FAIR原则和LLM自然语言处理技术。

Result: 在五个欧洲试点成功应用，显著提升数据可访问性、分析能力和保护决策效率。

Conclusion: 该框架能适应多样化文化遗产场景，有效支持数据驱动的保护工作。

Abstract: Cultural heritage preservation faces significant challenges in managing
diverse, multi-source, and multi-scale data for effective monitoring and
conservation. This paper documents a comprehensive data historicity and
migration framework implemented within the ARGUS project, which addresses the
complexities of processing heterogeneous cultural heritage data. We describe a
systematic data processing pipeline encompassing standardization, enrichment,
integration, visualization, ingestion, and publication strategies. The
framework transforms raw, disparate datasets into standardized formats
compliant with FAIR principles. It enhances sparse datasets through established
imputation techniques, ensures interoperability through database integration,
and improves querying capabilities through LLM-powered natural language
processing. This approach has been applied across five European pilot sites
with varying preservation challenges, demonstrating its adaptability to diverse
cultural heritage contexts. The implementation results show improved data
accessibility, enhanced analytical capabilities, and more effective
decision-making for conservation efforts.

</details>


### [89] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 提出了一种针对氮化硼纳米片（BNNS）聚合物导热复合材料的语言原生数据库，通过半结构化信息整合论文内容，支持语义检索，为LLM驱动的材料发现提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统化学与材料研究依赖语言描述，限制了数据库和机器学习的应用潜力，因此需要一种语言原生的数据库来更高效地整合和检索信息。

Method: 构建异质数据库，从论文中提取半结构化信息（制备、表征、理论计算等），并通过语义、关键词和值过滤进行复合检索。

Result: 系统能够生成准确、可验证的专家风格指导，支持高保真的检索增强生成（RAG）和工具增强代理。

Conclusion: 该框架为LLM驱动的材料发现提供了语言丰富的基础，实现了可操作的标准化流程（SOP）。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


### [90] [MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration](https://arxiv.org/abs/2509.06298)
*Zihan Yan,Rui Xi,Mengshu Hou*

Main category: cs.DB

TL;DR: MCTuner是一个高效的数据库参数调优框架，通过Mixture-of-Experts和空间分解算法减少无效探索，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统调优方法在高维参数空间中效率低下，且忽略了领域知识，导致调优成本高且性能不佳。

Method: MCTuner结合Mixture-of-Experts机制和空间分解算法，递归分区并应用贝叶斯优化以快速找到接近最优配置。

Result: 在OLAP、OLTP和HTAP基准测试中，MCTuner实现了最高19.2%的性能提升和1.4倍的更快配置发现。

Conclusion: MCTuner通过自适应调优和高效空间探索，显著优于现有方法，解决了高维参数调优的挑战。

Abstract: Database knob tuning is essential for optimizing the performance of modern
database management systems, which often expose hundreds of knobs with
continuous or categorical values. However, the large number of knobs and the
vast configuration space make it difficult to identify optimal settings
efficiently. Although learning-based tuning has shown promise, existing
approaches either ignore domain knowledge by relying solely on benchmark
feedback or struggle to explore the high-dimensional knob space, resulting in
high tuning costs and suboptimal performance. To address these challenges, we
propose MCTuner, an adaptive knob tuning framework that minimizes exploration
in ineffective regions of the configuration space. MCTuner employs a
Mixture-of-Experts (MoE) mechanism with specialized LLMs to identify
performance-critical knobs. In further, MCTuner introduces the first spatial
decomposition algorithm that recursively partitions the space into hierarchical
subspaces, on which Bayesian Optimization is performed to efficiently search
for near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,
and HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster
configuration discovery per iteration compared to state-of-the-art methods.

</details>


### [91] [Relational Algebras for Subset Selection and Optimisation](https://arxiv.org/abs/2509.06439)
*David Robert Pratten,Luke Mathieson,Fahimeh Ramezani*

Main category: cs.DB

TL;DR: 本文提出了一种统一的代数基础，用于关系查询中的子集选择和优化查询，通过引入关系幂运算和解决方案集，增强了表达能力和理论清晰度。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的关系查询语言，用户表达和查询优化器在子集选择和优化查询方面受限，本文旨在填补这一空白。

Method: 扩展关系代数以支持域关系和解决方案集，提供结构保持的翻译语义，将其转换为标准关系代数。

Result: 实现了与现有最强大方法相同的表达能力，同时提供了理论清晰度和组合性。

Conclusion: 本文提出的代数框架为子集选择和优化查询提供了统一的表达和优化基础，并通过多态SQL展示了其实用性。

Abstract: The database community lacks a unified relational query language for subset
selection and optimisation queries, limiting both user expression and query
optimiser reasoning about such problems. Decades of research (latterly under
the rubric of prescriptive analytics) have produced powerful evaluation
algorithms with incompatible, ad-hoc SQL extensions that specify and filter
through distinct mechanisms. We present the first unified algebraic foundation
for these queries, introducing relational exponentiation to complete the
fundamental algebraic operations alongside union (addition) and cross product
(multiplication). First, we extend relational algebra to complete domain
relations-relations defined by characteristic functions rather than explicit
extensions-achieving the expressiveness of NP-complete/hard problems, while
simultaneously providing query safety for finite inputs. Second, we introduce
solution sets, a higher-order relational algebra over sets of relations that
naturally expresses search spaces as functions f: Base to Decision, yielding
|Decision|^|Base| candidate relations. Third, we provide structure-preserving
translation semantics from solution sets to standard relational algebra,
enabling mechanical translation to existing evaluation algorithms. This
framework achieves the expressiveness of the most powerful prior approaches
while providing the theoretical clarity and compositional properties absent in
previous work. We demonstrate the capabilities these algebras open up through a
polymorphic SQL where standard clauses seamlessly express data management,
subset selection, and optimisation queries within a single paradigm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [92] [Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device](https://arxiv.org/abs/2509.05451)
*Niansong Zhang,Wenbo Zhu,Courtney Golden,Dan Ilan,Hongzheng Chen,Christopher Batten,Zhiru Zhang*

Main category: cs.AR

TL;DR: 本文分析了商用计算内SRAM设备GSI APU的性能和能效，提出优化框架和三种优化方法，使其在检索增强生成任务中显著优于CPU和GPU。


<details>
  <summary>Details</summary>
Motivation: 计算内SRAM架构有望提升数据密集型应用的性能和能效，但先前研究依赖模拟器或小规模原型，限制了对其实际潜力的理解。

Method: 提出了一个分析框架，用于建模性能权衡，并提出三种优化方法：通信感知的归约映射、合并DMA和广播友好数据布局。

Result: 优化后的计算内SRAM系统在大型语料库上的检索速度提升4.8–6.6倍，端到端延迟提升1.1–1.8倍，且比GPU能效提升54.4–117.9倍。

Conclusion: 计算内SRAM技术适用于复杂实际应用，研究为技术发展提供了指导。

Abstract: Compute-in-SRAM architectures offer a promising approach to achieving higher
performance and energy efficiency across a range of data-intensive
applications. However, prior evaluations have largely relied on simulators or
small prototypes, limiting the understanding of their real-world potential. In
this work, we present a comprehensive performance and energy characterization
of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.
We compare the GSI APU against established architectures, including CPUs and
GPUs, to quantify its energy efficiency and performance potential. We introduce
an analytical framework for general-purpose compute-in-SRAM devices that
reveals fundamental optimization principles by modeling performance trade-offs,
thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute
architectures requires careful data management. We address this by proposing
three optimizations: communication-aware reduction mapping, coalesced DMA, and
broadcast-friendly data layouts. When applied to retrieval-augmented generation
(RAG) over large corpora (10GB--200GB), these optimizations enable our
compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over
an optimized CPU baseline, improving end-to-end RAG latency by
1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using
a simulated HBM, while all other components are measured on the real
compute-in-SRAM device. Critically, this system matches the performance of an
NVIDIA A6000 GPU for RAG while being significantly more energy-efficient
(54.4$\times$-117.9$\times$ reduction). These findings validate the viability
of compute-in-SRAM for complex, real-world applications and provide guidance
for advancing the technology.

</details>


### [93] [High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator](https://arxiv.org/abs/2509.05688)
*Kuan-Ting Lin,Ching-Te Chiu,Jheng-Yi Chang,Shi-Zong Huang,Yu-Ting Li*

Main category: cs.AR

TL;DR: 本文提出了一种针对边缘设备的高效实时推理加速器，通过优化计算单元、数据流和存储策略，显著降低数据访问量和计算延迟，提升硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 现有深度卷积神经网络（DCNN）在边缘设备上的推理计算复杂度和数据访问量过高，导致延迟大且不实用。

Method: 1. 使用1x1卷积核作为最小计算单元；2. 设计适合模型的专用计算单元；3. 引入Reuse Feature SRAM、Output Reuse策略和Ring Stream数据流以减少DRAM数据交换；4. 提出On-fly Pooling模块以在芯片内完成池化计算。

Result: 相比无重用策略的方法，数据访问量减少533倍；在VGG16和MobileNet上实现实时推理，速度提升7.52倍，能效提高1.92倍。

Conclusion: 所提加速器显著提升了边缘设备的推理效率和实时性，为实际应用提供了可行解决方案。

Abstract: Deep convolution Neural Network (DCNN) has been widely used in computer
vision tasks. However, for edge devices even inference has too large
computational complexity and data access amount. The inference latency of
state-of-the-art models are impractical for real-world applications. In this
paper, we propose a high utilization energy-aware real-time inference deep
convolutional neural network accelerator, which improves the performance of the
current accelerators. First, we use the 1x1 size convolution kernel as the
smallest unit of the computing unit. Then we design suitable computing unit
based on the requirements of each model. Secondly, we use Reuse Feature SRAM to
store the output of the current layer in the chip and use the value as the
input of the next layer. Moreover, we import Output Reuse Strategy and Ring
Stream Dataflow to reduce the amount of data exchange between chips and DRAM.
Finally, we present On-fly Pooling Module to let the calculation of the Pooling
layer directly complete in the chip. With the aid of the proposed method, the
implemented acceleration chip has an extremely high hardware utilization rate.
We reduce a generous amount of data transfer on the specific module, ECNN.
Compared to the methods without reuse strategy, we can reduce 533 times of data
access amount. At the same time, we have enough computing power to perform
real-time execution of the existing image classification model, VGG16 and
MobileNet. Compared with the design in VWA, we can speed up 7.52 times and have
1.92x energy efficiency

</details>


### [94] [Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems](https://arxiv.org/abs/2509.05937)
*Wei-Hsing Huang,Jianwei Jia,Yuyao Kong,Faaiq Waqar,Tai-Hao Wen,Meng-Fan Chang,Shimeng Yu*

Main category: cs.AR

TL;DR: KAN通过B样条函数减少参数量，但硬件加速复杂。本文提出算法-硬件协同设计，优化量化、稀疏性和电路技术，验证了在大规模任务中的扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 解决KAN架构中B样条函数硬件加速的高复杂度问题。

Method: 算法层面采用Alignment-Symmetry和PowerGap量化、稀疏映射策略；电路层面采用N:1时间调制动态电压生成器和模拟存内计算电路。

Result: 在22nm工艺下，参数量增加500Kx至807Kx时，面积仅增加28Kx至41Kx，功耗增加51x至94x，精度损失0.11%至0.23%。

Conclusion: 所提架构在保持高性能的同时展示了显著的可扩展性。

Abstract: Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an
innovative architectural paradigm capable of replicating conventional deep
neural network (DNN) capabilities while utilizing significantly reduced
parameter counts through the employment of parameterized B-spline functions
with trainable coefficients. Nevertheless, the B-spline functional components
inherent to KAN architectures introduce distinct hardware acceleration
complexities. While B-spline function evaluation can be accomplished through
look-up table (LUT) implementations that directly encode functional mappings,
thus minimizing computational overhead, such approaches continue to demand
considerable circuit infrastructure, including LUTs, multiplexers, decoders,
and related components. This work presents an algorithm-hardware co-design
approach for KAN acceleration. At the algorithmic level, techniques include
Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity
aware mapping strategy, and circuit-level techniques include N:1 Time
Modulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)
circuits. This work conducts evaluations on large-scale KAN networks to
validate the proposed methodologies. Non-ideality factors, including partial
sum deviations from process variations, have been evaluated with statistics
measured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally
determined KAN hyperparameters in conjunction with circuit optimizations
fabricated at the 22nm technology node, despite the parameter count for
large-scale tasks in this work increasing by 500Kx to 807Kx compared to
tiny-scale tasks in previous work, the area overhead increases by only 28Kx to
41Kx, with power consumption rising by merely 51x to 94x, while accuracy
degradation remains minimal at 0.11% to 0.23%, demonstrating the scaling
potential of our proposed architecture.

</details>


### [95] [SCREME: A Scalable Framework for Resilient Memory Design](https://arxiv.org/abs/2509.06101)
*Fan Li,Mimi Xie,Yanan Guo,Huize Li,Xin Xin*

Main category: cs.AR

TL;DR: 论文提出了一种创新的内存可靠性设计方案SCREME，通过利用低成本、低性能的ECC芯片和未充分利用的I/O资源，实现高效的内存保护。


<details>
  <summary>Details</summary>
Motivation: 随着内存技术的快速发展，可靠性挑战加剧，传统ECC方案因额外内存空间成本过高而难以扩展。

Method: 提出SCREME框架，利用低性能ECC芯片节省带宽和成本，并通过重新配置DIMM上的I/O资源实现灵活互联。

Result: SCREME能够以低成本满足日益增长的内存可靠性需求。

Conclusion: 该方案突破了传统ECC的限制，为内存可靠性设计提供了可扩展的解决方案。

Abstract: The continuing advancement of memory technology has not only fueled a surge
in performance, but also substantially exacerbate reliability challenges.
Traditional solutions have primarily focused on improving the efficiency of
protection schemes, i.e., Error Correction Codes (ECC), under the assumption
that allocating additional memory space for parity data is always expensive and
therefore not a scalable solution.
  We break the stereotype by proposing an orthogonal approach that provides
additional, cost-effective memory space for resilient memory design. In
particular, we recognize that ECC chips (used for parity storage) do not
necessarily require the same performance level as regular data chips. This
offers two-fold benefits: First, the bandwidth originally provisioned for a
regular-performance ECC chip can instead be used to accommodate multiple
low-performance chips. Second, the cost of ECC chips can be effectively
reduced, as lower performance often correlates with lower expense. In addition,
we observe that server-class memory chips are often provisioned with ample, yet
underutilized I/O resources. This further offers the opportunity to repurpose
these resources to enable flexible on-DIMM interconnections. Based on the above
two insights, we finally propose SCREME, a scalable memory framework leverages
cost-effective, albeit slower, chips -- naturally produced during rapid
technology evolution -- to meet the growing reliability demands driven by this
evolution.

</details>


### [96] [Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects](https://arxiv.org/abs/2509.06365)
*Omar Al Habsi,Safa Mohammed Sali,Anis Meribout,Mahmoud Meribout,Saif Almazrouei,Mohamed Seghier*

Main category: cs.AR

TL;DR: 这篇论文回顾了便携式MRI（pMRI）的最新进展，重点探讨硬件加速在提升图像获取和重建速度中的作用及其影响。


<details>
  <summary>Details</summary>
Motivation: 便携式MRI在远程或资源有限环境中具有潜力，但其计算复杂度（尤其是图像重建和机器学习算法）带来挑战，硬件加速可解决这些问题，但目前研究较少。

Method: 论文综述了GPU、FPGA和ASIC等硬件加速技术的性能，同时提出未来研究方向，包括AI驱动的重建和边缘硬件解决方案。

Result: 硬件加速能提高pMRI的图像质量、降低功耗并增强便携性。

Conclusion: 为推进可重现的AI研究，论文建议成立低场MRI联盟并提供标准化数据集和测试平台。

Abstract: There is a growing interest in portable MRI (pMRI) systems for point-of-care
imaging, particularly in remote or resource-constrained environments. However,
the computational complexity of pMRI, especially in image reconstruction and
machine learning (ML) algorithms for enhanced imaging, presents significant
challenges. Such challenges can be potentially addressed by harnessing hardware
application solutions, though there is little focus in the current pMRI
literature on hardware acceleration. This paper bridges that gap by reviewing
recent developments in pMRI, emphasizing the role and impact of hardware
acceleration to speed up image acquisition and reconstruction. Key technologies
such as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays
(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent
performance in terms of reconstruction speed and power consumption. This review
also highlights the promise of AI-powered reconstruction, open low-field pMRI
datasets, and innovative edge-based hardware solutions for the future of pMRI
technology. Overall, hardware acceleration can enhance image quality, reduce
power consumption, and increase portability for next-generation pMRI
technology. To accelerate reproducible AI for portable MRI, we propose forming
a Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,
retrospective multi-center testing, prospective reader and non-inferiority
trials) to provide standardized datasets, benchmarks, and regulator-ready
testbeds.

</details>


### [97] [VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing](https://arxiv.org/abs/2509.06698)
*Leidy Mabel Alvero-Gonzalez,Matias Miguez,Eric Gutierrez,Juan Sapriza,Susana Patón,David Atienza,José Miranda*

Main category: cs.AR

TL;DR: 该研究提出了一种基于电压控制振荡器的模拟读取系统（VCO-CARE），用于连续监测皮肤电活动（EDA），具有高灵敏度、低功耗和低噪声的特点。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴设备中皮肤电活动监测的高灵敏度、低功耗和低校准需求问题。

Method: 设计了基于电压控制振荡器的模拟读取系统（VCO-CARE）。

Result: 系统在0-20 uS范围内达到40 pS的高灵敏度，固定电阻相对误差小于0.0025%，功耗仅2.3 uW，噪声低至0.8 uVrms。

Conclusion: VCO-CARE系统有望推动可穿戴传感器的发展，提升适应性、功耗和噪声性能。

Abstract: Continuous monitoring of electrodermal activity (EDA) through wearable
devices has attracted much attention in recent times. However, the persistent
challenge demands analog front-end (AFE) systems with high sensitivity, low
power consumption, and minimal calibration requirements to ensure practical
usability in wearable technologies. In response to this challenge, this
research introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog
Readout tailored for continuous EDA sensing. The results show that our system
achieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS
range and a negligible relative error of less than 0.0025% for
fixed-resistance. Furthermore, the proposed system consumes only an average of
2.3 uW based on post-layout validations and introduces a low noise
contribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.
This research aims to drive the evolution of wearable sensors characterized by
seamless adaptability to diverse users, minimal power consumption, and
outstanding noise resilience.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [98] [Scalable Learning of One-Counter Automata via State-Merging Algorithms](https://arxiv.org/abs/2509.05762)
*Shibashis Guha,Anirban Majumdar,Prince Mathew,A. V. Sreejith*

Main category: cs.FL

TL;DR: 提出了OPNI算法，用于学习确定性实时单计数器自动机（DROCA），并通过结合主动学习方法实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 受RPNI算法启发，希望为DROCA设计一种被动学习算法，并扩展到主动学习领域。

Method: OPNI算法与主动学习结合，构建与给定样本一致的DROCA模型。

Result: 实验显示该方法比现有算法更高效，且适用于可见单计数器自动机学习。

Conclusion: OPNI结合主动学习的方法在DROCA学习中表现优越，扩展性更强。

Abstract: We propose One-counter Positive Negative Inference (OPNI), a passive learning
algorithm for deterministic real-time one-counter automata (DROCA). Inspired by
the RPNI algorithm for regular languages, OPNI constructs a DROCA consistent
with any given valid sample set.
  We further present a method for combining OPNI with active learning of DROCA,
and provide an implementation of the approach. Our experimental results
demonstrate that this approach scales more effectively than existing
state-of-the-art algorithms. We also evaluate the performance of the proposed
approach for learning visibly one-counter automata.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [99] [Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search](https://arxiv.org/abs/2509.05750)
*Ilias Azizi,Karima Echihab,Themis Palpanas,Vassilis Christophides*

Main category: cs.IR

TL;DR: 对12种先进的图向量搜索算法在7个真实数据集上进行了全面评估，发现增量插入和邻域多样化是最佳方法，并指出基础图选择可能影响扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着向量数据的普及和规模增长，需要更高效的图向量搜索算法，但缺乏对现有方法的系统比较。

Method: 对12种最先进的图向量搜索算法进行了实验评估，比较了不同范式（如增量插入、邻域多样化）的效果。

Result: 增量插入和邻域多样化表现最佳，但基础图选择可能限制算法扩展性。

Conclusion: 未来研究方向包括改进数据自适应的种子选择和多样化策略。

Abstract: Vector data is prevalent across business and scientific applications, and its
popularity is growing with the proliferation of learned embeddings. Vector data
collections often reach billions of vectors with thousands of dimensions, thus,
increasing the complexity of their analysis. Vector search is the backbone of
many critical analytical tasks, and graph-based methods have become the best
choice for analytical tasks that do not require guarantees on the quality of
the answers. Although several paradigms (seed selection, incremental insertion,
neighborhood propagation, neighborhood diversification, and divide-and-conquer)
have been employed to design in-memory graph-based vector search algorithms, a
systematic comparison of the key algorithmic advances is still missing. We
conduct an exhaustive experimental evaluation of twelve state-of-the-art
methods on seven real data collections, with sizes up to 1 billion vectors. We
share key insights about the strengths and limitations of these methods; e.g.,
the best approaches are typically based on incremental insertion and
neighborhood diversification, and the choice of the base graph can hurt
scalability. Finally, we discuss open research directions, such as the
importance of devising more sophisticated data adaptive seed selection and
diversification strategies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [100] [Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection](https://arxiv.org/abs/2509.05370)
*Tanya Joshi,Krishnendu Guha*

Main category: cs.CR

TL;DR: 研究探讨了量子机器学习（QML）算法在增强网络安全威胁检测中的应用，特别是在高维数据集中对恶意软件进行分类和入侵检测。QML算法表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在处理复杂、混淆的恶意软件模式和大量网络入侵数据时存在局限性，QML为解决这些问题提供了可能性。

Method: 研究实现了多种QML算法（如QNN、QSVM和QCNN），并在两个数据集上进行了实验，同时提出了集成了量子特征提取和分类的新框架。

Result: QML方法在实验中表现优异，QNN和QSVM的准确率分别达到95%和94%。

Conclusion: QML在恶意软件检测中具有显著优势，未来可进一步推广量子增强的实时分析框架。

Abstract: This study explores the application of quantum machine learning (QML)
algorithms to enhance cybersecurity threat detection, particularly in the
classification of malware and intrusion detection within high-dimensional
datasets. Classical machine learning approaches encounter limitations when
dealing with intricate, obfuscated malware patterns and extensive network
intrusion data. To address these challenges, we implement and evaluate various
QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector
Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for
malware detection tasks. Our experimental analysis utilized two datasets: the
Intrusion dataset, comprising 150 samples with 56 memory-based features derived
from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,
containing 58,596 samples with 57 features representing benign and malicious
software. Remarkably, our QML methods demonstrated superior performance
compared to classical approaches, achieving accuracies of 95% for QNN and 94%
for QSVM. These quantum-enhanced methods leveraged quantum superposition and
entanglement principles to accurately identify complex patterns within highly
obfuscated malware samples that were imperceptible to classical methods. To
further advance malware analysis, we propose a novel real-time malware analysis
framework that incorporates Quantum Feature Extraction using Quantum Fourier
Transform, Quantum Feature Maps, and Classification using Variational Quantum
Circuits. This system integrates explainable AI methods, including GradCAM++
and ScoreCAM algorithms, to provide interpretable insights into the quantum
decision-making processes.

</details>


### [101] [MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm](https://arxiv.org/abs/2509.05891)
*Mahfuzul I. Nissan*

Main category: cs.CR

TL;DR: 本文提出了一种名为MemTraceDB的工具，通过分析MySQL数据库进程的内存快照重构用户活动时间线，解决了磁盘日志被篡改的问题。


<details>
  <summary>Details</summary>
Motivation: 数据库审计和事务日志是法医调查的基础，但易受特权攻击者篡改，MemTraceDB通过内存分析提供了一种可靠的替代方案。

Method: MemTraceDB采用新颖的算法ActiviTimeTrace，从内存快照中系统提取并关联用户连接和执行查询等法医证据。

Result: 实验证明MemTraceDB的有效性，并发现MySQL查询栈的有限操作容量为约9,997条查询，为内存快照采集频率提供了数据驱动的公式。

Conclusion: MemTraceDB能够独立于被篡改的磁盘日志，实现法证可靠的用户活动重建。

Abstract: Database audit and transaction logs are fundamental to forensic
investigations, but they are vulnerable to tampering by privileged attackers.
Malicious insiders or external threats with administrative access can alter,
purge, or temporarily disable logging mechanisms, creating significant blind
spots and rendering disk-based records unreliable. Memory analysis offers a
vital alternative, providing investigators direct access to volatile artifacts
that represent a ground-truth source of recent user activity, even when log
files have been compromised.
  This paper introduces MemTraceDB, a tool that reconstructs user activity
timelines by analyzing raw memory snapshots from the MySQL database process.
MemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically
extract and correlate forensic artifacts such as user connections and executed
queries. Through a series of experiments, I demonstrate MemTraceDB's
effectiveness and reveal a critical empirical finding: the MySQL query stack
has a finite operational capacity of approximately 9,997 queries. This
discovery allows me to establish a practical, data-driven formula for
determining the optimal frequency for memory snapshot collection, providing a
clear, actionable guideline for investigators. The result is a
forensically-sound reconstruction of user activity, independent of compromised
disk-based logs.

</details>


### [102] [Introduction to Number Theoretic Transform](https://arxiv.org/abs/2509.05884)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: NTT是离散傅里叶变换的变体，用于后量子密码学和同态加密，通过多项式模拟正弦波，快速NTT在格密码学中显著降低多项式乘法复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究NTT及其快速版本在后量子密码学和同态加密中的应用，以提升计算效率。

Method: 引入循环、负循环卷积及NTT及其逆变换的快速版本，分析其在多项式乘法中的应用。

Result: 快速NTT将多项式乘法复杂度从二次降至拟线性，显著提升计算效率。

Conclusion: NTT及其快速版本是后量子密码学和同态加密中的关键工具，能高效处理多项式运算。

Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the
Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in
developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier
Transform essentially decomposes a signal into its frequencies. They are
traditionally sine or cosine waves. NTT works more over groups or finite fields
rather than on a continuous signal and polynomials work as the analog of sine
waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has
been proven to be useful in lattice-based cryptography due to its ability to
reduce the complexity of polynomial multiplication from quadratic to
quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions
along with NTT and its inverse and their fast versions.

</details>


### [103] [VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles](https://arxiv.org/abs/2509.06133)
*Pradyumna Kaushal*

Main category: cs.CR

TL;DR: VehiclePassport是一种基于区块链和零知识证明的数字护照，用于透明且隐私保护的车辆生命周期记录管理。


<details>
  <summary>Details</summary>
Motivation: 现代车辆的周期记录分散且易受欺诈，需一种可靠、透明的解决方案。

Method: 采用区块链存储数据，结合零知识证明和JWT实现选择性披露。

Result: 开源参考栈成本低、验证快，适用于大规模应用，支持GDPR合规。

Conclusion: 该架构为保险、转售等提供了信任基础，适用于全球市场。

Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.

</details>


### [104] [A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks](https://arxiv.org/abs/2509.05366)
*Umair Amjid,M. Umar Khan,S. A. Manan Kirmani*

Main category: cs.CR

TL;DR: 提出了一个基于AI的框架，利用机器学习算法分析网络流量以检测异常行为，快速响应潜在入侵，解决物联网网络中摄像头的安全问题。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的普及导致其网络安全性问题日益凸显，尤其是摄像头易受暴力破解、零日攻击和拒绝服务攻击等威胁。

Method: 框架通过机器学习算法分析网络流量，检测异常行为，并使用真实数据集进行训练和评估。

Result: 该方法能够快速检测并响应潜在入侵，提高物联网网络的安全性。

Conclusion: 基于AI的框架为解决物联网网络中的安全问题提供了一种有效的方法。

Abstract: The increasing use of Internet of Things (IoT) devices has led to a rise in
security related concerns regarding IoT Networks. The surveillance cameras in
IoT networks are vulnerable to security threats such as brute force and
zero-day attacks which can lead to unauthorized access by hackers and potential
spying on the users activities. Moreover, these cameras can be targeted by
Denial of Service (DOS) attacks, which will make it unavailable for the user.
The proposed AI based framework will leverage machine learning algorithms to
analyze network traffic and detect anomalous behavior, allowing for quick
detection and response to potential intrusions. The framework will be trained
and evaluated using real-world datasets to learn from past security incidents
and improve its ability to detect potential intrusion.

</details>


### [105] [FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets](https://arxiv.org/abs/2509.05643)
*Carmine Cesarano,Roberto Natella*

Main category: cs.CR

TL;DR: FuzzBox利用动态仪器化在虚拟化环境中进行漏洞检测，无需重新编译源代码或依赖硬件，适用于工业系统和IoT固件。


<details>
  <summary>Details</summary>
Motivation: 针对工业系统和专有编译工具链中难以应用传统漏洞检测方法的问题。

Method: 通过虚拟化环境动态仪器化代码，注入模糊输入并检测故障和覆盖率。

Result: 在专有的MILS hypervisor和商业IoT固件中验证了其有效性和广泛适用性。

Conclusion: FuzzBox为工业系统和IoT设备提供了一种高效、通用的漏洞检测解决方案。

Abstract: Coverage-guided fuzzing has been widely applied to address zero-day
vulnerabilities in general-purpose software and operating systems. This
approach relies on instrumenting the target code at compile time. However,
applying it to industrial systems remains challenging, due to proprietary and
closed-source compiler toolchains and lack of access to source code. FuzzBox
addresses these limitations by integrating emulation with fuzzing: it
dynamically instruments code during execution in a virtualized environment, for
the injection of fuzz inputs, failure detection, and coverage analysis, without
requiring source code recompilation and hardware-specific dependencies. We show
the effectiveness of FuzzBox through experiments in the context of a
proprietary MILS (Multiple Independent Levels of Security) hypervisor for
industrial applications. Additionally, we analyze the applicability of FuzzBox
across commercial IoT firmware, showcasing its broad portability.

</details>


### [106] [Network-level Censorship Attacks in the InterPlanetary File System](https://arxiv.org/abs/2509.06626)
*Jan Matter,Muoi Tran*

Main category: cs.CR

TL;DR: 研究了IPFS网络中BGP路由攻击的威胁，通过模拟攻击证明了内容可以被有效审查，并提出了合作内容复制和备份节点的对策。


<details>
  <summary>Details</summary>
Motivation: 尽管IPFS是去中心化的，但实际运行中节点和内容提供商趋向于集中在大型公有云中，导致BGP路由攻击（如拦截和劫持）成为潜在威胁。

Method: 收集了3,000个内容块（CIDs），模拟了BGP劫持和被动拦截攻击。

Result: 单个恶意AS可以审查75%的IPFS内容；即使仅劫持62个前缀，也能达到70%的攻击效果。

Conclusion: 提出了通过全局节点合作内容复制和备份节点来抵御攻击的方法，呼吁加强IPFS网络的防护。

Abstract: The InterPlanetary File System (IPFS) has been successfully established as
the de facto standard for decentralized data storage in the emerging Web3.
Despite its decentralized nature, IPFS nodes, as well as IPFS content
providers, have converged to centralization in large public clouds.
Centralization introduces BGP routing-based attacks, such as passive
interception and BGP hijacking, as potential threats. Although this attack
vector has been investigated for many other Web3 protocols, such as Bitcoin and
Ethereum, to the best of our knowledge, it has not been analyzed for the IPFS
network. In our work, we bridge this gap and demonstrate that BGP routing
attacks can be effectively leveraged to censor content in IPFS. For the
analysis, we collected 3,000 content blocks called CIDs and conducted a
simulation of BGP hijacking and passive interception against them. We find that
a single malicious AS can censor 75% of the IPFS content for more than 57% of
all requester nodes. Furthermore, we show that even with a small set of only 62
hijacked prefixes, 70% of the full attack effectiveness can already be reached.
We further propose and validate countermeasures based on global collaborative
content replication among all nodes in the IPFS network, together with
additional robust backup content provider nodes that are well-hardened against
BGP hijacking. We hope this work raises awareness about the threat BGP
routing-based attacks pose to IPFS and triggers further efforts to harden the
live IPFS network against them.

</details>


### [107] [From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)](https://arxiv.org/abs/2509.06368)
*Kunlin Cai,Jinghuai Zhang,Ying Li,Zhiyuan Wang,Xun Chen,Tianshi Li,Yuan Tian*

Main category: cs.CR

TL;DR: 研究了XR开发者对安全和隐私威胁的认知与应对，发现开发者在决策中可能忽略风险，现有缓解方法不足，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索XR开发者如何看待新兴安全和隐私威胁，填补当前缺乏开发者视角研究的空白。

Method: 访谈23位专业XR开发者，分析其感知和应对威胁的方式。

Result: 发现开发者决策可能放大威胁，但缺乏意识和有效支持。

Conclusion: 提出改进XR开发过程中安全和隐私的建议，强调开发者中心的重要性。

Abstract: The immersive nature of XR introduces a fundamentally different set of
security and privacy (S&P) challenges due to the unprecedented user
interactions and data collection that traditional paradigms struggle to
mitigate. As the primary architects of XR applications, developers play a
critical role in addressing novel threats. However, to effectively support
developers, we must first understand how they perceive and respond to different
threats. Despite the growing importance of this issue, there is a lack of
in-depth, threat-aware studies that examine XR S&P from the developers'
perspective. To fill this gap, we interviewed 23 professional XR developers
with a focus on emerging threats in XR. Our study addresses two research
questions aiming to uncover existing problems in XR development and identify
actionable paths forward.
  By examining developers' perceptions of S&P threats, we found that: (1) XR
development decisions (e.g., rich sensor data collection, user-generated
content interfaces) are closely tied to and can amplify S&P threats, yet
developers are often unaware of these risks, resulting in cognitive biases in
threat perception; and (2) limitations in existing mitigation methods, combined
with insufficient strategic, technical, and communication support, undermine
developers' motivation, awareness, and ability to effectively address these
threats. Based on these findings, we propose actionable and stakeholder-aware
recommendations to improve XR S&P throughout the XR development process. This
work represents the first effort to undertake a threat-aware,
developer-centered study in the XR domain -- an area where the immersive,
data-rich nature of the XR technology introduces distinctive challenges.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [108] [FireRedChat: A Pluggable, Full-Duplex Voice Interaction System with Cascaded and Semi-Cascaded Implementations](https://arxiv.org/abs/2509.06502)
*Junjie Chen,Yao Hu,Junjie Li,Kangyue Li,Kun Liu,Wenpeng Li,Xu Li,Ziyuan Li,Feiyu Shen,Xu Tang,Manzhen Wei,Yichen Wu,Fenglong Xie,Kaituo Xu,Kun Xie*

Main category: cs.SD

TL;DR: 本文提出了一种完整的全双工语音交互系统，通过模块化设计和内部模型实现了高自然度和低延迟的实时交互。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案要么难以控制和设计（端到端），要么依赖非开放组件（模块化），限制了整体优化。本文旨在解决这些问题。

Method: 系统由轮流控制器、交互模块和对话管理器组成，采用流式个性化VAD和语义端点检测器提升交互质量，并支持多种半双工管道的升级。

Result: 实验表明，系统减少了误中断，提高了语义端点检测精度，且延迟接近工业系统水平。

Conclusion: 该系统实现了高效、自然且稳健的全双工语音交互，具备实际应用潜力。

Abstract: Full-duplex voice interaction allows users and agents to speak simultaneously
with controllable barge-in, enabling lifelike assistants and customer service.
Existing solutions are either end-to-end, difficult to design and hard to
control, or modular pipelines governed by turn-taking controllers that ease
upgrades and per-module optimization; however, prior modular frameworks depend
on non-open components and external providers, limiting holistic optimization.
In this work, we present a complete, practical full-duplex voice interaction
system comprising a turn-taking controller, an interaction module, and a
dialogue manager. The controller integrates streaming personalized VAD (pVAD)
to suppress false barge-ins from noise and non-primary speakers, precisely
timestamp primary-speaker segments, and explicitly enable primary-speaker
barge-ins; a semantic end-of-turn detector improves stop decisions. It upgrades
heterogeneous half-duplex pipelines, cascaded, semi-cascaded, and
speech-to-speech, to full duplex. Using internal models, we implement cascaded
and semi-cascaded variants; the semi-cascaded one captures emotional and
paralinguistic cues, yields more coherent responses, lowers latency and error
propagation, and improves robustness. A dialogue manager extends capabilities
via tool invocation and context management. We also propose three system-level
metrics, barge-in, end-of-turn detection accuracy, and end-to-end latency, to
assess naturalness, control accuracy, and efficiency. Experiments show fewer
false interruptions, more accurate semantic ends, and lower latency approaching
industrial systems, enabling robust, natural, real-time full-duplex
interaction. Demos: https://fireredteam.github.io/demos/firered_chat.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [109] [Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation](https://arxiv.org/abs/2509.05675)
*Amin Pakzad,Pedro Arduino,Wenyang Zhang,Ertugrul Tacirouglu*

Main category: math.NA

TL;DR: 该论文提出了一个用于桩基础结构动态分析的标准工作流程，涵盖域缩减方法、完美匹配层元素、嵌入界面元素和域分解技术，并通过数值模拟展示了其高效性和精确性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高保真数值模拟的标准化工作流程，论文旨在填补这一空白。

Method: 提出了一种逐步指南，包括域缩减方法加载、完美匹配层元素波吸收、嵌入界面元素建模和域分解。

Result: 结果表明域缩减方法减小了模拟规模，完美匹配层元素精确建模无限域，嵌入界面元素高效连接结构与土壤域，工作流程整体效果显著。

Conclusion: 虽然研究基于简化几何和加载场景，但为未来研究复杂结构和动态加载条件提供了基础框架。

Abstract: The demand for high-fidelity numerical simulations in soil-structure
interaction analysis is on the rise, yet a standardized workflow to guide the
creation of such simulations remains elusive. This paper aims to bridge this
gap by presenting a step-by-step guideline proposing a workflow for dynamic
analysis of structures with pile foundations. The proposed workflow encompasses
instructions on how to use Domain Reduction Method for loading, Perfectly
Matched Layer elements for wave absorption, soil-structure interaction modeling
using Embedded interface elements, and domain decomposition for efficient use
of processing units. Through a series of numerical simulations, we showcase the
practical application of this workflow. Our results reveal the efficacy of the
Domain Reduction Method in reducing simulation size without compromising model
fidelity, show the precision of Perfectly Matched Layer elements in modeling
infinite domains, highlight the efficiency of Embedded Interface elements in
establishing connections between structures and the soil domain, and
demonstrate the overall effectiveness of the proposed workflow in conducting
high-fidelity simulations. While our study focuses on simplified geometries and
loading scenarios, it serves as a foundational framework for future research
endeavors aimed at exploring more intricate structural configurations and
dynamic loading conditions

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [110] [Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections](https://arxiv.org/abs/2509.05391)
*Christian Masuhr,Julian Koch,Thorsten Schüppstuhl*

Main category: cs.RO

TL;DR: 该论文通过系统评估Magic Leap 2（ML2）控制器的追踪性能，填补了现代头戴显示器（HMD）工具追踪公共基准的空白。


<details>
  <summary>Details</summary>
Motivation: 商业增强现实（AR）硬件的严格评估至关重要，但目前缺乏现代HMD工具追踪的公共基准。

Method: 使用机器人手臂进行可重复运动（EN ISO 9283），并以光学追踪系统作为地面真相，评估静态和动态性能，包括氢泄漏检查用例的实际情况路径。

Result: 为ML2控制器的准确性和重复性提供了定量基准，并提出了一种稳健、可转移的评估方法。

Conclusion: 研究结果为评估控制器在氢泄漏检查等工业传感器AR引导任务中的适用性提供了基础。

Abstract: Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,
yet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)
are limited. This paper addresses this gap by systematically assessing the
Magic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for
repeatable motion (EN ISO 9283) and an optical tracking system as ground truth,
our protocol evaluates static and dynamic performance under various conditions,
including realistic paths from a hydrogen leak inspection use case. The results
provide a quantitative baseline of the ML2 controller's accuracy and
repeatability and present a robust, transferable evaluation methodology. The
findings provide a basis to assess the controllers suitability for the
inspection use case and similar industrial sensor-based AR guidance tasks.

</details>


### [111] [Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain](https://arxiv.org/abs/2509.06061)
*Faiza Babakano,Ahmed Fahmin,Bojie Shen,Muhammad Aamir Cheema,Isma Farah Siddiqui*

Main category: cs.RO

TL;DR: 本文提出了对象拾取最小能量路径问题（OMEPP），旨在解决自主移动机器人在拾取对象并送达目的地时的能量效率问题。通过基线算法和并发PCPD搜索方法，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人在户外环境中运行时，地形变化和拾取对象的动作会影响能量消耗，现有研究未充分考虑此类场景。

Method: 提出OMEPP问题，并开发基线算法（基于Z star算法）和并发PCPD搜索方法，利用Payload-Constrained Path Database（PCPD）优化搜索过程。

Result: 并发PCPD搜索在保证接近最优解的同时，计算速度比基线算法快1-2个数量级。

Conclusion: PCPD方法有效解决了OMEPP问题，显著提升了能量效率和计算性能。

Abstract: Autonomous Mobile Robots (AMRs) operate on battery power, making energy
efficiency a critical consideration, particularly in outdoor environments where
terrain variations affect energy consumption. While prior research has
primarily focused on computing energy-efficient paths from a source to a
destination, these approaches often overlook practical scenarios where a robot
needs to pick up an object en route - an action that can significantly impact
energy consumption due to changes in payload. This paper introduces the
Object-Pickup Minimum Energy Path Problem (OMEPP), which addresses
energy-efficient route planning for AMRs required to pick up an object from one
of many possible locations and deliver it to a destination. To address OMEPP,
we first introduce a baseline algorithm that employs the Z star algorithm, a
variant of A star tailored for energy-efficient routing, to iteratively visit
each pickup point. While this approach guarantees optimality, it suffers from
high computational cost due to repeated searches at each pickup location. To
mitigate this inefficiency, we propose a concurrent PCPD search that manages
multiple Z star searches simultaneously across all pickup points. Central to
our solution is the Payload-Constrained Path Database (PCPD), an extension of
the Compressed Path Database (CPD) that incorporates payload constraints. We
demonstrate that PCPD significantly reduces branching factors during search,
improving overall performance. Although the concurrent PCPD search may produce
slightly suboptimal solutions, extensive experiments on real-world datasets
show it achieves near-optimal performance while being one to two orders of
magnitude faster than the baseline algorithm.

</details>


### [112] [TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs](https://arxiv.org/abs/2509.05547)
*Ziling Chen,Yeo Jung Yoon,Rolando Bautista-Montesano,Zhen Zhao,Ajay Mandlekar,John Liu*

Main category: cs.RO

TL;DR: TeleopLab是一种移动设备遥操作系统，用于远程教育中的实验操作，通过机器人臂和学生界面实现高效互动。研究表明其显著提高任务效率和学生体验。


<details>
  <summary>Details</summary>
Motivation: 解决远程教育中实地设备互动成本高且体验不直观的问题。

Method: 开发TeleopLab系统，包含机器人臂、自适应夹具、摄像设备及智能手机界面，并进行用户研究评估性能和学生反馈。

Result: 任务完成时间减少46.1%，学生体验提升，NASA TLX和SUS评分分别为38.2和73.8。

Conclusion: TeleopLab为远程STEM教育提供了可扩展且高效的解决方案。

Abstract: Teleoperation offers a promising solution for enabling hands-on learning in
remote education, particularly in environments requiring interaction with
real-world equipment. However, such remote experiences can be costly or
non-intuitive. To address these challenges, we present TeleopLab, a mobile
device teleoperation system that allows students to control a robotic arm and
operate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,
cameras, lab equipment for a diverse range of applications, a user interface
accessible through smartphones, and video call software. We conducted a user
study, focusing on task performance, students' perspectives toward the system,
usability, and workload assessment. Our results demonstrate a 46.1% reduction
in task completion time as users gained familiarity with the system.
Quantitative feedback highlighted improvements in students' perspectives after
using the system, while NASA TLX and SUS assessments indicated a manageable
workload of 38.2 and a positive usability of 73.8. TeleopLab successfully
bridges the gap between physical labs and remote education, offering a scalable
and effective platform for remote STEM learning.

</details>


### [113] [Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization](https://arxiv.org/abs/2509.06582)
*Carlos A. Pinheiro de Sousa,Niklas Gröne,Mathias Günther,Oliver Deussen*

Main category: cs.RO

TL;DR: 一种多用户VR共位框架，结合运动捕捉与SLAM跟踪，提供低延迟、高帧率的共享虚拟环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中外部跟踪带来的延迟和抖动问题，或一次性校准无法修正漂移的局限性。

Method: 结合运动捕捉系统和SLAM内部跟踪，实现本地HMD SLAM跟踪的响应性和外部源实时对齐的灵活性。

Result: 框架在空间准确性上满足多用户交互需求，同时在舒适性、可扩展性和鲁棒性上优于现有方案。

Conclusion: 该框架提供了一种高效、可靠的多用户VR共位解决方案。

Abstract: We introduce a multi-user VR co-location framework that synchronizes users
within a shared virtual environment aligned to physical space. Our approach
combines a motion capture system with SLAM-based inside-out tracking to deliver
smooth, high-framerate, low-latency performance. Previous methods either rely
on continuous external tracking, which introduces latency and jitter, or on
one-time calibration, which cannot correct drift over time. In contrast, our
approach combines the responsiveness of local HMD SLAM tracking with the
flexibility to realign to an external source when needed. It also supports
real-time pose sharing across devices, ensuring consistent spatial alignment
and engagement between users. Our evaluation demonstrates that our framework
achieves the spatial accuracy required for natural multi-user interaction while
offering improved comfort, scalability, and robustness over existing co-located
VR solutions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [114] [Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes](https://arxiv.org/abs/2509.05364)
*Abdollah Baghaei Daemei*

Main category: cs.CY

TL;DR: 新西兰住宅节能AI决策支持工具的设计与评估，通过模块化仪表盘整合数据分析和情景模拟，专家测试显示高实用性。


<details>
  <summary>Details</summary>
Motivation: 新西兰住宅能效问题和现有政策改进的局限性促使开发AI工具，以提供个性化决策支持。

Method: 使用Python和Streamlit开发原型工具，包括数据摄入、异常检测、基线建模和情景模拟，并通过专家访谈评估。

Result: 工具展示出高可用性（M=4.3）和情景输出价值（M=4.5），具潜力补充补贴与法规。

Conclusion: AI工具能为家庭提供个性化指导，未来需拓展碳指标、电价模型和国家数据集整合。

Abstract: Residential buildings contribute significantly to energy use, health
outcomes, and carbon emissions. In New Zealand, housing quality has
historically been poor, with inadequate insulation and inefficient heating
contributing to widespread energy hardship. Recent reforms, including the
Warmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code
upgrades, have delivered health and comfort improvements, yet challenges
persist. Many retrofits remain partial, data on household performance are
limited, and decision-making support for homeowners is fragmented. This study
presents the design and evaluation of an AI-powered decision-support tool for
residential energy efficiency in New Zealand. The prototype, developed using
Python and Streamlit, integrates data ingestion, anomaly detection, baseline
modeling, and scenario simulation (e.g., LED retrofits, insulation upgrades)
into a modular dashboard. Fifteen domain experts, including building
scientists, consultants, and policy practitioners, tested the tool through
semi-structured interviews. Results show strong usability (M = 4.3), high value
of scenario outputs (M = 4.5), and positive perceptions of its potential to
complement subsidy programs and regulatory frameworks. The tool demonstrates
how AI can translate national policies into personalized, household-level
guidance, bridging the gap between funding, standards, and practical
decision-making. Its significance lies in offering a replicable framework for
reducing energy hardship, improving health outcomes, and supporting climate
goals. Future development should focus on carbon metrics, tariff modeling,
integration with national datasets, and longitudinal trials to assess
real-world adoption.

</details>


### [115] [AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations](https://arxiv.org/abs/2509.06176)
*Zsolt Almási,Hannah Bleher,Johannes Bleher,Rozanne Tuesday Flores,Guo Xuanyang,Paweł Pujszo,Raphaël Weuts*

Main category: cs.CY

TL;DR: 本文提出了一种模块化、跨学科的AI伦理教育课程，旨在培养能够应对伦理、法律和管理挑战的专业人士。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键领域的普及，亟需能够解决伦理、法律和管理问题的专业人士。当前AI伦理教育仍然分散且与实践脱节。

Method: 本文综合文献和法规发展，提出了一种结合技术基础与伦理、法律和政策的课程，强调了AI操作失败的主要原因（如偏见、目标错误等），并将这些与教学策略相结合。

Result: 基于欧盟、中国和国际框架的视角，本文设计了一个学期的教学计划，重点包括整合伦理、利益相关者参与和实践学习。

Conclusion: 该课程旨在培养学生诊断风险、遵守法规并参与多元利益相关者的能力，为负责任的AI治理培养适应性强且具有伦理基础的专业人士。

Abstract: As artificial intelligence (AI) systems permeate critical sectors, the need
for professionals who can address ethical, legal and governance challenges has
become urgent. Current AI ethics education remains fragmented, often siloed by
discipline and disconnected from practice. This paper synthesizes literature
and regulatory developments to propose a modular, interdisciplinary curriculum
that integrates technical foundations with ethics, law and policy. We highlight
recurring operational failures in AI - bias, misspecified objectives,
generalization errors, misuse and governance breakdowns - and link them to
pedagogical strategies for teaching AI governance. Drawing on perspectives from
the EU, China and international frameworks, we outline a semester plan that
emphasizes integrated ethics, stakeholder engagement and experiential learning.
The curriculum aims to prepare students to diagnose risks, navigate regulation
and engage diverse stakeholders, fostering adaptive and ethically grounded
professionals for responsible AI governance.

</details>


### [116] [Social Dynamics of DAOs: Power, Onboarding, and Inclusivity](https://arxiv.org/abs/2509.06163)
*Victoria Kozlova,Ben Biedermann*

Main category: cs.CY

TL;DR: 报告探讨了DAO中文化和社交动态对参与和权力的影响，指出金融特权、非正式把关、可见性偏见和入门结构等问题阻碍了真正的包容性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示DAO表面上的无许可和平等性背后，实际存在的复杂权力结构和隐性规范。

Method: 采用了定性访谈和民族志观察的方法。

Result: 研究发现DAO中的去中心化不仅是协议层面的特性，还是一个需要信任、归属感和认知多样性培育的社会过程。

Conclusion: 结论强调需要提高对结构性盲点的认识，并呼吁构建更具包容性和文化意识的去中心化系统。

Abstract: This report explores the often-overlooked cultural and social dynamics
shaping participation and power in DAOs. Drawing on qualitative interviews and
ethnographic observations, it shows how factors such as financial privilege,
informal gatekeeping, visibility bias, and onboarding structures create
barriers to meaningful inclusion. While DAOs are frequently framed as
permissionless and egalitarian, the lived experiences of contributors reveal a
more complex reality, one in which soft power and implicit norms determine
people's position within DAOs. Instead of offering solutionist prescriptions,
this report argues for a deeper cultural reflection within the DAO ecosystem.
It highlights that decentralisation is not solely a protocol-level feature, but
an ongoing social process that requires intentional cultivation of trust,
belonging, and epistemic plurality. With this report, we want to sharpen the
collective awareness of structural blind spots and call for building more
inclusive and culturally conscious decentralised systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [117] [Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading](https://arxiv.org/abs/2509.06466)
*Erwan Meunier,Julien M. Hendrickx*

Main category: math.OC

TL;DR: 分析了分散式在线优化算法的性能，发现现有性能保证过于保守，可能误导算法选择，部分算法在初期未从通信中获益。改进步长可提升性能。


<details>
  <summary>Details</summary>
Motivation: 旨在准确评估分散式在线优化算法的性能，揭示现有性能保证的保守性并优化算法选择。

Method: 使用性能估计问题方法自动计算算法的最坏情况性能，改进步长以优化性能。

Result: 发现现有性能保证过于保守，改进步长后可减少20%的最坏情况性能损失。

Conclusion: 通过调整步长可显著提升算法性能，为分散式在线优化提供更精确的性能评估和优化方向。

Abstract: We analyze Decentralized Online Optimization algorithms using the Performance
Estimation Problem approach which allows, to automatically compute exact
worst-case performance of optimization algorithms. Our analysis shows that
several available performance guarantees are very conservative, sometimes by
multiple orders of magnitude, and can lead to misguided choices of algorithm.
Moreover, at least in terms of worst-case performance, some algorithms appear
not to benefit from inter-agent communications for a significant period of
time. We show how to improve classical methods by tuning their step-sizes, and
find that we can save up to 20% on their actual worst-case performance regret.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [118] [A Dynamic Programming Framework for Vehicular Task Offloading with Successive Action Improvement](https://arxiv.org/abs/2509.05907)
*Qianren Li,Yuncong Hong,Bojie Lv,Rui Wang*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的动态编程框架，用于优化随机速度车辆的卸载任务，通过两时间尺度框架降低了算法复杂度，并在仿真中展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于车辆速度随机性导致其轨迹难预测，传统的确定性优化方法不适用，需要一种新的优化框架来处理此问题。

Method: 采用有限时域马尔可夫决策过程建模任务卸载问题，并提出两时间尺度框架：在超级时隙内通过确定性优化获取参考调度方案，在每个时隙根据系统状态改进近似值函数。

Result: 提出的调度框架在仿真中表现出显著优于基线的性能，且能够推导出非平凡的平均成本上界。

Conclusion: 该动态编程框架有效解决了随机速度车辆的任务卸载问题，具有较低的算法复杂度和显著的性能优势。

Abstract: In this paper, task offloading from vehicles with random velocities is
optimized via a novel dynamic programming framework. Particularly, in a
vehicular network with multiple vehicles and base stations (BSs), computing
tasks of vehicles are offloaded via BSs to an edge server. Due to the random
velocities, the exact locations of vehicles versus time, namely trajectories,
cannot be determined in advance. Hence, instead of deterministic optimization,
the cell association, uplink time, and throughput allocation of multiple
vehicles during a period of task offloading are formulated as a finite-horizon
Markov decision process. In order to derive a low-complexity solution
algorithm, a two-time-scale framework is proposed. The scheduling period is
divided into super slots, each super slot is further divided into a number of
time slots. At the beginning of each super slot, we first obtain a reference
scheduling scheme of cell association, uplink time and throughput allocation
via deterministic optimization, yielding an approximation of the optimal value
function. Within the super slot, the actual scheduling action of each time slot
is determined by making improvement to the approximate value function according
to the system state. Due to the successive improvement framework, a non-trivial
average cost upper bound could be derived. In the simulation, the random
trajectories of vehicles are generated from a high-fidelity traffic simulator.
It is shown that the performance gain of the proposed scheduling framework over
the baselines is significant.

</details>


### [119] [Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity](https://arxiv.org/abs/2509.06588)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 论文提出了一种基于多智能体系统的自动发电控制方法，通过信息共识算法优化并处理生成器的爬坡率限制（RRL），解决了现有线性/非线性优化方法中忽略的实际问题。


<details>
  <summary>Details</summary>
Motivation: 现有的优化方案通常忽略发电机的爬坡率限制（RRL），导致实际运行时无法实现最优成本或失去可行性，因此需提出一种能够处理RRL等约束的优化方法。

Method: 采用多智能体系统分布优化方案，基于信息共识算法，同时考虑RRL、功率上下限（box约束）和发电-需求平衡约束（耦合约束），并在算法中引入基于符号的非线性以提升收敛速度。

Result: 所提方案能够在算法迭代的任何时间点保持发电与需求的平衡（anytime可行性），并且能够容忍通信链路的移除。

Conclusion: 该方法不仅解决了RRL等实际约束问题，还具备良好的收敛性和通信容错性，适用于实际电力系统优化。

Abstract: This paper considers automatic generation control over an information-sharing
network of communicating generators as a multi-agent system. The optimization
solution is distributed among the agents based on information consensus
algorithms, while addressing the generators' ramp-rate-limits (RRL). This is
typically ignored in the existing linear/nonlinear optimization solutions but
they exist in real-time power generation scenarios. Without addressing the RRL,
the generators cannot follow the assigned rate of generating power by the
optimization algorithm; therefore, the existing solutions may not necessarily
converge to the exact optimal cost or may lose feasibility in practice. The
proposed solution in this work addresses the ramp-rate-limit constraint along
with the box constraint (limits on the generated powers) and the
coupling-constraint (generation-demand balance) at all iteration times of the
algorithm. The latter is referred to as the anytime feasibility and implies
that at every termination point of the algorithm, the balance between the
demand and generated power holds. To improve the convergence rate of the
algorithm we further consider internal signum-based nonlinearity. We also show
that our solution can tolerate communication link removal. This follows from
the uniform-connectivity assumption on the communication network.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [120] [Robustness and accuracy of mean opinion scores with hard and soft outlier detection](https://arxiv.org/abs/2509.06554)
*Dietmar Saupe,Tim Bleile*

Main category: eess.IV

TL;DR: 该论文提出了一种对抗性攻击方法，用于分析和比较图像和视频质量主观评估中的异常值检测方法，并提出了两种新的低复杂度、高性能异常值检测方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对异常值检测方法的全面性能分析，尤其是对抗性攻击下的表现。

Method: 采用进化优化的对抗性黑盒攻击方法，最大化尺度值对真实值的扭曲，测试多种硬性和软性异常值检测方法。

Result: 分析展示了不同方法在压力测试中的表现差异，并提出了两种新方法，具有低复杂度和优异的极端情况性能。

Conclusion: 该方法为异常值检测的性能分析提供了一个可靠的解决方案，且新方法在实际应用中表现优异。

Abstract: In subjective assessment of image and video quality, observers rate or
compare selected stimuli. Before calculating the mean opinion scores (MOS) for
these stimuli from the ratings, it is recommended to identify and deal with
outliers that may have given unreliable ratings. Several methods are available
for this purpose, some of which have been standardized. These methods are
typically based on statistics and sometimes tested by introducing synthetic
ratings from artificial outliers, such as random clickers. However, a reliable
and comprehensive approach is lacking for comparative performance analysis of
outlier detection methods. To fill this gap, this work proposes and applies an
empirical worst-case analysis as a general solution. Our method involves
evolutionary optimization of an adversarial black-box attack on outlier
detection algorithms, where the adversary maximizes the distortion of scale
values with respect to ground truth. We apply our analysis to several hard and
soft outlier detection methods for absolute category ratings and show their
differing performance in this stress test. In addition, we propose two new
outlier detection methods with low complexity and excellent worst-case
performance. Software for adversarial attacks and data analysis is available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 本文研究了视频扩散变换器的注意力机制，并提出了一种可视化生成视频模型中交叉注意力地图的方法。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家启发，探索注意力机制在AI艺术中的分析工具和创作媒介潜力。

Method: 基于开源Wan模型，提取并可视化文本到视频生成中的时空注意力行为。

Result: 通过实验和艺术案例研究，证实注意力地图可作为分析与艺术创作的工具。

Conclusion: 该项目推动了可解释AI在艺术领域的应用，鼓励艺术家将AI内在机制作为创作媒介。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [122] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种多代理系统，用于在真实街景图像上编辑和重新设计自行车设施。系统通过整合多种技术，快速生成视觉连贯且符合要求的设计，适合复杂街景场景。


<details>
  <summary>Details</summary>
Motivation: 传统的街景设计渲染方法耗时且难以实现精确的空间变化。AI辅助生成设计有潜力，但现有方法需要大量领域特定数据且难以处理复杂场景。

Method: 提出多代理框架，整合车道定位、提示优化、设计生成和自动评估技术，直接在街景图像上编辑和设计自行车设施。

Result: 实验表明，系统能适应不同道路几何和环境条件，生成视觉连贯且符合指令的设计。

Conclusion: 该工作为多代理管道在交通基础设施规划和设施设计中的应用奠定了基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [123] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 提出了一个评估多轮迭代优化的框架，涵盖创意、代码和数学等领域，通过12轮对话评测LLMs的表现，发现改进效果因领域而异，并提供了指标来衡量和比较不同模型的迭代效果。


<details>
  <summary>Details</summary>
Motivation: 缺乏明确的方法来衡量多轮迭代优化对LLMs表现的影响，尤其是在不同领域中的效果。

Method: 设计了12轮对话评测协议，采用多种提示策略，记录每轮输出，并使用领域特定的评分标准（如单元测试、答案等价性、创意可行性）和行为指标（语义变化、输出大小增长）。

Result: 改进效果因领域不同：创意和代码早期见效，数学则在后期优化表现突出。目标明确的提示更有效，而模糊反馈可能导致性能停滞或下降。

Conclusion: 该框架和指标使迭代优化可衡量，并为模型选择和策略调整提供了依据。

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [124] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

TL;DR: 论文介绍了一种基于智能手机技术、低成本且用户友好的方法来测量羽毛球扣球速度，利用YOLOv5和卡尔曼滤波进行检测与轨迹跟踪，并通过视频分析计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统的运动性能测量技术昂贵且复杂，业余球员难以使用。论文旨在填补这一空缺，为羽毛球运动提供易于获取的高性能分析工具。

Method: 结合YOLOv5模型和卡尔曼滤波器进行羽毛球检测与轨迹跟踪，通过视频分析实现速度估算，并开发了移动应用。

Result: 该系统成功实现了通过智能手机视频自动计算羽毛球速度的目标，且成本低廉、操作简单。

Conclusion: 论文提出的方法为业余球员提供了高性能分析工具，有望提升运动表现分析的可及性。

Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [125] [BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring](https://arxiv.org/abs/2509.06690)
*Usman Haider,Lukasz Szemet,Daniel Kelly,Vasileios Sergis,Andrew C. Daly,Karl Mason*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级的语义分割框架BioLite U-Net，专为实时生物打印应用设计，在准确性和效率上优于MobileNet基线模型。


<details>
  <summary>Details</summary>
Motivation: 生物打印领域中，实时监测打印质量和生物活性面临成像数据有限和嵌入式硬件资源受限的挑战，急需高效的语义分割方法。

Method: 研究提出BioLite U-Net架构，利用深度可分离卷积降低计算量，并使用787张标注RGB图像的数据集进行评估。

Result: BioLite U-Net在Raspberry Pi 4B上实现了92.85%的mIoU和96.17%的Dice分数，帧处理时间为335毫秒。

Conclusion: BioLite U-Net在准确性、效率和可部署性上表现优异，适用于智能闭环生物打印系统。

Abstract: Bioprinting is a rapidly advancing field that offers a transformative
approach to fabricating tissue and organ models through the precise deposition
of cell-laden bioinks. Ensuring the fidelity and consistency of printed
structures in real-time remains a core challenge, particularly under
constraints imposed by limited imaging data and resource-constrained embedded
hardware. Semantic segmentation of the extrusion process, differentiating
between nozzle, extruded bioink, and surrounding background, enables in situ
monitoring critical to maintaining print quality and biological viability. In
this work, we introduce a lightweight semantic segmentation framework tailored
for real-time bioprinting applications. We present a novel, manually annotated
dataset comprising 787 RGB images captured during the bioprinting process,
labeled across three classes: nozzle, bioink, and background. To achieve fast
and efficient inference suitable for integration with bioprinting systems, we
propose a BioLite U-Net architecture that leverages depthwise separable
convolutions to drastically reduce computational load without compromising
accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based
segmentation baselines using mean Intersection over Union (mIoU), Dice score,
and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess
real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%
and a Dice score of 96.17%, while being over 1300x smaller than
MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,
demonstrating near real-time capability. Compared to MobileNet baselines,
BioLite U-Net offers a superior tradeoff between segmentation accuracy,
efficiency, and deployability, making it highly suitable for intelligent,
closed-loop bioprinting systems.

</details>


### [126] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: 开发了一个名为VILOD的交互式标注工具，通过视觉分析和人机协作优化目标检测的标注效率。


<details>
  <summary>Details</summary>
Motivation: 目标检测的深度学习方法需要大量标注数据，而传统主动学习缺乏透明性和人机协同。

Method: 结合视觉分析技术（如t-SNE、不确定性热图），设计了一个交互式标注工具VILOD。

Result: 实验表明VILOD能支持多种标注策略，效果与自动主动学习相当。

Conclusion: VILOD提高了人机协作标注的透明性和效率，为目标检测数据标注提供了新思路。

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [127] [Beamforming-LLM: What, Where and When Did I Miss?](https://arxiv.org/abs/2509.06221)
*Vishal Choudhari*

Main category: eess.AS

TL;DR: Beamforming-LLM是一个结合空间音频捕捉和检索增强生成技术的系统，用于在多说话者环境中语义检索用户可能错过的对话内容。


<details>
  <summary>Details</summary>
Motivation: 解决用户在多说话者环境中可能错过重要对话的问题，为智能听觉记忆系统和辅助技术提供基础。

Method: 通过波束形成分离方向性音频流，用Whisper转录，嵌入向量数据库；用户查询时检索相关段落，用轻量级大模型（GPT-4o-mini）总结。

Result: 系统提供对比性总结、空间上下文和带时间戳的音频播放功能。

Conclusion: 该工作为智能听觉记忆系统提供了基础，并在辅助技术、会议总结和空间计算中有广泛应用。

Abstract: We present Beamforming-LLM, a system that enables users to semantically
recall conversations they may have missed in multi-speaker environments. The
system combines spatial audio capture using a microphone array with
retrieval-augmented generation (RAG) to support natural language queries such
as, "What did I miss when I was following the conversation on dogs?"
Directional audio streams are separated using beamforming, transcribed with
Whisper, and embedded into a vector database using sentence encoders. Upon
receiving a user query, semantically relevant segments are retrieved,
temporally aligned with non-attended segments, and summarized using a
lightweight large language model (GPT-4o-mini). The result is a user-friendly
interface that provides contrastive summaries, spatial context, and timestamped
audio playback. This work lays the foundation for intelligent auditory memory
systems and has broad applications in assistive technology, meeting
summarization, and context-aware personal spatial computing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [128] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: PCN（Proof-Carrying Numbers）是一种用于防止大语言模型中数字幻觉的验证协议，通过渲染层的机械验证确保数字的准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）可能生成不符合实际数据的数字（数字幻觉），现有方法无法完全保证数字的真实性。

Method: PCN使用声明绑定令牌和结构化声明，通过验证器在渲染层进行机械验证，确保只有经过验证的数字才能显示为已验证。

Result: PCN实现了数字的可靠性验证，具备完整性、失败关闭行为和策略单调性。

Conclusion: PCN通过强制验证机制为大语言模型的数字输出提供了可信保障。

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [129] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: 论文介绍了EuroParlVote，一个新基准用于在政治敏感环境中评估大语言模型，揭示了模型在性别分类和投票预测任务中的偏见模式。


<details>
  <summary>Details</summary>
Motivation: 目的是评估大语言模型在政治敏感任务中的表现，尤其是性别和政治倾向相关的偏见问题。

Method: 使用EuroParlVote数据集，包含欧洲议会辩论演讲和投票结果，以及丰富的议员人口统计数据，进行性别分类和投票预测任务。

Result: 发现大语言模型在性别分类中常将女性议员误判为男性，且在模拟女性议员投票时准确性降低；政治倾向上偏向中间派，对极左和极右表现较差。GPT-4o在稳健性和公平性上优于开源模型。

Conclusion: 研究强调了在政治场景中NLP模型的公平性和责任问题，并发布了EuroParlVote数据集以支持未来研究。

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [130] [DeepStream: Prototyping Deep Joint Source-Channel Coding for Real-Time Multimedia Transmissions](https://arxiv.org/abs/2509.05971)
*Kaiyi Chi,Yinghui He,Qianqian Yang,Zhiping Jiang,Yuanchao Shu,Zhiqin Wang,Jun Luo,Jiming Chen*

Main category: eess.SP

TL;DR: DeepStream是一种基于OFDM的高效DeepJSCC原型，用于多媒体传输，通过优化映射和预处理方法以及渐进编码策略，显著提升了传输性能。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepJSCC在6G中低信噪比环境下的实际应用，验证其可行性。

Method: 开发特征到符号映射方法和跨子载波预处理方法，提出渐进编码策略，使用软件定义无线电实现实时传输。

Result: 在10 dB SNR下，DeepStream的图像传输PSNR达35 dB，视频流MS-SSIM达20 dB，显著优于标准方案。

Conclusion: DeepStream验证了DeepJSCC的实际可行性，并展示了其在多媒体传输中的优越性能。

Abstract: Deep learning-based joint source-channel coding (DeepJSCC) has emerged as a
promising technique in 6G for enhancing the efficiency and reliability of data
transmission across diverse modalities, particularly in low signal-to-noise
ratio (SNR) environments. This advantage is realized by leveraging powerful
neural networks to learn an optimal end-to-end mapping from the source data
directly to the transmit symbol sequence, eliminating the need for separate
source coding, channel coding, and modulation. Although numerous efforts have
been made towards efficient DeepJSCC, they have largely stayed at numerical
simulations that can be far from practice, leaving the real-world viability of
DeepJSCC largely unverified. To this end, we prototype DeepStream upon
orthogonal frequency division multiplexing (OFDM) technology to offer efficient
and robust DeepJSCC for multimedia transmission. In conforming to OFDM, we
develop both a feature-to-symbol mapping method and a cross-subcarrier
precoding method to improve the subcarrier independence and reduce
peak-to-average power ratio. To reduce system complexity and enable flexibility
in accommodating varying quality of service requirements, we further propose a
progressive coding strategy that adjusts the compression ratio based on latency
with minimal performance loss. We implement DeepStream for real-time image
transmission and video streaming using software-defined radio. Extensive
evaluations verify that DeepStream outperforms both the standard scheme and the
direct deployment scheme. Particularly, at an SNR of 10 dB, DeepStream achieves
a PSNR of 35 dB for image transmission and an MS-SSIM of 20 dB for video
streaming, whereas the standard scheme fails to recover meaningful information.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [131] [From Digital Distrust to Codified Honesty: Experimental Evidence on Generative AI in Credence Goods Markets](https://arxiv.org/abs/2509.06069)
*Alexander Erlei*

Main category: econ.GN

TL;DR: 生成式AI正在改变专家服务的提供方式，研究发现Human-Human市场的效率通常高于AI-AI和Human-AI市场，但LLM专家仍能获得更高的剩余。透明规则下，Human-AI-Human市场表现优于Human-Human市场。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI（如LLMs）在专家服务市场中的行为、福利和分配影响，揭示其潜在的机遇与风险。

Method: 通过一系列一次性实验，利用信用商品框架，分析AI-AI、Human-Human、Human-AI和Human-AI-Human市场的表现。

Result: Human-Human市场通过亲社会专家偏好和更高的消费者信任实现更高效率；透明规则下，Human-AI-Human市场效率更优。

Conclusion: LLMs在专家服务中既可能通过自动化减少亲社会行为，也可能通过透明规则提升效率，引发监管挑战。

Abstract: Generative AI is transforming the provision of expert services. This article
uses a series of one-shot experiments to quantify the behavioral, welfare and
distribution consequences of large language models (LLMs) on AI-AI,
Human-Human, Human-AI and Human-AI-Human expert markets. Using a credence goods
framework where experts have private information about the optimal service for
consumers, we find that Human-Human markets generally achieve higher levels of
efficiency than AI-AI and Human-AI markets through pro-social expert
preferences and higher consumer trust. Notably, LLM experts still earn
substantially higher surplus than human experts -- at the expense of consumer
surplus - suggesting adverse incentives that may spur the harmful deployment of
LLMs. Concurrently, a majority of human experts chooses to rely on LLM agents
when given the opportunity in Human-AI-Human markets, especially if they have
agency over the LLM's (social) objective function. Here, a large share of
experts prioritizes efficiency-loving preferences over pure self-interest.
Disclosing these preferences to consumers induces strong efficiency gains by
marginalizing self-interested LLM experts and human experts. Consequently,
Human-AI-Human markets outperform Human-Human markets under transparency rules.
With obfuscation, however, efficiency gains disappear, and adverse expert
incentives remain. Our results shed light on the potential opportunities and
risks of disseminating LLMs in the context of expert services and raise several
regulatory challenges. On the one hand, LLMs can negatively affect human trust
in the presence of information asymmetries and partially crowd-out experts'
other-regarding preferences through automation. On the other hand, LLMs allow
experts to codify and communicate their objective function, which reduces
information asymmetries and increases efficiency.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [132] [The compact double category $\mathbf{Int}(\mathbf{Poly}_*)$ models control flow and data transformations](https://arxiv.org/abs/2509.05462)
*Grigory Kondyrev,David I. Spivak*

Main category: math.CT

TL;DR: 该论文定义了一个操作数$\mathscr{W}$，用于建模包含数据转换的控制流，并探讨了其在双范畴中的扩展和通用性质。


<details>
  <summary>Details</summary>
Motivation: 动机是扩展Hasegawa的工作，将数据转换（如删除、复制、排列等）纳入控制流建模，以更全面地描述编程语言的动态行为。

Method: 方法包括定义操作数$\mathscr{W}$，证明$\mathbf{Poly}_*$及其多变量版本的可追踪性，并在一致条件下扩展为双范畴$\mathbb{I}\mathbf{nt}(\mathscr{T})$。

Result: 结果显示，操作数$\mathscr{W}$和双范畴$\mathbb{I}\mathbf{nt}(\mathscr{T})$能够有效建模控制流和数据转换，满足通用性质。

Conclusion: 结论表明，该方法为编程语言的控制流和数据转换提供了强大的数学工具，具有潜在的应用价值。

Abstract: Hasegawa showed that control flow in programming languages -- while loops and
if-then-else statements -- can be modeled using traced cocartesian categories,
such as the category $\mathbf{Set}_*$ of pointed sets. In this paper we define
an operad $\mathscr{W}$ of wiring diagrams that provides syntax for categories
whose control flow moreover includes data transformations, including deleting,
duplicating, permuting, and applying pre-specified functions to variables. In
the most basic version, the operad underlies $\mathbf{Int}(\mathbf{Poly}_*)$,
where $\mathbf{Int}(\mathscr{T})$ denotes the free compact category on a traced
category $\mathscr{T}$, as defined by Joyal, Street, and Verity; to do so, we
show that $\mathbf{Poly}_*$, as well as any multivariate version of it, is
traced. We show moreover that whenever $\mathscr{T}$ is uniform -- a condition
also defined by Hasegawa and satisfied by $\mathbf{Int}(\mathscr{T})$ -- the
resulting $\mathbf{Int}$-construction extends to a double category
$\mathbb{I}\mathbf{nt}(\mathscr{T})$, which is compact in the sense of
Patterson. Finally, we define a universal property of the double category
$\mathbb{I}\mathbf{nt}(\mathbf{Poly}_*)$ and
$\mathbb{I}\mathbf{nt}(\mathbf{Set}_*)$ by which one can track trajectories as
they move through the control flow associated to a wiring diagram.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [133] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: MambaLite-Micro 是首个在资源受限的微控制器（MCU）上部署 Mamba 模型的解决方案，通过优化内存和操作符实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决 Mamba 模型在 MCU 上部署时遇到的内存限制、缺乏原生操作符支持和嵌入式工具链的问题。

Method: 通过导出轻量级模型权重格式，并用 C 语言实现手工优化的 Mamba 层、操作符融合和内存布局优化。

Result: 减少了 83.0% 的峰值内存，数值误差仅为 1.7x10-5，在关键词识别和人类活动识别任务中与 PyTorch 基线完全一致。

Conclusion: MambaLite-Micro 展示了在不同嵌入式平台上的便携性，为 Mamba 等高级序列模型在资源受限场景中的应用铺平了道路。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [134] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: 这篇论文研究了融合潜在数据表示与独立数据表示在城市数据分析中的效果差异，发现融合表示能生成更结构化的模式。


<details>
  <summary>Details</summary>
Motivation: 针对城市数据的复杂性和多样性，研究融合潜在表示是否能提供比独立数据源更深入的洞察。

Method: 开发了一个可视化辅助框架，比较融合与独立潜在数据表示在动态和静态城市数据分析中的效果。

Result: 融合潜在表示能生成更结构化的模式，而独立表示在特定情况下也有用。

Conclusion: 融合潜在表示在城市数据分析中更具优势，但独立表示仍有其适用场景。

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [135] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent是一种基于LLM的自动化模型压缩方法，通过静态和动态指标优化剪枝和量化策略，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在资源受限平台上部署时遇到的算力和内存瓶颈问题，传统压缩方法通常忽略架构和运行时异质性。

Method: 使用LLM构建模块化多智能体系统，结合静态指标（如MACs、参数量）和动态信号（延迟、内存）设计特定架构的剪枝和量化策略。

Result: 在多个数据集和模型上，剪枝保持或提升准确率（ImageNet-1K下降约1%，ViT-B/16在小数据集上提升2%），量化节省74%内存且精度损失小于0.5%。

Conclusion: Agentic系统是扩展性强且高效的优化方法，LLM推理质量对迭代剪枝至关重要。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [136] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于时空图卷积网络（ST-GCN）的方法，用于快速预测大规模时序电路中的长周期故障影响概率（FIPs），以支持定量风险评估。该方法显著减少了仿真时间并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 静默数据错误（SDEs）会导致安全关键系统的性能和可靠性下降。传统的功能测试检测SDE相关故障成本高且耗时，因此需要一种高效且准确的预测方法。

Method: 通过将门级网表建模为时空图，捕捉电路拓扑和信号时序；使用专用的空间和时间编码器高效预测多周期FIPs。

Result: 在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持了较高的预测精度（5周期预测的平均绝对误差为0.024）。

Conclusion: 该方法不仅适用于测试点选择的优化，还能扩展到SoC级别的测试策略优化，适用于电子设计自动化流程。

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [137] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: MCIGLE是一种新颖的框架，通过多模态图特征对齐和利用串联递归最小二乘法来解决类增量学习中的问题，如灾难性遗忘和分布偏差。


<details>
  <summary>Details</summary>
Motivation: 随着多模态图结构数据的普及，现有方法在灾难性遗忘、分布偏差、内存限制和弱泛化问题上表现不佳。

Method: MCIGLE通过提取和对齐多模态图特征，并应用串联递归最小二乘法来保留知识。

Result: 在公共数据集上的实验验证了其有效性和泛化能力。

Conclusion: MCIGLE在平衡准确性和内存保存方面表现优异，适用于多模态图数据的类增量学习。

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [138] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: 论文提出了一种名为X-SQL的新颖Text-to-SQL框架，强调数据库模式信息的重要性，并通过X-Linking和X-Admin组件提升模式链接和理解能力，最终在Spider数据集上取得了领先的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成任务中表现出色，但Text-to-SQL任务中数据库模式信息的重要性常被忽视。研究发现模式信息对生成高质量SQL查询至关重要，因此需要对此进行深入研究。

Method: 提出了X-SQL框架，包含两个组件：X-Linking（基于LLM的有监督微调方法）和X-Admin（专注于模式理解）。此外，系统采用Multi-LLMs策略进一步提升性能。

Result: X-SQL在Spider-Dev和Spider-Test数据集上的执行准确率分别达到84.9%和82.5%，成为基于开源模型的领先Text-to-SQL框架。

Conclusion: 研究证明数据库模式信息在Text-to-SQL任务中至关重要，X-SQL框架通过新颖的组件设计和Multi-LLMs策略显著提升了性能。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [139] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: 该论文提出了一种通过近似计算减少神经网络中小型算术运算符（如加法器和乘法器）面积的方法，改进了一种称为XPAT的布尔重写技术，并通过实验验证了新方法在降低面积和提升近似效果上的优越性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署神经网络时，需要在推理能量消耗和分类准确性之间取得平衡，近似计算是一种有效的方法。论文旨在通过改进XPAT技术，进一步减少算术运算符的面积，同时保持较高的准确性。

Method: 论文提出了一种基于参数化乘积共享的新模板，作为合成面积的近似代理，改进了XPAT的布尔重写技术。该方法利用模板参数作为目标指标的代理，优化电路的面积和准确性。

Result: 实验证明，新方法能更好地收敛到低面积解决方案，并且在近似效果上优于原始的XPAT和两种其他先进方法。

Conclusion: 论文提出的方法在减少算术运算符面积和优化准确性方面表现出色，为边缘设备上的神经网络部署提供了更高效的解决方案。

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [140] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 论文提出了一种结合数据并行和完全解耦并行反向传播算法的分布式训练方法，以提高DNN的训练效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练时间长，尤其是深层网络和大数据集时，需解决这一挑战。

Method: 集成数据并行和解耦并行反向传播算法，利用多计算单元并行处理数据并减少锁问题。

Result: 方法在CIFAR-10数据集上验证有效，理论证明其在特定条件下收敛。

Conclusion: 提出的方法显著提升了训练效率并具备理论保证。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [141] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: Persona是一种新颖的个性化方法，通过原型驱动的无反向传播参数编辑框架，提升轻量级设备模型的泛化能力，无需部署后重新训练。


<details>
  <summary>Details</summary>
Motivation: 设备上的实时数据分布变化对轻量级设备模型的泛化能力提出了挑战，而现有研究主要依赖数据密集且计算成本高的微调方法，这一问题亟待解决。

Method: Persona利用云端神经适配器生成参数编辑矩阵，基于实时设备数据动态调整模型参数，并通过原型聚类和多层知识转移实现高效模型演化。

Result: 在视觉任务和推荐任务的多数据集实验中，Persona表现出显著的有效性和广泛适用性。

Conclusion: Persona提供了一种高效、无需重新训练的个性化方法，显著提升了设备模型的泛化能力，解决了实时数据分布变化的挑战。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [142] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: 提出了一种基于对比学习的网络入侵检测新方法CLAN，通过将增强样本视为负面视图以提升分类效果，实验证明优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有监督学习和异常检测方法在依赖标注数据和高误报率上的局限性，探索自监督学习在网络入侵检测中的应用。

Method: 采用对比自监督学习（CLAN），将增强样本作为负面对，其他良性样本作为正面对，以此学习区分潜在恶意流量的隐表示。

Result: 在Lycos2017数据集上，CLAN在二元分类任务中优于现有技术，且在有限标注数据下微调后，多类分类性能更优。

Conclusion: CLAN通过创新的负面视图定义，有效提升网络入侵检测的准确性和效率，为实际应用提供新思路。

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [143] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: 提出一种简单算法，仅需少量图数据样本即可学习适用于多种几何随机图的本地路由策略，解决了全对近似最短路径问题。


<details>
  <summary>Details</summary>
Motivation: 通过利用深度学习，设计高效且可扩展的路由策略，仅需节点及其邻居状态即可实现路由，解决传统路由方法的局限性。

Method: 算法结合网络领域知识选择输入特征和策略函数，训练深度神经网络（DNN）学习近似最优策略，并验证其泛化能力。

Result: 训练出的DNN之一（仅使用目标距离作为输入）与贪婪转发策略完全匹配；新策略GreedyTensile结合目标距离和节点拉伸特征，性能更优。

Conclusion: GreedyTensile路由具有可解释性和超低延迟运行特性，通过符号化解释其DNN为低复杂度线性操作，展示了其实际应用潜力。

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [144] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: PyFair是一个用于评估和验证深度神经网络（DNN）个体公平性的正式框架，通过双网络架构和路径约束生成实现系统性公平性分析。


<details>
  <summary>Details</summary>
Motivation: 旨在为预训练DNN提供一个严格的、系统性的公平性测试和验证方法，以推动关键领域中的算法公平性。

Method: 利用PyCT工具生成公平性特定的路径约束，并通过双网络架构系统性探索DNN行为。

Result: 在25个基准模型上验证了PyFair的有效性，能够检测歧视性实例并验证公平性，但在复杂模型上存在扩展性挑战。

Conclusion: PyFair为DNN的公平性评估提供了一种创新且系统的方法，尽管在处理复杂模型时需要进一步优化扩展性。

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>
