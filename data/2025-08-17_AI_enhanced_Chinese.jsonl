{"id": "2508.10059", "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "categories": ["cs.SE"], "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad\u901a\u8fc7\u5c06\u5f62\u5f0f\u5316\u65b9\u6cd5\u96c6\u6210\u5230\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u5faa\u73af\u4e2d\uff0c\u89e3\u51b3\u4e86LLM\u751f\u6210\u4ee3\u7801\u5728\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u4e25\u683c\u7ea6\u675f\u9886\u57df\u4e2d\u7f3a\u4e4f\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165FormalGrad\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u89c6\u4e3a\u53ef\u5fae\u5206\u53d8\u91cf\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u7ea6\u675f\u548c\u7ed3\u6784\u5316\u53cd\u9988\u751f\u6210\u6587\u672c\u4f2a\u68af\u5ea6\uff0c\u6307\u5bfc\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728HumanEval\u3001HumanEval+\u548cLiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u8fbe27%\uff0c\u76f8\u5bf9\u63d0\u534741%\u3002", "conclusion": "FormalGrad\u80fd\u591f\u751f\u6210\u5f62\u5f0f\u5316\u8bc1\u660e\u7684\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u4ee3\u7801\uff0c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u9760AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.10068", "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder\u662f\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u7279\u5f81\u4f18\u5316\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u5173\u7cfb\u3001\u53bb\u91cd\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u8bc4\u4f30\u548c\u591a\u6837\u6027\u91cd\u6392\u5e8f\uff0c\u89e3\u51b3\u4e86\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u8bed\u4e49\u8bef\u5bfc\u548c\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4ee3\u7801\u8865\u5168\u4e2d\u56e0\u6587\u672c\u76f8\u4f3c\u6027\u5bfc\u81f4\u7684\u8bed\u4e49\u8bef\u5bfc\u3001\u5197\u4f59\u548c\u540c\u8d28\u5316\uff0c\u4ee5\u53ca\u5916\u90e8\u7b26\u53f7\u6b67\u4e49\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Hierarchical Feature Optimization\u6a21\u5757\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u5173\u7cfb\u3001\u53bb\u91cd\u3001\u56fe\u7ed3\u6784\u76f8\u4f3c\u6027\u8bc4\u4f30\u548c\u91cd\u6392\u5e8f\uff0c\u5e76\u91c7\u7528External-Aware Identifier Disambiguator\u6a21\u5757\u89e3\u6790\u8de8\u6587\u4ef6\u7b26\u53f7\u6b67\u4e49\u3002", "result": "\u5728CrossCodeEval\u548cRepoEval-Updated\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u6a21\u578b\u3002", "conclusion": "\u7cfb\u7edf\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\u53ef\u4e3a\u6784\u5efa\u66f4\u51c6\u786e\u3001\u7a33\u5065\u7684\u4ee3\u7801\u8865\u5168\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.10074", "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Next Edit Prediction\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u8005\u7684\u4ea4\u4e92\u5386\u53f2\u9884\u6d4b\u4e0b\u4e00\u4e2a\u7f16\u8f91\u52a8\u4f5c\u7684\u4f4d\u7f6e\u548c\u5185\u5bb9\uff0c\u4ee5\u6539\u5584AI\u4ee3\u7801\u52a9\u624b\u7684\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u5b8c\u6210\u548c\u804a\u5929\u5f0f\u7f16\u8f91\u65b9\u5f0f\u65e0\u6cd5\u4e3b\u52a8\u9884\u6d4b\u5f00\u53d1\u8005\u7684\u8fde\u7eed\u7f16\u8f91\u610f\u56fe\uff0c\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5bf9\u4e00\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u4e86\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u4f5c\u8005\u53d1\u73b0\u4e86\u4e00\u4e9b\u65b0\u7684\u7ed3\u8bba\uff0c\u4e3a\u672a\u6765\u7684\u4ea4\u4e92\u6a21\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u6a21\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u53ef\u4ee5\u4e3b\u52a8\u9884\u6d4b\u5f00\u53d1\u8005\u7684\u52a8\u4f5c\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u54cd\u5e94\u660e\u786e\u7684\u6307\u4ee4\u3002"}}
{"id": "2508.10157", "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "categories": ["cs.SE"], "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PTLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u8de8\u5e73\u53f0\u5f00\u53d1\uff08\u5982GitHub\u548cHugging Face\uff09\u7684\u534f\u8c03\u95ee\u9898\u5bfc\u81f4\u7248\u672c\u4e0d\u4e00\u81f4\u548c\u540c\u6b65\u56f0\u96be\uff0c\u7814\u7a76\u53d1\u73b0\u516b\u79cd\u540c\u6b65\u6a21\u5f0f\u5e76\u63ed\u793a\u4e86\u7ed3\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76PTLM\u5728GitHub\u548cHugging Face\u4e0a\u7684\u8de8\u5e73\u53f0\u5f00\u53d1\u534f\u8c03\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u7248\u672c\u4e0d\u4e00\u81f4\u3001\u540c\u6b65\u56f0\u96be\u53ca\u7531\u6b64\u5e26\u6765\u7684\u7528\u6237\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76325\u4e2aPTLM\u5bb6\u65cf\uff08904\u4e2aHugging Face\u53d8\u4f53\uff09\uff0c\u5206\u6790\u63d0\u4ea4\u6d3b\u52a8\u7684\u534f\u8c03\u65b9\u5f0f\u53ca\u540c\u6b65\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u516b\u79cd\u540c\u6b65\u6a21\u5f0f\uff0c\u90e8\u5206\u540c\u6b65\u6a21\u5f0f\uff08\u5982\u5206\u6563\u540c\u6b65\u548c\u7a00\u758f\u540c\u6b65\uff09\u63ed\u793a\u8de8\u5e73\u53f0\u53d1\u5e03\u5b9e\u8df5\u4e2d\u7684\u7ed3\u6784\u6027\u8131\u8282\uff0c\u5bfc\u81f4\u6539\u8fdb\u6216\u4fee\u590d\u672a\u80fd\u8de8\u5e73\u53f0\u590d\u5236\u3002", "conclusion": "\u8bc6\u522b\u8fd9\u4e9b\u540c\u6b65\u6a21\u5f0f\u5bf9\u6539\u8fdbPTLM\u53d1\u5e03\u6d41\u7a0b\u7684\u76d1\u7763\u548c\u53ef\u8ffd\u6eaf\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.10201", "pdf": "https://arxiv.org/pdf/2508.10201", "abs": "https://arxiv.org/abs/2508.10201", "authors": ["Yilin Liu", "Niladri Shekhar Dutt", "Changjian Li", "Niloy J. Mitra"], "title": "B-repLer: Semantic B-rep Latent Editor using Large Language Models", "categories": ["cs.GR"], "comment": null, "summary": "Multimodal large language models (mLLMs), trained in a mixed modal setting as\na universal model, have been shown to compete with or even outperform many\nspecialized algorithms for imaging and graphics tasks. As demonstrated across\nmany applications, mLLMs' ability to jointly process image and text data makes\nthem suitable for zero-shot applications or efficient fine-tuning towards\nspecialized tasks. However, they have had limited success in 3D analysis and\nediting tasks. This is due to both the lack of suitable (annotated) 3D data as\nwell as the idiosyncrasies of 3D representations. In this paper, we investigate\nwhether mLLMs can be adapted to support high-level editing of Boundary\nRepresentation (B-rep) CAD objects. B-reps remain the industry-standard for\nprecisely encoding engineering objects, but are challenging as the\nrepresentation is fragile (i.e. can easily lead to invalid CAD objects) and no\npublicly available data source exists with semantically-annotated B-reps or CAD\nconstruction history. We present B-repLer as a finetuned mLLM that can\nunderstand text prompts and make semantic edits on given B-Reps to produce\nvalid outputs. We enable this via a novel multimodal architecture, specifically\ndesigned to handle B-rep models, and demonstrate how existing CAD tools, in\nconjunction with mLLMs, can be used to automatically generate the required\nreasoning dataset, without relying on external annotations. We extensively\nevaluate B-repLer and demonstrate several text-based B-rep edits of various\ncomplexity, which were not previously possible.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08mLLMs\uff09\u5e94\u7528\u4e8e\u9ad8\u5c42\u6b21\u7684\u8fb9\u754c\u8868\u793a\uff08B-rep\uff09CAD\u5bf9\u8c61\u7f16\u8f91\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aB-repLer\u7684\u5fae\u8c03\u6a21\u578b\uff0c\u80fd\u591f\u7406\u89e3\u6587\u672c\u63d0\u793a\u5e76\u5bf9B-reps\u8fdb\u884c\u8bed\u4e49\u7f16\u8f91\u3002", "motivation": "\u5c3d\u7ba1mLLMs\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u57283D\u5206\u6790\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\u3002B-reps\u4f5c\u4e3a\u5de5\u7a0b\u5bf9\u8c61\u7684\u884c\u4e1a\u6807\u51c6\uff0c\u7f3a\u4e4f\u516c\u5f00\u7684\u6807\u6ce8\u6570\u636e\u548c\u5de5\u5177\u652f\u6301\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u67b6\u6784B-repLer\uff0c\u80fd\u591f\u5904\u7406B-rep\u6a21\u578b\uff0c\u5e76\u5229\u7528\u73b0\u6709CAD\u5de5\u5177\u81ea\u52a8\u751f\u6210\u63a8\u7406\u6570\u636e\u96c6\uff0c\u65e0\u9700\u5916\u90e8\u6807\u6ce8\u3002", "result": "B-repLer\u80fd\u591f\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u5bf9B-reps\u8fdb\u884c\u590d\u6742\u8bed\u4e49\u7f16\u8f91\uff0c\u751f\u6210\u6709\u6548\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u4ee5\u5f80\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cmLLMs\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u67b6\u6784\u548c\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u9002\u5e94B-rep\u7f16\u8f91\u4efb\u52a1\uff0c\u4e3aCAD\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.10438", "pdf": "https://arxiv.org/pdf/2508.10438", "abs": "https://arxiv.org/abs/2508.10438", "authors": ["Yifan He", "Munyque Mittelmann", "Aniello Murano", "Abdallah Saffidine", "Michael Thielscher"], "title": "Repairing General Game Descriptions (extended version)", "categories": ["cs.LO"], "comment": null, "summary": "The Game Description Language (GDL) is a widely used formalism for specifying\nthe rules of general games. Writing correct GDL descriptions can be\nchallenging, especially for non-experts. Automated theorem proving has been\nproposed to assist game design by verifying if a GDL description satisfies\ndesirable logical properties. However, when a description is proved to be\nfaulty, the repair task itself can only be done manually. Motivated by the work\non repairing unsolvable planning domain descriptions, we define a more general\nproblem of finding minimal repairs for GDL descriptions that violate formal\nrequirements, and we provide complexity results for various computational\nproblems related to minimal repair. Moreover, we present an Answer Set\nProgramming-based encoding for solving the minimal repair problem and\ndemonstrate its application for automatically repairing ill-defined game\ndescriptions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u4fee\u590d\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fee\u6b63\u4e0d\u7b26\u5408\u903b\u8f91\u8981\u6c42\u7684Game Description Language (GDL)\u63cf\u8ff0\u3002", "motivation": "GDL\u63cf\u8ff0\u7684\u7f16\u5199\u5bf9\u975e\u4e13\u5bb6\u6765\u8bf4\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u73b0\u6709\u7684\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u65e0\u6cd5\u81ea\u52a8\u4fee\u590d\u9519\u8bef\uff0c\u9700\u8981\u624b\u52a8\u5b8c\u6210\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u4e86GDL\u63cf\u8ff0\u7684\u6700\u5c0f\u4fee\u590d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u76f8\u5173\u8ba1\u7b97\u95ee\u9898\u7684\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eAnswer Set Programming\u7684\u7f16\u7801\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u4fee\u590d\u4e0d\u7b26\u5408\u903b\u8f91\u8981\u6c42\u7684\u6e38\u620f\u63cf\u8ff0\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aGDL\u63cf\u8ff0\u7684\u81ea\u52a8\u5316\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10580", "pdf": "https://arxiv.org/pdf/2508.10580", "abs": "https://arxiv.org/abs/2508.10580", "authors": ["Jason Clarke", "Yoshihiko Gotoh", "Stefan Goetze"], "title": "Ensembling Synchronisation-based and Face-Voice Association Paradigms for Robust Active Speaker Detection in Egocentric Recordings", "categories": ["cs.MM", "cs.SD"], "comment": "Accepted to SPECOM 2025, 13 pages, 4 figures. To appear in the\n  Proceedings of the 27th International Conference on Speech and Computer\n  (SPECOM) 2025, October 13-14, 2025, Szeged, Hungary", "summary": "Audiovisual active speaker detection (ASD) in egocentric recordings is\nchallenged by frequent occlusions, motion blur, and audio interference, which\nundermine the discernability of temporal synchrony between lip movement and\nspeech. Traditional synchronisation-based systems perform well under clean\nconditions but degrade sharply in first-person recordings. Conversely,\nface-voice association (FVA)-based methods forgo synchronisation modelling in\nfavour of cross-modal biometric matching, exhibiting robustness to transient\nvisual corruption but suffering when overlapping speech or front-end\nsegmentation errors occur. In this paper, a simple yet effective ensemble\napproach is proposed to fuse synchronisation-dependent and\nsynchronisation-agnostic model outputs via weighted averaging, thereby\nharnessing complementary cues without introducing complex fusion architectures.\nA refined preprocessing pipeline for the FVA-based component is also introduced\nto optimise ensemble integration. Experiments on the Ego4D-AVD validation set\ndemonstrate that the ensemble attains 70.2% and 66.7% mean Average Precision\n(mAP) with TalkNet and Light-ASD backbones, respectively. A qualitative\nanalysis stratified by face image quality and utterance masking prevalence\nfurther substantiates the complementary strengths of each component.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u52a0\u6743\u5e73\u5747\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u540c\u6b65\u4f9d\u8d56\u548c\u540c\u6b65\u65e0\u5173\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0b\u7684\u4e3b\u52a8\u8bf4\u8bdd\u4eba\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bb0\u5f55\u4e2d\uff0c\u9891\u7e41\u7684\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u97f3\u9891\u5e72\u6270\u964d\u4f4e\u4e86\u5634\u5507\u8fd0\u52a8\u4e0e\u8bed\u97f3\u540c\u6b65\u7684\u53ef\u8fa8\u522b\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u96c6\u6210\u540c\u6b65\u4f9d\u8d56\u548c\u540c\u6b65\u65e0\u5173\u6a21\u578b\uff0c\u5e76\u4f18\u5316\u4e86\u9884\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u5728Ego4D-AVD\u9a8c\u8bc1\u96c6\u4e0a\uff0cmAP\u5206\u522b\u8fbe\u523070.2%\u548c66.7%\u3002", "conclusion": "\u96c6\u6210\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4e92\u8865\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.10781", "pdf": "https://arxiv.org/pdf/2508.10781", "abs": "https://arxiv.org/abs/2508.10781", "authors": ["Abtin Molavi", "Amanda Xu", "Ethan Cecchetti", "Swamit Tannu", "Aws Albarghouthi"], "title": "Generating Compilers for Qubit Mapping and Routing", "categories": ["cs.PL", "quant-ph"], "comment": null, "summary": "Quantum computers promise to solve important problems faster than classical\ncomputers, potentially unlocking breakthroughs in materials science, chemistry,\nand beyond. Optimizing compilers are key to realizing this potential, as they\nminimize expensive resource usage and limit error rates. A critical compilation\nstep is qubit mapping and routing (QMR), which finds mappings from circuit\nqubits to qubits on a target device and plans instruction execution while\nsatisfying the device's connectivity constraints. The challenge is that the\nlandscape of quantum architectures is incredibly diverse and fast-evolving.\nGiven this diversity, hundreds of papers have addressed the QMR problem for\ndifferent qubit hardware, connectivity constraints, and quantum error\ncorrection schemes.\n  We present an approach for automatically generating qubit mapping and routing\ncompilers for arbitrary quantum architectures. Though each QMR problem is\ndifferent, we identify a common core structure-device state machine-that we use\nto formulate an abstract QMR problem. Our formulation naturally leads to a\ndomain-specific language, Marol, for specifying QMR problems-for example, the\nwell-studied NISQ mapping and routing problem requires only 12 lines of Marol.\nWe demonstrate that QMR problems, defined in Marol, can be solved with a\npowerful parametric solver that can be instantiated for any Marol program. We\nevaluate our approach through case studies of important QMR problems from prior\nand recent work, covering noisy and fault-tolerant quantum architectures on all\nmajor hardware platforms. Our thorough evaluation shows that generated\ncompilers are competitive with handwritten, specialized compilers in terms of\nruntime and solution quality. We envision that our approach will simplify\ndevelopment of future quantum compilers as new quantum architectures continue\nto emerge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u9002\u7528\u4e8e\u4efb\u610f\u91cf\u5b50\u67b6\u6784\u7684\u91cf\u5b50\u6bd4\u7279\u6620\u5c04\u548c\u8def\u7531\uff08QMR\uff09\u7f16\u8bd1\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Marol\u8bed\u8a00\u7b80\u5316\u4e86QMR\u95ee\u9898\u7684\u5b9a\u4e49\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u89e3\u51b3\u65b9\u6848\u7684\u7ade\u4e89\u529b\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u6f5c\u529b\u4f9d\u8d56\u4e8e\u7f16\u8bd1\u5668\u7684\u4f18\u5316\uff0c\u800cQMR\u662f\u5173\u952e\u6b65\u9aa4\u3002\u4f46\u7531\u4e8e\u91cf\u5b50\u67b6\u6784\u7684\u591a\u6837\u6027\u548c\u5feb\u901f\u6f14\u53d8\uff0c\u624b\u5de5\u7f16\u5199\u7f16\u8bd1\u5668\u590d\u6742\u4e14\u8017\u65f6\u3002", "method": "\u4f5c\u8005\u8bc6\u522b\u4e86QMR\u95ee\u9898\u7684\u5171\u540c\u6838\u5fc3\u7ed3\u6784\uff08\u8bbe\u5907\u72b6\u6001\u673a\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e00\u95e8\u9886\u57df\u7279\u5b9a\u8bed\u8a00Marol\uff0c\u7528\u4e8e\u5b9a\u4e49QMR\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u5316\u6c42\u89e3\u5668\u89e3\u51b3\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\uff0c\u81ea\u52a8\u751f\u6210\u7684\u7f16\u8bd1\u5668\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u4e0e\u624b\u5de5\u7f16\u5199\u7684\u4e13\u4e1a\u7f16\u8bd1\u5668\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u672a\u6765\u91cf\u5b50\u7f16\u8bd1\u5668\u7684\u5f00\u53d1\uff0c\u9002\u5e94\u4e0d\u65ad\u6d8c\u73b0\u7684\u65b0\u91cf\u5b50\u67b6\u6784\u3002"}}
{"id": "2508.10251", "pdf": "https://arxiv.org/pdf/2508.10251", "abs": "https://arxiv.org/abs/2508.10251", "authors": ["Shweta Salaria", "Zhuoran Liu", "Nelson Mimura Gonzalez"], "title": "Meta-Metrics and Best Practices for System-Level Inference Performance Benchmarking", "categories": ["cs.PF"], "comment": null, "summary": "Benchmarking inference performance (speed) of Foundation Models such as Large\nLanguage Models (LLM) involves navigating a vast experimental landscape to\nunderstand the complex interactions between hardware and software components.\nHowever, evaluating every possible test configuration is impractical,\nunfeasible and unnecessary. To address this challenge, we introduce FMwork, a\ncomprehensive and methodical approach to creating a controlled testing\nenvironment that accurately reflects and characterizes performance. FMwork\ncomprises a set of benchmkaring best practices with three key components: 1)\nmeta-metrics, 2) parameter selection, and 3) strategic cost-performance\nevaluation. Meta-metrics account for time and resources spent on benchmarking\nand the relative accuracy of the results compared to a larger body of\nmeasurements, representing the complete experimental space. FMwork\noperationalizes the meta-metrics and provides efficient strategies for\nparameter selection and cost-performance analysis. Using the framework, we show\nup to 24x improvement (speedup and/or resource savings) running sweeps of\nexperiments compared to the ground truth. Even already considering a subset of\nexperiments as reference point (using the power of two for batch sizes),\nreducing experimental output size from 1024 to 128 tokens yields another 2.7x\ngain while keeping 96.6% accuracy for an evaluation using Llama 3.1 8B model.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86FMwork\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u901a\u8fc7\u5143\u6307\u6807\u3001\u53c2\u6570\u9009\u62e9\u548c\u6210\u672c\u6027\u80fd\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u6548\u7387\u3002", "motivation": "\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u9762\u4e34\u5b9e\u9a8c\u914d\u7f6e\u590d\u6742\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6d4b\u8bd5\u6d41\u7a0b\u3002", "method": "FMwork\u6846\u67b6\u5305\u62ec\u5143\u6307\u6807\u3001\u53c2\u6570\u9009\u62e9\u548c\u6218\u7565\u6210\u672c\u6027\u80fd\u8bc4\u4f30\uff0c\u901a\u8fc7\u4f18\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u51cf\u5c11\u4e0d\u5fc5\u8981\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFMwork\u80fd\u5b9e\u73b0\u6700\u9ad824\u500d\u7684\u6548\u7387\u63d0\u5347\uff0c\u5e76\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "FMwork\u4e3a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u4e86\u5b9e\u9a8c\u6d41\u7a0b\u548c\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2508.10247", "pdf": "https://arxiv.org/pdf/2508.10247", "abs": "https://arxiv.org/abs/2508.10247", "authors": ["Laura Landon", "Vipindev Adat Vasudevan", "Junmo Sung", "Muriel M\u00e9dard"], "title": "Rethinking Reliability Using Network Coding: a Practical 5G Evaluation", "categories": ["cs.NI"], "comment": "LCN Conference 2025", "summary": "This work presents the design and implementation of a real-time network\ncoding system integrated into the IP layer of a 5G testbed, offering an\nalternative to conventional retransmission-based reliability mechanisms such as\nARQ and HARQ. Using a netfilter-based packet interception framework, we inject\nforward erasure correction using Random Linear Network Coding (RLNC) into live\ntraffic between a gNB and UE over a 3GPP RF link. We evaluate a block coding\nscheme, analyzing its impact on throughput, jitter, and resource usage. Results\nshow that with appropriate code rate selection, RLNC can fully recover from\npacket losses using fewer transmissions than ARQ/HARQ and maintain a high\nthroughput, particularly under moderate-to-high packet loss rates. These\nfindings demonstrate that network coding can effectively replace\nretransmission-based reliability in future wireless systems, with the potential\nfor more efficient resource utilization.", "AI": {"tldr": "\u57285G\u6d4b\u8bd5\u5e73\u53f0\u7684IP\u5c42\u4e2d\u96c6\u6210\u5b9e\u65f6\u7f51\u7edc\u7f16\u7801\u7cfb\u7edf\uff0c\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e\u91cd\u4f20\u7684\u53ef\u9760\u6027\u673a\u5236\uff08\u5982ARQ/HARQ\uff09\uff0c\u901a\u8fc7RLNC\u5b9e\u73b0\u524d\u5411\u7ea0\u9519\uff0c\u9a8c\u8bc1\u5176\u5728\u541e\u5410\u91cf\u3001\u6296\u52a8\u548c\u8d44\u6e90\u4f7f\u7528\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u7f51\u7edc\u7f16\u7801\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u66ff\u4ee3\u4f20\u7edf\u91cd\u4f20\u673a\u5236\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8enetfilter\u7684\u6570\u636e\u5305\u62e6\u622a\u6846\u67b6\uff0c\u5728gNB\u4e0eUE\u4e4b\u95f4\u7684\u5b9e\u65f6\u6d41\u91cf\u4e2d\u6ce8\u5165RLNC\u7684\u5757\u7f16\u7801\u65b9\u6848\uff0c\u5206\u6790\u5176\u5bf9\u541e\u5410\u91cf\u3001\u6296\u52a8\u548c\u8d44\u6e90\u4f7f\u7528\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRLNC\u5728\u9002\u5ea6\u81f3\u9ad8\u4e22\u5305\u7387\u4e0b\u80fd\u4ee5\u66f4\u5c11\u7684\u4f20\u8f93\u6b21\u6570\u5b8c\u5168\u6062\u590d\u4e22\u5305\uff0c\u5e76\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u3002", "conclusion": "\u7f51\u7edc\u7f16\u7801\u672a\u6765\u53ef\u6709\u6548\u66ff\u4ee3\u57fa\u4e8e\u91cd\u4f20\u7684\u53ef\u9760\u6027\u673a\u5236\uff0c\u6709\u671b\u63d0\u5347\u65e0\u7ebf\u7cfb\u7edf\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2508.09995", "pdf": "https://arxiv.org/pdf/2508.09995", "abs": "https://arxiv.org/abs/2508.09995", "authors": ["Rui Zhou", "Haohui Ma", "Tianle Xin", "Lixin Zou", "Qiuyue Hu", "Hongxi Cheng", "Mingzhi Lin", "Jingjing Guo", "Sheng Wang", "Guoqing Zhang", "Yanjie Wei", "Liangzhen Zheng"], "title": "zERExtractor:An Automated Platform for Enzyme-Catalyzed Reaction Data Extraction from Scientific Literature", "categories": ["q-bio.BM", "cs.ET", "cs.LG"], "comment": null, "summary": "The rapid expansion of enzyme kinetics literature has outpaced the curation\ncapabilities of major biochemical databases, creating a substantial barrier to\nAI-driven modeling and knowledge discovery. We present zERExtractor, an\nautomated and extensible platform for comprehensive extraction of\nenzyme-catalyzed reaction and activity data from scientific literature.\nzERExtractor features a unified, modular architecture that supports\nplug-and-play integration of state-of-the-art models, including large language\nmodels (LLMs), as interchangeable components, enabling continuous system\nevolution alongside advances in AI. Our pipeline combines domain-adapted deep\nlearning, advanced OCR, semantic entity recognition, and prompt-driven LLM\nmodules, together with human expert corrections, to extract kinetic parameters\n(e.g., kcat, Km), enzyme sequences, substrate SMILES, experimental conditions,\nand molecular diagrams from heterogeneous document formats. Through active\nlearning strategies integrating AI-assisted annotation, expert validation, and\niterative refinement, the system adapts rapidly to new data sources. We also\nrelease a large benchmark dataset comprising over 1,000 annotated tables and\n5,000 biological fields from 270 P450-related enzymology publications.\nBenchmarking demonstrates that zERExtractor consistently outperforms existing\nbaselines in table recognition (Acc 89.9%), molecular image interpretation (up\nto 99.1%), and relation extraction (accuracy 94.2%). zERExtractor bridges the\nlongstanding data gap in enzyme kinetics with a flexible, plugin-ready\nframework and high-fidelity extraction, laying the groundwork for future\nAI-powered enzyme modeling and biochemical knowledge discovery.", "AI": {"tldr": "zERExtractor\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u9176\u50ac\u5316\u53cd\u5e94\u548c\u6d3b\u6027\u6570\u636e\uff0c\u6574\u5408\u4e86\u591a\u79cdAI\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u9176\u52a8\u529b\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u8d85\u8fc7\u4e86\u6570\u636e\u5e93\u7684\u6574\u7406\u80fd\u529b\uff0c\u963b\u788d\u4e86AI\u9a71\u52a8\u7684\u5efa\u6a21\u548c\u77e5\u8bc6\u53d1\u73b0\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86zERExtractor\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548cLLM\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u652f\u6301OCR\u3001\u8bed\u4e49\u5b9e\u4f53\u8bc6\u522b\u7b49\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u548c\u4e13\u5bb6\u9a8c\u8bc1\u4f18\u5316\u7cfb\u7edf\u3002", "result": "\u5728\u8868\u8bc6\u522b\u3001\u5206\u5b50\u56fe\u50cf\u89e3\u6790\u548c\u5173\u7cfb\u63d0\u53d6\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523089.9%\u300199.1%\u548c94.2%\u3002", "conclusion": "zERExtractor\u586b\u8865\u4e86\u9176\u52a8\u529b\u5b66\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7684AI\u5efa\u6a21\u548c\u751f\u5316\u77e5\u8bc6\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10160", "pdf": "https://arxiv.org/pdf/2508.10160", "abs": "https://arxiv.org/abs/2508.10160", "authors": ["Timon Merk", "Saeed Salehi", "Richard M. Koehler", "Qiming Cui", "Maria Olaru", "Amelia Hahn", "Nicole R. Provenza", "Simon Little", "Reza Abbasi-Asl", "Phil A. Starr", "Wolf-Julian Neumann"], "title": "Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training", "categories": ["cs.HC", "cs.LG"], "comment": "5 pages, 6 figures", "summary": "Neural decoding of pathological and physiological states can enable\npatient-individualized closed-loop neuromodulation therapy. Recent advances in\npre-trained large-scale foundation models offer the potential for generalized\nstate estimation without patient-individual training. Here we present a\nfoundation model trained on chronic longitudinal deep brain stimulation\nrecordings spanning over 24 days. Adhering to long time-scale symptom\nfluctuations, we highlight the extended context window of 30 minutes. We\npresent an optimized pre-training loss function for neural electrophysiological\ndata that corrects for the frequency bias of common masked auto-encoder loss\nfunctions due to the 1-over-f power law. We show in a downstream task the\ndecoding of Parkinson's disease symptoms with leave-one-subject-out\ncross-validation without patient-individual training.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u7801\u75c5\u7406\u548c\u751f\u7406\u72b6\u6001\uff0c\u652f\u6301\u4e2a\u4f53\u5316\u95ed\u73af\u795e\u7ecf\u8c03\u63a7\u6cbb\u7597\uff0c\u65e0\u9700\u60a3\u8005\u7279\u5f02\u6027\u8bad\u7ec3\u3002", "motivation": "\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u901a\u7528\u72b6\u6001\u4f30\u8ba1\u6f5c\u529b\uff0c\u5f00\u53d1\u65e0\u9700\u60a3\u8005\u4e2a\u4f53\u8bad\u7ec3\u7684\u795e\u7ecf\u89e3\u7801\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u6162\u6027\u7eb5\u5411\u6df1\u90e8\u8111\u523a\u6fc0\u8bb0\u5f55\u6570\u636e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u4ee5\u6821\u6b63\u9891\u7387\u504f\u5dee\uff0c\u5e76\u5c55\u793a30\u5206\u949f\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "result": "\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u89e3\u7801\u5e15\u91d1\u68ee\u75c5\u75c7\u72b6\uff0c\u91c7\u7528\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u65e0\u9700\u60a3\u8005\u4e2a\u4f53\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5728\u795e\u7ecf\u89e3\u7801\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u4e2a\u4f53\u5316\u795e\u7ecf\u8c03\u63a7\u6cbb\u7597\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.10303", "pdf": "https://arxiv.org/pdf/2508.10303", "abs": "https://arxiv.org/abs/2508.10303", "authors": ["Arkapravo Ghosh", "Abhishek Moitra", "Abhiroop Bhattacharjee", "Ruokai Yin", "Priyadarshini Panda"], "title": "DiffAxE: Diffusion-driven Hardware Accelerator Generation and Design Space Exploration", "categories": ["cs.AR"], "comment": "14 pages, 24 figures, 8 tables, 54 references", "summary": "Design space exploration (DSE) is critical for developing optimized hardware\narchitectures, especially for AI workloads such as deep neural networks (DNNs)\nand large language models (LLMs), which require specialized acceleration. As\nmodel complexity grows, accelerator design spaces have expanded to O(10^17),\nbecoming highly irregular, non-convex, and exhibiting many-to-one mappings from\ndesign configurations to performance metrics. This complexity renders direct\ninverse derivation infeasible and necessitates heuristic or sampling-based\noptimization. Conventional methods - including Bayesian optimization, gradient\ndescent, reinforcement learning, and genetic algorithms - depend on iterative\nsampling, resulting in long runtimes and sensitivity to initialization. Deep\nlearning-based approaches have reframed DSE as classification using\nrecommendation models, but remain limited to small-scale (O(10^3)), less\ncomplex design spaces. To overcome these constraints, we propose a generative\napproach that models hardware design as 1-D image synthesis conditioned on\ntarget performance, enabling efficient learning of non-differentiable,\nnon-bijective hardware-performance mappings. Our framework achieves 0.86% lower\ngeneration error than Bayesian optimization with a 17000x speedup, and\noutperforms GANDSE with 30% lower error at only 1.83x slower search. We further\nextend the method to a structured DSE setting, attaining 9.8% lower\nenergy-delay product (EDP) and 6% higher performance, with up to 145.6x and\n1312x faster search compared to existing optimization methods on O(10^17)\ndesign spaces. For LLM inference, our method achieves 3.37x and 7.75x lower EDP\non a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the\nstate-of-the-art DOSA framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u7684\u786c\u4ef6\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4e3a1-D\u56fe\u50cf\u751f\u6210\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5728\u5927\u89c4\u6a21\u3001\u975e\u51f8\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740AI\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982DNN\u548cLLM\uff09\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u589e\u957f\uff0c\u4f20\u7edf\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u96be\u4ee5\u5e94\u5bf9O(10^17)\u89c4\u6a21\u7684\u590d\u6742\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u751f\u6210\u6a21\u578b\u5c06\u786c\u4ef6\u8bbe\u8ba1\u5efa\u6a21\u4e3a1-D\u56fe\u50cf\u5408\u6210\uff0c\u4ee5\u76ee\u6807\u6027\u80fd\u4e3a\u6761\u4ef6\uff0c\u9ad8\u6548\u5b66\u4e60\u975e\u53ef\u5fae\u3001\u975e\u53cc\u5c04\u7684\u786c\u4ef6-\u6027\u80fd\u6620\u5c04\u5173\u7cfb\u3002", "result": "\u5728O(10^17)\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\uff0c\u751f\u6210\u8bef\u5dee\u6bd4\u8d1d\u53f6\u65af\u4f18\u5316\u4f4e0.86%\uff0c\u901f\u5ea6\u63d0\u534717000\u500d\uff1b\u7ed3\u6784\u5316DSE\u4e2d\uff0c\u80fd\u8017\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u964d\u4f4e9.8%\uff0c\u6027\u80fd\u63d0\u53476%\uff0c\u641c\u7d22\u901f\u5ea6\u6700\u9ad8\u63d0\u53471312\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u590d\u6742\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\uff0c\u4e3aAI\u786c\u4ef6\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10141", "pdf": "https://arxiv.org/pdf/2508.10141", "abs": "https://arxiv.org/abs/2508.10141", "authors": ["Laura Lawniczak", "Tobias Distler"], "title": "Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization", "categories": ["cs.DC"], "comment": "Extended version of publication in 44th International Symposium on\n  Reliable Distributed Systems (SRDS '25)", "summary": "Hybrid fault models are known to be an effective means for enhancing the\nrobustness of consensus-based replicated systems. However, existing\nhybridization approaches suffer from limited flexibility with regard to the\ncomposition of crash-tolerant and Byzantine fault-tolerant system parts and/or\nare associated with a significant diversification overhead. In this paper we\naddress these issues with ShellFT, a framework that leverages the concept of\nmicro replication to allow system designers to freely choose the parts of the\nreplication logic that need to be resilient against Byzantine faults. As a key\nbenefit, such a selective hybridization makes it possible to develop hybrid\nsolutions that are tailored to the specific characteristics and requirements of\nindividual use cases. To illustrate this flexibility, we present three custom\nShellFT protocols and analyze the complexity of their implementations. Our\nevaluation shows that compared with traditional hybridization approaches,\nShellFT is able to decrease diversification costs by more than 70%.", "AI": {"tldr": "ShellFT\u6846\u67b6\u901a\u8fc7\u5fae\u590d\u5236\u6280\u672f\u63d0\u5347\u4e86\u5171\u8bc6\u7cfb\u7edf\u4e2d\u6df7\u5408\u5bb9\u9519\u6a21\u578b\u7684\u7075\u6d3b\u6027\uff0c\u51cf\u5c11\u4e86\u591a\u6837\u5316\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u5bb9\u9519\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u6210\u672c\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u5fae\u590d\u5236\u6280\u672f\uff0c\u5141\u8bb8\u9009\u62e9\u6027\u589e\u5f3a\u62dc\u5360\u5ead\u5bb9\u9519\u7684\u90e8\u5206\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u591a\u6837\u5316\u6210\u672c\u964d\u4f4e70%\u4ee5\u4e0a\u3002", "conclusion": "ShellFT\u4e3a\u5b9a\u5236\u6df7\u5408\u5bb9\u9519\u65b9\u6848\u63d0\u4f9b\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.10373", "pdf": "https://arxiv.org/pdf/2508.10373", "abs": "https://arxiv.org/abs/2508.10373", "authors": ["Yingfan Liu", "Yandi Zhang", "Jiadong Xie", "Hui Li", "Jeffrey Xu Yu", "Jiangtao Cui"], "title": "Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional Data", "categories": ["cs.DB"], "comment": "This paper has been accepted by ICDE 2025", "summary": "In the era of cloud computing and AI, data owners outsource ubiquitous\nvectors to the cloud, which furnish approximate $k$-nearest neighbors\n($k$-ANNS) services to users. To protect data privacy against the untrusted\nserver, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental\nand urgent problem. However, existing PP-ANNS solutions fall short of meeting\nthe requirements of data privacy, efficiency, accuracy, and minimal user\ninvolvement concurrently. To tackle this challenge, we introduce a novel\nsolution that primarily executes PP-ANNS on a single cloud server to avoid the\nheavy communication overhead between the cloud and the user. To ensure data\nprivacy, we introduce a novel encryption method named distance comparison\nencryption, facilitating secure, efficient, and exact distance comparisons. To\noptimize the trade-off between data privacy and search performance, we design a\nprivacy-preserving index that combines the state-of-the-art $k$-ANNS method\nwith an approximate distance computation method. Then, we devise a search\nmethod using a filter-and-refine strategy based on the index. Moreover, we\nprovide the security analysis of our solution and conduct extensive experiments\nto demonstrate its superiority over existing solutions. Based on our\nexperimental results, our method accelerates PP-ANNS by up to 3 orders of\nmagnitude compared to state-of-the-art methods, while not compromising the\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9690\u79c1\u4fdd\u62a4k-ANNS\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u5355\u4e00\u4e91\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u907f\u514d\u4e86\u7528\u6237\u4e0e\u4e91\u4e4b\u95f4\u7684\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5728\u4e91\u8ba1\u7b97\u548cAI\u65f6\u4ee3\uff0c\u6570\u636e\u6240\u6709\u8005\u5c06\u6570\u636e\u5916\u5305\u7ed9\u4e91\u670d\u52a1\u5668\u4ee5\u63d0\u4f9bk-\u8fd1\u90bb\u641c\u7d22\u670d\u52a1\uff0c\u4f46\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u3001\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u6700\u5c0f\u5316\u7528\u6237\u53c2\u4e0e\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u540d\u4e3a\u8ddd\u79bb\u6bd4\u8f83\u52a0\u5bc6\u7684\u65b0\u52a0\u5bc6\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u9690\u79c1\u4fdd\u62a4\u7d22\u5f15\u7ed3\u5408\u5148\u8fdbk-ANNS\u65b9\u6cd5\u548c\u8fd1\u4f3c\u8ddd\u79bb\u8ba1\u7b97\uff0c\u91c7\u7528\u8fc7\u6ee4-\u7cbe\u70bc\u641c\u7d22\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6848\u5feb3\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.10517", "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "categories": ["cs.SE"], "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Solidity\u7248\u672c\u6f14\u5316\u4e2d\u7684\u7f16\u8bd1\u9519\u8bef\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u8fd9\u4e9b\u9519\u8bef\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u548cLLM\u7684\u65b0\u6846\u67b6SMCFIXER\u3002", "motivation": "Solidity\u7684\u9891\u7e41\u7248\u672c\u66f4\u65b0\u5e26\u6765\u4e86\u7f16\u8bd1\u9519\u8bef\u3001\u4ee3\u7801\u8fc1\u79fb\u548c\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u9700\u7814\u7a76\u5982\u4f55\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u667a\u80fd\u5408\u7ea6\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790Solidity\u7248\u672c\u6f14\u5316\u4e2d\u7684\u9519\u8bef\u95ee\u9898\uff0c\u7cfb\u7edf\u8bc4\u4f30LLMs\u7684\u4fee\u590d\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u68c0\u7d22\u548cLLM\u7684\u6846\u67b6SMCFIXER\u3002", "result": "\u7814\u7a76\u53d1\u73b081.68%\u7684\u5408\u7ea6\u5728\u4e0d\u540c\u7248\u672c\u4e2d\u9047\u5230\u7f16\u8bd1\u9519\u8bef\uff0cLLMs\u5728\u8bed\u4e49\u7ea7\u95ee\u9898\u4e0a\u6548\u679c\u6709\u9650\uff0c\u4f46SMCFIXER\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u51c6\u786e\u7387\uff0c\u8fbe\u523096.97%\u3002", "conclusion": "SMCFIXER\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u548cLLM\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86Solidity\u7248\u672c\u8fc1\u79fb\u4e2d\u7684\u7f16\u8bd1\u9519\u8bef\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u5408\u7ea6\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.10898", "pdf": "https://arxiv.org/pdf/2508.10898", "abs": "https://arxiv.org/abs/2508.10898", "authors": ["Chaoyue Song", "Xiu Li", "Fan Yang", "Zhongcong Xu", "Jiacheng Wei", "Fayao Liu", "Jiashi Feng", "Guosheng Lin", "Jianfeng Zhang"], "title": "Puppeteer: Rig and Animate Your 3D Models", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://chaoyuesong.github.io/Puppeteer/", "summary": "Modern interactive applications increasingly demand dynamic 3D content, yet\nthe transformation of static 3D models into animated assets constitutes a\nsignificant bottleneck in content creation pipelines. While recent advances in\ngenerative AI have revolutionized static 3D model creation, rigging and\nanimation continue to depend heavily on expert intervention. We present\nPuppeteer, a comprehensive framework that addresses both automatic rigging and\nanimation for diverse 3D objects. Our system first predicts plausible skeletal\nstructures via an auto-regressive transformer that introduces a joint-based\ntokenization strategy for compact representation and a hierarchical ordering\nmethodology with stochastic perturbation that enhances bidirectional learning\ncapabilities. It then infers skinning weights via an attention-based\narchitecture incorporating topology-aware joint attention that explicitly\nencodes inter-joint relationships based on skeletal graph distances. Finally,\nwe complement these rigging advances with a differentiable optimization-based\nanimation pipeline that generates stable, high-fidelity animations while being\ncomputationally more efficient than existing approaches. Extensive evaluations\nacross multiple benchmarks demonstrate that our method significantly\noutperforms state-of-the-art techniques in both skeletal prediction accuracy\nand skinning quality. The system robustly processes diverse 3D content, ranging\nfrom professionally designed game assets to AI-generated shapes, producing\ntemporally coherent animations that eliminate the jittering issues common in\nexisting methods.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86Puppeteer\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u5316\u7ed1\u5b9a\u548c\u52a8\u753b3D\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u9aa8\u9abc\u7ed3\u6784\u3001\u76ae\u80a4\u6743\u91cd\u548c\u4f18\u5316\u52a8\u753b\u6d41\u7a0b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u4ee3\u4ea4\u4e92\u5e94\u7528\u9700\u8981\u52a8\u60013D\u5185\u5bb9\uff0c\u4f46\u9759\u60013D\u6a21\u578b\u7684\u52a8\u753b\u8f6c\u5316\u4ecd\u4f9d\u8d56\u4e13\u5bb6\u5e72\u9884\uff0c\u6210\u4e3a\u5185\u5bb9\u521b\u4f5c\u7684\u74f6\u9888\u3002", "method": "Puppeteer\u901a\u8fc7\u81ea\u56de\u5f52\u53d8\u538b\u5668\u9884\u6d4b\u9aa8\u9abc\u7ed3\u6784\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u67b6\u6784\u63a8\u65ad\u76ae\u80a4\u6743\u91cd\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u52a8\u753b\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u753b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9aa8\u9abc\u9884\u6d4b\u548c\u76ae\u80a4\u6743\u91cd\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u80fd\u5904\u7406\u591a\u6837\u5316\u76843D\u5185\u5bb9\u3002", "conclusion": "Puppeteer\u4e3a\u81ea\u52a8\u53163D\u52a8\u753b\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10813", "pdf": "https://arxiv.org/pdf/2508.10813", "abs": "https://arxiv.org/abs/2508.10813", "authors": ["Philippe Balbiani", "Tinko Tinchev"], "title": "Modal definability in Euclidean modal logics", "categories": ["cs.LO", "math.LO"], "comment": null, "summary": "This paper is about the computability of the modal definability problem in\nclasses of frames determined by Euclidean modal logics. We characterize those\nEuclidean modal logics such that the classes of frames they determine give rise\nto an undecidable modal definability problem.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6b27\u51e0\u91cc\u5f97\u6a21\u6001\u903b\u8f91\u6240\u786e\u5b9a\u7684\u6846\u67b6\u7c7b\u4e2d\u7684\u6a21\u6001\u53ef\u5b9a\u4e49\u6027\u95ee\u9898\u7684\u53ef\u8ba1\u7b97\u6027\uff0c\u5e76\u523b\u753b\u4e86\u5bfc\u81f4\u8be5\u95ee\u9898\u4e0d\u53ef\u5224\u5b9a\u7684\u6b27\u51e0\u91cc\u5f97\u6a21\u6001\u903b\u8f91\u7c7b\u522b\u3002", "motivation": "\u63a2\u8ba8\u6b27\u51e0\u91cc\u5f97\u6a21\u6001\u903b\u8f91\u5728\u6846\u67b6\u7c7b\u4e2d\u7684\u6a21\u6001\u53ef\u5b9a\u4e49\u6027\u95ee\u9898\u7684\u53ef\u8ba1\u7b97\u6027\uff0c\u65e8\u5728\u8bc6\u522b\u54ea\u4e9b\u903b\u8f91\u4f1a\u5bfc\u81f4\u8be5\u95ee\u9898\u4e0d\u53ef\u5224\u5b9a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u51e0\u91cc\u5f97\u6a21\u6001\u903b\u8f91\u6240\u786e\u5b9a\u7684\u6846\u67b6\u7c7b\uff0c\u7814\u7a76\u5176\u6a21\u6001\u53ef\u5b9a\u4e49\u6027\u95ee\u9898\u7684\u53ef\u8ba1\u7b97\u6027\u3002", "result": "\u6210\u529f\u523b\u753b\u4e86\u5bfc\u81f4\u6a21\u6001\u53ef\u5b9a\u4e49\u6027\u95ee\u9898\u4e0d\u53ef\u5224\u5b9a\u7684\u6b27\u51e0\u91cc\u5f97\u6a21\u6001\u903b\u8f91\u7c7b\u522b\u3002", "conclusion": "\u672c\u6587\u4e3a\u6b27\u51e0\u91cc\u5f97\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u53ef\u8ba1\u7b97\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63ed\u793a\u4e86\u67d0\u4e9b\u903b\u8f91\u5728\u6b64\u95ee\u9898\u4e0a\u7684\u4e0d\u53ef\u5224\u5b9a\u6027\u3002"}}
{"id": "2508.10393", "pdf": "https://arxiv.org/pdf/2508.10393", "abs": "https://arxiv.org/abs/2508.10393", "authors": ["Liyun Zhang", "Jingcheng Ke", "Shenli Fan", "Xuanmeng Sha", "Zheng Lian"], "title": "A Unified Evaluation Framework for Multi-Annotator Tendency Learning", "categories": ["cs.LG", "cs.MM", "Artificial intelligence"], "comment": "9 pages", "summary": "Recent works have emerged in multi-annotator learning that shift focus from\nConsensus-oriented Learning (CoL), which aggregates multiple annotations into a\nsingle ground-truth prediction, to Individual Tendency Learning (ITL), which\nmodels annotator-specific labeling behavior patterns (i.e., tendency) to\nprovide explanation analysis for understanding annotator decisions. However, no\nevaluation framework currently exists to assess whether ITL methods truly\ncapture individual tendencies and provide meaningful behavioral explanations.\nTo address this gap, we propose the first unified evaluation framework with two\nnovel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies\nhow well models capture annotator tendencies by comparing predicted\ninter-annotator similarity structures with ground-truth; (2) Behavior Alignment\nExplainability (BAE) evaluates how well model explanations reflect annotator\nbehavior and decision relevance by aligning explainability-derived with\nground-truth labeling similarity structures via Multidimensional Scaling (MDS).\nExtensive experiments validate the effectiveness of our proposed evaluation\nframework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u4e2a\u4f53\u503e\u5411\u5b66\u4e60\u65b9\u6cd5\uff08ITL\uff09\u662f\u5426\u771f\u6b63\u6355\u6349\u4e86\u6807\u6ce8\u8005\u7684\u6807\u6ce8\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\uff1aDIC\u548cBAE\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30ITL\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u6355\u6349\u6807\u6ce8\u8005\u4e2a\u4f53\u503e\u5411\u5e76\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u884c\u4e3a\u89e3\u91ca\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u65b0\u6307\u6807\uff1aDIC\u7528\u4e8e\u91cf\u5316\u6a21\u578b\u6355\u6349\u6807\u6ce8\u8005\u503e\u5411\u7684\u80fd\u529b\uff0cBAE\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u89e3\u91ca\u4e0e\u6807\u6ce8\u8005\u884c\u4e3a\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u8bc4\u4f30\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86ITL\u65b9\u6cd5\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u89e3\u91ca\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.10202", "pdf": "https://arxiv.org/pdf/2508.10202", "abs": "https://arxiv.org/abs/2508.10202", "authors": ["Sreeram Venkat", "Kasia Swirydowicz", "Noah Wolfe", "Omar Ghattas"], "title": "Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices", "categories": ["cs.DC", "cs.NA", "cs.PF", "math.NA", "65Y20, 65Y05, 65Y10, 68Q25, 68W40, 65M32, 5B05", "F.2; G.4; C.4"], "comment": null, "summary": "The hardware diversity displayed in leadership-class computing facilities,\nalongside the immense performance boosts exhibited by today's GPUs when\ncomputing in lower precision, provide a strong incentive for scientific HPC\nworkflows to adopt mixed-precision algorithms and performance portability\nmodels. We present an on-the-fly framework using Hipify for performance\nportability and apply it to FFTMatvec-an HPC application that computes\nmatrix-vector products with block-triangular Toeplitz matrices. Our approach\nenables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD\nGPUs with excellent observed performance. Performance optimizations for AMD\nGPUs are integrated directly into the open-source rocBLAS library, keeping the\napplication code unchanged. We then present a dynamic mixed-precision framework\nfor FFTMatvec; a Pareto front analysis determines the optimal mixed-precision\nconfiguration for a desired error tolerance. Results are shown for AMD Instinct\nMI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,\nmixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier\nsupercomputer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHipify\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eFFTMatvec\uff0c\u4f7f\u5176\u80fd\u5728AMD GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7Pareto\u524d\u6cbf\u5206\u6790\u4f18\u5316\u8ba1\u7b97\u914d\u7f6e\u3002", "motivation": "\u9886\u5bfc\u7ea7\u8ba1\u7b97\u8bbe\u65bd\u7684\u786c\u4ef6\u591a\u6837\u6027\u4ee5\u53caGPU\u5728\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u4e2d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4fc3\u4f7f\u79d1\u5b66HPC\u5de5\u4f5c\u6d41\u7a0b\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u548c\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6a21\u578b\u3002", "method": "\u4f7f\u7528Hipify\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eFFTMatvec\uff1b\u5f00\u53d1\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7Pareto\u524d\u6cbf\u5206\u6790\u786e\u5b9a\u6700\u4f73\u7cbe\u5ea6\u914d\u7f6e\u3002", "result": "FFTMatvec\u5728AMD GPU\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u5316\u901a\u8fc7rocBLAS\u5e93\u76f4\u63a5\u96c6\u6210\uff1b\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u6846\u67b6\u53ef\u6839\u636e\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u4f18\u5316\u914d\u7f6e\uff1b\u5728OLCF Frontier\u8d85\u7ea7\u8ba1\u7b97\u673a\u76842,048\u4e2aGPU\u4e0a\u5b9e\u73b0\u6269\u5c55\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u6027\u80fd\u53ef\u79fb\u690d\u6027\u548c\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u79d1\u5b66HPC\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10283", "pdf": "https://arxiv.org/pdf/2508.10283", "abs": "https://arxiv.org/abs/2508.10283", "authors": ["Zekun Wang", "Binghao Yue", "Weitao Pan", "Jiangyi Shi", "Yue Hao"], "title": "Design of a Timer Queue Supporting Dynamic Update Operations", "categories": ["cs.NI"], "comment": null, "summary": "Large-scale timers are ubiquitous in network processing, including flow table\nentry expiration control in software defined network (SDN) switches, MAC\naddress aging in Ethernet bridges, and retransmission timeout management in\nTCP/IP protocols. Conventional implementations suffer from critical\nlimitations: low timing accuracy due to large-scale timer traversal and high\ncomputational overhead for new timer insertion. This paper presents a\nhybrid-architecture hardware priority queue based on systolic arrays and shift\nregisters for efficient timer queue management. The design uniquely supports\nfive operations: enqueue, dequeue, delete, update, and peek.To the best of our\nknowledge, it is the first hardware priority queue enabling in-queue priority\nupdates. By leveraging centralized Boolean logic encoding within systolic\nblocks, the design efficiently generates set/shift control signals while the\nnovel push-first operation ensures FIFO ordering for same-priority timers\nwithout additional metadata. Experimental results demonstrate that the design\noperates at over 400 MHz on FPGAs, achieving a 2.2-2.8x reduction in resource\nconsumption compared to state-of-the-art implementations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u52a8\u9635\u5217\u548c\u79fb\u4f4d\u5bc4\u5b58\u5668\u7684\u6df7\u5408\u67b6\u6784\u786c\u4ef6\u4f18\u5148\u7ea7\u961f\u5217\uff0c\u7528\u4e8e\u9ad8\u6548\u7ba1\u7406\u8ba1\u65f6\u5668\u961f\u5217\uff0c\u652f\u6301\u4e94\u79cd\u64cd\u4f5c\uff0c\u5e76\u5728FPGA\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u5927\u89c4\u6a21\u8ba1\u65f6\u5668\u5728\u7f51\u7edc\u5904\u7406\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4f20\u7edf\u5b9e\u73b0\u5b58\u5728\u8ba1\u65f6\u7cbe\u5ea6\u4f4e\u548c\u8ba1\u7b97\u5f00\u9500\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\u786c\u4ef6\u4f18\u5148\u7ea7\u961f\u5217\uff0c\u7ed3\u5408\u8109\u52a8\u9635\u5217\u548c\u79fb\u4f4d\u5bc4\u5b58\u5668\u7684\u4f18\u52bf\uff0c\u652f\u6301\u4e94\u79cd\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u96c6\u4e2d\u5f0f\u5e03\u5c14\u903b\u8f91\u7f16\u7801\u9ad8\u6548\u751f\u6210\u63a7\u5236\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u8bbe\u8ba1\u5728FPGA\u4e0a\u8fd0\u884c\u9891\u7387\u8d85\u8fc7400 MHz\uff0c\u8d44\u6e90\u6d88\u8017\u6bd4\u73b0\u6709\u6700\u4f18\u5b9e\u73b0\u51cf\u5c11\u4e862.2-2.8\u500d\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u9996\u6b21\u5b9e\u73b0\u4e86\u961f\u5217\u5185\u4f18\u5148\u7ea7\u66f4\u65b0\uff0c\u5e76\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002"}}
{"id": "2508.10051", "pdf": "https://arxiv.org/pdf/2508.10051", "abs": "https://arxiv.org/abs/2508.10051", "authors": ["Satrajit S. Ghosh"], "title": "An Intelligent Infrastructure as a Foundation for Modern Science", "categories": ["q-bio.NC", "cs.ET"], "comment": null, "summary": "Infrastructure shapes societies and scientific discovery. Traditional\nscientific infrastructure, often static and fragmented, leads to issues like\ndata silos, lack of interoperability and reproducibility, and unsustainable\nshort-lived solutions. Our current technical inability and social reticence to\nconnect and coordinate scientific research and engineering leads to\ninefficiencies and impedes progress. With AI technologies changing how we\ninteract with the world around us, there is an opportunity to transform\nscientific processes. Neuroscience's exponential growth of multimodal and\nmultiscale data, and urgent clinical relevance demand an infrastructure itself\nlearns, coordinates, and improves. Using neuroscience as a stress test, this\nperspective argues for a paradigm shift: infrastructure must evolve into a\ndynamic, AI-aligned ecosystem to accelerate science. Building on several\nexisting principles for data, collective benefit, and digital repositories, I\nrecommend operational guidelines for implementing them to create this dynamic\necosystem, aiming to foster a decentralized, self-learning, and self-correcting\nsystem where humans and AI can collaborate seamlessly. Addressing the chronic\nunderfunding of scientific infrastructure, acknowledging diverse contributions\nbeyond publications, and coordinating global efforts are critical steps for\nthis transformation. By prioritizing an intelligent infrastructure as a central\nscientific instrument for knowledge generation, we can overcome current\nlimitations, accelerate discovery, ensure reproducibility and ethical\npractices, and ultimately translate neuroscientific understanding into tangible\nsocietal benefits, setting a blueprint for other scientific domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u52a8\u6001AI\u751f\u6001\u7cfb\u7edf\u6539\u8fdb\u4f20\u7edf\u79d1\u5b66\u57fa\u7840\u8bbe\u65bd\uff0c\u5c24\u5176\u662f\u795e\u7ecf\u79d1\u5b66\u9886\u57df\uff0c\u4ee5\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u57fa\u7840\u8bbe\u65bd\u5b58\u5728\u6570\u636e\u5b64\u5c9b\u3001\u7f3a\u4e4f\u4e92\u64cd\u4f5c\u6027\u548c\u4e0d\u53ef\u6301\u7eed\u7b49\u95ee\u9898\uff0cAI\u6280\u672f\u4e3a\u53d8\u9769\u63d0\u4f9b\u4e86\u673a\u9047\u3002", "method": "\u4ee5\u795e\u7ecf\u79d1\u5b66\u4e3a\u4f8b\uff0c\u63d0\u51fa\u52a8\u6001AI\u751f\u6001\u7cfb\u7edf\uff0c\u6574\u5408\u6570\u636e\u3001\u96c6\u4f53\u5229\u76ca\u548c\u6570\u5b57\u5b58\u50a8\u5e93\u539f\u5219\uff0c\u5e76\u5236\u5b9a\u64cd\u4f5c\u6307\u5357\u3002", "result": "\u547c\u5401\u5168\u7403\u534f\u8c03\u3001\u89e3\u51b3\u8d44\u91d1\u4e0d\u8db3\u95ee\u9898\uff0c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u590d\u5236\u7684\u79d1\u5b66\u7814\u7a76\u3002", "conclusion": "\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u5c06\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u5e76\u5e26\u6765\u793e\u4f1a\u6548\u76ca\uff0c\u4e3a\u5176\u4ed6\u9886\u57df\u6811\u7acb\u5178\u8303\u3002"}}
{"id": "2508.10195", "pdf": "https://arxiv.org/pdf/2508.10195", "abs": "https://arxiv.org/abs/2508.10195", "authors": ["Yiannos Demetriou", "Manasvi Parikh", "Sara Eskandari", "Westley Weimer", "Madeline Endres"], "title": "Training Spatial Ability in Virtual Reality", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "Background: Spatial reasoning has been identified as a critical skill for\nsuccess in STEM. Unfortunately, under-represented groups often have lower\nincoming spatial ability. Courses that improve spatial skills exist but are not\nwidely used. Virtual reality (VR) has been suggested as a possible tool for\nteaching spatial reasoning since students are more accurate and complete\nspatial tasks more quickly in three dimensions. However, no prior work has\ndeveloped or evaluated a fully-structured VR spatial skills course. Objectives:\nWe seek to assess the effectiveness of teaching spatial reasoning in VR, both\nin isolation as a structured training curriculum and also in comparison to\ntraditional methods. Methods: We adapted three modules of an existing\npencil-and-paper course to VR, leveraging educational scaffolding and real-time\nfeedback in the design. We evaluated our three-week course in a study with\n$n=24$ undergraduate introductory STEM students, capturing both quantitative\nspatial ability gains (using pre- and post test scores on validated\nassessments) and qualitative insights (from a post-study questionnaire). We\nalso compared our VR course to an offering of a baseline non-VR course (using\ndata collected in a previous study). Results and Conclusions: Students who took\nour VR course had significant spatial ability gains. Critically, we find no\nsignificant difference in outcomes between our VR course (3 meetings of 120\nminutes each) and a baseline pencil and paper course (10 meetings of 90 minutes\neach), suggesting that spatial reasoning can be very efficiently taught in VR.\nWe observed cybersickness at lower rates than are generally reported and most\nstudents reported enjoying learning in VR.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u53ef\u4ee5\u6709\u6548\u6559\u6388\u7a7a\u95f4\u63a8\u7406\u6280\u80fd\uff0c\u4e0e\u4f20\u7edf\u7684\u7eb8\u7b14\u8bfe\u7a0b\u6548\u679c\u76f8\u5f53\uff0c\u4f46\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u7531\u4e8e\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5bf9STEM\u5b66\u79d1\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5c11\u6570\u7fa4\u4f53\u5e38\u5728\u6b64\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cVR\u88ab\u63d0\u8bae\u4e3a\u4e00\u79cd\u53ef\u80fd\u7684\u9ad8\u6548\u6559\u5b66\u5de5\u5177\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7ed3\u6784\u5316VR\u8bfe\u7a0b\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u5c06\u73b0\u6709\u7eb8\u7b14\u8bfe\u7a0b\u7684\u4e09\u90e8\u5206\u6a21\u5757\u6539\u7f16\u4e3aVR\u8bfe\u7a0b\uff0c\u7ed3\u5408\u6559\u5b66\u811a\u624b\u67b6\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u5bf924\u540d\u672c\u79d1\u751f\u8fdb\u884c\u4e86\u4e3a\u671f\u4e09\u5468\u7684\u8bc4\u4f30\u3002", "result": "VR\u8bfe\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u7684\u7a7a\u95f4\u80fd\u529b\uff0c\u4e0e\u4f20\u7edf\u8bfe\u7a0b\u6548\u679c\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5b66\u4e60\u65f6\u95f4\u66f4\u77ed\u3002VR\u7684\u4f7f\u7528\u8fd8\u51cf\u5c11\u4e86\u6655\u52a8\u75c7\u7684\u53d1\u751f\u7387\u3002", "conclusion": "VR\u53ef\u4ee5\u9ad8\u6548\u6559\u6388\u7a7a\u95f4\u63a8\u7406\u6280\u80fd\uff0c\u4e14\u5b66\u751f\u4f53\u9a8c\u826f\u597d\uff0c\u4e3a\u7a7a\u95f4\u80fd\u529b\u57f9\u8bad\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.10409", "pdf": "https://arxiv.org/pdf/2508.10409", "abs": "https://arxiv.org/abs/2508.10409", "authors": ["Zihao Chen", "Ji Zhuang", "Jinyi Shen", "Xiaoyue Ke", "Xinyi Yang", "Mingjie Zhou", "Zhuoyao Du", "Xu Yan", "Zhouyang Wu", "Zhenyu Xu", "Jiangli Huang", "Li Shang", "Xuan Zeng", "Fan Yang"], "title": "AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "In this paper, we propose AnalogSeeker, an effort toward an open-source\nfoundation language model for analog circuit design, with the aim of\nintegrating domain knowledge and giving design assistance. To overcome the\nscarcity of data in this field, we employ a corpus collection strategy based on\nthe domain knowledge framework of analog circuits. High-quality, accessible\ntextbooks across relevant subfields are systematically curated and cleaned into\na textual domain corpus. To address the complexity of knowledge of analog\ncircuits, we introduce a granular domain knowledge distillation method. Raw,\nunlabeled domain corpus is decomposed into typical, granular learning nodes,\nwhere a multi-agent framework distills implicit knowledge embedded in\nunstructured text into question-answer data pairs with detailed reasoning\nprocesses, yielding a fine-grained, learnable dataset for fine-tuning. To\naddress the unexplored challenges in training analog circuit foundation models,\nwe explore and share our training methods through both theoretical analysis and\nexperimental validation. We finally establish a fine-tuning-centric training\nparadigm, customizing and implementing a neighborhood self-constrained\nsupervised fine-tuning algorithm. This approach enhances training outcomes by\nconstraining the perturbation magnitude between the model's output\ndistributions before and after training. In practice, we train the\nQwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%\naccuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,\nwith a 15.67% point improvement over the original model and is competitive with\nmainstream commercial models. Furthermore, AnalogSeeker also shows\neffectiveness in the downstream operational amplifier design task. AnalogSeeker\nis open-sourced at https://huggingface.co/analogllm/analogseeker for research\nuse.", "AI": {"tldr": "AnalogSeeker\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u6574\u5408\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u7684\u9886\u57df\u77e5\u8bc6\u5e76\u63d0\u4f9b\u8bbe\u8ba1\u8f85\u52a9\u3002\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u84b8\u998f\u548c\u5b9a\u5236\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6a21\u578b\u5728\u77e5\u8bc6\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5b9e\u9645\u8bbe\u8ba1\u4e2d\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u9886\u57df\u6570\u636e\u7a00\u7f3a\u548c\u77e5\u8bc6\u590d\u6742\u6027\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5f00\u6e90\u5de5\u5177\u4ee5\u652f\u6301\u7814\u7a76\u548c\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u9886\u57df\u77e5\u8bc6\u6846\u67b6\u6536\u96c6\u8bed\u6599\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u6846\u67b6\u84b8\u998f\u77e5\u8bc6\u4e3a\u95ee\u7b54\u5bf9\uff0c\u5e76\u4f7f\u7528\u5b9a\u5236\u5316\u76d1\u7763\u5fae\u8c03\u7b97\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728AMSBench-TQA\u57fa\u51c6\u4e0a\u8fbe\u523085.04%\u51c6\u786e\u7387\uff0c\u6bd4\u539f\u59cb\u6a21\u578b\u63d0\u534715.67%\uff0c\u5e76\u5728\u5b9e\u9645\u8bbe\u8ba1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "AnalogSeeker\u901a\u8fc7\u6570\u636e\u7b56\u7565\u548c\u8bad\u7ec3\u65b9\u6cd5\u6539\u8fdb\uff0c\u6210\u4e3a\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u9886\u57df\u7684\u6709\u6548\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2508.10381", "pdf": "https://arxiv.org/pdf/2508.10381", "abs": "https://arxiv.org/abs/2508.10381", "authors": ["Paul-Julius Hillmann", "Stephan A. Fahrenkrog-Petersen", "Jan Mendling"], "title": "Cross-Organizational Analysis of Parliamentary Processes: A Case Study", "categories": ["cs.DB"], "comment": "Accepted to ICPM 2025 (7th International Conference on Process\n  Mining)", "summary": "Process Mining has been widely adopted by businesses and has been shown to\nhelp organizations analyze and optimize their processes. However, so far,\nlittle attention has gone into the cross-organizational comparison of\nprocesses, since many companies are hesitant to share their data. In this\npaper, we explore the processes of German state parliaments that are often\nlegally required to share their data and run the same type of processes for\ndifferent geographical regions. This paper is the first attempt to apply\nprocess mining to parliamentary processes and, therefore, contributes toward a\nnovel interdisciplinary research area that combines political science and\nprocess mining. In our case study, we analyze legislative processes of three\nGerman state parliaments and generate insights into their differences and best\npractices. We provide a discussion of the relevance of our results that are\nbased on knowledge exchange with a political scientist and a domain expert from\nthe German federal parliament.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u6d41\u7a0b\u6316\u6398\u5e94\u7528\u4e8e\u8bae\u4f1a\u6d41\u7a0b\uff0c\u5206\u6790\u4e86\u4e09\u4e2a\u5fb7\u56fd\u5dde\u8bae\u4f1a\u7684\u7acb\u6cd5\u6d41\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u8de8\u7ec4\u7ec7\u6bd4\u8f83\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4f01\u4e1a\u901a\u5e38\u4e0d\u613f\u5171\u4eab\u6570\u636e\uff0c\u8de8\u7ec4\u7ec7\u6d41\u7a0b\u6bd4\u8f83\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u5229\u7528\u6cd5\u5f8b\u8981\u6c42\u516c\u5f00\u6570\u636e\u7684\u5fb7\u56fd\u5dde\u8bae\u4f1a\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61\uff0c\u63a2\u7d22\u6d41\u7a0b\u6316\u6398\u5728\u8de8\u7ec4\u7ec7\u6bd4\u8f83\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u4e2a\u5fb7\u56fd\u5dde\u8bae\u4f1a\u7684\u7acb\u6cd5\u6d41\u7a0b\u6570\u636e\uff0c\u7ed3\u5408\u6d41\u7a0b\u6316\u6398\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\uff0c\u5e76\u4e0e\u653f\u6cbb\u79d1\u5b66\u5bb6\u548c\u8054\u90a6\u8bae\u4f1a\u7684\u4e13\u5bb6\u8fdb\u884c\u77e5\u8bc6\u4ea4\u6d41\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u5dde\u8bae\u4f1a\u6d41\u7a0b\u7684\u5dee\u5f02\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u4e3a\u6d41\u7a0b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u4e3a\u653f\u6cbb\u79d1\u5b66\u4e0e\u6d41\u7a0b\u6316\u6398\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u516c\u5f00\u6570\u636e\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.10852", "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "categories": ["cs.SE"], "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat\u662f\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5bc6\u5ea6\u6563\u70b9\u56fe\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u53ef\u89c6\u5316\u5927\u89c4\u6a21\u5386\u53f2\u6570\u636e\u96c6\uff0c\u652f\u6301\u8f6f\u4ef6\u9879\u76ee\u7684\u6f14\u8fdb\u5206\u6790\u3002", "motivation": "\u957f\u671f\u8f6f\u4ef6\u9879\u76ee\u5305\u542b\u5927\u91cf\u5386\u53f2\u53d8\u66f4\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5206\u6790\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u4ea4\u4e92\u5f0f\u5bc6\u5ea6\u6563\u70b9\u56fe\uff0c\u652f\u6301\u65f6\u95f4\u8f74\u914d\u7f6e\u3001\u6392\u5e8f\u548c\u989c\u8272\u6620\u5c04\uff0c\u4ee5\u53ef\u89c6\u5316\u767e\u4e07\u7ea7\u4e8b\u4ef6\u3002", "result": "EvoScat\u5c55\u793a\u4e86\u5728\u591a\u4e2a\u573a\u666f\uff08\u5982\u53d8\u66f4\u901f\u5ea6\u6bd4\u8f83\u3001\u514b\u9686\u68c0\u6d4b\uff09\u4e2d\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u9ad8\u6548\u5206\u6790\u548c\u6bd4\u8f83\u8f6f\u4ef6\u6f14\u8fdb\u6570\u636e\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.10265", "pdf": "https://arxiv.org/pdf/2508.10265", "abs": "https://arxiv.org/abs/2508.10265", "authors": ["Jingde Cheng"], "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?", "categories": ["cs.AI", "cs.LO"], "comment": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2412.12408", "summary": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"understanding ability\" and \"reasoning ability\" of the LLMs. The\npresent author considers that the so-called \"understanding ability\" and\n\"reasoning ability\" of LLMs are just illusions of those people who with vague\nconcepts. In fact, the LLMs can never have the true understanding ability and\ntrue reasoning ability. This paper intents to explain that, because the\nessential limitations of their working principle, the LLMs can never have the\nability of true correct reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u7406\u89e3\u80fd\u529b\u201d\u548c\u201c\u63a8\u7406\u80fd\u529b\u201d\u662f\u8bef\u89e3\uff0c\u6307\u51fa\u5176\u5de5\u4f5c\u539f\u7406\u5b58\u5728\u672c\u8d28\u9650\u5236\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u6f84\u6e05LLMs\u7684\u6240\u8c13\u201c\u7406\u89e3\u201d\u548c\u201c\u63a8\u7406\u201d\u80fd\u529b\u53ea\u662f\u4eba\u4eec\u7684\u9519\u89c9\uff0c\u5e76\u63ed\u793a\u5176\u672c\u8d28\u7684\u5de5\u4f5c\u539f\u7406\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u7684\u57fa\u672c\u5de5\u4f5c\u539f\u7406\uff0c\u6307\u51fa\u5176\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u63a8\u7406\u3002", "result": "\u8bba\u8bc1\u4e86LLMs\u5728\u672c\u8d28\u4e0a\u7f3a\u4e4f\u771f\u6b63\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "LLMs\u56e0\u5176\u5de5\u4f5c\u539f\u7406\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u5177\u5907\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.10605", "pdf": "https://arxiv.org/pdf/2508.10605", "abs": "https://arxiv.org/abs/2508.10605", "authors": ["Xinyi Wang", "Angeliki Katsenou", "David Bull"], "title": "DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "6 pages, 1 figure. Accepted for presentation at the 2025 IEEE\n  International Conference on Image Processing (ICIP)", "summary": "The rapid growth of user-generated (video) content (UGC) has driven increased\ndemand for research on no-reference (NR) perceptual video quality assessment\n(VQA). NR-VQA is a key component for large-scale video quality monitoring in\nsocial media and streaming applications where a pristine reference is not\navailable. This paper proposes a novel NR-VQA model based on spatio-temporal\nfragmentation driven by inter-frame variations. By leveraging these inter-frame\ndifferences, the model progressively analyses quality-sensitive regions at\nmultiple levels: frames, patches, and fragmented frames. It integrates frames,\nfragmented residuals, and fragmented frames aligned with residuals to\neffectively capture global and local information. The model extracts both 2D\nand 3D features in order to characterize these spatio-temporal variations.\nExperiments conducted on five UGC datasets and against state-of-the-art models\nranked our proposed method among the top 2 in terms of average rank correlation\n(DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered\nat a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on\naverage compared to the fastest existing NR-VQA method. Code and models are\npublicly available at: https://github.com/xinyiW915/DIVA-VQA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u788e\u7247\u5316\u7684NR-VQA\u6a21\u578b\uff0c\u5229\u7528\u5e27\u95f4\u5dee\u5f02\u5206\u6790\u8d28\u91cf\u654f\u611f\u533a\u57df\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7528\u6237\u751f\u6210\u89c6\u9891\u5185\u5bb9\uff08UGC\uff09\u7684\u5feb\u901f\u589e\u957f\u63a8\u52a8\u4e86\u65e0\u53c2\u8003\uff08NR\uff09\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u7814\u7a76\u7684\u9700\u6c42\uff0c\u8be5\u65b9\u6cd5\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u6d41\u5a92\u4f53\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u5e27\u95f4\u5dee\u5f02\u5206\u6790\uff08\u5e27\u3001\u5757\u3001\u788e\u7247\u5e27\uff09\uff0c\u6574\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u63d0\u53d62D\u548c3D\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2aUGC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08DIVA-VQA-L:0.898\uff0cDIVA-VQA-B:0.886\uff09\uff0c\u8fd0\u884c\u590d\u6742\u5ea6\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709NR-VQA\u6a21\u578b\uff0c\u4ee3\u7801\u4e0e\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.10338", "pdf": "https://arxiv.org/pdf/2508.10338", "abs": "https://arxiv.org/abs/2508.10338", "authors": ["Bo Wu", "Pengfei Zhou"], "title": "Near-realtime Earth Observation Via Starlink LEO Satellite Constellation", "categories": ["cs.NI"], "comment": null, "summary": "Earth observation (EO) satellites in Low Earth Orbit (LEO) are collecting\nvast amounts of data, which are invaluable for applications such as monitoring\nforest fires. However, data downloading from EO satellites faces significant\nchallenges due to the limited number of ground stations and the brief\ncommunication windows with them. Conversely, emerging LEO constellations like\nStarlink have enabled continuous connectivity and revolutionized access for\nordinary users globally, who can connect via a simple satellite dish. In this\npaper, we study the feasibility of supporting EO satellites with Starlink\nsatellite infrastructure and introduce a novel data delivery system, designated\nas \"Starlink Space User\" (SSU), for relaying data from observation satellites.\nSSU treats EO satellites as space users of Starlink, facilitating efficient\ndata transfer to Earth. At the core of SSU is a novel class of algorithms\ndesigned for link and PoP selection, as well as system scheduling optimization,\nthat operate effectively atop Starlink's proprietary infrastructure. We assess\nthe performance of SSU using trace-driven simulations alongside real-world\nStarlink performance measurements. Our results demonstrate that the proposed\nStarlink-aided design can significantly reduce the median backlog (data not\ndelivered) per satellite.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cStarlink Space User\u201d\uff08SSU\uff09\u7684\u65b0\u7cfb\u7edf\uff0c\u5229\u7528Starlink\u536b\u661f\u57fa\u7840\u8bbe\u65bd\u4e3a\u5730\u7403\u89c2\u6d4b\u536b\u661f\u63d0\u4f9b\u9ad8\u6548\u6570\u636e\u4e2d\u7ee7\u670d\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u79ef\u538b\u95ee\u9898\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u536b\u661f\u7684\u6570\u636e\u4e0b\u8f7d\u56e0\u5730\u9762\u7ad9\u6570\u91cf\u6709\u9650\u548c\u901a\u4fe1\u7a97\u53e3\u77ed\u6682\u800c\u9762\u4e34\u6311\u6218\uff0c\u800cStarlink\u7b49\u65b0\u5174LEO\u661f\u5ea7\u63d0\u4f9b\u4e86\u6301\u7eed\u8fde\u63a5\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u8bba\u6587\u8bbe\u8ba1\u4e86SSU\u7cfb\u7edf\uff0c\u5c06\u5730\u7403\u89c2\u6d4b\u536b\u661f\u89c6\u4e3aStarlink\u7684\u201c\u592a\u7a7a\u7528\u6237\u201d\uff0c\u5f00\u53d1\u4e86\u94fe\u8def\u548cPoP\u9009\u62e9\u7b97\u6cd5\u4ee5\u53ca\u7cfb\u7edf\u8c03\u5ea6\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u5b9e\u9645Starlink\u6027\u80fd\u6570\u636e\u8fdb\u884c\u4e86\u4eff\u771f\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cSSU\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6bcf\u9897\u536b\u661f\u7684\u4e2d\u4f4d\u6570\u6570\u636e\u79ef\u538b\u91cf\u3002", "conclusion": "Starlink\u8f85\u52a9\u8bbe\u8ba1\u65b9\u6848\u4e3a\u5730\u7403\u89c2\u6d4b\u536b\u661f\u7684\u6570\u636e\u4e2d\u7ee7\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10056", "pdf": "https://arxiv.org/pdf/2508.10056", "abs": "https://arxiv.org/abs/2508.10056", "authors": ["Aske Nord Raahauge", "Martin Bom Marchioro", "Rasmus Ross Nylandsted"], "title": "Approximating Entanglement Based on Abstract Interpretation", "categories": ["quant-ph", "cs.ET", "81P42"], "comment": "13 pages, no figures", "summary": "Entanglement is a fundamental property of quantum systems, essential for\nnon-trivial quantum programs. Identifying when qubits become entangled is\ncritical for circuit optimization, and for arguing for the correctness of\nquantum algorithms. This paper presents a static analysis method for\napproximating entanglement by extending an already existing abstract\ninterpretation, thus avoiding the exponential slowdown of an exact analysis.\nThe approach is shown to be sound and an implementation is provided in Standard\nML with linear-time scalability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u4f30\u8ba1\u91cf\u5b50\u6bd4\u7279\u7684\u7ea0\u7f20\u60c5\u51b5\uff0c\u907f\u514d\u4e86\u7cbe\u786e\u5206\u6790\u7684\u6307\u6570\u7ea7\u6162\u901f\u95ee\u9898\u3002", "motivation": "\u91cf\u5b50\u6bd4\u7279\u7684\u7ea0\u7f20\u662f\u91cf\u5b50\u7cfb\u7edf\u7684\u5173\u952e\u7279\u6027\uff0c\u51c6\u786e\u8bc6\u522b\u7ea0\u7f20\u72b6\u6001\u5bf9\u4e8e\u91cf\u5b50\u7535\u8def\u4f18\u5316\u548c\u7b97\u6cd5\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5df2\u6709\u7684\u62bd\u8c61\u89e3\u91ca\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u4f30\u8ba1\u91cf\u5b50\u6bd4\u7279\u7684\u7ea0\u7f20\u60c5\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u88ab\u8bc1\u660e\u662f\u53ef\u9760\u7684\uff0c\u5e76\u5728Standard ML\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u53ef\u6269\u5c55\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u9759\u6001\u5206\u6790\u65b9\u6cd5\u4e3a\u91cf\u5b50\u7a0b\u5e8f\u7684\u7ea0\u7f20\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8fd1\u4f3c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10239", "pdf": "https://arxiv.org/pdf/2508.10239", "abs": "https://arxiv.org/abs/2508.10239", "authors": ["Yifan Song", "Wing Yee Au", "Hon Yung Wong", "Brian P. Bailey", "Tal August"], "title": "Personalized Real-time Jargon Support for Online Meetings", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Effective interdisciplinary communication is frequently hindered by\ndomain-specific jargon. To explore the jargon barriers in-depth, we conducted a\nformative diary study with 16 professionals, revealing critical limitations in\ncurrent jargon-management strategies during workplace meetings. Based on these\ninsights, we designed ParseJargon, an interactive LLM-powered system providing\nreal-time personalized jargon identification and explanations tailored to\nusers' individual backgrounds. A controlled experiment comparing ParseJargon\nagainst baseline (no support) and general-purpose (non-personalized) conditions\ndemonstrated that personalized jargon support significantly enhanced\nparticipants' comprehension, engagement, and appreciation of colleagues' work,\nwhereas general-purpose support negatively affected engagement. A follow-up\nfield study validated ParseJargon's usability and practical value in real-time\nmeetings, highlighting both opportunities and limitations for real-world\ndeployment. Our findings contribute insights into designing personalized jargon\nsupport tools, with implications for broader interdisciplinary and educational\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8de8\u5b66\u79d1\u4ea4\u6d41\u4e2d\u7684\u672f\u8bed\u969c\u788d\uff0c\u5f00\u53d1\u4e86\u540d\u4e3aParseJargon\u7684\u4e2a\u6027\u5316\u672f\u8bed\u8bc6\u522b\u4e0e\u89e3\u91ca\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u7406\u89e3\u4e0e\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u8de8\u5b66\u79d1\u4ea4\u6d41\u5e38\u56e0\u9886\u57df\u4e13\u7528\u672f\u8bed\u800c\u53d7\u963b\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u65e5\u8bb0\u7814\u7a76\u548c\u5b9e\u9a8c\u8bbe\u8ba1ParseJargon\u7cfb\u7edf\uff0c\u5e76\u8fdb\u884c\u5bf9\u7167\u5b9e\u9a8c\u548c\u5b9e\u5730\u7814\u7a76\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u4e2a\u6027\u5316\u672f\u8bed\u652f\u6301\u663e\u8457\u63d0\u5347\u7406\u89e3\u3001\u53c2\u4e0e\u5ea6\u548c\u5de5\u4f5c\u6b23\u8d4f\uff0c\u901a\u7528\u652f\u6301\u5219\u9002\u5f97\u5176\u53cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u4e2a\u6027\u5316\u672f\u8bed\u5de5\u5177\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u9002\u7528\u4e8e\u8de8\u5b66\u79d1\u548c\u6559\u80b2\u9886\u57df\u3002"}}
{"id": "2508.10691", "pdf": "https://arxiv.org/pdf/2508.10691", "abs": "https://arxiv.org/abs/2508.10691", "authors": ["Alish Kanani", "Lukas Pfromm", "Harsh Sharma", "Janardhan Rao Doppa", "Partha Pratim Pande", "Umit Y. Ogras"], "title": "THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures", "categories": ["cs.AR"], "comment": "Paper accepted at ESWEEK 2025 (CODES+ISSS) conference", "summary": "Chiplet-based integration enables large-scale systems that combine diverse\ntechnologies, enabling higher yield, lower costs, and scalability, making them\nwell-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a\npromising solution for AI inference, leveraging technologies such as ReRAM,\nSRAM, and FeFET, each offering unique advantages and trade-offs. A\nheterogeneous chiplet-based PIM architecture can harness the complementary\nstrengths of these technologies to enable higher performance and energy\nefficiency. However, scheduling AI workloads across such a heterogeneous system\nis challenging due to competing performance objectives, dynamic workload\ncharacteristics, and power and thermal constraints. To address this need, we\npropose THERMOS, a thermally-aware, multi-objective scheduling framework for AI\nworkloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a\nsingle multi-objective reinforcement learning (MORL) policy that is capable of\nachieving Pareto-optimal execution time, energy, or a balanced objective at\nruntime, depending on the target preferences. Comprehensive evaluations show\nthat THERMOS achieves up to 89% faster average execution time and 57% lower\naverage energy consumption than baseline AI workload scheduling algorithms with\nonly 0.14% runtime and 0.022% energy overhead.", "AI": {"tldr": "THERMOS\u662f\u4e00\u4e2a\u9488\u5bf9\u5f02\u6784\u591a\u82af\u7247PIM\u67b6\u6784\u7684AI\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u6027\u80fd\u3001\u80fd\u8017\u548c\u70ed\u7ba1\u7406\u7684\u4f18\u5316\u3002", "motivation": "\u5f02\u6784\u82af\u7247PIM\u67b6\u6784\u867d\u7136\u6709\u6f5c\u529b\u63d0\u5347AI\u63a8\u7406\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4f46\u52a8\u6001\u8d1f\u8f7d\u7279\u6027\u548c\u529f\u8017\u9650\u5236\u4f7f\u5176\u8c03\u5ea6\u590d\u6742\uff0c\u9700\u8981\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1THERMOS\u6846\u67b6\uff0c\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5316\u6267\u884c\u65f6\u95f4\u3001\u80fd\u8017\u53ca\u70ed\u7ba1\u7406\u76ee\u6807\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cTHERMOS\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb89%\u3001\u8282\u80fd57%\uff0c\u4e14\u8fd0\u884c\u65f6\u548c\u80fd\u8017\u5f00\u9500\u6781\u4f4e\u3002", "conclusion": "THERMOS\u4e3a\u5f02\u6784PIM\u67b6\u6784\u7684\u9ad8\u6548AI\u8d1f\u8f7d\u8c03\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10305", "pdf": "https://arxiv.org/pdf/2508.10305", "abs": "https://arxiv.org/abs/2508.10305", "authors": ["Ruoyu Li", "Yafan Huang", "Longtao Zhang", "Zhuoxun Yang", "Sheng Di", "Jiajun Huang", "Jinyang Liu", "Jiannan Tian", "Xin Liang", "Guanpeng Li", "Hanqi Guo", "Franck Cappello", "Kai Zhao"], "title": "GPZ: GPU-Accelerated Lossy Compressor for Particle Data", "categories": ["cs.DC"], "comment": null, "summary": "Particle-based simulations and point-cloud applications generate massive,\nirregular datasets that challenge storage, I/O, and real-time analytics.\nTraditional compression techniques struggle with irregular particle\ndistributions and GPU architectural constraints, often resulting in limited\nthroughput and suboptimal compression ratios. In this paper, we present GPZ, a\nhigh-performance, error-bounded lossy compressor designed specifically for\nlarge-scale particle data on modern GPUs. GPZ employs a novel four-stage\nparallel pipeline that synergistically balances high compression efficiency\nwith the architectural demands of massively parallel hardware. We introduce a\nsuite of targeted optimizations for computation, memory access, and GPU\noccupancy that enables GPZ to achieve near-hardware-limit throughput. We\nconduct an extensive evaluation on three distinct GPU architectures\n(workstation, data center, and edge) using six large-scale, real-world\nscientific datasets from five distinct domains. The results demonstrate that\nGPZ consistently and significantly outperforms five state-of-the-art GPU\ncompressors, delivering up to 8x higher end-to-end throughput while\nsimultaneously achieving superior compression ratios and data quality.", "AI": {"tldr": "GPZ\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u7c92\u5b50\u6570\u636e\u7684\u9ad8\u6027\u80fd\u3001\u8bef\u5dee\u6709\u754c\u7684\u65e0\u635f\u538b\u7f29\u5668\uff0c\u4e13\u4e3a\u73b0\u4ee3GPU\u8bbe\u8ba1\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u5e76\u884c\u6d41\u6c34\u7ebf\u548c\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u538b\u7f29\u6280\u672f\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u7c92\u5b50\u5206\u5e03\u548cGPU\u67b6\u6784\u9650\u5236\uff0c\u5bfc\u81f4\u541e\u5410\u91cf\u4f4e\u548c\u538b\u7f29\u6bd4\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9GPU\u4f18\u5316\u7684\u65b0\u578b\u538b\u7f29\u5668\u3002", "method": "\u63d0\u51faGPZ\uff0c\u91c7\u7528\u56db\u9636\u6bb5\u5e76\u884c\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u8ba1\u7b97\u3001\u5185\u5b58\u8bbf\u95ee\u548cGPU\u5360\u7528\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u5e76\u884c\u786c\u4ef6\u3002", "result": "\u5728\u4e09\u79cdGPU\u67b6\u6784\u548c\u516d\u4e2a\u771f\u5b9e\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGPZ\u6bd4\u4e94\u79cd\u5148\u8fdbGPU\u538b\u7f29\u5668\u541e\u5410\u91cf\u63d0\u9ad88\u500d\uff0c\u540c\u65f6\u538b\u7f29\u6bd4\u548c\u6570\u636e\u8d28\u91cf\u66f4\u4f18\u3002", "conclusion": "GPZ\u5728\u6027\u80fd\u548c\u538b\u7f29\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u79cdGPU\u67b6\u6784\u548c\u79d1\u5b66\u9886\u57df\u3002"}}
{"id": "2508.10460", "pdf": "https://arxiv.org/pdf/2508.10460", "abs": "https://arxiv.org/abs/2508.10460", "authors": ["Wei Tian", "Jieming Shi", "Man Lung Yiu"], "title": "Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching", "categories": ["cs.DB", "cs.LG"], "comment": "13 pages, accepted by 2025 IEEE 41st International Conference on Data\n  Engineering (ICDE)", "summary": "Real-world trajectories are often sparse with low-sampling rates (i.e., long\nintervals between consecutive GPS points) and misaligned with road networks,\nyet many applications demand high-quality data for optimal performance. To\nimprove data quality with sparse trajectories as input, we systematically study\ntwo related research problems: trajectory recovery on road network, which aims\nto infer missing points to recover high-sampling trajectories, and map\nmatching, which aims to map GPS points to road segments to determine underlying\nroutes. In this paper, we present efficient methods TRMMA and MMA for accurate\ntrajectory recovery and map matching, respectively, where MMA serves as the\nfirst step of TRMMA. In MMA, we carefully formulate a classification task to\nmap a GPS point from sparse trajectories to a road segment over a small\ncandidate segment set, rather than the entire road network. We develop\ntechniques in MMA to generate effective embeddings that capture the patterns of\nGPS data, directional information, and road segments, to accurately align\nsparse trajectories to routes. For trajectory recovery, TRMMA focuses on the\nsegments in the route returned by MMA to infer missing points with position\nratios on road segments, producing high-sampling trajectories efficiently by\navoiding evaluation of all road segments. Specifically, in TRMMA, we design a\ndual-transformer encoding process to cohesively capture latent patterns in\ntrajectories and routes, and an effective decoding technique to sequentially\npredict the position ratios and road segments of missing points. We conduct\nextensive experiments to compare TRMMA and MMA with numerous existing methods\nfor trajectory recovery and map matching, respectively, on 4 large real-world\ndatasets. TRMMA and MMA consistently achieve the best result quality, often by\na significant margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TRMMA\u548cMMA\u4e24\u79cd\u65b9\u6cd5\uff0c\u5206\u522b\u7528\u4e8e\u4f4e\u91c7\u6837\u7387\u8f68\u8ff9\u7684\u6062\u590d\u548c\u5730\u56fe\u5339\u914d\uff0c\u65e8\u5728\u63d0\u9ad8\u7a00\u758f\u8f68\u8ff9\u6570\u636e\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8f68\u8ff9\u6570\u636e\u901a\u5e38\u7a00\u758f\u4e14\u4e0e\u8def\u7f51\u4e0d\u5bf9\u9f50\uff0c\u8bb8\u591a\u5e94\u7528\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002", "method": "MMA\u9996\u5148\u901a\u8fc7\u5206\u7c7b\u4efb\u52a1\u5c06GPS\u70b9\u6620\u5c04\u5230\u8def\u7f51\u7684\u5019\u9009\u6bb5\uff0cTRMMA\u57fa\u4e8eMMA\u7684\u7ed3\u679c\u63a8\u65ad\u7f3a\u5931\u70b9\u5e76\u751f\u6210\u9ad8\u91c7\u6837\u8f68\u8ff9\u3002", "result": "TRMMA\u548cMMA\u5728\u56db\u4e2a\u5927\u578b\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TRMMA\u548cMMA\u80fd\u9ad8\u6548\u4e14\u663e\u8457\u5730\u63d0\u5347\u7a00\u758f\u8f68\u8ff9\u6570\u636e\u7684\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u8f68\u8ff9\u6062\u590d\u548c\u5730\u56fe\u5339\u914d\u4efb\u52a1\u3002"}}
{"id": "2508.09991", "pdf": "https://arxiv.org/pdf/2508.09991", "abs": "https://arxiv.org/abs/2508.09991", "authors": ["Lovedeep Gondara", "Gregory Arbour", "Raymond Ng", "Jonathan Simkin", "Shebnum Devji"], "title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u5728\u52a0\u62ff\u5927\u4e0d\u5217\u98a0\u54e5\u4f26\u6bd4\u4e9a\u764c\u75c7\u767b\u8bb0\u5904\u5b9e\u65bd\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6a21\u578b\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u5f3a\u8c03\u57fa\u4e8e\u4e1a\u52a1\u76ee\u6807\u5b9a\u4e49\u95ee\u9898\u3001\u91c7\u7528\u8fed\u4ee3\u5f00\u53d1\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u901a\u8fc7NLP\u6280\u672f\u4ece\u4e34\u5e8a\u6587\u6863\u4e2d\u81ea\u52a8\u5316\u63d0\u53d6\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u533b\u7597\u4fdd\u5065\u7ba1\u7406\u7684\u6548\u7387\u548c\u60a3\u8005\u62a4\u7406\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u5f00\u53d1\u65b9\u6cd5\uff0c\u7ed3\u5408\u8de8\u5b66\u79d1\u534f\u4f5c\uff0c\u5305\u62ec\u9886\u57df\u4e13\u5bb6\u3001\u6700\u7ec8\u7528\u6237\u548c\u673a\u5668\u5b66\u4e60\u4e13\u5bb6\u7684\u5171\u540c\u8bbe\u8ba1\u3002\u6a21\u578b\u9009\u62e9\u6ce8\u91cd\u5b9e\u7528\u6027\u548c\u6570\u636e\u8d28\u91cf\u3002", "result": "\u63d0\u4f9b\u4e86\u6210\u529f\u5b9e\u65bdAI/NLP\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u8003\u91cf\uff0c\u5305\u62ec\u95ee\u9898\u5b9a\u4e49\u3001\u6a21\u578b\u9009\u62e9\u3001\u6570\u636e\u8d28\u91cf\u7ba1\u7406\u548c\u9519\u8bef\u7f13\u89e3\u7b56\u7565\u3002", "conclusion": "\u8fd9\u4e9b\u7ecf\u9a8c\u6559\u8bad\u4e0d\u4ec5\u9002\u7528\u4e8e\u764c\u75c7\u767b\u8bb0\u5904\uff0c\u4e5f\u53ef\u4e3a\u5176\u4ed6\u533b\u7597\u4fdd\u5065\u7ec4\u7ec7\u5728\u5b9e\u65bdAI/NLP\u89e3\u51b3\u65b9\u6848\u65f6\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.10650", "pdf": "https://arxiv.org/pdf/2508.10650", "abs": "https://arxiv.org/abs/2508.10650", "authors": ["Bugra Kilictas", "Faruk Alpay"], "title": "The phi-Process: Operator-Algebraic Embeddings of Possibilities, Transfinite Stabilization, and a Quantitative Application to Sensory Depletion", "categories": ["math.FA", "cs.LO", "math.DS", "47H10, 47A10, 47A60, 06B35, 68Q45, 60J10", "F.1.1; F.4.3; F.4.1"], "comment": "12 pages, 1 figure, 1 appendix with reproducible code", "summary": "We formalize a transfinite Phi process that treats all possibility embeddings\nas operators on structured state spaces including complete lattices, Banach and\nHilbert spaces, and orthomodular lattices. We prove a determinization lemma\nshowing that lifting to sets or distributions yields a deterministic global\ndynamic, an ordinal stabilization theorem sending operator transforms to the\nfixed subspace by stage omega under normal spectral contraction, and a product\nof Riesz projections theorem for commuting layers. We establish a\ncompositionality law for lifted maps, show closure of Phi packings, and present\na quantitative application to sensory depletion that models tissue removal as a\nprojection and derives strict decreases in the attainable fixed point under\nminimal monotonicity and positivity assumptions. We also state measurable\nconditions for probabilistic lifts, give explicit non normal and non commuting\ncounterexamples, and provide finite dimensional and stochastic witnesses\ntogether with per theorem scope tables and a small reproducible code appendix.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u53ef\u80fd\u6027\u5d4c\u5165\u7684transfinite Phi\u8fc7\u7a0b\uff0c\u8bc1\u660e\u4e86\u5176\u786e\u5b9a\u6027\u3001\u7a33\u5b9a\u6027\u548c\u7ec4\u5408\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u5e94\u7528\u793a\u4f8b\u548c\u53cd\u4f8b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\uff08\u5982\u5b8c\u5168\u683c\u3001Banach\u548cHilbert\u7a7a\u95f4\u3001\u6b63\u4ea4\u6a21\u683c\uff09\u4e2d\u5f62\u5f0f\u5316\u5904\u7406\u53ef\u80fd\u6027\u5d4c\u5165\u7684\u7b97\u5b50\uff0c\u5e76\u63a2\u8ba8\u5176\u52a8\u6001\u884c\u4e3a\u548c\u5b9a\u91cf\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316transfinite Phi\u8fc7\u7a0b\uff0c\u8bc1\u660e\u5176\u786e\u5b9a\u6027\u5f15\u7406\u3001\u5e8f\u6570\u7a33\u5b9a\u5b9a\u7406\u548cRiesz\u6295\u5f71\u4e58\u79ef\u5b9a\u7406\uff0c\u5e76\u7814\u7a76\u5176\u7ec4\u5408\u6027\u548c\u95ed\u5408\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u786e\u5b9a\u6027\u5168\u5c40\u52a8\u6001\u3001\u5e8f\u6570\u7a33\u5b9a\u6027\u548c\u7ec4\u5408\u6027\u5b9a\u5f8b\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u611f\u5b98\u8017\u7aed\u7684\u5b9a\u91cf\u6a21\u578b\u3002", "conclusion": "\u8fc7\u7a0b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\u4e14\u53ef\u7ec4\u5408\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6570\u5b66\u5efa\u6a21\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.10745", "pdf": "https://arxiv.org/pdf/2508.10745", "abs": "https://arxiv.org/abs/2508.10745", "authors": ["Sayan Nag", "K J Joseph", "Koustava Goswami", "Vlad I Morariu", "Balaji Vasan Srinivasan"], "title": "Agentic Design Review System", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "comment": null, "summary": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4ee3\u7406\u534f\u4f5c\u7684Agentic\u8bbe\u8ba1\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u7ed3\u5408\u56fe\u5339\u914d\u548c\u63d0\u793a\u6269\u5c55\u65b9\u6cd5\uff0c\u6709\u6548\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u5e76\u751f\u6210\u5b9e\u7528\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u56fe\u5f62\u8bbe\u8ba1\u8bc4\u4f30\u9700\u8981\u7efc\u5408\u591a\u4e2a\u4e13\u5bb6\u8bc4\u5ba1\u7684\u53cd\u9988\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAgenticDRS\uff0c\u901a\u8fc7\u534f\u4f5c\u4ee3\u7406\u548c\u5143\u4ee3\u7406\u5206\u6790\u8bbe\u8ba1\uff0c\u91c7\u7528\u56fe\u5339\u914d\u548c\u63d0\u793a\u6269\u5c55\u65b9\u6cd5\u4f7f\u4ee3\u7406\u66f4\u5177\u8bbe\u8ba1\u610f\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660eAgenticDRS\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u80fd\u6709\u6548\u8bc4\u4f30\u8bbe\u8ba1\u5e76\u63d0\u4f9b\u5b9e\u7528\u53cd\u9988\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u56fe\u5f62\u8bbe\u8ba1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.10413", "pdf": "https://arxiv.org/pdf/2508.10413", "abs": "https://arxiv.org/abs/2508.10413", "authors": ["Sanghoon Lee", "Hyung-Seok Park", "Jiyeong Chae", "Kyung-Joon Park"], "title": "Probabilistic Latency Analysis of the Data Distribution Service in ROS 2", "categories": ["cs.NI", "cs.RO"], "comment": "12 pages, 5 figures", "summary": "Robot Operating System 2 (ROS 2) is now the de facto standard for robotic\ncommunication, pairing UDP transport with the Data Distribution Service (DDS)\npublish-subscribe middleware. DDS achieves reliability through periodic\nheartbeats that solicit acknowledgments for missing samples and trigger\nselective retransmissions. In lossy wireless networks, the tight coupling among\nheartbeat period, IP fragmentation, and retransmission interval obscures end to\nend latency behavior and leaves practitioners with little guidance on how to\ntune these parameters. To address these challenges, we propose a probabilistic\nlatency analysis (PLA) that analytically models the reliable transmission\nprocess of ROS 2 DDS communication using a discrete state approach. By\nsystematically analyzing both middleware level and transport level events, PLA\ncomputes the steady state probability distribution of unacknowledged messages\nand the retransmission latency. We validate our PLA across 270 scenarios,\nexploring variations in packet delivery ratios, message sizes, and both\npublishing and retransmission intervals, demonstrating a close alignment\nbetween analytical predictions and experimental results. Our findings establish\na theoretical basis to systematically optimize reliability, latency, and\nperformance in wireless industrial robotics.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9ROS 2 DDS\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u53ef\u9760\u4f20\u8f93\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u5ef6\u8fdf\u5206\u6790\uff08PLA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u72b6\u6001\u5efa\u6a21\u548c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u4e3a\u4f18\u5316\u65e0\u7ebf\u5de5\u4e1a\u673a\u5668\u4eba\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "ROS 2 DDS\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7531\u4e8e\u5fc3\u8df3\u5468\u671f\u3001IP\u5206\u7247\u548c\u91cd\u4f20\u95f4\u9694\u7684\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u7aef\u5230\u7aef\u5ef6\u8fdf\u884c\u4e3a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u8c03\u4f18\u53c2\u6570\u7684\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u63d0\u51faPLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u72b6\u6001\u5efa\u6a21\u548c\u7cfb\u7edf\u6027\u5206\u6790\u4e2d\u95f4\u4ef6\u53ca\u4f20\u8f93\u5c42\u4e8b\u4ef6\uff0c\u8ba1\u7b97\u672a\u786e\u8ba4\u6d88\u606f\u7684\u7a33\u6001\u6982\u7387\u5206\u5e03\u548c\u91cd\u4f20\u5ef6\u8fdf\u3002", "result": "\u5728270\u79cd\u573a\u666f\u4e2d\u9a8c\u8bc1PLA\uff0c\u7ed3\u679c\u663e\u793a\u5206\u6790\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "PLA\u4e3a\u65e0\u7ebf\u5de5\u4e1a\u673a\u5668\u4eba\u4e2d\u53ef\u9760\u6027\u3001\u5ef6\u8fdf\u548c\u6027\u80fd\u7684\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.10156", "pdf": "https://arxiv.org/pdf/2508.10156", "abs": "https://arxiv.org/abs/2508.10156", "authors": ["Nitin Rai", "Nathan S. Boyd", "Gary E. Vallad", "Arnold W. Schumann"], "title": "Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "The current advancements in generative artificial intelligence (GenAI) models\nhave paved the way for new possibilities for generating high-resolution\nsynthetic images, thereby offering a promising alternative to traditional image\nacquisition for training computer vision models in agriculture. In the context\nof crop disease diagnosis, GenAI models are being used to create synthetic\nimages of various diseases, potentially facilitating model creation and\nreducing the dependency on resource-intensive in-field data collection.\nHowever, limited research has been conducted on evaluating the effectiveness of\nintegrating real with synthetic images to improve disease classification\nperformance. Therefore, this study aims to investigate whether combining a\nlimited number of real images with synthetic images can enhance the prediction\naccuracy of an EfficientNetV2-L model for classifying watermelon\n\\textit{(Citrullus lanatus)} diseases. The training dataset was divided into\nfive treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1\nreal-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to\nimprove variability and model generalization). All treatments were trained\nusing a custom EfficientNetV2-L architecture with enhanced fine-tuning and\ntransfer learning techniques. Models trained on H2, H3, and H4 treatments\ndemonstrated high precision, recall, and F1-score metrics. Additionally, the\nweighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying\nthat the addition of a small number of real images with a considerable volume\nof synthetic images improved model performance and generalizability. Overall,\nthis validates the findings that synthetic images alone cannot adequately\nsubstitute for real images; instead, both must be used in a hybrid manner to\nmaximize model performance for crop disease classification.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u7ed3\u5408\u4f7f\u7528\u662f\u5426\u80fd\u63d0\u5347\u897f\u74dc\u75c5\u5bb3\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u6df7\u5408\u4f7f\u7528\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u5728\u897f\u74dc\u75c5\u5bb3\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u51cf\u5c11\u5bf9\u8d44\u6e90\u5bc6\u96c6\u578b\u5b9e\u5730\u6570\u636e\u6536\u96c6\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528EfficientNetV2-L\u6a21\u578b\uff0c\u7ed3\u5408\u589e\u5f3a\u5fae\u8c03\u548c\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u8bad\u7ec3\u4e94\u7ec4\u4e0d\u540c\u6bd4\u4f8b\u7684\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\uff08H0-H4\uff09\u3002", "result": "\u6df7\u5408\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u56fe\u50cf\u4e0e\u5927\u91cf\u5408\u6210\u56fe\u50cf\uff08H3-H4\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684F1\u5206\u6570\uff08\u4ece0.65\u52301.00\uff09\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6216\u5408\u6210\u56fe\u50cf\u3002", "conclusion": "\u5355\u72ec\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u4e0d\u8db3\u4ee5\u66ff\u4ee3\u771f\u5b9e\u56fe\u50cf\uff0c\u6df7\u5408\u4f7f\u7528\u4e24\u8005\u53ef\u6700\u5927\u5316\u4f5c\u7269\u75c5\u5bb3\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10252", "pdf": "https://arxiv.org/pdf/2508.10252", "abs": "https://arxiv.org/abs/2508.10252", "authors": ["Tao Long", "Sitong Wang", "\u00c9milie Fabre", "Tony Wang", "Anup Sathya", "Jason Wu", "Savvas Petridis", "Dingzeyu Li", "Tuhin Chakrabarty", "Yue Jiang", "Jingyi Li", "Tiffany Tseng", "Ken Nakagaki", "Qian Yang", "Nikolas Martelaro", "Jeffrey V. Nickerson", "Lydia B. Chilton"], "title": "Facilitating Longitudinal Interaction Studies of AI Systems", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "Accepted workshop proposal @ UIST 2025 Busan, Korea. Workshop\n  website: https://longitudinal-workshop.github.io/", "summary": "UIST researchers develop tools to address user challenges. However, user\ninteractions with AI evolve over time through learning, adaptation, and\nrepurposing, making one time evaluations insufficient. Capturing these dynamics\nrequires longer-term studies, but challenges in deployment, evaluation design,\nand data collection have made such longitudinal research difficult to\nimplement. Our workshop aims to tackle these challenges and prepare researchers\nwith practical strategies for longitudinal studies. The workshop includes a\nkeynote, panel discussions, and interactive breakout groups for discussion and\nhands-on protocol design and tool prototyping sessions. We seek to foster a\ncommunity around longitudinal system research and promote it as a more embraced\nmethod for designing, building, and evaluating UIST tools.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u957f\u671f\u7814\u7a76\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u89e3\u51b3\u7528\u6237\u4e0eAI\u4e92\u52a8\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u95ee\u9898\uff0c\u5de5\u4f5c\u574a\u65e8\u5728\u63d0\u4f9b\u5b9e\u7528\u7b56\u7565\u3002", "motivation": "\u7531\u4e8e\u7528\u6237\u4e0eAI\u7684\u4e92\u52a8\u4f1a\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u5355\u6b21\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u6355\u6349\u52a8\u6001\u95ee\u9898\uff0c\u9700\u8981\u957f\u671f\u7814\u7a76\u3002", "method": "\u5de5\u4f5c\u574a\u5305\u62ec\u4e3b\u9898\u6f14\u8bb2\u3001\u5c0f\u7ec4\u8ba8\u8bba\u548c\u4e92\u52a8\u5c0f\u7ec4\u6d3b\u52a8\uff0c\u65e8\u5728\u8bbe\u8ba1\u534f\u8bae\u548c\u5de5\u5177\u539f\u578b\u3002", "result": "\u76ee\u6807\u662f\u5efa\u7acb\u957f\u671f\u7cfb\u7edf\u7814\u7a76\u7684\u793e\u533a\uff0c\u5e76\u63a8\u5e7f\u5176\u4f5c\u4e3a\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5de5\u5177\u7684\u5e38\u7528\u65b9\u6cd5\u3002", "conclusion": "\u957f\u671f\u7814\u7a76\u662f\u89e3\u51b3\u7528\u6237\u4e0eAI\u4e92\u52a8\u52a8\u6001\u53d8\u5316\u7684\u5173\u952e\uff0c\u5de5\u4f5c\u574a\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u7b56\u7565\u3002"}}
{"id": "2508.10349", "pdf": "https://arxiv.org/pdf/2508.10349", "abs": "https://arxiv.org/abs/2508.10349", "authors": ["Tianjun Yuan", "Jiaxiang Geng", "Pengchao Han", "Xianhao Chen", "Bing Luo"], "title": "Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models", "categories": ["cs.DC", "cs.LG"], "comment": "10 pages, Submitted to INFOCOM2026", "summary": "Fine-tuning foundation models is critical for superior performance on\npersonalized downstream tasks, compared to using pre-trained models.\nCollaborative learning can leverage local clients' datasets for fine-tuning,\nbut limited client data and heterogeneous data distributions hinder effective\ncollaboration. To address the challenge, we propose a flexible personalized\nfederated learning paradigm that enables clients to engage in collaborative\nlearning while maintaining personalized objectives. Given the limited and\nheterogeneous computational resources available on clients, we introduce\n\\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based on\nsplit learning, FlexP-SFL allows each client to train a portion of the model\nlocally while offloading the rest to a server, according to resource\nconstraints. Additionally, we propose an alignment strategy to improve\npersonalized model performance on global data. Experimental results show that\nFlexP-SFL outperforms baseline models in personalized fine-tuning efficiency\nand final accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\uff08FlexP-SFL\uff09\uff0c\u901a\u8fc7\u5206\u5272\u5b66\u4e60\u548c\u5bf9\u9f50\u7b56\u7565\u89e3\u51b3\u5ba2\u6237\u7aef\u6570\u636e\u4e0d\u8db3\u548c\u5f02\u6784\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6709\u9650\u5ba2\u6237\u7aef\u6570\u636e\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\u5bf9\u534f\u540c\u5b66\u4e60\u7684\u963b\u788d\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u76ee\u6807\u7684\u540c\u65f6\u8fdb\u884c\u534f\u4f5c\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u5206\u5272\u5b66\u4e60\uff0c\u5ba2\u6237\u7aef\u6839\u636e\u8d44\u6e90\u9650\u5236\u90e8\u5206\u672c\u5730\u8bad\u7ec3\u6a21\u578b\uff0c\u5269\u4f59\u90e8\u5206\u5378\u8f7d\u5230\u670d\u52a1\u5668\uff0c\u5e76\u91c7\u7528\u5bf9\u9f50\u7b56\u7565\u4f18\u5316\u5168\u5c40\u6570\u636e\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlexP-SFL\u5728\u4e2a\u6027\u5316\u5fae\u8c03\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "FlexP-SFL\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10504", "pdf": "https://arxiv.org/pdf/2508.10504", "abs": "https://arxiv.org/abs/2508.10504", "authors": ["Zhliang Xiang", "Meghyn Bienvenu", "Gianluca Cima", "V\u00edctor Guti\u00e9rrez-Basulto", "Yazm\u00edn Ib\u00e1\u00f1ez-Garc\u00eda"], "title": "Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria", "categories": ["cs.DB", "cs.AI"], "comment": "Full version of a paper accepted at KR 2025", "summary": "In this paper, we present ASPEN+, which extends an existing ASP-based system,\nASPEN,for collective entity resolution with two important functionalities:\nsupport for local merges and new optimality criteria for preferred solutions.\nIndeed, ASPEN only supports so-called global merges of entity-referring\nconstants (e.g. author ids), in which all occurrences of matched constants are\ntreated as equivalent and merged accordingly. However, it has been argued that\nwhen resolving data values, local merges are often more appropriate, as e.g.\nsome instances of 'J. Lee' may refer to 'Joy Lee', while others should be\nmatched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+\noffers new optimality criteria for selecting solutions, such as minimizing rule\nviolations or maximising the number of rules supporting a merge. Our main\ncontributions are thus (1) the formalisation and computational analysis of\nvarious notions of optimal solution, and (2) an extensive experimental\nevaluation on real-world datasets, demonstrating the effect of local merges and\nthe new optimality criteria on both accuracy and runtime.", "AI": {"tldr": "ASPEN+\u6269\u5c55\u4e86ASPEN\u7cfb\u7edf\uff0c\u589e\u52a0\u4e86\u5c40\u90e8\u5408\u5e76\u548c\u65b0\u4f18\u5316\u6807\u51c6\u7684\u529f\u80fd\uff0c\u4ee5\u63d0\u5347\u96c6\u4f53\u5b9e\u4f53\u89e3\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684ASPEN\u7cfb\u7edf\u4ec5\u652f\u6301\u5168\u5c40\u5408\u5e76\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c40\u90e8\u5408\u5e76\u66f4\u5408\u9002\uff0c\u4f8b\u5982\u4e0d\u540c\u5b9e\u4f8b\u7684\u540c\u540d\u53ef\u80fd\u9700\u8981\u5339\u914d\u4e0d\u540c\u5b9e\u4f53\u3002", "method": "ASPEN+\u5f15\u5165\u4e86\u5c40\u90e8\u5408\u5e76\u548c\u65b0\u7684\u4f18\u5316\u6807\u51c6\uff08\u5982\u6700\u5c0f\u5316\u89c4\u5219\u8fdd\u53cd\u6216\u6700\u5927\u5316\u652f\u6301\u5408\u5e76\u7684\u89c4\u5219\u6570\u91cf\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u548c\u8ba1\u7b97\u5206\u6790\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5c40\u90e8\u5408\u5e76\u548c\u65b0\u4f18\u5316\u6807\u51c6\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "ASPEN+\u901a\u8fc7\u652f\u6301\u5c40\u90e8\u5408\u5e76\u548c\u5f15\u5165\u65b0\u4f18\u5316\u6807\u51c6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u96c6\u4f53\u5b9e\u4f53\u89e3\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10017", "pdf": "https://arxiv.org/pdf/2508.10017", "abs": "https://arxiv.org/abs/2508.10017", "authors": ["Rodrigo Tertulino"], "title": "A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "comment": "This is being prepared to be submitted to the Journal of the\n  Brazilian Computer Society (JBCS), which is still under construction", "summary": "Federated Learning (FL) presents a groundbreaking approach for collaborative\nhealth research, allowing model training on decentralized data while\nsafeguarding patient privacy. FL offers formal security guarantees when\ncombined with Differential Privacy (DP). The integration of these technologies,\nhowever, introduces a significant trade-off between privacy and clinical\nutility, a challenge further complicated by the severe class imbalance often\npresent in medical datasets. The research presented herein addresses these\ninterconnected issues through a systematic, multi-stage analysis. An FL\nframework was implemented for cardiovascular risk prediction, where initial\nexperiments showed that standard methods struggled with imbalanced data,\nresulting in a recall of zero. To overcome such a limitation, we first\nintegrated the hybrid Synthetic Minority Over-sampling Technique with Tomek\nLinks (SMOTETomek) at the client level, successfully developing a clinically\nuseful model. Subsequently, the framework was optimized for non-IID data using\na tuned FedProx algorithm. Our final results reveal a clear, non-linear\ntrade-off between the privacy budget (epsilon) and model recall, with the\noptimized FedProx consistently out-performing standard FedAvg. An optimal\noperational region was identified on the privacy-utility frontier, where strong\nprivacy guarantees (with epsilon 9.0) can be achieved while maintaining high\nclinical utility (recall greater than 77%). Ultimately, our study provides a\npractical methodological blueprint for creating effective, secure, and accurate\ndiagnostic tools that can be applied to real-world, heterogeneous healthcare\ndata.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548cSMOTETomek\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u6570\u636e\u4e2d\u7684\u9690\u79c1\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u9ad8\u5b9e\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\u3002", "motivation": "\u533b\u7597\u6570\u636e\u5177\u6709\u9690\u79c1\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u7279\u70b9\uff0c\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\uff0c\u5c24\u5176\u662f\u9690\u79c1\u4fdd\u62a4\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "\u91c7\u7528SMOTETomek\u6280\u672f\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7684FedProx\u7b97\u6cd5\u5e94\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\uff0c\u540c\u65f6\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u9690\u79c1\u3002", "result": "\u4f18\u5316\u7684FedProx\u4f18\u4e8e\u6807\u51c6FedAvg\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u9884\u7b97\uff08epsilon\uff09\u4e0e\u53ec\u56de\u7387\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u6743\u8861\uff0c\u627e\u5230\u6700\u4f18\u5e73\u8861\u70b9\uff08epsilon 9.0\uff0c\u53ec\u56de\u7387>77%\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5904\u7406\u5b9e\u9645\u533b\u7597\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u5b89\u5168\u4e14\u7cbe\u786e\u7684\u65b9\u6cd5\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u4e0e\u4e34\u5e8a\u5b9e\u7528\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2508.10769", "pdf": "https://arxiv.org/pdf/2508.10769", "abs": "https://arxiv.org/abs/2508.10769", "authors": ["Zhiqi Shen", "Shaojing Fan", "Danni Xu", "Terence Sim", "Mohan Kankanhalli"], "title": "Modeling Human Responses to Multimodal AI Content", "categories": ["cs.AI", "cs.MM"], "comment": null, "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7MhAIM\u6570\u636e\u96c6\u548cT-Lens\u7cfb\u7edf\uff0c\u7814\u7a76\u4e86\u4eba\u7c7b\u5bf9AI\u751f\u6210\u5185\u5bb9\u7684\u53cd\u5e94\uff0c\u63d0\u51fa\u4e86\u65b0\u6307\u6807\u548c\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9AI\u9a71\u52a8\u9519\u8bef\u4fe1\u606f\u7684\u98ce\u9669\u3002", "motivation": "\u7814\u7a76AI\u751f\u6210\u5185\u5bb9\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u4ea4\u6613\u6216\u80a1\u7968\u5e02\u573a\u7b49\u9886\u57df\uff0c\u9884\u6d4b\u7528\u6237\u53cd\u5e94\u6bd4\u9a8c\u8bc1\u4e8b\u5b9e\u51c6\u786e\u6027\u66f4\u91cd\u8981\u3002", "method": "\u5f15\u5165\u5305\u542b154,552\u6761\u5728\u7ebf\u5e16\u5b50\uff08111,153\u6761\u4e3aAI\u751f\u6210\uff09\u7684MhAIM\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4fe1\u4efb\u5ea6\u3001\u5f71\u54cd\u529b\u548c\u5f00\u653e\u6027\u4e09\u4e2a\u65b0\u6307\u6807\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684T-Lens\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u662fHR-MCP\u534f\u8bae\u3002", "result": "\u4eba\u7c7b\u5728\u540c\u65f6\u770b\u5230\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u65f6\u66f4\u6613\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u4e24\u8005\u4e0d\u4e00\u81f4\u65f6\u3002T-Lens\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u548c\u9002\u5e94\u7528\u6237\u53cd\u5e94\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347LLM\u7684\u4eba\u7c7b\u610f\u8bc6\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u5b9e\u8bc1\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u51cf\u8f7bAI\u9a71\u52a8\u9519\u8bef\u4fe1\u606f\u7684\u98ce\u9669\u3002"}}
{"id": "2508.10574", "pdf": "https://arxiv.org/pdf/2508.10574", "abs": "https://arxiv.org/abs/2508.10574", "authors": ["Anshika Singh", "Siddhartha S. Borkotoky"], "title": "Federated Learning Over LoRa Networks: Simulator Design and Performance Evaluation", "categories": ["cs.NI"], "comment": null, "summary": "Federated learning (FL) over long-range (LoRa) low-power wide area networks\nfaces unique challenges due to limited bandwidth, interference, and strict\nduty-cycle constraints. We develop a Python-based simulator that integrates and\nextends the Flower and LoRaSim frameworks to evaluate centralized FL over LoRa\nnetworks. The simulator employs a detailed link-level model for FL update\ntransfer over LoRa channels, capturing LoRa's receiver sensitivity,\ninterference characteristics, block-fading effects, and constraints on the\nmaximum transmission unit. It supports update sparsification, quantization,\ncompression, forward frame-erasure correction (FEC), and duty cycling.\nNumerical results illustrate the impact of transmission parameters (spreading\nfactor, FEC rate) and interference on FL performance. Demonstrating the\ncritical role of FEC in enabling FL over LoRa networks, we perform an in-depth\nevaluation of the impact of FEC on FL convergence and device airtime, providing\ninsights for communication protocol design for FL over LoRa networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u96c6\u4e2d\u5f0f\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728LoRa\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u4e86\u5e72\u6270\u548c\u4f20\u8f93\u53c2\u6570\u5bf9FL\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u524d\u5411\u5e27\u64e6\u9664\u6821\u6b63\uff08FEC\uff09\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u5728LoRa\u4f4e\u529f\u8017\u5e7f\u57df\u7f51\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u5e26\u5bbd\u9650\u5236\u3001\u5e72\u6270\u548c\u4e25\u683c\u7684\u5360\u7a7a\u6bd4\u7ea6\u675f\u3002", "method": "\u5f00\u53d1Python\u6a21\u62df\u5668\uff0c\u6574\u5408Flower\u548cLoRaSim\u6846\u67b6\uff0c\u6a21\u62dfFL\u66f4\u65b0\u4f20\u8f93\u7684\u94fe\u8def\u7ea7\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u4f18\u5316\u6280\u672f\u5982\u7a00\u758f\u5316\u3001\u91cf\u5316\u3001\u538b\u7f29\u3001FEC\u548c\u5360\u7a7a\u6bd4\u63a7\u5236\u3002", "result": "FEC\u5728\u4fdd\u8bc1FL\u6536\u655b\u548c\u8bbe\u5907\u901a\u4fe1\u65f6\u95f4\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u6a21\u62df\u7ed3\u679c\u63ed\u793a\u4e86\u4f20\u8f93\u53c2\u6570\u548c\u5e72\u6270\u5bf9FL\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd\u3002", "conclusion": "\u4e3aLoRa\u7f51\u7edc\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u534f\u8bae\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86FEC\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.10171", "pdf": "https://arxiv.org/pdf/2508.10171", "abs": "https://arxiv.org/abs/2508.10171", "authors": ["Aaditya Baranwal", "Abdul Mueez", "Jason Voelker", "Guneet Bhatia", "Shruti Vyas"], "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data", "categories": ["cs.CV", "cs.ET"], "comment": "Accepted at ICCV (VISION'25 Workshop) 2025", "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u7684\u65b9\u6cd5\uff0c\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5de5\u4e1a\u6cc4\u6f0f\u68c0\u6d4b\u7b49\u5c0f\u4f17\u3001\u5b89\u5168\u5173\u952e\u9886\u57df\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\uff0c\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86YOLO\u548cDETR\u7b49\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u6cc4\u6f0f\u68c0\u6d4b\u7b49\u9886\u57df\u7684\u7a00\u7f3a\u6027\u548c\u6570\u636e\u654f\u611f\u6027\u5bfc\u81f4\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u627e\u5230\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7PEFT\u5fae\u8c03VLMs\u548c\u5bf9\u8c61\u68c0\u6d4b\u5668\u3002", "result": "\u5408\u6210\u6570\u636e\uff08SynSpill\u6570\u636e\u96c6\uff09\u663e\u8457\u63d0\u5347\u4e86VLMs\u548c\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u672a\u89c1\u8fc7\u7684\u6cc4\u6f0f\u573a\u666f\u4e2d\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u53ef\u4ee5\u6709\u6548\u5f25\u5408\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\uff0c\u4e3a\u5de5\u4e1a\u73af\u5883\u4e2d\u89c6\u89c9\u7cfb\u7edf\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10286", "pdf": "https://arxiv.org/pdf/2508.10286", "abs": "https://arxiv.org/abs/2508.10286", "authors": ["Yupei Li", "Qiyang Sun", "Michelle Schlicher", "Yee Wen Lim", "Bj\u00f6rn W. Schuller"], "title": "Artificial Emotion: A Survey of Theories and Debates on Realising Emotion in Artificial Intelligence", "categories": ["cs.HC"], "comment": null, "summary": "Affective Computing (AC) has enabled Artificial Intelligence (AI) systems to\nrecognise, interpret, and respond to human emotions - a capability also known\nas Artificial Emotional Intelligence (AEI). It is increasingly seen as an\nimportant component of Artificial General Intelligence (AGI). We discuss\nwhether in order to peruse this goal, AI benefits from moving beyond emotion\nrecognition and synthesis to develop internal emotion-like states, which we\nterm as Artificial Emotion (AE). This shift potentially allows AI to benefit\nfrom the paradigm of `inner emotions' in ways we - as humans - do. Although\nrecent research shows early signs that AI systems may exhibit AE-like\nbehaviours, a clear framework for how emotions can be realised in AI remains\nunderexplored. In this paper, we discuss potential advantages of AE in AI,\nreview current manifestations of AE in machine learning systems, examine\nemotion-modulated architectures, and summarise mechanisms for modelling and\nintegrating AE into future AI. We also explore the ethical implications and\nsafety risks associated with `emotional' AGI, while concluding with our opinion\non how AE could be beneficial in the future.", "AI": {"tldr": "\u8ba8\u8bba\u4e86\u60c5\u611f\u8ba1\u7b97\uff08AC\uff09\u5982\u4f55\u8ba9AI\u7cfb\u7edf\u8bc6\u522b\u548c\u54cd\u5e94\u4eba\u7c7b\u60c5\u7eea\uff0c\u5e76\u63a2\u8ba8\u4e86AI\u662f\u5426\u9700\u8981\u53d1\u5c55\u5185\u90e8\u60c5\u611f\u72b6\u6001\uff08Artificial Emotion\uff0cAE\uff09\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u667a\u80fd\uff08AGI\uff09\u3002", "motivation": "\u73b0\u6709AI\u5728\u60c5\u7eea\u8bc6\u522b\u548c\u5408\u6210\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5185\u90e8\u60c5\u611f\u72b6\u6001\u7684\u5f00\u53d1\u4ecd\u4e0d\u660e\u786e\uff0c\u8fd9\u53ef\u80fd\u4e3aAI\u5e26\u6765\u7c7b\u4f3c\u4eba\u7c7b\u7684\u2018\u5185\u5728\u60c5\u7eea\u2019\u4f18\u52bf\u3002", "method": "\u56de\u987e\u4e86AE\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u60c5\u611f\u8c03\u5236\u7684\u67b6\u6784\uff0c\u5e76\u603b\u7ed3\u4e86\u5efa\u6a21\u548c\u96c6\u6210AE\u5230\u672a\u6765AI\u7684\u673a\u5236\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660eAI\u53ef\u80fd\u8868\u73b0\u51faAE\u884c\u4e3a\uff0c\u4f46\u7f3a\u5c11\u6e05\u6670\u6846\u67b6\u3002AE\u53ef\u80fd\u4e3aAI\u5e26\u6765\u597d\u5904\uff0c\u4f46\u4e5f\u5b58\u5728\u4f26\u7406\u548c\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "AE\u5728\u672a\u6765\u53ef\u80fd\u5bf9AI\u6709\u76ca\uff0c\u4f46\u9700\u8c28\u614e\u5bf9\u5f85\u5176\u4f26\u7406\u548c\u5b89\u5168\u95ee\u9898\u3002"}}
{"id": "2508.10481", "pdf": "https://arxiv.org/pdf/2508.10481", "abs": "https://arxiv.org/abs/2508.10481", "authors": ["Adrien Cassagne", "No\u00e9 Amiot", "Manuel Bouyer"], "title": "Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster", "categories": ["cs.DC"], "comment": null, "summary": "Dalek is an experimental compute cluster designed to evaluate the performance\nof heterogeneous, consumer-grade hardware for software design, prototyping, and\nalgorithm development. In contrast to traditional computing centers that rely\non costly, server-class components, Dalek integrates CPUs and GPUs typically\nfound in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet\nversatile platform. This document details the cluster's architecture and\nsoftware stack, and presents results from synthetic benchmarks. Furthermore, it\nintroduces a custom energy monitoring platform capable of delivering 1000\naveraged samples per second with milliwatt-level resolution. This\nhigh-precision monitoring capability enables a wide range of energy-aware\nresearch experiments in applied Computer Science.", "AI": {"tldr": "Dalek\u662f\u4e00\u4e2a\u5b9e\u9a8c\u6027\u8ba1\u7b97\u96c6\u7fa4\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f02\u6784\u6d88\u8d39\u7ea7\u786c\u4ef6\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u3001\u539f\u578b\u5f00\u53d1\u548c\u7b97\u6cd5\u5f00\u53d1\u4e2d\u7684\u6027\u80fd\u3002\u5b83\u91c7\u7528\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u5e76\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u80fd\u6e90\u76d1\u63a7\u3002", "motivation": "\u8bc4\u4f30\u6d88\u8d39\u7ea7\u786c\u4ef6\u5728\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4f4e\u6210\u672c\u4e14\u591a\u529f\u80fd\u7684\u8ba1\u7b97\u5e73\u53f0\u3002", "method": "\u96c6\u7fa4\u91c7\u7528\u5f02\u6784\u786c\u4ef6\uff08\u5982mini-PC\u3001\u7b14\u8bb0\u672c\u7535\u8111\u548c\u6e38\u620f\u53f0\u5f0f\u673a\u7684CPU\u548cGPU\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u7cbe\u5ea6\u80fd\u6e90\u76d1\u63a7\u5e73\u53f0\u3002", "result": "\u901a\u8fc7\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u96c6\u7fa4\u6027\u80fd\uff0c\u80fd\u6e90\u76d1\u63a7\u5e73\u53f0\u80fd\u5b9e\u73b0\u6beb\u74e6\u7ea7\u5206\u8fa8\u7387\u7684\u9ad8\u7cbe\u5ea6\u91c7\u6837\u3002", "conclusion": "Dalek\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u80fd\u6e90\u610f\u8bc6\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u7814\u7a76\u5e73\u53f0\u3002"}}
{"id": "2508.10516", "pdf": "https://arxiv.org/pdf/2508.10516", "abs": "https://arxiv.org/abs/2508.10516", "authors": ["Micka\u00ebl Martin Nevot"], "title": "Emerging Skycube", "categories": ["cs.DB"], "comment": "Knowledge and Information Systems (KAIS), 2025", "summary": "Combining multi-criteria decision analysis and trend reversal discovery make\nit possible to extract globally optimal, or non-dominated, data in relation to\nseveral criteria, and then to observe their evolution according to a\ndecision-making property. Thus, we introduce Emerging Skycube, a concept\nassociating Skycube and emerging datacube. As far as we know, no\nDBMS-integrated solution exists to compute an emerging Skycube, and hence\ntaking advantage of ROLAP analysis tools. An emerging datacube has only one\nmeasure: we propose to use several to comply to multi-criteria decision\nanalysis constraints which requires multiple attributes. A datacube is\nexpensive to compute. An emerging datacube is about twice as expensive. On the\nother hand, an emerging Skycube is cheaper as the trend reversal is computed\nafter two Skycube calculations, which considerably reduces the relation volume\nin comparison with the initial one. It is possible to save even more computing\ntime and storage space. To this end, we propose two successive reductions.\nFirst, a Skycube lossless partial materialisation using Skylines concepts\nlattice, based on the agree concepts lattice and partitions lattice. Then,\neither the closed emerging Skycube for an information-loss reduction, or the\nclosed emerging L-Skycube for a smaller but lossless reduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u548c\u8d8b\u52bf\u9006\u8f6c\u53d1\u73b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165Emerging Skycube\u6982\u5ff5\uff0c\u6709\u6548\u63d0\u53d6\u5168\u5c40\u6700\u4f18\u6570\u636e\uff0c\u5e76\u4f18\u5316\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u7f3a\u4e4f\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u4e2d\u7684\u590d\u6742\u6570\u636e\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u5c5e\u6027\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faEmerging Skycube\u6982\u5ff5\uff0c\u7ed3\u5408Skycube\u548c\u65b0\u5174\u6570\u636e\u7acb\u65b9\u4f53\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u8fde\u7eed\u7684\u51cf\u5c11\u65b9\u6cd5\u6765\u8282\u7701\u8ba1\u7b97\u65f6\u95f4\u548c\u5b58\u50a8\u7a7a\u95f4\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u548c\u5b58\u50a8\u9700\u6c42\u7684\u540c\u65f6\uff0c\u4ecd\u80fd\u63d0\u53d6\u51fa\u975e\u652f\u914d\u6570\u636e\uff0c\u5e76\u89c2\u5bdf\u5176\u6f14\u5316\u8d8b\u52bf\u3002", "conclusion": "Emerging Skycube\u4e3a\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.10111", "pdf": "https://arxiv.org/pdf/2508.10111", "abs": "https://arxiv.org/abs/2508.10111", "authors": ["Niels M\u00fcndler", "Jasper Dekoninck", "Martin Vechev"], "title": "Constrained Decoding of Diffusion LLMs with Context-Free Grammars", "categories": ["cs.LG", "cs.FL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have shown promising performance across diverse\ndomains. Many practical applications of LLMs, such as code completion and\nstructured data extraction, require adherence to syntactic constraints\nspecified by a formal language. Yet, due to their probabilistic nature, LLM\noutput is not guaranteed to adhere to such formal languages. Prior work has\nproposed constrained decoding as a means to restrict LLM generation to\nparticular formal languages. However, existing works are not applicable to the\nemerging paradigm of diffusion LLMs, when used in practical scenarios such as\nthe generation of formally correct C++ or JSON output. In this paper we address\nthis challenge and present the first constrained decoding method for diffusion\nmodels, one that can handle formal languages captured by context-free grammars.\nWe begin by reducing constrained decoding to the more general additive\ninfilling problem, which asks whether a partial output can be completed to a\nvalid word in the target language. This problem also naturally subsumes the\npreviously unaddressed multi-region infilling constrained decoding. We then\nreduce this problem to the task of deciding whether the intersection of the\ntarget language and a regular language is empty and present an efficient\nalgorithm to solve it for context-free languages. Empirical results on various\napplications, such as C++ code infilling and structured data extraction in\nJSON, demonstrate that our method achieves near-perfect syntactic correctness\nwhile consistently preserving or improving functional correctness. Importantly,\nour efficiency optimizations ensure that the computational overhead remains\npractical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u53d7\u9650\u89e3\u7801\u65b9\u6cd5\uff0c\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u7b26\u5408\u5f62\u5f0f\u8bed\u8a00\uff08\u5982C++\u6216JSON\uff09\u7684\u8bed\u6cd5\u7ea6\u675f\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6982\u7387\u6027\u8f93\u51fa\u53ef\u80fd\u4e0d\u7b26\u5408\u5f62\u5f0f\u8bed\u8a00\u7684\u8bed\u6cd5\u7ea6\u675f\uff0c\u73b0\u6709\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u53d7\u9650\u89e3\u7801\u65b9\u6cd5\u3002", "method": "\u5c06\u53d7\u9650\u89e3\u7801\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u52a0\u6027\u586b\u5145\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u7b80\u5316\u4e3a\u5224\u65ad\u76ee\u6807\u8bed\u8a00\u4e0e\u6b63\u5219\u8bed\u8a00\u4ea4\u96c6\u662f\u5426\u4e3a\u7a7a\u7684\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7b97\u6cd5\u89e3\u51b3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728C++\u4ee3\u7801\u586b\u5145\u548cJSON\u6570\u636e\u63d0\u53d6\u7b49\u4efb\u52a1\u4e2d\uff0c\u51e0\u4e4e\u5b8c\u5168\u4fdd\u8bc1\u4e86\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u529f\u80fd\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u7684\u53d7\u9650\u89e3\u7801\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10588", "pdf": "https://arxiv.org/pdf/2508.10588", "abs": "https://arxiv.org/abs/2508.10588", "authors": ["Siddhartha S. Borkotoky"], "title": "Balancing the Energy Consumption and Latency of Over-the-Air Firmware Updates in LoRaWAN", "categories": ["cs.NI"], "comment": null, "summary": "Over-the-air firmware updates are crucial for mitigating security threats and\nmaintaining up-to-date device functionality in Long Range Wide Area Networks\n(LoRaWANs). LoRaWAN end devices are usually energy-constrained, and LoRaWAN\ntransmissions are subject to duty-cycle restrictions. Consequently, controlling\nthe energy expenditure and update-delivery latency of FUOTA are key challenges.\nWe propose a flexible scheme that achieves a tunable trade-off between the\nenergy consumption and delivery delay. The scheme employs the LoRa spreading\nfactors sequentially to transmit update-carrying frames, sending a fixed number\nof frames with a given spreading factor before moving to the next. By adjusting\nthe smallest spreading factor to be used and the number of transmissions per\nspreading factor, a suitable energy-delay trade-off can be achieved. Thus,\ntime-sensitive updates, such as security patches, may be sent with a\nlow-delay-high-energy setting, whereas a more energy-efficient but higher-delay\nsetting may be used for non-critical updates.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728LoRaWAN\u4e2d\u5b9e\u73b0\u56fa\u4ef6\u66f4\u65b0\uff08FUOTA\uff09\u65f6\u5728\u80fd\u8017\u548c\u5ef6\u8fdf\u4e4b\u95f4\u53ef\u8c03\u8282\u7684\u6743\u8861\uff0c\u9002\u5e94\u4e0d\u540c\u66f4\u65b0\u9700\u6c42\u3002", "motivation": "LoRaWAN\u7ec8\u7aef\u8bbe\u5907\u80fd\u91cf\u53d7\u9650\u4e14\u53d7\u9650\u4e8e\u4f20\u8f93\u5360\u7a7a\u6bd4\uff0c\u56e0\u6b64\u63a7\u5236\u56fa\u4ef6\u66f4\u65b0\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528LoRa\u6269\u9891\u56e0\u5b50\u987a\u5e8f\u4f20\u8f93\u66f4\u65b0\u5e27\uff0c\u901a\u8fc7\u8c03\u6574\u6700\u5c0f\u6269\u9891\u56e0\u5b50\u548c\u6bcf\u4e2a\u6269\u9891\u56e0\u5b50\u7684\u4f20\u8f93\u6b21\u6570\uff0c\u5b9e\u73b0\u80fd\u8017\u4e0e\u5ef6\u8fdf\u7684\u53ef\u8c03\u8282\u6743\u8861\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u66f4\u65b0\u9700\u6c42\u7075\u6d3b\u9009\u62e9\u4f4e\u5ef6\u8fdf\u9ad8\u80fd\u8017\u6216\u9ad8\u5ef6\u8fdf\u4f4e\u80fd\u8017\u6a21\u5f0f\uff0c\u9002\u5e94\u5b89\u5168\u548c\u5e38\u89c4\u66f4\u65b0\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6848\u4e3aLoRaWAN\u4e2d\u7684FUOTA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u80fd\u91cf\u4e0e\u5ef6\u8fdf\u6743\u8861\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002"}}
{"id": "2508.10310", "pdf": "https://arxiv.org/pdf/2508.10310", "abs": "https://arxiv.org/abs/2508.10310", "authors": ["Kaixun Yang", "Yizhou Fan", "Luzhen Tang", "Mladen Rakovi\u0107", "Xinyu Li", "Dragan Ga\u0161evi\u0107", "Guanliang Chen"], "title": "Beyond Self-Regulated Learning Processes: Unveiling Hidden Tactics in Generative AI-Assisted Writing", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The integration of Generative AI (GenAI) into education is reshaping how\nstudents learn, making self-regulated learning (SRL) - the ability to plan,\nmonitor, and adapt one's learning - more important than ever. To support\nlearners in these new contexts, it is essential to understand how SRL unfolds\nduring interaction with GenAI tools. Learning analytics offers powerful\ntechniques for analyzing digital trace data to infer SRL behaviors. However,\nexisting approaches often assume SRL processes are linear, segmented, and\nnon-overlapping-assumptions that overlook the dynamic, recursive, and\nnon-linear nature of real-world learning. We address this by conceptualizing\nSRL as a layered system: observable learning patterns reflect hidden tactics\n(short, purposeful action states), which combine into broader SRL strategies.\nUsing Hidden Markov Models (HMMs), we analyzed trace data from higher education\nstudents engaged in GenAI-assisted academic writing. We identified three\ndistinct groups of learners, each characterized by different SRL strategies.\nThese groups showed significant differences in performance, indicating that\nstudents' use of different SRL strategies in GenAI-assisted writing led to\nvarying task outcomes. Our findings advance the methodological toolkit for\nmodeling SRL and inform the design of adaptive learning technologies that more\neffectively support learners in GenAI-enhanced educational environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u751f\u6210\u5f0fAI\u8f85\u52a9\u5b66\u4e60\u4e2d\u5982\u4f55\u901a\u8fc7\u5206\u5c42\u7cfb\u7edf\u6982\u5ff5\u5316\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\uff0c\u5e76\u5229\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMMs\uff09\u5206\u6790\u5b66\u4e60\u884c\u4e3a\uff0c\u53d1\u73b0\u4e0d\u540cSRL\u7b56\u7565\u7684\u5b66\u4e60\u8005\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5728\u6559\u80b2\u4e2d\u7684\u6574\u5408\uff0c\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u4e3a\u4e86\u652f\u6301\u5b66\u4e60\u8005\u5728\u8fd9\u79cd\u65b0\u73af\u5883\u4e2d\u5b66\u4e60\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3SRL\u4e0eGenAI\u4e92\u52a8\u4e2d\u7684\u52a8\u6001\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7cfb\u7edf\u6982\u5ff5\u5316SRL\uff0c\u5c06\u5176\u5206\u4e3a\u53ef\u89c2\u5bdf\u7684\u5b66\u4e60\u6a21\u5f0f\u548c\u9690\u85cf\u7684\u6218\u672f\u884c\u4e3a\uff0c\u8fdb\u800c\u5229\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMMs\uff09\u5206\u6790\u9ad8\u7b49\u6559\u80b2\u5b66\u751f\u5728GenAI\u8f85\u52a9\u5199\u4f5c\u4e2d\u7684\u6570\u5b57\u75d5\u8ff9\u6570\u636e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u4e60\u8005\u53ef\u5206\u4e3a\u4e09\u4e2a\u5177\u6709\u4e0d\u540cSRL\u7b56\u7565\u7684\u7fa4\u4f53\uff0c\u4e14\u8fd9\u4e9b\u7fa4\u4f53\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u8868\u660eSRL\u7b56\u7565\u7684\u9009\u62e9\u5728GenAI\u8f85\u52a9\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684SRL\u5efa\u6a21\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u81ea\u9002\u5e94\u6027\u5b66\u4e60\u6280\u672f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u652f\u6301GenAI\u589e\u5f3a\u7684\u6559\u80b2\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u8005\u3002"}}
{"id": "2508.10854", "pdf": "https://arxiv.org/pdf/2508.10854", "abs": "https://arxiv.org/abs/2508.10854", "authors": ["Oliver Thomson Brown", "Mateusz Meller", "James Richings"], "title": "Introducing CQ: A C-like API for Quantum Accelerated HPC", "categories": ["cs.DC", "quant-ph"], "comment": "8 pages, 1 figure. Submitted to the 1st International Workshop for\n  Software Frameworks and Workload Management on Quantum and HPC Ecosystems at\n  SC25", "summary": "In this paper we present CQ, a specification for a C-like API for quantum\naccelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written\nin C99, and built on top of the statevector simulator QuEST. CQ focuses on\nenabling the incremental integration of quantum computing into classical HPC\ncodes by supporting runtime offloading from languages such as C and Fortran. It\nprovides a way of describing and offloading quantum computations which is\ncompatible with strictly and strongly typed compiled languages, and gives the\nprogrammer fine-grained control over classical data movement. The CQ Simulated\nBackend (CQ-SimBE) provides both a way to demonstrate the usage and utility of\nCQ, and a space to experiment with new features such as support for analogue\nquantum computing. Both the CQ specification and CQ-SimBE are open-source, and\navailable in public repositories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCQ\u7684\u91cf\u5b50\u52a0\u901fHPC\u7684C-like API\u89c4\u8303\uff0c\u4ee5\u53ca\u5176\u53c2\u8003\u5b9e\u73b0CQ-SimBE\uff0c\u65e8\u5728\u5c06\u91cf\u5b50\u8ba1\u7b97\u9010\u6b65\u96c6\u6210\u5230\u4f20\u7edfHPC\u4ee3\u7801\u4e2d\u3002", "motivation": "\u652f\u6301\u4eceC\u548cFortran\u7b49\u8bed\u8a00\u8fd0\u884c\u65f6\u5378\u8f7d\u91cf\u5b50\u8ba1\u7b97\uff0c\u4e3a\u4e25\u683c\u548c\u5f3a\u7c7b\u578b\u7f16\u8bd1\u8bed\u8a00\u63d0\u4f9b\u63cf\u8ff0\u548c\u5378\u8f7d\u91cf\u5b50\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u5e76\u8d4b\u4e88\u7a0b\u5e8f\u5458\u5bf9\u7ecf\u5178\u6570\u636e\u79fb\u52a8\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86CQ\u89c4\u8303\u53ca\u5176\u53c2\u8003\u5b9e\u73b0CQ-SimBE\uff0c\u57fa\u4e8estatevector\u6a21\u62df\u5668QuEST\uff0c\u91c7\u7528C99\u7f16\u5199\u3002", "result": "CQ-SimBE\u5c55\u793a\u4e86CQ\u7684\u7528\u9014\u548c\u5b9e\u7528\u6027\uff0c\u5e76\u4e3a\u652f\u6301\u6a21\u62df\u91cf\u5b50\u8ba1\u7b97\u7b49\u65b0\u529f\u80fd\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u7a7a\u95f4\u3002", "conclusion": "CQ\u89c4\u8303\u548cCQ-SimBE\u5747\u4e3a\u5f00\u6e90\uff0c\u53ef\u5728\u516c\u5171\u4ed3\u5e93\u4e2d\u83b7\u53d6\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u4e0e\u4f20\u7edfHPC\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.10497", "pdf": "https://arxiv.org/pdf/2508.10497", "abs": "https://arxiv.org/abs/2508.10497", "authors": ["Abdullah Farrukh", "Achim Wagner", "Martin Ruskowski"], "title": "Enabling Generic Robot Skill Implementation Using Object Oriented Programming", "categories": ["cs.RO", "cs.SE"], "comment": "34th International Conference on Robotics in Alpe-Adria-Danube Region\n  (RAAD 2025)", "summary": "Developing robotic algorithms and integrating a robotic subsystem into a\nlarger system can be a difficult task. Particularly in small and medium-sized\nenterprises (SMEs) where robotics expertise is lacking, implementing,\nmaintaining and developing robotic systems can be a challenge. As a result,\nmany companies rely on external expertise through system integrators, which, in\nsome cases, can lead to vendor lock-in and external dependency. In the academic\nresearch on intelligent manufacturing systems, robots play a critical role in\nthe design of robust autonomous systems. Similar challenges are faced by\nresearchers who want to use robotic systems as a component in a larger smart\nsystem, without having to deal with the complexity and vastness of the robot\ninterfaces in detail. In this paper, we propose a software framework that\nreduces the effort required to deploy a working robotic system. The focus is\nsolely on providing a concept for simplifying the different interfaces of a\nmodern robot system and using an abstraction layer for different manufacturers\nand models. The Python programming language is used to implement a prototype of\nthe concept. The target system is a bin-picking cell containing a Yaskawa\nMotoman GP4.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u73b0\u4ee3\u673a\u5668\u4eba\u7cfb\u7edf\u63a5\u53e3\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u65e8\u5728\u964d\u4f4e\u90e8\u7f72\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u9488\u5bf9\u4e2d\u5c0f\u4f01\u4e1a\u548c\u7814\u7a76\u4eba\u5458\u3002", "motivation": "\u4e2d\u5c0f\u4f01\u4e1a\u548c\u7814\u7a76\u4eba\u5458\u5728\u7f3a\u4e4f\u673a\u5668\u4eba\u4e13\u4e1a\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u5b9e\u73b0\u548c\u7ef4\u62a4\u673a\u5668\u4eba\u7cfb\u7edf\u3002\u4f9d\u8d56\u5916\u90e8\u96c6\u6210\u5546\u53ef\u80fd\u5bfc\u81f4\u4f9b\u5e94\u5546\u9501\u5b9a\u548c\u5916\u90e8\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u62bd\u8c61\u5c42\u7edf\u4e00\u4e0d\u540c\u5236\u9020\u5546\u548c\u578b\u53f7\u7684\u673a\u5668\u4eba\u63a5\u53e3\uff0c\u4f7f\u7528Python\u5b9e\u73b0\u539f\u578b\uff0c\u5e94\u7528\u4e8e\u5305\u542bYaskawa Motoman GP4\u7684\u62e3\u9009\u5355\u5143\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u90e8\u7f72\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u6846\u67b6\u7b80\u5316\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u63a5\u53e3\u548c\u90e8\u7f72\u8fc7\u7a0b\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u548c\u5916\u90e8\u4f9d\u8d56\u7684\u9700\u6c42\u3002"}}
{"id": "2508.10185", "pdf": "https://arxiv.org/pdf/2508.10185", "abs": "https://arxiv.org/abs/2508.10185", "authors": ["Ren\u00e9 Mayrhofer", "Michael Roland", "Tobias H\u00f6ller", "Philipp Hofer", "Mario Lins"], "title": "An Architecture for Distributed Digital Identities in the Physical World", "categories": ["cs.CR", "cs.CY", "cs.NI"], "comment": null, "summary": "Digital identities are increasingly important for mediating not only digital\nbut also physical service transactions. Managing such identities through\ncentralized providers can cause both availability and privacy concerns: single\npoints of failure and control are ideal targets for global attacks on\ntechnical, organizational, or legal fronts. We design, analyze, and build a\ndistributed digital identity architecture for physical world transactions in\ncommon scenarios like unlocking doors, public transport, or crossing country\nborders. This architecture combines (biometric and other) sensors, (established\nand upcoming) identity authorities, attribute verifiers, and a new core\ncomponent we call the \\emph{Personal Identity Agent (PIA)} that represents\nindividuals with their identity attributes in the digital domain. All\ntransactions are conducted in a completely decentralized manner, and the\ncomponents for which we currently assume central coordination are optional and\nonly used for assisting with service discovery and latency reduction. We\npresent a first protocol between these parties and formally verify that it\nachieves relevant security properties based on a realistic threat model\nincluding strong global adversaries. A proof-of-concept implementation\ndemonstrates practical feasibility of both architecture and initial protocol\nfor applications that can tolerate end-to-end latencies in the range of a few\nseconds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u6570\u5b57\u8eab\u4efd\u67b6\u6784\uff0c\u7528\u4e8e\u7269\u7406\u4e16\u754c\u4e2d\u7684\u4ea4\u6613\uff0c\u5982\u5f00\u95e8\u3001\u516c\u5171\u4ea4\u901a\u6216\u8de8\u5883\uff0c\u89e3\u51b3\u4e86\u96c6\u4e2d\u5f0f\u8eab\u4efd\u7ba1\u7406\u7684\u9690\u79c1\u548c\u53ef\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u96c6\u4e2d\u5f0f\u8eab\u4efd\u7ba1\u7406\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u5b89\u5168\u7684\u7269\u7406\u4e16\u754c\u4ea4\u6613\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4f20\u611f\u5668\u3001\u8eab\u4efd\u673a\u6784\u3001\u5c5e\u6027\u9a8c\u8bc1\u5668\u548c\u65b0\u578b\u4e2a\u4eba\u8eab\u4efd\u4ee3\u7406\uff08PIA\uff09\u7684\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u534f\u8bae\u5b9e\u73b0\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u8bc1\u660e\u534f\u8bae\u6ee1\u8db3\u5b89\u5168\u5c5e\u6027\uff0c\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u5c55\u793a\u4e86\u67b6\u6784\u548c\u534f\u8bae\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5206\u5e03\u5f0f\u6570\u5b57\u8eab\u4efd\u67b6\u6784\u5728\u5ef6\u8fdf\u53ef\u63a5\u53d7\u7684\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.10590", "pdf": "https://arxiv.org/pdf/2508.10590", "abs": "https://arxiv.org/abs/2508.10590", "authors": ["Viswak R Balaji", "Samuel Punch"], "title": "Simulating Mass-Dependent Decoherence in Quantum Computers: Baseline Signatures for Testing Gravity-Induced Collapse", "categories": ["quant-ph", "cs.ET", "physics.comp-ph"], "comment": null, "summary": "We present a quantum computing simulation study of mass-dependent decoherence\nmodels inspired by Penrose's gravity-induced collapse hypothesis. According to\nobjective reduction (OR) theory, quantum superpositions become unstable when\nthe gravitational self-energy difference between branches exceeds a certain\nthreshold, leading to a collapse time $\\tau \\approx \\hbar / E_G$. In this work,\nwe implement a mass-dependent dephasing noise channel, $p(m) = 1 - e^{-k\nm^{\\alpha}}$, within the Qiskit AerSimulator, where $m$ is a proxy for the\neffective mass of a superposition, mapped to circuit parameters such as the\nnumber of entangled qubits or branch size. We apply this model to three\ncanonical quantum computing experiments: GHZ state parity measurements,\nbranch-mass entanglement tests, and Grover's search to generate distinctive\ncollapse signatures that differ qualitatively from constant-rate dephasing. The\nresulting patterns serve as a baseline reference: if future hardware\nexperiments exhibit the same scaling trends under ideal isolation, this could\nindicate a contribution from mass-dependent collapse processes. Conversely,\ndeviation toward constant-noise behaviour would suggest the absence of such\ngravitationally induced effects. Our results provide a reproducible protocol\nand reference for using quantum computers as potential testbeds for probing\nfundamental questions in quantum mechanics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u7814\u7a76\u4e86\u8d28\u91cf\u76f8\u5173\u9000\u76f8\u5e72\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86Penrose\u5f15\u529b\u8bf1\u5bfc\u574d\u584c\u5047\u8bbe\u7684\u91cf\u5b50\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u91cf\u5b50\u53e0\u52a0\u6001\u7684\u5f15\u529b\u8bf1\u5bfc\u574d\u584c\u73b0\u8c61\uff0c\u4e3a\u91cf\u5b50\u529b\u5b66\u57fa\u672c\u539f\u7406\u63d0\u4f9b\u5b9e\u9a8c\u53c2\u8003\u3002", "method": "\u5229\u7528Qiskit AerSimulator\u5b9e\u73b0\u8d28\u91cf\u76f8\u5173\u9000\u76f8\u566a\u58f0\u6a21\u578b\uff0c\u5e94\u7528\u4e8eGHZ\u6001\u3001\u5206\u652f\u8d28\u91cf\u7ea0\u7f20\u548cGrover\u641c\u7d22\u7b49\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u751f\u6210\u72ec\u7279\u7684\u574d\u584c\u7279\u5f81\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u68c0\u6d4b\u5f15\u529b\u8bf1\u5bfc\u6548\u5e94\u7684\u57fa\u51c6\u53c2\u8003\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u6d4b\u8bd5\u91cf\u5b50\u529b\u5b66\u57fa\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u534f\u8bae\u548c\u53c2\u8003\u6846\u67b6\u3002"}}
{"id": "2508.10353", "pdf": "https://arxiv.org/pdf/2508.10353", "abs": "https://arxiv.org/abs/2508.10353", "authors": ["G. Kalyan Ramana", "Sumit Yempalle", "Prasad S. Onkar"], "title": "Mental Effort Estimation in Motion Exploration and Concept Generation Design Tasks using Inter-Band Relative Power Difference of EEG", "categories": ["cs.HC", "stat.ME"], "comment": null, "summary": "Conceptual design is a cognitively complex task, especially in the\nengineering design of products having relative motion between components.\nDesigners prefer sketching as a medium for conceptual design and use gestures\nand annotations to represent such relative motion. Literature suggests that\nstatic representations of motion in sketches may not achieve the intended\nfunctionality when realised, because it primarily depends on the designers'\nmental capabilities for motion simulation. Thus, it is important to understand\nthe cognitive phenomena when designers are exploring concepts of articulated\nproducts. The current work is an attempt to understand design neurocognition by\ncategorising the tasks and measuring the mental effort involved in these tasks\nusing EEG. The analysis is intended to validate design intervention tools to\nsupport the conceptual design involving motion exploration. A novel EEG-based\nmetric, inter-Band Relative Power Difference (inter-BRPD), is introduced to\nquantify mental effort. A design experiment is conducted with 32 participants,\nwhere they have to perform one control task and 2 focus tasks corresponding to\nthe motion exploration task (MET) and the concept generation task (CGT),\nrespectively. EEG data is recorded during the 3 tasks, cleaned, processed and\nanalysed using the MNE library in Python. It is observed from the results that\ninter-BRPD captures the essence of mental effort with half the number of\nconventionally used parameters. The reliability and efficacy of the inter-BRPD\nmetric are also statistically validated against literature-based cognitive\nmetrics. With these new insights, the study opens up possibilities for creating\nsupport for conceptual design and its evaluation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bbe\u8ba1\u5e08\u5728\u6982\u5ff5\u8bbe\u8ba1\u4e2d\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u8fd0\u52a8\u63a2\u7d22\u4efb\u52a1\u65f6\uff0c\u901a\u8fc7EEG\u6d4b\u91cf\u5fc3\u7406\u52aa\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u6307\u6807inter-BRPD\u3002", "motivation": "\u7406\u89e3\u8bbe\u8ba1\u5e08\u5728\u63a2\u7d22\u5173\u8282\u4ea7\u54c1\u6982\u5ff5\u65f6\u7684\u8ba4\u77e5\u73b0\u8c61\uff0c\u4ee5\u652f\u6301\u6982\u5ff5\u8bbe\u8ba1\u5de5\u5177\u7684\u5f00\u53d1\u3002", "method": "\u901a\u8fc7EEG\u8bb0\u5f5532\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\uff0c\u4f7f\u7528\u65b0\u6307\u6807inter-BRPD\u91cf\u5316\u5fc3\u7406\u52aa\u529b\u3002", "result": "inter-BRPD\u4ec5\u9700\u4e00\u534a\u4f20\u7edf\u53c2\u6570\u5373\u53ef\u6709\u6548\u6355\u6349\u5fc3\u7406\u52aa\u529b\uff0c\u7edf\u8ba1\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6982\u5ff5\u8bbe\u8ba1\u7684\u652f\u6301\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.10862", "pdf": "https://arxiv.org/pdf/2508.10862", "abs": "https://arxiv.org/abs/2508.10862", "authors": ["Brendan Kobayashi Chou", "Andrew Lewis-Pye", "Patrick O'Grady"], "title": "Minimmit: Fast Finality with Even Faster Blocks", "categories": ["cs.DC"], "comment": null, "summary": "Minimmit is a new protocol for State-Machine-Replication (SMR) that extends\nthe '2-round finality' approach of protocols such as Alpenglow to further\nreduce latency, by allowing for faster progression through 'views'. This\npreliminary draft provides motivation and pseudocode, together with proofs of\nconsistency and liveness. An updated draft with a proof of optimistic\nresponsiveness, suggested optimizations, and experiments, is to follow.", "AI": {"tldr": "Minimmit\u662f\u4e00\u79cd\u65b0\u7684\u72b6\u6001\u673a\u590d\u5236\u534f\u8bae\uff0c\u901a\u8fc7\u5feb\u901f\u5207\u6362\u89c6\u56fe\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u6269\u5c55Alpenglow\u7b49\u534f\u8bae\u7684'2\u8f6e\u6700\u7ec8\u6027'\u65b9\u6cd5\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u5ef6\u8fdf\u3002", "method": "\u5f15\u5165\u5feb\u901f\u5207\u6362\u89c6\u56fe\u7684\u673a\u5236\uff0c\u63d0\u4f9b\u4e00\u81f4\u6027\u53ca\u6d3b\u8dc3\u6027\u8bc1\u660e\u3002", "result": "\u521d\u6b65\u63d0\u4f9b\u4e86\u534f\u8bae\u7684\u8bbe\u8ba1\u3001\u8bc1\u660e\u548c\u4f2a\u4ee3\u7801\uff0c\u540e\u7eed\u5c06\u8865\u5145\u4f18\u5316\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "Minimmit\u5c55\u793a\u4e86\u964d\u4f4eSMR\u5ef6\u8fdf\u7684\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u5b8c\u5584\u3002"}}
{"id": "2508.10346", "pdf": "https://arxiv.org/pdf/2508.10346", "abs": "https://arxiv.org/abs/2508.10346", "authors": ["Md Ashraf Uddin", "Nam H. Chu", "Reza Rafeh"], "title": "A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks", "categories": ["cs.LG", "cs.NI"], "comment": "13 pages, and 4 figures", "summary": "The Internet of Medical Things (IoMT) is driving a healthcare revolution but\nremains vulnerable to cyberattacks such as denial of service, ransomware, data\nhijacking, and spoofing. These networks comprise resource constrained,\nheterogeneous devices (e.g., wearable sensors, smart pills, implantables),\nmaking traditional centralized Intrusion Detection Systems (IDSs) unsuitable\ndue to response delays, privacy risks, and added vulnerabilities. Centralized\nIDSs require all sensors to transmit data to a central server, causing delays\nor network disruptions in dense environments. Running IDSs locally on IoMT\ndevices is often infeasible due to limited computation, and even lightweight\nIDS components remain at risk if updated models are delayed leaving them\nexposed to zero-day attacks that threaten patient health and data security. We\npropose a multi level IoMT IDS framework capable of detecting zero day attacks\nand distinguishing between known and unknown threats. The first layer (near\nEdge) filters traffic at a coarse level (attack or not) using meta-learning or\nOne Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far\nEdge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024\ndataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first\nlayer detects zero-day attacks with high accuracy without needing new datasets,\nensuring strong applicability in IoMT environments. Additionally, the\nmeta-learning approach achieves high.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u7ea7IoMT\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u6846\u67b6\uff0c\u80fd\u591f\u68c0\u6d4b\u96f6\u65e5\u653b\u51fb\u5e76\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u5a01\u80c1\uff0c\u5b9e\u9a8c\u663e\u793a\u9ad8\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "IoMT\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0fIDS\u4e0d\u9002\u5408\uff0c\u53ef\u80fd\u5bfc\u81f4\u5ef6\u8fdf\u6216\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u591a\u5c42\u6846\u67b6\uff0c\u7b2c\u4e00\u5c42\u4f7f\u7528\u5143\u5b66\u4e60\u6216OCC\u7c97\u7565\u8fc7\u6ee4\u6d41\u91cf\uff0c\u540e\u7eed\u5c42\u8bc6\u522b\u653b\u51fb\u7c7b\u578b\u548c\u65b0\u9896\u6027\u3002", "result": "\u5728CICIoMT2024\u6570\u636e\u96c6\u4e0a\u5b9e\u73b099.77%\u51c6\u786e\u7387\u548c97.8% F1\u5206\u6570\uff0c\u7b2c\u4e00\u5c42\u80fd\u9ad8\u6548\u68c0\u6d4b\u96f6\u65e5\u653b\u51fb\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728IoMT\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f3a\u9002\u7528\u6027\u548c\u9ad8\u5b89\u5168\u6027\u3002"}}
{"id": "2508.10856", "pdf": "https://arxiv.org/pdf/2508.10856", "abs": "https://arxiv.org/abs/2508.10856", "authors": ["Bastian Heinlein", "Kaikai Zhu", "S\u00fcmeyye Carkit-Yilmaz", "Sebastian Lotter", "Helene M. Loos", "Andrea Buettner", "Yansha Deng", "Robert Schober", "Vahid Jamali"], "title": "Molecule Mixture Detection and Alphabet Design for Non-linear, Cross-reactive Receiver Arrays in MC", "categories": ["eess.SP", "cs.ET"], "comment": "7 pages, 3 figures. Accepted at ACM NanoCom 2025", "summary": "Air-based molecular communication (MC) has the potential to be one of the\nfirst MC systems to be deployed in real-world applications, enabled by existing\nsensor technologies such as metal-oxide semi-conductor (MOS) sensors. However,\ncommercially available sensors usually exhibit non-linear and cross-reactive\nbehavior, contrary to the idealizing assumptions about linear and perfectly\nmolecule type-specific sensing often made in the MC literature. To address this\ngap, we propose a detector for molecule mixture communication with a general\nnon-linear, cross-reactive receiver (RX) array that performs approximate\nmaximum likelihood detection on the sensor outputs. Additionally, we introduce\nan algorithm for the design of mixture alphabets that accounts for the RX\ncharacteristics. We evaluate our detector and alphabet design algorithm through\nsimulations that are based on measurements reported for two commercial MOS\nsensors. Our simulations demonstrate that the proposed detector achieves\nsimilar symbol error rates as data-driven methods without requiring large\nnumbers of training samples and that the alphabet design algorithm outperforms\nmethods that do not account for the RX characteristics. Since the proposed\ndetector and alphabet design algorithm are also applicable to other chemical\nsensors, they pave the way for reliable air-based MC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u3001\u4ea4\u53c9\u654f\u611f\u63a5\u6536\u5668\u7684\u5206\u5b50\u6df7\u5408\u7269\u901a\u4fe1\u68c0\u6d4b\u5668\u548c\u5b57\u6bcd\u8868\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5546\u7528\u4f20\u611f\u5668\u7684\u975e\u7ebf\u6027\u548c\u4ea4\u53c9\u654f\u611f\u884c\u4e3a\u4e0e\u7406\u60f3\u5047\u8bbe\u4e0d\u7b26\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b9e\u9645\u7684\u68c0\u6d4b\u5668\u548c\u5b57\u6bcd\u8868\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u7684\u68c0\u6d4b\u5668\uff0c\u5e76\u7ed3\u5408\u63a5\u6536\u5668\u7279\u6027\u7684\u5b57\u6bcd\u8868\u8bbe\u8ba1\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u68c0\u6d4b\u5668\u6027\u80fd\u63a5\u8fd1\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e14\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6837\u672c\uff1b\u5b57\u6bcd\u8868\u8bbe\u8ba1\u7b97\u6cd5\u4f18\u4e8e\u4e0d\u8003\u8651\u63a5\u6536\u5668\u7279\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u53ef\u9760\u7684\u6c14\u4f53\u5206\u5b50\u901a\u4fe1\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u5316\u5b66\u4f20\u611f\u5668\u3002"}}
{"id": "2508.10364", "pdf": "https://arxiv.org/pdf/2508.10364", "abs": "https://arxiv.org/abs/2508.10364", "authors": ["Xueer Lin", "Chenyu Li", "Yuhan Lyu", "Zhicong Lu", "Zhenhui Peng"], "title": "\"Here Comes the Makeup Tutorial You Asked For!\": Exploring Communication Strategies and Viewer Engagement in Beauty Videos on Rednote", "categories": ["cs.HC"], "comment": null, "summary": "More and more people, especially females, create and view beauty videos\ncovering topics like makeup tutorials and vlogs on social media platforms.\nUnderstanding the communication strategies that creators use in these videos\nand how they affect viewers' engagement can help spread beauty knowledge. By\ncoding 352 beauty videos in Rednote, this study presents a comprehensive\ntaxonomy of communication strategies used by the creators, such as using home\nas the video background and displaying makeup effects when starting the\nnarrative at the beginning. We further label and computationally classify six\ncategories of comments that reveal viewers' engagement with beauty videos. The\nregression analyses reveal the effects of beauty video communication strategies\non viewers' engagement; for example, calling viewers to take action at the end\ntends to attract more comments that debate the product's efficacy. We discuss\ninsights into fostering the creation of beauty videos and the communication of\nbeauty knowledge.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u7f8e\u5986\u89c6\u9891\u7684\u4f20\u64ad\u7b56\u7565\u53ca\u5176\u5bf9\u89c2\u4f17\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7f16\u7801352\u4e2a\u89c6\u9891\u548c\u8bc4\u8bba\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u4f8b\u5982\u547c\u5401\u884c\u52a8\u80fd\u589e\u52a0\u8ba8\u8bba\u7684\u89c4\u5f8b\u3002", "motivation": "\u63a2\u8ba8\u7f8e\u5986\u89c6\u9891\u521b\u4f5c\u8005\u4f7f\u7528\u7684\u4f20\u64ad\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u89c2\u4f17\u53c2\u4e0e\uff0c\u4ee5\u4fc3\u8fdb\u7f8e\u5986\u77e5\u8bc6\u7684\u4f20\u64ad\u3002", "method": "\u7f16\u7801352\u4e2aRednote\u5e73\u53f0\u7684\u7f8e\u5986\u89c6\u9891\uff0c\u5206\u7c7b\u8bc4\u8bba\u5e76\u56de\u5f52\u5206\u6790\u7b56\u7565\u5bf9\u89c2\u4f17\u53c2\u4e0e\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u7279\u5b9a\u7b56\u7565\u5982\u7ed3\u5c3e\u547c\u5401\u884c\u52a8\u80fd\u5f15\u53d1\u66f4\u591a\u5173\u4e8e\u4ea7\u54c1\u6548\u679c\u7684\u8ba8\u8bba\u3002", "conclusion": "\u4e3a\u7f8e\u5986\u89c6\u9891\u521b\u4f5c\u548c\u77e5\u8bc6\u4f20\u64ad\u63d0\u4f9b\u4e86\u5b9e\u9645\u542f\u793a\u3002"}}
{"id": "2508.10350", "pdf": "https://arxiv.org/pdf/2508.10350", "abs": "https://arxiv.org/abs/2508.10350", "authors": ["Samer Lahoud", "Kinda Khawam"], "title": "Semantic Communication with Distribution Learning through Sequential Observations", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Semantic communication aims to convey meaning rather than bit-perfect\nreproduction, representing a paradigm shift from traditional communication.\nThis paper investigates distribution learning in semantic communication where\nreceivers must infer the underlying meaning distribution through sequential\nobservations. While semantic communication traditionally optimizes individual\nmeaning transmission, we establish fundamental conditions for learning source\nstatistics when priors are unknown. We prove that learnability requires full\nrank of the effective transmission matrix, characterize the convergence rate of\ndistribution estimation, and quantify how estimation errors translate to\nsemantic distortion. Our analysis reveals a fundamental trade-off: encoding\nschemes optimized for immediate semantic performance often sacrifice long-term\nlearnability. Experiments on CIFAR-10 validate our theoretical framework,\ndemonstrating that system conditioning critically impacts both learning rate\nand achievable performance. These results provide the first rigorous\ncharacterization of statistical learning in semantic communication and offer\ndesign principles for systems that balance immediate performance with\nadaptation capability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u5206\u5e03\u5b66\u4e60\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5728\u672a\u77e5\u5148\u9a8c\u60c5\u51b5\u4e0b\u5b66\u4e60\u6e90\u7edf\u8ba1\u91cf\u7684\u57fa\u672c\u6761\u4ef6\uff0c\u5206\u6790\u4e86\u5b66\u4e60\u901f\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49\u901a\u4fe1\u4f18\u5316\u5355\u4e2a\u610f\u4e49\u7684\u4f20\u8f93\uff0c\u4f46\u672c\u6587\u65e8\u5728\u89e3\u51b3\u63a5\u6536\u7aef\u901a\u8fc7\u5e8f\u5217\u89c2\u5bdf\u63a8\u65ad\u6f5c\u5728\u610f\u4e49\u5206\u5e03\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5148\u9a8c\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u53ef\u5b66\u4e60\u6027\u9700\u8981\u6709\u6548\u4f20\u8f93\u77e9\u9635\u7684\u6ee1\u79e9\u6761\u4ef6\uff0c\u5e76\u91cf\u5316\u4f30\u8ba1\u8bef\u5dee\u5bf9\u8bed\u4e49\u5931\u771f\u7684\u5f71\u54cd\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6761\u4ef6\u5bf9\u5b66\u4e60\u901f\u7387\u548c\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4e00\u79cd\u6839\u672c\u6027\u6743\u8861\uff1a\u4f18\u5316\u5373\u65f6\u8bed\u4e49\u6027\u80fd\u7684\u7f16\u7801\u65b9\u6848\u5f80\u5f80\u727a\u7272\u957f\u671f\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u4e25\u683c\u63cf\u8ff0\u4e86\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u7edf\u8ba1\u5b66\u4e60\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e73\u8861\u5373\u65f6\u6027\u80fd\u4e0e\u9002\u5e94\u80fd\u529b\u7684\u7cfb\u7edf\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2508.10414", "pdf": "https://arxiv.org/pdf/2508.10414", "abs": "https://arxiv.org/abs/2508.10414", "authors": ["Yuan-Yi Fan"], "title": "MCP2OSC: Parametric Control by Natural Language", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Text prompts enable intuitive content creation but may fall short in\nachieving high precision for intricate tasks; knob or slider controls offer\nprecise adjustments at the cost of increased complexity. To address the gap\nbetween knobs and prompts, a new MCP (Model Context Protocol) server and a\nunique set of prompt design criteria are presented to enable exploring\nparametric OSC (OpenSoundControl) control by natural language prompts.\nDemonstrated by 14 practical QA examples with best practices and the\ngeneralized prompt templates, this study finds Claude integrated with the\nMCP2OSC server effective in generating OSC messages by natural language,\ninterpreting, searching, and visualizing OSC messages, validating and debugging\nOSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine\ncollaboration by leveraging LLM (Large Language Model) to handle intricate OSC\ndevelopment tasks, and by empowering human creativity with an intuitive\nlanguage interface featuring flexible precision controls: a prompt-based OSC\ntool. This study provides a novel perspective on the creative MCP application\nat the network protocol level by utilizing LLM's strength in directly\nprocessing and generating human-readable OSC messages. The results suggest its\npotential for a LLM-based universal control mechanism for multimedia devices.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bMCP\u670d\u52a1\u5668\u548c\u63d0\u793a\u8bbe\u8ba1\u6807\u51c6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u7ed3\u5408LLM\u5904\u7406OSC\u6d88\u606f\uff0c\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u63d0\u793a\u5728\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u4ee5\u53ca\u65cb\u94ae\u63a7\u5236\u7684\u590d\u6742\u6027\uff0c\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u5728OSC\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMCP\u670d\u52a1\u5668\u548c\u63d0\u793a\u8bbe\u8ba1\u6807\u51c6\uff0c\u7ed3\u5408Claude\u6a21\u578b\u548cMCP2OSC\u670d\u52a1\u5668\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u548c\u5904\u7406OSC\u6d88\u606f\u3002", "result": "\u6210\u529f\u5b9e\u73b0OSC\u6d88\u606f\u7684\u751f\u6210\u3001\u89e3\u91ca\u3001\u641c\u7d22\u3001\u53ef\u89c6\u5316\u3001\u9a8c\u8bc1\u548c\u8c03\u8bd5\uff0c\u5c55\u793a\u4e86LLM\u5728\u591a\u5a92\u4f53\u8bbe\u5907\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "MCP2OSC\u901a\u8fc7LLM\u548c\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\uff0c\u4e3a\u591a\u5a92\u4f53\u8bbe\u5907\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u901a\u7528\u673a\u5236\u3002"}}
{"id": "2508.10613", "pdf": "https://arxiv.org/pdf/2508.10613", "abs": "https://arxiv.org/abs/2508.10613", "authors": ["Mengyao Li", "Qiaolun Zhang", "Zongshuai Yang", "Stefano Bregni", "Alberto Gatto", "Raouf Boutaba", "Massimo Tornatore"], "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD Networks", "categories": ["quant-ph", "cs.NI"], "comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025", "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3amaxNAR\u7684\u65b0\u6307\u6807\u6765\u91cf\u5316\u7269\u7406\u5c42\u653b\u51fb\u7684\u6700\u574f\u5f71\u54cd\uff0c\u5e76\u7814\u7a76\u4e86RWA-MAR\u95ee\u9898\u3002\u901a\u8fc7ILP\u6a21\u578b\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7ed3\u5408QKPs\u7f13\u5b58\u673a\u5236\uff0c\u4f18\u5316\u4e86\u5b89\u5168\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1(QKD)\u867d\u7136\u5177\u6709\u5b89\u5168\u6027\uff0c\u4f46\u5bb9\u6613\u56e0\u7269\u7406\u5c42\u653b\u51fb\uff08\u5982\u9ad8\u529f\u7387\u5e72\u6270\uff09\u4e2d\u65ad\u5bc6\u94a5\u4ea4\u6362\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u9700\u8981\u91cf\u5316\u653b\u51fb\u5f71\u54cd\u5e76\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u3002", "method": "\u5f15\u5165maxNAR\u6307\u6807\u8bc4\u4f30\u653b\u51fb\u5f71\u54cd\uff0c\u63d0\u51faRWA-MAR\u95ee\u9898\u5e76\u7528ILP\u5efa\u6a21\u3002\u8bbe\u8ba1\u4e86\u7ed3\u5408QKPs\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86OB\u548cTR\u67b6\u6784\u7684\u52a8\u6001\u63a7\u5236\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7maxNAR\u548cRWA-MAR\u6846\u67b6\uff0c\u8be5\u7814\u7a76\u6709\u6548\u63d0\u5347\u4e86QKD\u7f51\u7edc\u5bf9\u7269\u7406\u5c42\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u67b6\u6784\u9009\u62e9\u589e\u5f3a\u4e86\u9002\u5e94\u6027\u3002"}}
{"id": "2508.10468", "pdf": "https://arxiv.org/pdf/2508.10468", "abs": "https://arxiv.org/abs/2508.10468", "authors": ["Paul Schreiber", "Beyza Cinar", "Lennart Mackert", "Maria Maleshkova"], "title": "Stress Detection from Multimodal Wearable Sensor Data", "categories": ["cs.HC", "I.2.6; J.3"], "comment": null, "summary": "Human-Computer Interaction (HCI) is a multi-modal, interdisciplinary field\nfocused on designing, studying, and improving the interactions between people\nand computer systems. This involves the design of systems that can recognize,\ninterpret, and respond to human emotions or stress. Developing systems to\nmonitor and react to stressful events can help prevent severe health\nimplications caused by long-term stress exposure. Currently, the publicly\navailable datasets and standardized protocols for data collection in this\ndomain are limited. Therefore, we introduce a multi-modal dataset intended for\nwearable affective computing research, specifically the development of\nautomated stress recognition systems. We systematically review the publicly\navailable datasets recorded in controlled laboratory settings. Based on a\nproposed framework for the standardization of stress experiments and data\ncollection, we collect physiological and motion signals from wearable devices\n(e.g., electrodermal activity, photoplethysmography, three-axis accelerometer).\nDuring the experimental protocol, we differentiate between the following four\naffective/activity states: neutral, physical, cognitive stress, and\nsocio-evaluative stress. These different phases are meticulously labeled,\nallowing for detailed analysis and reconstruction of each experiment. Meta-data\nsuch as body positions, locations, and rest phases are included as further\nannotations. In addition, we collect psychological self-assessments after each\nstressor to evaluate subjects' affective states. The contributions of this\npaper are twofold: 1) a novel multi-modal, publicly available dataset for\nautomated stress recognition, and 2) a benchmark for stress detection with 89\\%\nin a binary classification (baseline vs. stress) and 82\\% in a multi-class\nclassification (baseline vs. stress vs. physical exercise).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u7684\u60c5\u611f\u8ba1\u7b97\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u81ea\u52a8\u5316\u538b\u529b\u8bc6\u522b\u7cfb\u7edf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u6846\u67b6\u548c\u5206\u7c7b\u57fa\u51c6\u3002", "motivation": "\u76ee\u524d\u5173\u4e8e\u538b\u529b\u548c\u60c5\u611f\u8ba1\u7b97\u7684\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u534f\u8bae\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6765\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6807\u51c6\u5316\u6846\u67b6\u6536\u96c6\u7a7f\u6234\u8bbe\u5907\u7684\u751f\u7406\u548c\u8fd0\u52a8\u4fe1\u53f7\uff0c\u5e76\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u5bf9\u4e0d\u540c\u538b\u529b\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\u548c\u6807\u6ce8\u3002", "result": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u5728\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u5230\u4e8689%\u548c82%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8bba\u6587\u8d21\u732e\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u591a\u6a21\u6001\u538b\u529b\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u538b\u529b\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.10561", "pdf": "https://arxiv.org/pdf/2508.10561", "abs": "https://arxiv.org/abs/2508.10561", "authors": ["Andrea Gargano", "Jasin Machkour", "Mimma Nardelli", "Enzo Pasquale Scilingo", "Michael Muma"], "title": "Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling", "categories": ["cs.HC", "cs.LG", "eess.SP"], "comment": "Submitted to 2025 IEEE International Conference on Metrology for\n  eXtended Reality, Artificial Intelligence and Neural Engineering\n  (MetroXRAINE). 6 pages, 3 figures", "summary": "In Affective Computing, a key challenge lies in reliably linking subjective\nemotional experiences with objective physiological markers. This preliminary\nstudy addresses the issue of reproducibility by identifying physiological\nfeatures from cardiovascular and electrodermal signals that are associated with\ncontinuous self-reports of arousal levels. Using the Continuously Annotated\nSignal of Emotion dataset, we analyzed 164 features extracted from cardiac and\nelectrodermal signals of 30 participants exposed to short emotion-evoking\nvideos. Feature selection was performed using the Terminating-Random\nExperiments (T-Rex) method, which performs variable selection systematically\ncontrolling a user-defined target False Discovery Rate. Remarkably, among all\ncandidate features, only two electrodermal-derived features exhibited\nreproducible and statistically significant associations with arousal, achieving\na 100\\% confirmation rate. These results highlight the necessity of rigorous\nreproducibility assessments in physiological features selection, an aspect\noften overlooked in Affective Computing. Our approach is particularly promising\nfor applications in safety-critical environments requiring trustworthy and\nreliable white box models, such as mental disorder recognition and human-robot\ninteraction systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e25\u683c\u7684\u751f\u7406\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u53d1\u73b0\u4ec5\u6709\u4e24\u79cd\u76ae\u80a4\u7535\u7279\u5f81\u4e0e\u5524\u9192\u6c34\u5e73\u663e\u8457\u76f8\u5173\uff0c\u5f3a\u8c03\u4e86\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u5728\u60c5\u611f\u8ba1\u7b97\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u4e2d\u4e3b\u89c2\u60c5\u7eea\u4f53\u9a8c\u4e0e\u5ba2\u89c2\u751f\u7406\u6807\u8bb0\u7684\u53ef\u9760\u5173\u8054\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528T-Rex\u65b9\u6cd5\u5bf9164\u4e2a\u5fc3\u8840\u7ba1\u548c\u76ae\u80a4\u7535\u4fe1\u53f7\u7279\u5f81\u8fdb\u884c\u9009\u62e9\uff0c\u63a7\u5236\u5047\u53d1\u73b0\u7387\u3002", "result": "\u4ec5\u4e24\u79cd\u76ae\u80a4\u7535\u7279\u5f81\u4e0e\u5524\u9192\u6c34\u5e73\u663e\u8457\u76f8\u5173\uff0c\u786e\u8ba4\u7387\u8fbe100%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9700\u8981\u9ad8\u53ef\u9760\u6027\u7684\u5e94\u7528\uff08\u5982\u5fc3\u7406\u5065\u5eb7\u8bc6\u522b\u548c\u4eba\u673a\u4ea4\u4e92\uff09\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.10586", "pdf": "https://arxiv.org/pdf/2508.10586", "abs": "https://arxiv.org/abs/2508.10586", "authors": ["Birgit Nierula", "Mustafa Tevfik Lafci", "Anna Melnik", "Mert Akg\u00fcl", "Farelle Toumaleu Siewe", "Sebastian Bosse"], "title": "Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions", "categories": ["cs.HC", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Proxemics, the study of spatial behavior, is fundamental to social\ninteraction and increasingly relevant for virtual reality (VR) applications.\nWhile previous research has established that users respond to personal space\nviolations in VR similarly as in real-world settings, phase-specific\nphysiological responses and the modulating effects of facial expressions remain\nunderstudied. We investigated physiological and subjective responses to\npersonal space violations by virtual avatars, to understand how threatening\nfacial expressions and interaction phases (approach vs. standing) influence\nthese responses. Sixteen participants experienced a 2x2 factorial design\nmanipulating Personal Space (intrusion vs. respect) and Facial Expression\n(neutral vs. angry) while we recorded skin conductance response (SCR), heart\nrate variability (HRV), and discomfort ratings. Personal space boundaries were\nindividually calibrated using a stop-distance procedure. Results show that SCR\nresponses are significantly higher during the standing phase compared to the\napproach phase when personal space was violated, indicating that prolonged\nproximity within personal space boundaries is more physiologically arousing\nthan the approach itself. Angry facial expressions significantly reduced HRV,\nreflecting decreased parasympathetic activity, and increased discomfort\nratings, but did not amplify SCR responses. These findings demonstrate that\ndifferent physiological modalities capture distinct aspects of proxemic\nresponses: SCR primarily reflects spatial boundary violations, while HRV\nresponds to facial threat cues. Our results provide insights for developing\ncomprehensive multi-modal assessments of social behavior in virtual\nenvironments and inform the design of more realistic avatar interactions.", "AI": {"tldr": "\u7814\u7a76\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u4e2a\u4eba\u7a7a\u95f4\u4fb5\u72af\uff0c\u63a2\u8ba8\u4e86\u9762\u90e8\u8868\u60c5\u548c\u4ea4\u4e92\u9636\u6bb5\u5bf9\u751f\u7406\u53cd\u5e94\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u751f\u7406\u6307\u6807\u6355\u6349\u4e86\u4e0d\u540c\u7684\u793e\u4ea4\u884c\u4e3a\u7279\u5f81\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u4e2d\u4e2a\u4eba\u7a7a\u95f4\u4fb5\u72af\u7684\u751f\u7406\u53cd\u5e94\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u9762\u90e8\u8868\u60c5\u548c\u4ea4\u4e92\u9636\u6bb5\u7684\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "method": "\u91c7\u75282x2\u56e0\u5b50\u8bbe\u8ba1\uff0c\u8bb0\u5f55\u76ae\u80a4\u7535\u53cd\u5e94\uff08SCR\uff09\u3001\u5fc3\u7387\u53d8\u5f02\u6027\uff08HRV\uff09\u548c\u4e0d\u9002\u8bc4\u5206\uff0c\u5206\u6790\u4e2a\u4eba\u7a7a\u95f4\u4fb5\u72af\u548c\u9762\u90e8\u8868\u60c5\u7684\u5f71\u54cd\u3002", "result": "\u7ad9\u7acb\u9636\u6bb5\u7684SCR\u53cd\u5e94\u66f4\u5f3a\uff0c\u6124\u6012\u8868\u60c5\u964d\u4f4eHRV\u5e76\u589e\u52a0\u4e0d\u9002\u611f\uff0c\u4f46\u672a\u589e\u5f3aSCR\u53cd\u5e94\u3002", "conclusion": "\u4e0d\u540c\u751f\u7406\u6307\u6807\u53cd\u6620\u4e0d\u540c\u7684\u793e\u4ea4\u884c\u4e3a\u7279\u5f81\uff0c\u4e3a\u865a\u62df\u73af\u5883\u4e2d\u793e\u4ea4\u884c\u4e3a\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2508.10618", "pdf": "https://arxiv.org/pdf/2508.10618", "abs": "https://arxiv.org/abs/2508.10618", "authors": ["Ana\u00efs Halin", "Christel Devue", "Marc Van Droogenbroeck"], "title": "DEV: A Driver-Environment-Vehicle Closed-Loop Framework for Risk-Aware Adaptive Automation of Driving", "categories": ["cs.HC"], "comment": null, "summary": "The increasing integration of automation in vehicles aims to enhance both\nsafety and comfort, but it also introduces new risks, including driver\ndisengagement, reduced situation awareness, and mode confusion. In this work,\nwe propose the DEV framework, a closed-loop framework for risk-aware adaptive\ndriving automation that captures the dynamic interplay between the driver, the\nenvironment, and the vehicle. The framework promotes to continuously adjusting\nthe operational level of automation based on a risk management strategy. The\nreal-time risk assessment supports smoother transitions and effective\ncooperation between the driver and the automation system. Furthermore, we\nintroduce a nomenclature of indexes corresponding to each core component,\nnamely driver involvement, environment complexity, and vehicle engagement, and\ndiscuss how their interaction influences driving risk. The DEV framework offers\na comprehensive perspective to align multidisciplinary research efforts and\nguide the development of dynamic, risk-aware driving automation systems.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86DEV\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u98ce\u9669\u611f\u77e5\u7684\u9a7e\u9a76\u81ea\u52a8\u5316\uff0c\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574\u81ea\u52a8\u5316\u7ea7\u522b\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u9a7e\u9a76\u4f53\u9a8c\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u81ea\u52a8\u5316\u7684\u666e\u53ca\uff0c\u9a7e\u9a76\u5458\u5206\u5fc3\u3001\u60c5\u5883\u610f\u8bc6\u964d\u4f4e\u4ee5\u53ca\u6a21\u5f0f\u6df7\u6dc6\u7b49\u65b0\u98ce\u9669\u51f8\u663e\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8bc4\u4f30\u548c\u8c03\u6574\u98ce\u9669\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDEV\u6846\u67b6\uff0c\u901a\u8fc7\u9a7e\u9a76\u5458\u3001\u73af\u5883\u548c\u8f66\u8f86\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u5b9e\u65f6\u8bc4\u4f30\u98ce\u9669\u5e76\u8c03\u6574\u81ea\u52a8\u5316\u7ea7\u522b\u3002", "result": "DEV\u6846\u67b6\u80fd\u591f\u652f\u6301\u66f4\u5e73\u6ed1\u7684\u8fc7\u6e21\u548c\u66f4\u6709\u6548\u7684\u9a7e\u9a76\u5458\u4e0e\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u534f\u4f5c\uff0c\u5e76\u901a\u8fc7\u6307\u6807\u91cf\u5316\u9a71\u52a8\u98ce\u9669\u3002", "conclusion": "DEV\u6846\u67b6\u4e3a\u591a\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7efc\u5408\u89c6\u89d2\uff0c\u6307\u5bfc\u5f00\u53d1\u52a8\u6001\u3001\u98ce\u9669\u611f\u77e5\u7684\u9a7e\u9a76\u81ea\u52a8\u5316\u7cfb\u7edf\u3002"}}
{"id": "2508.10620", "pdf": "https://arxiv.org/pdf/2508.10620", "abs": "https://arxiv.org/abs/2508.10620", "authors": ["Ana\u00efs Halin", "Marc Van Droogenbroeck", "Christel Devue"], "title": "Are Electrodermal Activity-Based Indicators of Driver Cognitive Distraction Robust to Varying Traffic Conditions and Adaptive Cruise Control Use?", "categories": ["cs.HC"], "comment": null, "summary": "In this simulator study, we investigate whether and how electrodermal\nactivity (EDA) reflects driver cognitive distraction under varying traffic\nconditions and adaptive cruise control (ACC) use. Participants drove in six\nscenarios, combining two levels of cognitive distraction (presence/absence of a\nmental calculation task) and three levels of driving environment complexity\n(different traffic conditions). Throughout the experiment, they were free to\nactivate or deactivate ACC (ACC use, two levels). We analyzed three EDA-based\nindicators of cognitive distraction: SCL (mean skin conductance level), SCR\namplitude (mean amplitude of skin conductance responses), and SCR rate (rate of\nskin conductance responses). Results indicate that all three indicators were\nsignificantly influenced by cognitive distraction and ACC use, while\nenvironment complexity influenced SCL and SCR amplitude, but not SCR rate.\nThese findings suggest that EDA-based indicators reflect variations in drivers'\nmental workload due not only to cognitive distraction, but also to driving\nenvironment and automation use.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u9a7e\u9a76\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u76ae\u80a4\u7535\u6d3b\u52a8\uff08EDA\uff09\u5982\u4f55\u53cd\u6620\u9a7e\u9a76\u5458\u5728\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\u548c\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\uff08ACC\uff09\u4f7f\u7528\u4e0b\u7684\u8ba4\u77e5\u5206\u5fc3\u3002", "motivation": "\u65e8\u5728\u7406\u89e3EDA\u6307\u6807\u5982\u4f55\u8868\u5f81\u9a7e\u9a76\u5458\u8ba4\u77e5\u5206\u5fc3\u53ca\u5176\u53d7\u9a7e\u9a76\u73af\u5883\u548c\u81ea\u52a8\u5316\u4f7f\u7528\u7684\u5f71\u54cd\u3002", "method": "\u516d\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u7ed3\u5408\u8ba4\u77e5\u5206\u5fc3\uff08\u6709\u65e0\u5fc3\u7b97\u4efb\u52a1\uff09\u548c\u9a7e\u9a76\u73af\u5883\u590d\u6742\u5ea6\uff08\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\uff09\uff0c\u8bb0\u5f55\u4e09\u79cdEDA\u6307\u6807\u3002", "result": "EDA\u6307\u6807\u663e\u8457\u53d7\u8ba4\u77e5\u5206\u5fc3\u548cACC\u4f7f\u7528\u5f71\u54cd\uff0c\u73af\u5883\u590d\u6742\u5ea6\u4ec5\u5f71\u54cdSCL\u548cSCR\u5e45\u5ea6\uff0c\u4e0d\u5f71\u54cdSCR\u9891\u7387\u3002", "conclusion": "EDA\u6307\u6807\u53ef\u6709\u6548\u53cd\u6620\u9a7e\u9a76\u5458\u5fc3\u7406\u8d1f\u8377\u7684\u591a\u79cd\u53d8\u5316\u6e90\uff0c\u5305\u62ec\u5206\u5fc3\u3001\u73af\u5883\u548c\u81ea\u52a8\u5316\u4f7f\u7528\u3002"}}
{"id": "2508.10624", "pdf": "https://arxiv.org/pdf/2508.10624", "abs": "https://arxiv.org/abs/2508.10624", "authors": ["Ana\u00efs Halin", "Adrien Deli\u00e8ge", "Christel Devue", "Marc Van Droogenbroeck"], "title": "Gaze-Based Indicators of Driver Cognitive Distraction: Effects of Different Traffic Conditions and Adaptive Cruise Control Use", "categories": ["cs.HC"], "comment": null, "summary": "In this simulator study, we investigate how gaze parameters reflect driver\ncognitive distraction under varying traffic conditions and adaptive cruise\ncontrol (ACC) use. Participants completed six driving scenarios that combined\ntwo levels of cognitive distraction (with/without mental calculations) and\nthree levels of driving environment complexity. Throughout the experiment,\nparticipants were free to activate or deactivate an ACC. We analyzed two\ngaze-based indicators of driver cognitive distraction: the percent road center,\nand the gaze dispersions (horizontal and vertical). Our results show that\nvertical gaze dispersion increases with traffic complexity, while ACC use leads\nto gaze concentration toward the road center. Cognitive distraction reduces\nroad center gaze and increases vertical dispersion. Complementary analyses\nrevealed that these observations actually arise mainly between mental\ncalculations, while periods of mental calculations are characterized by a\ntemporary increase in gaze concentration.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5782\u76f4\u6ce8\u89c6\u5206\u6563\u5ea6\u968f\u4ea4\u901a\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u800c\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\uff08ACC\uff09\u4f7f\u6ce8\u89c6\u96c6\u4e2d\u4e8e\u9053\u8def\u4e2d\u5fc3\uff1b\u8ba4\u77e5\u5206\u5fc3\u51cf\u5c11\u9053\u8def\u4e2d\u5fc3\u6ce8\u89c6\u5e76\u589e\u52a0\u5782\u76f4\u5206\u6563\u5ea6\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\u548cACC\u4f7f\u7528\u4e0b\uff0c\u6ce8\u89c6\u53c2\u6570\u5982\u4f55\u53cd\u6620\u9a7e\u9a76\u5458\u8ba4\u77e5\u5206\u5fc3\u3002", "method": "\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5668\u5b9e\u9a8c\uff0c\u7ed3\u5408\u8ba4\u77e5\u5206\u5fc3\uff08\u6709\u65e0\u5fc3\u7b97\uff09\u548c\u9a7e\u9a76\u73af\u5883\u590d\u6742\u5ea6\uff08\u4e09\u4e2a\u6c34\u5e73\uff09\uff0c\u8bb0\u5f55\u4e24\u540d\u6ce8\u89c6\u6307\u6807\uff08\u9053\u8def\u4e2d\u5fc3\u6ce8\u89c6\u767e\u5206\u6bd4\u548c\u6ce8\u89c6\u5206\u6563\u5ea6\uff09\u3002", "result": "\u4ea4\u901a\u590d\u6742\u5ea6\u589e\u52a0\u5782\u76f4\u6ce8\u89c6\u5206\u6563\u5ea6\uff0cACC\u4f7f\u6ce8\u89c6\u96c6\u4e2d\uff1b\u8ba4\u77e5\u5206\u5fc3\u51cf\u5c11\u9053\u8def\u4e2d\u5fc3\u6ce8\u89c6\uff0c\u589e\u52a0\u5782\u76f4\u5206\u6563\u5ea6\uff0c\u5fc3\u7b97\u671f\u95f4\u6ce8\u89c6\u96c6\u4e2d\u6682\u65f6\u589e\u52a0\u3002", "conclusion": "\u6ce8\u89c6\u53c2\u6570\u80fd\u6709\u6548\u533a\u5206\u8ba4\u77e5\u5206\u5fc3\uff0c\u5fc3\u7b97\u671f\u95f4\u6ce8\u89c6\u884c\u4e3a\u5b58\u5728\u52a8\u6001\u53d8\u5316\u3002"}}
{"id": "2508.10700", "pdf": "https://arxiv.org/pdf/2508.10700", "abs": "https://arxiv.org/abs/2508.10700", "authors": ["Ambre Assor", "Mickael Sereno", "Jean-Daniel Fekete"], "title": "Visualization of Electronic Health Record Sequences at Scale", "categories": ["cs.HC"], "comment": null, "summary": "We present ParcoursVis, a Progressive Visual Analytics tool designed to\nexplore electronic health record sequences of patients at scale. Existing tools\nprocess and aggregate the whole dataset upfront before showing the\nvisualization, taking a time proportional to the data size. Therefore, to\nremain interactive, existing tools are limited to data sizes that can be\nprocessed in under a few seconds to meet the latency constraints of human\nattention. To overcome this limitation and scale to larger sizes, ParcoursVis\nrelies on a progressive algorithm that quickly shows an approximate initial\nresult of the aggregation, visualized as an Icicle tree, and improves it\niteratively, updating the visualization until the whole computation is done.\nWith its architecture, ParcoursVis remains interactive while visualizing the\nsequences of tens of millions of patients, each described with thousands of\nevents; three to five orders of magnitude more than similar systems. Managing\nlarge datasets allows for exploring rare medical conditions or unexpected\npatient pathways, contributing to improving treatments. We describe the\nalgorithms we use and our evaluation concerning their scalability, convergence,\nand stability. We also report on a set of guidelines to support visualization\ndesigners in developing scalable progressive systems. ParcoursVis already\nallows practitioners to perform analyses on two large real medical datasets.\nOur prototype is open-source.", "AI": {"tldr": "ParcoursVis\u662f\u4e00\u79cd\u6e10\u8fdb\u5f0f\u89c6\u89c9\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u63a2\u7d22\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5e8f\u5217\uff0c\u901a\u8fc7\u9010\u6b65\u8ba1\u7b97\u548c\u66f4\u65b0\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u5206\u6790\u6570\u5343\u4e07\u60a3\u8005\u7684\u8bb0\u5f55\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u56e0\u8ba1\u7b97\u5ef6\u8fdf\u800c\u65e0\u6cd5\u4fdd\u6301\u4ea4\u4e92\u6027\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u7f55\u89c1\u533b\u7597\u6761\u4ef6\u6216\u610f\u5916\u60a3\u8005\u8def\u5f84\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u7b97\u6cd5\uff0c\u5feb\u901f\u751f\u6210\u8fd1\u4f3c\u7684\u521d\u59cb\u805a\u5408\u7ed3\u679c\uff08\u51b0\u67f1\u6811\u53ef\u89c6\u5316\uff09\uff0c\u5e76\u8fed\u4ee3\u4f18\u5316\uff0c\u76f4\u81f3\u5b8c\u6210\u5168\u90e8\u8ba1\u7b97\u3002", "result": "ParcoursVis\u80fd\u591f\u5904\u7406\u6570\u5343\u4e07\u60a3\u8005\u3001\u6bcf\u4e2a\u60a3\u8005\u6570\u5343\u4e8b\u4ef6\u7684\u8bb0\u5f55\uff0c\u6bd4\u7c7b\u4f3c\u7cfb\u7edf\u591a\u5904\u74063\u52305\u4e2a\u6570\u91cf\u7ea7\u7684\u6570\u636e\u3002", "conclusion": "\u5de5\u5177\u7684\u6210\u529f\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u652f\u6301\u533b\u7597\u5b9e\u8df5\u4e2d\u7684\u6539\u8fdb\uff0c\u540c\u65f6\u5f00\u6e90\u539f\u578b\u4fc3\u8fdb\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.10757", "pdf": "https://arxiv.org/pdf/2508.10757", "abs": "https://arxiv.org/abs/2508.10757", "authors": ["Zhanming Chen", "Juan F. Maestre", "May Hang", "Alisha Ghaju", "Ji Youn Shin"], "title": "\"I Want My Chart to Be Just for Me\": Community-Engaged Design to Support Outpatient Healthcare for Resettled Communities", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Individuals resettled in a new environment often face challenges in accessing\nadequate healthcare services, particularly within the complex processes of\noutpatient clinic care. Cultural differences, language barriers, and low\nsocioeconomic status contribute to these difficulties. While previous studies\nhave identified barriers and proposed technology-mediated solutions for\nresettled populations, many focus on addressing deficits rather than building\non the strengths these communities already possess, which limits the\nsustainability and relevance of these solutions in everyday life. We conducted\ntwo community-based participatory design workshops with 30 Hmong community\nmembers in a large metropolitan area in the US. Through this process, we\nidentified four types of assets the community has gradually developed,\nincluding intergenerational support for health management and\nstorytelling-based communication practices that facilitate relatable and\nculturally grounded interactions. We show how participatory design workshops\ncan foster asset-based approaches, and discuss design implications for\ntechnologies that leverage patients' existing strengths to support their health\nmanagement during outpatient visits.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u91cd\u65b0\u5b9a\u5c45\u4eba\u7fa4\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u51fa\u57fa\u4e8e\u793e\u533a\u8d44\u4ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\u8bc6\u522b\u793e\u533a\u4f18\u52bf\u3002", "motivation": "\u91cd\u65b0\u5b9a\u5c45\u4eba\u7fa4\u5728\u83b7\u53d6\u533b\u7597\u670d\u52a1\u65f6\u9762\u4e34\u6587\u5316\u3001\u8bed\u8a00\u548c\u793e\u4f1a\u7ecf\u6d4e\u969c\u788d\uff0c\u73b0\u6709\u6280\u672f\u65b9\u6848\u5e38\u5ffd\u89c6\u793e\u533a\u81ea\u8eab\u4f18\u52bf\u3002", "method": "\u7814\u7a76\u8005\u4e0e30\u540d\u82d7\u65cf\u793e\u533a\u6210\u5458\u8fdb\u884c\u4e86\u4e24\u6b21\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\u3002", "result": "\u8bc6\u522b\u51fa\u793e\u533a\u62e5\u6709\u7684\u56db\u79cd\u8d44\u4ea7\uff0c\u5982\u4ee3\u9645\u5065\u5eb7\u652f\u6301\u548c\u57fa\u4e8e\u6545\u4e8b\u7684\u6c9f\u901a\u65b9\u5f0f\u3002", "conclusion": "\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u80fd\u4fc3\u8fdb\u57fa\u4e8e\u8d44\u4ea7\u7684\u65b9\u6cd5\uff0c\u4e3a\u6280\u672f\u8bbe\u8ba1\u63d0\u4f9b\u5229\u7528\u60a3\u8005\u4f18\u52bf\u652f\u6301\u5065\u5eb7\u7ba1\u7406\u7684\u542f\u793a\u3002"}}
{"id": "2508.10004", "pdf": "https://arxiv.org/pdf/2508.10004", "abs": "https://arxiv.org/abs/2508.10004", "authors": ["Andr\u00e9s Carvallo", "Denis Parra", "Peter Brusilovsky", "Hernan Valdivieso", "Gabriel Rada", "Ivania Donoso", "Vladimir Araujo"], "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": null, "summary": "The attention mechanism is a core component of the Transformer architecture.\nBeyond improving performance, attention has been proposed as a mechanism for\nexplainability via attention weights, which are associated with input features\n(e.g., tokens in a document). In this context, larger attention weights may\nimply more relevant features for the model's prediction. In evidence-based\nmedicine, such explanations could support physicians' understanding and\ninteraction with AI systems used to categorize biomedical literature. However,\nthere is still no consensus on whether attention weights provide helpful\nexplanations. Moreover, little research has explored how visualizing attention\naffects its usefulness as an explanation aid. To bridge this gap, we conducted\na user study to evaluate whether attention-based explanations support users in\nbiomedical document classification and whether there is a preferred way to\nvisualize them. The study involved medical experts from various disciplines who\nclassified articles based on study design (e.g., systematic reviews, broad\nsynthesis, randomized and non-randomized trials). Our findings show that the\nTransformer model (XLNet) classified documents accurately; however, the\nattention weights were not perceived as particularly helpful for explaining the\npredictions. However, this perception varied significantly depending on how\nattention was visualized. Contrary to Munzner's principle of visual\neffectiveness, which favors precise encodings like bar length, users preferred\nmore intuitive formats, such as text brightness or background color. While our\nresults do not confirm the overall utility of attention weights for\nexplanation, they suggest that their perceived helpfulness is influenced by how\nthey are visually presented.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6ce8\u610f\u529b\u6743\u91cd\u5728\u751f\u7269\u533b\u5b66\u6587\u6863\u5206\u7c7b\u4e2d\u867d\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u4f5c\u4e3a\u89e3\u91ca\u5de5\u5177\u7684\u5b9e\u7528\u6027\u6709\u9650\uff0c\u4e14\u53ef\u89c6\u5316\u65b9\u5f0f\u5bf9\u7528\u6237\u611f\u77e5\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u6ce8\u610f\u529b\u673a\u5236\u662f\u5426\u80fd\u4e3a\u751f\u7269\u533b\u5b66\u6587\u732e\u5206\u7c7b\u63d0\u4f9b\u6709\u6548\u7684\u89e3\u91ca\u652f\u6301\uff0c\u4ee5\u53ca\u4e0d\u540c\u53ef\u89c6\u5316\u65b9\u5f0f\u5bf9\u7528\u6237\u7406\u89e3\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30\u6ce8\u610f\u529b\u6743\u91cd\u5728\u6587\u6863\u5206\u7c7b\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u53ef\u89c6\u5316\u683c\u5f0f\u7684\u6548\u679c\u3002", "result": "XLNet\u6a21\u578b\u5206\u7c7b\u51c6\u786e\uff0c\u4f46\u6ce8\u610f\u529b\u6743\u91cd\u89e3\u91ca\u6548\u679c\u4e0d\u4f73\uff1b\u7528\u6237\u504f\u597d\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u65b9\u5f0f\uff08\u5982\u6587\u5b57\u4eae\u5ea6\u6216\u80cc\u666f\u8272\uff09\u3002", "conclusion": "\u6ce8\u610f\u529b\u6743\u91cd\u7684\u89e3\u91ca\u6548\u679c\u53d7\u53ef\u89c6\u5316\u65b9\u5f0f\u5f71\u54cd\uff0c\u672a\u6765\u9700\u4f18\u5316\u5c55\u793a\u5f62\u5f0f\u4ee5\u63d0\u9ad8\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.10028", "pdf": "https://arxiv.org/pdf/2508.10028", "abs": "https://arxiv.org/abs/2508.10028", "authors": ["Xiao Fu", "Hossein A. Rahmani", "Bin Wu", "Jerome Ramos", "Emine Yilmaz", "Aldo Lipani"], "title": "PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "7 pages", "summary": "Personalised text generation is essential for user-centric information\nsystems, yet most evaluation methods overlook the individuality of users. We\nintroduce \\textbf{PREF}, a \\textbf{P}ersonalised \\textbf{R}eference-free\n\\textbf{E}valuation \\textbf{F}ramework that jointly measures general output\nquality and user-specific alignment without requiring gold personalised\nreferences. PREF operates in a three-step pipeline: (1) a coverage stage uses a\nlarge language model (LLM) to generate a comprehensive, query-specific\nguideline covering universal criteria such as factuality, coherence, and\ncompleteness; (2) a preference stage re-ranks and selectively augments these\nfactors using the target user's profile, stated or inferred preferences, and\ncontext, producing a personalised evaluation rubric; and (3) a scoring stage\napplies an LLM judge to rate candidate answers against this rubric, ensuring\nbaseline adequacy while capturing subjective priorities. This separation of\ncoverage from preference improves robustness, transparency, and reusability,\nand allows smaller models to approximate the personalised quality of larger\nones. Experiments on the PrefEval benchmark, including implicit\npreference-following tasks, show that PREF achieves higher accuracy, better\ncalibration, and closer alignment with human judgments than strong baselines.\nBy enabling scalable, interpretable, and user-aligned evaluation, PREF lays the\ngroundwork for more reliable assessment and development of personalised\nlanguage generation systems.", "AI": {"tldr": "PREF\u662f\u4e00\u4e2a\u65e0\u9700\u4e2a\u6027\u5316\u53c2\u8003\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u6b65\u6d41\u7a0b\uff08\u8986\u76d6\u3001\u504f\u597d\u3001\u8bc4\u5206\uff09\u8054\u5408\u8bc4\u4f30\u6587\u672c\u751f\u6210\u8d28\u91cf\u548c\u7528\u6237\u5bf9\u9f50\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0cPREF\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u6d41\u7a0b\uff1a\u8986\u76d6\u9636\u6bb5\u751f\u6210\u901a\u7528\u51c6\u5219\uff0c\u504f\u597d\u9636\u6bb5\u6839\u636e\u7528\u6237\u8d44\u6599\u8c03\u6574\uff0c\u8bc4\u5206\u9636\u6bb5\u7528LLM\u6253\u5206\u3002", "result": "\u5728PrefEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPREF\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u51c6\u786e\u6027\u66f4\u9ad8\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u3002", "conclusion": "PREF\u4e3a\u4e2a\u6027\u5316\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u7684\u53ef\u9760\u8bc4\u4f30\u548c\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10071", "pdf": "https://arxiv.org/pdf/2508.10071", "abs": "https://arxiv.org/abs/2508.10071", "authors": ["Jay L. Cunningham", "Kevin Zhongyang Shao", "Rock Yuren Pang", "Nathaniel Mengist"], "title": "Advancing Data Equity: Practitioner Responsibility and Accountability in NLP Data Practices", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 6 Pages (References and Appendices). The archival version\n  has been accepted to AAAI (AIES 2025) without the extended Appendices. This\n  extended version includes Appendices", "summary": "While research has focused on surfacing and auditing algorithmic bias to\nensure equitable AI development, less is known about how NLP practitioners -\nthose directly involved in dataset development, annotation, and deployment -\nperceive and navigate issues of NLP data equity. This study is among the first\nto center practitioners' perspectives, linking their experiences to a\nmulti-scalar AI governance framework and advancing participatory\nrecommendations that bridge technical, policy, and community domains. Drawing\non a 2024 questionnaire and focus group, we examine how U.S.-based NLP data\npractitioners conceptualize fairness, contend with organizational and systemic\nconstraints, and engage emerging governance efforts such as the U.S. AI Bill of\nRights. Findings reveal persistent tensions between commercial objectives and\nequity commitments, alongside calls for more participatory and accountable data\nworkflows. We critically engage debates on data diversity and diversity\nwashing, arguing that improving NLP equity requires structural governance\nreforms that support practitioner agency and community consent.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86NLP\u4ece\u4e1a\u8005\u5bf9\u6570\u636e\u516c\u5e73\u6027\u7684\u8ba4\u77e5\u53ca\u5176\u9762\u4e34\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u6280\u672f\u4e0e\u653f\u7b56\u7684\u6cbb\u7406\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8NLP\u4ece\u4e1a\u8005\u5728\u6570\u636e\u516c\u5e73\u6027\u95ee\u9898\u4e0a\u7684\u5b9e\u9645\u4f53\u9a8c\u548c\u6311\u6218\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e2024\u5e74\u7684\u95ee\u5377\u548c\u7126\u70b9\u5c0f\u7ec4\u7814\u7a76\uff0c\u5206\u6790\u7f8e\u56fdNLP\u4ece\u4e1a\u8005\u7684\u89c2\u70b9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5546\u4e1a\u76ee\u6807\u4e0e\u516c\u5e73\u627f\u8bfa\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u547c\u5401\u66f4\u5177\u53c2\u4e0e\u6027\u548c\u95ee\u8d23\u6027\u7684\u6570\u636e\u6d41\u7a0b\u3002", "conclusion": "\u6539\u8fdbNLP\u516c\u5e73\u6027\u9700\u8981\u7ed3\u6784\u6027\u6cbb\u7406\u6539\u9769\uff0c\u652f\u6301\u4ece\u4e1a\u8005\u80fd\u52a8\u6027\u548c\u793e\u533a\u540c\u610f\u3002"}}
{"id": "2508.10268", "pdf": "https://arxiv.org/pdf/2508.10268", "abs": "https://arxiv.org/abs/2508.10268", "authors": ["Yujie Zhao", "Jiabei Zeng", "Shiguang Shan"], "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted for British Machine Vision Conference (BMVC) 2025", "summary": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6821\u51c6\u7b56\u7565\uff0c\u901a\u8fc7\u7528\u6237\u5728\u79fb\u52a8\u8bbe\u5907\u65f6\u6ce8\u89c6\u6821\u51c6\u70b9\uff0c\u5f15\u5165\u5934\u90e8\u59ff\u6001\u53d8\u5316\uff0c\u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u5916\u89c2\u7684\u51dd\u89c6\u70b9\uff08PoG\uff09\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u51dd\u89c6\u70b9\u4f30\u8ba1\u65b9\u6cd5\u56e0\u4e2a\u4f53\u5dee\u5f02\u548c\u5934\u90e8\u59ff\u6001\u53d8\u5316\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u901a\u8fc7\u6821\u51c6\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efaMobilePoG\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5206\u6790\u6821\u51c6\u70b9\u548c\u5934\u90e8\u59ff\u6001\u591a\u6837\u6027\u5bf9\u4f30\u8ba1\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u52a8\u6001\u6821\u51c6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6821\u51c6\u65f6\u5f15\u5165\u5934\u90e8\u59ff\u6001\u53d8\u5316\u80fd\u663e\u8457\u63d0\u5347\u4f30\u8ba1\u5668\u5bf9\u59ff\u6001\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u52a8\u6001\u6821\u51c6\u7b56\u7565\u5728\u4fdd\u8bc1\u7528\u6237\u53cb\u597d\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51dd\u89c6\u70b9\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10332", "pdf": "https://arxiv.org/pdf/2508.10332", "abs": "https://arxiv.org/abs/2508.10332", "authors": ["Abhijit Sinha", "Harishankar Kumar", "Mohit Joshi", "Hemant Kumar Kathania", "Shrikanth Narayanan", "Sudarsana Reddy Kadiri"], "title": "Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children's Speech", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG", "cs.SD"], "comment": "Accepted at Workshop on Child Computer Interaction (WOCCI 2025)", "summary": "Children's speech presents challenges for age and gender classification due\nto high variability in pitch, articulation, and developmental traits. While\nself-supervised learning (SSL) models perform well on adult speech tasks, their\nability to encode speaker traits in children remains underexplored. This paper\npresents a detailed layer-wise analysis of four Wav2Vec2 variants using the\nPFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture\nspeaker-specific cues more effectively than deeper layers, which increasingly\nfocus on linguistic information. Applying PCA further improves classification,\nreducing redundancy and highlighting the most informative components. The\nWav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU\nKids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These\nresults reveal how speaker traits are structured across SSL model depth and\nsupport more targeted, adaptive strategies for child-aware speech interfaces.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86SSL\u6a21\u578b\uff08\u7279\u522b\u662fWav2Vec2\u53d8\u4f53\uff09\u5728\u513f\u7ae5\u8bed\u97f3\u5e74\u9f84\u548c\u6027\u522b\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65e9\u671f\u5c42\u66f4\u9002\u5408\u6355\u6349\u8bf4\u8bdd\u8005\u7279\u5f81\uff0c\u5e76\u901a\u8fc7PCA\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u513f\u7ae5\u8bed\u97f3\u5728\u97f3\u9ad8\u3001\u53d1\u97f3\u7b49\u65b9\u9762\u7684\u9ad8\u53d8\u5f02\u6027\uff0c\u73b0\u6709SSL\u6a21\u578b\u5bf9\u5176\u5e74\u9f84\u548c\u6027\u522b\u5206\u7c7b\u80fd\u529b\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u5206\u6790\u5c42\u95f4\u7279\u5f81\u3002", "method": "\u4f7f\u7528PFSTAR\u548cCMU Kids\u6570\u636e\u96c6\uff0c\u5bf9\u56db\u79cdWav2Vec2\u53d8\u4f53\u8fdb\u884c\u5206\u5c42\u5206\u6790\uff0c\u5e76\u7ed3\u5408PCA\u964d\u4f4e\u5197\u4f59\u3002", "result": "Wav2Vec2-large-lv60\u5728CMU Kids\u4e0a\u5206\u522b\u8fbe\u523097.14%\uff08\u5e74\u9f84\uff09\u548c98.20%\uff08\u6027\u522b\uff09\uff0c\u5728PFSTAR\u4e0a\u8868\u73b0\u4e5f\u663e\u8457\u3002PCA\u6709\u6548\u63d0\u5347\u4e86\u5206\u7c7b\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86SSL\u6a21\u578b\u4e2d\u8bf4\u8bdd\u8005\u7279\u5f81\u7684\u5c42\u6b21\u5206\u5e03\uff0c\u4e3a\u513f\u7ae5\u8bed\u97f3\u63a5\u53e3\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2508.10474", "pdf": "https://arxiv.org/pdf/2508.10474", "abs": "https://arxiv.org/abs/2508.10474", "authors": ["Lisa Haxel", "Jaivardhan Kapoor", "Ulf Ziemann", "Jakob H. Macke"], "title": "EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation", "categories": ["cs.LG", "cs.HC", "q-bio.NC"], "comment": "Preprint", "summary": "Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural\nsignals drift over time and vary across users, requiring frequent recalibration\nthat limits practical deployment. We introduce EDAPT, a task- and\nmodel-agnostic framework that eliminates calibration through continual model\nadaptation. EDAPT first trains a baseline decoder using data from multiple\nusers, then continually personalizes this model via supervised finetuning as\nthe neural patterns evolve during use. We tested EDAPT across nine datasets\ncovering three BCI tasks, and found that it consistently improved accuracy over\nconventional, static methods. These improvements primarily stem from combining\npopulation-level pretraining and online continual finetuning, with unsupervised\ndomain adaptation providing further gains on some datasets. EDAPT runs\nefficiently, updating models within 200 milliseconds on consumer-grade\nhardware. Finally, decoding accuracy scales with total data budget rather than\nits allocation between subjects and trials. EDAPT provides a practical pathway\ntoward calibration-free BCIs, reducing a major barrier to BCI deployment.", "AI": {"tldr": "EDAPT\u901a\u8fc7\u6301\u7eed\u6a21\u578b\u9002\u5e94\u6d88\u9664\u4e86BCI\u6821\u51c6\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u90e8\u7f72\u969c\u788d\u3002", "motivation": "\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u56e0\u795e\u7ecf\u4fe1\u53f7\u6f02\u79fb\u548c\u7528\u6237\u5dee\u5f02\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff0c\u9700\u8981\u9891\u7e41\u6821\u51c6\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u3002", "method": "EDAPT\u5148\u8bad\u7ec3\u591a\u7528\u6237\u6570\u636e\u7684\u57fa\u7ebf\u89e3\u7801\u5668\uff0c\u540e\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6301\u7eed\u4e2a\u6027\u5316\u6a21\u578b\u3002", "result": "EDAPT\u5728\u4e09\u4e2aBCI\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9759\u6001\u65b9\u6cd5\uff0c\u8fd0\u884c\u9ad8\u6548\uff0c200\u6beb\u79d2\u5185\u66f4\u65b0\u6a21\u578b\u3002", "conclusion": "EDAPT\u4e3a\u65e0\u6821\u51c6BCI\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u51cf\u5c11\u90e8\u7f72\u969c\u788d\u3002"}}
{"id": "2508.10603", "pdf": "https://arxiv.org/pdf/2508.10603", "abs": "https://arxiv.org/abs/2508.10603", "authors": ["Agnes Axelsson", "Merle Reimann", "Ronald Cumbal", "Hannah Pelikan", "Divesh Lala"], "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages", "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6c11\u65cf\u5fd7\u5c0f\u6545\u4e8b\uff08vignettes\uff09\u6765\u63ed\u793a\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u4e2d\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u5c24\u5176\u662f\u7814\u7a76\u4e2d\u5e38\u88ab\u5ffd\u7565\u7684\u5931\u8d25\u60c5\u51b5\uff0c\u5e76\u63d0\u5021\u5c06\u5176\u4f5c\u4e3a\u73b0\u6709\u4ea4\u4e92\u8bc4\u4f30\u65b9\u6cd5\u7684\u8865\u5145\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u8d28\u91cf\uff0c\u4f46\u4e0e\u4eba\u4eba\u4ea4\u4e92\u76f8\u6bd4\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u7531\u4e8e\u5931\u8d25\u7684\u6027\u8d28\u548c\u4e25\u91cd\u6027\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u666e\u904d\u5316\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u6c11\u65cf\u5fd7\u5c0f\u6545\u4e8b\uff08vignettes\uff09\u6765\u8bb0\u5f55\u5931\u8d25\u6848\u4f8b\uff0c\u5e76\u4ee5\u4e2a\u4eba\u7ecf\u5386\u4e3a\u57fa\u7840\u7f16\u5199\u5c0f\u6545\u4e8b\uff0c\u5f3a\u8c03\u5176\u591a\u5b66\u79d1\u89c6\u89d2\u7684\u4f18\u52bf\u3002", "result": "\u5c0f\u6545\u4e8b\u80fd\u6e05\u6670\u5c55\u793a\u5931\u8d25\u3001\u4fc3\u8fdb\u673a\u5668\u4eba\u80fd\u529b\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u8bb0\u5f55\u7814\u7a76\u4e2d\u672a\u63d0\u53ca\u7684\u610f\u5916\u884c\u4e3a\u3002", "conclusion": "\u5efa\u8bae\u5728HRI\u7814\u7a76\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u5c0f\u6545\u4e8b\uff0c\u4ee5\u8865\u5145\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
