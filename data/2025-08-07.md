<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 23]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.IR](#cs.IR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [math.NA](#math.NA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 论文介绍了17条可操作的共情指南，帮助软件工程团队提升沟通与决策能力，并提出可视化优先级框架以促进实施。


<details>
  <summary>Details</summary>
Motivation: 共情在软件工程中被忽视，但能改善团队协作与沟通，因此研究如何将共情融入实践。

Method: 基于先前研究，提出17条共情指南，并通过实际案例探讨实施策略与挑战。

Result: 开发了可视化优先级框架，将指南按重要性、实施难易和采纳意愿分类，提供灵活实践建议。

Conclusion: 共情指南为软件工程团队提供了从理论到可持续行动的具体路径，促进共情在日常工作中的整合。

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [2] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: 研究软件安全性现状分析及改进建议


<details>
  <summary>Details</summary>
Motivation: 研究软件的安全性对科学结果的完整性和可重复性至关重要，但目前相关研究较少。

Method: 使用OpenSSF Scorecard分析3,248个高质量、大多经过同行评审的研究软件仓库。

Result: 研究发现安全态势普遍较弱，平均得分仅为3.5/10，重要安全措施如签名发布和分支保护很少实施。

Conclusion: 提出了可行的低工作量建议，帮助研究团队提升软件安全性，减少对科学完整性的潜在威胁。

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [3] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究探讨了能否通过应用属性预测用户对解释的需求，发现关联性较弱，且预测模型效果有限，建议结合用户反馈以更好地设计软件系统。


<details>
  <summary>Details</summary>
Motivation: 在数字化时代，软件系统需帮助用户理解交互及行为原因，研究旨在通过应用属性预测用户解释需求，以支持开发和需求挖掘。

Method: 分析了4,495条包含元数据的应用评论，进行相关性分析和线性回归建模，并在495条手动标记数据上验证。

Result: 应用属性与解释需求间多为弱关联，仅特定特征（如版本、评论数和评分）有中度关联，预测模型效果不佳。

Conclusion: 解释需求高度依赖上下文，无法仅从元数据中精确推断，建议开发者结合用户反馈设计更易理解的系统。

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [4] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: 论文研究了GitHub Copilot如何适应不同用户的技术水平，并提出了一个以人为中心的需求评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架往往忽略人类因素，而AI编程助手需更好地满足用户需求。

Method: 通过分析Copilot的聊天交互，测量其适应性和协作能力，建立评估框架。

Result: 提出了明确的指标评估Copilot的人性化表现，并讨论了测试结果。

Conclusion: 未来自动化编程需更关注人类需求，研究为相关评估提供了基础。

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [5] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: 该研究对四种大型语言模型（ChatGPT、Copilot、Gemini和DeepSeek）在150道LeetCode题目上的表现进行了系统比较，评估其执行时间、内存使用和算法复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件开发中的普及，需要对其性能进行系统比较，以优化实际应用中的使用。

Method: 研究对四种模型在Java和Python中生成解决方案的表现进行基准测试，涵盖易、中、难三种难度的题目。

Result: ChatGPT在执行时间和内存使用上表现一致高效，Copilot和DeepSeek随任务复杂度增加表现不稳定，Gemini在简单任务中有效但难度增加时需要更多尝试。

Conclusion: 研究结果提供了每种模型的优缺点，为开发者选择适合特定编程任务的模型提供了指导。

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [6] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 论文研究了代码语言模型压缩技术（如剪枝、量化和知识蒸馏）在对抗攻击下的鲁棒性，发现压缩模型性能类似未压缩模型，但对抗攻击下鲁棒性显著降低。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解代码语言模型压缩技术在对抗场景中的鲁棒性，以保障其在真实世界应用中的安全部署。

Method: 研究对三种广泛使用的代码语言模型在三种软件分析任务中进行了全面评估，使用了六种评估指标和四种经典对抗攻击。

Result: 结果表明，压缩模型在性能上与未压缩模型相当，但在对抗攻击下鲁棒性显著降低。

Conclusion: 研究揭示了模型压缩与对抗鲁棒性之间的权衡，并指出在安全关键应用中需要谨慎考虑压缩模型的使用。

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [7] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）在软件工程任务中的应用，重点关注项目级别的代码生成生产力和人机交互（HLI）特征，揭示了关键影响因素和改进指南。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂软件工程任务中的应用，弥补现有研究在项目级别和HLI特征上的不足。

Method: 设计实验，分析36名参与者在项目级任务中使用GPT助手的交互行为和代码生成效果。

Result: 识别出15个HLI特征中的3个对生产力有显著影响，提出5条改进指南和29种错误分类及缓解方案。

Conclusion: 研究为提升LLM在代码生成中的生产力提供了实用指南和错误处理框架。

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [8] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust是一个自动化框架，用于将整个C项目转换为等效的Rust项目，通过骨架引导的翻译策略解决规则和LLM方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了提高C到Rust转换的安全性和语义准确性，解决现有方法在小规模程序上的局限性。

Method: 采用三阶段进化策略：模块分解和骨架生成、逐步函数翻译、错误修复。

Result: 在语法和语义准确性上分别提升17.24%和14.32%，代码安全性提高96.79%。

Conclusion: EvoC2Rust在项目级C到Rust转换中表现出色，适用于复杂代码库。

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [9] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: Vanilla-Converter是一种命令行工具，可自动化将Camunda 7的BPMN模型迁移至Camunda 8，支持广泛的BPMN元素并生成转换日志。


<details>
  <summary>Details</summary>
Motivation: 由于Camunda 7和8平台间的根本差异，手动迁移复杂且耗时。为解决这一问题，开发了Vanilla-Converter工具。

Method: 工具自动化转换BPMN模型，支持多种元素，并生成详细日志记录自动更改和剩余手动任务。

Result: 通过三个实际工业案例验证，工具成功将Camunda 7模型转换为有效且可执行的Camunda 8模型。

Conclusion: Vanilla-Converter显著简化了Camunda 7到8的迁移过程，证明了其在实际应用中的有效性。

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [10] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: 论文提出了一种基于开发者编码习惯的软件缺陷预测框架，通过非代码指标提升预测性能和实用性，优于现有代码和历史指标。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用人类因素理论解决软件缺陷的根源问题，提出开发者编码习惯作为预测指标，填补非软件指标研究的空白。

Method: 方法包括设计基于人类错误的预测指标框架，并与现有最佳代码和历史指标进行比较，分析各指标的重要性。

Result: 结果显示新指标在平均预测性能和重要性上优于现有指标，并提升了模型的可解释性和实用性。

Conclusion: 结论是通过开发者编码习惯的框架显著推进了缺陷预测领域，并提供了实际操作的依据。

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [11] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: 本文对六种自动化代码分析工具（三种静态分析工具和三种大语言模型）进行了定量与定性评估，发现大语言模型在召回率上表现更优，但误报率和定位精度较低，推荐结合使用两种方法以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 评估现代自动化测试工具和大型语言模型在不同场景下的表现，为开发者提供更高效的代码安全保障方案。

Method: 使用包含63个漏洞的10个C#项目作为测试集，对比了六种工具在检测准确性、延迟和开发者审核工作量上的表现。

Result: 大语言模型的平均F1分数（0.797、0.753、0.750）高于静态工具（0.260、0.386、0.546），但其误报率和定位精度较差。

Conclusion: 建议结合大语言模型的广域扫描和静态工具的高精度验证，以优化代码安全审计流程。

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [12] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 研究发现共情在软件工程中的表现、动机及影响因素，并通过访谈和调查提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 探讨共情在软件工程实践中的具体表现、动机及影响因素。

Method: 通过22次访谈和116名软件从业者的大规模调查进行研究。

Result: 揭示了共情在软件工程中的表达方式、驱动因素及其在不同活动中的作用。

Conclusion: 提供了将共情有效整合到软件工程流程中的实用建议。

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [If-T: A Benchmark for Type Narrowing](https://arxiv.org/abs/2508.03830)
*Hanwen Guo,Ben Greenman*

Main category: cs.PL

TL;DR: 该论文提出了If-T，一种用于评估类型细化系统的设计基准，帮助权衡精度与性能。


<details>
  <summary>Details</summary>
Motivation: 动态类型语言中，类型细化是验证代码的关键，但现有系统缺乏统一标准，复杂性收益不明确。

Method: 通过文献、语言文档和实验，设计If-T基准，包含正负测试用例，评估五种类型检查器。

Result: If-T揭示了不同系统的差异，如变量逻辑关系跟踪和用户定义谓词支持。

Conclusion: If-T为未来类型系统设计提供了明确的权衡工具，促进精度与性能的平衡。

Abstract: **Context:** The design of static type systems that can validate
dynamically-typed programs (**gradually**) is an ongoing challenge. A key
difficulty is that dynamic code rarely follows datatype-driven design. Programs
instead use runtime tests to narrow down the proper usage of incoming data.
Type systems for dynamic languages thus need a **type narrowing** mechanism
that refines the type environment along individual control paths based on
dominating tests, a form of flow-sensitive typing. In order to express
refinements, the type system must have some notion of sets and subsets. Since
set-theoretic types are computationally and ergonomically complex, the need for
type narrowing raises design questions about how to balance precision and
performance. **Inquiry:** To date, the design of type narrowing systems has
been driven by intuition, past experience, and examples from users in various
language communities. There is no standard that captures desirable and
undesirable behaviors. Prior formalizations of narrowing are also significantly
more complex than a standard type system, and it is unclear how the extra
complexity pays off in terms of concrete examples. This paper addresses the
problems through If-T, a language-agnostic **design benchmark** for type
narrowing that characterizes the abilities of implementations using simple
programs that draw attention to fundamental questions. Unlike a traditional
performance-focused benchmark, If-T measures a narrowing system's ability to
validate correct code and reject incorrect code. Unlike a test suite, systems
are not required to fully conform to If-T. Deviations are acceptable provided
they are justified by well-reasoned design considerations, such as compile-time
performance. **Approach:** If-T is guided by the literature on type narrowing,
the documentation of gradual languages such as TypeScript, and experiments with
typechecker implementations. We have identified a set of core technical
dimensions for type narrowing. For each dimension, the benchmark contains a set
of topics and (at least) two characterizing programs per topic: one that should
typecheck and one that should not typecheck. **Knowledge:** If-T provides a
baseline to measure type narrowing systems. For researchers, it provides
criteria to categorize future designs via its collection of positive and
negative examples. For language designers, the benchmark demonstrates the
payoff of typechecker complexity in terms of concrete examples. Designers can
use the examples to decide whether supporting a particular example is
worthwhile. Both the benchmark and its implementations are freely available
online. **Grounding:** We have implemented the benchmark for five typecheckers:
TypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight
important differences, such as the ability to track logical implications among
program variables and typechecking for user-defined narrowing predicates.
**Importance:** Type narrowing is essential for gradual type systems, but the
tradeoffs between systems with different complexity have been unclear. If-T
clarifies these tradeoffs by illustrating the benefits and limitations of each
level of complexity. With If-T as a way to assess implementations in a fair,
cross-language manner, future type system designs can strive for a better
balance among precision, annotation burden, and performance.

</details>


### [14] [A Type System for Data Privacy Compliance in Active Object Languages](https://arxiv.org/abs/2508.03831)
*Chinmayi Prabhu Baramashetru,Paola Giannini,Silvia Lizeth Tapia Tarifa,Olaf Owe*

Main category: cs.PL

TL;DR: 论文提出了一种基于语言的隐私集成方法，通过静态和运行时技术结合，确保数据流符合GDPR要求。


<details>
  <summary>Details</summary>
Motivation: 数据保护法律（如GDPR）要求系统在设计和执行中保护个人数据，但抽象的隐私设计原则难以转化为具体方法。

Method: 使用类型检查和类型推断技术，在活动对象语言中跟踪授权数据流，并自动生成基于用户同意的运行时约束。

Result: 通过类型系统集成合规检查和用户同意变更，在系统执行中验证GDPR合规性，并通过示例和证明展示了其可行性。

Conclusion: 该方法为系统设计提供了一种自动化的GDPR合规方案，适用于医疗、金融等数据隐私关键领域。

Abstract: Data protection laws such as GDPR aim to give users unprecedented control
over their personal data. Compliance with these regulations requires
systematically considering information flow and interactions among entities
handling sensitive data. Privacy-by-design principles advocate embedding data
protection into system architectures as a default. However, translating these
abstract principles into concrete, explicit methods remains a significant
challenge. This paper addresses this gap by proposing a language-based approach
to privacy integration, combining static and runtime techniques. By employing
type checking and type inference in an active object language, the framework
enables the tracking of authorised data flows and the automatic generation of
constraints checked at runtime based on user consent. This ensures that
personal data is processed in compliance with GDPR constraints. The key
contribution of this work is a type system that gather the compliance checks
and the changes to users consent and integrates data privacy compliance
verification into system execution. The paper demonstrates the feasibility of
this approach through a soundness proof and several examples, illustrating how
the proposed language addresses common GDPR requirements, such as user consent,
purpose limitation, and data subject rights. This work advances the state of
the art in privacy-aware system design by offering a systematic and automated
method for integrating GDPR compliance into programming languages. This
capability has implications for building trustworthy systems in domains such as
healthcare or finance, where data privacy is crucial.

</details>


### [15] [Generating Inputs for Grammar Mining using Dynamic Symbolic Execution](https://arxiv.org/abs/2508.03832)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.PL

TL;DR: 论文提出了一种自动生成输入的语法挖掘方法，结合动态符号执行（DSE）和新型三阶段输入生成策略，解决了传统方法难以覆盖完整输入语言的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的语法挖掘方法依赖输入数据覆盖完整输入语言，但实践中很难获取全面数据，导致生成的语法遗漏边缘情况或旧功能。

Method: 提出结合DSE的自动化输入生成方法，通过迭代扩展输入和新型三阶段生成策略，优化语法挖掘。

Result: 在11个基准应用中验证，生成的语法接近Mimid等现有工具精度，且能发现传统方法遗漏的细微特征和边缘情况。

Conclusion: 该方法为软件工程提供了一种自动化、可扩展且高精度的语法挖掘解决方案，无需手动生成输入，提升了语法的全面性。

Abstract: A vast number of software systems include components that parse and process
structured input. In addition to programming languages, which are analyzed by
compilers or interpreters, there are numerous components that process
standardized or proprietary data formats of varying complexity. Even if such
components were initially developed and tested based on a specification, such
as a grammar, numerous modifications and adaptations over the course of
software evolution can make it impossible to precisely determine which inputs
they actually accept. In this situation, grammar mining can be used to
reconstruct the specification in the form of a grammar. Established approaches
already produce useful results, provided that sufficient input data is
available to fully cover the input language. However, achieving this
completeness is a major challenge. In practice, only input data recorded during
the operation of the software systems is available. If this data is used for
grammar mining, the resulting grammar reflects only the actual processed inputs
but not the complete grammar of the input language accepted by the software
component. As a result, edge cases or previously supported features that no
longer appear in the available input data are missing from the generated
grammar. This work addresses this challenge by introducing a novel approach for
the automatic generation of inputs for grammar mining. Although input
generators have already been used for fuzz testing, it remains unclear whether
they are also suitable for grammar miners. Building on the grammar miner Mimid,
this work presents a fully automated approach to input generation. The approach
leverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms
to overcome the limitations of DSE regarding structured input parsers. First,
the search for new inputs is guided by an iterative expansion that starts with
a single-character input and gradually extends it. Second, input generation is
structured into a novel three-phase approach, which separates the generation of
inputs for parser functions. The proposed method was evaluated against a
diverse set of eleven benchmark applications from the existing literature.
Results demonstrate that the approach achieves precision and recall for
extracted grammars close to those derived from state-of-the-art grammar miners
such as Mimid. Notably, it successfully uncovers subtle features and edge cases
in parsers that are typically missed by such grammar miners. The effectiveness
of the method is supported by empirical evidence, showing that it can achieve
high performance in various domains without requiring prior input samples. This
contribution is significant for researchers and practitioners in software
engineering, offering an automated, scalable, and precise solution for grammar
mining. By eliminating the need for manual input generation, the approach not
only reduces workload but also enhances the robustness and comprehensiveness of
the extracted grammars. Following this approach, software engineers can
reconstruct specification from existing (legacy) parsers.

</details>


### [16] [Weak Memory Model Formalisms: Introduction and Survey](https://arxiv.org/abs/2508.04115)
*Roger C. Su,Robert J. Colvin*

Main category: cs.PL

TL;DR: 本文综述了弱内存模型的规范化，包括其规范、对执行的影响以及推理工具，并介绍了操作语义和公理语义两种形式化表示方法。


<details>
  <summary>Details</summary>
Motivation: 由于微架构特性或编译器优化，程序顺序不能可靠反映执行顺序，这使得并发编程更具挑战性。因此，严格规范弱内存模型对开发安全关键的低级软件至关重要。

Method: 通过操作语义和公理语义两种形式化方法（以简化的x86为例）介绍弱内存模型的规范，并综述该领域的工具和推理系统。

Result: 综述了导致弱行为的长存硬件特性、实践与理论的发展历史，以及计算性和复杂性结果，并展望了未来方向。

Conclusion: 弱内存模型的严格规范是解决并发编程挑战的关键，本文为该领域的进一步研究提供了基础和展望。

Abstract: Memory consistency models define the order in which accesses to shared memory
in a concurrent system may be observed to occur. Such models are a necessity
since program order is not a reliable indicator of execution order, due to
microarchitectural features or compiler transformations. Concurrent
programming, already a challenging task, is thus made even harder when weak
memory effects must be addressed. A rigorous specification of weak memory
models is therefore essential to make this problem tractable for developers of
safety- and security-critical, low-level software.
  In this paper we survey the field of formalisations of weak memory models,
including their specification, their effects on execution, and tools and
inference systems for reasoning about code. To assist the discussion we also
provide an introduction to two styles of formal representation found commonly
in the literature (using a much simplified version of Intel's x86 as the
example): a step-by-step construction of traces of the system (operational
semantics); and with respect to relations between memory events (axiomatic
semantics). The survey covers some long-standing hardware features that lead to
observable weak behaviours, a description of historical developments in
practice and in theory, an overview of computability and complexity results,
and outlines current and future directions in the field.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [17] [ARMS: Adaptive and Robust Memory Tiering System](https://arxiv.org/abs/2508.04417)
*Sujay Yadalam,Konstantinos Kanellis,Michael Swift,Shivaram Venkataraman*

Main category: cs.OS

TL;DR: 论文提出了一种自适应的内存分层系统ARMS，通过动态识别冷热数据和优化迁移策略，无需手动调优即可接近最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有内存分层系统依赖固定阈值，难以适应不同工作负载，调优可带来显著性能提升，但缺乏通用性。

Method: 设计了基于短期和长期移动平均的热冷数据识别机制，成本/效益分析的迁移策略，以及带宽感知的批量迁移调度。

Result: ARMS在无需调优的情况下，性能接近现有系统的最佳调优结果（相差3%），且优于未调优系统的1.26-2.3倍。

Conclusion: ARMS通过动态自适应策略解决了固定阈值系统的局限性，提供了高性能的通用解决方案。

Abstract: Memory tiering systems seek cost-effective memory scaling by adding multiple
tiers of memory. For maximum performance, frequently accessed (hot) data must
be placed close to the host in faster tiers and infrequently accessed (cold)
data can be placed in farther slower memory tiers. Existing tiering solutions
such as HeMem, Memtis, and TPP use rigid policies with pre-configured
thresholds to make data placement and migration decisions. We perform a
thorough evaluation of the threshold choices and show that there is no single
set of thresholds that perform well for all workloads and configurations, and
that tuning can provide substantial speedups. Our evaluation identified three
primary reasons why tuning helps: better hot/cold page identification, reduced
wasteful migrations, and more timely migrations.
  Based on this study, we designed ARMS - Adaptive and Robust Memory tiering
System - to provide high performance without tunable thresholds. We develop a
novel hot/cold page identification mechanism relying on short-term and
long-term moving averages, an adaptive migration policy based on cost/benefit
analysis, and a bandwidth-aware batched migration scheduler. Combined, these
approaches provide out-of-the-box performance within 3% the best tuned
performance of prior systems, and between 1.26x-2.3x better than prior systems
without tuning.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors](https://arxiv.org/abs/2508.03862)
*Abdul Saboor,Zhuangzhuang Cui,Achiel Colpaert,Evgenii Vinogradov,Sofie Pollin*

Main category: cs.NI

TL;DR: 论文提出了一种基于上下文感知的智能切换协议（CASH），通过预测无人机轨迹减少切换频率，显著提升了城市空中交通的通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通中频繁切换导致网络性能下降的问题，提升无人机在3D空中走廊的通信可靠性。

Method: 提出CASH协议，利用无人机轨迹的预测评分机制，主动优化切换决策，并在模拟环境中与现有协议对比。

Result: CASH将切换频率降低78%，同时保持较低的中断概率，并通过实验确定了基站密度和安全裕度的最优配置。

Conclusion: CASH协议显著提升了城市空中交通的通信性能，为未来3D空中交通网络提供了可靠解决方案。

Abstract: Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial
Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,
such as air taxis. A key challenge in these high-mobility aerial corridors is
ensuring reliable connectivity, where frequent handovers can degrade network
performance. To resolve this, we present a Context-Aware Smart Handover (CASH)
protocol that uses a forward-looking scoring mechanism based on UAV trajectory
to make proactive handover decisions. We evaluate the performance of the
proposed CASH against existing handover protocols in a custom-built simulator.
Results show that CASH reduces handover frequency by up to 78% while
maintaining low outage probability. We then investigate the impact of base
station density and safety margin on handover performance, where their optimal
setups are empirically obtained to ensure reliable UAM communication.

</details>


### [19] [Confidence Driven Classification of Application Types in the Presence of Background Network](https://arxiv.org/abs/2508.03891)
*Eun Hun Choi,Jasleen Kaur,Vladas Pipiras,Nelson Gomes Rodrigues Antunes,Brendan Massey*

Main category: cs.NI

TL;DR: 使用深度学习模型准确分类网络流量类型时，因背景流量干扰，传统分类器效果不佳。通过引入高斯混合模型框架，提升了分类置信度，避免误分类。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量分类器在现实数据中效果不佳，主要原因是忽略了广告、分析等非应用特定的背景流量，导致分类准确率下降。

Method: 提出一种基于高斯混合模型的分类框架，用于更可靠地评估深度学习分类器的置信度，避免对不确定样本的错误分类。

Result: 该方法显著提升了分类器的可靠性，减少了对背景流量的误分类。

Conclusion: 通过引入背景流量类别和高斯混合模型，有效改善了网络流量分类的准确性和可靠性。

Abstract: Accurately classifying the application types of network traffic using deep
learning models has recently gained popularity. However, we find that these
classifiers do not perform well on real-world traffic data due to the presence
of non-application-specific generic background traffic originating from
advertisements, analytics, shared APIs, and trackers. Unfortunately,
state-of-the-art application classifiers overlook such traffic in curated
datasets and only classify relevant application traffic. To address this issue,
when we label and train using an additional class for background traffic, it
leads to additional confusion between application and background traffic, as
the latter is heterogeneous and encompasses all traffic that is not relevant to
the application sessions. To avoid falsely classifying background traffic as
one of the relevant application types, a reliable confidence measure is
warranted, such that we can refrain from classifying uncertain samples.
Therefore, we design a Gaussian Mixture Model-based classification framework
that improves the indication of the deep learning classifier's confidence to
allow more reliable classification.

</details>


### [20] [Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3](https://arxiv.org/abs/2508.04004)
*Tanguy Ropitault,Matteo Bordin,Paolo Testolina,Michele Polese,Pedram Johari,Nada Golmie,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文扩展了系统级仿真器5G-LENA，引入基于几何的通道模型，提升对特定场景的建模能力，验证了其在波束管理和端到端分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统统计通道模型无法捕捉特定场景的几何现象（如衍射、遮挡等），限制了系统级仿真的精确性。

Method: 扩展5G-LENA模块，整合基于射线追踪或实测的多径分量（MPCs），构建频域通道矩阵并与现有物理层/介质访问控制层兼容。

Result: 新模型展示了统计模型无法揭示的性能变化，为高保真系统级研究和数字孪生应用提供支持。

Conclusion: 基于几何的通道模型显著提升了仿真的场景特异性能力，是迈向数字孪生应用的关键一步。

Abstract: Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is
challenging because the performance emerges from the tight coupling of
propagation, beam management, scheduling, and higher-layer interactions.
System-level simulation is therefore indispensable, yet the vast majority of
studies rely on the statistical 3GPP channel models. These are well suited to
capture average behavior across many statistical realizations, but cannot
reproduce site-specific phenomena such as corner diffraction, street-canyon
blockage, or deterministic line-of-sight conditions and
angle-of-departure/arrival relationships that drive directional links. This
paper extends 5G-LENA, an NR module for the system-level Network Simulator 3
(ns-3), with a trace-based channel model that processes the Multipath
Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer
(RT)) or measurement campaigns. Our module constructs frequency-domain channel
matrices and feeds them to the existing Physical (PHY)/Medium Access Control
(MAC) stack without any further modifications. The result is a geometry-based
channel model that remains fully compatible with the standard 3GPP
implementation in 5G-LENA, while delivering site-specific geometric fidelity.
This new module provides a key building block toward Digital Twin (DT)
capabilities by offering realistic site-specific channel modeling, unlocking
studies that require site awareness, including beam management, blockage
mitigation, and environment-aware sensing. We demonstrate its capabilities for
precise beam-steering validation and end-to-end metric analysis. In both cases,
the trace-driven engine exposes performance inflections that the statistical
model does not exhibit, confirming its value for high-fidelity system-level
cellular networks research and as a step toward DT applications.

</details>


### [21] [A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks](https://arxiv.org/abs/2508.04015)
*Haoxiang Luo,Kun Yang,Qi Huang,Schahram Dustdar*

Main category: cs.NI

TL;DR: 本文提出了一种两阶段协同优化框架（TSCO），用于同时管理电力系统调度和计算能力网络（CPN）任务调度，以实现低碳运营。该框架通过分解为日前和实时两个阶段，结合Benders分解和深度强化学习（DRL），有效减少了碳排放、运营成本及可再生能源弃电量。


<details>
  <summary>Details</summary>
Motivation: 应对大规模人工智能和数据密集型应用带来的能源消耗问题，以及高比例可再生能源引入的电网不稳定性挑战，提出协同优化的解决方案。

Method: 采用两阶段优化框架（TSCO），结合Benders分解和DRL智能调度，实现电力系统与CPN任务的高效协同管理。

Result: 在IEEE 30-bus系统的模拟中，该框架显著降低了碳排放和运营成本，同时减少了60%以上的可再生能源弃电量，并确保了任务的服务质量。

Conclusion: TSCO框架为解决能源消耗和电网不稳定性问题提供了一种高效、低碳的解决方案，具有实际应用潜力。

Abstract: The proliferation of large-scale artificial intelligence and data-intensive
applications has spurred the development of Computing Power Networks (CPNs),
which promise to deliver ubiquitous and on-demand computational resources.
However, the immense energy consumption of these networks poses a significant
sustainability challenge. Simultaneously, power grids are grappling with the
instability introduced by the high penetration of intermittent renewable energy
sources (RES). This paper addresses these dual challenges through a novel
Two-Stage Co-Optimization (TSCO) framework that synergistically manages power
system dispatch and CPN task scheduling to achieve low-carbon operations. The
framework decomposes the complex, large-scale problem into a day-ahead
stochastic unit commitment (SUC) stage and a real-time operational stage. The
former is solved using Benders decomposition for computational tractability,
while in the latter, economic dispatch of generation assets is coupled with an
adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)
agent. This agent makes intelligent, carbon-aware decisions by responding to
dynamic grid conditions, including real-time electricity prices and marginal
carbon intensity. Through extensive simulations on an IEEE 30-bus system
integrated with a CPN, the TSCO framework is shown to significantly outperform
baseline approaches. Results demonstrate that the proposed framework reduces
total carbon emissions and operational costs, while simultaneously decreasing
RES curtailment by more than 60% and maintaining stringent Quality of Service
(QoS) for computational tasks.

</details>


### [22] [Metaverse Framework for Wireless Systems Management](https://arxiv.org/abs/2508.04150)
*Ilias Chrysovergis,Alexandros-Apostolos A. Boulogeorgos,Theodoros A. Tsiftsis,Dusit Niyato*

Main category: cs.NI

TL;DR: 该论文提出了一个元宇宙框架，用于无线系统的模拟、仿真和交互，整合了XR、DT、AI、IoT、区块链和6G技术。


<details>
  <summary>Details</summary>
Motivation: 目标是创建一个动态、沉浸式的平台，以支持无线系统的开发和管理，并为未来网络环境提供洞察。

Method: 通过XR实现可视化与交互，DT用于实时监控优化，AI生成3D内容并提升决策能力，IoT提供实时数据，区块链确保安全性。

Result: 该框架为无线系统的探索、开发和优化提供了强大工具，推动了未来网络的发展。

Conclusion: 该框架展示了元宇宙技术在无线系统领域的潜力，为未来的研究和应用奠定了基础。

Abstract: This article introduces a comprehensive metaverse framework, which is
designed for the simulation, emulation, and interaction with wireless systems.
The proposed framework integrates core metaverse technologies such as extended
reality (XR), digital twins (DTs), artificial intelligence (AI), internet of
things (IoT), blockchain, and advanced 6G networking solutions to create a
dynamic, immersive platform for both system development and management. By
leveraging XR, users can visualize and engage with complex systems, while DTs
enable real-time monitoring and optimization. AI generates the
three-dimensional (3D) content, enhances decision-making and system
performance, whereas IoT devices provide real-time sensor data for boosting the
simulation accuracy. Additionally, blockchain ensures secure, decentralized
interactions, and 5G/6G networks offer the necessary infrastructure for
seamless, low-latency communication. This framework serves as a robust tool for
exploring, developing, and optimizing wireless systems, aiming to provide
valuable insights into the future of networked environments.

</details>


### [23] [DSNS: The Deep Space Network Simulator](https://arxiv.org/abs/2508.04317)
*Joshua Smailes,Filip Futera,Sebastian Köhler,Simon Birnbach,Martin Strohmeier,Ivan Martinovic*

Main category: cs.NI

TL;DR: DSNS是一种针对大规模卫星网络的新型网络模拟器，解决了现有模拟工具无法应对数千节点卫星网络的局限性，展示了更高的灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星网络规模扩大至数千节点以及星际互联网概念的兴起，现有卫星和网络模拟工具已无法满足需求。

Method: 开发了Deep Space Network Simulator (DSNS)，并通过实现现有协议和CCSDS推荐的DTN模拟参考场景，展示其灵活性和扩展性。

Result: DSNS在可扩展性和模拟保真度上优于现有工具，适用于标准制定机构和卫星运营商。

Conclusion: DSNS通过去除研究和创新的障碍，加速了未来卫星网络的开发，确保了通信的高速和安全性。

Abstract: Simulation tools are commonly used in the development and testing of new
protocols or new networks. However, as satellite networks start to grow to
encompass thousands of nodes, and as companies and space agencies begin to
realize the interplanetary internet, existing satellite and network simulation
tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network
simulator with a focus on large-scale satellite networks. We demonstrate its
improved capabilities compared to existing offerings, showcase its flexibility
and extensibility through an implementation of existing protocols and the DTN
simulation reference scenarios recommended by CCSDS, and evaluate its
scalability, showing that it exceeds existing tools while providing better
fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite
operators, enabling fast iteration on protocol development and testing of
parameters under highly realistic conditions. By removing roadblocks to
research and innovation, we can accelerate the development of upcoming
satellite networks and ensure that their communication is both fast and secure.

</details>


### [24] [Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection](https://arxiv.org/abs/2508.04415)
*Xuan Chen,Yu Huang,Miaowen Wen,Shahid Mumtaz,Fatih Gulec,Anwer Al-Dulaimi,Andrew W. Eckford*

Main category: cs.NI

TL;DR: 论文探讨了利用分子通信（MC）构建生物纳米物联网（IoBNT）以应对流行病控制的挑战，包括病毒传播建模、检测病毒/感染者及识别病毒突变。


<details>
  <summary>Details</summary>
Motivation: IoBNT在医疗保健领域具有革命性潜力，但构建其用于流行病防控面临挑战，论文旨在通过MC技术解决这些问题。

Method: 分析了宏观和微观尺度下的MC通道以匹配病毒传播，研究了检测方法及定位机制，并提出了病毒突变识别策略，通过ORF3a蛋白验证。

Result: 论文提出了多尺度检测和病毒突变识别的方法，并通过仿真验证了其有效性。

Conclusion: 论文通过MC和信号处理技术分析了病毒传播并提出了防控策略，同时指出了未来研究方向。

Abstract: The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary
healthcare paradigm, shows promise for epidemic control. This paper explores
the potential of using molecular communication (MC) to address the challenges
in constructing IoBNT for epidemic prevention, specifically focusing on
modeling viral transmission, detecting the virus/infected individuals, and
identifying virus mutations. First, the MC channels in macroscale and
microscale scenarios are discussed to match viral transmission in both scales
separately. Besides, the detection methods for these two scales are also
studied, along with the localization mechanism designed for the virus/infected
individuals. Moreover, an identification strategy is proposed to determine
potential virus mutations, which is validated through simulation using the
ORF3a protein as a benchmark. Finally, open research issues are discussed. In
summary, this paper aims to analyze viral transmission through MC and combat
viral spread using signal processing techniques within MC.

</details>


### [25] [Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions](https://arxiv.org/abs/2508.04526)
*Fannya R. Sandjaja,Ayesha A. Majeed,Abdullah Abdullah,Gyan Wickremasinghe,Karen Rafferty,Vishal Sharma*

Main category: cs.NI

TL;DR: 论文探讨了零信任架构（ZTA）在分布式网络中的策略设计挑战与解决方案，并使用UPPAAL进行策略的形式化验证，同时讨论了责任与问责在系统安全中的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统安全架构因过度依赖信任而容易受到分布式攻击，尤其在引入智能代理AI后，问题更加突出。零信任架构（ZTA）虽然提供了一种解决方案，但其策略设计不完善可能导致未授权访问。

Method: 论文研究了零信任分布式网络（ZTDN）的策略设计问题，并通过UPPAAL工具对策略进行了形式化验证，结合案例研究分析其有效性。

Result: 研究提出了在分布式网络中设计ZTA策略的挑战与解决方案，并通过形式化验证工具证实了策略的有效性，同时强调了系统安全中责任与问责的重要性。

Conclusion: 零信任架构在分布式网络中具有潜力，但策略设计需要严格验证，且责任与问责机制是确保安全性的关键因素。

Abstract: Traditional security architectures are becoming more vulnerable to
distributed attacks due to significant dependence on trust. This will further
escalate when implementing agentic AI within the systems, as more components
must be secured over a similar distributed space. These scenarios can be
observed in consumer technologies, such as the dense Internet of things (IoT).
Here, zero-trust architecture (ZTA) can be seen as a potential solution, which
relies on a key principle of not giving users explicit trust, instead always
verifying their privileges whenever a request is made. However, the overall
security in ZTA is managed through its policies, and unverified policies can
lead to unauthorized access. Thus, this paper explores challenges and solutions
for ZTA policy design in the context of distributed networks, which is referred
to as zero-trust distributed networks (ZTDN). This is followed by a case-study
on formal verification of policies using UPPAAL. Subsequently, the importance
of accountability and responsibility in the system's security is discussed.

</details>


### [26] [CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps](https://arxiv.org/abs/2508.04556)
*Filipe B. Teixeira,Carolina Simões,Paulo Fidalgo,Wagner Pedrosa,André Coelho,Manuel Ricardo,Luis M. Pessoa*

Main category: cs.NI

TL;DR: 论文提出了一种将视觉数据与无线通信结合的新架构，用于实时提供无线和视频感知信息，以支持集成感知与通信。


<details>
  <summary>Details</summary>
Motivation: 高频无线链路需要视距传输，视觉数据可以帮助预测信道动态并优化技术（如波束成形或切换），以克服障碍。

Method: 通过多智能体方法将实时的无线和视频感知信息传递给O-RAN xApps，并引入新的视频功能生成障碍信息。

Result: 实验表明，感知信息的延迟小于1毫秒，xApp能成功利用无线和视频感知信息实时控制5G/6G RAN。

Conclusion: 该架构有效实现了无线通信与视觉感知的集成，为实时RAN控制提供了可行方案。

Abstract: Telecommunications and computer vision have evolved independently. With the
emergence of high-frequency wireless links operating mostly in line-of-sight,
visual data can help predict the channel dynamics by detecting obstacles and
help overcoming them through beamforming or handover techniques.
  This paper proposes a novel architecture for delivering real-time radio and
video sensing information to O-RAN xApps through a multi-agent approach, and
introduces a new video function capable of generating blockage information for
xApps, enabling Integrated Sensing and Communications. Experimental results
show that the delay of sensing information remains under 1\,ms and that an xApp
can successfully use radio and video sensing information to control the 5G/6G
RAN in real-time.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content](https://arxiv.org/abs/2508.04353)
*Anderson de Lima Luiz*

Main category: cs.MM

TL;DR: LUST框架通过视频内容和用户文本描述分析主题相关性，采用多模态分析和LLM评分机制。


<details>
  <summary>Details</summary>
Motivation: 目的是量化视频片段与用户定义的文本主题之间的相关性，提供动态的叙事理解。

Method: 利用多模态分析结合视觉和听觉信息，采用两阶段LLM评分机制（直接相关性和上下文相关性）。

Result: 输出带注释的视频和相关分数，提供全面的分析日志。

Conclusion: LUST能动态捕捉视频叙事的主题演变，为用户提供详细的分析工具。

Abstract: This paper introduces the Learned User Significance Tracker (LUST), a
framework designed to analyze video content and quantify the thematic relevance
of its segments in relation to a user-provided textual description of
significance. LUST leverages a multi-modal analytical pipeline, integrating
visual cues from video frames with textual information extracted via Automatic
Speech Recognition (ASR) from the audio track. The core innovation lies in a
hierarchical, two-stage relevance scoring mechanism employing Large Language
Models (LLMs). An initial "direct relevance" score, $S_{d,i}$, assesses
individual segments based on immediate visual and auditory content against the
theme. This is followed by a "contextual relevance" score, $S_{c,i}$, that
refines the assessment by incorporating the temporal progression of preceding
thematic scores, allowing the model to understand evolving narratives. The LUST
framework aims to provide a nuanced, temporally-aware measure of user-defined
significance, outputting an annotated video with visualized relevance scores
and comprehensive analytical logs.

</details>


### [28] [Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2508.04418)
*Jinxing Zhou,Yanghao Zhou,Mingfei Han,Tong Wang,Xiaojun Chang,Hisham Cholakkal,Rao Muhammad Anwer*

Main category: cs.MM

TL;DR: 该论文提出了一种名为TGS-Agent的新方法，通过模仿人类推理过程，将音频-视觉分割任务分解为Think-Ground-Segment三个步骤，取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态融合学习潜在嵌入，缺乏解释性且需要强像素级监督。本文从显式参考理解的角度出发，提出更符合人类推理的方式。

Method: 提出TGS-Agent，包含Ref-Thinker多模态语言模型进行推理，生成对象描述作为Grounding-DINO和SAM2的显式提示，避免像素级监督。

Result: 方法在标准Ref-AVSBench和新提出的R²-AVSBench上均达到最先进性能。

Conclusion: 论文通过显式推理和多模态分析，提出了一种更高效且解释性更强的音频-视觉分割方法。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects
in audible videos based on given reference expressions. Prior works typically
rely on learning latent embeddings via multimodal fusion to prompt a tunable
SAM/SAM2 decoder for segmentation, which requires strong pixel-level
supervision and lacks interpretability. From a novel perspective of explicit
reference understanding, we propose TGS-Agent, which decomposes the task into a
Think-Ground-Segment process, mimicking the human reasoning procedure by first
identifying the referred object through multimodal analysis, followed by
coarse-grained grounding and precise segmentation. To this end, we first
propose Ref-Thinker, a multimodal language model capable of reasoning over
textual, visual, and auditory cues. We construct an instruction-tuning dataset
with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The
object description inferred by Ref-Thinker is used as an explicit prompt for
Grounding-DINO and SAM2, which perform grounding and segmentation without
relying on pixel-level supervision. Additionally, we introduce
R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and
reasoning-intensive references for better evaluating model generalization. Our
approach achieves state-of-the-art results on both standard Ref-AVSBench and
proposed R\textsuperscript{2}-AVSBench. Code will be available at
https://github.com/jasongief/TGS-Agent.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [Control Closure Certificates](https://arxiv.org/abs/2508.03947)
*Vishnu Murali,Mohammed Adib Oumer,Majid Zamani*

Main category: cs.LO

TL;DR: 提出了一种控制闭包证书的方法，用于合成离散时间控制系统满足ω-regular规范的控制器，通过组合不变量和析取良基性证明，验证和设计控制器。


<details>
  <summary>Details</summary>
Motivation: 现有的控制器合成方法通常依赖于归纳不变式和良基性证明，缺乏自动化和高效性。本文提出控制闭包证书的概念，以更有效的方式验证和设计满足ω-regular规范的控制器。

Method: 结合不变量和析取良基性论证，构建控制闭包证书，并通过乘积空间中的证书设计控制器。采用平方和优化方法合成证书。

Result: 展示了控制闭包证书在验证和控制器设计中的有效性，并通过实例研究验证了其性能。

Conclusion: 控制闭包证书提供了一种自动化且高效的方法，能够合成满足ω-regular规范的控制器，扩展了现有技术的能力。

Abstract: This paper introduces the notion of control closure certificates to
synthesize controllers for discrete-time control systems against
$\omega$-regular specifications. Typical functional approaches to synthesize
controllers against $\omega$-regular specifications rely on combining inductive
invariants (for example, via barrier certificates) with proofs of
well-foundedness (for example, via ranking functions). Transition invariants,
provide an alternative where instead of standard well-foundedness arguments one
may instead search for disjunctive well-foundedness arguments that together
ensure a well-foundedness argument. Closure certificates, functional analogs of
transition invariants, provide an effective, automated approach to verify
discrete-time dynamical systems against linear temporal logic and
$\omega$-regular specifications. We build on this notion to synthesize
controllers to ensure the satisfaction of $\omega$-regular specifications. To
do so, we first illustrate how one may construct control closure certificates
to visit a region infinitely often (or only finitely often) via disjunctive
well-founded arguments. We then combine these arguments to provide an argument
for parity specifications. Thus, finding an appropriate control closure
certificate over the product of the system and a parity automaton specifying a
desired $\omega$-regular specification ensures that there exists a controller
$\kappa$ to enforce the $\omega$-regular specification. We propose a
sum-of-squares optimization approach to synthesize such certificates and
demonstrate their efficacy in designing controllers over some case studies.

</details>


### [30] [GradSTL: Comprehensive Signal Temporal Logic for Neurosymbolic Reasoning and Learning](https://arxiv.org/abs/2508.04438)
*Mark Chevallier,Filip Smola,Richard Schmoetten,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: GradSTL是首个完全支持信号时序逻辑（STL）的开源实现，适用于神经符号学习，能评估任何信号上的任何STL约束，并通过自动生成的正式验证保证正确性。


<details>
  <summary>Details</summary>
Motivation: 为神经符号学习提供一种可无缝集成且形式化验证的信号时序逻辑实现，以支持基于梯度下降的学习。

Method: 通过形式化规范定义平滑STL语义，并自动生成实现代码，确保构造即正确。

Result: 案例研究表明，神经符号过程能够通过学习满足预设的STL约束。

Conclusion: GradSTL为信号时序逻辑与梯度下降学习的结合提供了严谨的基础。

Abstract: We present GradSTL, the first fully comprehensive implementation of signal
temporal logic (STL) suitable for integration with neurosymbolic learning. In
particular, GradSTL can successfully evaluate any STL constraint over any
signal, regardless of how it is sampled. Our formally verified approach
specifies smooth STL semantics over tensors, with formal proofs of soundness
and of correctness of its derivative function. Our implementation is generated
automatically from this formalisation, without manual coding, guaranteeing
correctness by construction. We show via a case study that using our
implementation, a neurosymbolic process learns to satisfy a pre-specified STL
constraint. Our approach offers a highly rigorous foundation for integrating
signal temporal logic and learning by gradient descent.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [31] [MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning](https://arxiv.org/abs/2508.03700)
*Liujian Tang,Shaokang Dong,Yijia Huang,Minqi Xiang,Hongtao Ruan,Bin Wang,Shuo Li,Zhihui Cao,Hailiang Pang,Heng Kong,He Yang,Mingxu Chai,Zhilin Gao,Xingyu Liu,Yingnan Fu,Jiaming Liu,Tao Gui,Xuanjing Huang,Yu-Gang Jiang,Qi Zhang,Kang Wang,Yunke Zhang,Yuran Wang*

Main category: cs.HC

TL;DR: MagicGUI是一种移动GUI代理框架，通过六项关键技术解决感知、接地和推理问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决移动GUI环境中感知、接地和推理的挑战。

Method: 构建多样化的数据集、增强感知和接地能力、统一的动作空间、规划导向的推理机制、两阶段训练流程。

Result: 在多个基准测试中表现优异，展示出强大的泛化能力和实际部署潜力。

Conclusion: MagicGUI为移动GUI任务提供了高效解决方案，具有广泛的实用性和推广价值。

Abstract: This paper presents MagicGUI, a foundational mobile GUI agent designed to
address critical challenges in perception, grounding, and reasoning within
real-world mobile GUI environments. The framework is underpinned by following
six key components: (1) a comprehensive and accurate dataset, constructed via
the scalable GUI Data Pipeline, which aggregates the largest and most diverse
GUI-centric multimodal data to date from open-source repositories, automated
crawling, and targeted manual annotation; (2) enhanced perception and grounding
capabilities, facilitating fine-grained multimodal alignment for UI element
referencing, grounding, and screen comprehension; (3) a comprehensive and
unified action space, encompassing both fundamental UI operations and complex
interactive intents to support human-agent interactions; (4) planning-oriented
reasoning mechanisms that enable the model to decompose complex user
instructions into sequential actions with explicit intermediate meta-paln
reasoning; (5) an iterative two-stage training procedure, combining large-scale
continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing
a spatially enhanced composite reward and dual filtering strategy; and (6)
competitive performance on both the proprietary Magic-RICH benchmark and over a
dozen public benchmarks, achieving superior performance across GUI perception
and agent tasks, while demonstrating robust generalization and real-world
deployment potential in practical mobile GUI scenarios, as detailed in Figure
1.

</details>


### [32] [Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth](https://arxiv.org/abs/2508.03705)
*Kanan Eldarov*

Main category: cs.HC

TL;DR: 研究探讨电脑与智能手机对青少年注意力、挫折感和创造力的影响，显示不同设备交互方式存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索数字交互方式（电脑vs智能手机）如何影响青少年的认知和行为表现。

Method: 结合任务日志、视线追踪及专家评估，对824名11-17岁学生进行随机分层实验。

Result: 发现设备对持续性注意力、挫折感和创造性产出有显著但中等程度影响。

Conclusion: 数字交互方式不仅影响屏幕时间，还可能对教育设计和学习环境产生实际影响。

Abstract: This study explores how different modes of digital interaction -- namely,
computers versus smartphones -- affect attention, frustration, and creative
performance in adolescents. Using a combination of digital task logs,
webcam-based gaze estimation, and expert evaluation of task outcomes, we
analyzed data from a diverse sample of 824 students aged 11-17. Participants
were assigned to device groups in a randomized and stratified design to control
for age, gender, and prior experience. Results suggest moderate but
statistically significant differences in sustained attention, perceived
frustration, and creative output. These findings indicate that the nature of
digital interaction -- beyond mere screen time -- may influence cognitive and
behavioral outcomes relevant to educational design. Practical implications for
user interface development and learning environments are discussed.

</details>


### [33] [Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention](https://arxiv.org/abs/2508.03713)
*Minsuk Chang,Yao Wang,Huichen Will Wang,Yuanhong Zhou,Andreas Bulling,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 研究通过分析视觉注意力模式与视觉素养的关系，提出Lit2Sal和Sal2Lit模型，分别预测注意力分布和视觉素养，提升了可视化设计的个性化效果。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉素养差异如何影响注意力模式，以优化可视化设计的个性化效果。

Method: 基于235名参与者的用户研究数据，提出Lit2Sal和Sal2Lit两个计算模型，分别预测注意力分布和视觉素养。

Result: Lit2Sal优于现有显著性模型，Sal2Lit预测视觉素养的准确率达86%，且评估时间少于1分钟。

Conclusion: 结合个体差异的显著性模型和视觉注意力分析，为个性化视觉数据沟通提供了新方向。

Abstract: Accounting for individual differences can improve the effectiveness of
visualization design. While the role of visual attention in visualization
interpretation is well recognized, existing work often overlooks how this
behavior varies based on visual literacy levels. Based on data from a
235-participant user study covering three visualization tests (mini-VLAT,
CALVI, and SGL), we show that distinct attention patterns in visual data
exploration can correlate with participants' literacy levels: While experts
(high-scorers) generally show a strong attentional focus, novices (low-scorers)
focus less and explore more. We then propose two computational models
leveraging these insights: Lit2Sal -- a novel visual saliency model that
predicts observer attention given their visualization literacy level, and
Sal2Lit -- a model to predict visual literacy from human visual attention data.
Our quantitative and qualitative evaluation demonstrates that Lit2Sal
outperforms state-of-the-art saliency models with literacy-aware
considerations. Sal2Lit predicts literacy with 86% accuracy using a single
attention map, providing a time-efficient supplement to literacy assessment
that only takes less than a minute. Taken together, our unique approach to
consider individual differences in salience models and visual attention in
literacy assessments paves the way for new directions in personalized visual
data communication to enhance understanding.

</details>


### [34] ["Think First, Verify Always": Training Humans to Face AI Risks](https://arxiv.org/abs/2508.03714)
*Yuksel Aydin*

Main category: cs.HC

TL;DR: 论文提出“Think First, Verify Always”（TFVA）协议，将人类作为抵御AI威胁的第一道防线，基于五项原则（AIJET）提升认知安全。实验显示，短短3分钟的干预显著提高了任务表现。建议将TFVA嵌入GenAI平台，取代被动警告。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全以设备为中心，忽略了AI对人类认知的攻击威胁，亟需将人类作为防御的核心。

Method: 提出TFVA协议，基于五项原则（AIJET），并通过随机对照试验（n=151）验证其效果。

Result: 3分钟干预显著提升认知安全任务表现，绝对增益7.87%。

Conclusion: TFVA协议可快速增强人类对AI驱动的认知操纵的抵御能力，建议将其作为GenAI的标准提示，弥合技术与人类因素的鸿沟。

Abstract: Artificial intelligence enables unprecedented attacks on human cognition, yet
cybersecurity remains predominantly device-centric. This paper introduces the
"Think First, Verify Always" (TFVA) protocol, which repositions humans as
'Firewall Zero', the first line of defense against AI-enabled threats. The
protocol is grounded in five operational principles: Awareness, Integrity,
Judgment, Ethical Responsibility, and Transparency (AIJET). A randomized
controlled trial (n=151) demonstrated that a minimal 3-minute intervention
produced statistically significant improvements in cognitive security task
performance, with participants showing an absolute +7.87% gains compared to
controls. These results suggest that brief, principles-based training can
rapidly enhance human resilience against AI-driven cognitive manipulation. We
recommend that GenAI platforms embed "Think First, Verify Always" as a standard
prompt, replacing passive warnings with actionable protocols to enhance
trustworthy and ethical AI use. By bridging the gap between technical
cybersecurity and human factors, the TFVA protocol establishes human-empowered
security as a vital component of trustworthy AI systems.

</details>


### [35] [Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery](https://arxiv.org/abs/2508.03717)
*Muhammad Akmal Bin Mohammed Zaffir,Daisuke Sakai,Yuki Sato,Takahiro Wada*

Main category: cs.HC

TL;DR: 研究表明，主动运动中的非自愿眼球运动比被动运动更稳定，且操纵感（SoA）可能影响这种稳定性。本研究通过系统调节骑乘机器的动态特性，探讨了其对非自愿眼球运动准确性和感知操控性的影响。


<details>
  <summary>Details</summary>
Motivation: 探索骑乘机器动态特性的系统变化如何影响非自愿眼球运动的准确性及感知操控性，填补现有研究的空白。

Method: 参与者操作一台时间常数可调的偏转平台，记录其眼球运动并收集感知操控性和认知负荷的主观评分。

Result: 时间常数增加导致感知操控性下降、认知负荷上升，同时非自愿眼球运动准确性降低；操控性与眼球运动表现呈正相关，与认知负荷呈负相关。

Conclusion: 骑乘机器的动态特性显著影响非自愿眼球运动的准确性，感知操控性和认知负荷是关键中介因素。

Abstract: Studies suggest that involuntary eye movements exhibit greater stability
during active motion compared to passive motion, and this effect may also apply
to the operation of ride-on machinery. Moreover, a study suggested that
experimentally manipulating the sense of agency (SoA) by introducing delays may
influence the stability of involuntary eye movements. Although a preliminary
investigation examined involuntary eye movements and perceived maneuverability
under two distinct machine dynamics with preserved SoA, it remains unclear how
systematic variations in motion dynamics influence these factors. Therefore,
the purpose of the present research was to investigate whether systematic
variations in the dynamic properties of a ride-on machine, where the perceived
maneuverability is modulated, influence the accuracy of involuntary eye
movements in human operators. Participants rode a yaw-rotational platform whose
time constant from joystick input to motor torque of a rotational machine was
systematically manipulated. During the operation, eye movements were recorded
while participants fixated on a visual target. After each condition,
participants provided subjective ratings of maneuverability and cognitive load.
As the platform's time constant increased, the perceived maneuverability scores
decreased while the cognitive loads increased. Concurrently, involuntary eye
movement accuracy decreased. Moderate to weak positive correlations emerged
between the perceived maneuverability scores and the eye movement gain and
accuracy, while a weak negative correlation was found with cognitive load.

</details>


### [36] [Recommending With, Not For: Co-Designing Recommender Systems for Social Good](https://arxiv.org/abs/2508.03792)
*Michael D. Ekstrand,Afsaneh Razi,Aleksandra Sarcevic,Maria Soledad Pera,Robin Burke,Katherine Landau Wright*

Main category: cs.HC

TL;DR: 论文主张推荐系统的设计应由用户和其他利益相关者共同参与，而不只是由设计师决定，以更好地实现社会公益。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统的设计和评估主要由开发团队决定，其他利益相关者的需求未得到充分重视，尤其是涉及社会公益时。

Method: 提出通过参与式和民主化的过程来定义社会目标和系统设计，使用户和其他利益相关者成为共同设计者。

Result: 强调推荐系统应为社会公益服务，需以更多利益相关者的参与为前提。

Conclusion: 推荐系统的设计应由用户和利益相关者共同完成，而不仅仅是为他们设计，以实现真正的社会公益。

Abstract: Recommender systems are usually designed by engineers, researchers,
designers, and other members of development teams. These systems are then
evaluated based on goals set by the aforementioned teams and other business
units of the platforms operating the recommender systems. This design approach
emphasizes the designers' vision for how the system can best serve the
interests of users, providers, businesses, and other stakeholders. Although
designers may be well-informed about user needs through user experience and
market research, they are still the arbiters of the system's design and
evaluation, with other stakeholders' interests less emphasized in user-centered
design and evaluation. When extended to recommender systems for social good,
this approach results in systems that reflect the social objectives as
envisioned by the designers and evaluated as the designers understand them.
Instead, social goals and operationalizations should be developed through
participatory and democratic processes that are accountable to their
stakeholders. We argue that recommender systems aimed at improving social good
should be designed *by* and *with*, not just *for*, the people who will
experience their benefits and harms. That is, they should be designed in
collaboration with their users, creators, and other stakeholders as full
co-designers, not only as user study participants.

</details>


### [37] [A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers](https://arxiv.org/abs/2508.03852)
*Zhuohao,Zhang,Haichang Li,Chun Meng Yu,Faraz Faruqi,Junan Xie,Gene S-H Kim,Mingming Fan,Angus G. Forbes,Jacob O. Wobbrock,Anhong Guo,Liang He*

Main category: cs.HC

TL;DR: A11yShape是一个为盲人和低视力用户设计的系统，通过LLMs和OpenSCAD结合，提供非视觉交互支持，帮助用户理解、修改和迭代3-D模型。


<details>
  <summary>Details</summary>
Motivation: 解决盲人和低视力用户在构建3-D模型时面临的非视觉交互支持不足的问题。

Method: A11yShape结合LLMs和OpenSCAD，提供3-D模型的可访问描述、版本控制和层次化组件表示，并通过跨表示高亮机制同步模型的多重表示。

Result: 用户研究中，4名盲人程序员独立完成12个模型任务，成功理解、创建和修改3-D模型，无需视力正常者协助。

Conclusion: A11yShape有效支持盲人和低视力用户独立操作3-D模型，填补现有工具的非视觉交互空白。

Abstract: Building 3-D models is challenging for blind and low-vision (BLV) users due
to the inherent complexity of 3-D models and the lack of support for non-visual
interaction in existing tools. To address this issue, we introduce A11yShape, a
novel system designed to help BLV users who possess basic programming skills
understand, modify, and iterate on 3-D models. A11yShape leverages LLMs and
integrates with OpenSCAD, a popular open-source editor that generates 3-D
models from code. Key functionalities of A11yShape include accessible
descriptions of 3-D models, version control to track changes in models and
code, and a hierarchical representation of model components. Most importantly,
A11yShape employs a cross-representation highlighting mechanism to synchronize
semantic selections across all model representations -- code, semantic
hierarchy, AI description, and 3-D rendering. We conducted a multi-session user
study with four BLV programmers, where, after an initial tutorial session,
participants independently completed 12 distinct models across two testing
sessions, achieving results that aligned with their own satisfaction. The
result demonstrates that participants were able to comprehend provided 3-D
models, as well as independently create and modify 3-D models -- tasks that
were previously impossible without assistance from sighted individuals.

</details>


### [38] [ReVISit 2: A Full Experiment Life Cycle User Study Framework](https://arxiv.org/abs/2508.03876)
*Zach Cutler,Jack Wilburn,Hilson Shrestha,Yiren Ding,Brian Bollen,Khandaker Abrar Nadib,Tingying He,Andrew McNutt,Lane Harrison,Alexander Lex*

Main category: cs.HC

TL;DR: reVISit 2是一个支持可视化研究者设计和进行基于浏览器的用户研究的软件框架，旨在简化研究流程并提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 在线用户研究在可视化研究中非常普遍，但设计和分析这些研究仍然是一个重大负担，现有工具难以支持复杂的研究设计或交互。

Method: reVISit 2提供了一个框架，涵盖研究的设计、调试、数据收集、分析和传播，并提供技术和社会技术支持。

Result: 该系统已被用于高质量研究中，并通过一系列实验复现和访谈验证其有效性。

Conclusion: reVISit 2提升了研究的便捷性、可重复性，并支持高级交互研究的构建。

Abstract: Online user studies of visualizations, visual encodings, and interaction
techniques are ubiquitous in visualization research. Yet, designing,
conducting, and analyzing studies effectively is still a major burden. Although
various packages support such user studies, most solutions address only facets
of the experiment life cycle, make reproducibility difficult, or do not cater
to nuanced study designs or interactions. We introduce reVISit 2, a software
framework that supports visualization researchers at all stages of designing
and conducting browser-based user studies. ReVISit supports researchers in the
design, debug & pilot, data collection, analysis, and dissemination experiment
phases by providing both technical affordances (such as replay of participant
interactions) and sociotechnical aids (such as a mindfully maintained community
of support). It is a proven system that can be (and has been) used in
publication-quality studies -- which we demonstrate through a series of
experimental replications. We reflect on the design of the system via
interviews and an analysis of its technical dimensions. Through this work, we
seek to elevate the ease with which studies are conducted, improve the
reproducibility of studies within our community, and support the construction
of advanced interactive studies.

</details>


### [39] [Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective](https://arxiv.org/abs/2508.03969)
*Wei Xu*

Main category: cs.HC

TL;DR: 本章从人本人工智能（HCAI）的角度系统推进了人-人工智能交互（HAII）这一新兴交叉领域的研究，提出了人本HAII（HC-HAII）框架。


<details>
  <summary>Details</summary>
Motivation: 强调以人为核心的研究方法优于技术为中心的方法，为后续章节奠定基础。

Method: 介绍了HC-HAII方法论，包括人本方法、流程、跨学科团队和多层次设计范式。

Result: 提出了HC-HAII框架，并总结了关键研究挑战与未来方向。

Conclusion: 本章为本书提供了一个以HCAI方法为核心的HAII研究与应用基础框架。

Abstract: This chapter systematically promotes an emerging interdisciplinary field of
human-artificial intelligence interaction (human-AI interaction, HAII) from a
human-centered AI (HCAI) perspective. It introduces a framework of
human-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII
research and applications, emphasizing the importance of adopting a
human-centered approach over a technology-centered one. The chapter presents
the HC-HAII methodology, including human-centered methods, process,
interdisciplinary teams, and multi-level design paradigms. It also highlights
key research challenges and future directions. As the first chapter, this
chapter also provides a structural overview of this book, which brings together
contributions from an interdisciplinary community of researchers and
practitioners to advance the theory, methodology, and applications of HCAI in
diverse domains of HAII. The purpose of this chapter is to provide a
fundamental framework for this book, centered on HAII research and applications
based on the HCAI approach, which will pave the way for the content of
subsequent chapters.

</details>


### [40] [Managing Data for Scalable and Interactive Event Sequence Visualization](https://arxiv.org/abs/2508.03974)
*Sayef Azad Sakin,Katherine E. Isaacs*

Main category: cs.HC

TL;DR: 论文介绍了ESeMan系统，用于优化大规模并行事件序列的可视化交互性能，通过分层数据结构和智能缓存实现高精度渲染。


<details>
  <summary>Details</summary>
Motivation: 解决并行事件序列（如程序执行轨迹）可视化中因数据量增大导致的交互延迟问题，同时避免传统方法因降采样而牺牲精度。

Method: 提出ESeMan系统，结合分层数据结构和智能缓存技术，动态提供所需数据以实现快速且精确的可视化总结。

Result: ESeMan在多种程序执行轨迹上测试，性能优于现有方法，数据获取时间低于100ms且保持像素级精度。

Conclusion: ESeMan为事件序列可视化提供了一种高效且高精度的解决方案，并为未来性能评估提供了基准框架。

Abstract: Parallel event sequences, such as those collected in program execution traces
and automated manufacturing pipelines, are typically visualized as interactive
parallel timelines. As the dataset size grows, these charts frequently
experience lag during common interactions such as zooming, panning, and
filtering. Summarization approaches can improve interaction performance, but at
the cost of accuracy in representation. To address this challenge, we introduce
ESeMan (Event Sequence Manager), an event sequence management system designed
to support interactive rendering of timeline visualizations with tunable
accuracy. ESeMan employs hierarchical data structures and intelligent caching
to provide visualizations with only the data necessary to generate accurate
summarizations with significantly reduced data fetch time. We evaluate ESeMan's
query times against summed area tables, M4 aggregation, and statistical
sub-sampling on a variety of program execution traces. Our results demonstrate
ESeMan provides better performance, achieving sub-100ms fetch times while
maintaining visualization accuracy at the pixel level. We further present our
benchmarking harness, enabling future performance evaluations for event
sequence visualization.

</details>


### [41] [SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions](https://arxiv.org/abs/2508.03980)
*Md Sabbir Ahmed,Arafat Rahman,Mark Rucker,Laura E. Barnes*

Main category: cs.HC

TL;DR: 提出了一种基于可穿戴设备的实时社交互动检测系统，利用迁移学习和语音特征捕捉面对面及虚拟互动，实验证明其效果。


<details>
  <summary>Details</summary>
Motivation: 现有系统在真实环境中通用性不足，难以捕捉多样化的社交互动。

Method: 开发实时腕戴系统，利用迁移学习检测前景语音（FS）并通过语音和对话线索（如低语）推断互动边界。

Result: 在11名参与者38天的真实评估中，系统互动检测准确率为73.18%；后续6名参与者召回率为100%。

Conclusion: 系统初步证明能有效捕捉日常互动，为社交焦虑等个性化干预提供基础。

Abstract: Social interactions are a fundamental part of daily life and play a critical
role in well-being. As emerging technologies offer opportunities to
unobtrusively monitor behavior, there is growing interest in using them to
better understand social experiences. However, automatically detecting
interactions, particularly via wearable devices, remains underexplored.
Existing systems are often limited to controlled environments, constrained to
in-person interactions, and rely on rigid assumptions such as the presence of
two speakers within a fixed time window. These limitations reduce their
generalizability to capture diverse real-world interactions. To address these
challenges, we developed a real-time, on-watch system capable of detecting both
in-person and virtual interactions. The system leverages transfer learning to
detect foreground speech (FS) and infers interaction boundaries based upon FS
and conversational cues like whispering. In a real-world evaluation involving
11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the
system achieved an interaction detection accuracy of 73.18%. Follow-up with six
participants indicated perfect recall for detecting interactions. These
preliminary findings demonstrate the potential of our system to capture
interactions in daily life, providing a foundation for applications such as
personalized interventions targeting social anxiety.

</details>


### [42] [StepWrite: Adaptive Planning for Speech-Driven Text Generation](https://arxiv.org/abs/2508.04011)
*Hamza El Alaoui,Atieh Taheri,Yi-Hao Peng,Jeffrey P. Bigham*

Main category: cs.HC

TL;DR: StepWrite是一款基于大型语言模型的语音交互系统，支持用户在移动中免提、免视撰写长文本，通过分解写作任务和提供情境感知的音频提示，显著降低认知负荷并提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统语音转文本系统和语音助手难以支持复杂情境下的长文本撰写，尤其是在用户无法视觉跟踪进度的移动场景中。

Method: StepWrite使用大型语言模型，将写作过程分解为子任务，并通过情境感知的非视觉音频提示逐步引导用户。

Result: 实证评估显示，StepWrite显著降低认知负荷，提升可用性和用户满意度，技术评估证实其能动态生成提示、准确对齐语气并有效进行事实核查。

Conclusion: StepWrite展示了情境感知语音交互在免提、免视的多任务日常场景中的潜力。

Abstract: People frequently use speech-to-text systems to compose short texts with
voice. However, current voice-based interfaces struggle to support composing
more detailed, contextually complex texts, especially in scenarios where users
are on the move and cannot visually track progress. Longer-form communication,
such as composing structured emails or thoughtful responses, requires
persistent context tracking, structured guidance, and adaptability to evolving
user intentions--capabilities that conventional dictation tools and voice
assistants do not support. We introduce StepWrite, a large language
model-driven voice-based interaction system that augments human writing ability
by enabling structured, hands-free and eyes-free composition of longer-form
texts while on the move. StepWrite decomposes the writing process into
manageable subtasks and sequentially guides users with contextually-aware
non-visual audio prompts. StepWrite reduces cognitive load by offloading the
context-tracking and adaptive planning tasks to the models. Unlike baseline
methods like standard dictation features (e.g., Microsoft Word) and
conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite
dynamically adapts its prompts based on the evolving context and user intent,
and provides coherent guidance without compromising user autonomy. An empirical
evaluation with 25 participants engaging in mobile or stationary hands-occupied
activities demonstrated that StepWrite significantly reduces cognitive load,
improves usability and user satisfaction compared to baseline methods.
Technical evaluations further confirmed StepWrite's capability in dynamic
contextual prompt generation, accurate tone alignment, and effective fact
checking. This work highlights the potential of structured, context-aware voice
interactions in enhancing hands-free and eye-free communication in everyday
multitasking scenarios.

</details>


### [43] [VeriGUI: Verifiable Long-Chain GUI Dataset](https://arxiv.org/abs/2508.04026)
*Shunyu Liu,Minghao Liu,Huichi Zhou,Zhenyu Cui,Yang Zhou,Yuhao Zhou,Wendong Fan,Ge Zhang,Jiajun Shi,Weihao Xuan,Jiaxing Huang,Shuang Luo,Fang Wu,Heli Qi,Qingcheng Zeng,Ziqi Ren,Jialiang Gao,Jindi Lv,Junjie Wang,Aosong Feng,Heng Zhou,Wangchunshu Zhou,Zhenfei Yin,Wenlong Zhang,Guohao Li,Wenhao Yu,Irene Li,Lei Ma,Lei Bai,Qunshu Lin,Mingli Song,Dacheng Tao*

Main category: cs.HC

TL;DR: VeriGUI是一个可验证的长链GUI数据集，旨在促进通用GUI代理在真实计算机环境中的开发和评估。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理研究多关注短期交互和结果验证，难以应对需要长期任务分解和执行的现实应用。

Method: 构建包含长链复杂性和子任务可验证性的数据集，涵盖桌面和网页GUI任务轨迹。

Result: 实验显示不同基础模型的GUI代理在长链任务上性能差异显著，表明需要更强大的规划和决策能力。

Conclusion: VeriGUI为GUI代理的长链任务处理能力提供了测试基准，并揭示了改进方向。

Abstract: Recent studies have delved into constructing autonomous agents capable of
performing complex Graphical User Interface (GUI)-based computer tasks, with
the potential to revolutionize human-computer interaction. Despite encouraging
results, existing efforts mainly focus on short-term interactions and rely on
outcome-only verification, thereby limiting their scalability in real-world GUI
applications that demand long-horizon task decomposition and execution. In this
work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed
to facilitate the development and evaluation of generalist GUI agents operating
in realistic computer environments. Our dataset emphasizes two critical
dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of
interdependent subtasks spanning hundreds of steps, explicitly designed to
allow any subtask to serve as a valid starting point; and (2) subtask-level
verifiability, which enables diverse exploration strategies within each
subtask, while ensuring that each subtask-level goal remains verifiable and
consistent. The dataset consists of GUI task trajectories across both desktop
and web, annotated by human experts. Extensive experiments on VeriGUI using
various agents with different foundation models reveal significant performance
gaps in handling long-horizon tasks, highlighting the need for more robust
planning and decision-making capabilities in GUI agents.

</details>


### [44] [XARP Tools: An Extended Reality Platform for Humans and AI Agents](https://arxiv.org/abs/2508.04108)
*Arthur Caetano,Misha Sra*

Main category: cs.HC

TL;DR: XARP Tools是一个扩展现实（XR）框架，为人类和AI开发者提供支持，包含服务器端Python库和平台特定的XR客户端。


<details>
  <summary>Details</summary>
Motivation: 为简化XR开发并整合AI与XR设备的交互。

Method: 通过Python库提供高级API，与客户端通过WebSockets的JSON协议通信，封装设备细节以实现低延迟交互。

Result: 支持三种使用方式：简化人类开发的库、供AI驱动的交互工具，以及连接XR设备与AI生态的协议服务器。

Conclusion: XARP Tools成功实现了XR开发的多功能支持，并开源代码以促进社区使用。

Abstract: This technical report presents XARP Tools, an extended reality (XR) framework
designed for human and AI developers alike. XARP comprises a server-side Python
library and platform-specific XR clients. The library offers high-level APIs
and communicates with clients via a JSON-based protocol over WebSockets. XR
clients encapsulate device and runtime specifics, providing responsive,
low-latency user interaction. XARP can be utilized in three ways: (i) as a
library that abstracts XR development for humans; (ii) as a set of callable
tools that allow AI agents to drive on-the-fly interactions with users; and
(iii) as a Model Context Protocol server that plugs XR devices into AI
ecosystems. XARP code and working examples are released openly at
https://github.com/HAL-UCSB/xarp.

</details>


### [45] [DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment](https://arxiv.org/abs/2508.04160)
*Angela Locoro,Silvia Golia,Davide Falessi*

Main category: cs.HC

TL;DR: 本文提出了DRIVE-T方法，用于设计和评估数据可视化素养的测量工具，通过任务项的区分度和代表性来确定难度级别。


<details>
  <summary>Details</summary>
Motivation: 现有数据可视化素养测量工具在难度级别表达上的不足限制了测量效果的准确性。

Method: DRIVE-T方法包括三个步骤：(1) 为数据可视化任务项打标签，(2) 由独立评分者评定任务难度，(3) 通过多面Rasch模型分析评分数据。

Result: 该方法成功生成了基于区分度和代表性排序的难度级别，并通过试点研究验证了其有效性。

Conclusion: DRIVE-T为数据可视化素养的测量提供了可操作的方法，能够更好地反映测量构念的难度级别。

Abstract: The underspecification of progressive levels of difficulty in measurement
constructs design and assessment tests for data visualization literacy may
hinder the expressivity of measurements in both test design and test reuse. To
mitigate this problem, this paper proposes DRIVE-T (Discriminating and
Representative Items for Validating Expressive Tests), a methodology designed
to drive the construction and evaluation of assessment items. Given a data
vizualization, DRIVE-T supports the identification of task-based items
discriminability and representativeness for measuring levels of data
visualization literacy. DRIVE-T consists of three steps: (1) tagging task-based
items associated with a set of data vizualizations; (2) rating them by
independent raters for their difficulty; (3) analysing raters' raw scores
through a Many-Facet Rasch Measurement model. In this way, we can observe the
emergence of difficulty levels of the measurement construct, derived from the
discriminability and representativeness of task-based items for each data
vizualization, ordered into Many-Facets construct levels. In this study, we
show and apply each step of the methodology to an item bank, which models the
difficulty levels of a measurement construct approximating a latent construct
for data visualization literacy. This measurement construct is drawn from
semiotics, i.e., based on the syntax, semantics and pragmatics knowledge that
each data visualization may require to be mastered by people. The DRIVE-T
methodology operationalises an inductive approach, observable in a post-design
phase of the items preparation, for formative-style and practice-based
measurement construct emergence. A pilot study with items selected through the
application of DRIVE-T is also presented to test our approach.

</details>


### [46] [Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes](https://arxiv.org/abs/2508.04202)
*Abdulrhman Alorini,Yufeng Wu,Abdullah Bin Sawad,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: 研究了沙特阿拉伯用户如何在使用智能音箱时应对隐私问题，揭示了他们的隐私保护行为及其背后的文化因素。


<details>
  <summary>Details</summary>
Motivation: 探索非西方文化背景下智能音箱的隐私问题，补充现有研究中未充分探讨的地区和文化因素。

Method: 采用文化探针和半结构化访谈，收集了16位参与者的数据，分析其隐私保护行为及其成因。

Result: 发现用户通过拔掉设备、静音麦克风或避免语音交互来保护隐私，这些行为受家庭规范、房间配置和人际动态影响。

Conclusion: 研究补充了全球智能音箱隐私讨论，提出了文化响应的设计方向，促进更包容的人机交互实践。

Abstract: Smart speakers are increasingly integrated into domestic life worldwide, yet
their privacy risks remain underexplored in non-Western cultural contexts. This
study investigates how Saudi Arabian users of smart speakers navigate privacy
concerns within collectivist, gendered, and often multigenerational households.
Using cultural probes followed by semi-structured interviews with 16
participants, we uncover everyday privacy-protective behaviours including
unplugging devices, muting microphones, and avoiding voice interactions
altogether. These practices are shaped not only by individual risk perceptions
but also by household norms, room configurations, and interpersonal dynamics.
We contribute empirical insights from an underrepresented region, theoretical
extensions to contextual integrity frameworks, and design directions for
culturally responsive voice interfaces. This work expands the global
conversation on smart speaker privacy and informs more inclusive HCI practices
in increasingly diverse smart home environments.

</details>


### [47] [Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows](https://arxiv.org/abs/2508.04357)
*Gloria Fernández-Nieto,Vanessa Echeverria,Yuheng Li,Yi-Shan Tsai,Lele Sha,Guanliang Chen,Dragan Gasevic,Zachari Swiecki*

Main category: cs.HC

TL;DR: 该论文提出了一种名为VPR的设计方法，结合了SPM、知识管理和叙事技术，通过可视化将专家工作流数据转化为直观的流程表示，并评估了其效果。


<details>
  <summary>Details</summary>
Motivation: 高等教育机构面临高员工流动率导致的知识流失问题，传统的工作流记录方式耗时且分散专家精力，需要自动化解决方案。

Method: 采用SPM技术分析日志数据，结合VPR设计方法，将专家工作流转化为可视化表示，并通过实验比较不同视觉表现形式的效果。

Result: 研究发现VPR能提升任务表现、可用性和参与度，尤其是富视觉化版本，但对流程记忆性和任务时间的改善有限。

Conclusion: VPR在可视化工作流和支持新手教师方面具有潜力，但需进一步优化以增强其效果。

Abstract: Knowledge Management is crucial for capturing and transferring expertise
within universities, especially in high staff turnover contexts where expertise
loss disrupts teaching. Documenting teachers' workflows is time-intensive and
diverts experts from core responsibilities. Sequential Pattern Mining (SPM)
leverages log data to identify expert workflows, offering an automated
alternative to represent workflows but requiring transformation into intuitive
formats for novice educators. This paper introduces Visual Process
Representations (VPR), a design approach combining SPM, Knowledge Management
processes, and storytelling techniques to convert expert log data into clear
visualisations. We detail the design phases and report a study evaluating
visual affordances (text lists vs. pictorial-style) and teachers' perceptions
of four versions of the VPR with 160 higher teachers on Prolific. Results
indicate improved task performance, usability, and engagement, particularly
with enriched visuals, though process memorability and task time improvements
were limited. The findings highlight VPR's potential to visualise workflows and
support novice educators.

</details>


### [48] [GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design](https://arxiv.org/abs/2508.04377)
*Gloria Fernández-Nieto,Lele Sha,Yuheng Li,Yi-Shan Tsai,Guanliang Chen,Yinwei Wei,Weiqing Wang,Jinchun Wen,Shaveen Singh,Ivan Silva,Yuanfang Li,Dragan Gasěvić,Zachari Swiecki*

Main category: cs.HC

TL;DR: 本文介绍了GoldMind知识管理系统的设计与评估，通过为期两年的以用户为中心的研究，探讨了教师在数字教学任务中的知识管理需求，并总结了技术、设计和人因三方面的关键发现。


<details>
  <summary>Details</summary>
Motivation: 高等教育中知识管理系统（KMS）的设计需解决复杂的人机交互问题，尤其是在员工流动和角色变化导致知识重用挑战的情况下。现有系统常忽视教育工作者的实际工作流程，导致低采纳率和有限影响。

Method: 通过为期两年的以用户为中心的设计研究，与108名高等教育教师合作，进行了GoldMind系统的迭代共设计和评估，包括三个设计-评估循环。

Result: 研究从三个主题总结关键发现：(1)用户交互数据的技术教训，(2)共设计和可用性测试形成的设计考量，(3)通过认知网络分析揭示的认知负荷和知识行为等人因因素。

Conclusion: 通过研究强调了KMS设计中需关注教育工作者实际需求，并为未来系统开发提供了基于实证的指导。

Abstract: Designing Knowledge Management Systems (KMSs) for higher education requires
addressing complex human-technology interactions, especially where staff
turnover and changing roles create ongoing challenges for reusing knowledge.
While advances in process mining and Generative AI enable new ways of designing
features to support knowledge management, existing KMSs often overlook the
realities of educators' workflows, leading to low adoption and limited impact.
This paper presents findings from a two-year human-centred design study with
108 higher education teachers, focused on the iterative co-design and
evaluation of GoldMind, a KMS supporting in-the-flow knowledge management
during digital teaching tasks. Through three design-evaluation cycles, we
examined how teachers interacted with the system and how their feedback
informed successive refinements. Insights are synthesised across three themes:
(1) Technology Lessons from user interaction data, (2) Design Considerations
shaped by co-design and usability testing, and (3) Human Factors, including
cognitive load and knowledge behaviours, analysed using Epistemic Network
Analysis.

</details>


### [49] [Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis](https://arxiv.org/abs/2508.04391)
*Ze Gao,Mengyao Guo,Zheng Wang,Xiaolin Zhang,Sihuang Man*

Main category: cs.HC

TL;DR: 数字生态艺术探讨了生物媒介与虚拟环境的融合。研究提出BCTI框架，促进植物为中心的创作模式，通过多模态案例验证其在元宇宙中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统艺术框架未系统指导艺术家利用植物主体性进行数字共生创作，需新方法超越人类中心主义。

Method: 提出Biocentric-Creation Transformation Ideology (BCTI)框架，并通过2013-2023年的生物艺术、NFT和VR生态系统案例验证。

Result: 研究发现元宇宙中植物-算法共创增长133%，区块链DAO实现植物主导协作，VR实时生物数据重塑生态美学。

Conclusion: BCTI框架为后人类世创作提供蓝图，重新定义虚拟环境意识并建立跨物种数字协作新协议。

Abstract: Digital ecological art represents an emergent frontier where biological media
converge with virtual environments. This study examines the paradigm shift from
anthropocentric to plant-centered artistic narratives within the metaverse,
contextualizing how digital platforms transform ecological expression. However,
current frameworks fail to systematically guide artists in leveraging plant
agency for digital symbiosis that transcends human-centered creation. We
propose the Biocentric-Creation Transformation Ideology (BCTI) framework and
validate it through multimodal case studies spanning bio-art, NFTs, and VR
ecosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable
unprecedented plant-algorithm co-creation, with biological artworks increasing
by 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests
through blockchain DAOs where plants govern human-plant collaborations; (3)
Algorithmic photosynthesis in VR environments reshapes ecological aesthetics
through real-time biodata translation. The BCTI framework advances ecological
art theory by systematizing the transition from representation to
plant-centered agency, offering artists a blueprint for post-anthropocene
creation. This redefines environmental consciousness in virtual realms while
establishing new protocols for cross-species digital collaboration.

</details>


### [50] [Measuring Information Richness in Product Images: Implications for Online Sales](https://arxiv.org/abs/2508.04541)
*Zhu Yuting,Cao Xinyu,Su Yuzhuo,Ma Yongbin*

Main category: cs.HC

TL;DR: 论文提出了一种衡量图像集信息丰富度的新指标k-value，并通过实验验证了其对消费者购买决策的影响。


<details>
  <summary>Details</summary>
Motivation: 电商卖家常面临选择产品图片的挑战，需要量化图片集的信息丰富度对购买决策的影响。

Method: 利用Vision Transformers的嵌入和k-means聚类定义k-value，并通过在线购物实验验证。

Result: k-value与人类感知的信息丰富度一致，但信息更丰富的图片集虽缩短决策时间却降低购买倾向。

Conclusion: 研究揭示了视觉信息丰富度与消费者行为的复杂关系，为卖家提供了量化工具。

Abstract: A common challenge for e-commerce sellers is to decide what product images to
display on online shopping sites. In this paper, we propose and validate a
novel metric, k-value, to quantify the information richness of an image set,
and we further investigate its effect on consumers' purchase decisions. We
leverage patch-level embeddings from Vision Transformers (ViT) and apply
k-means clustering to identify distinct visual features, defining k-value as
the number of clusters. An online experiment demonstrates that k-value aligns
with human-perceived information richness, validating the metric. A simulated
online shopping experiment further reveals a significant yet counterintuitive
result: while an image set with a higher k-value (richer information) shortens
decision time, it paradoxically reduces purchase propensity. Our findings
illuminate the complex relationship between visual information richness and
consumer behavior, providing sellers a quantifiable tool for image selection.

</details>


### [51] [VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations](https://arxiv.org/abs/2508.04634)
*Mohammed Almutairi,Charles Chiang,Haoze Guo,Matthew Belcher,Nandini Banerjee,Maria Milkowski,Svitlana Volkova,Daniel Nguyen,Tim Weninger,Michael Yankoski,Trenton W. Ford,Diego Gomez-Zara*

Main category: cs.HC

TL;DR: VirtLab是一个基于多智能体的团队模拟系统，用于研究复杂环境中的团队协作行为，支持定制化和大规模模拟，解决了现有框架在灵活性和空间设置上的限制。


<details>
  <summary>Details</summary>
Motivation: 研究团队协作行为需要灵活且可定制的模拟工具，而现有框架在设计和技术上存在局限性。

Method: 开发了VirtLab系统，包含模拟引擎和网络界面，支持非技术用户进行团队模拟。

Result: 系统通过对比真实数据与模拟场景验证了其实用性。

Conclusion: VirtLab为研究团队协作行为提供了高效且易用的工具，弥补了现有技术的不足。

Abstract: Simulating how team members collaborate within complex environments using
Agentic AI is a promising approach to explore hypotheses grounded in social
science theories and study team behaviors. We introduce VirtLab, a
user-friendly, customizable, multi-agent, and scalable team simulation system
that enables testing teams with LLM-based agents in spatial and temporal
settings. This system addresses the current frameworks' design and technical
limitations that do not consider flexible simulation scenarios and spatial
settings. VirtLab contains a simulation engine and a web interface that enables
both technical and non-technical users to formulate, run, and analyze team
simulations without programming. We demonstrate the system's utility by
comparing ground truth data with simulated scenarios.

</details>


### [52] [How are CS students using resources and AI tools for coding tasks?](https://arxiv.org/abs/2508.04667)
*Natalia Echeverry,Arun Lekshmi Narayanan*

Main category: cs.HC

TL;DR: 针对26名计算机科学学生的调查显示，AI编程助手主要用于写代码（仅次于在线搜索），而AI聊天机器人是调试的首选资源。不同编程经验的学生更倾向于在线帮助而非直接寻求同行或教师的帮助。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解学生在编程过程中对不同资源的偏好，以及AI工具在编程任务中的使用情况。

Method: 通过对26名计算机科学学生进行问卷调查，分析他们在编程和调试任务中对AI工具和其他资源的偏好。

Result: 结果显示，AI编程助手主要用于写代码，而AI聊天机器人是调试的主要工具；学生普遍更倾向于使用在线资源而非直接向他人求助。

Conclusion: AI工具在编程学习中扮演重要角色，尤其是在写代码和调试任务中。学生的资源偏好表明他们更依赖在线和AI工具而非传统的人际互动。

Abstract: A survey of 26 CS students reveals that AI coding assistants are mainly used
for writing code (second to online searches) while AI chatbots are the top
resource for debugging. Participants with different coding experience prefer
online help over direct human help from peers and instructors.

</details>


### [53] [MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models](https://arxiv.org/abs/2508.04679)
*Amit Kumar Das,Klaus Mueller*

Main category: cs.HC

TL;DR: MisVisFix是一个交互式仪表板，利用Claude和GPT模型检测、解释和纠正误导性可视化，准确率为96%，并提供详细解释和自动生成修正图表。


<details>
  <summary>Details</summary>
Motivation: 当前利用大型语言模型检测可视化误导的研究虽多，但缺乏实用的解释和纠正工具，MisVisFill旨在填补这一空白。

Method: 结合Claude和GPT模型，通过交互式仪表板支持全流程的检测、解释和纠正，包括自动生成修正图表和交互式聊天功能。

Result: MisVisFix准确识别96%的可视化问题，涵盖74种已知误导类型，并提供详细解释和修正建议，用户研究表明其效果显著。

Conclusion: MisVisFix通过交互式平台提升可视化素养，支持更可信的数据传播，为LLM检测工具的实际应用提供了新方向。

Abstract: Misleading visualizations pose a significant challenge to accurate data
interpretation. While recent research has explored the use of Large Language
Models (LLMs) for detecting such misinformation, practical tools that also
support explanation and correction remain limited. We present MisVisFix, an
interactive dashboard that leverages both Claude and GPT models to support the
full workflow of detecting, explaining, and correcting misleading
visualizations. MisVisFix correctly identifies 96% of visualization issues and
addresses all 74 known visualization misinformation types, classifying them as
major, minor, or potential concerns. It provides detailed explanations,
actionable suggestions, and automatically generates corrected charts. An
interactive chat interface allows users to ask about specific chart elements or
request modifications. The dashboard adapts to newly emerging misinformation
strategies through targeted user interactions. User studies with visualization
experts and developers of fact-checking tools show that MisVisFix accurately
identifies issues and offers useful suggestions for improvement. By
transforming LLM-based detection into an accessible, interactive platform,
MisVisFix advances visualization literacy and supports more trustworthy data
communication.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [54] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: RLGS是一种基于强化学习的自适应超参数调优框架，显著提升了3D高斯泼溅（3DGS）的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS中的超参数调优过程繁琐且依赖专家经验，导致重建结果不一致且效果欠佳。

Method: 提出RLGS框架，通过轻量级策略模块动态调整学习率和密集化阈值等关键超参数，无需修改现有3DGS架构。

Result: RLGS在多种3DGS变体（如Taming-3DGS和3DGS-MCMC）和数据集上表现稳健，提升渲染质量（如PSNR提高0.7dB）。

Conclusion: RLGS为3DGS训练中的超参数调优提供了高效通用的自动化解决方案，填补了强化学习在3DGS中的应用空白。

Abstract: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

</details>


### [55] [Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research](https://arxiv.org/abs/2508.04326)
*Ke Li,Mana Masuda,Susanne Schmidt,Shohei Mori*

Main category: cs.GR

TL;DR: 本文分析了辐射场（RF）在XR领域的研究现状，包括愿景、实际应用与剩余的研究缺口，并通过对365篇相关文献的系统调查，为XR社区提供了导航资源。


<details>
  <summary>Details</summary>
Motivation: 尽管RF技术（如3DGS和NeRF）在交互式逼真视图合成方面取得了重大突破，但RF在XR社区中的贡献仍然不足。本研究旨在探究RF在XR中的应用前景与实际进展。

Method: 系统调查了365篇与RF相关的XR文献，并详细分析了其中66篇，以回答RF在XR中的愿景、实现方式及研究缺口。

Result: 研究扩展并定位了RF在XR领域的研究主题，为XR社区提供了快速发展的RF研究导航资源。

Conclusion: 本文填补了RF在XR领域的研究空白，并为未来研究提供了方向与资源。

Abstract: The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.

</details>


### [56] [Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds](https://arxiv.org/abs/2508.04508)
*Haodong Zhu,Changbai Li,Yangyang Ren,Zichao Feng,Xuhui Liu,Hanlin Chen,Xiantong Zhen,Baochang Zhang*

Main category: cs.GR

TL;DR: Surf3R是一种无需相机校准的多视图3D重建方法，通过多分支和解码架构在10秒内完成场景重建，性能优异。


<details>
  <summary>Details</summary>
Motivation: 当前多视图3D重建方法依赖复杂的相机校准和位姿估计，限制了实际应用。

Method: 采用多分支和多视图解码架构，结合分支级处理、跨视图注意力和分支间融合，无需相机校准；引入D-Normal正则化器优化3D几何。

Result: 在ScanNet++和Replica数据集上达到最佳性能，展示出卓越的泛化能力和效率。

Conclusion: Surf3R为无需相机校准的快速3D重建提供了高效解决方案，性能优于现有方法。

Abstract: Current multi-view 3D reconstruction methods rely on accurate camera
calibration and pose estimation, requiring complex and time-intensive
pre-processing that hinders their practical deployment. To address this
challenge, we introduce Surf3R, an end-to-end feedforward approach that
reconstructs 3D surfaces from sparse views without estimating camera poses and
completes an entire scene in under 10 seconds. Our method employs a
multi-branch and multi-view decoding architecture in which multiple reference
views jointly guide the reconstruction process. Through the proposed
branch-wise processing, cross-view attention, and inter-branch fusion, the
model effectively captures complementary geometric cues without requiring
camera calibration. Moreover, we introduce a D-Normal regularizer based on an
explicit 3D Gaussian representation for surface reconstruction. It couples
surface normals with other geometric parameters to jointly optimize the 3D
geometry, significantly improving 3D consistency and surface detail accuracy.
Experimental results demonstrate that Surf3R achieves state-of-the-art
performance on multiple surface reconstruction metrics on ScanNet++ and Replica
datasets, exhibiting excellent generalization and efficiency.

</details>


### [57] [MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics](https://arxiv.org/abs/2508.04687)
*Ye Pan,Ruisi Zhang,Jingying Wang,Nengfu Chen,Yilin Qiu,Yu Ding,Kenny Mitchell*

Main category: cs.GR

TL;DR: 论文提出了结合传统blendshape动画技术和机器学习模型的方法，实现了非实时和实时的3D风格化角色动画系统，在感知效果和几何一致性上优于商业产品Faceware。


<details>
  <summary>Details</summary>
Motivation: 通过改进基于性能的动画技术，实现更具感知效果的3D风格化角色动画，提升动画的表现力和效率。

Method: 结合blendshape技术与多个机器学习模型，提出非实时（3D情感迁移网络）和实时（blendshape适应网络）解决方案。

Result: 系统生成的动画在识别度、强度和吸引力上的评分显著高于Faceware商业产品，具有更高的几何一致性和时间稳定性。

Conclusion: 该方法可集成到动画制作流程中，帮助动画师更快速、准确地生成所需表情，提升动画制作效率。

Abstract: Our purpose is to improve performance-based animation which can drive
believable 3D stylized characters that are truly perceptual. By combining
traditional blendshape animation techniques with multiple machine learning
models, we present both non-real time and real time solutions which drive
character expressions in a geometrically consistent and perceptually valid way.
For the non-real time system, we propose a 3D emotion transfer network makes
use of a 2D human image to generate a stylized 3D rig parameters. For the real
time system, we propose a blendshape adaption network which generates the
character rig parameter motions with geometric consistency and temporally
stability. We demonstrate the effectiveness of our system by comparing to a
commercial product Faceware. Results reveal that ratings of the recognition,
intensity, and attractiveness of expressions depicted for animated characters
via our systems are statistically higher than Faceware. Our results may be
implemented into the animation pipeline, and provide animators with a system
for creating the expressions they wish to use more quickly and accurately.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [58] [FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication](https://arxiv.org/abs/2508.03760)
*Qingyuan Li,Bo Zhang,Hui Kang,Tianhao Xu,Yulei Qian,Yuchen Xie,Lin Ma*

Main category: cs.DC

TL;DR: 论文提出FlashCommunication V2，通过比特分割和尖峰保留技术，实现任意比特宽度的跨GPU高效传输，显著提升通信系统的灵活性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 分布式训练和部署大型语言模型时，通信瓶颈成为关键挑战，需要新的通信范式来解决低比特量化的挑战。

Method: 引入比特分割技术将不规则比特宽度分解为基础单元，尖峰保留技术保留数值异常点为浮点数以减少动态范围，兼容硬件能力。

Result: 在NVLink和PCIe架构下，实现AllReduce通信最高3.2倍加速和All2All通信2倍加速。

Conclusion: FlashCommunication V2通过软硬件协同设计，显著提升通信效率并降低开销。

Abstract: Nowadays, communication bottlenecks have emerged as a critical challenge in
the distributed training and deployment of large language models (LLMs). This
paper introduces FlashCommunication V2, a novel communication paradigm enabling
efficient cross-GPU transmission at arbitrary bit widths. Its core innovations
lie in the proposed bit splitting and spike reserving techniques, which address
the challenges of low-bit quantization. Bit splitting decomposes irregular bit
widths into basic units, ensuring compatibility with hardware capabilities and
thus enabling transmission at any bit width. Spike reserving, on the other
hand, retains numerical outliers (i.e., minima and maxima) as floating-point
numbers, which shrinks the dynamic numerical range and pushes the quantization
limits to 2-bit with acceptable losses. FlashCommunication V2 significantly
enhances the flexibility and resource utilization of communication systems.
Through meticulous software-hardware co-design, it delivers robust performance
and reduced overhead across both NVLink-based and PCIe-based architectures,
achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All
communication.

</details>


### [59] [Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training](https://arxiv.org/abs/2508.03854)
*Xin Zhang,Quanyu Zhu,Liangbei Xu,Zain Huda,Wang Zhou,Jin Fang,Dennis van der Staay,Yuxi Hu,Jade Nie,Jiyan Yang,Chunzhi Yang*

Main category: cs.DC

TL;DR: 提出了一种新的二维稀疏并行方法，通过结合数据并行和模型并行，解决了大规模推荐模型训练中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型中的稀疏嵌入表存在内存限制和扩展性问题，传统并行策略面临不平衡、通信负载高等挑战。

Method: 提出二维稀疏并行方法，结合数据并行和模型并行，减少内存消耗和通信开销，并开发了动量缩放的行级AdaGrad算法以优化性能。

Result: 实验表明，该方法在4K GPU上实现了接近线性的训练速度提升，同时保持模型性能。

Conclusion: 该方法显著提升了推荐模型训练的效率，为大规模推荐系统设定了新的性能标杆。

Abstract: The increasing complexity of deep learning recommendation models (DLRM) has
led to a growing need for large-scale distributed systems that can efficiently
train vast amounts of data. In DLRM, the sparse embedding table is a crucial
component for managing sparse categorical features. Typically, these tables in
industrial DLRMs contain trillions of parameters, necessitating model
parallelism strategies to address memory constraints. However, as training
systems expand with massive GPUs, the traditional fully parallelism strategies
for embedding table post significant scalability challenges, including
imbalance and straggler issues, intensive lookup communication, and heavy
embedding activation memory. To overcome these limitations, we propose a novel
two-dimensional sparse parallelism approach. Rather than fully sharding tables
across all GPUs, our solution introduces data parallelism on top of model
parallelism. This enables efficient all-to-all communication and reduces peak
memory consumption. Additionally, we have developed the momentum-scaled
row-wise AdaGrad algorithm to mitigate performance losses associated with the
shift in training paradigms. Our extensive experiments demonstrate that the
proposed approach significantly enhances training efficiency while maintaining
model performance parity. It achieves nearly linear training speed scaling up
to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model
training.

</details>


### [60] [Reputation-based partition scheme for IoT security](https://arxiv.org/abs/2508.03981)
*Zhikui Chen,Muhammad Zeeshan Haider,Naiwen Luo,Shuo Yu,Xu Yuan,Yaochen Zhang,Tayyaba Noreen*

Main category: cs.DC

TL;DR: 本文提出了一种基于信誉的分区方案（RSPC），以解决众包感知中的安全和扩展性问题，并通过实验验证了其高效性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着智能终端的普及，众包感知成为重要数据聚合范式，但集中管理带来安全漏洞和扩展性问题。

Method: RSPC结合节点信誉值计算最优分区大小，并定期重组网络以避免攻击，同时提出四阶段确认协议确保跨分区交易安全。

Result: 实验表明，RSPC在可扩展性、低延迟和高吞吐量方面表现优异。

Conclusion: RSPC为众包感知提供了安全高效的解决方案。

Abstract: With the popularity of smart terminals, such as the Internet of Things,
crowdsensing is an emerging data aggregation paradigm, which plays a pivotal
role in data-driven applications. There are some key issues in the development
of crowdsensing such as platform security and privacy protection. As the
crowdsensing is usually managed by a centralized platform, centralized
management will bring various security vulnerabilities and scalability issues.
To solve these issues, an effective reputation-based partition scheme (RSPC) is
proposed in this article. The partition scheme calculates the optimal partition
size by combining the node reputation value and divides the node into several
disjoint partitions according to the node reputation value. By selecting the
appropriate partition size, RSPC provides a mechanism to ensure that each
partition is valid, as long as themaximum permissible threshold for the failed
node is observed. At the same time, the RSPC reorganizes the network
periodically to avoid partition attacks. In addition, for cross-partition
transactions, this paper innovatively proposes a four-stage confirmation
protocol to ensure the efficient and safe completion of cross-partition
transactions. Finally, experiments show that RSPC improves scalability, low
latency, and high throughput for crowdsensing.

</details>


### [61] [High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines](https://arxiv.org/abs/2508.03984)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: cs.DC

TL;DR: 该论文提出了利用低精度矩阵引擎模拟高精度矩阵乘法（SGEMM和DGEMM）的新方法，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有低精度矩阵引擎在深度学习中的高性能表现激发了对其模拟高精度矩阵乘法的研究需求。

Method: 论文提出了新型模拟方法，通过优化低精度矩阵引擎的使用，实现高精度矩阵乘法的加速和能效提升。

Result: 在GH200 Grace Hopper Superchip上，DGEMM模拟比原生方法提速1.4倍、能效提升43%；SGEMM模拟提速3.0倍、能效提升154%。

Conclusion: 新方法在性能与能效上远超传统模拟方法，为高精度矩阵乘法提供了高效解决方案。

Abstract: Recent architectures integrate high-performance and power-efficient matrix
engines. These engines demonstrate remarkable performance in low-precision
matrix multiplication, which is crucial in deep learning. Several techniques
have been proposed to emulate single- and double-precision general
matrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such
low-precision matrix engines. In this study, we present emulation methods that
significantly outperforms conventional approaches. On a GH200 Grace Hopper
Superchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\%
improvement in power efficiency compared to native DGEMM for sufficiently large
problems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\%
improvement in power efficiency compared to native SGEMM for sufficiently large
problems. Furthermore, compared to conventional emulation methods, the proposed
emulation achieves more than 2x higher performance and superior power
efficiency.

</details>


### [62] [Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability](https://arxiv.org/abs/2508.04000)
*Tayyaba Noreen,Qiufen Xia,Muhammad Zeeshan Haider*

Main category: cs.DC

TL;DR: 本文提出了一种基于DAG的ADR协议，用于解决区块链的吞吐量和可扩展性问题，特别适合物联网应用。


<details>
  <summary>Details</summary>
Motivation: 区块链当前的吞吐量低、可扩展性差和高延迟问题限制了其在物联网等领域的应用，因此需要一种改进的协议。

Method: ADR协议采用DAG结构，通过节点排名、三步骤验证（节点认证、DAG账本构建和恶意节点过滤排名）来提升性能。

Result: 在100多个节点的模拟环境中，ADR相比现有DAG区块链（如IOTA和ByteBall）显著提高了吞吐量和网络活跃度。

Conclusion: ADR协议是一种高效的解决方案，适用于需要高吞吐量和可扩展性的物联网应用。

Abstract: In the past decade, blockchain has emerged as a promising solution for
building secure distributed ledgers and has attracted significant attention.
However, current blockchain systems suffer from limited throughput, poor
scalability, and high latency. Due to limitations in consensus mechanisms,
especially in managing node identities, blockchain is often considered
unsuitable for applications such as the Internet of Things (IoT). This paper
proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain
scalability and throughput. ADR employs a directed acyclic graph (DAG)
structure where nodes are positioned based on their rankings. Unlike
traditional chains, ADR allows honest nodes to write blocks and verify
transactions using a DAG-based topology. The protocol follows a three-step
approach to secure the network against double-spending and enhance performance.
First, it verifies nodes using their public and private keys before granting
entry. Second, it builds an advanced DAG ledger enabling block production and
transaction validation. Third, a ranking algorithm filters out malicious nodes,
ranks the remaining nodes based on performance, and arranges them
topologically. This process increases throughput and ensures robust
scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,
including scenarios with injected malicious nodes. Simulation results
demonstrate that ADR significantly improves transaction throughput and network
liveness compared to existing DAG-based blockchains such as IOTA and ByteBall,
making it well-suited for IoT applications.

</details>


### [63] [High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2508.04013)
*Sameh Abdulah,Mary Lai O. Salvana,Ying Sun,David E. Keyes,Marc G. Genton*

Main category: cs.DC

TL;DR: 论文探讨了统计计算（SC）社区如何融入高性能计算（HPC）环境，提出建立高性能统计计算（HPSC）社区的愿景。


<details>
  <summary>Details</summary>
Motivation: 统计计算社区虽开发了广泛使用的软件，但在HPC领域（如Top500或Green500平台）中参与较少，希望通过与HPC社区合作推动快速、可扩展的统计应用发展。

Method: 通过梳理SC的历史，分析其在HPC环境中的应用潜力、现有挑战和机遇，提出建设HPSC社区的路线图。

Result: 展示了SC社区如何通过技术和社区创新，将其优势融入HPC环境，实现高性能统计计算的目标。

Conclusion: 建立HPSC社区是可能的，但需SC与HPC社区的紧密合作，克服技术和文化障碍，以实现统计计算在高性能环境中的广泛应用。

Abstract: We recognize the emergence of a statistical computing community focused on
working with large computing platforms and producing software and applications
that exemplify high-performance statistical computing (HPSC). The statistical
computing (SC) community develops software that is widely used across
disciplines. However, it remains largely absent from the high-performance
computing (HPC) landscape, particularly on platforms such as those featured on
the Top500 or Green500 lists. Many disciplines already participate in HPC,
mostly centered around simulation science, although data-focused efforts under
the artificial intelligence (AI) label are gaining popularity. Bridging this
gap requires both community adaptation and technical innovation to align
statistical methods with modern HPC technologies. We can accelerate progress in
fast and scalable statistical applications by building strong connections
between the SC and HPC communities. We present a brief history of SC, a vision
for how its strengths can contribute to statistical science in the HPC
environment (such as HPSC), the challenges that remain, and the opportunities
currently available, culminating in a possible roadmap toward a thriving HPSC
community.

</details>


### [64] [SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning](https://arxiv.org/abs/2508.04265)
*Borui Li,Li Yan,Jianmin Liu*

Main category: cs.DC

TL;DR: 提出了一种名为SelectiveShield的轻量级混合防御框架，通过选择性同态加密和差分隐私结合，解决联邦学习中梯度泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有的防御机制（如差分隐私和同态加密）在隐私保护、模型效用和系统开销之间存在权衡，尤其是在异构环境中，问题更加突出。

Method: SelectiveShield利用Fisher信息量化参数敏感性，通过协作协商协议确定关键参数进行同态加密保护，非关键参数则采用自适应差分隐私噪声保护。

Result: 实验证明，SelectiveShield在保持模型效用的同时显著降低梯度泄漏风险，适用于实际联邦学习部署。

Conclusion: SelectiveShield提供了一种实用且可扩展的防御机制，有效解决了联邦学习中的隐私与效用平衡问题。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data but remains vulnerable to gradient leakage attacks that can reconstruct
sensitive user information. Existing defense mechanisms, such as differential
privacy (DP) and homomorphic encryption (HE), often introduce a trade-off
between privacy, model utility, and system overhead, a challenge that is
exacerbated in heterogeneous environments with non-IID data and varying client
capabilities. To address these limitations, we propose SelectiveShield, a
lightweight hybrid defense framework that adaptively integrates selective
homomorphic encryption and differential privacy. SelectiveShield leverages
Fisher information to quantify parameter sensitivity, allowing clients to
identify critical parameters locally. Through a collaborative negotiation
protocol, clients agree on a shared set of the most sensitive parameters for
protection via homomorphic encryption. Parameters that are uniquely important
to individual clients are retained locally, fostering personalization, while
non-critical parameters are protected with adaptive differential privacy noise.
Extensive experiments demonstrate that SelectiveShield maintains strong model
utility while significantly mitigating gradient leakage risks, offering a
practical and scalable defense mechanism for real-world federated learning
deployments.

</details>


### [65] [S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge](https://arxiv.org/abs/2508.04271)
*JinYi Yoon,JiHo Lee,Ting He,Nakjung Choi,Bo Ji*

Main category: cs.DC

TL;DR: S2M3是一种多模态架构，通过模块拆分和共享在边缘设备上实现多任务推理，减少资源占用并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态AI服务依赖云计算带来的带宽、延迟、隐私及可靠性问题，同时减轻边缘设备多任务的资源压力。

Method: 提出S2M3架构，拆分多模态模型的功能模块并共享通用模块；采用贪心算法进行模块级放置和并行路由。

Result: 在多任务场景下内存使用减少62%，延迟降低56.9%，准确性未受影响。

Conclusion: S2M3在资源受限设备上高效支持多模态多任务推理，优于云AI方案。

Abstract: With the advancement of Artificial Intelligence (AI) towards multiple
modalities (language, vision, speech, etc.), multi-modal models have
increasingly been used across various applications (e.g., visual question
answering or image generation/captioning). Despite the success of AI as a
service for multi-modal applications, it relies heavily on clouds, which are
constrained by bandwidth, latency, privacy concerns, and unavailability under
network or server failures. While on-device AI becomes popular, supporting
multiple tasks on edge devices imposes significant resource challenges. To
address this, we introduce S2M3, a split-and-share multi-modal architecture for
multi-task inference on edge devices. Inspired by the general-purpose nature of
multi-modal models, which are composed of multiple modules (encoder, decoder,
classifier, etc.), we propose to split multi-modal models at functional-level
modules; and then share common modules to reuse them across tasks, thereby
reducing resource usage. To address cross-model dependency arising from module
sharing, we propose a greedy module-level placement with per-request parallel
routing by prioritizing compute-intensive modules. Through experiments on a
testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,
we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in
single-task and multi-task settings, respectively, without sacrificing
accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95
instances (93.7%) while reducing inference latency by up to 56.9% on
resource-constrained devices, compared to cloud AI.

</details>


### [66] [Optimizing Microgrid Composition for Sustainable Data Centers](https://arxiv.org/abs/2508.04284)
*Julius Irion,Philipp Wiesner,Jonathan Bader,Odej Kao*

Main category: cs.DC

TL;DR: 提出了一种优化框架，用于评估数据中心微电网的长期可持续性和电力可靠性，结合计算与能源系统的协同模拟。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心能源需求增长和电网基础设施滞后，微电网的可再生能源与储能配置需要更有效的工具来评估其对可持续性和可靠性的影响。

Method: 扩展了Vessim协同模拟器，结合NREL的SAM模型，模拟计算负载、可再生能源生产与储能的交互，并通过多时间尺度的黑盒优化探索微电网配置。

Result: 框架为数据中心能源系统规划提供了更全面的评估工具，帮助优化微电网配置。

Conclusion: 该研究填补了现有工具在微电网可持续性和可靠性评估上的空白，为决策者提供了更科学的规划依据。

Abstract: As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.

</details>


### [67] [Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing](https://arxiv.org/abs/2508.04334)
*Noor Islam S. Mohammad*

Main category: cs.DC

TL;DR: 本文提出了一种结合深度强化学习和蚁群优化的混合调度算法，用于高效处理物联网-云系统中的动态负载和网络变化，显著降低了响应时间、提高了资源利用率和减少了能耗。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长导致大量异构数据流，现有调度方法难以适应动态负载和网络变化，亟需高效解决方案以满足延迟、能耗和服务质量需求。

Method: 结合深度强化学习和蚁群优化，前者学习动态任务分配策略，后者全局优化资源分配和负载均衡。

Result: 实验表明，算法平均响应时间降低18.4%，资源利用率提升12.7%，能耗减少9.3%，且严格遵守SLA。

Conclusion: 深度强化学习与群体智能的结合为物联网-云平台提供了一种可扩展、高效的调度方法。

Abstract: The rapid growth of Internet of Things (IoT) devices produces massive,
heterogeneous data streams, demanding scalable and efficient scheduling in
cloud environments to meet latency, energy, and Quality-of-Service (QoS)
requirements. Existing scheduling methods often lack adaptability to dynamic
workloads and network variability inherent in IoT-cloud systems. This paper
presents a novel hybrid scheduling algorithm combining deep Reinforcement
Learning (RL) and Ant Colony Optimization (ACO) to address these challenges.
The deep RL agent utilizes a model-free policy-gradient approach to learn
adaptive task allocation policies responsive to real-time workload fluctuations
and network states. Simultaneously, the ACO metaheuristic conducts a global
combinatorial search to optimize resource distribution, mitigate congestion,
and balance load across distributed cloud nodes. Extensive experiments on
large-scale synthetic IoT datasets, reflecting diverse workloads and QoS
constraints, demonstrate that the proposed method achieves up to 18.4%
reduction in average response time, 12.7% improvement in resource utilization,
and 9.3% decrease in energy consumption compared to leading heuristics and
RL-only baselines. Moreover, the algorithm ensures strict Service Level
Agreement (SLA) compliance through deadline-aware scheduling and dynamic
prioritization. The results confirm the effectiveness of integrating model-free
RL with swarm intelligence for scalable, energy-efficient IoT data scheduling,
offering a promising approach for next-generation IoT-cloud platforms.

</details>


### [68] [Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis](https://arxiv.org/abs/2508.04596)
*Chuan-Chi Lai,Yan-Lin Chen,Bo-Xin Liu,Chuan-Ming Liu*

Main category: cs.DC

TL;DR: 本文提出了一种基于边缘计算的并行不确定Skyline算法（EPUS），用于处理IoE中的大数据分析问题，显著降低了数据传输延迟和云计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 随着IoE的发展，数据量激增，传统的云计算模式在处理大数据时面临高延迟和高资源消耗的问题。边缘计算被提出以在数据上传至云端前进行预处理，从而减少网络传输和云端计算负担。

Method: 提出EPUS算法，利用Skyline候选集在并行边缘计算节点上修剪不太可能成为Skyline的数据，仅将必要信息传输至服务器更新全局Skyline，减少网络数据传输量。

Result: 仿真结果显示，EPUS算法在二维数据上处理延迟降低超过50%，在高维数据上也优于现有方法。

Conclusion: EPUS算法有效解决了IoE大数据分析中的延迟和资源消耗问题，是一种高效的低延迟解决方案。

Abstract: Due to the Internet of Everything (IoE), data generated in our life become
larger. As a result, we need more effort to analyze the data and extract
valuable information. In the cloud computing environment, all data analysis is
done in the cloud, and the client only needs less computing power to handle
some simple tasks. However, with the rapid increase in data volume, sending all
data to the cloud via the Internet has become more expensive. The required
cloud computing resources have also become larger. To solve this problem, edge
computing is proposed. Edge is granted with more computation power to process
data before sending it to the cloud. Therefore, the data transmitted over the
Internet and the computing resources required by the cloud can be effectively
reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline
(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the
concept of skyline candidate set to prune data that are less likely to become
the skyline data on the parallel edge computing nodes. With the candidate
skyline set, each edge computing node only sends the information required to
the server for updating the global skyline, which reduces the amount of data
that transfer over the internet. According to the simulation results, the
proposed method is better than two comparative methods, which reduces the
latency of processing two-dimensional data by more than 50%. For
high-dimensional data, the proposed EPUS method also outperforms the other
existing methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution](https://arxiv.org/abs/2508.03767)
*Sandeepa Kannangara,Arman Abrahamyan,Daniel Elias,Thomas Kilby,Nadav Dar,Luiz Pizzato,Anna Leontjeva,Dan Jermyn*

Main category: cs.DB

TL;DR: MERAI是一个高效且可扩展的实体解析（ER）管道，专为大规模数据集设计，通过其高准确性和扩展能力在实验中优于Dedupe和Splink。


<details>
  <summary>Details</summary>
Motivation: 实体解析在处理大规模数据集时仍具挑战性，MERAI旨在提供一个可扩展且高效的解决方案，以满足企业级需求。

Method: MERAI采用人工智能技术，构建了一个稳健的管道，专注于记录去重和链接，并通过与Dedupe和Splink的对比实验验证其性能。

Result: MERAI成功处理了高达1570万条记录的数据集，实验显示其在匹配准确性和F1分数上均优于基线系统。

Conclusion: MERAI为大规模实体解析提供了一个可靠且可扩展的解决方案，确保了实际应用中的数据完整性和一致性。

Abstract: Entity resolution (ER) remains a significant challenge in data management,
especially when dealing with large datasets. This paper introduces MERAI
(Massive Entity Resolution using AI), a robust and efficient pipeline designed
to address record deduplication and linkage issues in high-volume datasets at
an enterprise level. The pipeline's resilience and accuracy have been validated
through various large-scale record deduplication and linkage projects. To
evaluate MERAI's performance, we compared it with two well-known entity
resolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2
million records due to memory constraints, MERAI successfully processed
datasets of up to 15.7 million records and produced accurate results across all
experiments. Experimental data demonstrates that MERAI outperforms both
baseline systems in terms of matching accuracy, with consistently higher F1
scores in both deduplication and record linkage tasks. MERAI offers a scalable
and reliable solution for enterprise-level large-scale entity resolution,
ensuring data integrity and consistency in real-world applications.

</details>


### [70] [Raqlet: Cross-Paradigm Compilation for Recursive Queries](https://arxiv.org/abs/2508.03978)
*Amir Shaikhha,Youning Xia,Meisam Tarabkhah,Jazal Saleem,Anna Herlihy*

Main category: cs.DB

TL;DR: Raqlet是一个源代码到源代码的编译框架，旨在解决递归查询引擎在关系型、图型和演绎型系统中的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于SQL:2023的SQL/PGQ和GQL标准在不同系统中的实现不一致，需要一种方法来统一和跨范式翻译递归查询。

Method: Raqlet通过中间表示（IRs）将递归查询从Cypher或SQL/PGQ转换为PGIR、DLIR，再到SQIR，提供语义基础和静态分析支持。

Result: Raqlet提供了一个共享语义基础，可作为语言标准的参考实现，并支持性能优化。

Conclusion: 目标是使Raqlet成为支持跨范式快速原型设计、便携式递归查询和形式化推理的强大平台。

Abstract: We introduce Raqlet, a source-to-source compilation framework that addresses
the fragmentation of recursive querying engines spanning relational (recursive
SQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards
such as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for
querying graph data within relational and graph databases; however, real-world
support remains inconsistent across systems. Raqlet bridges this gap by
translating recursive queries across paradigms through leveraging intermediate
representations (IRs) grounded in well-defined semantics; it translates Cypher
or SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),
and finally to SQIR (inspired by recursive SQL). Raqlet provides a shared
semantic basis that can serve as a golden reference implementation for language
standards, while supporting static analysis and transformations (e.g.,
magic-set transformation) for performance tuning. Our vision is to make Raqlet
a robust platform that enables rapid cross-paradigm prototyping, portable
recursive queries, and formal reasoning about recursion even when targeting
diverse query execution engines.

</details>


### [71] [BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases](https://arxiv.org/abs/2508.04031)
*Lianggui Weng,Dandan Liu,Rong Zhu,Bolin Ding,Jingren Zhou*

Main category: cs.DB

TL;DR: BridgeScope是一种连接大语言模型（LLM）与数据库的通用工具包，通过模块化SQL操作、对齐权限与安全策略以及引入代理机制，解决了现有LLM与数据库交互中的实用性、安全性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与数据库的交互设计存在实用性、安全性和效率方面的局限，需要一种解决方案来优化这些方面。

Method: BridgeScope通过模块化SQL操作、对齐数据库权限与用户安全策略、引入代理机制来实现高效安全的LLM与数据库交互。

Result: 评估表明，BridgeScope显著提升了LLM操作数据库的效率，减少了80%的token使用，并支持数据密集型工作流。

Conclusion: BridgeScope为下一代智能数据自动化提供了坚实的基础。

Abstract: As large language models (LLMs) demonstrate increasingly powerful reasoning
and orchestration capabilities, LLM-based agents are rapidly proliferating for
complex data-related tasks. Despite this progress, the current design of how
LLMs interact with databases exhibits critical limitations in usability,
security, privilege management, and data transmission efficiency. To resolve
these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs
and databases through three key innovations. First, it modularizes SQL
operations into fine-grained tools for context retrieval, CRUD execution, and
ACID-compliant transaction management, enabling more precise and LLM-friendly
functionality controls. Second, it aligns tool implementations with both
database privileges and user security policies to steer LLMs away from unsafe
or unauthorized operations, improving task execution efficiency while
safeguarding database security. Third, it introduces a proxy mechanism for
seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All
of these designs are database-agnostic and can be transparently integrated with
existing agent architectures. We also release an open-source implementation of
BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate
that BridgeScope enables LLM agents to operate databases more effectively,
reduces token usage by up to 80% through improved security awareness, and
uniquely supports data-intensive workflows beyond existing toolkits,
establishing BridgeScope as a robust foundation for next-generation intelligent
data automation.

</details>


### [72] [Rethinking Analytical Processing in the GPU Era](https://arxiv.org/abs/2508.04701)
*Bobbi Yogatama,Yifei Yang,Kevin Kristensen,Devesh Sarda,Abigale Kim,Adrian Cockcroft,Yu Teng,Joshua Patterson,Gregory Kimball,Wes McKinney,Weiwei Gong,Xiangyao Yu*

Main category: cs.DB

TL;DR: 本文介绍了一个名为Sirius的原型开源GPU原生SQL引擎，通过利用GPU硬件和软件的最新进展，实现了对现有数据系统的无缝加速。


<details>
  <summary>Details</summary>
Motivation: 随着GPU内存增大、互联和IO速度提升以及成本下降，加上软件生态的成熟，GPU数据分析的广泛采用成为可能。

Method: Sirius利用libcudf等库实现高性能关系操作，并通过Substrait查询表示标准无缝替换CPU引擎。

Result: 在TPC-H测试中，Sirius在单节点上与DuckDB集成时实现了7倍加速，分布式环境下与Apache Doris集成时加速高达12.5倍。

Conclusion: Sirius展示了GPU作为数据分析主要引擎的潜力，为现有系统提供了高效的无缝加速方案。

Abstract: The era of GPU-powered data analytics has arrived. In this paper, we argue
that recent advances in hardware (e.g., larger GPU memory, faster interconnect
and IO, and declining cost) and software (e.g., composable data systems and
mature libraries) have removed the key barriers that have limited the wider
adoption of GPU data analytics. We present Sirius, a prototype open-source
GPU-native SQL engine that offers drop-in acceleration for diverse data
systems. Sirius treats GPU as the primary engine and leverages libraries like
libcudf for high-performance relational operators. It provides drop-in
acceleration for existing databases by leveraging the standard Substrait query
representation, replacing the CPU engine without changing the user-facing
interface. On TPC-H, Sirius achieves 7x speedup when integrated with DuckDB in
a single node at the same hardware rental cost, and up to 12.5x speedup when
integrated with Apache Doris in a distributed setting.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [73] [Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems](https://arxiv.org/abs/2508.03837)
*Davide Zoni,Andrea Galimberti,Adriano Guarisco*

Main category: cs.AR

TL;DR: Rhea是一个统一的框架，用于设计和验证RTL缓存一致性内存子系统，通过结合Verilator和gem5实现高效设计和验证，展示了其在多核系统中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代多核系统芯片架构中，设计和验证高效缓存一致性内存子系统复杂且重要，需要简化流程并提高验证准确性。

Method: Rhea框架生成可综合的RTL，支持多种架构参数，并集成Verilator和gem5的仿真工具，用于系统级验证。

Result: 在16核场景下，Rhea的性能介于gem5的MI和MOESI模型之间，仿真开销为1.6倍，但仿真保真度更高。

Conclusion: Rhea能够高效、可扩展地加速RTL缓存一致性内存子系统的开发，并提供了更高的硬件仿真准确性。

Abstract: Designing and validating efficient cache-coherent memory subsystems is a
critical yet complex task in the development of modern multi-core
system-on-chip architectures. Rhea is a unified framework that streamlines the
design and system-level validation of RTL cache-coherent memory subsystems. On
the design side, Rhea generates synthesizable, highly configurable RTL
supporting various architectural parameters. On the validation side, Rhea
integrates Verilator's cycle-accurate RTL simulation with gem5's full-system
simulation, allowing realistic workloads and operating systems to run alongside
the actual RTL under test. We apply Rhea to design MSI-based RTL memory
subsystems with one and two levels of private caches and scaling up to sixteen
cores. Their evaluation with 22 applications from state-of-the-art benchmark
suites shows intermediate performance relative to gem5 Ruby's MI and MOESI
models. The hybrid gem5-Verilator co-simulation flow incurs a moderate
simulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher
fidelity by simulating real RTL hardware. This overhead decreases with scale,
down to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's
effectiveness and scalability in enabling fast development of RTL
cache-coherent memory subsystem designs.

</details>


### [74] [FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead](https://arxiv.org/abs/2508.03866)
*Seock-Hwan Noh,Hoyeon Lee,Junkyum Kim,Junsu Im,Jay H. Park,Sungjin Lee,Sam H. Noh,Yeseong Kim,Jaeha Kung*

Main category: cs.AR

TL;DR: FlashVault是一种在4D V-NAND结构中嵌入可重构加密引擎的自加密架构，支持多种加密算法，无需外部加密芯片，性能和效率优于CPU和近核心处理架构。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统NAND闪存在数据加密上依赖外部芯片的问题，并满足多样化的加密需求。

Method: 在NAND芯片的未使用硅面积中嵌入加密引擎，支持块密码、公钥和后量子算法，实现片上自加密。

Result: 性能和功耗优于CPU和近核心架构，满足企业标准和监管要求。

Conclusion: FlashVault是一种高效的SSD安全架构，能够满足多种加密需求且无需额外芯片。

Abstract: We present FlashVault, an in-NAND self-encryption architecture that embeds a
reconfigurable cryptographic engine into the unused silicon area of a
state-of-the-art 4D V-NAND structure. FlashVault supports not only block
ciphers for data encryption but also public-key and post-quantum algorithms for
digital signatures, all within the NAND flash chip. This design enables each
NAND chip to operate as a self-contained enclave without incurring area
overhead, while eliminating the need for off-chip encryption. We implement
FlashVault at the register-transfer level (RTL) and perform place-and-route
(P&R) for accurate power/area evaluation. Our analysis shows that the power
budget determines the number of cryptographic engines per NAND chip. We
integrate this architectural choice into a full-system simulation and evaluate
its performance on a wide range of cryptographic algorithms. Our results show
that FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x)
and near-core processing architecture (1.02~2.01x), demonstrating its
effectiveness as a secure SSD architecture that meets diverse cryptographic
requirements imposed by regulatory standards and enterprise policies.

</details>


### [75] [TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads](https://arxiv.org/abs/2508.03900)
*Navaneeth Kunhi Purayil,Diyou Shen,Matteo Perotti,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出TROOP，通过硬件优化提升向量处理单元（VPEs）的L1内存带宽利用，实现接近理论性能（at-the-roofline），显著加速内存密集型任务。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的快速演进需要灵活高效的硬件解决方案，现有向量处理单元在低数据重用任务中性能不足。

Method: TROOP包括解耦的加载-存储接口、改进的向量链接、影子缓冲和地址置乱等技术优化VPE微架构。

Result: 在12nm工艺中实现TROOP，GEMV、DOTP和AXPY任务分别提速1.5x、2.2x和2.6x，能效提升45%，面积开销仅7%。

Conclusion: TROOP显著提高了VPEs在内存密集型任务中的性能和能效，同时保持高效能计算任务的竞争力。

Abstract: The fast evolution of Machine Learning (ML) models requires flexible and
efficient hardware solutions as hardwired accelerators face rapid obsolescence.
Vector processors are fully programmable and achieve high energy efficiencies
by exploiting data parallelism, amortizing instruction fetch and decoding
costs. Hence, a promising design choice is to build accelerators based on
shared L1-memory clusters of streamlined Vector Processing Elements (VPEs).
However, current state-of-the-art VPEs are limited in L1 memory bandwidth and
achieve high efficiency only for computational kernels with high data reuse in
the Vector Register File (VRF), such as General Matrix Multiplication (GEMM).
Performance is suboptimal for workloads with lower data reuse like General
Matrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at
the L1 memory interface, the VPE micro-architecture must be optimized to
achieve near-ideal utilization, i.e., to be as close as possible to the L1
memory roofline (at-the-roofline). In this work, we propose TROOP, a set of
hardware optimizations that include decoupled load-store interfaces, improved
vector chaining, shadow buffers to hide VRF conflicts, and address scrambling
techniques to achieve at-the-roofline performance for VPEs without compromising
their area and energy efficiency. We implement TROOP on an open-source
streamlined vector processor in a 12nm FinFET technology. TROOP achieves
significant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key
memory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline
performance. Additionally, TROOP enhances the energy efficiency by up to 45%,
reaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high
energy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area
overhead of less than 7%.

</details>


### [76] [OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite](https://arxiv.org/abs/2508.04106)
*Shan Shen,Xingyang Li,Zhuohua Liu,Yikai Wang,Yiheng Wu,Junhao Ma,Yuquan Sun,Wei W. Xing*

Main category: cs.AR

TL;DR: OpenYield是解决SRAM良率分析中学术与工业脱节问题的开源生态系统，提供真实SRAM电路生成器、标准化评估与优化平台。


<details>
  <summary>Details</summary>
Motivation: 学术模型过于简化，与工业复杂的现实脱节，导致研究成果难以在工业实践中复现与应用。

Method: 通过开源工具OpenYield，引入工业关键的二阶效应、漏电耦合等，并提供标准化评估与优化平台。

Result: OpenYield为学术与工业合作提供了基础，提升了SRAM设计的稳健性和效率。

Conclusion: OpenYield填补了学术与工业间的鸿沟，加速了内存设计的创新。

Abstract: Static Random-Access Memory (SRAM) yield analysis is essential for
semiconductor innovation, yet research progress faces a critical challenge: the
significant disconnect between simplified academic models and complex
industrial realities. The absence of open, realistic benchmarks has created a
reproducibility crisis, where promising academic techniques often fail to
translate to industrial practice. We present \textit{OpenYield}, a
comprehensive open-source ecosystem designed to address this critical gap
through three core contributions: (1) A realistic SRAM circuit generator that
uniquely incorporates critical second-order-effect parasitics, inter-cell
leakage coupling, and peripheral circuit variations, which are typically
omitted in academic studies but decisive in industrial designs. (2) A
standardized evaluation platform with a simple interface and implemented
baseline yield analysis algorithms, enabling fair comparisons and reproducible
research. (3) A standardized SRAM optimization platform, demonstrating
OpenYield's utility in enhancing SRAM design robustness and efficiency,
providing a comprehensive benchmark for optimization algorithms. OpenYield
creates a foundation for meaningful academia-industry collaboration,
accelerating innovation in memory design. The framework is publicly available
on \href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}

</details>


### [77] [ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs](https://arxiv.org/abs/2508.04516)
*Ishraq Tashdid,Dewan Saiham,Nafisa Anjum,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.AR

TL;DR: ECOLogic是一种混合设计范式，通过在ASIC中嵌入轻量级eFPGA结构，实现高性能、可更新和环保的计算。ECOScore框架指导RTL分区，在保持90% ASIC性能的同时，显著降低功耗和碳排放。


<details>
  <summary>Details</summary>
Motivation: 传统硬件平台（ASIC和FPGA）在性能、灵活性和可持续性之间存在矛盾。ASIC高效但不灵活，FPGA可重构但存在高功耗和性能开销。ECOLogic旨在提供一种既高性能又可更新且环保的解决方案。

Method: 提出ECOLogic混合设计范式，嵌入eFPGA结构于ASIC中，结合ECOScore框架评估IP适应性、安全性、性能容忍度和资源适配性，指导RTL分区。

Result: 在六种SoC模块中，ECOLogic平均保留90% ASIC性能（最高2 GHz），时序裕度9.8 ns（FPGA为5.1 ns），功耗平均降低480倍。碳排放比FPGA实现降低300至500倍。

Conclusion: ECOLogic是一种高性能、安全且环境可持续的下一代可重构系统解决方案。

Abstract: Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs
among performance, flexibility, and sustainability. ASICs provide high
efficiency but are inflexible post-fabrication, require costly re-spins for
updates, and expose IPs to piracy risks. FPGAs offer reconfigurability and
reuse, yet suffer from substantial area, power, and performance overheads,
resulting in higher carbon footprints. We present ECOLogic, a hybrid design
paradigm that embeds lightweight eFPGA fabric within ASICs to enable secure,
updatable, and resource-aware computation. Central to this architecture is
ECOScore, a quantitative scoring framework that evaluates IPs based on
adaptability, piracy threat, performance tolerance, and resource fit to guide
RTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an
average of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns
timing slack (versus 5.1 ns in FPGA), and reduces power by 480 times on
average. Moreover, sustainability analysis shows a 99.7 percent reduction in
deployment carbon footprint and 300 to 500 times lower emissions relative to
FPGA-only implementations. These results position ECOLogic as a
high-performance, secure, and environmentally sustainable solution for
next-generation reconfigurable systems.

</details>


### [78] [Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems](https://arxiv.org/abs/2508.04609)
*Osama Abdelaleim,Arun Prakash,Ayhan Irfanoglu,Veljko Milutinovic*

Main category: cs.AR

TL;DR: 本论文提出了一种通用模拟直接求解电路，用于加速正定对称线性方程组的求解，利用负电阻电路实现O(1)复杂度的高效求解。


<details>
  <summary>Details</summary>
Motivation: 线性方程组在科学计算、数据分析和机器学习中至关重要，传统求解方法速度有限，亟需加速方案。

Method: 采用非反相运算放大器配置设计负电阻电路，模拟对称系统，并优化系统架构。

Result: 设计能够以O(1)复杂度求解对角占优对称矩阵，对非对角占优系统的求解速度依赖于矩阵特性而非规模。

Conclusion: 该电路设计为对称线性方程组的快速求解提供了一种高效、通用的模拟解决方案。

Abstract: Accelerating the solution of linear systems of equations is critical due to
their central role in numerous applications, such as scientific simulations,
data analytics, and machine learning. This paper presents a general-purpose
analog direct solver circuit designed to accelerate the solution of positive
definite symmetric linear systems of equations. The proposed design leverages
non-inverting operational amplifier configurations to create a negative
resistance circuit, effectively modeling any symmetric system. The paper
details the principles behind the design, optimizations of the system
architecture, and numerical results that demonstrate the robustness of the
design. The findings reveal that the proposed system solves diagonally dominant
symmetric matrices with O(1) complexity, achieving the theoretical maximum
speed as the circuit relies solely on resistors. For non-diagonally dominant
symmetric positive-definite systems, the solution speed depends on matrix
properties such as eigenvalues and the maximum off-diagonal term, but remains
independent of matrix size.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [79] [Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task](https://arxiv.org/abs/2508.03699)
*Subin Raj Peter*

Main category: cs.CV

TL;DR: 提出了一种利用大语言模型（LLM）自动从文本生成虚拟指令的新方法，以简化VR培训应用的开发。


<details>
  <summary>Details</summary>
Motivation: VR培训应用开发复杂且耗时，需要解决效率和可扩展性问题。

Method: 结合LLM模块提取任务信息，智能模块将其转化为VR环境中的动态演示和视觉提示。

Result: 提升了培训效果，降低了开发成本，使VR培训更易扩展和适应工业需求。

Conclusion: 该方法为VR培训应用的自动化和高效开发提供了可行方案。

Abstract: Virtual Reality (VR) has emerged as a powerful tool for workforce training,
offering immersive, interactive, and risk-free environments that enhance skill
acquisition, decision-making, and confidence. Despite its advantages,
developing VR applications for training remains a significant challenge due to
the time, expertise, and resources required to create accurate and engaging
instructional content. To address these limitations, this paper proposes a
novel approach that leverages Large Language Models (LLMs) to automate the
generation of virtual instructions from textual input. The system comprises two
core components: an LLM module that extracts task-relevant information from the
text, and an intelligent module that transforms this information into animated
demonstrations and visual cues within a VR environment. The intelligent module
receives input from the LLM module and interprets the extracted information.
Based on this, an instruction generator creates training content using relevant
data from a database. The instruction generator generates the instruction by
changing the color of virtual objects and creating animations to illustrate
tasks. This approach enhances training effectiveness and reduces development
overhead, making VR-based training more scalable and adaptable to evolving
industrial needs.

</details>


### [80] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 提出一种结合特权信息与深度学习目标检测的新方法，提高垃圾检测的准确性和效率，实验证明其在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 全球垃圾污染日益严重，开发高效自动检测工具具有挑战性。

Method: 首次结合特权信息与深度学习目标检测，引入二进制掩码编码边界框信息优化检测。

Result: 在多个数据集上实现性能提升，保持模型计算高效性。

Conclusion: 该方法为垃圾检测提供了一种平衡准确性与效率的实用解决方案。

Abstract: As litter pollution continues to rise globally, developing automated tools
capable of detecting litter effectively remains a significant challenge. This
study presents a novel approach that combines, for the first time, privileged
information with deep learning object detection to improve litter detection
while maintaining model efficiency. We evaluate our method across five widely
used object detection models, addressing challenges such as detecting small
litter and objects partially obscured by grass or stones. In addition to this,
a key contribution of our work can also be attributed to formulating a means of
encoding bounding box information as a binary mask, which can be fed to the
detection model to refine detection guidance. Through experiments on both
within-dataset evaluation on the renowned SODA dataset and cross-dataset
evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate
consistent performance improvements across all models. Our approach not only
bolsters detection accuracy within the training sets but also generalises well
to other litter detection contexts. Crucially, these improvements are achieved
without increasing model complexity or adding extra layers, ensuring
computational efficiency and scalability. Our results suggest that this
methodology offers a practical solution for litter detection, balancing
accuracy and efficiency in real-world applications.

</details>


### [81] [Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning](https://arxiv.org/abs/2508.04161)
*Yuqin Cao,Yixuan Gao,Wei Sun,Xiaohong Liu,Yulun Zhang,Xiongkuo Min*

Main category: cs.CV

TL;DR: 该论文提出了一种通用的音频辅助人脸视频修复网络（GAVN），通过身份和时间互补学习解决多种流媒体视频失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频修复方法大多忽略了视觉与音频特征的内在关联，尤其是嘴部区域。少数音频辅助方法仅关注压缩伪影去除。因此，作者提出了一种更通用的解决方案。

Method: GAVN分三阶段：1) 低分辨率空间捕捉帧间时间特征以粗恢复，节省计算成本；2) 高分辨率空间结合音频和面部关键点提取帧内身份特征，恢复更丰富的面部细节；3) 重建模块整合时间与身份特征生成高质量视频。

Result: 实验表明，GAVN在视频压缩伪影去除、去模糊和超分辨率任务上均优于现有最先进方法。

Conclusion: GAVN通过综合利用音频和视觉特征，显著提升了多类视频失真修复的效果，代码将在发表后开源。

Abstract: Face videos accompanied by audio have become integral to our daily lives,
while they often suffer from complex degradations. Most face video restoration
methods neglect the intrinsic correlations between the visual and audio
features, especially in mouth regions. A few audio-aided face video restoration
methods have been proposed, but they only focus on compression artifact
removal. In this paper, we propose a General Audio-assisted face Video
restoration Network (GAVN) to address various types of streaming video
distortions via identity and temporal complementary learning. Specifically,
GAVN first captures inter-frame temporal features in the low-resolution space
to restore frames coarsely and save computational cost. Then, GAVN extracts
intra-frame identity features in the high-resolution space with the assistance
of audio signals and face landmarks to restore more facial details. Finally,
the reconstruction module integrates temporal features and identity features to
generate high-quality face videos. Experimental results demonstrate that GAVN
outperforms the existing state-of-the-art methods on face video compression
artifact removal, deblurring, and super-resolution. Codes will be released upon
publication.

</details>


### [82] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: LayerT2V通过分层生成视频，解决了多对象运动轨迹控制的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前T2V生成模型在多对象运动场景中表现不佳，尤其是在对象轨迹交叉时语义冲突问题严重，亟需一种新方法。

Method: 提出了LayerT2V，通过分层生成背景和前景对象，独立控制各对象层，实现灵活的多对象合成。

Result: 实验表明，LayerT2V在mIoU和AP50指标上分别比SOTA方法提升了1.4倍和4.5倍。

Conclusion: LayerT2V在多对象视频生成中表现优越，提供了更高的控制性和合成能力。

Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a
challenging and relatively under-explored area, particularly in scenarios
involving multiple moving objects. Most community models and datasets in the
T2V domain are designed for single-object motion, limiting the performance of
current generative models in multi-object tasks. Additionally, existing motion
control methods in T2V either lack support for multi-object motion scenes or
experience severe performance degradation when object trajectories intersect,
primarily due to the semantic conflicts in colliding regions. To address these
limitations, we introduce LayerT2V, the first approach for generating video by
compositing background and foreground objects layer by layer. This layered
generation enables flexible integration of multiple independent elements within
a video, positioning each element on a distinct "layer" and thus facilitating
coherent multi-object synthesis while enhancing control over the generation
process. Extensive experiments demonstrate the superiority of LayerT2V in
generating complex multi-object scenarios, showcasing 1.4x and 4.5x
improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.
Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .

</details>


### [83] [MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning](https://arxiv.org/abs/2508.04549)
*Quang-Trung Truong,Yuk-Kwan Wong,Vo Hoang Kim Tuyen Dang,Rinaldi Gotama,Duc Thanh Nguyen,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 论文提出了一种两阶段的海洋视频字幕生成方法，结合视频、文本和分割掩码的三元组，提升了视频理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕数据集难以适应海洋环境的复杂性，缺乏对海洋生物的深入理解，因此需要专门的方法来处理海洋视频的独特挑战。

Method: 采用两阶段的海洋视频字幕生成流程，结合视频、文本和分割掩码的三元组，并通过视频分割检测显著对象变化以丰富字幕语义。

Result: 提出的方法在海洋视频理解和生成方面表现优异，数据集和代码已公开。

Conclusion: 该研究填补了海洋视频字幕生成的空白，为海洋生物研究和视频分析提供了新工具。

Abstract: Marine videos present significant challenges for video understanding due to
the dynamics of marine objects and the surrounding environment, camera motion,
and the complexity of underwater scenes. Existing video captioning datasets,
typically focused on generic or human-centric domains, often fail to generalize
to the complexities of the marine environment and gain insights about marine
life. To address these limitations, we propose a two-stage marine
object-oriented video captioning pipeline. We introduce a comprehensive video
understanding benchmark that leverages the triplets of video, text, and
segmentation masks to facilitate visual grounding and captioning, leading to
improved marine video understanding and analysis, and marine video generation.
Additionally, we highlight the effectiveness of video splitting in order to
detect salient object transitions in scene changes, which significantly enrich
the semantics of captioning content. Our dataset and code have been released at
https://msc.hkustvgd.com.

</details>


### [84] [CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization](https://arxiv.org/abs/2508.04566)
*Jinxing Zhou,Ziheng Zhou,Yanghao Zhou,Yuxin Mao,Zhangling Duan,Dan Guo*

Main category: cs.CV

TL;DR: 本文介绍了在弱监督条件下（W-DAVEL任务）进行密集音视频事件定位的方法，通过跨模态显著锚点和互事件一致性评估提升定位性能。


<details>
  <summary>Details</summary>
Motivation: 探讨在仅提供视频级事件标签且未知时间边界的情况下，如何利用跨模态一致性实现音视频事件定位的挑战。

Method: 提出跨模态显著锚点识别和互事件一致性评估模块，通过全局和局部机制识别锚点特征，增强事件语义编码。

Result: 在UnAV-100和ActivityNet1.3数据集上取得最优性能。

Conclusion: 该方法在弱监督条件下显著提升了音视频事件定位的准确性。

Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally
localize events in untrimmed videos that occur simultaneously in both the audio
and visual modalities. This paper explores DAVEL under a new and more
challenging weakly-supervised setting (W-DAVEL task), where only video-level
event labels are provided and the temporal boundaries of each event are
unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors},
which are defined as reliable timestamps that are well predicted under weak
supervision and exhibit highly consistent event semantics across audio and
visual modalities. Specifically, we propose a \textit{Mutual Event Agreement
Evaluation} module, which generates an agreement score by measuring the
discrepancy between the predicted audio and visual event classes. Then, the
agreement score is utilized in a \textit{Cross-modal Salient Anchor
Identification} module, which identifies the audio and visual anchor features
through global-video and local temporal window identification mechanisms. The
anchor features after multimodal integration are fed into an
\textit{Anchor-based Temporal Propagation} module to enhance event semantic
encoding in the original temporal audio and visual features, facilitating
better temporal localization under weak supervision. We establish benchmarks
for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive
experiments demonstrate that our method achieves state-of-the-art performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是一个专为代理型AI系统设计的运行时治理框架，通过六个组件解决传统治理方法无法处理的运行时风险和意外行为。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统的运行时行为和风险无法完全通过部署前治理预测，需要新的运行时治理框架。

Method: MI9通过机构风险指数、代理语义遥测捕获、连续授权监控等六个组件实现实时控制。

Result: MI9能够系统地覆盖传统方法无法解决的治理挑战，为代理型AI的安全部署提供技术基础。

Conclusion: MI9填补了代理型AI运行时治理的关键空白，支持大规模安全部署。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [86] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识驱动和数据驱动的方法，用于改进AI代理在非预先协调团队中的协作能力。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的方法依赖大量标注数据、缺乏透明度且难以及时更新，随着代理数量增加，协作效率下降。

Method: 结合非单调逻辑推理，利用先验常识、快速学习的预测模型和基础模型的抽象目标推断。

Result: 在VirtualHome模拟环境中验证了方法的有效性。

Conclusion: 知识驱动与数据驱动的结合能够提升非协调团队的协作效率。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [87] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 多智能体系统中LLM的协作问题通过MAGRPO算法解决，实验表明其能有效提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM独立预训练且未针对协作优化，个体奖励机制设计复杂，需新方法。

Method: 将LLM协作建模为MARL问题，提出MAGRPO算法，结合RL与MARL技术。

Result: 实验表明MAGRPO能提升LLM在写作和编程协作中的响应质量和效率。

Conclusion: MAGRPO为LLM协作提供了新思路，并揭示了MARL方法的应用潜力与挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [88] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自进化框架，帮助计算机使用代理通过实验学习掌握新软件，提升了在没有人类标注情景下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有大规模视觉语言模型在新颖和专用软件中表现不佳的问题，尤其是在缺乏人工标注的场景。

Method: 设计了World State Model和Curriculum Generator，通过实验学习（包括失败行为的对抗模仿和成功行为的GRPO优化）更新代理策略，并结合专家到通才的训练策略。

Result: 在五个新软件环境中验证，SEAgent的成功率从11.3%提升到34.5%，超过竞品UI-TARS 23.2%。

Conclusion: SEAgent通过自主学习和融合专家经验，显著提升了计算机使用代理的表现，证明了其可行性和有效性。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


### [89] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 本文提出D2Snap，一种首次提出的DOM降采样算法，用于解决基于DOM的网页代理中状态序列化问题。通过实验验证，D2Snap在降采样DOM快照上的成功率（67%）接近基于GUI快照的基线（65%），且输入令牌量级相同。最佳配置甚至比基线高出8%。


<details>
  <summary>Details</summary>
Motivation: 当前基于GUI快照的网页代理虽然模拟人类感知，但图像处理对LLM的能力要求较高，而DOM快照虽结构更优，但因令牌量问题难以实现。本文旨在通过降采样解决这一问题。

Method: 提出D2Snap算法，对DOM进行降采样，以减少输入令牌量，同时保留关键UI结构信息。实验基于GPT-4o后端，并在Online-Mind2Web数据集上验证效果。

Result: 降采样后的DOM快照成功率（67%）与GUI快照基线（65%）相当，且输入令牌量级相同。最佳配置比基线高8%，并证实DOM层次结构对LLM有重要作用。

Conclusion: D2Snap通过降采样解决了DOM快照的令牌量问题，性能接近甚至优于GUI快照，为网页代理提供了一种更高效的解决方案。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [90] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 论文介绍了MedCheck，一个专门为医疗基准测试设计的生命周期导向评估框架，揭示了现有医疗基准测试的普遍问题，并提供了标准和透明的改进指南。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域潜力巨大，但现有基准测试的可靠性问题突出，亟需改进。

Method: 提出MedCheck框架，分五个阶段评估医疗基准测试，包含46项医学定制标准，并对53个医疗LLM基准进行了实证分析。

Result: 发现普遍问题，包括与临床实践脱节、数据完整性危机及忽视安全性评估维度。

Conclusion: MedCheck可作为诊断工具和行动指南，推动医疗AI评估的标准化与可靠性。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [91] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）如何为不同受众生成关于幸福感的定制化解释，并通过大规模数据集和评估框架提升了模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的人使用LLMs获取幸福感相关的建议，如何生成既准确又能满足不同受众需求的解释成为一个关键问题。

Method: 研究者构建了一个包含43,880条解释的数据集，采用十种不同的LLMs生成解释，并提出了一种基于原则的LLM-as-a-judge评估框架。此外，通过监督微调（SFT）和直接偏好优化（DPO）对开源LLM进行了优化。

Result: 结果显示：1) LLM法官评估与人类评估一致；2) 解释质量因模型、受众和类别而异；3) 经过DPO和SFT优化的模型表现优于更大的模型。

Conclusion: 偏好学习（如DPO）在专业解释任务中表现优异，为LLMs生成高质量定制化解释提供了有效途径。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [92] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 论文提出了一种新标注框架用于科学文献修辞角色分类，并通过大规模实验展示了LLMs在标注任务中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 科学文献的修辞角色标注能提升AI对文献的分析质量，并为自动化生成高质量文献综述提供支持，但目前缺乏相关标注框架和大规模标注策略。

Method: 1)设计新标注框架支持文献综述生成；2)评估37种LLMs在标注任务中的表现，包括零样本学习和微调方法；3)引入Sci-Sentence多学科标注数据集。

Result: 微调后的LLMs表现优异（F1>96%）；GPT-4o最优，但轻量开源模型也有竞争力；LLM生成的半合成数据能提升小模型性能。

Conclusion: LLMs在修辞角色分类任务中潜力巨大，开源模型和半合成数据的结合为实际应用提供了高效解决方案。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [93] [Delving Deeper Into Astromorphic Transformers](https://arxiv.org/abs/2312.10925)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 论文探讨了神经元-突触-星形胶质细胞相互作用的生物合理性建模，以模拟Transformer的自注意力机制，并展示在多项任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究星形胶质细胞在神经形态计算中的关键作用，以提升自注意力机制的生物合理性。

Method: 通过神经元-星形胶质细胞网络的Hebbian和突触前后可塑性建模，结合非线性与反馈效应，并将其映射到自注意力机制中。

Result: Astromorphic Transformers在情感和图像分类（IMDB和CIFAR10）中提高了准确性和学习速度，在WikiText-2上语言生成表现更优。

Conclusion: 该模型展示了在多样化机器学习任务中的优异泛化和稳定性，为生物启发式算法提供了新方向。

Abstract: Preliminary attempts at incorporating the critical role of astrocytes - cells
that constitute more than 50\% of human brain cells - in brain-inspired
neuromorphic computing remain in infancy. This paper seeks to delve deeper into
various key aspects of neuron-synapse-astrocyte interactions to mimic
self-attention mechanisms in Transformers. The cross-layer perspective explored
in this work involves bioplausible modeling of Hebbian and presynaptic
plasticities in neuron-astrocyte networks, incorporating effects of
non-linearities and feedback along with algorithmic formulations to map the
neuron-astrocyte computations to self-attention mechanism and evaluating the
impact of incorporating bio-realistic effects from the machine learning
application side. Our analysis on sentiment and image classification tasks
(IMDB and CIFAR10 datasets) highlights the advantages of Astromorphic
Transformers, offering improved accuracy and learning speed. Furthermore, the
model demonstrates strong natural language generation capabilities on the
WikiText-2 dataset, achieving better perplexity compared to conventional
models, thus showcasing enhanced generalization and stability across diverse
machine learning tasks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [94] [Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems](https://arxiv.org/abs/2508.04214)
*Yasaman Khorsandmanesh,Emil Björnson,Joakim Jaldén,Bengt Lindoff*

Main category: eess.SP

TL;DR: 提出了一种毫米波宽带MIMO系统中的两阶段数字组合方案，降低硬件复杂度并提高性能。


<details>
  <summary>Details</summary>
Motivation: 解决移动用户设备在处理大量基带样本时的计算和硬件复杂度问题。

Method: 采用两阶段数字组合方案，第一阶段利用信道几何结构降维，第二阶段基于信道估计更新组合。

Result: 数值结果表明，该方法优于混合波束成形，展现了全数字收发器的优势。

Conclusion: 两阶段全数字收发器在未来系统中具有吸引力，尤其在毫米波宽带MIMO场景。

Abstract: This paper considers a millimeter-wave wideband point-to-point MIMO system
with fully digital transceivers at the base station and the user equipment
(UE), focusing on mobile UE scenarios. A main challenge when building a digital
UE combining is the large volume of baseband samples to handle. To mitigate
computational and hardware complexity, we propose a novel two-stage digital
combining scheme at the UE. The first stage reduces the $N_{\text{r}}$ received
signals to $N_{\text{c}}$ streams before baseband processing, leveraging
channel geometry for dimension reduction and updating at the beam coherence
time, which is longer than the channel coherence time of the small-scale
fading. By contrast, the second-stage combining is updated per fading
realization. We develop a pilot-based channel estimation framework for this
hardware setup based on maximum likelihoodestimation in both uplink and
downlink. Digital precoding and combining designs are proposed, and a spectral
efficiency expression that incorporates imperfect channel knowledge is derived.
The numerical results demonstrate that the proposed approach outperforms hybrid
beamforming, showcasing the attractiveness of using two-stage fully digital
transceivers in future systems.

</details>


### [95] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: 该研究利用智能设备被动收集用户全天行为与睡眠数据，并结合主观报告，构建了一个全面的生活日志数据集，旨在探索人类日常生活的模式。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地理解个体的生理和心理状态，以改善健康和生活质量。

Method: 使用智能设备（如智能手机、智能手表和睡眠传感器）24小时被动收集数据，并结合睡眠前后的主观调查。

Result: 构建了包含定量行为和主观报告的综合性数据集，部分数据已公开供进一步研究。

Conclusion: ETRI Lifelog Dataset 2024为研究人类生活方式提供了基础资源，并展示了机器学习在预测睡眠质量和压力方面的应用潜力。

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


### [96] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 提出了一种基于多模态可穿戴传感器的非侵入式机器学习框架，用于检测脊髓损伤患者的自主神经功能障碍，效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 目前检测自主神经功能障碍的方法多为侵入性或依赖主观报告，限制日常应用，需开发非侵入且准确的监测技术。

Method: 使用多模态传感器数据，结合BorutaSHAP特征选择和堆叠集成模型，通过交叉验证评估性能。

Result: HR和ECG特征最有效，集成模型表现最佳（Macro F1=0.77），HR模态的AUC达0.93，模型对传感器丢失鲁棒。

Conclusion: 该框架为脊髓损伤患者的个性化实时监测提供了重要进展。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [97] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: ASTRA是一种自动化代理系统，用于系统性发现AI驱动的代码生成和安全指导系统中的安全缺陷，通过知识图谱和自适应探索技术，显著提升了漏洞发现效率和模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手的安全性在关键领域（如网络安全）仍不确定，现有工具的固定基准和不现实提示难以捕捉真实漏洞。

Method: ASTRA通过三阶段方法：构建领域知识图谱、自适应探索输入空间和推理过程、生成高质量违规案例。

Result: 在两项主要评估中，ASTRA比现有技术多发现11-66%的问题，测试案例使对齐训练效果提升17%。

Conclusion: ASTRA在提升AI系统安全性方面具有实际价值。

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


### [98] [SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios](https://arxiv.org/abs/2508.04100)
*Borui Li,Li Yan,Junhao Han,Jianmin Liu,Lei Yu*

Main category: cs.CR

TL;DR: SenseCrypt是一种基于敏感度的选择性同态加密框架，旨在在跨设备联邦学习中平衡安全性和计算开销，减少训练时间并保持模型准确度。


<details>
  <summary>Details</summary>
Motivation: 传统的选择性同态加密方法在异构数据和系统能力下表现不佳，导致客户端性能下降和开销增加。SenseCrypt通过敏感度分析解决这些问题。

Method: SenseCrypt通过隐私保护方法聚类具有相似数据分布的客户端，开发评分机制确定加密比例，并通过多目标优化选择模型参数。

Result: 实验显示，SenseCrypt能有效防御攻击，保持模型准确度，并将训练时间减少58.4%-88.7%。

Conclusion: SenseCrypt在安全性和计算效率上显著优于传统方法，适合跨设备联邦学习场景。

Abstract: Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but
suffers from high overhead and adaptation cost. Selective HE methods, which
partially encrypt model parameters by a global mask, are expected to protect
privacy with reduced overhead and easy adaptation. However, in cross-device
scenarios with heterogeneous data and system capabilities, traditional
Selective HE methods deteriorate client straggling, and suffer from degraded HE
overhead reduction performance. Accordingly, we propose SenseCrypt, a
Sensitivity-guided selective Homomorphic EnCryption framework, to adaptively
balance security and HE overhead per cross-device FL client. Given the
observation that model parameter sensitivity is effective for measuring
clients' data distribution similarity, we first design a privacy-preserving
method to respectively cluster the clients with similar data distributions.
Then, we develop a scoring mechanism to deduce the straggler-free ratio of
model parameters that can be encrypted by each client per cluster. Finally, for
each client, we formulate and solve a multi-objective model parameter selection
optimization problem, which minimizes HE overhead while maximizing model
security without causing straggling. Experiments demonstrate that SenseCrypt
ensures security against the state-of-the-art inversion attacks, while
achieving normal model accuracy as on IID data, and reducing training time by
58.4%-88.7% as compared to traditional HE methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [99] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过结合监督对比学习和特权信息学习，提升了情感计算模型在真实环境中的鲁棒性，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从实验室到现实环境迁移时的可靠性问题。

Method: 采用监督对比学习和特权信息学习结合的PriCon框架。

Result: 在RECOLA和AGAIN数据集上表现优于传统LUPI和端到端模型，接近全模态训练效果。

Conclusion: PriCon能有效缩小实验室与真实环境间的模型性能差距，具有实用潜力。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [100] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 论文提出了一种基于脉冲神经网络的终身网络入侵检测系统架构，通过静态和动态SNN结合，实现高效攻击检测和分类，并引入生物启发机制以减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 受到大脑的层次处理和能量效率启发，旨在设计一种能够持续学习新威胁并保持低能耗的网络入侵检测系统。

Method: 采用静态SNN初步识别潜在入侵，动态SNN分类攻击类型，结合GWR结构可塑性和新型Ad-STDP学习规则。

Result: 在UNSW-NB15基准测试中，系统表现出强大适应性，减少遗忘，整体准确率达85.3%，且在神经形态硬件上实现高操作稀疏性。

Conclusion: 该架构展示出在终身学习和低功耗部署方面的潜力，适合实际应用。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [101] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于时空预测框架的方法，利用众包用户侧KPI和监管数据集预测频谱需求，通过高级特征工程和相关分析提升预测准确性和跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 频谱需求预测对于频谱分配、监管规划和无线通信网络可持续发展至关重要，尤其是支持ITU等机构制定公平分配政策和满足5G、6G及IoT等新兴技术的需求。

Method: 结合高级特征工程、相关性分析和迁移学习技术，利用众包KPI和监管数据建模频谱需求，考虑时空变化。

Result: 相比传统ITU模型，该方法提供了更现实且可操作的预测，实验验证了其高效性。

Conclusion: 该框架为政策制定者和监管机构提供了一种稳健的频谱管理方法，具有实际应用潜力。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [102] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 提出SICKLE框架，通过智能子采样减少数据量，提升模型准确性并降低38倍能耗。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放的终结，高效训练需减少数据量，探索智能子采样是否可行。

Method: 开发SICKLE框架，采用最大熵（MaxEnt）采样方法，并与随机及相空间采样对比。

Result: 在Frontier上验证，子采样预处理可提升模型准确性并显著降低能耗（部分案例能耗减少38倍）。

Conclusion: 智能子采样是高效训练的有效方法，能大幅降低数据需求和能耗。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [103] [I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation](https://arxiv.org/abs/2508.04247)
*Huilin Chen,Miaomiao Cai,Fan Liu,Zhiyong Cheng,Richang Hong,Meng Wang*

Main category: cs.IR

TL;DR: 提出了I3-MRec方法，通过不变学习和信息瓶颈原则处理多模态推荐系统中的模态缺失问题，提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统（MRS）依赖于多种模态的信息，但在实际应用中，模态缺失（如图像或描述不完整）会显著降低模型的性能。

Method: I3-MRec结合不变学习和信息瓶颈原则，确保用户偏好的跨模态一致性，并提取紧凑有效的模态表示。

Result: 在三个真实数据集的实验中，I3-MRec在多种模态缺失场景下均优于现有方法。

Conclusion: I3-MRec通过鲁棒的模态处理和有效的信息提取，展示了其在多模态推荐系统中的实用性和优越性。

Abstract: Multimodal recommender systems (MRS) improve recommendation performance by
integrating diverse semantic information from multiple modalities. However, the
assumption of the availability of all modalities rarely holds in practice due
to missing images, incomplete descriptions, or inconsistent user content. These
challenges significantly degrade the robustness and generalization capabilities
of current models. To address these challenges, we introduce a novel method
called \textbf{I$^3$-MRec}, which uses \textbf{I}nvariant learning with
\textbf{I}nformation bottleneck principle for \textbf{I}ncomplete
\textbf{M}odality \textbf{Rec}ommendation. To achieve robust performance in
missing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)
cross-modal preference invariance, which ensures consistent user preference
modeling across varying modality environments, and (ii) compact yet effective
modality representation, which filters out task-irrelevant modality information
while maximally preserving essential features relevant to recommendation. By
treating each modality as a distinct semantic environment, I$^3$-MRec employs
invariant risk minimization (IRM) to learn modality-specific item
representations. In parallel, a missing-aware fusion module grounded in the
Information Bottleneck (IB) principle extracts compact and effective item
embeddings by suppressing modality noise and preserving core user preference
signals. Extensive experiments conducted on three real-world datasets
demonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art
MRS methods across various modality-missing scenarios, highlighting its
effectiveness and robustness in practical applications. The code and processed
datasets are released at https://github.com/HuilinChenJN/I3-MRec.

</details>


### [104] [Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval](https://arxiv.org/abs/2508.04273)
*Junan Lin,Daizong Liu,Xianke Chen,Xiaoye Qu,Xun Yang,Jixiang Zhu,Sanyuan Zhang,Jianfeng Dong*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的重要性感知多粒度融合模型（IMG），用于视频时刻检索（VMR），动态选择性地融合音频-视觉-文本模态，通过预测音频重要性分数和多粒度融合模块，优化音频利用。


<details>
  <summary>Details</summary>
Motivation: 现有VMR方法忽视音频模态或简单平等处理所有模态，而实际中音频可能无意义或干扰检索。本文旨在动态利用音频模态，提升VMR性能。

Method: 设计音频重要性预测器（伪标签监督）和多粒度音频融合模块（局部、事件、全局级），并提出跨模态知识蒸馏策略以应对推理中音频缺失。

Result: 构建新数据集Charades-AudioMatter，实验证明IMG方法在音频-视频融合中达到最先进性能。

Conclusion: IMG模型通过动态选择和加权音频模态，显著提升VMR性能，为多模态推理提供有效方案。

Abstract: Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically
related to the given query. To tackle this task, most existing VMR methods
solely focus on the visual and textual modalities while neglecting the
complementary but important audio modality. Although a few recent works try to
tackle the joint audio-vision-text reasoning, they treat all modalities equally
and simply embed them without fine-grained interaction for moment retrieval.
These designs are counter-practical as: Not all audios are helpful for video
moment retrieval, and the audio of some videos may be complete noise or
background sound that is meaningless to the moment determination. To this end,
we propose a novel Importance-aware Multi-Granularity fusion model (IMG), which
learns to dynamically and selectively aggregate the audio-vision-text contexts
for VMR. Specifically, after integrating the textual guidance with vision and
audio separately, we first design a pseudo-label-supervised audio importance
predictor that predicts the importance score of the audio, and accordingly
assigns weights to mitigate the interference caused by noisy audio. Then, we
design a multi-granularity audio fusion module that adaptively fuses audio and
visual modalities at local-, event-, and global-level, fully capturing their
complementary contexts. We further propose a cross-modal knowledge distillation
strategy to address the challenge of missing audio modality during inference.
To evaluate our method, we further construct a new VMR dataset, i.e.,
Charades-AudioMatter, where audio-related samples are manually selected and
re-organized from the original Charades-STA to validate the model's capability
in utilizing audio modality. Extensive experiments validate the effectiveness
of our method, achieving state-of-the-art with audio-video fusion in VMR
methods. Our code is available at https://github.com/HuiGuanLab/IMG.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [105] [Moveless: Minimizing Overhead on QCCDs via Versatile Execution and Low Excess Shuttling](https://arxiv.org/abs/2508.03914)
*Sahil Khan,Suhas Vittal,Kenneth Brown,Jonathan Baker*

Main category: quant-ph

TL;DR: 该论文提出了一种针对量子纠错（QEC）电路结构特点的编译方案，显著提升了执行速度和逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 量子纠错码在编译时若采用通用方法，会导致硬件执行效率低下，因此需要一种针对其结构特性的专门编译方法。

Method: 通过观察QEC电路的特点（如仅需移动辅助比特或数据比特、稳定器可动态执行等），设计了专门的编译器。

Result: 新编译器使QEC电路执行速度平均提升3.38倍，逻辑错误率在现实物理错误率下最多降低两个数量级。

Conclusion: 针对QEC电路的专门编译方案能显著优化执行效率和逻辑错误率。

Abstract: One of the most promising paths towards large scale fault tolerant quantum
computation is the use of quantum error correcting stabilizer codes. Just like
every other quantum circuit, these codes must be compiled to hardware in a way
to minimize the total physical error introduced into the system, for example
either due to high latency execution or excessive gates to meet connectivity
limitations of the target hardware. However, unlike arbitrary quantum circuits,
all syndrome extraction circuits have several common properties, for example
they have a bipartite connectivity graph, consist only of commuting
subcircuits, among other properties. For the most part, compilation methods
have aimed at being generic, able to map any input circuit into executables on
the hardware, and therefore cannot appropriately exploit these properties and
result in executables which have higher physical error. In the case of modular
trapped ion systems, specifically QCCDs, this corresponds to the insertion of
excessive shuttling operations necessary to realize arbitrary qubit
interactions. We propose a compilation scheme explicitly tailored for the
structural regularity of QEC circuits based on several key observations: 1.
only ancilla or data (but not both) should be shuttled, 2. stabilizers can be
executed in any order meaning we can dynamically modify circuit execution on a
per-cycle basis 3. ancilla are indistinguishable meaning any can be selected to
begin a stabilizer measurement and retain a fixed-point mapping between cycles,
and 4. QCCD hardware limits the number of parallel operations equal to the
number traps in the system, meaning fewer ancilla are necessary and can be
reused. Our resulting compiler, leads to QEC circuits which are on average
3.38x faster to execute, and lead to up to two orders of magnitude of
improvement in logical error rates with realistic physical error rates.

</details>


### [106] [The decohered ZX-calculus](https://arxiv.org/abs/2508.04296)
*Titouan Carette,Daniela Cojocaru,Renaud Vilmart*

Main category: quant-ph

TL;DR: 研究了丢弃ZX-演算中的经典片段，证明其对F2上的概率分布具有普适性和完备性。


<details>
  <summary>Details</summary>
Motivation: ZX-演算的量子部分已有深入研究，但其经典部分尚未充分探讨。

Method: 通过退相干处理ZX-演算的生成元，提出一种混合图形线性代数与图解傅里叶变换的规范形式。

Result: 该演算对F2上的概率分布具有普适性和完备性。

Conclusion: 为处理混合经典-量子过程和更一般的随机变量提供了基础。

Abstract: The discard ZX-calculus is known to be complete and universal for mixed-state
quantum mechanics, allowing for both quantum and classical processes. However,
if the quantum aspects of ZX-calculus have been explored in depth, little work
has been done on the classical side. In this paper, we investigate a fragment
of discard ZX-calculus obtained by decohering the usual generators of
ZX-calculus. We show that this calculus is universal and complete for affinely
supported probability distributions over $\mathbb{F}_{2}^{n}$. To do so, we
exhibit a normal form, mixing ideas from the graphical linear algebra program
and diagrammatic Fourier transforms. Our results both clarify how to handle
hybrid classical-quantum processes in the discard ZX-calculus and pave the way
to the picturing of more general random variables and probabilistic processes.

</details>


### [107] [Advantages of Co-locating Quantum-HPC Platforms: A Survey for Near-Future Industrial Applications](https://arxiv.org/abs/2508.04171)
*Daigo Honda,Yuta Nishiyama,Junya Ishikawa,Kenichi Matsuzaki,Satoshi Miyata,Tadahiro Chujo,Yasuhisa Yamamoto,Masahiko Kiminami,Taro Kato,Jun Towada,Naoki Yoshioka,Naoto Aoki,Nobuyasu Ito*

Main category: quant-ph

TL;DR: 本研究审查了量子计算机与高性能计算（HPC）系统共置平台的潜力，发现其对混合作业吞吐量有显著提升，且需要HPC资源支持大规模实际问题的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探讨量子-HPC平台是否能为工业应用带来实际效益。

Method: 系统调查了共置对延迟、带宽、作业调度的影响，并评估了HPC能力对混合算法性能和大规模量子电路优化的支持。

Result: 共置量子与HPC系统可提高混合作业吞吐量，且大规模问题需HPC资源支持。

Conclusion: 量子-HPC共置平台对提升混合计算效率具有实际价值，未来工业应用潜力巨大。

Abstract: We conducted a systematic survey of emerging quantum-HPC platforms, which
integrate quantum computers and High-Performance Computing (HPC) systems
through co-location. Currently, it remains unclear whether such platforms
provide tangible benefits for near-future industrial applications. To address
this, we examined the impact of co-location on latency reduction, bandwidth
enhancement, and advanced job scheduling. Additionally, we assessed how
HPC-level capabilities could enhance hybrid algorithm performance, support
large-scale error mitigation, and facilitate complex quantum circuit
partitioning and optimization. Our findings demonstrate that co-locating
quantum and HPC systems can yield measurable improvements in overall hybrid job
throughput. We also observe that large-scale real-world problems can require
HPC-level computational resources for executing hybrid algorithms.

</details>


### [108] [Entanglement distribution in quantum networks via swapping of partially entangled states](https://arxiv.org/abs/2508.04536)
*Henrique Guerra,Tailan S. Sarubi,Rafael Chaves,Jonas Maziero*

Main category: quant-ph

TL;DR: 本文研究了部分纠缠态在不同拓扑结构量子网络中的纠缠交换协议，分析了初始态变换及成功概率，为实际量子通信设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 研究部分纠缠态在量子网络中的纠缠分布动态，扩展纠缠交换协议的应用场景。

Method: 通过Bell基测量分析线性、星型和混合拓扑网络中的纠缠交换协议。

Result: 揭示了纠缠态的演化规律和生成最大纠缠态的成功概率。

Conclusion: 为量子网络中的鲁棒通信策略设计提供了理论和实践支持。

Abstract: The entanglement swapping protocol (ESP) is a fundamental primitive for
distributing quantum correlations across distant nodes in a quantum network.
Recent studies have demonstrated that even when the involved qubit pairs are
only partially entangled, it is still possible to concentrate and transmit
entanglement via Bell-basis measurements. In this work, we extend these ideas
to quantum networks with various topologies - including linear, star, and
hybrid configurations - by analyzing the application of the ESP to initially
partially entangled states. We investigate how entanglement evolves under such
protocols by examining the transformations of the initial states and evaluating
the success probabilities for generating maximally entangled states at the
output. Our results offer new insights into the dynamics of the entanglement
distribution in quantum networks and provide practical guidelines for designing
robust quantum communication strategies under realistic conditions.

</details>


### [109] [Dynamic Solutions for Hybrid Quantum-HPC Resource Allocation](https://arxiv.org/abs/2508.04217)
*Roberto Rocco,Simone Rizzo,Matteo Barbieri,Gabriella Bettonte,Elisabetta Boella,Fulvio Ganz,Sergio Iserte,Antonio J. Peña,Petter Sandås,Alberto Scionti,Olivier Terzo,Chiara Vercellino,Giacomo Vitali,Paolo Viviani,Jonathan Frassineti,Sara Marzella,Daniele Ottaviani,Iacopo Colonnelli,Daniele Gregori*

Main category: quant-ph

TL;DR: 本文提出了一种基于可塑性和工作流的策略，优化HPC-量子混合负载中的资源分配，动态释放和重新分配资源，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机与经典HPC基础设施的集成需求增加，资源分配成为关键技术挑战。论文旨在解决这一问题。

Method: 提出了一种基于可塑性的方法和工作流策略，动态调整HPC资源，以适应量子计算任务的负载变化。

Result: 实验表明，动态资源分配能显著提升混合负载的效率，验证了该解决方案的潜力。

Conclusion: 该研究为HPC-量子混合负载的资源优化提供了可行方案，展示了动态分配的实用价值。

Abstract: The integration of quantum computers within classical High-Performance
Computing (HPC) infrastructures is receiving increasing attention, with the
former expected to serve as accelerators for specific computational tasks.
However, combining HPC and quantum computers presents significant technical
challenges, including resource allocation. This paper presents a novel
malleability-based approach, alongside a workflow-based strategy, to optimize
resource utilization in hybrid HPC-quantum workloads. With both these
approaches, we can release classical resources when computations are offloaded
to the quantum computer and reallocate them once quantum processing is
complete. Our experiments with a hybrid HPC-quantum use case show the benefits
of dynamic allocation, highlighting the potential of those solutions.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [110] [The Ubiquitous Sparse Matrix-Matrix Products](https://arxiv.org/abs/2508.04077)
*Aydın Buluç*

Main category: math.NA

TL;DR: 该论文探讨了稀疏矩阵乘法在不同代数半环或异构代数下的广泛应用，涵盖了机器学习、计算生物学、化学等多个领域。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵乘法是数据科学中许多应用的核心操作，但现有研究通常局限于特定场景，缺乏对广义代数框架的统一处理。

Method: 论文提供了一个统一的理论框架，适用于任意代数半环或异构代数中的稀疏矩阵乘法操作。

Result: 研究展示了该框架在机器学习、计算生物学、化学、图算法和科学计算中的广泛应用潜力。

Conclusion: 通过统一处理稀疏矩阵乘法及其丰富的应用场景，该研究为跨领域的数据科学问题提供了通用解决方案。

Abstract: Multiplication of a sparse matrix with another (dense or sparse) matrix is a
fundamental operation that captures the computational patterns of many data
science applications, including but not limited to graph algorithms, sparsely
connected neural networks, graph neural networks, clustering, and many-to-many
comparisons of biological sequencing data.
  In many application scenarios, the matrix multiplication takes places on an
arbitrary algebraic semiring where the scalar operations are overloaded with
user-defined functions with certain properties or a more general heterogenous
algebra where even the domains of the input matrices can be different. Here, we
provide a unifying treatment of the sparse matrix-matrix operation and its rich
application space including machine learning, computational biology and
chemistry, graph algorithms, and scientific computing.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [111] [Live Music Models](https://arxiv.org/abs/2508.04651)
*Lyria Team,Antoine Caillon,Brian McWilliams,Cassie Tarakajian,Ian Simon,Ilaria Manco,Jesse Engel,Noah Constant,Pen Li,Timo I. Denk,Alberto Lalama,Andrea Agostinelli,Anna Huang,Ethan Manilow,George Brower,Hakan Erdogan,Heidi Lei,Itai Rolnick,Ivan Grishchenko,Manu Orsini,Matej Kastelic,Mauricio Zuluaga,Mauro Verzetti,Michael Dooley,Ondrej Skopek,Rafael Ferrer,Zalán Borsos,Äaron van den Oord,Douglas Eck,Eli Collins,Jason Baldridge,Tom Hume,Chris Donahue,Kehang Han,Adam Roberts*

Main category: cs.SD

TL;DR: 介绍了实时音乐生成模型Magenta RealTime和Lyria RealTime，支持通过文本或音频提示控制音乐风格，并在音乐质量上优于其他公开模型。


<details>
  <summary>Details</summary>
Motivation: 探索一种新的AI辅助音乐创作范式，强调实时音乐表演中的人机交互。

Method: 开发了两种实时音乐生成模型，Magenta（开源）和Lyria（API），支持文本或音频提示控制风格。

Result: Magenta RealTime在音乐质量上表现优异，且参数更少；Lyria RealTime提供更广泛的提示控制。

Conclusion: 这些模型展示了AI辅助音乐创作的新方向，突出了实时交互的重要性。

Abstract: We introduce a new class of generative models for music called live music
models that produce a continuous stream of music in real-time with synchronized
user control. We release Magenta RealTime, an open-weights live music model
that can be steered using text or audio prompts to control acoustic style. On
automatic metrics of music quality, Magenta RealTime outperforms other
open-weights music generation models, despite using fewer parameters and
offering first-of-its-kind live generation capabilities. We also release Lyria
RealTime, an API-based model with extended controls, offering access to our
most powerful model with wide prompt coverage. These models demonstrate a new
paradigm for AI-assisted music creation that emphasizes human-in-the-loop
interaction for live music performance.

</details>
