<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 14]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.HC](#cs.HC) [Total: 37]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 20]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CR](#cs.CR) [Total: 11]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [cs.CV](#cs.CV) [Total: 12]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [math.LO](#math.LO) [Total: 3]
- [cs.AI](#cs.AI) [Total: 8]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review](https://arxiv.org/abs/2507.03156)
*Amr Mohamed,Maram Assi,Mariam Guizani*

Main category: cs.SE

TL;DR: LLM助手在软件开发中提高了效率，但也带来风险和挑战，未来研究需更多维度和纵向评估。


<details>
  <summary>Details</summary>
Motivation: 研究LLM助手对开发者生产力的影响，填补现有研究的空白。

Method: 系统综述37项2014-2024年的同行评审研究。

Result: LLM助手能加速开发，但也可能降低代码质量和协作；研究多关注效率，忽视沟通和活动。

Conclusion: 未来需更全面的研究，特别是在团队协作和多维度评估方面。

Abstract: Large language model assistants (LLM-assistants) present new opportunities to
transform software development. Developers are increasingly adopting these
tools across tasks, including coding, testing, debugging, documentation, and
design. Yet, despite growing interest, there is no synthesis of how
LLM-assistants affect software developer productivity. In this paper, we
present a systematic literature review of 37 peer-reviewed studies published
between January 2014 and December 2024 that examine this impact. Our analysis
reveals that LLM-assistants offer both considerable benefits and critical
risks. Commonly reported gains include minimized code search, accelerated
development, and the automation of trivial and repetitive tasks. However,
studies also highlight concerns around cognitive offloading, reduced team
collaboration, and inconsistent effects on code quality. While the majority of
studies (92%) adopt a multi-dimensional perspective by examining at least two
SPACE dimensions, reflecting increased awareness of the complexity of developer
productivity, only 14% extend beyond three dimensions, indicating substantial
room for more integrated evaluations. Satisfaction, Performance, and Efficiency
are the most frequently investigated dimensions, whereas Communication and
Activity remain underexplored. Most studies are exploratory (64%) and
methodologically diverse, but lack longitudinal and team-based evaluations.
This review surfaces key research gaps and provides recommendations for future
research and practice. All artifacts associated with this study are publicly
available at https://zenodo.org/records/15788502.

</details>


### [2] [Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks](https://arxiv.org/abs/2507.03160)
*Md Mahade Hasan,Muhammad Waseem,Kai-Kristian Kemell,Jussi Raskua,Juha Ala-Rantalaa,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 该研究对20个开源小型语言模型（SLMs）在代码生成任务中的性能进行了实证评估，发现某些小型模型在性能和效率之间取得了平衡，适用于资源受限环境；而更大的模型虽性能更好，但计算资源消耗显著增加。


<details>
  <summary>Details</summary>
Motivation: 随着小型语言模型（SLMs）的发展，其在代码生成任务中的应用潜力受到关注，但其实际性能与局限仍需实证研究。

Method: 研究评估了20个参数从0.4B到10B的开源SLMs在五个代码相关基准上的表现，涵盖功能正确性、计算效率和多语言性能。

Result: 小型模型在资源受限环境中表现良好，但提升准确性需转向更大模型，后者性能更优但资源消耗显著增加；多语言性能差异不显著。

Conclusion: 研究为实际代码生成任务中SLMs的设计与选择提供了指导，强调了性能与资源消耗之间的权衡。

Abstract: The recent advancements of Small Language Models (SLMs) have opened new
possibilities for efficient code generation. SLMs offer lightweight and
cost-effective alternatives to Large Language Models (LLMs), making them
attractive for use in resource-constrained environments. However, empirical
understanding of SLMs, particularly their capabilities, limitations, and
performance trade-offs in code generation remains limited. This study presents
a comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B
to 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,
Mercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three
dimensions: i) functional correctness of generated code, ii) computational
efficiency and iii) performance across multiple programming languages. The
findings of this study reveal that several compact SLMs achieve competitive
results while maintaining a balance between performance and efficiency, making
them viable for deployment in resource-constrained environments. However,
achieving further improvements in accuracy requires switching to larger models.
These models generally outperform their smaller counterparts, but they require
much more computational power. We observe that for 10% performance
improvements, models can require nearly a 4x increase in VRAM consumption,
highlighting a trade-off between effectiveness and scalability. Besides, the
multilingual performance analysis reveals that SLMs tend to perform better in
languages such as Python, Java, and PHP, while exhibiting relatively weaker
performance in Go, C++, and Ruby. However, statistical analysis suggests these
differences are not significant, indicating a generalizability of SLMs across
programming languages. Based on the findings, this work provides insights into
the design and selection of SLMs for real-world code generation tasks.

</details>


### [3] [Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools](https://arxiv.org/abs/2507.03263)
*Haiqiao Gu,Yiliang Zhao,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 该论文首次建立了C/C++库迁移数据集，分析了19,943个项目，比较了C/C++与其他语言（Python、JavaScript、Java）的库迁移特点，发现其迁移趋势与Java相似，并揭示了C/C++特有的迁移原因。


<details>
  <summary>Details</summary>
Motivation: 填补C/C++生态系统中库迁移研究的空白，因C/C++依赖管理复杂，缺乏相关数据。

Method: 分析19,943个C/C++项目，建立首个C/C++库迁移数据集，比较其与Python、JavaScript、Java的迁移特点。

Result: C/C++库迁移趋势与Java相似；迁移主要发生在GUI、Build和OS开发领域；83.46%的源库仅有一个迁移目标；揭示了四个C/C++特有的迁移原因。

Conclusion: 研究结果有助于C/C++开发者更明智地决策库迁移，并为相关工具设计提供启示。

Abstract: Library migration happens when a library can not meet the project's
requirements and is non-trivial to accomplish. To mitigate the problem,
substantial efforts have been devoted to understanding its characteristics and
recommending alternative libraries, especially for programming language (PL)
ecosystems with a central package hosting platform, such as Python (PyPI).
However, to the best of our knowledge, understanding of C/C++ library
migrations is still lacking, possibly due to challenges resulting from the
fragmented and complicated dependency management practices in the C/C++
ecosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++
projects that utilize different package management tools and establishes the
first C/C++ library migration dataset. Based on the dataset, we investigate the
prevalence, domains, target library, and rationale of C/C++ library migrations
and compare the results with three widely investigated PLs: Python, JavaScript,
and Java. We find that the overall trend in the number of C/C++ library
migrations is similar to Java. Migrations across different package management
tools are also observed. In C/C++, library migrations mainly occur in GUI,
Build, and OS development, but are rare in domains (e.g., Testing and Logging)
that dominate library migrations in the three compared PLs. 83.46\% of C/C++
source libraries only have one migration target, suggesting that our library
migration dataset could be used directly to recommend migration targets. We
find four C/C++-specific migration reasons, such as less compile time and
unification of dependency management, revealing the unique dependency
management requirements in C/C++ projects. We believe our findings can help
C/C++ developers make more informed library migration decisions and shed light
on the design of C/C++ library migration tools.

</details>


### [4] [scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software](https://arxiv.org/abs/2507.03328)
*S. Lee,C. Myers,A. Yang,T. Zhang,S. J. L. Billinge*

Main category: cs.SE

TL;DR: 该论文介绍了scikit-package项目，旨在通过教程和自动化工作流帮助非专业软件工程师的科学家编写更可重用和维护的代码。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖于结果的可共享和可重复性，但科学家编写的代码在版本、质量和共享方面面临挑战。

Method: scikit-package提供教程和集中化的工作流，支持从简单脚本到完整软件包的代码复用。

Result: 项目提供工具和路线图，帮助提升代码的可重复性和共享性。

Conclusion: scikit-package为科学家提供了提升代码质量的实用工具和社区支持。

Abstract: Scientific advancement relies on the ability to share and reproduce results.
When data analysis or calculations are carried out using software written by
scientists there are special challenges around code versions, quality and code
sharing. scikit-package provides a roadmap to facilitate code reuse and sharing
with minimal effort through tutorials coupled with automated and centralized
reusable workflows. The goal of the project is to provide pedagogical and
practical tools for scientists who are not professionally trained software
engineers to write more reusable and maintainable software code. Code reuse can
occur at multiple levels of complexity-from turning a code block into a
function within a single script, to publishing a publicly installable, fully
tested, and documented software package scikit-package provides a community
maintained set of tools, and a roadmap, to help scientists bring their software
higher levels of reproducibility and shareability.

</details>


### [5] [Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering](https://arxiv.org/abs/2507.03405)
*Krishna Ronanki,Simon Arvidsson,Johan Axell*

Main category: cs.SE

TL;DR: 研究探讨了如何将现有的提示工程指南应用于需求工程(RE)中，填补了领域特定指南的不足。


<details>
  <summary>Details</summary>
Motivation: 生成式AI(如大语言模型)在需求工程中的应用广泛，但缺乏专门的提示工程指南以确保输出质量。

Method: 通过系统文献综述汇编提示工程指南，并访谈RE专家评估其适用性。

Result: 研究发现RE领域缺乏特定提示工程指南，提出了填补这一空缺的映射方法。

Conclusion: 研究为未来在RE中优化提示工程提供了重要方向。

Abstract: The rapid emergence of generative AI models like Large Language Models (LLMs)
has demonstrated its utility across various activities, including within
Requirements Engineering (RE). Ensuring the quality and accuracy of
LLM-generated output is critical, with prompt engineering serving as a key
technique to guide model responses. However, existing literature provides
limited guidance on how prompt engineering can be leveraged, specifically for
RE activities. The objective of this study is to explore the applicability of
existing prompt engineering guidelines for the effective usage of LLMs within
RE. To achieve this goal, we began by conducting a systematic review of primary
literature to compile a non-exhaustive list of prompt engineering guidelines.
Then, we conducted interviews with RE experts to present the extracted
guidelines and gain insights on the advantages and limitations of their
application within RE. Our literature review indicates a shortage of prompt
engineering guidelines for domain-specific activities, specifically for RE. Our
proposed mapping contributes to addressing this shortage. We conclude our study
by identifying an important future line of research within this field.

</details>


### [6] [Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain](https://arxiv.org/abs/2507.03515)
*Radouane Bouchekir,Michell Guzman Cancimance*

Main category: cs.SE

TL;DR: 通过结合环境条件的风险因果分析增强传统不确定性量化，提出一种动态、上下文感知的安全评估方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习组件因不确定性和对环境变化的敏感性导致的自主系统运行时安全问题。

Method: 利用HARA和故障树模型识别关键操作条件，结合数据和模型不确定性，构建统一的贝叶斯网络进行实时推断。

Result: 通过自动驾驶泊车系统中的目标检测组件案例研究验证了方法的有效性。

Conclusion: 提出的方法能够动态评估安全性能及其不确定性，为自主系统提供更可靠的安全保障。

Abstract: Ensuring the runtime safety of autonomous systems remains challenging due to
deep learning components' inherent uncertainty and their sensitivity to
environmental changes. In this paper, we propose an enhancement of traditional
uncertainty quantification by explicitly incorporating environmental conditions
using risk-based causal analysis. We leverage Hazard Analysis and Risk
Assessment (HARA) and fault tree modeling to identify critical operational
conditions affecting system functionality. These conditions, together with
uncertainties from the data and model, are integrated into a unified Bayesian
Network (BN). At runtime, this BN is instantiated using real-time environmental
observations to infer a probabilistic distribution over the safety estimation.
This distribution enables the computation of both expected performance and its
associated variance, providing a dynamic and context-aware measure of
uncertainty. We demonstrate our approach through a case study of the Object
Detection (OD) component in an Automated Valet Parking (AVP).

</details>


### [7] [The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy](https://arxiv.org/abs/2507.03527)
*Dulaji Hidellaarachchi,John Grundy,Rashina Hoda*

Main category: cs.SE

TL;DR: 该论文通过文献综述提出了一个分类法，研究幽默在软件工程团队中的特征和使用，旨在提升生产力、改善沟通并营造积极的工作环境，同时强调负责任的幽默使用以避免潜在负面影响。


<details>
  <summary>Details</summary>
Motivation: 尽管幽默在多个领域已被证明能提升创造力、团队效能和员工福祉，但其在软件工程团队中的具体应用和影响尚缺乏深入研究。

Method: 通过综合心理学、社会学和组织行为学的研究，论文构建了一个分类框架，将幽默分为不同的理论、风格、模型和量表，为软件工程专业人士和研究者提供结构化理解幽默的工具。

Result: 提出的框架为软件工程团队提供了理解和应用幽默的指南，同时指出需要进一步实证研究验证其在软件工程环境中的效果。

Conclusion: 通过合理使用幽默，该研究旨在促进软件工程团队的凝聚力、创造力和心理支持性工作环境。

Abstract: Humour has long been recognized as a key factor in enhancing creativity,
group effectiveness, and employee well-being across various domains. However,
its occurrence and impact within software engineering (SE) teams remains
under-explored. This paper introduces a comprehensive, literature review-based
taxonomy exploring the characterisation and use of humour in SE teams, with the
goal of boosting productivity, improving communication, and fostering a
positive work environment while emphasising the responsible use of humour to
mitigate its potential negative impacts. Drawing from a wide array of studies
in psychology, sociology, and organizational behaviour, our proposed framework
categorizes humour into distinct theories, styles, models, and scales, offering
SE professionals and researchers a structured approach to understanding humour
in their work. This study also addresses the unique challenges of applying
humour in SE, highlighting its potential benefits while acknowledging the need
for further empirical validation in this context. Ultimately, our study aims to
pave the way for more cohesive, creative, and psychologically supportive SE
environments through the strategic use of humour.

</details>


### [8] [ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings](https://arxiv.org/abs/2507.03536)
*Adam Tornhill,Markus Borg,Nadim Hagatulah,Emma Söderberg*

Main category: cs.SE

TL;DR: ACE工具利用LLM自动改进代码，提升代码可理解性和质量，减轻开发者的代码理解负担。


<details>
  <summary>Details</summary>
Motivation: AI和大型语言模型（LLM）的进步使代码生成更高效，但代码理解仍是软件开发的主要瓶颈，消耗开发者70%时间。因此，通过改进代码可理解性释放开发者资源至关重要。

Method: 提出Augmented Code Engineering (ACE)工具，基于数据驱动方法，利用验证过的LLM输出来自动化代码改进，同时兼顾代码质量和正确性。

Result: 初步用户反馈显示，AI辅助的代码重构能有效减少技术债务。

Conclusion: 在AI辅助编码时代，ACE通过自动化改进代码可理解性，帮助有限开发者应对日益增长的代码库。

Abstract: The remarkable advances in AI and Large Language Models (LLMs) have enabled
machines to write code, accelerating the growth of software systems. However,
the bottleneck in software development is not writing code but understanding
it; program understanding is the dominant activity, consuming approximately 70%
of developers' time. This implies that improving existing code to make it
easier to understand has a high payoff and - in the age of AI-assisted coding -
is an essential activity to ensure that a limited pool of developers can keep
up with ever-growing codebases. This paper introduces Augmented Code
Engineering (ACE), a tool that automates code improvements using validated LLM
output. Developed through a data-driven approach, ACE provides reliable
refactoring suggestions by considering both objective code quality improvements
and program correctness. Early feedback from users suggests that AI-enabled
refactoring helps mitigate code-level technical debt that otherwise rarely gets
acted upon.

</details>


### [9] [Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy](https://arxiv.org/abs/2507.03620)
*Francisca Lemos,Victor Alves,Filipa Ferraz*

Main category: cs.SE

TL;DR: DSPy框架通过程序化优化提示词，改善了大型语言模型在某些任务中的表现，但效果因任务而异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决手动设计提示词的效率低下问题，探索程序化优化的潜力。

Method: 使用DSPy框架优化提示词，并在五个用例中测试其效果，包括护栏执行、幻觉检测等。

Result: 部分任务表现显著提升（如提示评估任务的准确率从46.2%提高到64.0%），但也有一些任务提升有限。

Conclusion: DSPy的提示优化能提升模型表现，但效果依赖于具体任务，需结合指令调整和示例选择。

Abstract: Although prompt engineering is central to unlocking the full potential of
Large Language Models (LLMs), crafting effective prompts remains a
time-consuming trial-and-error process that relies on human intuition. This
study investigates Declarative Self-improving Python (DSPy), an optimization
framework that programmatically creates and refines prompts, applied to five
use cases: guardrail enforcement, hallucination detection in code, code
generation, routing agents, and prompt evaluation. Each use case explores how
prompt optimization via DSPy influences performance. While some cases
demonstrated modest improvements - such as minor gains in the guardrails use
case and selective enhancements in hallucination detection - others showed
notable benefits. The prompt evaluation criterion task demonstrated a
substantial performance increase, rising accuracy from 46.2% to 64.0%. In the
router agent case, the possibility of improving a poorly performing prompt and
of a smaller model matching a stronger one through optimized prompting was
explored. Although prompt refinement increased accuracy from 85.0% to 90.0%,
using the optimized prompt with a cheaper model did not improve performance.
Overall, this study's findings suggest that DSPy's systematic prompt
optimization can enhance LLM performance, particularly when instruction tuning
and example selection are optimized together. However, the impact varies by
task, highlighting the importance of evaluating specific use cases in prompt
optimization research.

</details>


### [10] [Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs](https://arxiv.org/abs/2507.03659)
*Valentina Wu,Alexandra Mendes,Alexandre Abreu*

Main category: cs.SE

TL;DR: 论文提出了一种结合形式化验证与LLM驱动的自动程序修复工具，用于Dafny语言，显著提升了故障定位和修复的效率。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复技术依赖测试套件验证，但无法覆盖所有场景。形式化规范提供了更强的正确性标准，但调试和修复过程复杂耗时。

Method: 利用形式化规范（如前置条件、后置条件和不变式）作为故障定位和修复的依据，结合Hoare Logic和多种先进LLM（如GPT-4o mini）生成候选修复方案。

Result: 在DafnyBench基准测试中，故障定位准确率为89.6%，GPT-4o mini的修复成功率达74.18%。

Conclusion: 形式化推理与LLM驱动的程序合成相结合，为自动程序修复提供了高效且可靠的解决方案。

Abstract: Formal verification offers strong assurances of software correctness.
However, debugging and repairing the underlying faults can be complex and
time-consuming when verification fails. Automated Program Repair (APR) aims to
ease this by automatically identifying and fixing faults. Traditional APR
techniques often depend on test suites for validation, but these may fail to
capture all scenarios. In contrast, formal specifications provide stronger
correctness criteria for effective repairs.
  We present an innovative APR tool for Dafny, a verification-aware programming
language that uses formal specifications - including pre-conditions,
post-conditions, and invariants - as oracles for fault localization and repair.
Assuming the correctness of the specifications and focusing on arithmetic bugs,
we localize faults through a series of steps, which include using Hoare Logic
to determine the state of each statement within the program and
state-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.
The chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.
  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny
programs. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o
mini yielding the highest repair success rate (74.18%). These results highlight
the potential of combining formal reasoning with LLM-driven program synthesis
for automated program repair.

</details>


### [11] [Efficient Detection of Intermittent Job Failures Using Few-Shot Learning](https://arxiv.org/abs/2507.04173)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文提出了一种基于小样本学习（FSL）的新方法，用于检测持续集成中的间歇性任务失败，解决了现有方法误标和性能不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的解决方案依赖大量手动标注数据或启发式方法，导致间歇性任务失败被误标，影响检测性能。

Method: 通过小样本学习，微调小型语言模型生成丰富嵌入，用于训练分类器。仅需少量手动标注数据即可实现高性能。

Result: 新方法在6个项目中的F1分数达70-88%，显著优于现有方法（34-52%），且仅需12个样本即可实现。

Conclusion: 研究表明数据质量比数量更重要，提供了一种高效、实用的间歇性任务失败检测框架。

Abstract: One of the main challenges developers face in the use of continuous
integration (CI) and deployment pipelines is the occurrence of intermittent job
failures, which result from unexpected non-deterministic issues (e.g., flaky
tests or infrastructure problems) rather than regular code-related errors such
as bugs. Prior studies developed machine-learning (ML) models trained on large
datasets of job logs to classify job failures as either intermittent or
regular. As an alternative to costly manual labeling of large datasets, the
state-of-the-art (SOTA) approach leveraged a heuristic based on
non-deterministic job reruns. However, this method mislabels intermittent job
failures as regular in contexts where rerunning suspicious job failures is not
an explicit policy, and therefore limits the SOTA's performance in practice. In
fact, our manual analysis of 2,125 job failures from 5 industrial and 1
open-source projects reveals that, on average, 32\% of intermittent job
failures are mislabeled as regular. To address these limitations, this paper
introduces a novel approach to intermittent job failure detection using
few-shot learning (FSL). Specifically, we fine-tune a small language model
using a few number of manually labeled log examples to generate rich
embeddings, which are then used to train an ML classifier. Our FSL-based
approach achieves 70-88\% F1-score with only 12 shots in all projects,
outperforming the SOTA, which proved ineffective (34-52\% F1-score) in 4
projects. Overall, this study underlines the importance of data quality over
quantity and provides a more efficient and practical framework for the
detection of intermittent job failures in organizations.

</details>


### [12] [From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law](https://arxiv.org/abs/2507.04185)
*Aniket Kesari,Travis Breaux,Tom Norton,Sarah Santos,Anmol Singhal*

Main category: cs.SE

TL;DR: 研究了如何利用大语言模型（LLM）在需求工程中弥合法律要求与技术实现之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法（如CCPA）将“同意”作为合法依据，但软件开发过程不透明，导致法律要求难以技术实现。

Method: 采用三步流程：LLM分类软件用例合规性，生成非合规用例的修改建议，并人工验证修改是否符合法律标准。

Result: 初步发现LLM在自动化合规任务中有潜力，但其推理能力有限。

Conclusion: 研究表明AI驱动解决方案能提升软件法律合规性，但其局限性需进一步研究。

Abstract: Privacy law and regulation have turned to "consent" as the legitimate basis
for collecting and processing individuals' data. As governments have rushed to
enshrine consent requirements in their privacy laws, such as the California
Consumer Privacy Act (CCPA), significant challenges remain in understanding how
these legal mandates are operationalized in software. The opaque nature of
software development processes further complicates this translation. To address
this, we explore the use of Large Language Models (LLMs) in requirements
engineering to bridge the gap between legal requirements and technical
implementation. This study employs a three-step pipeline that involves using an
LLM to classify software use cases for compliance, generating LLM modifications
for non-compliant cases, and manually validating these changes against legal
standards. Our preliminary findings highlight the potential of LLMs in
automating compliance tasks, while also revealing limitations in their
reasoning capabilities. By benchmarking LLMs against real-world use cases, this
research provides insights into leveraging AI-driven solutions to enhance legal
compliance of software.

</details>


### [13] [Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing](https://arxiv.org/abs/2507.04354)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Kexin Zhao,An Guo,Zhenyu Chen*

Main category: cs.SE

TL;DR: 该论文提出了一种名为ModelMeta的模型级蜕变测试方法，用于解决深度学习框架测试中的挑战，通过多样化接口组合和精细分析来检测bug。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中的bug可能导致严重后果，但现有测试方法因浮点误差、随机性和测试输入复杂性而效果有限，且缺乏合适的测试预言。

Method: 提出ModelMeta方法，基于深度学习模型的结构特性设计四种蜕变关系，利用QR-DQN策略生成多样化测试输入，并分析训练损失/梯度、内存/GPU使用和执行时间。

Result: ModelMeta能够生成多样化的测试输入，并通过精细分析检测多接口组合和运行时指标相关的bug。

Conclusion: ModelMeta克服了现有方法的局限性，为深度学习框架提供了一种更有效的测试方法。

Abstract: Deep learning (DL) frameworks are essential to DL-based software systems, and
framework bugs may lead to substantial disasters, thus requiring effective
testing. Researchers adopt DL models or single interfaces as test inputs and
analyze their execution results to detect bugs. However, floating-point errors,
inherent randomness, and the complexity of test inputs make it challenging to
analyze execution results effectively, leading to existing methods suffering
from a lack of suitable test oracles. Some researchers utilize metamorphic
testing to tackle this challenge. They design Metamorphic Relations (MRs) based
on input data and parameter settings of a single framework interface to
generate equivalent test inputs, ensuring consistent execution results between
original and generated test inputs. Despite their promising effectiveness, they
still face certain limitations. (1) Existing MRs overlook structural
complexity, limiting test input diversity. (2) Existing MRs focus on limited
interfaces, which limits generalization and necessitates additional
adaptations. (3) Their detected bugs are related to the result consistency of
single interfaces and far from those exposed in multi-interface combinations
and runtime metrics (e.g., resource usage). To address these limitations, we
propose ModelMeta, a model-level metamorphic testing method for DL frameworks
with four MRs focused on the structure characteristics of DL models. ModelMeta
augments seed models with diverse interface combinations to generate test
inputs with consistent outputs, guided by the QR-DQN strategy. It then detects
bugs through fine-grained analysis of training loss/gradients, memory/GPU
usage, and execution time.

</details>


### [14] [DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation](https://arxiv.org/abs/2507.04360)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Yinglong Zou,Tao Zheng,Zhenyu Chen*

Main category: cs.SE

TL;DR: 提出了一种新的深度学习框架测试方法DevMuT，通过开发者的专业知识生成更有效的测试模型，显著提升了缺陷检测的多样性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法检测的缺陷多为边缘案例或被开发者忽略的问题，而DevMuT旨在识别对开发者真正重要的缺陷。

Method: 采用开发者常见的操作作为突变操作，结合约束条件生成测试模型，覆盖深度学习模型生命周期的多个阶段。

Result: 在三个主流DL框架上的实验表明，DevMuT在模型生成多样性和合法性上显著优于基线方法，并检测到117个缺陷，其中63个被确认，24个已修复。

Conclusion: DevMuT有效检测了接近真实场景且对开发者重要的缺陷，已被实际应用于MindSpore社区。

Abstract: Deep learning (DL) frameworks are the fundamental infrastructure for various
DL applications. Framework defects can profoundly cause disastrous accidents,
thus requiring sufficient detection. In previous studies, researchers adopt DL
models as test inputs combined with mutation to generate more diverse models.
Though these studies demonstrate promising results, most detected defects are
considered trivial (i.e., either treated as edge cases or ignored by the
developers). To identify important bugs that matter to developers, we propose a
novel DL framework testing method DevMuT, which generates models by adopting
mutation operators and constraints derived from developer expertise. DevMuT
simulates developers'common operations in development and detects more diverse
defects within more stages of the DL model lifecycle (e.g., model training and
inference). We evaluate the performance of DevMuT on three widely used DL
frameworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine
types of industry tasks. The experiment results show that DevMuT outperforms
state-of-the-art baselines: it can achieve at least 71.68% improvement on
average in the diversity of generated models and 28.20% improvement on average
in the legal rates of generated models. Moreover, DevMuT detects 117 defects,
63 of which are confirmed, 24 are fixed, and eight are of high value confirmed
by developers. Finally, DevMuT has been deployed in the MindSpore community
since December 2023. These demonstrate the effectiveness of DevMuT in detecting
defects that are close to the real scenes and are of concern to developers.

</details>


### [15] [Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered](https://arxiv.org/abs/2507.04390)
*Vanesya Aura Ardity,Yusuf Sulistyo Nugroho,Syful Islam*

Main category: cs.SE

TL;DR: 该研究分析了Stack Overflow上React相关问题回答率和难度的关联因素，发现视图数、代码片段、用户声誉等正面影响回答率，而评论数、问题长度和图片则降低回答概率。


<details>
  <summary>Details</summary>
Motivation: 尽管React流行，但许多相关问题在Stack Overflow上仍未解答，研究旨在识别其影响因素。

Method: 采用文本挖掘和统计分析，包括逻辑回归分析回答率和线性回归分析用户声誉与问题难度的关系。

Result: 代码片段和用户声誉提升回答率，评论和问题长度则降低回答概率；用户声誉越高，问题难度越低。

Conclusion: 研究为技术问答平台用户提供了提问策略参考，需关注回答率相关因素。

Abstract: React is a popular JavaScript framework in modern web application
development. Due to its high performance and efficiency, many developers use
this framework. Although React library offers many advantages, it is not
without its challenges. When using React library, developers often face
problems where they often seek solutions through question-and-answer forums,
such as Stack Overflow (SO). However, despite its high popularity, many
React-related questions on SO remain unanswered. Thus, this study aims to
analyze the factors associated with question answerability and difficulty
levels of React-related questions on SO. To facilitate our study, Exploratory
Data Analysis was applied to 534,820 questions, where they are filtered based
on 23 React-related tags. We implemented a quantitative approach through text
mining and statistical analysis. A logistic regression model was used to
identify attributes associated with question answerability, while a simple
linear regression model was employed to examine the correlation between user
reputations and performance difficulty scores (PD Score). The results show that
some attributes, such as number of views, code snippet inclusion, number of
lines of code, and user reputation, positively affect the likelihood of
question answerability. In contrast, the number of comments, question lengths,
and presence of images in React-related questions reduce the probability of a
question receiving responses from users. Further investigation indicates a
negative correlation between user reputations and PD Score, where reputation
increase corresponds to -0.092 reduction in PD score, signaling experienced
users tend to propose more complex technical inquiries. This study provides
insights into the characteristics of technical question-and-answer platforms,
such as SO, that users need to consider the answerability factors when posting
questions related to React.

</details>


### [16] [Learning Software Bug Reports: A Systematic Literature Review](https://arxiv.org/abs/2507.04422)
*Guoming Long,Jingzhi Gong,Hui Fang,Tao Chen*

Main category: cs.SE

TL;DR: 论文进行了关于机器学习在bug报告分析中的系统文献综述，总结了当前研究的趋势和不足，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习在软件工程中的应用日益重要，但缺乏对bug报告分析领域的全面综述，因此本文旨在填补这一空白。

Method: 通过对1,825篇论文的系统筛选，选取204篇进行详细分析，总结了七项关键发现。

Result: 发现CNN、LSTM等模型广泛应用，但BERT等复杂模型使用较少；Word2Vec和TF-IDF流行；Eclipse和Mozilla是最常研究的项目。

Conclusion: 本研究为实践者提供了有用见解，并提出了六个未来研究方向。

Abstract: The recent advancement of artificial intelligence, especially machine
learning (ML), has significantly impacted software engineering research,
including bug report analysis. ML aims to automate the understanding,
extraction, and correlation of information from bug reports. Despite its
growing importance, there has been no comprehensive review in this area. In
this paper, we present a systematic literature review covering 1,825 papers,
selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive
use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like
BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular
for feature representation, with a rise in deep learning approaches. 3) Stop
word removal is the most common preprocessing, with structural methods rising
after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software
projects. 5) Bug categorization is the most common task, followed by bug
localization and severity prediction. 6) There is increasing attention on
specific bugs like non-functional and performance bugs. 7) Common evaluation
metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold
cross-validation preferred for model evaluation. 8) Many studies lack robust
statistical tests. We also identify six promising future research directions to
provide useful insights for practitioners.

</details>


### [17] [SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection](https://arxiv.org/abs/2507.04548)
*Renato Cordeiro Ferreira,Dayanne Gomes,Vitor Tamae,Francisco Wernke,Alfredo Goldman*

Main category: cs.SE

TL;DR: 总结SPIRA系统的构建经验，用于从声音中检测呼吸功能不全，并分享在数据收集、训练和推理过程中的教训。


<details>
  <summary>Details</summary>
Motivation: 解决呼吸功能不全的早期检测问题，通过智能系统从声音中识别相关症状。

Method: 构建SPIRA系统，经历了两代架构实现，总结了数据收集、训练和推理的挑战与解决方案。

Result: 成功开发SPIRA系统，为未来类似项目提供了经验教训。

Conclusion: SPIRA系统的构建过程为呼吸功能不全检测提供了实用经验，未来项目可借鉴其中的教训。

Abstract: Respiratory insufficiency is a medic symptom in which a person gets a reduced
amount of oxygen in the blood. This paper reports the experience of building
SPIRA: an intelligent system for detecting respiratory insufficiency from
voice. It compiles challenges faced in two succeeding implementations of the
same architecture, summarizing lessons learned on data collection, training,
and inference for future projects in similar systems.

</details>


### [18] [Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework](https://arxiv.org/abs/2507.04555)
*Gabriella Waters*

Main category: cs.SE

TL;DR: 本文提出了一种针对数字孪生模型的全面测试与验证框架（TEVV），旨在解决其复杂性和动态性带来的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生技术在复杂系统建模和模拟中的广泛应用，确保其准确性、可靠性和伦理实现变得至关重要。

Method: 论文提出了一个名为TEVV（测试、评估、验证和确认）的综合框架，专门用于数字孪生模型的开发和验证。

Result: 该框架能够有效应对数字孪生模型的动态性和复杂性，为其在决策支持中的应用提供可靠保障。

Conclusion: TEVV框架为数字孪生技术的进一步发展和应用奠定了坚实的基础，尤其是在确保模型质量和伦理合规方面。

Abstract: Digital twins have emerged as a powerful technology for modeling and
simulating complex systems across various domains (Fuller et al., 2020; Tao et
al., 2019). As virtual representations of physical assets, processes, or
systems, digital twins enable real-time monitoring, predictive analysis, and
optimization. However, as digital twins become more sophisticated and integral
to decision-making processes, ensuring their accuracy, reliability, and ethical
implementation is essential. This paper presents a comprehensive framework for
the Testing, Evaluation, Verification and Validation (TEVV) of digital twins to
address the unique challenges posed by these dynamic and complex virtual
models.

</details>


### [19] [Supporting Software Formal Verification with Large Language Models: An Experimental Study](https://arxiv.org/abs/2507.04857)
*Weiqi Wang,Marie Farrell,Lucas C. Cordeiro,Liping Zhao*

Main category: cs.SE

TL;DR: SpecVerify结合大型语言模型（LLMs）与形式化验证工具，自动化验证需求，展示46.5%的准确率，并减少了假阳性情况。


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言需求自动提取属性的难题，提升形式化验证的灵活性。

Method: 整合Claude 3.5 Sonnet与ESBMC验证器，生成自动化工作流程。

Result: 在9个洛克希德·马丁的系统中验证，效果与NASA的CoCoSim相当，但假阳性更低。

Conclusion: LLMs能显著降低形式化验证门槛，但高质量需求文档和人工监控仍是关键。

Abstract: Formal methods have been employed for requirements verification for a long
time. However, it is difficult to automatically derive properties from natural
language requirements. SpecVerify addresses this challenge by integrating large
language models (LLMs) with formal verification tools, providing a more
flexible mechanism for expressing requirements. This framework combines Claude
3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on
nine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%
verification accuracy, comparable to NASA's CoCoSim, but with lower false
positives. Our framework formulates assertions that extend beyond the
expressive power of LTL and identifies falsifiable cases that are missed by
more traditional methods. Counterexample analysis reveals CoCoSim's limitations
stemming from model connection errors and numerical approximation issues. While
SpecVerify advances verification automation, our comparative study of Claude,
ChatGPT, and Llama shows that high-quality requirements documentation and human
monitoring remain critical, as models occasionally misinterpret specifications.
Our results demonstrate that LLMs can significantly reduce the barriers to
formal verification, while highlighting the continued importance of
human-machine collaboration in achieving optimal results.

</details>


### [20] [Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2507.04871)
*Jerome Pfeiffer,Jingxi Zhang,Benoit Combemale,Judith Michael,Bernhard Rumpe,Manuel Wimmer,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文提出了一种统一的数字孪生参考模型，旨在弥合抽象概念与工业实践之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生的定义和参考模型过于抽象，导致其理解和实施指导不足，工业实践与理论间存在显著鸿沟。

Method: 分析现有流行的数字孪生参考模型，并将其整合为一个更详细的统一参考模型。

Result: 统一的参考模型减少了概念与实施之间的差距，促进了工业实践中数字孪生的工程化实现。

Conclusion: 该模型不仅增强了数字孪生概念及其关系的理解，还为开发者提供了有效的实施指导。

Abstract: Digital twins are sophisticated software systems for the representation,
monitoring, and control of cyber-physical systems, including automotive,
avionics, smart manufacturing, and many more. Existing definitions and
reference models of digital twins are overly abstract, impeding their
comprehensive understanding and implementation guidance. Consequently, a
significant gap emerges between abstract concepts and their industrial
implementations. We analyze popular reference models for digital twins and
combine these into a significantly detailed unifying reference model for
digital twins that reduces the concept-implementation gap to facilitate their
engineering in industrial practice. This enhances the understanding of the
concepts of digital twins and their relationships and guides developers to
implement digital twins effectively.

</details>


### [21] [Understanding Everything as Code: A Taxonomy and Conceptual Model](https://arxiv.org/abs/2507.05100)
*Haoran Wei,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 该研究通过多声文献综述系统分析了Everything as Code（EaC）的现有知识，提出了包含25种实践的分类法和概念模型，填补了学术空白。


<details>
  <summary>Details</summary>
Motivation: EaC作为一种新兴范式，缺乏明确的行业标准和学术研究，本研究旨在澄清其范围和边界，并为实践者提供指导。

Method: 采用多声文献综述（MLR）方法，综合学术和灰色文献，定量和主题分析发现，开发并验证了EaC的分类法和概念模型。

Result: 提出了包含25种EaC实践的分类法及概念模型，展示了实践间的相互作用，并通过业界合作开发了代码示例。

Conclusion: 研究填补了EaC学术空白，增强了概念清晰度，为实践者和未来研究提供了基础。

Abstract: Background: Everything as Code (EaC) is an emerging paradigm aiming to codify
all aspects of modern software systems. Despite its growing popularity,
comprehensive industry standards and peer-reviewed research clarifying its
scope and guiding its adoption remain scarce. Aims: This study systematically
analyzes existing knowledge and perceptions of EaC, clarifies its scope and
boundaries, and provides structured guidance for researchers and practitioners.
Method: We conducted a large-scale multivocal literature review (MLR),
synthesizing academic and grey literature sources. Findings were analyzed
quantitatively and thematically. Based on this analysis, we developed a
taxonomy and conceptual model of EaC, validated through collaboration with
industry experts. Results: The resulting taxonomy comprises 25 distinct EaC
practices organized into six layers based on industry awareness and functional
roles. The conceptual model illustrates focus areas, overlaps, and interactions
among these EaC practices within the software delivery lifecycle. Additionally,
practical code examples demonstrating the implementation of these practices
were developed in collaboration with industry experts. Conclusions: This work
addresses the current scarcity of academic discourse on EaC by providing the
first comprehensive taxonomy and conceptual model. These contributions enhance
conceptual clarity, offer actionable guidance to practitioners, and lay the
groundwork for future research in this emerging domain.

</details>


### [22] [In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code](https://arxiv.org/abs/2507.05200)
*Susmita Das,Madhusudan Ghosh,Priyanka Swami,Debasis Ganguly,Gul Calikli*

Main category: cs.SE

TL;DR: 本文提出了一种基于上下文学习的方法，用于在没有测试用例的情况下评估LLM生成代码的功能正确性，通过少样本示例提高代码质量估计的性能。


<details>
  <summary>Details</summary>
Motivation: 在快速开发或功能驱动的软件项目中，LLM生成的代码缺乏测试用例时，需要一种可靠的方法来评估其功能正确性。

Method: 采用上下文学习（ICL）方法，通过提供少量功能正确的代码示例，改进现有的查询性能预测（QPP）方法和零样本方法的性能。

Result: 实验结果表明，少样本示例能显著提升代码质量估计的准确性。

Conclusion: 上下文学习方法在代码质量估计中有效，为生成式软件开发提供了实用工具。

Abstract: When applying LLM-based code generation to software development projects that
follow a feature-driven or rapid application development approach, it becomes
necessary to estimate the functional correctness of the generated code in the
absence of test cases. Just as a user selects a relevant document from a ranked
list of retrieved ones, a software generation workflow requires a developer to
choose (and potentially refine) a generated solution from a ranked list of
alternative solutions, ordered by their posterior likelihoods. This implies
that estimating the quality of a ranked list -- akin to estimating "relevance"
for query performance prediction (QPP) in IR -- is also crucial for generative
software development, where quality is defined in terms of "functional
correctness". In this paper, we propose an in-context learning (ICL) based
approach for code quality estimation. Our findings demonstrate that providing
few-shot examples of functionally correct code from a training set enhances the
performance of existing QPP approaches as well as a zero-shot-based approach
for code quality estimation.

</details>


### [23] [An Investigation into Maintenance Support for Neural Networks](https://arxiv.org/abs/2507.05245)
*Fatema Tuz Zohra,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文探讨了神经网络维护中的测试、调试和质量保障问题，指出了传统软件工程方法在神经网络领域的不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在日常生活中的应用增加，确保其质量变得至关重要。然而，传统方法在神经网络维护中存在显著的研究和实践空白，尤其是对不良行为的理解和缓解方法。

Method: 通过访谈和调查，收集从业者的见解，分析当前研究与实践的现状，评估现有工具的局限性。

Result: 研究发现，现有工具主要集中在模型的构建和训练上，但对理解及解决模型意外行为的支持不足。

Conclusion: 研究旨在提供开发者视角，指出当前实践的不足，并为改进神经网络维护支持提供方向。

Abstract: As the potential for neural networks to augment our daily lives grows,
ensuring their quality through effective testing, debugging, and maintenance is
essential. This is especially the case as we acknowledge the prospects of
negative impacts from these technologies. Traditional software engineering
methods, such as testing and debugging, have proven effective in maintaining
software quality; however, they reveal significant research and practice gaps
in maintaining neural networks. In particular, there is a limited understanding
of how practitioners currently address challenges related to understanding and
mitigating undesirable behaviors in neural networks. In our ongoing research,
we explore the current state of research and practice in maintaining neural
networks by curating insights from practitioners through a preliminary study
involving interviews and supporting survey responses. Our findings thus far
indicate that existing tools primarily concentrate on building and training
models. While these tools can be beneficial, they often fall short of
supporting practitioners' understanding and addressing the underlying causes of
unexpected model behavior. By evaluating current procedures and identifying the
limitations of traditional methodologies, our study aims to offer a
developer-centric perspective on where current practices fall short and
highlight opportunities for improving maintenance support in neural networks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [24] [Towards Automatic Error Recovery in Parsing Expression](https://arxiv.org/abs/2507.03629)
*Sérgio Queiroz de Medeiros,Fabio Mascarenhas*

Main category: cs.PL

TL;DR: 论文提出了一种自动为PEG语法添加标签和恢复表达式的算法，以简化错误恢复解析器的开发。在Titan语言的解析器上进行了验证，效果良好。


<details>
  <summary>Details</summary>
Motivation: 集成开发环境（IDE）中的解析器需要支持错误恢复功能，以便即使程序有语法错误也能构建抽象语法树。手动为PEG语法添加标签和恢复表达式很困难，因此需要自动化解决方案。

Method: 提出一种算法，自动为PEG语法添加标签并构建对应的恢复表达式。在Titan编程语言的解析器上进行了测试。

Result: 算法在Titan语言的解析器中效果良好，只需少量人工干预即可实现错误恢复功能，尤其适用于大多数备选方案不重叠的PEG语法。

Conclusion: 自动化算法能够显著简化PEG语法中错误恢复解析器的开发，尤其适用于不重叠的语法结构，为IDE中的解析器提供了实用工具。

Abstract: Error recovery is an essential feature for a parser that should be plugged in
Integrated Development Environments (IDEs), which must build Abstract Syntax
Trees (ASTs) even for syntactically invalid programs in order to offer features
such as automated refactoring and code completion.
  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes
recursive top-down parsers using a restricted form of backtracking. Labeled
failures are a conservative extension of PEGs that adds an error reporting
mechanism for PEG parsers, and these labels can also be associated with
recovery expressions to also be an error recovery mechanism. These expressions
can use the full expressivity of PEGs to recover from syntactic errors.
  Manually annotating a large grammar with labels and recovery expressions can
be difficult. In this work, we present an algorithm that automatically
annotates a PEG with labels, and builds their corresponding recovery
expressions. We evaluate this algorithm by adding error recovery to the parser
of the Titan programming language. The results shown that with a small amount
of manual intervention our algorithm can be used to produce error recovering
parsers for PEGs where most of the alternatives are disjoint.

</details>


### [25] [Semantically Separating Nominal Wyvern for Usability and Decidability](https://arxiv.org/abs/2507.03867)
*Yu Xiang Zhu,Amos Robinson,Sophia Roshal,Timothy Mou,Julian Mackay,Jonathan Aldrich,Alex Potanin*

Main category: cs.PL

TL;DR: Nominal Wyvern提出了一种DOT类依赖类型系统，通过名义声明与结构精化的分离，解决了DOT中递归类型导致的可判定性问题，同时保留了表达力。


<details>
  <summary>Details</summary>
Motivation: DOT算法结合了函数式和面向对象特性，但增加了子类型可判定性的难度。论文旨在提出一种新方法，既保留表达力又确保可判定性。

Method: 采用名义声明与结构精化的分离设计，避免不可判定的递归类型，并借鉴了material/shape分离技术。

Result: 设计了Nominal Wyvern类型系统，实现了子类型可判定性，同时保持了F有界多态和模块系统的表达力。

Conclusion: Nominal Wyvern通过分离设计解决了DOT的可判定性问题，为用户提供了直观且强大的类型系统。

Abstract: The Dependent Object Types (DOT) calculus incorporates concepts from
functional languages (e.g. modules) with traditional object-oriented features
(e.g. objects, subtyping) to achieve greater expressivity (e.g. F-bounded
polymorphism). However, this merger of paradigms comes at the cost of subtype
decidability. Recent work on bringing decidability to DOT has either sacrificed
expressiveness or ease of use. The unrestricted construction of recursive types
and type bounds has made subtype decidability a much harder problem than in
traditional object-oriented programming.
  Recognizing this, our paper introduces Nominal Wyvern, a DOT-like dependent
type system that takes an alternative approach: instead of having a uniform
structural syntax like DOT, Nominal Wyvern is designed around a "semantic
separation" between the nominal declaration of recursive types on the one hand,
and the structural refinement of those types when they are used on the other.
This design naturally guides the user to avoid writing undecidably recursive
structural types.
  From a technical standpoint, this separation also makes guaranteeing
decidability possible by allowing for an intuitive adaptation of material/shape
separation, a technique for achieving subtype decidability by separating types
responsible for subtyping constraints from types that represent concrete data.
The result is a type system with syntax and structure familiar to OOP users
that achieves decidability without compromising the expressiveness of F-bounded
polymorphism and module systems as they are used in practice.

</details>


### [26] [CCR 2.0: High-level Reasoning for Conditional Refinements](https://arxiv.org/abs/2507.04298)
*Youngju Song,Minki Cho*

Main category: cs.PL

TL;DR: 本文提出了一种改进的模型CCR 2.0，结合了精化和分离逻辑的优点，提供了更好的组合性定理和隐藏模型细节的证明技术。


<details>
  <summary>Details</summary>
Motivation: 结合精化和分离逻辑的互补优势，提出统一的机制CCR 1.0，并进一步改进为CCR 2.0。

Method: 开发了新的推理原则，改进了组合性定理，并设计了隐藏模型细节的证明技术。

Result: CCR 2.0在Coq中实现，具有更好的组合性和用户友好性。

Conclusion: CCR 2.0成功融合了两种方法的优点，解决了非平凡反例带来的挑战。

Abstract: In recent years, great progress has been made in the field of formal
verification for low-level systems. Many of them are based on one of two
popular approaches: refinement or separation logic. These two approaches are
very different in nature and offer complementary benefits in terms of
compositionality. Recently, to fuse these benefits in a unified mechanism, a
new approach called Conditional Contextual Refinement (CCR 1.0 for short) was
proposed. In this paper, we advance the model of CCR 1.0 and provide novel and
intuitive reasoning principles, resulting in: CCR 2.0. Specifically, CCR 2.0
(i) comes with a better compositionality theorem, having the practical benefit
of facilitating more proof reuse, and (ii) provides a proof technique that
hides model-level (i.e., resources of the separation logic) details from the
user. Achieving this goal was challenging due to non-trivial counterexamples
which necessitated us to devise novel notions. Our results are formalized in
Coq.

</details>


### [27] [Retargeting an Abstract Interpreter for a New Language by Partial Evaluation](https://arxiv.org/abs/2507.04316)
*Jay Lee*

Main category: cs.PL

TL;DR: 提出了一种通过部分评估自动为不同语言生成抽象解释器的新技术，无需从头开发新语言的分析器。


<details>
  <summary>Details</summary>
Motivation: 减少开发静态分析器的时间和精力，实现抽象解释器的自动重定向。

Method: 利用部分评估技术，基于源语言的抽象解释器，通过目标语言的语义进行特化。

Result: 证明该方法可以有效地将一个语言的抽象解释器重定向为另一个语言的正确分析器。

Conclusion: 该方法显著简化了开发过程，为多语言静态分析提供了高效途径。

Abstract: It is well-known that abstract interpreters can be systematically derived
from their concrete counterparts using a "recipe," but developing sound static
analyzers remains a time-consuming task. Reducing the effort required and
mechanizing the process of developing analyzers continues to be a significant
challenge. Is it possible to automatically retarget an existing abstract
interpreter for a new language?
  We propose a novel technique to automatically derive abstract interpreters
for various languages from an existing abstract interpreter. By leveraging
partial evaluation, we specialize an abstract interpreter for a source
language. The specialization is performed using the semantics of target
languages written in the source language. Our approach eliminates the need to
develop analyzers for new targets from scratch. We show that our method can
effectively retarget an abstract interpreter for one language into a correct
analyzer for another language.

</details>


### [28] [React-tRace: A Semantics for Understanding React Hooks](https://arxiv.org/abs/2507.05234)
*Jay Lee,Joongwon Ahn,Kwangkeun Yi*

Main category: cs.PL

TL;DR: React-tRace是一个形式化React Hooks语义的框架，通过理论和实证验证其准确性，并提供可视化工具帮助开发者理解Hooks的行为。


<details>
  <summary>Details</summary>
Motivation: React Hooks的语义对开发者来说不够透明，容易导致UI错误，因此需要形式化其语义以提高理解。

Method: 提出React-tRace框架，通过定义解释器和测试套件验证其准确性，并开发可视化工具。

Result: 理论证明React-tRace捕获了Hooks的固有特性，实证测试验证了其与React行为的一致性。

Conclusion: React-tRace形式和可视化工具有效帮助开发者理解Hooks的语义，减少UI错误。

Abstract: React has become the most widely used web front-end framework, enabling the
creation of user interfaces in a declarative and compositional manner. Hooks
are a set of APIs that manage side effects in functional components in React.
However, their semantics are often seen as opaque to developers, leading to UI
bugs. In this paper, we formalize the semantics of the essence of React Hooks
we name React-tRace, providing a framework that clarifies their behavior. We
demonstrate that our model captures the behavior of React, by theoretically
showing that it embodies essential properties of Hooks and empirically
comparing our React-tRace-definitional interpreter against a test suite.
Furthermore, we showcase a practical visualization tool based on the
formalization to demonstrate how developers can better understand the semantics
of Hooks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [29] [On Combining Two Server Control Policies for Energy Efficiency](https://arxiv.org/abs/2507.03510)
*Jingze Dai,Douglas G. Down*

Main category: cs.PF

TL;DR: 研究探讨服务器速度调整和开关机制在节能中的协同效应，发现两者之间缺乏显著的协同作用。


<details>
  <summary>Details</summary>
Motivation: 探索服务器速度调整和开关机制在节能中是否存在协同效应，以优化能源消耗并保持性能。

Method: 使用连续时间马尔可夫链模型，服务器可开关且速度可调为正常或低速，分析线性成本函数下的表现。

Result: 两种机制在所有系统负载下均缺乏显著协同效应，其中一种机制占主导地位，另一种仅带来小幅成本降低。

Conclusion: 服务器节能机制的选择应以单一的占优策略为主，结合另一种机制带来的额外收益有限。

Abstract: Two popular server control policies are available for reducing energy
consumption while maintaining acceptable performance levels: server speed
scaling and the ability to turn servers off (and on). In this work, we explore
the question of whether there are synergistic effects between these two
mechanisms. To do this, we employ a continuous-time Markov chain model where
the server can be turned off (and turning the server back on takes some time)
and where the speed of the server can take on two values: a nominal operating
speed and a reduced operating speed. For a cost function that is linear in the
mean response time and server power consumption, we suggest that the mechanisms
are not synergistic in that for all system loads, one mechanism is dominant in
that if the other mechanism is also employed, there is only a small decrease in
cost.

</details>


### [30] [Affine Frequency Division Multiplexing Over Wideband Doubly-Dispersive Channels With Time-Scaling Effects](https://arxiv.org/abs/2507.03537)
*Xiangxiang Li,Haiyan Wang,Yao Ge,Xiaohong Shen,Yong Liang Guan,Miaowen Wen,Chau Yuen*

Main category: cs.PF

TL;DR: 研究了AFDM调制在超宽带双弥散信道中的时间缩放效应，提出了一种高效传输结构，并优化了AFDM系统参数和检测算法。


<details>
  <summary>Details</summary>
Motivation: 现有的AFDM调制技术在窄带双弥散信道中表现出色，但在超宽带信道中的时间缩放效应未被研究，需探索其适应性。

Method: 提出带有CPP和CPS的传输结构，优化AFDM的调频参数，并开发CD-D-OAMP检测算法及其状态演化分析。

Result: 优化的AFDM系统在超宽带信道中优于现有方案，CD-D-OAMP检测器实现了复杂度和性能的理想平衡。

Conclusion: AFDM系统在超宽带场景中具有潜力，优化的参数和检测算法提升了性能并降低了延迟。

Abstract: The recently proposed affine frequency division multiplexing (AFDM)
modulation has been considered as a promising technology for narrowband
doubly-dispersive channels. However, the time-scaling effects, i.e., pulse
widening and pulse shortening phenomena, in extreme wideband doubly-dispersive
channels have not been considered in the literatures. In this paper, we
investigate such wideband transmission and develop an efficient transmission
structure with chirp-periodic prefix (CPP) and chirp-periodic suffix (CPS) for
AFDM system. We derive the input-output relationship of AFDM system under
time-scaled wideband doubly-dispersive channels and demonstrate the sparsity in
discrete affine Fourier (DAF) domain equivalent channels. We further optimize
the AFDM chirp parameters to accommodate the time-scaling characteristics in
wideband doubly-dispersive channels and verify the superiority of the derived
chirp parameters by pairwise error probability (PEP) analysis. We also develop
an efficient cross domain distributed orthogonal approximate message passing
(CD-D-OAMP) algorithm for AFDM symbol detection and analyze its corresponding
state evolution. By analyzing the detection complexity of CD-D-OAMP detector
and evaluating the error performance of AFDM systems based on simulations, we
demonstrate that the AFDM system with our optimized chirp parameters
outperforms the existing competitive modulation schemes in time-scaled wideband
doubly-dispersive channels. Moreover, our proposed CD-D-OAMP detector can
achieve the desirable trade-off between the complexity and performance, while
supporting parallel computing to significantly reduce the computational
latency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [31] [An End-to-End Assurance Framework for AI/ML Workloads in Datacenters](https://arxiv.org/abs/2507.03158)
*Jit Gupta,Tarun Banka,Rahul Gupta,Mithun Dharmaraj,Jasleen Kaur*

Main category: cs.NI

TL;DR: 论文展示了基于SaaS的AI/ML工作负载性能监控与自动化故障排除，利用跨层遥测数据解决分布式训练中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 分布式AI/ML工作负载的性能问题复杂，需跨层分析以快速定位根源并提供修复方案。

Method: 通过收集应用遥测、通信日志、GPU健康指标等跨层数据，构建依赖图和自动化根因分析工具。

Result: 展示了多种用例，如跨层依赖图、服务级别期望和GPU间应用路径追踪，实现端到端性能保障。

Conclusion: SaaS化监控与自动化工具能有效提升分布式AI/ML工作负载的性能可靠性和故障恢复效率。

Abstract: Modern machine learning workloads such as large language model training,
fine-tuning jobs are highly distributed and span across hundreds of systems
with multiple GPUs. Job completion time for these workloads is the artifact of
the application, compute, network and storage performance. In case of failure
or degraded performance it is imperative to understand the root cause and
possible remediation for the problem for end-to-end assurance. This demo
showcases SaaSbased observability and automated troubleshooting for AI/ML
workload performance issues using cross-layer telemetry and logs (e.g.,
Application telemetry, Collective communication logs, GPU Health metrics,
Network Flow Data, NIC ROCEv2 telemetry). Different use cases are demonstrated
for end-to-end assurance such as Cross-layer Dependency Graph, Cross-layer
Service Level Expectations, Automated Root Cause Analysis, GPU-toGPU
application path tracing.

</details>


### [32] [RCA Copilot: Transforming Network Data into Actionable Insights via Large Language Models](https://arxiv.org/abs/2507.03224)
*Alexander Shan,Jasleen Kaur,Rahul Singh,Tarun Banka,Raj Yavatkar,T. Sridhar*

Main category: cs.NI

TL;DR: 论文介绍了RCACopilot系统，结合统计测试和大语言模型推理，自动化网络问题的根因分析，并为工程师提供解释和解决步骤。


<details>
  <summary>Details</summary>
Motivation: 传统的根因分析方法耗时且难以理解，统计推断方法缺乏解释性，工程师难以信任黑盒模型的预测。

Method: RCACopilot结合统计测试和大语言模型（LLM）推理，自动收集和合成运行时的诊断信息，预测根因并提供解释和解决步骤。

Result: RCACopilot为操作员提供了准确且实用的支持，解决了传统方法的不足。

Conclusion: RCACopilot通过LLM技术提升了根因分析的效率和可解释性，为工程师提供了高效的支持。

Abstract: Ensuring the reliability and availability of complex networked services
demands effective root cause analysis (RCA) across cloud environments, data
centers, and on-premises networks. Traditional RCA methods, which involve
manual inspection of data sources such as logs and telemetry data, are often
time-consuming and challenging for on-call engineers. While statistical
inference methods have been employed to estimate the causality of network
events, these approaches alone are similarly challenging and suffer from a lack
of interpretability, making it difficult for engineers to understand the
predictions made by black-box models. In this paper, we present RCACopilot, an
advanced on-call system that combines statistical tests and large language
model (LLM) reasoning to automate RCA across various network environments.
RCACopilot gathers and synthesizes critical runtime diagnostic information,
predicts the root cause of incidents, provides a clear explanatory narrative,
and offers targeted action steps for engineers to resolve the issues. By
utilizing LLM reasoning techniques and retrieval, RCACopilot delivers accurate
and practical support for operators.

</details>


### [33] [OpenSN: An Open Source Library for Emulating LEO Satellite Networks](https://arxiv.org/abs/2507.03248)
*Wenhao Lu,Zhiyuan Wang,Hefan Zhang,Shan Zhang,Hongbin Luo*

Main category: cs.NI

TL;DR: OpenSN是一个开源库，用于模拟大规模卫星网络，具有高效、可扩展和功能可扩展的优势，比现有工具更快且更灵活。


<details>
  <summary>Details</summary>
Motivation: 为了系统性、可重复地评估低地球轨道（LEO）卫星网络研究，需要一个高效、可扩展且功能丰富的模拟工具。

Method: OpenSN采用基于容器的虚拟化技术，简化与Docker命令行接口的交互，并通过键值数据库分离用户配置与容器网络管理。

Result: OpenSN构建大型星座的速度比StarryNet快5-10倍，更新链路状态比LeoEM快2-4倍，成功模拟了4408颗卫星的Starlink星座。

Conclusion: OpenSN在效率、可扩展性和功能扩展性上具有显著优势，是LEO卫星网络研究的宝贵工具。

Abstract: Low-earth-orbit (LEO) satellite constellations (e.g., Starlink) are becoming
a necessary component of future Internet. There have been increasing studies on
LEO satellite networking. It is a crucial problem how to evaluate these studies
in a systematic and reproducible manner. In this paper, we present OpenSN,
i.e., an open source library for emulating large-scale satellite network (SN).
Different from Mininet-based SN emulators (e.g., LeoEM), OpenSN adopts
container-based virtualization, thus allows for running distributed routing
software on each node, and can achieve horizontal scalability via flexible
multi-machine extension. Compared to other container-based SN emulators (e.g.,
StarryNet), OpenSN streamlines the interaction with Docker command line
interface and significantly reduces unnecessary operations of creating virtual
links. These modifications improve emulation efficiency and vertical
scalability on a single machine. Furthermore, OpenSN separates user-defined
configuration from container network management via a Key-Value Database that
records the necessary information for SN emulation. Such a separation
architecture enhances the function extensibility. To sum up, OpenSN exhibits
advantages in efficiency, scalability, and extensibility, thus is a valuable
open source library that empowers research on LEO satellite networking.
Experiment results show that OpenSN constructs mega-constellations 5X-10X
faster than StarryNet, and updates link state 2X-4X faster than LeoEM. We also
verify the scalability of OpenSN by successfully emulating the five-shell
Starlink constellation with a total of 4408 satellites.

</details>


### [34] [Low-power Wireless Network with Real-Time Guarantees for Edge-Cloud Applications](https://arxiv.org/abs/2507.03317)
*Don Tan*

Main category: cs.NI

TL;DR: 论文探讨了基于树莓派（RPI）构建可扩展、易部署的实时LoRa测试平台的可行性，通过实验验证了其性能指标及参数配置对通信稳定性和延迟的影响。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够将LoRa通信有效整合到实时环境中的大规模测试平台，因此研究基于RPI的解决方案。

Method: 使用多台RPI管理各自LoRa无线电，实验评估了性能指标（RSSI、SNR、PLR等），并比较了不同配置（扩频因子、传输频率）及多址接入方式（TDMA与CSMA）。

Result: 结果表明，在合适参数配置下，系统可实现稳定且低延迟的通信，具备实时操作的可行性。

Conclusion: 证实了RPI方案的理论可行性，未来工作将扩展单台RPI控制的无线电数量，实现真正并行传输及多RPI集成。

Abstract: The goal of this project is to explore the feasibility of building a scalable
& easy-to-deploy real-time LoRa testbed, made from multiple units of Raspberry
Pi (RPI), where each RPI manages its own set of LoRa radios. This project is
motivated by the lack of concrete large-scale LoRa testbeds that effectively
integrate LoRa communications into the real-time world. The paper introduces
how the idea of using RPI came about and why it should work in theory. The
paper then carries out experiments on a component of the large-scale testbed,
to evaluate the feasibility of the said component based on performance metrics
such as RSSI, SNR, PLR and the ability to carry out millisecond-accurate
transmissions. The performance metrics are also used to explore the impact of
using different combinations of spread factors and transmission frequencies, as
well as making comparisons between time-division multiple access (TDMA) and
carrier-sense multiple access (CSMA) approaches. The results show that with the
right parameters configured, the system can achieve stable and low-latency
communications, proving some feasibility to operate under real-time situations.
Future work includes giving each RPI control over more radios, carrying out
true parallel transmissions, and finally integrating multiple RPIs for a more
complete large-scale real-time LoRa testbed.

</details>


### [35] [AoI-Energy-Spectrum Optimization in Post-Disaster Powered Communication Intelligent Network via Hierarchical Heterogeneous Graph Neural Network](https://arxiv.org/abs/2507.03401)
*Hanjian Liu,Jinsong Gui*

Main category: cs.NI

TL;DR: 该论文设计了一个灾后供电通信智能网络（PDPCIN），利用无人机和低轨卫星解决通信中断问题，提出了多种优化机制，并通过HHGNN框架解决了多维度优化的复杂性。


<details>
  <summary>Details</summary>
Motivation: 灾后通信基础设施损坏导致通信中断，亟需一种高效、灵活的解决方案来恢复通信并为受灾区域提供持续支持。

Method: 结合无人机（UAV）和低轨卫星（LEO SATs），提出了IS-UAV架构、AFTU机制和DMLA策略，并利用HHGNN框架进行系统优化。

Result: 提出的方案在多目标优化（AoI、能效、频谱效率）上优于现有基准，并推导了AoI期望值和停滞AoI比例的表达式。

Conclusion: PDPCIN及相关优化机制为灾后通信恢复提供了有效解决方案。

Abstract: This paper designs a post-disaster powered communication intelligent network
(PDPCIN) to address communication disruptions caused by ground base station
(GBS) failures within the post-disaster area. PDPCIN employs unmanned aerial
vehicles (UAVs) to provide wireless data collection (WDC) and wireless energy
transmission (WET) for affected areas and leverages low earth orbit satellites
(LEO SATs) to relay UAV data to the nearest survival GBS. To ensure basic
post-disaster communication while co-optimizing age of information (AoI),
energy efficiency, and spectrum efficiency, intelligent synchronization-UAV
(IS-UAV) architecture, AoI-based four thresholds updating (AFTU) mechanism, and
Dynamic multi-LEO access (DMLA) strategy are proposed. However, three key
challenges remain: time-varying task-resource imbalances, complex topology
caused by multi-device scheduling, and nonlinear coupling in multidimensional
metric optimization, making system optimization NP-hard. Therefore, this paper
proposes a hierarchical heterogeneous graph neural networks (HHGNN) framework.
It models heterogeneous device nodes and their communication relations as a
hierarchical heterogeneous graph structure, integrating our defined graph
sensing, exchange, and mask layer to handle the network's input, feature
propagation, and output. To search appropriate number of single-LEO SATs, we
propose single-LEO SAT density optimization (S-LSDO) algorithm. Finally, we
compare the proposed scheme with state-of-the-art benchmarks to validate its
superior collaborative optimization of AoI, energy efficiency, and spectrum
efficiency. Based on this, we derive the expressions for the expected values of
AoI and stagnant AoI proportion.

</details>


### [36] [RateCount: Learning-Free Device Counting by Wi-Fi Probe Listening](https://arxiv.org/abs/2507.03873)
*Tianlang He,Zhangyu Chang,S. -H. Gary Chan*

Main category: cs.NI

TL;DR: 论文提出了一种名为RateCount的轻量级、无需学习的方法，用于在Wi-Fi设备随机化MAC地址的情况下准确计数，避免了机器学习部署的高成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的设备计数方法虽然准确，但存在数据清洗、模型训练和超参数调优等耗时问题，部署效率低。论文旨在提出一种更高效的计数方法。

Method: RateCount通过计算AP在窗口期内接收PRF的速率，使用无偏闭式表达式估计设备数量，并结合校准方案扩展到人员计数。

Result: 实验表明，RateCount无需机器学习部署成本，仍能达到与基于学习的计数方法相当的精度，并在人员计数上大幅超越现有方案。

Conclusion: RateCount提供了一种高效、轻量级的计数解决方案，解决了现有机器学习方法的部署效率问题，同时保持了计数准确性。

Abstract: A Wi-Fi-enabled device, or simply Wi-Fi device, sporadically broadcasts probe
request frames (PRFs) to discover nearby access points (APs), whether connected
to an AP or not. To protect user privacy, unconnected devices often randomize
their MAC addresses in the PRFs, known as MAC address randomization. While
prior works have achieved accurate device counting under MAC address
randomization, they typically rely on machine learning, resulting in
inefficient deployment due to the time-consuming processes of data cleaning,
model training, and hyperparameter tuning. To enhance deployment efficiency, we
propose RateCount, an accurate, lightweight, and learning-free counting
approach based on the rate at which APs receive PRFs within a window. RateCount
employs a provably unbiased closed-form expression to estimate the device count
time-averaged over the window and an error model to compute the lower bound of
the estimation variance. We also demonstrate how to extend RateCount to people
counting by incorporating a device-to-person calibration scheme. Through
extensive real-world experiments conducted at multiple sites spanning a wide
range of counts, we show that RateCount, without any deployment costs for
machine learning, achieves comparable counting accuracy with the
state-of-the-art learning-based device counting and improves previous people
counting schemes by a large margin.

</details>


### [37] [Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks](https://arxiv.org/abs/2507.03950)
*Yizhou Luo,Kwan-Wu Chin,Ruyi Guan,Xi Xiao,Caimeng Wang,Jingyin Feng,Tengjiao He*

Main category: cs.NI

TL;DR: 本文提出了一种基于无人机的物联网设备认证框架，利用太阳能充电站和深度强化学习优化无人机的飞行轨迹和充电计划，显著提高了认证效率和网络吞吐量。


<details>
  <summary>Details</summary>
Motivation: 物联网设备分布广泛且易受攻击，需要频繁认证，但传统方式效率低。

Method: 采用无人机辅助认证框架，结合太阳能充电站和深度强化学习优化充电计划与设备选择。

Result: 仿真结果显示，信任年龄平均减少88%，吞吐量损失减少30%。

Conclusion: 该方案高效且实用，适用于大规模物联网网络。

Abstract: Devices operating in Internet of Things (IoT) networks may be deployed across
vast geographical areas and interconnected via multi-hop communications.
Further, they may be unguarded. This makes them vulnerable to attacks and
motivates operators to check on devices frequently. To this end, we propose and
study an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in
IoT networks with a charging station powered by solar. A key challenge is
optimizing the trajectory of the UAV to ensure it attests as many devices as
possible. A trade-off here is that devices being checked by the UAV are
offline, which affects the amount of data delivered to a gateway. Another
challenge is that the charging station experiences time-varying energy
arrivals, which in turn affect the flight duration and charging schedule of the
UAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL)
solution to optimize the UAV's charging schedule and the selection of devices
to be attested during each flight. The simulation results show that our
solution reduces the average age of trust by 88% and throughput loss due to
attestation by 30%.

</details>


### [38] [In-Network Memory Access: Bridging SmartNIC and Host Memory](https://arxiv.org/abs/2507.04001)
*Mohammed Zain Farooqi,Masoud Hemmatpour,Tore Heide Larsen*

Main category: cs.NI

TL;DR: 本文研究了SmartNIC与主机间内存访问的不同方法，以支持网络内应用并指导设计选择。


<details>
  <summary>Details</summary>
Motivation: SmartNIC在卸载计算任务时引入了通信挑战，需解决主机与卸载组件间高效通信问题。

Method: 评估了主机与SmartNIC间的多种内存访问方法。

Result: 分析了SmartNIC和主机上的内存访问性能。

Conclusion: 研究结果为网络内应用提供了内存访问设计的选择指导。

Abstract: SmartNICs have been increasingly utilized across various applications to
offload specific computational tasks, thereby enhancing overall system
performance. However, this offloading process introduces several communication
challenges that must be addressed for effective integration. A key challenge
lies in establishing efficient communication between the offloaded components
and the main application running on the host. In this study, we evaluate
different approaches for achieving memory access between the host and SmartNIC.
We analyze memory access performance on both the SmartNIC and the host to
support in-network applications and guide the selection of an appropriate
memory access design.

</details>


### [39] [Graph Diffusion-Based AeBS Deployment and Resource Allocation for RSMA-Enabled URLLC Low-Altitude Economy Networks](https://arxiv.org/abs/2507.04081)
*Xudong Wang,Lei Feng,Jiacheng Wang,Hongyang Du,Changyuan Zhao,Wenjing Li,Zehui Xiong,Dusit Niyato,Ping Zhang*

Main category: cs.NI

TL;DR: 该论文提出了一种基于速率分割多址接入（RSMA）的传输设计，通过生成图扩散模型和交替优化框架，解决多空中基站（AeBS）网络中干扰管理和资源分配的挑战。


<details>
  <summary>Details</summary>
Motivation: 在频谱资源有限和同频干扰严重的多AeBS网络中，如何高效部署基站、分配资源和增强超可靠低延迟通信（URLLC）服务。

Method: 采用生成图扩散模型进行AeBS部署和用户关联的策略探索，结合逐次凸逼近（SCA）方法优化波束成形和RSMA速率分配。

Result: 仿真显示，算法在收敛速度、总速率和覆盖率上优于现有方法，且在不同网络密度和干扰水平下表现稳健。

Conclusion: 提出的RSMA-enabled传输设计和优化框架显著提升了多AeBS网络的性能和适应性。

Abstract: As a key component of low-altitude economic networks, aerial base stations
(AeBSs) provide flexible and reliable wireless coverage to support 6G
ultra-reliable and low-latency communication (URLLC) services. However, limited
spectrum resources and severe co-channel interference pose significant
challenges to the deployment and resource allocation of AeBSs. To address these
limitations, this paper proposes a novel rate-splitting multiple access
(RSMA)-enabled transmission design to flexibly manage interference and
effectively enhance URLLC services in spectrum-constrained multi-AeBS networks.
On this basis, we formulate a joint optimization problem involving AeBS
deployment, user association, and resource allocation to maximize the
achievable sum rate and coverage of the total system. Given the NP-hard nature
of the problem and the highly dynamic environment, we propose a novel
alternating optimization framework based on the generative graph diffusion
models. Specifically, we model AeBSs and ground users as graph nodes, then we
employ a discrete graph generation process solved via denoising diffusion is
employed to explore the combinatorial space of deployment and association
strategies. Moreover, the algorithm adopts the successive convex approximation
(SCA) method to optimize AeBS beamforming and RSMA rate allocation under finite
blocklength constraints. Extensive simulations demonstrate that the proposed
algorithm outperforms existing methods in terms of convergence speed, sum rate,
and coverage, while also exhibiting robust performance under varying network
densities and interference levels.

</details>


### [40] [Resource-Efficient Seamless Transitions For High-Performance Multi-hop UAV Multicasting](https://arxiv.org/abs/2507.04421)
*Wanqing Tu*

Main category: cs.NI

TL;DR: 研究了一种高效的无人机（UAV）群组通信算法（ETF），通过快速低复杂度的计算优化无人机轨迹，提升多播性能。


<details>
  <summary>Details</summary>
Motivation: 无人机群组通信需要高效可靠地传输多媒体内容并扩展空对地视距覆盖。因此，研究如何实现快速且资源高效的无人机过渡，同时保持高多播性能。

Method: 开发了高效过渡形成（ETF）算法，通过评估直线轨迹（SLT）的连续性，或快速检查链来处理不同过渡场景。若SLT中断，ETF建立由最少无缝直线组成的新轨迹。

Result: 仿真研究表明，ETF在多播性能上优于其他方法，能够无缝过渡无人机群组成员。

Conclusion: ETF算法通过低复杂度计算和优化轨迹，显著提升了无人机群组通信的多播性能和过渡效率。

Abstract: Many UAV-related applications require group communications between UAVs to
reliably and efficiently deliver rich media content as well as to extend
line-of-sight coverage between sky and ground. This paper studies fast yet
resource-efficient UAV transitions while maintaining high multicasting
performance. We develop a set of analytic and algorithmic results to form the
efficient transition formation (ETF) algorithm that deals with different UAV
transition scenarios in a multicasting environment. The ETF algorithm first
evaluates the seamlessness of a straight-line trajectory (SLT), by processing
low-complexity computations (e.g., Euclidean distances) or a chain of fast
checks with controlled traffic overheads. For an interrupted SLT, ETF
establishes a new trajectory consisting of a minimum number of seamless
straight lines that join at specially selected locations in terms of
controlling mobile UAVs' seamless travel distances. Our simulation studies
quantify the multicasting performance gains that ETF allows, outperforming
compared studies when seamlessly transiting UAV group members.

</details>


### [41] [TeleSim: A Network-Aware Testbed and Benchmark Dataset for Telerobotic Applications](https://arxiv.org/abs/2507.04425)
*Zexin Deng,Zhenhui Yuan,Longhao Zou*

Main category: cs.NI

TL;DR: 论文介绍了TeleSim，一种网络感知的远程操作数据集和测试平台，用于评估不同网络条件下远程操作系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够反映网络延迟影响的数据集和测试平台，因此需要开发一种系统化的工具来评估远程操作在不同网络条件下的性能。

Method: TeleSim通过OMNeT++进行网络模拟，收集精细操作任务在高中低三个网络质量层级下的性能数据，包括完成时间、成功率等指标。

Result: 在最差网络条件下，完成时间增加了221.8%，成功率下降了64%。

Conclusion: 研究表明网络质量对远程操作有显著负面影响，需要开发自适应和弹性的远程操作协议。

Abstract: Telerobotic technologies are becoming increasingly essential in fields such
as remote surgery, nuclear decommissioning, and space exploration. Reliable
datasets and testbeds are essential for evaluating telerobotic system
performance prior to real-world deployment. However, there is a notable lack of
datasets that capture the impact of network delays, as well as testbeds that
realistically model the communication link between the operator and the robot.
This paper introduces TeleSim, a network-aware teleoperation dataset and
testbed designed to assess the performance of telerobotic applications under
diverse network conditions. TeleSim systematically collects performance data
from fine manipulation tasks executed under three predefined network quality
tiers: High, Medium, and Low. Each tier is characterized through controlled
settings of bandwidth, latency, jitter, and packet loss. Using OMNeT++ for
precise network simulation, we record a wide range of metrics, including
completion time, success rates, video quality indicators (Peak Signal-to-Noise
Ratio (PSNR) and Structural Similarity Index Measure (SSIM)), and quality of
service (QoS) parameters. TeleSim comprises 300 experimental trials, providing
a robust benchmark for evaluating teleoperation systems across heterogeneous
network scenarios. In the worst network condition, completion time increases by
221.8% and success rate drops by 64%. Our findings reveal that network
degradation leads to compounding negative impacts, notably reduced video
quality and prolonged task execution, highlighting the need for adaptive,
resilient teleoperation protocols. The full dataset and testbed software are
publicly available on our GitHub repository:
https://github.com/ConnectedRoboticsLab and YouTube channel:
https://youtu.be/Fz_1iOYe104.

</details>


### [42] [On-Demand Multimedia Delivery in 6G: An Optimal-Cost Steiner Tree Approach](https://arxiv.org/abs/2507.04589)
*Zien Wang,Xiucheng Wang,Nan Cheng,Wenchao Xu,Wei Quan,Ruijin Sun,Conghao Zhou*

Main category: cs.NI

TL;DR: 论文提出了动态规划增强型按需Steiner树（OST）算法，解决了6G网络中多媒体数据流的最小流量问题（MFP），优化了多目标和异构服务质量（QoS）需求。


<details>
  <summary>Details</summary>
Motivation: 6G网络中多媒体数据流量的激增对按需交付超高清、多质量流媒体提出了挑战，传统路由方法无法满足异构QoS需求。

Method: 提出了两阶段动态规划增强型OST算法，联合优化流聚合和QoS感知路径选择。

Result: OST在6G类似场景中减少了10%以上的总网络流量，并满足QoS需求。

Conclusion: OST算法为6G网络中的高效多媒体分发提供了创新解决方案。

Abstract: The exponential growth of multimedia data traffic in 6G networks poses
unprecedented challenges for immersive communication, where
ultra-high-definition, multi-quality streaming must be delivered on demand
while minimizing network operational costs. Traditional routing approaches,
such as shortest-path algorithms, fail to optimize flow multiplexing across
multiple destinations, while conventional Steiner tree methods cannot
accommodate heterogeneous quality-of-service (QoS) requirements-a critical need
for 6G's personalized services. In this paper, we address a fundamental but
unsolved challenge: the minimum flow problem (MFP) with multi-destination,
heterogeneous outflow demands, which is pivotal for efficient multimedia
distribution such as adaptive-resolution video streaming. To overcome the
limitations of existing methods, we propose a two-stage dynamic
programming-enhanced On-demand Steiner Tree (OST) algorithm, the first approach
that jointly optimizes flow aggregation and QoS-aware path selection for
arbitrary outflow requirements. We rigorously prove the optimality of OST using
mathematical induction, demonstrating that it guarantees the minimum-cost
multicast flow under differentiated service constraints. Extensive experiments
in 6G-like multimedia transmission scenarios show that OST reduces total
network flow by over 10% compared to state-of-the-art methods while ensuring
on-demand QoS fulfillment. The complete code is available at
https://github.com/UNIC-Lab/OST.

</details>


### [43] [Low-Latency Software Polar Encoders and Decoders for Short Blocklengths](https://arxiv.org/abs/2507.04734)
*Mathieu Leonardon,Mohammed El Houcine Ayoubi,Adrien Cassagne,Romain Tajan,Camille Leroux*

Main category: cs.NI

TL;DR: 提出了低延迟Polar码编码器和解码器，用于ISTC 2025竞赛，采用自适应连续取消列表解码器（ASCL），并通过设计空间探索优化了性能与解码时间的权衡。


<details>
  <summary>Details</summary>
Motivation: 为ISTC 2025竞赛开发最快的信道码编码器和解码器，重点优化延迟和性能。

Method: 基于Polar码和ASCL解码器，提出新型ASCL展开式解码器生成器，并探索了码构造、CRC选择及列表大小的设计空间。

Result: 实现了在不同帧错误率和码率下的性能优化，并开源了相关工具。

Conclusion: 通过设计空间探索和优化，展示了低延迟、高性能的Polar码编码器和解码器的可行性。

Abstract: This paper presents our low-latency Polar code encoders and decoders
developed for the 2025 International Symposium on Topics in Coding (ISTC 2025)
contest, which challenges participants to implement the fastest possible
channel code encoders and decoders in terms of average and maximum latency on a
CPU target. Our solution is based on Polar codes with an Adaptive Successive
Cancellation List (ASCL) decoder. We introduce a novel ASCL unrolled decoder
generator. We conduct an extensive exploration of the design space, including
code construction, CRC selection, and list size, to identify optimal trade-offs
between signal-to-noise ratio and decoding time across various operating
points. The considered operating points are frame error rates of 10^{-3} and
10^{-5}, information bit lengths of 64, 128, 256, and 512, and code rates of
1/4, 1/2, and 4/5. We also propose an optimized bit-packed encoder. All
implementations of the encoders and decoders, along with the code construction
and the unrolled decoders generator, are released as open source in the AFF3CT
toolbox.

</details>


### [44] [User Association in the Presence of Jamming in Wireless Networks Using the Whittle Index](https://arxiv.org/abs/2507.04968)
*Pramod N Chine,Suven Jagtiani,Mandar R Nalavade,Gaurav S Kasbekar*

Main category: cs.NI

TL;DR: 提出了一种基于Whittle指数的用户关联策略，旨在最小化无线网络中用户的总平均持有成本。


<details>
  <summary>Details</summary>
Motivation: 无线网络中用户关联算法对性能影响显著，且现有方法难以解决复杂约束问题。

Method: 利用Whittle框架将硬约束松弛为长期平均约束，采用拉格朗日乘子分解问题并证明其Whittle可索引性。

Result: 仿真显示，该策略在平均成本、延迟和公平性上优于现有方法。

Conclusion: 基于Whittle指数的策略有效解决了复杂网络中的用户关联问题。

Abstract: In wireless networks, algorithms for user association, i.e., the task of
choosing the base station (BS) that every arriving user should join,
significantly impact the network performance. A wireless network with multiple
BSs, operating on non-overlapping channels, is considered. The channels of the
BSs are susceptible to jamming by attackers. During every time slot, a user
arrives with a certain probability. There exists a holding cost in each slot
for every user associated with a BS. The goal here is to design a user
association scheme, which assigns a BS to each user upon arrival with the
objective of minimizing the long-run total average holding cost borne within
the network. This objective results in low average delays attained by the
users. This association problem is an instance of restless multi-armed bandit
problems, and is known to be hard to solve. By making use of the framework
presented by Whittle, the hard per-stage constraint that every arriving user
must connect to exactly one BS in a time slot is relaxed to a long-term
time-averaged constraint. Subsequently, we employ the Lagrangian multiplier
strategy to reformulate the problem into an unconstrained form and decompose it
into separate Markov Decision Processes at the BSs. Further, the problem is
proven to be Whittle indexable and a method for calculating the Whittle indices
corresponding to different BSs is presented. We design a user association
policy under which, upon arrival of a user in a time slot, it is assigned to
the BS having the least Whittle index in that slot. Through extensive
simulations, we show that our proposed association policy based on the Whittle
index outperforms various user association policies proposed in previous work
in terms of different metrics such as average cost, average delay, and Jain's
fairness index.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [45] [Music2Palette: Emotion-aligned Color Palette Generation via Cross-Modal Representation Learning](https://arxiv.org/abs/2507.04758)
*Jiayun Hu,Yueyi He,Tianyi Liang,Changbo Wang,Chenhui Li*

Main category: cs.MM

TL;DR: 论文提出了Music2Palette方法，通过跨模态表示学习生成与音乐情感对齐的调色板。


<details>
  <summary>Details</summary>
Motivation: 现有方法常仅生成单一主色或依赖间接映射，导致情感细节丢失，影响多媒体内容效果。

Method: 构建了MuCED数据集，提出跨模态表示学习框架，结合音乐编码器和颜色解码器，并通过多目标优化增强情感对齐、颜色多样性与调色板连贯性。

Result: 实验表明，该方法在音乐情感解读及生成多样化调色板方面优于现有方法。

Conclusion: Music2Palette填补了听觉与视觉情感体验间的鸿沟，适用于音乐驱动的图像重新着色、视频生成等应用。

Abstract: Emotion alignment between music and palettes is crucial for effective
multimedia content, yet misalignment creates confusion that weakens the
intended message. However, existing methods often generate only a single
dominant color, missing emotion variation. Others rely on indirect mappings
through text or images, resulting in the loss of crucial emotion details. To
address these challenges, we present Music2Palette, a novel method for
emotion-aligned color palette generation via cross-modal representation
learning. We first construct MuCED, a dataset of 2,634 expert-validated
music-palette pairs aligned through Russell-based emotion vectors. To directly
translate music into palettes, we propose a cross-modal representation learning
framework with a music encoder and color decoder. We further propose a
multi-objective optimization approach that jointly enhances emotion alignment,
color diversity, and palette coherence. Extensive experiments demonstrate that
our method outperforms current methods in interpreting music emotion and
generating attractive and diverse color palettes. Our approach enables
applications like music-driven image recoloring, video generating, and data
visualization, bridging the gap between auditory and visual emotion
experiences.

</details>


### [46] [CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset Separation](https://arxiv.org/abs/2507.05113)
*Binyan Xu,Fan Yang,Xilin Dai,Di Tang,Kehuan Zhang*

Main category: cs.MM

TL;DR: CGD是一种利用CLIP模型高效检测和防御深度神经网络中后门攻击的方法，实验证明其效果显著且适应性广。


<details>
  <summary>Details</summary>
Motivation: 现有的后门防御方法计算成本高且对高级攻击效果不佳，需要一种更高效和有效的解决方案。

Method: 通过CLIP模型识别并分类干净和受污染的数据，利用其输出指导模型重新训练以消除后门。

Result: 在4个数据集和11种攻击类型上，CGD将攻击成功率降至1%以下，且干净数据准确率仅下降0.3%。

Conclusion: CGD是一种高效、有效且适用于实际场景的后门防御方法，即使CLIP模型被攻击也能保持鲁棒性。

Abstract: Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where
adversaries poison training data to implant backdoor into the victim model.
Current backdoor defenses on poisoned data often suffer from high computational
costs or low effectiveness against advanced attacks like clean-label and
clean-image backdoors. To address them, we introduce CLIP-Guided backdoor
Defense (CGD), an efficient and effective method that mitigates various
backdoor attacks. CGD utilizes a publicly accessible CLIP model to identify
inputs that are likely to be clean or poisoned. It then retrains the model with
these inputs, using CLIP's logits as a guidance to effectively neutralize the
backdoor. Experiments on 4 datasets and 11 attack types demonstrate that CGD
reduces attack success rates (ASRs) to below 1% while maintaining clean
accuracy (CA) with a maximum drop of only 0.3%, outperforming existing
defenses. Additionally, we show that clean-data-based defenses can be adapted
to poisoned data using CGD. Also, CGD exhibits strong robustness, maintaining
low ASRs even when employing a weaker CLIP model or when CLIP itself is
compromised by a backdoor. These findings underscore CGD's exceptional
efficiency, effectiveness, and applicability for real-world backdoor defense
scenarios. Code: https://github.com/binyxu/CGD.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [47] [The Dependently Typed Higher-Order Form for the TPTP World](https://arxiv.org/abs/2507.03208)
*Daniel Ranalter,Cezary Kaliszyk,Florian Rabe,Geoff Sutcliffe*

Main category: cs.LO

TL;DR: 本文介绍了TPTP语言中的依赖类型高阶形式（DTF），扩展了现有的THF形式，并提供了一套问题集以展示其实用性。


<details>
  <summary>Details</summary>
Motivation: TPTP语言在自动推理领域起着重要作用，本文旨在通过DTF扩展其功能，同时最小化对现有语法的改动。

Method: 利用TPTP语言中已有的绑定器，提出了DTF形式，作为THF的扩展。

Result: 提供了一套包含100多个问题的集合，展示了DTF的实用性，并讨论了一些支持DTF的工具。

Conclusion: DTF是一种有效的扩展，能够推动TPTP语言的进一步应用和发展。

Abstract: Much of the current research and development in the field of automated
reasoning builds on the infrastructure provided by the TPTP World. The TPTP
language for logical formulae is central to the far-reaching adoption of the
TPTP World. This paper introduces the Dependently Typed higher-order Form (DTF)
of the TPTP language. It takes advantage of already established binders in the
syntax, and is thus a minimally intrusive extension to the Typed Higher-order
Form (THF). A starting set of over 100 problems is provided to exhibit the
usefulness and incite interest in DTF. Some tools that are already able to
reason about problems in the DTF language are discussed.

</details>


### [48] [Partial Label Learning for Automated Theorem Proving](https://arxiv.org/abs/2507.03314)
*Zsolt Zombori,Balázs Indruck*

Main category: cs.LO

TL;DR: 该论文将学习引导的自动定理证明形式化为部分标签学习，建立了这两个研究领域之间的桥梁，并提出了一种处理学习中替代证明的理论框架。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于将学习引导的自动定理证明与部分标签学习联系起来，为处理学习过程中的替代证明提供理论支持。

Method: 使用部分标签学习的方法，并通过plCoP定理证明器展示了这些方法对学习辅助定理证明器性能的提升。

Result: 研究表明，部分标签学习方法能够提高学习辅助定理证明器的性能。

Conclusion: 该研究为自动定理证明和学习领域搭建了桥梁，展示了部分标签学习方法在此领域的潜力。

Abstract: We formulate learning guided Automated Theorem Proving as Partial Label
Learning, building the first bridge across these fields of research and
providing a theoretical framework for dealing with alternative proofs during
learning. We use the plCoP theorem prover to demonstrate that methods from the
Partial Label Learning literature tend to increase the performance of learning
assisted theorem provers.

</details>


### [49] [Difference of Constrained Patterns in Logically Constrained Term Rewrite Systems (Full Version)](https://arxiv.org/abs/2507.04080)
*Naoki Nishida,Misaki Kojima,Yuto Nakamura*

Main category: cs.LO

TL;DR: 论文扩展了差异运算符和补码算法到约束线性模式，用于逻辑约束项重写系统（LCTRSs），证明了在可判定内置理论下，拟约简性是可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究目标是扩展差异运算符和补码算法的适用范围，特别是针对约束线性模式和逻辑约束项重写系统（LCTRSs），以解决拟约简性的判定问题。

Method: 通过扩展差异运算符和补码算法到约束线性模式，并结合左线性TRSs的特性，实现了对拟约简性的判定。

Result: 证明了在可判定内置理论的LCTRSs中，拟约简性是可判定的，且差异运算符仅要求除数模式是线性的。

Conclusion: 通过扩展差异运算符和补码算法，成功解决了约束线性模式下拟约简性的判定问题，为逻辑约束项重写系统的理论发展提供了新的工具。

Abstract: Considering patterns as sets of their instances, a difference operator over
patterns computes a finite set of two given patterns, which represents the
difference between the dividend pattern and the divisor pattern. A complement
of a pattern is a set of patterns, the ground constructor instances of which
comprise the complement set of the ground constructor instances of the former
pattern. Given finitely many unconstrained linear patterns, using a difference
operator over linear patterns, a known complement algorithm returns a finite
set of linear patterns as a complement of the given patterns. In this paper, we
extend the difference operator and complement algorithm to constrained linear
patterns used in logically constrained term rewrite systems (LCTRSs, for short)
that have no user-defined constructor term with a sort for built-in values.
Then, as for left-linear TRSs, using the complement algorithm, we show that
quasi-reducibility is decidable for such LCTRSs with decidable built-in
theories. For the single use of the difference operator over (constrained)
patterns, only divisor patterns are required to be linear.

</details>


### [50] [Omega-regular Verification and Control for Distributional Specifications in MDPs](https://arxiv.org/abs/2507.04286)
*S. Akshay,Ouldouz Neysari,Đorđe Žikelić*

Main category: cs.LO

TL;DR: 本文提出了一种新的自动化方法，用于验证和控制满足分布ω-正则规范的MDP，提出了分布证明规则，并设计了首个完全自动化算法。


<details>
  <summary>Details</summary>
Motivation: 解决MDP在分布视图下验证和控制中的计算难题，特别是针对分布ω-正则规范的问题。

Method: 提出了分布证明规则（分布证明书），并设计基于模板的合成算法，确保算法的正确性与相对完备性。

Result: 算法在PSPACE复杂度下运行，原型实现展示了其在实际挑战性问题中的适用性。

Conclusion: 该工作填补了MDP在分布ω-正则规范验证和控制领域的空白，为复杂问题提供了有效且自动化的解决方案。

Abstract: A classical approach to studying Markov decision processes (MDPs) is to view
them as state transformers. However, MDPs can also be viewed as distribution
transformers, where an MDP under a strategy generates a sequence of probability
distributions over MDP states. This view arises in several applications, even
as the probabilistic model checking problem becomes much harder compared to the
classical state transformer counterpart. It is known that even distributional
reachability and safety problems become computationally intractable (Skolem-
and positivity-hard). To address this challenge, recent works focused on sound
but possibly incomplete methods for verification and control of MDPs under the
distributional view. However, existing automated methods are applicable only to
distributional reachability, safety and reach-avoidance specifications.
  In this work, we present the first automated method for verification and
control of MDPs with respect to distributional omega-regular specifications. To
achieve this, we propose a novel notion of distributional certificates, which
are sound and complete proof rules for proving that an MDP under a
distributionally memoryless strategy satisfies some distributional
omega-regular specification. We then use our distributional certificates to
design the first fully automated algorithms for verification and control of
MDPs with respect to distributional omega-regular specifications. Our
algorithms follow a template-based synthesis approach and provide soundness and
relative completeness guarantees, while running in PSPACE. Our prototype
implementation demonstrates practical applicability of our algorithms to
challenging examples collected from the literature.

</details>


### [51] [Shininess, strong politeness, and unicorns](https://arxiv.org/abs/2507.04445)
*Benjamin Przybocki,Guilherme V. Toledo,Yoni Zohar*

Main category: cs.LO

TL;DR: 该论文通过证明闪亮理论始终可判定且存在非闪亮的强礼貌理论，完善了Casal和Rasga关于闪亮与强礼貌理论等价性的研究，最终解决了理论组合属性关系中所有未决问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在完善和细化Casal和Rasga关于闪亮与强礼貌理论等价性的结果，同时解决理论组合属性关系分类中的剩余开放问题。

Method: 通过理论分析，证明了闪亮理论的可判定性及其与强礼貌理论的关系，并构造了反例展示强礼貌理论不一定闪亮。

Result: 发现闪亮理论始终可判定且必然强礼貌，但存在非闪亮的强礼貌理论。最终完成了理论组合属性关系的全面分类。

Conclusion: 该研究彻底解决了理论组合属性关系的分类问题，明确了闪亮与强礼貌理论的关系，填补了先前研究的空白。

Abstract: Shininess and strong politeness are properties related to theory combination
procedures. In a paper titled "Many-sorted equivalence of shiny and strongly
polite theories", Casal and Rasga proved that for decidable theories, these
properties are equivalent. We refine their result by showing that: (i) shiny
theories are always decidable, and therefore strongly polite; and (ii) there
are (undecidable) strongly polite theories that are not shiny. This line of
research is tightly related to a recent series of papers that have sought to
classify all the relations between theory combination properties. We finally
complete this project, resolving all of the remaining problems that were
previously left open.

</details>


### [52] [Proof Analysis of A Foundational Classical Singlesuccedent Sequent Calculus](https://arxiv.org/abs/2507.04449)
*Khashayar Irani*

Main category: cs.LO

TL;DR: 本文旨在提出一种新的经典单后继式序列演算G-Calculus（Gc），以弥补现有系统如LK及其变种的不足。


<details>
  <summary>Details</summary>
Motivation: 现有经典单后继式序列演算（如Negri和von Plato的系统）在证明理论潜力上无法满足专业需求，亟需一种更强大的基础系统。

Method: 通过设计包含新颖规则的Gc系统，专注于命题部分而非谓词部分，以实现目标。

Result: Gc系统被证明能够满足经典证明理论家的形式化期望。

Conclusion: Gc作为一种新的经典单后继式序列演算，填补了现有系统的不足，具有重要的理论意义。

Abstract: In this paper we investigate the question: 'How can A Foundational Classical
Singlesuccedent Sequent Calculus be formulated?' The choice of this particular
area of proof-theoretic study is based on a particular ground that is, to
formulate a robust and foundational classical singlesuccedent sequent calculus
that includes a number of novel rules with the ultimate aim of deriving the
singlesuccedent sequent {\Gamma} sequent arrow C. To this end, we argue that
among all standard sequent calculi (at least to the best of our knowledge)
there is no classical singlesuccedent sequent calculus that can be considered
the rightful successor to Gerhard Gentzen's (1935) original LK system. However,
we also contend that while several classical singlesuccedent sequent calculi
exist such as Sara Negri's and Jan von Plato's (2001 & 2011) G3ip+Gem-at and
G0ip+Gem0-at calculi, none of these proof systems possess the classical
proof-theoretic potential to meet the formal expectations of a dedicated
classical proof theorist. Conversely, we shall demonstrate that our forthcoming
system, namely G-Calculus through its classical division i.e. Gc has been
entirely designed to meet these expectations. Prior to commencing our enquiry,
a supplementary note must be made and that is in this work when discussing
various sequent calculi, for proof-theoretic purposes, we are primarily
concerned with their propositional components rather than their predicate
divisions except in G-Calculus where we examine both aspects.

</details>


### [53] [A Note on Runtime Verification of Concurrent Systems](https://arxiv.org/abs/2507.04830)
*Martin Leucker*

Main category: cs.LO

TL;DR: 论文提出了一种基于部分序执行的LTrL三值逻辑方法，用于验证并发系统，通过监视器合成提供一致的验证结果。


<details>
  <summary>Details</summary>
Motivation: 为了在单次执行中最大化信息获取并验证并发系统，论文提出了一种基于trace的逻辑替代传统的序列逻辑。

Method: 论文采用Mazurkiewicz Traces上的线性时序逻辑（LTrL），通过部分序表示执行，并扩展为三值逻辑（正确、错误或不确定），设计监视器合成程序。

Result: 实现了对并发系统的单次执行验证，所有等效交错执行均被隐式考虑，监视器能为任何线性化trace提供一致的验证结果。

Conclusion: 通过三值LTrL和监视器合成，论文提供了一种高效且一致的并发系统验证方法。

Abstract: To maximize the information gained from a single execution when verifying a
concurrent system, one can derive all concurrency-aware equivalent executions
and check them against linear specifications. This paper offers an alternative
perspective on verification of concurrent systems by leveraging trace-based
logics rather than sequence-based formalisms. Linear Temporal Logic over
Mazurkiewicz Traces (LTrL) operates on partial-order representations of
executions, meaning that once a single execution is specified, all equivalent
interleavings are implicitly considered. This paper introduces a three valued
version of LTrL, indicating whether the so-far observed execution of the
concurrent system is one of correct, incorrect or inconclusive, together with a
suitable monitor synthesis procedure. To this end, the paper recalls a
construction of trace-consistent B\"uchi automata for LTrL formulas and
explains how to employ it in well-understood monitor synthesis procedures. In
this way, a monitor results that yields for any linearization of an observed
trace the same verification verdict.

</details>


### [54] [Testing for Renamability to Classes of Clause Sets](https://arxiv.org/abs/2507.05044)
*Albert Brandl,Christian G. Fermüller,Gernot Salzer*

Main category: cs.LO

TL;DR: 本文研究了如何通过重命名测试子句集是否属于某些已知类的问题，并讨论了这些类在多项式时间内可判定性的差异。


<details>
  <summary>Details</summary>
Motivation: 探讨通过谓词重命名来确定子句集是否属于特定类（如Horn或OCC1N）的可能性。

Method: 基于超分辨率的决策过程，若存在重命名，可从饱和子句集中提取。

Result: 对于Horn和OCC1N类，重命名的存在性可在多项式时间内判定，而对PVD类该问题为NP完全。

Conclusion: 不同类的重命名问题在复杂性上存在显著差异，超分辨率方法为此提供了有效工具。

Abstract: This paper investigates the problem of testing clause sets for membership in
classes known from literature. In particular, we are interested in classes
defined via renaming: Is it possible to rename the predicates in a way such
that positive and negative literals satisfy certain conditions? We show that
for classes like Horn or OCC1N, the existence of such renamings can be decided
in polynomial time, whereas the same problem is NP-complete for class PVD. The
decision procedures are based on hyper-resolution; if a renaming exists, it can
be extracted from the final saturated clause set.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [55] [Enhancing the Aesthetic Appeal of AI-Generated Physical Product Designs through LoRA Fine-Tuning with Human Feedback](https://arxiv.org/abs/2507.02865)
*Dinuo Liao,James Derek Lomas,Cehao Yu*

Main category: cs.HC

TL;DR: 研究表明，通过人类审美评估指导的LoRA微调可以提升生成式AI在灯具设计中的输出效果，结合人类反馈显著提高设计的吸引力和美学评分。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过人类审美反馈指导LoRA微调，以提高生成式AI在实物产品设计中的表现，尤其是美学吸引力和实用性。

Method: 采用了LoRA微调Stable Diffusion模型，结合人类反馈和提示优化技术，并研究如何通过3D打印将AI设计转化为实物产品。

Result: 实验结果表明，LoRA微调能有效将AI设计与人类审美偏好对齐，显著提升设计的美学评分和吸引力。

Conclusion: 研究展示了人机协作在实物产品设计中的潜力，为整合人类反馈到AI设计流程提供了实用见解。

Abstract: This study explores how Low-Rank Adaptation (LoRA) fine-tuning, guided by
human aesthetic evaluations, can enhance the outputs of generative AI models in
tangible product design, using lamp design as a case study. By integrating
human feedback into the AI model, we aim to improve both the desirability and
aesthetic appeal of the generated designs. Comprehensive experiments were
conducted, starting with prompt optimization techniques and focusing on LoRA
fine-tuning of the Stable Diffusion model. Additionally, methods to convert
AI-generated designs into tangible products through 3D realization using 3D
printing technologies were investigated. The results indicate that LoRA
fine-tuning effectively aligns AI-generated designs with human aesthetic
preferences, leading to significant improvements in desirability and aesthetic
appeal scores. These findings highlight the potential of human-AI collaboration
in tangible product design and provide valuable insights into integrating human
feedback into AI design processes.

</details>


### [56] [Engineering Trust, Creating Vulnerability: A Socio-Technical Analysis of AI Interface Design](https://arxiv.org/abs/2507.02866)
*Ben Kereopa-Yorke*

Main category: cs.HC

TL;DR: 通过界面设计研究AI跨学科文化的形成，提出IMCS框架，揭示AI界面作为边界对象的角色，并识别四种认知安全漏洞向量，挑战传统跨学科理论的三大假设。


<details>
  <summary>Details</summary>
Motivation: 探讨AI跨学科文化如何通过界面设计形成，揭示传统学科界限在研究复杂社会技术现象时的不足。

Method: 采用IMCS框架，结合视觉分析和案例研究，分析生成式AI平台及公共、医疗和教育领域的界面设计。

Result: 识别出四种界面介导的认知安全漏洞向量，证明界面是学科方法论碰撞的场所。

Conclusion: AI界面作为边界对象，推动了方法论融合而非简单协作，挑战了跨学科理论的三大预设。

Abstract: This paper examines how distinct cultures of AI interdisciplinarity emerge
through interface design, revealing the formation of new disciplinary cultures
at these intersections. Through the Interface-Mediated Cognitive Security
(IMCS) framework, I demonstrate how the collision of cybersecurity engineering,
cognitive psychology, critical technology studies, and human-computer
interaction generates research cultures that transcend traditional disciplinary
boundaries. AI interfaces function as transformative boundary objects that
necessitate methodological fusion rather than mere collaboration,
simultaneously embodying technical architectures, psychological design
patterns, and social interaction models. Through systematic visual analysis of
generative AI platforms and case studies across public sector, medical, and
educational domains, I identify four vulnerability vectors, Reflection
Simulation, Authority Modulation, Cognitive Load Exploitation, and
Market-Security Tension, that structure interface-mediated cognitive security.
This research challenges three significant gaps in interdisciplinary theory:
the assumption that disciplines maintain distinct methodological boundaries
during collaboration, the belief that technical and social knowledge practices
can be cleanly separated, and the presumption that disciplinary integration
occurs through formal rather than cultural mechanisms. The empirical evidence
demonstrates how interfaces function as sites of epistemological collision,
creating methodological pressure zones where traditional disciplinary
approaches prove insufficient for analysing the complex socio-technical
phenomena at the interface.

</details>


### [57] [Identifying Ethical Challenges in XR Implementations in the Industrial Domain: A Case of Off-Highway Machinery](https://arxiv.org/abs/2507.02868)
*Anastasia Sergeeva,Claudia Negri-Ribalta,Gabriele Lenzini*

Main category: cs.HC

TL;DR: 摘要讨论了在工业场景中如何以伦理和隐私保护的方式实现扩展现实(XR)技术，并总结了在高性能机械领域开发XR实现的经验与挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨工业应用中XR技术的伦理和隐私保护问题，为未来研究提供基础。

Method: 通过总结在高性能机械领域开发XR实现的经验，识别主要挑战。

Result: 提出了XR技术在工业应用中的隐私和伦理挑战，为后续讨论和研究铺路。

Conclusion: 研究结果为未来工业XR应用的伦理和隐私研究提供了起点。

Abstract: Although extended reality(XR)-using technologies have started to be discussed
in the industrial setting, it is becoming important to understand how to
implement them ethically and privacy-preservingly. In our paper, we summarise
our experience of developing XR implementations for the off-highway machinery
domain by pointing to the main challenges we identified during the work. We
believe that our findings can be a starting point for further discussion and
future research regarding privacy and ethical challenges in industrial
applications of XR.

</details>


### [58] [Zara: An LLM-based Candidate Interview Feedback System](https://arxiv.org/abs/2507.02869)
*Nima Yazdani,Aruj Mahajan,Ali Ansari*

Main category: cs.HC

TL;DR: Zara是一个基于GPT-4o的AI招聘支持系统，通过个性化面试支持提升候选人体验。


<details>
  <summary>Details</summary>
Motivation: 传统招聘因物流和法律限制难以提供个性化反馈，导致候选人不满意。

Method: 利用GPT-4o动态生成个性化模拟面试、AI驱动评估、结构化反馈和RAG系统回答问题。

Result: 系统高效提供个性化支持，并开源反馈生成方法。

Conclusion: Zara展示了LLMs如何改进招聘流程的候选体验。

Abstract: This paper introduces Zara, an AI-driven recruitment support system developed
by micro1, as a practical case study illustrating how large language models
(LLMs) can enhance the candidate experience through personalized, scalable
interview support. Traditionally, recruiters have struggled to deliver
individualized candidate feedback due to logistical and legal constraints,
resulting in widespread candidate dissatisfaction. Leveraging OpenAI's GPT-4o,
Zara addresses these limitations by dynamically generating personalized
practice interviews, conducting conversational AI-driven assessments,
autonomously delivering structured and actionable feedback, and efficiently
answering candidate inquiries using a Retrieval-Augmented Generation (RAG)
system. To promote transparency, we have open-sourced the approach Zara uses to
generate candidate feedback.

</details>


### [59] [Preference-Optimal Multi-Metric Weighting for Parallel Coordinate Plots](https://arxiv.org/abs/2507.02905)
*Chisa Mori,Shuhei Watanabe,Masaki Onishi,Takayuki Itoh*

Main category: cs.HC

TL;DR: 提出了一种基于用户偏好的多指标权重计算方法，并通过UMAP降维和雷达图实现直观可视化。


<details>
  <summary>Details</summary>
Motivation: 解决多指标情况下平行坐标图（PCPs）无法有效提供颜色渐变的挑战，以及用户对权重分配不明确的问题。

Method: 提出了一种计算最优权重的原理性方法，使用UMAP降维和雷达图在二维平面上直观展示指标权衡。

Result: 在行人流引导规划中，该方法能够根据用户偏好识别控制参数重要性的独特模式。

Conclusion: 该方法能够有效帮助用户在多指标情况下直观选择偏好，并揭示参数重要性模式。

Abstract: Parallel coordinate plots (PCPs) are a prevalent method to interpret the
relationship between the control parameters and metrics. PCPs deliver such an
interpretation by color gradation based on a single metric. However, it is
challenging to provide such a gradation when multiple metrics are present.
Although a naive approach involves calculating a single metric by linearly
weighting each metric, such weighting is unclear for users. To address this
problem, we first propose a principled formulation for calculating the optimal
weight based on a specific preferred metric combination. Although users can
simply select their preference from a two-dimensional (2D) plane for bi-metric
problems, multi-metric problems require intuitive visualization to allow them
to select their preference. We achieved this using various radar charts to
visualize the metric trade-offs on the 2D plane reduced by UMAP. In the
analysis using pedestrian flow guidance planning, our method identified unique
patterns of control parameter importance for each user preference, highlighting
the effectiveness of our method.

</details>


### [60] [OAK -- Onboarding with Actionable Knowledge](https://arxiv.org/abs/2507.02914)
*Steve Devènes,Marine Capallera,Robin Cherix,Elena Mugellini,Omar Abou Khaled,Francesco Carrino*

Main category: cs.HC

TL;DR: 提出了一种结合知识图谱嵌入和多模态接口的新方法，以收集和检索专业知识，并支持决策制定。


<details>
  <summary>Details</summary>
Motivation: 解决因熟练操作员离职导致知识流失的问题，这些知识多样且非结构化。

Method: 结合知识图谱嵌入和多模态接口，并利用LLMs改进查询理解和提供适应性答案。

Result: 开发了一个用于高精度制造质量控制的验证案例。

Conclusion: 该方法有效解决了知识流失问题，并支持实际决策制定。

Abstract: The loss of knowledge when skilled operators leave poses a critical issue for
companies. This know-how is diverse and unstructured. We propose a novel method
that combines knowledge graph embeddings and multi-modal interfaces to collect
and retrieve expertise, making it actionable. Our approach supports
decision-making on the shop floor. Additionally, we leverage LLMs to improve
query understanding and provide adapted answers. As application case studies,
we developed a proof-of-concept for quality control in high precision
manufacturing.

</details>


### [61] [Visual-Conversational Interface for Evidence-Based Explanation of Diabetes Risk Prediction](https://arxiv.org/abs/2507.02920)
*Reza Samimi,Aditya Bhattacharya,Lucija Gosak,Gregor Stiglic,Katrien Verbert*

Main category: cs.HC

TL;DR: 提出了一种结合交互式可视化和对话代理的糖尿病风险评估决策支持系统，解决现有系统的复杂性和科学依据不足问题。


<details>
  <summary>Details</summary>
Motivation: 医疗专业人员需要有效的方法来使用、理解和验证AI驱动的临床决策支持系统，当前系统存在可视化复杂和缺乏科学依据的问题。

Method: 采用混合提示处理方法，结合微调语言模型和通用大语言模型，并基于科学依据的AI解释方法及特征范围分析技术。

Result: 30名医疗专业人员的混合方法研究表明，对话交互增强了模型评估的理解，科学依据的整合校准了对系统决策的信任。

Conclusion: 系统在患者风险评估和推荐方面得到了多数参与者的认可。

Abstract: Healthcare professionals need effective ways to use, understand, and validate
AI-driven clinical decision support systems. Existing systems face two key
limitations: complex visualizations and a lack of grounding in scientific
evidence. We present an integrated decision support system that combines
interactive visualizations with a conversational agent to explain diabetes risk
assessments. We propose a hybrid prompt handling approach combining fine-tuned
language models for analytical queries with general Large Language Models
(LLMs) for broader medical questions, a methodology for grounding AI
explanations in scientific evidence, and a feature range analysis technique to
support deeper understanding of feature contributions. We conducted a
mixed-methods study with 30 healthcare professionals and found that the
conversational interactions helped healthcare professionals build a clear
understanding of model assessments, while the integration of scientific
evidence calibrated trust in the system's decisions. Most participants reported
that the system supported both patient risk evaluation and recommendation.

</details>


### [62] [Enhanced knowledge retention through MedScrab: an interactive mobile game](https://arxiv.org/abs/2507.03032)
*Don Roosan,Tiffany Khao,Huong Phan,Yan Li*

Main category: cs.HC

TL;DR: 研究人员设计了一款移动游戏以提高药物依从性，通过对比游戏与传统宣传资料的效果，发现游戏组在知识测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 慢性病患者药物依从性差导致健康问题，需要创新方法改善这一状况。

Method: 参与者分为游戏组和宣传资料组，通过前后测试对比效果。

Result: 游戏组测试成绩显著提升，且游戏时长与成绩呈正相关趋势。

Conclusion: 移动游戏可作为提升药物知识的有力工具，未来需优化其应用范围。

Abstract: Noncompliance with medication regimens poses an immense challenge in the
management of chronic diseases, often resulting in exacerbated health
complications and recurrent hospital admissions. Addressing this gap, our team
designed an innovative mobile game aimed at bolstering medication adherence and
information retention within the general population. Employing Amazon
Mechanical Turk, participants were enlisted and allocated into two cohorts: one
engaged with our mobile game and the other perused an informational pamphlet
about medication. Both cohorts underwent a pre-intervention quiz, followed by
their respective interventions, and concluded with a post-intervention quiz.
Primary outcome measures included the difference in quiz scores and the game
play duration. The investigation encompassed 243 participants with homogenous
baseline attributes. Participants interacting with the mobile game depicted a
significant enhancement in their post-intervention scores compared to the
pre-intervention scores. We observed a notable correlation of 0.346 (p<0.001)
with a robust medium effect size of 0.641 (0.503 - 0.779). Although the
duration of game play and post-intervention scores didn't exhibit a direct
correlation, a tendency towards superior post-intervention scores was evident
among participants who dedicated more time to the game. The interactive mobile
game we developed exhibits potential as an engaging instrument for empowering
patients and caregivers. Providing critical medication information and the
potential side effects in a manner that increases retention would thereby
mitigate medication noncompliance. Future research endeavors should focus on
optimizing and broadening the application of such mobile interfaces to fortify
public health initiatives.

</details>


### [63] [Gaze and Glow: Exploring Editing Processes on Social Media through Interactive Exhibition](https://arxiv.org/abs/2507.03286)
*Yang Hong,Jie-Yi Feng,Yi-Chun Yao,I-Hsuan Cho,Yu-Ting Lin,Ying-Yu Chen*

Main category: cs.HC

TL;DR: Gaze and Glow是一个揭示社交媒体编辑背后努力的互动装置，通过叙事角色和传感器互动，探讨观众注意力如何影响编辑实践和情感体验。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体编辑中常被忽视的努力，以及观众注意力对编辑行为和情感的影响。

Method: 使用叙事角色、实验视频和传感器互动，结合公众展览和观众反馈的反思性主题分析。

Result: 装置引发了观众对真实性、能动性和表演性的反思，并提出了支持选择性记忆和用户控制可见性的设计建议。

Conclusion: 该研究为设计支持批判性数字自我呈现的互动系统提供了见解。

Abstract: We present Gaze and Glow, an interactive installation that reveals the
often-invisible efforts of social media editing. Through narrative personas,
experimental videos, and sensor-based interactions, the installation explores
how audience attention shapes users' editing practices and emotional
experiences. Deployed in a two-month public exhibition, Gaze and Glow engaged
viewers and elicited responses. Reflexive thematic analysis of audience
feedback highlights how making editing visible prompts new reflections on
authenticity, agency, and performativity. We discuss implications for designing
interactive systems that support selective memory, user-controlled visibility,
and critical engagement with everyday digital self-presentation.

</details>


### [64] [DeepGesture: A conversational gesture synthesis system based on emotions and semantics](https://arxiv.org/abs/2507.03147)
*Thanh Hoang-Minh*

Main category: cs.HC

TL;DR: DeepGesture是一种基于扩散模型的手势合成框架，通过文本、语音、情感和种子动作等多模态信号生成自然的手势，提升了语义对齐和情感表达。


<details>
  <summary>Details</summary>
Motivation: 当前数字人生成的瓶颈在于如何根据文本或语音输入自然地生成角色动作，DeepGesture旨在解决这一问题。

Method: 基于DiffuseStyleGesture模型，引入快速文本转录作为语义条件，实现情感引导的无分类器扩散，并采用轻量级Transformer架构融合多模态特征。

Result: 在ZeroEGGS数据集上，DeepGesture生成的手势在人类相似度和上下文适当性上优于基线，支持情感状态插值和泛化到非分布语音。

Conclusion: DeepGesture朝着多模态、情感感知的数字人类迈进了一步。

Abstract: Along with the explosion of large language models, improvements in speech
synthesis, advancements in hardware, and the evolution of computer graphics,
the current bottleneck in creating digital humans lies in generating character
movements that correspond naturally to text or speech inputs.
  In this work, we present DeepGesture, a diffusion-based gesture synthesis
framework for generating expressive co-speech gestures conditioned on
multimodal signals-text, speech, emotion, and seed motion. Built upon the
DiffuseStyleGesture model, DeepGesture introduces novel architectural
enhancements that improve semantic alignment and emotional expressiveness in
generated gestures. Specifically, we integrate fast text transcriptions as
semantic conditioning and implement emotion-guided classifier-free diffusion to
support controllable gesture generation across affective states. A lightweight
Transformer backbone combines full self-attention and cross-local attention for
effective feature fusion of heterogeneous modalities. To visualize results, we
implement a full rendering pipeline in Unity based on BVH output from the
model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces
gestures with improved human-likeness and contextual appropriateness,
outperforming baselines on Mean Opinion Score and Frechet Gesture Distance
metrics. Our system supports interpolation between emotional states and
demonstrates generalization to out-of-distribution speech, including synthetic
voices-marking a step forward toward fully multimodal, emotionally aware
digital humans.

</details>


### [65] [ASCRIBE-XR: Virtual Reality for Visualization of Scientific Imagery](https://arxiv.org/abs/2507.03170)
*Ronald J. Pandolfi,Jeffrey J. Donatelli,Julian Todd,Daniela Ushizima*

Main category: cs.HC

TL;DR: ASCRIBE-XR 是一个基于 Godot 和 PC-VR 技术的新计算平台，用于同步辐射实验中的 3D 体数据和网格数据可视化与探索。


<details>
  <summary>Details</summary>
Motivation: 为了解决同步辐射实验中 3D 数据动态加载、操作和多用户实时协作的需求，开发了 ASCRIBE-XR。

Method: 利用 Godot 和 PC-VR 技术实现 3D 数据的动态加载与操作，通过 WebRTC 和 MQTT 技术实现多用户实时协作。

Result: 平台支持多用户实时共享和可视化数据，提升了研究的互动性和参与感。

Conclusion: ASCRIBE-XR 为同步辐射研究提供了强大的工具，具有广泛的应用潜力和科学价值。

Abstract: ASCRIBE-XR, a novel computational platform designed to facilitate the
visualization and exploration of 3D volumetric data and mesh data in the
context of synchrotron experiments, is described. Using Godot and PC-VR
technologies, the platform enables users to dynamically load and manipulate 3D
data sets to gain deeper insights into their research. The program's multi-user
capabilities, enabled through WebRTC, and MQTT, allow multiple users to share
data and visualize together in real-time, promoting a more interactive and
engaging research experience. We describe the design and implementation of
ASCRIBE-XR, highlighting its key features and capabilities. We will also
discuss its utility in the context of synchrotron research, including examples
of its application and potential benefits for the scientific community.

</details>


### [66] [Assessing the Viability of Wave Field Synthesis in VR-Based Cognitive Research](https://arxiv.org/abs/2507.03797)
*Benjamin Kahl*

Main category: cs.HC

TL;DR: 该论文探讨了波场合成（WFS）在增强VR认知研究中听觉沉浸感的可行性，发现WFS虽在某些方面不如传统立体声耳机精确，但能提供更自然的听觉体验。


<details>
  <summary>Details</summary>
Motivation: VR技术在人类感知和行为研究中具有优势，但听觉线索常被忽视。WFS作为一种先进的音频渲染技术，有望提高生态效度。

Method: 研究通过实验比较WFS和传统立体声耳机在静态和动态声源定位中的表现，分析虚拟环境、声音类型和时长对定位准确性和搜索行为的影响。

Result: 研究发现立体声耳机定位更精确，但WFS提供更自然的听觉体验，尤其在方向性线索方面。WFS目前存在高度定位、遮挡模拟等局限性。

Conclusion: 尽管存在局限性，WFS在复杂的声景研究，尤其是方向信息关键的场景中，显示出潜力。

Abstract: This paper investigates the viability of Wave Field Synthesis (WFS) for
enhancing auditory immersion in VR-based cognitive research. While Virtual
Reality (VR) offers significant advantages for studying human perception and
behavior, auditory cues are often underutilized. WFS, an advanced audio
rendering technique, can create highly realistic and spatially accurate
soundscapes, potentially increasing ecological validity. This study evaluates
WFS by implementing a sample experiment where participants localize static and
moving sound sources in both a WFS-rendered environment and a conventional
stereo headphone setup. The research explores the impact of virtual
environments, sound types, and durations on localization accuracy and search
behavior. Findings indicate that while stereo setups can achieve higher
accuracy, WFS provides a more natural and intuitive auditory experience,
particularly for directional cues. The study also highlights limitations of
current WFS systems, such as the lack of height localization, occlusion
simulation, and user-dependent optimization, which affect performance,
especially for centrally located sound sources. Despite these challenges, WFS
shows promise for specialized auditory perception research, particularly for
complex soundscapes where directional information is paramount.

</details>


### [67] [Beyond Charging Anxiety: An Explainable Approach to Understanding User Preferences of EV Charging Stations Using Review Data](https://arxiv.org/abs/2507.03243)
*Zifei Wang,Emmanuel Abolarin,Kai Wu,Venkatarao Rebba,Jian Hu,Zhen Hu,Shan Bao,Feng Zhou*

Main category: cs.HC

TL;DR: 利用 ChatGPT 4.0 对 Google 地图上的充电站评论进行情感分析，研究影响电动车用户充电体验的关键因素，并开发了微观和宏观预测模型。


<details>
  <summary>Details</summary>
Motivation: 电动车充电基础设施对用户体验和广泛采用至关重要，因此需要了解影响用户满意度的关键因素。

Method: 使用 ChatGPT 4.0 对 17,000 条充电站评论进行情感分析，识别影响用户满意度的 12 个关键方面，并基于 LightGBM 算法开发微观和宏观预测模型。

Result: 研究发现，'设施与位置'的积极情感和'可靠性与维护'的消极情感对用户满意度有显著影响。

Conclusion: 研究结果为充电站运营商、政策制定者和电动车制造商提供了优化用户体验和推广电动车的实用建议。

Abstract: Electric vehicles (EVs) charging infrastructure is directly related to the
overall EV user experience and thus impacts the widespread adoption of EVs.
Understanding key factors that affect EV users' charging experience is
essential for building a robust and user-friendly EV charging infrastructure.
This study leverages about $17,000$ charging station (CS) reviews on Google
Maps to explore EV user preferences for charging stations, employing ChatGPT
4.0 for aspect-based sentiment analysis. We identify twelve key aspects
influencing user satisfaction, ranging from accessibility and reliability to
amenities and pricing. Two distinct preference models are developed: a
micro-level model focused on individual user satisfaction and a macro-level
model capturing collective sentiment towards specific charging stations. Both
models utilize the LightGBM algorithm for user preference prediction, achieving
strong performance compared to other machine learning approaches. To further
elucidate the impact of each aspect on user ratings, we employ SHAP (SHapley
Additive exPlanations), a game-theoretic approach for interpreting machine
learning models. Our findings highlight the significant impact of positive
sentiment towards "amenities and location", coupled with negative sentiment
regarding "reliability and maintenance", on overall user satisfaction. These
insights offer actionable guidance to charging station operators, policymakers,
and EV manufacturers, empowering them to enhance user experience and foster
wider EV adoption.

</details>


### [68] [The shortcomings of video conferencing technology, methods for revealing them, and emerging XR solutions](https://arxiv.org/abs/2507.03902)
*Dani Paul Hove,Benjamin Watson*

Main category: cs.HC

TL;DR: 视频会议在COVID-19疫情期间成为日常生活的重要组成部分，但其局限性（如社交行为支持不足和“Zoom疲劳”）需要新技术（尤其是混合现实技术）来解决。本文旨在推动研究并测试这些技术。


<details>
  <summary>Details</summary>
Motivation: 由于视频会议在疫情期间被广泛应用，其局限性（如社交行为支持不足和“Zoom疲劳”）日益突出，需要通过新技术（尤其是混合现实技术）来解决。

Method: 本文首先调研了视频会议系统的缺陷（疫情前后），然后探讨了评估社交行为支持的研究方法，并建议这些方法应用于新技术的开发和验证。

Result: 研究发现混合现实（XR）技术（尤其是非头戴式设备）可以解决视频会议的局限性。

Conclusion: 本文呼吁未来研究开发并测试新技术（尤其是XR技术）以改进视频会议体验。

Abstract: Video conferencing has become a central part of our daily lives, thanks to
the COVID-19 pandemic. Unfortunately, so have its many limitations, resulting
in poor support for communicative and social behavior and ultimately, Zoom
fatigue. New technologies will be required to address these limitations,
including many drawn from mixed reality (XR). In this paper, our goals are to
equip and encourage future researchers to develop and test such technologies.
Toward this end, we first survey research on the shortcomings of video
conferencing systems, as defined before and after the pandemic. We then
consider the methods that research uses to evaluate support for communicative
behavior, and argue that those same methods should be employed in identifying,
improving, and validating promising video conferencing technologies. Next, we
survey emerging XR solutions to video conferencing's limitations, most off
which do not employ head-mounted displays.

</details>


### [69] [AnnoGram: An Annotative Grammar of Graphics Extension](https://arxiv.org/abs/2507.04236)
*Md Dilshadur Rahman,Md Rahat-uz- Zaman,Andrew McNutt,Paul Rosen*

Main category: cs.HC

TL;DR: 论文提出了一种新的扩展方法，将注释作为一等设计元素，增强了数据可视化的表达性和便携性。


<details>
  <summary>Details</summary>
Motivation: 现有可视化工具将注释作为次要结构，难以复用且与可视化语法松散耦合，影响了数据沟通的效果。

Method: 采用Wilkinson的图形语法扩展，通过声明式方法将注释目标、类型和定位策略结构化。

Result: 开发了Vega-Lite Annotation原型，与8种现有工具对比显示，该方法提升了表达性、减少了创作工作量并实现了便携的注释工作流。

Conclusion: 该方法成功将注释提升为一等设计元素，为数据可视化提供了更高效的注释支持。

Abstract: Annotations are central to effective data communication, yet most
visualization tools treat them as secondary constructs -- manually defined,
difficult to reuse, and loosely coupled to the underlying visualization
grammar. We propose a declarative extension to Wilkinson's Grammar of Graphics
that reifies annotations as first-class design elements, enabling structured
specification of annotation targets, types, and positioning strategies. To
demonstrate the utility of our approach, we develop a prototype extension
called Vega-Lite Annotation. Through comparison with eight existing tools, we
show that our approach enhances expressiveness, reduces authoring effort, and
enables portable, semantically integrated annotation workflows.

</details>


### [70] [Scaffolding Recursive Divergence and Convergence in Story Ideation](https://arxiv.org/abs/2507.03307)
*Taewook Kim,Matthew Kay,Yuqian Sun,Melissa Roemmele,Max Kreminski,John Joon Young Chung*

Main category: cs.HC

TL;DR: Reverger是一种AI驱动的创造力支持工具，支持用户通过灵活交替发散与收敛过程，生成故事修改的多样化方向。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助工具缺乏对发散与收敛过程的高级协调支持，这限制了创意生成的深度与多样性。

Method: Reverger通过递归探索高级修改方向（发散）和合成具体变体（收敛），支持用户迭代优化创意。

Result: 研究表明，Reverger用户能探索更多意外且多样化的方向，并感受到更精细的控制和更有价值的成果。

Conclusion: Reverger通过有效协调发散与收敛过程，显著提升了创意生成的多样性和用户满意度。

Abstract: Human creative ideation involves both exploration of diverse ideas
(divergence) and selective synthesis of explored ideas into coherent
combinations (convergence). While processes of divergence and convergence are
often interleaved and nested, existing AI-powered creativity support tools
(CSTs) lack support for sophisticated orchestration of divergence and
convergence. We present Reverger, an AI-powered CST that helps users ideate
variations of conceptual directions for modifying a story by scaffolding
flexible iteration between divergence and convergence. For divergence, our tool
enables recursive exploration of alternative high-level directions for
modifying a specific part of the original story. For convergence, it allows
users to collect explored high-level directions and synthesize them into
concrete variations. Users can then iterate between divergence and convergence
until they find a satisfactory outcome. A within-subject study revealed that
Reverger permitted participants to explore more unexpected and diverse
high-level directions than a comparable baseline. Reverger users also felt that
they had more fine-grained control and discovered more effort-worthy outcomes.

</details>


### [71] [On the dynamics of affective states during play and the role of confusion](https://arxiv.org/abs/2507.03391)
*Thomas Vase Schultz Volden,Oleg Jarma Montoya,Paolo Burelli,Marco Scirea*

Main category: cs.HC

TL;DR: 论文探讨了电子游戏中“困惑”的积极作用，认为它可以促进玩家学习，并通过实验验证了困惑与学习体验的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管游戏设计师通常认为困惑是消极的，但研究表明它可能促进玩家的学习动机。因此，需要进一步研究困惑在游戏中的角色及其对玩家体验的影响。

Method: 设计了一个实验，让玩家体验故意引发困惑的游戏原型，并收集学习相关的情感数据，与复杂学习模型进行对比分析。

Result: 实验证实，在某些情况下，玩家的体验与学习体验一致。此外，困惑情感与“玩家体验量表”中的“心流体验”存在相关性。

Conclusion: 困惑在游戏中可以发挥积极作用，尤其是在促进学习和心流体验方面，为游戏设计提供了新的视角。

Abstract: Video game designers often view confusion as undesirable, yet it is
inevitable, as new players must adapt to new interfaces and mechanics in an
increasingly varied and innovative game market, which is more popular than
ever. Research suggests that confusion can contribute to a positive experience,
potentially motivating players to learn. The state of confusion in video games
should be further investigated to gain more insight into the learning
experience of play and how it affects the player experience. In this article,
we design a study to collect learning-related affects for users playing a game
prototype that intentionally confuses the player. We assess the gathered
affects against a complex learning model, affirming that, in specific
instances, the player experience aligns with the learning experiences.
Moreover, we identify correlations between these affects and the Player
Experience Inventory constructs, particularly concerning flow experiences.

</details>


### [72] [TILES-2018 Sleep Benchmark Dataset: A Longitudinal Wearable Sleep Data Set of Hospital Workers for Modeling and Understanding Sleep Behaviors](https://arxiv.org/abs/2507.03520)
*Tiantian Feng,Brandon M Booth,Karel Mundnich,Emily Zhou,Benjamin Girault,Kristina Lerman,Shrikanth Narayanan*

Main category: cs.HC

TL;DR: 睡眠研究对日常功能和生活质量至关重要。TILES-2018睡眠基准数据集提供了可穿戴设备收集的睡眠数据和自报数据，用于睡眠模式分析和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴设备在自然环境中监测睡眠模式，填补睡眠研究的数据空白，支持睡眠行为建模。

Method: 使用手腕可穿戴设备（加速度计和心率传感器）收集139名医院员工的睡眠数据，并结合自报调查数据进行分析。

Result: 数据集包含6,000多次睡眠记录，并通过机器学习任务验证了其用途，如睡眠阶段分类和睡眠质量预测。

Conclusion: TILES-2018数据集为睡眠行为建模提供了重要资源，推动了可穿戴设备在睡眠研究中的应用。

Abstract: Sleep is important for everyday functioning, overall well-being, and quality
of life. Recent advances in wearable sensing technology have enabled
continuous, noninvasive, and cost-effective monitoring of sleep patterns in
real-world natural living settings. Wrist-worn devices, in particular, are
capable of tracking sleep patterns using accelerometers and heart rate sensors.
To support sleep research in naturalistic environments using wearable sensors,
we introduce the TILES-2018 Sleep Benchmark dataset, which we make publicly
available to the research community. This dataset was collected over a 10-week
period from 139 hospital employees and includes over 6,000 unique sleep
recordings, alongside self-reported survey data from each participant, which
includes sleep quality, stress, and anxiety among other measurements. We
present in-depth analyses of sleep patterns by combining the TILES-2018 Sleep
Benchmark dataset with a previously released dataset (TILES-2018), which
follows a similar study protocol. Our analyses include sleep duration, sleep
stages, and sleep diaries. Moreover, we report machine learning benchmarks
using this dataset as a testbed for tasks including sleep stage classification,
prediction of self-reported sleep quality, and classifying demographics.
Overall, this dataset provides a valuable resource for advancing foundational
studies in sleep behavior modeling.

</details>


### [73] [Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI](https://arxiv.org/abs/2507.03670)
*Nikhita Joshi,Daniel Vogel*

Main category: cs.HC

TL;DR: 通过修改AI助手提示输入界面的交互技术（如按压提交按钮和滑动条）可以增加用户提示长度和心理所有权感，而AI生成建议虽能进一步增加提示长度但对心理所有权无提升。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过界面交互设计鼓励用户为AI助手编写更长的提示，从而提升其心理所有权感。

Method: 通过两个实验，评估按压、滑动条等交互技术及AI生成建议对提示长度和心理所有权的影响。

Result: 交互技术显著增加提示长度和心理所有权，AI生成建议仅增加提示长度。

Conclusion: 简单界面修改可有效促使用户更多写作并增强心理所有权。

Abstract: Writing longer prompts for an AI assistant to generate a short story
increases psychological ownership, a user's feeling that the writing belongs to
them. To encourage users to write longer prompts, we evaluated two interaction
techniques that modify the prompt entry interface of chat-based generative AI
assistants: pressing and holding the prompt submission button, and continuously
moving a slider up and down when submitting a short prompt. A within-subjects
experiment investigated the effects of such techniques on prompt length and
psychological ownership, and results showed that these techniques increased
prompt length and led to higher psychological ownership than baseline
techniques. A second experiment further augmented these techniques by showing
AI-generated suggestions for how the prompts could be expanded. This further
increased prompt length, but did not lead to improvements in psychological
ownership. Our results show that simple interface modifications like these can
elicit more writing from users and improve psychological ownership.

</details>


### [74] [Is AI mingling or bullying me? Exploring User Interactions with a Chatbot in China](https://arxiv.org/abs/2507.03892)
*Nuo Chen,Pu Yan,Jia Li,Qixuan Zhao*

Main category: cs.HC

TL;DR: 本文研究了社交聊天机器人Robert的自主评论行为对人类-AI互动的影响，揭示其复杂情感谱和情感差距。


<details>
  <summary>Details</summary>
Motivation: 研究Robert这种新型AI驱动的社交媒体参与形式，探讨其如何重塑日常在线环境中的人类-AI互动。

Method: 使用计算语言学技术（如主题分类和情感分析），分析了3900多条用户提交的与Robert的互动数据。

Result: 发现六个关键主题，情感分析显示Robert的评论既能引发温暖与幽默，也可能隐含敌意，揭示了人类与自主AI之间的情感差距。

Conclusion: 研究为人类-AI互动提供了新见解，探讨了未请求AI参与数字公共领域的情感动态和社会技术影响。

Abstract: Since its viral emergence in early 2024, Comment Robert-a Weibo-launched
social chatbot-has gained widespread attention on the Chinese Internet for its
unsolicited and unpredictable comments on user posts. Unlike conventional
chatbots that respond only to user prompts, Robert autonomously intervenes in
public discourse, representing a novel form of AI-driven social media
engagement. This study examines how such autonomous, algorithmic communication
reshapes human-AI interaction in everyday online contexts. Using computational
linguistics techniques, including topic classification and sentiment analysis,
we analyze over 3,900 user-submitted interactions from the "Robert Victims
Alliance", a grassroots community documenting their exchanges with the chatbot.
Topic modeling reveals six key themes: interpersonal relationships,
self-identity, academic and career concerns, subcultures, sensitive topics, and
social events. Complementing this, mixed-methods emotional analysis uncovers a
complex affective spectrum: Robert's casual remarks can evoke warmth and humor
but may also conceal covert hostility beneath neutral or polite language. These
ambivalent interactions reveal an emerging emotional divide between humans and
socially proactive AI, suggesting that while Robert simulates social presence,
it often falls short of users' emotional needs. Our study contributes to
human-AI interaction research by offering new insights into the affective
dynamics and socio-technical implications of unsolicited AI bots' participation
in digital public spheres.

</details>


### [75] [More than One Step at a Time: Designing Procedural Feedback for Non-visual Makeup Routines](https://arxiv.org/abs/2507.03942)
*Franklin Mingzhe Li,Akihiko Oharazawa,Chloe Qingyu Zhu,Misty Fan,Daisuke Sato,Chieko Asakawa,Patrick Carrington*

Main category: cs.HC

TL;DR: 这篇论文探讨了视力障碍者在化妆过程中面临的挑战，并提出了一种辅助技术设计方案，以提供实时、目标一致的反馈。


<details>
  <summary>Details</summary>
Motivation: 化妆在自我表达、身份认同和信心方面起着重要作用，但对于视力障碍者来说，现有的辅助工具未能充分解决化妆过程中的复杂性问题，如步骤协调和最终效果评估。

Method: 研究通过对15名视力障碍化妆用户进行实地调研，记录他们的化妆行为和需求，并采访了5名专业化妆师，以获取专家反馈。

Result: 研究揭示了视力障碍者在化妆时采用的触觉优先策略，以及在对称性和评估方面的持续挑战，同时提出了一种反馈需求分类法。

Conclusion: 未来的辅助系统应注重免手操作、对话式交互和上下文感知支持，以帮助视力障碍者实现独立且富有表现力的化妆。

Abstract: Makeup plays a vital role in self-expression, identity, and confidence - yet
remains an underexplored domain for assistive technology, especially for people
with vision impairments. While existing tools support isolated tasks such as
color identification or product labeling, they rarely address the procedural
complexity of makeup routines: coordinating step sequences, managing product
placement, and assessing the final look with accessible feedback. To understand
the real-world process, we conducted a contextual inquiry with 15 visually
impaired makeup users, capturing real-time makeup application behaviors and
their step-by-step information needs and assessment approaches. Our findings
reveal embodied, tactile-first strategies; persistent challenges in blending,
symmetry, and assessment; and a desire for honest, real-time, goal-aligned
feedback. We also interviewed five professional makeup artists, who reviewed
participant makeup videos and provided expert responses to participant-raised
questions and assessment practices. We contribute a taxonomy of feedback needs
in non-visual makeup, and outline design implications for future assistive
systems - emphasizing hands-free, conversational interaction and context-aware,
procedural support for expressive and independent beauty practices.

</details>


### [76] [Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents](https://arxiv.org/abs/2507.04005)
*Baiqiao Zhang,Xiangxian Li,Chao Zhou,Xinyu Gai,Zhifeng Liao,Juan Liu,Xue Yang,Niqi Liu,Xiaojuan Ma,Yong-jin Liu,Yulong Bian*

Main category: cs.HC

TL;DR: 该研究提出了一种基于多性格表征的游戏化人格评估框架Multi-PR GPA，利用大语言模型生成多样化虚拟角色，通过互动游戏实现两种人格评估方式，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索一种交互式人格评估方法，关注人格表征的多样性。

Method: 提出Multi-PR GPA框架，利用大语言模型生成多样化虚拟角色，通过互动游戏收集多类型文本数据，实现直接评估和基于队列的评估。

Result: 原型系统验证了Multi-PR GPA在人格评估中的有效性，尤其在考虑人格表征多样性时表现更优。

Conclusion: Multi-PR GPA框架为基于多性格表征的人格评估提供了一种有效且解释性强的解决方案。

Abstract: The execution of effective and imperceptible personality assessments is
receiving increasing attention in psychology and human-computer interaction
fields. This study explores an interactive approach for personality assessment,
focusing on the multiplicity of personality representation. We propose a
framework of gamified personality assessment through multi-personality
representations (Multi-PR GPA). The framework leverages Large Language Models
to empower virtual agents with diverse personalities. These agents elicit
multifaceted human personality representations through engaging in interactive
games. Drawing upon the multi-type textual data generated throughout the
interaction, it achieves two ways of personality assessments (i.e., Direct
Assessment and Que-based Assessment) and provides interpretable insights.
Grounded in the classic Big Five theory, we implemented a prototype system and
conducted a user study to assess the efficacy of Multi-PR GPA. The results
underscore the effectiveness of our approach in personality assessment and
demonstrate that it achieves superior performance when considering the
multiplicity of personality representation.

</details>


### [77] [Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study](https://arxiv.org/abs/2507.04043)
*Kai Deng*

Main category: cs.HC

TL;DR: 研究探讨了ChatGPT-4o的不同交互方式（被动、主动、协作）对高中生完成编程任务的影响。协作方式显著提升了任务完成时间和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在教育工具中的普及，如何设计其交互方式以优化用户体验和学习效果成为重要问题。

Method: 采用组内实验设计，15名高中生在不同交互模式的ChatGPT-4o下完成编程任务，量化分析表现和反馈。

Result: 协作交互方式在任务完成时间和用户满意度上表现最佳。

Conclusion: LLMs的交互设计对学习效果有显著影响，需注重交互性和用户中心化，尤其对初学者。

Abstract: As large language models (LLMs) become more common in educational tools and
programming environments, questions arise about how these systems should
interact with users. This study investigates how different interaction styles
with ChatGPT-4o (passive, proactive, and collaborative) affect user performance
on simple programming tasks. I conducted a within-subjects experiment where
fifteen high school students participated, completing three problems under
three distinct versions of the model. Each version was designed to represent a
specific style of AI support: responding only when asked, offering suggestions
automatically, or engaging the user in back-and-forth dialogue.Quantitative
analysis revealed that the collaborative interaction style significantly
improved task completion time compared to the passive and proactive conditions.
Participants also reported higher satisfaction and perceived helpfulness when
working with the collaborative version. These findings suggest that the way an
LLM communicates, how it guides, prompts, and responds, can meaningfully impact
learning and performance. This research highlights the importance of designing
LLMs that go beyond functional correctness to support more interactive,
adaptive, and user-centered experiences, especially for novice programmers.

</details>


### [78] [Human-centered AI with focus on Human-robot interaction (Book chapter)](https://arxiv.org/abs/2507.04095)
*Alireza Mortezapour,Giuliana Vitiello*

Main category: cs.HC

TL;DR: 本文提出了一种名为“双金字塔”的新框架，用于描述人与机器人互动中的多层次需求。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人在第四次工业革命中的应用，其与人类互动中的挑战和问题逐渐显现，促使研究者思考如何使这些机器人更加人性化以满足用户需求。

Method: 基于以人为中心的人工智能原则，首次提出“双金字塔”框架，涵盖从基本机器人效能到联合国17项可持续发展目标等宏观需求的多层次人类需求。

Result: 通过“双金字塔”框架，系统地展示了人与机器人互动中的多层次需求，为未来机器人设计提供了指导。

Conclusion: “双金字塔”框架为理解人与机器人互动的复杂性提供了一个全面视角，强调了面向人类需求的设计在机器人技术发展中的重要性。

Abstract: Modern social robots can be considered the descendants of steam engines from
the First Industrial Revolution (IR 1.0) and industrial robotic arms from the
Third Industrial Revolution (IR 3.0). As some time has passed since the
introduction of these robots during the Fourth Industrial Revolution (IR 4.0),
challenges and issues in their interaction with humans have emerged, leading
researchers to conclude that, like any other AI-based technology, these robots
must also be human-centered to meet the needs of their users. This chapter aims
to introduce humans and their needs in interactions with robots, ranging from
short-term, one-on-one interactions (micro-level) to long-term, macro-level
needs at the societal scale. Building upon the principles of human-centered AI,
this chapter presents, for the first time, a new framework of human needs
called the Dual Pyramid. This framework encompasses a comprehensive list of
human needs in robot interactions, from the most fundamental, robot
effectiveness to macro level requirements, such as the collaboration with
robots in achieving the United Nations 17 Sustainable Development Goals.

</details>


### [79] [HyperSumm-RL: A Dialogue Summarization Framework for Modeling Leadership Perception in Social Robots](https://arxiv.org/abs/2507.04160)
*Subasish Das*

Main category: cs.HC

TL;DR: HyperSumm-RL是一个基于超文本的框架，用于分析社交机器人领导力的人类感知，通过长对话总结和交互分析，支持动态对话的语义化组织和导航。


<details>
  <summary>Details</summary>
Motivation: 研究人类对社交机器人领导力的感知，弥补现有静态或任务导向的人机交互研究的不足。

Method: 结合基于Transformer的长对话总结、领导风格建模和用户响应分析，构建超文本化交互模型。

Result: 开发了支持对话主题、参与者响应和机器人行为模式导航的系统，并验证了其在分析信任、参与度和期望变化中的有效性。

Conclusion: 超文本工作流能增强人机交互研究的透明度和语义化分析能力。

Abstract: This paper introduces HyperSumm-RL, a hypertext-aware summarization and
interaction analysis framework designed to investigate human perceptions of
social robot leadership through long-form dialogue. The system utilizes a
structured Natural Language Processing (NLP) workflow that combines
transformer-based long dialogue summarization, leadership style modeling, and
user response analysis, enabling scalable evaluation of social robots in
complex human-robot interaction (HRI) settings. Unlike prior work that focuses
on static or task-oriented HRI, HyperSumm-RL captures and hypertextually
organizes dynamic conversational exchanges into navigable, semantically rich
representations which allows researchers to trace interaction threads, identify
influence cues, and analyze leadership framing over time. The contributions of
this study are threefold: (1) we present a novel infrastructure for summarizing
and linking long, multi-turn dialogues using leadership-style taxonomies; (2)
we propose an interactive hypertext model that supports relational navigation
across conversational themes, participant responses, and robot behavior modes;
and (3) we demonstrate the utility of this system in interpreting participant
trust, engagement, and expectation shifts during social robot leadership
scenarios. The findings reveal how hypertextual workflows can augment HRI
research by enabling transparent, interpretable, and semantically grounded
analysis of emergent social dynamics.

</details>


### [80] [iBreath: Usage Of Breathing Gestures as Means of Interactions](https://arxiv.org/abs/2507.04162)
*Mengxi Liu,Daniel Geißler,Deepika Gurung,Hymalai Bello,Bo Zhou,Sizhen Bian,Paul Lukowicz,Passant Elagroudy*

Main category: cs.HC

TL;DR: iBreath是一个通过生物阻抗检测呼吸手势的新型系统，研究显示其检测准确率高（F1分数>95.2%），用户体验良好，并为未来呼吸手势开发提供了八项实用指南。


<details>
  <summary>Details</summary>
Motivation: 探索呼吸作为一种无需手动的交互方式，以生物阻抗技术为基础，提升交互的自然性和便捷性。

Method: 采用两轮实验室研究（n=34），评估iBreath系统的检测准确性和用户体验，并开发了用户依赖和独立模型。

Result: 系统检测准确率高（F1分数>95.2%），用户对单次点击手势接受度高，手势持续时间中位数为3.5-5.3秒。

Conclusion: iBreath为呼吸手势交互的研究奠定了基础，提供了实用指南，未来可进一步优化和扩展。

Abstract: Breathing is a spontaneous but controllable body function that can be used
for hands-free interaction. Our work introduces "iBreath", a novel system to
detect breathing gestures similar to clicks using bio-impedance. We evaluated
iBreath's accuracy and user experience using two lab studies (n=34). Our
results show high detection accuracy (F1-scores > 95.2%). Furthermore, the
users found the gestures easy to use and comfortable. Thus, we developed eight
practical guidelines for the future development of breathing gestures. For
example, designers can train users on new gestures within just 50 seconds (five
trials), and achieve robust performance with both user-dependent and
user-independent models trained on data from 21 participants, each yielding
accuracies above 90%. Users preferred single clicks and disliked triple clicks.
The median gesture duration is 3.5-5.3 seconds. Our work provides solid ground
for researchers to experiment with creating breathing gestures and
interactions.

</details>


### [81] [WSCoach: Wearable Real-time Auditory Feedback for Reducing Unwanted Words in Daily Communication](https://arxiv.org/abs/2507.04238)
*Zhang Youpeng,Nuwan Janaka,Ashwin Ram,Yin Peilin,Tian Yang,Shengdong Zhao,Pierre Dragicevic*

Main category: cs.HC

TL;DR: 研究探索了通过可穿戴设备的听觉反馈减少日常交流中不良用词的方法，开发了WSCoach系统，并证明其优于传统移动应用。


<details>
  <summary>Details</summary>
Motivation: 利用可穿戴设备的行为干预潜力，填补设计有效系统的空白。

Method: 通过设计空间探索、初步研究和原型开发，构建了实时反馈系统WSCoach，并与移动应用对比评估。

Result: WSCoach在长期减少不良用词方面比移动应用更有效。

Conclusion: 可穿戴技术对行为矫正有潜力，并提出了设计指南。

Abstract: The rise of wearable smart devices raises unprecedented opportunities for
self-improvement through ubiquitous behavior tracking and guidance. However,
the design of effective wearable behavior intervention systems remains
relatively unexplored. To address this gap, we conducted controlled studies
focusing on the reduction of unwanted words (e.g., filler words, swear words)
in daily communication through auditory feedback using wearable technology. We
started with a design space exploration, considering various factors such as
the type, duration, and timing of the auditory feedback. Then, we conducted
pilot studies to reduce the space of design choices and prototyped a system
called WSCoach (Wearable Speech Coach), which informs users when they utter
unwanted words in near-real-time. To evaluate WSCoach, we compared it with a
state-of-the-art mobile application supporting post-hoc conversation analysis.
Both approaches were effective in reducing the occurrence of unwanted words,
but WSCoach appears to be more effective in the long run. Finally, we discuss
guidelines for the design of wearable audio-based behavior monitoring and
intervention systems and highlight the potential of wearable technology for
facilitating behavior correction and improvement. For supplementary material,
please see the META Appendix and our OSF project at
https://osf.io/6vhwn/?view_only=489498d3ac2d4703a17475fc6ca65dfa.

</details>


### [82] [RunPacer: A Smartwatch-Based Vibrotactile Feedback System for Symmetric Co-Running by Visually Impaired Individuals and Guides](https://arxiv.org/abs/2507.04241)
*Yichen Yu,Huan-Song Xu*

Main category: cs.HC

TL;DR: RunPacer是一款基于智能手表的触觉反馈系统，旨在帮助视障跑者与陪跑员同步节奏，减少对口头提示或物理连接的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案多关注导航或避障，忽视了跑步中实时节奏协调的重要性，RunPacer旨在填补这一空白。

Method: 通过预设目标步频或动态适应陪跑员的自然节奏，向双方提供同步的触觉提示。

Result: RunPacer提供了一种轻量级、社交合作的非视觉辅助框架，实现直觉且高效的协同跑步体验。

Conclusion: RunPacer重新定义了协同跑步的共享、具体化和无障碍体验，并展望了未来多模态反馈的扩展。

Abstract: Visually impaired individuals often require a guide runner to safely
participate in outdoor running. However, maintaining synchronized pacing with
verbal cues or tethers can be mentally taxing and physically restrictive.
Existing solutions primarily focus on navigation or obstacle avoidance but
overlook the importance of real-time interpersonal rhythm coordination during
running. We introduce RunPacer, a smartwatch-based vibrotactile feedback system
that delivers synchronized rhythmic pulses to both runners. In contrast to
conventional guide-running systems that rely heavily on continuous verbal
communication or mechanical tethering, RunPacer emphasizes interpersonal
cadence alignment as its core interaction model. By pre-setting a target step
frequency or dynamically adapting to the guide's natural pace, the system
ensures that both runners receive identical haptic cues, enabling them to
maintain coordinated motion intuitively and efficiently. This poster presents
the system architecture, positions it within prior research on haptic
entrainment, and outlines the vision for future field deployment, including
potential multimodal feedback extensions. RunPacer contributes a lightweight,
socially cooperative, and non-visual assistive framework that reimagines
co-running as a shared, embodied, and accessible experience.

</details>


### [83] [DMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth](https://arxiv.org/abs/2507.04278)
*Zheng Lian,Licai Sun,Haoyu Chen,Zebang Cheng,Fan Zhang,Ziyu Jia,Ziyang Ma,Fei Ma,Xiaojiang Peng,Jianhua Tao*

Main category: cs.HC

TL;DR: DMER-Ranker是一种新的评估策略，通过将传统“预测与真实描述”的比较转化为“预测与预测”的比较，解决了自由形式情感描述任务DMER的评估难题。


<details>
  <summary>Details</summary>
Motivation: 传统的判别性方法依赖于预定义的情感分类，限制了情感表达的灵活性。DMER任务虽然提供了更细粒度和可解释的情感表示，但其自由形式的预测范式引入了评估的挑战。

Method: 提出DMER-Ranker，利用“预测间比较”替代“预测与真实描述”比较，结合Bradley-Terry算法将成对比较结果转换为模型排名。

Result: DMER-Ranker消除了对真实描述的依赖，并引入了DMER-Preference数据集，为情感偏好预测提供了支持。

Conclusion: 该研究推动了DMER领域的发展，为更智能的人机交互系统奠定了基础。

Abstract: Descriptive Multimodal Emotion Recognition (DMER) is a newly proposed task
that aims to describe a person's emotional state using free-form natural
language. Unlike traditional discriminative methods that rely on predefined
emotion taxonomies, DMER provides greater flexibility in emotional expression,
enabling fine-grained and interpretable emotion representations. However, this
free-form prediction paradigm introduces significant challenges in evaluation.
Existing methods either depend on ground-truth descriptions that require
substantial manual effort or simplify the task by shifting the focus from
evaluating descriptions to evaluating emotion labels. However, the former
suffers from the labor-intensive collection of comprehensive descriptions,
while the latter overlooks critical aspects such as emotional temporal
dynamics, intensity, and uncertainty. To address these limitations, we propose
DMER-Ranker, a novel evaluation strategy that reformulates the traditional
``prediction-ground truth'' comparison into the ``prediction-prediction''
comparison, eliminating the need for ground-truth descriptions. We then employ
the Bradley-Terry algorithm to convert pairwise comparison results into
model-level rankings. Additionally, we explore the possibility of automatic
preference prediction and introduce DMER-Preference, the first preference
dataset specifically designed for human emotions. Our work advances the field
of DMER and lays the foundation for more intelligent human-computer interaction
systems.

</details>


### [84] [Do Students Write Better Post-AI Support? Effects of Generative AI Literacy and Chatbot Interaction Strategies on Multimodal Academic Writing](https://arxiv.org/abs/2507.04398)
*Yueqiao Jin,Kaixun Yang,Roberto Martinez-Maldonado,Dragan Gašević,Lixiang Yan*

Main category: cs.HC

TL;DR: 研究表明，GenAI素养和聊天机器人互动策略对学生独立多模态学术写作能力有显著影响，尤其在被动互动条件下。


<details>
  <summary>Details</summary>
Motivation: 探讨学生GenAI素养和聊天机器人互动策略对其独立多模态写作能力的影响。

Method: 79名高等教育学生在两种聊天机器人辅助条件下完成写作任务，并通过五个维度评估表现。

Result: GenAI素养高的学生在AI辅助移除后表现更好，被动互动策略效果更显著。

Conclusion: 应加强GenAI素养培训，平衡外部支持与自主学习，优化AI教学策略。

Abstract: Academic writing increasingly involves multimodal tasks requiring students to
integrate visual information and textual arguments. While generative AI (GenAI)
tools, like ChatGPT, offer new pathways for supporting academic writing, little
is known about how students' GenAI literacy influences their independent
multimodal writing skills or how chatbot interaction strategies (passive
reactive vs. proactive scaffolding) impact learning. This study examined 79
higher education students' multimodal academic writing performance using a
comparative research design. Students completed writing tasks integrating
visual data under two chatbot-assisted conditions (passive vs. proactive) and
subsequently without AI assistance. Their writing performance was rigorously
evaluated across five dimensions, including insightfulness, visual data
integration, organisation, linguistic quality, and critical thinking. Ordinal
logistic regression and correlation analyses revealed that higher levels of
GenAI literacy significantly predicted stronger independent multimodal writing
performance immediately after AI assistance removal, particularly for students
using passive chatbots requiring active prompting. These results highlight the
critical role of GenAI literacy and specific chatbot interaction strategies in
shaping students' capacities for independent multimodal academic writing. Our
findings emphasise the need for purposeful integration of GenAI literacy
training into curricula and balancing external scaffolding support with
autonomous learning opportunities. This research offers valuable
recommendations for educators leveraging AI-enhanced pedagogies to optimise
student writing outcomes and technological engagement strategies.

</details>


### [85] [Dude, where's my utterance? Evaluating the effects of automatic segmentation and transcription on CPS detection](https://arxiv.org/abs/2507.04454)
*Videep Venkatesha,Mariah Bradford,Nathaniel Blanchard*

Main category: cs.HC

TL;DR: 研究了自動化轉錄和語音分段對協作問題解決（CPS）標記檢測的影響，結果顯示性能與人工處理相當，但自動分段減少數據細粒度。


<details>
  <summary>Details</summary>
Motivation: 開發AI系統以檢測CPS標記，幫助教師識別團隊協作問題或成效。

Method: 評估自動化轉錄和語音分段對CPS檢測的影響，使用公開數據集WTD。

Result: 自動化方法性能接近人工處理，但分段方法減少26.5%的語句，影響數據細粒度。

Conclusion: 自動化方法可行但需注意數據細粒度損失，對AI支持協作學習工具有啟示。

Abstract: Collaborative Problem-Solving (CPS) markers capture key aspects of effective
teamwork, such as staying on task, avoiding interruptions, and generating
constructive ideas. An AI system that reliably detects these markers could help
teachers identify when a group is struggling or demonstrating productive
collaboration. Such a system requires an automated pipeline composed of
multiple components. In this work, we evaluate how CPS detection is impacted by
automating two critical components: transcription and speech segmentation. On
the public Weights Task Dataset (WTD), we find CPS detection performance with
automated transcription and segmentation methods is comparable to
human-segmented and manually transcribed data; however, we find the automated
segmentation methods reduces the number of utterances by 26.5%, impacting the
the granularity of the data. We discuss the implications for developing
AI-driven tools that support collaborative learning in classrooms.

</details>


### [86] [The role of large language models in UI/UX design: A systematic literature review](https://arxiv.org/abs/2507.04469)
*Ammar Ahmed,Ali Shariq Imran*

Main category: cs.HC

TL;DR: 系统文献综述探讨了大型语言模型（LLMs）在UI/UX设计中的应用，总结了38篇同行评审研究，发现LLMs正重塑设计流程，但也存在幻觉、提示不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs在UI/UX设计中的作用及其挑战，为未来技术整合提供方向。

Method: 综述38篇2022-2025年的研究，识别关键LLMs及其在设计生命周期中的集成方式。

Result: LLMs在设计流程中广泛应用，但仍需解决幻觉、提示不稳定等问题。

Conclusion: LLMs是设计的潜在协作伙伴，需推动其伦理和高效整合。

Abstract: This systematic literature review examines the role of large language models
(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies
published between 2022 and 2025. We identify key LLMs in use, including GPT-4,
Gemini, and PaLM, and map their integration across the design lifecycle, from
ideation to evaluation. Common practices include prompt engineering,
human-in-the-loop workflows, and multimodal input. While LLMs are reshaping
design processes, challenges such as hallucination, prompt instability, and
limited explainability persist. Our findings highlight LLMs as emerging
collaborators in design, and we propose directions for the ethical, inclusive,
and effective integration of these technologies.

</details>


### [87] [A validity-guided workflow for robust large language model research in psychology](https://arxiv.org/abs/2507.04491)
*Zhicheng Lin*

Main category: cs.HC

TL;DR: 论文指出大语言模型在心理学研究中存在测量不可靠的问题，提出六阶段工作流程以确保研究有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理学研究中应用广泛，但近期发现其测量结果存在严重不可靠性，威胁研究有效性。

Method: 基于双效度框架，提出六阶段工作流程，包括明确研究目标、验证计算工具、控制实验变量等。

Result: 通过系统验证，可以区分真实的计算现象与测量假象，为AI心理学研究奠定基础。

Conclusion: 工作流程通过透明实践和验证工具，为心理学研究提供了更可靠的实证基础。

Abstract: Large language models (LLMs) are rapidly being integrated into psychological
research as research tools, evaluation targets, human simulators, and cognitive
models. However, recent evidence reveals severe measurement unreliability:
Personality assessments collapse under factor analysis, moral preferences
reverse with punctuation changes, and theory-of-mind accuracy varies widely
with trivial rephrasing. These "measurement phantoms"--statistical artifacts
masquerading as psychological phenomena--threaten the validity of a growing
body of research. Guided by the dual-validity framework that integrates
psychometrics with causal inference, we present a six-stage workflow that
scales validity requirements to research ambition--using LLMs to code text
requires basic reliability and accuracy, while claims about psychological
properties demand comprehensive construct validation. Researchers must (1)
explicitly define their research goal and corresponding validity requirements,
(2) develop and validate computational instruments through psychometric
testing, (3) design experiments that control for computational confounds, (4)
execute protocols with transparency, (5) analyze data using methods appropriate
for non-independent observations, and (6) report findings within demonstrated
boundaries and use results to refine theory. We illustrate the workflow through
an example of model evaluation--"LLM selfhood"--showing how systematic
validation can distinguish genuine computational phenomena from measurement
artifacts. By establishing validated computational instruments and transparent
practices, this workflow provides a path toward building a robust empirical
foundation for AI psychology research.

</details>


### [88] [Using Psychophysiological Insights to Evaluate the Impact of Loot Boxes on Arousal](https://arxiv.org/abs/2507.04906)
*Gianmarco Tedeschi,Rune Kristian Lundedal Nielsen,Paolo Burelli*

Main category: cs.HC

TL;DR: 研究探讨了游戏内抽奖机制（战利品箱）的心理生理效应及其与赌博效应的相似性，通过EDA测量玩家兴奋程度，分析其与游戏成瘾严重度的关系。


<details>
  <summary>Details</summary>
Motivation: 探索战利品箱机制是否与赌博类似，及其对游戏成瘾的影响，为开发者和政策制定者提供风险参考。

Method: 使用定制游戏控制实验条件，通过IGDS9-SF评估游戏成瘾严重度，EDA测量兴奋程度（包括紧张和相位成分）。

Result: 研究发现战利品箱可能引发与赌博相似的心理生理反应，可能与游戏成瘾程度相关。

Conclusion: 研究为战利品箱的风险提供了科学依据，建议开发者和政策制定者关注其潜在危害。

Abstract: This study investigates the psychophysiological effects of loot box
interactions in video games and their potential similarities to those recorded
during gambling interactions. Using electrodermal activity (EDA) measurements,
the research examines player arousal during loot box interactions and explores
the relationship between Internet Gaming Disorder (IGD) severity and loot box
interactions from a psychophysiological perspective. The study employs a
custom-designed game to control experimental conditions and standardise loot
box interactions. Participants' IGD severity is assessed using the Internet
Gaming Disorder Scale - Short Form (IGDS9-SF), while arousal is measured
through EDA, analysing both tonic and phasic components. The study contributes
to the ongoing debate surrounding gaming disorder and loot boxes, offering
insights for game developers and policymakers on the potential risks associated
with random reward mechanisms in video games.

</details>


### [89] [Cat Royale: An Artistic Inquiry into Trust in Robots](https://arxiv.org/abs/2507.04970)
*Matt Adams,Nick Tandavanitj,Steve Benford,Ayse Kucukyilmaz,Victor Ngo,Simon Castle-Green,Guido Salimberi,Pepita Bernard,Joel Fischer,Alan Chamberlain,Eike Schneiders,Clara Mancini*

Main category: cs.HC

TL;DR: 艺术项目'Cat Royale'探讨是否信任机器人照顾宠物，通过猫的乌托邦环境和机器人互动引发思考。


<details>
  <summary>Details</summary>
Motivation: 研究人类是否应信任机器人照顾所爱之人（如宠物），探讨自主系统的信任问题。

Method: 艺术家创造猫的乌托邦环境，由机器人手臂与猫互动12天，决策引擎根据猫的幸福评估推荐游戏。

Result: 项目制作了8小时电影，全球巡展，引发观众对自主系统信任的讨论。

Conclusion: 通过艺术形式成功引发公众对机器人信任问题的关注和思考。

Abstract: Cat Royale is an artwork created by the artists Blast Theory to explore the
question of whether we should trust robots to care for our loved ones. The
artists endeavoured to create a `Cat Utopia', a luxurious environment that was
inhabited by a family of three cats for six hours a day for twelve days, at the
centre of which a robot arm played with them by wielding toys. Behind the
scenes, the decision engine recommended games based on ongoing assessment of
their happiness. A video installation featuring an eight-hour movie of the
cats' exploits is currently touring worldwide, provoking audiences to engage
with the question of trust in autonomous systems.

</details>


### [90] [What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User Attributes, Trust Dimensions, Task Context, and Societal Perceptions among University Students](https://arxiv.org/abs/2507.05046)
*Kadija Bouyzourn,Alexandra Birch*

Main category: cs.HC

TL;DR: 研究探讨了大学生对ChatGPT信任的四大影响因素：用户属性、信任维度、任务情境和社会影响。结果显示，使用频率提升信任，而了解大语言模型机制则降低信任；专业性和伦理风险是信任的主要预测因素。


<details>
  <summary>Details</summary>
Motivation: 了解大学生信任ChatGPT的关键因素，为学术场景中部署大型语言模型（LLMs）提供依据。

Method: 混合方法研究：对115名英国本科生和研究生进行问卷调查，辅以4次半结构化访谈。

Result: 信任与任务可验证性、感知能力、伦理对齐及直接经验相关；计算机科学学生对校对和写作任务更信任。

Conclusion: 在学术中使用ChatGPT需关注透明度、准确性提示和用户教育，以增强信任。

Abstract: This mixed-methods inquiry examined four domains that shape university
students' trust in ChatGPT: user attributes, seven delineated trust dimensions,
task context, and perceived societal impact. Data were collected through a
survey of 115 UK undergraduate and postgraduate students and four complementary
semi-structured interviews. Behavioural engagement outweighed demographics:
frequent use increased trust, whereas self-reported understanding of
large-language-model mechanics reduced it. Among the dimensions, perceived
expertise and ethical risk were the strongest predictors of overall trust; ease
of use and transparency had secondary effects, while human-likeness and
reputation were non-significant. Trust was highly task-contingent; highest for
coding and summarising, lowest for entertainment and citation generation, yet
confidence in ChatGPT's referencing ability, despite known inaccuracies, was
the single strongest correlate of global trust, indicating automation bias.
Computer-science students surpassed peers only in trusting the system for
proofreading and writing, suggesting technical expertise refines rather than
inflates reliance. Finally, students who viewed AI's societal impact positively
reported the greatest trust, whereas mixed or negative outlooks dampened
confidence. These findings show that trust in ChatGPT hinges on task
verifiability, perceived competence, ethical alignment and direct experience,
and they underscore the need for transparency, accuracy cues and user education
when deploying LLMs in academic settings.

</details>


### [91] [Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism](https://arxiv.org/abs/2507.05187)
*Andreas Mayer*

Main category: cs.HC

TL;DR: 提出Community-Defined AI Value Pluralism (CDAVP)框架，解决AI系统价值对齐问题，强调社区自治与用户代理。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统的价值对齐方法过于集中化，忽略了价值多元性，导致用户和社区无法挑战或塑造系统价值观，引发信任危机。

Method: 采用CDAVP框架，通过社区定义和维护明确的价值配置文件，用户可动态激活这些配置，AI应用透明解释并调解冲突。

Result: CDAVP框架支持多元价值表达，增强用户代理，提升算法透明度和可争议性，实现以人为中心的AI系统。

Conclusion: 通过结构化多元价值生态系统，CDAVP为算法问责制和真正的人类中心AI提供了可行路径。

Abstract: The proliferation of AI-driven systems presents a fundamental challenge to
Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work
(CSCW), often diminishing user agency and failing to account for value
pluralism. Current approaches to value alignment, which rely on centralized,
top-down definitions, lack the mechanisms for meaningful contestability. This
leaves users and communities unable to challenge or shape the values embedded
in the systems that govern their digital lives, creating a crisis of legitimacy
and trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),
a socio-technical framework that addresses this gap. It reframes the design
problem from achieving a single aligned state to infrastructuring a dynamic
ecosystem for value deliberation and application. At its core, CDAVP enables
diverse, self-organizing communities to define and maintain explicit value
profiles - rich, machine-readable representations that can encompass not only
preferences but also community-specific rights and duties. These profiles are
then contextually activated by the end-user, who retains ultimate control
(agency) over which values guide the AI's behavior. AI applications, in turn,
are designed to transparently interpret these profiles and moderate conflicts,
adhering to a set of non-negotiable, democratically-legitimated meta-rules. The
designer's role shifts from crafting static interfaces to becoming an architect
of participatory ecosystems. We argue that infrastructuring for pluralism is a
necessary pathway toward achieving robust algorithmic accountability and
genuinely contestable, human-centric AI.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [92] [MoDA: Multi-modal Diffusion Architecture for Talking Head Generation](https://arxiv.org/abs/2507.03256)
*Xinyang Li,Gen Li,Zhihui Lin,Yichen Qian,GongXin Yao,Weinan Jia,Weihua Chen,Fan Wang*

Main category: cs.GR

TL;DR: MoDA通过定义联合参数空间和引入多模态扩散架构，解决了基于扩散模型的说话头生成中的效率低、视觉伪影和表情不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在说话头生成中效率低、视觉伪影以及多模态信息交互不足导致的真实感问题。

Method: 1) 定义联合参数空间结合流匹配简化扩散学习；2) 引入多模态扩散架构增强表情丰富性；3) 采用粗到细融合策略整合多模态特征。

Result: MoDA显著提升了视频多样性、真实性和效率，适用于实际应用。

Conclusion: MoDA通过优化扩散学习和多模态交互，有效提升了说话头生成的质量和实用性。

Abstract: Talking head generation with arbitrary identities and speech audio remains a
crucial problem in the realm of digital humans and the virtual metaverse.
Recently, diffusion models have become a popular generative technique in this
field with their strong generation and generalization capabilities. However,
several challenges remain for diffusion-based methods: 1) inefficient inference
and visual artifacts, which arise from the implicit latent space of Variational
Auto-Encoders (VAE), complicating the diffusion process; 2) authentic facial
expressions and head movements, resulting from insufficient multi-modal
information interaction. In this paper, MoDA handle these challenges by 1)
defines a joint parameter space to bridge motion generation and neural
rendering, and leverages flow matching to simplify the diffusion learning
process; 2) introduces a multi-modal diffusion architecture to model the
interaction among noisy motion, audio, and auxiliary conditions, ultimately
enhancing overall facial expressiveness. Subsequently, a coarse-to-fine fusion
strategy is adopted to progressively integrate different modalities, ensuring
effective integration across feature spaces. Experimental results demonstrate
that MoDA significantly improves video diversity, realism, and efficiency,
making it suitable for real-world applications.

</details>


### [93] [3D PixBrush: Image-Guided Local Texture Synthesis](https://arxiv.org/abs/2507.03731)
*Dale Decatur,Itai Lang,Kfir Aberman,Rana Hanocka*

Main category: cs.GR

TL;DR: 3D PixBrush是一种无需用户输入即可在3D网格上实现图像驱动局部编辑的方法，通过预测定位掩模和合成纹理，实现全局一致且局部精确的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 为了在3D网格上实现更自然、精确的图像驱动局部编辑，避免依赖用户输入（如涂鸦或边界框）。

Method: 提出一种改进的分数蒸馏采样技术，结合预测的定位掩模和参考图像（称为定位调制图像引导），自动生成全局一致且局部精确的掩模和纹理。

Result: 实验验证了该方法在多种网格和图像上的有效性，能够实现高精度的局部编辑。

Conclusion: 3D PixBrush提供了一种无需用户干预的高效3D网格局部编辑方法，具有广泛的应用潜力。

Abstract: We present 3D PixBrush, a method for performing image-driven edits of local
regions on 3D meshes. 3D PixBrush predicts a localization mask and a
synthesized texture that faithfully portray the object in the reference image.
Our predicted localizations are both globally coherent and locally precise.
Globally - our method contextualizes the object in the reference image and
automatically positions it onto the input mesh. Locally - our method produces
masks that conform to the geometry of the reference image. Notably, our method
does not require any user input (in the form of scribbles or bounding boxes) to
achieve accurate localizations. Instead, our method predicts a localization
mask on the 3D mesh from scratch. To achieve this, we propose a modification to
the score distillation sampling technique which incorporates both the predicted
localization and the reference image, referred to as localization-modulated
image guidance. We demonstrate the effectiveness of our proposed technique on a
wide variety of meshes and images.

</details>


### [94] [F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding](https://arxiv.org/abs/2507.03836)
*Jianxin Sun,David Lenz,Hongfeng Yu,Tom Peterka*

Main category: cs.GR

TL;DR: F-Hash是一种新颖的基于特征的多分辨率Tesseract编码架构，用于加速时间变化体数据的隐式神经表示训练，提高收敛速度和渲染效率。


<details>
  <summary>Details</summary>
Motivation: 由于时间变化体数据的时空复杂性和大规模性，现有方法训练隐式神经表示（INR）耗时较长，亟需一种高效的编码方法加速训练。

Method: 提出F-Hash架构，采用多级无冲突哈希函数映射动态4D多分辨率嵌入网格，结合自适应光线行进算法优化渲染采样。

Result: F-Hash在多种时间变化体数据集上实现了最先进的训练收敛速度，并支持高效的特征跟踪和演化可视化。

Conclusion: F-Hash为时间变化体数据的隐式神经表示提供了一种高效、通用的编码和渲染解决方案。

Abstract: Interactive time-varying volume visualization is challenging due to its
complex spatiotemporal features and sheer size of the dataset. Recent works
transform the original discrete time-varying volumetric data into continuous
Implicit Neural Representations (INR) to address the issues of compression,
rendering, and super-resolution in both spatial and temporal domains. However,
training the INR takes a long time to converge, especially when handling
large-scale time-varying volumetric datasets. In this work, we proposed F-Hash,
a novel feature-based multi-resolution Tesseract encoding architecture to
greatly enhance the convergence speed compared with existing input encoding
methods for modeling time-varying volumetric data. The proposed design
incorporates multi-level collision-free hash functions that map dynamic 4D
multi-resolution embedding grids without bucket waste, achieving high encoding
capacity with compact encoding parameters. Our encoding method is agnostic to
time-varying feature detection methods, making it a unified encoding solution
for feature tracking and evolution visualization. Experiments show the F-Hash
achieves state-of-the-art convergence speed in training various time-varying
volumetric datasets for diverse features. We also proposed an adaptive ray
marching algorithm to optimize the sample streaming for faster rendering of the
time-varying neural representation.

</details>


### [95] [Attention-Guided Multi-Scale Local Reconstruction for Point Clouds via Masked Autoencoder Self-Supervised Learning](https://arxiv.org/abs/2507.04084)
*Xin Cao,Haoyu Wang,Yuzhu Mao,Xinda Liu,Linzhi Su,Kang Li*

Main category: cs.GR

TL;DR: 提出了一种名为PointAMaLR的自监督学习框架，通过注意力引导的多尺度局部重建来增强点云特征表示和处理精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型多关注高层编码器的重建任务，忽略了低层局部特征的有效利用，这些特征通常仅用于激活计算而非直接参与重建。

Method: PointAMaLR实现多层次局部区域的分层重建，低层专注于细粒度特征恢复，高层负责粗粒度特征重建，并结合局部注意力模块增强特征表示。

Result: 在ModelNet、ShapeNet、ScanObjectNN和S3DIS数据集上表现出优越的分类和重建性能，验证了其多尺度语义理解的实用性。

Conclusion: PointAMaLR不仅在理论上有效提升了特征表示能力，在实际场景中也展现出高度的适用性。

Abstract: Self-supervised learning has emerged as a prominent research direction in
point cloud processing. While existing models predominantly concentrate on
reconstruction tasks at higher encoder layers, they often neglect the effective
utilization of low-level local features, which are typically employed solely
for activation computations rather than directly contributing to reconstruction
tasks. To overcome this limitation, we introduce PointAMaLR, a novel
self-supervised learning framework that enhances feature representation and
processing accuracy through attention-guided multi-scale local reconstruction.
PointAMaLR implements hierarchical reconstruction across multiple local
regions, with lower layers focusing on fine-scale feature restoration while
upper layers address coarse-scale feature reconstruction, thereby enabling
complex inter-patch interactions. Furthermore, to augment feature
representation capabilities, we incorporate a Local Attention (LA) module in
the embedding layer to enhance semantic feature understanding. Comprehensive
experiments on benchmark datasets ModelNet and ShapeNet demonstrate
PointAMaLR's superior accuracy and quality in both classification and
reconstruction tasks. Moreover, when evaluated on the real-world dataset
ScanObjectNN and the 3D large scene segmentation dataset S3DIS, our model
achieves highly competitive performance metrics. These results not only
validate PointAMaLR's effectiveness in multi-scale semantic understanding but
also underscore its practical applicability in real-world scenarios.

</details>


### [96] [A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality](https://arxiv.org/abs/2507.04147)
*Shuo Xin,Haiyu Wang,Sai Qian Zhang*

Main category: cs.GR

TL;DR: A3FR框架通过并行化注视跟踪与渲染过程，降低了延迟，提升了VR实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 现有注视跟踪渲染虽能减少计算负担，但跟踪过程本身的计算开销可能导致延迟增加。

Method: 提出A3FR框架，并行化注视跟踪和基于3D高斯泼溅的渲染过程。

Result: A3FR降低端到端渲染延迟达2倍，同时保持视觉质量。

Conclusion: A3FR有效优化了VR渲染效率，为实时渲染提供了新方案。

Abstract: Virtual reality (VR) significantly transforms immersive digital interfaces,
greatly enhancing education, professional practices, and entertainment by
increasing user engagement and opening up new possibilities in various
industries. Among its numerous applications, image rendering is crucial.
Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high
computational demands, driven predominantly by user expectations for superior
visual quality. This results in notable processing delays for real-time image
rendering, which greatly affects the user experience. Additionally, VR devices
such as head-mounted displays (HMDs) are intricately linked to human visual
behavior, leveraging knowledge from perception and cognition to improve user
experience. These insights have spurred the development of foveated rendering,
a technique that dynamically adjusts rendering resolution based on the user's
gaze direction. The resultant solution, known as gaze-tracked foveated
rendering, significantly reduces the computational burden of the rendering
process.
  Although gaze-tracked foveated rendering can reduce rendering costs, the
computational overhead of the gaze tracking process itself can sometimes
outweigh the rendering savings, leading to increased processing latency. To
address this issue, we propose an efficient rendering framework
called~\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated
rendering via the parallelization of gaze tracking and foveated rendering
processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a
state-of-the-art neural rendering technique. Evaluation results demonstrate
that A3FR can reduce end-to-end rendering latency by up to $2\times$ while
maintaining visual quality.

</details>


### [97] [Neuralocks: Real-Time Dynamic Neural Hair Simulation](https://arxiv.org/abs/2507.05191)
*Gene Wei-Chin Lin,Egor Larionov,Hsiao-yu Chen,Doug Roble,Tuur Stuyck*

Main category: cs.GR

TL;DR: 提出了一种新型自监督神经方法，可高效稳定地模拟动态头发行为，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实时头发模拟对虚拟角色真实性至关重要，但现有神经方法无法捕捉动态行为。

Method: 采用紧凑高效的神经网络，自监督训练，无需人工干预或艺术数据。

Result: 方法在多种发型上验证有效，显示实际应用潜力。

Conclusion: 新方法突破局限，为头发模拟提供高效且通用的解决方案。

Abstract: Real-time hair simulation is a vital component in creating believable virtual
avatars, as it provides a sense of immersion and authenticity. The dynamic
behavior of hair, such as bouncing or swaying in response to character
movements like jumping or walking, plays a significant role in enhancing the
overall realism and engagement of virtual experiences. Current methods for
simulating hair have been constrained by two primary approaches: highly
optimized physics-based systems and neural methods. However, state-of-the-art
neural techniques have been limited to quasi-static solutions, failing to
capture the dynamic behavior of hair. This paper introduces a novel neural
method that breaks through these limitations, achieving efficient and stable
dynamic hair simulation while outperforming existing approaches. We propose a
fully self-supervised method which can be trained without any manual
intervention or artist generated training data allowing the method to be
integrated with hair reconstruction methods to enable automatic end-to-end
methods for avatar reconstruction. Our approach harnesses the power of compact,
memory-efficient neural networks to simulate hair at the strand level, allowing
for the simulation of diverse hairstyles without excessive computational
resources or memory requirements. We validate the effectiveness of our method
through a variety of hairstyle examples, showcasing its potential for
real-world applications.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [98] [A Survey on Integrating Quantum Computers into High Performance Computing Systems](https://arxiv.org/abs/2507.03540)
*Philip Döbler,Manpreet Singh Jattana*

Main category: cs.ET

TL;DR: 该论文对量子计算机与经典高性能计算系统的集成进行了结构化文献综述，分析了硬件架构和软件栈，强调未来需标准化接口和方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算机虽无法替代经典计算机，但能与经典系统互补形成混合系统，研究其与高性能计算的集成为当前热点。

Method: 通过系统性文献检索和手动评估107篇出版物，分为七类描述各领域现状，并进行定量分析和硬件、软件栈研究。

Result: 研究发现多种支持混合系统的工具，并指出需对接口和方法进行标准化以促进协同发展。

Conclusion: 论文总结了量子与经典系统集成的现状，强调未来标准化的重要性，为相关研究提供参考。

Abstract: Quantum computers use quantum mechanical phenomena to perform conventionally
intractable calculations for specific problems. Despite being universal
machines, quantum computers are not expected to replace classical computers,
but rather, to complement them and form hybrid systems. This makes integrating
quantum computers into high performance computing (HPC) systems an increasingly
relevant topic. We present a structured literature review on the integration
aspect. We methodologically search literature databases and manually evaluate
107 publications. These publications are divided into seven categories that
describe the state of the art in each category. After a brief quantitative
analysis of the literature, this survey deals with the hardware architecture of
hybrid quantum-classical systems, as well as the software stack. We observe the
development of a wide range of tools enabling hybrid systems and emphasize the
need for future standardization of interfaces and methods to foster synergy.

</details>


### [99] [2024 NSF CSSI-Cybertraining-SCIPE PI Meeting August 12 to 13, 2024, Charlotte, NC](https://arxiv.org/abs/2507.04171)
*Abani Patra,Mary Thomas,Elias Bou-Harb,Jeffrey Carver,Yuebin Guo,Ratnesh Kumar,Julien Langou,Guoyu Lu,Vivak Patel,Marianna Safronova,Isla Simpson,Dhruva Chakravorty,Jane Combs,Hantao Cui,Sushil Prasad,Adnan Rajib,Susan Rathbun,Erik Saule,Isla Simpson,Alan Sussman,Shaowen Wang,Sarina Zhe Zhang,Ben Brown,Varun Chandola,Daniel Crawford,Ian Foster,Dave Hart,Mike Heroux,Mary Ann Leung,Benjamin Lynch,Dan Negrut,D. K. Panda,Manish Parashar,Melissa Kline Struhl,George K. Thiruvathukal*

Main category: cs.ET

TL;DR: 总结NSF、OAC CSSI、CyberTraining等项目的第二次年度PI会议内容，包括参与者、活动和主要发现。


<details>
  <summary>Details</summary>
Motivation: 记录会议结构和社区对网络基础设施的当前观点，展示社区在推动科学中的作用。

Method: 通过主题演讲、小组讨论、分组会议和海报展示，促进PI、NSF工作人员和专家之间的互动。

Result: 286名与会者代表292个奖项，展示了250多张海报，展示了社区对网络基础设施的积极态度和进展。

Conclusion: 会议展示了活跃的社区通过CI推动科学，AI驱动的研究与HPC和数据工具相辅相成，劳动力发展与CSSI社区目标一致。

Abstract: The second annual NSF, OAC CSSI, CyberTraining and related programs PI
meeting was held August 12 to 13 in Charlotte, NC, with participation from PIs
or representatives of all major awards. Keynotes, panels, breakouts, and poster
sessions allowed PIs to engage with each other, NSF staff, and invited experts.
The 286 attendees represented 292 awards across CSSI, CyberTraining, OAC Core,
CIP, SCIPE CDSE, and related programs, and presented over 250 posters. This
report documents the meetings structure, findings, and recommendations,
offering a snapshot of current community perspectives on cyberinfrastructure. A
key takeaway is a vibrant, engaged community advancing science through CI.
AI-driven research modalities complement established HPC and data centric
tools. Workforce development efforts align well with the CSSI community.

</details>


### [100] [Enabling Security on the Edge: A CHERI Compartmentalized Network Stack](https://arxiv.org/abs/2507.04818)
*Donato Ferraro,Andrea Bastoni,Alexander Zuepke,Andrea Marongiu*

Main category: cs.ET

TL;DR: 探讨CHERI在嵌入式系统中隔离网络组件的潜力，提升安全性并保持性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统的安全性不足，CHERI通过硬件级别的隔离能减少攻击面。

Method: 使用CheriBSD系统在Arm Morello平台上隔离网络组件。

Result: CHERI能有效增强安全性且不影响性能。

Conclusion: CHERI在嵌入式环境中有广泛应用潜力。

Abstract: The widespread deployment of embedded systems in critical infrastructures,
interconnected edge devices like autonomous drones, and smart industrial
systems requires robust security measures. Compromised systems increase the
risks of operational failures, data breaches, and -- in safety-critical
environments -- potential physical harm to people. Despite these risks, current
security measures are often insufficient to fully address the attack surfaces
of embedded devices. CHERI provides strong security from the hardware level by
enabling fine-grained compartmentalization and memory protection, which can
reduce the attack surface and improve the reliability of such devices. In this
work, we explore the potential of CHERI to compartmentalize one of the most
critical and targeted components of interconnected systems: their network
stack. Our case study examines the trade-offs of isolating applications, TCP/IP
libraries, and network drivers on a CheriBSD system deployed on the Arm Morello
platform. Our results suggest that CHERI has the potential to enhance security
while maintaining performance in embedded-like environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [101] [ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration](https://arxiv.org/abs/2507.02871)
*Kia Silverbrook*

Main category: cs.DC

TL;DR: ZettaLith是一种创新的计算架构，旨在通过超过1000倍的AI推理成本和功耗降低，突破现有GPU系统的效率限制。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的计算成本和功耗已成为广泛部署和扩展的主要挑战，现有硬件存在根本效率限制。

Method: ZettaLith通过放弃通用GPU应用，结合多项协同设计的架构创新，利用成熟的数字电子技术，显著提升效率和性能。

Result: 预计到2027年，单个ZettaLith机架可能实现1.507 zettaFLOPS，推理性能提升1047倍，能效提升1490倍，成本效益提升2325倍。

Conclusion: ZettaLith为AI推理提供了高效的专用架构，适用于从桌面到移动设备的多种规模，但其不适用于AI训练。

Abstract: The high computational cost and power consumption of current and anticipated
AI systems present a major challenge for widespread deployment and further
scaling. Current hardware approaches face fundamental efficiency limits. This
paper introduces ZettaLith, a scalable computing architecture designed to
reduce the cost and power of AI inference by over 1,000x compared to current
GPU-based systems. Based on architectural analysis and technology projections,
a single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 -
representing a theoretical 1,047x improvement in inference performance, 1,490x
better power efficiency, and could be 2,325x more cost-effective than current
leading GPU racks for FP4 transformer inference. The ZettaLith architecture
achieves these gains by abandoning general purpose GPU applications, and via
the multiplicative effect of numerous co-designed architectural innovations
using established digital electronic technologies, as detailed in this paper.
ZettaLith's core architectural principles scale down efficiently to exaFLOPS
desktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x
advantage. ZettaLith presents a simpler system architecture compared to the
complex hierarchy of current GPU clusters. ZettaLith is optimized exclusively
for AI inference and is not applicable for AI training.

</details>


### [102] [Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications](https://arxiv.org/abs/2507.03114)
*Seonho Lee,Jihwan Oh,Junkyum Kim,Seokjin Go,Jongse Park,Divya Mahajan*

Main category: cs.DC

TL;DR: 本文深入分析了GPU加速系统中计算与通信重叠策略的效果，揭示了资源竞争和功耗增加的问题，并探讨了硬件特性对分布式训练的影响。


<details>
  <summary>Details</summary>
Motivation: 分布式训练中，计算与通信的重叠策略常用于缓解通信瓶颈，但当前共识认为应始终积极重叠，本研究质疑并评估其实际效果。

Method: 通过系统性地评估最新GPU硬件特性（如数值精度、专用核心和功耗限制），在不同场景下进行综合实验。

Result: 重叠策略平均导致计算速度下降18.9%，但比顺序执行快10.2%；专用数据路径和优化数值精度可缓解部分降速。

Conclusion: 重叠策略需平衡资源争用与功耗，优化能源效率和训练吞吐量。

Abstract: This paper provides an in-depth characterization of GPU-accelerated systems,
to understand the interplay between overlapping computation and communication
which is commonly employed in distributed training settings. Due to the large
size of models, distributing them across multiple devices is required.
Overlapping strategies, which enable concurrent computation and communication,
are critical for mitigating communication bottlenecks and maximizing GPU
utilization. However, the current consensus is that we should always and
aggressively overlap compute and communication to mitigate the overhead of
distribution. By systematically evaluating state-of-the-art GPUs, this study
investigates the impact of hardware features such as numeric precision,
specialized cores, and power capping on distributed training workloads.
Comprehensive experiments and studies showcase the effects of overlapping
strategies on performance and power consumption across varying scenarios. We
observe that overlapping computation and communication can result in an average
computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This
slowdown is in comparison to the scenario when no communication was happening
with the compute. We consider this an ideal execution scenario, where the
communication in parallel has not impact on the compute time. However,
performing computation and communication sequentially is, on average, 10.2%
slower than overlapped execution, with a maximum slowdown of 26.6%. We further
observe, while specialized datapath and optimized numeric precision mitigate
certain slowdowns, overlapping execution can lead to resource contention and
also increase power consumption under specific configurations. The analysis
also uncovers trade-offs introduced by power and frequency capping, emphasizing
the importance of balanced strategies to optimize energy efficiency and
training throughput.

</details>


### [103] [Symbiosis: Multi-Adapter Inference and Fine-Tuning](https://arxiv.org/abs/2507.03220)
*Saransh Gupta,Umesh Deshpande,Travis Janssen,Swami Sundararaman*

Main category: cs.DC

TL;DR: Symbiosis框架通过共享基础模型层和拆分执行技术，解决了多适配器微调和推理中的资源浪费和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有框架在多适配器微调和推理中存在GPU资源浪费、灵活性不足和隐私保护问题。

Method: 采用共享基础模型层和拆分执行技术，将客户特定适配器与基础模型解耦。

Result: 在Llama2-13B上，Symbiosis可同时微调4倍于基线的适配器。

Conclusion: Symbiosis显著提升了资源利用率和灵活性，支持多适配器混合使用和隐私保护。

Abstract: Parameter-efficient fine-tuning (PEFT) allows model builders to capture the
task specific parameters into adapters, which are a fraction of the size of the
original base model. Popularity of PEFT technique for fine-tuning has led to
creation of a large number of adapters for popular Large Language Models
(LLMs). However, existing frameworks fall short in supporting inference or
fine-tuning with multiple adapters in the following ways. 1) For fine-tuning,
each job needs to deploy its dedicated base model instance, which results in
excessive GPU memory consumption and poor GPU utilization. 2) While popular
inference platforms can serve multiple PEFT adapters, they do not allow
independent resource management or mixing of different PEFT methods. 3) They
cannot share resources (such as base model instance) between inference and
fine-tuning jobs. 4) They do not provide privacy to users who may not wish to
expose their fine-tuned parameters to service providers. In Symbiosis, we
address the above problems by enabling as-a-service deployment of base model.
The base model layers can be shared across multiple inference or fine-tuning
processes. Our split-execution technique decouples the execution of
client-specific adapters and layers from the frozen base model layers offering
them flexibility to manage their resources, to select their fine-tuning method,
to achieve their performance goals. Our approach is transparent to models and
works out-of-the-box for most models in the transformers library. Our
evaluation on Llama2-13B shows the compared to baseline, Symbiosis can
fine-tune 4X more adapters on the same set of GPUs in the same amount of time.

</details>


### [104] [Analysis and Optimized CXL-Attached Memory Allocation for Long-Context LLM Fine-Tuning](https://arxiv.org/abs/2507.03305)
*Yong-Cheng Liaw,Shuo-Han Chen*

Main category: cs.DC

TL;DR: 研究探讨了利用CXL内存扩展CPU内存以支持更大模型和更长上下文微调的有效性，提出了CXL感知分配策略以减少性能下降，并展示了多AIC减少带宽争用的优势。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，CPU内存成为瓶颈，研究旨在通过CXL内存扩展解决这一问题。

Method: 通过基准测试量化CXL内存与CPU、GPU间数据传输的性能开销，提出CXL感知分配策略。

Result: 优化后的方案支持高效长上下文LLM微调，CXL成为释放CPU卸载潜力的途径。

Conclusion: CXL内存扩展是CPU卸载的有效解决方案，优化后显著提升性能。

Abstract: The growing prevalence of Large Language Models (LLMs) and their substantial
memory requirements have prompted renewed interest in CPU offloading as a
method to compensate for limited GPU memory. In particular, when CPU memory is
leveraged to temporarily store intermediate states of LLMs, CPU memory becomes
a new bottleneck and soon reaches the capacity limitation of commodity CPUs. In
this work, we investigate the effectiveness of Compute Express Link (CXL)
add-in card (AIC) memory as an extension to CPU memory, enabling larger model
sizes and longer context lengths during fine-tuning. Through extensive
benchmarking, this study quantifies the performance overhead introduced by
transferring data between CXL memory, CPU, and GPUs, focusing on how
concurrency and data volume influence bandwidth utilization and latency. This
study also compares CPUbased optimizer steps when model parameters, gradients,
and optimizer states reside in local memory versus CXL memory, revealing that
naive adoption of CXL often degrades performance during the optimizer phase. To
overcome these challenges, this study proposes a CXL-aware allocation to
strategically partition CPU offloading workloads across both local and CXL
memory. This study further demonstrates that employing multiple AICs
significantly reduces bandwidth contention, thus improving scalability.
Experimental results show that these optimizations enable efficient
long-context LLM fine-tuning, underscoring CXL as a promising avenue for
unlocking the full potential of CPU offloading in long-context LLM fine-tuning.

</details>


### [105] [A Distributed Consensus Algorithm for Autonomous Vehicles Deciding Entering Orders on Intesections without Traffic Signals](https://arxiv.org/abs/2507.03486)
*Younjeong Lee,Young Yoon*

Main category: cs.DC

TL;DR: 提出了一种用于混合交通中无人车在无信号交叉路口快速确定通行优先级的分布式共识算法。


<details>
  <summary>Details</summary>
Motivation: 解决无人车与人类驾驶车辆在无信号交叉路口共存时的优先级确定问题，提升交通效率。

Method: 基于Raft的投票分布式共识算法，结合计算机视觉和最小共识法定数，确保安全与活性。

Result: 实验表明，算法在典型无信号四路交叉口平均30-40毫秒内达成共识，适用于混合交通。

Conclusion: 该算法能高效解决混合交通中的优先级问题，提升无信号交叉路口的交通流畅性。

Abstract: We propose a methodology for connected autonomous vehicles (CAVs) to
determine their passing priority at unsignalized intersections where they
coexist with human-driven vehicles (HVs). Assuming that CAVs can perceive the
entry order of surrounding vehicles using computer vision technology and are
capable of avoiding collisions, we introduce a voting-based distributed
consensus algorithm inspired by Raft to resolve tie-breaking among
simultaneously arriving CAVs. The algorithm is structured around the candidate
and leader election processes and incorporates a minimal consensus quorum to
ensure both safety and liveness among CAVs under typical asynchronous
communication conditions. Assuming CAVs to be SAE (Society of Automotive
Engineers) Level-4 or higher autonomous vehicles, we implemented the proposed
distributed consensus algorithm using gRPC. By adjusting variables such as the
CAV-to-HV ratio, intersection scale, and the processing time of computer vision
modules, we demonstrated that stable consensus can be achieved even under
mixed-traffic conditions involving HVs without adequate functionalities to
interact with CAVs. Experimental results show that the proposed algorithm
reached consensus at a typical unsignalized four-way, two-lane intersection in
approximately 30-40 ms on average. A secondary vision-based system is employed
to complete the crossing priorities based on the recognized lexicographical
order of the license plate numbers in case the consensus procedure times out on
an unreliable vehicle-to-vehicle communication network. The significance of
this study lies in its ability to improve traffic flow at unsignalized
intersections by enabling rapid determination of passing priority through
distributed consensus even under mixed traffic with faulty vehicles.

</details>


### [106] [On Optimizing Resource Utilization in Distributed Connected Components](https://arxiv.org/abs/2507.03695)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: 提出两种分布式连通分量（CC）新算法SiskinCC和RobinCC，优化内存和网络带宽利用，性能提升58.5倍。


<details>
  <summary>Details</summary>
Motivation: 连通分量（CC）是图计算中的核心问题，现有分布式算法在内存和网络带宽利用上有优化空间。

Method: 基于Jayanti-Tarjan不相交集并算法设计SiskinCC和RobinCC，分别优化内存共享和网络带宽利用。

Result: 在5000亿边和117亿顶点的图上测试，2048核CPU上最高提速58.5倍。

Conclusion: 新型算法显著优化了分布式CC计算的效率，适用于真实及合成大规模图。

Abstract: Connected Components (CC) is a core graph problem with numerous applications.
This paper investigates accelerating distributed CC by optimizing memory and
network bandwidth utilization. We present two novel distributed CC algorithms,
SiskinCC and RobinCC, which are built upon the Jayanti-Tarjan disjoint set
union algorithm. To optimize memory utilization, SiskinCC and RobinCC are
designed to facilitate efficient access to a shared array for all cores running
in a machine. This allows execution of faster algorithms with larger memory
bounds. SiskinCC leverages the continuous inter-machine communication during
the computation phase to reduce the final communication overhead and RobinCC
leverages the structural properties of real-world graphs to optimize network
bandwidth utilization. Our evaluation against state-of-the-art CC algorithms,
using real-world and synthetic graphs with up to 500 billion edges and 11.7
billion vertices, and on up to 2048 CPU cores, demonstrates that SiskinCC and
RobinCC achieve up to 58.5 times speedup.

</details>


### [107] [On Fault Tolerance of Data Storage Systems: A Holistic Perspective](https://arxiv.org/abs/2507.03849)
*Mai Zheng,Duo Zhang,Ahmed Dajani*

Main category: cs.DC

TL;DR: 现代数据存储系统架构复杂，硬件与软件层相互作用可能导致潜在错误，影响数据完整性和系统恢复。文章介绍了其典型架构、组件及跨层级错误检测和容错技术，并总结了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 数据存储系统是数字社会的基础，其故障容忍度对数据安全和系统稳定至关重要。

Method: 从整体视角介绍现代数据存储系统的架构和组件，并讨论跨层级的错误检测与容错技术。

Result: 总结了现有技术对系统恢复和数据完整性的影响。

Conclusion: 提出了未来研究和解决存储系统问题的开放挑战。

Abstract: Data storage systems serve as the foundation of digital society. The enormous
data generated by people on a daily basis make the fault tolerance of data
storage systems increasingly important. Unfortunately, modern storage systems
consist of complicated hardware and software layers interacting with each
other, which may contain latent bugs that elude extensive testing and lead to
data corruption, system downtime, or even unrecoverable data loss in practice.
In this chapter, we take a holistic view to introduce the typical architecture
and major components of modern data storage systems (e.g., solid state drives,
persistent memories, local file systems, and distributed storage management at
scale). Next, we discuss a few representative bug detection and fault tolerance
techniques across layers with a focus on issues that affect system recovery and
data integrity. Finally, we conclude with open challenges and future work.

</details>


### [108] [FedFog: Resource-Aware Federated Learning in Edge and Fog Networks](https://arxiv.org/abs/2507.03952)
*Somayeh Sobati-M*

Main category: cs.DC

TL;DR: FedFog是一个模拟框架，将服务器无与联合学习结合，优化边缘-雾计算环境下的模型收敛、延迟和能效。


<details>
  <summary>Details</summary>
Motivation: 边缘和雾计算日益重要，但现有工具未能有效模拟服务器无与联合学习的集成。

Method: 扩展FogFaaS环境，加入自适应FL调度器、隐私保护数据流和资源感知编排，模拟动态物联网场景。

Result: FedFog相比传统方法加速模型收敛、降低延迟、提高能效。

Conclusion: FedFog是研究可扩展智能边缘系统的有力工具。

Abstract: As edge and fog computing become central to modern distributed systems,
there's growing interest in combining serverless architectures with
privacy-preserving machine learning techniques like federated learning (FL).
However, current simulation tools fail to capture this integration effectively.
In this paper, we introduce FedFog, a simulation framework that extends the
FogFaaS environment to support FL-aware serverless execution across edge-fog
infrastructures. FedFog incorporates an adaptive FL scheduler,
privacy-respecting data flow, and resource-aware orchestration to emulate
realistic, dynamic conditions in IoT-driven scenarios. Through extensive
simulations on benchmark datasets, we demonstrate that FedFog accelerates model
convergence, reduces latency, and improves energy efficiency compared to
conventional FL or FaaS setups-making it a valuable tool for researchers
exploring scalable, intelligent edge systems.

</details>


### [109] [One-Bit Model Aggregation for Differentially Private and Byzantine-Robust Personalized Federated Learning](https://arxiv.org/abs/2507.03973)
*Muhang Lan,Song Xiao,Wenyi Zhang*

Main category: cs.DC

TL;DR: 本文提出了一个名为PRoBit+的个性化联邦学习框架，通过模型正则化和动态调整参数更新步长，有效解决了通信开销、拜占庭攻击和隐私泄露等问题，并提供了理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习系统规模的扩大，其固有性能限制（如通信开销、拜占庭攻击和隐私泄露）日益突出。作者旨在通过提出PRoBit+算法来同时解决这些挑战。

Method: PRoBit+采用一位随机量化和最大似然估计进行参数聚合，并动态调整参数更新步长，以提高在低通信开销和异构数据分布下深度神经网络的训练稳定性。

Result: 理论分析证明了PRoBit+的拜占庭鲁棒性、(ε,0)-差分隐私性以及收敛上界。实验表明PRoBit+在拜占庭攻击下优于现有方法，隐私保护引起的性能下降最小。

Conclusion: PRoBit+在通信效率、安全性和隐私保护之间实现了平衡，并通过M的增加逐步消除传输误差和隐私保护带来的性能损失。

Abstract: As the scale of federated learning (FL) systems expands, their inherent
performance limitations like communication overhead, Byzantine vulnerability,
and privacy leakage have become increasingly critical. This paper considers a
personalized FL framework based on model regularization, and proposes a model
aggregation algorithm named PRoBit+ to concurrently overcome these limitations.
PRoBit+ employs one-bit stochastic quantization and maximum likelihood
estimation for parameter aggregation, and dynamically adjusts the step size of
parameter updates, improving training stability of deep neural networks under
low communication overhead and heterogeneous data distributions. PRoBit+'s
statistical analysis is then conducted and its Byzantine robustness is proved.
The $(\epsilon,0)$-differential privacy and a convergence upper bound of the
PRoBit+ based FL are also theoretically established in heterogeneous contexts.
The analysis illustrates the trade-off among transmission accuracy, security
guarantees, and convergence rates, and also indicates that the performance
degradation caused by transmission errors and privacy protection can be
progressively eliminated at a rate of $\mathcal{O}(1/M)$ as the number of
uploading clients $M$ increases. Comprehensive numerical experiments are
conducted to assess PRoBit+ in comparison to benchmark methods across different
Byzantine attacks and varying proportions of malicious clients. The
experimental results demonstrate that PRoBit+ exhibits improved Byzantine
robustness over existing bit-based transmission schemes, minimal performance
degradation related to privacy protection, and nearly identical performance to
full-precision FedAvg in a secure environment.

</details>


### [110] [Gathering Teams of Bounded Memory Agents on a Line](https://arxiv.org/abs/2507.04172)
*Younan Gao,Andrzej Pelc*

Main category: cs.DC

TL;DR: 研究了多个移动代理在无限线上的聚集问题，其可行性和时间复杂度取决于团队的大小。对于同等大小的团队，有向线在团队大小大于1时可行，时间复杂度为O(D)；无向线在团队大小为2时复杂度为Θ(DlogL)，大于3时为Θ(D)。对大小不同的团队，聚集总是可行且时间复杂度为O(D)。


<details>
  <summary>Details</summary>
Motivation: 研究多代理在同步环境中的聚集问题，特别是团队大小对聚集可行性和时间复杂度的直接影响。

Method: 使用确定性自动机模型代理行为，考虑团队大小和线类型（有向/无向），分析聚集的可行性及时间复杂度。

Result: 对于同类大小团队，有向线在x>1可行，无向线在x≥2可行，复杂度分别为O(D)和{Θ(DlogL), Θ(D)}；异类大小团队则总是可行且复杂度为O(D)。

Conclusion: 团队大小和线类型是聚集问题可行性和复杂度的关键因素，异类团队总能高效解决。

Abstract: Several mobile agents, modelled as deterministic automata, navigate in an
infinite line in synchronous rounds. All agents start in the same round. In
each round, an agent can move to one of the two neighboring nodes, or stay
idle. Agents have distinct labels which are integers from the set $\{1,\dots,
L\}$. They start in teams, and all agents in a team have the same starting
node. The adversary decides the compositions of teams, and their starting
nodes. Whenever an agent enters a node, it sees the entry port number and the
states of all collocated agents; this information forms the input of the agent
on the basis of which it transits to the next state and decides the current
action. The aim is for all agents to gather at the same node and stop.
Gathering is feasible, if this task can be accomplished for any decisions of
the adversary, and its time is the worst-case number of rounds from the start
till gathering.
  We consider the feasibility and time complexity of gathering teams of agents,
and give a complete solution of this problem. It turns out that both
feasibility and complexity of gathering depend on the sizes of teams. We first
concentrate on the case when all teams have the same size $x$. For the oriented
line, gathering is impossible if $x=1$, and it can be accomplished in time
$O(D)$, for $x>1$, where $D$ is the distance between the starting nodes of the
most distant teams. This complexity is of course optimal. For the unoriented
line, the situation is different. For $x=1$, gathering is also impossible, but
for $x=2$, the optimal time of gathering is $\Theta(D\log L)$, and for $x\geq
3$, the optimal time of gathering is $\Theta(D)$. In the case when there are
teams of different sizes, we show that gathering is always possible in time
$O(D)$, even for the unoriented line. This complexity is of course optimal.

</details>


### [111] [Static Analysis for Detecting Transaction Conflicts in Ethereum Smart Contracts](https://arxiv.org/abs/2507.04357)
*Zareh Chahoki Atefeh,Roveri Marco*

Main category: cs.DC

TL;DR: 本文提出了一种静态分析方法，用于检测以太坊智能合约中的潜在交易冲突，通过分析状态变量的访问模式，有效识别冲突，提升区块链的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 以太坊智能合约在处理并发交易时需顺序执行以保证正确性，但限制了多核架构的利用，降低了吞吐量。现有动态冲突检测方法开销较大，而静态预测方法的潜力尚未被充分研究。

Method: 本文提出了一种静态分析方法，通过分析Solidity合约中的状态变量访问模式，识别读写、写写和函数调用冲突，并实现了一个解析工具。

Result: 在真实以太坊智能合约数据集上的评估表明，该方法在识别潜在冲突方面具有高精度。

Conclusion: 该工具支持设计更高效的交易调度策略，减少运行时故障，提升吞吐量，从而推动区块链的可扩展性。

Abstract: Ethereum smart contracts operate in a concurrent environment where multiple
transactions can be submitted simultaneously. However, the Ethereum Virtual
Machine (EVM) enforces sequential execution of transactions within each block
to prevent conflicts arising from concurrent access to the same state
variables. Although this approach guarantees correct behavior, it limits the
ability of validators to leverage multi-core architectures for faster
transaction processing, thus restricting throughput. Existing solutions
introduce concurrency by allowing simultaneous transaction execution combined
with runtime conflict detection and rollback mechanisms to maintain
correctness. However, these methods incur significant overhead due to
continuous conflict tracking and transaction reversion. Recently, alternative
approaches have emerged that aim to predict conflicts statically, before
execution, by analyzing smart contract code for potential transaction
interactions. Despite their promise, there is a lack of comprehensive studies
that examine static conflict detection and its broader implications in specific
smart contracts. This paper fills this important gap by proposing a novel
static analysis method to detect potential transaction conflicts in Ethereum
smart contracts. Our method identifies read-write, write-write, and function
call conflicts between transaction pairs by analyzing state variable access
patterns in Solidity contracts. We implement a tool that parses contract code
and performs conflict detection. Evaluation on a dataset of real-world Ethereum
smart contracts demonstrates that our approach achieves high precision in
identifying potential conflicts. By enabling proactive conflict detection, our
tool supports further design of transaction scheduling strategies that reduce
runtime failures, enhance validator throughput, and contribute to blockchain
scalability.

</details>


### [112] [Skipper: Maximal Matching with a Single Pass over Edges](https://arxiv.org/abs/2507.04420)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: Skipper是一种增量异步最大匹配算法，能单次处理每条边且跳过大量边处理，同时最小化内存使用，实现高达47.1倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 现有并行最大匹配算法需多次处理边且占用额外内存，Skipper旨在解决这些问题。

Method: Skipper采用增量异步方式，单次处理边并跳过大部分边，仅需每个顶点1字节内存。

Result: 在1610亿边的图中，Skipper仅处理1.2%的边，实现47.1倍加速，匹配质量达88.6%。

Conclusion: Skipper在性能和输出质量上显著优于现有算法，适用于大规模图处理。

Abstract: Maximal Matching (MM) is a fundamental graph problem with diverse
applications. However, state-of-the-art parallel MM algorithms are limited by
their need to process graph edges repeatedly over multiple iterations.
Furthermore, optimized algorithms often require additional memory for graph
contraction or edge filtering. In this paper, we introduce Skipper, an
incremental asynchronous MM algorithm that (i) processes each edge
deterministically and only once, (ii) skips a large fraction of edges during
processing, and (iii) minimizes memory space utilization. Notably, Skipper
requires (a) a single pass over the edges, and (b) only a single byte of memory
space per vertex. Our evaluation of Skipper, using both real-world and
synthetic graphs with up to 161 billion edges, and across three different
computer architectures, shows that Skipper processes only 1.2% of the edges and
delivers a 47.1 times average speedup (geometric mean). Moreover, Skipper's
output quality is highly competitive, with an average size of 88.6% relative to
the output of the Lim-Chung algorithm as a state-of-the-art MM algorithm with
the largest output size.

</details>


### [113] [Agentic Distributed Computing](https://arxiv.org/abs/2507.04459)
*Ajay D. Kshemkalyani,Manish Kumar,Anisur Rahaman Molla,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文研究了分布式计算中的代理模型，提出了关于领导选举和最小生成树任务的确定性算法，优化了时间和内存复杂度，首次探讨了代理数量不超过节点数的情况。


<details>
  <summary>Details</summary>
Motivation: 扩展消息传递模型，引入可移动计算设备（代理），研究分布式计算中领导选举和最小生成树任务的新范式，填补了k≤n情况的研究空白。

Method: 提出两种领导选举的确定性算法（k<n和k=n），并基于此开发最小生成树算法，优化时间和内存复杂度。

Result: 针对k≤n的情况，提供了高效的领导选举和最小生成树算法，为代理模型下的分布式任务提供了新的解决方案。

Conclusion: 代理模型在分布式计算中具有潜力，本文提出的算法为研究代理数量受限的情况提供了基础，未来可进一步探索更复杂的任务和场景。

Abstract: The most celebrated and extensively studied model of distributed computing is
the {\em message-passing model,} in which each vertex/node of the (distributed
network) graph corresponds to a static computational device that communicates
with other devices through passing messages. In this paper, we consider the
{\em agentic model} of distributed computing which extends the message-passing
model in a new direction. In the agentic model, computational devices are
modeled as relocatable or mobile computational devices (called agents in this
paper), i.e., each vertex/node of the graph serves as a container for the
devices, and hence communicating with another device requires relocating to the
same node. We study two fundamental graph level tasks, leader election, and
minimum spanning tree, in the agentic model, which will enhance our
understanding of distributed computation across paradigms. The objective is to
minimize both time and memory complexities. Following the literature, we
consider the synchronous setting in which each agent performs its operations
synchronously with others, and hence the time complexity can be measured in
rounds. In this paper, we present two deterministic algorithms for leader
election: one for the case of $k<n$ and another for the case of $k=n$,
minimizing both time and memory complexities, where $k$ and $n$, respectively,
are the number of agents and number of nodes of the graph. Using these leader
election results, we develop deterministic algorithms for agents to construct a
minimum spanning tree of the graph, minimizing both time and memory
complexities. To the best of our knowledge, this is the first study of
distributed graph level tasks in the agentic model with $k\leq n$. Previous
studies only considered the case of $k=n$.

</details>


### [114] [RAPTOR: Practical Numerical Profiling of Scientific Applications](https://arxiv.org/abs/2507.04647)
*Faveo Hoerold,Ivan R. Ivanov,Akash Dhruv,William S. Moses,Anshu Dubey,Mohamed Wahib,Jens Domke*

Main category: cs.DC

TL;DR: 研究者提出了名为RAPTOR的数值分析工具，帮助科学家在高性能计算中评估低精度操作的可行性。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构中低精度单元的普及给科学家带来了负担，需要工具来评估代码中哪些部分可以降低精度而不影响结果。

Method: 使用LLVM透明地替换高精度计算为低精度操作或模拟用户定义的精度，为开发者提供易用的工具来分析数值需求和稳定性。

Result: RAPTOR在四个真实的多物理场Flash-X应用中证明了其有效性。

Conclusion: RAPTOR为科学家提供了评估和优化数值精度的新方法，解决了低精度架构下的计算挑战。

Abstract: The proliferation of low-precision units in modern high-performance
architectures increasingly burdens domain scientists. Historically, the choice
in HPC was easy: can we get away with 32 bit floating-point operations and
lower bandwidth requirements, or is FP64 necessary? Driven by Artificial
Intelligence, vendors introduced novel low-precision units for vector and
tensor operations, and FP64 capabilities stagnate or are reduced. This is
forcing scientists to re-evaluate their codes, but a trivial search-and-replace
approach to go from FP64 to FP16 will not suffice. We introduce RAPTOR: a
numerical profiling tool to guide scientists in their search for code regions
where precision lowering is feasible. Using LLVM, we transparently replace
high-precision computations using low-precision units, or emulate a
user-defined precision. RAPTOR is a novel, feature-rich approach -- with focus
on ease of use -- to change, profile, and reason about numerical requirements
and instabilities, which we demonstrate with four real-world multi-physics
Flash-X applications.

</details>


### [115] [Communication Round and Computation Efficient Exclusive Prefix-Sums Algorithms (for MPI_Exscan)](https://arxiv.org/abs/2507.04785)
*Jesper Larsson Träff*

Main category: cs.DC

TL;DR: 论文提出了一种新的并行扫描算法，能够在较少的通信轮次中计算排他前缀和，并通过实验验证了其性能优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 传统排他扫描算法通信轮次多或计算复杂度高，作者旨在提出更高效的算法。

Method: 设计了一种简单的新算法，减少了通信轮次和操作符应用次数。

Result: 新算法在36节点集群上优于MPI原生实现和传统算法。

Conclusion: 新算法有潜力改进标准实现，但仅适用于小输入向量场景。

Abstract: Parallel scan primitives compute element-wise inclusive or exclusive prefix
sums of input vectors contributed by $p$ consecutively ranked processors under
an associative, binary operator $\oplus$. In message-passing systems with
bounded, one-ported communication capabilities, at least $\lceil\log_2 p\rceil$
or $\lceil\log_2 (p-1)\rceil$ communication rounds are required to perform the
scans. While there are well-known, simple algorithms for the inclusive scan
that solve the problem in $\lceil\log_2 p\rceil$ communication rounds with
$\lceil\log_2 p\rceil$ applications of $\oplus$ (which could be expensive), the
exclusive scan appears more difficult. Conventionally, the problem is solved
with either $\lceil\log_2 (p-1)\rceil+1$ communication rounds (e.g., by
shifting the input vectors), or in $\lceil\log_2 p\rceil$ communication rounds
with $2\lceil\log_2 p\rceil-1$ applications of $\oplus$ (by a modified
inclusive scan algorithm). We give a new, simple algorithm that computes the
exclusive prefix sums in $q=\lceil\log_2 (p-1)+\log_2\frac{4}{3}\rceil$
simultaneous send-receive communication rounds with $q-1$ applications of
$\oplus$. We compare the three algorithms implemented in MPI against the MPI
library native MPI\_Exscan primitive on a small, $36$-node cluster with a
state-of-the-art MPI library, indicating possible and worthwhile improvements
to standard implementations. The algorithms assume input vectors to be small so
that performance is dominated by the number of communication rounds. For large
input vectors, other (pipelined, fixed-degree tree) algorithms must be used.

</details>


### [116] [Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms](https://arxiv.org/abs/2507.04786)
*Zhiyi Hu,Siyuan Shen,Tommaso Bonato,Sylvain Jeaugey,Cedell Alexander,Eric Spada,Jeff Hammond,Torsten Hoefler*

Main category: cs.DC

TL;DR: 本文对NVIDIA集体通信库（NCCL）进行了深入分析，揭示了其内部通信协议、数据移动机制及算法，为优化或模拟大规模集体通信提供了指导。


<details>
  <summary>Details</summary>
Motivation: 由于NCCL内部设计不透明，难以分析性能或识别瓶颈，本文旨在揭示其内部架构以帮助系统研究人员和性能工程师。

Method: 分析了NCCL的通信协议变体（Simple、LL、LL128）、数据移动机制及环状和树状集体通信算法。

Result: 研究成果为ATLAHS提供了一个能准确模拟NCCL通信模式的应用跟踪驱动网络仿真工具链。

Conclusion: 通过解析NCCL内部架构，本文为大规模AI训练中的集体通信优化与仿真提供了重要参考。

Abstract: The NVIDIA Collective Communication Library (NCCL) is a critical software
layer enabling high-performance collectives on large-scale GPU clusters.
Despite being open source with a documented API, its internal design remains
largely opaque. The orchestration of communication channels, selection of
protocols, and handling of memory movement across devices and nodes are not
well understood, making it difficult to analyze performance or identify
bottlenecks. This paper presents a comprehensive analysis of NCCL, focusing on
its communication protocol variants (Simple, LL, and LL128), mechanisms
governing intra-node and inter-node data movement, and ring- and tree-based
collective communication algorithms. The insights obtained from this study
serve as the foundation for ATLAHS, an application-trace-driven network
simulation toolchain capable of accurately reproducing NCCL communication
patterns in large-scale AI training workloads. By demystifying NCCL's internal
architecture, this work provides guidance for system researchers and
performance engineers working to optimize or simulate collective communication
at scale.

</details>


### [117] [Distributed Approximation Algorithms for Minimum Dominating Set in Locally Nice Graphs](https://arxiv.org/abs/2507.04960)
*Marthe Bonamy,Cyril Gavoille,Timothé Picavet,Alexandra Wesolek*

Main category: cs.DC

TL;DR: 该论文提出了一个新的、简短的证明，表明在给定Euler genus-$g$表面上可嵌入的图具有一个简单的$f(g)$轮分布式算法，用于最小支配集（MDS），近似比$α \le 906$，并通过改进将其降至$α \le 34 +\varepsilon$，超越了现有技术的$24g+O(1)$近似比。


<details>
  <summary>Details</summary>
Motivation: 改进当前Euler genus-$g$表面上可嵌入图的最小支配集问题分布式算法的近似比，提供更高的效率和更低的计算复杂度。

Method: 基于Heydt等人的技巧和局部算法，结合图的渐近维度理论，提出新的分布式算法，无需预先嵌入图结构。

Result: 实现了更低且改进的近似比$α \le 34 +\varepsilon$，显著优于现有技术。

Conclusion: 该算法在确定性LOCAL模型中有效，适用于更大类别的图结构，具有广泛的理论和应用潜力。

Abstract: We give a new, short proof that graphs embeddable in a given Euler genus-$g$
surface admit a simple $f(g)$-round $\alpha$-approximation distributed
algorithm for Minimum Dominating Set (MDS), where the approximation ratio
$\alpha \le 906$. Using tricks from Heydt et al. [European Journal of
Combinatorics (2025)], we in fact derive that $\alpha \le 34 +\varepsilon$,
therefore improving upon the current state of the art of $24g+O(1)$ due to
Amiri et al. [ACM Transactions on Algorithms (2019)]. It also improves the
approximation ratio of $91+\varepsilon$ due to Czygrinow et al. [Theoretical
Computer Science (2019)] in the particular case of orientable surfaces.
  All our distributed algorithms work in the deterministic LOCAL model. They do
not require any preliminary embedding of the graph and only rely on two things:
a LOCAL algorithm for MDS on planar graphs with ``uniform'' approximation
guarantees and the knowledge that graphs embeddable in bounded Euler genus
surfaces have asymptotic dimension $2$.
  More generally, our algorithms work in any graph class of bounded asymptotic
dimension where ``most vertices'' are locally in a graph class that admits a
LOCAL algorithm for MDS with uniform approximation guarantees.

</details>


### [118] [Silent Failures in Stateless Systems: Rethinking Anomaly Detection for Serverless Computing](https://arxiv.org/abs/2507.04969)
*Chanh Nguyen,Erik Elmroth,Monowar Bhuyan*

Main category: cs.DC

TL;DR: 论文探讨了无服务计算中的异常检测挑战，提出下一代检测框架的研究方向。


<details>
  <summary>Details</summary>
Motivation: 无服务计算的动态性和临时性使传统异常检测方法失效，需新方法应对。

Method: 系统分析无服务系统的独特挑战和威胁，提出研究议程。

Result: 明确了上下文感知、多源数据融合等关键研究方向。

Conclusion: 为下一代无服务生态系统中的异常检测奠定了基础。

Abstract: Serverless computing has redefined cloud application deployment by
abstracting infrastructure and enabling on-demand, event-driven execution,
thereby enhancing developer agility and scalability. However, maintaining
consistent application performance in serverless environments remains a
significant challenge. The dynamic and transient nature of serverless functions
makes it difficult to distinguish between benign and anomalous behavior, which
in turn undermines the effectiveness of traditional anomaly detection methods.
These conventional approaches, designed for stateful and long-running services,
struggle in serverless settings where executions are short-lived, functions are
isolated, and observability is limited.
  In this first comprehensive vision paper on anomaly detection for serverless
systems, we systematically explore the unique challenges posed by this
paradigm, including the absence of persistent state, inconsistent monitoring
granularity, and the difficulty of correlating behaviors across distributed
functions. We further examine a range of threats that manifest as anomalies,
from classical Denial-of-Service (DoS) attacks to serverless-specific threats
such as Denial-of-Wallet (DoW) and cold start amplification. Building on these
observations, we articulate a research agenda for next-generation detection
frameworks that address the need for context-aware, multi-source data fusion,
real-time, lightweight, privacy-preserving, and edge-cloud adaptive
capabilities.
  Through the identification of key research directions and design principles,
we aim to lay the foundation for the next generation of anomaly detection in
cloud-native, serverless ecosystems.

</details>


### [119] [MoLink: Distributed and Efficient Serving Framework for Large Models](https://arxiv.org/abs/2507.05043)
*Lewei Jin,Yongqi Chen,Kui Zhang,Yifan Zhuo,Yi Gao,Bowei Yang,Zhengong Cai,Wei Dong*

Main category: cs.DC

TL;DR: 论文摘要讨论了大型语言模型的成本问题，并提出了一种基于消费级GPU的高效分布式LLM服务系统MoLink。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高成本服务问题促使研究者探索利用消费级GPU来降低成本。

Method: 提出了MoLink系统，通过关键技术支持在异构和弱连接的消费级GPU上高效运行LLM。

Result: 实验表明，MoLink在吞吐量和成本效益上分别提升至458%和151%。

Conclusion: MoLink为跨平台用户提供了一种高效、易用的LLM服务解决方案。

Abstract: Large language models represent a groundbreaking shift in generative AI. Yet,
these advances come with a significant challenge: the high cost of model
serving. To mitigate these costs, consumer-grade GPUs emerge as a more
affordable alternative. This presents an opportunity for more cost-efficient
LLM serving by leveraging these GPUs.
  However, it is non-trivial to achieve high-efficiency LLM serving on
consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often
deployed in limited network conditions; 2) these GPUs often exhibit
heterogeneity in host systems. To address these challenges, we present MoLink,
a distributed LLM serving system for large models. It incorporates several key
techniques, enabling efficient LLM serving on heterogeneous and weakly
connected consumer-grade GPUs. Our experiments demonstrate that it achieves
throughput improvements of up to 458\% and cost-profit margin improvements of
up to 151\%, compared to state-of-the-art systems. MoLink allows users on
Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a
few lines of code over Ethernet or public networks. Currently, it supports 18
mainstream architectures of open-source large language models.

</details>


### [120] [Cooperative Gradient Coding](https://arxiv.org/abs/2507.05230)
*Shudi Weng,Ming Xiao,Chao Ren,Mikael Skoglund*

Main category: cs.DC

TL;DR: 该论文提出了一种名为CoGC的新型梯度共享框架，用于分布式训练中的不可靠通信场景，并通过互补解码机制GC⁺提高了系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式训练中因通信不可靠导致的问题，尤其是减少数据复制需求并提高通信效率。

Method: 采用了CoGC框架和GC⁺解码机制，前者实现严格的二元结果，后者利用失败时被丢弃的信息提高效率。

Result: CoGC在客户端通信良好时表现出强韧性，而GC⁺在通信差时显著提升了系统可靠性。理论分析和模拟验证了其有效性。

Conclusion: 该研究为分布式训练中的梯度编码提供了新思路，并通过理论框架和实验验证了其性能优势。

Abstract: This work studies gradient coding (GC) in the context of distributed training
problems with unreliable communication. We propose cooperative GC (CoGC), a
novel gradient-sharing-based GC framework that leverages cooperative
communication among clients. This approach ultimately eliminates the need for
dataset replication, making it both communication- and computation-efficient
and suitable for federated learning (FL). By employing the standard GC decoding
mechanism, CoGC yields strictly binary outcomes: either the global model is
exactly recovered, or the decoding fails entirely, with no intermediate
results. This characteristic ensures the optimality of the training and
demonstrates strong resilience to client-to-server communication failures when
the communication channels among clients are in good condition. However, it may
also result in communication inefficiency and hinder convergence due to its
lack of flexibility, especially when communication channels among clients are
in poor condition. To overcome this limitation and further harness the
potential of GC matrices, we propose a complementary decoding mechanism, termed
GC$^+$, which leverages information that would otherwise be discarded during GC
decoding failures. This approach significantly improves system reliability
under unreliable communication, as the full recovery of the global model
typically dominates in GC$^+$. To conclude, this work establishes solid
theoretical frameworks for both CoGC and GC$^+$. We provide complete outage
analyses for each decoding mechanism, along with a rigorous investigation of
how outages affect the structure and performance of GC matrices. Building on
these analyses, we derive convergence bounds for both decoding mechanisms.
Finally, the effectiveness of CoGC and GC$^+$ is validated through extensive
simulations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [121] [LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization](https://arxiv.org/abs/2507.03384)
*Suchen Liu,Jun Gao,Yinjun Han,Yang Lin*

Main category: cs.DB

TL;DR: 本论文提出LLM4Hint，利用中等规模的大型语言模型（LLM）为SQL查询优化提供提示，通过轻量级模型、查询改写策略和匹配提示，提高优化器的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 由于传统优化器需要复杂的人工调优，而基于学习的方法在泛化性上存在局限，论文探索如何利用LLM增强学习优化器的泛化性。

Method: LLM4Hint结合轻量级模型生成软提示、商用LLM进行查询改写，并引入匹配提示来加速模型收敛。

Result: 实验表明，LLM4Hint在效果和泛化性上优于当前最先进的学习优化器。

Conclusion: LLM4Hint通过LLM的强大理解能力，成功提升了查询优化的性能，并解决了高延迟和调优成本问题。

Abstract: Query optimization is essential for efficient SQL query execution in DBMS,
and remains attractive over time due to the growth of data volumes and advances
in hardware. Existing traditional optimizers struggle with the cumbersome
hand-tuning required for complex workloads, and the learning-based methods face
limitations in ensuring generalization. With the great success of Large
Language Model (LLM) across diverse downstream tasks, this paper explores how
LLMs can be incorporated to enhance the generalization of learned optimizers.
Though promising, such an incorporation still presents challenges, mainly
including high model inference latency, and the substantial fine-tuning cost
and suboptimal performance due to inherent discrepancy between the token
sequences in LLM and structured SQL execution plans with rich numerical
features.
  In this paper, we focus on recurring queries in offline optimization to
alleviate the issue of high inference latency, and propose \textbf{LLM4Hint}
that leverages moderate-sized backbone LLMs to recommend query optimization
hints. LLM4Hint achieves the goals through: (i) integrating a lightweight model
to produce a soft prompt, which captures the data distribution in DBMS and the
SQL predicates to provide sufficient optimization features while simultaneously
reducing the context length fed to the LLM, (ii) devising a query rewriting
strategy using a larger commercial LLM, so as to simplify SQL semantics for the
backbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit
matching prompt to facilitate alignment between the LLM and the lightweight
model, which can accelerate convergence of the combined model. Experiments show
that LLM4Hint, by leveraging the LLM's stronger capability to understand the
query statement, can outperform the state-of-the-art learned optimizers in
terms of both effectiveness and generalization.

</details>


### [122] [PFCS: Prime Factorization Cache System for Deterministic Data Relationship Discovery](https://arxiv.org/abs/2507.03919)
*Duy Le*

Main category: cs.DB

TL;DR: PFCS利用质因数分解的数学唯一性实现确定性关系发现，显著提升缓存系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统缓存系统依赖统计启发式方法，无法保证关系发现，导致预取不准确和资源浪费。

Method: 为数据元素分配唯一质数，将关系表示为合数，通过质因数分解恢复完美关系。

Result: 在数据库、ML和HPC工作负载中，平均性能提升6.2倍，命中率98.9%，功耗降低38%。

Conclusion: PFCS的数学基础提供了传统近似方法无法实现的正式保证，为缓存系统设计确立了新范式。

Abstract: Cache systems fundamentally limit modern computing performance due to their
inability to precisely capture data relationships. While achieving 85-92% hit
rates, traditional systems rely on statistical heuristics that cannot guarantee
relationship discovery, leading to suboptimal prefetching and resource waste.
We present PFCS (Prime Factorization Cache System), which leverages the
mathematical uniqueness of prime factorization to achieve deterministic
relationship discovery with zero false positives. PFCS assigns unique primes to
data elements and represents relationships as composite numbers, enabling the
recovery of perfect relationships through factorization. A comprehensive
evaluation across database, ML, and HPC workloads demonstrates an average
performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction
compared to state-of-the-art systems. The mathematical foundation provides
formal guarantees impossible with approximation-based approaches, establishing
a new paradigm for cache system design

</details>


### [123] [OneDB: A Distributed Multi-Metric Data Similarity Search System](https://arxiv.org/abs/2507.04256)
*Tang Qian,Yifan Zhu,Lu Chen,Xiangyu Ke,Jingwen Zhao,Tianyi Li,Yunjun Gao,Christian S. Jensen*

Main category: cs.DB

TL;DR: OneDB是一个分布式多度量数据相似性检索系统，支持多模态数据管理，通过创新技术和自动调优实现高效、准确的检索。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据日益增长，需要通用且高效的数据管理解决方案，支持用户友好且准确的检索。

Method: OneDB利用多度量模型统一多模态数据，提供扩展的Spart SQL查询接口、轻量级权重学习、智能剪枝策略、两层索引和自动调优。

Result: 实验表明，OneDB在检索效率、准确性、可扩展性和参数调优方面显著优于现有技术。

Conclusion: OneDB通过创新设计和自动调优，实现了高效、准确的多模态数据检索，具有实际应用潜力。

Abstract: Increasingly massive volumes of multi-modal data are being accumulated in
many {real world} settings, including in health care and e-commerce. This
development calls for effective general-purpose data management solutions for
multi-modal data. Such a solution must facilitate user-friendly and accurate
retrieval of any multi-modal data according to diverse application
requirements. Further, such a solution must be capable of efficient and
scalable retrieval.
  To address this need, we present OneDB, a distributed multi-metric data
similarity retrieval system. This system exploits the fact that data of diverse
modalities, such as text, images, and video, can be represented as metric data.
The system thus affords each data modality its own metric space with its own
distance function and then uses a multi-metric model to unify multi-modal data.
The system features several innovations: (i) an extended Spart SQL query
interface; (ii) lightweight means of learning appropriate weights of different
modalities when retrieving multi-modal data to enable accurate retrieval; (iii)
smart search-space pruning strategies that improve efficiency; (iv) two-layered
indexing of data to ensure load-balancing during distributed processing; and
(v) end-to-end system parameter autotuning.
  Experiments on three real-life datasets and two synthetic datasets offer
evidence that the system is capable of state-of-the-art performance: (i)
efficient and effective weight learning; (ii) retrieval accuracy improvements
of 12.63\%--30.75\% over the state-of-the-art vector similarity search system
at comparable efficiency; (iii) accelerated search by 2.5--5.75x over
state-of-the-art single- or multi-metric solutions; (iv) demonstrated high
scalability; and (v) parameter tuning that enables performance improvements of
15+%.

</details>


### [124] [AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset Discovery in Data Lakes](https://arxiv.org/abs/2507.04687)
*Zhenwei Dai,Chuan Lei,Asterios Katsifodimos,Xiao Qin,Christos Faloutsos,Huzefa Rangwala*

Main category: cs.DB

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）生成高质量且多样化的表格数据，用于评估数据集发现方法，特别是表连接性任务。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集发现方法的基准数据集存在三个主要问题：数据类型单一、缺乏人工标注列对、忽视语义列关系。LLMs因其广泛的通用和领域知识，提供了生成更优基准数据的潜力。

Method: 提出利用LLMs生成多样化、高质量的表格数据，并包含人工标注的列对，以解决现有基准数据的不足。

Result: LLMs可以生成适用于评估数据集发现方法的表格数据，特别是表连接性任务。

Conclusion: LLMs为构建更优的数据集发现方法评估基准提供了新途径，未来可以进一步探索其在不同领域中的应用。

Abstract: How to generate a large, realistic set of tables along with joinability
relationships, to stress-test dataset discovery methods? Dataset discovery
methods aim to automatically identify related data assets in a data lake. The
development and evaluation of such solutions for customers from a wide range of
business domains, relies on diverse, high quality and domain-specific tabular
benchmarks. Large language models (LLMs) are trained on a wide variety of text
data, which can provide a strong foundation of general and domain-specific
knowledge. In this paper, we ask the question -- \textit{can we leverage LLMs
to generate a tabular benchmark adequate for evaluating the dataset discovery
solutions?} In particular, we focus on the task of finding joinable tables
which is the cornerstone of virtually every dataset discovery method. Current
corpora for evaluating dataset discovery methods are mainly based on subsets of
open data, and they suffer from three important issues: $i)$ they focus on very
common and generic data types (e.g., address, id, name, etc.); $ii)$ they do
not contain human-annotated column pairs; instead, practitioners synthesize
ground truth using table splits (e.g., horizontal for table union search and
vertical ones for joinability) and $iii)$ they do not focus on semantic column
relationships.

</details>


### [125] [SHARP: Shared State Reduction for Efficient Matching of Sequential Patterns](https://arxiv.org/abs/2507.04872)
*Cong Yu,Tuo Shi,Matthias Weidlich,Bo Zhao*

Main category: cs.DB

TL;DR: 本文提出SHARP库，通过状态共享实现高效的模式匹配，适用于CEP、OLAP和RAG应用。


<details>
  <summary>Details</summary>
Motivation: 现有技术单独处理每个模式，忽略了模式匹配中状态共享的优化潜力。

Method: SHARP引入模式共享度（PSD）抽象，运行时分类索引部分匹配，并在超时后快速选择子集进行后续处理。

Result: 实验显示，SHARP在50%平均处理延迟下，CEP、OLAP和RAG应用中的召回率分别达到97%、96%和73%。

Conclusion: SHARP通过状态共享显著提升了模式匹配的效率与召回率。

Abstract: The detection of sequential patterns in data is a basic functionality of
modern data processing systems for complex event processing (CEP), OLAP, and
retrieval-augmented generation (RAG). In practice, pattern matching is
challenging, since common applications rely on a large set of patterns that
shall be evaluated with tight latency bounds. At the same time, matching needs
to maintain state, i.e., intermediate results, that grows exponentially in the
input size. Hence, systems turn to best-effort processing, striving for maximal
recall under a latency bound. Existing techniques, however, consider each
pattern in isolation, neglecting the optimization potential induced by state
sharing in pattern matching.
  In this paper, we present SHARP, a library that employs state reduction to
achieve efficient best-effort pattern matching. To this end, SHARP incorporates
state sharing between patterns through a new abstraction, coined
pattern-sharing degree (PSD). At runtime, this abstraction facilitates the
categorization and indexing of partial pattern matches. Based thereon, once a
latency bound is exceeded, SHARP realizes best-effort processing by selecting a
subset of partial matches for further processing in constant time. In
experiments with real-world data, SHARP achieves a recall of 97%, 96% and 73%
for pattern matching in CEP, OLAP, and RAG applications, under a bound of 50%
of the average processing latency.

</details>


### [126] [The Case for Instance-Optimized LLMs in OLAP Databases](https://arxiv.org/abs/2507.04967)
*Bardia Mohammadi,Laurent Bindschaedler*

Main category: cs.DB

TL;DR: IOLM-DB通过为每次查询生成轻量级专用模型，显著降低计算和内存开销，使大规模LLM增强查询变得可行。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在大规模数据处理时计算和内存开销大，需要一种方法降低成本。

Method: IOLM-DB针对每个查询生成专用模型，结合量化、稀疏化和结构剪枝等压缩技术。

Result: 模型体积减少76%，吞吐量提升3.31倍，同时保持准确性。

Conclusion: IOLM-DB证明LLM增强查询在大规模分析系统中可行，为未来OLAP应用开辟新可能。

Abstract: Large Language Models (LLMs) can enhance analytics systems with powerful data
summarization, cleaning, and semantic transformation capabilities. However,
deploying LLMs at scale -- processing millions to billions of rows -- remains
prohibitively expensive in computation and memory. We present IOLM-DB, a novel
system that makes LLM-enhanced database queries practical through
query-specific model optimization. Instead of using general-purpose LLMs,
IOLM-DB generates lightweight, specialized models tailored to each query's
specific needs using representative data samples. IOLM-DB reduces model
footprints by up to 76% and increases throughput by up to 3.31$\times$ while
maintaining accuracy through aggressive compression techniques, including
quantization, sparsification, and structural pruning. We further show how our
approach enables higher parallelism on existing hardware and seamlessly
supports caching and batching strategies to reduce overheads. Our prototype
demonstrates that leveraging LLM queries inside analytics systems is feasible
at scale, opening new possibilities for future OLAP applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [127] [ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis](https://arxiv.org/abs/2507.03255)
*Zedong Peng,Zeju Li,Mingzhe Gao,Qiang Xu,Chen Zhang,Jieru Zhao*

Main category: cs.AR

TL;DR: ForgeEDA是一个开源的综合电路数据集，涵盖多种电路表示形式，支持EDA算法的基准测试和AI模型训练，旨在推动IC设计和EDA领域的创新。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有数据集的局限性，ForgeEDA旨在推动现代IC设计的突破并支持EDA领域的下一代创新。

Method: ForgeEDA提供了多种电路表示形式（如RTL代码、PM网表等），并通过基准测试和AI模型训练展示了其效用。

Result: ForgeEDA在PPA优化等任务中展示了其能力，能够暴露性能差距并推动技术进展。

Conclusion: ForgeEDA通过其规模和多样性，有潜力改进AI模型性能并推动EDA领域的突破。

Abstract: We introduce ForgeEDA, an open-source comprehensive circuit dataset across
various categories. ForgeEDA includes diverse circuit representations such as
Register Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter
Graphs (AIGs), and placed netlists, enabling comprehensive analysis and
development. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art
EDA algorithms on critical tasks such as Power, Performance, and Area (PPA)
optimization, highlighting its ability to expose performance gaps and drive
advancements. Additionally, ForgeEDA's scale and diversity facilitate the
training of AI models for EDA tasks, demonstrating its potential to improve
model performance and generalization. By addressing limitations in existing
datasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and
support the next generation of innovations in EDA.

</details>


### [128] [Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA](https://arxiv.org/abs/2507.03308)
*Jindong Li,Tenglong Li,Ruiqi Chen,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng*

Main category: cs.AR

TL;DR: Hummingbird是一种专为嵌入式FPGA设计的LLM推理加速器，具有更小的体积、更高的性能和更低的功耗，支持更大规模的LLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究在嵌入式设备上部署大型语言模型（LLM）时面临计算和内存资源有限的挑战，而嵌入式FPGA的潜力尚未充分发挥。

Method: 设计了一个名为Hummingbird的FPGA加速器，优化了资源利用率并采用卸载策略克服内存限制。

Result: 在嵌入式FPGA（如KV260和ZCU104）上实现了更高的推理速度和模型带宽利用率，同时功耗显著降低。

Conclusion: Hummingbird为边缘设备提供了高效且成本优化的LLM解决方案，展示了工业应用的可行性。

Abstract: Deploying large language models (LLMs) on embedded devices remains a
significant research challenge due to the high computational and memory demands
of LLMs and the limited hardware resources available in such environments.
While embedded FPGAs have demonstrated performance and energy efficiency in
traditional deep neural networks, their potential for LLM inference remains
largely unexplored. Recent efforts to deploy LLMs on FPGAs have primarily
relied on large, expensive cloud-grade hardware and have only shown promising
results on relatively small LLMs, limiting their real-world applicability. In
this work, we present Hummingbird, a novel FPGA accelerator designed
specifically for LLM inference on embedded FPGAs. Hummingbird is smaller,
targeting embedded FPGAs such as the KV260 and ZCU104 with 67% LUT, 39% DSP,
and 42% power savings over existing research. Hummingbird is stronger,
targeting LLaMA3-8B and supporting longer contexts, overcoming the typical 4GB
memory constraint of embedded FPGAs through offloading strategies. Finally,
Hummingbird is faste, achieving 4.8 tokens/s and 8.6 tokens/s for LLaMA3-8B on
the KV260 and ZCU104 respectively, with 93-94% model bandwidth utilization,
outperforming the prior 4.9 token/s for LLaMA2-7B with 84% bandwidth
utilization baseline. We further demonstrate the viability of industrial
applications by deploying Hummingbird on a cost-optimized Spartan UltraScale
FPGA, paving the way for affordable LLM solutions at the edge.

</details>


### [129] [A Flexible Instruction Set Architecture for Efficient GEMMs](https://arxiv.org/abs/2507.03522)
*Alexandre de Limas Santana,Adrià Armejach,Francesc Martinez,Erich Focht,Marc Casas*

Main category: cs.AR

TL;DR: 本文提出了Matrix Tile Extension (MTE)，一种新型矩阵指令集架构，解决了现有矩阵ISA在运行GEMM工作负载时的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵ISA在处理GEMM工作负载时性能不理想，尤其是对小型或非标准形状矩阵，且无法动态适应应用程序特性。

Method: 提出MTE，通过解耦指令集架构与微架构，最小化实现开销，支持多维向量化和利用现有向量寄存器文件。

Result: MTE在常见卷积和Transformer模型上性能优于现有最佳矩阵ISA，速度提升达1.35倍。

Conclusion: MTE为矩阵工作负载提供了一种高效灵活的解决方案，显著提升了性能。

Abstract: GEneral Matrix Multiplications (GEMMs) are recurrent in high-performance
computing and deep learning workloads. Typically, high-end CPUs accelerate GEMM
workloads with Single-Instruction Multiple Data (SIMD) or vector Instruction
Set Architectures (ISAs). Since these ISAs face significant issues when running
GEMM workloads, particularly when dealing with small, tall, or skinny matrices,
matrix ISAs have been proposed and implemented by major hardware vendors in the
last years. Although these matrix ISAs deliver larger throughput when running
GEMMs than their SIMD/vector counterparts, they are rigid solutions unable to
dynamically adapt themselves to application-specific aspects like the data
format. This paper demonstrates that the state-of-the-art matrix ISAs deliver
suboptimal performance when running the most commonly used convolution and
transformer models.
  This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA
that completely decouples the instruction set architecture from the
microarchitecture and seamlessly interacts with existing vector ISAs. MTE
incurs minimal implementation overhead since it only requires a few additional
instructions and a 64-bit Control Status Register (CSR) to keep its state.
Specifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and
K; ii) leverage the capacity of the existing vector register file; and iii)
decouple the tile shape from the underlying microarchitecture. MTE achieves
speed-ups of 1.35x over the best state-of-the-art matrix ISA.

</details>


### [130] [FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification](https://arxiv.org/abs/2507.04276)
*Gwok-Waa Wan,Shengchu Su,Ruihu Wang,Qixiang Chen,Sam-Zaak Wong,Mengnv Xing,Hefei Feng,Yubo Wang,Yinan Zhu,Jingyi Zhang,Jianmin Ye,Xinlai Wan,Tao Ni,Qiang Xu,Nan Guan,Zhe Jiang,Xi Wang,Yang Jun*

Main category: cs.AR

TL;DR: 论文提出FIXME框架，首个用于评估大语言模型在硬件功能验证领域性能的开源工具，填补了当前研究的空白。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注RTL生成和基础调试，忽略了功能验证这一关键领域，随着硬件复杂度的快速提升，功能验证成为现代设计方法的主要瓶颈。

Method: FIXME是一个端到端、多模型的开源评估框架，包含三层次难度结构和六个验证子领域的180项任务，采用AI与人类协作的方式构建高质量数据集，覆盖100%硅验证设计。

Result: 通过专家指导优化，功能覆盖率提升了45.57%，并评估了GPT-4、Claude3和LlaMA3等先进大语言模型的表现。

Conclusion: FIXME为硬件设计验证中的LLM驱动自动化提供了重要基准，指出了改进方向和研究潜力。

Abstract: Despite the transformative potential of Large Language Models (LLMs) in
hardware design, a comprehensive evaluation of their capabilities in design
verification remains underexplored. Current efforts predominantly focus on RTL
generation and basic debugging, overlooking the critical domain of functional
verification, which is the primary bottleneck in modern design methodologies
due to the rapid escalation of hardware complexity. We present FIXME, the first
end-to-end, multi-model, and open-source evaluation framework for assessing LLM
performance in hardware functional verification (FV) to address this crucial
gap. FIXME introduces a structured three-level difficulty hierarchy spanning
six verification sub-domains and 180 diverse tasks, enabling in-depth analysis
across the design lifecycle. Leveraging a collaborative AI-human approach, we
construct a high-quality dataset using 100% silicon-proven designs, ensuring
comprehensive coverage of real-world challenges. Furthermore, we enhance the
functional coverage by 45.57% through expert-guided optimization. By rigorously
evaluating state-of-the-art LLMs such as GPT-4, Claude3, and LlaMA3, we
identify key areas for improvement and outline promising research directions to
unlock the full potential of LLM-driven automation in hardware design
verification. The benchmark is available at
https://github.com/ChatDesignVerification/FIXME.

</details>


### [131] [HLStrans: Dataset for LLM-Driven C-to-HLS Hardware Code Synthesis](https://arxiv.org/abs/2507.04315)
*Qingyun Zou,Nuo Chen,Yao Chen,Bingsheng He,WengFei Wong*

Main category: cs.AR

TL;DR: 本文介绍了HLStrans数据集，用于解决现有开源数据集中复杂性和优化多样性不足的问题，以支持AI与硬件合成领域的研究。


<details>
  <summary>Details</summary>
Motivation: 传统HLS代码生成存在困难，缺乏足够复杂和多样化的数据集，HLStrans数据集旨在填补这一空白。

Method: 通过收集137个真实程序并标注多种C-to-HLS转换，生成了23K带标签的设计变体。

Result: 数据集为LLMs提供了丰富的优化多样性，用于评估其生成高性能HLS代码的能力。

Conclusion: HLStrans数据集将扩展规模和程序多样性，推动AI与硬件合成研究。

Abstract: High-level synthesis (HLS) enables software developers to describe and
implement hardware at a higher level of abstraction by using C/C++ instead of
traditional hardware description languages to automatically generate FPGA-ready
designs. However, generating HLS code significantly differs from standard
C/C++: it disallows certain coding idioms, relies on specialized libraries, and
critically requires fine-grained transformations and the insertion of
optimization directives (pragmas) to achieve high performance. Large language
models (LLMs) have shown promise in automating such transformations, yet
existing open-source datasets lack sufficient complexity and optimization
diversity. To address this gap, we introduce the HLStrans dataset, a
comprehensive collection of 137 distinct real word programs, each annotated
with a variety of C-to-HLS transformations that yield over 23K labeled design
variants. These include a broad spectrum of pragmas and code-level
optimizations. We benchmark state-of-the-art LLMs on this dataset to evaluate
their ability to generate synthesizable, high-performance HLS code. As part of
an ongoing effort, we plan to expand the HLStrans dataset in both scale and
program variety, further empowering research at the intersection of AI and
hardware synthesis.

</details>


### [132] [da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs](https://arxiv.org/abs/2507.04535)
*Chang Sun,Zhiqiang Que,Vladimir Loncar,Wayne Luk,Maria Spiropulu*

Main category: cs.AR

TL;DR: 论文提出了一种基于分布式算法（DA）的高效算法，用于在FPGA上实现恒定矩阵向量乘法（CMVM），优化了面积占用和延迟，并将其集成到开源库hls4ml中。


<details>
  <summary>Details</summary>
Motivation: 由于在微秒级延迟要求下（如CERN大型强子对撞机中的应用），神经网络通常以全展开和流水线方式部署在FPGA上，而面积利用率成为瓶颈。

Method: 采用分布式算法（DA）实现CMVM操作，同时优化面积和延迟。

Result: 算法在保持与最先进算法相似资源减少的同时，显著提高了计算速度，并成功将片上资源减少多达三分之一。

Conclusion: 该算法通过降低资源占用和延迟，实现了此前不可行的神经网络部署，并被集成到开源库hls4ml中以供广泛应用。

Abstract: Neural networks with a latency requirement on the order of microseconds, like
the ones used at the CERN Large Hadron Collider, are typically deployed on
FPGAs fully unrolled and pipelined. A bottleneck for the deployment of such
neural networks is area utilization, which is directly related to the required
constant matrix-vector multiplication (CMVM) operations. In this work, we
propose an efficient algorithm for implementing CMVM operations with
distributed arithmetic (DA) on FPGAs that simultaneously optimizes for area
consumption and latency. The algorithm achieves resource reduction similar to
state-of-the-art algorithms while being significantly faster to compute. The
proposed algorithm is open-sourced and integrated into the \texttt{hls4ml}
library, a free and open-source library for running real-time neural network
inference on FPGAs. We show that the proposed algorithm can reduce on-chip
resources by up to a third for realistic, highly quantized neural networks
while simultaneously reducing latency, enabling the implementation of
previously infeasible networks.

</details>


### [133] [NeuroPDE: A Neuromorphic PDE Solver Based on Spintronic and Ferroelectric Devices](https://arxiv.org/abs/2507.04677)
*Siqing Fu,Lizhou Wu,Tiejun Li,Chunyuan Zhang,Sheng Ma,Jianmin Zhang,Yuhan Tang,Jixuan Tang*

Main category: cs.AR

TL;DR: NeuroPDE是一种利用新兴自旋电子和铁电器件的神经形态PDE求解器硬件设计，通过自旋神经元的概率传输和铁电突触的非易失性存储，显著提升了求解扩散方程的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构缺乏硬件固有的随机性，限制了PDE求解器的性能。NeuroPDE通过新兴器件（如自旋电子和铁电器件）的物理随机性解决了这一问题。

Method: 设计NeuroPDE硬件，结合自旋神经元的随机传输和铁电突触的非易失性存储，模拟随机行走以求解PDE。

Result: NeuroPDE在求解扩散方程时，方差小于1e-2，执行速度快3.48x至315x，能耗低2.7x至29.8x，优于先进CMOS神经形态芯片。

Conclusion: NeuroPDE利用新兴器件的物理随机性，为未来的概率神经形态计算系统开辟了新路径。

Abstract: In recent years, new methods for solving partial differential equations
(PDEs) such as Monte Carlo random walk methods have gained considerable
attention. However, due to the lack of hardware-intrinsic randomness in the
conventional von Neumann architecture, the performance of PDE solvers is
limited. In this paper, we introduce NeuroPDE, a hardware design for
neuromorphic PDE solvers that utilizes emerging spintronic and ferroelectric
devices. NeuroPDE incorporates spin neurons that are capable of probabilistic
transmission to emulate random walks, along with ferroelectric synapses that
store continuous weights non-volatilely. The proposed NeuroPDE achieves a
variance of less than 1e-2 compared to analytical solutions when solving
diffusion equations, demonstrating a performance advantage of 3.48x to 315x
speedup in execution time and an energy consumption advantage of 2.7x to 29.8x
over advanced CMOS-based neuromorphic chips. By leveraging the inherent
physical stochasticity of emerging devices, this study paves the way for future
probabilistic neuromorphic computing systems.

</details>


### [134] [Jack Unit: An Area- and Energy-Efficient Multiply-Accumulate (MAC) Unit Supporting Diverse Data Formats](https://arxiv.org/abs/2507.04772)
*Seock-Hwan Noh,Sungju Kim,Seohyun Kim,Daehoon Kim,Jaeha Kung,Yeseong Kim*

Main category: cs.AR

TL;DR: 这篇论文提出了一种名为Jack单元的高效乘加单元（MAC），支持多种数据格式，并通过优化设计显著提升了面积和能效。与基准MAC单元相比，它的面积更小、功耗更低，且在AI加速器上的应用表现更优。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有MAC单元在处理多种数据格式时效率低下的问题，研究人员设计了Jack单元，旨在提升硬件效率和灵活性。

Method: 通过采用精度可调的进位保存乘法器（CSM）、在CSM内调整有效位、以及利用2D子字并行性，实现了高效的MAC设计。

Result: Jack单元的面积比基准MAC单元小1.17~2.01倍，功耗低1.05~1.84倍；在AI基准测试中，能效比基准高1.32~5.41倍。

Conclusion: Jack单元是一种高效且灵活的设计，为AI加速器等应用提供了显著的能效提升。

Abstract: In this work, we introduce an area- and energy-efficient multiply-accumulate
(MAC) unit, named Jack unit, that is a jack-of-all-trades, supporting various
data formats such as integer (INT), floating point (FP), and microscaling data
format (MX). It provides bit-level flexibility and enhances hardware efficiency
by i) replacing the carry-save multiplier (CSM) in the FP multiplier with a
precision-scalable CSM, ii) performing the adjustment of significands based on
the exponent differences within the CSM, and iii) utilizing 2D sub-word
parallelism. To assess effectiveness, we implemented the layout of the Jack
unit and three baseline MAC units. Additionally, we designed an AI accelerator
equipped with our Jack units to compare with a state-of-the-art AI accelerator
supporting various data formats. The proposed MAC unit occupies 1.17~2.01x
smaller area and consumes 1.05~1.84x lower power compared to the baseline MAC
units. On five AI benchmarks, the accelerator designed with our Jack units
improves energy efficiency by 1.32~5.41x over the baseline across various data
formats.

</details>


### [135] [Optimizing Scalable Multi-Cluster Architectures for Next-Generation Wireless Sensing and Communication](https://arxiv.org/abs/2507.05012)
*Samuel Riedel,Yichao Zhang,Marco Bertuletti,Luca Benini*

Main category: cs.AR

TL;DR: 研究下一代无线技术中的多集群架构设计，重点分析了集群规模对同步、数据移动和编程性的影响，并提出了新的同步机制。


<details>
  <summary>Details</summary>
Motivation: 下一代无线技术需要高效的大规模并行架构，但当前多集群架构缺乏对集群规模的优化指导。

Method: 扩展MemPool为多集群架构，并设计双缓冲屏障以解耦处理器和DMA，比较不同集群配置的性能。

Result: 256核单集群比16个16核集群快2倍（内存受限）和24%（计算受限），因减少了同步和通信开销。

Conclusion: 大集群在减少同步和通信开销方面具有显著优势，适用于无线通信和传感负载。

Abstract: Next-generation wireless technologies (for immersive-massive communication,
joint communication and sensing) demand highly parallel architectures for
massive data processing. A common architectural template scales up by grouping
tens to hundreds of cores into shared-memory clusters, which are then scaled
out as multi-cluster manycore systems. This hierarchical design, used in GPUs
and accelerators, requires a balancing act between fewer large clusters and
more smaller clusters, affecting design complexity, synchronization,
communication efficiency, and programmability. While all multi-cluster
architectures must balance these trade-offs, there is limited insight into
optimal cluster sizes. This paper analyzes various cluster configurations,
focusing on synchronization, data movement overhead, and programmability for
typical wireless sensing and communication workloads. We extend the open-source
shared-memory cluster MemPool into a multi-cluster architecture and propose a
novel double-buffering barrier that decouples processor and DMA. Our results
show a single 256-core cluster can be twice as fast as 16 16-core clusters for
memory-bound kernels and up to 24% faster for compute-bound kernels due to
reduced synchronization and communication overheads.

</details>


### [136] [ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration Energy Harvesting](https://arxiv.org/abs/2507.05081)
*Xin Li,Mianxin Xiao,Xi Shen,Jiaqing Chu,Weifeng Huang,Jiashun Li,Yaoyi Li,Mingjing Cai,Jiaming Chen,Xinming Zhang,Daxing Zhang,Congsi Wang,Hong Tang,Bao Zhao,Qitao Lu,Yilong Wang,Jianjun Wang,Minyi Xu,Shitong Fang,Xuanyu Huang. Chaoyang Zhao,Zicheng Liu,Yaowen Yang,Guobiao Hu,Junrui Liang,Wei-Hsin Liao*

Main category: cs.AR

TL;DR: ViPSN2.0是一个模块化、可重构的物联网平台，支持多种振动能量收集器，并通过标准化热插拔接口适应不同应用需求，有效解决环境振动不稳定性带来的问题。


<details>
  <summary>Details</summary>
Motivation: 环境振动的不稳定性限制了无电池IoT系统的能量收集，ViPSN2.0旨在解决这一问题。

Method: 平台支持多种能量收集器（压电、电磁、摩擦电），并采用基于能量指示的电源管理框架，适应不同任务需求。

Result: 通过三种代表性应用验证了平台的多样性和鲁棒性，例如超低功耗无线信标传输和海洋环境中的高功率远程通信。

Conclusion: ViPSN2.0能在能量受限条件下可靠满足无电池IoT系统的多样化需求。

Abstract: Vibration energy harvesting is a promising solution for powering battery-free
IoT systems; however, the instability of ambient vibrations presents
significant challenges, such as limited harvested energy, intermittent power
supply, and poor adaptability to various applications. To address these
challenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT
platform that supports multiple vibration energy harvesters (piezoelectric,
electromagnetic, and triboelectric) and accommodates sensing tasks with varying
application requirements through standardized hot-swappable interfaces.
ViPSN~2.0 incorporates an energy-indication power management framework tailored
to various application demands, including light-duty discrete sampling,
heavy-duty high-power sensing, and complex-duty streaming tasks, thereby
effectively managing fluctuating energy availability. The platform's
versatility and robustness are validated through three representative
applications: ViPSN-Beacon, enabling ultra-low-power wireless beacon
transmission from a single transient fingertip press; ViPSN-LoRa, supporting
high-power, long-range wireless communication powered by wave vibrations in
actual marine environments; and ViPSN-Cam, enabling intermittent image capture
and wireless transfer. Experimental results demonstrate that ViPSN~2.0 can
reliably meet a wide range of requirements in practical battery-free IoT
deployments under energy-constrained conditions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [137] [Age-Aware CSI Acquisition of a Finite-State Markovian Channel](https://arxiv.org/abs/2507.05042)
*Onur Ayan,Jiping Luo,Xueli An,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究了部分可观测无线系统中信息新鲜度（AoI）与信道估计的交互问题，提出了一种基于POMDP的优化方法。


<details>
  <summary>Details</summary>
Motivation: 探索信息新鲜度与信道估计的交互关系，解决无线系统中数据传输与信道状态信息获取的权衡问题。

Method: 使用有限状态马尔可夫信道模型，构建POMDP优化问题，并通过相对值迭代算法求解最优策略。

Result: 模拟实验验证了所提解决方案的高效性。

Conclusion: 首次结合信道状态信息老化效应，实现了数据传输的最优调度策略。

Abstract: The Age of Information (AoI) has emerged as a critical metric for quantifying
information freshness; however, its interplay with channel estimation in
partially observable wireless systems remains underexplored. This work
considers a transmitter-receiver pair communicating over an unreliable channel
with time-varying reliability levels. The transmitter observes the
instantaneous link reliability through a channel state information acquisition
procedure, during which the data transmission is interrupted. This leads to a
fundamental trade-off between utilizing limited network resources for either
data transmission or channel state information acquisition to combat the
channel aging effect. Assuming the wireless channel is modeled as a
finite-state Markovian channel, we formulate an optimization problem as a
partially observable Markov decision process (POMDP), obtain the optimal policy
through the relative value iteration algorithm, and demonstrate the efficiency
of our solution through simulations. To the best of our knowledge, this is the
first work to aim for an optimal scheduling policy for data transmissions while
considering the effect of channel state information aging.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [DistZO2: High-Throughput and Memory-Efficient Zeroth-Order Fine-tuning LLMs with Distributed Parallel Computing](https://arxiv.org/abs/2507.03211)
*Liangyu Wang,Huanyi Xie,Di Wang*

Main category: cs.LG

TL;DR: DistZO2是一个高效内存、高吞吐量的分布式零阶微调框架，解决了大规模语言模型（LLMs）在单设备上内存和计算效率低的问题，通过并行策略和硬件感知通信策略提升了训练速度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调过程中资源密集的问题，尤其是在多百亿参数模型上，零阶优化（ZO）虽节省内存但受限于单设备执行和吞吐量。

Method: DistZO2引入三种并行策略：扰动并行（PertP）、分布式数据并行（DDP）及统一2D并行设计，并结合硬件感知通信策略优化参数加载。

Result: 在OPT-175B上的实验显示，DistZO2比ZO2实现了3倍的速度提升。

Conclusion: DistZO2成功扩展了零阶微调至多GPU系统，保持了内存效率并显著提升了吞吐量。

Abstract: Fine-tuning large language models (LLMs) remains resource-intensive due to
their sheer scale. While zeroth-order (ZO) optimization provides a
memory-efficient alternative by eliminating backward passes, its application to
multi-hundred-billion-parameter models is constrained by GPU memory and compute
throughput. The ZO2 framework addresses the memory bottleneck by offloading
model parameters to CPU memory and overlapping transformer block transfer with
dual forward computation on a single GPU. However, ZO2 remains limited by its
single-device execution and achieves modest throughput. In this work, we
present DistZO2, a high-throughput, memory-efficient framework for distributed
zeroth-order fine-tuning of LLMs. DistZO2 introduces three parallel strategies:
(1) Perturbation Parallelism (PertP), which parallelizes the two perturbed
forward passes across devices; (2) Distributed Data Parallelism (DDP), adapted
to the scalar-gradient nature of ZO training; and (3) a unified 2D Parallelism
design that combines PertP and DDP. To further mitigate communication
bottlenecks introduced by parameter offloading, we propose a hardware-aware
communication strategy that slices parameter blocks and redistributes them
across GPUs via high-speed interconnects such as NVLink. DistZO2 scales
zeroth-order fine-tuning to modern multi-GPU systems, preserving ZO2's memory
efficiency while substantially improving training throughput. In our
experiments on OPT-175B, DistZO2 achieves a 3x speedup over ZO2 with
distributed computing. DistZO2's code has been open-sourced in
https://github.com/liangyuwang/zo2.

</details>


### [139] [Hardware-efficient tractable probabilistic inference for TinyML Neurosymbolic AI applications](https://arxiv.org/abs/2507.05141)
*Jelin Leslin,Martin Trapp,Martin Andraud*

Main category: cs.LG

TL;DR: 通过硬件/软件紧密结合，提出了一种在TinyML硬件上实现神经符号AI的完整框架，显著减少了硬件资源消耗并提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决TinyML硬件上神经符号AI（NSAI）计算中符号模型稀疏性与计算分辨率不匹配的问题。

Method: 1. 训练一种硬件高效的确定性概率电路（PC）；2. 使用$n^{th}$-根压缩技术压缩PC；3. 在TinyML硬件上部署完整NSAI模型。

Result: 与64位精度基线相比，显著减少了FPGA硬件资源（FF节省82.3%，LUT节省52.6%，Flash节省18.0%），平均推理速度提升4.67倍。

Conclusion: 该框架有效解决了TinyML硬件上NSAI的部署难题，显著提升了资源利用率和推理效率。

Abstract: Neurosymbolic AI (NSAI) has recently emerged to mitigate limitations
associated with deep learning (DL) models, e.g. quantifying their uncertainty
or reason with explicit rules. Hence, TinyML hardware will need to support
these symbolic models to bring NSAI to embedded scenarios. Yet, although
symbolic models are typically compact, their sparsity and computation
resolution contrasts with low-resolution and dense neuro models, which is a
challenge on resource-constrained TinyML hardware severely limiting the size of
symbolic models that can be computed. In this work, we remove this bottleneck
leveraging a tight hardware/software integration to present a complete
framework to compute NSAI with TinyML hardware. We focus on symbolic models
realized with tractable probabilistic circuits (PCs), a popular subclass of
probabilistic models for hardware integration. This framework: (1) trains a
specific class of hardware-efficient \emph{deterministic} PCs, chosen for the
symbolic task; (2) \emph{compresses} this PC until it can be computed on TinyML
hardware with minimal accuracy degradation, using our $n^{th}$-root compression
technique, and (3) \emph{deploys} the complete NSAI model on TinyML hardware.
Compared to a 64b precision baseline necessary for the PC without compression,
our workflow leads to significant hardware reduction on FPGA (up to 82.3\% in
FF, 52.6\% in LUTs, and 18.0\% in Flash usage) and an average inference speedup
of 4.67x on ESP32 microcontroller.

</details>


### [140] [A Hybrid Game-Theory and Deep Learning Framework for Predicting Tourist Arrivals via Big Data Analytics and Opinion Leader Detection](https://arxiv.org/abs/2507.03411)
*Ali Nikseresht*

Main category: cs.LG

TL;DR: 该论文提出了一种结合非线性混合方法的新框架，用于预测国际游客到达量，整合了互联网大数据、游戏理论算法和深度学习技术，结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在工业5.0时代，数据驱动的决策对优化工业工程系统至关重要。论文旨在通过大数据分析提升旅游需求预测的准确性，并展示其在更广泛工业领域的应用潜力。

Method: 方法包括：1) 使用游戏理论算法识别社交媒体意见领袖；2) 利用经验小波变换处理非平稳数据；3) 采用堆叠双向长短期记忆网络进行预测。

Result: 实验结果表明，该方法在动态和波动条件下优于现有技术，适用于物流、供应链管理和生产规划等领域。

Conclusion: 通过结合深度学习和社交媒体数据分析，论文展示了大尺度数据如何提升决策质量和效率，为工业工程提供了新视角。

Abstract: In the era of Industry 5.0, data-driven decision-making has become
indispensable for optimizing systems across Industrial Engineering. This paper
addresses the value of big data analytics by proposing a novel non-linear
hybrid approach for forecasting international tourist arrivals in two different
contexts: (i) arrivals to Hong Kong from five major source nations
(pre-COVID-19), and (ii) arrivals to Sanya in Hainan province, China
(post-COVID-19). The method integrates multiple sources of Internet big data
and employs an innovative game theory-based algorithm to identify opinion
leaders on social media platforms. Subsequently, nonstationary attributes in
tourism demand data are managed through Empirical Wavelet Transform (EWT),
ensuring refined time-frequency analysis. Finally, a memory-aware Stacked
Bi-directional Long Short-Term Memory (Stacked BiLSTM) network is used to
generate accurate demand forecasts. Experimental results demonstrate that this
approach outperforms existing state-of-the-art techniques and remains robust
under dynamic and volatile conditions, highlighting its applicability to
broader Industrial Engineering domains, such as logistics, supply chain
management, and production planning, where forecasting and resource allocation
are key challenges. By merging advanced Deep Learning (DL), time-frequency
analysis, and social media insights, the proposed framework showcases how
large-scale data can elevate the quality and efficiency of decision-making
processes.

</details>


### [141] [Efficient Certified Reasoning for Binarized Neural Networks](https://arxiv.org/abs/2507.02916)
*Jiong Yang,Yong Kiam Tan,Mate Soos,Magnus O. Myreen,Kuldeep S. Meel*

Main category: cs.LG

TL;DR: 提出了一种可扩展且可信的BNN验证方法，支持定性和定量分析，大幅提升了速度和覆盖率。


<details>
  <summary>Details</summary>
Motivation: BNN在安全关键应用中需求高，但现有分析方法存在可扩展性或可靠性不足的问题。

Method: 采用定制求解器和近似模型计数器，结合专用证明生成和检查流程。

Result: 定性分析速度快9倍，定量分析速度快218倍；覆盖率达到99%和86%。

Conclusion: 该方法显著提升了BNN验证的效率与可靠性，适用于实际应用。

Abstract: Neural networks have emerged as essential components in safety-critical
applications -- these use cases demand complex, yet trustworthy computations.
Binarized Neural Networks (BNNs) are a type of neural network where each neuron
is constrained to a Boolean value; they are particularly well-suited for
safety-critical tasks because they retain much of the computational capacities
of full-scale (floating-point or quantized) deep neural networks, but remain
compatible with satisfiability solvers for qualitative verification and with
model counters for quantitative reasoning. However, existing methods for BNN
analysis suffer from either limited scalability or susceptibility to soundness
errors, which hinders their applicability in real-world scenarios.
  In this work, we present a scalable and trustworthy approach for both
qualitative and quantitative verification of BNNs. Our approach introduces a
native representation of BNN constraints in a custom-designed solver for
qualitative reasoning, and in an approximate model counter for quantitative
reasoning. We further develop specialized proof generation and checking
pipelines with native support for BNN constraint reasoning, ensuring
trustworthiness for all of our verification results. Empirical evaluations on a
BNN robustness verification benchmark suite demonstrate that our certified
solving approach achieves a $9\times$ speedup over prior certified CNF and
PB-based approaches, and our certified counting approach achieves a $218\times$
speedup over the existing CNF-based baseline. In terms of coverage, our
pipeline produces fully certified results for $99\%$ and $86\%$ of the
qualitative and quantitative reasoning queries on BNNs, respectively. This is
in sharp contrast to the best existing baselines which can fully certify only
$62\%$ and $4\%$ of the queries, respectively.

</details>


### [142] [Multimodal LLM Integrated Semantic Communications for 6G Immersive Experiences](https://arxiv.org/abs/2507.04621)
*Yusong Zhang,Yuxuan Sun,Lei Guo,Wei Chen,Bo Ai,Deniz Gunduz*

Main category: cs.LG

TL;DR: 6G网络中，MLLM-SC框架通过多模态大语言模型实现语义通信，支持任务导向的高效无线传输。


<details>
  <summary>Details</summary>
Motivation: 6G网络需支持高维多模态数据实时传输，但资源有限，需结合环境、上下文和用户意图以实现任务相关的高效通信。

Method: 提出MLLM-SC框架，采用设备-边缘协作架构，利用预训练模型生成语义注意力图并优化编解码器。

Result: 在AR/VR视觉问答和图像生成案例中验证了框架的有效性。

Conclusion: MLLM-SC框架在6G语义通信中展现出高效性和实用性。

Abstract: 6G networks promise revolutionary immersive communication experiences
including augmented reality (AR), virtual reality (VR), and holographic
communications. These applications demand high-dimensional multimodal data
transmission and intelligent data processing in real-time, which is extremely
challenging over resource-limited wireless communication systems. Moreover, a
joint understanding of the environment, context, and user intent is essential
to deliver task-relevant content effectively. This article presents a novel
multimodal large language model (MLLM) integrated semantic communications
framework, termed MLLM-SC, which fully leverages reasoning and generative
capabilities of pre-trained foundation models for context-aware and
task-oriented wireless communication. The MLLM-SC framework adopts a
device-edge collaborative architecture. At the edge, MLLM-empowered semantic
guidance module analyzes multimodal inputs, user intents, and channel
conditions to generate importance-aware attention maps prioritizing
semantically critical information. An importance-aware semantic encoder and a
resource-adaptive semantic decoder are jointly designed and optimized, which
can utilize the semantic guidance for adaptive bandwidth allocation and
high-quality content reconstruction or generation. Extensive case studies on
visual question answering for AR/VR applications and diffusion-driven image
generation validate the effectiveness of MLLM-SC.

</details>


### [143] [BLaST: High Performance Inference and Pretraining using BLock Sparse Transformers](https://arxiv.org/abs/2507.03117)
*Patrik Okanovic,Sameer Deshmukh,Grzegorz Kwasniewski,Kentaro Katayama,Takumi Honda,Maciej Besta,Torsten Hoefler*

Main category: cs.LG

TL;DR: BLaST是专为大型ML模型设计的稀疏化方法，通过块稀疏模式显著减少数据传输和能耗，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模ML模型中数据传输能耗高及现有稀疏化方法的性能与精度损失问题。

Method: BLaST通过迭代稀疏化权重矩阵为适合高效稀疏矩阵乘法的块稀疏模式。

Result: 实现高达95%稀疏度，性能提升16.7倍，能效显著优化。

Conclusion: BLaST在减少能耗、内存占用和延迟方面表现卓越，推动大规模AI系统发展。

Abstract: The energy consumption of large-scale ML models is dominated by data movement
- shuffling billions of parameters across memory hierarchies and data centers.
Effective sparsification to prune redundant parameters is still challenging:
existing methods incur significant accuracy degradation, performance overhead,
or both. We introduce (Bl)ock (a)nd (S)parse (T)ransformers (BLaST), a general,
robust, and reliable sparsification method applicable to linear layers in all
settings. Our method iteratively sparsifies weight matrices into a block
sparsity pattern suitable for efficient sparse matrix-matrix (SpMM)
multiplication. BLaST achieves up to 95% sparsity in MLP weights with
negligible accuracy loss. Our fused, highly optimized Sparse MLP kernel
delivers up to 16.7x speedup over dense MLPs across 9 architectures and 8
datasets, resulting in up to 1.6x inference speedup, 1.11x pretraining speedup
and up to 3.12x inference memory usage reduction. BLaST enables the next
generation of large-scale AI systems by reducing energy use, memory footprint,
and latency.

</details>


### [144] [Distributed Equivariant Graph Neural Networks for Large-Scale Electronic Structure Prediction](https://arxiv.org/abs/2507.03840)
*Manasa Kaniselvan,Alexander Maeder,Chen Hao Xia,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.LG

TL;DR: 论文提出了一种分布式eGNN实现，通过GPU直接通信和输入图分区策略，解决了大规模材料电子结构预测中的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统eGNN在大尺度材料电子结构预测中因密集连接的图表示和高内存需求难以在现代GPU上运行。

Method: 采用分布式eGNN实现，结合GPU直接通信和输入图分区策略，减少GPU间嵌入交换。

Result: 实现强扩展到128 GPU，弱扩展到512 GPU，在3000至190000原子结构上并行效率达87%。

Conclusion: 该方法成功解决了大规模电子结构预测的内存问题，展现了高效的可扩展性。

Abstract: Equivariant Graph Neural Networks (eGNNs) trained on density-functional
theory (DFT) data can potentially perform electronic structure prediction at
unprecedented scales, enabling investigation of the electronic properties of
materials with extended defects, interfaces, or exhibiting disordered phases.
However, as interactions between atomic orbitals typically extend over 10+
angstroms, the graph representations required for this task tend to be densely
connected, and the memory requirements to perform training and inference on
these large structures can exceed the limits of modern GPUs. Here we present a
distributed eGNN implementation which leverages direct GPU communication and
introduce a partitioning strategy of the input graph to reduce the number of
embedding exchanges between GPUs. Our implementation shows strong scaling up to
128 GPUs, and weak scaling up to 512 GPUs with 87% parallel efficiency for
structures with 3,000 to 190,000 atoms on the Alps supercomputer.

</details>


### [145] [Heterogeneous Federated Learning with Prototype Alignment and Upscaling](https://arxiv.org/abs/2507.04310)
*Gyuejeong Lee,Jihwan Shin,Daeyoung Choi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ProtoNorm的新方法，通过原型对齐和原型放大技术优化联邦学习中的原型分离问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的数据分布和模型架构异质性是一个重要挑战，现有的原型联邦学习方法在原型分离上表现欠佳。

Method: ProtoNorm结合了原型对齐（PA）和原型放大（PU）技术，前者通过优化全局原型配置以最大化角度分离，后者通过增加原型幅度提升欧几里得空间的分离。

Result: 实验表明，ProtoNorm能更好地分离原型，并在基准数据集上显著优于现有方法。

Conclusion: ProtoNorm不仅继承了原型联邦学习的通信效率，还因其服务器端处理而特别适合资源受限环境。

Abstract: Heterogeneity in data distributions and model architectures remains a
significant challenge in federated learning (FL). Various heterogeneous FL
(HtFL) approaches have recently been proposed to address this challenge. Among
them, prototype-based FL (PBFL) has emerged as a practical framework that only
shares per-class mean activations from the penultimate layer. However, PBFL
approaches often suffer from suboptimal prototype separation, limiting their
discriminative power. We propose Prototype Normalization (ProtoNorm), a novel
PBFL framework that addresses this limitation through two key components:
Prototype Alignment (PA) and Prototype Upscaling (PU). The PA method draws
inspiration from the Thomson problem in classical physics, optimizing global
prototype configurations on a unit sphere to maximize angular separation;
subsequently, the PU method increases prototype magnitudes to enhance
separation in Euclidean space. Extensive evaluations on benchmark datasets show
that our approach better separates prototypes and thus consistently outperforms
existing HtFL approaches. Notably, since ProtoNorm inherits the communication
efficiency of PBFL and the PA is performed server-side, it is particularly
suitable for resource-constrained environments.

</details>


### [146] [Performance Evaluation of General Purpose Large Language Models for Basic Linear Algebra Subprograms Code Generation](https://arxiv.org/abs/2507.04697)
*Daichi Mukunoki,Shun-ichiro Hayashi,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.LG

TL;DR: 评估通用LLM在CPU上生成BLAS代码的能力，结果显示GPT-4.1和o4-mini能生成优化后的高效代码。


<details>
  <summary>Details</summary>
Motivation: 研究通用LLM在BLAS代码生成中的能力，尤其是优化性能的应用。

Method: 使用GPT-4.1和o4-mini生成不同优化级别的C代码，并与参考代码比较性能。

Result: LLM能生成正确且优化的代码，性能优于参考代码。

Conclusion: 通用LLM在BLAS代码生成中表现优秀，具有实际应用潜力。

Abstract: Generative AI technology based on Large Language Models (LLM) has been
developed and applied to assist or automatically generate program codes. In
this paper, we evaluate the capability of existing general LLMs for Basic
Linear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs
provided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,
and o4-mini, one of the o-series of Reasoning models. Both have been released
in April 2025. For the routines from level-1 to 3 BLAS, we tried to generate
(1) C code without optimization from routine name only, (2) C code with basic
performance optimizations (thread parallelization, SIMD vectorization, and
cache blocking) from routine name only, and (3) C code with basic performance
optimizations based on Fortran reference code. As a result, we found that
correct code can be generated in many cases even when only routine name are
given. We also confirmed that thread parallelization with OpenMP, SIMD
vectorization, and cache blocking can be implemented to some extent, and that
the code is faster than the reference code.

</details>


### [147] [Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to Increase Machine Learning Accuracy and Explainability](https://arxiv.org/abs/2507.02922)
*V. C. Storey,J. Parsons,A. Castellanos,M. Tremblay,R. Lukyanenko,W. Maass,A. Castillo*

Main category: cs.LG

TL;DR: 该研究提出了一种利用概念模型中的领域知识来改进机器学习数据准备的方法CMML，并通过实验和专家评估证明了其对机器学习性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 机器学习在处理大规模多样化数据时面临性能和透明度问题，主要原因是模型对领域知识的利用不足。

Method: 开发了概念建模指导下的机器学习数据准备方法CMML，并应用于实际问题验证其效果。

Result: 实验结果显示CMML能提升模型性能，数据科学家的评估也支持其适用性。

Conclusion: CMML通过整合领域知识有效改进了机器学习的性能和数据准备过程。

Abstract: Machine learning enables the extraction of useful information from large,
diverse datasets. However, despite many successful applications, machine
learning continues to suffer from performance and transparency issues. These
challenges can be partially attributed to the limited use of domain knowledge
by machine learning models. This research proposes using the domain knowledge
represented in conceptual models to improve the preparation of the data used to
train machine learning models. We develop and demonstrate a method, called the
Conceptual Modeling for Machine Learning (CMML), which is comprised of
guidelines for data preparation in machine learning and based on conceptual
modeling constructs and principles. To assess the impact of CMML on machine
learning outcomes, we first applied it to two real-world problems to evaluate
its impact on model performance. We then solicited an assessment by data
scientists on the applicability of the method. These results demonstrate the
value of CMML for improving machine learning outcomes.

</details>


### [148] [A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime](https://arxiv.org/abs/2507.03866)
*Shuning Jiang,Wei-Lun Chao,Daniel Haehn,Hanspeter Pfister,Jian Chen*

Main category: cs.LG

TL;DR: CNN在条形图比率估计能力上优于人类，其偏差仅取决于训练-测试距离。


<details>
  <summary>Details</summary>
Motivation: 量化CNN在图形感知行为中的表现，并与人类进行比较。

Method: 设计数据域采样机制，评估CNN对训练-测试分布差异的敏感性、样本不足时的稳定性及与人类的相对能力。

Result: 分析了1600万次CNN实验和6825次人类实验，发现CNN表现优于人类且偏差简单依赖于训练-测试距离。

Conclusion: CNN在可视化图像解释中表现出简单且高效的行为，其性能可直接由训练-测试距离预测。

Abstract: We present a data-domain sampling regime for quantifying CNNs' graphic
perception behaviors. This regime lets us evaluate CNNs' ratio estimation
ability in bar charts from three perspectives: sensitivity to training-test
distribution discrepancies, stability to limited samples, and relative
expertise to human observers. After analyzing 16 million trials from 800 CNNs
models and 6,825 trials from 113 human participants, we arrived at a simple and
actionable conclusion: CNNs can outperform humans and their biases simply
depend on the training-test distance. We show evidence of this simple, elegant
behavior of the machines when they interpret visualization images. osf.io/gfqc3
provides registration, the code for our sampling regime, and experimental
results.

</details>


### [149] [Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States](https://arxiv.org/abs/2507.03871)
*Karine Karine,Benjamin M. Marlin*

Main category: cs.LG

TL;DR: 本文提出了一种利用大型语言模型（LLM）扩展强化学习（RL）在健康行为干预中的状态空间的方法，以提高策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 健康行为干预（如戒烟和促进体育活动）中，RL方法因数据稀缺问题常受限于少数上下文变量。本文旨在通过自然语言描述和LLM推理扩展状态空间，不降低数据效率。

Method: 利用参与者提供的自然语言描述，结合预训练LLM进行推理，优化RL策略的基础方法。开发了一个模拟环境，通过辅助LLM生成基于潜在状态变量的文本描述。

Result: 实验表明，该方法能显著提升在线策略学习方法的性能。

Conclusion: 通过自然语言和LLM扩展状态空间，为RL在健康行为干预中的应用提供了新思路。

Abstract: The use of reinforcement learning (RL) methods to support health behavior
change via personalized and just-in-time adaptive interventions is of
significant interest to health and behavioral science researchers focused on
problems such as smoking cessation support and physical activity promotion.
However, RL methods are often applied to these domains using a small collection
of context variables to mitigate the significant data scarcity issues that
arise from practical limitations on the design of adaptive intervention trials.
In this paper, we explore an approach to significantly expanding the state
space of an adaptive intervention without impacting data efficiency. The
proposed approach enables intervention participants to provide natural language
descriptions of aspects of their current state. It then leverages inference
with pre-trained large language models (LLMs) to better align the policy of a
base RL method with these state descriptions. To evaluate our method, we
develop a novel physical activity intervention simulation environment that
generates text-based state descriptions conditioned on latent state variables
using an auxiliary LLM. We show that this approach has the potential to
significantly improve the performance of online policy learning methods.

</details>


### [150] [Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.04340)
*Jan Kompatscher,Danqing Shi,Giovanna Varni,Tino Weinkauf,Antti Oulasvirta*

Main category: cs.LG

TL;DR: 论文提出了一种基于交互式可视化的强化学习人类反馈（RLHF）方法，通过分组比较提升数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF依赖两两比较，效率较低，无法充分利用人类的视觉比较能力。

Method: 开发了一个包含探索视图和比较视图的交互式可视化界面，支持用户高效探索行为集合并采用主动学习策略建议比较组。

Result: 在六个模拟机器人任务中，该方法将最终策略回报提升69.34%，且降低了错误率和优化了策略。

Conclusion: 该方法显著提升了RLHF的效果，开源代码支持进一步研究人机对齐。

Abstract: Reinforcement learning from human feedback (RLHF) has emerged as a key
enabling technology for aligning AI behavior with human preferences. The
traditional way to collect data in RLHF is via pairwise comparisons: human
raters are asked to indicate which one of two samples they prefer. We present
an interactive visualization that better exploits the human visual ability to
compare and explore whole groups of samples. The interface is comprised of two
linked views: 1) an exploration view showing a contextual overview of all
sampled behaviors organized in a hierarchical clustering structure; and 2) a
comparison view displaying two selected groups of behaviors for user queries.
Users can efficiently explore large sets of behaviors by iterating between
these two views. Additionally, we devised an active learning approach
suggesting groups for comparison. As shown by our evaluation in six simulated
robotics tasks, our approach increases the final policy returns by 69.34%. It
leads to lower error rates and better policies. We open-source the code that
can be easily integrated into the RLHF training loop, supporting research on
human-AI alignment.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [151] [Disclosing Generative AI Use in Digital Humanities Research](https://arxiv.org/abs/2507.03216)
*Rongqian Ma,Xuhan Zhang,Adrian Wisnicki*

Main category: cs.CY

TL;DR: 数字人文领域学者对生成式AI披露的态度与实践中披露率低的矛盾。


<details>
  <summary>Details</summary>
Motivation: 了解数字人文领域学者如何看待及实践生成式AI在研究中使用的披露。

Method: 通过调查研究收集数字人文学者的观点和实践情况。

Result: 学者们认识到披露重要性，但实际披露率低；对披露范围和方法的看法不一；普遍支持通过机构政策规范披露。

Conclusion: 研究结果为学者、机构领导者和资助者制定有效披露政策提供了实证依据。

Abstract: This survey study investigates how digital humanists perceive and approach
generative AI disclosure in research. The results indicate that while digital
humanities scholars acknowledge the importance of disclosing GenAI use, the
actual rate of disclosure in research practice remains low. Respondents differ
in their views on which activities most require disclosure and on the most
appropriate methods for doing so. Most also believe that safeguards for AI
disclosure should be established through institutional policies rather than
left to individual decisions. The study's findings will offer empirical
guidance to scholars, institutional leaders, funders, and other stakeholders
responsible for shaping effective disclosure policies.

</details>


### [152] [Ethics by Design: A Lifecycle Framework for Trustworthy AI in Medical Imaging From Transparent Data Governance to Clinically Validated Deployment](https://arxiv.org/abs/2507.04249)
*Umer Sadiq Khan,Saif Ur Rehman Khan*

Main category: cs.CY

TL;DR: 该研究探讨了AI在医学影像中五个关键开发阶段的伦理问题，提出了确保数据隐私、公平性、透明度和问责制的原则和方法。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像中的应用引发了从数据收集到部署的全过程伦理问题，需确保系统尊重患者权利并促进公平。

Method: 采用分析方法审查文献、指南和法规，识别AI开发的各个阶段（数据收集、处理、模型训练、评估和部署）的伦理挑战。

Result: 研究发现关键伦理问题包括患者同意、数据匿名化、模型训练中的偏见、透明性和公平性，以及部署时的持续伦理评估。

Conclusion: 伦理原则需系统整合到AI开发的每个阶段，支持创建更透明、公平且符合患者护理和数据控制的AI系统。

Abstract: The integration of artificial intelligence (AI) in medical imaging raises
crucial ethical concerns at every stage of its development, from data
collection to deployment. Addressing these concerns is essential for ensuring
that AI systems are developed and implemented in a manner that respects patient
rights and promotes fairness. This study aims to explore the ethical
implications of AI in medical imaging, focusing on five key stages: data
collection, data processing, model training, model evaluation, and deployment.
The goal is to evaluate how these stages adhere to fundamental ethical
principles, including data privacy, fairness, transparency, accountability, and
autonomy. An analytical approach was employed to examine the ethical challenges
associated with each stage of AI development. We reviewed existing literature,
guidelines, and regulations concerning AI ethics in healthcare and identified
critical ethical issues at each stage. The study outlines specific inquiries
and principles for each phase of AI development. The findings highlight key
ethical issues: ensuring patient consent and anonymization during data
collection, addressing biases in model training, ensuring transparency and
fairness during model evaluation, and the importance of continuous ethical
assessments during deployment. The analysis also emphasizes the impact of
accessibility issues on different stakeholders, including private, public, and
third-party entities. The study concludes that ethical considerations must be
systematically integrated into each stage of AI development in medical imaging.
By adhering to these ethical principles, AI systems can be made more robust,
transparent, and aligned with patient care and data control. We propose
tailored ethical inquiries and strategies to support the creation of ethically
sound AI systems in medical imaging.

</details>


### [153] [Toward Cyclic A.I. Modelling of Self-Regulated Learning: A Case Study with E-Learning Trace Data](https://arxiv.org/abs/2507.02913)
*Andrew Schwabe,Özgür Akgün,Ella Haig*

Main category: cs.CY

TL;DR: 论文探讨了如何在机器学习框架中建模自我调节学习（SRL），通过SRL特征提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然许多电子学习平台声称能提升学生的SRL能力，但SRL的循环和无定向特性对机器学习建模提出了挑战。

Method: 应用SRL特征分析学习数据，以改进SRL活动的建模。

Result: 这些特征提高了预测准确性，并验证了循环建模技术的价值。

Conclusion: 进一步研究循环建模技术对SRL的潜在影响具有重要价值。

Abstract: Many e-learning platforms assert their ability or potential to improve
students' self-regulated learning (SRL), however the cyclical and undirected
nature of SRL theoretical models represent significant challenges for
representation within contemporary machine learning frameworks. We apply
SRL-informed features to trace data in order to advance modelling of students'
SRL activities, to improve predictability and explainability regarding the
causal effects of learning in an eLearning environment. We demonstrate that
these features improve predictive accuracy and validate the value of further
research into cyclic modelling techniques for SRL.

</details>


### [154] [LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop](https://arxiv.org/abs/2507.04295)
*Runcong Zhao,Artem Borov,Jiazheng Li,Yulan He*

Main category: cs.CY

TL;DR: LearnLens是一个基于LLM的系统，生成个性化且符合课程的科学教育反馈，解决传统反馈耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 教师提供有效反馈对学生学习至关重要，但传统方法耗时。LearnLens旨在通过模块化系统解决这一问题。

Method: 系统包括三个模块：错误感知评估、课程基础的生成（使用结构化记忆链）和教师参与的界面。

Result: LearnLens提供了可扩展且高质量的反馈，减少噪音并提升关联性。

Conclusion: 该系统为教师和学生提供了高效的支持，解决了现有系统的关键挑战。

Abstract: Effective feedback is essential for student learning but is time-intensive
for teachers. We present LearnLens, a modular, LLM-based system that generates
personalised, curriculum-aligned feedback in science education. LearnLens
comprises three components: (1) an error-aware assessment module that captures
nuanced reasoning errors; (2) a curriculum-grounded generation module that uses
a structured, topic-linked memory chain rather than traditional
similarity-based retrieval, improving relevance and reducing noise; and (3) an
educator-in-the-loop interface for customisation and oversight. LearnLens
addresses key challenges in existing systems, offering scalable, high-quality
feedback that empowers both teachers and students.

</details>


### [155] [AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments](https://arxiv.org/abs/2507.04352)
*Greg Nyilasy,Harsha Gangadharbatla*

Main category: cs.CY

TL;DR: 该论文研究了企业夸大或淡化AI使用（AI-washing）对消费者态度和购买意向的影响，发现虚假否认比诚实否定引发更负面的道德判断，而虚假夸大则无显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI热度的上升，企业可能为了迎合市场而夸大或淡化其AI使用情况，这种不实行为可能影响消费者信任。

Method: 研究采用2x2实验设计（N=401），探讨虚假夸大和虚假否认如何影响消费者的道德判断和购买意向。

Result: 虚假否认引发更负面的道德判断，而虚假夸大无显著影响，这种效应由感知背叛中介。

Conclusion: 研究揭示了AI-washing对信任的破坏作用，为政策制定者、营销者和研究者提供了伦理启示。

Abstract: As AI hype continues to grow, organizations face pressure to broadcast or
downplay purported AI initiatives - even when contrary to truth. This paper
introduces AI-washing as overstating (deceptive boasting) or understating
(deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401)
examines how these false claims affect consumer attitudes and purchase
intentions. Results reveal a pronounced asymmetry: deceptive denial evokes more
negative moral judgments than honest negation, while deceptive boasting has no
effects. We show that perceived betrayal mediates these outcomes. By clarifying
how AI-washing erodes trust, the study highlights clear ethical implications
for policymakers, marketers, and researchers striving for transparency.

</details>


### [156] [From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems](https://arxiv.org/abs/2507.04996)
*Jiangbo Yu*

Main category: cs.CY

TL;DR: 论文提出‘agentic vehicles’（AgVs）概念，结合AI智能体的高维推理和交互能力，区别于传统自主车辆（AuVs），以填补技术自主性与未来移动系统需求的认知和社会能力鸿沟。


<details>
  <summary>Details</summary>
Motivation: 传统自主车辆的定义未能涵盖如人机交互、目标适应等新兴行为，需扩展为更具认知和社交能力的AgVs。

Method: 提出系统级框架，整合智能体AI、机器人学等多领域进展，突出AgVs的认知和通信层。

Result: AgVs通过高级推理和工具使用，可融入移动生态系统作为交互式智能体。

Conclusion: 开发AgVs面临安全、伦理等挑战，需多领域协作解决。

Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity
to operate according to internal rules without external control. Accordingly,
autonomous vehicles (AuVs) are defined as systems capable of perceiving their
environment and executing preprogrammed tasks independently of external input.
However, both research and real-world deployments increasingly showcase
vehicles that demonstrate behaviors beyond this definition (including the SAE
levels 1 to 6), such as interaction with humans and machines, goal adaptation,
contextual reasoning, external tool use, and long-term planning, particularly
with the integration of large language models (LLMs) and agentic AI systems.
These developments reveal a conceptual gap between technical autonomy and the
broader cognitive and social capabilities needed for future human-centered
mobility systems. To address this, we introduce the concept of agentic vehicles
(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and
interact within complex environments. This paper presents a systems-level
framework to characterize AgVs, focusing on their cognitive and communicative
layers and differentiating them from conventional AuVs. It synthesizes relevant
advances in agentic AI, robotics, multi-agent systems, and human-machine
interaction, and highlights how agentic AI, through high-level reasoning and
tool use, can function not merely as computational tools but as interactive
agents embedded in mobility ecosystems. The paper concludes by identifying key
challenges in the development and governance of AgVs, including safety,
real-time control, public acceptance, ethical alignment, and regulatory
frameworks.

</details>


### [157] [Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good](https://arxiv.org/abs/2507.05030)
*Celeste Campos-Castillo,Xuan Kang,Linnea I. Laestadius*

Main category: cs.CY

TL;DR: 论文摘要探讨了社会学在聊天机器人研究中的滞后性，并提出了四种社会学理论以推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 社会学在聊天机器人研究中落后于其他学科，论文旨在通过社会学理论填补这一空白，提升对人类与聊天机器人互动的理解。

Method: 提出了四种社会学理论（资源替代理论、权力依赖理论、情感控制理论和疾病根本原因理论），以分析聊天机器人使用的驱动因素和开发干预措施。

Result: 这些理论为聊天机器人的使用提供了新的视角，并有助于设计更安全、公平的干预措施，促进社会福祉。

Conclusion: 应用社会学理论可以推动人类与聊天机器人互动的理论研究，并开发更具社会价值的聊天机器人。

Abstract: Recently, research into chatbots (also known as conversational agents, AI
agents, voice assistants), which are computer applications using artificial
intelligence to mimic human-like conversation, has grown sharply. Despite this
growth, sociology lags other disciplines (including computer science, medicine,
psychology, and communication) in publishing about chatbots. We suggest
sociology can advance understanding of human-chatbot interaction and offer four
sociological theories to enhance extant work in this field. The first two
theories (resource substitution theory, power-dependence theory) add new
insights to existing models of the drivers of chatbot use, which overlook
sociological concerns about how social structure (e.g., systemic
discrimination, the uneven distribution of resources within networks) inclines
individuals to use chatbots, including problematic levels of emotional
dependency on chatbots. The second two theories (affect control theory,
fundamental cause of disease theory) help inform the development of
chatbot-driven interventions that minimize safety risks and enhance equity by
leveraging sociological insights into how chatbot outputs could attend to
cultural contexts (e.g., affective norms) to promote wellbeing and enhance
communities (e.g., opportunities for civic participation). We discuss the value
of applying sociological theories for advancing theorizing about human-chatbot
interaction and developing chatbots for social good.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [158] [ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models](https://arxiv.org/abs/2507.02919)
*Dai Li,Linzhuo Li,Huilian Sophie Qiu*

Main category: cs.CL

TL;DR: 研究指出，大型语言模型（LLMs）在模拟人类意见时可能无法准确反映人口层面的观点，存在结构不一致性和少数派意见代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能作为‘硅样本’可靠地模拟人类观点，尤其是与真实调查数据的差异。

Method: 通过向ChatGPT和Meta的Llama模型提问ANES 2020中的问题（如堕胎和非法移民），分析其回答结构。

Result: 发现LLMs存在显著的结构不一致性和少数派意见严重不足现象。

Conclusion: LLMs不适合直接替代人类调查数据，可能导致偏见和政策误导。

Abstract: Large language models (LLMs) in the form of chatbots like ChatGPT and Llama
are increasingly proposed as "silicon samples" for simulating human opinions.
This study examines this notion, arguing that LLMs may misrepresent
population-level opinions. We identify two fundamental challenges: a failure in
structural consistency, where response accuracy doesn't hold across demographic
aggregation levels, and homogenization, an underrepresentation of minority
opinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama
3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized
immigration from the American National Election Studies (ANES) 2020. Our
findings reveal significant structural inconsistencies and severe
homogenization in LLM responses compared to human data. We propose an
"accuracy-optimization hypothesis," suggesting homogenization stems from
prioritizing modal responses. These issues challenge the validity of using
LLMs, especially chatbots AI, as direct substitutes for human survey data,
potentially reinforcing stereotypes and misinforming policy.

</details>


### [159] [Graph Repairs with Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.03410)
*Hrishikesh Terdalkar,Angela Bonifati,Andrea Mauri*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）自动化修复属性图中的错误，并与传统方法对比，展示了LLMs的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 属性图中存在的错误（如不一致、缺失数据或模式违规）需要高效修复方法，传统方法在适应性和大规模图上表现不足，LLMs提供了新的解决方案。

Method: 评估了六种开源LLM在修复属性图中的性能，分析了修复质量、计算成本和模型特性。

Result: 实验表明LLMs能有效检测和纠正错误，但准确性和效率因模型而异。

Conclusion: LLMs在属性图修复中有潜力，但在可扩展性和可解释性方面仍需进一步研究。

Abstract: Property graphs are widely used in domains such as healthcare, finance, and
social networks, but they often contain errors due to inconsistencies, missing
data, or schema violations. Traditional rule-based and heuristic-driven graph
repair methods are limited in their adaptability as they need to be tailored
for each dataset. On the other hand, interactive human-in-the-loop approaches
may become infeasible when dealing with large graphs, as the cost--both in
terms of time and effort--of involving users becomes too high. Recent
advancements in Large Language Models (LLMs) present new opportunities for
automated graph repair by leveraging contextual reasoning and their access to
real-world knowledge. We evaluate the effectiveness of six open-source LLMs in
repairing property graphs. We assess repair quality, computational cost, and
model-specific performance. Our experiments show that LLMs have the potential
to detect and correct errors, with varying degrees of accuracy and efficiency.
We discuss the strengths, limitations, and challenges of LLM-driven graph
repair and outline future research directions for improving scalability and
interpretability.

</details>


### [160] [ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation](https://arxiv.org/abs/2507.04952)
*Chenchen Zhang,Yuhang Li,Can Xu,Jiaheng Liu,Ao Liu,Shihui Hu,Dengpeng Wu,Guanhua Huang,Kejiao Li,Qi Yi,Ruibin Xiong,Haotian Zhu,Yuanxing Zhang,Yuhao Jiang,Yue Zhang,Zenan Xu,Bohui Zhai,Guoxiang He,Hebin Li,Jie Zhao,Le Zhang,Lingyun Tan,Pengyu Guo,Xianshu Pang,Yang Ruan,Zhifeng Zhang,Zhonghu Wang,Ziyan Xu,Zuopu Yin,Wiggin Zhou,Chayse Zhou,Fengzong Lian*

Main category: cs.CL

TL;DR: 该研究为解决LLMs在动态交互视觉生成中的评估瓶颈，提出了ArtifactsBench基准测试框架，通过多模态评估，实现了高效自动化且与人类专家高度一致的评分。


<details>
  <summary>Details</summary>
Motivation: 由于现有基准测试仅关注算法正确性，忽视视觉保真度和交互完整性，研究团队试图填补这一评估空白。

Method: ArtifactsBench通过程序化渲染生成的视觉代码，动态捕捉行为，并利用多模态LLM结合任务清单进行评分。

Result: 该框架在1,825个任务上测试了30多个LLMs，自动化评分与人类偏好一致性达94.4%，且开源资源为社区提供工具。

Conclusion: ArtifactsBench是首个能大规模可靠评估生成模型人类感知质量的框架，显示通用模型常优于专业模型。

Abstract: The generative capabilities of Large Language Models (LLMs) are rapidly
expanding from static code to dynamic, interactive visual artifacts. This
progress is bottlenecked by a critical evaluation gap: established benchmarks
focus on algorithmic correctness and are blind to the visual fidelity and
interactive integrity that define modern user experiences. To bridge this gap,
we introduce ArtifactsBench, a new benchmark and paradigm for the automated,
multimodal evaluation of visual code generation. Our framework programmatically
renders each generated artifact and captures its dynamic behavior through
temporal screenshots. This visual evidence, alongside the source code, is then
assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a
fine-grained, per-task checklist to ensure holistic and reproducible scoring.
We construct a new benchmark of 1,825 diverse tasks and evaluate over 30
leading LLMs. Our automated evaluation achieves a striking 94.4% ranking
consistency with WebDev Arena, the gold-standard for human preference in web
development, and over 90% pairwise agreement with human experts. This
establishes ArtifactsBench as the first framework to reliably automate the
assessment of human-perceived quality at scale. Our analysis provides a
high-resolution map of the current SOTA, revealing that generalist models often
outperform domain-specific ones. We open-source ArtifactsBench, including the
benchmark, evaluation harness, and baseline results at
https://artifactsbenchmark.github.io/, to provide the community with a scalable
and accurate tool to accelerate the development of user-centric generative
models.

</details>


### [161] [Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria](https://arxiv.org/abs/2507.02950)
*Keita Kiuchi,Yoshikazu Fujimoto,Hideyuki Goto,Tomonori Hosokawa,Makoto Nishimura,Yosuke Sato,Izumi Sezai*

Main category: cs.CL

TL;DR: 该研究首次全面评估了大型语言模型在日本语治疗环境中三种咨询角色的表现，发现SMDP显著提升咨询AI性能，评估AI与人类评分者在某些指标上表现相当，但仍存在偏见和局限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估AI在非英语（日语）治疗环境中的表现，为开发文化敏感的AI心理健康工具提供基准和改进方向。

Method: 通过人类专家（n=15）使用MITI编码手册4.2.1评估AI生成的对话，比较不同AI系统（如GPT-4-turbo、Claude-3-Opus等）的性能。

Result: SMDP显著提升咨询AI表现；评估AI在部分指标上与人类评分者相当，但存在系统高估和模型偏见；客户AI模拟情感范围有限，需提高真实性。

Conclusion: 研究为AI辅助非英语咨询设立了基准，并指出通过提示工程、微调等方法改进的方向，对开发文化敏感的AI心理健康工具有重要意义。

Abstract: This study provides the first comprehensive evaluation of large language
model (LLM) performance across three counseling roles in Japanese-language
therapeutic contexts. We simultaneously assessed counselor artificial
intelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured
Multi-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,
and evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human
experts (n = 15) with extensive counseling experience evaluated AI-generated
dialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding
Manual 4.2.1.
  Notably, SMDP implementation significantly enhanced counselor AI performance
across all MITI global ratings compared with zeroshot prompting, with no
significant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed
comparable performance to human raters for Cultivating Change Talk but
systematically overestimated Softening Sustain Talk and the overall quality
metrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3
focused on technical proficiency, and Sonnet prioritized emotional expression.
Client AI simulations exhibited a limited emotional range and unnaturally high
compliance, indicating the need for enhanced realism.
  These findings establish benchmarks for AI-assisted counseling in non-English
contexts and identify critical areas for improvement through advanced prompt
engineering, retrieval-augmented generation, and targeted fine-tuning, with
important implications for developing culturally sensitive AI mental health
tools.

</details>


### [162] [Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents](https://arxiv.org/abs/2507.04009)
*Ziyang Miao,Qiyu Sun,Jingyuan Wang,Yuchen Gong,Yaowei Zheng,Shiqi Li,Richong Zhang*

Main category: cs.CL

TL;DR: Easy Dataset是一个通过GUI从非结构化文档中生成微调数据的统一框架，显著提升了LLM在特定领域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在特定领域适应中高质量数据稀缺的问题。

Method: 提出Easy Dataset框架，结合用户配置的文本提取和分块策略，以及人物驱动提示方法生成QA对，并通过人机交互界面确保数据质量。

Result: 实验显示，基于合成数据微调的LLM在金融问答任务中性能显著提升，同时保留通用知识。

Conclusion: Easy Dataset有效解决了领域数据稀缺问题，工具已开源并获得广泛认可。

Abstract: Large language models (LLMs) have shown impressive performance on
general-purpose tasks, yet adapting them to specific domains remains
challenging due to the scarcity of high-quality domain data. Existing data
synthesis tools often struggle to extract reliable fine-tuning data from
heterogeneous documents effectively. To address this limitation, we propose
Easy Dataset, a unified framework for synthesizing fine-tuning data from
unstructured documents via an intuitive graphical user interface (GUI).
Specifically, Easy Dataset allows users to easily configure text extraction
models and chunking strategies to transform raw documents into coherent text
chunks. It then leverages a persona-driven prompting approach to generate
diverse question-answer pairs using public-available LLMs. Throughout the
pipeline, a human-in-the-loop visual interface facilitates the review and
refinement of intermediate outputs to ensure data quality. Experiments on a
financial question-answering task show that fine-tuning LLMs on the synthesized
dataset significantly improves domain-specific performance while preserving
general knowledge. The source code and installable package are available at
https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub
stars.

</details>


### [163] [SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding](https://arxiv.org/abs/2507.04189)
*Runcong Zhao,Qinglin Zhu,Hainiu Xu,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: SymbolicThought是一个结合大型语言模型（LLM）和符号推理的框架，用于构建可编辑的角色关系图，优化标注准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决手动标注耗时且覆盖率低，以及LLM输出存在幻觉或逻辑不一致的问题。

Method: 结合LLM提取和符号推理，通过七种逻辑约束优化关系图，并提供交互式界面实时验证和解决冲突。

Result: 实验表明SymbolicThought显著提高标注准确性和一致性，同时减少时间成本。

Conclusion: 该框架为叙事理解、可解释AI和LLM评估提供了实用工具。

Abstract: Understanding character relationships is essential for interpreting complex
narratives and conducting socially grounded AI research. However, manual
annotation is time-consuming and low in coverage, while large language models
(LLMs) often produce hallucinated or logically inconsistent outputs. We present
SymbolicThought, a human-in-the-loop framework that combines LLM-based
extraction with symbolic reasoning. The system constructs editable character
relationship graphs, refines them using seven types of logical constraints, and
enables real-time validation and conflict resolution through an interactive
interface. To support logical supervision and explainable social analysis, we
release a dataset of 160 interpersonal relationships with corresponding logical
structures. Experiments show that SymbolicThought improves annotation accuracy
and consistency while significantly reducing time cost, offering a practical
tool for narrative understanding, explainable AI, and LLM evaluation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [164] [Image-driven Robot Drawing with Rapid Lognormal Movements](https://arxiv.org/abs/2507.03166)
*Daniel Berio,Guillaume Clivaz,Michael Stroh,Oliver Deussen,Réjean Plamondon,Sylvain Calinon,Frederic Fol Leymarie*

Main category: cs.RO

TL;DR: 该论文提出了一种结合图像空间成本函数和人类手势模型的方法，用于生成机器人绘画的自然路径。


<details>
  <summary>Details</summary>
Motivation: 现有大型图像生成和视觉模型忽略了人类绘画/书写时的物理特征，影响了视觉美感和人机协作的直观性。

Method: 采用sigma-lognormal人类手势模型，结合可微分矢量图形渲染器（DiffVG），通过梯度优化生成自然运动轨迹。

Result: 方法成功生成了机器人可执行的轨迹，应用于合成涂鸦和图像抽象任务。

Conclusion: 该方法有效提升了机器人绘画的美观性和人机协作的自然性。

Abstract: Large image generation and vision models, combined with differentiable
rendering technologies, have become powerful tools for generating paths that
can be drawn or painted by a robot. However, these tools often overlook the
intrinsic physicality of the human drawing/writing act, which is usually
executed with skillful hand/arm gestures. Taking this into account is important
for the visual aesthetics of the results and for the development of closer and
more intuitive artist-robot collaboration scenarios. We present a method that
bridges this gap by enabling gradient-based optimization of natural human-like
motions guided by cost functions defined in image space. To this end, we use
the sigma-lognormal model of human hand/arm movements, with an adaptation that
enables its use in conjunction with a differentiable vector graphics (DiffVG)
renderer. We demonstrate how this pipeline can be used to generate feasible
trajectories for a robot by combining image-driven objectives with a
minimum-time smoothing criterion. We demonstrate applications with generation
and robotic reproduction of synthetic graffiti as well as image abstraction.

</details>


### [165] [NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving](https://arxiv.org/abs/2507.05227)
*Qucheng Peng,Chen Bai,Guoxiang Zhang,Bo Xu,Xiaotong Liu,Xiaoyin Zheng,Chen Chen,Cheng Lu*

Main category: cs.RO

TL;DR: 论文提出NavigScene数据集及三种方法，通过融入导航上下文提升自动驾驶系统的感知、预测和规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统在全局导航信息整合上的不足，模拟人类驾驶环境以提升性能。

Method: 1) 导航引导推理；2) 导航引导偏好优化；3) 导航引导视觉-语言-动作模型。

Result: 实验表明，方法显著提升了任务性能及泛化能力。

Conclusion: 为复杂环境下的自动驾驶系统提供了更可靠的解决方案。

Abstract: Autonomous driving systems have made significant advances in Q&A, perception,
prediction, and planning based on local visual information, yet they struggle
to incorporate broader navigational context that human drivers routinely
utilize. We address this critical gap between local sensor data and global
navigation information by proposing NavigScene, an auxiliary navigation-guided
natural language dataset that simulates a human-like driving environment within
autonomous driving systems. Moreover, we develop three complementary paradigms
to leverage NavigScene: (1) Navigation-guided Reasoning, which enhances
vision-language models by incorporating navigation context into the prompting
approach; (2) Navigation-guided Preference Optimization, a reinforcement
learning method that extends Direct Preference Optimization to improve
vision-language model responses by establishing preferences for
navigation-relevant summarized information; and (3) Navigation-guided
Vision-Language-Action model, which integrates navigation guidance and
vision-language models with conventional driving models through feature fusion.
Extensive experiments demonstrate that our approaches significantly improve
performance across perception, prediction, planning, and question-answering
tasks by enabling reasoning capabilities beyond visual range and improving
generalization to diverse driving scenarios. This work represents a significant
step toward more comprehensive autonomous driving systems capable of navigating
complex, unfamiliar environments with greater reliability and safety.

</details>


### [166] [Personalised Explanations in Long-term Human-Robot Interactions](https://arxiv.org/abs/2507.03049)
*Ferran Gebellí,Anaís Garrell,Jan-Gerrit Habekost,Séverin Lemaignan,Stefan Wermter,Raquel Ros*

Main category: cs.RO

TL;DR: 提出了一个基于用户知识记忆模型的XHRI框架，通过适应解释的详细程度来提升人机交互的理解效果，并在两种场景下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决人机交互中解释的个性化需求，提升用户对机器人行为的理解和可用性。

Method: 开发了一个更新和检索用户知识记忆模型的框架，并评估了三种基于大型语言模型的架构。

Result: 实验表明，两阶段架构（先生成解释再个性化）能有效减少解释的详细程度。

Conclusion: 该框架通过个性化解释提高了人机交互的效果，未来可拓展至更多场景。

Abstract: In the field of Human-Robot Interaction (HRI), a fundamental challenge is to
facilitate human understanding of robots. The emerging domain of eXplainable
HRI (XHRI) investigates methods to generate explanations and evaluate their
impact on human-robot interactions. Previous works have highlighted the need to
personalise the level of detail of these explanations to enhance usability and
comprehension. Our paper presents a framework designed to update and retrieve
user knowledge-memory models, allowing for adapting the explanations' level of
detail while referencing previously acquired concepts. Three architectures
based on our proposed framework that use Large Language Models (LLMs) are
evaluated in two distinct scenarios: a hospital patrolling robot and a kitchen
assistant robot. Experimental results demonstrate that a two-stage
architecture, which first generates an explanation and then personalises it, is
the framework architecture that effectively reduces the level of detail only
when there is related user knowledge.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [167] [RVISmith: Fuzzing Compilers for RVV Intrinsics](https://arxiv.org/abs/2507.03773)
*Yibo He,Cunjian Huang,Xianmiao Qu,Hongdeng Chen,Wei Yang,Tao Xie*

Main category: cs.CR

TL;DR: RVISmith是一个针对SIMD intrinsics编译器bug的随机模糊测试工具，能高效检测并报告编译器中的未知问题。


<details>
  <summary>Details</summary>
Motivation: 编译器在SIMD intrinsics处理中的bug可能导致安全风险，现有自动向量化技术受限，需手动干预。

Method: 设计RVISmith生成包含RVV intrinsics的C程序，通过高覆盖率、多样化序列和无未定义行为等特性检测bug。

Result: 实验显示，RVISmith在覆盖率上远超现有工具，并检测出13个未知bug，其中10个已确认，3个已修复。

Conclusion: RVISmith能有效提升编译器对SIMD intrinsics的支持质量，检测潜在问题。

Abstract: Modern processors are equipped with single instruction multiple data (SIMD)
instructions for fine-grained data parallelism. Compiler auto-vectorization
techniques that target SIMD instructions face performance limitations due to
insufficient information available at compile time, requiring programmers to
manually manipulate SIMD instructions. SIMD intrinsics, a type of built-in
function provided by modern compilers, enable programmers to manipulate SIMD
instructions within high-level programming languages. Bugs in compilers for
SIMD intrinsics can introduce potential threats to software security, producing
unintended calculation results, data loss, program crashes, etc.
  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a
randomized fuzzer that generates well-defined C programs that include various
invocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design
RVISmith to achieve the following objectives: (i) achieving high intrinsic
coverage, (ii) improving sequence variety, and (iii) without known undefined
behaviors. We implement RVISmith based on the ratified RVV intrinsic
specification and evaluate our approach with three modern compilers: GCC, LLVM,
and XuanTie. Experimental results show that RVISmith achieves 11.5 times higher
intrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By
differential testing that compares results across different compilers,
optimizations, and equivalent programs, we detect and report 13 previously
unknown bugs of the three compilers under test to date. Of these bugs, 10 are
confirmed and another 3 are fixed by the compiler developers.

</details>


### [168] [Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills](https://arxiv.org/abs/2507.03694)
*Jovonni L. PHarr*

Main category: cs.CR

TL;DR: 提出了一种新型的去中心化数字遗产规划协议，结合分布式计算和密码学，通过现代链间通信连接异构链，实现安全、私密的数字资产分配。


<details>
  <summary>Details</summary>
Motivation: 解决传统数字遗产规划中的隐私和安全问题，利用区块链技术革新法律和个人领域。

Method: 开发了一层协议，使用现代密码学原语和链间通信，构建异构智能合约作为入口点，并引入用户交互模型。

Result: 实现了无需转移资产的公平、安全数字资产分配，提升了隐私性和用户友好性。

Conclusion: 该协议通过密码经济网络革新数字遗产规划，展示了区块链技术在传统领域中的潜力。

Abstract: This work presents a novel decentralized protocol for digital estate planning
that integrates advances distributed computing, and cryptography. The original
proof-of-concept was constructed using purely solidity contracts. Since then,
we have enhanced the implementation into a layer-1 protocol that uses modern
interchain communication to connect several heterogeneous chain types. A key
contribution of this research is the implementation of several modern
cryptographic primitives to support various forms of claims for information
validation. These primitives introduce an unmatched level of privacy to the
process of digital inheritance. We also demonstrate on a set of heterogeneous
smart contracts, following the same spec, on each chain to serve as entry
points, gateways, or bridge contracts that are invoked via a path from the will
module on our protocol, to the contract. This ensures a fair and secure
distribution of digital assets in accordance with the wishes of the decedent
without the requirement of moving their funds. This research further extends
its innovations with a user interaction model, featuring a check-in system and
account abstraction process, which enhances flexibility and user-friendliness
without compromising on security. By developing a dedicated permissionless
blockchain that is secured by a network of validators, and interchain relayers,
the proposed protocol signifies a transformation in the digital estate planning
industry and illustrates the potential of blockchain technology in
revolutionizing traditional legal and personal spheres. Implementing a
cryptoeconomic network at the core of inheritance planning allows for unique
incentive compatible economic mechanisms to be constructed.

</details>


### [169] [Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security](https://arxiv.org/abs/2507.03136)
*Ricardo Queiroz de Araujo Fernandes,Anderson Santos,Daniel Maier de Carvalho,André Luiz Bandeira Molina*

Main category: cs.CR

TL;DR: 论文通过理论物理中的全息原理与数字安全中的攻击表面类比，探讨了复杂基础设施如何将漏洞投射到外部接口，并提出了相关的网络安全实践策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过物理学的全息原理为数字安全提供新的视角，揭示内部架构安全状态与外部攻击表面的关系。

Method: 方法包括理论类比（如黑洞事件视界与攻击表面的平行关系）和实践策略（如攻击表面减少、持续扫描工具使用和零信任架构实施）。

Result: 研究成果表明，边界防御对保护庞大内部基础设施至关重要，类比框架为数字安全提供了新的理论支持。

Conclusion: 结论是，通过全息原理的类比，可以更好地理解和优化数字安全的防御策略，强调了边界防御的重要性。

Abstract: This article presents an in-depth exploration of the analogy between the
Holographic Principle in theoretical physics and cyber attack surfaces in
digital security. Building on concepts such as black hole entropy and AdS/CFT
duality, it highlights how complex infrastructures project their
vulnerabilities onto their external interfaces. The paper draws a parallel
between a black hole's event horizon, which encodes all internal information,
and the attack surface, which reflects the internal architecture's security
posture. Additionally, the article outlines how this conceptual framework can
guide cybersecurity practices, emphasizing strategies such as attack surface
reduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,
and the implementation of Zero Trust Architecture. This analogy not only
provides a unique perspective on digital security but also underscores the
critical importance of boundary-level defenses in protecting vast internal
infrastructures.

</details>


### [170] [MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation](https://arxiv.org/abs/2507.03993)
*Dipo Dunsin,Mohamed Chahine Ghanem,Eduardo Almeida Palmieri*

Main category: cs.CR

TL;DR: 提出一种高质量恶意软件数据集生成方法，填补现有数据集的不足，支持机器学习和代理AI训练。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多样性和复杂性，无法满足高级分析需求。

Method: 在虚拟环境中自动执行恶意软件，结合动态监控工具生成多样化数据集。

Result: 数据集包含多种恶意软件家族和操作系统，支持RL检测与响应策略。

Conclusion: 该数据集推动自适应网络安全防御和数字取证研究，具有广泛应用潜力。

Abstract: This paper addresses the critical need for high-quality malware datasets that
support advanced analysis techniques, particularly machine learning and agentic
AI frameworks. Existing datasets often lack diversity, comprehensive labelling,
and the complexity necessary for effective machine learning and agent-based AI
training. To fill this gap, we developed a systematic approach for generating a
dataset that combines automated malware execution in controlled virtual
environments with dynamic monitoring tools. The resulting dataset comprises
clean and infected memory snapshots across multiple malware families and
operating systems, capturing detailed behavioural and environmental features.
Key design decisions include applying ethical and legal compliance, thorough
validation using both automated and manual methods, and comprehensive
documentation to ensure replicability and integrity. The dataset's distinctive
features enable modelling system states and transitions, facilitating RL-based
malware detection and response strategies. This resource is significant for
advancing adaptive cybersecurity defences and digital forensic research. Its
scope supports diverse malware scenarios and offers potential for broader
applications in incident response and automated threat mitigation.

</details>


### [171] [Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions](https://arxiv.org/abs/2507.04752)
*Shuo Yang,Xinran Zheng,Xinchen Zhang,Jinfeng Xu,Jinze Li,Donglin Xie,Weicai Long,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: 本文探讨了大型语言模型（LLMs）在提升网络入侵检测系统（NIDS）方面的潜力，分析了当前挑战、方法及未来机遇。


<details>
  <summary>Details</summary>
Motivation: 现有智能NIDS缺乏上下文感知和可解释性，而LLMs能够处理结构化与非结构化安全数据，实现更深层次的推理与决策。

Method: 提出LLM驱动的认知NIDS框架，包括LLM作为处理器、检测器和解释器，并建议LLM中心控制器协调工作流。

Result: LLMs能显著提升NIDS的上下文推理、可解释性和自动化响应能力。

Conclusion: LLMs为下一代网络安全系统提供了可靠、自适应和可解释的解决方案，具有广阔的发展前景。

Abstract: Large Language Models (LLMs) have revolutionized various fields with their
exceptional capabilities in understanding, processing, and generating
human-like text. This paper investigates the potential of LLMs in advancing
Network Intrusion Detection Systems (NIDS), analyzing current challenges,
methodologies, and future opportunities. It begins by establishing a
foundational understanding of NIDS and LLMs, exploring the enabling
technologies that bridge the gap between intelligent and cognitive systems in
AI-driven NIDS. While Intelligent NIDS leverage machine learning and deep
learning to detect threats based on learned patterns, they often lack
contextual awareness and explainability. In contrast, Cognitive NIDS integrate
LLMs to process both structured and unstructured security data, enabling deeper
contextual reasoning, explainable decision-making, and automated response for
intrusion behaviors. Practical implementations are then detailed, highlighting
LLMs as processors, detectors, and explainers within a comprehensive AI-driven
NIDS pipeline. Furthermore, the concept of an LLM-centered Controller is
proposed, emphasizing its potential to coordinate intrusion detection
workflows, optimizing tool collaboration and system performance. Finally, this
paper identifies critical challenges and opportunities, aiming to foster
innovation in developing reliable, adaptive, and explainable NIDS. By
presenting the transformative potential of LLMs, this paper seeks to inspire
advancement in next-generation network security systems.

</details>


### [172] [Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench](https://arxiv.org/abs/2507.02976)
*Amirali Sajadi,Kostadin Damevski,Preetha Chatterjee*

Main category: cs.CR

TL;DR: 研究发现大型语言模型（LLM）生成的代码修补程序在真实开发环境中引入的安全漏洞比开发者多9倍，且代理框架在高自主权时也易产生漏洞。


<details>
  <summary>Details</summary>
Motivation: 探究LLM及其代理框架在真实软件开发任务中的安全性表现，填补现有研究对现实环境评估的空白。

Method: 基于SWE-bench数据集的20,000+问题，分析LLM（Llama 3.3）和开发者编写的修补程序，并评估三种代理框架（OpenHands等）的生成内容。

Result: LLM修补程序引入新漏洞的概率是开发者的9倍，代理框架在高自主权时易出错；漏洞更易出现在涉及多文件、多代码行或信息不完整的Issue中。

Conclusion: 上下文因素对生成代码的安全性至关重要，需开发结合代码和任务信息的风险评估方法以补充现有工具。

Abstract: Large Language Models (LLMs) and their agentic frameworks are increasingly
adopted to automate software development tasks such as issue resolution and
program repair. While prior work has identified security risks in LLM-generated
code, most evaluations have focused on synthetic or isolated settings, leaving
open questions about the security of these systems in real-world development
contexts. In this study, we present the first large-scale security analysis of
LLM-generated patches using 20,000+ issues from the SWE-bench dataset. We
evaluate patches produced by a standalone LLM (Llama 3.3) and compare them to
developer-written patches. We also assess the security of patches generated by
three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)
on a subset of our data. Finally, we analyze a wide range of code, issue, and
project-level factors to understand the conditions under which LLMs and agents
are most likely to generate insecure code. Our findings reveal that the
standalone LLM introduces nearly 9x more new vulnerabilities than developers,
with many of these exhibiting unique patterns not found in developers' code.
Agentic workflows also generate a significant number of vulnerabilities,
particularly when granting LLMs more autonomy, potentially increasing the
likelihood of misinterpreting project context or task requirements. We find
that vulnerabilities are more likely to occur in LLM patches associated with a
higher number of files, more lines of generated code, and GitHub issues that
lack specific code snippets or information about the expected code behavior and
steps to reproduce. These results suggest that contextual factors play a
critical role in the security of the generated code and point toward the need
for proactive risk assessment methods that account for both code and
issue-level information to complement existing vulnerability detection tools.

</details>


### [173] [Securing Mixed Rust with Hardware Capabilities](https://arxiv.org/abs/2507.03344)
*Jason Zhijingcheng Yu,Fangqi Han,Kaustab Choudhury,Trevor E. Carlson,Prateek Saxena*

Main category: cs.CR

TL;DR: CapsLock是一种运行时安全机制，通过能力基硬件抽象检测混合Rust代码中的原则违反，提供跨语言内存安全保护。


<details>
  <summary>Details</summary>
Motivation: Rust项目常使用不安全代码、FFI和内联汇编，编译器无法静态检查这些混合代码，导致安全漏洞。

Method: 提出CapsLock机制，基于能力基硬件设计，实现revoke-on-use抽象，访问内存时自动撤销相关能力，无需显式无效化。

Result: 原型在QEMU上实现，兼容性高（99.7%测试通过），检测到8个未知漏洞。

Conclusion: CapsLock首次实现了对Rust原则的跨语言运行时安全保护，适用于混合代码环境。

Abstract: The Rust programming language enforces three basic Rust principles, namely
ownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security
bugs such as memory safety violations and data races. However, Rust projects
often have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign
Function Interfaces), and inline assembly for low-level control. The Rust
compiler is unable to statically enforce Rust principles in mixed Rust code
which can lead to many security vulnerabilities. In this paper, we propose
CapsLock, a security enforcement mechanism that can run at the level of machine
code and detect Rust principle violations at run-time in mixed code. CapsLock
is kept simple enough to be implemented into recent capability-based hardware
abstractions that provide low-cost spatial memory safety. CapsLock introduces a
novel revoke-on-use abstraction for capability-based designs, wherein accessing
a memory object via a capability implicitly invalidates certain other
capabilities pointing to it, thereby also providing temporal memory safety
automatically, without requiring software to explicitly specify such
invalidation. Thus, CapsLock is the first mechanism capable of providing
cross-language enforcement of Rust principles. We implemented a prototype of
CapsLock on QEMU. Evaluation results show that CapsLock is highly compatible
with existing Rust code (passing 99.7% of the built-in test cases of the 100
most popular crates) and flags Rust principle violations in real-world Rust
projects that use FFI or inline assembly. We discovered 8 previously unknown
bugs in such crates in our experiments.

</details>


### [174] [Novel Blockchain-based Protocols for Electronic Voting and Auctions](https://arxiv.org/abs/2507.03258)
*Zhaorun Lin*

Main category: cs.CR

TL;DR: 该论文提出了一种基于区块链的高效、安全、隐私保护的电子投票协议（Blind Vote）和私密拍卖算法，显著降低了Gas消耗，同时确保了去中心化和不可篡改性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进现有区块链协议的安全性与效率，特别是在电子投票和拍卖领域，以实现更高的隐私保护和成本效益。

Method: 利用算法和密码学工具（如Chaum盲签名）在以太坊智能合约上实现电子投票协议（Blind Vote）和私密拍卖算法，强调去中心化和Gas效率。

Result: Blind Vote比Tornado Vote等现有方案更省Gas，同时保持相同的安全性；拍卖算法在保护隐私的同时防止串通和篡改。

Conclusion: 提出的协议在安全性、隐私性和效率上优于现有方案，为去中心化应用提供了更优解决方案。

Abstract: Programmable blockchains have long been a hot research topic given their
tremendous use in decentralized applications. Smart contracts, using
blockchains as their underlying technology, inherit the desired properties such
as verifiability, immutability, and transparency, which make it a great suit in
trustless environments.
  In this thesis, we consider several decentralized protocols to be built on
blockchains, specifically using smart contracts on Ethereum. We used
algorithmic and cryptographic tools in our implementations to further improve
the level of security and efficiency beyond the state-of-the-art works. We
proposed a new approach called Blind Vote, which is an untraceable, secure,
efficient, secrecy-preserving, and fully on-chain electronic voting protocol
based on the well-known concept of Chaum's blind signatures. We illustrate that
our approach achieves the same security guarantees as previous methods such as
Tornado Vote [1], while consuming significantly less gas. Thus, we provide a
cheaper and considerably more gas-efficient alternative for anonymous
blockchain-based voting. On the other hand, we propose a new family of
algorithms for private, trustless auctions that protect bidder identities and
bid values while remaining practical for smart contract execution. We ensure
trustlessness by running the auction logic in a smart contract, thereby
eliminating reliance on any single trusted party. This approach prevents bid
tampering, front-running, and collusion by enforcing immutability and
decentralized verification of bids. The resulting protocol uniquely combines
efficiency, trustlessness, and enduring bid privacy, offering a scalable and
secure solution for blockchain-based marketplaces and other decentralized
applications.

</details>


### [175] [Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG](https://arxiv.org/abs/2507.04055)
*Yufan Chen,Daoyuan Wu,Juantao Zhong,Zicheng Zhang,Debin Gao,Shuai Wang,Yingjiu Li,Ning Liu*

Main category: cs.CR

TL;DR: 研究探讨了利用传统二进制字符串特征在LLM和RAG时代进行恶意软件家族分类的可行性。


<details>
  <summary>Details</summary>
Motivation: 恶意软件家族分类（MFC）能自动化标记样本并理解大规模恶意分析平台数据，传统方法在新技术下的适用性值得研究。

Method: 通过家族特定字符串（FSS）特征，类似RAG的方式，构建评估框架分析25百万字符串，研究67家族的4,347样本。

Result: 详细消融实验评估了四大模块中不同设计选择的影响。

Conclusion: 传统字符串特征在新技术背景下仍有潜力，但需进一步优化设计。

Abstract: Malware Family Classification (MFC) aims to identify the fine-grained family
(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in
contrast to malware detection or sample classification that predicts only an
Yes/No. Accurate family identification can greatly facilitate automated sample
labeling and understanding on crowdsourced malware analysis platforms such as
VirusTotal and MalwareBazaar, which generate vast amounts of data daily. In
this paper, we explore and assess the feasibility of using traditional binary
string features for MFC in the new era of large language models (LLMs) and
Retrieval-Augmented Generation (RAG). Specifically, we investigate how
Family-Specific String (FSS) features could be utilized in a manner similar to
RAG to facilitate MFC. To this end, we develop a curated evaluation framework
covering 4,347 samples from 67 malware families, extract and analyze over 25
million strings, and conduct detailed ablation studies to assess the impact of
different design choices in four major modules.

</details>


### [176] [BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2507.04903)
*Thinh Dao,Dung Thuy Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.CR

TL;DR: 本文介绍了BackFed，一个标准化联邦学习中后门攻击与防御评估的基准测试套件，旨在解决现有研究中实验设置不一致等问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有联邦学习系统中后门攻击与防御研究的实验设置不一致、实现错误和假设不现实，阻碍了公平比较和有效结论的得出，因此需要一个标准化、高效的评估框架。

Method: 提出BackFed基准套件，采用多进程实现加速实验，模块化设计支持新方法的无缝集成，并通过标准化评估流程进行大规模实验。

Result: 通过BackFed对计算机视觉和自然语言处理任务中的代表性攻击与防御进行大规模研究，揭示了其在实际条件下的未知局限性和失败模式。

Conclusion: BackFed为研究人员提供了一个可靠的环境来评估新方法，并为联邦学习系统的安全性提升提供了实用指导。

Abstract: Federated Learning (FL) systems are vulnerable to backdoor attacks, where
adversaries train their local models on poisoned data and submit poisoned model
updates to compromise the global model. Despite numerous proposed attacks and
defenses, divergent experimental settings, implementation errors, and
unrealistic assumptions hinder fair comparisons and valid conclusions about
their effectiveness in real-world scenarios. To address this, we introduce
BackFed - a comprehensive benchmark suite designed to standardize, streamline,
and reliably evaluate backdoor attacks and defenses in FL, with a focus on
practical constraints. Our benchmark offers key advantages through its
multi-processing implementation that significantly accelerates experimentation
and the modular design that enables seamless integration of new methods via
well-defined APIs. With a standardized evaluation pipeline, we envision BackFed
as a plug-and-play environment for researchers to comprehensively and reliably
evaluate new attacks and defenses. Using BackFed, we conduct large-scale
studies of representative backdoor attacks and defenses across both Computer
Vision and Natural Language Processing tasks with diverse model architectures
and experimental settings. Our experiments critically assess the performance of
proposed attacks and defenses, revealing unknown limitations and modes of
failures under practical conditions. These empirical insights provide valuable
guidance for the development of new methods and for enhancing the security of
FL systems. Our framework is openly available at
https://github.com/thinh-dao/BackFed.

</details>


### [177] [Bullshark on Narwhal: Implementation-level Workflow Analysis of Round-based DAG Consensus in Theory and Practice](https://arxiv.org/abs/2507.04956)
*Yusei Tanaka*

Main category: cs.CR

TL;DR: Bullshark是一种基于DAG的BFT协议，优化性能达29.7万TPS，2秒延迟，结合Narwhal内存池，未来研究将提升拜占庭容错性能。


<details>
  <summary>Details</summary>
Motivation: 利用Round-based DAG提升BFT共识性能，填补理论协议与实际性能评估之间的差距。

Method: 分析Bullshark与Narwhal的工作流程，从交易提交到区块链确认，逐层分解功能与交互。

Result: 实现29.7万TPS和2秒延迟的优化性能。

Conclusion: 未来研究将优化拜占庭容错性能及CAP定理权衡。

Abstract: Round-based DAGs enable high-performance Byzantine fault-tolerant consensus,
yet their technical advantages remain underutilized due to their short history.
While research on consensus protocols is active in both academia and industry,
many studies overlook implementation-level algorithms, leaving actual
performance unclear - particularly for theoretical protocols whose practical
performance cannot often be evaluated. Bullshark, a Round-based DAG BFT
protocol on Narwhal mempool, achieves optimal performance: 297,000 transactions
per second with 2-second latency. We analyze the algorithm's workflow, from
transaction submission to blockchain commitment, breaking it down layer by
layer at the functional level and delineating the key features and interactions
of the Bullshark and Narwhal components. Future work aims to improve
performance in Byzantine fault environments and optimize trade-offs in the CAP
theorem.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [178] [Optimized Bistable Vortex Memory Arrays for Superconducting In-Memory Matrix-Vector Multiplication](https://arxiv.org/abs/2507.04648)
*Mustafa Altay Karamuftuoglu,Changxu Song,Beyza Zeynep Ucpinar,Sasan Razmkhah,Massoud Pedram*

Main category: cond-mat.supr-con

TL;DR: 本文提出了一种基于超导双稳态涡旋存储器（BVM）的内存计算方法，用于实现高效的矩阵向量乘法（MVM）运算，支持并行计算，具有高速度和低能耗的特点。


<details>
  <summary>Details</summary>
Motivation: 为了解决数据驱动算法和神经网络中的矩阵向量乘法（MVM）挑战，本文利用BVM技术实现超高速、高能效的内存计算，为高效神经网络提供支持。

Method: 采用BVM阵列进行内存算术运算，结合量化缓冲器（QB）单元和T1加法器，构建功能完整的乘法器单元，并通过优化的BVM阵列结构提升计算效率。

Result: 成功实现4位乘法器在20 GHz频率下运行，延迟为50 ps，同时展示了20 GHz的MVM结构运行效果，支持乘加（MAC）运算扩展。

Conclusion: 本工作为高能效神经网络提供了高速内存计算的新途径，扩展了BVM技术在计算领域的应用潜力。

Abstract: Building upon previously introduced Bistable Vortex Memory (BVM) as a novel,
nonvolatile, high-density, and scalable superconductor memory technology, this
work presents a methodology that uses BVM arrays to address challenges in
data-driven algorithms and neural networks, specifically focusing on
matrix-vector multiplication (MVM). The BVM approach introduces a novel
superconductor-based methodology for in-memory arithmetic, achieving
ultra-high-speed and energy-efficient computation by utilizing BVM arrays for
in-memory computation. The design employs a tiled multiplier structure where
BVM's inherent current summation capability is combined with Quantizer Buffer
(QB) cells to convert the analog accumulated current into a variable number of
digital Single Flux Quantum (SFQ) pulses. These pulses are then processed by T1
adder cells, which handle binary addition and carry propagation, thereby
forming a complete functional multiplier unit. This paper thus presents an
efficient MVM architecture that uses these BVM-based multipliers in a systolic
array configuration to enable parallel computation. A key innovation is an
optimized BVM array structure specifically tailored for multiplication
applications, involving a restructuring of Sense Lines (SLs) with diagonal
connections to reduce area and an adjusted input scheme to enhance
computational efficiency compared to the general-purpose BVM array design. We
demonstrate the efficacy of this approach with a 4-bit multiplier operating at
20 GHz with 50 ps latency and an MVM structure demonstrating operation at 20
GHz. Furthermore, we showcase how this multiplier design can be extended to
support Multiply-Accumulate (MAC) operations. This work paves the way for
power-efficient neural networks by enabling high-speed in-memory computation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [179] [Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions](https://arxiv.org/abs/2507.02900)
*Vineet Kumar Rakesh,Soumya Mazumdar,Research Pratim Maity,Sarbajit Pal,Amitabha Das,Tapas Samanta*

Main category: cs.CV

TL;DR: 本文全面综述了说话头部生成（THG）技术，分类了多种方法，并评估了算法、数据集及评估指标，指出了现有挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: THG技术在计算机视觉中具有重要应用价值，如数字虚拟人、视频配音等，但面临诸多技术挑战，需系统性总结和前瞻性研究。

Method: 文章将THG方法分为2D、3D、NeRF、扩散模型等多种技术类别，并分析了算法、数据集和评估指标。

Result: 综述揭示了THG在感知真实性和技术效率上的进展，同时指出对预训练模型的依赖、极端姿态处理等挑战。

Conclusion: 未来研究方向包括模块化架构、混合模型等，本文为THG领域的研究者和从业者提供了实用见解。

Abstract: Talking Head Generation (THG) has emerged as a transformative technology in
computer vision, enabling the synthesis of realistic human faces synchronized
with image, audio, text, or video inputs. This paper provides a comprehensive
review of methodologies and frameworks for talking head generation,
categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields
(NeRF)--based, diffusion--based, parameter-driven techniques and many other
techniques. It evaluates algorithms, datasets, and evaluation metrics while
highlighting advancements in perceptual realism and technical efficiency
critical for applications such as digital avatars, video dubbing, ultra-low
bitrate video conferencing, and online education. The study identifies
challenges such as reliance on pre--trained models, extreme pose handling,
multilingual synthesis, and temporal consistency. Future directions include
modular architectures, multilingual datasets, hybrid models blending
pre--trained and task-specific layers, and innovative loss functions. By
synthesizing existing research and exploring emerging trends, this paper aims
to provide actionable insights for researchers and practitioners in the field
of talking head generation. For the complete survey, code, and curated resource
list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.

</details>


### [180] [PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection](https://arxiv.org/abs/2507.02393)
*Seokyeong Lee,Sithu Aung,Junyong Choi,Seungryong Kim,Ig-Jae Kim,Junghyun Cho*

Main category: cs.CV

TL;DR: 提出了一种仅使用视频数据且无需多视角设置或其他传感器的伪标签框架，提升单目3D目标检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D目标检测中数据稀缺和2D到3D的模糊性问题，现有的弱监督和伪标签方法受到领域特定学习或单一观察的限制。

Method: 通过视频数据，利用物体点跟踪技术聚合静态和动态物体的伪LiDAR点云，无需额外传感器或多视角设置。

Result: 实验表明该方法在遮挡情况下表现鲁棒，具有高准确性和强扩展性。

Conclusion: 该方法为单目3D目标检测提供了实用且高效的解决方案。

Abstract: Monocular 3D object detection (M3OD) has long faced challenges due to data
scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.
Although various weakly supervised methods and pseudo-labeling methods have
been proposed to address these issues, they are mostly limited by
domain-specific learning or rely solely on shape information from a single
observation. In this paper, we propose a novel pseudo-labeling framework that
uses only video data and is more robust to occlusion, without requiring a
multi-view setup, additional sensors, camera poses, or domain-specific
training. Specifically, we explore a technique for aggregating the
pseudo-LiDARs of both static and dynamic objects across temporally adjacent
frames using object point tracking, enabling 3D attribute extraction in
scenarios where 3D data acquisition is infeasible. Extensive experiments
demonstrate that our method ensures reliable accuracy and strong scalability,
making it a practical and effective solution for M3OD.

</details>


### [181] [GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation](https://arxiv.org/abs/2507.02941)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.CV

TL;DR: GameTileNet是一个低分辨率游戏图块的语义数据集，旨在通过视觉-语言对齐支持叙事驱动的程序化内容生成。


<details>
  <summary>Details</summary>
Motivation: 解决游戏视觉资产生成中视觉与叙事对齐的挑战，以及训练数据分布不平衡导致的生成内容多样性不足问题。

Method: 收集艺术家创作的游戏图块并提供语义标注，包括对象检测、语义、连通性和对象分类。

Result: 创建了一个支持叙事驱动的PCG方法和低分辨率非真实感图像对象检测的数据集。

Conclusion: GameTileNet是一个改进PCG方法和支持丰富叙事游戏内容的宝贵资源。

Abstract: GameTileNet is a dataset designed to provide semantic labels for
low-resolution digital game art, advancing procedural content generation (PCG)
and related AI research as a vision-language alignment task. Large Language
Models (LLMs) and image-generative AI models have enabled indie developers to
create visual assets, such as sprites, for game interactions. However,
generating visuals that align with game narratives remains challenging due to
inconsistent AI outputs, requiring manual adjustments by human artists. The
diversity of visual representations in automatically generated game content is
also limited because of the imbalance in distributions across styles for
training data. GameTileNet addresses this by collecting artist-created game
tiles from OpenGameArt.org under Creative Commons licenses and providing
semantic annotations to support narrative-driven content generation. The
dataset introduces a pipeline for object detection in low-resolution tile-based
game art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object
classifications. GameTileNet is a valuable resource for improving PCG methods,
supporting narrative-rich game content, and establishing a baseline for object
detection in low-resolution, non-photorealistic images.
  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles
designed to support narrative-driven procedural content generation through
visual-language alignment.

</details>


### [182] [Unlearning the Noisy Correspondence Makes CLIP More Robust](https://arxiv.org/abs/2507.03434)
*Haochen Han,Alex Jinpeng Wang,Peijun Ye,Fangming Liu*

Main category: cs.CV

TL;DR: 提出了NCU框架，通过遗忘学习到的噪声知识增强视觉语言模型的鲁棒性，解决了噪声对应样本的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的数据规模不断增大，但数据质量下降导致噪声对应样本问题，影响模型性能。传统方法需要从头训练模型，资源消耗大。本文提出直接消除预训练模型中噪声影响的新视角。

Method: 提出了NCU框架，通过最优传输目标学习最难负样本信息，同时对假阳性和假阴性进行统一的遗忘学习精细调优。

Result: 在CLIP模型上验证，NCU在零样本迁移任务中表现优于鲁棒预训练方法，且计算开销更低。

Conclusion: NCU有效解决了噪声对应问题，提升了模型性能，具有实际应用潜力。

Abstract: The data appetite for Vision-Language Models (VLMs) has continuously scaled
up from the early millions to billions today, which faces an untenable
trade-off with data quality and inevitably introduces Noisy Correspondence (NC)
samples. Undoubtedly, such semantically unrelated data significantly impairs
the performance of VLMs. Previous efforts mainly address this challenge by
estimating refined alignment for more precise guidance. However, such
resource-intensive pipelines that train VLMs from scratch struggle to meet
realistic data demands. In this paper, we present a brand new perspective that
seeks to directly eliminate the harmful effects of NC in pre-trained VLMs.
Specifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning
framework that efficiently enhances VLMs' robustness by forgetting learned
noisy knowledge. The key to NCU is learning the hardest negative information,
which can provide explicit unlearning direction for both false positives and
false negatives. Such twin goals unlearning process can be formalized into one
unified optimal transport objective for fast fine-tuning. We validate our
approach with the prevailing CLIP model over various downstream tasks.
Remarkably, NCU surpasses the robust pre-trained method on zero-shot transfer
while with lower computational overhead. The code will be released upon
acceptance.

</details>


### [183] [Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles](https://arxiv.org/abs/2507.04139)
*Mahdi Rezaei,Mohsen Azarmi*

Main category: cs.CV

TL;DR: Driver-Net 是一种新颖的深度学习框架，利用多摄像头融合来评估驾驶员接管自动驾驶车辆的准备状态，通过时空数据整合和跨模态融合策略，达到了95.8%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决自动驾驶车辆控制权安全转移的问题，传统驾驶员监控系统仅依赖头部或眼部姿态是不够的，需要更全面的视觉线索。

Method: Driver-Net 采用三摄像头的多视角输入，捕捉驾驶员头部、手部和身体姿态的同步视觉信息，通过双路径架构（Context Block 和 Feature Block）和跨模态融合策略提升预测准确性。

Result: 在利兹大学驾驶模拟器收集的多样化数据集上，Driver-Net 的驾驶员准备状态分类准确率达到95.8%，优于现有方法。

Conclusion: Driver-Net 是一个实时、非侵入式的解决方案，显著提升了自动驾驶车辆的安全性，符合新的监管要求和安全标准。

Abstract: Ensuring safe transition of control in automated vehicles requires an
accurate and timely assessment of driver readiness. This paper introduces
Driver-Net, a novel deep learning framework that fuses multi-camera inputs to
estimate driver take-over readiness. Unlike conventional vision-based driver
monitoring systems that focus on head pose or eye gaze, Driver-Net captures
synchronised visual cues from the driver's head, hands, and body posture
through a triple-camera setup. The model integrates spatio-temporal data using
a dual-path architecture, comprising a Context Block and a Feature Block,
followed by a cross-modal fusion strategy to enhance prediction accuracy.
Evaluated on a diverse dataset collected from the University of Leeds Driving
Simulator, the proposed method achieves an accuracy of up to 95.8% in driver
readiness classification. This performance significantly enhances existing
approaches and highlights the importance of multimodal and multi-view fusion.
As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to
the development of safer and more reliable automated vehicles and aligns with
new regulatory mandates and upcoming safety standards.

</details>


### [184] [Pedestrian Intention Prediction via Vision-Language Foundation Models](https://arxiv.org/abs/2507.04141)
*Mohsen Azarmi,Mahdi Rezaei,He Wang*

Main category: cs.CV

TL;DR: 该研究探索了基于视觉语言基础模型（VLFMs）的多模态数据集成方法，通过分层提示模板预测行人过街意图，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉方法在预测行人过街意图时泛化性和上下文理解能力不足，研究旨在利用VLFMs解决这些问题。

Method: 采用分层提示模板整合视觉帧、物理线索和自车动态等多模态数据，优化提示设计以提升VLFM的预测效果。

Result: 在JAAD、PIE和FU-PIP数据集上，结合车速和时序提示的VLFMs将预测精度提升19.8%，自动提示优化进一步带来12.5%的增益。

Conclusion: VLFMs在行人过街意图预测中表现优于传统视觉模型，为自动驾驶提供了更好的泛化性和上下文理解能力。

Abstract: Prediction of pedestrian crossing intention is a critical function in
autonomous vehicles. Conventional vision-based methods of crossing intention
prediction often struggle with generalizability, context understanding, and
causal reasoning. This study explores the potential of vision-language
foundation models (VLFMs) for predicting pedestrian crossing intentions by
integrating multimodal data through hierarchical prompt templates. The
methodology incorporates contextual information, including visual frames,
physical cues observations, and ego-vehicle dynamics, into systematically
refined prompts to guide VLFMs effectively in intention prediction. Experiments
were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results
demonstrate that incorporating vehicle speed, its variations over time, and
time-conscious prompts significantly enhances the prediction accuracy up to
19.8%. Additionally, optimised prompts generated via an automatic prompt
engineering framework yielded 12.5% further accuracy gains. These findings
highlight the superior performance of VLFMs compared to conventional
vision-based models, offering enhanced generalisation and contextual
understanding for autonomous driving applications.

</details>


### [185] [Consistent and Invariant Generalization Learning for Short-video Misinformation Detection](https://arxiv.org/abs/2507.04061)
*Hanghui Guo,Weijie Shi,Mengze Li,Juncheng Li,Hao Chen,Yue Cui,Jiajie Xu,Jia Zhu,Jiawei Shen,Zhangze Chen,Sirui Han*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DOCTOR的新模型，通过一致性和不变性学习来提升短视频虚假信息检测的领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于领域差异，现有模型在跨领域识别短视频虚假信息时表现不佳，因此需要一种能够适应不同领域特性的方法。

Method: DOCTOR模型包含两个核心模块：跨模态特征插值和插值蒸馏以同步多模态学习；以及扩散模型增强领域不变特征。

Result: 实验表明DOCTOR模型在跨领域短视频虚假信息检测中表现优异。

Conclusion: DOCTOR通过多模态一致性和特征不变性学习，有效提升了虚假信息检测的泛化能力。

Abstract: Short-video misinformation detection has attracted wide attention in the
multi-modal domain, aiming to accurately identify the misinformation in the
video format accompanied by the corresponding audio. Despite significant
advancements, current models in this field, trained on particular domains
(source domains), often exhibit unsatisfactory performance on unseen domains
(target domains) due to domain gaps. To effectively realize such domain
generalization on the short-video misinformation detection task, we propose
deep insights into the characteristics of different domains: (1) The detection
on various domains may mainly rely on different modalities (i.e., mainly
focusing on videos or audios). To enhance domain generalization, it is crucial
to achieve optimal model performance on all modalities simultaneously. (2) For
some domains focusing on cross-modal joint fraud, a comprehensive analysis
relying on cross-modal fusion is necessary. However, domain biases located in
each modality (especially in each frame of videos) will be accumulated in this
fusion process, which may seriously damage the final identification of
misinformation. To address these issues, we propose a new DOmain generalization
model via ConsisTency and invariance learning for shORt-video misinformation
detection (named DOCTOR), which contains two characteristic modules: (1) We
involve the cross-modal feature interpolation to map multiple modalities into a
shared space and the interpolation distillation to synchronize multi-modal
learning; (2) We design the diffusion model to add noise to retain core
features of multi modal and enhance domain invariant features through
cross-modal guided denoising. Extensive experiments demonstrate the
effectiveness of our proposed DOCTOR model. Our code is public available at
https://github.com/ghh1125/DOCTOR.

</details>


### [186] [SeqTex: Generate Mesh Textures in Video Sequence](https://arxiv.org/abs/2507.04285)
*Ze Yuan,Xin Yu,Yangtian Sun,Yuan-Chen Guo,Yan-Pei Cao,Ding Liang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: SeqTex是一个新颖的端到端框架，利用预训练视频基础模型的视觉知识直接生成完整的UV纹理贴图，避免了传统方法的误差累积和空间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D纹理生成方法依赖于多视角图像生成和后续处理，导致误差累积和空间不一致。由于缺乏高质量3D纹理数据集，泛化能力受限。

Method: SeqTex将任务重新定义为序列生成问题，利用预训练视频模型的图像空间先验知识，结合多视角和UV分支设计、几何感知注意力机制及自适应令牌分辨率。

Result: SeqTex在图像和文本条件下的3D纹理生成任务中达到了最先进性能，具有出色的3D一致性、纹理-几何对齐和真实世界泛化能力。

Conclusion: SeqTex通过端到端框架直接生成UV纹理贴图，显著提升了3D纹理生成的效率和质量。

Abstract: Training native 3D texture generative models remains a fundamental yet
challenging problem, largely due to the limited availability of large-scale,
high-quality 3D texture datasets. This scarcity hinders generalization to
real-world scenarios. To address this, most existing methods finetune
foundation image generative models to exploit their learned visual priors.
However, these approaches typically generate only multi-view images and rely on
post-processing to produce UV texture maps -- an essential representation in
modern graphics pipelines. Such two-stage pipelines often suffer from error
accumulation and spatial inconsistencies across the 3D surface. In this paper,
we introduce SeqTex, a novel end-to-end framework that leverages the visual
knowledge encoded in pretrained video foundation models to directly generate
complete UV texture maps. Unlike previous methods that model the distribution
of UV textures in isolation, SeqTex reformulates the task as a sequence
generation problem, enabling the model to learn the joint distribution of
multi-view renderings and UV textures. This design effectively transfers the
consistent image-space priors from video foundation models into the UV domain.
To further enhance performance, we propose several architectural innovations: a
decoupled multi-view and UV branch design, geometry-informed attention to guide
cross-domain feature alignment, and adaptive token resolution to preserve fine
texture details while maintaining computational efficiency. Together, these
components allow SeqTex to fully utilize pretrained video priors and synthesize
high-fidelity UV texture maps without the need for post-processing. Extensive
experiments show that SeqTex achieves state-of-the-art performance on both
image-conditioned and text-conditioned 3D texture generation tasks, with
superior 3D consistency, texture-geometry alignment, and real-world
generalization.

</details>


### [187] [Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions](https://arxiv.org/abs/2507.04377)
*Xiao Zhang,Johan Bos*

Main category: cs.CV

TL;DR: 论文提出了一种多模态框架，利用视觉语言模型（VLMs）将墓碑图像转化为结构化表示（TMRs），显著提高了解析准确率（F1从36.1提升至89.5），并验证了模型在多样文化和物理退化条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 墓碑作为历史和文化的重要载体，面临多种保护挑战，如物理侵蚀、破坏和环境退化，需要一种高效的方法来数字化和保护其内容。

Method: 采用视觉语言模型（VLMs）和检索增强生成（RAG）技术，将墓碑图像转化为结构化表示（TMRs），整合图像和文本信息，并引入外部依赖元素如地名和职业代码。

Result: 与传统OCR方法相比，解析准确率显著提升（F1从36.1到89.5），且在多样文化和物理退化条件下表现出鲁棒性。

Conclusion: 论文首次尝试利用大规模视觉语言模型形式化墓碑理解，为文化遗产保护提供了新思路和工具。

Abstract: Tombstones are historically and culturally rich artifacts, encapsulating
individual lives, community memory, historical narratives and artistic
expression. Yet, many tombstones today face significant preservation
challenges, including physical erosion, vandalism, environmental degradation,
and political shifts. In this paper, we introduce a novel multi-modal framework
for tombstones digitization, aiming to improve the interpretation, organization
and retrieval of tombstone content. Our approach leverages vision-language
models (VLMs) to translate tombstone images into structured Tombstone Meaning
Representations (TMRs), capturing both image and text information. To further
enrich semantic parsing, we incorporate retrieval-augmented generation (RAG)
for integrate externally dependent elements such as toponyms, occupation codes,
and ontological concepts. Compared to traditional OCR-based pipelines, our
method improves parsing accuracy from an F1 score of 36.1 to 89.5. We
additionally evaluate the model's robustness across diverse linguistic and
cultural inscriptions, and simulate physical degradation through image fusion
to assess performance under noisy or damaged conditions. Our work represents
the first attempt to formalize tombstone understanding using large
vision-language models, presenting implications for heritage preservation.

</details>


### [188] [Boosting Temporal Sentence Grounding via Causal Inference](https://arxiv.org/abs/2507.04958)
*Kefan Tang,Lihuo He,Jisheng Dang,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出了一种新的时序语句定位（TSG）框架，通过因果干预和反事实推理来解决视频和文本查询之间的虚假关联问题。


<details>
  <summary>Details</summary>
Motivation: 现有TSG方法忽视了视频和文本查询之间的虚假关联，这种关联源于文本数据的固有偏差和模型对视频中显着或重复模式的过拟合，导致不可靠预测和泛化能力差。

Method: 论文首先从因果视角构建了TSG的结构因果模型，然后提出文本因果干预和视觉反事实推理方法，消除了未观测的混杂因素和视频偏差。

Result: 在公开数据集上的实验表明，该方法优于现有方法。

Conclusion: 通过因果推断方法，论文有效提升了TSG模型的鲁棒性和泛化能力。

Abstract: Temporal Sentence Grounding (TSG) aims to identify relevant moments in an
untrimmed video that semantically correspond to a given textual query. Despite
existing studies having made substantial progress, they often overlook the
issue of spurious correlations between video and textual queries. These
spurious correlations arise from two primary factors: (1) inherent biases in
the textual data, such as frequent co-occurrences of specific verbs or phrases,
and (2) the model's tendency to overfit to salient or repetitive patterns in
video content. Such biases mislead the model into associating textual cues with
incorrect visual moments, resulting in unreliable predictions and poor
generalization to out-of-distribution examples. To overcome these limitations,
we propose a novel TSG framework, causal intervention and counterfactual
reasoning that utilizes causal inference to eliminate spurious correlations and
enhance the model's robustness. Specifically, we first formulate the TSG task
from a causal perspective with a structural causal model. Then, to address
unobserved confounders reflecting textual biases toward specific verbs or
phrases, a textual causal intervention is proposed, utilizing do-calculus to
estimate the causal effects. Furthermore, visual counterfactual reasoning is
performed by constructing a counterfactual scenario that focuses solely on
video features, excluding the query and fused multi-modal features. This allows
us to debias the model by isolating and removing the influence of the video
from the overall effect. Experiments on public datasets demonstrate the
superiority of the proposed method. The code is available at
https://github.com/Tangkfan/CICR.

</details>


### [189] [AI for the Routine, Humans for the Complex: Accuracy-Driven Data Labelling with Mixed Integer Linear Programming](https://arxiv.org/abs/2507.04990)
*Mohammad Hossein Amini,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.CV

TL;DR: 论文提出了一种名为OPAL的人工辅助标注方法，通过混合整数线性规划（MILP）最小化标注工作量并确保标签高准确率。实验表明，OPAL在多种任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习（DL）中，准确的标注数据稀缺是一个主要挑战。虽然训练可以容忍部分标签噪声，但测试时需要高准确率以确保模型验证的可靠性。

Method: 作者提出OPAL方法，利用混合整数线性规划（MILP）来优化标注工作量，达到指定的准确率目标，并进一步结合主动学习以减少人工标注。

Result: 在2500多次实验中，OPAL在七种数据集上平均准确率达到98.8%，比基线方法显著提升，并将人工标注工作量减少一半以上。

Conclusion: OPAL在保证高准确率的同时大幅降低人工标注成本，尤其在测试数据自动标注和验证任务中表现优异，展示了其在实际应用中的潜力。

Abstract: The scarcity of accurately labelled data remains a major challenge in deep
learning (DL). Many DL approaches rely on semi-supervised methods, which focus
on constructing large datasets that require only a minimal amount of
human-labelled data. Since DL training algorithms can tolerate moderate label
noise, it has generally been acceptable for the accuracy of labels in large
training datasets to fall well short of a perfect 100%. However, when it comes
to testing DL models, achieving high label accuracy-as close to 100% as
possible-is paramount for reliable verification. In this article, we introduce
OPAL, a human-assisted labelling method that can be configured to target a
desired accuracy level while minimizing the manual effort required for
labelling. The main contribution of OPAL is a mixed-integer linear programming
(MILP) formulation that minimizes labelling effort subject to a specified
accuracy target. We evaluate OPAL for two tasks in the context of testing
vision systems: automatic labelling of test data and automated validation of
test data. Our evaluation, based on more than 2500 experiments performed on
seven datasets, comparing OPAL with eight baseline methods, shows that OPAL,
relying on its MILP formulation, achieves an average accuracy of 98.8%, just
1.2% below perfect accuracy, while cutting manual labelling by more than half.
Further, OPAL significantly outperforms automated labelling baselines in
labelling accuracy across all seven datasets, with large effect sizes, when all
methods are provided with the same manual-labelling budget. For automated
test-input validation, on average, OPAL reduces manual effort by 28.8% while
achieving 4.5% higher accuracy than the SOTA validation baselines. Finally, we
show that augmenting OPAL with an active learning loop leads to an additional
4.5% reduction in required manual labelling, without compromising accuracy.

</details>


### [190] [Less is More: Empowering GUI Agent with Context-Aware Simplification](https://arxiv.org/abs/2507.03730)
*Gongwei Chen,Xurui Zhou,Rui Shao,Yibo Lyu,Kaiwen Zhou,Shuai Wang,Wentao Li,Yinchuan Li,Zhongang Qi,Liqiang Nie*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SimpAgent的上下文感知简化框架，旨在解决GUI代理中元素和历史上下文建模的挑战，通过屏蔽无关元素和压缩冗余历史信息，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前纯视觉GUI代理方法过于关注预训练数据收集，而忽视了元素和历史上下文建模的挑战，导致无关元素和冗余历史信息影响代理效率和效果。

Method: 提出SimpAgent框架，包括基于屏蔽的元素剪枝方法和一致性引导的历史压缩模块，分别解决无关元素干扰和冗余历史信息问题。

Result: SimpAgent减少了27%的FLOPs，并在多样化的网页和移动环境中展示出优越的导航性能。

Conclusion: SimpAgent通过高效的上下文简化机制，显著提升了GUI代理的性能和效率，具有广泛的应用潜力。

Abstract: The research focus of GUI agents is shifting from text-dependent to
pure-vision-based approaches, which, though promising, prioritize comprehensive
pre-training data collection while neglecting contextual modeling challenges.
We probe the characteristics of element and history contextual modeling in GUI
agent and summarize: 1) the high-density and loose-relation of element context
highlight the existence of many unrelated elements and their negative
influence; 2) the high redundancy of history context reveals the inefficient
history modeling in current GUI agents. In this work, we propose a
context-aware simplification framework for building an efficient and effective
GUI Agent, termed SimpAgent. To mitigate potential interference from numerous
unrelated elements, we introduce a masking-based element pruning method that
circumvents the intractable relation modeling through an efficient masking
mechanism. To reduce the redundancy in historical information, we devise a
consistency-guided history compression module, which enhances implicit
LLM-based compression through innovative explicit guidance, achieving an
optimal balance between performance and efficiency. With the above components,
SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances.
Comprehensive navigation experiments across diverse web and mobile environments
demonstrate the effectiveness and potential of our agent.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [191] [Reconstructing Biological Pathways by Applying Selective Incremental Learning to (Very) Small Language Models](https://arxiv.org/abs/2507.04432)
*Pranta Saha,Joyce Reimer,Brook Byrns,Connor Burbridge,Neeraj Dhar,Jeffrey Chen,Steven Rayan,Gordon Broderick*

Main category: q-bio.MN

TL;DR: 研究提出使用小型、领域特定的语言模型（LM）在生物医学研究中更有效地预测分子间调控相互作用，以减少大型通用语言模型的“幻觉”问题。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在生物医学领域因“幻觉”问题导致准确性不足，限制了其应用。研究旨在寻找更合适的方法以提高预测准确性。

Method: 采用小型BERT架构的LM（约1.1亿参数），结合主动学习策略，选择信息量最大的示例进行微调，以预测结核病相关的分子相互作用。

Result: 在仅使用25%的调控关系数据情况下，模型预测准确性超过80%。研究发现，选择最具信息量的错误示例（低熵）可显著提高准确性。

Conclusion: 小型、领域特定的LM在生物医学研究中具有潜力，通过主动学习策略可高效提取关键信息，减少“幻觉”问题。

Abstract: The use of generative artificial intelligence (AI) models is becoming
ubiquitous in many fields. Though progress continues to be made, general
purpose large language AI models (LLM) show a tendency to deliver creative
answers, often called "hallucinations", which have slowed their application in
the medical and biomedical fields where accuracy is paramount. We propose that
the design and use of much smaller, domain and even task-specific LM may be a
more rational and appropriate use of this technology in biomedical research. In
this work we apply a very small LM by today's standards to the specialized task
of predicting regulatory interactions between molecular components to fill gaps
in our current understanding of intracellular pathways. Toward this we attempt
to correctly posit known pathway-informed interactions recovered from manually
curated pathway databases by selecting and using only the most informative
examples as part of an active learning scheme. With this example we show that a
small (~110 million parameters) LM based on a Bidirectional Encoder
Representations from Transformers (BERT) architecture can propose molecular
interactions relevant to tuberculosis persistence and transmission with over
80% accuracy using less than 25% of the ~520 regulatory relationships in
question. Using information entropy as a metric for the iterative selection of
new tuning examples, we also find that increased accuracy is driven by favoring
the use of the incorrectly assigned statements with the highest certainty
(lowest entropy). In contrast, the concurrent use of correct but least certain
examples contributed little and may have even been detrimental to the learning
rate.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [192] [Dominance or Fair Play in Social Networks? A Model of Influencer Popularity Dynamic](https://arxiv.org/abs/2507.03448)
*Franco Galante,Chiara Ravazzi,Luca Vassio,Michele Garetto,Emilio Leonardi*

Main category: cs.SI

TL;DR: 本文提出了一种数据驱动的平均场方法，用于模拟寻求公众关注的用户（即影响者）的流行度动态。


<details>
  <summary>Details</summary>
Motivation: 研究影响者流行度动态的建模，以理解个体活动模式、专业知识、外生事件和平台作用对成功的影响。

Method: 开发了一种新颖的分析模型，整合了多种因素，并通过解析推导系统遍历性条件来预测流行度分布。

Result: 敏感性分析揭示了系统配置对不同结果（如主导或公平竞争）的影响。

Conclusion: 研究结果为社交网络向更公平或更偏颇的影响力生态系统演变提供了有价值的见解。

Abstract: This paper presents a data-driven mean-field approach to model the popularity
dynamics of users seeking public attention, i.e., influencers. We propose a
novel analytical model that integrates individual activity patterns, expertise
in producing viral content, exogenous events, and the platform's role in
visibility enhancement, ultimately determining each influencer's success. We
analytically derive sufficient conditions for system ergodicity, enabling
predictions of popularity distributions. A sensitivity analysis explores
various system configurations, highlighting conditions favoring either
dominance or fair play among influencers. Our findings offer valuable insights
into the potential evolution of social networks towards more equitable or
biased influence ecosystems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [193] [HiPerMotif: Novel Parallel Subgraph Isomorphism in Large-Scale Property Graphs](https://arxiv.org/abs/2507.04130)
*Mohammad Dindoost,Oliver Alvarado Rodriguez,Bartosz Bryg,Ioannis Koutis,David A. Bader*

Main category: cs.DS

TL;DR: HiPerMotif是一种新型混合并行算法，通过边中心初始化策略和结构重排序，显著提升了属性丰富图中的子图同构搜索效率，实现了高达66倍的加速，并能处理海量数据集。


<details>
  <summary>Details</summary>
Motivation: 传统子图同构算法在处理属性丰富的图时面临搜索空间大、剪枝机会少的问题，影响其在神经科学等领域的应用。HiPerMotif旨在解决这一挑战。

Method: HiPerMotif通过结构重排序优先处理高度数顶点，从目标图中提取第一边的候选映射，并通过有效验证器快速验证，将这些部分映射作为搜索的起点，从而减少早期搜索开销并支持并行化。

Result: 在多种数据集上，HiPerMotif比现有算法（如VF2-PS）快66倍，并可处理1.47亿边的H01连接组数据，突破了传统算法的内存限制。

Conclusion: HiPerMotif通过创新的初始化策略和并行化设计，显著提升了子图同构算法的效率和可扩展性，为复杂图分析提供了新工具。

Abstract: Subgraph isomorphism, essential for pattern detection in large-scale graphs,
faces scalability challenges in attribute-rich property graphs used in
neuroscience, systems biology, and social network analysis. Traditional
algorithms explore search spaces vertex-by-vertex from empty mappings, leading
to extensive early-stage exploration with limited pruning opportunities. We
introduce HiPerMotif, a novel hybrid parallel algorithm that fundamentally
shifts the search initialization strategy. After structurally reordering the
pattern graph to prioritize high-degree vertices, HiPerMotif systematically
identifies all possible mappings for the first edge (vertices 0,1) in the
target graph, validates these edge candidates using efficient vertex and edge
validators, and injects the validated partial mappings as states at depth 2.
The algorithm then continues with traditional vertex-by-vertex exploration from
these pre-validated starting points, effectively pruning the expensive early
search tree branches while enabling natural parallelization over edge
candidates. Our contributions include the edge-centric initialization paradigm
with state injection, a structural reordering strategy achieving up to 5x
speedup, rapid edge and vertex validators for attribute-rich graphs, and
efficient parallel enumeration over target graph edges. Implemented in the
open-source Arachne framework, HiPerMotif achieves up to 66x speedup over
state-of-the-art baselines (VF2-PS, VF3P, Glasgow) on diverse datasets where
baselines successfully complete execution. Additionally, HiPerMotif
successfully processes massive datasets such as the H01 connectome with 147
million edges, which existing methods cannot handle due to memory constraints.
Comprehensive evaluation across synthetic and real-world graphs demonstrates
HiPerMotif's scalability, enabling advanced analysis in computational
neuroscience and beyond.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [194] [Navigating Speech Recording Collections with AI-Generated Illustrations](https://arxiv.org/abs/2507.04182)
*Sirina Håland,Trond Karlsen Strøm,Petra Galuščáková*

Main category: cs.IR

TL;DR: 提出一种基于生成模型的新型语音档案导航方法，通过Web应用实现结构化数据组织和交互式思维导图。


<details>
  <summary>Details</summary>
Motivation: 随着口语内容的增加，传统的信息检索方法已不足以高效提取知识，需要新方法来导航和搜索语音内容。

Method: 利用语言和多模态生成模型的最新进展，开发了一个Web应用，使用交互式思维导图和图像生成工具组织数据。

Result: 在TED-LIUM~3数据集上实现，用户测试显示系统能简化大规模语音集合的探索。

Conclusion: 该方法展示了生成模型在语音档案导航中的潜力和实用性。

Abstract: Although the amount of available spoken content is steadily increasing,
extracting information and knowledge from speech recordings remains
challenging. Beyond enhancing traditional information retrieval methods such as
speech search and keyword spotting, novel approaches for navigating and
searching spoken content need to be explored and developed. In this paper, we
propose a novel navigational method for speech archives that leverages recent
advances in language and multimodal generative models. We demonstrate our
approach with a Web application that organizes data into a structured format
using interactive mind maps and image generation tools. The system is
implemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech
transcripts and audio files of TED Talks. Initial user tests using a System
Usability Scale (SUS) questionnaire indicate the application's potential to
simplify the exploration of large speech collections.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [195] [On Complementation of Nondeterministic Finite Automata without Full Determinization (Technical Report)](https://arxiv.org/abs/2507.03439)
*Lukáš Holík,Ondřej Lengál,Juraj Major,Adéla Štěpková,Jan Strejček*

Main category: cs.FL

TL;DR: 研究了几种基于反向幂集构造或新结构的有限自动机补全方法，实验表明这些方法能生成更小的补全结果。


<details>
  <summary>Details</summary>
Motivation: 有限自动机补全是基本操作，但传统方法（NFA转DFA后补全）可能导致DFA指数级增大，需探索更高效的方法。

Method: 采用反向幂集构造或两种新结构构建补全方法，利用了NFA的常见结构。

Result: 在大数据集实验中发现，新方法能显著减小补全后的自动机规模。

Conclusion: 与传统方法相比，新补全方法能更高效地生成较小的补全结果，适用于多种应用场景。

Abstract: Complementation of finite automata is a basic operation used in numerous
applications. The standard way to complement a nondeterministic finite
automaton (NFA) is to transform it into an equivalent deterministic finite
automaton (DFA) and complement the DFA. The DFA can, however, be exponentially
larger than the corresponding NFA. In this paper, we study several alternative
approaches to complementation, which are based either on reverse powerset
construction or on two novel constructions that exploit a commonly occurring
structure of NFAs. Our experiment on a large data set shows that using a
different than the classical approach can in many cases yield significantly
smaller complements.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [196] [Improving BERT for Symbolic Music Understanding Using Token Denoising and Pianoroll Prediction](https://arxiv.org/abs/2507.04776)
*Jun-You Wang,Li Su*

Main category: cs.SD

TL;DR: 提出了一种基于BERT的预训练模型用于符号音乐理解，设计了两个新的预训练目标（token校正和钢琴卷预测），并在12个下游任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提升符号音乐理解的性能，研究者设计了新的预训练目标，旨在帮助模型学习特定的音乐知识（如音高间隔）。

Method: 1. token校正：对部分音符token加入噪声并训练模型去噪；2. 钢琴卷预测：从损坏的音符token预测条级和局部钢琴卷表示。

Result: 模型在12个下游任务中表现优异，证明了预训练目标的有效性。

Conclusion: 提出的预训练目标能够有效提升符号音乐理解模型的性能。

Abstract: We propose a pre-trained BERT-like model for symbolic music understanding
that achieves competitive performance across a wide range of downstream tasks.
To achieve this target, we design two novel pre-training objectives, namely
token correction and pianoroll prediction. First, we sample a portion of note
tokens and corrupt them with a limited amount of noise, and then train the
model to denoise the corrupted tokens; second, we also train the model to
predict bar-level and local pianoroll-derived representations from the
corrupted note tokens. We argue that these objectives guide the model to better
learn specific musical knowledge such as pitch intervals. For evaluation, we
propose a benchmark that incorporates 12 downstream tasks ranging from chord
estimation to symbolic genre classification. Results confirm the effectiveness
of the proposed pre-training objectives on downstream tasks.

</details>


### [197] [EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation](https://arxiv.org/abs/2507.04955)
*Fathinah Izzati,Xinyue Li,Gus Xia*

Main category: cs.SD

TL;DR: Expotion是一种利用面部表情、上半身动作和文本提示生成音乐的模型，通过参数高效微调和时间对齐策略，提升了生成音乐的质量。


<details>
  <summary>Details</summary>
Motivation: 探索多模态视觉控制（面部表情和身体动作）与文本提示结合，以生成更具表现力和时间准确性的音乐。

Method: 使用预训练的文本到音乐生成模型，采用参数高效微调（PEFT）和小数据集进行细粒度适配，引入时间平滑策略确保多模态同步。

Result: 实验表明，结合视觉特征和文本描述显著提升了生成音乐的多个维度质量，超越了现有基准和先进模型。

Conclusion: Expotion在多模态音乐生成中表现出色，并提供了一个新的同步视频-音乐数据集，为未来研究提供了潜力。

Abstract: We propose Expotion (Facial Expression and Motion Control for Multimodal
Music Generation), a generative model leveraging multimodal visual controls -
specifically, human facial expressions and upper-body motion - as well as text
prompts to produce expressive and temporally accurate music. We adopt
parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music
generation model, enabling fine-grained adaptation to the multimodal controls
using a small dataset. To ensure precise synchronization between video and
music, we introduce a temporal smoothing strategy to align multiple modalities.
Experiments demonstrate that integrating visual features alongside textual
descriptions enhances the overall quality of generated music in terms of
musicality, creativity, beat-tempo consistency, temporal alignment with the
video, and text adherence, surpassing both proposed baselines and existing
state-of-the-art video-to-music generation models. Additionally, we introduce a
novel dataset consisting of 7 hours of synchronized video recordings capturing
expressive facial and upper-body gestures aligned with corresponding music,
providing significant potential for future research in multimodal and
interactive music generation.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [198] [Abstract computation over first-order structures. Part IIb: Moschovakis' operator and other non-determinisms](https://arxiv.org/abs/2507.03827)
*Christine Gaßner*

Main category: math.LO

TL;DR: 该论文研究了BSS RAMs中不同类型的二进制非确定性，并分析了身份关系和有限常数集的可判定性对机器性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为BSS RAMs提供一个数学框架，以刻画在一阶结构上的算法，并探讨不同非确定性模型的影响。

Method: 比较了由非确定性分支过程产生的二进制非确定性、数字非确定性以及其他非确定性，并分析了身份关系和常数集的半可判定性对机器性能的影响。

Result: 身份关系和常数集的半可判定性会显著影响机器的性能和效率，且其顺序对应影响强度。

Conclusion: 论文表明，特定性质（如身份关系和常数集的半可判定性）对BSS RAMs的性能和效率有重要影响，且这些性质的可判定性是关键。

Abstract: BSS RAMs were introduced to provide a mathematical framework for
characterizing algorithms over first-order structures. Non-deterministic BSS
RAMs help to model different non-deterministic approaches. Here, we deal with
different types of binary non-determinisms and study the consequences of the
decidability of the identity relation and the decidability of finite sets
consisting of one or two constants. We compare the binary non-determinism
resulting from a non-deterministic branching process, the digital
non-determinism resulting from the restriction of guesses to two constants, and
some other non-determinisms resulting from the use of Moschovakis' operator
applied to oracle sets restricted to tuples of constants. Moreover, we show
that the performance capability and the efficiency of individual machines are
influenced by the following properties. 1. The identity relation belongs to the
underlying structure. 2. The identity is semi-decidable over the underlying
structure. 3. Two single-element sets of constants are semi-decidable. 4. A set
of two constants is semi-decidable. The order of these properties corresponds
to the strength of their influence. In all cases mentioned, the
semi-decidability of the sets implies their decidability.

</details>


### [199] [The Myhill isomorphism theorem does not generalize much](https://arxiv.org/abs/2507.05028)
*Cécilia Pradic*

Main category: math.LO

TL;DR: 论文研究了Myhill同构定理（Cantor-Bernstein定理的变体）在非自然数无限集上的扩展性，证明在Markov原理下可扩展至共自然数集，但对其他集合则普遍不可行。


<details>
  <summary>Details</summary>
Motivation: 探讨Myhill同构定理在不同无限集上的适用性，以扩展这一经典构造性定理的应用范围。

Method: 通过构造性证明和反例分析，结合Markov原理，测试定理在共自然数集和其他集合上的可行性。

Result: 定理可扩展至共自然数集（需限制双补集条件），但无法推广到其他测试集合（如乘积集、指数集等）。

Conclusion: Myhill同构定理的扩展性高度依赖于集合结构，仅对特定集合（如共自然数集）在限制条件下成立。

Abstract: The Myhill isomorphism is a variant of the Cantor-Bernstein theorem. It
states that, from two injections that reduces two subsets of $\mathbb{N}$ to
each other, there exists a bijection $\mathbb{N} \to \mathbb{N}$ that preserves
them. This theorem can be proven constructively. We investigate to which extent
the theorem can be extended to other infinite sets other than $\mathbb{N}$. We
show that, assuming Markov's principle, the theorem can be extended to the
conatural numbers $\mathbb{N}_{\infty}$ provided that we only require that
bicomplemented sets are preserved by the bijection. This restriction is
essential. Otherwise, the picture is overall negative: among other things, it
is impossible to extend that result to either $2 \times \mathbb{N}_{\infty}$,
$\mathbb{N} + \mathbb{N}_{\infty}$, $\mathbb{N} \times \mathbb{N}_{\infty}$,
$\mathbb{N}_{\infty}^2$, $2^{\mathbb{N}}$ or $\mathbb{N}^{\mathbb{N}}$.

</details>


### [200] [Interleaving Logic and Counting](https://arxiv.org/abs/2507.05219)
*Johan van Benthem,Thomas Icard*

Main category: math.LO

TL;DR: 该论文探讨了自然语言中量化表达的逻辑与算术特征结合，提出了一种小型逻辑片段，用于表示数值推理，并通过强化方法探讨了其与算术理论的联系。


<details>
  <summary>Details</summary>
Motivation: 研究自然语言中量化表达的逻辑与算术特征的结合，超越定性与定量的严格划分，以理解和扩展这种结合在语言和数学实践中的应用。

Method: 回顾一阶逻辑与计数操作符，提出小型逻辑片段表示数值推理，使用范式方法进行强化，探讨其与加性Presburger算术和丢番图方程的联系。

Result: 确定了在有限和无限模型中可以定义的算术概念，探讨了逻辑与算术的相互定义关系，并设计了结合模态逻辑与计数的系统。

Conclusion: 总结了逻辑与计数在形式系统中的进一步结合，反思了定性与定量的划分，并建议将其与认知科学的实证研究联系起来。

Abstract: Reasoning with quantifier expressions in natural language combines logical
and arithmetical features, transcending strict divides between qualitative and
quantitative. Our topic is this cooperation of styles as it occurs in common
linguistic usage and its extension into the broader practice of natural
language plus "grassroots mathematics".
  We begin with a brief review of first-order logic with counting operators and
cardinality comparisons. This system is known to be of high complexity, and
drowns out finer aspects of the combination of logic and counting. We move to a
small fragment that can represent numerical syllogisms and basic reasoning
about comparative size: monadic first-order logic with counting. We provide
normal forms that allow for axiomatization, determine which arithmetical
notions can be defined on finite and on infinite models, and conversely, we
discuss which logical notions can be defined out of purely arithmetical ones,
and what sort of (non-)classical logics can be induced.
  Next, we investigate a series of strengthenings, again using normal form
methods. The monadic second-order version is close, in a precise sense, to
additive Presburger Arithmetic, while versions with the natural device of tuple
counting take us to Diophantine equations, making the logic undecidable. We
also define a system that combines basic modal logic over binary accessibility
relations with counting, needed to formulate ubiquitous reasoning patterns such
as the Pigeonhole Principle.
  We return to our starting point in natural language, confronting the
architecture of our formal systems with linguistic quantifier vocabulary and
syntax. We conclude with some general thoughts on yet further entanglements of
logic and counting in formal systems, on rethinking the
qualitative/quantitative divide, and on connecting our analysis to empirical
findings in cognitive science.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [201] [Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)](https://arxiv.org/abs/2507.03608)
*Sarat Ahmad,Zeinab Nezami,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.AI

TL;DR: 本文比较了传统RAG、GraphRAG和Hybrid GraphRAG在ORAN架构中的性能，发现GraphRAG和Hybrid GraphRAG在上下文相关性和事实正确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 在ORAN架构中，为电信任务优化大语言模型（LLMs）成本高昂，作者希望通过检索增强生成（RAG）提供一种无需完全重新训练的替代方案。

Method: 作者比较了传统RAG、GraphRAG和Hybrid GraphRAG的性能，评估了它们在ORAN规范中的应用，使用生成指标如忠实度、答案相关性、上下文相关性和事实正确性。

Result: GraphRAG和Hybrid GraphRAG表现优于传统RAG，Hybrid GraphRAG将事实正确性提高了8%，而GraphRAG将上下文相关性提高了7%。

Conclusion: GraphRAG和Hybrid GraphRAG在ORAN场景中具有显著优势，为未来无线网络中的自主优化提供了有效方法。

Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling
autonomous optimization in future wireless networks. Within the ORAN
architecture, Large Language Models (LLMs) can be specialized to generate xApps
and rApps by leveraging specifications and API definitions from the RAN
Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for
telecom-specific tasks remains expensive and resource-intensive.
Retrieval-Augmented Generation (RAG) offers a practical alternative through
in-context learning, enabling domain adaptation without full retraining. While
traditional RAG systems rely on vector-based retrieval, emerging variants such
as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval
strategies to support multi-hop reasoning and improve factual grounding.
Despite their promise, these methods lack systematic, metric-driven
evaluations, particularly in high-stakes domains such as ORAN. In this study,
we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid
GraphRAG using ORAN specifications. We assess performance across varying
question complexities using established generation metrics: faithfulness,
answer relevance, context relevance, and factual correctness. Results show that
both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG
improves factual correctness by 8%, while GraphRAG improves context relevance
by 7%.

</details>


### [202] [ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning](https://arxiv.org/abs/2507.04736)
*Zhirong Chen,Kaiyan Chang,Zhuolin Li,Xinyang He,Chujie Chen,Cangyuan Li,Mengdi Wang,Haobo Xu,Yinhe Han,Ying Wang*

Main category: cs.AI

TL;DR: 论文提出了一种基于强化学习的框架ChipSeek-R1，用于训练大型语言模型生成同时满足功能正确性和硬件质量（PPA）优化的RTL代码。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法同时优化功能正确性和硬件质量（PPA），亟需一种新方法来解决这一问题。

Method: 采用分层奖励驱动的强化学习框架，结合语法、功能正确性和PPA指标的反馈训练模型。

Result: 在标准测试中表现优异，生成了27个PPA优于人工编写的RTL设计。

Conclusion: 强化学习结合工具链反馈可有效生成超越人工的RTL代码，具有实际应用潜力。

Abstract: Large Language Models (LLMs) show significant potential for automating
Register-Transfer Level (RTL) code generation. However, current approaches face
a critical challenge: they can not simultaneously optimize for functional
correctness and hardware quality (Power, Performance, Area - PPA). Methods
based on supervised fine-tuning often generate functionally correct but
PPA-suboptimal code, lacking mechanisms to learn optimization principles. In
contrast, post-processing techniques that attempt to improve PPA metrics after
generation are often inefficient because they operate externally without
updating the LLM's parameters, thus failing to enhance the model's intrinsic
design capabilities.
  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven
reinforcement learning framework to train LLMs to generate RTL code that
achieves both functional correctness and optimized PPA metrics. ChipSeek-R1
employs a hierarchical reward system, which incorporates direct feedback on
syntax, functional correctness (from simulators) and PPA metrics (from
synthesis tools) during reinforcement learning. This enables the model to learn
complex hardware design trade-offs via trial-and-error, generating RTL code
that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on
standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results
in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1
generated 27 RTL designs surpassing the PPA metrics of the original
human-written code. Our findings demonstrate the effectiveness of integrating
toolchain feedback into LLM training and highlight the potential for
reinforcement learning to enable automated generation of human-surpassing RTL
code. We open-source our code in anonymous github.

</details>


### [203] [From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM](https://arxiv.org/abs/2507.03868)
*Xinyi Wu,Yanhao Jia,Luwei Xiao,Shuai Zhao,Fengkuang Chiang,Erik Cambria*

Main category: cs.AI

TL;DR: Uni-RAG是一种轻量级多模态检索生成框架，结合检索与生成技术，提升了教育场景中的多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统无法处理教育场景中的多样性和模糊性，需要一个更高效的解决方案。

Method: 开发了Uni-Retrieval模块，结合MoE-LoRA技术动态匹配查询与Prompt Bank，并与指令调优的语言模型集成形成Uni-RAG。

Result: 在SER等多模态基准测试中，Uni-RAG在检索准确性和生成质量上均优于基线系统，且计算成本低。

Conclusion: Uni-RAG为智能教育系统提供了可扩展、基于教学的方法，支持跨STEM场景的个性化学习。

Abstract: In AI-facilitated teaching, leveraging various query styles to interpret
abstract educational content is crucial for delivering effective and accessible
learning experiences. However, existing retrieval systems predominantly focus
on natural text-image matching and lack the capacity to address the diversity
and ambiguity inherent in real-world educational scenarios. To address this
limitation, we develop a lightweight and efficient multi-modal retrieval
module, named Uni-Retrieval, which extracts query-style prototypes and
dynamically matches them with tokens from a continually updated Prompt Bank.
This Prompt Bank encodes and stores domain-specific knowledge by leveraging a
Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to
enhance Uni-Retrieval's capability to accommodate unseen query types at test
time. To enable natural language educational content generation, we integrate
the original Uni-Retrieval with a compact instruction-tuned language model,
forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given
a style-conditioned query, Uni-RAG first retrieves relevant educational
materials and then generates human-readable explanations, feedback, or
instructional content aligned with the learning objective. Experimental results
on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline
retrieval and RAG systems in both retrieval accuracy and generation quality,
while maintaining low computational cost. Our framework provides a scalable,
pedagogically grounded solution for intelligent educational systems, bridging
retrieval and generation to support personalized, explainable, and efficient
learning assistance across diverse STEM scenarios.

</details>


### [204] [Participatory Evolution of Artificial Life Systems via Semantic Feedback](https://arxiv.org/abs/2507.03839)
*Shuowen Li,Kexin Wang,Minglu Fang,Danqi Huang,Ali Asadipour,Haipeng Mi,Yitong Sun*

Main category: cs.AI

TL;DR: 提出了一种语义反馈框架，通过自然语言指导人工生命系统的演化，结合提示编码、优化器和评估模块，实现用户意图对系统行为的调控。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言更直观地指导人工生命系统的演化和行为设计，减少手动调整的复杂性。

Method: 结合提示编码器（prompt-to-parameter）、CMA-ES优化器和CLIP评估模块，构建交互式生态系统模拟，支持提示细化、多智能体交互和规则合成。

Result: 用户研究表明，该系统在语义对齐方面优于手动调整，具备参与式生成设计和开放演化的潜力。

Conclusion: 该框架为人工生命系统提供了一种高效且直观的自然语言交互方式，适用于生成设计和开放式演化研究。

Abstract: We present a semantic feedback framework that enables natural language to
guide the evolution of artificial life systems. Integrating a
prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the
system allows user intent to modulate both visual outcomes and underlying
behavioral rules. Implemented in an interactive ecosystem simulation, the
framework supports prompt refinement, multi-agent interaction, and emergent
rule synthesis. User studies show improved semantic alignment over manual
tuning and demonstrate the system's potential as a platform for participatory
generative design and open-ended evolution.

</details>


### [205] [An ASP-Based Framework for MUSes](https://arxiv.org/abs/2507.03929)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.AI

TL;DR: 论文提出了一个基于答案集编程（ASP）的框架MUS-ASP，用于在线枚举最小不可满足子集（MUS），展示了其在MUS枚举和计数任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 在多个应用中，理解不可满足公式的核心原因至关重要。最小不可满足子集（MUS）是捕捉这种原因的有效方法，但其枚举和计数任务在现有研究中仍具挑战性。

Method: 通过将MUS枚举问题转化为答案集求解问题，利用ASP在知识表示和组合问题求解中的优势，提出了MUS-ASP框架，并与混合求解器结合。

Result: 实验结果表明，MUS-ASP在MUS枚举和计数任务中表现高效，尤其是在与混合求解器集成时速度显著提升。

Conclusion: MUS-ASP框架通过利用ASP的计算效率，为MUS的在线枚举和计数提供了有效解决方案。

Abstract: Given an unsatisfiable formula, understanding the core reason for
unsatisfiability is crucial in several applications. One effective way to
capture this is through the minimal unsatisfiable subset (MUS), the
subset-minimal set of clauses that remains unsatisfiable. Current research
broadly focuses on two directions: (i) enumerating as many MUSes as possible
within a given time limit, and (ii) counting the total number of MUSes for a
given unsatisfiable formula.
  In this paper, we introduce an answer set programming-based framework, named
MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for
its strengths in knowledge representation and is particularly suitable for
specifying complex combinatorial problems. By translating MUS enumeration into
answer set solving, MUS-ASP leverages the computational efficiency of
state-of-the-art ASP systems. Our extensive experimental evaluation
demonstrates the effectiveness of MUS-ASP and highlights the acceleration in
both MUS enumeration and counting tasks, particularly when integrated within
hybrid solvers, including the framework proposed in this paper.

</details>


### [206] [Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs](https://arxiv.org/abs/2507.04719)
*Roozbeh Yousefzadeh,Xuenan Cao*

Main category: cs.AI

TL;DR: 本文批评并讨论了当前形式化推理和自动定理证明领域的基准测试与评估实践，倡导开放代码、开放数据及无错误的基准测试以推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 旨在通过讨论和改革现有实践，消除领域内的贡献障碍，促进自动化定理证明等相关领域的进步。

Method: 通过批判性分析当前实践，识别问题并提出改进建议，同时探讨可能产生误导性评估信息的做法。

Result: 呼吁开放、透明的基准测试和数据共享，以加速领域发展。

Conclusion: 希望通过跨群体讨论，推动自动化定理证明及相关领域的合作与进步。

Abstract: This position paper provides a critical but constructive discussion of
current practices in benchmarking and evaluative practices in the field of
formal reasoning and automated theorem proving. We take the position that open
code, open data, and benchmarks that are complete and error-free will
accelerate progress in this field. We identify practices that create barriers
to contributing to this field and suggest ways to remove them. We also discuss
some of the practices that might produce misleading evaluative information. We
aim to create discussions that bring together people from various groups
contributing to automated theorem proving, autoformalization, and informal
reasoning.

</details>


### [207] [MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents](https://arxiv.org/abs/2507.04376)
*Georgios Ioannides,Christos Constantinou,Vinija Jain,Aman Chadha,Aaron Elkins*

Main category: cs.AI

TL;DR: MOD-X是一个模块化、开放、去中心化的架构框架，旨在解决异构智能体间的互操作性问题，提供了分层架构、通用消息总线、状态管理和区块链安全机制等创新功能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从单一模型发展为专业化智能体生态系统，标准化通信协议的需求日益重要。现有的协议无法有效支持异构智能体的互操作性。

Method: 提出MOD-X框架，采用分层架构，包括通用消息总线、状态管理、语义能力发现和动态工作流编排，并结合区块链安全机制。

Result: MOD-X通过实际案例展示了如何集成不同类型（如基于规则、神经网络、符号推理等）的异构智能体，且无需中央协调即可扩展。

Conclusion: MOD-X为去中心化、可互操作的智能体生态系统提供了理论和实践结合的解决方案，弥补了现有协议的局限性。

Abstract: As Artificial Intelligence systems evolve from monolithic models to
ecosystems of specialized agents, the need for standardized communication
protocols becomes increasingly critical. This paper introduces MOD-X (Modular
Open Decentralized eXchange), a novel architectural framework proposal for
agent interoperability that addresses key limitations of existing protocols.
Unlike current approaches, MOD-X proposes a layered architecture with a
Universal Message Bus, thorough state management, translation capabilities, and
blockchain-based security mechanisms. We present MOD-X's architecture, compare
it with existing protocols, and demonstrate its application through a worked
example how it enables integration between heterogeneous specialist agents
(agents with different architectures, vendors, capabilities, and knowledge
representations--including rule-based systems, neural networks, symbolic
reasoning engines, and legacy software with agent wrappers). MOD-X's key
innovations include a publish-subscribe communication model, semantic
capability discovery, and dynamic workflow orchestration--providing a framework
that bridges theoretical formalism with practical implementation. This
architecture addresses the growing need for truly decentralized, interoperable
agent ecosystems that can scale effectively without the need for central
coordination.

</details>


### [208] [Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking](https://arxiv.org/abs/2507.03330)
*Franklin Mingzhe Li,Kaitlyn Ng,Bin Zhu,Patrick Carrington*

Main category: cs.AI

TL;DR: OSCAR是一种利用物体状态识别来支持无视觉烹饪的技术框架，通过整合食谱解析和实时步进跟踪，显著提高了烹饪步骤预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 烹饪对视力障碍者的日常生活至关重要，但缺乏跟踪进度和提供上下文反馈的支持。物体状态（食材和工具的变换状态）为上下文感知烹饪支持提供了新方向。

Method: OSCAR结合食谱解析、物体状态提取、烹饪步骤视觉对齐和时序因果建模，实现实时步进跟踪。在173个教学视频和12个真实无视觉烹饪会话中验证。

Result: 物体状态显著提升了跨视觉语言模型的步骤预测准确性，并揭示了影响性能的关键因素（如隐含任务、摄像头位置和光线）。

Conclusion: OSCAR为未来上下文感知辅助烹饪系统提供了技术框架、真实数据集和设计见解。

Abstract: Cooking plays a vital role in everyday independence and well-being, yet
remains challenging for people with vision impairments due to limited support
for tracking progress and receiving contextual feedback. Object status - the
condition or transformation of ingredients and tools - offers a promising but
underexplored foundation for context-aware cooking support. In this paper, we
present OSCAR (Object Status Context Awareness for Recipes), a technical
pipeline that explores the use of object status recognition to enable recipe
progress tracking in non-visual cooking. OSCAR integrates recipe parsing,
object status extraction, visual alignment with cooking steps, and time-causal
modeling to support real-time step tracking. We evaluate OSCAR on 173
instructional videos and a real-world dataset of 12 non-visual cooking sessions
recorded by BLV individuals in their homes. Our results show that object status
consistently improves step prediction accuracy across vision-language models,
and reveal key factors that impact performance in real-world conditions, such
as implicit tasks, camera placement, and lighting. We contribute the pipeline
of context-aware recipe progress tracking, an annotated real-world non-visual
cooking dataset, and design insights to guide future context-aware assistive
cooking systems.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [209] [AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations](https://arxiv.org/abs/2507.02877)
*Chi Zhang,Yu Dong,Yang Wang,Yuetong Han,Guihua Shan,Bixia Tang*

Main category: q-bio.GN

TL;DR: AuraGenome是一个基于大型语言模型（LLM）的框架，用于快速、可重用且可扩展的多层环形基因组可视化。


<details>
  <summary>Details</summary>
Motivation: 现有工具通常需要复杂的脚本和手动配置，导致过程耗时、易出错且难学。

Method: AuraGenome结合语义驱动的多代理工作流程和交互式可视化分析系统，使用七个专门化的LLM代理完成从原始数据到定制可视化的转换。

Result: 系统支持多视图、实时优化和高质量报告导出，并通过案例研究和用户研究验证了其有效性。

Conclusion: AuraGenome解决了基因组可视化中的效率和易用性问题，是一个高效的工具。

Abstract: Circular genome visualizations are essential for exploring structural
variants and gene regulation. However, existing tools often require complex
scripting and manual configuration, making the process time-consuming,
error-prone, and difficult to learn. To address these challenges, we introduce
AuraGenome, an LLM-powered framework for rapid, reusable, and scalable
generation of multi-layered circular genome visualizations. AuraGenome combines
a semantic-driven multi-agent workflow with an interactive visual analytics
system. The workflow employs seven specialized LLM-driven agents, each assigned
distinct roles such as intent recognition, layout planning, and code
generation, to transform raw genomic data into tailored visualizations. The
system supports multiple coordinated views tailored for genomic data, offering
ring, radial, and chord-based layouts to represent multi-layered circular
genome visualizations. In addition to enabling interactions and configuration
reuse, the system supports real-time refinement and high-quality report export.
We validate its effectiveness through two case studies and a comprehensive user
study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [210] [A Study of Gate-Based and Boson Sampling Quantum Random Number Generation on IBM and Xanadu Quantum Devices](https://arxiv.org/abs/2507.03823)
*Mohamed Messaoud Louamri,Achraf Boussahi,Nacer Eddine Belaloui,Abdellah Tounsi,Mohamed Taha Rouabah*

Main category: quant-ph

TL;DR: 该论文研究了从IBM Quantum的基于门电路和Xanadu Borealis的高斯玻色子采样中生成随机数的实用性，提出并评估了三种后处理方法的效率和成本。


<details>
  <summary>Details</summary>
Motivation: 量子测量的固有概率性提供了不可预测的熵源，适合用于安全的随机数生成，但实际应用中的效率和成本是需要解决的问题。

Method: 在IBM Quantum和Xanadu Borealis平台上实现随机数生成，使用Von Neumann提取器及其两种变体进行后处理，并通过NIST测试评估去偏效果。

Result: 两种平台均可生成无偏比特流，但吞吐量较低且每比特成本高于专用QRNG设备。

Conclusion: 量子随机数生成在安全性上有优势，但在效率和成本方面仍需改进。

Abstract: Quantum mechanics offers a fundamentally unpredictable entropy source due to
the intrinsic probabilistic nature of quantum measurements, making it
attractive for secure random number generation. This paper explores the
practicality of generating random numbers from two quantum platforms:
gate-based circuits on IBM Quantum and (Gaussian) boson sampling with Xanadu
Borealis. We implement simple post-processing methods, including the classic
Von Neumann extractor and two tailored variants designed to address the
correlated structure of boson sampling outputs. We evaluate debiased output
from real quantum hardware using the NIST SP800-22r1a test suite and measure
the extraction efficiency of each debiasing method. Results show that, while
unbiased bitstreams can be achieved on both platforms, throughput remains low
and cost per random bit is high compared to specialized QRNG devices.

</details>


### [211] [Node Replacement based Approximate Quantum Simulation with Decision Diagrams](https://arxiv.org/abs/2507.04335)
*Yexin Yan,Stefan Hillmich,Robert Wille,Christian Mayr*

Main category: quant-ph

TL;DR: 通过相似节点替换而不是删除不重要的节点，结合局部敏感哈希算法，提高量子电路决策图表征的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在模拟复杂量子电路时内存需求大且效率低，需在准确性和内存使用间进行权衡。

Method: 使用相似节点替换和局部敏感哈希算法优化决策图表征，减少计算复杂度。

Result: 新方法在高深度量子电路上展现了优于线性的内存-保真度权衡，显著降低了资源需求。

Conclusion: 该方法为经典计算机上近似模拟量子霸权基准电路提供了潜在的高效解决方案。

Abstract: Simulating a quantum circuit with a classical computer requires exponentially
growing resources. Decision diagrams exploit the redundancies in quantum
circuit representation to efficiently represent and simulate quantum circuits.
But for complicated quantum circuits like the quantum supremacy benchmark,
there is almost no redundancy to exploit. Therefore, it often makes sense to do
a trade-off between simulation accuracy and memory requirement. Previous work
on approximate simulation with decision diagrams exploits this trade-off by
removing less important nodes. In this work, instead of removing these nodes,
we try to find similar nodes to replace them, effectively slowing down the
fidelity loss when reducing the memory. In addition, we adopt Locality
Sensitive Hashing (LSH) to drastically reduce the computational complexity for
searching for replacement nodes. Our new approach achieves a better
memory-accuracy trade-off for representing a quantum circuit with decision
diagrams with minimal run time overhead. Notably, our approach shows good
scaling properties when increasing the circuit size and depth. For the first
time, a strong better-than-linear trade-off between memory and fidelity is
demonstrated for a decision diagram based quantum simulation when representing
the quantum supremacy benchmark circuits at high circuit depths, showing the
potential of drastically reducing the resource requirement for approximate
simulation of the quantum supremacy benchmarks on a classical computer.

</details>


### [212] [DYNAMO: Dynamic Neutral Atom Multi-programming Optimizer Towards Quantum Operating Systems](https://arxiv.org/abs/2507.04874)
*Wenjie Sun,Xiaoyu Li,Zhigang Wang,Geng Chen,Lianhui Yu,Guowu Yang*

Main category: quant-ph

TL;DR: DYNAMO是一种多程序量子编译优化方法，通过智能资源分配和并行编译提升量子计算资源利用率。


<details>
  <summary>Details</summary>
Motivation: 量子计算向实用化发展，量子操作系统需要多程序并发以提高硬件利用率，但现有方法多为单电路执行，限制了资源效率。

Method: 提出DYNAMO方法，在中性原子量子架构上实现多程序优化，解决资源分配和调度冲突问题。

Result: 实验显示，DYNAMO编译速度提升14.39倍，执行阶段减少50.47%，资源利用均衡。

Conclusion: DYNAMO为实用量子操作系统奠定了基础，提升了多程序并发执行效率。

Abstract: As quantum computing advances towards practical applications, quantum
operating systems become inevitable, where multi-programming -- the core
functionality of operating systems -- enables concurrent execution of multiple
quantum programs to enhance hardware utilization. However, most quantum
compilation work focuses solely on single-circuit execution, severely limiting
resource efficiency and hindering quantum operating system development. We
propose Dynamic Neutral Atom Multi-programming Optimizer (DYNAMO), a method
that realizes multi-programming on neutral atom quantum architectures through
parallel compilation and intelligent resource allocation across multiple
quantum processing units (QPUs). DYNAMO addresses two critical challenges:
inefficient and difficult resource partitioning, and complex scheduling
conflicts from concurrent program. Our method enables efficient spatial and
temporal resource sharing while maintaining circuit correctness and hardware
constraints. Experimental evaluation across circuits ranging from 12 to over
1200 gates demonstrates that DYNAMO achieves up to 14.39x compilation speedup
while reducing execution stages by an average of 50.47%. Furthermore, DYNAMO
successfully distributes workloads across multiple QPUs with balanced resource
utilization. By enabling efficient multi-programming capabilities, DYNAMO
establishes a critical foundation towards realizing practical quantum operating
systems.

</details>
