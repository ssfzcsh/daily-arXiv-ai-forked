<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [math.LO](#math.LO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 8]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 该论文提出了一种利用增强检索生成（RAG）管道的大型语言模型（LLM）的新型模型到文本/代码转换研究方向，专注于量子及混合量子-经典软件系统。通过实验验证了从UML模型实例生成代码的思路，并展示了良好设计的提示能够显著提高代码质量。


<details>
  <summary>Details</summary>
Motivation: 量子及混合量子-经典软件系统的异构平台和开发者技能不足增加了成本和风险，模型驱动方法可以缓解这些问题。

Method: 使用RAG管道增强LLM，从公开GitHub仓库中提取Qiskit代码样本，生成基于UML模型实例的量子代码。

Result: 实验表明，优化后的提示可将CodeBLEU分数提高多达四倍，生成更准确、一致的量子代码。

Conclusion: 该研究方向潜力巨大，未来可进一步探索模型实例作为RAG信息源或LLM用于代码转换（如转译用例）。

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [2] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: 论文提出了一种名为UTRL的强化学习框架，用于训练大型语言模型生成高质量单元测试，并通过对抗性训练提升测试质量。


<details>
  <summary>Details</summary>
Motivation: 当前用于自动生成单元测试的LLM方法训练不足，作者旨在通过对抗性训练提升测试生成质量。

Method: 采用对抗性强化学习框架，训练测试生成器和代码生成器互相优化，分别最大化鉴别奖励和代码奖励。

Result: UTRL生成的测试质量优于监督微调方法，甚至超越GPT-4.1等前沿模型。

Conclusion: UTRL在训练LLM生成高质量单元测试方面表现出色，验证了对抗性学习的有效性。

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [3] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: 提出一种轻量级框架，利用指令调优的LLM和LoRA适配器，结合候选约束解码，改进Bug分配任务。


<details>
  <summary>Details</summary>
Motivation: 解决大型项目中Bug分配任务效率低且不一致的问题。

Method: 使用指令调优的LLM和LoRA适配器，结合候选约束解码。

Result: 在EclipseJDT和Mozilla数据集上表现良好（Hit at 10达0.753），近期快照中准确率显著提升。

Conclusion: 指令调优的LLM可作为成本高昂的特征工程和基于图方法的实用替代方案。

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [4] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 研究发现，在大语言模型代理中，简单的观察掩码策略比复杂的汇总方法更经济且表现相当。


<details>
  <summary>Details</summary>
Motivation: 探讨在大语言模型代理中，复杂汇总方法是否比简单的忽略旧观察策略更具优势。

Method: 在SWE-agent上比较了观察掩码策略与LLM汇总策略，使用了五种模型配置。

Result: 观察掩码策略成本减半，且解决率与汇总相当，甚至略高。

Conclusion: 简单的上下文管理方法可能更有效且经济。

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [5] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 提出了LMPA，一种将大语言模型（LLMs）集成到指针分析中以提升精度和可扩展性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析框架因语义理解不足，导致对用户定义函数的处理过于保守，引入了错误事实的传播。

Method: LMPA通过LLMs识别类似系统API的用户定义函数并建模，改进基于摘要的分析，推断初始指向集并引入自然语言增强的摘要策略。

Result: LMPA缓解了跨调用上下文的错误传播，提升了分析的精度和可扩展性。

Conclusion: LMPA为指针分析引入新范式，但仍面临实现挑战。

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [6] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: 提出了一种名为MPTCS的多策略测试用例选择方法，用于提取基于可解性、多样性和通用难度的测试用例，以提高RL策略的可靠性验证。


<details>
  <summary>Details</summary>
Motivation: 验证RL代理策略的行为可靠性和性能仍然具有挑战性，当前测试方法生成的测试用例对特定策略依赖性强，通用性不足。

Method: 通过多策略集合从候选池中基于难度分数选择多样化的测试用例，并结合质量-多样性算法提升测试套件的多样性。

Result: 评估了难度分数的有效性以及方法效果与策略集合大小的关系，并分析了多样性方法对状态空间的覆盖和对策略行为的触发。

Conclusion: MPTCS能够生成多样化的、策略无关的测试用例，有效揭示代理行为的典型缺陷。

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [7] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: 论文比较了AI生成的代码与人类编写的代码在缺陷、安全漏洞和结构复杂性等方面的差异，揭示了AI代码的特点和质量问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI代码助手生成的代码与人类代码在可靠性、可维护性和安全性方面的差异，以推进AI辅助编程的质量保障实践。

Method: 通过对50万份Python和Java代码样本进行大规模分析，使用正交缺陷分类和通用弱点枚举评估缺陷和安全漏洞。

Result: AI生成的代码更简单但重复度高，容易包含未使用的结构和硬编码调试；人类代码结构复杂但可维护性差。AI代码的高风险安全漏洞更多。

Conclusion: AI和人类代码的缺陷特征不同，需专门的质量保障措施以提升AI辅助编程的可靠性。

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [8] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 摘要讨论了敏捷方法和DevOps在IT行业中的结合应用，通过访谈和主题分析，研究了敏捷方法在DevOps实践中的可行性和适用性。


<details>
  <summary>Details</summary>
Motivation: 随着IT行业对快速软件交付的需求增加，敏捷方法和DevOps的协作模式成为关键，研究旨在评估两者结合的可行性。

Method: 通过11次半结构化访谈，收集敏捷和DevOps从业者的意见，并进行主题分析，提取了51个代码并归纳为19个主题。

Result: 研究发现敏捷方法可以有效融入DevOps生命周期，提出了新的理解框架以满足研究目标。

Conclusion: 研究证实了敏捷方法在DevOps实践中的适用性，并为两者的整合提供了理论支持。

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL是一种通用编程语言翻译器，通过统一的中间表示CrossGL实现多语言间的双向翻译，简化了传统需要多对多翻译器的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每对语言单独开发翻译器，导致复杂度呈指数增长。CrossTL旨在通过统一中间表示简化这一过程，实现高效多语言互译。

Method: 系统包含语言特定的词法/语法分析器将源代码转为AST，双向CrossGL翻译模块（ToCrossGLConverter和CodeGen类）及后端实现，支持多种语言和平台。

Result: 通过多领域评估显示，CrossTL在所有支持的后端上成功编译和运行，验证了其有效性。统一IR设计降低了添加新语言的成本。

Conclusion: CrossTL是迈向语言无关编程的重要一步，实现了“一次编写，随处部署”的目标，为多语言互译提供了可行方案。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [10] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: 论文讨论了如何管理快速增长的Mathlib库，包括解决破坏性变更、代码质量分析、优化编译时间、处理技术债务和定制工具以支持审查。


<details>
  <summary>Details</summary>
Motivation: Mathlib库的快速增长需要有效的管理策略，以避免维护者过载和保持库的可持续性发展。

Method: 采用多种策略，如使用弃用系统处理破坏性变更、代码质量分析工具反馈常见问题、优化库设计以提高编译速度、解决技术债务以及开发定制工具支持新贡献的审查。

Result: 这些策略有效管理了库的增长，同时避免了维护者的过载。

Conclusion: 通过系统化的管理工具和方法，可以高效管理大型数学库的增长和维护。

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting](https://arxiv.org/abs/2508.21516)
*Federico Chiariotti,Fabio Saggese,Andrea Munari,Leonardo Badia,Petar Popovski*

Main category: cs.NI

TL;DR: 论文提出了一种动态调度通信资源的推拉调度器（PPS），用于平衡数字孪生（DT）的正常对齐与异常报告，优化资源使用并显著减少信息漂移。


<details>
  <summary>Details</summary>
Motivation: 数字孪生的准确性依赖于与物理系统的实时同步，而现有方法在资源分配和异常检测效率上存在不足。

Method: 设计了推拉调度器（PPS）框架，动态分配用于拉取更新和推送更新的通信资源。

Result: PPS将漂移年龄（AoII）降低了20%以上，同时保持了相同的异常检测能力，并将最坏情况下的异常检测AoII从70 ms降至20 ms。

Conclusion: PPS框架在数字孪生同步和异常检测方面表现出显著优势，优化了资源使用和信息准确性。

Abstract: A digital twin (DT) contains a set of virtual models of real systems and
processes that are synchronized to their physical counterparts. This enables
experimentation and examination of counterfactuals, simulating the consequences
of decisions in real time. However, the DT accuracy relies on timely updates
that maintain alignment with the real system. We can distinguish between: (i)
pull-updates, which follow a request from the DT to the sensors, to decrease
its drift from the physical state; (ii) push-updates, which are sent directly
by the sensors since they represent urgent information, such as anomalies. In
this work, we devise a push-pull scheduler (PPS) medium access framework, which
dynamically allocates the communication resources used for these two types of
updates. Our scheme strikes a balance in the trade-off between DT alignment in
normal conditions and anomaly reporting, optimizing resource usage and reducing
the drift age of incorrect information (AoII) by over 20% with respect to
state-of-the-art solutions, while maintaining the same anomaly detection
guarantees, as well as reducing the worst-case anomaly detection AoII from 70
ms to 20 ms when considering a 1 ms average drift AoII constraint.

</details>


### [12] [QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective](https://arxiv.org/abs/2508.21783)
*Mohamed Seliem,Utz Roedig,Cormac Sreenan,Dirk Pesch*

Main category: cs.NI

TL;DR: 论文提出一种扩展Simu5G的方法，支持按QFI建模，并引入新型QoS-PF调度器，优化工业5G中的资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有模拟框架缺乏对多流行为的QFI级建模能力，阻碍了智能工厂中5G网络的QoS研究。

Method: 扩展Simu5G以支持按QFI建模，设计动态平衡延迟、GBR和优先级的QoS-PF调度器。

Result: 在智能工厂场景中，QoS-PF提升截止时间遵守率和公平性，同时保持吞吐量。

Conclusion: 研究为工业5G中的高级QoS策略模拟与分析提供方法和架构基础。

Abstract: Private 5G networks are emerging as key enablers for smart factories, where a
single device often handles multiple concurrent traffic flows with distinct
Quality of Service (QoS) requirements. Existing simulation frameworks, however,
lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier
(QFI) level. This paper addresses this gap by extending Simu5G to support
per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness
(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit
Rate (GBR), and priority metrics to optimize resource allocation across
heterogeneous flows. We evaluate the proposed approach in a realistic smart
factory scenario featuring edge-hosted machine vision, real-time control loops,
and bulk data transfer. Results show that QoS-PF improves deadline adherence
and fairness without compromising throughput. All extensions are implemented in
a modular and open-source manner to support future research. Our work provides
both a methodological and architectural foundation for simulating and analyzing
advanced QoS policies in industrial 5G deployments.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [13] [lifeXplore at the Lifelog Search Challenge 2020](https://arxiv.org/abs/2508.21397)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: LSC是一个交互式比赛，目的是检索个人生活日志。作者介绍了改进的lifeXplore系统，结合了多种技术手段。


<details>
  <summary>Details</summary>
Motivation: 通过改进lifeXplore系统，提升在LSC比赛中快速检索个人生活日志的能力。

Method: 结合特征地图浏览、概念搜索、过滤、手绘草图、YOLO9000、OCR和均匀采样等技术。

Result: 系统在LSC2018和LSC2019中得到应用并改进。

Conclusion: 改进的系统提供了更高效的生活日志检索工具。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC) - an
interactive competition for retrieving lifelogging moments - is co-located at
the annual ACM International Conference on Multimedia Retrieval (ICMR) and has
drawn international attention. With the goal of making an ever growing public
lifelogging dataset searchable, several teams develop systems for quickly
solving time-limited queries during the challenge. Having participated in both
previous LSC iterations, i.e. LSC2018 and LSC2019, we present our lifeXplore
system - a video exploration and retrieval tool combining feature map browsing,
concept search and filtering as well as hand-drawn sketching. The system is
improved by including additional deep concept YOLO9000, optical character
recognition (OCR) as well as adding uniform sampling as an alternative to the
system's traditional underlying shot segmentation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [14] [Interpolation for Converse PDL](https://arxiv.org/abs/2508.21485)
*Johannes Kloibhofer,Valentina Trucco Dalmas,Yde Venema*

Main category: cs.LO

TL;DR: Converse PDL扩展了命题动态逻辑，增加了程序的反向操作。主要成果表明其具有Craig插值性和Beth可定义性，基于证明论方法的证明引入了一个完整的循环序列系统。


<details>
  <summary>Details</summary>
Motivation: 研究Converse PDL的逻辑性质，特别是插值性和可定义性，以验证其在形式逻辑中的表达能力。

Method: 通过Maehara的证明论方法，引入了一个带有分析切割规则和焦点机制的循环序列系统。

Result: 证明了Converse PDL在原子程序和命题变量上具有Craig插值性和Beth可定义性。

Conclusion: Converse PDL的逻辑性质已验证，其循环序列系统为证明提供了有效工具。

Abstract: Converse PDL is the extension of propositional dynamic logic with a converse
operation on programs. Our main result states that Converse PDL enjoys the
(local) Craig Interpolation Property, with respect to both atomic programs and
propositional variables. As a corollary we establish the Beth Definability
Property for the logic.
  Our interpolation proof is based on an adaptation of Maehara's
proof-theoretic method. For this purpose we introduce a sound and complete
cyclic sequent system for this logic. This calculus features an analytic cut
rule and uses a focus mechanism for recognising successful cycles.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [15] [Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?](https://arxiv.org/abs/2508.21087)
*Bin Han,Deuksin Kwon,Spencer Lin,Kaleen Shrestha,Jonathan Gratch*

Main category: cs.HC

TL;DR: 该研究提出了一种利用大型语言模型（LLM）生成符合人格特质的虚拟代理行为的框架，重点关注外向性，并通过实验验证了生成行为的准确性及用户的感知能力。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用LLM生成符合人格特质的虚拟代理行为，以提升虚拟代理的自然性和个性化。

Method: 设计了两个实验：实验一通过代理对代理模拟进行语言分析和人格分类评估；实验二通过用户研究验证行为的可感知性。

Result: 结果显示LLM可以生成符合人格特质的行为，并且用户能够通过行为识别这些特质。

Conclusion: 研究表明LLM在塑造人格一致的虚拟代理方面具有潜力。

Abstract: This study proposes a framework that employs personality prompting with Large
Language Models to generate verbal and nonverbal behaviors for virtual agents
based on personality traits. Focusing on extraversion, we evaluated the system
in two scenarios: negotiation and ice breaking, using both introverted and
extroverted agents. In Experiment 1, we conducted agent to agent simulations
and performed linguistic analysis and personality classification to assess
whether the LLM generated language reflected the intended traits and whether
the corresponding nonverbal behaviors varied by personality. In Experiment 2,
we carried out a user study to evaluate whether these personality aligned
behaviors were consistent with their intended traits and perceptible to human
observers. Our results show that LLMs can generate verbal and nonverbal
behaviors that align with personality traits, and that users are able to
recognize these traits through the agents' behaviors. This work underscores the
potential of LLMs in shaping personality aligned virtual agents.

</details>


### [16] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 该论文通过两项研究探讨了巴西儿童（9-11岁）如何利用对话代理（CAs）完成学业、探索和娱乐，并分析了结构化支架如何提升这些互动效果。研究1通过七周的在线调查揭示了儿童的三种CA功能和使用模式；研究2通过模拟对话验证了结构化提示优于非结构化基线，并提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 动机是探索巴西儿童与对话代理的互动行为，并通过结构化支架提升互动效果，以支持儿童的学习、探索和娱乐需求。

Method: 研究1采用访谈、观察和认知工作分析方法，研究2则利用GPT-4o-mini模拟儿童-CA对话，比较结构化与非结构化方法的差异。

Result: 结果显示结构化支架（"配方"提示）在可读性、问题深度/多样性和连贯性上优于非结构化方法。

Conclusion: 结论是结构化对话树、儿童专用配置和家长定制内容是优化儿童-CA互动的关键设计建议。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [17] [Design and evaluation of a serious game in virtual reality to increase empathy towards students with phonological dyslexia](https://arxiv.org/abs/2508.21283)
*Jose Manuel Alcalde-Llergo,Andrea Zingoni,Pilar Aparicio-Martinez,Sara Pinzi,Enrique Yeguas-Bolivar*

Main category: cs.HC

TL;DR: 这篇论文研究了如何通过虚拟现实游戏提升非阅读障碍群体对阅读障碍患者的同理心。


<details>
  <summary>Details</summary>
Motivation: 阅读障碍（尤其是音韵性阅读障碍）影响了部分人群的阅读能力，但社会对此认识不足，支持方法的重要性常被低估。

Method: 研究者开发了一款面向教育者和非阅读障碍群体的虚拟现实游戏，模拟阅读障碍者的日常挑战。

Result: 101名参与者测试后，数据显示游戏的同理心提升效果显著，平均提高20%。

Conclusion: 虚拟现实工具能有效增强对阅读障碍的理解和支持意愿。

Abstract: Dyslexia is a neurodevelopmental disorder estimated to strike approximately 5
to 10 per cent of the population. In particular, phonological dyslexia causes
problems in connecting the sounds of words with their written forms.
Consequently, affected individuals may encounter issues such as slow reading
speed, inaccurate reading, and difficulty decoding unfamiliar words. To address
these complexities, the use of compensatory tools and strategies is essential
to ensure equitable opportunities for dyslexic students. However, the general
underestimation of the issue and lack of awareness regarding the significance
of support methodologies pose significant obstacles. One of the ways to enhance
consciousness towards a certain issue is by stimulating empathy with whom is
affected by it. In light of this, this study introduces a serious game in
virtual reality, targeted at educators, students, and, in general, at the
non-dyslexic community. The game seeks to enhance understanding of the
challenges that individuals with dyslexia experience daily, highlighting the
relevance of supportive measures. This approach encourages players to empathize
with the struggles of dyslexic individuals and to learn firsthand the
importance of supportive methodologies. The final version of the experience was
tested by 101 participants and evaluated through a specific collection of
questionnaires validated in the literature. The results show that using the
proposed virtual reality tool to promote empathy for individuals with
phonological dyslexia is highly effective, leading to an average 20 per cent
increase in participants' empathy after playing the game.

</details>


### [18] [Conflict in Community-Based Design: A Case Study of a Relationship Breakdown](https://arxiv.org/abs/2508.21308)
*Alekhya Gandu,Aakash Gautam*

Main category: cs.HC

TL;DR: 社区设计中遇到价值观冲突时，设计师应如何应对有害结构的问题，探讨了设计与反抗压迫之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 研究目的是在设计过程中如何处理与社区成员价值观冲突的问题，尤其是当这些价值观可能导致压迫结构时。

Method: 通过与非营利组织两年合作，分析知识管理和转移中的问题，并在设计中抵制有害的种姓制度实践。

Result: 设计干预导致与合作伙伴关系破裂，但提出了设计师在价值观冲突中的多元路径。

Conclusion: 建议设计师在合作前预留反思空间，调整角色或方法，必要时退出，以灵活应对复杂价值冲突。

Abstract: Community-based design efforts rightly seek to reduce the power differences
between researchers and community participants by aligning with community
values and furthering their priorities. However, what should designers do when
key community members' practices seem to enact an oppressive and harmful
structure? We reflect on our two-year-long engagement with a non-profit
organization in southern India that supports women subjected to domestic abuse
or facing mental health crises. We highlight the organizational gaps in
knowledge management and transfer, which became an avenue for our design
intervention. During design, we encountered practices that upheld caste
hierarchies. These practices were expected to be incorporated into our
technology. Anticipating harms to indirect stakeholders, we resisted this
incorporation. It led to a breakdown in our relationship with the partner
organization. Reflecting on this experience, we outline pluralistic pathways
that community-based designers might inhabit when navigating value conflicts.
These include making space for reflection before and during engagements,
strategically repositioning through role reframing or appreciative inquiry, and
exiting the engagement if necessary.

</details>


### [19] [MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality](https://arxiv.org/abs/2508.21736)
*Simon Burbach,Maria Maleshkova,Florian Centler,Tanja Joan Schmidt*

Main category: cs.HC

TL;DR: 该研究提出了一种名为MicroLabVR的用户友好工具，通过虚拟现实技术帮助用户交互式探索微生物组的时空模拟数据。


<details>
  <summary>Details</summary>
Motivation: 微生物组对人体健康至关重要，但现有工具功能有限且操作复杂，MicroLabVR旨在解决这些问题。

Method: 开发了一个基于VR的工具，支持导入CSV数据集（如种群增长、物质浓度和代谢通量数据），并提供交互式可视化功能。

Result: MicroLabVR实现了微生物组数据的空间上下文分析，提升了用户体验和数据探索效率。

Conclusion: MicroLabVR为微生物组研究提供了一个直观且易用的工具，有望促进相关领域的科学发现。

Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food
digestion and immune defense. Their structure and function must be understood
in order to promote host health and facilitate swift recovery during disease.
Due to the difficulties in experimentally studying these systems in situ, more
research is being conducted in the field of mathematical modeling. Visualizing
spatiotemporal data is challenging, and current tools that simulate microbial
communities' spatial and temporal development often only provide limited
functionalities, often requiring expert knowledge to generate useful results.
To overcome these limitations, we provide a user-friendly tool to interactively
explore spatiotemporal simulation data, called MicroLabVR, which transfers
spatial data into virtual reality (VR) while following guidelines to enhance
user experience (UX). With MicroLabVR, users can import CSV datasets containing
population growth, substance concentration development, and metabolic flux
distribution data. The implemented visualization methods allow users to
evaluate the dataset in a VR environment interactively. MicroLabVR aims to
improve data analysis for the user by allowing the exploration of microbiome
data in their spatial context.

</details>


### [20] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: 摘要介绍了一种名为Morae的UI代理，旨在通过识别决策点并暂停任务以征求用户选择，从而提高盲人和低视力用户在界面操作中的自主性。


<details>
  <summary>Details</summary>
Motivation: 现有UI代理通常在不征求用户意见的情况下完成任务，降低了用户体验和自主性。研究旨在通过改进代理设计，增强用户的参与感和选择权。

Method: 提出Morae代理，利用多模态模型解析用户查询和界面信息，在决策点暂停并征求用户选择。

Result: 在实际网络任务测试中，Morae比基线代理（如OpenAI Operator）帮助用户更高效完成任务且更符合用户偏好。

Conclusion: 这一工作展示了混合主动性方法的优势，既利用自动化提升效率，又通过用户参与确保选择权。

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


### [21] [Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education](https://arxiv.org/abs/2508.21666)
*Imran S. A. Khan,Emmanuel G. Blanchard,Sébastien George*

Main category: cs.HC

TL;DR: FACTS结合物联网和生成式AI，通过实时大气数据和个性化反馈提升气候韧性教育效果。


<details>
  <summary>Details</summary>
Motivation: 通过技术手段提升气候韧性教育的参与度和效果，应对气候变化挑战。

Method: 使用物联网传感器收集实时大气数据，结合生成式AI动态生成个性化学习任务和反馈。

Result: 用户评价显示系统易用且有效，提升了气候韧性知识的掌握。

Conclusion: 物联网和生成式AI的结合为气候教育提供了创新且有前景的解决方案。

Abstract: This paper introduces the Future Atmospheric Conditions Training System
(FACTS), a novel platform that advances climate resilience education through
place-based, adaptive learning experiences. FACTS combines real-time
atmospheric data collected by IoT sensors with curated resources from a
Knowledge Base to dynamically generate localized learning challenges. Learner
responses are analyzed by a Generative AI powered server, which delivers
personalized feedback and adaptive support. Results from a user evaluation
indicate that participants found the system both easy to use and effective for
building knowledge related to climate resilience. These findings suggest that
integrating IoT and Generative AI into atmospherically adaptive learning
technologies holds significant promise for enhancing educational engagement and
fostering climate awareness.

</details>


### [22] [Developer Insights into Designing AI-Based Computer Perception Tools](https://arxiv.org/abs/2508.21733)
*Maya Guhan,Meghan E. Hurley,Eric A. Storch,John Herrington,Casey Zampella,Julia Parish-Morris,Gabriel Lázaro-Muñoz,Kristin Kostick-Quenet*

Main category: cs.HC

TL;DR: 论文探讨了AI计算机感知技术在临床决策中的应用，分析了开发者在设计中的四大优先事项，并提出建议以实现技术与临床需求的平衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解AI工具开发者如何在临床效用与用户接受度之间取得平衡，以促进技术的有效整合。

Method: 通过20次深度访谈，采用归纳主题分析法，识别出设计中的关键优先事项。

Result: 研究发现四大设计重点：解释性、临床流程匹配、用户接受度和创新与传统的平衡。

Conclusion: 开发者需扮演伦理管家角色，通过透明设计和跨学科合作实现技术成功整合。

Abstract: Artificial intelligence (AI)-based computer perception (CP) technologies use
mobile sensors to collect behavioral and physiological data for clinical
decision-making. These tools can reshape how clinical knowledge is generated
and interpreted. However, effective integration of these tools into clinical
workflows depends on how developers balance clinical utility with user
acceptability and trustworthiness. Our study presents findings from 20 in-depth
interviews with developers of AI-based CP tools. Interviews were transcribed
and inductive, thematic analysis was performed to identify 4 key design
priorities: 1) to account for context and ensure explainability for both
patients and clinicians; 2) align tools with existing clinical workflows; 3)
appropriately customize to relevant stakeholders for usability and
acceptability; and 4) push the boundaries of innovation while aligning with
established paradigms. Our findings highlight that developers view themselves
as not merely technical architects but also ethical stewards, designing tools
that are both acceptable by users and epistemically responsible (prioritizing
objectivity and pushing clinical knowledge forward). We offer the following
suggestions to help achieve this balance: documenting how design choices around
customization are made, defining limits for customization choices,
transparently conveying information about outputs, and investing in user
training. Achieving these goals will require interdisciplinary collaboration
between developers, clinicians, and ethicists.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [23] [ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes](https://arxiv.org/abs/2508.21095)
*Thomas Besnier,Sylvain Arguillère,Mohamed Daoudi*

Main category: cs.GR

TL;DR: 提出了一种无需绑定的数据驱动框架，用于处理未注册的3D扫描网格的运动预测和转移，通过结合动作嵌入网络和顶点特征场生成时空变形场。


<details>
  <summary>Details</summary>
Motivation: 未注册的3D扫描网格由于缺乏点对点对应关系和噪声问题，难以自动计算合理的变形，因此需要新的解决方法。

Method: 结合鲁棒的动作嵌入网络和学习的顶点特征场，生成时空变形场以驱动网格变形。

Result: 在行走和跑步等任务上进行了定量和定性评估，证明了该方法在具有挑战性的未注册网格上的有效性和多功能性。

Conclusion: 该框架在处理未注册3D扫描网格的运动预测和变形问题上表现出色。

Abstract: Unregistered surface meshes, especially raw 3D scans, present significant
challenges for automatic computation of plausible deformations due to the lack
of established point-wise correspondences and the presence of noise in the
data. In this paper, we propose a new, rig-free, data-driven framework for
motion prediction and transfer on such body meshes. Our method couples a robust
motion embedding network with a learned per-vertex feature field to generate a
spatio-temporal deformation field, which drives the mesh deformation. Extensive
evaluations, including quantitative benchmarks and qualitative visuals on tasks
such as walking and running, demonstrate the effectiveness and versatility of
our approach on challenging unregistered meshes.

</details>


### [24] [ARGS: Advanced Regularization on Aligning Gaussians over the Surface](https://arxiv.org/abs/2508.21344)
*Jeong Uk Lee,Sung Hee Choi*

Main category: cs.GR

TL;DR: 本文提出两种正则化策略，改进3D高斯泼溅（3DGS）的重建质量，包括有效秩正则化和神经SDF集成，以提升视觉保真度和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SuGaR）在渲染方面效果显著，但在视觉保真度和场景一致性方面仍有改进空间。

Method: 1. 引入有效秩正则化，避免高斯原语过度各向异性；2. 结合神经SDF优化，通过Eikonal损失维持距离特性并提供全局表面先验。

Result: 模型能从3DGS数据中生成更准确和一致的视觉输出。

Conclusion: 两种正则化策略协同提升了高斯原语的个体行为和集体表面一致性。

Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian
Splatting(3DGS) still remains a central challenge in computer graphics.
Although existing models such as SuGaR offer effective solutions for rendering,
there is is still room to improve improve both visual fidelity and scene
consistency. This work builds upon SuGaR by introducing two complementary
regularization strategies that address common limitations in both the shape of
individual Gaussians and the coherence of the overall surface. The first
strategy introduces an effective rank regularization, motivated by recent
studies on Gaussian primitive structures. This regularization discourages
extreme anisotropy-specifically, "needle-like" shapes-by favoring more
balanced, "disk-like" forms that are better suited for stable surface
reconstruction. The second strategy integrates a neural Signed Distance
Function (SDF) into the optimization process. The SDF is regularized with an
Eikonal loss to maintain proper distance properties and provides a continuous
global surface prior, guiding Gaussians toward better alignment with the
underlying geometry. These two regularizations aim to improve both the fidelity
of individual Gaussian primitives and their collective surface behavior. The
final model can make more accurate and coherent visuals from 3DGS data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [25] [Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores](https://arxiv.org/abs/2508.21230)
*Brian Curless,Michael Gowanlock*

Main category: cs.DC

TL;DR: 论文提出了一种名为FaSTED的算法，利用GPU的张量核心（TCs）进行欧几里得距离计算，通过层次化数据复用和最大化内存利用率，实现了高性能。与现有技术相比，混合精度（FP16-32）方法的性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的张量核心（TCs）通常用于人工智能中的矩阵乘法，但其高计算吞吐量也适用于其他算法。本文探讨了如何利用TCs高效计算数据分析和相似性搜索中常用的欧几里得距离。

Method: 设计了FaSTED算法，通过层次化数据复用和最大化内存利用率（全局内存、共享内存和寄存器）实现高性能。采用了混合精度（FP16-32）计算，避免了TCs的数据饥饿问题。

Result: 在四个高维数据集（128-960维）上，FaSTED比现有TC算法（FP64）快2.5-51倍，同时混合精度算法的精度损失小于0.06%。

Conclusion: FaSTED算法通过混合精度和层次化数据复用，显著提升了欧几里得距离计算的性能，且在精度损失极小的情况下实现了高速度，适用于数据分析和相似性搜索。

Abstract: Modern GPUs are equipped with tensor cores (TCs) that are commonly used for
matrix multiplication in artificial intelligence workloads. However, because
they have high computational throughput, they can lead to significant
performance gains in other algorithms if they can be successfully exploited. We
examine using TCs to compute Euclidean distance calculations, which are used in
many data analytics applications. Prior work has only investigated using 64 bit
floating point (FP64) data for computation; however, TCs can operate on lower
precision floating point data (i.e., 16 bit matrix multiplication and 32 bit
accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so
high that TCs are easily starved of data. We propose a Fast and Scalable Tensor
core Euclidean Distance (FaSTED) algorithm. To achieve high computational
throughput, we design FaSTED for significant hierarchical reuse of data and
maximize memory utilization at every level (global memory, shared memory, and
registers). We apply FaSTED to the application of similarity searches, which
typically employ an indexing data structure to eliminate superfluous Euclidean
distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean
distance algorithm in the literature that employs FP64, as well as to two
single precision (FP32) CUDA core algorithms that both employ an index. We find
that across four real-world high-dimensional datasets spanning 128-960
dimensions, the mixed-precision brute force approach achieves a speedup over
the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed
precision algorithm to be less than <0.06% when compared to the FP64 baseline.

</details>


### [26] [Decentralized Federated Averaging via Random Walk](https://arxiv.org/abs/2508.21286)
*Changheng Wang,Zhiqing Wei,Lizhe Liu,Qiao Deng,Yingda Wu,Yangyang Niu,Yashan Pang,Zhiyong Feng*

Main category: cs.DC

TL;DR: 该论文研究了基于随机行走的去中心化联邦平均（DFedRW），解决了传统联邦学习中数据异构性和不平衡导致的收敛慢和次优模型问题，并通过量化版本进一步提高了通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式联邦学习可能引入单点故障和隐私泄露问题，而去中心化联邦学习虽增强了鲁棒性和隐私保护，但在数据异构性下仍面临收敛慢和次优模型问题。本研究旨在解决这些问题。

Method: 提出DFedRW算法，通过随机行走更新替代多轮本地更新，允许部分随机行走更新聚合以减少数据损失。同时引入量化版本进一步提高通信效率。

Result: 证明了DFedRW在凸条件下的收敛上界为$\mathcal{O}(\frac{1}{k^{1-q}})$，数值分析显示该算法在异构数据下比传统联邦平均（FedAvg）收敛更快，测试精度提高了38.3%和37.5%。

Conclusion: DFedRW及其量化版本在去中心化联邦学习中表现优异，尤其在异构数据环境下显著提升了收敛速度和模型精度。

Abstract: Federated Learning (FL) is a communication-efficient distributed machine
learning method that allows multiple devices to collaboratively train models
without sharing raw data. FL can be categorized into centralized and
decentralized paradigms. The centralized paradigm relies on a central server to
aggregate local models, potentially resulting in single points of failure,
communication bottlenecks, and exposure of model parameters. In contrast, the
decentralized paradigm, which does not require a central server, provides
improved robustness and privacy. The essence of federated learning lies in
leveraging multiple local updates for efficient communication. However, this
approach may result in slower convergence or even convergence to suboptimal
models in the presence of heterogeneous and imbalanced data. To address this
challenge, we study decentralized federated averaging via random walk (DFedRW),
which replaces multiple local update steps on a single device with random walk
updates. Traditional Federated Averaging (FedAvg) and its decentralized
versions commonly ignore stragglers, which reduces the amount of training data
and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial
random walk updates, ensuring that each computation contributes to the model
update. To further improve communication efficiency, we also propose a
quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves
convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex
conditions. Furthermore, we propose a sufficient condition that reveals when
quantization balances communication and convergence. Numerical analysis
indicates that our proposed algorithms outperform (decentralized) FedAvg in
both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in
test accuracy under high levels of heterogeneities.

</details>


### [27] [Addressing Reproducibility Challenges in HPC with Continuous Integration](https://arxiv.org/abs/2508.21289)
*Valérie Hayot-Sasson,Nathaniel Hudson,André Bauer,Maxime Gonthier,Ian Foster,Kyle Chard*

Main category: cs.DC

TL;DR: 该论文讨论了高性能计算（HPC）领域中可重复性研究的挑战，提出了一种名为CORRECT的GitHub Action工具，用于在远程HPC资源上安全执行测试，以提升应用的可重复性。


<details>
  <summary>Details</summary>
Motivation: 由于HPC基础设施和软件的独特性和严格的访问限制，许多论文难以满足可重复性要求，因此需要替代方案。

Method: 通过调查现有可重复性倡议和障碍，作者开发了CORRECT工具，支持在远程HPC资源上自动化测试和文档记录。

Result: CORRECT工具在三种不同类型的HPC应用中验证了其有效性，能够自动化并记录可重复性评估。

Conclusion: 更好的HPC兼容CI解决方案（如CORRECT）可以显著提升应用的可重复性。

Abstract: The high-performance computing (HPC) community has adopted incentive
structures to motivate reproducible research, with major conferences awarding
badges to papers that meet reproducibility requirements. Yet, many papers do
not meet such requirements. The uniqueness of HPC infrastructure and software,
coupled with strict access requirements, may limit opportunities for
reproducibility. In the absence of resource access, we believe that regular
documented testing, through continuous integration (CI), coupled with complete
provenance information, can be used as a substitute. Here, we argue that better
HPC-compliant CI solutions will improve reproducibility of applications. We
present a survey of reproducibility initiatives and describe the barriers to
reproducibility in HPC. To address existing limitations, we present a GitHub
Action, CORRECT, that enables secure execution of tests on remote HPC
resources. We evaluate CORRECT's usability across three different types of HPC
applications, demonstrating the effectiveness of using CORRECT for automating
and documenting reproducibility evaluations.

</details>


### [28] [A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling](https://arxiv.org/abs/2508.21328)
*Zhiyu Wang,Mohammad Goudarzi,Mingming Gong,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 论文提出KD-AFRL框架，利用知识蒸馏和自适应联邦强化学习解决IoT分布式调度中的异构性和非IID数据问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决IoT在云边端环境中分布式调度的挑战，包括异构计算、非IID数据分布和跨域协作不足。

Method: 提出双区域神经网络生成、隐私保护联邦学习和跨架构知识蒸馏三大创新机制。

Result: 实验显示KD-AFRL比基线收敛快21%，性能提升15.7%~13.9%，可扩展性提高3-5倍。

Conclusion: KD-AFRL能有效优化多域IoT应用调度，适用于异构环境。

Abstract: The rapid proliferation of Internet of Things (IoT) applications across
heterogeneous Cloud-Edge-IoT environments presents significant challenges in
distributed scheduling optimization. Existing approaches face issues, including
fixed neural network architectures that are incompatible with computational
heterogeneity, non-Independent and Identically Distributed (non-IID) data
distributions across IoT scheduling domains, and insufficient cross-domain
collaboration mechanisms. This paper proposes KD-AFRL, a Knowledge
Distillation-empowered Adaptive Federated Reinforcement Learning framework that
addresses multi-domain IoT application scheduling through three core
innovations. First, we develop a resource-aware hybrid architecture generation
mechanism that creates dual-zone neural networks enabling heterogeneous devices
to participate in collaborative learning while maintaining optimal resource
utilization. Second, we propose a privacy-preserving environment-clustered
federated learning approach that utilizes differential privacy and K-means
clustering to address non-IID challenges and facilitate effective collaboration
among compatible domains. Third, we introduce an environment-oriented
cross-architecture knowledge distillation mechanism that enables efficient
knowledge transfer between heterogeneous models through temperature-regulated
soft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure
demonstrate KD-AFRL's effectiveness using diverse IoT applications. Results
show significant improvements over the best baseline, with 21% faster
convergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,
energy consumption, and weighted cost, respectively. Scalability experiments
reveal that KD-AFRL achieves 3-5 times better performance retention compared to
existing solutions as the number of domains increases.

</details>


### [29] [Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage](https://arxiv.org/abs/2508.21473)
*Daniil Vostrikov,Yash Madhwal,Andrey Seoev,Anastasiia Smirnova,Yury Yanovich,Alexey Smirnov,Vladimir Gorgadze*

Main category: cs.DC

TL;DR: 本文探讨了Polygon区块链上的MEV问题，尤其是原子套利交易，分析了其行为、盈利模式及对网络的影响。


<details>
  <summary>Details</summary>
Motivation: 区块链技术的演变带来了MEV等新挑战，研究目的是理解MEV在Polygon链上的动态及其影响。

Method: 使用22个月的数据集，分析了2300万个区块，重点关注Spam-based和Auction-based策略。

Result: Spam-based交易更常见，但Auction-based交易盈利更高。

Conclusion: 研究强调了对强大交易排序机制的需求，并揭示了MEV策略对区块链网络的潜在影响。

Abstract: The evolution of blockchain technology, from its origins as a decentralized
ledger for cryptocurrencies to its broader applications in areas like
decentralized finance (DeFi), has significantly transformed financial
ecosystems while introducing new challenges such as Maximum Extractable Value
(MEV). This paper explores MEV on the Polygon blockchain, with a particular
focus on Atomic Arbitrage (AA) transactions. We establish criteria for
identifying AA transactions and analyze key factors such as searcher behavior,
bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and
covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based
and Auction-based backrunning strategies. Our findings reveal that while
Spam-based transactions are more prevalent, Auction-based transactions
demonstrate greater profitability. Through detailed examples and analysis, we
investigate the interactions between network architecture, transaction
sequencing, and MEV extraction, offering comprehensive insights into the
evolution and challenges of MEV in decentralized ecosystems. These results
emphasize the need for robust transaction ordering mechanisms and highlight the
implications of emerging MEV strategies for blockchain networks.

</details>


### [30] [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)
*Yuhang Zhou,Zhibin Wang,Peng Jiang,Haoran Xia,Junhe Lu,Qianyu Jiang,Rong Gu,Hengxi Xu,Xinjing Huang,Guanghuan Fang,Zhiheng Hu,Jingyi Zhang,Yongjin Cai,Jian He,Chen Tian*

Main category: cs.DC

TL;DR: Odyssey是一种自适应容错系统，通过智能选择最佳恢复策略，减少大型语言模型训练中的中断。


<details>
  <summary>Details</summary>
Motivation: 现有方法在容错时存在性能损失，需要一种更高效的解决方案。

Method: Odyssey结合统一性能模型、快速执行计划搜索、准确性能估计和通信优化。

Result: 在32卡集群上，Odyssey的恢复后性能损失仅11.00%，且吞吐量优于现有方法。

Conclusion: Odyssey在容错和性能上实现了显著改进。

Abstract: Training large language models faces frequent interruptions due to various
faults, demanding robust fault-tolerance. Existing backup-free methods, such as
redundant computation, dynamic parallelism, and data rerouting, each incur
performance penalties, whether from ongoing overhead, lengthy reconfigurations,
or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant
system that intelligently selects optimal recovery strategies when a failure
occurs. Odyssey achieves this through a unified performance model, expedient
execution plan search, accurate performance estimation, and efficient
communication optimizations. Experiments on a 32-card cluster show that Odyssey
maintains a performance gap of within 11.00% between post-recovery and
failure-free training, while preserving model convergence and efficient memory
usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and
1.355x higher average throughput than Oobleck and Recycle, respectively.

</details>


### [31] [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)
*Zhibin Wang,Zhonghui Zhang,Yuhang Zhou,Zibo Wang,Mo Zhou,Peng Jiang,Weilin Cai,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: SpecMoEOff利用推测解码技术提升MoE模型的硬件利用率，通过GPU和CPU协同工作，显著提高解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型卸载技术因I/O瓶颈和稀疏计算导致硬件利用率低。

Method: 提出SpecMoEOff，结合推测解码技术，通过理论分析和实证优化CPU和GPU工作负载，并开发专用注意力验证内核。

Result: 实验显示解码吞吐量提升2.5倍。

Conclusion: SpecMoEOff显著优于现有MoE卸载技术。

Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly
increased their parameter scale as well as model performance. Extensive
offloading techniques have been proposed to address the GPU memory limitations
of MoE inference. However, due to the I/O bottleneck and sparse computation of
MoE models, existing offloading techniques still suffer from low hardware
utilization. To fully utilize the hardware resources, we propose SpecMoEOff,
which employs the speculative decoding technique to enlarge the workload of
each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and
empirical roofline analysis. In addition, we develop a dedicated CPU chunked
attention verification kernel to fit the speculative decoding in offloading
scenarios as well as minimizing the additional overhead led by draft models.
SpecMoEOff further integrates an optimizer to automatically tune the
hyperparameters of speculative decoding under given hardware and workload.
Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput
improvement over the state-of-the-art MoE offloading techniques.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [32] [ORCA: ORchestrating Causal Agent](https://arxiv.org/abs/2508.21304)
*Joanie Hayoun Chung,Chaemyung Lim,Sumin Lee,Sungbin Lim*

Main category: cs.DB

TL;DR: ORCA是一个基于LLM的代理系统，用于自动化关系数据库中的因果推断工作流，通过人机交互保持专家监督，显著提升处理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模增长，非专家在关系数据库中执行复杂工作流时面临瓶颈，影响业务洞察的及时性。

Method: ORCA通过解析自然语言查询、生成SQL代码、预处理数据及配置因果推断模型来自动化工作流。

Result: 在基准测试中，ORCA在表理解、查询生成和因果效应估计上表现优异，较GPT-4o mini提升7倍以上。

Conclusion: ORCA有效降低了因果推断的技术门槛，同时通过人机交互确保决策的稳健性。

Abstract: Causal inference is essential for decision-making science while the
complexity of the data analysis workflow, ranging from data wrangling to causal
analysis, increases substantially as the scale of data grows in complicated
business environments. Especially, the execution of the workflow in relational
databases by non-experts can result in repetitive bottlenecks which impede
timely and responsible business insights. To address this challenge, we propose
ORCA (Orchestrating Causal Agent), an LLM agentic system that can automate
routine workflows in RDBMS while preserving expert oversight via human-AI
interactions. ORCA orchestrates the full data analysis pipeline: interpreting
natural language queries, navigating tables from DB servers, generating proper
SQL codes, preprocessing data, and configuring modeling processes using causal
inference libraries. Domain experts still can control the automation through
iterative interactions with ORCA, enabling robust data-driven decision making
with less technical expertise in statistical computing. Empirical evaluations
on benchmark and synthetic e-commerce datasets demonstrate competitive
performance of ORCA in table understanding, query generation, and cause-effect
estimation -- achieving over $7\times$ improvement in estimating average
treatment compared to GPT-4o mini.

</details>


### [33] [Hilbert Forest in the SISAP 2025 Indexing Challenge](https://arxiv.org/abs/2508.21682)
*Yasunobu Imamura,Takeshi Shinohara,Naoya Higuchi,Kouichi Hirata,Tetsuji Kuboyama*

Main category: cs.DB

TL;DR: 作者团队在SISAP 2025索引挑战赛中使用了名为Hilbert forest的新索引技术，展示了其在高效排序和高维空间近似搜索中的竞争力。


<details>
  <summary>Details</summary>
Motivation: 参与SISAP 2025索引挑战赛，评估Hilbert forest在严格资源限制下的表现。

Method: 基于Hilbert空间填充曲线的快速排序算法，构建多棵Hilbert树以支持近似最近邻搜索。

Result: 在Task 1中表现竞争力，Task 2中构建时间最快且满足召回率要求。

Conclusion: Hilbert顺序索引在严格内存限制下具有实际有效性。

Abstract: We report our participation in the SISAP 2025 Indexing Challenge using a
novel indexing technique called the Hilbert forest. The method is based on the
fast Hilbert sort algorithm, which efficiently orders high-dimensional points
along a Hilbert space-filling curve, and constructs multiple Hilbert trees to
support approximate nearest neighbor search. We submitted implementations to
both Task 1 (approximate search on the PUBMED23 dataset) and Task 2 (k-nearest
neighbor graph construction on the GOOAQ dataset) under the official resource
constraints of 16 GB RAM and 8 CPU cores. The Hilbert forest demonstrated
competitive performance in Task 1 and achieved the fastest construction time in
Task 2 while satisfying the required recall levels. These results highlight the
practical effectiveness of Hilbert order-based indexing under strict memory
limitations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [34] [SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics](https://arxiv.org/abs/2508.21265)
*Sasan Razmkhah,Mingye Li,Zeming Cheng,Robert S. Aviles,Kyle Jackman,Joey Delport,Lieze Schindler,Wenhui Luo,Takuya Suzuki,Mehdi Kamal,Christopher L. Ayala,Coenrad J. Fourie,Nabuyuki Yoshikawa,Peter A. Beerel,Sandeep Gupta,Massoud Pedram*

Main category: cs.AR

TL;DR: 该研究利用超导电子学(SCE)加速全同态加密(FHE)中的数论变换(NTT)，提出了一种基于超导单通量量子(SFQ)逻辑的硬件加速器SCE-NTT，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 全同态加密(FHE)中的数论变换(NTT)是计算瓶颈，传统CMOS技术难以满足高能效需求，因此探索超导电子学的潜在优势。

Method: 设计了NTT-128架构，包括7个处理单元(PE)，每个PE带有优化的蝴蝶单元(BU)、双系数存储器和预计算旋转因子。采用新的RSFQ单元库，并通过JoSIM模拟和Verilog验证功能。

Result: NTT-128在34 GHz下达到5.31亿次NTT/秒，比现有CMOS快100倍以上，并可扩展至更大规模。

Conclusion: SCE加速器在可扩展性和能效方面展现出巨大潜力，有望推动后量子时代的安全计算发展。

Abstract: This research explores the use of superconductor electronics (SCE) for
accelerating fully homomorphic encryption (FHE), focusing on the
Number-Theoretic Transform (NTT), a key computational bottleneck in FHE
schemes. We present SCE-NTT, a dedicated hardware accelerator based on
superconductive single flux quantum (SFQ) logic and memory, targeting high
performance and energy efficiency beyond the limits of conventional CMOS. To
address SFQ constraints such as limited dense RAM and restricted fanin/fanout,
we propose a deeply pipelined NTT-128 architecture using shift register memory
(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7
processing elements (PEs), each featuring a butterfly unit (BU), dual
coefficient memories operating in ping-pong mode via FIFO-based SRM queues, and
twiddle factor buffers. The BU integrates a Shoup modular multiplier optimized
for a small area, leveraging precomputed twiddle factors. A new RSFQ cell
library with over 50 parameterized cells, including compound logic units, was
developed for implementation. Functional and timing correctness were validated
using JoSIM analog simulations and Verilog models. A multiphase clocking scheme
was employed to enhance robustness and reduce path-balancing overhead,
improving circuit reliability. Fabricated results show the NTT-128 unit
achieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art
CMOS equivalents. We also project that the architecture can scale to larger
sizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput
is estimated at 1.63 million operations/sec, significantly exceeding existing
hardware. These results demonstrate the strong potential of SCE-based
accelerators for scalable, energy-efficient secure computation in the
post-quantum era, with further gains anticipated through advances in
fabrication.

</details>


### [35] [Catwalk: Unary Top-K for Efficient Ramp-No-Leak Neuron Design for Temporal Neural Networks](https://arxiv.org/abs/2508.21267)
*Devon Lister,Prabhu Vellaisamy,John Paul Shen,Di Wu*

Main category: cs.AR

TL;DR: 提出了一种名为Catwalk的神经元实现方法，通过利用输入脉冲的稀疏性，优化硬件效率，比现有方法在面积和功耗上分别提升1.39倍和1.86倍。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SRM-RNL模型的神经元实现假设所有输入都携带脉冲，但实际中每个计算周期只有少量输入有脉冲。这种稀疏性可用于提高硬件效率。

Method: 提出了Catwalk神经元实现，通过将脉冲重新排序为子集簇（使用一元top-k），降低后续并行计数器的成本。

Result: Catwalk在面积和功耗上分别比现有SRM0-RNL神经元提升1.39倍和1.86倍。

Conclusion: Catwalk通过优化脉冲处理，显著提高了神经元的硬件效率。

Abstract: Temporal neural networks (TNNs) are neuromorphic neural networks that utilize
bit-serial temporal coding. TNNs are composed of columns, which in turn employ
neurons as their building blocks. Each neuron processes volleys of input
spikes, modulated by associated synaptic weights, on its dendritic inputs.
Recently proposed neuron implementation in CMOS employs a Spike Response Model
(SRM) with a ramp-no-leak (RNL) response function and assumes all the inputs
can carry spikes. However, in actual spike volleys, only a small subset of the
dendritic inputs actually carry spikes in each compute cycle. This form of
sparsity can be exploited to achieve better hardware efficiency. In this paper,
we propose a Catwalk neuron implementation by relocating spikes in a spike
volley as a sorted subset cluster via unary top-k. Such relocation can
significantly reduce the cost of the subsequent parallel counter (PC) for
accumulating the response functions from the spiking inputs. This can lead to
improvements on area and power efficiency in RNL neuron implementation.
Place-and-route results show Catwalk is 1.39x and 1.86x better in area and
power, respectively, as compared to existing SRM0-RNL neurons.

</details>


### [36] [SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural Network Accelerators](https://arxiv.org/abs/2508.21493)
*Yaman Umuroglu,Christoph Berganski,Felix Jentzsch,Michal Danilowicz,Tomasz Kryjak,Charalampos Bezaitis,Magnus Sjalander,Ian Colbert,Thomas Preusser,Jakoba Petri-Koenig,Michaela Blott*

Main category: cs.AR

TL;DR: 论文通过引入SIRA技术，优化量化神经网络中的非矩阵乘法操作，显著减少了资源占用。


<details>
  <summary>Details</summary>
Motivation: 激进的量化会暴露非矩阵乘法操作在嵌入式系统中的性能瓶颈，需要一种全面的方法优化计算精度。

Method: 引入SIRA静态分析技术，通过区间运算确定量化张量的范围、比例和偏置，优化比特位宽和操作聚合。

Result: 平均减少了17%的LUTs、66%的DSPs和22%的累加器比特位宽。

Conclusion: SIRA技术有效优化资源占用，开源以促进社区探索其在更多应用和平台上的潜力。

Abstract: While neural network quantization effectively reduces the cost of matrix
multiplications, aggressive quantization can expose non-matrix-multiply
operations as significant performance and resource bottlenecks on embedded
systems. Addressing such bottlenecks requires a comprehensive approach to
tailoring the precision across operations in the inference computation. To this
end, we introduce scaled-integer range analysis (SIRA), a static analysis
technique employing interval arithmetic to determine the range, scale, and bias
for tensors in quantized neural networks. We show how this information can be
exploited to reduce the resource footprint of FPGA dataflow neural network
accelerators via tailored bitwidth adaptation for accumulators and downstream
operations, aggregation of scales and biases, and conversion of consecutive
elementwise operations to thresholding operations. We integrate SIRA-driven
optimizations into the open-source FINN framework, then evaluate their
effectiveness across a range of quantized neural network workloads and compare
implementation alternatives for non-matrix-multiply operations. We demonstrate
an average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator
bitwidths with SIRA optimizations, providing detailed benchmark analysis and
analytical models to guide the implementation style for non-matrix layers.
Finally, we open-source SIRA to facilitate community exploration of its
benefits across various applications and hardware platforms.

</details>


### [37] [Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators](https://arxiv.org/abs/2508.21524)
*Wenyong Zhou,Zhengwu Liu,Yuan Ren,Ngai Wong*

Main category: cs.AR

TL;DR: 提出了一个新的二值权重多比特激活(BWMA)方法，用于CIM加速器上的CNN，平衡了硬件效率和精度，实验显示在各个数据集上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有的CIM加速器上CNN量化方法在硬件效率和准确性之间的权衡问题。

Method: 提出了二值权重多比特激活(BWMA)方法，包括推导权重量化的闭式解和改进激活量化的可微函数。

Result: 在CIFAR-10和ImageNet数据集上分别获得1.44%-5.46%和0.35%-5.37%的精度提升，硬件模拟显示4位激活量化是最佳平衡点。

Conclusion: BWMA方法在CIM加速器上实现了硬件效率与模型精度的高效平衡。

Abstract: Compute-in-memory (CIM) accelerators have emerged as a promising way for
enhancing the energy efficiency of convolutional neural networks (CNNs).
Deploying CNNs on CIM platforms generally requires quantization of network
weights and activations to meet hardware constraints. However, existing
approaches either prioritize hardware efficiency with binary weight and
activation quantization at the cost of accuracy, or utilize multi-bit weights
and activations for greater accuracy but limited efficiency. In this paper, we
introduce a novel binary weight multi-bit activation (BWMA) method for CNNs on
CIM-based accelerators. Our contributions include: deriving closed-form
solutions for weight quantization in each layer, significantly improving the
representational capabilities of binarized weights; and developing a
differentiable function for activation quantization, approximating the ideal
multi-bit function while bypassing the extensive search for optimal settings.
Through comprehensive experiments on CIFAR-10 and ImageNet datasets, we show
that BWMA achieves notable accuracy improvements over existing methods,
registering gains of 1.44\%-5.46\% and 0.35\%-5.37\% on respective datasets.
Moreover, hardware simulation results indicate that 4-bit activation
quantization strikes the optimal balance between hardware cost and model
performance.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [38] [Epsilon-saturation for stable graphs and Littlestone classes](https://arxiv.org/abs/2508.21807)
*Maryanthe Malliaris,Olga Medrano Martín del Campo,Shay Moran*

Main category: math.LO

TL;DR: 论文研究了Littlestone类（稳定图）的ε-饱和闭包，探讨了其在学习理论与模型论中的双重意义，并证明了在一定参数下闭包仍保持Littlestone或稳定性质。


<details>
  <summary>Details</summary>
Motivation: 探讨虚拟元素在学习理论和模型论中的作用，以及对Littlestone类或稳定图进行ε-饱和闭包的性质。

Method: 引入ε-饱和概念，对Littlestone类或稳定图进行闭包操作，通过参数化分析闭包的性质。

Result: 在合理参数下，闭包仍保持Littlestone或稳定性质，但维度可能变化。

Conclusion: 揭示了ε参数选择对Littlestone/stability与VC维度关系的影响。

Abstract: Any Littlestone class, or stable graph, has finite sets which function as
``virtual elements'': these can be seen from the learning side as representing
hypotheses which are expressible as weighted majority opinions of hypotheses in
the class, and from the model-theoretic side as an approximate finitary version
of realizing types. We introduce and study the epsilon-saturation of a
Littlestone class, or stable graph, which is essentially the closure of the
class under inductively adding all such virtual elements. We characterize this
closure and prove that under reasonable choices of parameters, it remains
Littlestone (or stable), though not always of the same Littlestone dimension.
This highlights some surprising phenomena having to do with regimes of epsilon
and the relation between Littlestone/stability and VC dimension.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SLAC开发了SNL框架和Auto-SNL工具，用于在FPGA上实时部署机器学习模型，解决了高速实验环境中的数据流处理挑战。


<details>
  <summary>Details</summary>
Motivation: 解决1~MHz X射线脉冲数据流的高速处理和存储问题，传统机器学习方法延迟高，不适合高要求环境。

Method: 开发SLAC Neural Network Library (SNL)框架和Auto-SNL工具，支持动态更新FPGA模型权重，无需重新合成。

Result: SNL在多种神经网络架构和配置中表现出低延迟和资源节约，优于当前工具hls4ml。

Conclusion: SNL为高能物理、医学成像等领域的研究人员提供了灵活的实时机器学习解决方案。

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [40] [An Optimistic Gradient Tracking Method for Distributed Minimax Optimization](https://arxiv.org/abs/2508.21431)
*Yan Huang,Jinming Xu,Jiming Chen,Karl Henrik Johansson*

Main category: math.OC

TL;DR: 本文研究了分布式极小极大优化问题，提出了分布式乐观梯度跟踪方法（DOGT）和加速版（ADOGT），证明了其在强凸-强凹目标函数下的线性收敛性，并通过数值实验验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式网络中极小极大优化问题的收敛性能，作者希望通过设计一种分布式乐观梯度跟踪方法，同时兼顾目标函数的相似性和本地化近似，以提高收敛速度和鲁棒性。

Method: 提出了DOGT方法，通过局部代理函数捕捉目标函数的相似性，并结合Lyapunov分析证明其收敛性。进一步设计了加速共识协议的ADOGT算法，优化收敛速度和通信复杂度。

Result: DOGT在强凸-强凹目标函数下实现线性收敛；ADOGT达到最优收敛率O(κlog(ε^{-1}))和通信复杂度O(κlog(ε^{-1})/√(1-√ρ_W))。

Conclusion: DOGT和ADOGT在理论和实验上均展示了优异的性能，为分布式极小极大优化提供了高效解决方案。

Abstract: This paper studies the distributed minimax optimization problem over
networks. To enhance convergence performance, we propose a distributed
optimistic gradient tracking method, termed DOGT, which solves a surrogate
function that captures the similarity between local objective functions to
approximate a centralized optimistic approach locally. Leveraging a
Lyapunov-based analysis, we prove that DOGT achieves linear convergence to the
optimal solution for strongly convex-strongly concave objective functions while
remaining robust to the heterogeneity among them. Moreover, by integrating an
accelerated consensus protocol, the accelerated DOGT (ADOGT) algorithm achieves
an optimal convergence rate of $\mathcal{O} \left( \kappa \log \left( \epsilon
^{-1} \right) \right)$ and communication complexity of $\mathcal{O} \left(
\kappa \log \left( \epsilon ^{-1} \right) /\sqrt{1-\sqrt{\rho _W}} \right)$ for
a suboptimality level of $\epsilon>0$, where $\kappa$ is the condition number
of the objective function and $\rho_W$ is the spectrum gap of the network.
Numerical experiments illustrate the effectiveness of the proposed algorithms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [41] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了Misviz和Misviz-synth数据集，用于检测误导性可视化，并评估了多种模型在此任务上的表现，发现其仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 误导性可视化在社交媒体和网络上广泛传播，导致读者得出错误结论。目前缺乏大规模、多样化的开放数据集来训练和评估AI模型。

Method: 提出了包含2,604个真实世界可视化标注的Misviz数据集和81,814个合成可视化生成的Misviz-synth数据集，并评估了MLLMs、规则系统和微调分类器。

Result: 任务仍然极具挑战性。

Conclusion: 此研究提供了开放数据集和代码，支持未来对误导性可视化检测的进一步研究。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [42] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究表明，不同人格特质的用户对大语言模型（LLM）有显著偏好：理性型更倾向于选择GPT-4，而理想型则偏好Claude 3.5。


<details>
  <summary>Details</summary>
Motivation: 探讨用户人格特质是否会影响其对不同LLM的偏好。

Method: 32名参与者（四种人格类型）与GPT-4和Claude 3.5完成四项协作任务，比较其偏好与反馈。

Result: 理性型偏好GPT-4（目标导向任务），理想型偏好Claude 3.5（创意与分析任务）；其他类型偏好任务相关。

Conclusion: 人格分析揭示了传统评估忽视的LLM差异，用户偏好与人格特质相关。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [GLENDA: Gynecologic Laparoscopy Endometriosis Dataset](https://arxiv.org/abs/2508.21398)
*Andreas Leibetseder,Sabrina Kletz,Klaus Schoeffmann,Simon Keckstein,Jörg Keckstein*

Main category: cs.CV

TL;DR: 本文介绍了GLENDA数据集，用于辅助计算机视觉和机器学习技术分析妇科腹腔镜手术视频，以解决手动分析耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 目前，妇科腹腔镜手术视频的手动分析耗时且繁琐，需要更高效的自动分析方法。

Method: 作者开发了GLENDA数据集，包含子宫内膜异位症的区域标注图像，是该领域的首个此类数据集。

Result: 通过发布GLENDA数据集，为计算机视觉和机器学习在妇科腹腔镜手术中的应用提供了稀缺的样本数据。

Conclusion: GLENDA数据集填补了医学领域相关数据的空白，有望推动手术视频分析的自动化发展。

Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is
performed via a live feed of a patient's abdomen surveying the insertion and
handling of various instruments for conducting treatment. Adopting this kind of
surgical intervention not only facilitates a great variety of treatments, the
possibility of recording said video streams is as well essential for numerous
post-surgical activities, such as treatment planning, case documentation and
education. Nonetheless, the process of manually analyzing surgical recordings,
as it is carried out in current practice, usually proves tediously
time-consuming. In order to improve upon this situation, more sophisticated
computer vision as well as machine learning approaches are actively developed.
Since most of such approaches heavily rely on sample data, which especially in
the medical field is only sparsely available, with this work we publish the
Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset
containing region-based annotations of a common medical condition named
endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the
first of its kind and it has been created in collaboration with leading medical
experts in the field.

</details>


### [44] [Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation](https://arxiv.org/abs/2508.21399)
*Sabrina Kletz,Klaus Schoeffmann,Jenny Benois-Pineau,Heinrich Husslein*

Main category: cs.CV

TL;DR: 研究了腹腔镜妇科手术视频中手术器械的分割与识别，结果显示器械分割准确性高，但器械类型识别仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 手术视频是医学内窥镜领域的重要信息源，但自动内容索引（如基于内容的搜索）仍面临挑战，尤其是在特殊视频内容中识别和分割手术器械。

Method: 采用基于区域的完全卷积网络进行实例感知的手术器械分割和识别，分为二元分割（区分器械与背景）和多类器械识别（识别器械类型）。

Result: 在训练样本较少的情况下，器械分割准确性较高，但器械类型识别因器械高度相似而仍具挑战。

Conclusion: 器械分割技术已较成熟，但器械类型识别需进一步优化，尤其针对相似器械的区分。

Abstract: Recorded videos from surgeries have become an increasingly important
information source for the field of medical endoscopy, since the recorded
footage shows every single detail of the surgery. However, while video
recording is straightforward these days, automatic content indexing - the basis
for content-based search in a medical video archive - is still a great
challenge due to the very special video content. In this work, we investigate
segmentation and recognition of surgical instruments in videos recorded from
laparoscopic gynecology. More precisely, we evaluate the achievable performance
of segmenting surgical instruments from their background by using a
region-based fully convolutional network for instance-aware (1) instrument
segmentation as well as (2) instrument recognition. While the first part
addresses only binary segmentation of instances (i.e., distinguishing between
instrument or background) we also investigate multi-class instrument
recognition (i.e., identifying the type of instrument). Our evaluation results
show that even with a moderately low number of training examples, we are able
to localize and segment instrument regions with a pretty high accuracy.
However, the results also reveal that determining the particular instrument is
still very challenging, due to the inherently high similarity of surgical
instruments.

</details>


### [45] [Learning from Silence and Noise for Visual Sound Source Localization](https://arxiv.org/abs/2508.21761)
*Xavier Juanola,Giovana Morais,Magdalena Fuentes,Gloria Haro*

Main category: cs.CV

TL;DR: 论文提出了一个改进的视觉声源定位方法，通过加入沉默和噪声的训练策略，提升了性能，并引入了新的评估指标和数据扩展。


<details>
  <summary>Details</summary>
Motivation: 当前方法在低音频-视觉语义对应（如沉默、噪声和非画面声音）情况下表现不佳，且评估仅限于单一可见声源的场景。

Method: 提出了新的训练策略，结合沉默和噪声，开发了自监督模型SSL-SaN，并引入了新的评估指标和扩展的数据集IS3+。

Result: SSL-SaN模型在声源定位和跨模态检索上取得了最先进的性能，新指标能更好地衡量正负音频-视觉对的特性。

Conclusion: 改进的训练策略和数据集扩展提高了模型在复杂场景下的鲁棒性和性能，为未来研究提供了新的工具和数据支持。

Abstract: Visual sound source localization is a fundamental perception task that aims
to detect the location of sounding sources in a video given its audio. Despite
recent progress, we identify two shortcomings in current methods: 1) most
approaches perform poorly in cases with low audio-visual semantic
correspondence such as silence, noise, and offscreen sounds, i.e. in the
presence of negative audio; and 2) most prior evaluations are limited to
positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key
contributions. First, we propose a new training strategy that incorporates
silence and noise, which improves performance in positive cases, while being
more robust against negative sounds. Our resulting self-supervised model,
SSL-SaN, achieves state-of-the-art performance compared to other
self-supervised models, both in sound localization and cross-modal retrieval.
Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative
audio-visual pairs. Third, we present IS3+, an extended and improved version of
the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the
https://xavijuanola.github.io/SSL-SaN/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [47] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [48] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: 提出了一种直接在原始PCAP数据上运行的加密流量分类新模型，无需事先假设流量类型，并利用跨流信号提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要假设流量类型，且缺乏对不同分类任务的通用性，因此需要一种更通用的高效分类模型。

Method: 利用创新的跨流信号表示方法，捕捉流间的时间相关性和数据包体积分布。

Result: 实验结果表明，该模型在多数分类任务和数据集上优于现有方法，某些情况下准确率达99%。

Conclusion: 该模型展示了强大的鲁棒性和适应性，适用于多种加密流量分类场景。

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [49] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: 提出了名为Locus的新框架，通过合成语义化的中间状态谓词改进定向模糊测试效率，平均加速41.6倍。


<details>
  <summary>Details</summary>
Motivation: 定向模糊测试旨在找到导致特定目标状态的输入，但现有方法依赖分支距离或手动约束，效率低且泛化能力差。

Method: Locus通过程序分析工具合成中间状态谓词，作为里程碑指导搜索，避免无效执行并提供覆盖引导。

Result: 实验表明Locus显著提升8种现有模糊测试器的效率，发现8个未修复漏洞，平均加速41.6倍。

Conclusion: Locus通过语义化中间状态谓词有效提升定向模糊测试效率，具有良好的泛化能力和实际应用价值。

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [50] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: 论文探讨了欧盟五项核心网络安全立法中风险的定义，分析其一致性与差异性，并提出对合规性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟网络安全立法中风险的框架，揭示其复杂性及对合规性的影响。

Method: 采用定性法律解释和分类构建的方法。

Result: 立法涵盖多种网络安全风险，但存在可接受风险等方面的缺口。

Conclusion: 欧盟网络安全立法扩展了风险基础方法，但带来复杂性，论文提出合规建议。

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [51] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: 该论文研究了LLM依赖供应链中的安全风险，发现LLM生态系统中漏洞披露时间显著长于Python生态系统，且多数LLM包含易受攻击的依赖项。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于LLM依赖供应链中的漏洞风险被忽视，缺乏对此类问题的实证分析。

Method: 通过对52个开源LLM的依赖项进行实证分析，并比较Python生态系统中的漏洞处理情况。

Result: 结果显示LLM生态系统中50%的漏洞未披露时间超过56.2个月，75.8%的LLM配置文件中存在易受攻击的依赖项。

Conclusion: 研究填补了LLM供应链安全风险的空白，为实践者提供了改进方向。

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [52] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: RepoMark是一种新型的数据标记框架，用于审计代码大型语言模型的数据使用情况，确保开源代码库的授权合规性。它通过生成语义等效的代码变体并引入数据标记，显著提高了样本效率，实验显示其检测成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 解决代码大型语言模型在训练中未经授权使用开源代码库的伦理和法律问题，增强数据使用的透明度。

Method: 提出RepoMark框架，通过生成语义等效的代码变体并嵌入数据标记，利用基于排名的假设测试检测模型记忆。

Result: 在5%的假检测率保证下，RepoMark对小代码库的检测成功率超过90%，显著优于现有技术。

Conclusion: RepoMark为代码LLM训练提供了透明且理论可靠的解决方案，有效保护了代码库所有者的权益。

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [53] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [54] [Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models](https://arxiv.org/abs/2508.21248)
*Subham Kutum,Abhijit Sinha,Hemant Kumar Kathania,Sudarsana Reddy Kadiri,Mahesh Chandra Govil*

Main category: eess.AS

TL;DR: 本文提出了一种利用自监督学习模型的零样本关键词检测方法，解决了儿童语音与成人语音的声学和语言学差异问题，并在多个数据集上取得了优越表现。


<details>
  <summary>Details</summary>
Motivation: 儿童语音具有独特的声学和语言学特征，与成人语音不同，传统关键词检测系统面对儿童语音时效果不佳。本文旨在通过自监督学习模型提升儿童语音的关键词检测性能。

Method: 使用Wav2Vec2、HuBERT和Data2Vec等自监督学习模型提取分层特征，训练基于Kaldi的DNN关键词检测系统。训练数据为WSJCAM0成人语音数据集，测试数据为PFSTAR儿童语音数据集。

Result: Wav2Vec2（尤其是第22层）表现最佳，ATWV得分为0.691，MTWV得分为0.7003。系统在噪声环境和多个儿童年龄段均表现优异，显著优于传统MFCC基线。

Conclusion: 自监督学习特征显著提升了零样本关键词检测在儿童语音中的表现，有效应对了儿童语音的独特挑战。

Abstract: Numerous methods have been proposed to enhance Keyword Spotting (KWS) in
adult speech, but children's speech presents unique challenges for KWS systems
due to its distinct acoustic and linguistic characteristics. This paper
introduces a zero-shot KWS approach that leverages state-of-the-art
self-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec.
Features are extracted layer-wise from these SSL models and used to train a
Kaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for
training, while the PFSTAR children's speech dataset was used for testing,
demonstrating the zero-shot capability of our method. Our approach achieved
state-of-the-art results across all keyword sets for children's speech.
Notably, the Wav2Vec2 model, particularly layer 22, performed the best,
delivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of
false alarm and probability of miss of 0.0164 and 0.0547 respectively, for a
set of 30 keywords. Furthermore, age-specific performance evaluation confirmed
the system's effectiveness across different age groups of children. To assess
the system's robustness against noise, additional experiments were conducted
using the best-performing layer of the best-performing Wav2Vec2 model. The
results demonstrated a significant improvement over traditional MFCC-based
baseline, emphasizing the potential of SSL embeddings even in noisy conditions.
To further generalize the KWS framework, the experiments were repeated for an
additional CMU dataset. Overall the results highlight the significant
contribution of SSL features in enhancing Zero-Shot KWS performance for
children's speech, effectively addressing the challenges associated with the
distinct characteristics of child speakers.

</details>
