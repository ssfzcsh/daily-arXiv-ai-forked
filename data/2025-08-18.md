<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 27]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: 研究表明，GPT辅助的代码审查过程能显著减少解决时间和优化各阶段性能，其中审查时间减少33%，等待接受时间减少87%，整体解决时间中位数减少60%以上。


<details>
  <summary>Details</summary>
Motivation: 探索GPT在GitHub拉取请求（PR）工作流程中的具体效果，尤其是在减少解决时间、优化各阶段性能和辅助开发者方面的作用。

Method: 使用半自动启发式方法筛选并验证GPT辅助的PR，然后通过统计建模（如多元线性回归和Mann-Whitney U检验）分析差异。

Result: GPT辅助的PR中位数解决时间从23小时降至9小时，审查时间减少33%，等待接受时间减少87%。开发人员主要将GPT用于代码优化（60%）、修复错误（26%）和更新文档（12%）。

Conclusion: GPT在代码审查过程中的早期采用能够大幅提升效率，为软件开发团队优化协作提供了可行建议。

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [2] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 论文探讨了如何利用预训练的代码扩散模型进行‘最后一英里修复’，通过加噪和采样中间程序生成训练数据。


<details>
  <summary>Details</summary>
Motivation: 研究代码扩散模型在后期生成的代码片段中展现出‘最后一英里修复’的潜力，提出利用这一特性解决修复问题。

Method: 通过加噪并重新扩散来修复代码，以及采样中间程序与最终程序生成训练数据，实验覆盖Python、Excel和PowerShell。

Result: 展示了代码扩散模型在‘最后一英里修复’中的潜在应用和高效性。

Conclusion: 预训练的代码扩散模型可有效用于代码修复任务，并能生成大量训练数据。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [3] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: AI代理编程是一种新兴范式，利用大语言模型自主规划、执行并与外部工具交互，完成复杂软件开发任务，本文对其进行全面综述。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理编程快速发展，需要明确其范畴、巩固技术基础并识别研究挑战。

Method: 提出代理行为与系统架构的分类法，研究规划、记忆、工具集成等核心技术，分析评估方法。

Result: 发现处理长上下文、持久记忆等关键挑战，探讨可靠性、透明度的提升机会。

Conclusion: 本文为下一代智能可信AI编码代理的研发提供基础。

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [4] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 论文提出了一种名为RevPerf的工具，通过分析Google Play的应用评论来复现移动应用的性能问题，结合提示工程和多方面检测方法，成功率达到70%。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能对用户体验至关重要，但开发环境中性能问题难以检测和诊断。

Method: 利用Google Play评论提取信息，通过提示工程丰富问题细节，生成并执行命令复现问题，结合日志、GUI和资源监控进行检测。

Result: 在手动验证的数据集上，复现性能问题的成功率为70%。

Conclusion: RevPerf能有效帮助开发者在开发环境外复现性能问题。

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [5] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: 该论文提出了PTMPicker，通过结构化模板和嵌入相似性计算，解决预训练模型选择中的关键词搜索不足问题，实验表明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型选择方法依赖关键词搜索，难以全面捕捉用户需求，尤其是关于偏见、硬件或许可证等因素的需求。

Method: 定义了预训练模型的结构化模板，通过嵌入相似性和特定提示评估模型是否符合用户需求，并在Hugging Face上收集了大量模型数据。

Result: 实验证明，PTMPicker能帮助用户在排名前10的候选模型中成功找到85%的合适模型。

Conclusion: PTMPicker通过结构化表示和智能匹配，显著提升了预训练模型选择的准确性和效率。

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [6] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 论文提出ORFuzz框架，用于检测和分析大语言模型（LLMs）的过度拒绝问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs的过度拒绝行为（无害查询被错误拒绝）损害其可靠性和可用性，现有测试方法不足。

Method: ORFuzz整合三类核心：安全类别感知种子选择、自适应突变优化和人类对齐的OR-Judge模型。

Result: ORFuzz生成多样化的测试用例，平均检测率6.98%；ORFuzzSet基准在10种LLMs中平均拒绝率达63.56%。

Conclusion: ORFuzz为开发更可靠LLM系统提供了自动化测试框架和有价值的社区资源。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [7] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLM）在代码生成中的幻觉现象，特别是在汽车领域。通过评估多款先进代码生成模型（如GPT-4.1、Codex和GPT-4o），发现其在高复杂度提示下仍存在问题，需开发缓解技术以确保安全可靠的代码生成。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成中展现了潜力，但其实际应用中因幻觉问题（如错误、不可验证或不合逻辑的输出）而受限，尤其在安全关键的汽车领域需要更可靠的解决方案。

Method: 通过案例研究，评估了多款LLM在不同提示复杂度（从简单单行提示到包含VSS规范和代码骨架的复杂提示）下的表现，分析其生成代码的正确性。

Result: 发现所有评估模型在不同提示复杂度下均存在高频率的语法错误、无效引用和API冲突。仅GPT-4.1和GPT-4o在最优提示下生成了正确代码。

Conclusion: 需开发有效的缓解技术以确保LLM生成代码的安全性和可靠性，尤其是在安全关键领域。

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [8] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 该论文提出了一个全面的日志代码缺陷分类法，构建了一个基准数据集，并评估了大型语言模型（LLMs）在检测日志代码缺陷方面的能力。


<details>
  <summary>Details</summary>
Motivation: 日志代码对系统调试和监控至关重要，但缺陷可能导致日志误解。现有研究对缺陷模式的覆盖不足，且大型语言模型在此领域的潜力尚未充分探索。

Method: 作者首先建立了包含7种缺陷模式和14种具体场景的分类法，然后构建了164个真实缺陷的数据集。通过多种提示策略和上下文信息，评估了LLMs的缺陷检测和推理能力。

Result: 实验显示LLMs仅基于源代码难以准确检测日志缺陷，但引入缺陷模式的详细场景信息后，检测精度可提升10.9%。

Conclusion: 研究为开发者避免常见缺陷提供了指导，并为改进基于LLM的日志缺陷检测奠定了基础。

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [9] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY是首个评估LLM翻译代码执行效率的基准测试，填补了现有研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码翻译中虽正确性提升，但执行效率被忽视。

Method: 通过两阶段LLM驱动流程构建TRACY，包含1011个任务和测试。

Result: 评估26个LLM发现效率问题突出，大模型未必表现最佳。

Conclusion: 未来需同时优化正确性和效率。

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [10] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: 论文探讨了从微服务系统获取时间网络并用时间网络方法分析的挑战，数据集限制为7个时间实例和42个微服务。


<details>
  <summary>Details</summary>
Motivation: 研究微服务架构随时间变化的行为，通过时间网络分析方法揭示其动态特性。

Method: 利用网络科学中的时间网络分析方法，分析微服务架构中的服务依赖图。

Result: 数据集规模较小（7个时间实例和42个微服务），限制了分析的深度和广度。

Conclusion: 时间网络分析对微服务系统研究有潜力，但数据获取和规模是主要挑战。

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Generic Reduction-Based Interpreters (Extended Version)](https://arxiv.org/abs/2508.11297)
*Casper Bach*

Main category: cs.PL

TL;DR: 该论文利用通用编程技术减少归约式解释器中的样板代码。


<details>
  <summary>Details</summary>
Motivation: 传统的归约式解释器需要大量重复的代码实现，虽然实现过程系统化，但增加了工程师的负担。

Method: 应用通用编程技术（generic programming）来优化归约式解释器的实现。

Result: 有效减少了样板代码的数量。

Conclusion: 通用编程技术可以显著简化归约式解释器的实现过程。

Abstract: Reduction-based interpreters are traditionally defined in terms of a one-step
reduction function which systematically decomposes a term into a potential
redex and context, contracts the redex, and recomposes it to construct the new
term to be further reduced. While implementing such interpreters follows a
systematic recipe, they often require interpreter engineers to write a
substantial amount of code -- much of it boilerplate. In this paper, we apply
well-known techniques from generic programming to reduce boilerplate code in
reduction-based interpreters.

</details>


### [12] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: 本文系统性地推导了一种数据并行的两级静态无冲突哈希映射实现，通过功能化Fredman等人的结构并扁平化处理。讨论了在功能数组语言中提供灵活、多态和抽象的哈希映射接口的挑战，特别是动态大小键的问题。算法在Futhark中实现，GPU执行性能表现优于传统树/搜索方法，但较cuCollections库在构造上稍慢。分析了性能差异的原因，并探讨了功能数组语言模型的改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种功能化的数据并行哈希映射实现，解决动态大小键的挑战，并在GPU上实现高性能。

Method: 通过功能化Fredman等人的构造并扁平化，实现了两级静态无冲突哈希映射。在Futhark中实现，并与cuCollections库进行性能对比。

Result: 实现的哈希映射性能优于传统方法，但cuCollections库在构造上更快。差异部分源于Futhark编译器限制，部分源于功能数组语言的表达能力不足。

Conclusion: 功能数组语言模型可能需要扩展以弥补当前限制，进一步提升性能。

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [13] [Inference performance evaluation for LLMs on edge devices with a novel benchmarking framework and metric](https://arxiv.org/abs/2508.11269)
*Hao Chen,Cong Tian,Zixuan He,Bin Yu,Yepang Liu,Jialun Cao*

Main category: cs.PF

TL;DR: 介绍了一个名为ELIB的边缘LLM推理基准测试工具，提出新指标MBU优化内存使用，并在三个边缘平台上进行测试。


<details>
  <summary>Details</summary>
Motivation: 满足移动和PC设备上基于边缘计算的LLM推理需求，解决不同硬件部署LLM的挑战。

Method: 开发ELIB工具，利用MBU指标结合FLOPS、吞吐量、延迟和准确性等指标优化内存带宽使用。

Result: 在三个边缘平台上测试五种量化模型，分析优化MBU的关键因素和约束。

Conclusion: ELIB和MBU指标为边缘设备部署LLM提供了优化指导。

Abstract: With the significant success achieved by large language models (LLMs) like
LLaMA, edge computing-based LLM inference services for mobile and PC are in
high demand for data privacy. However, different edge platforms have different
hardware characteristics and the large demand for memory capacity and bandwidth
makes it very challenging to deploy and benchmark LLMs on edge devices. In this
paper, we introduce a benchmarking tool named ELIB (edge LLM inference
benchmarking) to evaluate LLM inference performance of different edge
platforms, and propose a novel metric named MBU to indicate the percentage of
the theoretically efficient use of available memory bandwidth for a specific
model running on edge hardware to optimize memory usage. We deploy ELIB on
three edge platforms and benchmark using five quantized models to optimize MBU
in combination with other metrics such as FLOPS, throughput, latency and
accuracy. And we analyze the results to derive the key factors, constraints,
unpredictability in optimizing MBU that can guide deploying LLMs on more edge
platforms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [14] [CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices](https://arxiv.org/abs/2508.11342)
*Linh-An Phan,MingXue Wang,Guangyu Wu,Wang Dawei,Chen Liqun,Li Jin*

Main category: cs.NI

TL;DR: CrossTrace是一种基于eBPF的零代码分布式追踪解决方案，通过延迟模式和TCP包头嵌入标识符实现高效、安全的跨服务追踪。


<details>
  <summary>Details</summary>
Motivation: 现有零代码追踪方案在跨服务追踪时依赖线程关联或牺牲系统安全性，难以满足大规模微服务应用的需求。

Method: 使用贪心算法分析延迟模式推断服务内跨度关系，并通过eBPF在TCP包头嵌入标识符实现跨服务关联。

Result: 实验表明，CrossTrace能在数秒内以超过90%的准确性关联数千个跨度，适用于生产环境。

Conclusion: CrossTrace提供了一种高效且安全的分布式追踪方案，适合微服务应用的观测和诊断。

Abstract: Distributed tracing has become an essential technique for debugging and
troubleshooting modern microservice-based applications, enabling software
engineers to detect performance bottlenecks, identify failures, and gain
insights into system behavior. However, implementing distributed tracing in
large-scale applications remains challenging due to the need for extensive
instrumentation. To reduce this burden, zero-code instrumentation solutions,
such as those based on eBPF, have emerged, allowing span data to be collected
without modifying application code. Despite this promise, span correlation, the
process of establishing causal relationships between spans, remains a critical
challenge in zero-code approaches. Existing solutions often rely on thread
affinity, compromise system security by requiring the kernel integrity mode to
be disabled, or incur significant computational overhead due to complex
inference algorithms. This paper presents CrossTrace, a practical and efficient
distributed tracing solution designed to support the debugging of microservice
applications without requiring source code modifications. CrossTrace employs a
greedy algorithm to infer intra-service span relationships from delay patterns,
eliminating reliance on thread identifiers. For inter-service correlation,
CrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling
secure and efficient correlation compromising system security policies.
Evaluation results show that CrossTrace can correlate thousands of spans within
seconds with over 90% accuracy, making it suitable for production deployment
and valuable for microservice observability and diagnosis.

</details>


### [15] [Optimizing ROS 2 Communication for Wireless Robotic Systems](https://arxiv.org/abs/2508.11366)
*Sanghoon Lee,Taehun Kim,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: ROS 2中无线传输大数据负载存在问题，本文提出了一个轻量级DDS优化框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决ROS 2中DDS通信栈在无线环境下性能下降的问题，尤其是大数据负载传输的瓶颈。

Method: 提出一个轻量级DDS优化框架，通过调整通信参数适应链路和数据负载特性，无需协议修改。

Result: 实验表明该框架在多种无线场景下成功传输大数据负载，保持了低延迟。

Conclusion: 该优化框架有效解决了ROS 2无线传输中的关键问题，具有高兼容性和易用性。

Abstract: Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.

</details>


### [16] [D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications](https://arxiv.org/abs/2508.11475)
*Ioannis Panitsas,Akrit Mudvari,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出了基于强化学习的D2Q Synchronizer算法，用于分布式SDN中的控制器同步，优化网络和用户性能。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SDN的同步策略未充分优化网络和用户性能，需要更高效的解决方案。

Method: 采用强化学习算法D2Q Synchronizer，将时敏感任务卸载到经济型边缘服务器，满足延迟需求。

Result: 相比启发式和其他学习策略，D2Q Synchronizer降低网络成本至少45%和10%，确保QoS需求。

Conclusion: D2Q Synchronizer在多域动态SDN中高效优化网络成本，确保任务延迟要求。

Abstract: In distributed Software-Defined Networking (SDN), distributed SDN controllers
require synchronization to maintain a global network state. Despite the
availability of synchronization policies for distributed SDN architectures,
most policies do not consider joint optimization of network and user
performance. In this work, we propose a reinforcement learning-based algorithm
called D2Q Synchronizer, to minimize long-term network costs by strategically
offloading time-sensitive tasks to cost-effective edge servers while satisfying
the latency requirements for all tasks. Evaluation results demonstrate the
superiority of our synchronizer compared to heuristic and other learning
policies in literature, by reducing network costs by at least 45% and 10%,
respectively, while ensuring the QoS requirements for all user tasks across
dynamic and multi-domain SDN networks.

</details>


### [17] [Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles](https://arxiv.org/abs/2508.11574)
*Mohammad Sajid Shahriar,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 本文提出了一种分布式计算架构，结合数字孪生和移动边缘计算，用于智能交通系统，显著提升了系统的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 下一代网络为智能交通系统（ITS）提供了巨大潜力，但如何通过高效的计算资源管理确保数字孪生（DTs）的持续运行仍是一个挑战。

Method: 开发了一个网络感知的可扩展协作任务调度算法，训练自主代理，并在实际的自动驾驶汽车（CAV）交通模拟中进行了评估。

Result: 该框架将同步错误降至5%，并实现了高达99.5%的边缘计算资源利用率。

Conclusion: 提出的架构为智能交通系统提供了低延迟的解决方案，同时显著提升了数字孪生操作的鲁棒性和可扩展性。

Abstract: The next generation networks offers significant potential to advance
Intelligent Transportation Systems (ITS), particularly through the integration
of Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs
through efficient computing resource management remains an open challenge. This
paper introduces a distributed computing archi tecture that integrates DTs and
Mobile Edge Computing (MEC) within a software-defined vehicular networking
framework to enable intelligent, low-latency transportation services. A network
aware scalable collaborative task provisioning algorithm is de veloped to train
an autonomous agent, which is evaluated using a realistic connected autonomous
vehicle (CAV) traffic simulation. The proposed framework significantly enhances
the robustness and scalability of DT operations by reducing synchronization
errors to as low as 5% while achieving up to 99.5% utilization of edge
computing resources.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [Failures to Surface Harmful Contents in Video Large Language Models](https://arxiv.org/abs/2508.10974)
*Yuxin Cao,Wei Song,Derui Wang,Jingling Xue,Jin Song Dong*

Main category: cs.MM

TL;DR: 视频大语言模型（VideoLLMs）在关键应用中被广泛使用，但其设计中存在安全漏洞，导致模型经常忽略视频中的有害内容。根本原因包括稀疏帧采样、空间信息丢失和编码器-解码器断开连接。


<details>
  <summary>Details</summary>
Motivation: 揭示当前VideoLLMs在检测视频中有害内容方面的设计缺陷，并提出改进需求。

Method: 通过分析三种设计缺陷（帧采样不足、空间信息丢失、编码器-解码器断开），开发了三种零查询黑盒攻击方法，并在五种主流VideoLLMs上进行了大规模评估。

Result: 实验显示，大多数情况下有害内容遗漏率超过90%，即使有害内容在所有帧中明显可见。

Conclusion: 当前VideoLLMs的设计存在根本性漏洞，需要改进采样策略、令牌压缩和解码机制，以确保语义覆盖而不仅是速度。

Abstract: Video Large Language Models (VideoLLMs) are increasingly deployed on numerous
critical applications, where users rely on auto-generated summaries while
casually skimming the video stream. We show that this interaction hides a
critical safety gap: if harmful content is embedded in a video, either as
full-frame inserts or as small corner patches, state-of-the-art VideoLLMs
rarely mention the harmful content in the output, despite its clear visibility
to human viewers. A root-cause analysis reveals three compounding design flaws:
(1) insufficient temporal coverage resulting from the sparse, uniformly spaced
frame sampling used by most leading VideoLLMs, (2) spatial information loss
introduced by aggressive token downsampling within sampled frames, and (3)
encoder-decoder disconnection, whereby visual cues are only weakly utilized
during text generation. Leveraging these insights, we craft three zero-query
black-box attacks, aligning with these flaws in the processing pipeline. Our
large-scale evaluation across five leading VideoLLMs shows that the harmfulness
omission rate exceeds 90% in most cases. Even when harmful content is clearly
present in all frames, these models consistently fail to identify it. These
results underscore a fundamental vulnerability in current VideoLLMs' designs
and highlight the urgent need for sampling strategies, token compression, and
decoding mechanisms that guarantee semantic coverage rather than speed alone.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [Characterizing NC1 with Typed Monoids](https://arxiv.org/abs/2508.11019)
*Anuj Dawar,Aidan T. Evans*

Main category: cs.LO

TL;DR: 论文通过扩展一元量词逻辑，提出了一种对 NC1 复杂度类的新代数刻画，并解决了 Lautemann 等人（2001）的问题。


<details>
  <summary>Details</summary>
Motivation: 扩展 Krebs 等人（2007）对 TC0 的代数刻画方法，研究更高复杂度类 NC1 的代数特性。

Method: 通过扩展一元量词的逻辑表达，结合 Bojańczyk 等人（2019）的字符串解释结果，证明了 NC1 的特性。

Result: 证明了 NC1 可以通过一元量词逻辑刻画，同时更高维度的有限幺半群量词可被一元量词替代。

Conclusion: 该研究为 NC1 提供了新的代数刻画，并推动了代数自动机理论在更高复杂度类中的应用。

Abstract: Krebs et al. (2007) gave a characterization of the complexity class TC0 as
the class of languages recognized by a certain class of typed monoids. The
notion of typed monoid was introduced to extend methods of algebraic automata
theory to infinite monoids and hence characterize classes beyond the regular
languages. We advance this line of work beyond TC0 by giving a characterization
of NC1. This is obtained by first showing that NC1 can be defined as the
languages expressible in an extension of first-order logic using only unary
quantifiers over regular languages. The expressibility result is a consequence
of a general result showing that finite monoid multiplication quantifiers of
higher dimension can be replaced with unary quantifiers in the context of
interpretations over strings, which also answers a question of Lautemann et al.
(2001). We establish this collapse result for a much more general class of
interpretations using results on interpretations due to Boja\'nczyk et al.
(2019), which may be of independent interest.

</details>


### [20] [Automating the Derivation of Unification Algorithms: A Case Study in Deductive Program Synthesis](https://arxiv.org/abs/2508.11136)
*Richard Waldinger*

Main category: cs.LO

TL;DR: 本文研究了通过演绎程序合成自动推导统一算法的方法，基于定理证明的思路，从逻辑规范中提取满足条件的程序，并通过证明其正确性。


<details>
  <summary>Details</summary>
Motivation: 统一算法的自动推导一直是程序合成研究的目标，但完全自动化的实现仍有待突破。本文旨在通过演绎程序合成的方法，从逻辑规范中自动生成正确的统一算法。

Method: 本文扩展并自动化了Manna和Waldinger（1981）的手动证明方法，提出了一个三参数（两个表达式加一个环境替换）的统一算法，通过递归调用记录环境替换的变化。

Result: 新算法能够生成一个最广义幂等统一器，或在不满足条件时返回失败。三参数版本比两参数版本更高效且更易于自动合成。

Conclusion: 基于环境的统一算法在效率与自动合成难度上优于传统方法，为程序合成研究提供了新的思路。

Abstract: The unification algorithm has long been a target for program synthesis
research, but a fully automatic derivation remains a research goal. In
deductive program synthesis, computer programming is phrased as a task in
theorem proving; a declarative specification is expressed in logical form and
presented to an automatic theorem prover, and a program meeting the
specification is extracted from the proof. The correctness of the program is
supported by the proof, which also provides an explanation of how the program
works. The proof is conducted in an appropriate axiomatic subject-domain
theory, which defines the concepts in the specification and the constructs in
the target programming language and provides the background knowledge necessary
to connect them.
  For the unification proof, we generalize and automate the manual proof
presented in Manna and Waldinger [1981]. The new program unifies two given
symbolic expressions (s-expressions) relative to a given "environment"
substitution. The proof establishes the existence of an output substitution
that is a most-general idempotent unifier of the given expressions and is an
"extension" of the environment substitution. If no such substitution exists and
the expressions are not unifiable, the program is to produce a failure
indicator.
  Initially the environment substitution is the empty substitution, which makes
no replacements at all; during execution of recursive calls, the environment
substitution records the replacements that have been found so far. Our own
unification algorithm employs an environment, and such algorithms appear in the
literature [e.g., Luger and Stubblefield, 1997]. We suspect, in addition to
being more efficient, the three-argument algorithm with an environment is
easier to synthesize automatically than the two-argument version from the
Manna-Waldinger paper.

</details>


### [21] [Encoding and Reasoning About Arrays in Set Theory](https://arxiv.org/abs/2508.11447)
*Maximiliano Cristiá,Gianfranco Rossi*

Main category: cs.LO

TL;DR: 该论文提出了一种将数组编码为函数和有序对集合的方法，通过集合论片段定义程序规范，并提供了一个决策过程。


<details>
  <summary>Details</summary>
Motivation: 动机是通过集合论简化数组推理，将其转化为集合推理，从而在同一语言和求解器中实现集合、函数和数组的统一处理。

Method: 方法是将数组编码为函数和有序对集合，定义集合论片段用于程序规范，并开发决策过程。

Result: 结果是实现了{log}工具的扩展，支持数组推理，并验证了集合、函数和数组的统一处理能力。

Conclusion: 结论是通过集合论片段和决策过程，成功将数组推理无缝集成到现有的集合论求解器中。

Abstract: We encode arrays as functions which, in turn, are encoded as sets of ordered
pairs. The set cardinality of each of these functions coincides with the length
of the array it is representing. Then we define a fragment of set theory that
is used to give the specifications of a non-trivial class of programs with
arrays. In this way, array reasoning becomes set reasoning. Furthermore, a
decision procedure for this fragment is also provided and implemented as part
of the {log} (read 'setlog') tool. {log} is a constraint logic programming
language and satisfiability solver where sets and binary relations are
first-class citizens. The tool already implements a few decision procedures for
different fragments of set theory. In this way, arrays are seamlessly
integrated into {log} thus allowing users to reason about sets, functions and
arrays all in the same language and with the same solver. The decision
procedure presented in this paper is an extension of decision procedures
defined in earlier works not supporting arrays.

</details>


### [22] [Interpolation in Classical Propositional Logic](https://arxiv.org/abs/2508.11449)
*Patrick Koopmann,Christoph Wernhard,Frank Wolter*

Main category: cs.LO

TL;DR: 本文介绍了Craig插值及其相关概念，提出了四种计算插值的方法，并讨论了插值大小与电路复杂度的联系。


<details>
  <summary>Details</summary>
Motivation: 探讨经典命题逻辑中的插值理论及其应用。

Method: 通过量词消除、析取范式公式、从解析或表推演中提取等四种方法计算插值。

Result: 提出了多种插值计算方法，并分析了插值的大小。

Conclusion: 插值理论与电路复杂度有重要联系，为逻辑研究提供了新的视角。

Abstract: We introduce Craig interpolation and related notions such as uniform
interpolation, Beth definability, and theory decomposition in classical
propositional logic. We present four approaches to computing interpolants: via
quantifier elimination, from formulas in disjunctive normal form, and by
extraction from resolution or tableau refutations. We close with a discussion
of the size of interpolants and links to circuit complexity.

</details>


### [23] [Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations](https://arxiv.org/abs/2508.11515)
*Qipeng Kuang,Václav Kůla,Ondřej Kuželka,Yuanhong Wang,Yuyi Wang*

Main category: cs.LO

TL;DR: 本文研究了加权一阶模型计数问题（WFOMC）在两变量逻辑片段（FO²）中扩展到两个关系的复杂性边界，既展示了负面的#P₁-难结果，也提供了正面的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于单一关系的扩展，而忽视了多关系公理的复杂性边界，本文旨在填补这一空白。

Method: 通过分析两变量逻辑片段（FO²）扩展到两个关系的情况，展示其#P₁-难性，并提出多项式时间算法解决C²片段的特定扩展。

Result: 证明了FO²加两个线性序关系或两个无环关系的WFOMC是#P₁-难的，同时为C²加线性序关系及两个后继关系提供了多项式时间算法。

Conclusion: 多关系公理的扩展显著提高了WFOMC的复杂性，但在特定情况下仍存在高效算法。

Abstract: The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the
weighted sum of models of a given first-order logic sentence over a given
domain. The boundary between fragments for which WFOMC can be computed in
polynomial time relative to the domain size lies between the two-variable
fragment ($\text{FO}^2$) and the three-variable fragment ($\text{FO}^3$). It is
known that WFOMC for \FOthree{} is $\mathsf{\#P_1}$-hard while polynomial-time
algorithms exist for computing WFOMC for $\text{FO}^2$ and $\text{C}^2$,
possibly extended by certain axioms such as the linear order axiom, the
acyclicity axiom, and the connectedness axiom. All existing research has
concentrated on extending the fragment with axioms on a single distinguished
relation, leaving a gap in understanding the complexity boundary of axioms on
multiple relations. In this study, we explore the extension of the two-variable
fragment by axioms on two relations, presenting both negative and positive
results. We show that WFOMC for $\text{FO}^2$ with two linear order relations
and $\text{FO}^2$ with two acyclic relations are $\mathsf{\#P_1}$-hard.
Conversely, we provide an algorithm in time polynomial in the domain size for
WFOMC of $\text{C}^2$ with a linear order relation, its successor relation and
another successor relation.

</details>


### [24] [Robust Topology and the Hausdorff-Smyth Monad on Metric Spaces over Continuous Quantales](https://arxiv.org/abs/2508.11623)
*Francesco Dagnino,Amin Farjudian Eugenio Moggi*

Main category: cs.LO

TL;DR: 该论文定义了一个预序丰富的范畴$\mathsf{Met}$，研究连续量词值度量空间及一致连续映射，并提出了Hausdorff-Smyth单子$\mathsf{P}_S$，用于捕获稳健拓扑。


<details>
  <summary>Details</summary>
Motivation: 为解决计算和物理系统中关于不精确性和稳健性的定量推理问题，提出一个基于连续量词值度量的统一框架。

Method: 定义范畴$\mathsf{Met}$及其对象$(X,d,Q)$，引入开放球拓扑$\tau_d$和稳健拓扑$\tau_{d,R}$，并构造Hausdorff-Smyth单子$\mathsf{P}_S$。

Result: 证明了每个拓扑均可由量词值度量生成，且单子$\mathsf{P}_S$的开放球拓扑与稳健拓扑一致。

Conclusion: 该框架为广泛的计算和物理系统提供了关于不精确性和稳健性的定量推理基础。

Abstract: We define a (preorder-enriched) category $\mathsf{Met}$ of quantale-valued
metric spaces and uniformly continuous maps, with the essential requirement
that the quantales are continuous. For each object $(X,d,Q)$ in this category,
where $X$ is the carrier set, $Q$ is a continuous quantale, and $d: X \times X
\to Q$ is the metric, we consider a topology $\tau_d$ on $X$, which generalizes
the open ball topology, and a topology $\tau_{d,R}$ on the powerset
$\mathsf{P}(X)$, called the robust topology, which captures robustness with
respect to small perturbations of parameters. We define a (preorder-enriched)
monad $\mathsf{P}_S$ on $\mathsf{Met}$, called the Hausdorff-Smyth monad, which
captures the robust topology, in the sense that the open ball topology of the
object $\mathsf{P}_S(X,d,Q)$ coincides with the robust topology $\tau_{d,R}$
for the object $(X,d,Q)$. We prove that every topology arises from a
quantale-valued metric. As such, our framework provides a foundation for
quantitative reasoning about imprecision and robustness in a wide range of
computational and physical systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [25] [How do Data Journalists Design Maps to Tell Stories?](https://arxiv.org/abs/2508.10903)
*Arlindo Gomes,Emilly Brito,Luis Morais,Nivan Ferreira*

Main category: cs.HC

TL;DR: 本文探讨了新闻媒体中地图设计的挑战与过程，通过分析462个新闻地图和采访数据记者，提出了包含八个维度的设计空间，并总结了常见的设计理由和实践差距。


<details>
  <summary>Details</summary>
Motivation: 新闻地图设计面临多重挑战，如美学、受众数据素养、紧迫的截止日期和技术能力限制，缺乏专业背景的数据记者需要更系统的设计指导。

Method: 通过分析462个新闻地图的语料库，并采访四位数据记者，确定设计空间和设计实践。

Result: 提出了包含八个维度的设计空间，识别了常见设计理由和实践中的潜在差距。

Conclusion: 研究结果为研究人员和记者提供了设计和研究新闻地图的实证依据。

Abstract: Maps are essential to news media as they provide a familiar way to convey
spatial context and present engaging narratives. However, the design of
journalistic maps may be challenging, as editorial teams need to balance
multiple aspects, such as aesthetics, the audience's expected data literacy,
tight publication deadlines, and the team's technical skills. Data journalists
often come from multiple areas and lack a cartography, data visualization, and
data science background, limiting their competence in creating maps. While
previous studies have examined spatial visualizations in data stories, this
research seeks to gain a deeper understanding of the map design process
employed by news outlets. To achieve this, we strive to answer two specific
research questions: what is the design space of journalistic maps? and how do
editorial teams produce journalistic map articles? To answer the first one, we
collected and analyzed a large corpus of 462 journalistic maps used in news
articles from five major news outlets published over three months. As a result,
we created a design space comprised of eight dimensions that involved both
properties describing the articles' aspects and the visual/interactive features
of maps. We approach the second research question via semi-structured
interviews with four data journalists who create data-driven articles daily.
Through these interviews, we identified the most common design rationales made
by editorial teams and potential gaps in current practices. We also collected
the practitioners' feedback on our design space to externally validate it. With
these results, we aim to provide researchers and journalists with empirical
data to design and study journalistic maps.

</details>


### [26] [Designing for Engaging Communication Between Parents and Young Adult Children Through Shared Music Experiences](https://arxiv.org/abs/2508.10907)
*Euihyeok Lee,Souneil Park,Jin Yu,Seungchul Lee,Seungwoo Kang*

Main category: cs.HC

TL;DR: 这篇论文通过音乐促进父母与成年子女的社交互动，开发了一款名为DJ-Fam的移动应用，通过分享歌曲来丰富日常交流。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改善父母与成年子女因分居而减少的社交互动，利用音乐作为媒介增强他们的沟通。

Method: 通过调查现有的亲子沟通需求及音乐互动体验，开发了DJ-Fam应用，并进行了为期四周的实地测试。

Result: DJ-Fam显著增加了亲子沟通的频率和多样性，提升了彼此的理解和关系。

Conclusion: 研究表明，音乐可以成为促进亲子互动的有效媒介，DJ-Fam的应用显示出积极的社交效果。

Abstract: This paper aims to foster social interaction between parents and young adult
children living apart via music. Our approach transforms their music-listening
moment into an opportunity to listen to the other's favorite songs and enrich
interaction in their daily lives. To this end, we explore the current practice
and needs of parent-child communication and the experience and perception of
music-mediated interaction. Based on the findings, we developed DJ-Fam, a
mobile application that enables parents and children to listen to their
favorite songs and use them as conversation starters to foster parent-child
interaction. From our deployment study with seven families over four weeks in
South Korea, we show the potential of DJ-Fam to influence parent-child
interaction and their mutual understanding and relationship positively.
Specifically, DJ-Fam considerably increases the frequency of communication and
diversifies the communication channels and topics, all of which are
satisfactory to the participants.

</details>


### [27] [Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil](https://arxiv.org/abs/2508.10911)
*Luis Vitor Zerkowski,Nina S. T. Hirata*

Main category: cs.HC

TL;DR: 利用AI技术提升巴西土著文化遗产的在线可访问性和互动探索。


<details>
  <summary>Details</summary>
Motivation: 针对土著文化因边缘化和城市化而面临的保存挑战，通过技术手段增强文化遗产的互动性和可及性。

Method: 开发视觉和文本两条语义管道，构建嵌入空间，并集成到交互式可视化工具中。

Result: 系统支持策展任务、促进公众参与，并揭示藏品中的潜在联系。

Conclusion: AI可伦理地为文化保护实践做贡献。

Abstract: Indigenous communities face ongoing challenges in preserving their cultural
heritage, particularly in the face of systemic marginalization and urban
development. In Brazil, the Museu Nacional dos Povos Indigenas through the
Tainacan platform hosts the country's largest online collection of Indigenous
objects and iconographies, providing a critical resource for cultural
engagement. Using publicly available data from this repository, we present a
data-driven initiative that applies artificial intelligence to enhance
accessibility, interpretation, and exploration. We develop two semantic
pipelines: a visual pipeline that models image-based similarity and a textual
pipeline that captures semantic relationships from item descriptions. These
embedding spaces are projected into two dimensions and integrated into an
interactive visualization tool we also developed. In addition to
similarity-based navigation, users can explore the collection through temporal
and geographic lenses, enabling both semantic and contextualized perspectives.
The system supports curatorial tasks, aids public engagement, and reveals
latent connections within the collection. This work demonstrates how AI can
ethically contribute to cultural preservation practices.

</details>


### [28] [Generation and Evaluation in the Human Invention Process through the Lens of Game Design](https://arxiv.org/abs/2508.10914)
*Katherine M. Collins,Graham Todd,Cedegao E. Zhang,Adrian Weller,Julian Togelius,Junyi Chu,Lionel Wong,Thomas L. Griffiths,Joshua B. Tenenbaum*

Main category: cs.HC

TL;DR: 研究探讨人类创造新游戏时的认知机制，重点关注基于经验的联想提议和基于模型的评估。


<details>
  <summary>Details</summary>
Motivation: 人类不仅遵循规则和解决问题，还擅长创造新规则和问题。研究旨在理解这种创新能力的认知基础，尤其是低风险环境下的游戏设计。

Method: 分析450多个由人类设计的游戏，研究两种认知机制：基于先前游戏的联想提议和基于模型的评估。

Result: 生成的游戏最能由结合群体水平游戏质量评估的模型描述。

Conclusion: 人类创新不仅依赖于提议，还依赖于如何评估提议，研究为开放式创新的实证研究提供了计算工具。

Abstract: The human ability to learn rules and solve problems has been a central
concern of cognitive science research since the field's earliest days. But we
do not just follow rules and solve problems given to us by others: we modify
those rules, create new problems, and set new goals and tasks for ourselves and
others. Arguably, even more than rule following and problem solving, human
intelligence is about creatively breaking and stretching the rules, changing
the game, and inventing new problems worth thinking about. Creating a good rule
or a good problem depends not just on the ideas one can think up but on how one
evaluates such proposals. Here, we study invention through the lens of game
design. We focus particularly on the early stages of novice, "everyday" game
creation, where the stakes are low. We draw on a dataset of over 450 human
created games, created by participants who saw an initial seed set of
two-player grid-based strategy games. We consider two different cognitive
mechanisms that may be at work during the early processes of intuitive game
invention: an associative proposal based on previous games one has seen and
compute-bounded model-based evaluation that an everyday game creator may use to
refine their initial draft proposals. In our preliminary work, we conduct a
model-based analysis of how people invented new games based on prior experience
and find that generated games are best described by a model which incorporates
model-based estimates of game quality at a population level. Our work points to
how human invention is based not only on what people propose, but how they
evaluate and offers a computational toolkit to scale empirical studies of
model-based simulation in open-ended human innovation.

</details>


### [29] [Multimodal Quantitative Measures for Multiparty Behaviour Evaluation](https://arxiv.org/abs/2508.10916)
*Ojas Shirekar,Wim Pouw,Chenxu Hao,Vrushank Phadnis,Thabo Beeler,Chirag Raman*

Main category: cs.HC

TL;DR: 论文提出了一个统一的干预驱动框架，用于评估多党派社交行为，包括同步性、时间对齐和结构相似性三个维度，并通过实验验证了指标的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估指标忽略了多党派互动中的上下文协调动态，因此需要一种新的方法来客观评估社交行为。

Method: 通过跨递归量化分析、多尺度经验模态分解和软动态时间规整三个互补维度评估多党派社交行为，并采用理论驱动的扰动验证指标。

Result: 实验表明，提出的三个指标能够捕捉行为的不同方面，如同步性、时间对齐和结构相似性，且在感知研究中表现出有效性。

Conclusion: 该框架为评估和优化具有社交智能的代理提供了一个强大的工具包。

Abstract: Digital humans are emerging as autonomous agents in multiparty interactions,
yet existing evaluation metrics largely ignore contextual coordination
dynamics. We introduce a unified, intervention-driven framework for objective
assessment of multiparty social behaviour in skeletal motion data, spanning
three complementary dimensions: (1) synchrony via Cross-Recurrence
Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode
Decompositionbased Beat Consistency, and (3) structural similarity via Soft
Dynamic Time Warping. We validate metric sensitivity through three
theory-driven perturbations -- gesture kinematic dampening, uniform
speech-gesture delays, and prosodic pitch-variance reduction-applied to
$\approx 145$ 30-second thin slices of group interactions from the DnD dataset.
Mixed-effects analyses reveal predictable, joint-independent shifts: dampening
increases CRQA determinism and reduces beat consistency, delays weaken
cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A
complementary perception study ($N=27$) compares judgments of full-video and
skeleton-only renderings to quantify representation effects. Our three measures
deliver orthogonal insights into spatial structure, timing alignment, and
behavioural variability. Thereby forming a robust toolkit for evaluating and
refining socially intelligent agents. Code available on
\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.

</details>


### [30] [Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses](https://arxiv.org/abs/2508.10917)
*Chidera W. Amazu,Joseph Mietkiewicz,Ammar N. Abbas,Gabriele Baldissone,Davide Fissore,Micaela Demichela,Anders L. Madsen,Maria Chiara Leva*

Main category: cs.HC

TL;DR: 論文探討如何利用實時數據分析控制室操作員的行為和反應，無需侵入性設備。


<details>
  <summary>Details</summary>
Motivation: 現有生理測量工具可能干擾日常操作，需尋找替代方法來評估操作員行為和認知狀態。

Method: 使用甲醛生產模擬器和四種人機實驗配置，結合逐步邏輯回歸和貝葉斯網絡模型分析數據。

Result: 發現一些可預測指標，可用於即時評估操作員表現並支持決策。

Conclusion: 實時行為指標可幫助預測操作員表現，提供及時支持。

Abstract: Data from psychophysiological measures can offer new insight into control
room operators' behaviour, cognition, and mental workload status. This can be
particularly helpful when combined with appraisal of capacity to respond to
possible critical plant conditions (i.e. critical alarms response scenarios).
However, wearable physiological measurement tools such as eye tracking and EEG
caps can be perceived as intrusive and not suitable for usage in daily
operations. Therefore, this article examines the potential of using real-time
data from process and operator-system interactions during abnormal scenarios
that can be recorded and retrieved from the distributed control system's
historian or process log, and their capacity to provide insight into operator
behavior and predict their response outcomes, without intruding on daily tasks.
Data for this study were obtained from a design of experiment using a
formaldehyde production plant simulator and four human-in-the-loop experimental
support configurations. A comparison between the different configurations in
terms of both behaviour and performance is presented in this paper. A step-wise
logistic regression and a Bayesian network models were used to achieve this
objective. The results identified some predictive metrics and the paper discuss
their value as precursor or predictor of overall system performance in alarm
response scenarios. Knowledge of relevant and predictive behavioural metrics
accessible in real time can better equip decision-makers to predict outcomes
and provide timely support measures for operators.

</details>


### [31] [Stories and Systems: Educational Interactive Storytelling to Teach Media Literacy and Systemic Thinking](https://arxiv.org/abs/2508.11059)
*Christian Roth,Rahmin Bender-Salazar,Breanne Pitt*

Main category: cs.HC

TL;DR: 论文探讨交互式数字叙事（IDN）如何帮助学习者培养解决复杂社会问题（如气候变化）所需的批判性素养，提出CLASS框架指导设计。


<details>
  <summary>Details</summary>
Motivation: 数字技术普及带来信息过载和问题简化，IDN通过非线性互动促进深层次理解。

Method: 提出CLASS框架，结合系统思维、设计思维和叙事，应用于两个案例（商业模拟和教育原型）。

Result: IDN通过叙事和系统映射成为系统导向学习的有效工具。

Conclusion: IDN结合CLASS框架可支持复杂世界中的变革性学习。

Abstract: This paper explores how Interactive Digital Narratives (IDNs) can support
learners in developing the critical literacies needed to address complex
societal challenges, so-called wicked problems, such as climate change,
pandemics, and social inequality. While digital technologies offer broad access
to narratives and data, they also contribute to misinformation and the
oversimplification of interconnected issues. IDNs enable learners to navigate
nonlinear, interactive stories, fostering deeper understanding and engagement.
We introduce Systemic Learning IDNs: interactive narrative experiences
explicitly designed to help learners explore and reflect on complex systems and
interdependencies. To guide their creation and use, we propose the CLASS
framework, a structured model that integrates systems thinking, design
thinking, and storytelling. This transdisciplinary approach supports learners
in developing curiosity, critical thinking, and collaborative problem-solving.
Focusing on the classroom context, we apply CLASS to two cases, one commercial
narrative simulation and one educational prototype, offering a comparative
analysis and practical recommendations for future design and implementation. By
combining narrative, systems mapping, and participatory design, this paper
highlights how IDNs can become powerful tools for transformative,
systems-oriented learning in an increasingly complex world.

</details>


### [32] [Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?](https://arxiv.org/abs/2508.10919)
*Mohammed Saqr,Kamila Misiejuk,Sonsoles López-Pernas*

Main category: cs.HC

TL;DR: 研究探讨了人类与AI在解决复杂问题时的互动模式，发现当前的LLM（如ChatGPT）更倾向于指令遵循而非认知合作，缺乏协同效应，且任务复杂度与学生表现无显著关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注语言学习，忽略了AI在认知需求任务中的动态合作。本研究填补了这一空白。

Method: 通过定性编码和多种分析方法（如过渡网络、序列分析等）研究学生与AI的互动模式。

Result: 发现互动以指令性为主，缺乏协同；任务复杂度与成绩无显著关联。

Conclusion: 当前LLM更适合指令遵循而非认知合作，设计AI系统需更注重认知对齐与合作。

Abstract: While research on human-AI collaboration exists, it mainly examined language
learning and used traditional counting methods with little attention to
evolution and dynamics of collaboration on cognitively demanding tasks. This
study examines human-AI interactions while solving a complex problem.
Student-AI interactions were qualitatively coded and analyzed with transition
network analysis, sequence analysis and partial correlation networks as well as
comparison of frequencies using chi-square and Person-residual shaded Mosaic
plots to map interaction patterns, their evolution, and their relationship to
problem complexity and student performance. Findings reveal a dominant
Instructive pattern with interactions characterized by iterative ordering
rather than collaborative negotiation. Oftentimes, students engaged in long
threads that showed misalignment between their prompts and AI output that
exemplified a lack of synergy that challenges the prevailing assumptions about
LLMs as collaborative partners. We also found no significant correlations
between assignment complexity, prompt length, and student grades suggesting a
lack of cognitive depth, or effect of problem difficulty. Our study indicates
that the current LLMs, optimized for instruction-following rather than
cognitive partnership, compound their capability to act as cognitively
stimulating or aligned collaborators. Implications for designing AI systems
that prioritize cognitive alignment and collaboration are discussed.

</details>


### [33] [GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality](https://arxiv.org/abs/2508.11022)
*Lauren W. Wang,Parastoo Abtahi*

Main category: cs.HC

TL;DR: 提出一种通过增强现实（AR）中的虚拟孪生对象（GhostObjects）来指导机器人完成任务的新方法，取代传统的直接控制方式。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人自主性增强，但个性化指令仍需人类交互，传统方法如演示编程或遥操作存在局限性。

Method: 利用AR中的GhostObjects，用户可直接操作虚拟对象来精确指定物理目标和空间参数。

Result: 该方法实现了多对象选择和默认位置回位等功能，适用于比简单的拾取-放置更复杂的任务。

Conclusion: GhostObjects提供了一种更直观、灵活的机器人指令方式。

Abstract: Robots are increasingly capable of autonomous operations, yet human
interaction remains essential for issuing personalized instructions. Instead of
directly controlling robots through Programming by Demonstration (PbD) or
teleoperation, we propose giving instructions by interacting with
GhostObjects-world-aligned, life-size virtual twins of physical objects-in
augmented reality (AR). By direct manipulation of GhostObjects, users can
precisely specify physical goals and spatial parameters, with features
including real-world lasso selection of multiple objects and snapping back to
default positions, enabling tasks beyond simple pick-and-place.

</details>


### [34] [Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas](https://arxiv.org/abs/2508.11278)
*Francesco Sovrano,Gabriele Dominici,Rita Sevastjanova,Alessandra Stramiglio,Alberto Bacchelli*

Main category: cs.HC

TL;DR: 研究发现通用AI系统在软件工程任务中表现出认知偏差，其敏感性随任务复杂度增加而上升。


<details>
  <summary>Details</summary>
Motivation: 探讨通用AI系统是否因训练数据而表现出人类认知偏差，以评估其在软件工程中的潜在风险。

Method: 提出动态基准框架，通过16个任务测试8种认知偏差，并利用AI生成任务变体以确保多样性和正确性。

Result: 主要AI系统（GPT、LLaMA、DeepSeek）均表现出认知偏差（5.9%-35%），且偏差敏感性随任务复杂度显著增加（最高49%）。

Conclusion: AI系统的认知偏差在复杂任务中风险显著，需进一步研究以提升其在软件工程中的可靠性。

Abstract: Human cognitive biases in software engineering can lead to costly errors.
While general-purpose AI (GPAI) systems may help mitigate these biases due to
their non-human nature, their training on human-generated data raises a
critical question: Do GPAI systems themselves exhibit cognitive biases?
  To investigate this, we present the first dynamic benchmarking framework to
evaluate data-induced cognitive biases in GPAI within software engineering
workflows. Starting with a seed set of 16 hand-crafted realistic tasks, each
featuring one of 8 cognitive biases (e.g., anchoring, framing) and
corresponding unbiased variants, we test whether bias-inducing linguistic cues
unrelated to task logic can lead GPAI systems from correct to incorrect
conclusions.
  To scale the benchmark and ensure realism, we develop an on-demand
augmentation pipeline relying on GPAI systems to generate task variants that
preserve bias-inducing cues while varying surface details. This pipeline
ensures correctness (88--99% on average, according to human evaluation),
promotes diversity, and controls reasoning complexity by leveraging
Prolog-based reasoning and LLM-as-a-judge validation. It also verifies that the
embedded biases are both harmful and undetectable by logic-based, unbiased
reasoners.
  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent
tendency to rely on shallow linguistic heuristics over deep reasoning. All
systems exhibit cognitive biases (ranging from 5.9% to 35% across types), with
bias sensitivity increasing sharply with task complexity (up to 49%),
highlighting critical risks in real-world software engineering deployments.

</details>


### [35] [Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats](https://arxiv.org/abs/2508.11030)
*Zikai Wen,Lanjing Liu,Yaxing Yao*

Main category: cs.HC

TL;DR: 研究探讨了家庭如何利用生成式AI（GenAI）多代理系统来应对安全和隐私挑战，提出了四种隐私保护原则。


<details>
  <summary>Details</summary>
Motivation: 随着家庭面临日益复杂的数字和物理环境安全挑战，生成式AI为家庭安全提供了新的支持机会。

Method: 通过两阶段定性研究，包括与13对亲子组的个人访谈和协作会议，探讨家庭对GenAI的设想和AI代理的使用。

Result: 家庭倾向于将安全支持分散到多个AI代理，每个代理扮演熟悉角色，并强调隐私边界、信任代际差异和家庭沟通。

Conclusion: 提出了一种多代理系统设计，包含四项隐私保护原则，以平衡家庭环境中的安全、隐私和自主性。

Abstract: As families face increasingly complex safety challenges in digital and
physical environments, generative AI (GenAI) presents new opportunities to
support household safety through multiple specialized AI agents. Through a
two-phase qualitative study consisting of individual interviews and
collaborative sessions with 13 parent-child dyads, we explored families'
conceptualizations of GenAI and their envisioned use of AI agents in daily
family life. Our findings reveal that families preferred to distribute
safety-related support across multiple AI agents, each embodying a familiar
caregiving role: a household manager coordinating routine tasks and mitigating
risks such as digital fraud and home accidents; a private tutor providing
personalized educational support, including safety education; and a family
therapist offering emotional support to address sensitive safety issues such as
cyberbullying and digital harassment. Families emphasized the need for
agent-specific privacy boundaries, recognized generational differences in trust
toward AI agents, and stressed the importance of maintaining open family
communication alongside the assistance of AI agents. Based on these findings,
we propose a multi-agent system design featuring four privacy-preserving
principles: memory segregation, conversational consent, selective data sharing,
and progressive memory management to help balance safety, privacy, and autonomy
within family contexts.

</details>


### [36] [AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching](https://arxiv.org/abs/2508.11052)
*Evey Jiaxin Huang,Matthew Easterday,Elizabeth Gerber*

Main category: cs.HC

TL;DR: 论文提出了一种结合认知模型与大语言模型（LLM）的人机辅导系统，旨在帮助创业新手和导师解决创业中的不确定性，提升会议质量和情感共鸣。


<details>
  <summary>Details</summary>
Motivation: 创业中开放式问题的复杂性使新手和导师面临挑战，亟需一种能够主动提供支持的系统。

Method: 开发了一个结合领域认知模型和LLM的系统，主动提出问题并支持导师修改模型逻辑。

Result: 实地测试表明，系统提升了新手的元认知能力，帮助导师规划情感策略，并改善了会议质量。

Conclusion: 论文提出了主动AI系统的设计原则，可应用于医疗、教育等类似复杂领域。

Abstract: Entrepreneurship requires navigating open-ended, ill-defined problems:
identifying risks, challenging assumptions, and making strategic decisions
under deep uncertainty. Novice founders often struggle with these metacognitive
demands, while mentors face limited time and visibility to provide tailored
support. We present a human-AI coaching system that combines a domain-specific
cognitive model of entrepreneurial risk with a large language model (LLM) to
proactively scaffold both novice and mentor thinking. The system proactively
poses diagnostic questions that challenge novices' thinking and helps both
novices and mentors plan for more focused and emotionally attuned meetings.
Critically, mentors can inspect and modify the underlying cognitive model,
shaping the logic of the system to reflect their evolving needs. Through an
exploratory field deployment, we found that using the system supported novice
metacognition, helped mentors plan emotionally attuned strategies, and improved
meeting depth, intentionality, and focus--while also surfaced key tensions
around trust, misdiagnosis, and expectations of AI. We contribute design
principles for proactive AI systems that scaffold metacognition and human-human
collaboration in complex, ill-defined domains, offering implications for
similar domains like healthcare, education, and knowledge work.

</details>


### [37] [Human-in-the-Loop Systems for Adaptive Learning Using Generative AI](https://arxiv.org/abs/2508.11062)
*Bhavishya Tarun,Haoze Du,Dinesh Kannan,Edward F. Gehringer*

Main category: cs.HC

TL;DR: 使用人在回路（HITL）方法，通过学生反馈和AI生成解决方案的结合，提升个性化学习效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过学生直接参与AI生成内容的修改和反馈，提升学习参与度和理解深度，尤其是在STEM教育领域。

Method: 采用预定义反馈标签和提示工程技术，结合检索增强生成（RAG）系统，实时调整教学内容以满足学生需求。

Result: 初步研究表明，与传统AI工具相比，学生通过该方法的STEM学习效果和自信心有所提高。

Conclusion: AI可以通过动态反馈驱动和个性化的学习环境，提升学生的保留率和参与度。

Abstract: A Human-in-the-Loop (HITL) approach leverages generative AI to enhance
personalized learning by directly integrating student feedback into
AI-generated solutions. Students critique and modify AI responses using
predefined feedback tags, fostering deeper engagement and understanding. This
empowers students to actively shape their learning, with AI serving as an
adaptive partner. The system uses a tagging technique and prompt engineering to
personalize content, informing a Retrieval-Augmented Generation (RAG) system to
retrieve relevant educational material and adjust explanations in real time.
This builds on existing research in adaptive learning, demonstrating how
student-driven feedback loops can modify AI-generated responses for improved
student retention and engagement, particularly in STEM education. Preliminary
findings from a study with STEM students indicate improved learning outcomes
and confidence compared to traditional AI tools. This work highlights AI's
potential to create dynamic, feedback-driven, and personalized learning
environments through iterative refinement.

</details>


### [38] [DriveSimQuest: A VR Driving Simulator and Research Platform on Meta Quest with Unity](https://arxiv.org/abs/2508.11072)
*Nishanth Chidambaram,Weichen Liu,Manas Satish Bedmutha,Nadir Weibel,Chen Chen*

Main category: cs.HC

TL;DR: DriveSimQuest是一个基于Meta Quest Pro和Unity开发的VR驾驶模拟器，能够实时捕捉丰富的驾驶行为信号，解决了现有模拟器的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的VR驾驶模拟器通常只能跟踪眼部运动，且设备笨重、架构复杂，限制了交互研究。

Method: 开发了DriveSimQuest平台，利用Meta Quest Pro和Unity技术，实时捕捉多种行为信号。

Result: 提供了一个易于部署的平台，支持研究者研究驾驶员的 affective states和行为，设计未来的情境感知驾驶辅助系统。

Conclusion: DriveSimQuest为驾驶行为研究和辅助系统设计提供了一个高效、多功能的解决方案。

Abstract: Using head-mounted Virtual Reality (VR) displays to simulate driving is
critical to studying driving behavior and designing driver assistance systems.
But existing VR driving simulators are often limited to tracking only eye
movements. The bulky outside-in tracking setup and Unreal-based architecture
also present significant engineering challenges for interaction researchers and
practitioners. We present DriveSimQuest, a VR driving simulator and research
platform built on the Meta Quest Pro and Unity, capable of capturing rich
behavioral signals such as gaze, facial expressions, hand activities, and
full-body gestures in real-time. DriveSimQuest offers a preliminary,
easy-to-deploy platform that supports researchers and practitioners in studying
drivers' affective states and behaviors, and in designing future context-aware
driving assistance systems.

</details>


### [39] [Toward Needs-Conscious Design: Co-Designing a Human-Centered Framework for AI-Mediated Communication](https://arxiv.org/abs/2508.11149)
*Robert Wolfe,Aayushi Dangol,JaeWon Kim,Alexis Hiniker*

Main category: cs.HC

TL;DR: 论文提出了一个以人为中心的AI交流框架‘Needs-Conscious Design’，基于非暴力沟通（NVC）原则，旨在通过三个支柱（意向性、在场性和需求开放性）促进人际关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨如何利用NVC原则设计AI交流技术，以增强人际关系而非替代或模糊它们。

Method: 方法包括对N=14名NVC认证培训师的访谈研究，以及对N=13名在线交流技术用户的日记研究和共同设计。

Result: 结果包括定义了Needs-Conscious Design的三个支柱，提出了相关设计概念，并识别了AI交流中的‘Empathy Fog’现象。

Conclusion: 结论是Needs-Conscious Design为利用AI促进人际连接提供了基础，并提出了基于同意框架的设计指导问题。

Abstract: We introduce Needs-Conscious Design, a human-centered framework for
AI-mediated communication that builds on the principles of Nonviolent
Communication (NVC). We conducted an interview study with N=14 certified NVC
trainers and a diary study and co-design with N=13 lay users of online
communication technologies to understand how NVC might inform design that
centers human relationships. We define three pillars of Needs-Conscious Design:
Intentionality, Presence, and Receptiveness to Needs. Drawing on participant
co-designs, we provide design concepts and illustrative examples for each of
these pillars. We further describe a problematic emergent property of
AI-mediated communication identified by participants, which we call Empathy
Fog, and which is characterized by uncertainty over how much empathy,
attention, and effort a user has actually invested via an AI-facilitated online
interaction. Finally, because even well-intentioned designs may alter user
behavior and process emotional data, we provide guiding questions for
consentful Needs-Conscious Design, applying an affirmative consent framework
used in social media contexts. Needs-Conscious Design offers a foundation for
leveraging AI to facilitate human connection, rather than replacing or
obscuring it.

</details>


### [40] [From Misunderstandings to Learning Opportunities: Leveraging Generative AI in Discussion Forums to Support Student Learning](https://arxiv.org/abs/2508.11150)
*Stanislav Pozdniakov,Jonathan Brazil,Oleksandra Poquet,Stephan Krusche,Santiago Berrezueta-Guzman,Shazia Sadiq,Hassan Khosravi*

Main category: cs.HC

TL;DR: 论文探讨了如何利用大型语言模型（LLMs）和检索增强生成（RAG）技术识别学生讨论中的常见误解，并提出解决方案，最终通过真实课程数据验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在大课堂讨论论坛中，学生生成的内容量大且难以有效识别常见误解，亟需技术手段帮助教师快速发现问题并提供解决方案。

Method: 提出Misunderstanding to Mastery（M2M）方法，结合LLMs和RAG技术，分析学生讨论内容，识别误解并生成可操作建议。

Result: 实验数据显示，该方法能有效识别误解并为教师提供有价值的教学反馈，但仍需改进分组粒度、指标清晰度和数据匿名性等。

Conclusion: 该方法为教师提供了实用工具，但仍需进一步优化，尤其是在分组细节、指标设计及数据伦理方面。

Abstract: In the contemporary educational landscape, particularly in large classroom
settings, discussion forums have become a crucial tool for promoting
interaction and addressing student queries. These forums foster a collaborative
learning environment where students engage with both the teaching team and
their peers. However, the sheer volume of content generated in these forums
poses two significant interconnected challenges: How can we effectively
identify common misunderstandings that arise in student discussions? And once
identified, how can instructors use these insights to address them effectively?
This paper explores the approach to integrating large language models (LLMs)
and Retrieval-Augmented Generation (RAG) to tackle these challenges. We then
demonstrate the approach Misunderstanding to Mastery (M2M) with authentic data
from three computer science courses, involving 1355 students with 2878 unique
posts, followed by an evaluation with five instructors teaching these courses.
Results show that instructors found the approach promising and valuable for
teaching, effectively identifying misunderstandings and generating actionable
insights. Instructors highlighted the need for more fine-grained groupings,
clearer metrics, validation of the created resources, and ethical
considerations around data anonymity.

</details>


### [41] [GulliVR: A Walking-Oriented Technique for Navigation in Virtual Reality Games Based on Virtual Body Resizing](https://arxiv.org/abs/2508.11304)
*Andrey Krekhov,Sebastian Cmentowski,Katharina Emmerich,Maic Masuch,Jens Krüger*

Main category: cs.HC

TL;DR: 论文探讨了一种通过比例缩放视距使玩家成为‘巨人’的技术，以增强虚拟现实中的物理行走体验，而非使用传统传送方法。


<details>
  <summary>Details</summary>
Motivation: 现代VR系统的房间尺度运动受限，开发者常使用传送等虚拟导航技术，但这些方法未能充分利用自然行走，降低了沉浸感。

Method: 提出了一种新导航隐喻，通过按比例增加视距，让玩家感觉身处微型世界，既覆盖大距离又避免晕动症。

Result: 相比传送技术，该方法显著提升了沉浸感和行走距离。

Conclusion: 该技术为VR游戏设计提供了新的方向，强调物理行走的增强效果。

Abstract: Virtual reality games are often centered around our feeling of "being there".
That presence can be significantly enhanced by supporting physical walking.
Although modern virtual reality systems enable room-scale motions, the size of
our living rooms is not enough to explore vast virtual environments. Developers
bypass that limitation by adding virtual navigation such as teleportation.
Although such techniques are intended (or designed) to extend but not replace
natural walking, what we often observe are nonmoving players beaming to a
location that is one real step ahead. Our navigation metaphor emphasizes
physical walking by promoting players into giants on demand to cover large
distances. In contrast to flying, our technique proportionally increases the
modeled eye distance, preventing cybersickness and creating the feeling of
being in a miniature world. Our evaluations underpin a significantly increased
presence and walking distance compared to the teleportation approach. Finally,
we derive a set of game design implications related to the integration of our
technique.

</details>


### [42] [Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games](https://arxiv.org/abs/2508.11314)
*Sebastian Cmentowski,Fabian Kievelitz,Jens Krüger*

Main category: cs.HC

TL;DR: 提出了一种新型虚拟现实游戏行走增强方法，通过虚拟隧道隐藏视觉流动，避免晕动症，同时增强身体活动并保持临场感。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟环境中物理行走空间不足的问题，同时避免因视觉流动增加导致的晕动症。

Method: 使用虚拟隧道设计，外部看起来覆盖整个行走距离，内部实际长度较短，通过窗户显示实际加速运动。

Result: 评估显示该方法成功避免晕动症，增强身体活动，并保持用户临场感。

Conclusion: 该方法有效解决了运动加速带来的视觉流动问题，但需进一步考虑设计和限制因素。

Abstract: The size of most virtual environments exceeds the tracking space available
for physical walking. One solution to this disparity is to extend the available
walking range by augmenting users' actual movements. However, the resulting
increase in visual flow can easily cause cybersickness. Therefore, we present a
novel augmented-walking approach for virtual reality games. Our core concept is
a virtual tunnel that spans the entire travel distance when viewed from the
outside. However, its interior is only a fraction as long, allowing users to
cover the distance by real walking. Whereas the tunnel hides the visual flow
from the applied movement acceleration, windows on the tunnel's walls still
reveal the actual expedited motion. Our evaluation reveals that our approach
avoids cybersickness while enhancing physical activity and preserving presence.
We finish our paper with a discussion of the design considerations and
limitations of our proposed locomotion technique.

</details>


### [43] [The User-first Approach to AI Ethics: Preferences for Ethical Principles in AI Systems across Cultures and Contexts](https://arxiv.org/abs/2508.11327)
*Benjamin J. Carroll,Jianlong Zhou,Paul F. Burke,Sabine Ammon*

Main category: cs.HC

TL;DR: 论文通过离散选择实验量化了用户对11项伦理原则的偏好，发现文化与应用场景影响偏好，提出用户为中心的AI伦理方法。


<details>
  <summary>Details</summary>
Motivation: 探索用户对AI伦理原则的重视程度，弥补现有研究空白。

Method: 在四国进行离散选择实验，利用潜在类别分析用户数据。

Result: 用户偏好隐私、公正与透明，但存在文化差异；发现四类用户群体，最大群体为伦理疏离型。

Conclusion: 研究为AI伦理的实践、监管及用户中心方法提供了实证依据与指导。

Abstract: As AI systems increasingly permeate everyday life, designers and developers
face mounting pressure to balance innovation with ethical design choices. To
date, the operationalisation of AI ethics has predominantly depended on
frameworks that prescribe which ethical principles should be embedded within AI
systems. However, the extent to which users value these principles remains
largely unexplored in the existing literature. In a discrete choice experiment
conducted in four countries, we quantify user preferences for 11 ethical
principles. Our findings indicate that, while users generally prioritise
privacy, justice & fairness, and transparency, their preferences exhibit
significant variation based on culture and application context. Latent class
analysis further revealed four distinct user cohorts, the largest of which is
ethically disengaged and defers to regulatory oversight. Our findings offer (1)
empirical evidence of uneven user prioritisation of AI ethics principles, (2)
actionable guidance for operationalising ethics tailored to culture and
context, (3) support for the development of robust regulatory mechanisms, and
(4) a foundation for advancing a user-centred approach to AI ethics, motivated
independently from abstract moral theory.

</details>


### [44] [Towards Smart Workplaces: Understanding Mood-Influencing Factors of the Physical Workspace in Collaborative Group Settings](https://arxiv.org/abs/2508.11335)
*Tzu-Hui Wu,Sebastian Cmentowski,Yunyin Lou,Jun Hu,Regina Bernhaupt*

Main category: cs.HC

TL;DR: 本文探讨了物理工作空间对团队情绪的影响，并研究了智能情绪感知技术的潜力。


<details>
  <summary>Details</summary>
Motivation: 团队情绪对工作环境体验和团队表现至关重要，但目前对其与物理工作空间的关系知之甚少。

Method: 通过定性用户研究（8个工作组，共26名参与者），探索物理工作空间如何影响团队情绪及员工对智能技术的看法。

Result: 研究发现影响团队情绪的关键因素，以及员工对支持性技术的隐私和自主性期望。

Conclusion: 自适应工作空间具有潜力，但需以人为中心的设计来提升团队福祉。

Abstract: Group mood plays a crucial role in shaping workspace experiences, influencing
group dynamics, team performance, and creativity. The perceived group mood
depends on many, often subconscious, aspects such as individual emotional
states or group life, which make it challenging to maintain a positive
atmosphere. Intelligent technology could support mood regulation in physical
office environments, for example, as adaptive ambient lighting for mood
regulation. However, little is known about the relationship between the
physical workspace and group mood dynamics. To address this knowledge gap, we
conducted a qualitative user study (N=8 workgroups and overall 26 participants)
to explore how the physical workspace shapes group mood experiences and
investigate employees' perspectives on intelligent mood-aware technologies. Our
findings reveal key factors influencing group mood, and participants'
expectations for supportive technology to preserve privacy and autonomy. Our
work highlights the potential of adaptive and responsive workspaces while also
emphasizing the need for human-centered, technology-driven interventions that
benefit group well-being.

</details>


### [45] [Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis](https://arxiv.org/abs/2508.11398)
*Mithat Can Ozgun,Jiahuan Pei,Koen Hindriks,Lucia Donatelli,Qingzhi Liu,Xin Sun,Junxiao Wang*

Main category: cs.HC

TL;DR: 提出DSM5AgentFlow，首个基于LLM的代理工作流，用于自动生成DSM-5 Level-1诊断问卷，解决现有方法在心理健康诊断中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在心理健康诊断等专业领域效果不佳，数据稀缺且难以模拟临床医生的主动提问能力，缺乏多轮对话理解和专家临床推理对齐。

Method: 提出DSM5AgentFlow框架，通过模拟治疗师-客户对话，生成透明、逐步的障碍预测，提供可解释的结果。

Result: 实验评估了主流LLM在对话真实性、诊断准确性和可解释性三个维度的表现，数据集和实现已开源。

Conclusion: DSM5AgentFlow为心理健康诊断提供了一种互补工具，符合伦理和法律标准。

Abstract: LLM-based agents have emerged as transformative tools capable of executing
complex tasks through iterative planning and action, achieving significant
advancements in understanding and addressing user needs. Yet, their
effectiveness remains limited in specialized domains such as mental health
diagnosis, where they underperform compared to general applications. Current
approaches to integrating diagnostic capabilities into LLMs rely on scarce,
highly sensitive mental health datasets, which are challenging to acquire.
These methods also fail to emulate clinicians' proactive inquiry skills, lack
multi-turn conversational comprehension, and struggle to align outputs with
expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the
first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1
diagnostic questionnaires. By simulating therapist-client dialogues with
specific client profiles, the framework delivers transparent, step-by-step
disorder predictions, producing explainable and trustworthy results. This
workflow serves as a complementary tool for mental health diagnosis, ensuring
adherence to ethical and legal standards. Through comprehensive experiments, we
evaluate leading LLMs across three critical dimensions: conversational realism,
diagnostic accuracy, and explainability. Our datasets and implementations are
fully open-sourced.

</details>


### [46] [FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets](https://arxiv.org/abs/2508.11401)
*Jana Gonnermann-Müller,Jennifer Haase,Konstantin Fackeldey,Sebastian Pokutta*

Main category: cs.HC

TL;DR: FACET框架是一个基于多智能体的教师辅助系统，结合认知和动机维度生成个性化教学材料，通过测试和教师反馈验证了其可行性和潜力。


<details>
  <summary>Details</summary>
Motivation: 针对学生群体的多样性，尤其是数学教育中的认知、动机和情感差异，当前AI工具多为成绩导向，缺乏对教师和教学需求的全面支持。

Method: 开发了FACET框架，包含三个智能体：模拟学习者、调整教学内容、自动评估。使用八年级数学课程进行测试，并通过自动化评估和教师反馈验证。

Result: 系统生成的材料与学习者档案高度匹配，教师反馈强调了任务的结构和适用性。

Conclusion: 多智能体LLM架构在异构课堂中有潜力，未来可扩展到更丰富的学习者档案和实际课堂试验。

Abstract: The increasing heterogeneity of student populations poses significant
challenges for teachers, particularly in mathematics education, where
cognitive, motivational, and emotional differences strongly influence learning
outcomes. While AI-driven personalization tools have emerged, most remain
performance-focused, offering limited support for teachers and neglecting
broader pedagogical needs. This paper presents the FACET framework, a
teacher-facing, large language model (LLM)-based multi-agent system designed to
generate individualized classroom materials that integrate both cognitive and
motivational dimensions of learner profiles. The framework comprises three
specialized agents: (1) learner agents that simulate diverse profiles
incorporating topic proficiency and intrinsic motivation, (2) a teacher agent
that adapts instructional content according to didactical principles, and (3)
an evaluator agent that provides automated quality assurance. We tested the
system using authentic grade 8 mathematics curriculum content and evaluated its
feasibility through a) automated agent-based assessment of output quality and
b) exploratory feedback from K-12 in-service teachers. Results from ten
internal evaluations highlighted high stability and alignment between generated
materials and learner profiles, and teacher feedback particularly highlighted
structure and suitability of tasks. The findings demonstrate the potential of
multi-agent LLM architectures to provide scalable, context-aware
personalization in heterogeneous classroom settings, and outline directions for
extending the framework to richer learner profiles and real-world classroom
trials.

</details>


### [47] [Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in Extended Reality](https://arxiv.org/abs/2508.11412)
*Jens Grubert,Yvonne Sedelmaier,Dieter Landes*

Main category: cs.HC

TL;DR: 探讨了在扩展现实(XR)环境中使用具身对话代理(ECAs)支持学生准备口语考试的潜力，提出了一种结合逼真ECAs和实时大型语言模型(LLMs)的系统概念。


<details>
  <summary>Details</summary>
Motivation: 口语考试在高等教育中普遍存在但心理压力大，学生焦虑会损害认知表现和学业成功，因此需要一种安全、可重复的练习方式。

Method: 提出一种系统概念，整合逼真具身对话代理和实时大型语言模型，创造心理安全、自适应且可重复的口语考试模拟环境。

Result: 该系统有望为学生提供安全、高效的考试准备工具，但仍需解决技术和心理接受度等挑战。

Conclusion: 扩展现实中的具身对话代理和大型语言模型结合，具备潜力改善口语考试准备过程，但需要进一步研究和实践验证。

Abstract: Oral examinations are a prevalent but psychologically demanding form of
assessment in higher education. Many students experience intense anxiety, which
can impair cognitive performance and hinder academic success. This position
paper explores the potential of embodied conversational agents (ECAs) in
extended reality (XR) environments to support students preparing for oral
exams. We propose a system concept that integrates photorealistic ECAs with
real-time capable large language models (LLMs) to enable psychologically safe,
adaptive, and repeatable rehearsal of oral examination scenarios. We also
discuss the potential benefits and challenges of such an envisioned system.

</details>


### [48] [ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality](https://arxiv.org/abs/2508.11426)
*Steffen Hauck,Diar Abdlkarim,John Dudley,Per Ola Kristensson,Eyal Ofek,Jens Grubert*

Main category: cs.HC

TL;DR: 研究探索了一种简化的可达性编码方法（ReachVox）在VR中辅助远程操作者与机器人协作的效果。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，机器人运动路径的规划和理解是主要挑战，需要适应性强的方法。

Method: 通过用户研究（n=20）比较ReachVox可视化方法与点基可达性检查的效果。

Result: 研究显示了ReachVox可视化相较于传统方法的优势。

Conclusion: ReachVox在VR人机协作中展现了潜力，能有效辅助操作者。

Abstract: Human-Robot-Collaboration can enhance workflows by leveraging the mutual
strengths of human operators and robots. Planning and understanding robot
movements remain major challenges in this domain. This problem is prevalent in
dynamic environments that might need constant robot motion path adaptation. In
this paper, we investigate whether a minimalistic encoding of the reachability
of a point near an object of interest, which we call ReachVox, can aid the
collaboration between a remote operator and a robotic arm in VR. Through a user
study (n=20), we indicate the strength of the visualization relative to a
point-based reachability check-up.

</details>


### [49] [Grand Challenge: Mediating Between Confirmatory and Exploratory Research Cultures in Health Sciences and Visual Analytics](https://arxiv.org/abs/2508.11544)
*Viktor von Wyl,Jürgen Bernard*

Main category: cs.HC

TL;DR: 健康科学与视觉分析研究合作常因研究设计方法不同而受阻，需从文化、标准、流程三方面提出七项行动以促进跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 解决健康科学与视觉分析研究因方法差异导致的合作障碍，如术语不统一、验证标准不同等。

Method: 提出七项研究需求和行动，包括指南制定、标准统一、工作流程整合等，并构建文化、标准、流程三方面的框架。

Result: 提供了一个促进跨学科合作的框架，为开发可靠、可重复且临床相关的数据驱动方法奠定基础。

Conclusion: 通过文化和流程调整、标准统一，可有效解决跨学科合作中的挑战，推动研究进展。

Abstract: Collaboration between health science and visual analytics research is often
hindered by different, sometimes incompatible approaches to research design.
Health science often follows hypothesis-driven protocols, registered in
advance, and focuses on reproducibility and risk mitigation. Visual analytics,
in contrast, relies on iterative data exploration, prioritizing insight
generation and analytic refinement through user interaction. These differences
create challenges in interdisciplinary projects, including misaligned
terminology, unrealistic expectations about data readiness, divergent
validation norms, or conflicting explainability requirements. To address these
persistent tensions, we identify seven research needs and actions: (1)
guidelines for broader community adoption, (2) agreement on quality and
validation benchmarks, (3) frameworks for aligning research tasks, (4)
integrated workflows combining confirmatory and exploratory stages, (5) tools
for harmonizing terminology across disciplines, (6) dedicated bridging roles
for transdisciplinary work, and (7) cultural adaptation and mutual recognition.
We organize these needs in a framework with three areas: culture, standards,
and processes. They can constitute a research agenda for developing reliable,
reproducible, and clinically relevant data-centric methods.

</details>


### [50] [Adaptive Cardio Load Targets for Improving Fitness and Performance](https://arxiv.org/abs/2508.11613)
*Justin Phillips,Daniel Roggen,Cathy Speed,Robert Harle*

Main category: cs.HC

TL;DR: Google 2024年推出的Cardio Load（CL）是一种基于心率储备的心血管工作量测量方法，结合活动强度与时长，提供个性化的每周目标。


<details>
  <summary>Details</summary>
Motivation: 通过用户反馈和内部研究，开发一种能够更全面评估用户日常活动对心血管系统影响的测量工具。

Method: CL基于心率储备，计算活动强度与时长，并引入自适应和个性化目标。

Result: CL将于2025年9月在Fitbit应用中推出，并展示了不同CL对每周目标的影响示例。

Conclusion: CL与Active Zone Minutes（AZMs）目的不同，前者用于性能测量，后者用于健康指南，CL还能通过日常小活动累积。

Abstract: Cardio Load, introduced by Google in 2024, is a measure of cardiovascular
work (also known as training load) resulting from all the user's activities
across the day. It is based on heart rate reserve and captures both activity
intensity and duration. Thanks to feedback from users and internal research, we
introduce adaptive and personalized targets which will be set weekly. This
feature will be available in the Public Preview of the Fitbit app after
September 2025. This white paper provides a comprehensive overview of Cardio
Load (CL) and how weekly CL targets are established, with examples shown to
illustrate the effect of varying CL on the weekly target. We compare Cardio
Load and Active Zone Minutes (AZMs), highlighting their distinct purposes, i.e.
AZMs for health guidelines and CL for performance measurement. We highlight
that CL is accumulated both during active workouts and incidental daily
activities, so users are able top-up their CL score with small bouts of
activity across the day.

</details>


### [51] [Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand](https://arxiv.org/abs/2508.11620)
*Chi-Jung Lee,Jiaxin Li,Tianhong Catherine Yu,Ruidong Zhang,Vipin Gunda,François Guimbretière,Cheng Zhang*

Main category: cs.HC

TL;DR: Grab-n-Go是一种可穿戴设备，利用主动声学传感技术识别手持物体时的细微手部微动作，支持多种手势和物体形状的识别。


<details>
  <summary>Details</summary>
Motivation: 随着计算设备日益融入日常生活，用户需要在手部被占用时也能进行直观、随时可用的交互。

Method: 通过单个腕带捕捉手部微动作、抓取姿势和物体形状信息，结合深度学习框架识别30种微动作。

Result: 在10名参与者使用25种日常物体的测试中，平均识别准确率达92.0%。

Conclusion: Grab-n-Go展示了在不需修改现有物体的情况下实现无缝交互的潜力。

Abstract: As computing devices become increasingly integrated into daily life, there is
a growing need for intuitive, always-available interaction methods, even when
users' hands are occupied. In this paper, we introduce Grab-n-Go, the first
wearable device that leverages active acoustic sensing to recognize subtle hand
microgestures while holding various objects. Unlike prior systems that focus
solely on free-hand gestures or basic hand-object activity recognition,
Grab-n-Go simultaneously captures information about hand microgestures,
grasping poses, and object geometries using a single wristband, enabling the
recognition of fine-grained hand movements occurring within activities
involving occupied hands. A deep learning framework processes these complex
signals to identify 30 distinct microgestures, with 6 microgestures for each of
the 5 grasping poses. In a user study with 10 participants and 25 everyday
objects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A
follow-up study further validated Grab-n-Go's robustness against 10 more
challenging, deformable objects. These results underscore the potential of
Grab-n-Go to provide seamless, unobtrusive interactions without requiring
modifications to existing objects. The complete dataset, comprising data from
18 participants performing 30 microgestures with 35 distinct objects, is
publicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI:
https://doi.org/10.7298/7kbd-vv75.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [52] [LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation](https://arxiv.org/abs/2508.11177)
*I-Chao Shen,Ariel Shamir,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出了一种基于优化的LayoutRectifier方法，通过两阶段优化修复自动生成的设计布局中的错误，如未对齐和重叠，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习生成的布局中常见的对齐问题、重叠和不满足包含需求等缺陷，提出改进方案。

Method: 采用两阶段优化：一是通过离散搜索利用网格系统解决对齐问题；二是提出新的盒子包含函数调整元素位置和大小。

Result: 在内容无关和内容感知布局生成任务中，生成更高质量且适合下游设计任务的布局。

Conclusion: LayoutRectifier有效弥补了学习生成布局的不足，提升了设计的实用性。

Abstract: Recent deep learning methods can generate diverse graphic design layouts
efficiently. However, these methods often create layouts with flaws, such as
misalignment, unwanted overlaps, and unsatisfied containment. To tackle this
issue, we propose an optimization-based method called LayoutRectifier, which
gracefully rectifies auto-generated graphic design layouts to reduce these
flaws while minimizing deviation from the generated layout. The core of our
method is a two-stage optimization. First, we utilize grid systems, which
professional designers commonly use to organize elements, to mitigate
misalignments through discrete search. Second, we introduce a novel box
containment function designed to adjust the positions and sizes of the layout
elements, preventing unwanted overlapping and promoting desired containment. We
evaluate our method on content-agnostic and content-aware layout generation
tasks and achieve better-quality layouts that are more suitable for downstream
graphic design tasks. Our method complements learning-based layout generation
methods and does not require additional training.

</details>


### [53] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM是一个新颖框架，基于用户定义的文本描述构建风格化3D可变形模型（3DMM），通过扩散模型生成目标风格图像进行微调，并在风格化过程中保留身份和表情等关键属性。


<details>
  <summary>Details</summary>
Motivation: 为了能够根据文本描述生成风格化的3D人脸模型，同时确保身份和表情的一致性。

Method: 结合预训练的网格变形网络和纹理生成器，利用扩散模型生成的风格化图像作为目标进行微调，并在图像风格化过程中显式保留原始属性。

Result: 在定量和定性评估中表现出优于现有方法的身份多样性和风格化能力。

Conclusion: StyleMM通过保留关键属性和一致的顶点连接，实现了高质量的风格化3D人脸生成。

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [54] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: SPG是一种新颖的采样策略，通过构建风格噪声向量和利用其方向偏差引导扩散过程，实现风格特定的图像生成。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的文本到图像扩散模型在语义对齐方面表现优异，但控制生成图像的视觉风格仍然具有挑战性。

Method: 提出Style-Prompting Guidance (SPG)，结合Classifier-Free Guidance (CFG)，利用风格噪声向量的方向偏差引导扩散过程。

Result: SPG在保持语义忠实度和风格一致性的同时，表现出强大的鲁棒性和适用性。

Conclusion: SPG是一种简单、有效且广泛适用的方法，优于现有技术。

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [55] [RAG for Geoscience: What We Expect, Gaps and Opportunities](https://arxiv.org/abs/2508.11246)
*Runlong Yu,Shiyuan Luo,Rahul Ghosh,Lingyao Li,Yiqun Xie,Xiaowei Jia*

Main category: cs.ET

TL;DR: Geo-RAG提出了一种新的基于多模态检索和验证的模块化循环工作流，以增强地球科学中的语言模型应用。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的RAG系统在地球科学中的适用性有限，因为许多任务需要多模态数据和科学验证。

Method: Geo-RAG采用检索→推理→生成→验证的模块化循环，支持多模态数据检索、科学约束下的推理、科学级生成和验证。

Result: Geo-RAG为地球科学工作流提供了更可信和透明的解决方案。

Conclusion: Geo-RAG通过多模态和验证机制扩展了RAG的能力，为地球科学任务提供了新的可能性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by combining
retrieval with generation. However, its current workflow remains largely
text-centric, limiting its applicability in geoscience. Many geoscientific
tasks are inherently evidence-hungry. Typical examples involve imputing missing
observations using analog scenes, retrieving equations and parameters to
calibrate models, geolocating field photos based on visual cues, or surfacing
historical case studies to support policy analyses. A simple
``retrieve-then-generate'' pipeline is insufficient for these needs. We
envision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular
retrieve $\rightarrow$ reason $\rightarrow$ generate $\rightarrow$ verify loop.
Geo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth
data; (ii) reasoning under physical and domain constraints; (iii) generation of
science-grade artifacts; and (iv) verification of generated hypotheses against
numerical models, ground measurements, and expert assessments. This shift opens
new opportunities for more trustworthy and transparent geoscience workflows.

</details>


### [56] [Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance](https://arxiv.org/abs/2508.11395)
*Kevin McNamara,Rhea Pritham Marpu*

Main category: cs.ET

TL;DR: 稳定币是银行业自放弃金本位以来最重要的创新，通过结合加密货币与传统金融基础设施，推动‘银行2.0’，并可能成为金融领域的下一个重大颠覆者。


<details>
  <summary>Details</summary>
Motivation: 现代法定货币依赖机构信任而非实物支持，导致脆弱性，稳定币通过增强稳定性、降低欺诈风险和实现跨国统一交易来解决这些问题。

Method: 分析稳定币的加速机构采用案例，如美国‘2025年GENIUS法案’、摩根大通的加密货币贷款倡议及PayPal的‘用加密货币支付’服务。

Result: 稳定币的实施能够解决宏观经济失衡问题（如通胀与生产力差距），并通过多样化的支持机制提升金融系统稳定性。

Conclusion: 稳定币有望重塑银行业，推动更高效、互联的国际金融体系，具备变革潜力。

Abstract: The global financial system stands at an inflection point. Stablecoins
represent the most significant evolution in banking since the abandonment of
the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating
cryptocurrency innovation with traditional finance infrastructure. This
transformation rivals artificial intelligence as the next major disruptor in
the financial sector. Modern fiat currencies derive value entirely from
institutional trust rather than physical backing, creating vulnerabilities that
stablecoins address through enhanced stability, reduced fraud risk, and unified
global transactions that transcend national boundaries. Recent developments
demonstrate accelerating institutional adoption: landmark U.S. legislation
including the GENIUS Act of 2025, strategic industry pivots from major players
like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay
with Crypto" service. Widespread stablecoin implementation addresses critical
macroeconomic imbalances, particularly the inflation-productivity gap plaguing
modern monetary systems, through more robust and diversified backing
mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency
gains, paving the way for a more interconnected international financial system.
This whitepaper comprehensively explores how stablecoins are poised to reshape
banking, supported by real-world examples, current market data, and analysis of
their transformative potential.

</details>


### [57] [Open Questions about Time and Self-reference in Living Systems](https://arxiv.org/abs/2508.11423)
*Samson Abramsky,Wolfgang Banzhaf,Leo S. D. Caves,Michael Levin,Penousal Machado,Charles Ofria,Susan Stepney,Roger White*

Main category: cs.ET

TL;DR: 本文探讨了生命系统的自我参照和自我修改特性如何挑战传统科学方法，并提出需新理论框架。区分了‘自然时间’和‘表征时间’，并举例说明生命系统如何处理自我参照的矛盾。


<details>
  <summary>Details</summary>
Motivation: 研究生命系统的主动性和自我参照特性如何为传统科学方法带来挑战，并探索新理论框架的必要性。

Method: 通过进化、胚胎发育和变态的例子，分析生命系统如何处理自我参照矛盾，并提出新的建模方向。

Result: 指出传统形式主义难以建模自我参照系统，需采用域理论、共代数、遗传编程等新方法。

Conclusion: 自我参照和自我修改是生命系统的核心特征，需新模型以理解生命的开放创造力，对多学科有深远影响。

Abstract: Living systems exhibit a range of fundamental characteristics: they are
active, self-referential, self-modifying systems. This paper explores how these
characteristics create challenges for conventional scientific approaches and
why they require new theoretical and formal frameworks. We introduce a
distinction between 'natural time', the continuing present of physical
processes, and 'representational time', with its framework of past, present and
future that emerges with life itself. Representational time enables memory,
learning and prediction, functions of living systems essential for their
survival. Through examples from evolution, embryogenesis and metamorphosis we
show how living systems navigate the apparent contradictions arising from
self-reference as natural time unwinds self-referential loops into
developmental spirals. Conventional mathematical and computational formalisms
struggle to model self-referential and self-modifying systems without running
into paradox. We identify promising new directions for modelling
self-referential systems, including domain theory, co-algebra, genetic
programming, and self-modifying algorithms. There are broad implications for
biology, cognitive science and social sciences, because self-reference and
self-modification are not problems to be avoided but core features of living
systems that must be modelled to understand life's open-ended creativity.

</details>


### [58] [CoMoNM: A Cost Modeling Framework for Compute-Near-Memory Systems](https://arxiv.org/abs/2508.11451)
*Hamid Farzaneh,Asif Ali Khan,Jeronimo Castrillon*

Main category: cs.ET

TL;DR: CoMoNM是一个用于计算近内存（CNM）系统的通用成本建模框架，能够快速估算执行时间，简化优化决策。


<details>
  <summary>Details</summary>
Motivation: 解决CNM系统优化困难的问题，特别是自动化和高效评估执行时间的需求。

Method: 提出CoMoNM框架，通过高级硬件无关应用表示、目标系统规格和映射规范，快速估算执行时间。

Result: CoMoNM的估算误差在7.80%和2.99%以内，速度比UPMEM和HBM-PIM模拟器快七个数量级。

Conclusion: CoMoNM为CNM系统提供高效、准确的执行时间估算，显著提升编译器决策能力。

Abstract: Compute-Near-Memory (CNM) systems offer a promising approach to mitigate the
von Neumann bottleneck by bringing computational units closer to data. However,
optimizing for these architectures remains challenging due to their unique
hardware and programming models. Existing CNM compilers often rely on manual
programmer annotations for offloading and optimizations. Automating these
decisions by exploring the optimization space, common in CPU/GPU systems, is
difficult for CNMs as constructing and navigating the transformation space is
tedious and time consuming. This is particularly the case during system-level
design, where evaluation requires time-consuming simulations. To address this,
we present CoMoNM, a generic cost modeling framework for CNM systems for
execution time estimation in milliseconds. It takes a high-level,
hardware-agnostic application representation, target system specifications, and
a mapping specification as input and estimates the execution time for the given
application on the target CNM system. We show how CoMoNM can be seamlessly
integrated into state-of-the-art CNM compilers, providing improved offloading
decisions. Evaluation on established benchmarks for CNM shows estimation errors
within 7.80% and 2.99%, when compared to the real UPMEM CNM system and
Samsung's HBM-PIM simulator. Notably, CoMoNM delivers estimates seven orders of
magnitude faster compared to the UPMEM and HBM-PIM simulators.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [59] [EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training](https://arxiv.org/abs/2508.11035)
*Hasibul Jamil,MD S Q Zulkar Nine,Tevfik Kosar*

Main category: cs.DC

TL;DR: EMLIO是一个高效的机器学习I/O服务，通过优化数据加载延迟和能耗，适用于大规模深度学习任务。


<details>
  <summary>Details</summary>
Motivation: 随着数据集超出本地存储容量和GPU计算超越网络及磁盘延迟，大规模深度学习任务面临I/O瓶颈。现有系统忽略了I/O的能耗问题。

Method: EMLIO在存储节点部署轻量级数据服务守护进程，对原始样本进行序列化和批处理，并通过TCP流传输，同时与GPU加速预处理无缝集成。

Result: 在多种网络环境下，EMLIO的I/O速度提升8.6倍，能耗降低10.9倍，且性能与能耗不受网络距离影响。

Conclusion: EMLIO为下一代AI云提供了一个可扩展的能源感知I/O架构蓝图。

Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks
as datasets grow beyond local storage capacities and GPU compute outpaces
network and disk latencies. While recent systems optimize data-loading time,
they overlook the energy cost of I/O - a critical factor at large scale. We
introduce EMLIO, an Efficient Machine Learning I/O service that jointly
minimizes end-to-end data-loading latency T and I/O energy consumption E across
variable-latency networked storage. EMLIO deploys a lightweight data-serving
daemon on storage nodes that serializes and batches raw samples, streams them
over TCP with out-of-order prefetching, and integrates seamlessly with
GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive
evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)
environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use
compared to state-of-the-art loaders, while maintaining constant performance
and energy profiles irrespective of network distance. EMLIO's service-based
architecture offers a scalable blueprint for energy-aware I/O in
next-generation AI clouds.

</details>


### [60] [Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method](https://arxiv.org/abs/2508.11467)
*Shifang Liu,Huiyuan Li,Hongjiao Sheng,Haoyuan Gui,Xiaoyu Zhang*

Main category: cs.DC

TL;DR: 论文提出了一种以GPU为中心的SVD算法，通过新的GPU双向分治方法和优化数据布局，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法在异构系统中存在面板分解慢和CPU-GPU数据传输频繁的问题，限制了GPU计算能力的发挥。

Method: 提出了一种基于GPU的双向分治方法，重新设计了算法和数据布局，完全在GPU上进行面板级计算和尾随矩阵更新，并优化了BLAS利用率。

Result: 在AMD MI210和NVIDIA V100 GPU上，与rocSOLVER/cuSOLVER和MAGMA相比，速度分别提升了1293.64倍/7.47倍和14.10倍/12.38倍。

Conclusion: 该算法显著减少了CPU-GPU数据传输，提升了计算效率，展现了GPU在高性能线性代数计算中的潜力。

Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization
technique in linear algebra, widely applied in numerous matrix-related
problems. However, traditional SVD approaches are hindered by slow panel
factorization and frequent CPU-GPU data transfers in heterogeneous systems,
despite advancements in GPU computational capabilities. In this paper, we
introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based
bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and
data layout of different steps for SVD computation, performing all panel-level
computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU
data transfers. Furthermore, we integrate related computations to optimize BLAS
utilization, thereby increasing arithmetic intensity and fully leveraging the
computational capabilities of GPUs. Additionally, we introduce a newly
developed GPU-based BDC algorithm that restructures the workflow to eliminate
matrix-level CPU-GPU data transfers and enable asynchronous execution between
the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs
demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x
and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.

</details>


### [61] [Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets](https://arxiv.org/abs/2508.11266)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.DC

TL;DR: 提出了一种新的双层代币化架构，通过 Element Tokens 和 Everything Tokens 提升复杂资产的流动性和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统框架下难以交易或分割大型异质资产（如矿山、发电厂），需要一种新的方法来实现部分所有权和整体所有权的灵活转换。

Method: 设计双层代币系统（Element Tokens 和 Everything Tokens），引入双向可转换机制和套利机制以保持代币价格与资产净值一致。

Result: 通过能源和工业领域的案例，证明该系统能够将高价值、低流动性资产分割成类似股票或ETF的可交易单位。

Conclusion: 该方法降低了投资门槛，提升了价格发现能力，并为资产所有者和投资者提供了更灵活的融资和交易选择。

Abstract: Alternative assets such as mines, power plants, or infrastructure projects
are often large, heterogeneous bundles of resources, rights, and outputs whose
value is difficult to trade or fractionalize under traditional frameworks. This
paper proposes a novel two-tier tokenization architecture to enhance the
liquidity and transparency of such complex assets. We introduce the concepts of
Element Tokens and Everything Tokens: elemental tokens represent standardized,
fully collateralized components of an asset (e.g., outputs, rights, or
credits), while an everything token represents the entire asset as a fixed
combination of those elements. The architecture enables both fine-grained
partial ownership and integrated whole-asset ownership through a system of
two-way convertibility. We detail the design and mechanics of this system,
including an arbitrage mechanism that keeps the price of the composite token
aligned with the net asset value of its constituents. Through illustrative
examples in the energy and industrial sectors, we demonstrate that our approach
allows previously illiquid, high-value projects to be fractionalized and traded
akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for
investors and asset owners, such as lower entry barriers, improved price
discovery, and flexible financing, as well as the considerations for
implementation and regulation.

</details>


### [62] [Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive](https://arxiv.org/abs/2508.11298)
*Gabin Schieffer,Jacob Wahlgren,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: 论文研究了AMD MI300A APU在多APU系统中的通信效率，设计了基准测试并比较了不同编程接口的性能，提出了优化方法并验证了实际HPC应用的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着GPU计算性能的提升，HPC应用中高效数据传输的需求增加。AMD MI300A APU通过集成CPU、GPU和HBM来减少CPU-GPU数据传输，但多APU系统中的通信效率仍需研究。

Method: 设计了基准测试评估GPU直接内存访问、APU间数据传输和多APU集体通信，比较了HIP API、MPI和RCCL库的性能。

Result: 提出了优化多APU系统通信的关键设计选择，并通过优化Quicksilver和CloverLeaf应用验证了性能提升。

Conclusion: AMD MI300A APU在多APU系统中表现出高效通信潜力，合理的编程接口和优化策略可进一步提升HPC应用性能。

Abstract: The ever-increasing compute performance of GPU accelerators drives up the
need for efficient data movements within HPC applications to sustain
performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD
MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth
memory (HBM) within a single physical package. Leadership supercomputers, such
as El Capitan, group four APUs within a single compute node, using Infinity
Fabric Interconnect. In this work, we design specific benchmarks to evaluate
direct memory access from the GPU, explicit inter-APU data movement, and
collective multi-APU communication. We also compare the efficiency of HIP APIs,
MPI routines, and the GPU-specialized RCCL library. Our results highlight key
design choices for optimizing inter-APU communication on multi-APU AMD MI300A
systems with Infinity Fabric, including programming interfaces, allocators, and
data movement. Finally, we optimize two real HPC applications, Quicksilver and
CloverLeaf, and evaluate them on a four MI100A APU system.

</details>


### [63] [Space-efficient population protocols for exact majority in general graphs](https://arxiv.org/abs/2508.11384)
*Joel Rybicki,Jakob Solnerzik,Olivier Stietel,Robin Vacus*

Main category: cs.DC

TL;DR: 本文研究了群体协议模型中的精确多数共识问题，改进了在一般图中的上下界，提出了基于松弛时间和度不平衡的新协议，并在正则扩展图中匹配了最优空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究精确多数共识问题在群体协议模型中的效率和空间复杂度，旨在为一般图设计更优的协议。

Method: 通过分析图的随机游走松弛时间和度不平衡（Δ/δ），设计了一个协议，改进了时间复杂度和空间复杂度的上下界。

Result: 提出的协议在期望和高概率下稳定时间与空间复杂度上取得了改进，特别是在正则扩展图中实现了最优结果。

Conclusion: 研究提供了在一般图和正则扩展图中精确多数共识协议的新上下界，展示了协议设计的进步。

Abstract: We study exact majority consensus in the population protocol model. In this
model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in
each time step, a scheduler samples uniformly at random a pair of adjacent
nodes to interact. In the exact majority consensus task, each node is given a
binary input, and the goal is to design a protocol that almost surely reaches a
stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general
graphs. First, we give asymptotically tight time lower bounds for general
(unbounded space) protocols. Second, we obtain new upper bounds parameterized
by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced
by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically,
we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta}
\tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high
probability and uses $O\left( \log n \cdot \left(
\log\left(\tfrac{\Delta}{\delta}\right) + \log
\left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any
graph with minimum degree at least $\delta$ and maximum degree at most
$\Delta$.
  For regular expander graphs, this matches the optimal space complexity of
$\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA
2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of
$O(n \log^2 n)$ steps. Finally, we give a new upper bound of
$O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a
constant-state protocol.

</details>


### [64] [Time, Fences and the Ordering of Events in TSO](https://arxiv.org/abs/2508.11415)
*Raïssa Nataf,Yoram Moses*

Main category: cs.DC

TL;DR: 提出了一种语义框架，在TSO内存模型下精确判断何时需要同步操作，通过引入新的occurs-before关系，揭示了同步操作的必要性，并推广了线性化实现的下界。


<details>
  <summary>Details</summary>
Motivation: TSO内存模型虽提升了多处理器效率，但增加了正确性推理的复杂性，需插入高性能开销的同步操作。研究旨在确定这些操作何时必要。

Method: 引入TSO特有的occurs-before关系，扩展自Lamport的happens-before关系，通过定理证明事件排序的唯一方式。

Result: 揭示了同步操作的不可避免情况，推广了线性化实现的下界，为TSO模型提供理论基础。

Conclusion: 框架扩展了异步系统的通信推理至TSO模型，有助于理解信息流和因果性结构。

Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory
model in multiprocessor architectures, widely implemented, for example in
Intel's x86 and x64 platforms. It allows processes to delay the visibility of
writes through store buffering. While this supports hardware-level
optimizations and makes a significant contribution to multiprocessor
efficiency, it complicates reasoning about correctness, as executions may
violate sequential consistency. Ensuring correct behavior often requires
inserting synchronization primitives such as memory fences ($F$) or atomic
read-modify-write ($RMW$) operations, but this approach can incur significant
performance costs. In this work, we develop a semantic framework that precisely
characterizes when such synchronization is necessary under TSO. We introduce a
novel TSO-specific occurs-before relation, which adapts Lamport's celebrated
happens-before relation from asynchronous message-passing systems to the TSO
setting. Our main result is a theorem that proves that the only way to ensure
that two events that take place at different sites are temporally ordered is by
having the execution create an occurs-before chain between the events. By
studying the role of fences and $RMW$s in creating occurs-before chains, we are
then able to capture cases in which these costly synchronization operations are
unavoidable. Since proper real-time ordering of events is a fundamental aspect
of consistency conditions such as Linearizability, our analysis provides a
sound theoretical understanding of essential aspects of the TSO model. In
particular, we are able to generalize prior lower bounds for linearizable
implementations of shared memory objects. Our results capture the structure of
information flow and causality in the TSO model by extending the standard
communication-based reasoning from asynchronous systems to the TSO memory
model.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [65] [Tabularis Formatus: Predictive Formatting for Tables](https://arxiv.org/abs/2508.11121)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Gust Verbruggen*

Main category: cs.DB

TL;DR: TaFo是一种神经符号方法，用于自动生成条件格式（CF）建议，解决了用户技术门槛高和界面不友好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的条件格式规则创建复杂，依赖用户输入，TaFo旨在解决这些问题。

Method: TaFo结合了语言模型的语义知识和多样性规则排序，自动学习触发器和格式化属性。

Result: TaFo在188万公共工作簿上测试，准确性和多样性优于现有系统15.6%--26.5%。

Conclusion: TaFo通过完全自动化格式建议，显著提升了条件格式生成的效率和效果。

Abstract: Spreadsheet manipulation software are widely used for data management and
analysis of tabular data, yet the creation of conditional formatting (CF) rules
remains a complex task requiring technical knowledge and experience with
specific platforms. In this paper we present TaFo, a neuro-symbolic approach to
generating CF suggestions for tables, addressing common challenges such as user
unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo
takes inspiration from component based synthesis systems and extends them with
semantic knowledge of language models and a diversity preserving rule
ranking.Unlike previous methods focused on structural formatting, TaFo uniquely
incorporates value-based formatting, automatically learning both the rule
trigger and the associated visual formatting properties for CF rules. By
removing the dependency on user specification used by existing techniques in
the form of formatted examples or natural language instruction, TaFo makes
formatting completely predictive and automated for the user. To evaluate TaFo,
we use a corpus of 1.8 Million public workbooks with CF and manual formatting.
We compare TaFo against a diverse set of symbolic and neural systems designed
for or adapted for the task of table formatting. Our results show that TaFo
generates more accurate, diverse and complete formatting suggestions than
current systems and outperforms these by 15.6\%--26.5\% on matching user added
ground truth rules in tables.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs](https://arxiv.org/abs/2508.11477)
*Hyunsun Chung,Junhyeok Park,Taewan Noh,Seonghoon Ahn,Kihwan Kim,Ming Zhao,Youngjae Kim*

Main category: cs.AR

TL;DR: OpenCXD是一个结合模拟与硬件的评估框架，用于更准确地评估CXL-SSD的性能。


<details>
  <summary>Details</summary>
Motivation: CXL-SSD作为DRAM和传统存储之间的新层次，性能评估因缺乏原生硬件支持而困难。现有模拟方法无法精确建模固件级交互和存储动态。

Method: OpenCXD通过将主机端的CXL.mem模拟器与运行真实固件的物理OpenSSD平台集成，实现模拟内存请求触发的固件执行。

Result: OpenCXD能反映设备级现象，提供对CXL-SSD固件设计的宝贵洞察。

Conclusion: OpenCXD填补了模拟与硬件之间的鸿沟，为CXL-SSD的设计与优化提供了更准确的评估工具。

Abstract: The advent of Compute Express Link (CXL) enables SSDs to participate in the
memory hierarchy as large-capacity, byte-addressable memory devices. These
CXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and
traditional storage, combining NAND flash density with memory-like access
semantics. However, evaluating the performance of CXL-SSDs remains difficult
due to the lack of hardware that natively supports the CXL.mem protocol on
SSDs. As a result, most prior work relies on hybrid simulators combining CPU
models augmented with CXL.mem semantics and SSD simulators that approximate
internal flash behaviors. While effective for early-stage exploration, this
approach cannot faithfully model firmware-level interactions and low-level
storage dynamics critical to CXL-SSD performance. In this paper, we present
OpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap
between simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem
simulator on the host side with a physical OpenSSD platform running real
firmware. This enables in-situ firmware execution triggered by simulated memory
requests. Through these contributions, OpenCXD reflects device-level phenomena
unobservable in simulation-only setups, providing critical insights for future
firmware design tailored to CXL-SSDs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: 提出CRAFT-GUI框架，结合课程学习和GRPO，针对GUI任务的难度差异设计奖励函数，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法忽视GUI任务难度差异和奖励信号的粗糙性，导致学习效率低。

Method: 采用课程学习框架CRAFT-GUI及GRPO，设计结合规则和模型评估的奖励函数。

Result: 在Android Control和内部基准上分别提升5.6%和10.3%。

Conclusion: 验证了RL结合课程学习在GUI任务中的有效性。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [68] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: 论文提出Inclusion Arena平台，通过实时用户反馈动态评估大模型性能，弥补静态数据集的不足，采用创新技术提升排名可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型评估多依赖静态数据集或通用提示，未能反映实际应用表现。Inclusion Arena通过实时用户反馈填补这一空白。

Method: 平台整合用户交互中的模型对比，采用Bradley-Terry模型及两项创新（Placement Matches和Proximity Sampling）优化排名。

Result: 实验表明平台提供稳定排名，数据传递性更强，有效抑制恶意操纵风险。

Conclusion: Inclusion Arena通过连接模型与实际应用，加速优化实用性大模型的开发，平台已公开。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 本文提出了一种压缩元学习框架，通过神经网络改进压缩学习的编码和解码阶段，提高了速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的快速扩大，需要高效且快速的参数学习技术。传统压缩学习方法未利用数据的潜在结构。

Method: 使用神经网络元学习压缩学习的编码和解码阶段，应用于多种任务如压缩PCA、压缩岭回归等。

Result: 提出的框架在速度和准确性上优于现有方法。

Conclusion: 压缩元学习框架为大规模数据处理提供了高效且隐私保护的解决方案。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder利用大型语言模型（LLMs）设计了一种层次化算法到HDL的编码代理，旨在高效、可靠地将算法转换为硬件代码。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统对超低延迟和功耗的严格要求增加了对高效算法到硬件部署的需求，但算法设计与硬件实现之间仍存在显著差距。

Method: A2HCoder采用分层框架，将复杂算法分解为模块化功能块（水平维度），并逐步进行细粒度翻译（垂直维度），利用外部工具链确保正确性。

Result: 通过5G无线通信领域的实际部署案例验证，A2HCoder展示了其实用性、可靠性和部署效率。

Conclusion: A2HCoder为算法到硬件的翻译提供了一种敏捷且可靠的方法，显著减少了LLM生成代码中的幻觉问题。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [71] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个包含1,315个自然且复杂问题的基准测试，旨在评估大型语言模型（LLMs）在解决需多步推理的信息检索问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试缺乏反映人类信息检索中真实耗时问题的自然性问题，MoNaCo填补了这一空白。

Method: 通过分解注释流程，规模化收集并手动回答自然的耗时问题，构建了MoNaCo基准。

Result: 前沿LLMs在MoNaCo上最高仅达61.2% F1，受限于召回率低和幻觉问题。

Conclusion: MoNaCo为跟踪模型在复杂信息检索问题上的进展提供了有效资源，并突显了改进推理模型的必要性。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [72] [Insect-Wing Structured Microfluidic System for Reservoir Computing](https://arxiv.org/abs/2508.10915)
*Jacob Clouse,Thomas Ramsey,Samitha Somathilaka,Nicholas Kleinsasser,Sangjin Ryu,Sasitharan Balasubramaniam*

Main category: cs.NE

TL;DR: 研究探索了一种基于蜻蜓翅膀启发的微流控芯片的混合储备计算系统，通过流体相互作用编码时间输入模式，展示了其在低功耗、高适应性计算中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着对更高效、自适应计算需求的增长，受自然启发的架构提供了替代传统电子设计的可能性。微流控平台结合生物形态和流体动力学，在电子设备不适用的环境中提供了低功耗、高弹性计算的基础。

Method: 研究采用了基于蜻蜓翅膀启发的微流控芯片，通过三个染料入口通道和摄像头监控的检测区域，将离散空间模式转换为动态颜色输出信号，再通过可训练的读出层进行分类。

Result: 系统分类准确率高达91%，即使在低分辨率和有限训练数据下仍表现稳定，验证了微流控储备计算的可行性。

Conclusion: 该研究展示了微流控储备计算在高效、适应性计算中的潜力，尤其是在电子设备不适用的环境中。

Abstract: As the demand for more efficient and adaptive computing grows,
nature-inspired architectures offer promising alternatives to conventional
electronic designs. Microfluidic platforms, drawing on biological forms and
fluid dynamics, present a compelling foundation for low-power, high-resilience
computing in environments where electronics are unsuitable. This study explores
a hybrid reservoir computing system based on a dragonfly-wing inspired
microfluidic chip, which encodes temporal input patterns as fluid interactions
within the micro channel network.
  The system operates with three dye-based inlet channels and three
camera-monitored detection areas, transforming discrete spatial patterns into
dynamic color output signals. These reservoir output signals are then modified
and passed to a simple and trainable readout layer for pattern classification.
Using a combination of raw reservoir outputs and synthetically generated
outputs, we evaluated system performance, system clarity, and data efficiency.
The results demonstrate consistent classification accuracies up to $91\%$, even
with coarse resolution and limited training data, highlighting the viability of
the microfluidic reservoir computing.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [73] [The Constraint Satisfaction Problem Over Multisorted Cores](https://arxiv.org/abs/2508.11540)
*Dejan Delic,John Marcoux*

Main category: cs.CC

TL;DR: 论文讨论了约束满足问题（CSPs）在特殊关系核心（multisorted core）下的复杂性问题，结果显示这类问题可以归约为计算整数矩阵的行列式，属于DET类，可能严格包含于P类中。


<details>
  <summary>Details</summary>
Motivation: Schaeffer、Bulatov和Zhuk的研究已证明有限域上的CSPs要么在P类中，要么是NP完全问题。本文旨在探讨当实例为一种特殊关系核心（multisorted core）时，CSPs的复杂性是否会发生改变。

Method: 作者扩展了之前的研究，分析了multisorted core下的CSPs。具体而言，通过关联关系核心的多态代数，将其复杂性问题转化为计算整数矩阵的行列式问题。

Result: 研究发现，multisorted core下的CSPs可归约为计算整数矩阵的行列式，因此其复杂性属于DET类，而DET可能是P类的严格子集。

Conclusion: 该研究扩展了CSPs复杂性的分类框架，揭示了multisorted core下问题的复杂性界限，为进一步研究提供了新的方向。

Abstract: Constraint Satisfaction Problems (CSPs, for short) make up a class of
problems with applications in many areas of computer science. The first
classification of these problems was given by Schaeffer who showed that every
CSP over the domain {0,1} is either in P or is NP-complete. More recently this
was shown to hold for all CSPs over finite relational structures independently
by Bulatov and Zhuk. Furthermore, they characterized the complexity based
solely on the polymorphism algebra of the associated relational structure,
building upon the deep connections between universal algebra and complexity
theory. In this article we extend this and consider what happens if the
instance forms a special type of relational core called a multisorted core. Our
main result is that in this case the problem is reducible to computing the
determinant of an integer valued matrix which places it in the complexity class
DET, which is likely a strict subset of P.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [74] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: 该论文研究了如何检测伪装成自由形态的Artcodes标记，提出了一种新的特征描述符（形状方向直方图）来描述其拓扑结构，并通过实验验证了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机和VR/AR技术的普及，环境中将出现更多与虚拟元素连接的物体，识别这些物体（如Artcodes）是触发后续数字内容的第一步。

Method: 提出了一种名为形状方向直方图的特征描述符，用于描述Artcode的通用拓扑结构，并构建了Artcode检测系统。

Result: 实验结果表明，该特征描述符能有效表示拓扑结构，系统在检测Artcode提案方面表现良好。

Conclusion: 尽管这是基于特征的拓扑对象检测的初步尝试，但为未来交互和应用开辟了新方向。

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [75] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 本文综述了通过外部工具增强多模态大语言模型（MLLMs）的方法，探讨了其在数据获取、任务性能、评估协议及未来方向的潜力。


<details>
  <summary>Details</summary>
Motivation: MLLMs在复杂任务和高质量数据获取方面仍存在局限性，外部工具（如API、专家模型）的引入可提升其性能与可靠性。

Method: 通过四个维度分析外部工具的作用：高质量数据获取与标注、提升下游任务性能、优化评估协议、以及当前局限与未来方向。

Result: 外部工具有潜力显著提升MLLMs的能力，尤其在数据质量和任务表现方面。

Conclusion: 外部工具是MLLMs进一步发展的关键，未来需探索其更广泛的应用与发展方向。

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [76] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: ViPE是一种高效视频处理引擎，用于从未标注视频中估计相机参数和深度图，性能优越且适配多种场景和相机模型。


<details>
  <summary>Details</summary>
Motivation: 解决从野外视频中获取一致且精确的3D标注的挑战。

Method: 开发ViPE引擎，估计相机内参、运动和密集深度图，支持多种场景和相机模型。

Result: 在TUM/KITTI序列上超越基线方法18%/50%，并标注了大规模视频数据集。

Conclusion: 开源ViPE和标注数据集，推动空间AI系统发展。

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [77] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: MV-ScanQA是一个新的3D问答数据集，强调多视角推理，并提出TripAlign预训练数据集和LEGO方法，显著提升了3D视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉语言数据集在远距离多视角推理和多对象上下文对齐方面的不足，限制了模型的深入场景理解能力。

Method: 提出了MV-ScanQA数据集和TripAlign预训练数据集，并开发了LEGO方法，利用预训练的2D LVLMs知识转移到3D领域。

Result: LEGO在MV-ScanQA及现有3D密集标注和问答基准测试中均取得了最优性能。

Conclusion: MV-ScanQA和TripAlign为3D视觉语言学习提供了更好的多视角推理和多对象对齐支持，推动了该领域的进展。

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [78] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: LogicBench 是一个用于诊断视觉语言模型逻辑盲点的基准，LogicCLIP 是其改进框架，显著提升了模型在逻辑理解方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在逻辑理解能力上存在重大盲点，限制了其在实际应用中的可靠性。

Method: 提出 LogicBench 基准测试和 LogicCLIP 训练框架，通过逻辑感知数据生成和多目标对比学习提升逻辑理解能力。

Result: LogicCLIP 在所有基准测试中显著优于基线模型，同时保持了通用任务的竞争力。

Conclusion: LogicBench 和 LogicCLIP 是提升视觉语言模型逻辑能力的重要资源。

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [79] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: 提出了一种低成本、可重复的框架，利用车载摄像头实时评估和定位路边植被及基础设施，结合深度估计、误差校正和几何三角测量，实现了高精度的空间和结构数据分析。


<details>
  <summary>Details</summary>
Motivation: 为城市规划和公用事业公司提供一种快速、实时、低成本的对象级监测方案，补充传统遥感方法（如LiDAR）的不足。

Method: 开发了端到端流程，结合单目深度估计、深度误差校正和几何三角测量，利用车载摄像头视频数据进行实时分析。

Result: 深度校正模型预测性能强（R2=0.92，MAE=0.31），在低速车辆和内部摄像头条件下定位误差最小（2.83米）。

Conclusion: 该框架首次结合单目深度建模、GPS三角定位和实时结构评估，为动态城市环境提供了一种高效、经济的解决方案。

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [80] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: 提出一种基于潜在噪声自编码器的隐私增强机制，保护用户身份不被识别，同时保留数据的可用性。


<details>
  <summary>Details</summary>
Motivation: 解决注视信号中的隐私问题，防止用户身份被未经同意地重新识别。

Method: 使用潜在噪声自编码器，平衡隐私与实用性。

Result: 显著降低生物识别的可能性，同时实用性损失最小。

Conclusion: 该机制有效保护敏感注视数据，同时保持了数据的可用性。

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [81] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLMs）在模拟低视力人群视觉感知方面的能力，并通过实验发现，结合视觉信息和示例图像响应的提示能显著提高模型与人类回答的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在无障碍领域的潜力，尤其是模拟低视力人群的图像感知能力。

Method: 通过调查收集40名低视力参与者的数据，构建基准数据集，并设计不同信息的提示，测试VLMs生成回答与参与者原始答案的一致性。

Result: VLMs在仅提供少量信息时推断能力有限（一致性0.59），结合视觉信息和示例图像响应后一致性显著提高（0.70）。

Conclusion: 综合视觉信息和示例图像响应的提示对模拟低视力人群的视觉感知非常有效，单一样本即可显著提升性能。

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [82] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: UWB-PostureGuard是一种基于超宽带技术的坐姿监测系统，解决了传统方法的隐私和舒适性问题，通过高效的特征提取和模型设计实现了高精度监测。


<details>
  <summary>Details</summary>
Motivation: 长时间使用电脑时的不当坐姿已成为公共健康问题，传统监测方法存在隐私和舒适性障碍。

Method: 利用商业超宽带设备，通过特征工程提取坐姿特征，并开发PoseGBDT模型捕捉时间依赖性。

Result: 在10名参与者和19种不同坐姿的实测中达到99.11%的准确率，且对环境变量具有鲁棒性。

Conclusion: 该系统为预防性健康管理提供了可扩展、隐私保护的移动健康解决方案。

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER框架通过结合视觉-语言模型（VLM）和纯文本语言模型（LLM）增强意图推断能力，优化机器人导航与操作。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作中机器人推断用户意图的效率和透明性。

Method: 结合VLM和LLM形成语义先验，使用YOLO和Segment Anything模型检测对象，并通过VLM和LLM评分优化目标选择。

Result: 增强的GUIDER能更精准地识别上下文相关目标并适应意图变化。

Conclusion: 未来将在Isaac Sim中评估系统实时辅助性能。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [84] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: 研究探讨了AI辅助视觉裂缝检测在移动Jackal机器人平台中的有效性，结果显示人机协作（HRC）提升了检测准确性并降低了操作员负担。


<details>
  <summary>Details</summary>
Motivation: 传统核设施结构检测方法存在安全风险、认知负担高及人为误差等问题，AI和机器人技术的进步为更安全高效的检测提供了新可能。

Method: 研究采用配备先进检测算法的移动Jackal机器人平台，结合AI辅助视觉裂缝检测技术进行实验。

Result: 实验结果表明，HRC显著提高了检测准确性，同时减少了操作员的工作负担。

Conclusion: 人机协作在核设施结构检测中展现出优于传统手动方法的潜力。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>
