<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLM）在政府遗留代码现代化中的应用，重点解决了输入限制问题，发现LLM在代码分块和文档生成任务中表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分解决政府遗留代码（如ALC和MUMPS）的独特挑战，尤其是超出LLM上下文窗口长度和缺乏对遗留语言训练的问题。

Method: 通过多种代码分块方法优化遗留代码的摘要模块注释生成，评估了不同LLM（如GPT-4o、Claude 3 Sonnet等）的文档生成质量。

Result: LLM的分块选择与人类专家分块高度一致，且LLM生成的分块在文档生成任务中比人类分块更准确和实用（准确度高20%，实用性高10%）。

Conclusion: LLM可作为代码现代化的合适工具，替代人类分块任务。

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [2] [When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration](https://arxiv.org/abs/2506.20063)
*Zixuan Feng,Thomas Zimmermann,Lorenzo Pisani,Christopher Gooley,Jeremiah Wander,Anita Sarma*

Main category: cs.SE

TL;DR: 该研究探讨了跨学科软件开发中领域专家（DEs）和软件开发人员（SDEs）的期望差异及其导致的摩擦，通过活动理论分析揭示了21种摩擦点，并为实践提供了指导。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发团队的多样化，领域专家和开发人员之间的协作摩擦日益凸显，研究旨在探讨这些摩擦的成因和表现。

Method: 采用活动理论（AT）作为分析框架，结合24次访谈和293份调查问卷进行混合方法研究。

Result: 识别了SDEs的8种期望和DEs的6种期望，并通过AT组件揭示了21种协作摩擦的具体表现。

Conclusion: 研究为理解跨学科软件开发中的摩擦提供了理论框架，并为实践和研究提供了可行建议。

Abstract: Background: Software development teams are increasingly diverse, embedded,
and cross-disciplinary. Domain experts (DEs) from different disciplines
collaborate with professional software developers (SDEs), bringing
complementary expertise in creating and maintaining complex production
software. However, contested expectations, divergent problem-solving
perspectives, and conflicting priorities lead to friction. Aims: This study
aims to investigate the dynamics of emerging collaboration of
cross-disciplinary software development (CDSD) by exploring the expectations
held by DEs and SDEs and understanding how these frictions manifest in
practice. Method: We utilize Activity Theory (AT), a well-established
socio-technical framework, as an analytical lens in a grounded, empirical
investigation, conducted through a mixed-method study involving 24 interviews
(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants
(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the
CDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By
mapping these expectations to AT components, we revealed 21 frictions in CDSD
and illustrated where and how they arise. Conclusions: This study offers a
theoretical lens for understanding the dynamics and frictions in CDSD and
provides actionable insights for future research, practitioners, and
infrastructure design.

</details>


### [3] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: XP 2025研讨会上讨论了AI与敏捷开发的整合挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将人工智能与敏捷软件开发实践相结合，解决实际中的工具、治理、数据质量和技能缺口等问题。

Method: 通过互动会议系统分析挑战，确定根本原因，并共同制定研究路线图。

Result: 提出了一个结构化议程，旨在推动产业与学术界合作，从问题发现到成功实施。

Conclusion: 研讨会的成果为AI与敏捷开发的整合提供了明确的研究方向和行动指南。

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [4] [Ten simple rules for PIs to integrate Research Software Engineering into their research group](https://arxiv.org/abs/2506.20217)
*Stuart M. Allen,Neil Chue Hong,Stephan Druskat,Toby Hodges,Daniel S. Katz,Jan Linxweiler,Frank Löffler,Lars Grunske,Heidi Seibold,Jan Philipp Thiele,Samantha Wittke*

Main category: cs.SE

TL;DR: 本文介绍了研究软件工程（RSEng）的重要性，并提供了十条实用规则，帮助研究领导者更好地将其融入研究团队，以提高软件质量和研究成果的可重复性与可信度。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程（RSEng）是高质研究软件的关键，但许多研究者对其了解不足或面临技术复杂性的挑战。

Method: 作者提出了十条简单规则，旨在提供具体且可操作的建议，帮助研究领导者更轻松地采用RSEng。

Result: 通过遵循这些规则，研究软件的质、可重复性和可信度将得到提升，从而改善研究成果。

Conclusion: RSEng的普及和应用有助于推动高质量研究，十条规则为研究者提供了实用的入门指南。

Abstract: Research Software Engineering (RSEng) is a key success factor in producing
high-quality research software, which in turn enables and improves research
outcomes. However, as a principal investigator or leader of a research group
you may not know what RSEng is, where to get started with it, or how to use it
to maximize its benefit for your research. RSEng also often comes with
technical complexity, and therefore reduced accessibility to some researchers.
The ten simple rules presented in this paper aim to improve the accessibility
of RSEng, and provide practical and actionable advice to PIs and leaders for
integrating RSEng into their research group. By following these rules, readers
can improve the quality, reproducibility, and trustworthiness of their research
software, ultimately leading to better, more reproducible and more trustworthy
research outcomes.

</details>


### [5] [The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review](https://arxiv.org/abs/2506.20435)
*Mennatullah T. Khedr,John S. Fitzgerald*

Main category: cs.SE

TL;DR: 该论文回顾了数字孪生（DTs）在复杂系统中的组合与验证方法，指出当前研究中组合机制缺乏形式化，验证方法多样但缺乏标准化框架。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统分析数字孪生在复杂系统（如CPS和SoS）中的组合与验证方法，以解决模型不确定性和集成复杂性等挑战。

Method: 通过分析2022-2024年的21项研究，总结了DT的组合机制、SoS特性及验证与验证（V&V）的形式化、范围与挑战。

Result: 研究发现组合机制缺乏形式化，V&V方法多样但以半形式化和模拟为主，正式验证应用不足，且缺乏标准化框架。

Conclusion: 论文强调需要开发标准化的DT专用V&V框架，并加强组合方法的形式化，以支持复杂DT的实现。

Abstract: Digital Twins (DTs) are increasingly used to model complex systems,
especially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where
effective integration is key. This systematic literature review investigates DT
composition and verification and validation (V&V) methodologies. Analyzing 21
studies from 2022-2024, we examined composition mechanisms, SoS
characteristics, and V&V formality, scope, and challenges. While composition is
discussed, formalization is limited. V&V approaches vary, with semi-formal
methods and simulations dominating; formal verification is underutilized. Key
technical challenges include model uncertainty and integration complexity.
Methodological challenges highlight the lack of standardized DT-specific V&V
frameworks. There is a need to move beyond model validation to address
integration and cyber-physical consistency. This review contributes a
structured classification of V&V approaches and emphasizes the need for
standardized, scalable V&V and rigorous composition methodologies for complex
DT implementations.

</details>


### [6] [Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds](https://arxiv.org/abs/2506.20444)
*Xiang Lan,Tim Menzies,Bowen Xu*

Main category: cs.SE

TL;DR: 提出一种基于数据集映射的方法，通过识别和过滤低质量样本（'坏种子'）来提升漏洞检测中机器学习的训练效率，显著提高了F1分数并增强了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 漏洞检测中机器学习模型常受低质量数据（噪声、错误标签、不平衡）影响，需改进数据质量以提升效果。

Method: 提出一种数据集映射方法，分类训练样本的学习难度并整合到主动学习框架中，优先过滤有害样本并强调信息丰富样本。

Result: 实验显示，该方法在F1分数上比随机选择提高45.36%（DeepGini）和45.91%（K-Means），优于标准主动学习61.46%（DeepGini）和32.65%（K-Means）。

Conclusion: 通过优化样本选择，提升模型鲁棒性和性能，同时为未来数据集构建提供改进方向。

Abstract: Vulnerability detection is crucial for identifying security weaknesses in
software systems. However, the effectiveness of machine learning models in this
domain is often hindered by low-quality training datasets, which contain noisy,
mislabeled, or imbalanced samples. This paper proposes a novel dataset
maps-empowered approach that systematically identifies and mitigates
hard-to-learn outliers, referred to as "bad seeds", to improve model training
efficiency. Our approach can categorize training examples based on learning
difficulty and integrate this information into an active learning framework.
Unlike traditional methods that focus on uncertainty-based sampling, our
strategy prioritizes dataset quality by filtering out performance-harmful
samples while emphasizing informative ones. Our experimental results show that
our approach can improve F1 score over random selection by 45.36% (DeepGini)
and 45.91% (K-Means) and outperforms standard active learning by 61.46%
(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,
demonstrating the effectiveness of integrating dataset maps for optimizing
sample selection in vulnerability detection. Furthermore, our approach also
enhances model robustness, improves sample selection by filtering bad seeds,
and stabilizes active learning performance across iterations. By analyzing the
characteristics of these outliers, we provide insights for future improvements
in dataset construction, making vulnerability detection more reliable and
cost-effective.

</details>


### [7] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: 该研究通过引入基于大语言模型（LLM）的半自动化方法，解决了建筑信息模型（BIM）中人工代码合规性检查耗时且易出错的问题，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动检查BIM中的合规性既耗时又容易出错，亟需一种自动化解决方案来提升效率和准确性。

Method: 研究整合了GPT、Claude、Gemini和Llama等LLM与Revit软件，生成Python脚本并执行半自动化合规检查。

Result: 案例研究表明，该系统显著减少了合规检查的时间和精力，同时提高了准确性。

Conclusion: 提出的方法为BIM合规检查提供了全面、适应性强且经济的解决方案，具有广泛的应用潜力。

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [8] [CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency](https://arxiv.org/abs/2506.20558)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Jinxi Kuang,Zhihan Jiang,Guangba Yu,Yichen Li,David Lo,Michael R. Lyu*

Main category: cs.SE

TL;DR: 该论文提出了一种改进代码注释不一致性问题的方法，包括高质量数据集CCIBench和基于LLM的框架CCISolver，显著提升了检测和修复效果。


<details>
  <summary>Details</summary>
Motivation: 代码注释不一致性（CCI）对软件开发、测试和维护有负面影响，现有研究的数据集不准确且解决方案不足，需改进。

Method: 通过定量分析现有数据集，构建高质量数据集CCIBench，并设计LLM框架CCISolver检测和修复CCI。

Result: CCISolver在检测和修复任务中表现优异，检测F1-score达89.54%，修复GLEU分数相对提升18.84%，且推理速度快36%。

Conclusion: CCISolver具有显著优越性和实际应用潜力，能有效解决代码注释不一致性问题。

Abstract: Comments within code serve as a crucial foundation for software
documentation, facilitating developers to communicate and understand the code
effectively. However, code-comment inconsistency (CCI) can negatively affect
software development, testing, and maintenance. Recent efforts to mitigate this
issue have emerged, but existing studies often suffer from inaccurate datasets
and inadequate solutions, weakening their practical effectiveness. In this
study, we first conduct a quantitative analysis of existing datasets, revealing
a substantial portion of sampled data are mislabeled. To address these data
limitations, we introduce CCIBench, a refined dataset comprising high-quality
data, to support the training and evaluation of method-level CCI methods.
Furthermore, we present an innovative end-to-end LLM-based framework,
CCISolver, designed to improve code quality by identifying and rectifying CCIs.
Comprehensive evaluations demonstrate CCISolver's superior performance. For
detection, it establishes a new state-of-the-art with an F1-score of 89.54%. In
fixing task, it achieves a remarkable 18.84% relative improvement in GLEU score
over the strongest baseline. This superiority is confirmed by human evaluation,
where CCISolver's fixing success rate of 0.6533 significantly surpasses
existing methods. Critically, in a practical end-to-end setting, CCISolver's
innovative architecture is approximately 36% faster for inference than the
baseline model, underscoring its scalability and real-world applicability.

</details>


### [9] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Define-ML是一个扩展了Lean Inception的框架，通过结构化活动将数据和技术约束整合到早期ML产品构思中。


<details>
  <summary>Details</summary>
Motivation: 传统构思方法如Lean Inception缺乏对ML特定挑战（如数据依赖性和技术可行性）的支持，可能导致产品愿景不清晰。

Method: 遵循技术转移模型，通过静态验证（玩具问题）和动态验证（工业案例研究）开发和验证Define-ML，结合定量和定性分析。

Result: 参与者认为Define-ML能有效解决数据问题、对齐业务目标，并促进跨团队合作，但有一定学习门槛。

Conclusion: Define-ML是一个公开可用的验证方法，兼顾敏捷性和技术可行性，适用于ML产品构思。

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Unfolding Iterators: Specification and Verification of Higher-Order Iterators, in OCaml](https://arxiv.org/abs/2506.20310)
*Ion Chirica,Mário Pereira*

Main category: cs.PL

TL;DR: 提出了一个通用的方法来规范和验证高阶迭代器，使用OCaml语言，结合Gospel规范语言和Cameleer框架进行验证，并通过案例研究验证方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在编程语言中，迭代是核心概念，但对高阶迭代的形式化和模块化推理仍然具有挑战性，本文旨在解决这一问题。

Method: 使用Gospel规范语言描述迭代的一般行为，并利用Cameleer框架对迭代客户端进行演绎验证。

Result: 通过一系列案例研究验证了方法的可行性，包括经典列表迭代器和OCamlGraph库中的图算法。

Conclusion: 提出的方法能够有效规范和验证高阶迭代器，为编程语言的迭代推理提供了实用解决方案。

Abstract: Albeit being a central notion of every programming language, formally and
modularly reasoning about iteration proves itself to be a non-trivial feat,
specially in the context of higher-order iteration. In this paper, we present a
generic approach to the specification and deductive verification of
higher-order iterators, written in the OCaml language. Our methodology follows
two key principles: first, the usage of the Gospel specification language to
describe the general behaviour of any iteration schema; second, the usage of
the Cameleer framework to deductively verify that every iteration client is
correct with respect to its logical specification. To validate our approach we
develop a set of verified case studies, ranging from classic list iterators to
graph algorithms implemented in the widely used OCamlGraph library.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [11] [MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](https://arxiv.org/abs/2506.19884)
*Zhengxiang Huang,Chaoyue Niu,Zhaode Wang,Jiarui Xue,Hanming Zhang,Yugang Wang,Zewei Xin,Xiaotang Jiang,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.OS

TL;DR: MNN-AECS通过动态选择低功耗CPU核心，显著降低LLM解码阶段的能耗，同时保持解码速度在可接受范围内。


<details>
  <summary>Details</summary>
Motivation: 随着设备端LLM推理需求的增加，能效问题成为电池受限移动设备的主要关注点，目前的研究多集中于预填充阶段加速，忽视了解码阶段的能耗问题。

Method: 提出自适应能量中心核心选择（AECS）技术，并将其集成到MNN中，创建MNN-AECS系统，动态选择低功耗CPU核心以减少能耗。

Result: MNN-AECS在多个设备和数据集上平均降低23%的能耗，且不影响解码速度；与其他引擎相比，能节省39%至78%的能耗，提速12%至363%。

Conclusion: MNN-AECS为无需root或系统修改的高效能效LLM解码提供了首个引擎级解决方案。

Abstract: As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [12] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Main category: cs.NI

TL;DR: 论文提出了一种基于学习的信道占用预测框架MiLAAP，通过自注意力机制实现局部信道状态预测，避免了状态共享的开销，适用于动态网络。


<details>
  <summary>Details</summary>
Motivation: 为解决无线网络中信道状态信息共享带来的通信开销问题，并适应节点移动性以提高吞吐效率。

Method: 提出MiLAAP框架，利用自注意力机制和多头自注意力机制，预测信道占用状态和节点运动轨迹，仅依赖局部观察数据。

Result: MiLAAP在动态网络中的信道状态预测准确率接近100%，且具有零样本泛化能力。

Conclusion: MiLAAP在不增加通信开销的情况下，有效提升了动态网络中的信道调度效率。

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>


### [13] [Notes on Degeneracy and Robustness](https://arxiv.org/abs/2506.19974)
*Indrakshi Dey,Nicola Marchetti*

Main category: cs.NI

TL;DR: 本文定义了资源可替代性的新指标，以解决静态/移动/动态网络的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 研究网络中的退化性（degeneracy）现象，即不同结构的元素如何在约束下实现相同功能，以提高系统的适应性和恢复能力。

Method: 提出并形式化了多个新的资源可替代性指标，用于衡量网络中的退化性和鲁棒性。

Result: 这些指标能够有效评估不同网络的鲁棒性，并支持更快、更自适应的系统恢复。

Conclusion: 通过资源可替代性指标，网络设计可以在退化性的基础上提升鲁棒性和适应性。

Abstract: Degeneracy is the ability of structurally different elements to perform the
same function or yield the same output under certain constraints. In contrast
to redundancy, which implies identical backups, degeneracy allows diverse
components to step in and perform the same or similar role. Mathematically, it
is about mapping multiple distinct elements into the same function. In a
degenerate system, failure in one part can be compensated by others not
structurally linked. System functions are distributed within the system itself
or the entire network. This renders faster and more adaptive recovery. In this
work, we define and formulate several novel metrics for resource fungibility to
address robustness in networks (static/mobile/dynamic).

</details>


### [14] [A clusterability test for directed graphs](https://arxiv.org/abs/2506.20111)
*Mario R. Guarracino,Pierre Miasnikof,Alexander Y. Shestopaloff,Houyem Demni,Cristián Bravo,Yuri Lawryshyn*

Main category: cs.NI

TL;DR: 将统计测试$δ$扩展到无自环有向图，用于检测图的可聚类性，通过局部和全局密度比较判断，适用于大型网络采样分析。


<details>
  <summary>Details</summary>
Motivation: 验证有向图的可聚类性，并确定最佳采样大小。

Method: 基于$δ$测试，比较局部与全局密度，通过采样子集分析。

Result: $δ$测试在小样本（1%）下表现良好，能准确识别不可聚类图且对假设偏离鲁棒。

Conclusion: $δ$测试适用于大型有向图的可聚类性分析，采样效率高。

Abstract: In this article, we extend a statistical test of graph clusterability, the
$\delta$ test, to directed graphs with no self loops. The $\delta$ test,
originally designed for undirected graphs, is based on the premise that graphs
with a clustered structure display a mean local density that is statistically
higher than the graph's global density. We posit that graphs that do not meet
this necessary (but not sufficient) condition for clusterability can be
considered unsuited to clustering. In such cases, vertex clusters do not offer
a meaningful summary of the broader graph. Additionally in this study, we aim
to determine the optimal sample size (number of neighborhoods). Our test,
designed for the analysis of large networks, is based on sampling subsets of
neighborhoods/nodes. It is designed for cases where computing the density of
every node's neighborhood is infeasible. Our results show that the $\delta$
test performs very well, even with very small samples of neighborhoods ($1\%$).
It accurately detects unclusterable graphs and is also shown to be robust to
departures from the underlying assumptions of the $t$ test.

</details>


### [15] [A Detailed Measurement View on IPv6 Scanners and Their Adaption to BGP Signals](https://arxiv.org/abs/2506.20383)
*Isabell Egloff,Raphael Hiesgen,Maynard Koch,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 本文分析了IPv6扫描行为及其在11个月内四个网络望远镜中的表现，探讨了扫描器的目标选择、时间和网络策略，并提出了提升IPv6扫描器可见性的操作建议。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6网络逐渐普及，扫描IPv6节点的挑战成为越来越多扫描者尝试解决的问题。本文旨在通过实际观察和分析，提供对IPv6扫描行为的深入理解。

Method: 在11个月内，通过四个网络望远镜（其中一个定期通过BGP公告进行重新配置）观察扫描器行为，并分析其时间行为、目标选择、网络策略等特征。

Result: 研究发现，大型前缀的静默子网对扫描器不可见，而BGP前缀公告会迅速吸引扫描器注意力。此外，对扫描器的工具、指纹和跨类别相关性进行了分类。

Conclusion: 基于研究结果，本文提出了部署网络望远镜以增加IPv6扫描器可见性的操作指南。

Abstract: Scanners are daily visitors of public IPv4 hosts. Scanning IPv6 nodes
successfully is still a challenge, which an increasing crowd of actors tries to
master. In this paper, we analyze current IPv6 scanning under various network
conditions. We observe scanner behavior during eleven months in four network
telescopes, one of which is periodically reconfigured by changing BGP
announcements. We analyze and classify the observed scanners w.r.t. their
temporal behavior, their target, and network selection strategy, as well as
their individual tools, fingerprints, and correlations across categories. We
find that silent subnets of larger prefixes remain invisible, whereas BGP
prefix announcements quickly attract attention by scanners. Based on our
findings, we derive operational guidance on how to deploy network telescopes to
increase visibility of IPv6 scanners.

</details>


### [16] [Semantic Caching for Improving Web Affordability](https://arxiv.org/abs/2506.20420)
*Hafsa Akbar,Danish Athar,Muhammad Ayain Fida Rana,Chaudhary Hammad Javed,Zartash Afzal Uzmi,Ihsan Ayyub Qazi,Zafar Ayyub Qazi*

Main category: cs.NI

TL;DR: 提出一种基于大语言模型的语义缓存方法，通过重用语义相似的网页图像，显著减少数据传输，提升网络可负担性。


<details>
  <summary>Details</summary>
Motivation: 网页内容快速增长导致网页体积增大，增加了发展中国家用户的数据成本负担。

Method: 利用多模态大语言模型评估图像的语义可替换性，设计语义缓存架构。

Result: 实验显示，部分网站类别的图像可替换率达37%，用户可获得约10%的字节节省。

Conclusion: 该方法对用户和网站运营商均有益，但需解决语义保留、隐私等问题。

Abstract: The rapid growth of web content has led to increasingly large webpages,
posing significant challenges for Internet affordability, especially in
developing countries where data costs remain prohibitively high. We propose
semantic caching using Large Language Models (LLMs) to improve web
affordability by enabling reuse of semantically similar images within webpages.
Analyzing 50 leading news and media websites, encompassing 4,264 images and
over 40,000 image pairs, we demonstrate potential for significant data transfer
reduction, with some website categories showing up to 37% of images as
replaceable. Our proof-of-concept architecture shows users can achieve
approximately 10% greater byte savings compared to exact caching. We evaluate
both commercial and open-source multi-modal LLMs for assessing semantic
replaceability. GPT-4o performs best with a low Normalized Root Mean Square
Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA
3.1 model shows comparable performance, highlighting its viability for
large-scale applications. This approach offers benefits for both users and
website operators, substantially reducing data transmission. We discuss ethical
concerns and practical challenges, including semantic preservation, user-driven
cache configuration, privacy concerns, and potential resistance from website
operators

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [17] [Practical Exploration of Polyhedral Model Checking](https://arxiv.org/abs/2506.20176)
*Yuri Andriaccio,Vincenzo Ciancia,Diego Latella,Mieke Massink*

Main category: cs.LO

TL;DR: 该论文探讨了如何通过空间模型检查来分析多面体模型的空间属性，提出两种方法：一是用模型检查结果丰富原始模型以减少公式长度并获得更直观的可视化结果；二是通过模型最小化深入理解模型的空间结构。


<details>
  <summary>Details</summary>
Motivation: 多面体模型在计算机图形学中常见，如三角表面网格或四面体体积网格。论文旨在通过空间模型检查来分析这些模型的空间属性。

Method: 利用PolyLogicA空间模型检查器和PolyVisualizer可视化工具，通过空间逻辑SLCS的多面体语义，将模型检查结果作为原子命题添加到原始模型中。

Result: 实现了对多面体模型的丰富化和最小化，优化了公式检查长度，并提供了更直观的可视化结果和空间结构理解。

Conclusion: 论文通过空间模型检查技术，为多面体模型的分析提供了实用方法，丰富了模型的表达，并提升了其图形可视化的效果。

Abstract: This work explores the potential of spatial model checking of polyhedral
models on a number of selected examples. In computer graphics polyhedral models
can be found in the form of triangular surface meshes of tetrahedral volume
meshes which are abundant. Spatial model checking is used to analyse spatial
properties of interest of such models expressed in a suitable spatial logic.
The original contributions of this paper are twofold. First we illustrate how a
polyhedral model can be enriched by adding the outcome of one model checking
session as an atomic proposition to the original model. This is useful as it
provides a way to reduce the length of formulas to check on such models and to
obtain more insightful results when these models are used for graphical
visualisation. Second we show that this form of enrichment also enables
practical model minimisation providing deeper insights in the basic spatial
structure of the model in terms of the spatial logic properties it enjoys. This
work is performed in the context of the geometric spatial model checker
PolyLogicA, the visualizer PolyVisualizer and the polyhedral semantics of the
Spatial Logic for Closure Spaces SLCS.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [Refining Participatory Design for AAC Users](https://arxiv.org/abs/2506.19995)
*Blade Frisch,Keith Vertanen*

Main category: cs.HC

TL;DR: 本文探讨了如何通过改进参与式设计方法，使高科技增强和替代交流（AAC）系统的设计更贴近用户需求。


<details>
  <summary>Details</summary>
Motivation: 鉴于AAC系统对沟通障碍者的重要性，改进设计过程以更好地满足用户需求是关键。

Method: 提出一种分为两部分的改进参与式设计方法，并通过用户反馈优化其可操作性。

Result: 计划进一步根据参与者反馈优化设计流程，使其更具可访问性。

Conclusion: 通过更贴近用户的参与式设计，可以有效提升高科技AAC系统的适用性和用户体验。

Abstract: Augmentative and alternative communication (AAC) is a field of research and
practice that works with people who have a communication disability. One form
AAC can take is a high-tech tool, such as a software-based communication
system. Like all user interfaces, these systems must be designed and it is
critical to include AAC users in the design process for their systems. A
participatory design approach can include AAC users in the design process, but
modifications may be necessary to make these methods more accessible. We
present a two-part design process we are investigating for improving the
participatory design for high-tech AAC systems. We discuss our plans to refine
the accessibility of this process based on participant feedback.

</details>


### [19] ["I'm Petting the Laptop, Which Has You Inside It": Reflecting on Lived Experiences of Online Friendship](https://arxiv.org/abs/2506.20055)
*Seraphina Yong,Ashlee Milton,Evan Suma Rosenberg,Stevie Chancellor,Svetlana Yarosh*

Main category: cs.HC

TL;DR: 研究通过25个访谈分析多年在线友谊的独特挑战与策略，如现实圈子的污名化和技术再应用，为HCI研究中强/弱关系和时间稳定交互提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 探讨疫情期间在线友谊的普遍性与心理健康争议，弥补现有研究中跨平台动态理解的不足。

Method: 基于活动的访谈分析，涵盖25名长期线上友谊经历者的生活经验。

Result: 揭示在线友谊中的挑战（如污名化）与策略（如技术再应用），重新定义线上社交空间中的强弱关系。

Conclusion: 研究推动了HCI对技术中介实践的批判性反思，并呼吁关注在线朋友作为隐形边缘群体的设计需求。

Abstract: Online(-only) friendships have become increasingly common in daily lives
post-COVID despite debates around their mental health benefits and equivalence
to ''real'' relationships. Previous research has reflected a need to understand
how online friends engage beyond individual platforms, and the lack of
platform-agnostic inquiry limits our ability to fully understand the dynamics
of online friendship. We employed an activity-grounded analysis of 25
interviews on lived experiences of close online friendship spanning multiple
years. Our findings present unique challenges and strategies in online
friendships, such as stigma from real-life circles, an ambivalent relationship
with online communities, and counter-theoretical reappropriations of
communication technology. This study contributes to HCI research in online
communities and social interface design by refocusing prior impressions of
strong vs. weak-ties in online social spaces and foregrounding time-stable
interactions in design for relationship maintenance through technology. Our
work also promotes critical reflection on biased perspectives towards
technology-mediated practices and consideration of online friends as an
invisible marginalized community.

</details>


### [20] [Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents](https://arxiv.org/abs/2506.20062)
*Runlong Ye,Zeling Zhang,Boushra Almazroua,Michael Liut*

Main category: cs.HC

TL;DR: CopilotLens是一个新型交互框架，通过透明化AI代码助手的决策过程，提升开发者的理解和信任。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码助手缺乏解释性，导致开发者难以评估其建议，影响了信任和合作。

Method: 设计了CopilotLens框架，提供两层级动态界面，展示AI的高层计划和代码上下文。

Result: 通过CopilotLens，开发者能更深入地理解AI的决策过程，增强合作效果。

Conclusion: CopilotLens为未来透明化的代码助手设计提供了框架，强调推理清晰而非速度。

Abstract: AI-powered code assistants are widely used to generate code completions,
significantly boosting developer productivity. However, these tools typically
present suggestions without explaining their rationale, leaving their
decision-making process inscrutable. This opacity hinders developers' ability
to critically evaluate the output, form accurate mental models, and build
calibrated trust in the system. To address this, we introduce CopilotLens, a
novel interactive framework that reframes code completion from a simple
suggestion into a transparent, explainable event. CopilotLens operates as an
explanation layer that reveals the AI agent's "thought process" through a
dynamic two-level interface, surfacing everything from its reconstructed
high-level plans to the specific codebase context influencing the code. This
paper presents the design and rationale of CopilotLens, offering a concrete
framework for building future agentic code assistants that prioritize clarity
of reasoning over speed of suggestion, thereby fostering deeper comprehension
and more robust human-AI collaboration.

</details>


### [21] [From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive Multi-Agentic Systems](https://arxiv.org/abs/2506.20091)
*Sarah Schömbs,Yan Zhang,Jorge Goncalves,Wafa Johal*

Main category: cs.HC

TL;DR: 论文探讨了多智能体系统的架构和关键特性，通过以用户为中心的视角分析了其机会、风险和挑战，并提出了未来HCI研究的议程。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统（如AutoGen、OpenAI Swarm）的发展，用户可以与一组专业化的AI智能体而非单一通用型智能体交互。尽管这一新范式充满潜力，但HCI社区尚未全面研究其带来的机会、风险和用户为中心的挑战。

Method: 通过分析现有工具和框架，作者识别了一系列关键挑战（如协调和冲突解决），并通过示例阐述这些挑战，提出了设计考虑和研究机会。

Result: 研究为未来多智能体系统的用户中心设计奠定了基础，提出了指导性研究议程。

Conclusion: 论文强调了对多智能体系统进行用户为中心研究的重要性，并呼吁跨学科合作以推动未来探索。

Abstract: Recent advances in multi-agentic systems (e.g. AutoGen, OpenAI Swarm) allow
users to interact with a group of specialised AI agents rather than a single
general-purpose agent. Despite the promise of this new paradigm, the HCI
community has yet to fully examine the opportunities, risks, and user-centred
challenges it introduces. We contribute to research on multi-agentic systems by
exploring their architectures and key features through a human-centred lens.
While literature and use cases remain limited, we build on existing tools and
frameworks available to developers to identify a set of overarching challenges,
e.g. orchestration and conflict resolution, that can guide future research in
HCI. We illustrate these challenges through examples, offer potential design
considerations, and provide research opportunities to spark interdisciplinary
conversation. Our work lays the groundwork for future exploration and offers a
research agenda focused on user-centred design in multi-agentic systems.

</details>


### [22] [Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype](https://arxiv.org/abs/2506.20156)
*Xuefei Hou,Xizhao Tan*

Main category: cs.HC

TL;DR: 本文提出了一种名为'Insight Recall'的新范式，通过上下文触发的个人见解检索来支持自我调节学习（SRL）。


<details>
  <summary>Details</summary>
Motivation: 现有数字化工具在支持元认知反思方面存在不足，如间隔重复系统（SRS）忽略上下文，个人知识管理（PKM）工具需要高维护。

Method: 采用即时适应性干预（JITAI）框架，实现了一个原型系统'Irec'，结合动态知识图谱和大型语言模型（LLM）来检索和呈现相关见解。

Result: 提出了一种理论框架和可用系统平台，为下一代智能学习系统的设计提供了支持。

Conclusion: 'Insight Recall'为提升元认知和自我调节能力的智能学习系统提供了理论和实践基础。

Abstract: The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.

</details>


### [23] [User Understanding of Privacy Permissions in Mobile Augmented Reality: Perceptions and Misconceptions](https://arxiv.org/abs/2506.20207)
*Viktorija Paneva,Verena Winterhalter,Franziska Augustinowski,Florian Alt*

Main category: cs.HC

TL;DR: 研究了移动AR应用中用户对隐私权限的认知，发现普遍误解，并提出了改进权限设计的具体建议。


<details>
  <summary>Details</summary>
Motivation: 移动AR应用依赖多种传感器数据，带来隐私挑战，需了解用户对权限的认知以改进设计。

Method: 分析现有应用及对120名参与者进行在线调查。

Result: 发现用户对权限功能与标签的常见误解，如位置权限与物理距离测量的关系混淆。

Conclusion: 提出模块化权限请求、明确标签等改进建议，为开发者和政策制定者提供指导。

Abstract: Mobile Augmented Reality (AR) applications leverage various sensors to
provide immersive user experiences. However, their reliance on diverse data
sources introduces significant privacy challenges. This paper investigates user
perceptions and understanding of privacy permissions in mobile AR apps through
an analysis of existing applications and an online survey of 120 participants.
Findings reveal common misconceptions, including confusion about how
permissions relate to specific AR functionalities (e.g., location and
measurement of physical distances), and misinterpretations of permission labels
(e.g., conflating camera and gallery access). We identify a set of actionable
implications for designing more usable and transparent privacy mechanisms
tailored to mobile AR technologies, including contextual explanations, modular
permission requests, and clearer permission labels. These findings offer
actionable guidance for developers, researchers, and policymakers working to
enhance privacy frameworks in mobile AR.

</details>


### [24] [A Literature Review on Simulation in Conversational Recommender Systems](https://arxiv.org/abs/2506.20291)
*Haoran Zhang,Xin Zhao,Jinze Chen,Junpeng Guo*

Main category: cs.HC

TL;DR: 本文综述了对话推荐系统（CRSs）的研究，通过分类框架系统分析了模拟方法的作用及其在数据集构建、算法设计、系统评估和实证研究中的应用，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统通过多轮对话提供个性化推荐，但其研究面临数据集偏差、模型输出灵活性不足等挑战，模拟方法为解决这些问题提供了潜力。

Method: 采用分类框架对CRS研究进行系统分类，分析模拟方法在数据集构建、算法设计、系统评估和实证研究中的应用。

Result: 模拟方法（如基于LLM的方法）在解决CRS主要挑战中发挥了关键作用，但仍存在数据集偏差和语义差距等问题。

Conclusion: 模拟方法对CRS研究具有重要潜力，未来需进一步探索以克服当前挑战。

Abstract: Conversational Recommender Systems (CRSs) have garnered attention as a novel
approach to delivering personalized recommendations through multi-turn
dialogues. This review developed a taxonomy framework to systematically
categorize relevant publications into four groups: dataset construction,
algorithm design, system evaluation, and empirical studies, providing a
comprehensive analysis of simulation methods in CRSs research. Our analysis
reveals that simulation methods play a key role in tackling CRSs' main
challenges. For example, LLM-based simulation methods have been used to create
conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.
Despite several challenges, such as dataset bias, the limited output
flexibility of LLM-based simulations, and the gap between text semantic space
and behavioral semantics, persist due to the complexity in Human-Computer
Interaction (HCI) of CRSs, simulation methods hold significant potential for
advancing CRS research. This review offers a thorough summary of the current
research landscape in this domain and identifies promising directions for
future inquiry.

</details>


### [25] [The Role of Partisan Culture in Mental Health Language Online](https://arxiv.org/abs/2506.20377)
*Sachin R. Pendse,Ben Rochford,Neha Kumar,Munmun De Choudhury*

Main category: cs.HC

TL;DR: 研究探讨了美国共和党和民主党用户在在线心理健康社区中表达痛苦时，党派文化是否产生影响。通过大规模观察性研究，发现党派文化确实影响痛苦表达，强调了在平台设计中考虑文化差异的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着在线支持社区的兴起，文化如何影响人们表达痛苦成为CSCW和HCI领域的热点话题。美国的两党文化差异为研究提供了独特背景。

Method: 利用因果推断方法匹配党派用户，构建可比队列，并结合自然语言处理技术分析他们的痛苦表达。数据涵盖2013年至2022年的218万条帖子。

Result: 研究发现党派文化显著影响痛苦表达方式，凸显了在设计在线支持平台时需考虑党派文化差异。

Conclusion: 党派文化在在线心理健康支持社区中起重要作用，未来平台设计需更注重文化多样性。

Abstract: The impact of culture on how people express distress in online support
communities is increasingly a topic of interest within Computer Supported
Cooperative Work (CSCW) and Human-Computer Interaction (HCI). In the United
States, distinct cultures have emerged from each of the two dominant political
parties, forming a primary lens by which people navigate online and offline
worlds. We examine whether partisan culture may play a role in how U.S.
Republican and Democrat users of online mental health support communities
express distress. We present a large-scale observational study of 2,184,356
posts from 8,916 statistically matched Republican, Democrat, and unaffiliated
online support community members. We utilize methods from causal inference to
statistically match partisan users along covariates that correspond with
demographic attributes and platform use, in order to create comparable cohorts
for analysis. We then leverage methods from natural language processing to
understand how partisan expressions of distress compare between these sets of
closely matched opposing partisans, and between closely matched partisans and
typical support community members. Our data spans January 2013 to December
2022, a period of both rising political polarization and mental health
concerns. We find that partisan culture does play into expressions of distress,
underscoring the importance of considering partisan cultural differences in the
design of online support community platforms.

</details>


### [26] [Analyzing Security and Privacy Challenges in Generative AI Usage Guidelines for Higher Education](https://arxiv.org/abs/2506.20463)
*Bei Yi Ng,Jiarui Li,Xinyuan Tong,Kevin Ye,Gauthami Yenne,Varun Chandrasekaran,Jingjie Li*

Main category: cs.HC

TL;DR: 论文探讨了高等教育中生成式人工智能（GenAI）的隐私和安全问题，分析了多国大学的政策以提出保护措施。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在高等教育中普及，但其隐私和安全问题被忽视，需制定针对性政策。

Method: 通过定性分析12个国家大学的GenAI使用指南，识别挑战和机会。

Result: 发现需要为学术环境量身定制的GenAI隐私和安全保护措施。

Conclusion: 研究强调了学术机构需重视GenAI的隐私安全，并提出针对性政策建议。

Abstract: Educators and learners worldwide are embracing the rise of Generative
Artificial Intelligence (GenAI) as it reshapes higher education. However, GenAI
also raises significant privacy and security concerns, as models and
privacy-sensitive user data, such as student records, may be misused by service
providers. Unfortunately, end-users often have little awareness of or control
over how these models operate. To address these concerns, universities are
developing institutional policies to guide GenAI use while safeguarding
security and privacy. This work examines these emerging policies and
guidelines, with a particular focus on the often-overlooked privacy and
security dimensions of GenAI integration in higher education, alongside other
academic values. Through a qualitative analysis of GenAI usage guidelines from
universities across 12 countries, we identify key challenges and opportunities
institutions face in providing effective privacy and security protections,
including the need for GenAI safeguards tailored specifically to the academic
context.

</details>


### [27] [AI in the Writing Process: How Purposeful AI Support Fosters Student Writing](https://arxiv.org/abs/2506.20595)
*Momin N. Siddiqui,Roy Pea,Hari Subramonyam*

Main category: cs.HC

TL;DR: 研究探讨了不同AI写作辅助工具对学生写作自主权和知识深度转化的影响，发现集成AI工具效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨AI写作工具对学生写作自主性和内容深度的影响，以解决ChatGPT等技术可能带来的问题。

Method: 通过随机对照试验，比较聊天型LLM、集成AI写作工具和标准写作界面对90名本科生的影响。

Result: 集成AI工具组学生表现出更强的写作自主权和更深的知识转化。

Conclusion: 精心设计的AI写作辅助工具可帮助学生保持对作品的所有权，同时提升内容参与度。

Abstract: The ubiquity of technologies like ChatGPT has raised concerns about their
impact on student writing, particularly regarding reduced learner agency and
superficial engagement with content. While standalone chat-based LLMs often
produce suboptimal writing outcomes, evidence suggests that purposefully
designed AI writing support tools can enhance the writing process. This paper
investigates how different AI support approaches affect writers' sense of
agency and depth of knowledge transformation. Through a randomized control
trial with 90 undergraduate students, we compare three conditions: (1) a
chat-based LLM writing assistant, (2) an integrated AI writing tool to support
diverse subprocesses, and (3) a standard writing interface (control). Our
findings demonstrate that, among AI-supported conditions, students using the
integrated AI writing tool exhibited greater agency over their writing process
and engaged in deeper knowledge transformation overall. These results suggest
that thoughtfully designed AI writing support targeting specific aspects of the
writing process can help students maintain ownership of their work while
facilitating improved engagement with content.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [28] [RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer](https://arxiv.org/abs/2506.20202)
*Da Li,Donggang Jia,Yousef Rajeh,Dominik Engel,Ivan Viola*

Main category: cs.GR

TL;DR: 提出了一种结合光栅化和光线追踪的混合渲染框架，用于高效、高保真地裁剪高斯泼溅数据。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅技术的硬裁剪问题，提高其准确性和效率。

Method: 采用RaRa策略，先通过光栅化快速识别被裁剪平面截断的高斯，再用光线追踪计算部分遮挡的衰减权重。

Result: 实验表明方法在保持实时渲染性能的同时，提供了视觉上更优的结果和高保真度。

Conclusion: 该方法为高斯泼溅数据的精准裁剪提供了一种高效的解决方案。

Abstract: With the advancement of Gaussian Splatting techniques, a growing number of
datasets based on this representation have been developed. However, performing
accurate and efficient clipping for Gaussian Splatting remains a challenging
and unresolved problem, primarily due to the volumetric nature of Gaussian
primitives, which makes hard clipping incapable of precisely localizing their
pixel-level contributions. In this paper, we propose a hybrid rendering
framework that combines rasterization and ray tracing to achieve efficient and
high-fidelity clipping of Gaussian Splatting data. At the core of our method is
the RaRa strategy, which first leverages rasterization to quickly identify
Gaussians intersected by the clipping plane, followed by ray tracing to compute
attenuation weights based on their partial occlusion. These weights are then
used to accurately estimate each Gaussian's contribution to the final image,
enabling smooth and continuous clipping effects. We validate our approach on
diverse datasets, including general Gaussians, hair strand Gaussians, and
multi-layer Gaussians, and conduct user studies to evaluate both perceptual
quality and quantitative performance. Experimental results demonstrate that our
method delivers visually superior results while maintaining real-time rendering
performance and preserving high fidelity in the unclipped regions.

</details>


### [29] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: 提出了一种可解释的视觉变换器X-SiT，用于基于皮层表面数据的疾病诊断，实现了高性能和人类可理解的预测。


<details>
  <summary>Details</summary>
Motivation: 3D体积数据难以可视化，而皮层表面渲染更易于理解和探索，促使开发可解释的神经网络的必要性。

Method: 提出X-SiT，引入原型表面补丁解码器，结合基于案例的推理和皮层原型进行分类。

Result: 在阿尔茨海默病和额颞叶痴呆检测中表现优异，并提供与已知疾病模式一致的原型。

Conclusion: X-SiT不仅性能先进，还能提供直观的解释，有助于临床决策。

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


### [30] [DreamAnywhere: Object-Centric Panoramic 3D Scene Generation](https://arxiv.org/abs/2506.20367)
*Edoardo Alberto Dominici,Jozef Hladky,Floor Verhoeven,Lukas Radl,Thomas Deixelberger,Stefan Ainetter,Philipp Drescher,Stefan Hauswiesner,Arno Coomans,Giacomo Nazzaro,Konstantinos Vardis,Markus Steinberger*

Main category: cs.GR

TL;DR: DreamAnywhere是一个模块化系统，通过文本快速生成和原型化3D场景，解决了现有方法在视觉保真度、场景理解和覆盖范围方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D场景生成方法在视觉保真度、场景理解和场景范围（室内/室外）方面存在局限，DreamAnywhere旨在解决这些问题。

Method: 系统首先生成360度全景图像，分解背景和对象，利用混合修复构建完整3D表示，并将对象掩码提升为详细3D对象。

Result: 在新型视图合成的连贯性和图像质量上表现优异，用户研究显示明显优于现有方法。

Conclusion: DreamAnywhere在低预算电影制作等场景中具有实用价值，模块化设计使其高度可定制。

Abstract: Recent advances in text-to-3D scene generation have demonstrated significant
potential to transform content creation across multiple industries. Although
the research community has made impressive progress in addressing the
challenges of this complex task, existing methods often generate environments
that are only front-facing, lack visual fidelity, exhibit limited scene
understanding, and are typically fine-tuned for either indoor or outdoor
settings. In this work, we address these issues and propose DreamAnywhere, a
modular system for the fast generation and prototyping of 3D scenes. Our system
synthesizes a 360{\deg} panoramic image from text, decomposes it into
background and objects, constructs a complete 3D representation through hybrid
inpainting, and lifts object masks to detailed 3D objects that are placed in
the virtual environment. DreamAnywhere supports immersive navigation and
intuitive object-level editing, making it ideal for scene exploration, visual
mock-ups, and rapid prototyping -- all with minimal manual modeling. These
features make our system particularly suitable for low-budget movie production,
enabling quick iteration on scene layout and visual tone without the overhead
of traditional 3D workflows. Our modular pipeline is highly customizable as it
allows components to be replaced independently. Compared to current
state-of-the-art text and image-based 3D scene generation approaches,
DreamAnywhere shows significant improvements in coherence in novel view
synthesis and achieves competitive image quality, demonstrating its
effectiveness across diverse and challenging scenarios. A comprehensive user
study demonstrates a clear preference for our method over existing approaches,
validating both its technical robustness and practical usefulness.

</details>


### [31] [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](https://arxiv.org/abs/2506.20652)
*Roi Bar-On,Dana Cohen-Bar,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: EditP23是一种无需掩码的3D编辑方法，通过2D图像编辑传播到多视角表示的3D一致编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖文本提示或显式空间掩码，而EditP23利用图像对（原始图像和用户编辑后的图像）实现直观编辑，避免手动标记。

Method: EditP23通过在预训练的多视角扩散模型的潜在空间中引导编辑感知流，将编辑一致地传播到多视角。

Result: 在多种对象类别和编辑场景中表现出色，保持原始对象的结构和外观，无需手动掩码。

Conclusion: EditP23是一种高效、直观的3D编辑方法，适用于广泛的应用场景。

Abstract: We present EditP23, a method for mask-free 3D editing that propagates 2D
image edits to multi-view representations in a 3D-consistent manner. In
contrast to traditional approaches that rely on text-based prompting or
explicit spatial masks, EditP23 enables intuitive edits by conditioning on a
pair of images: an original view and its user-edited counterpart. These image
prompts are used to guide an edit-aware flow in the latent space of a
pre-trained multi-view diffusion model, allowing the edit to be coherently
propagated across views. Our method operates in a feed-forward manner, without
optimization, and preserves the identity of the original object, in both
structure and appearance. We demonstrate its effectiveness across a range of
object categories and editing scenarios, achieving high fidelity to the source
while requiring no manual masks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Main category: cs.DC

TL;DR: MAIZX框架通过动态资源排名优化云操作，显著减少碳排放，在测试中比基线减少85.68%的CO2排放。


<details>
  <summary>Details</summary>
Motivation: 云计算的能源消耗和碳排放问题日益严重，数据中心占全球能源使用的2-4%。私云基础设施被87%的组织使用，亟需高效透明的解决方案。

Method: MAIZX框架通过动态排名算法，基于实时和预测的碳强度、PUE和能源消耗，优化私有、混合和多云环境中的工作负载。

Result: 测试结果显示，MAIZX显著减少了85.68%的CO2排放，并展示了可扩展性和有效性。

Conclusion: MAIZX为云计算提供了可持续的解决方案，同时保持了操作效率。

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [33] [On the $h$-majority dynamics with many opinions](https://arxiv.org/abs/2506.20218)
*Francesco d'Amore,Niccolò D'Archivio,George Giakkoupis,Emanuele Natale*

Main category: cs.DC

TL;DR: 本文研究了同步设置下$h$-多数动态与$k$种意见的共识收敛时间的上界，证明了在初始偏差为$ω(√x)$且初始多意见支持$x = ω(γ n)$条件下，过程在$O(γ n)$轮内收敛到多意见共识。


<details>
  <summary>Details</summary>
Motivation: 研究$h$-多数动态在非恒定$h$和$k$下的收敛性质，填补现有理论空白。

Method: 假设初始意见存在$ω(√x)$的偏差，证明在特定条件下$h = ω(nγ n / x)$时过程快速收敛。

Result: 若$k = o(n / γ n)$且初始偏差为$ω(√n/k)$，则$h = ω(k γ n)$足以保证$O(γ n)$轮内高概率收敛。

Conclusion: 本研究改进了Becchetti等人(2017)的下界，展示了更优的偏差要求，扩展了对$h$-多数动态的理解。

Abstract: We present the first upper bound on the convergence time to consensus of the
well-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,
for $h$ and $k$ that are both non-constant values.
  We suppose that, at the beginning of the process, there is some initial
additive bias towards some plurality opinion, that is, there is an opinion that
is supported by $x$ nodes while any other opinion is supported by strictly
fewer nodes.
  We prove that, with high probability, if the bias is $\omega(\sqrt{x})$ and
the initial plurality opinion is supported by at least $x = \omega(\log n)$
nodes, then the process converges to plurality consensus in $O(\log n)$ rounds
whenever $h = \omega(n \log n / x)$.
  A main corollary is the following: if $k = o(n / \log n)$ and the process
starts from an almost-balanced configuration with an initial bias of magnitude
$\omega(\sqrt{n/k})$ towards the initial plurality opinion, then any function
$h = \omega(k \log n)$ suffices to guarantee convergence to consensus in
$O(\log n)$ rounds, with high probability.
  Our upper bound shows that the lower bound of $\Omega(k / h^2)$ rounds to
reach consensus given by Becchetti et al.\ (2017) cannot be pushed further than
$\widetilde{\Omega}(k / h)$.
  Moreover, the bias we require is asymptotically smaller than the
$\Omega(\sqrt{n\log n})$ bias that guarantees plurality consensus in the
$3$-majority dynamics: in our case, the required bias is at most any
(arbitrarily small) function in $\omega(\sqrt{x})$ for any value of $k \ge 2$.

</details>


### [34] [PAT: a new algorithm for all-gather and reduce-scatter operations at scale](https://arxiv.org/abs/2506.20252)
*Sylvain Jeaugey*

Main category: cs.DC

TL;DR: 本文介绍了一种名为PAT（并行聚合树）的新算法，用于实现all-gather和reduce-scatter操作，旨在提升NCCL库在环形算法效率低下时的性能。


<details>
  <summary>Details</summary>
Motivation: 解决环形算法在小规模或大规模操作时因线性延迟导致的性能问题。

Method: 提出PAT算法，支持任意数量的计算节点，优化网络传输和缓冲区使用。

Result: 实现了对数级的网络传输和缓冲区需求，减少了长距离通信。

Conclusion: PAT算法在小规模和大规模操作中均能显著提升性能。

Abstract: This paper describes a new algorithm called PAT, for Parallel Aggregated
Trees, and which can be used to implement all-gather and reduce-scatter
operations. This algorithm works on any number of ranks, has a logarithmic
number of network transfers for small size operations, minimizes long-distance
communication, and requires a logarithmic amount of internal buffers,
independently from the total operation size. It is aimed at improving the
performance of the NCCL library in cases where the ring algorithm would be
inefficient, as its linear latency would show poor performance for small sizes
and/or at scale.

</details>


### [35] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Main category: cs.DC

TL;DR: WattsOnAI是一个用于测量、分析和可视化AI工作负载中能源使用、功耗、硬件性能和碳排放的软件工具包，旨在解决现有工具的局限性，推动绿色AI实践。


<details>
  <summary>Details</summary>
Motivation: 随着AI（尤其是大语言模型）的快速发展，模型训练和推理的能源消耗与碳排放问题日益突出，但现有工具缺乏系统性指标整合和支持相关分析的能力。

Method: WattsOnAI通过集成现有的AI框架，提供标准化报告和细粒度时间序列数据，支持相关性分析和性能优化，且设计轻量化。

Result: 该工具能够深入分析硬件指标与模型性能的相关性，帮助识别瓶颈并提升性能，同时促进绿色AI实践。

Conclusion: WattsOnAI填补了现有工具的不足，鼓励研究社区在关注性能的同时考虑环境影响，推动可持续的绿色AI发展。

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


### [36] [SuperSONIC: Cloud-Native Infrastructure for ML Inferencing](https://arxiv.org/abs/2506.20657)
*Dmitry Kondratyev,Benedikt Riedel,Yuan-Tang Chou,Miles Cochran-Branson,Noah Paladino,David Schultz,Mia Liu,Javier Duarte,Philip Harris,Shih-Chieh Hsu*

Main category: cs.DC

TL;DR: SuperSONIC项目是一个可扩展的服务架构，用于优化ML推理的SONIC方法，通过Kubernetes集群和GPU加速任务处理，已在多个科学实验中成功部署。


<details>
  <summary>Details</summary>
Motivation: 由于数据速率和复杂ML算法的计算需求增加，需要一种方法来优化资源利用和加速ML推理。

Method: 采用SONIC方法，通过SuperSONIC项目构建可扩展的服务器架构，使用Kubernetes集群和NVIDIA Triton Inference Server，实现任务负载均衡和监控。

Result: SuperSONIC在CERN、IceCube和LIGO等实验中成功部署，提高了资源利用效率和任务处理速度。

Conclusion: SuperSONIC为云原生时代提供了可重用、可配置的框架，适用于多领域和行业的加速器推理部署。

Abstract: The increasing computational demand from growing data rates and complex
machine learning (ML) algorithms in large-scale scientific experiments has
driven the adoption of the Services for Optimized Network Inference on
Coprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it
to local or remote coprocessors to optimize resource utilization. Leveraging
its portability to different types of coprocessors, SONIC enhances data
processing and model deployment efficiency for cutting-edge research in high
energy physics (HEP) and multi-messenger astrophysics (MMA). We developed the
SuperSONIC project, a scalable server infrastructure for SONIC, enabling the
deployment of computationally intensive tasks to Kubernetes clusters equipped
with graphics processing units (GPUs). Using NVIDIA Triton Inference Server,
SuperSONIC decouples client workflows from server infrastructure, standardizing
communication, optimizing throughput, load balancing, and monitoring.
SuperSONIC has been successfully deployed for the CMS and ATLAS experiments at
the CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory
(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)
and tested on Kubernetes clusters at Purdue University, the National Research
Platform (NRP), and the University of Chicago. SuperSONIC addresses the
challenges of the Cloud-native era by providing a reusable, configurable
framework that enhances the efficiency of accelerator-based inference
deployment across diverse scientific domains and industries.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [Near Data Processing in Taurus Database](https://arxiv.org/abs/2506.20010)
*Shu Lin,Arunprasad P. Marathe,Per-Ȧke Larson,Chong Chen,Calvin Sun,Paul Lee,Weidong Yu*

Main category: cs.DB

TL;DR: 华为云原生数据库GaussDB for MySQL（Taurus）通过数据近处理（NDP）减少网络传输数据、释放计算层CPU，提升查询效率。


<details>
  <summary>Details</summary>
Motivation: 通过将数据处理操作（如选择、投影和聚合）推近存储层，减少数据传输和计算负载，提升系统吞吐量。

Method: 设计并实现存储层的数据近处理（NDP），利用存储服务器的计算能力进行部分数据处理。

Result: 在TPCH基准测试中，18/22查询受益，数据传输减少63%，CPU时间减少50%，某些查询性能提升更高。

Conclusion: NDP显著减少网络数据传输和计算资源占用，有效提升数据库系统性能。

Abstract: Huawei's cloud-native database system GaussDB for MySQL (also known as
Taurus) stores data in a separate storage layer consisting of a pool of storage
servers. Each server has considerable compute power making it possible to push
data reduction operations (selection, projection, and aggregation) close to
storage. This paper describes the design and implementation of near data
processing (NDP) in Taurus. NDP has several benefits: it reduces the amount of
data shipped over the network; frees up CPU capacity in the compute layer; and
reduces query run time, thereby enabling higher system throughput. Experiments
with the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited
from NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.
On Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU
time by 91 percent; and run time by 80 percent.

</details>


### [38] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: 论文探讨了误差有界分段线性近似（$ϵ$-PLA）在机器学习索引结构中的应用，改进了现有算法的理论下界，并通过实验比较了不同数据结构的性能权衡。


<details>
  <summary>Details</summary>
Motivation: 由于$ϵ$-PLA在机器学习索引结构中的广泛使用，但其拟合算法的设计与分析尚未充分研究，论文旨在从理论和实证角度重新审视这一技术。

Method: 论文首先改进了现有$ϵ$-PLA算法的理论下界，然后通过实验对多种$ϵ$-PLA算法在机器学习数据结构中的应用进行了全面比较。

Result: 研究揭示了模型精度、模型大小和查询性能之间的关键权衡，为未来机器学习数据结构的优化设计提供了实用指导。

Conclusion: 论文通过理论提升与实证分析，为$ϵ$-PLA在机器学习索引结构中的应用提供了基础性改进和设计准则。

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [39] [Communicating Smartly in Molecular Communication Environments: Neural Networks in the Internet of Bio-Nano Things](https://arxiv.org/abs/2506.20589)
*Jorge Torres Gómez,Pit Hofmann,Lisa Y. Debus,Osman Tugay Başaran,Sebastian Lotter,Roya Khanzadeh,Stefan Angerbauer,Bige Deniz Unluturk,Sergi Abadal,Werner Haselmayr,Frank H. P. Fitzek,Robert Schober,Falko Dressler*

Main category: eess.SP

TL;DR: 本文探讨了生物纳米物联网（IoBNT）中分子通信（MC）的数据驱动通信策略，重点研究了机器学习（ML）和神经网络（NN）架构在纳米尺度通信中的应用及其挑战。


<details>
  <summary>Details</summary>
Motivation: 研究纳米传感器在体内协作的局限性，探讨如何通过机器学习提高分子通信的效率和稳定性。

Method: 利用神经网络架构和机器学习方法优化分子通信通道的动态性，并分析其实施可行性、可解释性及数据集生成。

Result: 提供了一系列神经网络架构的应用趋势、可解释人工智能（XAI）技术、数据集生成的最佳实践及开源代码库。

Conclusion: 总结了当前研究的成果，并指出了未来研究方向，如设计更鲁棒的神经网络架构和生物集成模块。

Abstract: Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the
groundwork for innovative applications across the healthcare sector.
Nanodevices designed to operate within the body, managed remotely via the
internet, are envisioned to promptly detect and actuate on potential diseases.
In this vision, an inherent challenge arises due to the limited capabilities of
individual nanosensors; specifically, nanosensors must communicate with one
another to collaborate as a cluster. Aiming to research the boundaries of the
clustering capabilities, this survey emphasizes data-driven communication
strategies in molecular communication (MC) channels as a means of linking
nanosensors. Relying on the flexibility and robustness of machine learning (ML)
methods to tackle the dynamic nature of MC channels, the MC research community
frequently refers to neural network (NN) architectures. This interdisciplinary
research field encompasses various aspects, including the use of NNs to
facilitate communication in MC environments, their implementation at the
nanoscale, explainable approaches for NNs, and dataset generation for training.
Within this survey, we provide a comprehensive analysis of fundamental
perspectives on recent trends in NN architectures for MC, the feasibility of
their implementation at the nanoscale, applied explainable artificial
intelligence (XAI) techniques, and the accessibility of datasets along with
best practices for their generation. Additionally, we offer open-source code
repositories that illustrate NN-based methods to support reproducible research
for key MC scenarios. Finally, we identify emerging research challenges, such
as robust NN architectures, biologically integrated NN modules, and scalable
training strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 论文探讨了结合低延迟AI模型的实时决策支持系统，整合边缘物联网技术和人机协作方法，研究资源受限下的决策辅助。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用AI技术（如大语言模型和模型压缩）提升实时决策支持的效率和适应性。

Method: 分析了DeLLMa等技术发展、模型压缩方法和边缘设备分析改进，结合文献综述。

Result: 提供了开发策略和应用领域的实用视角，指出高效灵活AI系统的机会。

Conclusion: 结论展望了AI如何重塑实时决策支持，为未来突破奠定基础。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [41] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: Decrypto 是一个基于游戏的基准测试，旨在评估 LLMs 在多智能体推理和心智理论（ToM）方面的能力，填补现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估 LLM 的多智能体能力和 ToM 方面存在范围狭窄、数据泄漏、饱和度高和缺乏交互性等问题，需要新的评估工具。

Method: 提出 Decrypto，一个基于游戏的多智能体推理和 ToM 基准测试，结合认知科学、计算语用学和多智能体强化学习，并通过实验验证其设计。

Result: 实验表明，LLMs 的游戏能力落后于人类和简单的词嵌入基线模型，且在 ToM 任务上表现不如旧模型。

Conclusion: Decrypto 填补了当前多智能体推理和 ToM 评估的空白，为开发更强大的人工智能代理提供了新方向。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [42] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode是一种基于规则增强学习的框架，用于提升大型语言模型在新API场景下的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因依赖过时API知识而在动态环境中代码生成不可靠的问题。

Method: 构建数据集训练LLMs，引入改进的字符串相似度指标作为奖励函数。

Result: ReCode显著提升LLMs在动态API场景中的性能，并在CodeUpdateArena任务上表现优异。

Conclusion: ReCode比监督微调更能保持LLMs的通用代码生成能力，适用范围广。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [43] [A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem](https://arxiv.org/abs/2506.20400)
*Kristoffer Christensen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.MA

TL;DR: 该论文提出了一种基于Python的模块化仪表板框架，用于分析和可视化多代理电动车充电模拟的复杂数据，帮助快速识别异常并提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 电动车家庭充电生态系统的多代理模拟生成的数据复杂且随机，传统的静态后处理方法难以检测和解释系统级异常事件，如变压器过载或消费者不满。

Method: 论文提出了一个基于Dash by Plotly的模块化仪表板框架，包含三个协调视图（系统概览、系统分析和消费者分析），支持高分辨率可视化（如时间序列图、空间热图和代理特定工具）。

Result: 通过一个丹麦住宅网络的案例研究，展示了仪表板如何快速识别和解释异常，如变压器过载和充电失败，并生成了可操作的见解。

Conclusion: 该框架不仅适用于电动车充电系统，还可扩展到其他分布式能源资源和复杂能源系统，为研究人员和配电系统运营商提供了强大的分析工具。

Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging
ecosystems generate large, complex, and stochastic time-series datasets that
capture interactions between households, grid infrastructure, and energy
markets. These interactions can lead to unexpected system-level events, such as
transformer overloads or consumer dissatisfaction, that are difficult to detect
and explain through static post-processing. This paper presents a modular,
Python-based dashboard framework, built using Dash by Plotly, that enables
efficient, multi-level exploration and root-cause analysis of emergent behavior
in MABS outputs. The system features three coordinated views (System Overview,
System Analysis, and Consumer Analysis), each offering high-resolution
visualizations such as time-series plots, spatial heatmaps, and agent-specific
drill-down tools. A case study simulating full EV adoption with smart charging
in a Danish residential network demonstrates how the dashboard supports rapid
identification and contextual explanation of anomalies, including clustered
transformer overloads and time-dependent charging failures. The framework
facilitates actionable insight generation for researchers and distribution
system operators, and its architecture is adaptable to other distributed energy
resources and complex energy systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [44] [The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape](https://arxiv.org/abs/2506.20104)
*Malikussaid,Sutiyo*

Main category: cs.CY

TL;DR: 俄罗斯入侵乌克兰显著改变了IT风险格局，尤其是在云计算领域。本文研究如何通过多层方法整合地缘政治因素，提升数字韧性。


<details>
  <summary>Details</summary>
Motivation: 探讨地缘冲突对数据主权、网络安全和云基础设施的深远影响，揭示传统IT风险管理框架的不足。

Method: 分析2022年至2025年初的网络安全操作、监管反应和组织适应，提出融合弹性云架构、数据安全策略和地缘治理的多层方法。

Result: 研究发现，传统框架难以应对国家支持的威胁和数据治理冲突，而多层方法能更有效地抵御地缘冲突带来的复杂风险。

Conclusion: 结合地缘政治因素的多层方法为数字韧性提供了更全面的防御策略，弥补了传统框架的不足。

Abstract: The Russian invasion of Ukraine has fundamentally altered the information
technology (IT) risk landscape, particularly in cloud computing environments.
This paper examines how this geopolitical conflict has accelerated data
sovereignty concerns, transformed cybersecurity paradigms, and reshaped cloud
infrastructure strategies worldwide. Through an analysis of documented cyber
operations, regulatory responses, and organizational adaptations between 2022
and early 2025, this research demonstrates how the conflict has served as a
catalyst for a broader reassessment of IT risk. The research reveals that while
traditional IT risk frameworks offer foundational guidance, their standard
application may inadequately address the nuances of state-sponsored threats,
conflicting data governance regimes, and the weaponization of digital
dependencies without specific geopolitical augmentation. The contribution of
this paper lies in its focused synthesis and strategic adaptation of existing
best practices into a multi-layered approach. This approach uniquely synergizes
resilient cloud architectures (including sovereign and hybrid models), enhanced
data-centric security strategies (such as advanced encryption and
privacy-enhancing technologies), and geopolitically-informed governance to
build digital resilience. The interplay between these layers, emphasizing how
geopolitical insights directly shape architectural and security choices beyond
standard best practices-particularly by integrating the human element,
including personnel vulnerabilities and expertise, as a core consideration in
technical design and operational management-offers a more robust defense
against the specific, multifaceted risks arising from geopolitical conflict in
increasingly fractured digital territories.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM是一个预处理框架，用于训练稳健的时间序列填补模型，解决了人工掩码数据与真实缺失模式之间的差距。它通过模式聚类和自适应掩码策略，提升了处理真实复杂缺失数据的能力。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列填补模型使用完整数据集和人工掩码模拟缺失值，但在实际基础设施监测中，数据缺失量大且模式复杂。DIM-SUM旨在缩小人工掩码训练数据与真实缺失模式之间的差距。

Method: DIM-SUM结合模式聚类和自适应掩码策略，并提供了理论上的学习保证，以处理实际数据中的多样化缺失模式。

Result: 实验表明，DIM-SUM在加州水资源区、电力数据集和基准测试中，处理超过20亿条数据时，传统方法更高效且训练数据更少。与大型预训练模型相比，DIM-SUM平均准确率提高2倍，推理时间显著减少。

Conclusion: DIM-SUM通过创新预处理策略，显著提升了时间序列填补模型在实际复杂缺失数据中的性能，为实际应用提供了更高效的解决方案。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [46] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 多模态学习利用多种信息源（如图像、文本和音频）提升AI系统的理解和决策能力，但仍面临数据格式差异、输入不完整等问题。未来可能推动计算机视觉、自然语言处理等领域的发展。


<details>
  <summary>Details</summary>
Motivation: 通过结合不同模态的信息，增强AI系统对复杂事物的理解和决策能力。

Method: 核心技术包括表征学习、对齐方法和融合策略，同时探索无监督学习、AutoML工具等新方法。

Result: 已在多个领域取得进展，但仍需解决数据格式差异、对抗攻击等问题。

Conclusion: 多模态学习有望构建更接近人类理解的AI系统，推动多个领域的发展。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [47] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: 本文提出了一种通过硬件使用优化改进联邦学习中本地训练过程的方法，利用贪婪随机搜索优化本地批量大小，结果显示可以提升收敛速度并与参数优化情况相当。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的参与者在共享硬件时可能因训练配置不当而影响效率，因此需要优化本地训练过程。

Method: 利用联邦学习固有的并行处理特性，采用贪婪随机搜索优化本地批量大小。

Result: 与默认参数设置相比，该方法提高了收敛速度，且与参数优化的情况几乎一致。

Conclusion: 通过优化本地批量大小，可以有效提升联邦学习中本地训练的效率。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [48] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 论文分析了联邦学习中恶意梯度泄漏攻击的局限性，并提出了一种轻量级的客户端检测机制来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的梯度更新可能无意中泄露客户端的敏感信息，尤其是在恶意服务器操纵全局模型的情况下。论文从防御者的角度出发，全面分析了这类攻击及其局限性。

Method: 通过研究恶意梯度泄漏攻击与模型操纵技术的核心权衡关系，提出了一种轻量级的客户端检测机制，用于在本地训练开始前标记可疑的模型更新。

Result: 研究发现，这种攻击在实际的联邦学习环境中既难以高效重构私有数据，又难以完全隐蔽；检测机制能够以极小开销有效防御攻击。

Conclusion: 恶意梯度泄漏攻击在实际中有限且可检测，提出的防御机制为隐私敏感的联邦学习系统提供了可行且易部署的保护方案。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [49] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode²是一种新型的级联码书框架，通过大规模、语义对齐的视觉标记化解决了现有方法中词汇量小或训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于码书的方法要么词汇量小，缺乏细粒度语义，要么简单扩展导致标记利用率低和训练不稳定。

Method: 结合SigLIP序列嵌入聚类构建500K条目码书，通过级联设计（冻结码书锚定嵌入空间，训练码书优化特定任务语义）确保稳定性和高利用率。

Result: UniCode²在多样化基准测试中表现优异，支持高质量视觉合成，并保持语义对齐和模块化。

Conclusion: UniCode²证实了在视觉标记空间扩展中保持稳定性、语义和模块化的可行性。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [50] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种基于失真不变特征学习的新型深度学习框架，用于鲁棒图像零水印技术，在不改变原始图像的情况下学习参考签名。


<details>
  <summary>Details</summary>
Motivation: 传统的数字水印技术通常会对原始图像进行修改，这可能影响图像质量。本文旨在通过零水印技术解决这一问题，同时提升对失真的鲁棒性。

Method: 框架包含两个模块：1) 通过噪声对抗学习训练特征提取器，生成对失真不变且语义丰富的表示；2) 设计基于学习的多比特零水印方案，将不变特征投影到优化的参考码上以匹配目标消息。

Result: 在多种图像数据集和失真情况下，该方法在特征稳定性和水印恢复方面均达到最先进的鲁棒性。

Conclusion: 该方法在泛化性和鲁棒性方面优于现有自监督和深度学习水印技术，验证了其有效性。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [51] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文比较了五种对象检测架构在三个历史文档数据集上的表现，发现模型性能受架构、数据集特性和边界框表示影响，OBB对非笛卡尔文档至关重要。


<details>
  <summary>Details</summary>
Motivation: 历史文档布局复杂，需要鲁棒的DLA方法以实现自动化处理和理解。

Method: 评估了三种YOLO变体和两种Transformer模型在三个数据集（e-NDP、CATMuS、HORAE）上的性能。

Result: Co-DETR在e-NDP上表现最佳，而YOLOv11x-OBB在复杂数据集CATMuS和HORAE上更优。

Conclusion: Transformer适合结构化布局，CNN-OBB模型对复杂文档泛化能力更强；OBB是非笛卡尔文档建模的必备条件。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [52] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA是一种新型的深度学习框架，通过消除块效应和聚合开放数据，有效提升了在压缩图像上检测深度伪造的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法忽视了压缩引入的块效应，且过度依赖原始图像，导致在实际应用中效果不佳。

Method: PLADA包含两个核心模块：块效应消除器（B2E）和开放数据聚合（ODA），分别处理块效应和利用无标记数据。

Result: 在26个数据集上的实验表明，PLADA在压缩图像上的检测性能显著优于现有方法。

Conclusion: PLADA不仅填补了深度伪造检测在压缩图像上的空白，还为开放世界场景提供了稳健的解决方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Main category: cs.IR

TL;DR: 提出了一种名为FemmIR的多媒体检索框架，无需标注数据即可实现跨模态查询，并通过编辑距离衡量样本间相似性。


<details>
  <summary>Details</summary>
Motivation: 避免传统方法中基于监督分类的检索任务带来的标注开销，利用预训练编码器实现跨模态检索。

Method: 基于编辑距离的弱监督方法，通过样本属性替换成本衡量相关性，并利用多级交互分数计算查询样本与数据样本的匹配度。

Result: 在MuQNOL数据集上测试，FemmIR在精确和近似相似性检索任务中表现与其他系统相当。

Conclusion: FemmIR能够在无标注数据的情况下有效实现跨模态检索，适用于标注稀缺的实际应用场景。

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [54] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/abs/2506.16116)
*Ignacio Hernández Montilla,Alfonso Medela,Paola Pasquali,Andy Aguilar,Taig Mac Carthy,Gerardo Fernández,Antonio Martorell,Enrique Onieva*

Main category: eess.IV

TL;DR: 该论文提出了一种跨领域训练的IQA模型，结合皮肤科和非皮肤科的IQA数据集，以解决远程皮肤病诊疗中图像质量评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 远程皮肤病诊疗（Teledermatology）中图像质量差是一个未解决的问题，影响远程诊疗的实用性。目前皮肤病领域的IQA研究缺乏且未能利用非皮肤病领域的最新进展。

Method: 通过创建新的皮肤病IQA数据库（Legit.Health-DIQA-Artificial），并结合非皮肤病IQA数据集，采用跨领域训练方法优化IQA模型。

Result: 跨领域训练克服了皮肤病IQA数据规模小的限制，提升了模型性能，更好地管理远程皮肤病诊疗中的图像质量。

Conclusion: 跨领域训练IQA模型在皮肤病领域具有潜力，能显著提升远程诊疗的图像质量评估效果。

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [55] [Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings](https://arxiv.org/abs/2506.20609)
*Ankit Shah,Rita Singh,Bhiksha Raj,Alexander Hauptmann*

Main category: cs.SD

TL;DR: 研究通过声学分析枪声录音，提出低成本枪击检测和枪支分类方法，基于机器学习框架（SVM和CNN），CNN优于SVM，但数据质量和环境噪音是挑战。


<details>
  <summary>Details</summary>
Motivation: 商业枪声检测系统成本高昂，研究探索使用普及设备（如手机）的声学数据，以降低检测成本并提升执法效率。

Method: 利用3459条枪声录音数据集，分析枪声的声学特征（如枪口爆炸和冲击波），并比较SVM和CNN的检测与分类性能。

Result: CNN在干净数据上mAP为0.58，优于SVM（0.39），但在噪音数据上效果下降（mAP 0.35）。

Conclusion: 研究展示了低成本枪击检测系统的潜力，但需解决数据质量和噪音问题，以实现实时部署。

Abstract: The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [56] [Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning](https://arxiv.org/abs/2506.20212)
*Andrea Bussolan,Oliver Avram,Andrea Pignata,Gianvito Urgese,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: 论文提出了一种基于联邦学习的框架，用于在保护隐私的前提下实现工人心理状态的个性化评估，并优化人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着工业5.0的发展，制造商在追求大规模定制的同时越来越重视工人的福祉。应力感知的人机协作（HRC）成为关键，机器人需要根据人类心理状态调整行为以提高协作效率和安全性。

Method: 论文提出了一种集成联邦学习（FL）的框架，利用生理信号（如EEG、ECG、EDA、EMG和呼吸）训练多模态模型来实时预测操作员的压力水平，从而实现机器人行为的实时调整。FL支持分布式设备端训练，确保数据隐私的同时提升模型泛化和个性化能力。

Result: 结果显示，FL方法在压力预测准确性上与集中式训练方法相当，同时显著提升了个人化效果，优化了工业环境中的人机交互。

Conclusion: 该框架推动了隐私保护的适应性机器人技术的发展，有助于提升智能制造业中员工的福祉。

Abstract: With the advent of Industry 5.0, manufacturers are increasingly prioritizing
worker well-being alongside mass customization. Stress-aware Human-Robot
Collaboration (HRC) plays a crucial role in this paradigm, where robots must
adapt their behavior to human mental states to improve collaboration fluency
and safety. This paper presents a novel framework that integrates Federated
Learning (FL) to enable personalized mental state evaluation while preserving
user privacy. By leveraging physiological signals, including EEG, ECG, EDA,
EMG, and respiration, a multimodal model predicts an operator's stress level,
facilitating real-time robot adaptation. The FL-based approach allows
distributed on-device training, ensuring data confidentiality while improving
model generalization and individual customization. Results demonstrate that the
deployment of an FL approach results in a global model with performance in
stress prediction accuracy comparable to a centralized training approach.
Moreover, FL allows for enhancing personalization, thereby optimizing
human-robot interaction in industrial settings, while preserving data privacy.
The proposed framework advances privacy-preserving, adaptive robotics to
enhance workforce well-being in smart manufacturing.

</details>


### [57] [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
*Ruben Janssens,Jens De Bock,Sofie Labat,Eva Verhelst,Veronique Hoste,Tony Belpaeme*

Main category: cs.RO

TL;DR: 研究评估机器学习模型在检测人机对话中的沟通错误效果，发现即使使用先进模型，性能仅略优于随机猜测，揭示用户在感知错误时未必向机器人传递反馈的局限。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互中的沟通有效性，维持用户信任和参与感。

Method: 使用240段包含四种对话失败的多模态数据集，结合计算机视觉模型和用户反馈评估模型性能。

Result: 模型识别沟通错误的性能接近随机，但在情感表达更丰富的数据集上表现较好；人类评分者也仅能识别约一半错误。

Conclusion: 研究揭示了识别机器人沟通错误的根本局限，建议设计对话时主动获取反馈以优化交互。

Abstract: Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [58] [Development of an Open-Source Spacecraft Bus for the PULSE-A CubeSat](https://arxiv.org/abs/2506.20014)
*Graydon Schulze-Kalt,Robert Pitu,Spencer Shelton,Catherine Todd,Zane Ebel,Ian Goldberg,Leon Gold,Henry Czarnecki,Mason McCormack,Larry Li,Zumi Riekse,Brian Yu,Akash Piya,Vidya Suri,Dylan Hu,Colleen Kim,John Baird,Seth Knights,Logan Hanssler,Michael Lembeck,Tian Zhong*

Main category: physics.app-ph

TL;DR: PULSE-A项目旨在验证圆偏振调制激光卫星通信的可行性，其低成本开源航天器总线设计满足任务需求，并便于未来任务的配置升级。


<details>
  <summary>Details</summary>
Motivation: 展示低成本、开源航天器总线的可行性，同时支持圆偏振调制激光通信的需求。

Method: 采用双BeagleBone Black Industrial计算单元，基于PC/104标准，使用Goddard Space Flight Center的核心飞行系统（cFS），采用C语言开发模块化软件架构。

Result: 成功设计并测试了满足PULSE-A任务需求的航天器总线，具备良好的热控和机械性能。

Conclusion: PULSE-A的总线设计为低成本、开源航天器提供了可扩展的解决方案，适用于未来类似任务。

Abstract: The undergraduate-led Polarization-modUlated Laser Satellite Experiment
(PULSE-A) at the University of Chicago seeks to demonstrate the feasibility of
circular polarization shift keyed satellite-to-ground laser communication.
PULSE-A's low-cost open-source bus serves as the backbone of the mission and
has been designed in tandem with the Payload, with design driven by strict
requirements for pointing accuracy, component alignment, power demand, and
thermal stability. This work presents the design and testing of the PULSE-A
bus.
  The spacecraft bus was designed to fill two major needs: (1) to meet the
requirements of the PULSE-A mission, and (2) to be easily configurable for
future missions that desire enhanced capabilities over other low-cost
open-source designs. At its core, the bus features dual BeagleBone Black
Industrial compute units, selected for their flight heritage, integrated via a
PC/104 header standard. PULSE-A implements Goddard Space Flight Center's core
Flight System (cFS), which takes a modular software architecture approach and
is built in C. The use of C as the primary language aligns with the expertise
of the University of Chicago's Computer Science department, allowing for ease
of development by PULSE-A's undergraduate flight software team.
  The CubeSat structure utilizes Gran Systems' 3U frame, modified to
accommodate openings for various ports and deployable components. Inside, the
avionics stack uses the PC/104 standard quad rails, which terminate in
PULSE-A's custom-designed Payload Box that houses all of the Payload components
and optical fiber runs. This work also covers the techniques and iterative
engineering processes used to develop the thermal control and dissipation
mechanisms for the specific requirements, under volume, mass, and
temperature-range constraints.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [RepuNet: A Reputation System for Mitigating Malicious Clients in DFL](https://arxiv.org/abs/2506.19892)
*Isaac Marroqui Penalva,Enrique Tomás Martínez Beltrán,Manuel Gil Pérez,Alberto Huertas Celdrán*

Main category: cs.CR

TL;DR: RepuNet是一种去中心化声誉系统，用于检测和缓解去中心化联邦学习中的恶意行为，通过动态评估节点行为并调整其影响，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖固定配置或额外基础设施，导致计算负担和适应性不足，提出RepuNet以应对这些挑战。

Method: RepuNet通过模型相似性、参数变化、消息延迟和通信量等指标动态评估节点行为，调整其在模型聚合中的影响力。

Result: 在MNIST和CIFAR-10数据集上测试，RepuNet的F1分数分别超过95%和约76%，表现出高效性和鲁棒性。

Conclusion: RepuNet展示了在去中心化联邦学习环境中自适应且实用的威胁缓解潜力。

Abstract: Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.

</details>


### [60] [Quantum-Resistant Domain Name System: A Comprehensive System-Level Study](https://arxiv.org/abs/2506.19943)
*Juyoul Lee,Sanzida Hoque,Abdullah Aydeger,Engin Zeydan*

Main category: cs.CR

TL;DR: 本文研究了量子计算对DNS安全性的威胁，提出了一个统一的PQC-DNS框架，评估了多种后量子密码方案在DNS中的性能和安全影响。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机的威胁日益逼近，确保DNS在后量子时代的机密性、真实性和完整性变得至关重要。

Method: 作者提出了PQC-DNS框架，集成了晶格和哈希基密码原语，并在BIND9和TLS 1.3中实现，通过实验评估了不同方案的性能和安全影响。

Result: 实验表明，晶格基方案（如MLKEM和Falcon）具有较低的延迟和资源开销，而哈希基方案（如SPHINCS+）带来了显著的消息大小和处理开销增加。

Conclusion: 研究结果为实际部署量子安全DNS提供了指导，并为未来核心互联网协议的安全改进做出了贡献。

Abstract: The Domain Name System (DNS) plays a foundational role in Internet
infrastructure, yet its core protocols remain vulnerable to compromise by
quantum adversaries. As cryptographically relevant quantum computers become a
realistic threat, ensuring DNS confidentiality, authenticity, and integrity in
the post-quantum era is imperative. In this paper, we present a comprehensive
system-level study of post-quantum DNS security across three widely deployed
mechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose
Post-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS
security under legacy, post-quantum, and hybrid cryptographic configurations.
Our implementation leverages the Open Quantum Safe (OQS) libraries and
integrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We
formalize performance and threat models and analyze the impact of post-quantum
key encapsulation and digital signatures on end-to-end DNS resolution.
Experimental results on a containerized testbed reveal that lattice-based
primitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and
Falcon offer practical latency and resource profiles, while hash-based schemes
like SPHINCS+ significantly increase message sizes and processing overhead. We
also examine security implications including downgrade risks, fragmentation
vulnerabilities, and susceptibility to denial-of-service amplification. Our
findings inform practical guidance for deploying quantum-resilient DNS and
contribute to the broader effort of securing core Internet protocols for the
post-quantum future.

</details>


### [61] [Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing](https://arxiv.org/abs/2506.20000)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: Guardian-FC是一种新颖的双层框架，用于隐私保护的联邦计算，统一了多种隐私保护机制的安全性。


<details>
  <summary>Details</summary>
Motivation: 不同隐私保护机制（如加密和统计技术）的安全性执行需要统一和模块化，以提高灵活性和可扩展性。

Method: 通过DSL和插件执行解耦安全护栏与隐私机制，支持后端中立的计算单元和可交换的执行提供程序。

Result: 实现了后端无关的安全性和形式化验证基础，支持快速失败的任务准入和新后端的无缝扩展。

Conclusion: Guardian-FC为隐私保护的联邦计算提供了模块化和可扩展的解决方案，并提出了未来研究方向。

Abstract: We propose Guardian-FC, a novel two-layer framework for privacy preserving
federated computing that unifies safety enforcement across diverse privacy
preserving mechanisms, including cryptographic back-ends like fully homomorphic
encryption (FHE) and multiparty computation (MPC), as well as statistical
techniques such as differential privacy (DP). Guardian-FC decouples guard-rails
from privacy mechanisms by executing plug-ins (modular computation units),
written in a backend-neutral, domain-specific language (DSL) designed
specifically for federated computing workflows and interchangeable Execution
Providers (EPs), which implement DSL operations for various privacy back-ends.
An Agentic-AI control plane enforces a finite-state safety loop through signed
telemetry and commands, ensuring consistent risk management and auditability.
The manifest-centric design supports fail-fast job admission and seamless
extensibility to new privacy back-ends. We present qualitative scenarios
illustrating backend-agnostic safety and a formal model foundation for
verification. Finally, we outline a research agenda inviting the community to
advance adaptive guard-rail tuning, multi-backend composition, DSL
specification development, implementation, and compiler extensibility alongside
human-override usability.

</details>


### [62] [Generative AI for Vulnerability Detection in 6G Wireless Networks: Advances, Case Study, and Future Directions](https://arxiv.org/abs/2506.20488)
*Shuo Yang,Xinran Zheng,Jinfeng Xu,Jinze Li,Danyang Song,Zheyu Chen,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: 论文探讨了在6G无线网络中利用生成式AI（GAI）增强漏洞检测，提出一个三层框架，并通过案例研究展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着6G、物联网和边缘计算的发展，传统安全方法难以应对动态网络环境中的新型威胁，需要更智能的检测机制。

Method: 论文提出一个包含技术层、能力层和应用层的三层框架，利用VAEs、GANs、LLMs和GDMs等技术分析漏洞检测。

Result: 通过LLM驱动的代码漏洞检测案例研究，证明了GAI在安全领域的有效性，但需解决轻量化、数据真实性等问题。

Conclusion: 论文总结了GAI在6G网络安全中的潜力，并指出了未来研究方向，如隐私保护技术和外部知识整合。

Abstract: The rapid advancement of 6G wireless networks, IoT, and edge computing has
significantly expanded the cyberattack surface, necessitating more intelligent
and adaptive vulnerability detection mechanisms. Traditional security methods,
while foundational, struggle with zero-day exploits, adversarial threats, and
context-dependent vulnerabilities in highly dynamic network environments.
Generative AI (GAI) emerges as a transformative solution, leveraging synthetic
data generation, multimodal reasoning, and adaptive learning to enhance
security frameworks. This paper explores the integration of GAI-powered
vulnerability detection in 6G wireless networks, focusing on code auditing,
protocol security, cloud-edge defenses, and hardware protection. We introduce a
three-layer framework comprising the Technology Layer, Capability Layer, and
Application Layer to systematically analyze the role of VAEs, GANs, LLMs, and
GDMs in securing next-generation wireless ecosystems. To demonstrate practical
implementation, we present a case study on LLM-driven code vulnerability
detection, highlighting its effectiveness, performance, and challenges.
Finally, we outline future research directions, including lightweight models,
high-authenticity data generation, external knowledge integration, and
privacy-preserving technologies. By synthesizing current advancements and open
challenges, this work provides a roadmap for researchers and practitioners to
harness GAI for building resilient and adaptive security solutions in 6G
networks.

</details>


### [63] [Anti-Phishing Training Does Not Work: A Large-Scale Empirical Assessment of Multi-Modal Training Grounded in the NIST Phish Scale](https://arxiv.org/abs/2506.19899)
*Andrew T. Rozema,James C. Davis*

Main category: cs.CR

TL;DR: 研究表明钓鱼邮件培训效果有限，强调了多层次防御和技术改进的重要性。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击是重大网络安全威胁，但培训效果存疑，需要实证支持投资决策。

Method: 大型研究比较讲座式、互动式培训与对照组的钓鱼邮件点击率，采用NIST Phish Scale评估钓鱼难度。

Result: 培训无显著效果（点击率7%-15%随难度增加），效果量低于0.01。

Conclusion: 钓鱼培训效果不佳，应转向多层次防御和技术改进，减少对人力的依赖。

Abstract: Social engineering attacks using email, commonly known as phishing, are a
critical cybersecurity threat. Phishing attacks often lead to operational
incidents and data breaches. As a result, many organizations allocate a
substantial portion of their cybersecurity budgets to phishing awareness
training, driven in part by compliance requirements. However, the effectiveness
of this training remains in dispute. Empirical evidence of training
(in)effectiveness is essential for evidence-based cybersecurity investment and
policy development. Despite recent measurement studies, two critical gaps
remain in the literature:
  (1) we lack a validated measure of phishing lure difficulty, and
  (2) there are few comparisons of different types of training in real-world
business settings.
  To fill these gaps, we conducted a large-scale study ($N = 12{,}511$) of
phishing effectiveness at a US-based financial technology (``fintech'') firm.
Our two-factor design compared the effect of treatments (lecture-based,
interactive, and control groups) on subjects' susceptibility to phishing lures
of varying complexity (using the NIST Phish Scale). The NIST Phish Scale
successfully predicted behavior (click rates: 7.0\% easy to 15.0\% hard emails,
p $<$ 0.001), but training showed no significant main effects on clicks (p =
0.450) or reporting (p = 0.417). Effect sizes remained below 0.01, indicating
little practical value in any of the phishing trainings we deployed. Our
results add to the growing evidence that phishing training is ineffective,
reinforcing the importance of phishing defense-in-depth and the merit of
changes to processes and technology to reduce reliance on humans, as well as
rebuking the training costs necessitated by regulatory requirements.

</details>
