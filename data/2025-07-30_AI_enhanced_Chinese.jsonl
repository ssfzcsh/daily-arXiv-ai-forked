{"id": "2507.21271", "pdf": "https://arxiv.org/pdf/2507.21271", "abs": "https://arxiv.org/abs/2507.21271", "authors": ["Zhaorui Yang", "Yuxin Qiu", "Haichao Zhu", "Qian Zhang"], "title": "Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement", "categories": ["cs.SE", "K.6.3"], "comment": "ICSME 2025 Registered Reports", "summary": "[Context] Modern AI applications increasingly process highly structured data,\nsuch as 3D meshes and point clouds, where test input generation must preserve\nboth structural and semantic validity. However, existing fuzzing tools and\ninput generators are typically handcrafted for specific input types and often\ngenerate invalid inputs that are subsequently discarded, leading to\ninefficiency and poor generalizability. [Objective] This study investigates\nwhether test inputs for structured domains can be unified through a graph-based\nrepresentation, enabling general, reusable mutation strategies while enforcing\nstructural constraints. We will evaluate the effectiveness of this approach in\nenhancing input validity and semantic preservation across eight AI systems.\n[Method] We develop and evaluate GRAphRef, a graph-based test input generation\nframework that supports constraint-based mutation and refinement. GRAphRef maps\nstructured inputs to graphs, applies neighbor-similarity-guided mutations, and\nuses a constraint-refinement phase to repair invalid inputs. We will conduct a\nconfirmatory study across eight real-world mesh-processing AI systems,\ncomparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.\nEvaluation metrics include structural validity, semantic preservation (via\nprediction consistency), and performance overhead. Experimental data is derived\nfrom ShapeNetCore mesh seeds and model outputs from systems like MeshCNN and\nHodgeNet. Statistical analysis and component latency breakdowns will be used to\nassess each hypothesis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u7edf\u4e00\u6d4b\u8bd5\u8f93\u5165\u751f\u6210\u6846\u67b6GRAphRef\uff0c\u4ee5\u89e3\u51b3\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u4e2d\u7684\u8f93\u5165\u6709\u6548\u6027\u548c\u8bed\u4e49\u4fdd\u7559\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3AI\u5e94\u7528\u5904\u7406\u9ad8\u5ea6\u7ed3\u6784\u5316\u6570\u636e\uff08\u59823D\u7f51\u683c\u548c\u70b9\u4e91\uff09\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u751f\u6210\u7684\u8f93\u5165\u5e38\u65e0\u6548\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "GRAphRef\u5c06\u7ed3\u6784\u5316\u8f93\u5165\u6620\u5c04\u4e3a\u56fe\uff0c\u57fa\u4e8e\u90bb\u57df\u76f8\u4f3c\u6027\u8fdb\u884c\u53d8\u5f02\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u7ec6\u5316\u4fee\u590d\u65e0\u6548\u8f93\u5165\uff0c\u57288\u4e2a\u771f\u5b9eAI\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u6846\u67b6\u5728\u8f93\u5165\u6709\u6548\u6027\u548c\u8bed\u4e49\u4fdd\u7559\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u6bd4AFL\u7b49\u5de5\u5177\u5177\u6709\u4f18\u52bf\uff0c\u5b9e\u9a8c\u6570\u636e\u6765\u81eaShapeNetCore\u548c\u591a\u4e2a\u6a21\u578b\u8f93\u51fa\u3002", "conclusion": "GRAphRef\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u6d4b\u8bd5\u8f93\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2507.21280", "pdf": "https://arxiv.org/pdf/2507.21280", "abs": "https://arxiv.org/abs/2507.21280", "authors": ["Courtney Miller", "Rudrajit Choudhuri", "Mara Ulloa", "Sankeerti Haniyur", "Robert DeLine", "Margaret-Anne Storey", "Emerson Murphy-Hill", "Christian Bird", "Jenna L. Butler"], "title": "\"Maybe We Need Some More Examples:\" Individual and Team Drivers of Developer GenAI Tool Use", "categories": ["cs.SE"], "comment": null, "summary": "Despite the widespread availability of generative AI tools in software\nengineering, developer adoption remains uneven. This unevenness is problematic\nbecause it hampers productivity efforts, frustrates management's expectations,\nand creates uncertainty around the future roles of developers. Through paired\ninterviews with 54 developers across 27 teams -- one frequent and one\ninfrequent user per team -- we demonstrate that differences in usage result\nprimarily from how developers perceive the tool (as a collaborator vs.\nfeature), their engagement approach (experimental vs. conservative), and how\nthey respond when encountering challenges (with adaptive persistence vs. quick\nabandonment). Our findings imply that widespread organizational expectations\nfor rapid productivity gains without sufficient investment in learning support\ncreates a \"Productivity Pressure Paradox,\" undermining the very productivity\nbenefits that motivate adoption.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc754\u540d\u5f00\u53d1\u8005\u7684\u8bbf\u8c08\u53d1\u73b0\uff0c\u5bf9\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u91c7\u7eb3\u4e0d\u5747\u4e3b\u8981\u6e90\u4e8e\u5f00\u53d1\u8005\u5bf9\u5176\u8ba4\u77e5\uff08\u5408\u4f5c\u8005vs.\u529f\u80fd\uff09\u3001\u4f7f\u7528\u65b9\u5f0f\uff08\u5b9e\u9a8c\u6027vs.\u4fdd\u5b88\uff09\u548c\u6311\u6218\u5e94\u5bf9\uff08\u9002\u5e94\u6027\u575a\u6301vs.\u5feb\u901f\u653e\u5f03\uff09\uff0c\u7ec4\u7ec7\u671f\u671b\u5feb\u901f\u63d0\u5347\u751f\u4ea7\u529b\u4f46\u7f3a\u4e4f\u5b66\u4e60\u652f\u6301\u4f1a\u5f62\u6210\u201c\u751f\u4ea7\u529b\u538b\u529b\u6096\u8bba\u201d\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u5f00\u53d1\u4e2d\u7684\u91c7\u7eb3\u4e0d\u5747\uff0c\u5bfc\u81f4\u751f\u4ea7\u529b\u63d0\u5347\u53d7\u963b\uff0c\u7ba1\u7406\u671f\u671b\u53d7\u632b\uff0c\u5f00\u53d1\u8005\u672a\u6765\u89d2\u8272\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\uff0c\u9700\u63a2\u7a76\u539f\u56e0\u3002", "method": "\u901a\u8fc727\u4e2a\u56e2\u961f\u768454\u540d\u5f00\u53d1\u8005\u914d\u5bf9\u8bbf\u8c08\uff08\u6bcf\u961f\u4e00\u540d\u9891\u7e41\u548c\u4e00\u540d\u4e0d\u9891\u7e41\u7528\u6237\uff09\uff0c\u5206\u6790\u4f7f\u7528\u5dee\u5f02\u7684\u6839\u6e90\u3002", "result": "\u4f7f\u7528\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u5f00\u53d1\u8005\u5bf9\u5de5\u5177\u7684\u8ba4\u77e5\u3001\u4f7f\u7528\u65b9\u5f0f\u548c\u6311\u6218\u5e94\u5bf9\u7b56\u7565\u7684\u4e0d\u540c\uff1b\u7ec4\u7ec7\u671f\u671b\u8fc7\u9ad8\u4f46\u5b66\u4e60\u652f\u6301\u4e0d\u8db3\u53cd\u800c\u963b\u788d\u751f\u4ea7\u529b\u63d0\u5347\u3002", "conclusion": "\u7ec4\u7ec7\u9700\u5e73\u8861\u751f\u4ea7\u529b\u671f\u671b\u4e0e\u5b66\u4e60\u652f\u6301\uff0c\u907f\u514d\u201c\u751f\u4ea7\u529b\u538b\u529b\u6096\u8bba\u201d\uff0c\u624d\u80fd\u771f\u6b63\u5b9e\u73b0\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u6548\u76ca\u3002"}}
{"id": "2507.21318", "pdf": "https://arxiv.org/pdf/2507.21318", "abs": "https://arxiv.org/abs/2507.21318", "authors": ["Yeshayahu Weiss", "Gal Amram", "Achiya Elyasaf", "Eitan Farchi", "Oded Margalit", "Gera Weiss"], "title": "Black-Box Bug-Amplification for Multithreaded Software", "categories": ["cs.SE"], "comment": "35 pages, 5 figurs, 4 listing and 3 tables", "summary": "Bugs, especially those in concurrent systems, are often hard to reproduce\nbecause they manifest only under rare conditions. Testers frequently encounter\nfailures that occur only under specific inputs, even when occurring with low\nprobability. We propose an approach to systematically amplify the occurrence of\nsuch elusive bugs. We treat the system under test as a black-box and use\nrepeated trial executions to train a predictive model that estimates the\nprobability of a given input configuration triggering a bug. We evaluate this\napproach on a dataset of 17 representative concurrency bugs spanning diverse\ncategories. Several model-based search techniques are compared against a\nbrute-force random sampling baseline. Our results show that an ensemble of\nregression models can significantly increase bug occurrence rates across nearly\nall scenarios, often achieving an order-of-magnitude improvement over random\nsampling. The contributions of this work include: (i) a novel formulation of\nbug-amplification as a rare-event regression problem; (ii) an empirical\nevaluation of multiple techniques for amplifying bug occurrence, demonstrating\nthe effectiveness of model-guided search; and (iii) a practical, non-invasive\ntesting framework that helps practitioners expose hidden concurrency faults\nwithout altering the internal system architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u6765\u7cfb\u7edf\u6027\u5730\u653e\u5927\u5e76\u53d1\u7cfb\u7edf\u4e2d\u7f55\u89c1\u9519\u8bef\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9519\u8bef\u590d\u73b0\u7387\u3002", "motivation": "\u5e76\u53d1\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u5f80\u5f80\u96be\u4ee5\u590d\u73b0\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ec5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u51fa\u73b0\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u7cfb\u7edf\u89c6\u4e3a\u9ed1\u76d2\uff0c\u901a\u8fc7\u91cd\u590d\u8bd5\u9a8c\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\uff0c\u8bc4\u4f30\u591a\u79cd\u641c\u7d22\u6280\u672f\u3002", "result": "\u56de\u5f52\u6a21\u578b\u96c6\u6210\u80fd\u663e\u8457\u63d0\u9ad8\u9519\u8bef\u590d\u73b0\u7387\uff0c\u6bd4\u968f\u673a\u91c7\u6837\u6548\u679c\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u653e\u5927\u5e76\u53d1\u9519\u8bef\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u975e\u4fb5\u5165\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2507.21329", "pdf": "https://arxiv.org/pdf/2507.21329", "abs": "https://arxiv.org/abs/2507.21329", "authors": ["Saikat Mondal", "Chanchal K. Roy"], "title": "Does Editing Improve Answer Quality on Stack Overflow? A Data-Driven Investigation", "categories": ["cs.SE"], "comment": "Accepted in the 41st International Conference on Software Maintenance\n  and Evolution (ICSME 2025 - Research Track)", "summary": "High-quality answers in technical Q&A platforms like Stack Overflow (SO) are\ncrucial as they directly influence software development practices. Poor-quality\nanswers can introduce inefficiencies, bugs, and security vulnerabilities, and\nthus increase maintenance costs and technical debt in production software. To\nimprove content quality, SO allows collaborative editing, where users revise\nanswers to enhance clarity, correctness, and formatting. Several studies have\nexamined rejected edits and identified the causes of rejection. However, prior\nresearch has not systematically assessed whether accepted edits enhance key\nquality dimensions. While one study investigated the impact of edits on C/C++\nvulnerabilities, broader quality aspects remain unexplored. In this study, we\nanalyze 94,994 Python-related answers that have at least one accepted edit to\ndetermine whether edits improve (1) semantic relevance, (2) code usability, (3)\ncode complexity, (4) security vulnerabilities, (5) code optimization, and (6)\nreadability. Our findings show both positive and negative effects of edits.\nWhile 53.3% of edits improve how well answers match questions, 38.1% make them\nless relevant. Some previously broken code (9%) becomes executable, yet working\ncode (14.7%) turns non-parsable after edits. Many edits increase complexity\n(32.3%), making code harder to maintain. Instead of fixing security issues,\n20.5% of edits introduce additional issues. Even though 51.0% of edits optimize\nperformance, execution time still increases overall. Readability also suffers,\nas 49.7% of edits make code harder to read. This study highlights the\ninconsistencies in editing outcomes and provides insights into how edits impact\nsoftware maintainability, security, and efficiency that might caution users and\nmoderators and help future improvements in collaborative editing systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Stack Overflow\u4e0a\u7684\u534f\u540c\u7f16\u8f91\u5728\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u65b9\u9762\u6548\u679c\u4e0d\u4e00\uff0c\u65e2\u6709\u79ef\u6781\u4e5f\u6709\u6d88\u6781\u5f71\u54cd\u3002", "motivation": "\u6280\u672f\u95ee\u7b54\u5e73\u53f0\u7684\u9ad8\u8d28\u91cf\u56de\u7b54\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u672a\u7cfb\u7edf\u8bc4\u4f30\u7f16\u8f91\u662f\u5426\u63d0\u5347\u5173\u952e\u8d28\u91cf\u7ef4\u5ea6\u3002", "method": "\u5206\u6790\u4e8694,994\u6761Python\u76f8\u5173\u7b54\u6848\u7684\u7f16\u8f91\u6548\u679c\uff0c\u8bc4\u4f30\u4e86\u8bed\u4e49\u76f8\u5173\u6027\u3001\u4ee3\u7801\u53ef\u7528\u6027\u7b49\u516d\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u3002", "result": "53.3%\u7684\u7f16\u8f91\u63d0\u5347\u4e86\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u4f4638.1%\u964d\u4f4e\u4e86\uff1b\u90e8\u5206\u7f16\u8f91\u5bfc\u81f4\u4ee3\u7801\u590d\u6742\u5ea6\u589e\u52a0\u6216\u5b89\u5168\u6027\u95ee\u9898\uff0c\u4ec551.0%\u4f18\u5316\u4e86\u6027\u80fd\u3002", "conclusion": "\u7f16\u8f91\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u63d0\u9192\u7528\u6237\u548c\u5e73\u53f0\u6ce8\u610f\u5176\u5bf9\u8f6f\u4ef6\u7ef4\u62a4\u6027\u3001\u5b89\u5168\u6027\u548c\u6548\u7387\u7684\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2507.21395", "pdf": "https://arxiv.org/pdf/2507.21395", "abs": "https://arxiv.org/abs/2507.21395", "authors": ["Zeyu Deng", "Yanhui Lu", "Jiashu Liao", "Shuang Wu", "Chongfeng Wei"], "title": "Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for enabling emotionally\nintelligent systems that perceive and respond to human emotions. However,\nexisting methods suffer from limited cross-modal interaction and imbalanced\ncontributions across modalities. To address these issues, we propose Sync-TVA,\nan end-to-end graph-attention framework featuring modality-specific dynamic\nenhancement and structured cross-modal fusion. Our design incorporates a\ndynamic enhancement module for each modality and constructs heterogeneous\ncross-modal graphs to model semantic relations across text, audio, and visual\nfeatures. A cross-attention fusion mechanism further aligns multimodal cues for\nrobust emotion inference. Experiments on MELD and IEMOCAP demonstrate\nconsistent improvements over state-of-the-art models in both accuracy and\nweighted F1 score, especially under class-imbalanced conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSync-TVA\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u56fe\u6ce8\u610f\u529b\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u4e0d\u8db3\u548c\u8d21\u732e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u8de8\u6a21\u6001\u4ea4\u4e92\u4e0d\u8db3\u548c\u6a21\u6001\u8d21\u732e\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Sync-TVA\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u6001\u7279\u5f02\u6027\u52a8\u6001\u589e\u5f3a\u6a21\u5757\u3001\u5f02\u6784\u8de8\u6a21\u6001\u56fe\u6784\u5efa\u548c\u8de8\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u3002", "result": "\u5728MELD\u548cIEMOCAP\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u52a0\u6743F1\u5206\u6570\uff0c\u5c24\u5176\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u3002", "conclusion": "Sync-TVA\u901a\u8fc7\u589e\u5f3a\u6a21\u6001\u4ea4\u4e92\u548c\u5e73\u8861\u8d21\u732e\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.21288", "pdf": "https://arxiv.org/pdf/2507.21288", "abs": "https://arxiv.org/abs/2507.21288", "authors": ["Guanxiong Chen", "Shashwat Suri", "Yuhao Wu", "Etienne Voulga", "David I. W. Levin", "Dinesh Pai"], "title": "Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "Materials used in real clothing exhibit remarkable complexity and spatial\nvariation due to common processes such as stitching, hemming, dyeing, printing,\npadding, and bonding. Simulating these materials, for instance using finite\nelement methods, is often computationally demanding and slow. Worse, such\nmethods can suffer from numerical artifacts called ``membrane locking'' that\nmakes cloth appear artificially stiff. Here we propose a general framework,\ncalled Mass-Spring Net, for learning a simple yet efficient surrogate model\nthat captures the effects of these complex materials using only motion\nobservations. The cloth is discretized into a mass-spring network with unknown\nmaterial parameters that are learned directly from the motion data, using a\nnovel force-and-impulse loss function. Our approach demonstrates the ability to\naccurately model spatially varying material properties from a variety of data\nsources, and immunity to membrane locking which plagues FEM-based simulations.\nCompared to graph-based networks and neural ODE-based architectures, our method\nachieves significantly faster training times, higher reconstruction accuracy,\nand improved generalization to novel dynamic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMass-Spring Net\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u8fd0\u52a8\u89c2\u6d4b\u6570\u636e\u6765\u6784\u5efa\u9ad8\u6548\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u4ee5\u6a21\u62df\u590d\u6742\u5e03\u6599\u6750\u6599\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u819c\u9501\u5b9a\u95ee\u9898\u3002", "motivation": "\u771f\u5b9e\u5e03\u6599\u6750\u6599\u56e0\u7f1d\u5408\u3001\u67d3\u8272\u7b49\u8fc7\u7a0b\u8868\u73b0\u51fa\u590d\u6742\u6027\u548c\u7a7a\u95f4\u53d8\u5316\uff0c\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u6613\u53d7\u819c\u9501\u5b9a\u5f71\u54cd\uff0c\u4e9f\u9700\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5c06\u5e03\u6599\u79bb\u6563\u5316\u4e3a\u8d28\u91cf-\u5f39\u7c27\u7f51\u7edc\uff0c\u5229\u7528\u65b0\u7684\u529b-\u51b2\u91cf\u635f\u5931\u51fd\u6570\u4ece\u8fd0\u52a8\u6570\u636e\u4e2d\u5b66\u4e60\u672a\u77e5\u6750\u6599\u53c2\u6570\u3002", "result": "\u80fd\u591f\u51c6\u786e\u6a21\u62df\u7a7a\u95f4\u53d8\u5316\u7684\u6750\u6599\u7279\u6027\uff0c\u907f\u514d\u819c\u9501\u5b9a\u95ee\u9898\uff0c\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3001\u91cd\u5efa\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u57fa\u4e8e\u56fe\u548c\u795e\u7ecfODE\u7684\u65b9\u6cd5\u3002", "conclusion": "Mass-Spring Net\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5e03\u6599\u6750\u6599\u7684\u52a8\u6001\u6a21\u62df\uff0c\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.21070", "pdf": "https://arxiv.org/pdf/2507.21070", "abs": "https://arxiv.org/abs/2507.21070", "authors": ["Vladislav Li", "Ilias Siniosoglou", "Panagiotis Sarigiannidis", "Vasileios Argyriou"], "title": "Enhancing Manufacturing Training Through VR Simulations", "categories": ["cs.HC"], "comment": null, "summary": "In contemporary training for industrial manufacturing, reconciling\ntheoretical knowledge with practical experience continues to be a significant\ndifficulty. As companies transition to more intricate and technology-oriented\nsettings, conventional training methods frequently inadequately equip workers\nwith essential practical skills while maintaining safety and efficiency.\nVirtual Reality has emerged as a transformational instrument to tackle this\nissue by providing immersive, interactive, and risk-free teaching experiences.\nThrough the simulation of authentic industrial environments, virtual reality\nfacilitates the acquisition of vital skills for trainees within a regulated and\nstimulating context, therefore mitigating the hazards linked to experiential\nlearning in the workplace. This paper presents a sophisticated VR-based\nindustrial training architecture aimed at improving learning efficacy via\nhigh-fidelity simulations, dynamic and context-sensitive scenarios, and\nadaptive feedback systems. The suggested system incorporates intuitive\ngesture-based controls, reducing the learning curve for users across all skill\nlevels. A new scoring metric, namely, VR Training Scenario Score (VRTSS), is\nused to assess trainee performance dynamically, guaranteeing ongoing engagement\nand incentive. The experimental assessment of the system reveals promising\noutcomes, with significant enhancements in information retention, task\nexecution precision, and overall training efficacy. The results highlight the\ncapability of VR as a crucial instrument in industrial training, providing a\nscalable, interactive, and efficient substitute for conventional learning\nmethods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVR\u7684\u5de5\u4e1a\u57f9\u8bad\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u52a8\u6001\u573a\u666f\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u4fe1\u606f\u4fdd\u7559\u548c\u6267\u884c\u7cbe\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u5236\u9020\u57f9\u8bad\u4e2d\u7406\u8bba\u4e0e\u5b9e\u8df5\u8131\u8282\u7684\u95ee\u9898\uff0c\u5229\u7528VR\u63d0\u4f9b\u6c89\u6d78\u5f0f\u3001\u5b89\u5168\u7684\u57f9\u8bad\u4f53\u9a8c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aVR\u57f9\u8bad\u7cfb\u7edf\uff0c\u5305\u542b\u9ad8\u4fdd\u771f\u6a21\u62df\u3001\u52a8\u6001\u573a\u666f\u3001\u81ea\u9002\u5e94\u53cd\u9988\u548c\u624b\u52bf\u63a7\u5236\uff0c\u5e76\u4f7f\u7528VRTSS\u8bc4\u5206\u52a8\u6001\u8bc4\u4f30\u5b66\u5458\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u4fdd\u7559\u3001\u4efb\u52a1\u6267\u884c\u7cbe\u5ea6\u548c\u57f9\u8bad\u6548\u7387\u3002", "conclusion": "VR\u662f\u5de5\u4e1a\u57f9\u8bad\u4e2d\u7684\u5173\u952e\u5de5\u5177\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4e92\u52a8\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.21895", "pdf": "https://arxiv.org/pdf/2507.21895", "abs": "https://arxiv.org/abs/2507.21895", "authors": ["Shengcai Zhou", "Luping Xiang", "Kun Yang", "Kai Kit Wong", "Dapeng Oliver Wu", "Chan-Byoung Chae"], "title": "Beamforming-based Achievable Rate Maximization in ISAC System for Multi-UAV Networking", "categories": ["cs.PF"], "comment": null, "summary": "Airborne mobile Integrated Sensing and Communication (ISAC) base stations\nhave garnered significant attention recently, with ISAC technology being a\ncrucial application for 6G networks. Since ISAC can sense potential mobile\ncommunication users, this paper studies an effective scheme for a multi-UAV\nnetwork tailored for emergency communication. In this paper, we develop a\ntemporal-assisted frame structure utilizing integrated omnidirectional and\ndirectional beampattern to facilitate efficient and frequent searching, with\nextended Kalman filtering (EKF) as an aid to beam alignment. Further, we\naddress an optimization problem to maximize the total achievable rate per slot\nby jointly designing UAV beamforming, load management, and UAV direction\nplanning, all while adhering to the constraints of the predicted beam coverage.\nGiven the problem NP-hard, we introduce three robust mechanisms for its\nresolution: an enhanced distributed Successive Convex Approximation\n(SCA)-Iterative Rank Minimization (IRM) algorithm, an coalition game approach,\nand a Fermat point search method. In particular, the proposed SCA-IRM algorithm\ndecomposes the original complex optimization problem into several sub-problems\nand assigns them equally to each UAV, so as to realize distributed computing\nand improve computational efficiency. Our proposed simulations demonstrate the\nimproved system performance in terms of communication rate, fairness, and\nsensing accuracy, providing design guidelines of UAV-assisted emergency\ncommunication networking.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u591a\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u5e94\u6025\u901a\u4fe1\u65b9\u6848\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u8f85\u52a9\u5e27\u7ed3\u6784\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u901a\u4fe1\u901f\u7387\u548c\u611f\u77e5\u7cbe\u5ea6\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u4e2dISAC\u6280\u672f\u7684\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u57fa\u7ad9\u5728\u5e94\u6025\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u6548\u641c\u7d22\u548c\u901a\u4fe1\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u65f6\u95f4\u8f85\u52a9\u5e27\u7ed3\u6784\u548cEKF\u8f85\u52a9\u6ce2\u675f\u5bf9\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u673a\u5236\uff08SCA-IRM\u7b97\u6cd5\u3001\u8054\u76df\u535a\u5f08\u6cd5\u548c\u8d39\u9a6c\u70b9\u641c\u7d22\u6cd5\uff09\u6765\u4f18\u5316\u65e0\u4eba\u673a\u6ce2\u675f\u6210\u5f62\u548c\u65b9\u5411\u89c4\u5212\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u5728\u901a\u4fe1\u901f\u7387\u3001\u516c\u5e73\u6027\u548c\u611f\u77e5\u7cbe\u5ea6\u65b9\u9762\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u5e94\u6025\u901a\u4fe1\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u6307\u5357\u3002"}}
{"id": "2507.21200", "pdf": "https://arxiv.org/pdf/2507.21200", "abs": "https://arxiv.org/abs/2507.21200", "authors": ["Soren Pedersen", "Sanyam Jain", "Mikkel Chavez", "Viktor Ladehoff", "Bruna Neves de Freitas", "Ruben Pauwels"], "title": "PanoGAN A Deep Generative Model for Panoramic Dental Radiographs", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "comment": null, "summary": "This paper presents the development of a generative adversarial network (GAN)\nfor synthesizing dental panoramic radiographs. Although exploratory in nature,\nthe study aims to address the scarcity of data in dental research and\neducation. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss\nwith gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying\nquality. The focus was on the dentoalveolar regions, other anatomical\nstructures were cropped out. Extensive preprocessing and data cleaning were\nperformed to standardize the inputs while preserving anatomical variability. We\nexplored four candidate models by varying critic iterations, feature depth, and\nthe use of denoising prior to training. A clinical expert evaluated the\ngenerated radiographs based on anatomical visibility and realism, using a\n5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical\ndepiction, although some were degraded by artifacts. A trade-off was observed\nthe model trained on non-denoised data yielded finer details especially in\nstructures like the mandibular canal and trabecular bone, while a model trained\non denoised data offered superior overall image clarity and sharpness. These\nfindings provide a foundation for future work on GAN-based methods in dental\nimaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7528\u4e8e\u5408\u6210\u7259\u79d1\u5168\u666fX\u5149\u7247\uff0c\u65e8\u5728\u89e3\u51b3\u7259\u79d1\u7814\u7a76\u548c\u6559\u80b2\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7GAN\u751f\u6210\u7259\u79d1\u5168\u666fX\u5149\u7247\uff0c\u4ee5\u7f13\u89e3\u7259\u79d1\u7814\u7a76\u548c\u6559\u80b2\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528Wasserstein\u635f\u5931\u548c\u68af\u5ea6\u60e9\u7f5a\uff08WGANGP\uff09\u8bad\u7ec3\u6df1\u5ea6\u5377\u79efGAN\uff08DCGAN\uff09\uff0c\u5bf92322\u5f20\u4e0d\u540c\u8d28\u91cf\u7684X\u5149\u7247\u8fdb\u884c\u9884\u5904\u7406\u548c\u6570\u636e\u6e05\u6d17\uff0c\u5e76\u63a2\u7d22\u4e86\u56db\u79cd\u5019\u9009\u6a21\u578b\u3002", "result": "\u751f\u6210\u7684\u5f71\u54cd\u5728\u89e3\u5256\u53ef\u89c6\u6027\u548c\u771f\u5b9e\u6027\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u90e8\u5206\u56fe\u50cf\u5b58\u5728\u4f2a\u5f71\u3002\u672a\u53bb\u566a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u7ec6\u8282\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u53bb\u566a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6574\u4f53\u6e05\u6670\u5ea6\u548c\u9510\u5ea6\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u672a\u6765\u57fa\u4e8eGAN\u7684\u7259\u79d1\u6210\u50cf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.21056", "pdf": "https://arxiv.org/pdf/2507.21056", "abs": "https://arxiv.org/abs/2507.21056", "authors": ["Harshraj Bhoite"], "title": "AI-Driven Generation of Data Contracts in Modern Data Engineering Systems", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Data contracts formalize agreements between data producers and consumers\nregarding schema, semantics, and quality expectations. As data pipelines grow\nin complexity, manual authoring and maintenance of contracts becomes\nerror-prone and labor-intensive. We present an AI-driven framework for\nautomatic data contract generation using large language models (LLMs). Our\nsystem leverages parameter-efficient fine-tuning methods, including LoRA and\nPEFT, to adapt LLMs to structured data domains. The models take sample data or\nschema descriptions and output validated contract definitions in formats such\nas JSON Schema and Avro. We integrate this framework into modern data platforms\n(e.g., Databricks, Snowflake) to automate contract enforcement at scale.\nExperimental results on synthetic and real-world datasets demonstrate that the\nfine-tuned LLMs achieve high accuracy in generating valid contracts and reduce\nmanual workload by over 70%. We also discuss key challenges such as\nhallucination, version control, and the need for continuous learning. This work\ndemonstrates that generative AI can enable scalable, agile data governance by\nbridging the gap between intent and implementation in enterprise data\nmanagement.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u751f\u6210\u6570\u636e\u5408\u7ea6\u7684AI\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u63d0\u5347\u6570\u636e\u6cbb\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6570\u636e\u7ba1\u9053\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u624b\u52a8\u7f16\u5199\u548c\u7ef4\u62a4\u6570\u636e\u5408\u7ea6\u5bb9\u6613\u51fa\u9519\u4e14\u8017\u65f6\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\u548cPEFT\uff09\uff0c\u4f7fLLMs\u9002\u5e94\u7ed3\u6784\u5316\u6570\u636e\u9886\u57df\uff0c\u81ea\u52a8\u751f\u6210JSON Schema\u7b49\u683c\u5f0f\u7684\u5408\u7ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684LLMs\u5728\u751f\u6210\u6709\u6548\u5408\u7ea6\u65b9\u9762\u51c6\u786e\u6027\u9ad8\uff0c\u4eba\u5de5\u5de5\u4f5c\u91cf\u51cf\u5c1170%\u4ee5\u4e0a\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u53ef\u6865\u63a5\u610f\u56fe\u4e0e\u5b9e\u73b0\uff0c\u652f\u6301\u4f01\u4e1a\u6570\u636e\u7ba1\u7406\u7684\u53ef\u6269\u5c55\u548c\u654f\u6377\u6cbb\u7406\u3002"}}
{"id": "2507.21253", "pdf": "https://arxiv.org/pdf/2507.21253", "abs": "https://arxiv.org/abs/2507.21253", "authors": ["Abdullah Al Raqibul Islam", "Helen Xu", "Dong Dai", "Ayd\u0131n Bulu\u00e7"], "title": "Improving SpGEMM Performance Through Matrix Reordering and Cluster-wise Computation", "categories": ["cs.DC"], "comment": "Accepted to appear in the International Conference for High\n  Performance Computing, Networking, Storage, and Analysis (SC) 2025", "summary": "Sparse matrix-sparse matrix multiplication (SpGEMM) is a key kernel in many\nscientific applications and graph workloads. Unfortunately, SpGEMM is\nbottlenecked by data movement due to its irregular memory access patterns.\nSignificant work has been devoted to developing row reordering schemes towards\nimproving locality in sparse operations, but prior studies mostly focus on the\ncase of sparse-matrix vector multiplication (SpMV).\n  In this paper, we address these issues with hierarchical clustering for\nSpGEMM that leverages both row reordering and cluster-wise computation to\nimprove reuse in the second input (B) matrix with a novel row-clustered matrix\nformat and access pattern in the first input (A) matrix. We find that\nhierarchical clustering can speed up SpGEMM by 1.39x on average with low\npreprocessing cost (less than 20x the cost of a single SpGEMM on about 90% of\ninputs). Furthermore, we decouple the reordering algorithm from the clustered\nmatrix format so they can be applied as independent optimizations.\n  Additionally, this paper sheds light on the role of both row reordering and\nclustering independently and together for SpGEMM with a comprehensive empirical\nstudy of the effect of 10 different reordering algorithms and 3 clustering\nschemes on SpGEMM performance on a suite of 110 matrices. We find that\nreordering based on graph partitioning provides better SpGEMM performance than\nexisting alternatives at the cost of high preprocessing time. The evaluation\ndemonstrates that the proposed hierarchical clustering method achieves greater\naverage speedup compared to other reordering schemes with similar preprocessing\ntimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\uff08SpGEMM\uff09\u7684\u5206\u5c42\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u884c\u91cd\u65b0\u6392\u5e8f\u548c\u805a\u7c7b\u8ba1\u7b97\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747\u52a0\u901f1.39\u500d\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7b97\u6cd5\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "SpGEMM\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u56fe\u5f62\u5904\u7406\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u53d7\u9650\u4e8e\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u5bfc\u81f4\u7684\u6570\u636e\u79fb\u52a8\u74f6\u9888\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7a00\u758f\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\uff08SpMV\uff09\u7684\u4f18\u5316\uff0c\u800c\u5bf9SpGEMM\u7684\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u5c42\u805a\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u884c\u91cd\u65b0\u6392\u5e8f\u548c\u805a\u7c7b\u8ba1\u7b97\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u77e9\u9635\u683c\u5f0f\u548c\u8bbf\u95ee\u6a21\u5f0f\uff0c\u4ee5\u51cf\u5c11B\u77e9\u9635\u7684\u91cd\u590d\u8ba1\u7b97\u3002\u540c\u65f6\u5206\u6790\u4e8610\u79cd\u91cd\u65b0\u6392\u5e8f\u7b97\u6cd5\u548c3\u79cd\u805a\u7c7b\u65b9\u6848\u3002", "result": "\u5206\u5c42\u805a\u7c7b\u65b9\u6cd5\u5e73\u5747\u52a0\u901f1.39\u500d\uff0c\u9884\u5904\u7406\u6210\u672c\u4f4e\uff0890%\u7684\u8f93\u5165\u60c5\u51b5\u4e0b\u9884\u5904\u7406\u65f6\u95f4\u4e0d\u8d85\u8fc7\u4e00\u6b21SpGEMM\u8ba1\u7b97\u768420\u500d\uff09\u3002\u57fa\u4e8e\u56fe\u5206\u533a\u7684\u91cd\u65b0\u6392\u5e8f\u6027\u80fd\u6700\u4f73\u4f46\u9884\u5904\u7406\u65f6\u95f4\u8f83\u9ad8\u3002", "conclusion": "\u5206\u5c42\u805a\u7c7b\u4e3aSpGEMM\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u91cd\u65b0\u6392\u5e8f\u548c\u805a\u7c7b\u5404\u81ea\u53ca\u8054\u5408\u4f5c\u7528\u7684\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u9884\u5904\u7406\u6210\u672c\u4f4e\u4e14\u53ef\u72ec\u7acb\u5e94\u7528\u3002"}}
{"id": "2507.21295", "pdf": "https://arxiv.org/pdf/2507.21295", "abs": "https://arxiv.org/abs/2507.21295", "authors": ["Alexander Yu. Chunikhin"], "title": "Semantic Numeration Systems as Dynamical Systems", "categories": ["cs.LO", "cs.AI", "11A63, 47S20, 68Q55"], "comment": "11 pages, 6 figures", "summary": "The foundational concepts of semantic numeration systems theory are briefly\noutlined. The action of cardinal semantic operators unfolds over a set of\ncardinal abstract entities belonging to the cardinal semantic multeity. The\ncardinal abstract object (CAO) formed by them in a certain connectivity\ntopology is proposed to be considered as a linear discrete dynamical system\nwith nonlinear control. Under the assumption of ideal observability, the CAO\nstate equations are provided for both stationary and non-stationary cases. The\nfundamental role of the configuration matrix, which combines information about\nthe types of cardinal semantic operators in the CAO, their parameters and\ntopology of connectivity, is demonstrated.", "AI": {"tldr": "\u8bba\u6587\u6982\u8ff0\u4e86\u8bed\u4e49\u8ba1\u6570\u7cfb\u7edf\u7406\u8bba\u7684\u57fa\u7840\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u57fa\u6570\u8bed\u4e49\u591a\u91cd\u6027\u7684\u57fa\u6570\u62bd\u8c61\u5bf9\u8c61\uff08CAO\uff09\u4f5c\u4e3a\u975e\u7ebf\u6027\u63a7\u5236\u7684\u7ebf\u6027\u79bb\u6563\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u7ed9\u51fa\u4e86\u5176\u72b6\u6001\u65b9\u7a0b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u57fa\u6570\u8bed\u4e49\u7b97\u5b50\u5bf9\u57fa\u6570\u62bd\u8c61\u5b9e\u4f53\u7684\u4f5c\u7528\uff0c\u5e76\u6784\u5efa\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u63cf\u8ff0\u8fd9\u4e9b\u52a8\u6001\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u57fa\u6570\u8bed\u4e49\u591a\u91cd\u6027\u4e2d\u7684\u57fa\u6570\u62bd\u8c61\u5bf9\u8c61\uff0c\u63d0\u51fa\u5c06\u5176\u89c6\u4e3a\u7ebf\u6027\u79bb\u6563\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u63a8\u5bfc\u51fa\u9759\u6001\u548c\u975e\u9759\u6001\u60c5\u51b5\u4e0b\u7684\u72b6\u6001\u65b9\u7a0b\u3002", "result": "\u8bc1\u660e\u4e86\u914d\u7f6e\u77e9\u9635\u5728\u7ed3\u5408\u57fa\u6570\u8bed\u4e49\u7b97\u5b50\u7c7b\u578b\u3001\u53c2\u6570\u548c\u8fde\u63a5\u62d3\u6251\u4fe1\u606f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u6570\u8bed\u4e49\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u914d\u7f6e\u77e9\u9635\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.21372", "pdf": "https://arxiv.org/pdf/2507.21372", "abs": "https://arxiv.org/abs/2507.21372", "authors": ["Sarah McClure", "Sylvia Ratnasamy", "Scott Shenker"], "title": "Load Balancing for AI Training Workloads", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "We investigate the performance of various load balancing algorithms for\nlarge-scale AI training workloads that are running on dedicated infrastructure.\nThe performance of load balancing depends on both the congestion control and\nloss recovery algorithms, so our evaluation also sheds light on the appropriate\nchoices for those designs as well.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u89c4\u6a21AI\u8bad\u7ec3\u4efb\u52a1\u4e2d\u4e0d\u540c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5931\u6062\u590d\u7b97\u6cd5\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u89c4\u6a21AI\u8bad\u7ec3\u4efb\u52a1\u5bf9\u8d1f\u8f7d\u5747\u8861\u63d0\u51fa\u4e86\u9ad8\u8981\u6c42\uff0c\u800c\u7b97\u6cd5\u6027\u80fd\u53d7\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5931\u6062\u590d\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u591a\u79cd\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u5728\u4e0d\u540c\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5931\u6062\u590d\u7b56\u7565\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u6027\u80fd\u4e0e\u62e5\u585e\u63a7\u5236\u53ca\u4e22\u5931\u6062\u590d\u7b97\u6cd5\u7684\u5173\u7cfb\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u9700\u7efc\u5408\u8003\u8651\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5931\u6062\u590d\u673a\u5236\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.21317", "pdf": "https://arxiv.org/pdf/2507.21317", "abs": "https://arxiv.org/abs/2507.21317", "authors": ["Paulette Koronkevich", "William J. Bowman"], "title": "One Weird Trick to Untie Landin's Knot", "categories": ["cs.PL"], "comment": null, "summary": "In this work, we explore Landin's Knot, which is understood as a pattern for\nencoding general recursion, including non-termination, that is possible after\nadding higher-order references to an otherwise terminating language. We observe\nthat this isn't always true -- higher-order references, by themselves, don't\nlead to non-termination. The key insight is that Landin's Knot relies not\nprimarily on references storing functions, but on unrestricted quantification\nover a function's environment. We show this through a closure converted\nlanguage, in which the function's environment is made explicit and hides the\ntype of the environment through impredicative quantification. Once references\nare added, this impredicative quantification can be exploited to encode\nrecursion. We conjecture that by restricting the quantification over the\nenvironment, higher-order references can be safely added to terminating\nlanguages, without resorting to more complex type systems such as linearity,\nand without restricting references from storing functions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Landin's Knot\u4f5c\u4e3a\u7f16\u7801\u901a\u7528\u9012\u5f52\u6a21\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u9ad8\u9636\u5f15\u7528\u672c\u8eab\u4e0d\u4f1a\u5bfc\u81f4\u975e\u7ec8\u6b62\u884c\u4e3a\uff0c\u5173\u952e\u5728\u4e8e\u5bf9\u51fd\u6570\u73af\u5883\u7684\u65e0\u9650\u5236\u91cf\u5316\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u6f84\u6e05\u9ad8\u9636\u5f15\u7528\u4e0e\u9012\u5f52\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u907f\u514d\u8fc7\u5ea6\u4f9d\u8d56\u590d\u6742\u7c7b\u578b\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u95ed\u5305\u8f6c\u6362\u8bed\u8a00\uff0c\u663e\u5f0f\u5c55\u793a\u51fd\u6570\u73af\u5883\uff0c\u5e76\u5229\u7528\u975e\u9884\u6d4b\u6027\u91cf\u5316\u63ed\u793a\u9012\u5f52\u7f16\u7801\u673a\u5236\u3002", "result": "\u53d1\u73b0\u9650\u5236\u73af\u5883\u91cf\u5316\u53ef\u4ee5\u5b89\u5168\u5f15\u5165\u9ad8\u9636\u5f15\u7528\uff0c\u65e0\u9700\u590d\u6742\u7c7b\u578b\u7cfb\u7edf\u6216\u9650\u5236\u51fd\u6570\u5b58\u50a8\u3002", "conclusion": "\u63d0\u51fa\u901a\u8fc7\u9650\u5236\u73af\u5883\u91cf\u5316\u800c\u975e\u590d\u6742\u7c7b\u578b\u7cfb\u7edf\uff0c\u53ef\u5728\u7ec8\u6b62\u8bed\u8a00\u4e2d\u5b89\u5168\u4f7f\u7528\u9ad8\u9636\u5f15\u7528\u3002"}}
{"id": "2507.21382", "pdf": "https://arxiv.org/pdf/2507.21382", "abs": "https://arxiv.org/abs/2507.21382", "authors": ["Ruiyin Li", "Yiran Zhang", "Xiyu Zhou", "Peng Liang", "Weisong Sun", "Jifeng Xuan", "Zhi Jin", "Yang Liu"], "title": "MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration", "categories": ["cs.SE", "cs.AI"], "comment": "23 pages, 8 images, 1 table, Manuscript submitted to a journal (2025)", "summary": "Software architecture design is a critical, yet inherently complex and\nknowledge-intensive phase of software development. It requires deep domain\nexpertise, development experience, architectural knowledge, careful trade-offs\namong competing quality attributes, and the ability to adapt to evolving\nrequirements. Traditionally, this process is time-consuming and\nlabor-intensive, and relies heavily on architects, often resulting in limited\ndesign alternatives, especially under the pressures of agile development. While\nLarge Language Model (LLM)-based agents have shown promising performance across\nvarious SE tasks, their application to architecture design remains relatively\nscarce and requires more exploration, particularly in light of diverse domain\nknowledge and complex decision-making. To address the challenges, we proposed\nMAAD (Multi-Agent Architecture Design), an automated framework that employs a\nknowledge-driven Multi-Agent System (MAS) for architecture design. MAAD\norchestrates four specialized agents (i.e., Analyst, Modeler, Designer and\nEvaluator) to collaboratively interpret requirements specifications and produce\narchitectural blueprints enriched with quality attributes-based evaluation\nreports. We then evaluated MAAD through a case study and comparative\nexperiments against MetaGPT, a state-of-the-art MAS baseline. Our results show\nthat MAAD's superiority lies in generating comprehensive architectural\ncomponents and delivering insightful and structured architecture evaluation\nreports. Feedback from industrial architects across 11 requirements\nspecifications further reinforces MAAD's practical usability. We finally\nexplored the performance of the MAAD framework with three LLMs (GPT-4o,\nDeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance\nin producing architecture design, emphasizing the importance of LLM selection\nin MAS-driven architecture design.", "AI": {"tldr": "MAAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u81ea\u52a8\u5316\u67b6\u6784\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u751f\u6210\u67b6\u6784\u84dd\u56fe\u548c\u8d28\u91cf\u8bc4\u4f30\u62a5\u544a\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebfMetaGPT\uff0c\u5e76\u5728\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002GPT-4o\u5728\u67b6\u6784\u8bbe\u8ba1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8f6f\u4ef6\u67b6\u6784\u8bbe\u8ba1\u590d\u6742\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u8bbe\u8ba1\u9009\u62e9\u6709\u9650\u3002LLM\u4ee3\u7406\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u51b3\u7b56\u548c\u591a\u9886\u57df\u77e5\u8bc6\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51faMAAD\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08\u5206\u6790\u5e08\u3001\u5efa\u6a21\u5e08\u3001\u8bbe\u8ba1\u5e08\u3001\u8bc4\u4f30\u5e08\uff09\uff0c\u534f\u4f5c\u5904\u7406\u9700\u6c42\u5e76\u751f\u6210\u67b6\u6784\u84dd\u56fe\u53ca\u8bc4\u4f30\u62a5\u544a\u3002", "result": "MAAD\u5728\u751f\u6210\u67b6\u6784\u7ec4\u4ef6\u548c\u8bc4\u4f30\u62a5\u544a\u65b9\u9762\u4f18\u4e8eMetaGPT\uff0c\u5de5\u4e1a\u53cd\u9988\u8bc1\u5b9e\u5176\u5b9e\u7528\u6027\u3002GPT-4o\u5728\u4e09\u79cdLLM\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "MAAD\u4e3a\u81ea\u52a8\u5316\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0cLLM\u9009\u62e9\u5bf9MAS\u9a71\u52a8\u7684\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.21557", "pdf": "https://arxiv.org/pdf/2507.21557", "abs": "https://arxiv.org/abs/2507.21557", "authors": ["Chunling Fan", "Yun Zhang", "Dietmar Saupe", "Raouf Hamzaoui", "Weisi Lin"], "title": "PC-JND: Subjective Study and Dataset on Just Noticeable Difference for Point Clouds in 6DoF Virtual Reality", "categories": ["cs.MM"], "comment": "13 pages, 10 figures, Journal", "summary": "The Just Noticeable Difference (JND) accounts for the minimum distortion at\nwhich humans can perceive a difference between a pristine stimulus and its\ndistorted version. The JND concept has been widely applied in visual signal\nprocessing tasks, including coding, transmission, rendering, and quality\nassessment, to optimize human-centric media experiences. A point cloud is a\nmainstream volumetric data representation consisting of both geometry\ninformation and attributes (e.g. color). Point clouds are used for advanced\nimmersive 3D media such as Virtual Reality (VR). However, the JND\ncharacteristics of viewing point clouds in VR have not been explored before. In\nthis paper, we study the point cloud-wise JND (PCJND) characteristics in a Six\nDegrees of Freedom (6DoF) VR environment using a head-mounted display. Our\nfindings reveal that the texture PCJND of human eyes is smaller than the\ngeometry PCJND for most point clouds. Furthermore, we identify a correlation\nbetween colorfulness and texture PCJND. However, there is no significant\ncorrelation between colorfulness and the geometry PCJND, nor between the number\nof points and neither the texture or geometry PCJND. To support future research\nin JND prediction and perception-driven signal processing, we introduce PC-JND,\na novel point cloud-based JND dataset. This dataset will be made publicly\navailable to facilitate advancements in perceptual optimization for immersive\nmedia.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u516d\u81ea\u7531\u5ea6VR\u73af\u5883\u4e2d\u70b9\u4e91\u7684JND\u7279\u6027\uff0c\u53d1\u73b0\u7eb9\u7406JND\u5c0f\u4e8e\u51e0\u4f55JND\uff0c\u4e14\u989c\u8272\u4e30\u5bcc\u5ea6\u4e0e\u7eb9\u7406JND\u76f8\u5173\u3002\u53d1\u5e03\u4e86PC-JND\u6570\u636e\u96c6\u3002", "motivation": "\u63a2\u8ba8\u70b9\u4e91\u5728VR\u73af\u5883\u4e2d\u7684JND\u7279\u6027\uff0c\u586b\u8865\u6b64\u524d\u672a\u88ab\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4f18\u5316\u6c89\u6d78\u5f0f\u5a92\u4f53\u7684\u611f\u77e5\u4f53\u9a8c\u3002", "method": "\u5728\u516d\u81ea\u7531\u5ea6VR\u73af\u5883\u4e2d\u4f7f\u7528\u5934\u6234\u663e\u793a\u5668\u7814\u7a76\u70b9\u4e91\u7684JND\u7279\u6027\uff0c\u5206\u6790\u7eb9\u7406\u548c\u51e0\u4f55JND\u7684\u5dee\u5f02\u53ca\u76f8\u5173\u6027\u3002", "result": "\u7eb9\u7406JND\u5c0f\u4e8e\u51e0\u4f55JND\uff1b\u989c\u8272\u4e30\u5bcc\u5ea6\u4e0e\u7eb9\u7406JND\u76f8\u5173\uff0c\u4f46\u4e0e\u51e0\u4f55JND\u65e0\u5173\uff1b\u70b9\u4e91\u70b9\u6570\u4e0eJND\u65e0\u5173\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u70b9\u4e91JND\u7279\u6027\u7684\u7a7a\u767d\uff0c\u53d1\u5e03\u4e86PC-JND\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u611f\u77e5\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.21493", "pdf": "https://arxiv.org/pdf/2507.21493", "abs": "https://arxiv.org/abs/2507.21493", "authors": ["Longwen Zhang", "Qixuan Zhang", "Haoran Jiang", "Yinuo Bai", "Wei Yang", "Lan Xu", "Jingyi Yu"], "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics", "categories": ["cs.GR"], "comment": "Homepage: https://sites.google.com/view/bang7355608", "summary": "3D creation has always been a unique human strength, driven by our ability to\ndeconstruct and reassemble objects using our eyes, mind and hand. However,\ncurrent 3D design tools struggle to replicate this natural process, requiring\nconsiderable artistic expertise and manual labor. This paper introduces BANG, a\nnovel generative approach that bridges 3D generation and reasoning, allowing\nfor intuitive and flexible part-level decomposition of 3D objects. At the heart\nof BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of\nexploded states for an input geometry, progressively separating parts while\npreserving their geometric and semantic coherence.\n  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned\nfor exploded dynamics with a lightweight exploded view adapter, allowing\nprecise control over the decomposition process. It also incorporates a temporal\nattention module to ensure smooth transitions and consistency across time. BANG\nenhances control with spatial prompts, such as bounding boxes and surface\nregions, enabling users to specify which parts to decompose and how. This\ninteraction can be extended with multimodal models like GPT-4, enabling\n2D-to-3D manipulations for more intuitive and creative workflows.\n  The capabilities of BANG extend to generating detailed part-level geometry,\nassociating parts with functional descriptions, and facilitating\ncomponent-aware 3D creation and manufacturing workflows. Additionally, BANG\noffers applications in 3D printing, where separable parts are generated for\neasy printing and reassembly. In essence, BANG enables seamless transformation\nfrom imaginative concepts to detailed 3D assets, offering a new perspective on\ncreation that resonates with human intuition.", "AI": {"tldr": "BANG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u2018\u751f\u6210\u7206\u70b8\u52a8\u529b\u5b66\u2019\u5b9e\u73b0\u76f4\u89c2\u7684\u90e8\u5206\u7ea73D\u5bf9\u8c61\u5206\u89e3\uff0c\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u63d0\u5347\u4e863D\u521b\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u63a7\u5236\u6027\u3002", "motivation": "\u5f53\u524d3D\u8bbe\u8ba1\u5de5\u5177\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u6280\u80fd\u548c\u624b\u5de5\u52b3\u52a8\uff0c\u96be\u4ee5\u6a21\u62df\u4eba\u7c7b\u81ea\u7136\u7684\u521b\u4f5c\u8fc7\u7a0b\u3002BANG\u65e8\u5728\u901a\u8fc7\u90e8\u5206\u7ea7\u5206\u89e3\u548c\u751f\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u66f4\u76f4\u89c2\u548c\u7075\u6d3b\u76843D\u521b\u4f5c\u3002", "method": "BANG\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7206\u70b8\u89c6\u56fe\u9002\u914d\u5668\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u751f\u6210\u5e73\u6ed1\u7684\u7206\u70b8\u72b6\u6001\u5e8f\u5217\u3002\u652f\u6301\u7a7a\u95f4\u63d0\u793a\uff08\u5982\u8fb9\u754c\u6846\uff09\u548c\u591a\u6a21\u6001\u4ea4\u4e92\uff08\u5982GPT-4\uff09\u4ee5\u63a7\u5236\u5206\u89e3\u8fc7\u7a0b\u3002", "result": "BANG\u80fd\u591f\u751f\u6210\u8be6\u7ec6\u7684\u90e8\u5206\u7ea7\u51e0\u4f55\u5f62\u72b6\uff0c\u5173\u8054\u529f\u80fd\u63cf\u8ff0\uff0c\u5e76\u652f\u63013D\u6253\u5370\u548c\u7ec4\u88c5\u3002\u5176\u5de5\u4f5c\u6d41\u63d0\u5347\u4e86\u4ece\u6982\u5ff5\u52303D\u8d44\u4ea7\u7684\u8f6c\u6362\u6548\u7387\u3002", "conclusion": "BANG\u4e3a3D\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u901a\u8fc7\u751f\u6210\u7206\u70b8\u52a8\u529b\u5b66\u548c\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u521b\u4f5c\u7684\u81ea\u7136\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.21071", "pdf": "https://arxiv.org/pdf/2507.21071", "abs": "https://arxiv.org/abs/2507.21071", "authors": ["Qinglong Yang", "Haoming Li", "Haotian Zhao", "Xiaokai Yan", "Jingtao Ding", "Fengli Xu", "Yong Li"], "title": "FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Mobile GUI agents are becoming critical tools for enhancing human-device\ninteraction efficiency, with multimodal large language models (MLLMs) emerging\nas dominant paradigms in this domain. Current agents, however, are limited to\nfollowing explicit human instructions, resulting in insufficient capability for\nproactive intent anticipation. Additionally, these agents fail to leverage the\ncontextual information associated with users during task execution, thereby\nneglecting potentially vast differences in user preferences. To address these\nchallenges, we introduce the FingerTip benchmark. It contains two new tracks:\nproactive task suggestions by analyzing environment observation and users'\nprevious intents, and personalized task execution by catering to users' action\npreferences. We collected unique human demonstrations of multi-step Android\ndevice interactions across a variety of everyday apps. These demonstrations are\nnot isolated but are continuously acquired from the users' long-term usage in\ntheir real lives, and encompass essential user-related contextual information.\nOur experiments reveal challenges of the tasks we propose. The model fine-tuned\nwith the data we collected effectively utilized user information and achieved\ngood results, highlighting the potential of our approach in building more\nuser-oriented mobile GUI agents. Our code is open-source at\nhttps://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86FingerTip\u57fa\u51c6\uff0c\u901a\u8fc7\u5206\u6790\u73af\u5883\u89c2\u5bdf\u548c\u7528\u6237\u5386\u53f2\u610f\u56fe\u6765\u589e\u5f3a\u79fb\u52a8GUI\u4ee3\u7406\u7684\u4e3b\u52a8\u610f\u56fe\u9884\u6d4b\u548c\u4e2a\u6027\u5316\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u79fb\u52a8GUI\u4ee3\u7406\u4ec5\u80fd\u6267\u884c\u660e\u786e\u6307\u4ee4\u3001\u7f3a\u4e4f\u4e3b\u52a8\u610f\u56fe\u9884\u6d4b\u80fd\u529b\u53ca\u5ffd\u7565\u7528\u6237\u504f\u597d\u5dee\u5f02\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFingerTip\u57fa\u51c6\uff0c\u5305\u542b\u4e3b\u52a8\u4efb\u52a1\u5efa\u8bae\u548c\u4e2a\u6027\u5316\u4efb\u52a1\u6267\u884c\u4e24\u4e2a\u65b0\u8f68\u9053\uff0c\u5e76\u6536\u96c6\u4e86\u771f\u5b9e\u7528\u6237\u7684\u591a\u6b65\u9aa4Android\u8bbe\u5907\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6536\u96c6\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u7528\u6237\u4fe1\u606f\u5e76\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6784\u5efa\u66f4\u7528\u6237\u5bfc\u5411\u7684\u79fb\u52a8GUI\u4ee3\u7406\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.21151", "pdf": "https://arxiv.org/pdf/2507.21151", "abs": "https://arxiv.org/abs/2507.21151", "authors": ["Abel C. H. Chen"], "title": "NIST Post-Quantum Cryptography Standard Algorithms Based on Quantum Random Number Generators", "categories": ["cs.CR", "cs.PF", "quant-ph", "stat.AP"], "comment": "in Chinese language", "summary": "In recent years, the advancement of quantum computing technology has posed\npotential security threats to RSA cryptography and elliptic curve cryptography.\nIn response, the National Institute of Standards and Technology (NIST)\npublished several Federal Information Processing Standards (FIPS) of\npost-quantum cryptography (PQC) in August 2024, including the\nModule-Lattice-Based Key-Encapsulation Mechanism (ML-KEM), Module-Lattice-Based\nDigital Signature Algorithm (ML-DSA), and Stateless Hash-Based Digital\nSignature Algorithm (SLH-DSA). Although these PQC algorithms are designed to\nresist quantum computing attacks, they may not provide adequate security in\ncertain specialized application scenarios. To address this issue, this study\nproposes quantum random number generator (QRNG)-based PQC algorithms. These\nalgorithms leverage quantum computing to generate random numbers, which serve\nas the foundation for key pair generation, key encapsulation, and digital\nsignature generation. A generalized architecture of QRNG is proposed, along\nwith the design of six QRNGs. Each generator is evaluated according to the\nstatistical validation procedures outlined in NIST SP 800-90B, including tests\nfor verification of entropy sources and independent and identically distributed\n(IID) outputs. Experimental results assess the computation time of the six\nQRNGs, as well as the performance of QRNG-based ML-KEM, QRNG-based ML-DSA, and\nQRNG-based SLH-DSA. These findings provide valuable reference data for future\ndeployment of PQC systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u968f\u673a\u6570\u751f\u6210\u5668\uff08QRNG\uff09\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\uff0c\u4ee5\u589e\u5f3a\u73b0\u6709\u540e\u91cf\u5b50\u5bc6\u7801\u6280\u672f\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1NIST\u53d1\u5e03\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u6807\u51c6\uff08\u5982ML-KEM\u3001ML-DSA\u548cSLH-DSA\uff09\u80fd\u62b5\u6297\u91cf\u5b50\u8ba1\u7b97\u653b\u51fb\uff0c\u4f46\u5728\u67d0\u4e9b\u7279\u6b8a\u5e94\u7528\u573a\u666f\u4e0b\u53ef\u80fd\u4e0d\u591f\u5b89\u5168\u3002", "method": "\u8bbe\u8ba1\u4e86\u516d\u79cdQRNG\uff0c\u5e76\u901a\u8fc7NIST SP 800-90B\u7684\u7edf\u8ba1\u9a8c\u8bc1\u7a0b\u5e8f\u8bc4\u4f30\u5176\u6027\u80fd\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86QRNG\u4e3a\u57fa\u7840\u7684ML-KEM\u3001ML-DSA\u548cSLH-DSA\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u516d\u79cdQRNG\u7684\u8ba1\u7b97\u65f6\u95f4\u53ca\u57fa\u4e8eQRNG\u7684\u5bc6\u7801\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u90e8\u7f72\u540e\u91cf\u5b50\u5bc6\u7801\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53c2\u8003\u6570\u636e\u3002", "conclusion": "QRNG\u4e3a\u57fa\u7840\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u6709\u671b\u63d0\u5347\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e3a\u672a\u6765\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2507.21303", "pdf": "https://arxiv.org/pdf/2507.21303", "abs": "https://arxiv.org/abs/2507.21303", "authors": ["Viktoria Marcus", "Griffin Pitts", "Sanaz Motamedi"], "title": "Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems", "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "Accepted and to be presented at ASPIRE 2025 - the 69th International\n  Annual Meeting of HFES", "summary": "Each year, over half of global traffic fatalities involve vulnerable road\nusers (e.g. pedestrians), often due to human error. Level-5 automated driving\nsystems (ADSs) could reduce driver errors contributing to pedestrian accidents,\nthough effectiveness depends on clarity and understandability for other road\nusers. External human-machine interfaces (eHMIs) have been proposed to\nfacilitate pedestrian-ADS communication, though consensus on optimal eHMI\nfeatures remains unclear. In an online survey, 153 participants responded to\nroad-crossing scenarios involving level-5 ADSs, with and without eHMIs. With\neHMIs, pedestrians crossed earlier and more confidently, and reported\nsignificantly increased perceptions of safety, trust, and understanding when\ninteracting with level-5 ADSs. Visual eHMI features (including a text display\nand external speedometer) were ranked more necessary than auditory ones, though\nauditory cues received positive feedback. This study demonstrates that eHMIs\ncan significantly improve pedestrians' understanding of level-5 ADS intent and\nenhance perceived safety and trust, facilitating more intuitive pedestrian-ADS\ninteractions.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5916\u90e8\u4eba\u673a\u754c\u9762\uff08eHMIs\uff09\u80fd\u663e\u8457\u63d0\u5347\u884c\u4eba\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u610f\u56fe\u7684\u7406\u89e3\uff0c\u589e\u5f3a\u5b89\u5168\u611f\u548c\u4fe1\u4efb\u611f\u3002", "motivation": "\u5168\u7403\u534a\u6570\u4ee5\u4e0a\u7684\u4ea4\u901a\u4e8b\u6545\u6d89\u53ca\u884c\u4eba\uff0c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADSs\uff09\u53ef\u901a\u8fc7\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u6765\u6539\u5584\u884c\u4eba\u5b89\u5168\uff0c\u4f46eHMIs\u7684\u4f18\u5316\u8bbe\u8ba1\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u8c03\u67e5\uff0c153\u540d\u53c2\u4e0e\u8005\u5bf9\u6709\u65e0eHMIs\u7684L5\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fc7\u9a6c\u8def\u573a\u666f\u8fdb\u884c\u54cd\u5e94\uff0c\u5bf9\u6bd4\u884c\u4eba\u7684\u884c\u4e3a\u548c\u611f\u77e5\u3002", "result": "\u4f7f\u7528eHMIs\u65f6\uff0c\u884c\u4eba\u8fc7\u9a6c\u8def\u66f4\u65e9\u4e14\u66f4\u81ea\u4fe1\uff0c\u5b89\u5168\u611f\u3001\u4fe1\u4efb\u611f\u548c\u7406\u89e3\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u89c6\u89c9\u7279\u5f81\uff08\u5982\u6587\u5b57\u663e\u793a\u548c\u5916\u90e8\u901f\u5ea6\u8ba1\uff09\u6bd4\u542c\u89c9\u66f4\u53d7\u6b22\u8fce\u3002", "conclusion": "eHMIs\u80fd\u6709\u6548\u4fc3\u8fdb\u884c\u4eba\u4e0eL5\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u4e92\uff0c\u672a\u6765\u7684eHMI\u8bbe\u8ba1\u5e94\u4f18\u5148\u89c6\u89c9\u7279\u5f81\u3002"}}
{"id": "2507.21173", "pdf": "https://arxiv.org/pdf/2507.21173", "abs": "https://arxiv.org/abs/2507.21173", "authors": ["Chris Partridge", "Andrew Mitchell", "Andreas Cola"], "title": "Digitalizing Uncertain Information", "categories": ["cs.DB"], "comment": "9 pages. 2 figures. Conference: Semantic Technology for Intelligence,\n  Defense, and Security (STIDS 2024)", "summary": "The paper sketches some initial results from an ongoing project to develop an\nontology-based digital form for representing uncertain information. We frame\nthis work as a journey from lower to higher levels of digital maturity across a\ntechnology divide. The paper first sets a baseline by describing the basic\nchallenges any project dealing with digital uncertainty faces. It then\ndescribes how the project is facing them. It shows firstly how an extensional\nontology (such as the BORO Foundational Ontology or the Information Exchange\nStandard) can be extended with a Lewisian counterpart approach to formalizing\nuncertainty that is adapted to computing. And then it shows how this is\nexpressive enough to handle the challenges. Keywords: actuality, BORO\nFoundational Ontology, counterpart, Information Exchange Standard,\ninformational uncertainty, my doxastic actualities, two-dimensional semantics.", "AI": {"tldr": "\u8bba\u6587\u6982\u8ff0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u672c\u4f53\u7684\u6570\u5b57\u8868\u5355\u9879\u76ee\uff0c\u7528\u4e8e\u8868\u793a\u4e0d\u786e\u5b9a\u4fe1\u606f\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6269\u5c55\u672c\u4f53\u548cLewisian\u5bf9\u7b49\u65b9\u6cd5\u6765\u5e94\u5bf9\u6570\u5b57\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u6570\u5b57\u9879\u76ee\u4e2d\u4e0d\u786e\u5b9a\u4fe1\u606f\u7684\u8868\u793a\u95ee\u9898\uff0c\u63d0\u9ad8\u6570\u5b57\u6210\u719f\u5ea6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6269\u5c55\u672c\u4f53\uff08\u5982BORO\u57fa\u7840\u672c\u4f53\u6216\u4fe1\u606f\u4ea4\u6362\u6807\u51c6\uff09\uff0c\u5e76\u901a\u8fc7Lewisian\u5bf9\u7b49\u65b9\u6cd5\u5f62\u5f0f\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8db3\u4ee5\u5e94\u5bf9\u6570\u5b57\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u901a\u8fc7\u6269\u5c55\u672c\u4f53\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u8868\u793a\u548c\u5904\u7406\u6570\u5b57\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.21464", "pdf": "https://arxiv.org/pdf/2507.21464", "abs": "https://arxiv.org/abs/2507.21464", "authors": ["Marco Mambelli", "Bruno Moreira Coimbra", "Namratha Urs", "Ilya Baburashvili"], "title": "Using Containers to Speed Up Development, to Run Integration Tests and to Teach About Distributed Systems", "categories": ["cs.DC", "H.3.4; D.2.6"], "comment": "8 pages, 3 figures, for associated code, see [this https\n  URL](https://github.com/glideinWMS/containers), to be published in\n  proceedings of 27th International Conference on Computing in High Energy and\n  Nuclear Physics (CHEP 2024). 21-25 October 2024. Krakow,; Poland.\n  (C24-10-21.8)", "summary": "GlideinWMS is a workload manager provisioning resources for many experiments,\nincluding CMS and DUNE. The software is distributed both as native packages and\nspecialized production containers. Following an approach used in other\ncommunities like web development, we built our workspaces, system-like\ncontainers to ease development and testing. Developers can change the source\ntree or check out a different branch and quickly reconfigure the services to\nsee the effect of their changes. In this paper, we will talk about what\ndifferentiates workspaces from other containers. We will describe our base\nsystem, composed of three containers: a one-node cluster including a compute\nelement and a batch system, a GlideinWMS Factory controlling pilot jobs, and a\nscheduler and Frontend to submit jobs and provision resources. Additional\ncontainers can be used for optional components. This system can easily run on a\nlaptop, and we will share our evaluation of different container runtimes, with\nan eye for ease of use and performance. Finally, we will talk about our\nexperience as developers and with students. The GlideinWMS workspaces are\neasily integrated with IDEs like VS Code, simplifying debugging and allowing\ndevelopment and testing of the system even when offline. They simplified the\ntraining and onboarding of new team members and summer interns. And they were\nuseful in workshops where students could have first-hand experience with the\nmechanisms and components that, in production, run millions of jobs.", "AI": {"tldr": "GlideinWMS\u5de5\u4f5c\u7a7a\u95f4\u7684\u5bb9\u5668\u5316\u5f00\u53d1\u4e0e\u6d4b\u8bd5\u73af\u5883\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u3001\u8c03\u8bd5\u548c\u56e2\u961f\u57f9\u8bad\u3002", "motivation": "\u4e3aGlideinWMS\u5f00\u53d1\u8005\u548c\u5b66\u751f\u63d0\u4f9b\u6613\u4e8e\u4f7f\u7528\u7684\u5bb9\u5668\u5316\u5de5\u4f5c\u7a7a\u95f4\uff0c\u4ee5\u4fbf\u4e8e\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5b66\u4e60\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u4e09\u4e2a\u6838\u5fc3\u5bb9\u5668\u7684\u7cfb\u7edf\uff08\u5355\u8282\u70b9\u96c6\u7fa4\u3001\u5de5\u5382\u8c03\u5ea6\u5668\u548c\u524d\u7aef\uff09\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u5bb9\u5668\u7684\u6027\u80fd\u548c\u6613\u7528\u6027\u3002", "result": "\u5de5\u4f5c\u7a7a\u95f4\u6210\u529f\u7b80\u5316\u4e86\u5f00\u53d1\u3001\u8c03\u8bd5\u548c\u56e2\u961f\u57f9\u8bad\uff0c\u5e76\u5728\u79bb\u7ebf\u73af\u5883\u4e0b\u4e5f\u80fd\u6709\u6548\u8fd0\u884c\u3002", "conclusion": "GlideinWMS\u5de5\u4f5c\u7a7a\u95f4\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u56e2\u961f\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2507.21598", "pdf": "https://arxiv.org/pdf/2507.21598", "abs": "https://arxiv.org/abs/2507.21598", "authors": ["Beatrice Melani", "Ezio Bartocci", "Michele Chiari"], "title": "A Tree-Shaped Tableau for Checking the Satisfiability of Signal Temporal Logic with Bounded Temporal Operators", "categories": ["cs.LO"], "comment": "25 pages, 8 figures. Accepted for presentation at EMSOFT 2025, to\n  appear in ACM TECS", "summary": "Signal Temporal Logic (STL) is a widely recognized formal specification\nlanguage to express rigorous temporal requirements on mixed analog signals\nproduced by cyber-physical systems (CPS). A relevant problem in CPS design is\nhow to efficiently and automatically check whether a set of STL requirements is\nlogically consistent. This problem reduces to solving the STL satisfiability\nproblem, which is decidable when we assume that our system operates in discrete\ntime steps dictated by an embedded system's clock.\n  This paper introduces a novel tree-shaped, one-pass tableau method for\nsatisfiability checking of discrete-time STL with bounded temporal operators.\nOriginally designed to prove the consistency of a given set of STL\nrequirements, this method has a wide range of applications beyond consistency\nchecking. These include synthesizing example signals that satisfy the given\nrequirements, as well as verifying or refuting the equivalence and implications\nof STL formulas.\n  Our tableau exploits redundancy arising from large time intervals in STL\nformulas to speed up satisfiability checking, and can also be employed to check\nMission-Time Linear Temporal Logic (MLTL) satisfiability. We compare our\ntableau with Satisfiability Modulo Theories (SMT) and First-Order Logic\nencodings from the literature on a benchmark suite, partly collected from the\nliterature, and partly provided by an industrial partner. Our experiments show\nthat, in many cases, our tableau outperforms state-of-the-art encodings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6811\u5f62\u4e00\u6b21\u6027\u8868\u6cd5\uff0c\u7528\u4e8e\u68c0\u67e5\u79bb\u6563\u65f6\u95f4STL\u7684\u6ee1\u8db3\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u5197\u4f59\u65f6\u95f4\u95f4\u9694\u4e0a\u4f18\u5316\u6027\u80fd\uff0c\u5e76\u9002\u7528\u4e8eMLTL\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3STL\u9700\u6c42\u903b\u8f91\u4e00\u81f4\u6027\u7684\u9ad8\u6548\u81ea\u52a8\u68c0\u67e5\u95ee\u9898\uff0c\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6811\u5f62\u4e00\u6b21\u6027\u8868\u6cd5\uff0c\u5229\u7528STL\u516c\u5f0f\u4e2d\u7684\u5197\u4f59\u65f6\u95f4\u95f4\u9694\u52a0\u901f\u6ee1\u8db3\u6027\u68c0\u67e5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u4f18\u4e8eSMT\u548c\u4e00\u9636\u903b\u8f91\u7f16\u7801\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9ad8\u6548\uff0c\u8fd8\u53ef\u7528\u4e8e\u4fe1\u53f7\u5408\u6210\u3001\u9a8c\u8bc1\u548c\u516c\u5f0f\u7b49\u4ef7\u6027\u68c0\u67e5\u3002"}}
{"id": "2507.21385", "pdf": "https://arxiv.org/pdf/2507.21385", "abs": "https://arxiv.org/abs/2507.21385", "authors": ["Wei Mao", "Lili Wei", "Omid Semiari", "Shu-ping Yeh", "Hosein Nikopour"], "title": "Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving", "categories": ["cs.NI", "cs.AI"], "comment": "7 pages, 7 figures", "summary": "3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX)\nis an important new network energy saving feature for 5G. As a time-domain\ntechnique, it periodically aggregates the user data transmissions in a given\nduration of time when the traffic load is not heavy, so that the remaining time\ncan be kept silent and advanced sleep modes (ASM) can be enabled to shut down\nmore radio components and save more energy for the cell. However, inevitably\nthe packet delay is increased, as during the silent period no transmission is\nallowed. In this paper we study how to configure cell DTX/DRX to optimally\nbalance energy saving and packet delay, so that for delay-sensitive traffic\nmaximum energy saving can be achieved while the degradation of quality of\nservice (QoS) is minimized. As the optimal configuration can be different for\ndifferent network and traffic conditions, the problem is complex and we resort\nto deep reinforcement learning (DRL) framework to train an AI agent to solve\nit. Through careful design of 1) the learning algorithm, which implements a\ndeep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward\nfunction, which utilizes a smooth approximation of a theoretically optimal but\ndiscontinuous reward function, we are able to train an AI agent that always\ntries to select the best possible Cell DTX/DRX configuration under any network\nand traffic conditions. Simulation results show that compared to the case when\ncell DTX/DRX is not used, our agent can achieve up to ~45% energy saving\ndepending on the traffic load scenario, while always maintaining no more than\n~1% QoS degradation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u914d\u7f6e5G\u4e2d\u7684Cell DTX/DRX\uff0c\u4ee5\u4f18\u5316\u8282\u80fd\u4e0e\u6570\u636e\u5305\u5ef6\u8fdf\u7684\u5e73\u8861\uff0c\u5b9e\u73b0\u9ad8\u8fbe45%\u7684\u8282\u80fd\u6548\u679c\uff0c\u540c\u65f6\u5c06\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u4e0b\u964d\u63a7\u5236\u57281%\u4ee5\u5185\u3002", "motivation": "\u7531\u4e8e5G\u4e2dCell DTX/DRX\u5728\u8282\u80fd\u7684\u540c\u65f6\u4f1a\u4e0d\u53ef\u907f\u514d\u589e\u52a0\u6570\u636e\u5305\u5ef6\u8fdf\uff0c\u56e0\u6b64\u9700\u8981\u5728\u4e0d\u540c\u7f51\u7edc\u548c\u6d41\u91cf\u6761\u4ef6\u4e0b\u627e\u5230\u6700\u4f18\u7684\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u5316\u8282\u80fd\u6548\u679c\u5e76\u6700\u5c0f\u5316QoS\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u548c\u4e0a\u4e0b\u6587bandit\uff08CB\uff09\u6a21\u578b\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u6765\u8bad\u7ec3AI\u4ee3\u7406\uff0c\u4ee5\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684Cell DTX/DRX\u914d\u7f6e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4e0d\u4f7f\u7528Cell DTX/DRX\u7684\u60c5\u51b5\u76f8\u6bd4\uff0cAI\u4ee3\u7406\u53ef\u4ee5\u5728\u4e0d\u540c\u6d41\u91cf\u573a\u666f\u4e0b\u5b9e\u73b0\u9ad8\u8fbe45%\u7684\u8282\u80fd\uff0c\u540c\u65f6\u5c06QoS\u4e0b\u964d\u63a7\u5236\u57281%\u4ee5\u5185\u3002", "conclusion": "\u901a\u8fc7DRL\u8bbe\u8ba1\u7684AI\u4ee3\u7406\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\uff0c\u9ad8\u6548\u5e73\u8861\u8282\u80fd\u4e0e\u670d\u52a1\u8d28\u91cf\u7684\u9700\u6c42\u3002"}}
{"id": "2507.21439", "pdf": "https://arxiv.org/pdf/2507.21439", "abs": "https://arxiv.org/abs/2507.21439", "authors": ["Yong Qi Foo", "Brian Sze-Kai Cheong", "Michael D. Adams"], "title": "Fixed-Point-Oriented Programming: A Concise and Elegant Paradigm", "categories": ["cs.PL"], "comment": null, "summary": "Fixed-Point-Oriented Programming (FPOP) is an emerging paradigm designed to\nstreamline the implementation of problems involving self-referential\ncomputations. These include graph algorithms, static analysis, parsing, and\ndistributed computing-domains that traditionally require complex and\ntricky-to-implement work-queue algorithms. Existing programming paradigms lack\ndirect support for these inherently fixed-point computations, leading to\ninefficient and error-prone implementations.\n  This white paper explores the potential of the FPOP paradigm, which offers a\nhigh-level abstraction that enables concise and expressive problem\nformulations. By leveraging structured inference rules and user-directed\noptimizations, FPOP allows developers to write declarative specifications while\nthe compiler ensures efficient execution. It not only reduces implementation\ncomplexity for programmers but also enhances adaptability, making it easier for\nprogrammers to explore alternative solutions and optimizations without\nmodifying the core logic of their program.\n  We demonstrate how FPOP simplifies algorithm implementation, improves\nmaintainability, and enables rapid prototyping by allowing problems to be\nclearly and concisely expressed. For example, the graph distance problem can be\nexpressed in only two executable lines of code with FPOP, while it takes an\norder of magnitude more code in other paradigms. By bridging the gap between\ntheoretical fixed-point formulations and practical implementations, we aim to\nfoster further research and adoption of this paradigm.", "AI": {"tldr": "FPOP\uff08\u5b9a\u70b9\u5bfc\u5411\u7f16\u7a0b\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7f16\u7a0b\u8303\u5f0f\uff0c\u65e8\u5728\u7b80\u5316\u81ea\u5f15\u7528\u8ba1\u7b97\u7684\u5b9e\u73b0\uff0c\u5982\u56fe\u7b97\u6cd5\u3001\u9759\u6001\u5206\u6790\u548c\u5206\u5e03\u5f0f\u8ba1\u7b97\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cFPOP\u63d0\u4f9b\u4e86\u9ad8\u7ea7\u62bd\u8c61\uff0c\u4f7f\u95ee\u9898\u8868\u8fbe\u66f4\u7b80\u6d01\u4e14\u6613\u4e8e\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u7f16\u7a0b\u8303\u5f0f\u7f3a\u4e4f\u5bf9\u5b9a\u70b9\u8ba1\u7b97\u7684\u76f4\u63a5\u652f\u6301\uff0c\u5bfc\u81f4\u5b9e\u73b0\u590d\u6742\u4e14\u5bb9\u6613\u51fa\u9519\u3002FPOP\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FPOP\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u89c4\u5219\u548c\u7528\u6237\u5bfc\u5411\u7684\u4f18\u5316\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u7f16\u5199\u58f0\u660e\u5f0f\u89c4\u8303\uff0c\u7f16\u8bd1\u5668\u5219\u8d1f\u8d23\u9ad8\u6548\u6267\u884c\u3002", "result": "FPOP\u663e\u8457\u7b80\u5316\u4e86\u7b97\u6cd5\u5b9e\u73b0\uff0c\u63d0\u9ad8\u4e86\u53ef\u7ef4\u62a4\u6027\uff0c\u5e76\u652f\u6301\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002\u4f8b\u5982\uff0c\u56fe\u8ddd\u79bb\u95ee\u9898\u4ec5\u9700\u4e24\u884cFPOP\u4ee3\u7801\u5373\u53ef\u8868\u8fbe\uff0c\u8fdc\u5c11\u4e8e\u5176\u4ed6\u8303\u5f0f\u3002", "conclusion": "FPOP\u5728\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u6709\u671b\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u91c7\u7528\u8fd9\u4e00\u8303\u5f0f\u3002"}}
{"id": "2507.21447", "pdf": "https://arxiv.org/pdf/2507.21447", "abs": "https://arxiv.org/abs/2507.21447", "authors": ["Zachariah Sollenberger", "Rahul Patel", "Saieda Ali Zada", "Sunita Chandrasekaran"], "title": "LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests", "categories": ["cs.SE", "cs.ET"], "comment": null, "summary": "The usage of Large Language Models (LLMs) for software and test development\nhas continued to increase since LLMs were first introduced, but only recently\nhave the expectations of LLMs become more realistic. Verifying the correctness\nof code generated by LLMs is key to improving their usefulness, but there have\nbeen no comprehensive and fully autonomous solutions developed yet.\nHallucinations are a major concern when LLMs are applied blindly to problems\nwithout taking the time and effort to verify their outputs, and an inability to\nexplain the logical reasoning of LLMs leads to issues with trusting their\nresults. To address these challenges while also aiming to effectively apply\nLLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a\ndiscriminative LLM) and experiments with the usage of LLMs for the generation\nof a large volume of compiler tests. We experimented with a number of LLMs\npossessing varying parameter counts and presented results using ten\ncarefully-chosen metrics that we describe in detail in our narrative. Through\nour findings, it is evident that LLMs possess the promising potential to\ngenerate quality compiler tests and verify them automatically.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86LLM\u5728\u8f6f\u4ef6\u548c\u6d4b\u8bd5\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u53ccLLM\u7cfb\u7edf\uff08\u751f\u6210\u548c\u5224\u522b\uff09\u4ee5\u89e3\u51b3LLM\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86LLM\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u7f16\u8bd1\u5668\u6d4b\u8bd5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u548c\u6d4b\u8bd5\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9a8c\u8bc1\u5176\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u4e14\u5b8c\u5168\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6LLM\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u903b\u8f91\u63a8\u7406\u4e0d\u900f\u660e\u6027\u4e5f\u589e\u52a0\u4e86\u5bf9\u5176\u7ed3\u679c\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u53ccLLM\u7cfb\u7edf\uff08\u4e00\u4e2a\u751f\u6210LLM\u548c\u4e00\u4e2a\u5224\u522bLLM\uff09\uff0c\u5e76\u5b9e\u9a8c\u4e86LLM\u5728\u751f\u6210\u5927\u91cf\u7f16\u8bd1\u5668\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684LLM\uff0c\u5e76\u901a\u8fc7\u5341\u4e2a\u7cbe\u5fc3\u9009\u62e9\u7684\u6307\u6807\u8fdb\u884c\u7ed3\u679c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u7f16\u8bd1\u5668\u6d4b\u8bd5\u5e76\u81ea\u52a8\u9a8c\u8bc1\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "LLM\u53cc\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u89e3\u51b3\u4ee3\u7801\u9a8c\u8bc1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5728\u7f16\u8bd1\u5668\u6d4b\u8bd5\u751f\u6210\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.21926", "pdf": "https://arxiv.org/pdf/2507.21926", "abs": "https://arxiv.org/abs/2507.21926", "authors": ["Th\u00e9o Ladune", "Thomas Leguay", "Pierrick Philippe", "Gordon Clare", "F\u00e9lix Henry"], "title": "Efficient Sub-pixel Motion Compensation in Learned Video Codecs", "categories": ["cs.MM", "eess.IV"], "comment": null, "summary": "Motion compensation is a key component of video codecs. Conventional codecs\n(HEVC and VVC) have carefully refined this coding step, with an important focus\non sub-pixel motion compensation. On the other hand, learned codecs achieve\nsub-pixel motion compensation through simple bilinear filtering. This paper\noffers to improve learned codec motion compensation by drawing inspiration from\nconventional codecs. It is shown that the usage of more advanced interpolation\nfilters, block-based motion information and finite motion accuracy lead to\nbetter compression performance and lower decoding complexity. Experimental\nresults are provided on the Cool-chic video codec, where we demonstrate a rate\ndecrease of more than 10% and a lowering of motion-related decoding complexity\nfrom 391 MAC per pixel to 214 MAC per pixel. All contributions are made\nopen-source at https://github.com/Orange-OpenSource/Cool-Chic", "AI": {"tldr": "\u901a\u8fc7\u5b66\u4e60\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u6539\u8fdb\u5b66\u4e60\u578b\u7f16\u89e3\u7801\u5668\u7684\u8fd0\u52a8\u8865\u507f\u6280\u672f\uff0c\u63d0\u5347\u538b\u7f29\u6027\u80fd\u5e76\u964d\u4f4e\u89e3\u7801\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff08\u5982HEVC\u548cVVC\uff09\u5728\u8fd0\u52a8\u8865\u507f\u65b9\u9762\u5df2\u9ad8\u5ea6\u4f18\u5316\uff0c\u800c\u5b66\u4e60\u578b\u7f16\u89e3\u7801\u5668\u4ecd\u4f7f\u7528\u7b80\u5355\u7684\u53cc\u7ebf\u6027\u6ee4\u6ce2\u8fdb\u884c\u5b50\u50cf\u7d20\u8fd0\u52a8\u8865\u507f\uff0c\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\u3002", "method": "\u4ece\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u91c7\u7528\u66f4\u9ad8\u7ea7\u7684\u63d2\u503c\u6ee4\u6ce2\u5668\u3001\u57fa\u4e8e\u5757\u7684\u8fd0\u52a8\u4fe1\u606f\u548c\u6709\u9650\u8fd0\u52a8\u7cbe\u5ea6\u3002", "result": "\u5728Cool-chic\u89c6\u9891\u7f16\u89e3\u7801\u5668\u4e2d\uff0c\u5b9e\u73b0\u4e8610%\u4ee5\u4e0a\u7684\u7801\u7387\u964d\u4f4e\uff0c\u5e76\u5c06\u8fd0\u52a8\u76f8\u5173\u7684\u89e3\u7801\u590d\u6742\u5ea6\u4ece\u6bcf\u50cf\u7d20391 MAC\u964d\u81f3214 MAC\u3002", "conclusion": "\u901a\u8fc7\u501f\u9274\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5b66\u4e60\u578b\u7f16\u89e3\u7801\u5668\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.21124", "pdf": "https://arxiv.org/pdf/2507.21124", "abs": "https://arxiv.org/abs/2507.21124", "authors": ["Ayan Biswas", "Terece L. Turton", "Nishath Rajiv Ranasinghe", "Shawn Jones", "Bradley Love", "William Jones", "Aric Hagberg", "Han-Wei Shen", "Nathan DeBardeleben", "Earl Lawrence"], "title": "VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization", "categories": ["cs.HC", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "We present VizGenie, a self-improving, agentic framework that advances\nscientific visualization through large language model (LLM) by orchestrating of\na collection of domain-specific and dynamically generated modules. Users\ninitially access core functionalities--such as threshold-based filtering, slice\nextraction, and statistical analysis--through pre-existing tools. For tasks\nbeyond this baseline, VizGenie autonomously employs LLMs to generate new\nvisualization scripts (e.g., VTK Python code), expanding its capabilities\non-demand. Each generated script undergoes automated backend validation and is\nseamlessly integrated upon successful testing, continuously enhancing the\nsystem's adaptability and robustness. A distinctive feature of VizGenie is its\nintuitive natural language interface, allowing users to issue high-level\nfeature-based queries (e.g., ``visualize the skull\"). The system leverages\nimage-based analysis and visual question answering (VQA) via fine-tuned vision\nmodels to interpret these queries precisely, bridging domain expertise and\ntechnical implementation. Additionally, users can interactively query generated\nvisualizations through VQA, facilitating deeper exploration. Reliability and\nreproducibility are further strengthened by Retrieval-Augmented Generation\n(RAG), providing context-driven responses while maintaining comprehensive\nprovenance records. Evaluations on complex volumetric datasets demonstrate\nsignificant reductions in cognitive overhead for iterative visualization tasks.\nBy integrating curated domain-specific tools with LLM-driven flexibility,\nVizGenie not only accelerates insight generation but also establishes a\nsustainable, continuously evolving visualization practice. The resulting\nplatform dynamically learns from user interactions, consistently enhancing\nsupport for feature-centric exploration and reproducible research in scientific\nvisualization.", "AI": {"tldr": "VizGenie\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u7684\u79d1\u5b66\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u901a\u8fc7LLM\u751f\u6210\u811a\u672c\u6269\u5c55\u529f\u80fd\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u754c\u9762\u548c\u89c6\u89c9\u95ee\u7b54\u6280\u672f\uff0c\u63d0\u5347\u4ea4\u4e92\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u53ef\u89c6\u5316\u5de5\u5177\u529f\u80fd\u6709\u9650\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0cVizGenie\u65e8\u5728\u901a\u8fc7LLM\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u95e8\u69db\uff0c\u52a8\u6001\u6269\u5c55\u529f\u80fd\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u9884\u7f6e\u5de5\u5177\u548cLLM\u751f\u6210\u7684\u811a\u672c\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9a8c\u8bc1\u96c6\u6210\u65b0\u529f\u80fd\uff1b\u5229\u7528\u56fe\u50cf\u5206\u6790\u548cVQA\u7cbe\u786e\u89e3\u6790\u7528\u6237\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7RAG\u589e\u5f3a\u53ef\u9760\u6027\u3002", "result": "\u5728\u590d\u6742\u4f53\u6570\u636e\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fed\u4ee3\u53ef\u89c6\u5316\u4efb\u52a1\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u652f\u6301\u53ef\u6301\u7eed\u7684\u529f\u80fd\u6269\u5c55\u548c\u7528\u6237\u4ea4\u4e92\u5b66\u4e60\u3002", "conclusion": "VizGenie\u901a\u8fc7\u878d\u5408\u9886\u57df\u5de5\u5177\u4e0eLLM\u7075\u6d3b\u6027\uff0c\u4e0d\u4ec5\u52a0\u901f\u79d1\u5b66\u6d1e\u5bdf\uff0c\u8fd8\u63a8\u52a8\u4e86\u53ef\u89c6\u5316\u5b9e\u8df5\u7684\u53ef\u6301\u7eed\u8fdb\u5316\u3002"}}
{"id": "2507.21072", "pdf": "https://arxiv.org/pdf/2507.21072", "abs": "https://arxiv.org/abs/2507.21072", "authors": ["Di Wen", "Junwei Zheng", "Ruiping Liu", "Yi Xu", "Kunyu Peng", "Rainer Stiefelhagen"], "title": "Snap, Segment, Deploy: A Visual Data and Detection Pipeline for Wearable Industrial Assistants", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Industrial assembly tasks increasingly demand rapid adaptation to complex\nprocedures and varied components, yet are often conducted in environments with\nlimited computing, connectivity, and strict privacy requirements. These\nconstraints make conventional cloud-based or fully autonomous solutions\nimpractical for factory deployment. This paper introduces a mobile-device-based\nassistant system for industrial training and operational support, enabling\nreal-time, semi-hands-free interaction through on-device perception and voice\ninterfaces. The system integrates lightweight object detection, speech\nrecognition, and Retrieval-Augmented Generation (RAG) into a modular on-device\npipeline that operates entirely on-device, enabling intuitive support for part\nhandling and procedure understanding without relying on manual supervision or\ncloud services. To enable scalable training, we adopt an automated data\nconstruction pipeline and introduce a two-stage refinement strategy to improve\nvisual robustness under domain shift. Experiments on our generated dataset,\ni.e., Gear8, demonstrate improved robustness to domain shift and common visual\ncorruptions. A structured user study further confirms its practical viability,\nwith positive user feedback on the clarity of the guidance and the quality of\nthe interaction. These results indicate that our framework offers a deployable\nsolution for real-time, privacy-preserving smart assistance in industrial\nenvironments. We will release the Gear8 dataset and source code upon\nacceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u5de5\u4e1a\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5bf9\u8c61\u68c0\u6d4b\u3001\u8bed\u97f3\u8bc6\u522b\u548cRAG\u6280\u672f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u534a\u514d\u63d0\u7684\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4e14\u9690\u79c1\u8981\u6c42\u4e25\u683c\u7684\u5de5\u4e1a\u73af\u5883\u3002", "motivation": "\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u9700\u8981\u5feb\u901f\u9002\u5e94\u590d\u6742\u6d41\u7a0b\u548c\u591a\u6837\u7ec4\u4ef6\uff0c\u4f46\u4f20\u7edf\u4e91\u7aef\u6216\u5168\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u5728\u8d44\u6e90\u6709\u9650\u548c\u9690\u79c1\u4e25\u683c\u7684\u5de5\u5382\u73af\u5883\u4e2d\u96be\u4ee5\u90e8\u7f72\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86\u8f7b\u91cf\u7ea7\u5bf9\u8c61\u68c0\u6d4b\u3001\u8bed\u97f3\u8bc6\u522b\u548cRAG\u6280\u672f\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u5e76\u5168\u8bbe\u5907\u8fd0\u884c\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u63d0\u5347\u89c6\u89c9\u9c81\u68d2\u6027\u3002", "result": "\u5728Gear8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u5176\u5177\u5907\u66f4\u597d\u7684\u9886\u57df\u9002\u5e94\u6027\u548c\u89c6\u89c9\u9c81\u68d2\u6027\uff0c\u7528\u6237\u7814\u7a76\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u4e1a\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u90e8\u7f72\u7684\u5b9e\u65f6\u667a\u80fd\u8f85\u52a9\u89e3\u51b3\u65b9\u6848\uff0c\u8ba1\u5212\u516c\u5f00\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u3002"}}
{"id": "2507.21860", "pdf": "https://arxiv.org/pdf/2507.21860", "abs": "https://arxiv.org/abs/2507.21860", "authors": ["Micka\u00ebl Martin-Nevot", "Lotfi Lakhal"], "title": "Ranking Methods for Skyline Queries", "categories": ["cs.DB"], "comment": null, "summary": "{Multi-criteria decision analysis in databases has been actively studied,\nespecially through the Skyline operator. Yet, few approaches offer a relevant\ncomparison of Pareto optimal, or Skyline, points for high cardinality result\nsets. We propose to improve the dp-idp method, inspired by tf-idf, a recent\napproach computing a score for each Skyline point, by introducing the concept\nof dominance hierarchy. As dp-idp lacks efficiency and does not ensure a\ndistinctive rank, we introduce the RankSky method, the adaptation of Google's\nwell-known PageRank solution, using a square stochastic matrix, a teleportation\nmatrix, a damping factor, and then a row score eigenvector and the IPL\nalgorithm. For the same reasons as RankSky, and also to offer directly\nembeddable in DBMS solution, we establish the TOPSIS based CoSky method,\nderived from both information research and multi-criteria analysis. CoSky\nautomatically ponderates normalized attributes using the Gini index, then\ncomputes a score using Salton's cosine toward an ideal point. By coupling\nmultilevel Skyline to dp-idp, RankSky or CoSky, we introduce DeepSky.\nImplementations of dp-idp, RankSky and CoSky are evaluated experimentally.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u652f\u914d\u5c42\u6b21\u7ed3\u6784\u4f18\u5316dp-idp\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faRankSky\u548cCoSky\u65b9\u6cd5\uff0c\u6700\u7ec8\u7ed3\u5408\u591a\u5c42Skyline\u63d0\u51faDeepSky\u3002", "motivation": "\u73b0\u6709Skyline\u65b9\u6cd5\u5728\u9ad8\u57fa\u6570\u7ed3\u679c\u96c6\u4e2d\u7f3a\u4e4f\u6709\u6548\u7684\u6bd4\u8f83\u673a\u5236\uff0cdp-idp\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6392\u540d\u4e0d\u663e\u8457\u3002", "method": "\u63d0\u51faRankSky\uff08\u57fa\u4e8ePageRank\uff09\u548cCoSky\uff08\u57fa\u4e8eTOPSIS\u548cGini\u6307\u6570\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u5c42Skyline\u5f62\u6210DeepSky\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86dp-idp\u3001RankSky\u548cCoSky\u7684\u5b9e\u73b0\u6548\u679c\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u6539\u8fdb\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u7684\u6548\u7387\u548c\u6392\u540d\u6548\u679c\uff0c\u6700\u7ec8\u63d0\u51fa\u7efc\u5408\u65b9\u6848DeepSky\u3002"}}
{"id": "2507.21472", "pdf": "https://arxiv.org/pdf/2507.21472", "abs": "https://arxiv.org/abs/2507.21472", "authors": ["Marco Mambelli", "Shrijan Swaminathan"], "title": "GlideinBenchmark: collecting resource information to optimize provisioning", "categories": ["cs.DC", "H.3.4; K.6.2"], "comment": "6 pages, 3 figures, for associated code, see [this https\n  URL](https://github.com/glideinWMS/glideinbenchmark), to be published in\n  proceedings of 27th International Conference on Computing in High Energy and\n  Nuclear Physics (CHEP 2024). 21-25 October 2024. Krakow,; Poland.\n  (C24-10-21.8)", "summary": "Choosing the right resource can speed up job completion, better utilize the\navailable hardware, and visibly reduce costs, especially when renting computers\nin the cloud. This was demonstrated in earlier studies on HEPCloud. However,\nthe benchmarking of the resources proved to be a laborious and time-consuming\nprocess. This paper presents GlideinBenchmark, a new Web application leveraging\nthe pilot infrastructure of GlideinWMS to benchmark resources, and it shows how\nto use the data collected and published by GlideinBenchmark to automate the\noptimal selection of resources. An experiment can select the benchmark or the\nset of benchmarks that most closely evaluate the performance of its workflows.\nGlideinBenchmark, with the help of the GlideinWMS Factory, controls the\nbenchmark execution. Finally, a scheduler like HEPCloud's Decision Engine can\nuse the results to optimize resource provisioning.", "AI": {"tldr": "GlideinBenchmark\u662f\u4e00\u4e2a\u57fa\u4e8eGlideinWMS\u7684Web\u5e94\u7528\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8d44\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e2e\u52a9\u4f18\u5316\u8d44\u6e90\u9009\u62e9\uff0c\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u9009\u62e9\u5408\u9002\u7684\u8d44\u6e90\u53ef\u4ee5\u52a0\u901f\u4efb\u52a1\u5b8c\u6210\u3001\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u5229\u7528GlideinWMS\u7684\u57fa\u7840\u8bbe\u65bd\u5f00\u53d1GlideinBenchmark\uff0c\u81ea\u52a8\u5316\u6267\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u53ef\u4ee5\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u4f18\u5316\u8d44\u6e90\u9009\u62e9\uff0c\u8c03\u5ea6\u5668\u53ef\u636e\u6b64\u4f18\u5316\u8d44\u6e90\u4f9b\u5e94\u3002", "conclusion": "GlideinBenchmark\u901a\u8fc7\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\u663e\u7740\u63d0\u5347\u4e86\u8d44\u6e90\u9009\u62e9\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2507.21851", "pdf": "https://arxiv.org/pdf/2507.21851", "abs": "https://arxiv.org/abs/2507.21851", "authors": ["Christian Alrabbaa", "Stefan Borgwardt", "Philipp Herrmann", "Markus Kr\u00f6tzsch"], "title": "The Shape of $\\mathcal{EL}$ Proofs: A Tale of Three Calculi (Extended Version)", "categories": ["cs.LO"], "comment": "Extended version of a paper accepted at 38th International Workshop\n  on Description Logics (DL 2025)", "summary": "Consequence-based reasoning can be used to construct proofs that explain\nentailments of description logic (DL) ontologies. In the literature, one can\nfind multiple consequence-based calculi for reasoning in the $\\mathcal{EL}$\nfamily of DLs, each of which gives rise to proofs of different shapes. Here, we\nstudy three such calculi and the proofs they produce on a benchmark based on\nthe OWL Reasoner Evaluation. The calculi are implemented using a translation\ninto existential rules with stratified negation, which had already been\ndemonstrated to be effective for the calculus of the ELK reasoner. We then use\nthe rule engine NEMO to evaluate the rules and obtain traces of the rule\nexecution. After translating these traces back into DL proofs, we compare them\non several metrics that reflect different aspects of their complexity.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e09\u79cd\u57fa\u4e8e\u540e\u679c\u7684\u63a8\u7406\u6f14\u7b97\u5728\u63cf\u8ff0\u903b\u8f91\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u5f15\u64ce\u8bc4\u4f30\u5b83\u4eec\u7684\u590d\u6742\u6027\u6307\u6807\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u6f14\u7b97\u65b9\u6cd5\u5728\u63cf\u8ff0\u903b\u8f91\u4e2d\u7684\u8bc1\u660e\u751f\u6210\u6548\u679c\u548c\u590d\u6742\u6027\u3002", "method": "\u5c06\u6f14\u7b97\u8f6c\u5316\u4e3a\u5e26\u6709\u5206\u5c42\u5426\u5b9a\u7684\u5b58\u5728\u89c4\u5219\uff0c\u4f7f\u7528NEMO\u89c4\u5219\u5f15\u64ce\u6267\u884c\u5e76\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u6f14\u7b97\u751f\u6210\u7684\u8bc1\u660e\u5728\u591a\u4e2a\u590d\u6742\u6027\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u4e0d\u540c\u6f14\u7b97\u751f\u6210\u7684\u8bc1\u660e\u5728\u590d\u6742\u6027\u4e0a\u6709\u6240\u4e0d\u540c\uff0c\u4e3a\u9009\u62e9\u63a8\u7406\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.21728", "pdf": "https://arxiv.org/pdf/2507.21728", "abs": "https://arxiv.org/abs/2507.21728", "authors": ["Agastya Raj", "Zehao Wang", "Tingjun Chen", "Daniel C Kilper", "Marco Ruffini"], "title": "Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum", "categories": ["cs.NI", "cs.LG"], "comment": "This is a preprint of a paper accepted and published in the Journal\n  of Optical Communications and Networking (JOCN). The final published version\n  is available at: https://doi.org/10.1364/JOCN.560987", "summary": "Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers\n(EDFAs) is essential for optimizing optical network performance, particularly\nas networks evolve toward multi-vendor solutions. In this work, we propose a\ngeneralized few-shot transfer learning architecture based on a Semi-Supervised\nSelf-Normalizing Neural Network (SS-NN) that leverages internal EDFA features -\nsuch as VOA input or output power and attenuation, to improve gain spectrum\nprediction. Our SS-NN model employs a two-phase training strategy comprising\nunsupervised pre-training with noise-augmented measurements and supervised\nfine-tuning with a custom weighted MSE loss. Furthermore, we extend the\nframework with transfer learning (TL) techniques that enable both homogeneous\n(same-feature space) and heterogeneous (different-feature sets) model\nadaptation across booster, preamplifier, and ILA EDFAs. To address feature\nmismatches in heterogeneous TL, we incorporate a covariance matching loss to\nalign second-order feature statistics between source and target domains.\nExtensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland\ntestbeds demonstrate that the proposed approach significantly reduces the\nnumber of measurements requirements on the system while achieving lower mean\nabsolute errors and improved error distributions compared to benchmark methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u76d1\u7763\u81ea\u5f52\u4e00\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u4f18\u5316EDFA\u7684\u589e\u76ca\u8c31\u9884\u6d4b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u663e\u8457\u51cf\u5c11\u4e86\u6d4b\u91cf\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u591a\u5382\u5546\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u5149\u7ea4\u7f51\u7edc\u6027\u80fd\uff0c\u9700\u8981\u51c6\u786e\u5efa\u6a21EDFA\u7684\u589e\u76ca\u8c31\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u81ea\u5f52\u4e00\u5316\u795e\u7ecf\u7f51\u7edc\uff08SS-NN\uff09\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u4ee5\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u7684EDFA\u3002", "result": "\u572826\u4e2aEDFA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u6d4b\u91cf\u9700\u6c42\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u548c\u8bef\u5dee\u5206\u5e03\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u589e\u76ca\u8c31\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u5149\u7f51\u7edc\u73af\u5883\u3002"}}
{"id": "2507.22048", "pdf": "https://arxiv.org/pdf/2507.22048", "abs": "https://arxiv.org/abs/2507.22048", "authors": ["Di Wang"], "title": "Composable Effect Handling for Programming LLM-integrated Scripts", "categories": ["cs.PL"], "comment": null, "summary": "Implementing LLM-integrated scripts introduces challenges in modularity and\nperformance, as scripts are often coupled to specific LLM implementations and\nfail to exploit parallelization opportunities. This paper proposes using\ncomposable effect handling to separate workflow logic from effectful\noperations, such as LLM calls, I/O, and concurrency, enabling modularity\nwithout sacrificing the opportunity for performance optimization. By treating\nthese operations as abstract interfaces and discharging them via effect\nhandlers, this paper shows that scripts can achieve significant speedups (e.g.,\n10$\\times$ in a Tree-of-Thoughts case study) without compromising modularity.\nThis paper aims to promote composable effect handling as a programming style\nfor LLM scripting.", "AI": {"tldr": "\u901a\u8fc7\u53ef\u7ec4\u5408\u6548\u679c\u5904\u7406\u6765\u5206\u79bbLLM\u811a\u672c\u7684\u5de5\u4f5c\u6d41\u903b\u8f91\u4e0e\u6548\u679c\u64cd\u4f5c\uff0c\u63d0\u9ad8\u6a21\u5757\u5316\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u96c6\u6210\u811a\u672c\u4e2d\u6a21\u5757\u5316\u548c\u6027\u80fd\u4f18\u5316\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53ef\u7ec4\u5408\u6548\u679c\u5904\u7406\u6280\u672f\uff0c\u5c06\u6548\u679c\u64cd\u4f5c\uff08\u5982LLM\u8c03\u7528\u3001I/O\u548c\u5e76\u53d1\uff09\u62bd\u8c61\u4e3a\u63a5\u53e3\u3002", "result": "\u5728Tree-of-Thoughts\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e8610\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u5757\u5316\u3002", "conclusion": "\u53ef\u7ec4\u5408\u6548\u679c\u5904\u7406\u662f\u4e00\u79cd\u9002\u7528\u4e8eLLM\u811a\u672c\u7f16\u7a0b\u7684\u6709\u6548\u98ce\u683c\u3002"}}
{"id": "2507.21485", "pdf": "https://arxiv.org/pdf/2507.21485", "abs": "https://arxiv.org/abs/2507.21485", "authors": ["Jing Wang", "Shang Liu", "Yao Lu", "Zhiyao Xie"], "title": "HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions", "categories": ["cs.SE", "cs.AI"], "comment": "This work has been accepted at ICCAD 2025 (International Conference\n  on Computer-Aided Design)", "summary": "High-level synthesis (HLS) accelerates hardware design by enabling the\nautomatic translation of high-level descriptions into efficient hardware\nimplementations. However, debugging HLS code is a challenging and\nlabor-intensive task, especially for novice circuit designers or software\nengineers without sufficient hardware domain knowledge. The recent emergence of\nLarge Language Models (LLMs) is promising in automating the HLS debugging\nprocess. Despite the great potential, three key challenges persist when\napplying LLMs to HLS logic debugging: 1) High-quality circuit data for training\nLLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in\nhardware is inherently more complex than identifying software bugs with\nexisting golden test cases. 3) The absence of reliable test cases requires\nmulti-tasking solutions, performing both bug identification and correction.\ncomplicates the multi-tasking required for effective HLS debugging. In this\nwork, we propose a customized solution named HLSDebugger to address the\nchallenges. HLSDebugger first generates and releases a large labeled dataset\nwith 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts\nan encoder-decoder structure, performing bug location identification, bug type\nprediction, and bug correction with the same model. HLSDebugger significantly\noutperforms advanced LLMs like GPT-4 in bug identification and by more than 3x\nin bug correction. It makes a substantial advancement in the exploration of\nautomated debugging of HLS code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHLSDebugger\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u4ee3\u7801\u8c03\u8bd5\u7684\u6311\u6218\uff0c\u901a\u8fc7\u751f\u6210\u5927\u578b\u6807\u8bb0\u6570\u636e\u96c6\u548c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86HLS\u903b\u8f91\u9519\u8bef\u7684\u8bc6\u522b\u548c\u4fee\u6b63\u80fd\u529b\u3002", "motivation": "\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u4ee3\u7801\u8c03\u8bd5\u5bf9\u7f3a\u4e4f\u786c\u4ef6\u77e5\u8bc6\u7684\u521d\u5b66\u8005\u548c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6765\u8bf4\u975e\u5e38\u56f0\u96be\uff0c\u800c\u5f53\u524d\u7684LLM\u5728\u89e3\u51b3HLS\u8c03\u8bd5\u95ee\u9898\u65f6\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u903b\u8f91\u590d\u6742\u6027\u9ad8\u548c\u7f3a\u4e4f\u53ef\u9760\u6d4b\u8bd5\u7528\u4f8b\u7b49\u6311\u6218\u3002", "method": "HLSDebugger\u901a\u8fc7\u751f\u6210\u5305\u542b30\u4e07\u6837\u672c\u7684\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u540c\u65f6\u5b8c\u6210\u9519\u8bef\u5b9a\u4f4d\u3001\u7c7b\u578b\u9884\u6d4b\u548c\u4fee\u6b63\u4efb\u52a1\u3002", "result": "HLSDebugger\u5728\u9519\u8bef\u8bc6\u522b\u548c\u4fee\u6b63\u65b9\u9762\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8eGPT-4\u7b49\u5148\u8fdbLLM\uff0c\u5c24\u5176\u5728\u9519\u8bef\u4fee\u6b63\u65b9\u9762\u8d85\u51fa3\u500d\u4ee5\u4e0a\u3002", "conclusion": "HLSDebugger\u4e3aHLS\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u8c03\u8bd5\u63d0\u4f9b\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u591a\u4efb\u52a1\u5904\u7406\u7684\u6311\u6218\u3002"}}
{"id": "2507.20177", "pdf": "https://arxiv.org/pdf/2507.20177", "abs": "https://arxiv.org/abs/2507.20177", "authors": ["Yaozong Zheng", "Bineng Zhong", "Qihua Liang", "Shengping Zhang", "Guorong Li", "Xianxian Li", "Rongrong Ji"], "title": "Towards Universal Modal Tracking with Online Dense Temporal Token Learning", "categories": ["cs.CV", "cs.MM"], "comment": "arXiv admin note: text overlap with arXiv:2401.01686", "summary": "We propose a universal video-level modality-awareness tracking model with\nonline dense temporal token learning (called {\\modaltracker}). It is designed\nto support various tracking tasks, including RGB, RGB+Thermal, RGB+Depth, and\nRGB+Event, utilizing the same model architecture and parameters. Specifically,\nour model is designed with three core goals: \\textbf{Video-level Sampling}. We\nexpand the model's inputs to a video sequence level, aiming to see a richer\nvideo context from an near-global perspective. \\textbf{Video-level\nAssociation}. Furthermore, we introduce two simple yet effective online dense\ntemporal token association mechanisms to propagate the appearance and motion\ntrajectory information of target via a video stream manner. \\textbf{Modality\nScalable}. We propose two novel gated perceivers that adaptively learn\ncross-modal representations via a gated attention mechanism, and subsequently\ncompress them into the same set of model parameters via a one-shot training\nmanner for multi-task inference. This new solution brings the following\nbenefits: (i) The purified token sequences can serve as temporal prompts for\nthe inference in the next video frames, whereby previous information is\nleveraged to guide future inference. (ii) Unlike multi-modal trackers that\nrequire independent training, our one-shot training scheme not only alleviates\nthe training burden, but also improves model representation. Extensive\nexperiments on visible and multi-modal benchmarks show that our {\\modaltracker}\nachieves a new \\textit{SOTA} performance. The code will be available at\nhttps://github.com/GXNU-ZhongLab/ODTrack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89c6\u9891\u7ea7\u6a21\u6001\u611f\u77e5\u8ddf\u8e2a\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u7ea7\u91c7\u6837\u3001\u5173\u8054\u548c\u6a21\u6001\u6269\u5c55\u5b9e\u73b0\u9ad8\u6548\u8ddf\u8e2a\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u7edf\u4e00\u7684\u8ddf\u8e2a\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u6a21\u6001\uff08\u5982RGB\u3001\u70ed\u6210\u50cf\u7b49\uff09\uff0c\u51cf\u5c11\u72ec\u7acb\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u89c6\u9891\u7ea7\u91c7\u6837\u3001\u5728\u7ebf\u5bc6\u96c6\u65f6\u5e8f\u4ee4\u724c\u5b66\u4e60\u548c\u95e8\u63a7\u611f\u77e5\u5668\u6765\u5b66\u4e60\u548c\u538b\u7f29\u8de8\u6a21\u6001\u8868\u793a\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u964d\u4f4e\u4e86\u8bad\u7ec3\u8d1f\u62c5\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8868\u793a\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2507.21181", "pdf": "https://arxiv.org/pdf/2507.21181", "abs": "https://arxiv.org/abs/2507.21181", "authors": ["Smita Khapre", "Sudhanshu Semwal"], "title": "Mitigation of Social Media Platforms Impact on the Users", "categories": ["cs.CR", "cs.CY", "cs.GR"], "comment": "WSCG 2025 33. International Conference on Computer Graphics,\n  Visualization and Computer Vision 2025", "summary": "Social media platforms offer numerous benefits and allow people to come\ntogether for various causes. Many communities, academia, government agencies,\ninstitutions, healthcare, entertainment, and businesses are on social media\nplatforms. They are intuitive and free for users. It has become unimaginable to\nlive without social media. Their architecture and data handling are geared\ntowards scalability, uninterrupted availability, and both personal and\ncollaborative revenue generation. Primarily, artificial intelligence algorithms\nare employed on stored user data for optimization and feeds. This has the\npotential to impact user safety, privacy, and security, even when metadata is\nused. A new decentralized data arrangement framework based on the Fractal-tree\nand L-Systems algorithm is proposed to mitigate some of the impacts of social\nmedia platforms.\n  Future work will focus on demonstrating the effectiveness of the new\ndecentralized framework by comparing its results against state-of-the-art\nsecurity methods currently used in databases. A cryptographic algorithm could\nalso be implemented for the framework, employing a new key generation for each\nbranch. This will strengthen database security; for example, if a user key is\nleaked, regenerating the key for each branch will keep the data secure by\napplying defense mechanisms in the proposed L-System-based tree framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u6811\u548cL\u7cfb\u7edf\u7b97\u6cd5\u7684\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5bf9\u9690\u79c1\u548c\u5b89\u5168\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5e76\u8ba1\u5212\u901a\u8fc7\u5bf9\u6bd4\u73b0\u6709\u5b89\u5168\u65b9\u6cd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u867d\u7136\u5e26\u6765\u4fbf\u5229\uff0c\u4f46\u5176\u6570\u636e\u67b6\u6784\u548c\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u7684\u4f7f\u7528\u53ef\u80fd\u5a01\u80c1\u7528\u6237\u9690\u79c1\u548c\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u6811\u548cL\u7cfb\u7edf\u7b97\u6cd5\u7684\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u5b89\u6392\u6846\u67b6\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u5206\u652f\u5bc6\u94a5\u751f\u6210\u548c\u9632\u5fa1\u673a\u5236\u63d0\u5347\u6570\u636e\u5b89\u5168\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u52a0\u5bc6\u7b97\u6cd5\u7684\u5e94\u7528\u3002"}}
{"id": "2507.21074", "pdf": "https://arxiv.org/pdf/2507.21074", "abs": "https://arxiv.org/abs/2507.21074", "authors": ["Qian Huang", "Thijs Willems"], "title": "Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education", "categories": ["cs.HC", "cs.AI"], "comment": "20 pages", "summary": "As generative AI (Gen-AI) tools become more prevalent in education, there is\na growing need to understand how educators, not just students, can actively\nshape their design and use. This study investigates how two instructors\nintegrated four custom GPT tools into a Masters-level Qualitative Research\nMethods course for Urban Planning Policy students. Addressing two key gaps: the\ndominant framing of students as passive AI users, and the limited use of AI in\nqualitative methods education. The study explores how Gen-AI can support\ndisciplinary learning when aligned with pedagogical intent. Drawing on the\nTechnological Pedagogical Content Knowledge (TPACK) framework and action\nresearch methodology, the instructors designed GPTs to scaffold tasks such as\nresearch question formulation, interview practice, fieldnote analysis, and\ndesign thinking. Thematic analysis of student reflections, AI chat logs, and\nfinal assignments revealed that the tools enhanced student reflexivity,\nimproved interview techniques, and supported structured analytic thinking.\nHowever, students also expressed concerns about cognitive overload, reduced\nimmersion in data, and the formulaic nature of AI responses. The study offers\nthree key insights: AI can be a powerful scaffold for active learning when\npaired with human facilitation; custom GPTs can serve as cognitive partners in\niterative research practice; and educator-led design is critical to\npedagogically meaningful AI integration. This research contributes to emerging\nscholarship on AI in higher education by demonstrating how empowering educators\nto design custom tools can promote more reflective, responsible, and\ncollaborative learning with AI.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u6559\u5e08\u5982\u4f55\u8bbe\u8ba1AI\u5de5\u5177\u4ee5\u652f\u6301\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\u8bfe\u7a0b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u7814\u7a76\u4e2d\u5b66\u751f\u88ab\u89c6\u4e3a\u88ab\u52a8AI\u4f7f\u7528\u8005\uff0c\u4ee5\u53caAI\u5728\u5b9a\u6027\u65b9\u6cd5\u6559\u80b2\u4e2d\u5e94\u7528\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528TPACK\u6846\u67b6\u548c\u884c\u52a8\u7814\u7a76\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u56db\u79cd\u5b9a\u5236GPT\u5de5\u5177\u7528\u4e8e\u652f\u6301\u8bfe\u7a0b\u4efb\u52a1\u3002", "result": "AI\u5de5\u5177\u63d0\u9ad8\u4e86\u5b66\u751f\u7684\u53cd\u601d\u80fd\u529b\u3001\u8bbf\u8c08\u6280\u5de7\u548c\u5206\u6790\u601d\u7ef4\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u8ba4\u77e5\u8fc7\u8f7d\u548c\u6570\u636e\u6c89\u6d78\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "conclusion": "\u6559\u80b2\u8005\u8bbe\u8ba1\u7684AI\u5de5\u5177\u53ef\u4ee5\u4fc3\u8fdb\u66f4\u6709\u610f\u4e49\u7684\u5b66\u4e60\uff0c\u540c\u65f6\u9700\u8981\u5e73\u8861\u6280\u672f\u652f\u6301\u4e0e\u4eba\u6587\u5f15\u5bfc\u3002"}}
{"id": "2507.21448", "pdf": "https://arxiv.org/pdf/2507.21448", "abs": "https://arxiv.org/abs/2507.21448", "authors": ["Teng", "Ma", "Sile Yin", "Li-Chia Yang", "Shuo Zhang"], "title": "Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations", "categories": ["eess.AS", "cs.ET", "cs.LG"], "comment": "Accepted into Interspeech 2025", "summary": "Speech enhancement in audio-only settings remains challenging, particularly\nin the presence of interfering speakers. This paper presents a simple yet\neffective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which\nisolates and enhances the on-screen target speaker while suppressing\ninterfering speakers and background noise. We investigate how visual embeddings\nlearned from audio-visual speech recognition (AVSR) and active speaker\ndetection (ASD) contribute to AVSE across different SNR conditions and numbers\nof interfering speakers. Our results show concatenating embeddings from AVSR\nand ASD models provides the greatest improvement in low-SNR, multi-speaker\nenvironments, while AVSR embeddings alone perform best in noise-only scenarios.\nIn addition, we develop a real-time streaming system that operates on a\ncomputer CPU and we provide a video demonstration and code repository. To our\nknowledge, this is the first open-source implementation of a real-time AVSE\nsystem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edfRAVEN\uff0c\u901a\u8fc7\u7ed3\u5408\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u6765\u589e\u5f3a\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\uff0c\u5e76\u5728\u4f4e\u4fe1\u566a\u6bd4\u548c\u591a\u8bf4\u8bdd\u8005\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u97f3\u9891\u8bed\u97f3\u589e\u5f3a\u5728\u5e72\u6270\u8bf4\u8bdd\u8005\u548c\u566a\u58f0\u5b58\u5728\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "RAVEN\u7cfb\u7edf\u5229\u7528\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08AVSR\uff09\u548c\u4e3b\u52a8\u8bf4\u8bdd\u8005\u68c0\u6d4b\uff08ASD\uff09\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u901a\u8fc7\u62fc\u63a5\u8fd9\u4e9b\u5d4c\u5165\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u6d41\u5f0f\u5904\u7406\u7cfb\u7edf\u3002", "result": "\u5728\u4f4e\u4fe1\u566a\u6bd4\u548c\u591a\u8bf4\u8bdd\u8005\u73af\u5883\u4e2d\uff0c\u62fc\u63a5AVSR\u548cASD\u5d4c\u5165\u6548\u679c\u6700\u597d\uff1b\u7eaf\u566a\u58f0\u573a\u666f\u4e2dAVSR\u5d4c\u5165\u8868\u73b0\u6700\u4f73\u3002\u7cfb\u7edf\u6210\u529f\u5728CPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002", "conclusion": "RAVEN\u901a\u8fc7\u7ed3\u5408\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u663e\u8457\u63d0\u5347\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\uff0c\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u5b9e\u65f6AVSE\u7cfb\u7edf\u3002"}}
{"id": "2507.21989", "pdf": "https://arxiv.org/pdf/2507.21989", "abs": "https://arxiv.org/abs/2507.21989", "authors": ["Patrick Iff", "Paul Bruegger", "Marcin Chrapek", "Maciej Besta", "Torsten Hoefler"], "title": "Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors", "categories": ["cs.DB", "cs.DS", "cs.IR"], "comment": null, "summary": "Advances in embedding models for text, image, audio, and video drive progress\nacross multiple domains, including retrieval-augmented generation,\nrecommendation systems, vehicle/person reidentification, and face recognition.\nMany applications in these domains require an efficient method to retrieve\nitems that are close to a given query in the embedding space while satisfying a\nfilter condition based on the item's attributes, a problem known as Filtered\nApproximate Nearest Neighbor Search (FANNS). In this work, we present a\ncomprehensive survey and taxonomy of FANNS methods and analyze how they are\nbenchmarked in the literature. By doing so, we identify a key challenge in the\ncurrent FANNS landscape: the lack of diverse and realistic datasets,\nparticularly ones derived from the latest transformer-based text embedding\nmodels. To address this, we introduce a novel dataset consisting of embedding\nvectors for the abstracts of over 2.7 million research articles from the arXiv\nrepository, accompanied by 11 real-world attributes such as authors and\ncategories. We benchmark a wide range of FANNS methods on our novel dataset and\nfind that each method has distinct strengths and limitations; no single\napproach performs best across all scenarios. ACORN, for example, supports\nvarious filter types and performs reliably across dataset scales but is often\noutperformed by more specialized methods. SeRF shows excellent performance for\nrange filtering on ordered attributes but cannot handle categorical attributes.\nFiltered-DiskANN and UNG excel on the medium-scale dataset but fail on the\nlarge-scale dataset, highlighting the challenge posed by transformer-based\nembeddings, which are often more than an order of magnitude larger than earlier\nembeddings. We conclude that no universally best method exists.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8fc7\u6ee4\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08FANNS\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6587\u732e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8earXiv\u8bba\u6587\u6458\u8981\u7684\u65b0\u578b\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdFANNS\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dFANNS\u9886\u57df\u7f3a\u4e4f\u591a\u6837\u5316\u548c\u73b0\u5b9e\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u6700\u65b0Transformer\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5305\u542b270\u4e07\u7bc7arXiv\u8bba\u6587\u6458\u8981\u5d4c\u5165\u5411\u91cf\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b11\u79cd\u771f\u5b9e\u5c5e\u6027\uff08\u5982\u4f5c\u8005\u548c\u7c7b\u522b\uff09\uff0c\u5e76\u5bf9\u591a\u79cdFANNS\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f73\uff1b\u4e0d\u540c\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\uff0c\u4f8b\u5982ACORN\u652f\u6301\u591a\u79cd\u8fc7\u6ee4\u7c7b\u578b\u4f46\u5e38\u88ab\u66f4\u4e13\u7528\u65b9\u6cd5\u8d85\u8d8a\uff0c\u800cSeRF\u5728\u6709\u5e8f\u5c5e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u5bf9\u5206\u7c7b\u5c5e\u6027\u65e0\u6548\u3002", "conclusion": "FANNS\u9886\u57df\u4e0d\u5b58\u5728\u666e\u904d\u6700\u4f18\u7684\u65b9\u6cd5\uff0c\u4e0d\u540c\u573a\u666f\u9700\u8981\u9488\u5bf9\u6027\u7684\u65b9\u6cd5\u9009\u62e9\u3002"}}
{"id": "2507.21492", "pdf": "https://arxiv.org/pdf/2507.21492", "abs": "https://arxiv.org/abs/2507.21492", "authors": ["Yicong Luo", "Senhe Hao", "Brian Wheatman", "Prashant Pandey", "Helen Xu"], "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized In-Memory B-Skiplist", "categories": ["cs.DC"], "comment": "Accepted into ICPP 2025", "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5e76\u53d1B-skiplist\uff0c\u901a\u8fc7\u4f18\u5316\u7f13\u5b58\u5c40\u90e8\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f20\u7edfskiplist\u7684\u7b80\u5355\u6027\u3002", "motivation": "\u4f20\u7edfskiplist\u56e0\u6bcf\u4e2a\u8282\u70b9\u4ec5\u5b58\u50a8\u4e00\u4e2a\u5143\u7d20\uff0c\u5bfc\u81f4\u7f13\u5b58\u5c40\u90e8\u6027\u5dee\uff0c\u5f71\u54cd\u6027\u80fd\u3002\u4f18\u5316\u7f13\u5b58\u5c40\u90e8\u6027\u662f\u63d0\u5347\u5185\u5b58\u7d22\u5f15\u6027\u80fd\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u904d\u63d2\u5165\u7b97\u6cd5\u548c\u5bf9\u5e94\u7684\u5e76\u53d1\u63a7\u5236\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7f13\u5b58\u5c40\u90e8\u6027\u7684B-skiplist\u3002", "result": "\u5728128\u7ebf\u7a0b\u4e0b\uff0cB-skiplist\u7684\u541e\u5410\u91cf\u662f\u73b0\u6709\u5e76\u53d1skiplist\u76842x-9x\uff0c\u70b9\u67e5\u8be2\u541e\u5410\u91cf\u4e0e\u4f18\u5316\u6811\u7d22\u5f15\u76f8\u5f53\uff080.9x-1.7x\uff09\uff0c\u4e14\u5ef6\u8fdf\u663e\u8457\u66f4\u4f4e\u3002", "conclusion": "B-skiplist\u5728\u6027\u80fd\u548c\u5ef6\u8fdf\u4e0a\u4f18\u4e8e\u73b0\u6709\u5e76\u53d1skiplist\uff0c\u4e0e\u4f18\u5316\u6811\u7d22\u5f15\u76f8\u6bd4\u4e5f\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b80\u5355\u6027\u3002"}}
{"id": "2507.21955", "pdf": "https://arxiv.org/pdf/2507.21955", "abs": "https://arxiv.org/abs/2507.21955", "authors": ["Anselm Haak", "Patrick Koopmann", "Yasir Mahmood", "Anni-Yasmin Turhan"], "title": "Why not? Developing ABox Abduction beyond Repairs", "categories": ["cs.LO"], "comment": null, "summary": "Abduction is the task of computing a sufficient extension of a knowledge base\n(KB) that entails a conclusion not entailed by the original KB. It serves to\ncompute explanations, or hypotheses, for such missing entailments. While this\ntask has been intensively investigated for perfect data and under classical\nsemantics, less is known about abduction when erroneous data results in\ninconsistent KBs. In this paper we define a suitable notion of abduction under\nrepair semantics, and propose a set of minimality criteria that guides\nabduction towards `useful' hypotheses. We provide initial complexity results on\ndeciding existence of and verifying abductive solutions with these criteria,\nunder different repair semantics and for the description logics DL-Lite and\nEL_bot.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\uff08KB\uff09\u4e0b\u7684\u53cd\u7ece\u63a8\u7406\uff08abduction\uff09\u4efb\u52a1\uff0c\u5b9a\u4e49\u4e86\u9002\u7528\u4e8e\u4fee\u590d\u8bed\u4e49\u7684\u53cd\u7ece\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e86\u6307\u5bfc\u53cd\u7ece\u751f\u6210\u6709\u7528\u5047\u8bbe\u7684\u6700\u5c0f\u5316\u51c6\u5219\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5728\u4e0d\u540c\u4fee\u590d\u8bed\u4e49\u548c\u63cf\u8ff0\u903b\u8f91DL-Lite\u548cEL_bot\u4e0b\u7684\u521d\u6b65\u590d\u6742\u6027\u7ed3\u679c\u3002", "motivation": "\u9488\u5bf9\u9519\u8bef\u6570\u636e\u5bfc\u81f4\u7684\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\uff0c\u7814\u7a76\u53cd\u7ece\u63a8\u7406\u4efb\u52a1\uff0c\u65e8\u5728\u8ba1\u7b97\u7f3a\u5931\u8574\u6db5\u7684\u89e3\u91ca\u6216\u5047\u8bbe\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u5728\u5b8c\u7f8e\u6570\u636e\u548c\u7ecf\u5178\u8bed\u4e49\u4e0b\u7684\u4e0d\u8db3\u3002", "method": "\u5b9a\u4e49\u4fee\u590d\u8bed\u4e49\u4e0b\u7684\u53cd\u7ece\u6982\u5ff5\uff0c\u63d0\u51fa\u4e00\u7ec4\u6700\u5c0f\u5316\u51c6\u5219\u4ee5\u6307\u5bfc\u751f\u6210\u6709\u7528\u5047\u8bbe\uff0c\u5e76\u5206\u6790\u5176\u5728\u4e0d\u540c\u4fee\u590d\u8bed\u4e49\u548c\u63cf\u8ff0\u903b\u8f91\uff08DL-Lite\u548cEL_bot\uff09\u4e0b\u7684\u590d\u6742\u6027\uff0c\u5305\u62ec\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u6027\u548c\u9a8c\u8bc1\u3002", "result": "\u63d0\u4f9b\u4e86\u53cd\u7ece\u89e3\u51b3\u65b9\u6848\u5728\u4e0d\u540c\u4fee\u590d\u8bed\u4e49\u548c\u63cf\u8ff0\u903b\u8f91\u4e0b\u7684\u521d\u6b65\u590d\u6742\u6027\u5206\u6790\u7ed3\u679c\uff0c\u5305\u62ec\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u6027\u548c\u9a8c\u8bc1\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u4e49\u4fee\u590d\u8bed\u4e49\u4e0b\u7684\u53cd\u7ece\u4efb\u52a1\u548c\u5f15\u5165\u6700\u5c0f\u5316\u51c6\u5219\uff0c\u8bba\u6587\u4e3a\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\u4e0b\u7684\u53cd\u7ece\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u590d\u6742\u6027\u5206\u6790\uff0c\u6269\u5c55\u4e86\u53cd\u7ece\u63a8\u7406\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.21739", "pdf": "https://arxiv.org/pdf/2507.21739", "abs": "https://arxiv.org/abs/2507.21739", "authors": ["Zekai Sun", "Xiuxian Guan", "Zheng Lin", "Yuhao Qing", "Haoze Song", "Zihan Fang", "Zhe Chen", "Fangming Liu", "Heming Cui", "Wei Ni", "Jun Luo"], "title": "RRTO: A High-Performance Transparent Offloading System for Model Inference in Mobile Edge Computing", "categories": ["cs.NI"], "comment": "15 pages, 12 figures,", "summary": "Deploying Machine Learning (ML) applications on resource-constrained mobile\ndevices remains challenging due to limited computational resources and poor\nplatform compatibility. While Mobile Edge Computing (MEC) offers\noffloading-based inference paradigm using GPU servers, existing approaches are\ndivided into non-transparent and transparent methods, with the latter\nnecessitating modifications to the source code. Non-transparent offloading\nachieves high performance but requires intrusive code modification, limiting\ncompatibility with diverse applications. Transparent offloading, in contrast,\noffers wide compatibility but introduces significant transmission delays due to\nper-operator remote procedure calls (RPCs). To overcome this limitation, we\npropose RRTO, the first high-performance transparent offloading system tailored\nfor MEC inference. RRTO introduces a record/replay mechanism that leverages the\nstatic operator sequence in ML models to eliminate repetitive RPCs. To reliably\nidentify this sequence, RRTO integrates a novel Operator Sequence Search\nalgorithm that detects repeated patterns, filters initialization noise, and\naccelerates matching via a two-level strategy. Evaluation demonstrates that\nRRTO achieves substantial reductions of up to 98% in both per-inference latency\nand energy consumption compared to state-of-the-art transparent methods and\nyields results comparable to non-transparent approaches, all without\nnecessitating any source code modification.", "AI": {"tldr": "RRTO\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u900f\u660e\u7684MEC\u63a8\u7406\u5378\u8f7d\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bb0\u5f55/\u91cd\u653e\u673a\u5236\u51cf\u5c11RPC\u8c03\u7528\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u4e0aML\u5e94\u7528\u8d44\u6e90\u53d7\u9650\u53ca\u73b0\u6709\u900f\u660e\u5378\u8f7d\u65b9\u6cd5\u56e0\u9891\u7e41RPC\u8c03\u7528\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bb0\u5f55/\u91cd\u653e\u673a\u5236\u548cOperator Sequence Search\u7b97\u6cd5\uff0c\u9759\u6001\u5206\u6790ML\u6a21\u578b\u7684\u7b97\u5b50\u5e8f\u5217\u4ee5\u51cf\u5c11RPC\u8c03\u7528\u3002", "result": "\u6bd4\u73b0\u6709\u900f\u660e\u65b9\u6cd5\u964d\u4f4e98%\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u6027\u80fd\u63a5\u8fd1\u975e\u900f\u660e\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6e90\u4ee3\u7801\u3002", "conclusion": "RRTO\u5b9e\u73b0\u4e86\u9ad8\u6548\u900f\u660e\u7684MEC\u63a8\u7406\u5378\u8f7d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u517c\u5bb9\u6027\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2507.21583", "pdf": "https://arxiv.org/pdf/2507.21583", "abs": "https://arxiv.org/abs/2507.21583", "authors": ["Sergio Cobos", "Javier Luis C\u00e1novas Izquierdo"], "title": "Ethical Classification of Non-Coding Contributions in Open-Source Projects via Large Language Models", "categories": ["cs.SE"], "comment": "Accepted at the 2025 8th AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES'25)", "summary": "The development of Open-Source Software (OSS) is not only a technical\nchallenge, but also a social one due to the diverse mixture of contributors. To\nthis aim, social-coding platforms, such as GitHub, provide the infrastructure\nneeded to host and develop the code, but also the support for enabling the\ncommunity's collaboration, which is driven by non-coding contributions, such as\nissues (i.e., change proposals or bug reports) or comments to existing\ncontributions. As with any other social endeavor, this development process\nfaces ethical challenges, which may put at risk the project's sustainability.\nTo foster a productive and positive environment, OSS projects are increasingly\ndeploying codes of conduct, which define rules to ensure a respectful and\ninclusive participatory environment, with the Contributor Covenant being the\nmain model to follow. However, monitoring and enforcing these codes of conduct\nis a challenging task, due to the limitations of current approaches. In this\npaper, we propose an approach to classify the ethical quality of non-coding\ncontributions in OSS projects by relying on Large Language Models (LLM), a\npromising technology for text classification tasks. We defined a set of ethical\nmetrics based on the Contributor Covenant and developed a classification\napproach to assess ethical behavior in OSS non-coding contributions, using\nprompt engineering to guide the model's output.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u7c7b\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u975e\u4ee3\u7801\u8d21\u732e\u7684\u4f26\u7406\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u5b9a\u4e49\u4f26\u7406\u6307\u6807\u548c\u63d0\u793a\u5de5\u7a0b\u6765\u6539\u5584\u793e\u533a\u5408\u4f5c\u73af\u5883\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4f26\u7406\u6311\u6218\u53ef\u80fd\u5a01\u80c1\u9879\u76ee\u53ef\u6301\u7eed\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u76d1\u63a7\u548c\u5b9e\u65bd\u884c\u4e3a\u51c6\u5219\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u57fa\u4e8e\u300a\u8d21\u732e\u8005\u5951\u7ea6\u300b\u5b9a\u4e49\u4f26\u7406\u6307\u6807\uff0c\u5229\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u5bf9\u975e\u4ee3\u7801\u8d21\u732e\u7684\u4f26\u7406\u884c\u4e3a\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30OSS\u975e\u4ee3\u7801\u8d21\u732e\u4f26\u7406\u884c\u4e3a\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u63a7\u548c\u6539\u5584\u5f00\u6e90\u793e\u533a\u4f26\u7406\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.21195", "pdf": "https://arxiv.org/pdf/2507.21195", "abs": "https://arxiv.org/abs/2507.21195", "authors": ["Po-Yuan Mao", "Cheng-Chang Tsai", "Chun-Shien Lu"], "title": "MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models", "categories": ["cs.CR", "cs.AI", "cs.MM"], "comment": null, "summary": "The great success of the diffusion model in image synthesis led to the\nrelease of gigantic commercial models, raising the issue of copyright\nprotection and inappropriate content generation. Training-free diffusion\nwatermarking provides a low-cost solution for these issues. However, the prior\nworks remain vulnerable to rotation, scaling, and translation (RST) attacks.\nAlthough some methods employ meticulously designed patterns to mitigate this\nissue, they often reduce watermark capacity, which can result in identity (ID)\ncollusion. To address these problems, we propose MaXsive, a training-free\ndiffusion model generative watermarking technique that has high capacity and\nrobustness. MaXsive best utilizes the initial noise to watermark the diffusion\nmodel. Moreover, instead of using a meticulously repetitive ring pattern, we\npropose injecting the X-shape template to recover the RST distortions. This\ndesign significantly increases robustness without losing any capacity, making\nID collusion less likely to happen. The effectiveness of MaXsive has been\nverified on two well-known watermarking benchmarks under the scenarios of\nverification and identification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaXsive\u7684\u8bad\u7ec3\u514d\u8d39\u6269\u6563\u6a21\u578b\u6c34\u5370\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u65cb\u8f6c\u3001\u7f29\u653e\u548c\u5e73\u79fb\u653b\u51fb\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6c34\u5370\u5bb9\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5546\u7528\u6269\u6563\u6a21\u578b\u7684\u6210\u529f\u5e26\u6765\u4e86\u7248\u6743\u4fdd\u62a4\u548c\u4e0d\u6070\u5185\u5bb9\u751f\u6210\u7684\u6311\u6218\uff0c\u73b0\u6709\u8bad\u7ec3\u514d\u8d39\u6c34\u5370\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230RST\u653b\u51fb\uff0c\u4e14\u6c34\u5370\u5bb9\u91cf\u6709\u9650\u53ef\u80fd\u5bfc\u81f4\u8eab\u4efd\u51b2\u7a81\u3002", "method": "MaXsive\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\u690d\u5165\u6c34\u5370\uff0c\u5e76\u91c7\u7528X\u5f62\u6a21\u677f\u800c\u975e\u73af\u5f62\u6a21\u5f0f\u6765\u6062\u590dRST\u5931\u771f\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u548c\u5bb9\u91cf\u3002", "result": "\u5728\u4e24\u4e2a\u77e5\u540d\u6c34\u5370\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaXsive\u5728\u9a8c\u8bc1\u548c\u8bc6\u522b\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u62b5\u5fa1\u653b\u51fb\u4e14\u907f\u514d\u4e86ID\u51b2\u7a81\u3002", "conclusion": "MaXsive\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.21311", "pdf": "https://arxiv.org/pdf/2507.21311", "abs": "https://arxiv.org/abs/2507.21311", "authors": ["Martin de La Gorce", "Charlie Hewitt", "Tibor Takacs", "Robert Gerdisch", "Zafiirah Hosenie", "Givi Meishvili", "Marek Kowalski", "Thomas J. Cashman", "Antonio Criminisi"], "title": "VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Virtual 3D meetings offer the potential to enhance copresence, increase\nengagement and thus improve effectiveness of remote meetings compared to\nstandard 2D video calls. However, representing people in 3D meetings remains a\nchallenge; existing solutions achieve high quality by using complex hardware,\nmaking use of fixed appearance via enrolment, or by inverting a pre-trained\ngenerative model. These approaches lead to constraints that are unwelcome and\nill-fitting for videoconferencing applications. We present the first method to\npredict 3D Gaussian reconstructions in real time from a single 2D webcam feed,\nwhere the 3D representation is not only live and realistic, but also authentic\nto the input video. By conditioning the 3D representation on each video frame\nindependently, our reconstruction faithfully recreates the input video from the\ncaptured viewpoint (a property we call authenticity), while generalizing\nrealistically to novel viewpoints. Additionally, we introduce a stability loss\nto obtain reconstructions that are temporally stable on video sequences. We\nshow that our method delivers state-of-the-art accuracy in visual quality and\nstability metrics compared to existing methods, and demonstrate our approach in\nlive one-to-one 3D meetings using only a standard 2D camera and display. This\ndemonstrates that our approach can allow anyone to communicate volumetrically,\nvia a method for 3D videoconferencing that is not only highly accessible, but\nalso realistic and authentic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u4e002D\u6444\u50cf\u5934\u5b9e\u65f6\u9884\u6d4b3D\u9ad8\u65af\u91cd\u5efa\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e863D\u8fdc\u7a0b\u4f1a\u8bae\u7684\u903c\u771f\u6027\u548c\u771f\u5b9e\u6027\uff0c\u4e14\u65e0\u9700\u590d\u6742\u786c\u4ef6\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u4f1a\u8bae\u4e2d\u4eba\u50cf\u8868\u73b0\u8d28\u91cf\u9ad8\u4f46\u4f9d\u8d56\u590d\u6742\u786c\u4ef6\u6216\u56fa\u5b9a\u5916\u89c2\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u72ec\u7acb\u5904\u7406\u6bcf\u5e27\u89c6\u9891\uff0c\u7ed3\u5408\u7a33\u5b9a\u6027\u635f\u5931\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4e14\u7a33\u5b9a\u76843D\u9ad8\u65af\u91cd\u5efa\u3002", "result": "\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u4ec5\u75282D\u8bbe\u5907\u548c\u663e\u793a\u5668\u8fdb\u884c3D\u4f1a\u8bae\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a3D\u89c6\u9891\u4f1a\u8bae\u63d0\u4f9b\u4e86\u9ad8\u53ef\u8bbf\u95ee\u6027\u3001\u903c\u771f\u548c\u771f\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21075", "pdf": "https://arxiv.org/pdf/2507.21075", "abs": "https://arxiv.org/abs/2507.21075", "authors": ["Anushka Debnath", "Stephen Cranefield", "Emiliano Lorini", "Bastin Tony Roy Savarimuthu"], "title": "Can LLMs Reason About Trust?: A Pilot Study", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.MA"], "comment": "17 pages, 5 figures, 3 tables Accepted for presentation as a full\n  paper at the COINE 2025 workshop at AAMAS 2025 see\n  https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html", "summary": "In human society, trust is an essential component of social attitude that\nhelps build and maintain long-term, healthy relationships which creates a\nstrong foundation for cooperation, enabling individuals to work together\neffectively and achieve shared goals. As many human interactions occur through\nelectronic means such as using mobile apps, the potential arises for AI systems\nto assist users in understanding the social state of their relationships. In\nthis paper we investigate the ability of Large Language Models (LLMs) to reason\nabout trust between two individuals in an environment which requires fostering\ntrust relationships. We also assess whether LLMs are capable of inducing trust\nby role-playing one party in a trust based interaction and planning actions\nwhich can instil trust.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u4e0e\u6784\u5efa\u4eba\u9645\u5173\u7cfb\u4fe1\u4efb\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u5176\u662f\u5426\u80fd\u591f\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u6765\u8bf1\u5bfc\u4fe1\u4efb\u3002", "motivation": "\u4fe1\u4efb\u662f\u4eba\u9645\u5173\u7cfb\u4e2d\u7684\u5173\u952e\u56e0\u7d20\uff0c\u968f\u7740\u7535\u5b50\u4ea4\u4e92\u7684\u666e\u53ca\uff0cAI\u7cfb\u7edf\u53ef\u80fd\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u793e\u4ea4\u72b6\u6001\uff0cLLMs\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u503c\u5f97\u7814\u7a76\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30LLMs\u5728\u4fe1\u4efb\u63a8\u7406\u53ca\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u80fd\u5426\u89c4\u5212\u884c\u52a8\u4ee5\u5efa\u7acb\u4fe1\u4efb\u3002", "result": "\u7814\u7a76\u8bc1\u5b9eLLMs\u5177\u5907\u4e00\u5b9a\u7684\u4fe1\u4efb\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u4e92\u52a8\u4e2d\u901a\u8fc7\u884c\u52a8\u8bf1\u5bfc\u4fe1\u4efb\u3002", "conclusion": "LLMs\u5728\u4fe1\u4efb\u6784\u5efa\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u5728\u590d\u6742\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.21937", "pdf": "https://arxiv.org/pdf/2507.21937", "abs": "https://arxiv.org/abs/2507.21937", "authors": ["Michelle L. Wu"], "title": "A Grover-Based Quantum Algorithm for Solving Perfect Mazes via Fitness-Guided Search", "categories": ["quant-ph", "cs.ET", "math.QA", "81P68, 68Q12", "F.1.2; F.2.2; I.2.8"], "comment": "11 pages. Submitted under MIT-LL & MITRE Corporation 2025.\n  Theoretical proposal; future work will extend to implementation and\n  benchmarking", "summary": "We present a quantum algorithm for solving perfect mazes by casting the\npathfinding task as a structured search problem. Building on Grover's amplitude\namplification, the algorithm encodes all candidate paths in superposition and\nevaluates their proximity to the goal using a reversible fitness operator based\non quantum arithmetic. A Grover-compatible oracle marks high-fitness states,\nand an adaptive cutoff strategy refines the search iteratively. We provide\nformal definitions, unitary constructions, and convergence guarantees, along\nwith a resource analysis showing efficient scaling with maze size and path\nlength. The framework serves as a foundation for quantum-hybrid pathfinding and\nplanning. The full algorithmic pipeline is specified from encoding to\namplification, including oracle design and fitness evaluation. The approach is\nreadily extensible to other search domains, including navigation over tree-like\nor acyclic graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eGrover\u632f\u5e45\u653e\u5927\u91cf\u5b50\u7b97\u6cd5\u7684\u5b8c\u7f8e\u8ff7\u5bab\u6c42\u89e3\u65b9\u6cd5\uff0c\u5c06\u8def\u5f84\u641c\u7d22\u95ee\u9898\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u641c\u7d22\u95ee\u9898\u3002", "motivation": "\u901a\u8fc7\u91cf\u5b50\u7b97\u6cd5\u63d0\u5347\u8ff7\u5bab\u8def\u5f84\u641c\u7d22\u7684\u6548\u7387\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5176\u4ed6\u641c\u7d22\u9886\u57df\u7684\u6269\u5c55\u5e94\u7528\u3002", "method": "\u5229\u7528Grover\u7684\u632f\u5e45\u653e\u5927\u6280\u672f\uff0c\u7f16\u7801\u6240\u6709\u5019\u9009\u8def\u5f84\u4e3a\u53e0\u52a0\u6001\uff0c\u5e76\u901a\u8fc7\u53ef\u9006\u9002\u5e94\u5ea6\u7b97\u5b50\u8bc4\u4f30\u8def\u5f84\u63a5\u8fd1\u76ee\u6807\u7684\u7a0b\u5ea6\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u622a\u6b62\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\u641c\u7d22\u3002", "result": "\u7b97\u6cd5\u5728\u8ff7\u5bab\u5c3a\u5bf8\u548c\u8def\u5f84\u957f\u5ea6\u4e0a\u8868\u73b0\u9ad8\u6548\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u91cf\u5b50-\u6df7\u5408\u8def\u5f84\u89c4\u5212\u6846\u67b6\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u8ff7\u5bab\u6c42\u89e3\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u6811\u72b6\u6216\u65e0\u73af\u56fe\u7b49\u641c\u7d22\u9886\u57df\uff0c\u4e3a\u91cf\u5b50-\u6df7\u5408\u8def\u5f84\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.21096", "pdf": "https://arxiv.org/pdf/2507.21096", "abs": "https://arxiv.org/abs/2507.21096", "authors": ["Krishnendu Das"], "title": "HexaMorphHash HMH- Homomorphic Hashing for Secure and Efficient Cryptographic Operations in Data Integrity Verification", "categories": ["cs.CR", "cs.DB"], "comment": null, "summary": "In the realm of big data and cloud computing, distributed systems are tasked\nwith proficiently managing, storing, and validating extensive datasets across\nnumerous nodes, all while maintaining robust data integrity. Conventional\nhashing methods, though straightforward, encounter substan tial difficulties in\ndynamic settings due to the necessity for thorough rehashing when nodes are\naltered. Consistent hashing mitigates some of these challenges by reducing data\nredistribution; however, it still contends with limitations in load balancing\nand scalability under intensive update conditions. This paper introduces an\ninnovative approach using a lattice based homomorphic hash function\nHexaMorphHash that facilitates constant time, incremental updates while\npreserving a constant digest size. By utilizing the complexity of the Short\nInteger Solutions SIS problem, our method secures strong protective measures,\neven against quantum threats. We further com pare our method with existing ones\nsuch as direct signatures for each update, comprehensive database signing,\nMerkle tree based techniques, AdHash, MuHash, ECMH, and homomorphic sig nature\nschemes highlighting notable advancements in computational efficiency, memory\nusage, and scalability. Our contributions present a viable solution for\nfrequent update dissemination in expansive distributed systems, safeguarding\nboth data integrity and system performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u7684\u65b0\u578b\u540c\u6001\u54c8\u5e0c\u51fd\u6570HexaMorphHash\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u52a8\u6001\u6570\u636e\u66f4\u65b0\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u5b8c\u6574\u6027\u3002", "motivation": "\u4f20\u7edf\u54c8\u5e0c\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u8282\u70b9\u53d8\u66f4\u9700\u8981\u5168\u9762\u91cd\u65b0\u54c8\u5e0c\uff0c\u800c\u4e00\u81f4\u6027\u54c8\u5e0c\u5c3d\u7ba1\u51cf\u5c11\u4e86\u6570\u636e\u91cd\u5206\u5e03\uff0c\u4f46\u5728\u9ad8\u8d1f\u8f7d\u548c\u9891\u7e41\u66f4\u65b0\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u683c\u7684\u540c\u6001\u54c8\u5e0c\u51fd\u6570HexaMorphHash\uff0c\u7ed3\u5408\u77ed\u6574\u6570\u89e3\uff08SIS\uff09\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u5e38\u91cf\u65f6\u95f4\u7684\u589e\u91cf\u66f4\u65b0\u548c\u56fa\u5b9a\u5927\u5c0f\u7684\u6458\u8981\u3002", "result": "HexaMorphHash\u5728\u8ba1\u7b97\u6548\u7387\u3001\u5185\u5b58\u4f7f\u7528\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76f4\u63a5\u7b7e\u540d\u3001Merkle\u6811\u3001AdHash\u7b49\uff09\uff0c\u5e76\u80fd\u62b5\u6297\u91cf\u5b50\u5a01\u80c1\u3002", "conclusion": "HexaMorphHash\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u9891\u7e41\u6570\u636e\u66f4\u65b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u6570\u636e\u5b8c\u6574\u6027\u548c\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.21685", "pdf": "https://arxiv.org/pdf/2507.21685", "abs": "https://arxiv.org/abs/2507.21685", "authors": ["Marlon Etheredge", "Thomas Fahringer", "Felix Erlacher", "Elias Kohler", "Stefan Pedratscher", "Juan Aznar-Poveda", "Nishant Saurabh", "Adrien Lebre"], "title": "Collaborative State Machines: A Better Programming Model for the Cloud-Edge-IoT Continuum", "categories": ["cs.DC"], "comment": null, "summary": "The development of Cloud-Edge-IoT applications requires robust programming\nmodels. Existing models often struggle to manage the dynamic and stateful\nnature of these applications effectively. This paper introduces the\nCollaborative State Machines (CSM) programming model to address these\ncomplexities. CSM facilitates the development of reactive, event-driven, and\nstateful applications targeting the Cloud-Edge-IoT continuum. Applications\nbuilt with CSM are composed of state machines that collaborate autonomously and\ncan be distributed across different layers of the continuum. Key features of\nCSM include (i) a sophisticated collaboration mechanism among state machines\nutilizing events and persistent data; (ii) encapsulation of state through the\ninherent state of state machines and persistent data; (iii) integration of\nactions and service invocations within states and state transitions, thereby\ndecoupling complex application logic from compute and data processing services;\nand (iv) an advanced data model that supports the processing of local, static,\nand persistent data with defined scope and lifetime. In addition to introducing\nthe CSM programming model, we present a runtime system and a comprehensive\nevaluation of our approach. This evaluation is based on three use cases: a\nstress test on a large-scale infrastructure, a surveillance system application,\nand a complex smart factory scenario, all deployed on the Grid'5000 testbed.\nOur results demonstrate a 12x increase in throughput through novel language\nfeatures in the stress test. Compared to Serverless Workflow, a\nstate-of-the-art baseline system, we show a 2.3x improvement in processing time\nper processed image in a surveillance system use case, a 55x reduction in total\nprocessing time for a smart factory use case, and an overall improvement in\nproductivity across these use cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u534f\u4f5c\u72b6\u6001\u673a\uff08CSM\uff09\u7f16\u7a0b\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3Cloud-Edge-IoT\u5e94\u7528\u4e2d\u52a8\u6001\u548c\u72b6\u6001\u7ba1\u7406\u7684\u96be\u9898\u3002CSM\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u3001\u72b6\u6001\u5c01\u88c5\u548c\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5f00\u53d1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7f16\u7a0b\u6a21\u578b\u96be\u4ee5\u6709\u6548\u7ba1\u7406Cloud-Edge-IoT\u5e94\u7528\u7684\u52a8\u6001\u548c\u72b6\u6001\u5316\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7f16\u7a0b\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u72b6\u6001\u673a\uff08CSM\uff09\u6a21\u578b\uff0c\u652f\u6301\u4e8b\u4ef6\u9a71\u52a8\u3001\u72b6\u6001\u5c01\u88c5\u548c\u5206\u5e03\u5f0f\u534f\u4f5c\uff0c\u5e76\u5f00\u53d1\u4e86\u8fd0\u884c\u65f6\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCSM\u5728\u538b\u529b\u6d4b\u8bd5\u4e2d\u541e\u5410\u91cf\u63d0\u534712\u500d\uff0c\u5728\u76d1\u63a7\u7cfb\u7edf\u548c\u667a\u80fd\u5de5\u5382\u7528\u4f8b\u4e2d\u5206\u522b\u63d0\u53472.3\u500d\u548c55\u500d\u6027\u80fd\u3002", "conclusion": "CSM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u9002\u7528\u4e8eCloud-Edge-IoT\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e94\u7528\u6027\u80fd\u548c\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2507.21974", "pdf": "https://arxiv.org/pdf/2507.21974", "abs": "https://arxiv.org/abs/2507.21974", "authors": ["Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Yibin Kang", "Haozhe Zhang", "Merouane Debbah", "Fadhel Ayed"], "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks", "categories": ["cs.AI", "cs.NI"], "comment": null, "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u6839\u56e0\u5206\u6790\uff08RCA\uff09\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684RCA\u9700\u8981\u53ef\u89e3\u91ca\u6027\u3001\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u73b0\u6709\u5f00\u6e90LLM\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u9886\u57df\u9002\u914d\u3002", "method": "\u5f15\u5165TeleLogs\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "result": "\u591a\u89c4\u6a21LLM\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u968f\u673a\u6d4b\u8bd5\u53d8\u4f53\u3002", "conclusion": "\u9886\u57df\u9002\u914d\u548c\u63a8\u7406\u589e\u5f3a\u7684LLM\u5728\u7f51\u7edc\u8fd0\u7ef4\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6f5c\u529b\u3002"}}
{"id": "2507.21678", "pdf": "https://arxiv.org/pdf/2507.21678", "abs": "https://arxiv.org/abs/2507.21678", "authors": ["Yiming Xu", "Runzhi He", "Hengzhi Ye", "Minghui Zhou", "Huaimin Wang"], "title": "Predicting Maintenance Cessation of Open Source Software Repositories with An Integrated Feature Framework", "categories": ["cs.SE"], "comment": null, "summary": "The maintenance risks of open source software (OSS) projects pose significant\nthreats to the quality, security, and resilience of modern software supply\nchains. While prior research has proposed diverse approaches for predicting OSS\nmaintenance risk -- leveraging signals ranging from surface features (e.g.,\nstars, commits) to social network analyses and behavioral patterns -- existing\nmethods often suffer from ambiguous operational definitions, limited\ninterpretability, and datasets of insufficient scale or generalizability. In\nthis work, we introduce ``maintenance cessation'', grounded in both explicit\narchival status and rigorous semantic analysis of project documentation.\nBuilding on this foundation, we curate a large-scale, longitudinal dataset of\n115,466 GitHub repositories -- encompassing 57,733 confirmed cessation events\n-- complemented by comprehensive, timeline-based behavioral features. We\npropose an integrated, multi-perspective feature framework for predicting\nmaintenance cessation, systematically combining user-centric features,\nmaintainer-centric features and project evolution features. AFT survival\nanalysis demonstrates a high C-index (0.846), substantially outperforming\nmodels relying only on surface features. Feature ablation and SHAP analysis\nfurther confirm the effectiveness and interpretability of our approach.\nFinally, we demonstrate real-world applicability by deploying a GBSA classifier\nin the openEuler ecosystem for proactive package risk screening. Our work\nestablishes a scalable, interpretable foundation for maintenance-risk\nprediction, enabling reproducible risk management across large-scale open\nsource ecosystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u6790\u548c\u660e\u786e\u5f52\u6863\u72b6\u6001\u7684\u2018\u7ef4\u62a4\u7ec8\u6b62\u2019\u5b9a\u4e49\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u7279\u5f81\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u9879\u76ee\u7684\u7ef4\u62a4\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u5728\u751f\u5b58\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u9879\u76ee\u7684\u7ef4\u62a4\u98ce\u9669\u5bf9\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u7684\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u5f39\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5b9a\u4e49\u6a21\u7cca\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u548c\u6570\u636e\u5c40\u9650\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u2018\u7ef4\u62a4\u7ec8\u6b62\u2019\u5b9a\u4e49\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u6790\u548c\u5f52\u6863\u72b6\u6001\uff0c\u6784\u5efa\u5305\u542b115,466\u4e2aGitHub\u4ed3\u5e93\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u89c6\u89d2\u7279\u5f81\u6846\u67b6\uff08\u7528\u6237\u3001\u7ef4\u62a4\u8005\u548c\u9879\u76ee\u6f14\u5316\u7279\u5f81\uff09\uff0c\u5e76\u5e94\u7528AFT\u751f\u5b58\u5206\u6790\u548cSHAP\u5206\u6790\u3002", "result": "AFT\u751f\u5b58\u5206\u6790\u7684C-index\u8fbe\u52300.846\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u8868\u9762\u7279\u5f81\u7684\u6a21\u578b\uff0c\u7279\u5f81\u6d88\u878d\u548cSHAP\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u7684\u7ef4\u62a4\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u8bc1\u660e\u4e86\u5176\u9002\u7528\u6027\u3002"}}
{"id": "2507.21507", "pdf": "https://arxiv.org/pdf/2507.21507", "abs": "https://arxiv.org/abs/2507.21507", "authors": ["Shibo Gao", "Peipei Yang", "Yangyang Liu", "Yi Chen", "Han Zhu", "Xuyao Zhang", "Linlin Huang"], "title": "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding", "categories": ["cs.CV", "cs.MM"], "comment": "21 pages, 19 figures, 8 tables", "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events in videos and\naccurately determine their time intervals. Current VAD methods mainly fall into\ntwo categories: traditional DNN-based approaches that focus on temporal\nlocalization, and LLM-based approaches that emphasize semantic understanding.\nBoth anomaly understanding and grounding are essential for comprehensive video\nanomaly detection and can complement each other. However, no existing model or\ndataset supports both tasks simultaneously. To address this, we introduce VAGU\n(Video Anomaly Grounding and Understanding), the first benchmark to integrate\nboth tasks. Each VAGU instance includes annotations for anomaly category,\nsemantic explanation, precise temporal grounding and Video QA. We also provide\nmultiple-choice Video QA for objective evaluation. Based on this dataset, we\npropose Glance then Scrutinize (GtS), a training-free framework guided by\ntextual prompts. The framework first enables coarse localization of\nhigh-probability anomalous regions, followed by detailed anomaly interpretation\nand temporal boundary refinement. Additionally, we propose the JeAUG metric,\nwhich jointly evaluates semantic interpretability and temporal precision,\novercoming the limitations of traditional metrics. Extensive experiments verify\nthe effectiveness of our benchmark, framework, and evaluation metric.", "AI": {"tldr": "\u63d0\u51fa\u4e86VAGU\u6570\u636e\u96c6\u548cGtS\u6846\u67b6\uff0c\u9996\u6b21\u540c\u65f6\u652f\u6301\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u548c\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b8c\u6210\u5f02\u5e38\u5b9a\u4f4d\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4e14\u7f3a\u4e4f\u652f\u6301\u6b64\u7c7b\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faVAGU\u6570\u636e\u96c6\uff0c\u5305\u542b\u5f02\u5e38\u7c7b\u522b\u3001\u8bed\u4e49\u89e3\u91ca\u7b49\u6807\u6ce8\uff1b\u63d0\u51faGtS\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\u5b8c\u6210\u7c97\u5b9a\u4f4d\u548c\u7ec6\u7c92\u5ea6\u89e3\u91ca\u4e0e\u65f6\u5e8f\u8fb9\u754c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VAGU\u6570\u636e\u96c6\u548cGtS\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "VAGU\u6570\u636e\u96c6\u548cGtS\u6846\u67b6\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\uff0c\u540c\u65f6JeAUG\u6307\u6807\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4ef7\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.21411", "pdf": "https://arxiv.org/pdf/2507.21411", "abs": "https://arxiv.org/abs/2507.21411", "authors": ["Kentaro Takahira", "Yue Yu", "Takanori Fujiwara", "Suzuki Ryo", "Huamin Qu"], "title": "InSituTale: Enhancing Augmented Data Storytelling with Physical Objects", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Augmented data storytelling enhances narrative delivery by integrating\nvisualizations with physical environments and presenter actions. Existing\nsystems predominantly rely on body gestures or speech to control\nvisualizations, leaving interactions with physical objects largely\nunderexplored. We introduce augmented physical data storytelling, an approach\nenabling presenters to manipulate visualizations through physical object\ninteractions. To inform this approach, we first conducted a survey of\ndata-driven presentations to identify common visualization commands. We then\nconducted workshops with nine HCI/VIS researchers to collect mappings between\nphysical manipulations and these commands. Guided by these insights, we\ndeveloped InSituTale, a prototype that combines object tracking via a depth\ncamera with Vision-LLM for detecting real-world events. Through physical\nmanipulations, presenters can dynamically execute various visualization\ncommands, delivering cohesive data storytelling experiences that blend physical\nand digital elements. A user study with 12 participants demonstrated that\nInSituTale enables intuitive interactions, offers high utility, and facilitates\nan engaging presentation experience.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7269\u7406\u5bf9\u8c61\u4ea4\u4e92\u589e\u5f3a\u6570\u636e\u53d9\u4e8b\u7684\u7cfb\u7edfInSituTale\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6\u6444\u50cf\u5934\u548cVision-LLM\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u76f4\u89c2\u4e14\u6c89\u6d78\u5f0f\u7684\u6570\u636e\u5c55\u793a\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u624b\u52bf\u6216\u8bed\u97f3\u63a7\u5236\u6570\u636e\u53ef\u89c6\u5316\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u5bf9\u8c61\u4ea4\u4e92\u7684\u63a2\u7d22\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u589e\u5f3a\u6570\u636e\u53d9\u4e8b\u7684\u6c89\u6d78\u611f\u548c\u4e92\u52a8\u6027\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8c03\u67e5\u4e86\u6570\u636e\u9a71\u52a8\u6f14\u793a\u4e2d\u5e38\u89c1\u7684\u53ef\u89c6\u5316\u547d\u4ee4\uff0c\u5e76\u901a\u8fc7\u4e0eHCI/VIS\u7814\u7a76\u8005\u7684\u5de5\u4f5c\u574a\u6536\u96c6\u7269\u7406\u64cd\u4f5c\u4e0e\u547d\u4ee4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u6700\u7ec8\u5f00\u53d1\u4e86\u7ed3\u5408\u6df1\u5ea6\u6444\u50cf\u5934\u548cVision-LLM\u7684\u539f\u578bInSituTale\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cInSituTale\u80fd\u63d0\u4f9b\u76f4\u89c2\u7684\u4ea4\u4e92\u65b9\u5f0f\u3001\u9ad8\u5b9e\u7528\u6027\u548c\u5f15\u4eba\u5165\u80dc\u7684\u6f14\u793a\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7269\u7406\u5bf9\u8c61\u4ea4\u4e92\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u6570\u636e\u53d9\u4e8b\u7684\u6c89\u6d78\u611f\u548c\u4e92\u52a8\u6027\u3002"}}
{"id": "2507.21077", "pdf": "https://arxiv.org/pdf/2507.21077", "abs": "https://arxiv.org/abs/2507.21077", "authors": ["Naba Rizvi"], "title": "Data-Driven and Participatory Approaches toward Neuro-Inclusive AI", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation at UC San Diego (June 2025)", "summary": "Biased data representation in AI marginalizes up to 75 million autistic\npeople worldwide through medical applications viewing autism as a deficit of\nneurotypical social skills rather than an aspect of human diversity, and this\nperspective is grounded in research questioning the humanity of autistic\npeople. Turing defined artificial intelligence as the ability to mimic human\ncommunication, and as AI development increasingly focuses on human-like agents,\nthis benchmark remains popular. In contrast, we define Neuro-Inclusive AI as\ndatasets and systems that move away from mimicking humanness as a benchmark for\nmachine intelligence. Then, we explore the origins, prevalence, and impact of\nanti-autistic biases in current research. Our work finds that 90% of human-like\nAI agents exclude autistic perspectives, and AI creators continue to believe\nethical considerations are beyond the scope of their work. To improve the\nautistic representation in data, we conduct empirical experiments with\nannotators and LLMs, finding that binary labeling schemes sufficiently capture\nthe nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can\nbe used to evaluate or fine-tune models, and was developed to serve as a\nfoundation for more neuro-inclusive future work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u795e\u7ecf\u5305\u5bb9AI\u7684\u6982\u5ff5\uff0c\u6279\u8bc4\u73b0\u6709AI\u7814\u7a76\u4e2d\u6392\u65a5\u81ea\u95ed\u75c7\u89c6\u89d2\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6AUTALIC\u6765\u6539\u5584\u6570\u636e\u4ee3\u8868\u6027\u3002", "motivation": "\u5f53\u524dAI\u7814\u7a76\u4e2d\u5b58\u5728\u5bf9\u81ea\u95ed\u75c7\u7684\u504f\u89c1\uff0c\u5c06\u5176\u89c6\u4e3a\u795e\u7ecf\u5178\u578b\u793e\u4ea4\u80fd\u529b\u7684\u7f3a\u9677\u800c\u975e\u4eba\u7c7b\u591a\u6837\u6027\u7684\u4e00\u90e8\u5206\u3002\u8fd9\u79cd\u504f\u89c1\u5f71\u54cd\u4e86\u5168\u74037500\u4e07\u81ea\u95ed\u75c7\u60a3\u8005\u3002", "method": "\u5b9a\u4e49\u795e\u7ecf\u5305\u5bb9AI\uff0c\u63a2\u8ba8\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u504f\u89c1\u8d77\u6e90\u3001\u666e\u904d\u6027\u548c\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e8c\u5143\u6807\u7b7e\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "result": "90%\u7684\u4eba\u578bAI\u4ee3\u7406\u6392\u65a5\u81ea\u95ed\u75c7\u89c6\u89d2\uff0c\u5f00\u53d1\u7684AUTALIC\u57fa\u51c6\u53ef\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\u6216\u5fae\u8c03\u3002", "conclusion": "AUTALIC\u57fa\u51c6\u4e3a\u672a\u6765\u795e\u7ecf\u5305\u5bb9\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u547c\u5401AI\u5f00\u53d1\u8005\u91cd\u89c6\u4f26\u7406\u8003\u91cf\u3002"}}
{"id": "2507.21956", "pdf": "https://arxiv.org/pdf/2507.21956", "abs": "https://arxiv.org/abs/2507.21956", "authors": ["Atiquzzaman Mondal", "Amira Bendaimi", "Huseyin Arslan"], "title": "A Novel Framework for Near-Field Covert Communications with RIS and RSMA", "categories": ["eess.SP", "cs.ET"], "comment": "12 pages, 7 figures, IEEE Transactions on Communications", "summary": "This paper explores the near field (NF) covert communication with the aid of\nrate-splitting multiple access (RSMA) and reconfigurable intelligent surfaces\n(RIS). In particular, the RIS operates in the NF of both the legitimate user\nand the passive adversary, enhancing the legitimate users received signal while\nsuppressing the adversarys detection capability. Whereas, the base station (BS)\napplies RSMA to increase the covert communication rate composed of a private\nand a shared rate component. To characterize system covertness, we derive\nclosed form expressions for the detection error probability (DEP), outage\nprobability (OP), and optimal detection threshold for the adversary. We\nformulate a non-convex joint beamforming optimization problem at the BS and RIS\nunder unit-modulus constraints to maximize the covert rate. To tackle this, we\npropose an alternating optimization (AO) algorithm, where the BS beamformer is\ndesigned using a two-stage iterative method based on successive convex\napproximation (SCA). Additionally, two low-complexity techniques are introduced\nto further reduce the adversarys received power. Simulation results demonstrate\nthat the proposed algorithm effectively improves the covert communication rate,\nhighlighting the potential of near field RSMA-RIS integration in covert\ncommunication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u901f\u7387\u5206\u5272\u591a\u5740(RSMA)\u548c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u7684\u8fd1\u573a(NF)\u9690\u853d\u901a\u4fe1\u3002\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u7cfb\u7edf\u5728\u9690\u853d\u6027\u548c\u901a\u4fe1\u901f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u8fd1\u573a\u73af\u5883\u4e0b\u9690\u853d\u901a\u4fe1\u7684\u6f5c\u529b\uff0c\u7ed3\u5408RSMA\u548cRIS\u6280\u672f\u4ee5\u63d0\u9ad8\u901a\u4fe1\u901f\u7387\u5e76\u964d\u4f4e\u654c\u65b9\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528RSMA\u589e\u52a0\u9690\u853d\u901a\u4fe1\u901f\u7387\uff0cRIS\u589e\u5f3a\u7528\u6237\u4fe1\u53f7\u5e76\u6291\u5236\u654c\u65b9\u68c0\u6d4b\u3002\u63d0\u51fa\u4e86\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u95ee\u9898\u548c\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9690\u853d\u901a\u4fe1\u901f\u7387\uff0c\u5c55\u793a\u4e86RSMA\u548cRIS\u5728\u9690\u853d\u901a\u4fe1\u4e2d\u7684\u534f\u540c\u4f5c\u7528\u3002", "conclusion": "\u8fd1\u573aRSMA-RIS\u96c6\u6210\u5728\u9690\u853d\u901a\u4fe1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4f18\u5316\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.21340", "pdf": "https://arxiv.org/pdf/2507.21340", "abs": "https://arxiv.org/abs/2507.21340", "authors": ["Satyananda Kashyap", "Sola Shirai", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text", "summary": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86StructText\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff08\u5982\u952e\u503c\u5bf9\uff09\u7684\u8d28\u91cf\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7279\u5b9a\u9886\u57df\u6216\u7ec4\u7ec7\u6587\u6863\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u8d28\u91cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u201c\u8ba1\u5212-\u6267\u884c\u201d\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u73b0\u6709\u8868\u683c\u6570\u636e\u81ea\u52a8\u751f\u6210\u76f8\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u8bc4\u4f30\u7b56\u7565\u786e\u4fdd\u6587\u672c\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5bf9\u9f50\u3002", "result": "\u572849\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e8671,539\u4e2a\u793a\u4f8b\uff0c\u7ed3\u679c\u663e\u793aLLMs\u5728\u4e8b\u5b9e\u6027\u548c\u907f\u514d\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u751f\u6210\u7684\u6587\u672c\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u53ef\u63d0\u53d6\u6027\u4e0a\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "StructText\u4e3a\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u3001\u6570\u636e\u96c6\u548c\u5de5\u5177\uff0c\u652f\u6301\u540e\u7eed\u5de5\u4f5c\u7684\u6269\u5c55\u548c\u6539\u8fdb\u3002"}}
{"id": "2507.21791", "pdf": "https://arxiv.org/pdf/2507.21791", "abs": "https://arxiv.org/abs/2507.21791", "authors": ["Erin Carson", "Yuxin Ma"], "title": "The Performance of Low-Synchronization Variants of Reorthogonalized Block Classical Gram--Schmidt", "categories": ["cs.DC", "cs.NA", "math.NA", "65F10, 65F25, 65G50, 65Y20"], "comment": "7 pages, 2 figures", "summary": "Numerous applications, such as Krylov subspace solvers, make extensive use of\nthe block classical Gram-Schmidt (BCGS) algorithm and its reorthogonalized\nvariants for orthogonalizing a set of vectors. For large-scale problems in\ndistributed memory settings, the communication cost, particularly the global\nsynchronization cost, is a major performance bottleneck. In recent years, many\nlow-synchronization BCGS variants have been proposed in an effort to reduce the\nnumber of synchronization points. The work [E. Carson, Y. Ma, arXiv preprint\n2411.07077] recently proposed stable one-synchronization and\ntwo-synchronization variants of BCGS, i.e., BCGSI+P-1S and BCGSI+P-2S. In this\nwork, we evaluate the performance of BCGSI+P-1S and BCGSI+P-2S on a distributed\nmemory system compared to other well-known low-synchronization BCGS variants.\nIn comparison to the classical reorthogonalized BCGS algorithm (BCGSI+),\nnumerical experiments demonstrate that BCGSI+P-1S and BCGSI+P-2S can achieve up\nto 4 times and 2 times speedups, respectively, and perform similarly to other\n(less stable) one-synchronization and two-synchronization variants. BCGSI+P-1S\nand BCGSI+P-2S are therefore recommended as the best choice in practice for\ncomputing an economic QR factorization on distributed memory systems due to\ntheir superior stability when compared to other variants with the same\nsynchronization cost.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u4f4e\u540c\u6b65\u6210\u672c\u7684BCGS\u53d8\u4f53BCGSI+P-1S\u548cBCGSI+P-2S\u5728\u5206\u5e03\u5f0f\u5185\u5b58\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u4ed6\u53d8\u4f53\u7684\u7a33\u5b9a\u6027\u4e0e\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u5185\u5b58\u7cfb\u7edf\u4e2d\u5168\u5c40\u540c\u6b65\u6210\u672c\u7684\u9ad8\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u540c\u6b65\u53d8\u4f53\u63d0\u5347\u6b63\u4ea4\u5316\u7b97\u6cd5\u7684\u6548\u7387\u3002", "method": "\u8bc4\u4f30BCGSI+P-1S\u548cBCGSI+P-2S\u4e0e\u5176\u4ed6\u4f4e\u540c\u6b65BCGS\u53d8\u4f53\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5bf9\u6bd4\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "result": "BCGSI+P-1S\u548cBCGSI+P-2S\u5206\u522b\u5b9e\u73b0\u6700\u9ad84\u500d\u548c2\u500d\u52a0\u901f\uff0c\u8868\u73b0\u4e0e\u4e0d\u7a33\u5b9a\u53d8\u4f53\u76f8\u5f53\uff0c\u4f46\u7a33\u5b9a\u6027\u66f4\u4f18\u3002", "conclusion": "BCGSI+P-1S\u548cBCGSI+P-2S\u56e0\u5176\u9ad8\u7a33\u5b9a\u6027\u548c\u4f4e\u540c\u6b65\u6210\u672c\uff0c\u6210\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7ecf\u6d4eQR\u5206\u89e3\u7684\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2507.22019", "pdf": "https://arxiv.org/pdf/2507.22019", "abs": "https://arxiv.org/abs/2507.22019", "authors": ["Kritika Garg", "Sawood Alam", "Dietrich Ayala", "Michele C. Weigle", "Michael L. Nelson"], "title": "Not Here, Go There: Analyzing Redirection Patterns on the Web", "categories": ["cs.DL", "cs.IR", "cs.NI"], "comment": "Extended version of the paper accepted at the 2025 ACM Web Science\n  Conference (WebSci 2025)", "summary": "URI redirections are integral to web management, supporting structural\nchanges, SEO optimization, and security. However, their complexities affect\nusability, SEO performance, and digital preservation. This study analyzed 11\nmillion unique redirecting URIs, following redirections up to 10 hops per URI,\nto uncover patterns and implications of redirection practices. Our findings\nrevealed that 50% of the URIs terminated successfully, while 50% resulted in\nerrors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to\nHTTPS transitions, were prevalent, reflecting adherence to SEO best practices.\nNon-canonical redirects, often involving domain or path changes, highlighted\nsignificant web migrations, rebranding, and security risks. Notable patterns\nincluded \"sink\" URIs, where multiple redirects converged, ranging from traffic\nconsolidation by global websites to deliberate \"Rickrolling.\" The study also\nidentified 62,000 custom 404 URIs, almost half being soft 404s, which could\ncompromise SEO and user experience. These findings underscore the critical role\nof URI redirects in shaping the web while exposing challenges such as outdated\nURIs, server instability, and improper error handling. This research offers a\ndetailed analysis of URI redirection practices, providing insights into their\nprevalence, types, and outcomes. By examining a large dataset, we highlight\ninefficiencies in redirection chains and examine patterns such as the use of\n\"sink\" URIs and custom error pages. This information can help webmasters,\nresearchers, and digital archivists improve web usability, optimize resource\nallocation, and safeguard valuable online content.", "AI": {"tldr": "\u5206\u67901100\u4e07\u4e2aURI\u91cd\u5b9a\u5411\uff0c\u63ed\u793a50%\u6210\u529f\u91cd\u5b9a\u5411\uff0c50%\u51fa\u73b0\u9519\u8bef\uff0c\u5305\u62ec0.06%\u8d85\u8fc710\u8df3\u3002\u7814\u7a76\u8fd8\u53d1\u73b0SEO\u4f18\u5316\u548c\u5b89\u5168\u9690\u60a3\u7684\u5e38\u89c1\u6a21\u5f0f\u3002", "motivation": "URI\u91cd\u5b9a\u5411\u5bf9\u7f51\u7ad9\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u590d\u6742\u6027\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548cSEO\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u6a21\u5f0f\u548c\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u5206\u67901100\u4e07\u4e2a\u72ec\u7279URI\uff0c\u8ffd\u8e2a\u6bcf\u4e2aURI\u6700\u591a10\u8df3\u91cd\u5b9a\u5411\u3002", "result": "50%\u91cd\u5b9a\u5411\u6210\u529f\uff0c0.06%\u8d85\u8fc710\u8df3\uff1b\u53d1\u73b0SEO\u4f18\u5316\u3001\u8fc1\u79fb\u548c\u5b89\u5168\u98ce\u9669\u7b49\u5e38\u89c1\u6a21\u5f0f\u3002", "conclusion": "URI\u91cd\u5b9a\u5411\u5bf9\u7f51\u7edc\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e5f\u5e26\u6765\u6311\u6218\u3002\u7814\u7a76\u5e2e\u52a9\u4f18\u5316\u7f51\u7ad9\u7ba1\u7406\u548c\u4fdd\u62a4\u5185\u5bb9\u3002"}}
{"id": "2507.21693", "pdf": "https://arxiv.org/pdf/2507.21693", "abs": "https://arxiv.org/abs/2507.21693", "authors": ["Basak Demirok", "Mucahid Kutlu", "Selin Mergen"], "title": "MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) rapidly advance, their role in code\ngeneration has expanded significantly. While this offers streamlined\ndevelopment, it also creates concerns in areas like education and job\ninterviews. Consequently, developing robust systems to detect AI-generated code\nis imperative to maintain academic integrity and ensure fairness in hiring\nprocesses. In this study, we introduce MultiAIGCD, a dataset for AI-generated\ncode detection for Python, Java, and Go. From the CodeNet dataset's problem\ndefinitions and human-authored codes, we generate several code samples in Java,\nPython, and Go with six different LLMs and three different prompts. This\ngeneration process covered three key usage scenarios: (i) generating code from\nproblem descriptions, (ii) fixing runtime errors in human-written code, and\n(iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271\nAI-generated and 32,148 human-written code snippets. We also benchmark three\nstate-of-the-art AI-generated code detection models and assess their\nperformance in various test scenarios such as cross-model and cross-language.\nWe share our dataset and codes to support research in this field.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MultiAIGCD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u6db5\u76d6Python\u3001Java\u548cGo\u8bed\u8a00\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u68c0\u6d4bAI\u751f\u6210\u4ee3\u7801\u7684\u9700\u6c42\u53d8\u5f97\u8feb\u5207\uff0c\u4ee5\u7ef4\u62a4\u5b66\u672f\u8bda\u4fe1\u548c\u62db\u8058\u516c\u5e73\u3002", "method": "\u57fa\u4e8eCodeNet\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u516d\u79cdLLM\u548c\u4e09\u79cd\u63d0\u793a\u751f\u6210\u4ee3\u7801\uff0c\u8986\u76d6\u4e09\u79cd\u4f7f\u7528\u573a\u666f\uff0c\u6784\u5efa\u5305\u542b121,271\u4e2aAI\u751f\u6210\u548c32,148\u4e2a\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\u7684\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u68c0\u6d4b\u6a21\u578b\u5728\u8de8\u6a21\u578b\u548c\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "MultiAIGCD\u652f\u6301AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2507.21741", "pdf": "https://arxiv.org/pdf/2507.21741", "abs": "https://arxiv.org/abs/2507.21741", "authors": ["Shaojun E", "Yuchen Yang", "Jiaheng Wu", "Yan Zhang", "Tiejun Zhao", "Ziyan Chen"], "title": "MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces", "categories": ["cs.CV", "cs.MM"], "comment": "9 pages", "summary": "In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMAGE\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u9f50\u673a\u5236\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u89c6\u89c9\u6570\u636e\u7f16\u7801\u540e\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u635f\u5931\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u89c6\u89c9\u7f16\u7801\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8026\u5408\u4e0d\u8db3\u5bfc\u81f4\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "method": "\u63d0\u51faMAGE\u6846\u67b6\uff0c\u91c7\u7528\u667a\u80fd\u5bf9\u9f50\u7f51\u7edc\uff08IAN\uff09\u5b9e\u73b0\u7ef4\u5ea6\u4e0e\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u4ea4\u53c9\u71b5\u548c\u5747\u65b9\u8bef\u5dee\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "MAGE\u5728MME\u3001MMBench\u548cSEED\u7b49\u591a\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u5de5\u4f5c\u3002", "conclusion": "MAGE\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u9f50\u673a\u5236\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u5730\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.21686", "pdf": "https://arxiv.org/pdf/2507.21686", "abs": "https://arxiv.org/abs/2507.21686", "authors": ["Rene Winchenbach", "Andreas Kolb"], "title": "Solving Boundary Handling Analytically in Two Dimensions for Smoothed Particle Hydrodynamics", "categories": ["math.NA", "cs.GR", "cs.NA", "G.1.0; I.6.0; I.3.5"], "comment": null, "summary": "We present a fully analytic approach for evaluating boundary integrals in two\ndimensions for Smoothed Particle Hydrodynamics (SPH). Conventional methods\noften rely on boundary particles or wall re-normalization approaches derived\nfrom applying the divergence theorem, whereas our method directly evaluates the\narea integrals for SPH kernels and gradients over triangular boundaries. This\ndirect integration strategy inherently accommodates higher-order boundary\nconditions, such as piecewise cubic fields defined via Finite Element stencils,\nenabling analytic and flexible coupling with mesh-based solvers. At the core of\nour approach is a general solution for compact polynomials of arbitrary degree\nover triangles by decomposing the boundary elements into elementary integrals\nthat can be solved with closed-form solutions. We provide a complete,\nclosed-form solution for these generalized integrals, derived by relating the\nangular components to Chebyshev polynomials and solving the resulting radial\nintegral via a numerically stable evaluation of the Gaussian hypergeometric\nfunction $_2F_1$. Our solution is robust and adaptable and works regardless of\ntriangle geometries and kernel functions. We validate the accuracy against\nhigh-precision numerical quadrature rules, as well as in problems with known\nexact solutions. We provide an open-source implementation of our general\nsolution using differentiable programming to facilitate the adoption of our\napproach to SPH and other contexts that require analytic integration over\npolygonal domains. Our analytic solution outperforms existing numerical\nquadrature rules for this problem by up to five orders of magnitude, for\nintegrals and their gradients, while providing a flexible framework to couple\narbitrary triangular meshes analytically to Lagrangian schemes, building a\nstrong foundation for addressing several grand challenges in SPH and beyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u89e3\u6790\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e8c\u7ef4\u5e73\u6ed1\u7c92\u5b50\u6d41\u4f53\u52a8\u529b\u5b66\uff08SPH\uff09\u4e2d\u7684\u8fb9\u754c\u79ef\u5206\uff0c\u76f4\u63a5\u8ba1\u7b97\u4e09\u89d2\u5f62\u8fb9\u754c\u4e0a\u7684\u9762\u79ef\u79ef\u5206\uff0c\u652f\u6301\u9ad8\u9636\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7Chebyshev\u591a\u9879\u5f0f\u548cGaussian\u8d85\u51e0\u4f55\u51fd\u6570\u63d0\u4f9b\u95ed\u5408\u89e3\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u8fb9\u754c\u7c92\u5b50\u6216\u57fa\u4e8e\u6563\u5ea6\u5b9a\u7406\u7684\u58c1\u91cd\u6784\u65b9\u6cd5\uff0c\u96be\u4ee5\u7075\u6d3b\u5904\u7406\u9ad8\u9636\u8fb9\u754c\u6761\u4ef6\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u901a\u7528\u7684\u89e3\u6790\u79ef\u5206\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u4e0e\u7f51\u683c\u6c42\u89e3\u5668\u65e0\u7f1d\u8026\u5408\u3002", "method": "\u901a\u8fc7\u5c06\u8fb9\u754c\u5143\u7d20\u5206\u89e3\u4e3a\u53ef\u901a\u8fc7\u95ed\u5408\u89e3\u6c42\u89e3\u7684\u57fa\u672c\u79ef\u5206\uff0c\u5229\u7528Chebyshev\u591a\u9879\u5f0f\u548cGaussian\u8d85\u51e0\u4f55\u51fd\u6570$_2F_1$\uff0c\u63d0\u4f9b\u901a\u7528\u7684\u89e3\u6790\u79ef\u5206\u6846\u67b6\u3002", "result": "\u89e3\u6790\u89e3\u6bd4\u73b0\u6709\u6570\u503c\u79ef\u5206\u89c4\u5219\u5feb\u591a\u8fbe\u4e94\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u652f\u6301\u4efb\u610f\u4e09\u89d2\u5f62\u51e0\u4f55\u548c\u6838\u51fd\u6570\uff0c\u9002\u7528\u4e8eSPH\u53ca\u5176\u4ed6\u9700\u8981\u591a\u8fb9\u5f62\u57df\u89e3\u6790\u79ef\u5206\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSPH\u53ca\u5176\u4ed6\u9886\u57df\u7684\u9ad8\u6548\u89e3\u6790\u79ef\u5206\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u4e3a\u89e3\u51b3SPH\u4e2d\u7684\u91cd\u5927\u6311\u6218\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.21078", "pdf": "https://arxiv.org/pdf/2507.21078", "abs": "https://arxiv.org/abs/2507.21078", "authors": ["Carlo A. Furia", "Andrea Mocci"], "title": "What Makes a Level Hard in Super Mario Maker 2?", "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Games like Super Mario Maker 2 (SMM2) lower the barrier for casual users to\nbecome level designers. In this paper, we set out to analyze a vast amount of\ndata about SMM2 user-written levels, in order to understand what factors affect\na level's difficulty as experienced by other users. To this end, we perform two\nkinds of analyses: one based on regression models and one using natural\nlanguage processing techniques. The main results shed light on which level\ncharacteristics (e.g., its style, popularity, timing) and which topics and\nsentiments have a consistent association with easier or harder levels. While\nnone of our findings are startling, they help distill some key differences\nbetween easy and hard SMM2 levels, which, in turn, can pave the way for a\nbetter understanding of end-user level design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u300a\u8d85\u7ea7\u9a6c\u91cc\u5965\u5236\u90202\u300b\uff08SMM2\uff09\u7528\u6237\u8bbe\u8ba1\u5173\u5361\u7684\u6570\u636e\uff0c\u7814\u7a76\u54ea\u4e9b\u56e0\u7d20\u5f71\u54cd\u5173\u5361\u96be\u5ea6\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u63ed\u793a\u5173\u952e\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3SMM2\u7528\u6237\u8bbe\u8ba1\u5173\u5361\u4e2d\u54ea\u4e9b\u56e0\u7d20\u4f1a\u5f71\u54cd\u5176\u4ed6\u7528\u6237\u4f53\u9a8c\u5230\u7684\u96be\u5ea6\u3002", "method": "\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u5173\u5361\u6570\u636e\u3002", "result": "\u53d1\u73b0\u4e86\u5173\u5361\u7279\u5f81\uff08\u5982\u98ce\u683c\u3001\u6d41\u884c\u5ea6\u3001\u65f6\u95f4\uff09\u4ee5\u53ca\u4e3b\u9898\u548c\u60c5\u611f\u5bf9\u4e8e\u5173\u5361\u96be\u5ea6\u7684\u5173\u8054\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u7406\u89e3\u7528\u6237\u8bbe\u8ba1\u7684\u5173\u5361\u96be\u5ea6\u5dee\u5f02\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.21984", "pdf": "https://arxiv.org/pdf/2507.21984", "abs": "https://arxiv.org/abs/2507.21984", "authors": ["Jona Nagerl", "Natalia G. Berloff"], "title": "Higher-Order Kuramoto Oscillator Network for Dense Associative Memory", "categories": ["nlin.AO", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.ET", "cs.LG"], "comment": "13 pages, 7 figures", "summary": "Networks of phase oscillators can serve as dense associative memories if they\nincorporate higher-order coupling beyond the classical Kuramoto model's\npairwise interactions. Here we introduce a generalized Kuramoto model with\ncombined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling,\ninspired by dense Hopfield memory theory. Using mean-field theory and its\ndynamical approximation, we obtain a phase diagram for dense associative memory\nmodel that exhibits a tricritical point at which the continuous onset of memory\nretrieval is supplanted by a discontinuous, hysteretic transition. In the\nquartic-dominated regime, the system supports bistable phase-locked states\ncorresponding to stored memory patterns, with a sizable energy barrier between\nmemory and incoherent states. We analytically determine this bistable region\nand show that the escape time from a memory state (due to noise) grows\nexponentially with network size, indicating robust storage. Extending the\ntheory to finite memory load, we show that higher-order couplings achieve\nsuperlinear scaling of memory capacity with system size, far exceeding the\nlimit of pairwise-only oscillators. Large-scale simulations of the oscillator\nnetwork confirm our theoretical predictions, demonstrating rapid pattern\nretrieval and robust storage of many phase patterns. These results bridge the\nKuramoto synchronization with modern Hopfield memories, pointing toward\nexperimental realization of high-capacity, analog associative memory in\noscillator systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u9ad8\u9636\u8026\u5408\u7684\u5e7f\u4e49Kuramoto\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u73b0\u5bc6\u96c6\u8054\u60f3\u8bb0\u5fc6\u3002\u6a21\u578b\u5c55\u793a\u4e86\u8bb0\u5fc6\u68c0\u7d22\u7684\u8fde\u7eed\u548c\u4e0d\u8fde\u7eed\u76f8\u53d8\uff0c\u5e76\u8bc1\u660e\u4e86\u9ad8\u9636\u8026\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bb0\u5fc6\u5bb9\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u9636\u8026\u5408\uff0c\u5c06\u4f20\u7edf\u7684Kuramoto\u6a21\u578b\u6269\u5c55\u4e3a\u5bc6\u96c6\u8054\u60f3\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u7a81\u7834\u7ecf\u5178Hopfield\u8bb0\u5fc6\u7684\u5b58\u50a8\u5bb9\u91cf\u9650\u5236\u3002", "method": "\u5f15\u5165\u5305\u542b\u4e8c\u9636\uff08\u8c10\u6ce2\uff09\u548c\u56db\u9636\uff08\u56db\u6b21\uff09\u8026\u5408\u7684\u5e7f\u4e49Kuramoto\u6a21\u578b\uff0c\u5229\u7528\u5e73\u5747\u573a\u7406\u8bba\u5206\u6790\u76f8\u56fe\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5c55\u793a\u4e86\u8bb0\u5fc6\u68c0\u7d22\u7684\u590d\u6742\u76f8\u53d8\u884c\u4e3a\uff0c\u9ad8\u9636\u8026\u5408\u663e\u8457\u63d0\u5347\u4e86\u8bb0\u5fc6\u5bb9\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8d85\u7ebf\u6027\u8bb0\u5fc6\u5bb9\u91cf\u589e\u957f\u3002", "conclusion": "\u7814\u7a76\u5c06Kuramoto\u540c\u6b65\u4e0e\u73b0\u4ee3Hopfield\u8bb0\u5fc6\u7406\u8bba\u7ed3\u5408\uff0c\u4e3a\u5b9e\u9a8c\u5b9e\u73b0\u9ad8\u5bb9\u91cf\u6a21\u62df\u8054\u60f3\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.21111", "pdf": "https://arxiv.org/pdf/2507.21111", "abs": "https://arxiv.org/abs/2507.21111", "authors": ["Craig Wright"], "title": "A Formal Rebuttal of \"The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability\"", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.GT", "cs.SE", "68M14, 68P25, 68T30, 94A60, 18C10", "D.4.6; K.6.5; C.2.1; H.3.5; D.2.11"], "comment": "79 pages; A response and rebuttal of [Mssassi, Souhail, and Anas Abou\n  El Kalam. \"The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs\n  Among Decentralization, Security, and Scalability.\" Applied Sciences 15, no.\n  1 (2024): 19. https://doi.org/10.3390/app15010019.]", "summary": "This paper presents a comprehensive refutation of the so-called \"blockchain\ntrilemma,\" a widely cited but formally ungrounded claim asserting an inherent\ntrade-off between decentralisation, security, and scalability in blockchain\nprotocols. Through formal analysis, empirical evidence, and detailed critique\nof both methodology and terminology, we demonstrate that the trilemma rests on\nsemantic equivocation, misuse of distributed systems theory, and a failure to\ndefine operational metrics. Particular focus is placed on the conflation of\ntopological network analogies with protocol-level architecture, the\nmischaracterisation of Bitcoin's design--including the role of miners, SPV\nclients, and header-based verification--and the failure to ground claims in\ncomplexity-theoretic or adversarial models. By reconstructing Bitcoin as a\ndeterministic, stateless distribution protocol governed by evidentiary trust,\nwe show that scalability is not a trade-off but an engineering outcome. The\npaper concludes by identifying systemic issues in academic discourse and peer\nreview that have allowed such fallacies to persist, and offers formal criteria\nfor evaluating future claims in blockchain research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f62\u5f0f\u5206\u6790\u548c\u5b9e\u8bc1\u8bc1\u636e\u53cd\u9a73\u4e86\u533a\u5757\u94fe\u4e09\u5143\u8bba\uff0c\u6307\u51fa\u5176\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u662f\u5de5\u7a0b\u6210\u679c\u800c\u975e\u6743\u8861\u3002", "motivation": "\u53cd\u9a73\u533a\u5757\u94fe\u4e09\u5143\u8bba\u7684\u9519\u8bef\u89c2\u70b9\uff0c\u63ed\u793a\u5176\u65b9\u6cd5\u8bba\u548c\u672f\u8bed\u7684\u7f3a\u9677\uff0c\u4e3a\u533a\u5757\u94fe\u7814\u7a76\u63d0\u4f9b\u66f4\u4e25\u8c28\u7684\u6807\u51c6\u3002", "method": "\u7ed3\u5408\u5f62\u5f0f\u5206\u6790\u3001\u5b9e\u8bc1\u8bc1\u636e\u548c\u8be6\u7ec6\u6279\u5224\uff0c\u91cd\u6784\u6bd4\u7279\u5e01\u7684\u8bbe\u8ba1\uff0c\u5c55\u793a\u5176\u4f5c\u4e3a\u786e\u5b9a\u6027\u65e0\u72b6\u6001\u534f\u8bae\u7684\u7279\u70b9\u3002", "result": "\u8bc1\u660e\u4e09\u5143\u8bba\u57fa\u4e8e\u8bed\u4e49\u6df7\u6dc6\u548c\u7406\u8bba\u8bef\u7528\uff0c\u53ef\u6269\u5c55\u6027\u53ef\u901a\u8fc7\u5de5\u7a0b\u5b9e\u73b0\uff0c\u65e0\u9700\u727a\u7272\u53bb\u4e2d\u5fc3\u5316\u6216\u5b89\u5168\u6027\u3002", "conclusion": "\u8bba\u6587\u6279\u8bc4\u4e86\u5b66\u672f\u8ba8\u8bba\u4e2d\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u533a\u5757\u94fe\u7814\u7a76\u4e3b\u5f20\u7684\u5f62\u5f0f\u6807\u51c6\u3002"}}
{"id": "2507.21928", "pdf": "https://arxiv.org/pdf/2507.21928", "abs": "https://arxiv.org/abs/2507.21928", "authors": ["Christian Meske", "Tobias Hermanns", "Esther von der Weiden", "Kai-Uwe Loser", "Thorsten Berger"], "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u751f\u6210\u7684vibe coding\u5982\u4f55\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\uff0c\u5c06\u5f00\u53d1\u8005\u610f\u56fe\u4ece\u786e\u5b9a\u6027\u6307\u4ee4\u8f6c\u5411\u6982\u7387\u63a8\u65ad\uff0c\u91cd\u65b0\u5206\u914d\u8ba4\u77e5\u5de5\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u4ee3\u7801\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u8f6f\u4ef6\u5f00\u53d1\u6b63\u5728\u7ecf\u5386\u6839\u672c\u6027\u8f6c\u53d8\uff0c\u4f46\u5feb\u901f\u91c7\u7528\u4e0e\u6709\u9650\u7684\u6982\u5ff5\u7406\u89e3\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u8fd9\u4e00\u65b0\u5174\u8303\u5f0f\u3002", "method": "\u901a\u8fc7\u610f\u56fe\u89c6\u89d2\u548c\u5386\u53f2\u5206\u6790\uff0c\u5b9a\u4e49\u4e86vibe coding\u4f5c\u4e3a\u4eba\u7c7b\u4e0e\u751f\u6210AI\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u534f\u4f5c\u5f00\u53d1\u8f6f\u4ef6\u7684\u65b0\u8303\u5f0f\u3002", "result": "vibe coding\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u8ba4\u77e5\u52b3\u52a8\uff0c\u5c06\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e13\u4e1a\u77e5\u8bc6\u8f6c\u5411\u534f\u4f5c\u7f16\u6392\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u673a\u4f1a\u4e0e\u98ce\u9669\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u6db5\u76d6\u4eba\u7c7b\u3001\u6280\u672f\u548c\u7ec4\u7ec7\u4e09\u4e2a\u65b9\u5411\u7684\u672a\u6765\u7814\u7a76\u8bae\u7a0b\uff0c\u4ee5\u6307\u5bfc\u8fd9\u4e00\u8303\u5f0f\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2507.21079", "pdf": "https://arxiv.org/pdf/2507.21079", "abs": "https://arxiv.org/abs/2507.21079", "authors": ["Joe Hasei", "Yosuke Matsumoto", "Hiroki Kawai", "Yuko Okahisa", "Manabu Takaki", "Toshifumi Ozaki"], "title": "Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This study assessed metaverse-based support groups designed to reduce social\nisolation and suicide risk among LGBTQ+ youths. Using the Cluster platform,\nenhanced anonymity, avatar-based self-expression, and accessibility were\nprovided. Key findings showed that 79.2% chose avatars matching their gender\nidentity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean:\n1.79/5). Social confidence significantly improved in virtual spaces compared to\nreal-world interactions (p<0.001), particularly among participants with\ninitially low confidence, averaging an increase of 2.08 points. About half of\nthe first-time participants were 16 or younger, highlighting potential for\nearly intervention. The metaverse scored higher than real-world environments\nfor safety/privacy (3.94/5), self-expression (4.02/5), and accessibility\n(4.21/5). Additionally, 73.6% reported feeling more accepted virtually.\nHowever, some highly confident individuals offline experienced mild adaptation\nchallenges, averaging a confidence decrease of 0.58 points, indicating virtual\nsupport complements rather than replaces in-person services. These findings\nsuggest metaverse-based support effectively lowers psychological barriers and\nprovides affirming spaces, potentially reducing severe outcomes such as\nsuicidal ideation. Future studies should focus on integrating virtual support\nwith existing community and clinical frameworks to enhance long-term impacts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5143\u5b87\u5b99\u7684\u652f\u6301\u5c0f\u7ec4\uff0c\u65e8\u5728\u51cf\u5c11LGBTQ+\u9752\u5e74\u7684\u793e\u4f1a\u5b64\u7acb\u548c\u81ea\u6740\u98ce\u9669\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5143\u5b87\u5b99\u5728\u5b89\u5168\u6027\u3001\u81ea\u6211\u8868\u8fbe\u548c\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u5b9e\u73af\u5883\uff0c\u4e14\u663e\u8457\u63d0\u5347\u4e86\u53c2\u4e0e\u8005\u7684\u793e\u4ea4\u4fe1\u5fc3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5143\u5b87\u5b99\u6280\u672f\u5982\u4f55\u4e3aLGBTQ+\u9752\u5e74\u63d0\u4f9b\u5b89\u5168\u7684\u652f\u6301\u73af\u5883\uff0c\u51cf\u5c11\u793e\u4f1a\u5b64\u7acb\u548c\u81ea\u6740\u98ce\u9669\u3002", "method": "\u4f7f\u7528Cluster\u5e73\u53f0\uff0c\u63d0\u4f9b\u589e\u5f3a\u533f\u540d\u6027\u3001\u57fa\u4e8e\u5316\u8eab\u7684\u81ea\u6211\u8868\u8fbe\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u8bc4\u4f30\u53c2\u4e0e\u8005\u7684\u4f53\u9a8c\u548c\u6548\u679c\u3002", "result": "79.2%\u7684\u53c2\u4e0e\u8005\u9009\u62e9\u4e0e\u5176\u6027\u522b\u8ba4\u540c\u5339\u914d\u7684\u5316\u8eab\uff0c\u793e\u4ea4\u4fe1\u5fc3\u663e\u8457\u63d0\u5347\uff08p<0.001\uff09\uff0c\u7279\u522b\u662f\u5728\u521d\u59cb\u4fe1\u5fc3\u8f83\u4f4e\u7684\u7fa4\u4f53\u4e2d\u3002", "conclusion": "\u5143\u5b87\u5b99\u652f\u6301\u5c0f\u7ec4\u80fd\u6709\u6548\u964d\u4f4e\u5fc3\u7406\u969c\u788d\u5e76\u63d0\u4f9b\u80af\u5b9a\u7a7a\u95f4\uff0c\u4f46\u9700\u4e0e\u73b0\u6709\u670d\u52a1\u548c\u4e34\u5e8a\u6846\u67b6\u6574\u5408\u4ee5\u589e\u5f3a\u957f\u671f\u6548\u679c\u3002"}}
{"id": "2507.21115", "pdf": "https://arxiv.org/pdf/2507.21115", "abs": "https://arxiv.org/abs/2507.21115", "authors": ["Sven Lankester", "Manel Slokom", "Gustavo de Carvalho Bertoli", "Matias Vizcaino", "Emmanuelle Beauxis Aussalet", "Laura Hollink"], "title": "FedFlex: Federated Learning for Diverse Netflix Recommendations", "categories": ["cs.IR", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Federated learning is a decentralized approach that enables collaborative\nmodel training across multiple devices while preserving data privacy. It has\nshown significant potential in various domains, including healthcare and\npersonalized recommendation systems. However, most existing work on federated\nrecommendation systems has focused primarily on improving accuracy, with\nlimited attention to fairness and diversity. In this paper, we introduce\nFedFlex, a federated recommender system for Netflix-style TV series\nrecommendations. FedFlex integrates two state-of-the-art matrix factorization\nalgorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal\nRelevance (MMR) to re-rank items and enhance diversity. We conduct extensive\nexperiments comparing recommendations generated by SVD and BPR algorithms. In a\nlive two-week user study, participants received two recommendation lists: List\nA, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity.\nParticipants were asked to click on the movies they were interested in\nwatching. Our findings demonstrate that FedFlex effectively introduces diverse\ncontent, such as new genres, into recommendations without necessarily\ncompromising user satisfaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FedFlex\uff0c\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\uff0c\u7ed3\u5408\u77e9\u9635\u5206\u89e3\u548cMMR\u91cd\u6392\u5e8f\u4ee5\u63d0\u5347\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4fdd\u8bc1\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u540c\u65f6\u589e\u52a0\u4e86\u63a8\u8350\u5185\u5bb9\u7684\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u517c\u987e\u591a\u6837\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408SVD\u548cBPR\u4e24\u79cd\u77e9\u9635\u5206\u89e3\u7b97\u6cd5\u8fdb\u884c\u4e2a\u6027\u5316\u5fae\u8c03\uff0c\u5e76\u5e94\u7528MMR\u7b97\u6cd5\u5bf9\u63a8\u8350\u7ed3\u679c\u8fdb\u884c\u91cd\u6392\u5e8f\u4ee5\u589e\u5f3a\u591a\u6837\u6027\u3002", "result": "\u901a\u8fc7\u4e24\u5468\u671f\u7528\u6237\u5b9e\u9a8c\uff0c\u8bc1\u660eFedFlex\u80fd\u6709\u6548\u5f15\u5165\u591a\u6837\u5316\u5185\u5bb9\uff08\u5982\u65b0\u7c7b\u578b\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "FedFlex\u5728\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u6210\u529f\u5e73\u8861\u4e86\u591a\u6837\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.21952", "pdf": "https://arxiv.org/pdf/2507.21952", "abs": "https://arxiv.org/abs/2507.21952", "authors": ["Peihong Lin", "Pengfei Wang", "Xu Zhou", "Wei Xie", "Gen Zhang", "Kai Lu"], "title": "DeepGo: Predictive Directed Greybox Fuzzing", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "The state-of-the-art DGF techniques redefine and optimize the fitness metric\nto reach the target sites precisely and quickly. However, optimizations for\nfitness metrics are mainly based on heuristic algorithms, which usually rely on\nhistorical execution information and lack foresight on paths that have not been\nexercised yet. Thus, those hard-to-execute paths with complex constraints would\nhinder DGF from reaching the targets, making DGF less efficient. In this paper,\nwe propose DeepGo, a predictive directed grey-box fuzzer that can combine\nhistorical and predicted information to steer DGF to reach the target site via\nan optimal path. We first propose the path transition model, which models DGF\nas a process of reaching the target site through specific path transition\nsequences. The new seed generated by mutation would cause the path transition,\nand the path corresponding to the high-reward path transition sequence\nindicates a high likelihood of reaching the target site through it. Then, to\npredict the path transitions and the corresponding rewards, we use deep neural\nnetworks to construct a Virtual Ensemble Environment (VEE), which gradually\nimitates the path transition model and predicts the rewards of path transitions\nthat have not been taken yet. To determine the optimal path, we develop a\nReinforcement Learning for Fuzzing (RLF) model to generate the transition\nsequences with the highest sequence rewards. The RLF model can combine\nhistorical and predicted path transitions to generate the optimal path\ntransition sequences, along with the policy to guide the mutation strategy of\nfuzzing. Finally, to exercise the high-reward path transition sequence, we\npropose the concept of an action group, which comprehensively optimizes the\ncritical steps of fuzzing to realize the optimal path to reach the target\nefficiently.", "AI": {"tldr": "DeepGo\u662f\u4e00\u79cd\u9884\u6d4b\u6027\u7684\u5b9a\u5411\u7070\u76d2\u6a21\u7cca\u5668\uff0c\u7ed3\u5408\u5386\u53f2\u548c\u9884\u6d4b\u4fe1\u606f\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u5f15\u5bfc\u6a21\u7cca\u6d4b\u8bd5\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "motivation": "\u73b0\u6709DGF\u6280\u672f\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316\u9002\u5e94\u5ea6\u6307\u6807\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u672a\u6267\u884c\u8def\u5f84\u7684\u524d\u77bb\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u5230\u8fbe\u590d\u6742\u8def\u5f84\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u8def\u5f84\u8f6c\u6362\u6a21\u578b\uff0c\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u865a\u62df\u96c6\u5408\u73af\u5883\u9884\u6d4b\u8def\u5f84\u8f6c\u6362\u5956\u52b1\uff0c\u5e76\u5f00\u53d1\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u751f\u6210\u6700\u4f18\u8def\u5f84\u3002", "result": "\u5b9e\u73b0\u901a\u8fc7\u9ad8\u5956\u52b1\u8def\u5f84\u8f6c\u6362\u5e8f\u5217\u9ad8\u6548\u5230\u8fbe\u76ee\u6807\u3002", "conclusion": "DeepGo\u80fd\u6709\u6548\u7ed3\u5408\u5386\u53f2\u548c\u9884\u6d4b\u4fe1\u606f\uff0c\u4f18\u5316\u6a21\u7cca\u6d4b\u8bd5\u8def\u5f84\uff0c\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2507.21081", "pdf": "https://arxiv.org/pdf/2507.21081", "abs": "https://arxiv.org/abs/2507.21081", "authors": ["Katherine M. Collins", "Kartik Chandra", "Adrian Weller", "Jonathan Ragan-Kelley", "Joshua B. Tenenbaum"], "title": "Empathy in Explanation", "categories": ["cs.HC", "cs.AI"], "comment": "CogSci non-archival conference paper", "summary": "Why do we give the explanations we do? Recent work has suggested that we\nshould think of explanation as a kind of cooperative social interaction,\nbetween a why-question-asker and an explainer. Here, we apply this perspective\nto consider the role that emotion plays in this social interaction. We develop\na computational framework for modeling explainers who consider the emotional\nimpact an explanation might have on a listener. We test our framework by using\nit to model human intuitions about how a doctor might explain to a patient why\nthey have a disease, taking into account the patient's propensity for regret.\nOur model predicts human intuitions well, better than emotion-agnostic\nablations, suggesting that people do indeed reason about emotion when giving\nexplanations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u89e3\u91ca\u884c\u4e3a\u4e2d\u7684\u60c5\u611f\u56e0\u7d20\uff0c\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\u6765\u6a21\u62df\u89e3\u91ca\u8005\u5982\u4f55\u8003\u8651\u542c\u4f17\u7684\u60c5\u611f\u53cd\u5e94\uff0c\u5e76\u901a\u8fc7\u533b\u751f\u5411\u60a3\u8005\u89e3\u91ca\u75be\u75c5\u7684\u573a\u666f\u9a8c\u8bc1\u4e86\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u89e3\u91ca\u884c\u4e3a\u4e2d\u60c5\u611f\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u793e\u4f1a\u4e92\u52a8\u4e2d\u89e3\u91ca\u8005\u5982\u4f55\u8003\u8651\u542c\u4f17\u7684\u60c5\u611f\u53cd\u5e94\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u6a21\u62df\u89e3\u91ca\u8005\u5982\u4f55\u9884\u6d4b\u5e76\u8003\u8651\u542c\u4f17\u7684\u60c5\u611f\uff08\u5982\u540e\u6094\u503e\u5411\uff09\uff0c\u5e76\u901a\u8fc7\u533b\u751f\u89e3\u91ca\u75be\u75c5\u7684\u573a\u666f\u6d4b\u8bd5\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u8f83\u597d\u5730\u9884\u6d4b\u4eba\u7c7b\u76f4\u89c9\uff0c\u8868\u73b0\u4f18\u4e8e\u4e0d\u8003\u8651\u60c5\u611f\u7684\u6a21\u578b\uff0c\u8868\u660e\u4eba\u4eec\u5728\u89e3\u91ca\u65f6\u786e\u5b9e\u4f1a\u8003\u8651\u60c5\u611f\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u60c5\u611f\u5728\u89e3\u91ca\u884c\u4e3a\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u9a8c\u8bc1\u4e86\u8ba1\u7b97\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.21199", "pdf": "https://arxiv.org/pdf/2507.21199", "abs": "https://arxiv.org/abs/2507.21199", "authors": ["Xinye Cao", "Hongcan Guo", "Guoshun Nan", "Jiaoyang Cui", "Haoting Qian", "Yihan Lin", "Yilin Peng", "Diyang Zhang", "Yanzhao Hou", "Huici Wu", "Xiaofeng Tao", "Tony Q. S. Quek"], "title": "Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.HC"], "comment": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for\n  possible publication", "summary": "Interactive multimodal applications (IMAs), such as route planning in the\nInternet of Vehicles, enrich users' personalized experiences by integrating\nvarious forms of data over wireless networks. Recent advances in large language\nmodels (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple\nIMAs, with each LLM trained individually for a specific task that presents\ndifferent business workflows. In contrast to existing approaches that rely on\nmultiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes\nvarious IMAs using a single compositional LLM over wireless networks. The two\nprimary challenges include 1) guiding a single LLM to adapt to diverse IMA\nobjectives and 2) ensuring the flexibility and efficiency of the LLM in\nresource-constrained mobile environments. To tackle the first challenge, we\npropose ContextLoRA, a novel method that guides an LLM to learn the rich\nstructured context among IMAs by constructing a task dependency graph. We\npartition the learnable parameter matrix of neural layers for each IMA to\nfacilitate LLM composition. Then, we develop a step-by-step fine-tuning\nprocedure guided by task relations, including training, freezing, and masking\nphases. This allows the LLM to learn to reason among tasks for better\nadaptation, capturing the latent dependencies between tasks. For the second\nchallenge, we introduce ContextGear, a scheduling strategy to optimize the\ntraining procedure of ContextLoRA, aiming to minimize computational and\ncommunication costs through a strategic grouping mechanism. Experiments on\nthree benchmarks show the superiority of the proposed ContextLoRA and\nContextGear. Furthermore, we prototype our proposed paradigm on a real-world\nwireless testbed, demonstrating its practical applicability for various IMAs.\nWe will release our code to the community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u4e00\u7ec4\u5408\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b8c\u6210\u591a\u79cd\u4ea4\u4e92\u5f0f\u591a\u6a21\u6001\u5e94\u7528\uff08IMAs\uff09\uff0c\u5e76\u901a\u8fc7ContextLoRA\u548cContextGear\u89e3\u51b3\u4e86\u9002\u5e94\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684IMAs\u901a\u5e38\u4f9d\u8d56\u591a\u4e2aLLM\uff0c\u8fd9\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u8d44\u6e90\u6d88\u8017\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5355\u4e00LLM\u5b9e\u73b0\u591a\u6837\u5316\u7684IMAs\uff0c\u540c\u65f6\u89e3\u51b3\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86ContextLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4efb\u52a1\u4f9d\u8d56\u56fe\u548c\u5206\u9636\u6bb5\u5fae\u8c03\uff08\u8bad\u7ec3\u3001\u51bb\u7ed3\u548c\u63a9\u7801\uff09\u6765\u6307\u5bfcLLM\u5b66\u4e60IMAs\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u3002\u6b64\u5916\uff0cContextGear\u901a\u8fc7\u5206\u7ec4\u673a\u5236\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cContextLoRA\u548cContextGear\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9645\u65e0\u7ebf\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5355\u4e00LLM\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684IMAs\u652f\u6301\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21954", "pdf": "https://arxiv.org/pdf/2507.21954", "abs": "https://arxiv.org/abs/2507.21954", "authors": ["Zengyang Li", "Yimeng Li", "Binbin Huang", "Peng Liang", "Ran Mo", "Hui Liu", "Yutao Ma"], "title": "Fine-Tuning Code Language Models to Detect Cross-Language Bugs", "categories": ["cs.SE", "cs.AI"], "comment": "33 pages, 6 images, 9 tables, Manuscript submitted to a journal\n  (2025)", "summary": "Multilingual programming, which involves using multiple programming languages\n(PLs) in a single project, is increasingly common due to its benefits. However,\nit introduces cross-language bugs (CLBs), which arise from interactions between\ndifferent PLs and are difficult to detect by single-language bug detection\ntools. This paper investigates the potential of pre-trained code language\nmodels (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language\ncode identification tool, and constructed a CLB dataset involving three PL\ncombinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction\ntypes. We fine-tuned 13 CodeLMs on this dataset and evaluated their\nperformance, analyzing the effects of dataset size, token sequence length, and\ncode comments. Results show that all CodeLMs performed poorly before\nfine-tuning, but exhibited varying degrees of performance improvement after\nfine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,\nsmall fine-tuned CodeLMs tended to performe better than large ones. CodeLMs\nfine-tuned on single-language bug datasets performed poorly on CLB detection,\ndemonstrating the distinction between CLBs and single-language bugs.\nAdditionally, increasing the fine-tuning dataset size significantly improved\nperformance, while longer token sequences did not necessarily improve the model\nperformance. The impact of code comments varied across models. Some fine-tuned\nCodeLMs' performance was improved, while others showed degraded performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff08CodeLMs\uff09\u5728\u68c0\u6d4b\u8de8\u8bed\u8a00\u9519\u8bef\uff08CLB\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u5f00\u53d1\u4e86\u5de5\u5177CLCFinder\u5e76\u6784\u5efa\u4e86\u4e00\u4e2aCLB\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u540eCodeLMs\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u5176\u4e2dUniXcoder-base\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u591a\u8bed\u8a00\u7f16\u7a0b\u7684\u666e\u53ca\u6027\uff0c\u8de8\u8bed\u8a00\u9519\u8bef\uff08CLB\uff09\u7684\u68c0\u6d4b\u53d8\u5f97\u91cd\u8981\u3002\u5355\u8bed\u8a00\u9519\u8bef\u68c0\u6d4b\u5de5\u5177\u65e0\u6cd5\u6709\u6548\u68c0\u6d4bCLB\uff0c\u56e0\u6b64\u63a2\u7d22CodeLMs\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u5f88\u6709\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5f00\u53d1CLCFinder\u5de5\u5177\u5e76\u6784\u5efa\u4e00\u4e2a\u6d89\u53ca\u4e09\u79cd\u7f16\u7a0b\u8bed\u8a00\u7ec4\u5408\u7684CLB\u6570\u636e\u96c6\uff0c\u5fae\u8c0313\u79cdCodeLMs\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u5206\u6790\u6570\u636e\u96c6\u5927\u5c0f\u3001\u4ee3\u7801\u6ce8\u91ca\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86CodeLMs\u7684\u6027\u80fd\uff0cUniXcoder-base\u8868\u73b0\u6700\u4f73\uff08F1\u5206\u65700.7407\uff09\u3002\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u589e\u52a0\u6570\u636e\u96c6\u89c4\u6a21\u5bf9\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "CodeLMs\u5728CLB\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\uff0c\u4e14\u5c0f\u578b\u6a21\u578b\u548c\u6570\u636e\u96c6\u89c4\u6a21\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2507.21088", "pdf": "https://arxiv.org/pdf/2507.21088", "abs": "https://arxiv.org/abs/2507.21088", "authors": ["Bibeg Limbu", "Irene-Angelica Chounta", "Vilma Sukacke", "Andromachi Filippidi", "Chara Spyropoulou", "Marianna Anagnostopoulou", "Eleftheria Tsourlidaki", "Nikos Karacapilidis"], "title": "Eliciting User Requirements for AI-Enhanced Learning Environments using a Participatory Approach", "categories": ["cs.HC"], "comment": "12 pages, 2 figures, 15th International Conference on Methodologies\n  and Intelligent Systems for Technology Enhanced Learning (mis4tel), Workshop\n  Track: Workshop on Integration of Emerging Technologies into Education and\n  Training (ETELT) https://mis4tel-conference.net/tracks/workshops/etelt,\n  accepted", "summary": "This paper explores the needs \\& expectations of educational stakeholders for\nAI (Artificial Intelligence)-enhanced learning environments. Data was collected\nfollowing two-phased participatory workshops. The first workshop outlined\nstakeholders' profiles in terms of technical and pedagogical characteristics.\nThe qualitative data collected was analysed using deductive thematic analysis\nwith Activity Theory, explicating the user needs. The second workshop\narticulated expectations related to the integration of AI in education.\nInductive thematic analysis of the second workshop led to the elicitation of\nusers' expectations. We cross-examined the needs and expectations, identifying\ncontradictions, to generate user requirements for emerging technologies. The\npaper provides suggestions for future design initiatives that incorporate AI in\nlearning environments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6559\u80b2\u5229\u76ca\u76f8\u5173\u8005\u5bf9AI\u589e\u5f3a\u5b66\u4e60\u73af\u5883\u7684\u9700\u6c42\u4e0e\u671f\u671b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u53c2\u4e0e\u5f0f\u7814\u8ba8\u4f1a\u6536\u96c6\u6570\u636e\u5e76\u5206\u6790\uff0c\u6700\u7ec8\u63d0\u51faAI\u5728\u6559\u80b2\u4e2d\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76AI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u660e\u786e\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u548c\u671f\u671b\uff0c\u4ee5\u4f18\u5316\u5b66\u4e60\u73af\u5883\u7684\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u53c2\u4e0e\u5f0f\u7814\u8ba8\u4f1a\uff0c\u5206\u522b\u901a\u8fc7\u6f14\u7ece\u548c\u5f52\u7eb3\u4e3b\u9898\u5206\u6790\u6cd5\u5206\u6790\u6570\u636e\uff0c\u5e76\u8fd0\u7528\u6d3b\u52a8\u7406\u8bba\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u8bc6\u522b\u4e86\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u4e0e\u671f\u671b\uff0c\u53d1\u73b0\u77db\u76fe\u70b9\uff0c\u5e76\u751f\u6210\u4e86\u65b0\u5174\u6280\u672f\u7684\u7528\u6237\u9700\u6c42\u3002", "conclusion": "\u4e3a\u672a\u6765AI\u5728\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u5f3a\u8c03\u7528\u6237\u9700\u6c42\u548c\u671f\u671b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.21276", "pdf": "https://arxiv.org/pdf/2507.21276", "abs": "https://arxiv.org/abs/2507.21276", "authors": ["Yufei Li", "Zexin Li", "Yinglun Zhu", "Cong Liu"], "title": "LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems", "categories": ["cs.AI", "cs.CL", "cs.DC"], "comment": "Accepted by RTSS 2025", "summary": "Modern deployment of large language models (LLMs) frequently involves both\ninference serving and continuous retraining to stay aligned with evolving data\nand user feedback. Common practices separate these workloads onto distinct\nservers in isolated phases, causing substantial inefficiencies (e.g., GPU\nidleness) and delayed adaptation to new data in distributed settings. Our\nempirical analysis reveals that these inefficiencies stem from dynamic request\narrivals during serving and workload heterogeneity in pipeline-parallel\ntraining. To address these challenges, we propose LeMix, a system for\nco-locating and managing concurrent LLM serving and training workloads. LeMix\nintegrates offline profiling, execution prediction mechanisms, and runtime\nscheduling to dynamically adapt resource allocation based on workload\ncharacteristics and system conditions. By understanding task-specific behaviors\nand co-execution interference across shared nodes, LeMix improves utilization\nand serving quality without compromising serving responsiveness. Our evaluation\nshows that LeMix improves throughput by up to 3.53x, reduces inference loss by\nup to 0.61x, and delivers up to 2.12x higher response time SLO attainment over\ntraditional separate setups. To our knowledge, this is the first work to\nuncover and exploit the opportunities of joint LLM inference and training,\npaving the way for more resource-efficient deployment of LLMs in production\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LeMix\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u8c03\u5ea6LLM\u7684\u63a8\u7406\u548c\u8bad\u7ec3\u4efb\u52a1\uff0c\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u548c\u54cd\u5e94\u901f\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u90e8\u7f72\u65b9\u5f0f\u5c06LLM\u7684\u63a8\u7406\u548c\u8bad\u7ec3\u4efb\u52a1\u5206\u79bb\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1LeMix\u7cfb\u7edf\uff0c\u6574\u5408\u79bb\u7ebf\u5206\u6790\u3001\u6267\u884c\u9884\u6d4b\u548c\u52a8\u6001\u8c03\u5ea6\uff0c\u6839\u636e\u4efb\u52a1\u7279\u6027\u548c\u7cfb\u7edf\u72b6\u6001\u5206\u914d\u8d44\u6e90\u3002", "result": "LeMix\u5c06\u541e\u5410\u91cf\u63d0\u53473.53\u500d\uff0c\u63a8\u7406\u635f\u5931\u51cf\u5c110.61\u500d\uff0c\u54cd\u5e94\u65f6\u95f4SLO\u8fbe\u6807\u7387\u63d0\u9ad82.12\u500d\u3002", "conclusion": "LeMix\u9996\u6b21\u5b9e\u73b0LLM\u63a8\u7406\u4e0e\u8bad\u7ec3\u7684\u8054\u5408\u8c03\u5ea6\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.21089", "pdf": "https://arxiv.org/pdf/2507.21089", "abs": "https://arxiv.org/abs/2507.21089", "authors": ["Xiaotian Su", "Naim Zierau", "Soomin Kim", "April Yi Wang", "Thiemo Wambsganss"], "title": "Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Social media platforms increasingly employ proactive moderation techniques,\nsuch as detecting and curbing toxic and uncivil comments, to prevent the spread\nof harmful content. Despite these efforts, such approaches are often criticized\nfor creating a climate of censorship and failing to address the underlying\ncauses of uncivil behavior. Our work makes both theoretical and practical\ncontributions by proposing and evaluating two types of emotion monitoring\ndashboards to users' emotional awareness and mitigate hate speech. In a study\ninvolving 211 participants, we evaluate the effects of the two mechanisms on\nuser commenting behavior and emotional experiences. The results reveal that\nthese interventions effectively increase users' awareness of their emotional\nstates and reduce hate speech. However, our findings also indicate potential\nunintended effects, including increased expression of negative emotions (Angry,\nFear, and Sad) when discussing sensitive issues. These insights provide a basis\nfor further research on integrating proactive emotion regulation tools into\nsocial media platforms to foster healthier digital interactions.", "AI": {"tldr": "\u7814\u7a76\u4e86\u60c5\u7eea\u76d1\u63a7\u4eea\u8868\u677f\u5bf9\u7528\u6237\u60c5\u611f\u610f\u8bc6\u548c\u4ec7\u6068\u8a00\u8bba\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u63d0\u5347\u60c5\u611f\u610f\u8bc6\u5e76\u51cf\u5c11\u4ec7\u6068\u8a00\u8bba\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u8d1f\u9762\u60c5\u7eea\u7684\u8868\u8fbe\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5e38\u56e0\u4e3b\u52a8\u5ba1\u6838\u6280\u672f\u88ab\u6279\u8bc4\u4e3a\u5ba1\u67e5\u4e14\u672a\u80fd\u89e3\u51b3\u4e0d\u6587\u660e\u884c\u4e3a\u7684\u6839\u6e90\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u60c5\u7eea\u76d1\u63a7\u5de5\u5177\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u60c5\u7eea\u76d1\u63a7\u4eea\u8868\u677f\uff0c\u901a\u8fc7211\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u7814\u7a76\u5176\u5bf9\u8bc4\u8bba\u884c\u4e3a\u548c\u60c5\u611f\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4eea\u8868\u677f\u80fd\u63d0\u5347\u60c5\u611f\u610f\u8bc6\u3001\u51cf\u5c11\u4ec7\u6068\u8a00\u8bba\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8ba8\u8bba\u654f\u611f\u8bdd\u9898\u65f6\u7684\u8d1f\u9762\u60c5\u7eea\u8868\u8fbe\u3002", "conclusion": "\u60c5\u7eea\u76d1\u63a7\u5de5\u5177\u6709\u52a9\u4e8e\u6539\u5584\u6570\u5b57\u4e92\u52a8\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u6f5c\u5728\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.21932", "pdf": "https://arxiv.org/pdf/2507.21932", "abs": "https://arxiv.org/abs/2507.21932", "authors": ["Lars Hadidi", "Leonard G\u00f6ke", "Maximilian Hoffmann", "Mario Klostermeier", "Shima Sasanpour", "Tim Varelmann", "Vassilios Yfantis", "Jochen Lin\u00dfen", "Detlef Stolten", "Jann M. Weinand"], "title": "Large-Scale Linear Energy System Optimization: A Systematic Review on Parallelization Strategies via Decomposition", "categories": ["math.OC", "cs.DC", "cs.MS", "90-02 (Primary) 90C06 (Secondary)"], "comment": null, "summary": "As renewable energy integration, sector coupling, and spatiotemporal detail\nincrease, energy system optimization models grow in size and complexity, often\npushing solvers to their performance limits. This systematic review explores\nparallelization strategies that can address these challenges. We first propose\na classification scheme for linear energy system optimization models, covering\ntheir analytical focus, mathematical structure, and scope. We then review\nparallel decomposition methods, finding that while many offer performance\nbenefits, no single approach is universally superior. The lack of standardized\nbenchmark suites further complicates comparison. To address this, we recommend\nessential criteria for future benchmarks and minimum reporting standards. We\nalso survey available software tools for parallel decomposition, including\nmodular frameworks and algorithmic abstractions. Though centered on energy\nsystem models, our insights extend to the broader operations research field.", "AI": {"tldr": "\u5bf9\u80fd\u6e90\u7cfb\u7edf\u4f18\u5316\u6a21\u578b\u5e76\u884c\u5316\u7b56\u7565\u7684\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u5206\u7c7b\u65b9\u6cd5\u5e76\u8bc4\u4f30\u5e76\u884c\u5206\u89e3\u65b9\u6cd5\uff0c\u5efa\u8bae\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u96c6\u6210\u548c\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u6c42\u89e3\u5668\u9762\u4e34\u6027\u80fd\u6311\u6218\uff0c\u9700\u63a2\u7d22\u5e76\u884c\u5316\u7b56\u7565\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51fa\u80fd\u6e90\u7cfb\u7edf\u4f18\u5316\u6a21\u578b\u7684\u5206\u7c7b\u65b9\u6848\uff0c\u7efc\u8ff0\u5e76\u884c\u5206\u89e3\u65b9\u6cd5\u5e76\u5206\u6790\u5176\u6027\u80fd\uff0c\u8bc4\u4f30\u73b0\u6709\u8f6f\u4ef6\u5de5\u5177\u3002", "result": "\u591a\u79cd\u5e76\u884c\u5206\u89e3\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u65e0\u666e\u9002\u6700\u4f18\u65b9\u6848\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002\u5efa\u8bae\u672a\u6765\u57fa\u51c6\u6807\u51c6\u548c\u62a5\u544a\u89c4\u8303\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e0d\u4ec5\u9002\u7528\u4e8e\u80fd\u6e90\u7cfb\u7edf\u6a21\u578b\uff0c\u8fd8\u53ef\u63a8\u5e7f\u81f3\u8fd0\u7b79\u5b66\u9886\u57df\uff0c\u672a\u6765\u9700\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2507.21090", "pdf": "https://arxiv.org/pdf/2507.21090", "abs": "https://arxiv.org/abs/2507.21090", "authors": ["Yiling Zhao", "Audrey Michal", "Nithum Thain", "Hari Subramonyam"], "title": "Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As AI systems shape individual and societal decisions, fostering critical AI\nliteracy is essential. Traditional approaches, such as blog articles, static\nlessons, and social media discussions, often fail to support deep conceptual\nunderstanding and critical engagement. This study examines whether interactive\nsimulations can help learners think like a scientist by engaging them in\nhypothesis testing, experimentation, and direct observation of AI behavior. In\na controlled study with 605 participants, we assess how interactive AI\ntutorials impact learning of key concepts such as fairness, dataset\nrepresentativeness, and bias in language models. Results show that interactive\nsimulations effectively enhance AI literacy across topics, supporting greater\nknowledge transfer and self-reported confidence, though engagement alone does\nnot predict learning. This work contributes to the growing field of AI literacy\neducation, highlighting how interactive, inquiry-driven methodologies can\nbetter equip individuals to critically engage with AI in their daily lives.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4ea4\u4e92\u5f0f\u6a21\u62df\u662f\u5426\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u80fd\u63d0\u5347AI\u7d20\u517b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8eAI\u7cfb\u7edf\u5f71\u54cd\u793e\u4f1a\u548c\u4e2a\u4f53\u51b3\u7b56\uff0c\u63d0\u5347\u6279\u5224\u6027AI\u7d20\u517b\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u5728605\u540d\u53c2\u4e0e\u8005\u4e2d\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0fAI\u6559\u7a0b\u8bc4\u4f30\u5176\u5bf9\u516c\u5e73\u6027\u3001\u6570\u636e\u96c6\u4ee3\u8868\u6027\u548c\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u7406\u89e3\u3002", "result": "\u4ea4\u4e92\u5f0f\u6a21\u62df\u663e\u8457\u63d0\u5347AI\u7d20\u517b\uff0c\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\u548c\u81ea\u4fe1\u5fc3\uff0c\u4f46\u5355\u7eaf\u53c2\u4e0e\u4e0d\u80fd\u9884\u6d4b\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u3001\u63a2\u7a76\u9a71\u52a8\u7684\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684AI\u6279\u5224\u80fd\u529b\u3002"}}
{"id": "2507.21128", "pdf": "https://arxiv.org/pdf/2507.21128", "abs": "https://arxiv.org/abs/2507.21128", "authors": ["Ruomai Ren"], "title": "Security study based on the Chatgptplugin system: ldentifying Security Vulnerabilities", "categories": ["cs.CR", "cs.SE"], "comment": "Master's thesis", "summary": "Plugin systems are a class of external programmes that provide users with a\nwide range of functionality, and while they enhance the user experience, their\nsecurity is always a challenge. Especially due to the diversity and complexity\nof developers, many plugin systems lack adequate regulation. As ChatGPT has\nbecome a popular large-scale language modelling platform, its plugin system is\nalso gradually developing, and the open platform provides creators with the\nopportunity to upload plugins covering a wide range of application scenarios.\nHowever, current research and discussions mostly focus on the security issues\nof the ChatGPT model itself, while ignoring the possible security risks posed\nby the plugin system. This study aims to analyse the security of plugins in the\nChatGPT plugin shop, reveal its major security vulnerabilities, and propose\ncorresponding improvements.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86ChatGPT\u63d2\u4ef6\u5e97\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4e3b\u8981\u6f0f\u6d1e\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u63d2\u4ef6\u7cfb\u7edf\u867d\u7136\u4e30\u5bcc\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u56e0\u5176\u591a\u6837\u6027\u548c\u5f00\u53d1\u8005\u7684\u590d\u6742\u6027\uff0c\u7f3a\u4e4f\u8db3\u591f\u76d1\u7ba1\uff0c\u5c24\u5176\u662fChatGPT\u63d2\u4ef6\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\u88ab\u5ffd\u89c6\u3002", "method": "\u5206\u6790ChatGPT\u63d2\u4ef6\u5e97\u4e2d\u63d2\u4ef6\u7684\u5b89\u5168\u6027\u3002", "result": "\u63ed\u793a\u4e86\u4e3b\u8981\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "conclusion": "\u63d0\u51fa\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u63aa\u65bd\u4ee5\u63d0\u5347\u63d2\u4ef6\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.21722", "pdf": "https://arxiv.org/pdf/2507.21722", "abs": "https://arxiv.org/abs/2507.21722", "authors": ["Stefan Graser", "Martin Schrepp", "Stephan B\u00f6hm"], "title": "Identification of Design Recommendations for Augmented Reality Authors in Corporate Training", "categories": ["cs.HC", "cs.SE"], "comment": "9 pages, 1 table, 1 figure", "summary": "Innovative technologies, such as Augmented Reality (AR), introduce new\ninteraction paradigms, demanding the identification of software requirements\nduring the software development process. In general, design recommendations are\nrelated to this, supporting the design of applications positively and meeting\nstakeholder needs. However, current research lacks context-specific AR design\nrecommendations. This study addresses this gap by identifying and analyzing\npractical AR design recommendations relevant to the evaluation phase of the\nUser-Centered Design (UCD) process. We rely on an existing dataset of Mixed\nReality (MR) design recommendations. We applied a multi-method approach by (1)\nextending the dataset with AR-specific recommendations published since 2020,\n(2) classifying the identified recommendations using a NLP classification\napproach based on a pre-trained Sentence Transformer model, (3) summarizing the\ncontent of all topics, and (4) evaluating their relevance concerning AR in\nCorporate Training (CT) both based on a qualitative Round Robin approach with\nfive experts. As a result, an updated dataset of 597 practitioner design\nrecommendations, classified into 84 topics, is provided with new insights into\ntheir applicability in the context of AR in CT. Based on this, 32 topics with a\ntotal of 284 statements were evaluated as relevant for AR in CT. This research\ndirectly contributes to the authors' work for extending their AR-specific User\nExperience (UX) measurement approach, supporting AR authors in targeting the\nimprovement of AR applications for CT scenarios.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u65b9\u6cd5\u5206\u6790\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u4e86AR\u8bbe\u8ba1\u5efa\u8bae\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u4f01\u4e1a\u57f9\u8bad\u4e2d\u7684\u9002\u7528\u6027\uff0c\u6269\u5c55\u4e86AR\u7528\u6237\u4f53\u9a8c\u6d4b\u91cf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u7279\u5b9a\u80cc\u666f\u7684AR\u8bbe\u8ba1\u5efa\u8bae\uff0c\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4f01\u4e1a\u57f9\u8bad\u573a\u666f\u3002", "method": "\u91c7\u7528\u591a\u65b9\u6cd5\u5206\u6790\uff0c\u5305\u62ec\u6570\u636e\u96c6\u6269\u5c55\u3001NLP\u5206\u7c7b\u3001\u5185\u5bb9\u603b\u7ed3\u548c\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "\u63d0\u4f9b\u4e86597\u6761\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5206\u4e3a84\u4e2a\u4e3b\u9898\uff0c\u5176\u4e2d32\u4e2a\u4e3b\u9898\uff08284\u6761\u5efa\u8bae\uff09\u9002\u7528\u4e8e\u4f01\u4e1a\u57f9\u8bad\u3002", "conclusion": "\u7814\u7a76\u4e3aAR\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u652f\u6301AR\u7528\u6237\u4f53\u9a8c\u6d4b\u91cf\u65b9\u6cd5\u7684\u6269\u5c55\u3002"}}
{"id": "2507.21810", "pdf": "https://arxiv.org/pdf/2507.21810", "abs": "https://arxiv.org/abs/2507.21810", "authors": ["Yiyu Chen", "Yifan Wu", "Shuyu Shen", "Yupeng Xie", "Leixian Shen", "Hui Xiong", "Yuyu Luo"], "title": "ChartMark: A Structured Grammar for Chart Annotation", "categories": ["cs.CL", "cs.SE"], "comment": "IEEE VIS 2025", "summary": "Chart annotations enhance visualization accessibility but suffer from\nfragmented, non-standardized representations that limit cross-platform reuse.\nWe propose ChartMark, a structured grammar that separates annotation semantics\nfrom visualization implementations. ChartMark features a hierarchical framework\nmapping onto annotation dimensions (e.g., task, chart context), supporting both\nabstract intents and precise visual details. Our toolkit demonstrates\nconverting ChartMark specifications into Vega-Lite visualizations, highlighting\nits flexibility, expressiveness, and practical applicability.", "AI": {"tldr": "ChartMark\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u8bed\u6cd5\uff0c\u5c06\u6ce8\u91ca\u8bed\u4e49\u4e0e\u53ef\u89c6\u5316\u5b9e\u73b0\u5206\u79bb\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u91cd\u7528\u3002", "motivation": "\u89e3\u51b3\u56fe\u8868\u6ce8\u91ca\u788e\u7247\u5316\u548c\u975e\u6807\u51c6\u5316\u95ee\u9898\uff0c\u63d0\u5347\u53ef\u89c6\u5316\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u6620\u5c04\u6ce8\u91ca\u7ef4\u5ea6\uff08\u5982\u4efb\u52a1\u3001\u56fe\u8868\u4e0a\u4e0b\u6587\uff09\uff0c\u652f\u6301\u62bd\u8c61\u610f\u56fe\u548c\u5177\u4f53\u89c6\u89c9\u7ec6\u8282\u3002", "result": "\u5de5\u5177\u5305\u6210\u529f\u5c06ChartMark\u89c4\u8303\u8f6c\u6362\u4e3aVega-Lite\u53ef\u89c6\u5316\uff0c\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u3001\u8868\u8fbe\u529b\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "ChartMark\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u6cd5\u548c\u5206\u5c42\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u91ca\u6807\u51c6\u5316\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.21378", "pdf": "https://arxiv.org/pdf/2507.21378", "abs": "https://arxiv.org/abs/2507.21378", "authors": ["Kevin Pu", "Ting Zhang", "Naveen Sendhilnathan", "Sebastian Freitag", "Raj Sodhi", "Tanya Jonker"], "title": "ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to UIST'25", "summary": "Wearable AI systems aim to provide timely assistance in daily life, but\nexisting approaches often rely on user initiation or predefined task knowledge,\nneglecting users' current mental states. We introduce ProMemAssist, a smart\nglasses system that models a user's working memory (WM) in real-time using\nmulti-modal sensor signals. Grounded in cognitive theories of WM, our system\nrepresents perceived information as memory items and episodes with encoding\nmechanisms, such as displacement and interference. This WM model informs a\ntiming predictor that balances the value of assistance with the cost of\ninterruption. In a user study with 12 participants completing cognitively\ndemanding tasks, ProMemAssist delivered more selective assistance and received\nhigher engagement compared to an LLM baseline system. Qualitative feedback\nhighlights the benefits of WM modeling for nuanced, context-sensitive support,\noffering design implications for more attentive and user-aware proactive\nagents.", "AI": {"tldr": "ProMemAssist\u662f\u4e00\u79cd\u667a\u80fd\u773c\u955c\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u5b9e\u65f6\u5efa\u6a21\u7528\u6237\u7684\u5de5\u4f5c\u8bb0\u5fc6\uff0c\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u534f\u52a9\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234AI\u7cfb\u7edf\u4f9d\u8d56\u7528\u6237\u542f\u52a8\u6216\u9884\u5b9a\u4e49\u4efb\u52a1\u77e5\u8bc6\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u7684\u5f53\u524d\u5fc3\u7406\u72b6\u6001\u3002", "method": "\u57fa\u4e8e\u5de5\u4f5c\u8bb0\u5fc6\u8ba4\u77e5\u7406\u8bba\uff0c\u7cfb\u7edf\u5c06\u611f\u77e5\u4fe1\u606f\u5efa\u6a21\u4e3a\u8bb0\u5fc6\u9879\u548c\u7247\u6bb5\uff0c\u5e76\u7ed3\u5408\u7f16\u7801\u673a\u5236\u8bbe\u8ba1\u65f6\u95f4\u9884\u6d4b\u5668\u3002", "result": "\u572812\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0cProMemAssist\u6bd4LLM\u57fa\u7ebf\u7cfb\u7edf\u66f4\u7cbe\u51c6\u5730\u63d0\u4f9b\u534f\u52a9\uff0c\u5e76\u83b7\u5f97\u66f4\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u5de5\u4f5c\u8bb0\u5fc6\u5efa\u6a21\u80fd\u5b9e\u73b0\u66f4\u7ec6\u81f4\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u534f\u52a9\uff0c\u4e3a\u66f4\u5173\u6ce8\u7528\u6237\u7684\u4e3b\u52a8\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2507.21817", "pdf": "https://arxiv.org/pdf/2507.21817", "abs": "https://arxiv.org/abs/2507.21817", "authors": ["Yikun Li", "Ngoc Tan Bui", "Ting Zhang", "Martin Weyssow", "Chengran Yang", "Xin Zhou", "Jinfeng Jiang", "Junkai Chen", "Huihui Huang", "Huu Hung Nguyen", "Chiok Yew Ho", "Jie Tan", "Ruiyin Li", "Yide Yin", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 40.6% when evaluated on independent data, sometimes underperforming\nrandom guessing.\n  To address these limitations, we present a three-part solution. First, we\nintroduce a manually curated test dataset, BenchVul, covering the MITRE Top 25\nMost Dangerous CWEs. Second, we construct a high-quality training dataset,\nTitanVul, comprising 35,045 functions by aggregating seven public sources and\napplying deduplication and validation using a novel multi-agent LLM framework.\nThird, we propose a Realistic Vulnerability Generation (RVG) framework, which\nsynthesizes context-aware vulnerability examples for underrepresented but\ncritical CWE types through simulated development workflows.\n  Our evaluation shows the strengths of each component in closing the\ngeneralization gap. First, BenchVul shows the limitations of self-testing:\nmodels trained on existing datasets, such as BigVul and PrimeVul, experience\nperformance drops on BenchVul (from 0.776 to 0.519 and from 0.567 to 0.337).\nSecond, training models on TitanVul demonstrates improved generalization, with\nmodel performance increasing from 0.584 when evaluated on the same dataset to\n0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated\ndata yields further gains, increasing model performance by 14.0% to 0.874.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6f0f\u6d1e\u68c0\u6d4b\u6a21\u578b\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u4e09\u90e8\u5206\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6784\u5efa\u548c\u6f0f\u6d1e\u751f\u6210\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u6f0f\u6d1e\u68c0\u6d4b\u7814\u7a76\u5b58\u5728\u6570\u636e\u96c6\u6807\u7b7e\u4e0d\u51c6\u786e\u3001\u91cd\u590d\u7387\u9ad8\u53ca\u8986\u76d6\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u4e0b\u964d\u660e\u663e\u3002", "method": "1. \u6784\u5efa\u624b\u52a8\u6807\u6ce8\u7684\u6d4b\u8bd5\u6570\u636e\u96c6BenchVul\uff1b2. \u4f7f\u7528\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u53bb\u91cd\u548c\u9a8c\u8bc1\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u96c6TitanVul\uff1b3. \u63d0\u51fa\u73b0\u5b9e\u6f0f\u6d1e\u751f\u6210\uff08RVG\uff09\u6846\u67b6\uff0c\u751f\u6210\u5173\u952eCWE\u7c7b\u578b\u6f0f\u6d1e\u793a\u4f8b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1. BenchVul\u63ed\u793a\u4e86\u73b0\u6709\u6570\u636e\u96c6\u81ea\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff1b2. TitanVul\u8bad\u7ec3\u7684\u6a21\u578b\u5728BenchVul\u4e0a\u8868\u73b0\u63d0\u5347\uff1b3. \u7ed3\u5408RVG\u7684\u6570\u636e\u8fdb\u4e00\u6b65\u5c06\u6027\u80fd\u63d0\u534714%\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u663e\u8457\u7f29\u5c0f\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u6027\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.21882", "pdf": "https://arxiv.org/pdf/2507.21882", "abs": "https://arxiv.org/abs/2507.21882", "authors": ["Elmira Onagh", "Alireza Davoodi", "Maleknaz Nayebi"], "title": "The Impact of Foundational Models on Patient-Centric e-Health Systems", "categories": ["cs.AI", "cs.SE"], "comment": "Paper published in COMPSAC 2025", "summary": "As Artificial Intelligence (AI) becomes increasingly embedded in healthcare\ntechnologies, understanding the maturity of AI in patient-centric applications\nis critical for evaluating its trustworthiness, transparency, and real-world\nimpact. In this study, we investigate the integration and maturity of AI\nfeature integration in 116 patient-centric healthcare applications. Using Large\nLanguage Models (LLMs), we extracted key functional features, which are then\ncategorized into different stages of the Gartner AI maturity model. Our results\nshow that over 86.21\\% of applications remain at the early stages of AI\nintegration, while only 13.79% demonstrate advanced AI integration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u5e94\u7528\u4e2dAI\u7684\u6210\u719f\u5ea6\uff0c\u53d1\u73b0\u5927\u90e8\u5206\u5e94\u7528\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\u3002", "motivation": "\u8bc4\u4f30AI\u5728\u533b\u7597\u6280\u672f\u4e2d\u7684\u4fe1\u4efb\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u5229\u7528LLM\u63d0\u53d6\u529f\u80fd\u7279\u5f81\uff0c\u5e76\u57fa\u4e8eGartner AI\u6210\u719f\u5ea6\u6a21\u578b\u5206\u7c7b\u3002", "result": "86.21%\u7684\u5e94\u7528\u5904\u4e8eAI\u65e9\u671f\u9636\u6bb5\uff0c\u4ec513.79%\u8fbe\u5230\u9ad8\u7ea7\u9636\u6bb5\u3002", "conclusion": "AI\u5728\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u5e94\u7528\u4e2d\u6210\u719f\u5ea6\u8f83\u4f4e\uff0c\u9700\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.21435", "pdf": "https://arxiv.org/pdf/2507.21435", "abs": "https://arxiv.org/abs/2507.21435", "authors": ["JIaheng Wang", "Yucun Zhong", "Chengjie Huang", "Lin Yao"], "title": "MindChat: Enhancing BCI Spelling with Large Language Models in Realistic Scenarios", "categories": ["cs.HC"], "comment": null, "summary": "Brain-computer interface (BCI) spellers can render a new communication\nchannel independent of peripheral nervous system, which are especially valuable\nfor patients with severe motor disabilities. However, current BCI spellers\noften require users to type intended utterances letter-by-letter while spelling\nerrors grow proportionally due to inaccurate electroencephalogram (EEG)\ndecoding, largely impeding the efficiency and usability of BCIs in real-world\ncommunication. In this paper, we present MindChat, a large language model\n(LLM)-assisted BCI speller to enhance BCI spelling efficiency by reducing\nusers' manual keystrokes. Building upon prompt engineering, we prompt LLMs\n(GPT-4o) to continuously suggest context-aware word and sentence\ncompletions/predictions during spelling. Online copy-spelling experiments\nencompassing four dialogue scenarios demonstrate that MindChat saves more than\n62\\% keystrokes and over 32\\% spelling time compared with traditional BCI\nspellers. We envision high-speed BCI spellers enhanced by LLMs will potentially\nlead to truly practical applications.", "AI": {"tldr": "MindChat\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684BCI\u62fc\u5199\u8f85\u52a9\u5de5\u5177\uff0c\u901a\u8fc7\u51cf\u5c11\u624b\u52a8\u6309\u952e\u63d0\u9ad8\u6548\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u8282\u770162%\u6309\u952e\u548c32%\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edfBCI\u62fc\u5199\u5668\u9010\u5b57\u6bcd\u8f93\u5165\u6613\u51fa\u9519\u4e14\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528LLM\uff08\u5982GPT-4\uff09\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bcd\u53e5\u8865\u5168\u5efa\u8bae\u3002", "result": "\u5728\u7ebf\u5b9e\u9a8c\u4e2d\uff0cMindChat\u8282\u770162%\u6309\u952e\u548c32%\u65f6\u95f4\u3002", "conclusion": "LLM\u589e\u5f3a\u7684BCI\u62fc\u5199\u5668\u6709\u671b\u5b9e\u73b0\u9ad8\u6548\u5b9e\u7528\u7684\u901a\u4fe1\u5e94\u7528\u3002"}}
{"id": "2507.21899", "pdf": "https://arxiv.org/pdf/2507.21899", "abs": "https://arxiv.org/abs/2507.21899", "authors": ["Malik Uzair Mehmood", "Shahid Hussain", "Wen Li Wang", "Muhammad Usama Malik"], "title": "LLM-based Content Classification Approach for GitHub Repositories by the README Files", "categories": ["cs.AI", "cs.LG", "cs.SE"], "comment": "8 pages, 4 Figures", "summary": "GitHub is the world's most popular platform for storing, sharing, and\nmanaging code. Every GitHub repository has a README file associated with it.\nThe README files should contain project-related information as per the\nrecommendations of GitHub to support the usage and improvement of repositories.\nHowever, GitHub repository owners sometimes neglected these recommendations.\nThis prevents a GitHub repository from reaching its full potential. This\nresearch posits that the comprehensiveness of a GitHub repository's README file\nsignificantly influences its adoption and utilization, with a lack of detail\npotentially hindering its full potential for widespread engagement and impact\nwithin the research community. Large Language Models (LLMs) have shown great\nperformance in many text-based tasks including text classification, text\ngeneration, text summarization and text translation. In this study, an approach\nis developed to fine-tune LLMs for automatically classifying different sections\nof GitHub README files. Three encoder-only LLMs are utilized, including BERT,\nDistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a\ngold-standard dataset consisting of 4226 README file sections. This approach\noutperforms current state-of-the-art methods and has achieved an overall F1\nscore of 0.98. Moreover, we have also investigated the use of\nParameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation\n(LoRA) and shown an economical alternative to full fine-tuning without\ncompromising much performance. The results demonstrate the potential of using\nLLMs in designing an automatic classifier for categorizing the content of\nGitHub README files. Consequently, this study contributes to the development of\nautomated tools for GitHub repositories to improve their identifications and\npotential usages.", "AI": {"tldr": "GitHub \u4ed3\u5e93\u7684 README \u6587\u4ef6\u5b8c\u5584\u7a0b\u5ea6\u5f71\u54cd\u5176\u91c7\u7528\u7387\uff0c\u7814\u7a76\u5229\u7528 LLMs \u81ea\u52a8\u5206\u7c7b README \u5185\u5bb9\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "GitHub README \u6587\u4ef6\u7f3a\u4e4f\u8be6\u7ec6\u4fe1\u606f\u53ef\u80fd\u5f71\u54cd\u4ed3\u5e93\u7684\u6f5c\u529b\uff0c\u5e0c\u671b\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u5176\u8bc6\u522b\u548c\u4f7f\u7528\u7387\u3002", "method": "\u4f7f\u7528 BERT\u3001DistilBERT \u548c RoBERTa \u4e09\u79cd LLMs\uff0c\u901a\u8fc7\u5fae\u8c03\u6280\u672f\uff08\u542b PEFT\uff09\u5bf9 4226 \u4e2a README \u6587\u4ef6\u7247\u6bb5\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u65b9\u6cd5\u6574\u4f53 F1 \u5206\u6570\u8fbe 0.98\uff0cPEFT \u6280\u672f\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "LLMs \u53ef\u9ad8\u6548\u5206\u7c7b README \u6587\u4ef6\uff0c\u4e3a GitHub \u4ed3\u5e93\u7684\u81ea\u52a8\u5316\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.21462", "pdf": "https://arxiv.org/pdf/2507.21462", "abs": "https://arxiv.org/abs/2507.21462", "authors": ["Tingying He", "Maggie McCracken", "Daniel Hajas", "Sarah Creem-Regehr", "Alexander Lex"], "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals", "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether tactile charts support comprehension and learning of\ncomplex visualizations for blind and low-vision (BLV) individuals and\ncontribute four tactile chart designs and an interview study. Visualizations\nare powerful tools for conveying data, yet BLV individuals typically can rely\nonly on assistive technologies -- primarily alternative texts -- to access this\ninformation. Prior research shows the importance of mental models of chart\ntypes for interpreting these descriptions, yet BLV individuals have no means to\nbuild such a mental model based on images of visualizations. Tactile charts\nshow promise to fill this gap in supporting the process of building mental\nmodels. Yet studies on tactile data representations mostly focus on simple\nchart types, and it is unclear whether they are also appropriate for more\ncomplex charts as would be found in scientific publications. Working with two\nBLV researchers, we designed 3D-printed tactile template charts with\nexploration instructions for four advanced chart types: UpSet plots, violin\nplots, clustered heatmaps, and faceted line charts. We then conducted an\ninterview study with 12 BLV participants comparing whether using our tactile\ntemplates improves mental models and understanding of charts and whether this\nunderstanding translates to novel datasets experienced through alt texts.\nThematic analysis shows that tactile models support chart type understanding\nand are the preferred learning method by BLV individuals. We also report\nparticipants' opinions on tactile chart design and their role in BLV education.", "AI": {"tldr": "\u63a2\u7d22\u89e6\u89c9\u56fe\u8868\u662f\u5426\u6709\u52a9\u4e8e\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7406\u89e3\u590d\u6742\u56fe\u8868\uff0c\u63d0\u51fa\u56db\u79cd\u89e6\u89c9\u56fe\u8868\u8bbe\u8ba1\u5e76\u901a\u8fc7\u8bbf\u8c08\u7814\u7a76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u89c6\u89c9\u56fe\u8868\u662f\u4f20\u8fbe\u6570\u636e\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u53ea\u80fd\u4f9d\u8d56\u66ff\u4ee3\u6587\u672c\u8bbf\u95ee\u4fe1\u606f\uff1b\u7f3a\u4e4f\u5efa\u7acb\u56fe\u8868\u5fc3\u7406\u6a21\u578b\u7684\u9014\u5f84\u3002", "method": "\u4e0e\u4e24\u4f4d\u76f2\u4eba\u7814\u7a76\u4eba\u5458\u5408\u4f5c\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u590d\u6742\u56fe\u8868\u76843D\u6253\u5370\u89e6\u89c9\u6a21\u677f\uff0c\u5e76\u901a\u8fc712\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u8bbf\u8c08\u7814\u7a76\u3002", "result": "\u89e6\u89c9\u6a21\u578b\u652f\u6301\u5bf9\u56fe\u8868\u7c7b\u578b\u7684\u7406\u89e3\uff0c\u5e76\u6210\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u9996\u9009\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u89e6\u89c9\u56fe\u8868\u8bbe\u8ba1\u6709\u52a9\u4e8e\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u5efa\u7acb\u56fe\u8868\u5fc3\u7406\u6a21\u578b\uff0c\u5e76\u5728\u6559\u80b2\u4e2d\u53d1\u6325\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2507.21490", "pdf": "https://arxiv.org/pdf/2507.21490", "abs": "https://arxiv.org/abs/2507.21490", "authors": ["Hannah Kim", "Sergei L. Kosakovsky Pond", "Stephen MacNeil"], "title": "Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning", "categories": ["cs.HC", "cs.CY", "cs.IR", "J.3; K.3.2"], "comment": "9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education\n  (FIE) Conference preprint", "summary": "This full research paper investigates the impact of generative AI (GenAI) on\nthe learner experience, with a focus on how learners engage with and utilize\nthe information it provides. In e-learning environments, learners often need to\nnavigate a complex information space on their own. This challenge is further\ncompounded in interdisciplinary fields like bioinformatics, due to the varied\nprior knowledge and backgrounds. In this paper, we studied how GenAI influences\ninformation search in bioinformatics research: (1) How do interactions with a\nGenAI chatbot influence learner orienteering behaviors?; and (2) How do\nlearners identify information scent in GenAI chatbot responses? We adopted an\nautoethnographic approach to investigate these questions. GenAI was found to\nsupport orienteering once a learning plan was established, but it was\ncounterproductive prior to that. Moreover, traditionally value-rich information\nsources such as bullet points and related terms proved less effective when\napplied to GenAI responses. Information scents were primarily recognized\nthrough the presence or absence of prior knowledge of the domain. These\nfindings suggest that GenAI should be adopted into e-learning environments with\ncaution, particularly in interdisciplinary learning contexts.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5728\u751f\u7269\u4fe1\u606f\u5b66\u5b66\u4e60\u4e2d\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u8005\u7684\u4fe1\u606f\u641c\u7d22\u884c\u4e3a\u3002\u7814\u7a76\u53d1\u73b0\uff0cGenAI\u5728\u5b66\u4e60\u8ba1\u5212\u660e\u786e\u540e\u6709\u52a9\u4e8e\u5bfc\u5411\u884c\u4e3a\uff0c\u4f46\u5728\u521d\u671f\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002\u4fe1\u606f\u7ebf\u7d22\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u5b66\u4e60\u8005\u5bf9\u9886\u57df\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u63a2\u7d22GenAI\u5728\u590d\u6742\u7684\u8de8\u5b66\u79d1\u5b66\u4e60\u73af\u5883\uff08\u5982\u751f\u7269\u4fe1\u606f\u5b66\uff09\u4e2d\u5bf9\u5b66\u4e60\u8005\u4fe1\u606f\u641c\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u81ea\u6c11\u65cf\u5fd7\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u5b66\u4e60\u8005\u4e0eGenAI\u804a\u5929\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u884c\u4e3a\u3002", "result": "GenAI\u5728\u5b66\u4e60\u8ba1\u5212\u660e\u786e\u540e\u652f\u6301\u5bfc\u5411\u884c\u4e3a\uff0c\u800c\u5728\u521d\u671f\u53ef\u80fd\u963b\u788d\u5b66\u4e60\u3002\u4fe1\u606f\u7ebf\u7d22\u7684\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "\u5efa\u8bae\u5728\u8de8\u5b66\u79d1\u5b66\u4e60\u73af\u5883\u4e2d\u8c28\u614e\u5f15\u5165GenAI\u3002"}}
{"id": "2507.21654", "pdf": "https://arxiv.org/pdf/2507.21654", "abs": "https://arxiv.org/abs/2507.21654", "authors": ["Meryem Yilmaz Soylu", "Jeonghyun Lee", "Jui-Tse Hung", "Christopher Zhang Cui", "David A. Joyner"], "title": "AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind", "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.6"], "comment": "34 pages, 1 figure, 3 tables", "summary": "As Artificial Intelligence (AI) tools become increasingly embedded in higher\neducation, understanding how students interact with these systems is essential\nto supporting effective learning. This study examines how students' AI literacy\nand prior exposure to AI technologies shape their perceptions of Socratic Mind,\nan interactive AI-powered formative assessment tool. Drawing on\nSelf-Determination Theory and user experience research, we analyze\nrelationships among AI literacy, perceived usability, satisfaction, engagement,\nand perceived learning effectiveness. Data from 309 undergraduates in Computer\nScience and Business courses were collected through validated surveys. Partial\nleast squares structural equation modeling showed that AI literacy - especially\nself-efficacy, conceptual understanding, and application skills - significantly\npredicts usability, satisfaction, and engagement. Usability and satisfaction,\nin turn, strongly predict perceived learning effectiveness, while prior AI\nexposure showed no significant effect. These findings highlight that AI\nliteracy, rather than exposure alone, shapes student experiences. Designers\nshould integrate adaptive guidance and user-centered features to support\ndiverse literacy levels, fostering inclusive, motivating, and effective\nAI-based learning environments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5b66\u751fAI\u7d20\u517b\u53ca\u5bf9AI\u5de5\u5177\u7684\u611f\u77e5\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u6548\u679c\uff0c\u53d1\u73b0AI\u7d20\u517b\u663e\u8457\u9884\u6d4b\u7528\u6237\u4f53\u9a8c\u548c\u5b66\u4e60\u6548\u679c\uff0c\u800cAI\u63a5\u89e6\u65e0\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u6559\u80b2\u4e2d\u7684\u666e\u53ca\uff0c\u4e86\u89e3\u5b66\u751f\u7684AI\u7d20\u517b\u5982\u4f55\u5f71\u54cd\u5176\u5bf9AI\u5de5\u5177\u7684\u4f7f\u7528\u548c\u5b66\u4e60\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u95ee\u5377\u8c03\u67e5309\u540d\u672c\u79d1\u751f\uff0c\u7ed3\u5408Self-Determination Theory\u548c\u7528\u6237\u4f53\u9a8c\u7814\u7a76\uff0c\u901a\u8fc7\u504f\u6700\u5c0f\u4e8c\u4e58\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\u5206\u6790\u6570\u636e\u3002", "result": "AI\u7d20\u517b\uff08\u81ea\u6211\u6548\u80fd\u3001\u6982\u5ff5\u7406\u89e3\u548c\u5e94\u7528\u6280\u80fd\uff09\u663e\u8457\u9884\u6d4b\u53ef\u7528\u6027\u3001\u6ee1\u610f\u5ea6\u548c\u53c2\u4e0e\u5ea6\uff1b\u53ef\u7528\u6027\u4e0e\u6ee1\u610f\u5ea6\u8fdb\u4e00\u6b65\u9884\u6d4b\u5b66\u4e60\u6548\u679c\uff0c\u800cAI\u63a5\u89e6\u65e0\u5f71\u54cd\u3002", "conclusion": "\u8bbe\u8ba1AI\u5b66\u4e60\u5de5\u5177\u65f6\u5e94\u6ce8\u91cd\u7528\u6237\u4e2d\u5fc3\u5316\u548c\u9002\u5e94\u6027\u6307\u5bfc\uff0c\u4ee5\u652f\u6301\u4e0d\u540c\u7d20\u517b\u6c34\u5e73\u7684\u5b66\u751f\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2507.21811", "pdf": "https://arxiv.org/pdf/2507.21811", "abs": "https://arxiv.org/abs/2507.21811", "authors": ["Cynthia Zastudil", "Christine Holyfield", "Christine Kapp", "Kate Hamilton", "Kriti Baru", "Liam Newsam", "June A. Smith", "Stephen MacNeil"], "title": "Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC", "categories": ["cs.HC"], "comment": null, "summary": "Augmentative and alternative communication (AAC) devices are used by many\npeople around the world who experience difficulties in communicating verbally.\nOne AAC device which is especially useful for minimally verbal autistic\nchildren in developing language and communication skills are visual scene\ndisplays (VSD). VSDs use images with interactive hotspots embedded in them to\ndirectly connect language to real-world contexts which are meaningful to the\nAAC user. While VSDs can effectively support emergent communicators, their\nwidespread adoption is impacted by how difficult these devices are to\nconfigure. We developed a prototype that uses generative AI to automatically\nsuggest initial hotspots on an image to help non-experts efficiently create\nVSDs. We conducted a within-subjects user study to understand how effective our\nprototype is in supporting non-expert users, specifically pre-service\nspeech-language pathologists (SLP) who are not familiar with VSDs as an AAC\nintervention. Pre-service SLPs are actively studying to become clinically\ncertified SLPs and have domain-specific knowledge about language and\ncommunication skill development. We evaluated the effectiveness of our\nprototype based on creation time, quality, and user confidence. We also\nanalyzed the relevance and developmental appropriateness of the automatically\ngenerated hotspots and how often users interacted with the generated hotspots.\nOur results were mixed with SLPs becoming more efficient and confident.\nHowever, there were multiple negative impacts as well, including over-reliance\nand homogenization of communication options. The implications of these findings\nreach beyond the domain of AAC, especially as generative AI becomes more\nprevalent across domains, including assistive technology. Future work is needed\nto further identify and address these risks associated with integrating\ngenerative AI into assistive technology.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u539f\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u4e3a\u89c6\u89c9\u573a\u666f\u663e\u793a\uff08VSD\uff09\u751f\u6210\u70ed\u70b9\uff0c\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u5feb\u901f\u914d\u7f6eAAC\u8bbe\u5907\u3002", "motivation": "VSD\u5bf9\u975e\u8bed\u8a00\u81ea\u95ed\u75c7\u513f\u7ae5\u7684\u8bed\u8a00\u53d1\u5c55\u975e\u5e38\u6709\u6548\uff0c\u4f46\u914d\u7f6e\u56f0\u96be\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5f0fAI\u81ea\u52a8\u751f\u6210\u521d\u59cb\u70ed\u70b9\uff0c\u5e76\u8fdb\u884c\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u539f\u578b\u63d0\u5347\u4e86\u6548\u7387\u548c\u7528\u6237\u4fe1\u5fc3\uff0c\u4f46\u4e5f\u5bfc\u81f4\u8fc7\u5ea6\u4f9d\u8d56\u548c\u6c9f\u901a\u9009\u9879\u540c\u8d28\u5316\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u8f85\u52a9\u6280\u672f\u4e2d\u7684\u5e94\u7528\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2507.21837", "pdf": "https://arxiv.org/pdf/2507.21837", "abs": "https://arxiv.org/abs/2507.21837", "authors": ["Yotam Sechayk", "Ariel Shamir", "Amy Pavel", "Takeo Igarashi"], "title": "VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos", "categories": ["cs.HC"], "comment": "ASSETS '25, Denver, CO, USA", "summary": "Instructors often rely on visual actions such as pointing, marking, and\nsketching to convey information in educational presentation videos. These\nsubtle visual cues often lack verbal descriptions, forcing low-vision (LV)\nlearners to search for visual indicators or rely solely on audio, which can\nlead to missed information and increased cognitive load. To address this\nchallenge, we conducted a co-design study with three LV participants and\ndeveloped VeasyGuide, a tool that uses motion detection to identify instructor\nactions and dynamically highlight and magnify them. VeasyGuide produces\nfamiliar visual highlights that convey spatial context and adapt to diverse\nlearners and content through extensive personalization and real-time visual\nfeedback. VeasyGuide reduces visual search effort by clarifying what to look\nfor and where to look. In an evaluation with 8 LV participants, learners\ndemonstrated a significant improvement in detecting instructor actions, with\nfaster response times and significantly reduced cognitive load. A separate\nevaluation with 8 sighted participants showed that VeasyGuide also enhanced\nengagement and attentiveness, suggesting its potential as a universally\nbeneficial tool.", "AI": {"tldr": "VeasyGuide\u662f\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u9ad8\u4eae\u548c\u653e\u5927\u6559\u5e08\u52a8\u4f5c\u7684\u6559\u5b66\u8f85\u52a9\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u89c6\u529b\u5b66\u4e60\u8005\u7684\u5b66\u4e60\u6548\u679c\u548c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u6559\u5e08\u5728\u89c6\u9891\u4e2d\u5e38\u4f7f\u7528\u89c6\u89c9\u52a8\u4f5c\u4f20\u8fbe\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u7f3a\u4e4f\u8bed\u97f3\u63cf\u8ff0\u7684\u89c6\u89c9\u7ebf\u7d22\u5bf9\u4f4e\u89c6\u529b\u5b66\u4e60\u8005\u9020\u6210\u4fe1\u606f\u9057\u6f0f\u548c\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u901a\u8fc7\u4e0e\u4f4e\u89c6\u529b\u5b66\u4e60\u8005\u5171\u540c\u8bbe\u8ba1\uff0c\u5f00\u53d1\u4e86VeasyGuide\uff0c\u5229\u7528\u8fd0\u52a8\u68c0\u6d4b\u8bc6\u522b\u6559\u5e08\u52a8\u4f5c\u5e76\u52a8\u6001\u9ad8\u4eae\u548c\u653e\u5927\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f4e\u89c6\u529b\u5b66\u4e60\u8005\u80fd\u66f4\u5feb\u68c0\u6d4b\u6559\u5e08\u52a8\u4f5c\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\uff1b\u89c6\u529b\u6b63\u5e38\u5b66\u4e60\u8005\u7684\u4e13\u6ce8\u529b\u4e5f\u6709\u6240\u63d0\u5347\u3002", "conclusion": "VeasyGuide\u662f\u4e00\u79cd\u5bf9\u5404\u7c7b\u5b66\u4e60\u8005\u90fd\u6709\u76ca\u7684\u901a\u7528\u5de5\u5177\u3002"}}
{"id": "2507.21900", "pdf": "https://arxiv.org/pdf/2507.21900", "abs": "https://arxiv.org/abs/2507.21900", "authors": ["Swaroop Panda", "Arun Kumar Sekar"], "title": "Leveraging LLMs for Persona-Based Visualization of Election Data", "categories": ["cs.HC"], "comment": null, "summary": "Visualizations are essential tools for disseminating information regarding\nelections and their outcomes, potentially influencing public perceptions.\nPersonas, delineating distinctive segments within the populace, furnish a\nvaluable framework for comprehending the nuanced perspectives, requisites, and\nbehaviors of diverse voter demographics. In this work, we propose making\nvisualizations tailored to these personas to make election information easier\nto understand and more relevant. Using data from UK parliamentary elections and\nnew developments in Large Language Models (LLMs), we create personas that\nencompass the diverse demographics, technological preferences, voting\ntendencies, and information consumption patterns observed among\nvoters.Subsequently, we elucidate how these personas can inform the design of\nvisualizations through specific design criteria. We then provide illustrative\nexamples of visualization prototypes based on these criteria and evaluate these\nprototypes using these personas and LLMs. We finally propose some actionable\ninsights based upon the framework and the different design artifacts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9009\u6c11\u89d2\u8272\u5b9a\u5236\u9009\u4e3e\u53ef\u89c6\u5316\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u4fe1\u606f\u7684\u7406\u89e3\u4e0e\u76f8\u5173\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u89d2\u8272\u5212\u5206\u7406\u89e3\u4e0d\u540c\u9009\u6c11\u7684\u9700\u6c42\u4e0e\u884c\u4e3a\uff0c\u4ece\u800c\u4f18\u5316\u9009\u4e3e\u4fe1\u606f\u7684\u4f20\u64ad\u6548\u679c\u3002", "method": "\u5229\u7528\u82f1\u56fd\u8bae\u4f1a\u9009\u4e3e\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u521b\u5efa\u9009\u6c11\u89d2\u8272\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u89d2\u8272\u8bbe\u8ba1\u53ef\u89c6\u5316\u539f\u578b\u3002", "result": "\u901a\u8fc7\u89d2\u8272\u548cLLMs\u8bc4\u4f30\u53ef\u89c6\u5316\u539f\u578b\uff0c\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u8bbe\u8ba1\u6807\u51c6\u548c\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "\u57fa\u4e8e\u89d2\u8272\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u9009\u4e3e\u4fe1\u606f\u7684\u7406\u89e3\u4e0e\u76f8\u5173\u6027\uff0c\u4e3a\u672a\u6765\u9009\u4e3e\u4f20\u64ad\u5de5\u5177\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.21953", "pdf": "https://arxiv.org/pdf/2507.21953", "abs": "https://arxiv.org/abs/2507.21953", "authors": ["Yi Kong", "Dianxi Shi", "Guoli Yang", "Zhang ke-di", "Chenlin Huang", "Xiaopeng Li", "Songchang Jin"], "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMapAgent\u7684\u65b0\u578bLLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u6784\u5efa\u7684\u8bb0\u5fc6\u6765\u589e\u5f3a\u4efb\u52a1\u89c4\u5212\u80fd\u529b\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u79fb\u52a8\u5e94\u7528\u4efb\u52a1\u65f6\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u81ea\u52a8\u5316\u4efb\u52a1\u65f6\uff0c\u56e0\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u4e86\u89e3\uff0c\u5bfc\u81f4\u4efb\u52a1\u89c4\u5212\u65e0\u6548\u751a\u81f3\u51fa\u73b0\u5e7b\u89c9\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "1\uff09\u63d0\u51fa\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06\u4efb\u52a1\u6267\u884c\u8f68\u8ff9\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u7684\u7ed3\u6784\u5316\u9875\u9762\u8bb0\u5fc6\u6570\u636e\u5e93\uff1b2\uff09\u5f15\u5165\u7531\u7c97\u5230\u7ec6\u7684\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\uff0c\u4ece\u8bb0\u5fc6\u4e2d\u68c0\u7d22\u76f8\u5173\u9875\u9762\u4ee5\u589e\u5f3aLLM\u89c4\u5212\u80fd\u529b\uff1b3\uff09\u901a\u8fc7\u53ccLLM\u67b6\u6784\u7684\u4efb\u52a1\u6267\u884c\u5668\u5c06\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMapAgent\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MapAgent\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u7684\u4efb\u52a1\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u590d\u6742\u79fb\u52a8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.22051", "pdf": "https://arxiv.org/pdf/2507.22051", "abs": "https://arxiv.org/abs/2507.22051", "authors": ["Liwenhan Xie", "Jiayi Zhou", "Anyi Rao", "Huamin Qu", "Xinhuan Shu"], "title": "DataSway: Vivifying Metaphoric Visualization with Animation Clip Generation and Coordination", "categories": ["cs.HC"], "comment": "19 pages, 5 figures", "summary": "Animating metaphoric visualizations brings data to life, enhancing the\ncomprehension of abstract data encodings and fostering deeper engagement.\nHowever, creators face significant challenges in designing these animations,\nsuch as crafting motions that align semantically with the metaphors,\nmaintaining faithful data representation during animation, and seamlessly\nintegrating interactivity. We propose a human-AI co-creation workflow that\nfacilitates creating animations for SVG-based metaphoric visualizations. Users\ncan initially derive animation clips for data elements from vision-language\nmodels (VLMs) and subsequently coordinate their timelines based on entity\norder, attribute values, spatial layout, or randomness. Our design decisions\nwere informed by a formative study with experienced designers (N=8). We further\ndeveloped a prototype, DataSway, and conducted a user study (N=14) to evaluate\nits creativity support and usability. A gallery with 6 cases demonstrates its\ncapabilities and applications in web-based hypermedia. We conclude with\nimplications for future research on bespoke data visualization animation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u4e3aSVG\u9690\u55bb\u53ef\u89c6\u5316\u8bbe\u8ba1\u52a8\u753b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7528\u6237\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u52a8\u753b\u8bbe\u8ba1\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u6570\u636e\u5fe0\u5b9e\u5ea6\u548c\u4ea4\u4e92\u6027\u95ee\u9898\u3002", "motivation": "\u9690\u55bb\u53ef\u89c6\u5316\u52a8\u753b\u80fd\u63d0\u5347\u62bd\u8c61\u6570\u636e\u7684\u7406\u89e3\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u8bbe\u8ba1\u8fd9\u4e9b\u52a8\u753b\u5b58\u5728\u6311\u6218\uff0c\u5982\u8bed\u4e49\u5bf9\u9f50\u3001\u6570\u636e\u5fe0\u5b9e\u5ea6\u548c\u4ea4\u4e92\u6027\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u52a8\u753b\u7247\u6bb5\uff0c\u7528\u6237\u53ef\u6839\u636e\u5b9e\u4f53\u987a\u5e8f\u3001\u5c5e\u6027\u503c\u3001\u7a7a\u95f4\u5e03\u5c40\u6216\u968f\u673a\u6027\u534f\u8c03\u65f6\u95f4\u8f74\u3002", "result": "\u5f00\u53d1\u4e86\u539f\u578bDataSway\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u652f\u6301\u521b\u610f\u548c\u53ef\u7528\u6027\u30026\u4e2a\u6848\u4f8b\u5c55\u793a\u5176\u5728\u8d85\u5a92\u4f53\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5b9a\u5236\u5316\u6570\u636e\u53ef\u89c6\u5316\u52a8\u753b\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2507.21054", "pdf": "https://arxiv.org/pdf/2507.21054", "abs": "https://arxiv.org/abs/2507.21054", "authors": ["Robert Sparrow", "Joshua Hatherley"], "title": "High hopes for \"Deep Medicine\"? AI, economics, and the future of care", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "In the much-celebrated book Deep Medicine, Eric Topol argues that the\ndevelopment of artificial intelligence for health care will lead to a dramatic\nshift in the culture and practice of medicine. In the next several decades, he\nsuggests, AI will become sophisticated enough that many of the everyday tasks\nof physicians could be delegated to it. Topol is perhaps the most articulate\nadvocate of the benefits of AI in medicine, but he is hardly alone in spruiking\nits potential to allow physicians to dedicate more of their time and attention\nto providing empathetic care for their patients in the future. Unfortunately,\nseveral factors suggest a radically different picture for the future of health\ncare. Far from facilitating a return to a time of closer doctor-patient\nrelationships, the use of medical AI seems likely to further erode therapeutic\nrelationships and threaten professional and patient satisfaction.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86AI\u5728\u533b\u7597\u4e2d\u7684\u524d\u666f\uff0c\u8ba4\u4e3a\u867d\u7136\u67d0\u4e9b\u89c2\u70b9\u8ba4\u4e3aAI\u80fd\u6539\u5584\u533b\u60a3\u5173\u7cfb\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u53ef\u80fd\u76f8\u53cd\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u533b\u7597\u4e2d\u7684\u5e94\u7528\u5bf9\u533b\u60a3\u5173\u7cfb\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u53cd\u9a73\u4e50\u89c2\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u89c2\u70b9\u548c\u6f5c\u5728\u56e0\u7d20\uff0c\u63d0\u51fa\u4e0d\u540c\u89c6\u89d2\u3002", "result": "\u6307\u51faAI\u53ef\u80fd\u8fdb\u4e00\u6b65\u524a\u5f31\u533b\u60a3\u5173\u7cfb\uff0c\u5f71\u54cd\u6ee1\u610f\u5ea6\u3002", "conclusion": "AI\u5728\u533b\u7597\u4e2d\u7684\u5e94\u7528\u9700\u8c28\u614e\uff0c\u4ee5\u907f\u514d\u8d1f\u9762\u793e\u4f1a\u5f71\u54cd\u3002"}}
{"id": "2507.21065", "pdf": "https://arxiv.org/pdf/2507.21065", "abs": "https://arxiv.org/abs/2507.21065", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "comment": "submitted to ICSR2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u793e\u4ea4\u5316\u5b66\u4e60\u8303\u5f0f\uff08\u5982\u5bf9\u8bdd\u5f0f\u6559\u5b66\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355\u5411\u6559\u5b66\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfAI\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u548c\u7a00\u758f\u53cd\u9988\uff0c\u9650\u5236\u4e86\u6a21\u578b\u4ece\u4ea4\u4e92\u4e2d\u9ad8\u6548\u5b66\u4e60\u7684\u80fd\u529b\u3002\u53d7\u7ef4\u679c\u8328\u57fa\u793e\u4f1a\u6587\u5316\u7406\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u63a2\u7d22\u793e\u4ea4\u5316\u5b66\u4e60\u8303\u5f0f\u662f\u5426\u6709\u6548\u3002", "method": "\u5728\u2018AI Social Gym\u2019\u52a8\u6001\u73af\u5883\u4e2d\uff0cAI\u5b66\u4e60\u8005\u4e0e\u6559\u5e08\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u4e92\u52a8\u5b66\u4e60\uff0c\u7279\u522b\u662f\u6df7\u5408\u65b9\u5411\uff08\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u89e3\u91ca\u4e0e\u5b66\u4e60\u8005\u63d0\u95ee\uff09\u7684\u6559\u5b66\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u8bdd\u5f0f\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u6df7\u5408\u65b9\u5411\u4e92\u52a8\uff09\u663e\u8457\u63d0\u5347LLM\u77e5\u8bc6\u83b7\u53d6\u4e0e\u5e94\u7528\u80fd\u529b\uff0c\u4f18\u4e8e\u5355\u5411\u6559\u5b66\u6216\u76f4\u63a5\u8bbf\u95ee\u7ed3\u6784\u5316\u77e5\u8bc6\u3002", "conclusion": "\u5c06\u6559\u5b66\u548c\u5fc3\u7406\u6d1e\u89c1\u878d\u5165AI\u8bad\u7ec3\u53ef\u5927\u5e45\u63d0\u5347\u77e5\u8bc6\u83b7\u53d6\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u4e3a\u73b0\u6709\u7b56\u7565\uff08\u5982\u63d0\u793a\u5de5\u7a0b\uff09\u63d0\u4f9b\u4e86\u8865\u5145\u8def\u5f84\u3002"}}
{"id": "2507.21067", "pdf": "https://arxiv.org/pdf/2507.21067", "abs": "https://arxiv.org/abs/2507.21067", "authors": ["Jan Kapusta"], "title": "SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration", "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0\n  protocol specification, and formal BNF grammar", "summary": "Current AI systems rely on opaque reasoning processes that hinder human\noversight and collaborative potential. Conventional explainable AI approaches\noffer post-hoc justifications and often fail to establish genuine symbiotic\ncollaboration. In this paper, the Symbiotic Epistemology is presented as a\nphilosophical foundation for human-AI cognitive partnerships. Unlike frameworks\nthat treat AI as a mere tool or replacement, symbiotic epistemology positions\nAI as a reasoning partner, fostering calibrated trust by aligning human\nconfidence with AI reliability through explicit reasoning patterns and\nconfidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as\na formal protocol for transparent human-AI collaboration. The framework is\nempirically validated through actual human-AI dialogues demonstrating AI's\nadaptation to structured reasoning protocols and successful metacognitive\nintervention. The protocol defines two complementary mechanisms: TRACE for\nhigh-level reasoning patterns and TRACE_FE for detailed factor explanations. It\nalso integrates confidence quantification, declarative control over AI\nbehavior, and context inheritance for multi-agent coordination. By structuring\ncommunication and embedding confidence-calibrated transparency, SynLang,\ntogether with symbiotic epistemology, enables AI systems that enhance human\nintelligence, preserve human agency, and uphold ethical accountability in\ncollaborative decision-making. Through dual-level transparency, beginning with\nhigh-level reasoning patterns and progressing to granular explanations, the\nprotocol facilitates rapid comprehension and supports thorough verification of\nAI decision-making.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5171\u751f\u8ba4\u8bc6\u8bba\u4f5c\u4e3a\u4eba\u673a\u8ba4\u77e5\u5408\u4f5c\u7684\u54f2\u5b66\u57fa\u7840\uff0c\u5e76\u4ecb\u7ecd\u4e86SynLang\u4f5c\u4e3a\u900f\u660e\u4eba\u673a\u534f\u4f5c\u7684\u6b63\u5f0f\u534f\u8bae\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u7684\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u4eba\u7c7b\u76d1\u7763\u548c\u534f\u4f5c\u6f5c\u529b\uff0c\u4f20\u7edf\u53ef\u89e3\u91caAI\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u5171\u751f\u5408\u4f5c\u3002", "method": "\u901a\u8fc7SynLang\u534f\u8bae\uff08\u5305\u62ecTRACE\u548cTRACE_FE\u673a\u5236\uff09\u548c\u5171\u751f\u8ba4\u8bc6\u8bba\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u534f\u4f5c\u3001\u4fe1\u5fc3\u91cf\u5316\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SynLang\u5728\u5b9e\u9645\u4eba\u673a\u5bf9\u8bdd\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86AI\u5bf9\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8bae\u7684\u9002\u5e94\u548c\u5143\u8ba4\u77e5\u5e72\u9884\u80fd\u529b\u3002", "conclusion": "\u5171\u751f\u8ba4\u8bc6\u8bba\u4e0eSynLang\u901a\u8fc7\u53cc\u91cd\u900f\u660e\u6027\u548c\u4fe1\u5fc3\u6821\u51c6\uff0c\u589e\u5f3a\u4e86\u4eba\u7c7b\u667a\u80fd\uff0c\u4fdd\u7559\u4e86\u4eba\u7c7b\u80fd\u52a8\u6027\uff0c\u5e76\u652f\u6301\u4f26\u7406\u534f\u4f5c\u51b3\u7b56\u3002"}}
{"id": "2507.21069", "pdf": "https://arxiv.org/pdf/2507.21069", "abs": "https://arxiv.org/abs/2507.21069", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u7269\u7406\u6cbb\u7597\u7ec3\u4e60\u548c\u6b65\u6001\u76f8\u5173\u6d3b\u52a8\u7684\u6570\u636e\uff0c\u7528\u4e8e\u5f00\u53d1\u4f20\u611f\u5668\u5206\u7c7b\u6a21\u578b\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4f20\u611f\u5668\u5206\u7c7b\u6a21\u578b\u5f00\u53d1\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u95ee\u9898\uff0c\u63d0\u4f9b\u591a\u6837\u5316\u4e14\u7cbe\u786e\u7684\u6570\u636e\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u540c\u6b65\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u548c\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\uff08MoCap\uff09\u7cfb\u7edf\u8bb0\u5f55\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u5904\u7406\u540e\u7684IMU\u65b9\u5411\u548c\u5de5\u5177\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b19\u540d\u53c2\u4e0e\u8005\u7684\u539f\u59cb\u548c\u5904\u7406\u6570\u636e\uff0c\u652f\u6301\u591a\u79cd\u5206\u6790\u4efb\u52a1\uff0c\u5982\u81ea\u52a8\u7ec3\u4e60\u8bc4\u4f30\u548c\u6b65\u6001\u5206\u6790\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u65e8\u5728\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u4eba\u7c7b\u8fd0\u52a8\u5206\u6790\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4ee3\u7801\u4ee5\u652f\u6301\u91cd\u590d\u6027\u3002"}}
{"id": "2507.21073", "pdf": "https://arxiv.org/pdf/2507.21073", "abs": "https://arxiv.org/abs/2507.21073", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "categories": ["cs.CL", "cs.HC"], "comment": "45 pages, 11 figures", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86EFL\u4e2d\u5b66\u751f\u5728\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u5982\u4f55\u7f16\u8f91AI\u751f\u6210\u7684\u6587\u672c\uff0c\u5206\u6790\u4e86\u7f16\u8f91\u884c\u4e3a\u5bf9\u5176\u8bf4\u660e\u6587\u5199\u4f5c\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u751f\u6210\u7684\u6587\u5b57\u6570\u91cf\u5bf9\u6240\u6709\u8bc4\u5206\u7ef4\u5ea6\u6709\u6b63\u9762\u9884\u6d4b\u4f5c\u7528\uff0c\u800c\u7f16\u8f91\u884c\u4e3a\u7684\u6539\u5584\u6548\u679c\u6709\u9650\u3002", "motivation": "\u968f\u7740AI\u804a\u5929\u673a\u5668\u4eba\u5728EFL\u5199\u4f5c\u4e2d\u7684\u666e\u53ca\uff0c\u5176\u5bf9\u5b66\u751f\u7684\u5199\u4f5c\u8fc7\u7a0b\u548c\u4f5c\u54c1\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u5206\u679039\u540d\u9999\u6e2f\u4e2d\u5b66\u751f\u7684\u5c4f\u5e55\u8bb0\u5f55\u548c\u5199\u4f5c\u4f5c\u54c1\uff0c\u7ed3\u5408\u5b9a\u6027\u7f16\u7801\u3001\u63cf\u8ff0\u6027\u7edf\u8ba1\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3001\u4eba\u5de5\u8bc4\u5206\u548c\u591a\u7ebf\u6027\u56de\u5f52\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b66\u751f\u7684\u7f16\u8f91\u884c\u4e3a\u4e0e\u4f5c\u54c1\u8d28\u91cf\u63d0\u5347\u5173\u8054\u4e0d\u5927\uff0cAI\u751f\u6210\u7684\u6587\u5b57\u6570\u91cf\u5bf9\u8bc4\u5206\u6709\u79ef\u6781\u4f5c\u7528\uff0c\u8868\u660eAI\u662f\u8f85\u52a9\u5de5\u5177\u800c\u975e\u66ff\u4ee3\u54c1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u5728AI\u6574\u5408\u524d\u9700\u8fdb\u884c\u7279\u5b9a\u7c7b\u578b\u7684\u5199\u4f5c\u6307\u5bfc\u548c\u8fc7\u7a0b\u8bad\u7ec3\uff0c\u5e76\u5f00\u53d1\u8bc4\u4f30\u7cfb\u7edf\u4ee5\u91cd\u89c6\u5199\u4f5c\u8fc7\u7a0b\u548c\u7ed3\u679c\u3002"}}
{"id": "2507.21093", "pdf": "https://arxiv.org/pdf/2507.21093", "abs": "https://arxiv.org/abs/2507.21093", "authors": ["Ha Na Cho", "Kyuha Jung", "Daniel Eisenberg", "Cheryl A. King", "Kai Zheng"], "title": "Barriers to Digital Mental Health Services among College Students", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "This qualitative study explores barriers to utilization of digital mental\nhealth Intervention (DMHI) among college students. Data are from a large\nrandomized clinical trial of an intervention, eBridge, that used motivational\ninterviewing for online counseling to connect students with mental health\nissues to professional services. We applied thematic analysis to analyze the\nfeedback from the student participants regarding their experience of using the\nDMHI platform. We identified nine key barriers to DMHI adoption and the use of\nin-person mental health services: emotional distress, time constraints, privacy\nconcerns, resource accessibility, financial challenges, medication stigma,\ndissatisfaction with communication, content clarity, and treatment-related\nconcerns. Our findings emphasize the need for personalized, culturally\nsensitive interventions and improved strategies to enhance the access and\nengagement in mental health support for young adults.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u5b66\u751f\u5728\u5229\u7528\u6570\u5b57\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\uff08DMHI\uff09\u65f6\u9762\u4e34\u7684\u4e5d\u5927\u969c\u788d\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e2a\u6027\u5316\u548c\u6587\u5316\u654f\u611f\u6027\u5e72\u9884\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u963b\u788d\u5927\u5b66\u751f\u4f7f\u7528DMHI\u5e73\u53f0\u7684\u6838\u5fc3\u56e0\u7d20\uff0c\u4ee5\u5e2e\u52a9\u6539\u8fdb\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u63d0\u4f9b\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u5206\u6790\uff0c\u7814\u7a76\u8005\u5bf9\u53c2\u4e0eeBridge\u5e72\u9884\u8bd5\u9a8c\u7684\u5b66\u751f\u7684\u53cd\u9988\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e5d\u5927\u969c\u788d\uff08\u5982\u60c5\u7eea\u56f0\u6270\u3001\u65f6\u95f4\u9650\u5236\u3001\u9690\u79c1\u95ee\u9898\u7b49\uff09\uff0c\u5e76\u6307\u51fa\u9700\u6539\u8fdb\u5e72\u9884\u7b56\u7565\u4ee5\u63d0\u9ad8\u5e74\u8f7b\u6210\u4eba\u7684\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5236\u5b9a\u66f4\u4e2a\u6027\u5316\u548c\u6587\u5316\u654f\u611f\u6027\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u548c\u5438\u5f15\u529b\u3002"}}
{"id": "2507.21158", "pdf": "https://arxiv.org/pdf/2507.21158", "abs": "https://arxiv.org/abs/2507.21158", "authors": ["Nishani Fernando", "Bahareh Nakisa", "Adnan Ahmad", "Mohammad Naim Rastgoo"], "title": "Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams", "categories": ["cs.AI", "cs.HC", "H.1.2; I.2.6; I.2.4"], "comment": "15 pages, 1 figure, Accepted to MAI-XAI@ECAI2025", "summary": "Effective human-AI teaming heavily depends on swift trust, particularly in\nhigh-stakes scenarios such as emergency response, where timely and accurate\ndecision-making is critical. In these time-sensitive and cognitively demanding\nsettings, adaptive explainability is essential for fostering trust between\nhuman operators and AI systems. However, existing explainable AI (XAI)\napproaches typically offer uniform explanations and rely heavily on explicit\nfeedback mechanisms, which are often impractical in such high-pressure\nscenarios. To address this gap, we propose a conceptual framework for adaptive\nXAI that operates non-intrusively by responding to users' real-time cognitive\nand emotional states through implicit feedback, thereby enhancing swift trust\nin high-stakes environments. The proposed adaptive explainability trust\nframework (AXTF) leverages physiological and behavioral signals, such as EEG,\nECG, and eye tracking, to infer user states and support explanation adaptation.\nAt its core is a multi-objective, personalized trust estimation model that maps\nworkload, stress, and emotion to dynamic trust estimates. These estimates guide\nthe modulation of explanation features enabling responsive and personalized\nsupport that promotes swift trust in human-AI collaboration. This conceptual\nframework establishes a foundation for developing adaptive, non-intrusive XAI\nsystems tailored to the rigorous demands of high-pressure, time-sensitive\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u53ef\u89e3\u91caAI\u6846\u67b6\uff08AXTF\uff09\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4b\u7528\u6237\u7684\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\uff08\u5982EEG\u3001ECG\u548c\u773c\u52a8\u8ffd\u8e2a\uff09\uff0c\u63a8\u65ad\u8ba4\u77e5\u4e0e\u60c5\u7eea\u72b6\u6001\uff0c\u52a8\u6001\u8c03\u6574\u89e3\u91ca\u5185\u5bb9\uff0c\u4ee5\u5728\u9ad8\u538b\u7d27\u6025\u573a\u666f\u4e2d\u5feb\u901f\u5efa\u7acb\u4eba\u673a\u4fe1\u4efb\u3002", "motivation": "\u5728\u7d27\u6025\u54cd\u5e94\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u51b3\u7b56\u4f9d\u8d56\u4e8e\u4eba\u673a\u5feb\u901f\u4fe1\u4efb\uff0c\u4f46\u73b0\u6709XAI\u65b9\u6cd5\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u975e\u4fb5\u5165\u6027\u53cd\u9988\u673a\u5236\u3002", "method": "\u63d0\u51faAXTF\u6846\u67b6\uff0c\u5229\u7528\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\u63a8\u65ad\u7528\u6237\u72b6\u6001\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4e2a\u6027\u5316\u4fe1\u4efb\u4f30\u8ba1\u6a21\u578b\u52a8\u6001\u8c03\u6574\u89e3\u91ca\u5185\u5bb9\u3002", "result": "AXTF\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u53cd\u9988\u673a\u5236\u652f\u6301\u81ea\u9002\u5e94\u89e3\u91ca\uff0c\u4e3a\u9ad8\u538b\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "AXTF\u4e3a\u5f00\u53d1\u9002\u5e94\u9ad8\u538b\u73af\u5883\u7684\u975e\u4fb5\u5165\u6027XAI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u4fc3\u8fdb\u4eba\u673a\u5feb\u901f\u4fe1\u4efb\u3002"}}
{"id": "2507.21360", "pdf": "https://arxiv.org/pdf/2507.21360", "abs": "https://arxiv.org/abs/2507.21360", "authors": ["Nicholas Botti", "Flora Haberkorn", "Charlotte Hoopes", "Shaun Khan"], "title": "Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures", "categories": ["cs.AI", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "We utilize a within-subjects design with randomized task assignments to\nunderstand the effectiveness of using an AI retrieval augmented generation\n(RAG) tool to assist analysts with an information extraction and data\nannotation task. We replicate an existing, challenging real-world annotation\ntask with complex multi-part criteria on a set of thousands of pages of public\ndisclosure documents from global systemically important banks (GSIBs) with\nheterogeneous and incomplete information content. We test two treatment\nconditions. First, a \"naive\" AI use condition in which annotators use only the\ntool and must accept the first answer they are given. And second, an\n\"interactive\" AI treatment condition where annotators use the tool\ninteractively, and use their judgement to follow-up with additional information\nif necessary. Compared to the human-only baseline, the use of the AI tool\naccelerated task execution by up to a factor of 10 and enhanced task accuracy,\nparticularly in the interactive condition. We find that when extrapolated to\nthe full task, these methods could save up to 268 hours compared to the\nhuman-only approach. Additionally, our findings suggest that annotator skill,\nnot just with the subject matter domain, but also with AI tools, is a factor in\nboth the accuracy and speed of task performance.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u968f\u673a\u4efb\u52a1\u5206\u914d\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u4e86AI\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5de5\u5177\u5728\u4fe1\u606f\u63d0\u53d6\u548c\u6570\u636e\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u4ea4\u4e92\u5f0f\u4f7f\u7528AI\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u65e8\u5728\u9a8c\u8bc1AI\u5de5\u5177\u5728\u590d\u6742\u3001\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u6807\u6ce8\u4efb\u52a1\u4e2d\u662f\u5426\u80fd\u63d0\u5347\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u968f\u673a\u4efb\u52a1\u5206\u914d\u7684\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4e86\u201c\u7b80\u5355\u201dAI\u4f7f\u7528\u548c\u201c\u4ea4\u4e92\u5f0f\u201dAI\u4f7f\u7528\u4e24\u79cd\u6761\u4ef6\u4e0e\u7eaf\u4eba\u5de5\u57fa\u7ebf\u3002", "result": "AI\u5de5\u5177\u5c06\u4efb\u52a1\u6267\u884c\u901f\u5ea6\u63d0\u534710\u500d\uff0c\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4ea4\u4e92\u5f0f\u6761\u4ef6\u4e0b\u6548\u679c\u66f4\u4f73\uff1b\u9884\u8ba1\u53ef\u8282\u7701268\u5c0f\u65f6\u5de5\u4f5c\u91cf\u3002", "conclusion": "AI\u5de5\u5177\u80fd\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f46\u4f7f\u7528\u8005\u7684\u6280\u80fd\uff08\u5305\u62ec\u5bf9AI\u5de5\u5177\u7684\u719f\u6089\u5ea6\uff09\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2507.21431", "pdf": "https://arxiv.org/pdf/2507.21431", "abs": "https://arxiv.org/abs/2507.21431", "authors": ["Victor Liu", "Timothy Du", "Jordy Sehn", "Jack Collier", "Fran\u00e7ois Grondin"], "title": "Sound Source Localization for Human-Robot Interaction in Outdoor Environments", "categories": ["cs.RO", "cs.HC", "eess.AS"], "comment": null, "summary": "This paper presents a sound source localization strategy that relies on a\nmicrophone array embedded in an unmanned ground vehicle and an asynchronous\nclose-talking microphone near the operator. A signal coarse alignment strategy\nis combined with a time-domain acoustic echo cancellation algorithm to estimate\na time-frequency ideal ratio mask to isolate the target speech from\ninterferences and environmental noise. This allows selective sound source\nlocalization, and provides the robot with the direction of arrival of sound\nfrom the active operator, which enables rich interaction in noisy scenarios.\nResults demonstrate an average angle error of 4 degrees and an accuracy within\n5 degrees of 95\\% at a signal-to-noise ratio of 1dB, which is significantly\nsuperior to the state-of-the-art localization methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ea6\u514b\u98ce\u9635\u5217\u7684\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u53f7\u7c97\u5bf9\u9f50\u548c\u65f6\u57df\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u566a\u58f0\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u566a\u58f0\u73af\u5883\u4e2d\u51c6\u786e\u8bc6\u522b\u64cd\u4f5c\u5458\u58f0\u6e90\u65b9\u5411\u7684\u95ee\u9898\uff0c\u4ee5\u4fbf\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u9ea6\u514b\u98ce\u9635\u5217\u548c\u5f02\u6b65\u8fd1\u8ddd\u79bb\u9ea6\u514b\u98ce\uff0c\u7ed3\u5408\u4fe1\u53f7\u7c97\u5bf9\u9f50\u548c\u65f6\u57df\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u7b97\u6cd5\uff0c\u4f30\u7b97\u7406\u60f3\u6bd4\u7387\u63a9\u7801\u4ee5\u9694\u79bb\u76ee\u6807\u8bed\u97f3\u3002", "result": "\u5728\u4fe1\u566a\u6bd4\u4e3a1dB\u65f6\uff0c\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee\u4e3a4\u5ea6\uff0c95%\u7684\u60c5\u51b5\u4e0b\u8bef\u5dee\u57285\u5ea6\u4ee5\u5185\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u58f0\u6e90\u5b9a\u4f4d\uff0c\u4e3a\u673a\u5668\u4eba\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u5411\u4fe1\u606f\u3002"}}
{"id": "2507.21571", "pdf": "https://arxiv.org/pdf/2507.21571", "abs": "https://arxiv.org/abs/2507.21571", "authors": ["Laura Spillner", "Nima Zargham", "Mihai Pomarlan", "Robert Porzel", "Rainer Malaka"], "title": "Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations", "categories": ["cs.AI", "cs.HC"], "comment": "Presented at the IJCAI 2023 Workshop on Explainable Artificial\n  Intelligence (XAI)", "summary": "The need for explanations in AI has, by and large, been driven by the desire\nto increase the transparency of black-box machine learning models. However,\nsuch explanations, which focus on the internal mechanisms that lead to a\nspecific output, are often unsuitable for non-experts. To facilitate a\nhuman-centered perspective on AI explanations, agents need to focus on\nindividuals and their preferences as well as the context in which the\nexplanations are given. This paper proposes a personalized approach to\nexplanation, where the agent tailors the information provided to the user based\non what is most likely pertinent to them. We propose a model of the agent's\nworldview that also serves as a personal and dynamic memory of its previous\ninteractions with the same user, based on which the artificial agent can\nestimate what part of its knowledge is most likely new information to the user.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u7684AI\u89e3\u91ca\u65b9\u6cd5\uff0c\u6839\u636e\u7528\u6237\u7684\u504f\u597d\u548c\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u89e3\u91ca\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u8fc7\u4e8e\u6280\u672f\u5316\uff0c\u4e0d\u9002\u5408\u975e\u4e13\u5bb6\u7528\u6237\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u4eba\u6027\u5316\u7684\u89e3\u91ca\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4ee3\u7406\u7684\u4e16\u754c\u89c2\u6a21\u578b\uff0c\u52a8\u6001\u8bb0\u5fc6\u7528\u6237\u4ea4\u4e92\u5386\u53f2\uff0c\u4ece\u800c\u4f30\u8ba1\u7528\u6237\u6700\u9700\u8981\u7684\u65b0\u4fe1\u606f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u6839\u636e\u7528\u6237\u9700\u6c42\u63d0\u4f9b\u4e2a\u6027\u5316\u89e3\u91ca\u7684AI\u4ee3\u7406\u6a21\u578b\u3002", "conclusion": "\u4e2a\u6027\u5316\u89e3\u91ca\u65b9\u6cd5\u6539\u5584\u4e86AI\u900f\u660e\u6027\uff0c\u66f4\u9002\u5408\u975e\u4e13\u5bb6\u7528\u6237\u3002"}}
{"id": "2507.21664", "pdf": "https://arxiv.org/pdf/2507.21664", "abs": "https://arxiv.org/abs/2507.21664", "authors": ["Mariam Alsayyad", "Fayadh Kadhem"], "title": "Can the current trends of AI handle a full course of mathematics?", "categories": ["cs.AI", "cs.HC", "math.HO"], "comment": "36 pages", "summary": "This paper addresses the question of how able the current trends of\nArtificial Intelligence (AI) are in managing to take the responsibility of a\nfull course of mathematics at a college level. The study evaluates this ability\nin four significant aspects, namely, creating a course syllabus, presenting\nselected material, answering student questions, and creating an assessment. It\nshows that even though the AI is strong in some important parts like\norganization and accuracy, there are still some human aspects that are far away\nfrom the current abilities of AI. There is still a hidden emotional part, even\nin science, that cannot be fulfilled by the AI in its current state. This paper\nsuggests some recommendations to integrate the human and AI potentials to\ncreate better outcomes in terms of reaching the target of creating a full\ncourse of mathematics, at a university level, as best as possible.", "AI": {"tldr": "\u63a2\u8ba8AI\u5728\u7ba1\u7406\u5927\u5b66\u6570\u5b66\u5168\u7a0b\u8bfe\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u5206\u6790\u4e86\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u53d1\u73b0AI\u5728\u7ec4\u7ec7\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u60c5\u611f\u548c\u4eba\u6587\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76AI\u662f\u5426\u80fd\u591f\u627f\u62c5\u5927\u5b66\u6570\u5b66\u5168\u7a0b\u8bfe\u7a0b\u7684\u8d23\u4efb\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u8bc4\u4f30AI\u5728\u5236\u5b9a\u8bfe\u7a0b\u5927\u7eb2\u3001\u5c55\u793a\u8bfe\u7a0b\u5185\u5bb9\u3001\u89e3\u7b54\u5b66\u751f\u95ee\u9898\u548c\u8bbe\u8ba1\u8bc4\u4f30\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "AI\u5728\u7ec4\u7ec7\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u60c5\u611f\u548c\u4eba\u6587\u65b9\u9762\u4ecd\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u3002", "conclusion": "\u5efa\u8bae\u7ed3\u5408\u4eba\u7c7b\u4e0eAI\u7684\u4f18\u52bf\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u5927\u5b66\u6570\u5b66\u8bfe\u7a0b\u8bbe\u8ba1\u6548\u679c\u3002"}}
{"id": "2507.21859", "pdf": "https://arxiv.org/pdf/2507.21859", "abs": "https://arxiv.org/abs/2507.21859", "authors": ["Michael Kaiser", "Clemens Gro\u00df", "Lisa Marie Otto", "Steffen M\u00fcller"], "title": "Evaluating Interactions between Automated Vehicles and Cyclists using a coupled In-the-Loop Test Environment", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Testing and evaluating automated driving systems (ADS) in interactions with\nvulnerable road users (VRUs), such as cyclists, are essential for improving the\nsafety of VRUs, but often lack realism. This paper presents and validates a\ncoupled in-the-loop test environment that integrates a Cyclist-in-the Loop test\nbench with a Vehicle-in-the-Loop test bench via a virtual environment (VE)\ndeveloped in Unreal Engine 5. The setup enables closed-loop, bidirectional\ninteraction between a real human cyclist and a real automated vehicle under\nsafe and controllable conditions. The automated vehicle reacts to cyclist\ngestures via stimulated camera input, while the cyclist, riding a stationary\nbicycle, perceives and reacts to the vehicle in the VE in real time. Validation\nexperiments are conducted using a real automated shuttle bus with a\ntrack-and-follow function, performing three test maneuvers - straight-line\ndriving with stop, circular track driving, and double lane change - on a\nproving ground and in the coupled in-the-loop test environment. The performance\nis evaluated by comparing the resulting vehicle trajectories in both\nenvironments. Additionally, the introduced latencies of individual components\nin the test setup are measured. The results demonstrate the feasibility of the\napproach and highlight its strengths and limitations for realistic ADS\nevaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u901a\u8fc7\u9a8c\u8bc1\u4e86\u4e00\u79cd\u8026\u5408\u7684\u95ed\u73af\u6d4b\u8bd5\u73af\u5883\uff0c\u5c06\u81ea\u884c\u8f66\u624b\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u901a\u8fc7\u865a\u62df\u73af\u5883\uff08Unreal Engine 5\uff09\u5b9e\u65f6\u4ea4\u4e92\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u6027\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08\u5982\u9a91\u884c\u8005\uff09\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u6d4b\u8bd5\u7684\u771f\u5b9e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8026\u5408\u7684\u95ed\u73af\u6d4b\u8bd5\u73af\u5883\uff0c\u7ed3\u5408\u81ea\u884c\u8f66\u624b\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b9e\u65f6\u53cc\u5411\u4ea4\u4e92\uff0c\u5e76\u5229\u7528\u865a\u62df\u73af\u5883\uff08Unreal Engine 5\uff09\u5b9e\u73b0\u3002", "result": "\u9a8c\u8bc1\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u6d4b\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u79cd\u8026\u5408\u6d4b\u8bd5\u73af\u5883\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u771f\u5b9e\u4e14\u53ef\u63a7\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
