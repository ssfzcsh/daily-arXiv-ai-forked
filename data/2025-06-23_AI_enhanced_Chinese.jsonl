{"id": "2506.15884", "pdf": "https://arxiv.org/pdf/2506.15884", "abs": "https://arxiv.org/abs/2506.15884", "authors": ["Shamse Tasnim Cynthia", "Nuri Almarimi", "Banani Roy"], "title": "How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?", "categories": ["cs.SE"], "comment": null, "summary": "Community smells reflect poor organizational practices that often lead to\nsocio-technical issues and the accumulation of Self-Admitted Technical Debt\n(SATD). While prior studies have explored these problems in general software\nsystems, their interplay in machine learning (ML)-based projects remains\nlargely underexamined. In this study, we investigated the prevalence of\ncommunity smells and their relationship with SATD in open-source ML projects,\nanalyzing data at the release level. First, we examined the prevalence of ten\ncommunity smell types across the releases of 155 ML-based systems and found\nthat community smells are widespread, exhibiting distinct distribution patterns\nacross small, medium, and large projects. Second, we detected SATD at the\nrelease level and applied statistical analysis to examine its correlation with\ncommunity smells. Our results showed that certain smells, such as Radio Silence\nand Organizational Silos, are strongly correlated with higher SATD occurrences.\nThird, we considered the six identified types of SATD to determine which\ncommunity smells are most associated with each debt category. Our analysis\nrevealed authority- and communication-related smells often co-occur with\npersistent code and design debt. Finally, we analyzed how the community smells\nand SATD evolve over the releases, uncovering project size-dependent trends and\nshared trajectories. Our findings emphasize the importance of early detection\nand mitigation of socio-technical issues to maintain the long-term quality and\nsustainability of ML-based systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5f00\u6e90ML\u9879\u76ee\u4e2d\u793e\u533a\u6c14\u5473\u4e0e\u81ea\u627f\u6280\u672f\u503a(SATD)\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u67d0\u4e9b\u6c14\u5473(\u5982Radio Silence)\u4e0eSATD\u9ad8\u5ea6\u76f8\u5173\uff0c\u5f3a\u8c03\u4e86\u65e9\u671f\u68c0\u6d4b\u548c\u7f13\u89e3\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u9879\u76ee\u4e2d\u793e\u533a\u6c14\u5473\u4e0eSATD\u7684\u5173\u7cfb\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86155\u4e2aML\u9879\u76ee\u7684\u53d1\u5e03\u6570\u636e\uff0c\u68c0\u6d4b\u4e8610\u79cd\u793e\u533a\u6c14\u5473\u548cSATD\uff0c\u5e76\u8fdb\u884c\u4e86\u7edf\u8ba1\u5206\u6790\u548c\u6f14\u5316\u8d8b\u52bf\u7814\u7a76\u3002", "result": "\u67d0\u4e9b\u793e\u533a\u6c14\u5473(\u5982Radio Silence)\u4e0eSATD\u663e\u8457\u76f8\u5173\uff0c\u4e14\u6c14\u5473\u4e0eSATD\u7684\u6f14\u5316\u8d8b\u52bf\u4e0e\u9879\u76ee\u89c4\u6a21\u76f8\u5173\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u65e9\u671f\u68c0\u6d4b\u548c\u7f13\u89e3\u793e\u533a\u6c14\u5473\u5bf9ML\u9879\u76ee\u957f\u671f\u8d28\u91cf\u548c\u53ef\u6301\u7eed\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.16101", "pdf": "https://arxiv.org/pdf/2506.16101", "abs": "https://arxiv.org/abs/2506.16101", "authors": ["Yupeng Jiang", "Shuaiyi Sun", "Xi Zheng"], "title": "Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques", "categories": ["cs.SE"], "comment": null, "summary": "Regression testing plays a critical role in maintaining software reliability,\nparticularly for ROS-based autonomous systems (ROSAS), which frequently undergo\ncontinuous integration and iterative development. However, conventional\nregression testing techniques face significant challenges when applied to\nautonomous systems due to their dynamic and non-deterministic behaviors,\ncomplex multi-modal sensor data, asynchronous distributed architectures, and\nstringent safety and real-time constraints. Although numerous studies have\nexplored test optimization in traditional software contexts, regression testing\noptimization specifically for ROSAS remains largely unexplored. To address this\ngap, we present the first comprehensive survey systematically reviewing\nregression testing optimization techniques tailored for ROSAS. We analyze and\ncategorize 122 representative studies into regression test case prioritization,\nminimization, and selection methods. A structured taxonomy is introduced to\nclearly illustrate their applicability and limitations within ROSAS contexts.\nFurthermore, we highlight major challenges specific to regression testing for\nROSAS, including effectively prioritizing tests in response to frequent system\nmodifications, efficiently minimizing redundant tests, and difficulty in\naccurately selecting impacted test cases. Finally, we propose research insights\nand identify promising future directions, such as leveraging frame-to-vector\ncoverage metrics, multi-source foundation models, and neurosymbolic reasoning\nto enhance regression testing efficiency and effectiveness. This survey\nprovides a foundational reference and practical roadmap for advancing the\nstate-of-the-art in regression testing optimization for ROSAS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u57fa\u4e8eROS\u7684\u81ea\u4e3b\u7cfb\u7edf\uff08ROSAS\uff09\u7684\u56de\u5f52\u6d4b\u8bd5\u4f18\u5316\u6280\u672f\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u8c03\u67e5\uff0c\u5206\u6790\u4e86122\u9879\u4ee3\u8868\u6027\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u548c\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u7531\u4e8eROSAS\u7684\u52a8\u6001\u6027\u3001\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u3001\u590d\u6742\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u4ee5\u53ca\u4e25\u683c\u7684\u5b89\u5168\u548c\u5b9e\u65f6\u7ea6\u675f\uff0c\u4f20\u7edf\u56de\u5f52\u6d4b\u8bd5\u6280\u672f\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9ROSAS\u7684\u56de\u5f52\u6d4b\u8bd5\u4f18\u5316\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5bf9122\u9879\u4ee3\u8868\u6027\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u5c06\u5176\u5206\u4e3a\u56de\u5f52\u6d4b\u8bd5\u7528\u4f8b\u4f18\u5148\u7ea7\u6392\u5e8f\u3001\u6700\u5c0f\u5316\u548c\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86ROSAS\u56de\u5f52\u6d4b\u8bd5\u7684\u4e3b\u8981\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u6d4b\u8bd5\u4f18\u5148\u7ea7\u6392\u5e8f\u3001\u5197\u4f59\u6d4b\u8bd5\u6700\u5c0f\u5316\u548c\u53d7\u5f71\u54cd\u7684\u6d4b\u8bd5\u7528\u4f8b\u9009\u62e9\u7b49\u3002", "conclusion": "\u8be5\u8c03\u67e5\u4e3aROSAS\u56de\u5f52\u6d4b\u8bd5\u4f18\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\u548c\u5b9e\u7528\u8def\u7ebf\u56fe\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.16136", "pdf": "https://arxiv.org/pdf/2506.16136", "abs": "https://arxiv.org/abs/2506.16136", "authors": ["Kai Huang", "Jian Zhang", "Xiaofei Xie", "Chunyang Chen"], "title": "Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing", "categories": ["cs.SE"], "comment": null, "summary": "Large language model-(LLM) based automated program repair (APR) techniques\nhave shown promising results in resolving real-world GitHub issue tasks.\nExisting APR systems are primarily evaluated in unimodal settings (e.g.,\nSWE-bench). However, these autonomous systems struggle to resolve multimodal\nproblem scenarios (e.g., SWE-bench M) due to limitations in interpreting and\nleveraging visual information. In multimodal scenarios, LLMs need to rely on\nvisual information in the graphical user interface (GUI) to understand bugs and\ngenerate fixes. To bridge this gap, we propose GUIRepair, a cross-modal\nreasoning approach for resolving multimodal issue scenarios by understanding\nand capturing visual information. Specifically, GUIRepair integrates two key\ncomponents, Image2Code and Code2Image, to enhance fault comprehension and patch\nvalidation. Image2Code extracts relevant project documents based on the issue\nreport, then applies this domain knowledge to generate the reproduced code\nresponsible for the visual symptoms, effectively translating GUI images into\nexecutable context for better fault comprehension. Code2Image replays the\nvisual issue scenario using the reproduced code and captures GUI renderings of\nthe patched program to assess whether the fix visually resolves the issue,\nproviding feedback for patch validation. We evaluate GUIRepair on SWE-bench M,\nand the approach demonstrates significant effectiveness. When utilizing GPT-4o\nas the base model, GUIRepair solves 157 instances, outperforming the best\nopen-source baseline by 26 instances. Furthermore, when using o4-mini as the\nbase model, GUIRepair can achieve even better results and solve 175 instances,\noutperforming the top commercial system by 22 instances. This emphasizes the\nsuccess of our new perspective on incorporating cross-modal reasoning by\nunderstanding and capturing visual information to resolve multimodal issues.", "AI": {"tldr": "GUIRepair\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u89e3\u548c\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u89e3\u51b3\u591a\u6a21\u6001\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709APR\u7cfb\u7edf\u5728\u5355\u6a21\u6001\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u591a\u6a21\u6001\u573a\u666f\uff08\u5982\u5305\u542bGUI\u7684\u95ee\u9898\uff09\u3002GUIRepair\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408Image2Code\u548cCode2Image\uff0c\u524d\u8005\u5c06GUI\u56fe\u50cf\u8f6c\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u4ee5\u7406\u89e3\u95ee\u9898\uff0c\u540e\u8005\u901a\u8fc7GUI\u6e32\u67d3\u9a8c\u8bc1\u4fee\u590d\u6548\u679c\u3002", "result": "GUIRepair\u5728SWE-bench M\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f7f\u7528GPT-4o\u89e3\u51b3157\u4e2a\u95ee\u9898\uff0c\u4f18\u4e8e\u6700\u4f73\u5f00\u6e90\u57fa\u7ebf26\u4e2a\uff1bo4-mini\u4e0b\u89e3\u51b3175\u4e2a\uff0c\u8d85\u8d8a\u5546\u4e1a\u7cfb\u7edf22\u4e2a\u3002", "conclusion": "\u8de8\u6a21\u6001\u63a8\u7406\u7684\u6210\u529f\u8bc1\u660e\u89c6\u89c9\u4fe1\u606f\u5bf9\u89e3\u51b3\u591a\u6a21\u6001\u95ee\u9898\u7684\u5173\u952e\u4f5c\u7528\uff0cGUIRepair\u4e3aAPR\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.16214", "pdf": "https://arxiv.org/pdf/2506.16214", "abs": "https://arxiv.org/abs/2506.16214", "authors": ["Klara Borowa", "Andrzej Ratkowski", "Roberto Verdecchia"], "title": "The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture", "categories": ["cs.SE"], "comment": "Preprint accepted to Journal of Systems and Software", "summary": "Microservice architectures provide an intuitive promise of high\nmaintainability and evolvability due to loose coupling. However, these quality\nattributes are notably vulnerable to technical debt (TD). Few studies address\nTD in microservice systems, particularly on a large scale. This research\nexplores how TD manifests in a large-scale microservice-based industrial\nsystem. The research is based on a mixed-method case study of a project\nincluding over 100 microservices and serving over 15k locations. Results are\ncollected via a quantitative method based static code analyzers combined with\nqualitative insights derived from a focus group discussion with the development\nteam and a follow-up interview with the lead architect of the case study\nsystem. Results show that (1) simple static source code analysis can be an\nefficient and effective entry point for holistic TD discovery, (2) inadequate\ncommunication significantly contributes to TD, (3) misalignment between\narchitectural and organizational structures can exacerbate TD accumulation, (4)\nmicroservices can rapidly cycle through TD accumulation and resolution, a\nphenomenon referred to as \"microservice architecture technical debt gamble\".\nFinally, we identify a set of fitting strategies for TD management in\nmicroservice architectures.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u6280\u672f\u503a\u52a1\uff08TD\uff09\uff0c\u53d1\u73b0\u9759\u6001\u4ee3\u7801\u5206\u6790\u662f\u53d1\u73b0TD\u7684\u6709\u6548\u5207\u5165\u70b9\uff0c\u6c9f\u901a\u4e0d\u8db3\u548c\u7ec4\u7ec7\u67b6\u6784\u4e0d\u5339\u914d\u4f1a\u52a0\u5267TD\u3002", "motivation": "\u63a2\u7d22\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u6280\u672f\u503a\u52a1\u7684\u8868\u73b0\u5f62\u5f0f\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u6848\u4f8b\u7814\u7a76\uff0c\u7ed3\u5408\u9759\u6001\u4ee3\u7801\u5206\u6790\u5de5\u5177\u548c\u5f00\u53d1\u56e2\u961f\u7684\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u3002", "result": "\u53d1\u73b0\u4e86\u6280\u672f\u503a\u52a1\u7684\u56db\u79cd\u8868\u73b0\u5f62\u5f0f\uff0c\u5e76\u63d0\u51fa\u7ba1\u7406\u7b56\u7565\u3002", "conclusion": "\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u6280\u672f\u503a\u52a1\u9700\u901a\u8fc7\u7efc\u5408\u7b56\u7565\u7ba1\u7406\uff0c\u5305\u62ec\u6539\u8fdb\u6c9f\u901a\u548c\u7ec4\u7ec7\u67b6\u6784\u5bf9\u9f50\u3002"}}
{"id": "2506.16046", "pdf": "https://arxiv.org/pdf/2506.16046", "abs": "https://arxiv.org/abs/2506.16046", "authors": ["Alborz Jelvani", "Richard P Martin", "Santosh Nagarakatte"], "title": "How to Increase Energy Efficiency with a Single Linux Command", "categories": ["cs.PF", "cs.AR"], "comment": "8 pages", "summary": "Processors with dynamic power management provide a variety of settings to\ncontrol energy efficiency. However, tuning these settings does not achieve\noptimal energy savings. We highlight how existing power capping mechanisms can\naddress these limitations without requiring any changes to current power\ngovernors. We validate this approach using system measurements across a\nmonth-long data acquisition campaign from SPEC CPU 2017 benchmarks on a\nserver-class system equipped with dual Intel Xeon Scalable processors. Our\nresults indicate that setting a simple power cap can improve energy efficiency\nby up to 25% over traditional energy-saving system configurations with little\nperformance loss, as most default settings focus on thermal regulation and\nperformance rather than compute efficiency. Power capping is very accessible\ncompared to other approaches, as it can be implemented with a single Linux\ncommand. Our results point to programmers and administrators using power caps\nas a primary mechanism to maintain significant energy efficiency while\nretaining acceptable performance, as opposed to deploying complex DVFS\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u7b80\u5355\u7684\u7535\u6e90\u9650\u5236\uff08power capping\uff09\u6765\u63d0\u9ad8\u5904\u7406\u5668\u80fd\u6e90\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u53ef\u8fbe25%\u7684\u80fd\u6548\u63d0\u5347\u4e14\u6027\u80fd\u635f\u5931\u5c0f\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u7535\u6e90\u7ba1\u7406\u8bbe\u7f6e\u65e0\u6cd5\u5b9e\u73b0\u6700\u4f18\u80fd\u6548\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7535\u6e90\u9650\u5236\u673a\u5236\u5728\u670d\u52a1\u5668\u7cfb\u7edf\u4e0a\u7684\u6709\u6548\u6027\uff0c\u6d4b\u8bd5\u57fa\u4e8eSPEC CPU 2017\u57fa\u51c6\u3002", "result": "\u7535\u6e90\u9650\u5236\u53ef\u5b9e\u73b025%\u7684\u80fd\u6548\u63d0\u5347\uff0c\u6027\u80fd\u635f\u5931\u8f7b\u5fae\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u7535\u6e90\u9650\u5236\u4f5c\u4e3a\u4e3b\u8981\u80fd\u6548\u63d0\u5347\u624b\u6bb5\uff0c\u56e0\u5176\u7b80\u5355\u4e14\u9ad8\u6548\u3002"}}
{"id": "2506.15875", "pdf": "https://arxiv.org/pdf/2506.15875", "abs": "https://arxiv.org/abs/2506.15875", "authors": ["Dirk Van Essendelft", "Patrick Wingo", "Terry Jordan", "Ryan Smith", "Wissam Saidi"], "title": "A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures", "categories": ["cs.PL", "cs.AR", "cs.DC", "cs.ET", "D.3; D.1; I.6; J.2"], "comment": "26 pages, 5 figures, 14 listings", "summary": "We have developed a novel compiler called the Multiple-Architecture Compiler\nfor Advanced Computing Hardware (MACH) designed specifically for\nmassively-parallel, spatial, dataflow architectures like the Wafer Scale\nEngine. Additionally, MACH can execute code on traditional unified-memory\ndevices. MACH addresses the complexities in compiling for spatial architectures\nthrough a conceptual Virtual Machine, a flexible domain-specific language, and\na compiler that can lower high-level languages to machine-specific code in\ncompliance with the Virtual Machine concept. While MACH is designed to be\noperable on several architectures and provide the flexibility for several\nstandard and user-defined data mappings, we introduce the concept with dense\ntensor examples from NumPy and show lowering to the Wafer Scale Engine by\ntargeting Cerebras' hardware specific languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMACH\u7684\u65b0\u578b\u7f16\u8bd1\u5668\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u5e76\u884c\u3001\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u8bbe\u8ba1\uff0c\u540c\u65f6\u652f\u6301\u4f20\u7edf\u7edf\u4e00\u5185\u5b58\u8bbe\u5907\uff0c\u89e3\u51b3\u4e86\u7a7a\u95f4\u67b6\u6784\u7f16\u8bd1\u7684\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u4e3a\u7a7a\u95f4\u67b6\u6784\uff08\u5982Wafer Scale Engine\uff09\u7f16\u8bd1\u4ee3\u7801\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u652f\u6301\u591a\u67b6\u6784\u548c\u7075\u6d3b\u6570\u636e\u6620\u5c04\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u6027\u865a\u62df\u673a\u3001\u7075\u6d3b\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u7f16\u8bd1\u5668\uff0c\u5c06\u9ad8\u7ea7\u8bed\u8a00\u964d\u4f4e\u4e3a\u7b26\u5408\u865a\u62df\u673a\u6982\u5ff5\u7684\u673a\u5668\u7279\u5b9a\u4ee3\u7801\u3002", "result": "\u5c55\u793a\u4e86MACH\u5728NumPy\u5bc6\u96c6\u5f20\u91cf\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u76ee\u6807\u7279\u5b9a\u7684\u786c\u4ef6\u8bed\u8a00\uff08Cerebras\uff09\u5b9e\u73b0\u4e86Wafer Scale Engine\u7684\u4ee3\u7801\u751f\u6210\u3002", "conclusion": "MACH\u4e3a\u7a7a\u95f4\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7f16\u8bd1\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5177\u5907\u591a\u67b6\u6784\u652f\u6301\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.15947", "pdf": "https://arxiv.org/pdf/2506.15947", "abs": "https://arxiv.org/abs/2506.15947", "authors": ["Jinbo Wen", "Cheng Su", "Jiawen Kang", "Jiangtian Nie", "Yang Zhang", "Jianhang Tang", "Dusit Niyato", "Chau Yuen"], "title": "HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm\nto support various low-altitude services through integrated air-ground\ninfrastructure. To satisfy low-latency and high-computation demands, the\nintegration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC)\nsystems plays a vital role, which offloads computing tasks from terminal\ndevices to nearby UAVs, enabling flexible and resilient service provisions for\nground users. To promote the development of LAENets, it is significant to\nachieve low-carbon multi-UAV-assisted MEC networks. However, several challenges\nhinder this implementation, including the complexity of multi-dimensional UAV\nmodeling and the difficulty of multi-objective coupled optimization. To this\nend, this paper proposes a novel Retrieval Augmented Generation (RAG)-based\nLarge Language Model (LLM) agent framework for model formulation. Specifically,\nwe develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG,\nempowering LLM agents to efficiently retrieve structural information from\nexpert databases and generate more accurate optimization problems compared with\ntraditional RAG-based LLM agents. After customizing carbon emission\noptimization problems for multi-UAV-assisted MEC networks, we propose a Double\nRegularization Diffusion-enhanced Soft Actor-Critic (R\\textsuperscript{2}DSAC)\nalgorithm to solve the formulated multi-objective optimization problem. The\nR\\textsuperscript{2}DSAC algorithm incorporates diffusion entropy\nregularization and action entropy regularization to improve the performance of\nthe diffusion policy. Furthermore, we dynamically mask unimportant neurons in\nthe actor network to reduce the carbon emissions associated with model\ntraining. Simulation results demonstrate the effectiveness and reliability of\nthe proposed HybridRAG-based LLM agent framework and the\nR\\textsuperscript{2}DSAC algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6846\u67b6\uff08HybridRAG\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u8fb9\u7f18\u8ba1\u7b97\u7f51\u7edc\u4e2d\u7684\u78b3\u51cf\u6392\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u91cd\u6b63\u5219\u5316\u6269\u6563\u589e\u5f3a\u7684\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08R\u00b2DSAC\uff09\u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\uff08LAENets\uff09\u9700\u8981\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\uff0c\u591a\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7f51\u7edc\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u91cd\u8981\u9014\u5f84\u3002\u7136\u800c\uff0c\u591a\u7ef4\u65e0\u4eba\u673a\u5efa\u6a21\u7684\u590d\u6742\u6027\u548c\u591a\u76ee\u6807\u8026\u5408\u4f18\u5316\u7684\u96be\u5ea6\u963b\u788d\u4e86\u4f4e\u78b3\u591a\u65e0\u4eba\u673a\u8f85\u52a9MEC\u7f51\u7edc\u7684\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408KeywordRAG\u3001VectorRAG\u548cGraphRAG\u7684HybridRAG\u6846\u67b6\uff0c\u4ee5\u589e\u5f3aLLM\u4ee3\u7406\u4ece\u4e13\u5bb6\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u80fd\u529b\u3002\u63a5\u7740\uff0c\u8bbe\u8ba1\u4e86R\u00b2DSAC\u7b97\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u71b5\u6b63\u5219\u5316\u548c\u52a8\u4f5c\u71b5\u6b63\u5219\u5316\u4f18\u5316\u6269\u6563\u7b56\u7565\uff0c\u5e76\u52a8\u6001\u5c4f\u853d\u6f14\u5458\u7f51\u7edc\u4e2d\u4e0d\u91cd\u8981\u7684\u795e\u7ecf\u5143\u4ee5\u51cf\u5c11\u78b3\u6392\u653e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHybridRAG\u6846\u67b6\u548cR\u00b2DSAC\u7b97\u6cd5\u5728\u591a\u65e0\u4eba\u673a\u8f85\u52a9\u7684MEC\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u78b3\u6392\u653e\u3002", "conclusion": "HybridRAG\u6846\u67b6\u548cR\u00b2DSAC\u7b97\u6cd5\u4e3a\u89e3\u51b3\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u591a\u65e0\u4eba\u673a\u8f85\u52a9MEC\u7684\u4f4e\u78b3\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2506.16258", "pdf": "https://arxiv.org/pdf/2506.16258", "abs": "https://arxiv.org/abs/2506.16258", "authors": ["Yisu Wang", "Yixiang Zhu", "Xinjiao Li", "Yulong Zhang", "Ruilong Wu", "Dirk Kutscher"], "title": "ViFusion: In-Network Tensor Fusion for Scalable Video Feature Indexing", "categories": ["cs.MM"], "comment": null, "summary": "Large-scale video feature indexing in datacenters is critically dependent on\nefficient data transfer. Although in-network computation has emerged as a\ncompelling strategy for accelerating feature extraction and reducing overhead\nin distributed multimedia systems, harnessing advanced networking resources at\nboth the switch and host levels remains a formidable challenge. These\ndifficulties are compounded by heterogeneous hardware, diverse application\nrequirements, and complex multipath topologies. Existing methods focus\nprimarily on optimizing inference for large neural network models using\nspecialized collective communication libraries, which often face performance\ndegradation in network congestion scenarios.\n  To overcome these limitations, we present ViFusion, a communication aware\ntensor fusion framework that streamlines distributed video indexing by merging\nnumerous small feature tensors into consolidated and more manageable units. By\nintegrating an in-network computation module and a dedicated tensor fusion\nmechanism within datacenter environments, ViFusion substantially improves the\nefficiency of video feature indexing workflows. The deployment results show\nthat ViFusion improves the throughput of the video retrieval system by 8--22\ntimes with the same level of latency as state-of-the-art systems.", "AI": {"tldr": "ViFusion\u662f\u4e00\u4e2a\u901a\u4fe1\u611f\u77e5\u7684\u5f20\u91cf\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u5e76\u5c0f\u7279\u5f81\u5f20\u91cf\u4e3a\u66f4\u6613\u7ba1\u7406\u7684\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u89c6\u9891\u7279\u5f81\u7d22\u5f15\u6548\u7387\uff0c\u5728\u76f8\u540c\u5ef6\u8fdf\u6c34\u5e73\u4e0b\u541e\u5410\u91cf\u63d0\u53478-22\u500d\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u9891\u7279\u5f81\u7d22\u5f15\u4f9d\u8d56\u4e8e\u9ad8\u6548\u6570\u636e\u4f20\u8f93\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7f51\u7edc\u62e5\u585e\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4f18\u5316\u5206\u5e03\u5f0f\u591a\u5a92\u4f53\u7cfb\u7edf\u7684\u8ba1\u7b97\u548c\u6570\u636e\u4f20\u8f93\u6548\u7387\u3002", "method": "ViFusion\u7ed3\u5408\u7f51\u7edc\u5185\u8ba1\u7b97\u6a21\u5757\u548c\u4e13\u7528\u5f20\u91cf\u878d\u5408\u673a\u5236\uff0c\u5c06\u5c0f\u7279\u5f81\u5f20\u91cf\u5408\u5e76\u4e3a\u66f4\u6613\u7ba1\u7406\u7684\u5355\u5143\uff0c\u4f18\u5316\u5206\u5e03\u5f0f\u89c6\u9891\u7d22\u5f15\u3002", "result": "\u90e8\u7f72\u7ed3\u679c\u8868\u660e\uff0cViFusion\u5728\u76f8\u540c\u5ef6\u8fdf\u6c34\u5e73\u4e0b\u5c06\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u541e\u5410\u91cf\u63d0\u53478-22\u500d\u3002", "conclusion": "ViFusion\u901a\u8fc7\u5f20\u91cf\u878d\u5408\u548c\u7f51\u7edc\u5185\u8ba1\u7b97\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u7279\u5f81\u7d22\u5f15\u4e2d\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2506.15742", "pdf": "https://arxiv.org/pdf/2506.15742", "abs": "https://arxiv.org/abs/2506.15742", "authors": ["Black Forest Labs", "Stephen Batifol", "Andreas Blattmann", "Frederic Boesel", "Saksham Consul", "Cyril Diagne", "Tim Dockhorn", "Jack English", "Zion English", "Patrick Esser", "Sumith Kulal", "Kyle Lacey", "Yam Levi", "Cheng Li", "Dominik Lorenz", "Jonas M\u00fcller", "Dustin Podell", "Robin Rombach", "Harry Saini", "Axel Sauer", "Luke Smith"], "title": "FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space", "categories": ["cs.GR"], "comment": null, "summary": "We present evaluation results for FLUX.1 Kontext, a generative flow matching\nmodel that unifies image generation and editing. The model generates novel\noutput views by incorporating semantic context from text and image inputs.\nUsing a simple sequence concatenation approach, FLUX.1 Kontext handles both\nlocal editing and generative in-context tasks within a single unified\narchitecture. Compared to current editing models that exhibit degradation in\ncharacter consistency and stability across multiple turns, we observe that\nFLUX.1 Kontext improved preservation of objects and characters, leading to\ngreater robustness in iterative workflows.The model achieves competitive\nperformance with current state-of-the-art systems while delivering\nsignificantly faster generation times, enabling interactive applications and\nrapid prototyping workflows. To validate these improvements, we introduce\nKontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering\nfive task categories: local editing, global editing, character reference, style\nreference and text editing. Detailed evaluations show the superior performance\nof FLUX.1 Kontext in terms of both single-turn quality and multi-turn\nconsistency, setting new standards for unified image processing models.", "AI": {"tldr": "FLUX.1 Kontext\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u6d41\u5339\u914d\u6a21\u578b\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u8f93\u5165\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u8fdb\u884c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u3002\u6a21\u578b\u901a\u8fc7\u7b80\u5355\u7684\u5e8f\u5217\u8fde\u63a5\u65b9\u6cd5\u5904\u7406\u5c40\u90e8\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u5bf9\u8c61\u548c\u89d2\u8272\u7684\u4fdd\u6301\u80fd\u529b\uff0c\u540c\u65f6\u5728\u901f\u5ea6\u548c\u4ea4\u4e92\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u7f16\u8f91\u6a21\u578b\u5728\u591a\u8f6e\u64cd\u4f5c\u4e2d\u5b58\u5728\u89d2\u8272\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0cFLUX.1 Kontext\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u67b6\u6784\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u8fde\u63a5\u65b9\u6cd5\u6574\u5408\u6587\u672c\u548c\u56fe\u50cf\u8f93\u5165\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u5c40\u90e8\u7f16\u8f91\u548c\u751f\u6210\u4efb\u52a1\u3002\u6a21\u578b\u8bbe\u8ba1\u4e86KontextBench\u57fa\u51c6\uff081026\u5bf9\u56fe\u50cf-\u63d0\u793a\uff09\uff0c\u8986\u76d6\u4e94\u7c7b\u4efb\u52a1\u4ee5\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "FLUX.1 Kontext\u5728\u591a\u8f6e\u4e00\u81f4\u6027\u548c\u5355\u8f6e\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u751f\u6210\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5e94\u7528\u548c\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002", "conclusion": "FLUX.1 Kontext\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u9ad8\u6548\u6027\u80fd\uff0c\u4e3a\u56fe\u50cf\u5904\u7406\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5c24\u5176\u5728\u591a\u4efb\u52a1\u5904\u7406\u548c\u5de5\u4f5c\u6d41\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.16882", "pdf": "https://arxiv.org/pdf/2506.16882", "abs": "https://arxiv.org/abs/2506.16882", "authors": ["Takahiro Ishikawa-Aso", "Shinpei Kato"], "title": "ROS 2 Agnocast: Supporting Unsized Message Types for True Zero-Copy Publish/Subscribe IPC", "categories": ["cs.OS", "cs.RO", "C.3; D.4.7"], "comment": "10 pages, 13 figures. Accepted for IEEE ISORC 2025; this is the\n  author-accepted manuscript", "summary": "Robot applications, comprising independent components that mutually\npublish/subscribe messages, are built on inter-process communication (IPC)\nmiddleware such as Robot Operating System 2 (ROS 2). In large-scale ROS 2\nsystems like autonomous driving platforms, true zero-copy communication --\neliminating serialization and deserialization -- is crucial for efficiency and\nreal-time performance. However, existing true zero-copy middleware solutions\nlack widespread adoption as they fail to meet three essential requirements: 1)\nSupport for all ROS 2 message types including unsized ones; 2) Minimal\nmodifications to existing application code; 3) Selective implementation of\nzero-copy communication between specific nodes while maintaining conventional\ncommunication mechanisms for other inter-node communications including\ninter-host node communications. This first requirement is critical, as\nproduction-grade ROS 2 projects like Autoware rely heavily on unsized message\ntypes throughout their codebase to handle diverse use cases (e.g., various\nsensors), and depend on the broader ROS 2 ecosystem, where unsized message\ntypes are pervasive in libraries. The remaining requirements facilitate\nseamless integration with existing projects. While IceOryx middleware, a\npractical true zero-copy solution, meets all but the first requirement, other\nstudies achieving the first requirement fail to satisfy the remaining criteria.\nThis paper presents Agnocast, a true zero-copy IPC framework applicable to ROS\n2 C++ on Linux that fulfills all these requirements. Our evaluation\ndemonstrates that Agnocast maintains constant IPC overhead regardless of\nmessage size, even for unsized message types. In Autoware PointCloud\nPreprocessing, Agnocast achieves a 16% improvement in average response time and\na 25% improvement in worst-case response time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Agnocast\uff0c\u4e00\u79cd\u9002\u7528\u4e8eROS 2 C++\u7684\u96f6\u62f7\u8d1dIPC\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6ee1\u8db3\u7684\u4e09\u4e2a\u5173\u952e\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21ROS 2\u7cfb\u7edf\u4e2d\uff0c\u96f6\u62f7\u8d1d\u901a\u4fe1\u5bf9\u6548\u7387\u548c\u5b9e\u65f6\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u672a\u80fd\u5168\u9762\u652f\u6301ROS 2\u6d88\u606f\u7c7b\u578b\u5e76\u4e14\u96be\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u9879\u76ee\u4e2d\u3002", "method": "\u63d0\u51faAgnocast\u6846\u67b6\uff0c\u652f\u6301\u6240\u6709ROS 2\u6d88\u606f\u7c7b\u578b\uff08\u5305\u62ec\u65e0\u56fa\u5b9a\u5927\u5c0f\u7684\u6d88\u606f\uff09\uff0c\u4e14\u5bf9\u73b0\u6709\u4ee3\u7801\u4fee\u6539\u6781\u5c0f\uff0c\u53ef\u9009\u62e9\u6027\u5730\u5b9e\u73b0\u96f6\u62f7\u8d1d\u901a\u4fe1\u3002", "result": "Agnocast\u5b9e\u73b0\u4e86\u4e0e\u6d88\u606f\u5927\u5c0f\u65e0\u5173\u7684\u6052\u5b9aIPC\u5f00\u9500\uff0c\u5728Autoware\u70b9\u4e91\u9884\u5904\u7406\u4e2d\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u63d0\u534716%\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u63d0\u534725%\u3002", "conclusion": "Agnocast\u4e3aROS 2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u7f1d\u96c6\u6210\u7684\u96f6\u62f7\u8d1d\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u6ee1\u8db3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u9700\u6c42\u3002"}}
{"id": "2506.16000", "pdf": "https://arxiv.org/pdf/2506.16000", "abs": "https://arxiv.org/abs/2506.16000", "authors": ["Hemanth Kannamarlapudi", "Sowmya Chintalapudi"], "title": "Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal", "categories": ["cs.ET", "cs.AI", "cs.RO", "quant-ph", "I.2.9; I.2.6; K.4.4"], "comment": "5 pages, 2 figures, 17 references. Architectural proposal for quantum\n  AI integration in autonomous vehicle navigation systems for secured\n  navigation", "summary": "Navigation is a very crucial aspect of autonomous vehicle ecosystem which\nheavily relies on collecting and processing large amounts of data in various\nstates and taking a confident and safe decision to define the next vehicle\nmaneuver. In this paper, we propose a novel architecture based on Quantum\nArtificial Intelligence by enabling quantum and AI at various levels of\nnavigation decision making and communication process in Autonomous vehicles :\nQuantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum\nreinforcement learning for navigation policy optimization and finally\npost-quantum cryptographic protocols for secure communication. Quantum neural\nnetworks uses quantum amplitude encoding to fuse data from various sensors like\nLiDAR, radar, camera, GPS and weather etc., This approach gives a unified\nquantum state representation between heterogeneous sensor modalities. Nav-Q\nmodule processes the fused quantum states through variational quantum circuits\nto learn optimal navigation policies under swift dynamic and complex\nconditions. Finally, post quantum cryptographic protocols are used to secure\ncommunication channels for both within vehicle communication and V2X (Vehicle\nto Everything) communications and thus secures the autonomous vehicle\ncommunication from both classical and quantum security threats. Thus, the\nproposed framework addresses fundamental challenges in autonomous vehicles\nnavigation by providing quantum performance and future proof security. Index\nTerms Quantum Computing, Autonomous Vehicles, Sensor Fusion", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u4eba\u5de5\u667a\u80fd\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5bfc\u822a\u51b3\u7b56\u548c\u901a\u4fe1\uff0c\u5305\u62ec\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u3001Nav-Q\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5bfc\u822a\u7b56\u7565\uff0c\u4ee5\u53ca\u540e\u91cf\u5b50\u5bc6\u7801\u534f\u8bae\u786e\u4fdd\u5b89\u5168\u901a\u4fe1\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5bfc\u822a\u51b3\u7b56\u548c\u901a\u4fe1\u65b9\u9762\u7684\u6311\u6218\uff0c\u5229\u7528\u91cf\u5b50\u6280\u672f\u63d0\u5347\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\uff0cNav-Q\u6a21\u5757\u901a\u8fc7\u53d8\u5206\u91cf\u5b50\u7535\u8def\u5b66\u4e60\u6700\u4f18\u5bfc\u822a\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u540e\u91cf\u5b50\u5bc6\u7801\u534f\u8bae\u4fdd\u62a4\u901a\u4fe1\u5b89\u5168\u3002", "result": "\u67b6\u6784\u63d0\u4f9b\u4e86\u91cf\u5b50\u6027\u80fd\u63d0\u5347\u548c\u9762\u5411\u672a\u6765\u7684\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u5bfc\u822a\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u91cf\u5b50\u6280\u672f\u548c\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16206", "pdf": "https://arxiv.org/pdf/2506.16206", "abs": "https://arxiv.org/abs/2506.16206", "authors": ["James Carr"], "title": "Locality in Many-Valued Structures", "categories": ["cs.LO", "math.LO", "F.4.1; I.2.4"], "comment": null, "summary": "Many-valued models generalise the structures from classical model theory by\ndefining truth values for a model with an arbitrary algebra. Just as algebraic\nvarieties provide semantics for many non-classical propositional logics, models\ndefined over algebras in a variety provide the semantics for the corresponding\nnon-classical predicate logics. In particular models defined over varieties of\nresiduated lattices represent the model theory for first-order substructrual\nlogics.\n  In this paper we study the extent to which the classical locality theorems\nfrom Hanf and Gaifman hold true in the residuated lattice setting. We\ndemonstrate that the answer is sensitive both to how locality is understood in\nthe generalised context and the behaviour of the truth-defining algebra. In the\ncase of Hanf's theorem, we will show that the theorem fails for the natural\nunderstanding of local neighbourhoods, but is recoverable in one special case\nfor well-connected residuated lattices. For Gaifman's theorem, rather than\nconsider Gaifman normal forms directly we focus on the main lemma of the\ntheorem from textbook proofs. We prove that for a number of different\nunderstandings of locality, provided the algebra is well-behaved enough to\nexpress locality in its syntax, this main lemma can be recovered. In each case\nwe will see that importance of an order-interpreting connective which creates a\nlink between the modelling relation between models and formulas and the\nvaluation function from formulas into the algebra.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5269\u4f59\u683c\u4ee3\u6570\u80cc\u666f\u4e0b\uff0c\u7ecf\u5178\u5c40\u90e8\u6027\u5b9a\u7406\uff08Hanf\u548cGaifman\u5b9a\u7406\uff09\u662f\u5426\u6210\u7acb\uff0c\u5e76\u63a2\u8ba8\u4e86\u5c40\u90e8\u6027\u7684\u5b9a\u4e49\u548c\u4ee3\u6570\u884c\u4e3a\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u7ecf\u5178\u6a21\u578b\u8bba\u4e2d\u7684\u5c40\u90e8\u6027\u5b9a\u7406\u5728\u591a\u503c\u6a21\u578b\u4e2d\u662f\u5426\u4f9d\u7136\u9002\u7528\uff0c\u5c24\u5176\u662f\u5728\u5269\u4f59\u683c\u4ee3\u6570\u8fd9\u4e00\u975e\u7ecf\u5178\u8c13\u8bcd\u903b\u8f91\u8bed\u4e49\u80cc\u666f\u4e0b\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5206\u6790Hanf\u548cGaifman\u5b9a\u7406\u5728\u5269\u4f59\u683c\u4ee3\u6570\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u5c40\u90e8\u6027\u5b9a\u4e49\u548c\u4ee3\u6570\u884c\u4e3a\u5bf9\u5b9a\u7406\u6210\u7acb\u6027\u7684\u5f71\u54cd\u3002", "result": "Hanf\u5b9a\u7406\u5728\u81ea\u7136\u5c40\u90e8\u6027\u5b9a\u4e49\u4e0b\u4e0d\u6210\u7acb\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\uff08\u5982\u826f\u597d\u8fde\u63a5\u7684\u5269\u4f59\u683c\uff09\u53ef\u6062\u590d\uff1bGaifman\u5b9a\u7406\u7684\u5f15\u7406\u5728\u4ee3\u6570\u884c\u4e3a\u826f\u597d\u7684\u60c5\u51b5\u4e0b\u53ef\u6062\u590d\u3002", "conclusion": "\u5269\u4f59\u683c\u4ee3\u6570\u4e2d\u7684\u5c40\u90e8\u6027\u5b9a\u7406\u5bf9\u5c40\u90e8\u6027\u5b9a\u4e49\u548c\u4ee3\u6570\u884c\u4e3a\u9ad8\u5ea6\u654f\u611f\uff0c\u5176\u4e2d\u987a\u5e8f\u89e3\u91ca\u8fde\u63a5\u7b26\u5728\u6a21\u578b\u548c\u4ee3\u6570\u4e4b\u95f4\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2506.15834", "pdf": "https://arxiv.org/pdf/2506.15834", "abs": "https://arxiv.org/abs/2506.15834", "authors": ["Zachary D King", "Maryam Khalid", "Han Yu", "Kei Shibuya", "Khadija Zanna", "Marzieh Majd", "Ryan L Brown", "Yufei Shen", "Thomas Vaessen", "George Kypriotakis", "Christopher P Fagundes", "Akane Sano"], "title": "Machine Learning-based Context-Aware EMAs: An Offline Feasibility Study", "categories": ["cs.HC"], "comment": null, "summary": "Mobile health (mHealth) systems help researchers monitor and care for\npatients in real-world settings. Studies utilizing mHealth applications use\nEcological Momentary Assessment (EMAs), passive sensing, and contextual\nfeatures to develop emotion recognition models, which rely on EMA responses as\nground truth. Due to this, it is crucial to consider EMA compliance when\nconducting a successful mHealth study. Utilizing machine learning is one\napproach that can solve this problem by sending EMAs based on the predicted\nlikelihood of a response. However, literature suggests that this approach may\nlead to prompting participants more frequently during emotions associated with\nresponsiveness, thereby narrowing the range of emotions collected. We propose a\nmulti-objective function that utilizes machine learning to identify optimal\ntimes for sending EMAs. The function identifies optimal moments by combining\npredicted response likelihood with model uncertainty in emotion predictions.\nUncertainty would lead the function to prioritize time points when the model is\nless confident, which often corresponds to underrepresented emotions. We\ndemonstrate that this objective function would result in EMAs being sent when\nparticipants are responsive and experiencing less commonly observed emotions.\nThe evaluation is conducted offline using two datasets: (1) 91 spousal\ncaregivers of individuals with Alzheimer's Disease and Related dementias\n(ADRD), (2) 45 healthy participants. Results show that the multi-objective\nfunction tends to be higher when participants respond to EMAs and report less\ncommonly observed emotions. This suggests that using the proposed objective\nfunction to guide EMA delivery could improve receptivity rates and capture a\nbroader range of emotions.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e00\u79cd\u591a\u76ee\u6807\u51fd\u6570\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7684\u54cd\u5e94\u9884\u6d4b\u548c\u60c5\u611f\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u4f18\u5316EMA\u53d1\u9001\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u54cd\u5e94\u7387\u5e76\u6355\u6349\u66f4\u5e7f\u6cdb\u7684\u60c5\u611f\u3002", "motivation": "mHealth\u7814\u7a76\u4e2d\uff0cEMA\u7684\u54cd\u5e94\u7387\u5bf9\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u60c5\u611f\u6837\u672c\u4e0d\u5747\u8861\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u591a\u76ee\u6807\u51fd\u6570\uff0c\u7ed3\u5408\u54cd\u5e94\u9884\u6d4b\u548c\u60c5\u611f\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4f18\u5316EMA\u53d1\u9001\u65f6\u95f4\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u51fd\u6570\u5728\u54cd\u5e94\u7387\u9ad8\u4e14\u60c5\u611f\u4e0d\u5e38\u89c1\u65f6\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u591a\u76ee\u6807\u51fd\u6570\u53ef\u63d0\u9ad8EMA\u54cd\u5e94\u7387\u5e76\u6355\u6349\u66f4\u591a\u6837\u5316\u7684\u60c5\u611f\uff0c\u9002\u7528\u4e8emHealth\u7814\u7a76\u3002"}}
{"id": "2506.15697", "pdf": "https://arxiv.org/pdf/2506.15697", "abs": "https://arxiv.org/abs/2506.15697", "authors": ["Yi Liu", "Hongji Zhang", "Yunhao Zhou", "Zhengyuan Shi", "Changran Xu", "Qiang Xu"], "title": "DeepRTL2: A Versatile Model for RTL-Related Tasks", "categories": ["cs.AR", "cs.CL", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "The integration of large language models (LLMs) into electronic design\nautomation (EDA) has significantly advanced the field, offering transformative\nbenefits, particularly in register transfer level (RTL) code generation and\nunderstanding. While previous studies have demonstrated the efficacy of\nfine-tuning LLMs for these generation-based tasks, embedding-based tasks, which\nare equally critical to EDA workflows, have been largely overlooked. These\ntasks, including natural language code search, RTL code functionality\nequivalence checking, and performance prediction, are essential for\naccelerating and optimizing the hardware design process. To address this gap,\nwe present DeepRTL2, a family of versatile LLMs that unifies both generation-\nand embedding-based tasks related to RTL. By simultaneously tackling a broad\nrange of tasks, DeepRTL2 represents the first model to provide a comprehensive\nsolution to the diverse challenges in EDA. Through extensive experiments, we\nshow that DeepRTL2 achieves state-of-the-art performance across all evaluated\ntasks.", "AI": {"tldr": "DeepRTL2\u901a\u8fc7\u7edf\u4e00\u751f\u6210\u548c\u5d4c\u5165\u4efb\u52a1\uff0c\u4e3aEDA\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684LLM\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728LLM\u7684\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u5ffd\u7565\u4e86\u5d4c\u5165\u4efb\u52a1\u5728EDA\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5982\u4ee3\u7801\u641c\u7d22\u548c\u6027\u80fd\u9884\u6d4b\u3002DeepRTL2\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faDeepRTL2\uff0c\u4e00\u4e2a\u591a\u529f\u80fdLLM\u5bb6\u65cf\uff0c\u7edf\u4e00\u5904\u7406RTL\u76f8\u5173\u7684\u751f\u6210\u548c\u5d4c\u5165\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepRTL2\u5728\u6240\u6709\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DeepRTL2\u9996\u6b21\u4e3aEDA\u4e2d\u7684\u591a\u6837\u5316\u6311\u6218\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15831", "pdf": "https://arxiv.org/pdf/2506.15831", "abs": "https://arxiv.org/abs/2506.15831", "authors": ["Jongjun Park", "Fei Chiang", "Mostafa Milani"], "title": "Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report", "categories": ["cs.DB"], "comment": "Extended version (to be updated)", "summary": "Data changes to reflect evolving user behaviour, preferences, and changes in\nthe environment. Such changes may occur due to expected shifts in the data\ndistribution, i.e., concept drift, or unexpected anomalous changes. The\npresence of concept drift poses challenges for anomaly detection in time\nseries. While anomalies are caused by undesirable changes in the data,\ndifferentiating abnormal changes from varying normal behaviours is difficult\ndue to differing frequencies of occurrence, varying time intervals when normal\npatterns occur. Differentiating between concept drift and anomalies is critical\nfor accurate analysis as studies have shown that the compounding effects of\nerror propagation in downstream data analysis tasks lead to lower detection\naccuracy and increased overhead due to unnecessary model updates.\nUnfortunately, existing work has largely explored anomaly detection and concept\ndrift detection in isolation. We develop AnDri, a system for Anomaly detection\nin the presence of Drift, which adjusts the normal patterns temporally, and\ndistinguish abnormal subsequences and new concepts. Moreover, it introduces a\nnew clustering method, Adjacent Hierarchical Clustering (AHC), which groups\nsimilar subsequences while respecting their temporal locality.", "AI": {"tldr": "AnDri\u7cfb\u7edf\u7528\u4e8e\u5728\u6570\u636e\u6f02\u79fb\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u8c03\u6574\u65f6\u95f4\u4e0a\u7684\u6b63\u5e38\u6a21\u5f0f\u5e76\u533a\u5206\u5f02\u5e38\u5b50\u5e8f\u5217\u548c\u65b0\u6982\u5ff5\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u805a\u7c7b\u65b9\u6cd5AHC\u3002", "motivation": "\u6570\u636e\u5206\u5e03\u7684\u53d8\u5316\uff08\u6982\u5ff5\u6f02\u79fb\uff09\u548c\u5f02\u5e38\u53d8\u5316\u7ed9\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5b64\u7acb\u5730\u7814\u7a76\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u7f3a\u4e4f\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86AnDri\u7cfb\u7edf\uff0c\u901a\u8fc7\u65f6\u95f4\u8c03\u6574\u6b63\u5e38\u6a21\u5f0f\u533a\u5206\u5f02\u5e38\u548c\u65b0\u6982\u5ff5\uff0c\u5e76\u5f15\u5165Adjacent Hierarchical Clustering\uff08AHC\uff09\u805a\u7c7b\u65b9\u6cd5\u3002", "result": "AnDri\u80fd\u591f\u6709\u6548\u533a\u5206\u6982\u5ff5\u6f02\u79fb\u548c\u5f02\u5e38\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6a21\u578b\u66f4\u65b0\u5f00\u9500\u3002", "conclusion": "AnDri\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6982\u5ff5\u6f02\u79fb\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u8054\u5408\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.15961", "pdf": "https://arxiv.org/pdf/2506.15961", "abs": "https://arxiv.org/abs/2506.15961", "authors": ["Yunchi Lu", "Youshan Miao", "Cheng Tan", "Peng Huang", "Yi Zhu", "Xian Zhang", "Fan Yang"], "title": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Training large language models (LLMs) at scale requires parallel execution\nacross thousands of devices, incurring enormous computational costs. Yet, these\ncostly distributed trainings are rarely verified, leaving them prone to silent\nerrors and potentially wasting millions of GPU hours. We introduce TrainVerify,\na system for verifiable distributed training of LLMs. Given a deep learning\nmodel's logical specification as the ground truth, TrainVerify formally\nverifies that a distributed parallel execution plan is mathematically\nequivalent to it. Direct verification is notoriously difficult due to the sheer\nscale of LLMs which often involves billions of variables and highly intricate\ncomputation graphs. Therefore, TrainVerify introduces shape-reduction\ntechniques and a stage-wise parallel verification algorithm that significantly\nreduces complexity while preserving formal correctness. TrainVerify scales to\nfrontier LLMs, including the successful verification of the Llama3 (405B) and\nDeepSeek-V3 (671B) training plans.", "AI": {"tldr": "TrainVerify\u662f\u4e00\u4e2a\u7528\u4e8e\u9a8c\u8bc1\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u6b63\u786e\u6027\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u786e\u4fdd\u5206\u5e03\u5f0f\u6267\u884c\u8ba1\u5212\u4e0e\u903b\u8f91\u89c4\u8303\u4e00\u81f4\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u4e14\u6613\u53d7\u9690\u6027\u9519\u8bef\u5f71\u54cd\uff0c\u4e9f\u9700\u4e00\u79cd\u9a8c\u8bc1\u65b9\u6cd5\u4ee5\u786e\u4fdd\u8bad\u7ec3\u7684\u6b63\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u5f62\u72b6\u7f29\u51cf\u6280\u672f\u548c\u9636\u6bb5\u6027\u5e76\u884c\u9a8c\u8bc1\u7b97\u6cd5\uff0c\u5927\u5e45\u964d\u4f4e\u9a8c\u8bc1\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u5f62\u5f0f\u6b63\u786e\u6027\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u5305\u62ecLlama3\uff08405B\uff09\u548cDeepSeek-V3\uff08671B\uff09\u5728\u5185\u7684\u524d\u6cbfLLM\u8bad\u7ec3\u8ba1\u5212\u3002", "conclusion": "TrainVerify\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u9a8c\u8bc1\u624b\u6bb5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u98ce\u9669\u548c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002"}}
{"id": "2506.16440", "pdf": "https://arxiv.org/pdf/2506.16440", "abs": "https://arxiv.org/abs/2506.16440", "authors": ["Ebube Alor", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Evaluating the Use of LLMs for Documentation to Code Traceability", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) offer new potential for automating\ndocumentation-to-code traceability, yet their capabilities remain\nunderexplored. We present a comprehensive evaluation of LLMs (Claude 3.5\nSonnet, GPT-4o, and o3-mini) in establishing trace links between various\nsoftware documentation (including API references and user guides) and source\ncode. We create two novel datasets from two open-source projects (Unity Catalog\nand Crawl4AI). Through systematic experiments, we assess three key\ncapabilities: (1) trace link identification accuracy, (2) relationship\nexplanation quality, and (3) multi-step chain reconstruction. Results show that\nthe best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two\ndatasets, substantially outperforming our baselines (TF-IDF, BM25, and\nCodeBERT). While fully correct relationship explanations range from 42.9% to\n71.1%, partial accuracy exceeds 97%, indicating that fundamental connections\nare rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy\nbut vary in capturing precise intermediate links. Error analysis reveals that\nmany false positives stem from naming-based assumptions, phantom links, or\novergeneralization of architectural patterns. We demonstrate that task-framing,\nsuch as a one-to-many matching strategy, is critical for performance. These\nfindings position LLMs as powerful assistants for trace discovery, but their\nlimitations could necessitate human-in-the-loop tool design and highlight\nspecific error patterns for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u6863\u5230\u4ee3\u7801\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u5728\u67d0\u4e9b\u65b9\u9762\u4ecd\u9700\u4eba\u5de5\u8f85\u52a9\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u81ea\u52a8\u5316\u6587\u6863\u4e0e\u4ee3\u7801\u8ffd\u8e2a\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u8bc4\u4f30\u4e09\u79cdLLMs\u5728\u8ffd\u8e2a\u94fe\u63a5\u8bc6\u522b\u3001\u5173\u7cfb\u89e3\u91ca\u8d28\u91cf\u548c\u591a\u6b65\u9aa4\u94fe\u91cd\u5efa\u4e09\u4e2a\u5173\u952e\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73LLM\u5728F1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982TF-IDF\u548cCodeBERT\uff09\uff0c\u4f46\u5728\u5b8c\u5168\u6b63\u786e\u7684\u5173\u7cfb\u89e3\u91ca\u548c\u591a\u6b65\u9aa4\u94fe\u4e2d\u95f4\u94fe\u63a5\u7684\u6355\u83b7\u4e0a\u8868\u73b0\u4e0d\u4e00\u3002", "conclusion": "LLMs\u5728\u8ffd\u8e2a\u53d1\u73b0\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u4eba\u5de5\u5de5\u5177\u8bbe\u8ba1\uff0c\u5e76\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u9519\u8bef\u6a21\u5f0f\u3002"}}
{"id": "2506.16786", "pdf": "https://arxiv.org/pdf/2506.16786", "abs": "https://arxiv.org/abs/2506.16786", "authors": ["Qingyang Zhang", "Mohammad Dwipa Furqan", "Tasfia Nutzhat", "Fumio Machida", "Ermeson Andrade"], "title": "Dependability of UAV-Based Networks and Computing Systems: A Survey", "categories": ["cs.PF"], "comment": "54 pages, 13 figures", "summary": "Uncrewed Aerial Vehicle (UAV) computing and networking are becoming a\nfundamental computation infrastructure for diverse cyber-physical application\nsystems. UAVs can be empowered by AI on edge devices and can communicate with\nother UAVs and ground stations via wireless communication networks. Dynamic\ncomputation demands and heterogeneous computing resources are distributed in\nthe system and need to be controlled to maintain the quality of services and to\naccomplish critical missions. With the evolution of UAV-based systems,\ndependability assurance of such systems emerges as a crucial challenge.\nUAV-based systems confront diverse sources of uncertainty that may threaten\ntheir dependability, such as software bugs, component failures, network\ndisconnections, battery shortages, and disturbances from the real world. In\nthis paper, we conduct systematic literature reviews on the dependability of\nUAV-based networks and computing systems. The survey report reveals emerging\nresearch trends in this field and summarizes the literature into comprehensive\ncategories by threat types and adopted technologies. Based on our literature\nreviews, we identify eight research fields that require further exploration in\nthe future to achieve dependable UAV-based systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u65e0\u4eba\u673a\uff08UAV\uff09\u8ba1\u7b97\u4e0e\u7f51\u7edc\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u603b\u7ed3\u4e86\u5a01\u80c1\u7c7b\u578b\u548c\u5e94\u5bf9\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u9700\u8981\u63a2\u7d22\u7684\u516b\u4e2a\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u591a\u79cd\u7269\u7406\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u5176\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63ed\u793a\u7814\u7a76\u8d8b\u52bf\u5e76\u63d0\u51fa\u672a\u6765\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5bf9\u65e0\u4eba\u673a\u7f51\u7edc\u548c\u8ba1\u7b97\u7cfb\u7edf\u7684\u6587\u732e\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6309\u5a01\u80c1\u7c7b\u578b\u548c\u6280\u672f\u5206\u7c7b\u603b\u7ed3\u73b0\u6709\u7814\u7a76\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u5a01\u80c1\u6765\u6e90\uff08\u5982\u8f6f\u4ef6\u6545\u969c\u3001\u7f51\u7edc\u4e2d\u65ad\u7b49\uff09\u53ca\u5e94\u5bf9\u6280\u672f\uff0c\u5e76\u6307\u51fa\u516b\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u7814\u7a76\u4ecd\u9700\u6df1\u5316\uff0c\u672a\u6765\u9700\u5728\u591a\u9886\u57df\u8fdb\u4e00\u6b65\u63a2\u7d22\u4ee5\u5b9e\u73b0\u9ad8\u53ef\u9760\u6027\u7cfb\u7edf\u3002"}}
{"id": "2506.16048", "pdf": "https://arxiv.org/pdf/2506.16048", "abs": "https://arxiv.org/abs/2506.16048", "authors": ["Byeongjee Kang", "Harsh Desai", "Limin Jia", "Brandon Lucia"], "title": "WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction", "categories": ["cs.PL"], "comment": null, "summary": "WebAssembly (Wasm) is a portable bytecode format that serves as a compilation\ntarget for high-level languages, enabling their secure and efficient execution\nacross diverse platforms, including web browsers and embedded systems. To\nimprove support for high-level languages without incurring significant code\nsize or performance overheads, Wasm continuously evolves by integrating\nhigh-level features such as Garbage Collection and Stack Switching. However,\nexisting compilation approaches either lack reusable design -- requiring\nredundant implementation efforts for each language -- or lose abstraction by\nlowering high-level constructs into low-level shared representations like LLVM\nIR, which hinder the adoption of high-level features. MLIR compiler\ninfrastructure provides the compilation pipeline with multiple levels of\nabstraction, preserving high-level abstractions throughout the compilation\npipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm\ncode generation, thereby inheriting LLVM's limitations.\n  This paper presents a novel compilation pipeline for Wasm, featuring Wasm\ndialects explicitly designed to represent high-level Wasm constructs within\nMLIR. Our approach enables direct generation of high-level Wasm code from\ncorresponding high-level MLIR dialects without losing abstraction, providing a\nmodular and extensible way to incorporate high-level Wasm features. We\nillustrate this extensibility through a case study that leverages Stack\nSwitching, a recently introduced high-level feature of Wasm. Performance\nevaluations on PolyBench benchmarks show that our pipeline, benefiting from\noptimizations within the MLIR and Wasm ecosystems, produces code with at most\n7.7\\% slower, and faster in some execution environments, compared to LLVM-based\ncompilers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684WebAssembly\uff08Wasm\uff09\u7f16\u8bd1\u7ba1\u9053\uff0c\u901a\u8fc7MLIR\u4e2d\u7684Wasm\u65b9\u8a00\u76f4\u63a5\u751f\u6210\u9ad8\u7ea7Wasm\u4ee3\u7801\uff0c\u907f\u514d\u4e86\u4f20\u7edfLLVM\u540e\u7aef\u7684\u5c40\u9650\u6027\uff0c\u6027\u80fd\u63a5\u8fd1\u751a\u81f3\u4f18\u4e8eLLVM\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u5bf9\u9ad8\u7ea7\u8bed\u8a00\u7684\u652f\u6301\u5e76\u51cf\u5c11\u4ee3\u7801\u5927\u5c0f\u6216\u6027\u80fd\u5f00\u9500\uff0cWasm\u4e0d\u65ad\u96c6\u6210\u9ad8\u7ea7\u7279\u6027\uff08\u5982\u5783\u573e\u56de\u6536\u548c\u5806\u6808\u5207\u6362\uff09\uff0c\u4f46\u73b0\u6709\u7f16\u8bd1\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u53ef\u91cd\u7528\u8bbe\u8ba1\uff0c\u8981\u4e48\u56e0\u62bd\u8c61\u4e22\u5931\u800c\u96be\u4ee5\u91c7\u7528\u8fd9\u4e9b\u7279\u6027\u3002", "method": "\u5229\u7528MLIR\u4e2d\u7684Wasm\u65b9\u8a00\u76f4\u63a5\u751f\u6210\u9ad8\u7ea7Wasm\u4ee3\u7801\uff0c\u65e0\u9700\u4f9d\u8d56LLVM\u540e\u7aef\uff0c\u5e76\u901a\u8fc7\u5806\u6808\u5207\u6362\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728PolyBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7ba1\u9053\u751f\u6210\u7684\u4ee3\u7801\u6027\u80fd\u6700\u591a\u6bd4\u57fa\u4e8eLLVM\u7684\u7f16\u8bd1\u5668\u61627.7%\uff0c\u67d0\u4e9b\u73af\u5883\u4e0b\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u9ad8\u7ea7Wasm\u7279\u6027\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u4e14\u9ad8\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2506.16409", "pdf": "https://arxiv.org/pdf/2506.16409", "abs": "https://arxiv.org/abs/2506.16409", "authors": ["Mahbubur Rahman", "Abusayeed Saifullah"], "title": "LoRaIN: A Constructive Interference-Assisted Reliable and Energy-Efficient LoRa Indoor Network", "categories": ["cs.NI"], "comment": null, "summary": "LoRa is a promising communication technology for enabling the next-generation\nindoor Internet of Things applications. Very few studies, however, have\nanalyzed its performance indoors. Besides, these indoor studies investigate\nmostly the RSSI and SNR of the received packets at the gateway, which, as we\nshow, may not unfold the poor performance of LoRa and its MAC protocol,\nLoRaWAN, indoors in terms of reliability and energy-efficiency. In this paper,\nwe extensively evaluate the performance of LoRaWAN indoors and then use the key\ninsights to boost its reliability and energy-efficiency by proposing LoRaIN,\nLoRa Indoor Network, a new link-layer protocol that can be effectively used for\nindoor deployments. The approach to boosting the reliability and energy\nefficiency in LoRaIN is underpinned by enabling constructive interference with\nspecific timing requirements analyzed both empirically and mathematically for\ndifferent pairs of channel bandwidth and spreading factor and relaying precious\nacknowledgments to the end-devices with the assistance of several booster\nnodes. The booster nodes do not need any special capability and can be a subset\nof the LoRa end-devices. To our knowledge, LoRaIN is the first protocol for\nboosting reliability and energy-efficiency in indoor LoRa networks. We evaluate\nits performance in an indoor testbed consisting of one LoRaWAN gateway and 20\nend-devices. Our extensive evaluation shows that when 15% of the end-devices\noperate as booster nodes, the reliability at the gateway increases from 62% to\n95%, and the end-devices are approximately 2.5x energy-efficient.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LoRaIN\u534f\u8bae\uff0c\u65e8\u5728\u63d0\u5347\u5ba4\u5185LoRa\u7f51\u7edc\u7684\u53ef\u9760\u6027\u548c\u80fd\u6548\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u5b66\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "LoRa\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u901a\u4fe1\u6280\u672f\uff0c\u5ba4\u5185\u6027\u80fd\u7814\u7a76\u8f83\u5c11\uff0c\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63ed\u793a\u5176\u53ef\u9760\u6027\u548c\u80fd\u6548\u95ee\u9898\u3002", "method": "\u63d0\u51faLoRaIN\u534f\u8bae\uff0c\u5229\u7528\u7279\u6b8a\u5b9a\u65f6\u548c\u589e\u5f3a\u8282\u70b9\u5b9e\u73b0\u5e72\u6270\u6784\u9020\u548c\u786e\u8ba4\u4e2d\u7ee7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c15%\u7684\u7ec8\u7aef\u8bbe\u5907\u4f5c\u4e3a\u589e\u5f3a\u8282\u70b9\u65f6\uff0c\u53ef\u9760\u6027\u4ece62%\u63d0\u5347\u81f395%\uff0c\u80fd\u6548\u63d0\u9ad82.5\u500d\u3002", "conclusion": "LoRaIN\u662f\u9996\u4e2a\u63d0\u5347\u5ba4\u5185LoRa\u7f51\u7edc\u6027\u80fd\u7684\u534f\u8bae\uff0c\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2506.16495", "pdf": "https://arxiv.org/pdf/2506.16495", "abs": "https://arxiv.org/abs/2506.16495", "authors": ["Changsheng Gao", "Zijie Liu", "Li Li", "Dong Liu", "Xiaoyan Sun", "Weisi Lin"], "title": "DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Like image coding in visual data transmission, feature coding is essential\nfor the distributed deployment of large models by significantly reducing\ntransmission and storage overhead. However, prior studies have mostly targeted\ntask- or model-specific scenarios, leaving the challenge of universal feature\ncoding across diverse large models largely unaddressed. In this paper, we\npresent the first systematic study on universal feature coding for large\nmodels. The key challenge lies in the inherently diverse and distributionally\nincompatible nature of features extracted from different models. For example,\nfeatures from DINOv2 exhibit highly peaky, concentrated distributions, while\nthose from Stable Diffusion 3 (SD3) are more dispersed and uniform. This\ndistributional heterogeneity severely hampers both compression efficiency and\ncross-model generalization. To address this, we propose a learned\npeaky-to-balanced distribution transformation, which reshapes highly skewed\nfeature distributions into a common, balanced target space. This transformation\nis non-uniform, data-driven, and plug-and-play, enabling effective alignment of\nheterogeneous distributions without modifying downstream codecs. With this\nalignment, a universal codec trained on the balanced target distribution can\neffectively generalize to features from different models and tasks. We validate\nour approach on three representative large models-LLaMA3, DINOv2, and\nSD3-across multiple tasks and modalities. Extensive experiments show that our\nmethod achieves notable improvements in both compression efficiency and\ncross-model generalization over task-specific baselines. All source code will\nbe released for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u7279\u5f81\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u6a21\u578b\u4e2d\u7279\u5f81\u5206\u5e03\u591a\u6837\u4e14\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u5206\u5e03\u53d8\u6362\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u8de8\u6a21\u578b\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u7f16\u7801\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u6216\u6a21\u578b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e0d\u540c\u5927\u6a21\u578b\u7279\u5f81\u5206\u5e03\u591a\u6837\u4e14\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b66\u4e60\u7684\u5cf0\u503c\u5230\u5e73\u8861\u5206\u5e03\u53d8\u6362\uff0c\u5c06\u4e0d\u540c\u6a21\u578b\u7684\u975e\u5747\u5300\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u5230\u7edf\u4e00\u7684\u76ee\u6807\u7a7a\u95f4\uff0c\u65e0\u9700\u4fee\u6539\u4e0b\u6e38\u7f16\u7801\u5668\u3002", "result": "\u5728LLaMA3\u3001DINOv2\u548cSD3\u7b49\u5927\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u6548\u7387\u548c\u8de8\u6a21\u578b\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u4efb\u52a1\u7279\u5b9a\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u7279\u5f81\u7f16\u7801\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u5c06\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u8fdb\u4e00\u6b65\u63a8\u52a8\u8fdb\u5c55\u3002"}}
{"id": "2506.15786", "pdf": "https://arxiv.org/pdf/2506.15786", "abs": "https://arxiv.org/abs/2506.15786", "authors": ["Peter Yichen Chen", "Minghao Guo", "Hanspeter Pfister", "Ming Lin", "William Freeman", "Qixing Huang", "Han-Wei Shen", "Wojciech Matusik"], "title": "Graphics4Science: Computer Graphics for Scientific Impacts", "categories": ["cs.GR", "cs.AI", "cs.LG", "physics.comp-ph", "physics.optics"], "comment": null, "summary": "Computer graphics, often associated with films, games, and visual effects,\nhas long been a powerful tool for addressing scientific challenges--from its\norigins in 3D visualization for medical imaging to its role in modern\ncomputational modeling and simulation. This course explores the deep and\nevolving relationship between computer graphics and science, highlighting past\nachievements, ongoing contributions, and open questions that remain. We show\nhow core methods, such as geometric reasoning and physical modeling, provide\ninductive biases that help address challenges in both fields, especially in\ndata-scarce settings. To that end, we aim to reframe graphics as a modeling\nlanguage for science by bridging vocabulary gaps between the two communities.\nDesigned for both newcomers and experts, Graphics4Science invites the graphics\ncommunity to engage with science, tackle high-impact problems where graphics\nexpertise can make a difference, and contribute to the future of scientific\ndiscovery. Additional details are available on the course website:\nhttps://graphics4science.github.io", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e0e\u79d1\u5b66\u7684\u6df1\u5c42\u5173\u7cfb\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u79d1\u5b66\u5efa\u6a21\u8bed\u8a00\u7684\u4f5c\u7528\uff0c\u5e76\u9f13\u52b1\u56fe\u5f62\u5b66\u793e\u533a\u53c2\u4e0e\u79d1\u5b66\u95ee\u9898\u7684\u89e3\u51b3\u3002", "motivation": "\u7814\u7a76\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e0e\u79d1\u5b66\u7684\u878d\u5408\uff0c\u4ee5\u586b\u8865\u4e24\u9886\u57df\u95f4\u7684\u8bcd\u6c47\u9e3f\u6c9f\uff0c\u5e76\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u63a8\u7406\u548c\u7269\u7406\u5efa\u6a21\u7b49\u6838\u5fc3\u65b9\u6cd5\uff0c\u4e3a\u79d1\u5b66\u95ee\u9898\u63d0\u4f9b\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u63d0\u51fa\u5c06\u56fe\u5f62\u5b66\u4f5c\u4e3a\u79d1\u5b66\u5efa\u6a21\u8bed\u8a00\u7684\u6982\u5ff5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "Graphics4Science\u8bfe\u7a0b\u65e8\u5728\u63a8\u52a8\u56fe\u5f62\u5b66\u4e13\u5bb6\u53c2\u4e0e\u9ad8\u5f71\u54cd\u529b\u7684\u79d1\u5b66\u95ee\u9898\uff0c\u4fc3\u8fdb\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2506.16281", "pdf": "https://arxiv.org/pdf/2506.16281", "abs": "https://arxiv.org/abs/2506.16281", "authors": ["Martha Arbayani Zaidan", "Naser Hossein Motlagh", "Petteri Nurmi", "Tareq Hussein", "Markku Kulmala", "Tuukka Pet\u00e4j\u00e4", "Sasu Tarkoma"], "title": "Artificial Intelligence for Atmospheric Sciences: A Research Roadmap", "categories": ["cs.ET", "cs.AI"], "comment": null, "summary": "Atmospheric sciences are crucial for understanding environmental phenomena\nranging from air quality to extreme weather events, and climate change. Recent\nbreakthroughs in sensing, communication, computing, and Artificial Intelligence\n(AI) have significantly advanced atmospheric sciences, enabling the generation\nof vast amounts of data through long-term Earth observations and providing\npowerful tools for analyzing atmospheric phenomena and predicting natural\ndisasters. This paper contributes a critical interdisciplinary overview that\nbridges the fields of atmospheric science and computer science, highlighting\nthe transformative potential of AI in atmospheric research. We identify key\nchallenges associated with integrating AI into atmospheric research, including\nissues related to big data and infrastructure, and provide a detailed research\nroadmap that addresses both current and emerging challenges.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u5927\u6c14\u79d1\u5b66\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u53ca\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8AI\u5982\u4f55\u63a8\u52a8\u5927\u6c14\u79d1\u5b66\u7814\u7a76\uff0c\u89e3\u51b3\u6570\u636e\u4e0e\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u3002", "method": "\u63d0\u4f9b\u8de8\u5b66\u79d1\u7efc\u8ff0\uff0c\u8bc6\u522bAI\u5728\u5927\u6c14\u79d1\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u5e94\u5bf9\u5f53\u524d\u4e0e\u672a\u6765\u6311\u6218\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "conclusion": "AI\u5728\u5927\u6c14\u79d1\u5b66\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u4e0e\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u3002"}}
{"id": "2506.16244", "pdf": "https://arxiv.org/pdf/2506.16244", "abs": "https://arxiv.org/abs/2506.16244", "authors": ["Alejandro D\u00edaz-Caro", "Nicolas A. Monzon"], "title": "A Quantum-Control Lambda-Calculus with Multiple Measurement Bases", "categories": ["cs.LO", "quant-ph"], "comment": "18 pages + appendix", "summary": "We introduce Lambda-SX, a typed quantum lambda-calculus that supports\nmultiple measurement bases. By tracking duplicability relative to arbitrary\nbases within the type system, Lambda-SX enables more flexible control and\ncompositional reasoning about measurements. We formalise its syntax, typing\nrules, subtyping, and operational semantics, and establish its key\nmeta-theoretical properties. This proof-of-concept shows that support for\nmultiple bases can be coherently integrated into the type discipline of quantum\nprogramming languages.", "AI": {"tldr": "Lambda-SX\u662f\u4e00\u79cd\u652f\u6301\u591a\u6d4b\u91cf\u57fa\u7684\u91cf\u5b50\u03bb\u6f14\u7b97\uff0c\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u8ddf\u8e2a\u53ef\u590d\u5236\u6027\uff0c\u63d0\u5347\u6d4b\u91cf\u7684\u7075\u6d3b\u6027\u548c\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u7684\u7c7b\u578b\u7cfb\u7edf\u4e2d\u96c6\u6210\u591a\u6d4b\u91cf\u57fa\u7684\u652f\u6301\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u548c\u7ec4\u5408\u63a8\u7406\u3002", "method": "\u5b9a\u4e49\u4e86Lambda-SX\u7684\u8bed\u6cd5\u3001\u7c7b\u578b\u89c4\u5219\u3001\u5b50\u7c7b\u578b\u548c\u64cd\u4f5c\u8bed\u4e49\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5143\u7406\u8bba\u6027\u8d28\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u6d4b\u91cf\u57fa\u5728\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u7c7b\u578b\u7cfb\u7edf\u4e2d\u7684\u4e00\u81f4\u6027\u96c6\u6210\u3002", "conclusion": "Lambda-SX\u5c55\u793a\u4e86\u591a\u6d4b\u91cf\u57fa\u5728\u7c7b\u578b\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u7684\u7075\u6d3b\u6027\u548c\u7ec4\u5408\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.15873", "pdf": "https://arxiv.org/pdf/2506.15873", "abs": "https://arxiv.org/abs/2506.15873", "authors": ["Gregory Croisdale", "Emily Huang", "John Joon Young Chung", "Anhong Guo", "Xu Wang", "Austin Z. Henley", "Cyrus Omar"], "title": "DeckFlow: Iterative Specification on a Multimodal Generative Canvas", "categories": ["cs.HC"], "comment": null, "summary": "Generative AI promises to allow people to create high-quality personalized\nmedia. Although powerful, we identify three fundamental design problems with\nexisting tooling through a literature review. We introduce a multimodal\ngenerative AI tool, DeckFlow, to address these problems. First, DeckFlow\nsupports task decomposition by allowing users to maintain multiple\ninterconnected subtasks on an infinite canvas populated by cards connected\nthrough visual dataflow affordances. Second, DeckFlow supports a specification\ndecomposition workflow where an initial goal is iteratively decomposed into\nsmaller parts and combined using feature labels and clusters. Finally, DeckFlow\nsupports generative space exploration by generating multiple prompt and output\nvariations, presented in a grid, that can feed back recursively into the next\ndesign iteration. We evaluate DeckFlow for text-to-image generation against a\nstate-of-practice conversational AI baseline for image generation tasks. We\nthen add audio generation and investigate user behaviors in a more open-ended\ncreative setting with text, image, and audio outputs.", "AI": {"tldr": "DeckFlow\u662f\u4e00\u79cd\u591a\u6a21\u6001\u751f\u6210AI\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u7684\u4e09\u4e2a\u8bbe\u8ba1\u95ee\u9898\uff0c\u5305\u62ec\u4efb\u52a1\u5206\u89e3\u3001\u89c4\u8303\u5206\u89e3\u548c\u751f\u6210\u7a7a\u95f4\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210AI\u5de5\u5177\u5b58\u5728\u8bbe\u8ba1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7528\u6237\u521b\u5efa\u9ad8\u8d28\u91cf\u4e2a\u6027\u5316\u5a92\u4f53\u7684\u80fd\u529b\u3002", "method": "DeckFlow\u901a\u8fc7\u65e0\u9650\u753b\u5e03\u4e0a\u7684\u5361\u7247\u548c\u89c6\u89c9\u6570\u636e\u6d41\u5b9e\u73b0\u4efb\u52a1\u5206\u89e3\uff0c\u652f\u6301\u8fed\u4ee3\u5206\u89e3\u76ee\u6807\u548c\u751f\u6210\u591a\u6837\u6027\u63d0\u793a\u4e0e\u8f93\u51fa\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDeckFlow\u4f18\u4e8e\u73b0\u6709\u5bf9\u8bddAI\u57fa\u7ebf\uff0c\u5e76\u8fdb\u4e00\u6b65\u652f\u6301\u97f3\u9891\u751f\u6210\u548c\u5f00\u653e\u521b\u4f5c\u3002", "conclusion": "DeckFlow\u4e3a\u89e3\u51b3\u751f\u6210AI\u5de5\u5177\u7684\u8bbe\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u652f\u6301\u591a\u6a21\u6001\u521b\u610f\u4efb\u52a1\u3002"}}
{"id": "2506.15985", "pdf": "https://arxiv.org/pdf/2506.15985", "abs": "https://arxiv.org/abs/2506.15985", "authors": ["Mengming Li", "Qijun Zhang", "Yichuan Gao", "Wenji Fang", "Yao Lu", "Yongqing Ren", "Zhiyao Xie"], "title": "Profile-Guided Temporal Prefetching", "categories": ["cs.AR"], "comment": "In 52nd International Symposium on Computer Architecture (ISCA)", "summary": "Temporal prefetching shows promise for handling irregular memory access\npatterns, which are common in data-dependent and pointer-based data structures.\nRecent studies introduced on-chip metadata storage to reduce the memory traffic\ncaused by accessing metadata from off-chip DRAM. However, existing prefetching\nschemes struggle to efficiently utilize the limited on-chip storage. An\nalternative solution, software indirect access prefetching, remains ineffective\nfor optimizing temporal prefetching.\n  In this work, we propose Prophet--a hardware-software co-designed framework\nthat leverages profile-guided methods to optimize metadata storage management.\nProphet profiles programs using counters instead of traces, injects hints into\nprograms to guide metadata storage management, and dynamically tunes these\nhints to enable the optimized binary to adapt to different program inputs.\nProphet is designed to coexist with existing hardware temporal prefetchers,\ndelivering efficient, high-performance solutions for frequently executed\nworkloads while preserving the original runtime scheme for less frequently\nexecuted workloads. Prophet outperforms the state-of-the-art temporal\nprefetcher, Triangel, by 14.23%, effectively addressing complex temporal\npatterns where prior profile-guided solutions fall short (only achieving 0.1%\nperformance gain). Prophet delivers superior performance across all evaluated\nworkload inputs, introducing negligible profiling, analysis, and instruction\noverhead.", "AI": {"tldr": "\u63d0\u51faProphet\uff0c\u4e00\u79cd\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u5668\u7684\u65b9\u6cd5\u4f18\u5316\u5143\u6570\u636e\u5b58\u50a8\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u65f6\u7a7a\u9884\u6d4b\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u9884\u6d4b\u65b9\u6848\u65e0\u6cd5\u9ad8\u6548\u5229\u7528\u6709\u9650\u7684\u7247\u4e0a\u5b58\u50a8\uff0c\u800c\u8f6f\u4ef6\u95f4\u63a5\u8bbf\u95ee\u9884\u6d4b\u5bf9\u65f6\u7a7a\u9884\u6d4b\u7684\u4f18\u5316\u6548\u679c\u6709\u9650\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8ba1\u6570\u5668\u800c\u975e\u8ffd\u8e2a\u7684\u7a0b\u5e8f\u5206\u6790\u65b9\u6cd5\uff0c\u6ce8\u5165\u52a8\u6001\u8c03\u6574\u7684\u63d0\u793a\u4ee5\u6307\u5bfc\u5143\u6570\u636e\u5b58\u50a8\u7ba1\u7406\uff0c\u5e76\u4e0e\u73b0\u6709\u786c\u4ef6\u9884\u6d4b\u5668\u5171\u5b58\u3002", "result": "Prophet\u6bd4\u5f53\u524d\u6700\u4f73\u65f6\u7a7a\u9884\u6d4b\u5668Triangel\u6027\u80fd\u63d0\u534714.23%\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u8d1f\u8f7d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "Prophet\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u65f6\u7a7a\u6a21\u5f0f\u7684\u9884\u6d4b\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.15848", "pdf": "https://arxiv.org/pdf/2506.15848", "abs": "https://arxiv.org/abs/2506.15848", "authors": ["Jiazhen Peng", "Zheng Qu", "Xiaoye Miao", "Rong Zhu"], "title": "Delta: A Learned Mixed Cost-based Query Optimization Framework", "categories": ["cs.DB"], "comment": null, "summary": "Query optimizer is a crucial module for database management systems. Existing\noptimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic\nprogramming with cost models but face search space explosion and heuristic\npruning constraints; (2) value-based ones train value networks to enable\nefficient beam search, but incur higher training costs and lower accuracy. They\nalso lack mechanisms to detect queries where they may perform poorly. To\ndetermine more efficient plans, we propose Delta, a mixed cost-based query\noptimization framework that consists of a compatible query detector and a\ntwo-stage planner. Delta first employs a Mahalanobis distancebased detector to\npreemptively filter out incompatible queries where the planner might perform\npoorly. For compatible queries, Delta activates its two-stage mixed cost-based\nplanner. Stage I serves as a coarse-grained filter to generate high-quality\ncandidate plans based on the value network via beam search, relaxing precision\nrequirements and narrowing the search space. Stage II employs a fine-grained\nranker to determine the best plan from the candidate plans based on a learned\ncost model. Moreover, to reduce training costs, we reuse and augment the\ntraining data from stage I to train the model in stage II. Experimental results\non three workloads demonstrate that Delta identifies higher-quality plans,\nachieving an average 2.34x speedup over PostgreSQL and outperforming the\nstate-of-the-art learned methods by 2.21x.", "AI": {"tldr": "Delta\u662f\u4e00\u4e2a\u6df7\u5408\u6210\u672c\u67e5\u8be2\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u517c\u5bb9\u67e5\u8be2\u68c0\u6d4b\u5668\u548c\u4e24\u9636\u6bb5\u89c4\u5212\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f18\u5316\u5668\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u67e5\u8be2\u8ba1\u5212\u3002", "motivation": "\u73b0\u6709\u67e5\u8be2\u4f18\u5316\u5668\u5b58\u5728\u641c\u7d22\u7a7a\u95f4\u7206\u70b8\u3001\u542f\u53d1\u5f0f\u526a\u679d\u9650\u5236\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u51c6\u786e\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u6027\u80fd\u4e0d\u4f73\u67e5\u8be2\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "Delta\u91c7\u7528\u517c\u5bb9\u67e5\u8be2\u68c0\u6d4b\u5668\u9884\u7b5b\u6027\u80fd\u53ef\u80fd\u4e0d\u4f73\u7684\u67e5\u8be2\uff0c\u4e24\u9636\u6bb5\u89c4\u5212\u5668\u7ed3\u5408\u503c\u7f51\u7edc\u548c\u6210\u672c\u6a21\u578b\u751f\u6210\u9ad8\u6548\u8ba1\u5212\u3002", "result": "\u5728\u4e09\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cDelta\u5e73\u5747\u6027\u80fd\u63d0\u53472.34\u500d\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "Delta\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u4f18\u5316\u5668\u7684\u7f3a\u9677\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2506.15993", "pdf": "https://arxiv.org/pdf/2506.15993", "abs": "https://arxiv.org/abs/2506.15993", "authors": ["Yiwei Yang", "Yusheng Zheng", "Tong Yu", "Andi Quinn"], "title": "HetGPU: The pursuit of making binary compatibility towards GPUs", "categories": ["cs.AR", "cs.DC"], "comment": null, "summary": "Heterogeneous GPU infrastructures present a binary compatibility challenge:\ncode compiled for one vendor's GPU will not run on another due to divergent\ninstruction sets, execution models, and driver stacks . We propose hetGPU, a\nnew system comprising a compiler, runtime, and abstraction layer that together\nenable a single GPU binary to execute on NVIDIA, AMD, Intel, and Tenstorrent\nhardware. The hetGPU compiler emits an architecture-agnostic GPU intermediate\nrepresentation (IR) and inserts metadata for managing execution state. The\nhetGPU runtime then dynamically translates this IR to the target GPU's native\ncode and provides a uniform abstraction of threads, memory, and\nsynchronization. Our design tackles key challenges: differing SIMT vs. MIMD\nexecution (warps on NVIDIA/AMD vs. many-core RISC-V on Tenstorrent), varied\ninstruction sets, scheduling and memory model discrepancies, and the need for\nstate serialization for live migration. We detail the hetGPU architecture,\nincluding the IR transformation pipeline, a state capture/reload mechanism for\nlive GPU migration, and an abstraction layer that bridges warp-centric and\ncore-centric designs. Preliminary evaluation demonstrates that unmodified GPU\nbinaries compiled with hetGPU can be migrated across disparate GPUs with\nminimal overhead, opening the door to vendor-agnostic GPU computing.", "AI": {"tldr": "hetGPU\u662f\u4e00\u4e2a\u65b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f16\u8bd1\u5668\u3001\u8fd0\u884c\u65f6\u548c\u62bd\u8c61\u5c42\uff0c\u4f7f\u5355\u4e00GPU\u4e8c\u8fdb\u5236\u6587\u4ef6\u80fd\u5728\u4e0d\u540c\u5382\u5546\u7684\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784GPU\u57fa\u7840\u8bbe\u65bd\u56e0\u6307\u4ee4\u96c6\u3001\u6267\u884c\u6a21\u578b\u548c\u9a71\u52a8\u5806\u6808\u4e0d\u540c\u5bfc\u81f4\u7684\u4e8c\u8fdb\u5236\u517c\u5bb9\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u67b6\u6784\u65e0\u5173\u7684GPU\u4e2d\u95f4\u8868\u793a\uff08IR\uff09\u548c\u8fd0\u884c\u65f6\u52a8\u6001\u7ffb\u8bd1\u6280\u672f\uff0c\u7edf\u4e00\u7ebf\u7a0b\u3001\u5185\u5b58\u548c\u540c\u6b65\u64cd\u4f5c\u7684\u62bd\u8c61\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u672a\u4fee\u6539\u7684GPU\u4e8c\u8fdb\u5236\u6587\u4ef6\u80fd\u5728\u4e0d\u540cGPU\u95f4\u8fc1\u79fb\uff0c\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "hetGPU\u4e3a\u5b9e\u73b0\u5382\u5546\u65e0\u5173\u7684GPU\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2506.16453", "pdf": "https://arxiv.org/pdf/2506.16453", "abs": "https://arxiv.org/abs/2506.16453", "authors": ["Buthayna AlMulla", "Maram Assi", "Safwat Hassan"], "title": "Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study", "categories": ["cs.SE"], "comment": "45 pages, 24 figures, 7 tables", "summary": "The release of ChatGPT in 2022 triggered a rapid surge in generative\nartificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread\nadoption, little is known about how end users perceive and evaluate these\nGen-AI functionalities in practice. In this work, we conduct a user-centered\nanalysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We\nintroduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,\nand Analysis), that enables the systematic extraction of user insights using\nprompt-based LLM techniques. First, we demonstrate the reliability of LLMs in\ntopic extraction, achieving 91% accuracy through five-shot prompting and\nnon-informative review filtering. Then, we apply this method to the informative\nreviews, identify the top 10 user-discussed topics (e.g., AI Performance,\nContent Quality, and Content Policy & Censorship) and analyze the key\nchallenges and emerging opportunities. Finally, we examine how these topics\nevolve over time, offering insight into shifting user expectations and\nengagement patterns with Gen-AI apps. Based on our findings and observations,\nwe present actionable implications for developers and researchers.", "AI": {"tldr": "\u5206\u6790ChatGPT\u5f15\u53d1\u7684Gen-AI\u79fb\u52a8\u5e94\u7528\u7528\u6237\u8bc4\u4ef7\uff0c\u63d0\u51faSARA\u65b9\u6cd5\u63d0\u53d6\u7528\u6237\u6d1e\u5bdf\uff0c\u5e76\u5c55\u793aLLM\u5728\u4e3b\u9898\u63d0\u53d6\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76Gen-AI\u5e94\u7528\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u7528\u6237\u611f\u77e5\u548c\u8bc4\u4ef7\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5SARA\u65b9\u6cd5\uff08\u9009\u62e9\u3001\u83b7\u53d6\u3001\u7cbe\u70bc\u3001\u5206\u6790\uff09\uff0c\u7ed3\u5408LLM\u6280\u672f\u5bf9676,066\u6761Google Play\u8bc4\u8bba\u8fdb\u884c\u5206\u6790\u3002", "result": "LLM\u5728\u4e3b\u9898\u63d0\u53d6\u4e2d\u51c6\u786e\u7387\u8fbe91%\uff0c\u8bc6\u522b\u51fa\u7528\u6237\u5173\u6ce8\u7684\u5341\u5927\u4e3b\u9898\uff08\u5982AI\u6027\u80fd\u3001\u5185\u5bb9\u8d28\u91cf\u7b49\uff09\uff0c\u5e76\u5206\u6790\u5176\u968f\u65f6\u95f4\u6f14\u53d8\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7528\u6237\u53cd\u9988\u7684\u5b9e\u9645\u5efa\u8bae\uff0c\u5e76\u63ed\u793a\u4e86\u7528\u6237\u671f\u671b\u7684\u53d8\u5316\u8d8b\u52bf\u3002"}}
{"id": "2506.08911", "pdf": "https://arxiv.org/pdf/2506.08911", "abs": "https://arxiv.org/abs/2506.08911", "authors": ["Petar Jaku\u0161", "Hrvoje D\u017eapo"], "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "categories": ["cs.HC", "cs.AR", "cs.LG", "cs.PF", "cs.SD"], "comment": "4 pages", "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms.", "AI": {"tldr": "\u5728NXP MCXN947\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u7684\u5b9e\u65f6\u5173\u952e\u8bcd\u8bc6\u522b\u7cfb\u7edf\uff0c\u7ed3\u5408MFCC\u7279\u5f81\u63d0\u53d6\u548cCNN\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u4f4e\u529f\u8017\u8bed\u97f3\u4ea4\u4e92\u53ef\u884c\u6027\u3002", "motivation": "\u65e8\u5728\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408MFCC\u7279\u5f81\u63d0\u53d6\u548cCNN\u5206\u7c7b\u5668\uff0c\u4f7f\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\u5927\u5c0f\u3002", "result": "\u5229\u7528NPU\u5b9e\u73b059\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c97.06%\u51c6\u786e\u7387\uff0c\u6a21\u578b\u5927\u5c0f30.58 KB\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u4f4e\u529f\u8017\u8bed\u97f3\u63a5\u53e3\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.16883", "pdf": "https://arxiv.org/pdf/2506.16883", "abs": "https://arxiv.org/abs/2506.16883", "authors": ["Christoph Jung", "C. F. Bolz-Tereick"], "title": "Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine", "categories": ["cs.PL"], "comment": null, "summary": "Compared to the more commonly used time-based profiling, allocation profiling\nprovides an alternate view of the execution of allocation heavy dynamically\ntyped languages. However, profiling every single allocation in a program is\nvery inefficient. We present a sampling allocation profiler that is deeply\nintegrated into the garbage collector of PyPy, a Python virtual machine. This\nintegration ensures tunable low overhead for the allocation profiler, which we\nmeasure and quantify. Enabling allocation sampling profiling with a sampling\nperiod of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over\nun-profiled regular execution.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728PyPy\u865a\u62df\u673a\u4e2d\u96c6\u6210\u62bd\u6837\u5206\u914d\u5206\u6790\u5668\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5206\u6790\u5f00\u9500\uff0c\u6700\u5927\u65f6\u95f4\u5f00\u9500\u4ec5\u4e3a25%\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u65f6\u95f4\u7684\u5206\u6790\u5728\u52a8\u6001\u7c7b\u578b\u8bed\u8a00\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5206\u914d\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728PyPy\u7684\u5783\u573e\u56de\u6536\u5668\u4e2d\u6df1\u5ea6\u96c6\u6210\u62bd\u6837\u5206\u914d\u5206\u6790\u5668\uff0c\u5b9e\u73b0\u5bf9\u5206\u914d\u7684\u62bd\u6837\u5206\u6790\u3002", "result": "\u4f7f\u75284 MB\u7684\u62bd\u6837\u5468\u671f\u65f6\uff0c\u6700\u5927\u65f6\u95f4\u5f00\u9500\u4e3a25%\uff0c\u663e\u8457\u4f18\u4e8e\u5168\u90e8\u5206\u914d\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u5f00\u9500\u7684\u5206\u914d\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u7c7b\u578b\u8bed\u8a00\u3002"}}
{"id": "2506.16808", "pdf": "https://arxiv.org/pdf/2506.16808", "abs": "https://arxiv.org/abs/2506.16808", "authors": ["Louis Royer", "Emmanuel Lavinal", "Emmanuel Chaput"], "title": "Using SRv6 to access Edge Applications in 5G Networks", "categories": ["cs.NI"], "comment": "CoNEXT 2023: The 19th International Conference on emerging Networking\n  EXperiments and Technologies, Paris, France", "summary": "With the emergence of Multi-Access Edge Computing in 5G and beyond, it has\nbecome essential for operators to optimize the data path for the end-user while\nensuring resources are used according to their policy. In this paper, we review\nexisting solutions to access edge resources, underline their limits, and\npropose the use of Segment Routing over IPv6 (SRv6) in a 5G/edge architecture.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e865G\u53ca\u4ee5\u540e\u7684\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u6570\u636e\u8def\u5f84\u4f18\u5316\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528SRv6\u6280\u672f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u77405G\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\u7684\u5174\u8d77\uff0c\u8fd0\u8425\u5546\u9700\u8981\u4f18\u5316\u6570\u636e\u8def\u5f84\u5e76\u786e\u4fdd\u8d44\u6e90\u4f7f\u7528\u7b26\u5408\u5176\u7b56\u7565\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u56de\u987e\u73b0\u6709\u8fb9\u7f18\u8d44\u6e90\u8bbf\u95ee\u65b9\u6848\uff0c\u6307\u51fa\u5176\u5c40\u9650\uff0c\u5e76\u63d0\u51fa\u57285G/\u8fb9\u7f18\u67b6\u6784\u4e2d\u4f7f\u7528SRv6\u3002", "result": "\u63d0\u51faSRv6\u4f5c\u4e3a\u4e00\u79cd\u6f5c\u5728\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SRv6\u6709\u671b\u89e3\u51b35G/\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u6570\u636e\u8def\u5f84\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2506.15759", "pdf": "https://arxiv.org/pdf/2506.15759", "abs": "https://arxiv.org/abs/2506.15759", "authors": ["Siyi Xie", "Hanxin Zhu", "Tianyu He", "Xin Li", "Zhibo Chen"], "title": "Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration", "categories": ["cs.SD", "cs.MM", "eess.AS"], "comment": "17 pages, 7 figures. Project page:\n  https://x-drunker.github.io/Sonic4D-project-page/", "summary": "Recent advancements in 4D generation have demonstrated its remarkable\ncapability in synthesizing photorealistic renderings of dynamic 3D scenes.\nHowever, despite achieving impressive visual performance, almost all existing\nmethods overlook the generation of spatial audio aligned with the corresponding\n4D scenes, posing a significant limitation to truly immersive audiovisual\nexperiences. To mitigate this issue, we propose Sonic4D, a novel framework that\nenables spatial audio generation for immersive exploration of 4D scenes.\nSpecifically, our method is composed of three stages: 1) To capture both the\ndynamic visual content and raw auditory information from a monocular video, we\nfirst employ pre-trained expert models to generate the 4D scene and its\ncorresponding monaural audio. 2) Subsequently, to transform the monaural audio\ninto spatial audio, we localize and track the sound sources within the 4D\nscene, where their 3D spatial coordinates at different timestamps are estimated\nvia a pixel-level visual grounding strategy. 3) Based on the estimated sound\nsource locations, we further synthesize plausible spatial audio that varies\nacross different viewpoints and timestamps using physics-based simulation.\nExtensive experiments have demonstrated that our proposed method generates\nrealistic spatial audio consistent with the synthesized 4D scene in a\ntraining-free manner, significantly enhancing the immersive experience for\nusers. Generated audio and video examples are available at\nhttps://x-drunker.github.io/Sonic4D-project-page.", "AI": {"tldr": "Sonic4D\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e0e4D\u573a\u666f\u5bf9\u9f50\u7684\u7a7a\u95f4\u97f3\u9891\uff0c\u586b\u8865\u4e86\u73b0\u67094D\u751f\u6210\u6280\u672f\u4e2d\u97f3\u9891\u751f\u6210\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002", "motivation": "\u73b0\u67094D\u751f\u6210\u6280\u672f\u867d\u7136\u80fd\u5408\u6210\u903c\u771f\u7684\u52a8\u60013D\u573a\u666f\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e0e\u573a\u666f\u5bf9\u9f50\u7684\u7a7a\u95f4\u97f3\u9891\u751f\u6210\uff0c\u9650\u5236\u4e86\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u771f\u5b9e\u6027\u3002", "method": "Sonic4D\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u4ece\u5355\u76ee\u89c6\u9891\u751f\u62104D\u573a\u666f\u548c\u5355\u58f0\u9053\u97f3\u9891\uff1b2\uff09\u901a\u8fc7\u89c6\u89c9\u5b9a\u4f4d\u7b56\u7565\u4f30\u8ba1\u58f0\u6e90\u76843D\u5750\u6807\uff1b3\uff09\u57fa\u4e8e\u7269\u7406\u6a21\u62df\u5408\u6210\u4e0e\u89c6\u89d2\u548c\u65f6\u95f4\u53d8\u5316\u7684\u7a7a\u95f4\u97f3\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSonic4D\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u4e0e4D\u573a\u666f\u4e00\u81f4\u7684\u771f\u5b9e\u7a7a\u95f4\u97f3\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u6c89\u6d78\u611f\u3002", "conclusion": "Sonic4D\u6210\u529f\u89e3\u51b3\u4e864D\u573a\u666f\u4e2d\u7a7a\u95f4\u97f3\u9891\u751f\u6210\u7684\u95ee\u9898\uff0c\u4e3a\u6c89\u6d78\u5f0f\u4f53\u9a8c\u63d0\u4f9b\u4e86\u91cd\u8981\u8865\u5145\u3002"}}
{"id": "2506.15815", "pdf": "https://arxiv.org/pdf/2506.15815", "abs": "https://arxiv.org/abs/2506.15815", "authors": ["Narayan Kandel", "Daljit Singh J. S. Dhillon"], "title": "GratNet: A Photorealistic Neural Shader for Diffractive Surfaces", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Structural coloration is commonly modeled using wave optics for reliable and\nphotorealistic rendering of natural, quasi-periodic and complex nanostructures.\nSuch models often rely on dense, preliminary or preprocessed data to accurately\ncapture the nuanced variations in diffractive surface reflectances. This heavy\ndata dependency warrants implicit neural representation which has not been\naddressed comprehensively in the current literature. In this paper, we present\na multi-layer perceptron (MLP) based method for data-driven rendering of\ndiffractive surfaces with high accuracy and efficiency. We primarily approach\nthis problem from a data compression perspective to devise a nuanced training\nand modeling method which is attuned to the domain and range characteristics of\ndiffractive reflectance datasets. Importantly, our approach avoids over-fitting\nand has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),\nStructural Similarity Index Measure (SSIM) and a flipping difference evaluator\n(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of\nthe ground-truth. In comparison to a recent state-of-the-art offline,\nwave-optical, forward modeling approach, our method reproduces subjectively\nsimilar results with significant performance gains. We reduce the memory\nfootprint of the raw datasets by two orders of magnitude in general. Lastly, we\ndepict the working of our method with actual surface renderings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u9ad8\u6548\u5730\u6e32\u67d3\u884d\u5c04\u8868\u9762\uff0c\u901a\u8fc7\u6570\u636e\u538b\u7f29\u89c6\u89d2\u89e3\u51b3\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u884d\u5c04\u8868\u9762\u6e32\u67d3\u6a21\u578b\u4f9d\u8d56\u5bc6\u96c6\u6216\u9884\u5904\u7406\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u5168\u9762\u63a2\u8ba8\uff0c\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528MLP\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u6e32\u67d3\uff0c\u4ece\u6570\u636e\u538b\u7f29\u89d2\u5ea6\u8bbe\u8ba1\u8bad\u7ec3\u548c\u5efa\u6a21\u65b9\u6cd5\uff0c\u907f\u514d\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u91cd\u91c7\u6837\u9c81\u68d2\u6027\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u539f\u59cb\u6570\u636e\u96c6\u7684\u5185\u5b58\u5360\u7528\uff08\u4e24\u4e2a\u6570\u91cf\u7ea7\uff09\uff0c\u5e76\u901a\u8fc7PSNR\u3001SSIM\u548cFLIP\u6307\u6807\u9a8c\u8bc1\u4e86\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6ce2\u5149\u5b66\u65b9\u6cd5\u4e3b\u89c2\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u884d\u5c04\u8868\u9762\u7684\u9ad8\u6548\u6e32\u67d3\u3002"}}
{"id": "2506.16775", "pdf": "https://arxiv.org/pdf/2506.16775", "abs": "https://arxiv.org/abs/2506.16775", "authors": ["Lina Gerlach", "Christof L\u00f6ding", "Erika \u00c1brah\u00e1m"], "title": "A Hyperlogic for Strategies in Stochastic Games (Extended Version)", "categories": ["cs.LO"], "comment": "Accepted for publication at QEST+FORMATS 2025", "summary": "We propose a probabilistic hyperlogic called HyperSt$^2$ that can express\nhyperproperties of strategies in turn-based stochastic games. To the best of\nour knowledge, HyperSt$^2$ is the first hyperlogic for stochastic games.\nHyperSt$^2$ can relate probabilities of several independent executions of\nstrategies in a stochastic game. For example, in HyperSt$^2$ it is natural to\nformalize optimality, i.e., to express that some strategy is better than all\nother strategies, or to express the existence of Nash equilibria. We\ninvestigate the expressivity of HyperSt$^2$ by comparing it to existing logics\nfor stochastic games, as well as existing hyperlogics. Though the\nmodel-checking problem for HyperSt$^2$ is in general undecidable, we show that\nit becomes decidable for bounded memory and is in EXPTIME and PSPACE-hard over\nmemoryless deterministic strategies, and we identify a fragment for which the\nmodel-checking problem is PSPACE-complete.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6982\u7387\u8d85\u903b\u8f91HyperSt\u00b2\uff0c\u7528\u4e8e\u8868\u8fbe\u968f\u673a\u6e38\u620f\u4e2d\u7b56\u7565\u7684\u8d85\u5c5e\u6027\uff0c\u9996\u6b21\u89e3\u51b3\u4e86\u6b64\u7c7b\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u6e38\u620f\u4e2d\u7b56\u7565\u7684\u8d85\u5c5e\u6027\u8868\u8fbe\uff0c\u586b\u8865\u73b0\u6709\u8d85\u903b\u8f91\u5728\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1HyperSt\u00b2\u903b\u8f91\uff0c\u652f\u6301\u72ec\u7acb\u7b56\u7565\u6267\u884c\u7684\u6982\u7387\u5173\u8054\uff0c\u5e76\u5206\u6790\u5176\u8868\u8fbe\u80fd\u529b\u4e0e\u6a21\u578b\u68c0\u6d4b\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86HyperSt\u00b2\u7684\u8868\u8fbe\u80fd\u529b\u4f18\u4e8e\u73b0\u6709\u903b\u8f91\uff0c\u867d\u6a21\u578b\u68c0\u6d4b\u95ee\u9898\u4e00\u822c\u4e0d\u53ef\u5224\u5b9a\uff0c\u4f46\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u89e3\u3002", "conclusion": "HyperSt\u00b2\u4e3a\u968f\u673a\u6e38\u620f\u7684\u7b56\u7565\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u7279\u5b9a\u6761\u4ef6\u4e0b\u5177\u6709\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.15883", "pdf": "https://arxiv.org/pdf/2506.15883", "abs": "https://arxiv.org/abs/2506.15883", "authors": ["Jonathan Zong", "Isabella Pedraza Pineros", "Mengzhu Katie Chen", "Daniel Hajas", "Arvind Satyanarayan"], "title": "Semantic Scaffolding: Augmenting Textual Structures with Domain-Specific Groupings for Accessible Data Exploration", "categories": ["cs.HC"], "comment": null, "summary": "Drawing connections between interesting groupings of data and their\nreal-world meaning is an important, yet difficult, part of encountering a new\ndataset. A lay reader might see an interesting visual pattern in a chart but\nlack the domain expertise to explain its meaning. Or, a reader might be\nfamiliar with a real-world concept but struggle to express it in terms of a\ndataset's fields. In response, we developed semantic scaffolding, a technique\nfor using domain-specific information from large language models (LLMs) to\nidentify, explain, and formalize semantically meaningful data groupings. We\npresent groupings in two ways: as semantic bins, which segment a field into\ndomain-specific intervals and categories; and data highlights, which annotate\nsubsets of data records with their real-world meaning. We demonstrate and\nevaluate this technique in Olli, an accessible visualization tool that\nexemplifies tensions around explicitly defining groupings while respecting the\nagency of readers to conduct independent data exploration. We conducted a study\nwith 15 blind and low-vision (BLV) users and found that readers used semantic\nscaffolds to quickly understand the meaning of the data, but were often also\ncritically aware of its influence on their interpretation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u811a\u624b\u67b6\u201d\u7684\u6280\u672f\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u6765\u8bc6\u522b\u548c\u89e3\u91ca\u6570\u636e\u5206\u7ec4\u7684\u610f\u4e49\u3002\u901a\u8fc7\u201c\u8bed\u4e49\u5206\u7bb1\u201d\u548c\u201c\u6570\u636e\u9ad8\u4eae\u201d\u4e24\u79cd\u65b9\u5f0f\u5c55\u793a\u6570\u636e\u5206\u7ec4\u7684\u8bed\u4e49\u542b\u4e49\uff0c\u5e76\u5728\u53ef\u89c6\u5316\u5de5\u5177Olli\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u7528\u6237\u5728\u63a2\u7d22\u65b0\u6570\u636e\u96c6\u65f6\uff0c\u5e38\u5e38\u96be\u4ee5\u7406\u89e3\u6570\u636e\u5206\u7ec4\u7684\u771f\u5b9e\u542b\u4e49\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u8ba9\u7528\u6237\u80fd\u5feb\u901f\u7406\u89e3\u6570\u636e\u80cc\u540e\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u8bed\u4e49\u811a\u624b\u67b6\u6280\u672f\u3002", "method": "\u4f7f\u7528LLMs\u63d0\u53d6\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u751f\u6210\u8bed\u4e49\u5206\u7bb1\uff08\u5c06\u5b57\u6bb5\u5212\u5206\u4e3a\u9886\u57df\u76f8\u5173\u7684\u533a\u95f4\u548c\u7c7b\u522b\uff09\u548c\u6570\u636e\u9ad8\u4eae\uff08\u4e3a\u6570\u636e\u5b50\u96c6\u6807\u6ce8\u73b0\u5b9e\u610f\u4e49\uff09\u3002\u8fd9\u79cd\u6280\u672f\u5728\u53ef\u89c6\u5316\u5de5\u5177Olli\u4e2d\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u5bf915\u540d\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u6237\u80fd\u501f\u52a9\u8bed\u4e49\u811a\u624b\u67b6\u5feb\u901f\u7406\u89e3\u6570\u636e\u542b\u4e49\uff0c\u4f46\u4e5f\u6ce8\u610f\u5230\u5176\u5bf9\u6570\u636e\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bed\u4e49\u811a\u624b\u67b6\u6709\u6548\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u6570\u636e\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9\u6570\u636e\u89e3\u91ca\u4e3b\u89c2\u6027\u7684\u53cd\u601d\u3002\u6280\u672f\u9700\u5e73\u8861\u660e\u786e\u5b9a\u4e49\u5206\u7ec4\u4e0e\u7528\u6237\u81ea\u4e3b\u63a2\u7d22\u7684\u9700\u6c42\u3002"}}
{"id": "2506.15986", "pdf": "https://arxiv.org/pdf/2506.15986", "abs": "https://arxiv.org/abs/2506.15986", "authors": ["Jiancheng Ruan", "Tingyang Chen", "Renchi Yang", "Xiangyu Ke", "Yunjun Gao"], "title": "Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive Awareness Capabilities", "categories": ["cs.DB", "cs.IR"], "comment": "Accecpted by KDD2025", "summary": "Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds\nextensive applications in databases, information retrieval, recommender\nsystems, etc. While graph-based methods have emerged as the leading solution\nfor ANNS due to their superior query performance, they still face several\nchallenges, such as struggling with local optima and redundant computations.\nThese issues arise because existing methods (i) fail to fully exploit the\ntopological information underlying the proximity graph G, and (ii) suffer from\nsevere distribution mismatches between the base data and queries in practice.\n  To this end, this paper proposes GATE, high-tier proximity Graph with\nAdaptive Topology and Query AwarEness, as a lightweight and adaptive module\natop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates\nthe critical problem to identify an optimal entry point in the proximity graph\nfor a given query, facilitating faster online search. By leveraging the\ninherent clusterability of high-dimensional data, GATE first extracts a small\nset of hub nodes V as candidate entry points. Then, resorting to a contrastive\nlearning-based two-tower model, GATE encodes both the structural semantics\nunderlying G and the query-relevant features into the latent representations of\nthese hub nodes V. A navigation graph index on V is further constructed to\nminimize the model inference overhead. Extensive experiments demonstrate that\nGATE achieves a 1.2-2.0X speed-up in query performance compared to\nstate-of-the-art graph-based indexes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGATE\u7684\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u6a21\u5757\uff0c\u7528\u4e8e\u52a0\u901f\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u5165\u53e3\u70b9\u9009\u62e9\u548c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u56fe\u7684ANNS\u65b9\u6cd5\u5728\u62d3\u6251\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u548c\u5206\u5e03\u5931\u914d\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGATE\u6a21\u5757\uff0c\u5305\u62ec\u63d0\u53d6\u4e2d\u5fc3\u8282\u70b9\u3001\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u7f16\u7801\u7ed3\u6784\u548c\u67e5\u8be2\u7279\u5f81\uff0c\u5e76\u6784\u5efa\u5bfc\u822a\u56fe\u7d22\u5f15\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGATE\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u67e5\u8be2\u6027\u80fd\u4e0a\u63d0\u5347\u4e861.2-2.0\u500d\u3002", "conclusion": "GATE\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u5165\u53e3\u70b9\u9009\u62e9\u548c\u9ad8\u6548\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4ANNS\u7684\u6548\u7387\u3002"}}
{"id": "2506.16235", "pdf": "https://arxiv.org/pdf/2506.16235", "abs": "https://arxiv.org/abs/2506.16235", "authors": ["Yisu Wang", "Xinjiao Li", "Ruilong Wu", "Huangxun Chen", "Dirk Kutscher"], "title": "NetSenseML: Network-Adaptive Compression for Efficient Distributed Machine Learning", "categories": ["cs.DC"], "comment": null, "summary": "Training large-scale distributed machine learning models imposes considerable\ndemands on network infrastructure, often resulting in sudden traffic spikes\nthat lead to congestion, increased latency, and reduced throughput, which would\nultimately affect convergence times and overall training performance. While\ngradient compression techniques are commonly employed to alleviate network\nload, they frequently compromise model accuracy due to the loss of gradient\ninformation.\n  This paper introduces NetSenseML, a novel network adaptive distributed deep\nlearning framework that dynamically adjusts quantization, pruning, and\ncompression strategies in response to real-time network conditions. By actively\nmonitoring network conditions, NetSenseML applies gradient compression only\nwhen network congestion negatively impacts convergence speed, thus effectively\nbalancing data payload reduction and model accuracy preservation.\n  Our approach ensures efficient resource usage by adapting reduction\ntechniques based on current network conditions, leading to shorter convergence\ntimes and improved training efficiency. We present the design of the NetSenseML\nadaptive data reduction function and experimental evaluations show that\nNetSenseML can improve training throughput by a factor of 1.55 to 9.84 times\ncompared to state-of-the-art compression-enabled systems for representative DDL\ntraining jobs in bandwidth-constrained conditions.", "AI": {"tldr": "NetSenseML\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u68af\u5ea6\u538b\u7f29\u7b56\u7565\u7684\u7f51\u7edc\u81ea\u9002\u5e94\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6839\u636e\u5b9e\u65f6\u7f51\u7edc\u6761\u4ef6\u5e73\u8861\u8d1f\u8f7d\u4e0e\u6a21\u578b\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u5bf9\u7f51\u7edc\u9700\u6c42\u9ad8\uff0c\u4f20\u7edf\u68af\u5ea6\u538b\u7f29\u6280\u672f\u867d\u51cf\u5c11\u8d1f\u8f7d\u4f46\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "NetSenseML\u52a8\u6001\u8c03\u6574\u91cf\u5316\u3001\u526a\u679d\u548c\u538b\u7f29\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4b\u7f51\u7edc\u6761\u4ef6\u4ec5\u5728\u5fc5\u8981\u65f6\u538b\u7f29\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNetSenseML\u5728\u5e26\u5bbd\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.55\u81f39.84\u500d\u3002", "conclusion": "NetSenseML\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u6536\u655b\u65f6\u95f4\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2506.16557", "pdf": "https://arxiv.org/pdf/2506.16557", "abs": "https://arxiv.org/abs/2506.16557", "authors": ["Hern\u00e1n Gagliardi", "Victor Braberman", "Sebastian Uchitel"], "title": "Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control", "categories": ["cs.SE"], "comment": "To be published in CAV25", "summary": "We present a compositional approach to controller synthesis of discrete event\nsystem controllers with linear temporal logic (LTL) goals. We exploit the\nmodular structure of the plant to be controlled, given as a set of labelled\ntransition systems (LTS), to mitigate state explosion that monolithic\napproaches to synthesis are prone to. Maximally permissive safe controllers are\niteratively built for subsets of the plant LTSs by solving weaker control\nproblems. Observational synthesis equivalence is used to reduce the size of the\ncontrolled subset of the plant by abstracting away local events. The result of\nsynthesis is also compositional, a set of controllers that when run in parallel\nensure the LTL goal. We implement synthesis in the MTSA tool for an expressive\nsubset of LTL, GR(1), and show it computes solutions to that can be up to 1000\ntimes larger than those that the monolithic approach can solve.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u65b9\u6cd5\u7684\u79bb\u6563\u4e8b\u4ef6\u7cfb\u7edf\u63a7\u5236\u5668\u5408\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u5757\u5316\u7ed3\u6784\u7f13\u89e3\u72b6\u6001\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6784\u5efa\u6700\u5927\u5316\u5141\u8bb8\u7684\u5b89\u5168\u63a7\u5236\u5668\u5b9e\u73b0\u76ee\u6807\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u6574\u4f53\u5408\u6210\u65b9\u6cd5\u6613\u4ea7\u751f\u7684\u72b6\u6001\u7206\u70b8\u95ee\u9898\uff0c\u63d0\u51fa\u5229\u7528\u6a21\u5757\u5316\u7ed3\u6784\u6765\u4f18\u5316\u63a7\u5236\u5668\u5408\u6210\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u89e3\u51b3\u8f83\u5f31\u7684\u63a7\u5236\u95ee\u9898\uff0c\u6784\u5efa\u6700\u5927\u5316\u5141\u8bb8\u7684\u5b89\u5168\u63a7\u5236\u5668\uff0c\u5e76\u5229\u7528\u89c2\u6d4b\u5408\u6210\u7b49\u4ef7\u6027\u62bd\u8c61\u5c40\u90e8\u4e8b\u4ef6\u3002", "result": "\u5728MTSA\u5de5\u5177\u4e2d\u5b9e\u73b0\u4e86\u5bf9GR(1)\u903b\u8f91\u7684\u5408\u6210\uff0c\u89e3\u51b3\u89c4\u6a21\u6bd4\u6574\u4f53\u65b9\u6cd5\u59271000\u500d\u7684\u95ee\u9898\u3002", "conclusion": "\u7ec4\u5408\u65b9\u6cd5\u7684\u63a7\u5236\u5668\u5408\u6210\u80fd\u663e\u8457\u63d0\u5347\u89e3\u51b3\u89c4\u6a21\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u7cfb\u7edf\u7684LTL\u76ee\u6807\u5b9e\u73b0\u3002"}}
{"id": "2506.16589", "pdf": "https://arxiv.org/pdf/2506.16589", "abs": "https://arxiv.org/abs/2506.16589", "authors": ["Tal Zeevi", "El\u00e9onore V. Lieffrig", "Lawrence H. Staib", "John A. Onofrey"], "title": "Spatially-Aware Evaluation of Segmentation Uncertainty", "categories": ["cs.CV", "cs.AI", "cs.PF", "stat.ML"], "comment": "Presented at the 4th Workshop on Uncertainty Quantification for\n  Computer Vision (CVPR 2025), June 11, 2025. This version is not included in\n  the official proceedings", "summary": "Uncertainty maps highlight unreliable regions in segmentation predictions.\nHowever, most uncertainty evaluation metrics treat voxels independently,\nignoring spatial context and anatomical structure. As a result, they may assign\nidentical scores to qualitatively distinct patterns (e.g., scattered vs.\nboundary-aligned uncertainty). We propose three spatially aware metrics that\nincorporate structural and boundary information and conduct a thorough\nvalidation on medical imaging data from the prostate zonal segmentation\nchallenge within the Medical Segmentation Decathlon. Our results demonstrate\nimproved alignment with clinically important factors and better discrimination\nbetween meaningful and spurious uncertainty patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u8003\u8651\u7a7a\u95f4\u7ed3\u6784\u7684\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\uff0c\u76f8\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u80fd\u533a\u5206\u6709\u610f\u4e49\u548c\u865a\u5047\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u6307\u6807\u5ffd\u7565\u4e86\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u89e3\u5256\u7ed3\u6784\uff0c\u5bfc\u81f4\u5bf9\u4e0d\u540c\u7684\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\uff08\u5982\u6563\u70b9\u72b6\u6216\u8fb9\u754c\u5bf9\u9f50\uff09\u8bc4\u5206\u76f8\u540c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e34\u5e8a\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u7ed3\u5408\u7ed3\u6784\u548c\u8fb9\u754c\u4fe1\u606f\u7684\u7a7a\u95f4\u611f\u77e5\u6307\u6807\uff0c\u5e76\u5728\u524d\u5217\u817a\u5206\u533a\u5206\u5272\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u65b0\u6307\u6807\u80fd\u66f4\u597d\u5730\u4e0e\u4e34\u5e8a\u91cd\u8981\u56e0\u7d20\u5bf9\u9f50\uff0c\u5e76\u6709\u6548\u533a\u5206\u6709\u610f\u4e49\u548c\u865a\u5047\u7684\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u3002", "conclusion": "\u7a7a\u95f4\u611f\u77e5\u6307\u6807\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u8bc4\u4f30\u3002"}}
{"id": "2506.16065", "pdf": "https://arxiv.org/pdf/2506.16065", "abs": "https://arxiv.org/abs/2506.16065", "authors": ["Geonho Hwang", "Wonyeol Lee", "Yeachan Park", "Sejun Park", "Feras Saad"], "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators", "categories": ["cs.LG", "cs.LO", "cs.PL"], "comment": "70 pages, 4 figures. Appearing in CAV 2025", "summary": "The classical universal approximation (UA) theorem for neural networks\nestablishes mild conditions under which a feedforward neural network can\napproximate a continuous function $f$ with arbitrary accuracy. A recent result\nshows that neural networks also enjoy a more general interval universal\napproximation (IUA) theorem, in the sense that the abstract interpretation\nsemantics of the network using the interval domain can approximate the direct\nimage map of $f$ (i.e., the result of applying $f$ to a set of inputs) with\narbitrary accuracy. These theorems, however, rest on the unrealistic assumption\nthat the neural network computes over infinitely precise real numbers, whereas\ntheir software implementations in practice compute over finite-precision\nfloating-point numbers. An open question is whether the IUA theorem still holds\nin the floating-point setting.\n  This paper introduces the first IUA theorem for floating-point neural\nnetworks that proves their remarkable ability to perfectly capture the direct\nimage map of any rounded target function $f$, showing no limits exist on their\nexpressiveness. Our IUA theorem in the floating-point setting exhibits material\ndifferences from the real-valued setting, which reflects the fundamental\ndistinctions between these two computational models. This theorem also implies\nsurprising corollaries, which include (i) the existence of provably robust\nfloating-point neural networks; and (ii) the computational completeness of the\nclass of straight-line programs that use only floating-point additions and\nmultiplications for the class of all floating-point programs that halt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9002\u7528\u4e8e\u6d6e\u70b9\u6570\u795e\u7ecf\u7f51\u7edc\uff08Floating-Point Neural Networks\uff09\u7684\u533a\u95f4\u901a\u7528\u8fd1\u4f3c\u5b9a\u7406\uff08IUA\uff09\uff0c\u8bc1\u660e\u5176\u5728\u6d6e\u70b9\u6570\u73af\u5883\u4e0b\u4ecd\u80fd\u5b8c\u7f8e\u903c\u8fd1\u76ee\u6807\u51fd\u6570\u7684\u76f4\u63a5\u56fe\u50cf\u6620\u5c04\u3002", "motivation": "\u4ee5\u5f80\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u8fd1\u4f3c\u5b9a\u7406\uff08UA\uff09\u548c\u533a\u95f4\u901a\u7528\u8fd1\u4f3c\u5b9a\u7406\uff08IUA\uff09\u57fa\u4e8e\u65e0\u9650\u7cbe\u5ea6\u5b9e\u6570\u7684\u5047\u8bbe\uff0c\u800c\u5b9e\u9645\u5b9e\u73b0\u4e2d\u795e\u7ecf\u7f51\u7edc\u4f7f\u7528\u6709\u9650\u7cbe\u5ea6\u7684\u6d6e\u70b9\u6570\u8ba1\u7b97\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u6d6e\u70b9\u6570\u73af\u5883\u4e0bIUA\u5b9a\u7406\u662f\u5426\u4f9d\u7136\u6210\u7acb\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u6d6e\u70b9\u6570\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63d0\u51fa\u9996\u4e2a\u6d6e\u70b9\u6570\u73af\u5883\u4e0b\u7684IUA\u5b9a\u7406\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0e\u5b9e\u6570\u73af\u5883\u4e0b\u7684\u5dee\u5f02\u3002", "result": "\u8bc1\u660e\u4e86\u6d6e\u70b9\u6570\u795e\u7ecf\u7f51\u7edc\u5728\u6709\u9650\u7cbe\u5ea6\u4e0b\u4ecd\u80fd\u5b8c\u7f8e\u903c\u8fd1\u76ee\u6807\u51fd\u6570\u7684\u76f4\u63a5\u56fe\u50cf\u6620\u5c04\uff0c\u4e14\u65e0\u8868\u8fbe\u80fd\u529b\u7684\u9650\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u5f97\u51fa\u4e24\u4e2a\u91cd\u8981\u63a8\u8bba\uff1a\u5b58\u5728\u53ef\u8bc1\u660e\u9c81\u68d2\u7684\u6d6e\u70b9\u6570\u795e\u7ecf\u7f51\u7edc\uff1b\u4ee5\u53ca\u4ec5\u4f7f\u7528\u6d6e\u70b9\u6570\u52a0\u6cd5\u548c\u4e58\u6cd5\u7684\u76f4\u7ebf\u7a0b\u5e8f\u5bf9\u4e8e\u6240\u6709\u7ec8\u6b62\u7684\u6d6e\u70b9\u6570\u7a0b\u5e8f\u5177\u6709\u8ba1\u7b97\u5b8c\u5907\u6027\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u6d6e\u70b9\u6570\u73af\u5883\u4e0bIUA\u5b9a\u7406\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u6d6e\u70b9\u6570\u795e\u7ecf\u7f51\u7edc\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u80fd\u529b\u3002"}}
{"id": "2506.16914", "pdf": "https://arxiv.org/pdf/2506.16914", "abs": "https://arxiv.org/abs/2506.16914", "authors": ["Lukas Wildberger", "Anja Hamscher", "Jens B. Schmitt"], "title": "Minimal Per-Flow Backlog Bounds at an Aggregate FIFO Server under Piecewise-Linear Arrival Curves", "categories": ["cs.NI"], "comment": null, "summary": "Network Calculus (NC) is a versatile methodology based on min-plus algebra to\nderive worst-case per-flow performance bounds in networked systems with many\nconcurrent flows. In particular, NC can analyze many scheduling disciplines;\nyet, somewhat surprisingly, an aggregate FIFO server is a notoriously hard case\ndue to its min-plus non-linearity. A resort is to represent the FIFO residual\nservice by a family of functions with a free parameter instead of just a single\ncurve. For simple token-bucket arrival curves, literature provides optimal\nchoices for that free parameter to minimize delay and backlog bounds. In this\npaper, we tackle the challenge of more general arrival curves than just token\nbuckets. In particular, we derive residual service curves resulting in minimal\nbacklog bounds for general piecewise-linear arrival curves. To that end, we\nfirst show that a backlog bound can always be calculated at a breakpoint of\neither the arrival curve of the flow of interest or its residual service curve.\nFurther, we define a set of curves that characterize the backlog for a fixed\nbreakpoint, depending on the free parameter of the residual service curve. We\nshow that the backlog-minimizing residual service curve family parameter\ncorresponds to the largest intersection of those curves with the arrival curve.\nIn more complex scenarios finding this largest intersection can become\ninefficient as the search space grows in the number of flows. Therefore, we\npresent an efficient heuristic that finds, in many cases, the optimal parameter\nor at least a close conservative approximation. This heuristic is evaluated in\nterms of accuracy and execution time. Finally, we utilize these\nbacklog-minimizing residual service curves to enhance the DiscoDNC tool and\nobserve considerable reductions in the corresponding backlog bounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5e7f\u4e49\u5230\u8fbe\u66f2\u7ebf\uff08\u4e0d\u4ec5\u4ec5\u662f\u4ee4\u724c\u6876\uff09\u7684FIFO\u6b8b\u4f59\u670d\u52a1\u66f2\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u81ea\u7531\u53c2\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u79ef\u538b\u8fb9\u754c\u3002", "motivation": "\u7f51\u7edc\u6f14\u7b97\uff08NC\uff09\u5728\u5206\u6790FIFO\u670d\u52a1\u5668\u65f6\uff0c\u7531\u4e8emin-plus\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5e7f\u4e49\u5230\u8fbe\u66f2\u7ebf\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u4f18\u7684\u6b8b\u4f59\u670d\u52a1\u66f2\u7ebf\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc1\u660e\u4e86\u79ef\u538b\u8fb9\u754c\u53ef\u5728\u5230\u8fbe\u66f2\u7ebf\u6216\u6b8b\u4f59\u670d\u52a1\u66f2\u7ebf\u7684\u65ad\u70b9\u5904\u8ba1\u7b97\u3002\u968f\u540e\u5b9a\u4e49\u4e86\u4e00\u7ec4\u66f2\u7ebf\uff0c\u63cf\u8ff0\u79ef\u538b\u4e0e\u81ea\u7531\u53c2\u6570\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u4ea4\u70b9\u786e\u5b9a\u6700\u4f18\u53c2\u6570\u3002\u9488\u5bf9\u590d\u6742\u573a\u666f\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u79ef\u538b\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7DiscoDNC\u5de5\u5177\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u591f\u627e\u5230\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5e7f\u4e49\u5230\u8fbe\u66f2\u7ebf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u79ef\u538b\u8fb9\u754c\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u7b97\u6cd5\u89e3\u51b3\u4e86\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u7f51\u7edc\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.16273", "pdf": "https://arxiv.org/pdf/2506.16273", "abs": "https://arxiv.org/abs/2506.16273", "authors": ["Xin Jiang", "Meiqi Cao", "Hao Tang", "Fei Shen", "Zechao Li"], "title": "Fine-grained Image Retrieval via Dual-Vision Adaptation", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Fine-Grained Image Retrieval~(FGIR) faces challenges in learning\ndiscriminative visual representations to retrieve images with similar\nfine-grained features. Current leading FGIR solutions typically follow two\nregimes: enforce pairwise similarity constraints in the semantic embedding\nspace, or incorporate a localization sub-network to fine-tune the entire model.\nHowever, such two regimes tend to overfit the training data while forgetting\nthe knowledge gained from large-scale pre-training, thus reducing their\ngeneralization ability. In this paper, we propose a Dual-Vision Adaptation\n(DVA) approach for FGIR, which guides the frozen pre-trained model to perform\nFGIR through collaborative sample and feature adaptation. Specifically, we\ndesign Object-Perceptual Adaptation, which modifies input samples to help the\npre-trained model perceive critical objects and elements within objects that\nare helpful for category prediction. Meanwhile, we propose In-Context\nAdaptation, which introduces a small set of parameters for feature adaptation\nwithout modifying the pre-trained parameters. This makes the FGIR task using\nthese adjusted features closer to the task solved during the pre-training.\nAdditionally, to balance retrieval efficiency and performance, we propose\nDiscrimination Perception Transfer to transfer the discriminative knowledge in\nthe object-perceptual adaptation to the image encoder using the knowledge\ndistillation mechanism. Extensive experiments show that DVA has fewer learnable\nparameters and performs well on three in-distribution and three\nout-of-distribution fine-grained datasets.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDVA\u7684\u53cc\u89c6\u89c9\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u6837\u672c\u548c\u7279\u5f81\u7684\u534f\u540c\u9002\u5e94\u6539\u8fdb\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\uff0c\u907f\u514d\u8fc7\u62df\u5408\u5e76\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002DVA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DVA\u5305\u542b\u5bf9\u8c61\u611f\u77e5\u9002\u5e94\uff08\u4fee\u6539\u8f93\u5165\u6837\u672c\uff09\u548c\u4e0a\u4e0b\u6587\u5185\u9002\u5e94\uff08\u8c03\u6574\u5c11\u91cf\u53c2\u6570\uff09\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDVA\u53c2\u6570\u5c11\uff0c\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DVA\u901a\u8fc7\u534f\u540c\u9002\u5e94\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.15821", "pdf": "https://arxiv.org/pdf/2506.15821", "abs": "https://arxiv.org/abs/2506.15821", "authors": ["Pham Khai Nguyen Do", "Bao Nguyen Tran", "Nam Nguyen", "Duc Dung Nguyen"], "title": "VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal", "categories": ["cs.GR", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Recent advances in Novel View Synthesis (NVS) and 3D generation have\nsignificantly improved editing tasks, with a primary emphasis on maintaining\ncross-view consistency throughout the generative process. Contemporary methods\ntypically address this challenge using a dual-strategy framework: performing\nconsistent 2D inpainting across all views guided by embedded priors either\nexplicitly in pixel space or implicitly in latent space; and conducting 3D\nreconstruction with additional consistency guidance. Previous strategies, in\nparticular, often require an initial 3D reconstruction phase to establish\ngeometric structure, introducing considerable computational overhead. Even with\nthe added cost, the resulting reconstruction quality often remains suboptimal.\nIn this paper, we present VEIGAR, a computationally efficient framework that\noutperforms existing methods without relying on an initial reconstruction\nphase. VEIGAR leverages a lightweight foundation model to reliably align priors\nexplicitly in the pixel space. In addition, we introduce a novel supervision\nstrategy based on scale-invariant depth loss, which removes the need for\ntraditional scale-and-shift operations in monocular depth regularization.\nThrough extensive experimentation, VEIGAR establishes a new state-of-the-art\nbenchmark in reconstruction quality and cross-view consistency, while achieving\na threefold reduction in training time compared to the fastest existing method,\nhighlighting its superior balance of efficiency and effectiveness.", "AI": {"tldr": "VEIGAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b0\u89c6\u56fe\u5408\u6210\u6846\u67b6\uff0c\u65e0\u9700\u521d\u59cb3D\u91cd\u5efa\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u521d\u59cb3D\u91cd\u5efa\u9636\u6bb5\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u91cd\u5efa\u8d28\u91cf\u4e0d\u7406\u60f3\u3002VEIGAR\u65e8\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57fa\u7840\u6a21\u578b\u548c\u65b0\u578b\u76d1\u7763\u7b56\u7565\u6d88\u9664\u8fd9\u4e00\u9700\u6c42\u3002", "method": "VEIGAR\u5229\u7528\u50cf\u7d20\u7a7a\u95f4\u4e2d\u7684\u663e\u5f0f\u5148\u9a8c\u5bf9\u9f50\u548c\u57fa\u4e8e\u5c3a\u5ea6\u4e0d\u53d8\u6df1\u5ea6\u635f\u5931\u7684\u65b0\u578b\u76d1\u7763\u7b56\u7565\uff0c\u907f\u514d\u4f20\u7edf\u7684\u5355\u76ee\u6df1\u5ea6\u6b63\u5219\u5316\u64cd\u4f5c\u3002", "result": "VEIGAR\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230\u65b0\u6700\u9ad8\u6c34\u5e73\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e3a\u73b0\u6709\u6700\u5feb\u65b9\u6cd5\u7684\u4e09\u5206\u4e4b\u4e00\u3002", "conclusion": "VEIGAR\u5c55\u793a\u4e86\u6548\u7387\u548c\u6548\u679c\u7684\u6700\u4f73\u5e73\u8861\uff0c\u4e3a3D\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15936", "pdf": "https://arxiv.org/pdf/2506.15936", "abs": "https://arxiv.org/abs/2506.15936", "authors": ["Yu-Ting Kao", "Yeong-Jar Chang", "Ying-Wei Tseng"], "title": "Mixed-Signal Quantum Circuit Design for Option Pricing Using Design Compiler", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Prior studies have largely focused on quantum algorithms, often reducing\nparallel computing designs to abstract models or overly simplified circuits.\nThis has contributed to the misconception that most applications are feasible\nonly through VLSI circuits and cannot be implemented using quantum circuits. To\nchallenge this view, we present a mixed-signal quantum circuit framework\nincorporating three novel methods that reduce circuit complexity and improve\nnoise tolerance. In a 12 qubit case study comparing our design with JP Morgan's\noption pricing circuit, we reduced the gate count from 4095 to 392, depth from\n2048 to 6, and error rate from 25.86\\% to 1.64\\%. Our design combines analog\nsimplicity with digital flexibility and synthesizability, demonstrating that\nquantum circuits can effectively leverage classical VLSI techniques, such as\nthose enabled by Synopsys Design Compiler to address current quantum design\nlimitations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u4fe1\u53f7\u91cf\u5b50\u7535\u8def\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u65b0\u65b9\u6cd5\u964d\u4f4e\u7535\u8def\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u566a\u58f0\u5bb9\u5fcd\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u5ff5\uff0c\u8bc1\u660e\u91cf\u5b50\u7535\u8def\u53ef\u4ee5\u7ed3\u5408\u7ecf\u5178VLSI\u6280\u672f\uff0c\u89e3\u51b3\u5f53\u524d\u91cf\u5b50\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u4fe1\u53f7\u91cf\u5b50\u7535\u8def\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u65b0\u65b9\u6cd5\uff1a\u964d\u4f4e\u7535\u8def\u590d\u6742\u5ea6\u3001\u63d0\u9ad8\u566a\u58f0\u5bb9\u5fcd\u5ea6\uff0c\u5e76\u7ed3\u5408\u6a21\u62df\u4e0e\u6570\u5b57\u4f18\u52bf\u3002", "result": "\u572812\u91cf\u5b50\u6bd4\u7279\u6848\u4f8b\u4e2d\uff0c\u95e8\u6570\u4ece4095\u964d\u81f3392\uff0c\u6df1\u5ea6\u4ece2048\u964d\u81f36\uff0c\u9519\u8bef\u7387\u4ece25.86%\u964d\u81f31.64%\u3002", "conclusion": "\u91cf\u5b50\u7535\u8def\u53ef\u4ee5\u6709\u6548\u5229\u7528\u7ecf\u5178VLSI\u6280\u672f\uff0c\u5c55\u793a\u51fa\u5728\u7075\u6d3b\u6027\u548c\u53ef\u5408\u6210\u6027\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.17142", "pdf": "https://arxiv.org/pdf/2506.17142", "abs": "https://arxiv.org/abs/2506.17142", "authors": ["Adam Bjorndahl", "Philip Sink"], "title": "A Note on Proper Relational Structures", "categories": ["cs.LO", "math.LO", "03B42", "F.4.1"], "comment": null, "summary": "In this note we provide an algorithm for translating relational structures\ninto \"proper\" relational structures, i.e., those such that there is no pair of\nworlds w and u such that w is accessible from u for every agent. In particular,\nour method of translation preserves many classical properties of relational\nstructures, such as transitivity and the Euclidean property. As a result, this\nmethod of translation has many applications in the literature on Simplicial\nSemantics for modal logic, where the creation of proper canonical relational\nstructures is a common step in proofs of completeness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u5c06\u5173\u7cfb\u7ed3\u6784\u8f6c\u6362\u4e3a\u201c\u9002\u5f53\u201d\u5173\u7cfb\u7ed3\u6784\uff0c\u907f\u514d\u67d0\u4e9b\u5197\u4f59\u5173\u7cfb\u3002", "motivation": "\u5728\u5904\u7406\u6a21\u6001\u903b\u8f91\u7684Simplicial\u8bed\u4e49\u65f6\uff0c\u9700\u8981\u907f\u514d\u5173\u7cfb\u7ed3\u6784\u4e2d\u7684\u5197\u4f59\u5173\u7cfb\uff0c\u4ee5\u7b80\u5316\u8bc1\u660e\u8fc7\u7a0b\u3002", "method": "\u63d0\u4f9b\u4e00\u79cd\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u4fdd\u6301\u5173\u7cfb\u7ed3\u6784\u7684\u7ecf\u5178\u6027\u8d28\uff08\u5982\u4f20\u9012\u6027\u548c\u6b27\u51e0\u91cc\u5f97\u6027\uff09\u3002", "result": "\u7ffb\u8bd1\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u6a21\u6001\u903b\u8f91\u7684\u5b8c\u6574\u6027\u8bc1\u660e\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5173\u7cfb\u7ed3\u6784\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u9002\u7528\u4e8eSimplicial\u8bed\u4e49\u7684\u7814\u7a76\u3002"}}
{"id": "2506.16008", "pdf": "https://arxiv.org/pdf/2506.16008", "abs": "https://arxiv.org/abs/2506.16008", "authors": ["Yuichiro Fujimoto"], "title": "ChatAR: Conversation Support using Large Language Model and Augmented Reality", "categories": ["cs.HC"], "comment": null, "summary": "Engaging in smooth conversations with others is a crucial social skill.\nHowever, differences in knowledge between conversation participants can\nsometimes hinder effective communication. To tackle this issue, this study\nproposes a real-time support system that integrates head-mounted display\n(HMD)-based augmented reality (AR) technology with large language models\n(LLMs). This system facilitates conversation by recognizing keywords during\ndialogue, generating relevant information using the LLM, reformatting it, and\npresenting it to the user via the HMD. A significant issue with this system is\nthat the user's eye movements may reveal to the conversation partner that they\nare reading the displayed text. This study also proposes a method for\npresenting information that takes into account appropriate eye movements during\nconversation. Two experiments were conducted to evaluate the effectiveness of\nthe proposed system. The first experiment revealed that the proposed\ninformation presentation method reduces the likelihood of the conversation\npartner noticing that the user is reading the displayed text. The second\nexperiment demonstrated that the proposed method led to a more balanced speech\nratio between the user and the conversation partner, as well as a increase in\nthe perceived excitement of the conversation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408HMD\u548cLLM\u7684\u5b9e\u65f6\u5bf9\u8bdd\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7AR\u6280\u672f\u51cf\u5c11\u77e5\u8bc6\u5dee\u5f02\u5e26\u6765\u7684\u6c9f\u901a\u969c\u788d\uff0c\u5e76\u4f18\u5316\u4e86\u4fe1\u606f\u5c55\u793a\u65b9\u5f0f\u4ee5\u907f\u514d\u88ab\u5bf9\u8bdd\u4f19\u4f34\u5bdf\u89c9\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5bf9\u8bdd\u4e2d\u56e0\u77e5\u8bc6\u5dee\u5f02\u5bfc\u81f4\u7684\u6c9f\u901a\u969c\u788d\uff0c\u5e76\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6539\u5584\u5bf9\u8bdd\u4f53\u9a8c\u3002", "method": "\u7ed3\u5408HMD\u548cLLM\uff0c\u5b9e\u65f6\u8bc6\u522b\u5173\u952e\u8bcd\u3001\u751f\u6210\u5e76\u5c55\u793a\u4fe1\u606f\uff0c\u4f18\u5316\u4fe1\u606f\u5c55\u793a\u65b9\u5f0f\u4ee5\u51cf\u5c11\u5bdf\u89c9\u6027\u3002", "result": "\u4fe1\u606f\u5c55\u793a\u65b9\u5f0f\u964d\u4f4e\u88ab\u5bdf\u89c9\u7684\u98ce\u9669\uff0c\u5e73\u8861\u4e86\u5bf9\u8bdd\u53cc\u65b9\u7684\u53d1\u8a00\u6bd4\u4f8b\uff0c\u5e76\u63d0\u5347\u5bf9\u8bdd\u7684\u5174\u594b\u611f\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301\u5bf9\u8bdd\uff0c\u4f18\u5316\u4fe1\u606f\u5c55\u793a\u65b9\u5f0f\u5bf9\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.16591", "pdf": "https://arxiv.org/pdf/2506.16591", "abs": "https://arxiv.org/abs/2506.16591", "authors": ["Manno Versluis", "Yizhuo Wu", "Chang Gao"], "title": "SparseDPD: A Sparse Neural Network-based Digital Predistortion FPGA Accelerator for RF Power Amplifier Linearization", "categories": ["cs.AR", "eess.SP"], "comment": "Accepted to FPL 2025", "summary": "Digital predistortion (DPD) is crucial for linearizing radio frequency (RF)\npower amplifiers (PAs), improving signal integrity and efficiency in wireless\nsystems. Neural network (NN)-based DPD methods surpass traditional polynomial\nmodels but face computational challenges limiting their practical deployment.\nThis paper introduces SparseDPD, an FPGA accelerator employing a spatially\nsparse phase-normalized time-delay neural network (PNTDNN), optimized through\nunstructured pruning to reduce computational load without accuracy loss.\nImplemented on a Xilinx Zynq-7Z010 FPGA, SparseDPD operates at 170 MHz,\nachieving exceptional linearization performance (ACPR: -59.4 dBc, EVM: -54.0\ndBc, NMSE: -48.2 dB) with only 241 mW dynamic power, using 64 parameters with\n74% sparsity. This work demonstrates FPGA-based acceleration, making NN-based\nDPD practical and efficient for real-time wireless communication applications.\nCode is publicly available at https://github.com/MannoVersluis/SparseDPD.", "AI": {"tldr": "SparseDPD\u662f\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5b9e\u73b0\u4f4e\u529f\u8017\u9ad8\u6027\u80fd\u7684\u6570\u5b57\u9884\u5931\u771f\uff0c\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u9879\u5f0f\u6a21\u578b\u5728\u6570\u5b57\u9884\u5931\u771f(DPD)\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u867d\u4f18\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51faSparseDPD\uff0c\u91c7\u7528\u7a7a\u95f4\u7a00\u758f\u7684\u76f8\u4f4d\u5f52\u4e00\u5316\u65f6\u5ef6\u795e\u7ecf\u7f51\u7edc(PNTDNN)\uff0c\u901a\u8fc7\u975e\u7ed3\u6784\u5316\u526a\u679d\u4f18\u5316\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u5728Xilinx Zynq-7Z010 FPGA\u4e0a\u5b9e\u73b0170 MHz\u8fd0\u884c\uff0cACPR\u8fbe-59.4 dBc\uff0c\u52a8\u6001\u529f\u8017\u4ec5241 mW\uff0c\u53c2\u657064\u4e2a\u4e14\u7a00\u758f\u5ea674%\u3002", "conclusion": "SparseDPD\u8bc1\u660e\u4e86FPGA\u52a0\u901f\u7684\u53ef\u884c\u6027\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edcDPD\u5728\u5b9e\u65f6\u65e0\u7ebf\u901a\u4fe1\u4e2d\u66f4\u5b9e\u7528\u9ad8\u6548\u3002"}}
{"id": "2506.15987", "pdf": "https://arxiv.org/pdf/2506.15987", "abs": "https://arxiv.org/abs/2506.15987", "authors": ["Alireza Heidari", "Wei Zhang"], "title": "Filter-Centric Vector Indexing: Geometric Transformation for Efficient Filtered Vector Search", "categories": ["cs.DB", "math.MG"], "comment": "9 pages", "summary": "The explosive growth of vector search applications demands efficient handling\nof combined vector similarity and attribute filtering; a challenge where\ncurrent approaches force an unsatisfying choice between performance and\naccuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework\nthat transforms this fundamental trade-off by directly encoding filter\nconditions into the vector space through a mathematically principled\ntransformation $\\psi(v, f, \\alpha)$. Unlike specialized solutions, FCVI works\nwith any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical\nguarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI\nachieves 2.6-3.0 times higher throughput than state-of-the-art methods while\nmaintaining comparable recall. More remarkably, FCVI exhibits exceptional\nstability under distribution shifts; maintaining consistent performance when\nfilter patterns or vector distributions change, unlike traditional approaches\nthat degrade significantly. This combination of performance, compatibility, and\nresilience positions FCVI as an immediately applicable solution for production\nvector search systems requiring flexible filtering capabilities.", "AI": {"tldr": "FCVI\u6846\u67b6\u901a\u8fc7\u5c06\u8fc7\u6ee4\u6761\u4ef6\u76f4\u63a5\u7f16\u7801\u5230\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5411\u91cf\u641c\u7d22\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u5411\u91cf\u641c\u7d22\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u800cFCVI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "FCVI\u901a\u8fc7\u6570\u5b66\u53d8\u6362$\u03c8(v, f, \u03b1)$\u5c06\u8fc7\u6ee4\u6761\u4ef6\u7f16\u7801\u5230\u5411\u91cf\u7a7a\u95f4\uff0c\u517c\u5bb9\u73b0\u6709\u5411\u91cf\u7d22\u5f15\u3002", "result": "FCVI\u7684\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad82.6-3.0\u500d\uff0c\u4e14\u5728\u5206\u5e03\u53d8\u5316\u65f6\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "FCVI\u4e3a\u9700\u8981\u7075\u6d3b\u8fc7\u6ee4\u529f\u80fd\u7684\u751f\u4ea7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16350", "pdf": "https://arxiv.org/pdf/2506.16350", "abs": "https://arxiv.org/abs/2506.16350", "authors": ["Hen Kas-Sharir", "Gal Sela", "Erez Petrank"], "title": "A Study of Synchronization Methods for Concurrent Size", "categories": ["cs.DC"], "comment": "Code: https://github.com/henkassharir/ConcurrentSizeMethods", "summary": "The size of collections, maps, and data structures in general, constitutes a\nfundamental property. An implementation of the size method is required in most\nprogramming environments. Nevertheless, in a concurrent environment,\nintegrating a linearizable concurrent size introduces a noticeable overhead on\nall operations of the data structure, even when the size method is not invoked\nduring the execution. In this work we present a study of synchronization\nmethods in an attempt to improve the performance of the data structure. In\nparticular, we study a handshake technique that is commonly used with\nconcurrent garbage collection, an optimistic technique, and a lock-based\ntechnique. Evaluation against the state-of-the-art size methodology\ndemonstrates that the overhead can be significantly reduced by selecting the\nappropriate synchronization approach, but there is no one-size-fits-all method.\nDifferent scenarios call for different synchronization methods, as rigorously\nshown in this study. Nevertheless, our findings align with general trends in\nconcurrent computing. In scenarios characterized by low contention, optimistic\nand lock-based approaches work best, whereas under high contention, the most\neffective solutions are the handshake approach and the wait-free approach.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5e76\u53d1\u73af\u5883\u4e0b\u6570\u636e\u7ed3\u6784\u7684\u540c\u6b65\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u7ebf\u6027\u5316\u5e76\u53d1\u5927\u5c0f\u64cd\u4f5c\u7684\u6027\u80fd\u5f00\u9500\u3002", "motivation": "\u5728\u5e76\u53d1\u73af\u5883\u4e2d\uff0c\u7ebf\u6027\u5316\u5e76\u53d1\u5927\u5c0f\u7684\u5b9e\u73b0\u4f1a\u5728\u6240\u6709\u64cd\u4f5c\u4e2d\u5f15\u5165\u663e\u8457\u5f00\u9500\uff0c\u5373\u4f7f\u672a\u8c03\u7528\u5927\u5c0f\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u540c\u6b65\u65b9\u6cd5\uff1a\u63e1\u624b\u6280\u672f\uff08\u7528\u4e8e\u5e76\u53d1\u5783\u573e\u56de\u6536\uff09\u3001\u4e50\u89c2\u6280\u672f\u548c\u57fa\u4e8e\u9501\u7684\u6280\u672f\u3002", "result": "\u901a\u8fc7\u9009\u62e9\u9002\u5f53\u7684\u540c\u6b65\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u5f00\u9500\uff0c\u4f46\u65e0\u5355\u4e00\u6700\u4f73\u65b9\u6cd5\uff1b\u4f4e\u7ade\u4e89\u65f6\u4e50\u89c2\u548c\u9501\u65b9\u6cd5\u6700\u4f73\uff0c\u9ad8\u7ade\u4e89\u65f6\u63e1\u624b\u548c\u65e0\u7b49\u5f85\u65b9\u6cd5\u66f4\u4f18\u3002", "conclusion": "\u4e0d\u540c\u573a\u666f\u9700\u8981\u4e0d\u540c\u7684\u540c\u6b65\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u4e0e\u5e76\u53d1\u8ba1\u7b97\u7684\u666e\u904d\u8d8b\u52bf\u4e00\u81f4\u3002"}}
{"id": "2506.16586", "pdf": "https://arxiv.org/pdf/2506.16586", "abs": "https://arxiv.org/abs/2506.16586", "authors": ["Ihor Pysmennyi", "Roman Kyslyi", "Kyrylo Kleshch"], "title": "AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "Traditional quality assurance (QA) methods face significant challenges in\naddressing the complexity, scale, and rapid iteration cycles of modern software\nsystems and are strained by limited resources available, leading to substantial\ncosts associated with poor quality. The object of this research is the Quality\nAssurance processes for modern distributed software applications. The subject\nof the research is the assessment of the benefits, challenges, and prospects of\nintegrating modern AI-oriented tools into quality assurance processes. We\nperformed comprehensive analysis of implications on both verification and\nvalidation processes covering exploratory test analyses, equivalence\npartitioning and boundary analyses, metamorphic testing, finding\ninconsistencies in acceptance criteria (AC), static analyses, test case\ngeneration, unit test generation, test suit optimization and assessment, end to\nend scenario execution. End to end regression of sample enterprise application\nutilizing AI-agents over generated test scenarios was implemented as a proof of\nconcept highlighting practical use of the study. The results, with only 8.3%\nflaky executions of generated test cases, indicate significant potential for\nthe proposed approaches. However, the study also identified substantial\nchallenges for practical adoption concerning generation of semantically\nidentical coverage, \"black box\" nature and lack of explainability from\nstate-of-the-art Large Language Models (LLMs), the tendency to correct mutated\ntest cases to match expected results, underscoring the necessity for thorough\nverification of both generated artifacts and test execution results. The\nresearch demonstrates AI's transformative potential for QA but highlights the\nimportance of a strategic approach to implementing these technologies,\nconsidering the identified limitations and the need for developing appropriate\nverification methodologies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5c06AI\u5de5\u5177\u96c6\u6210\u5230\u73b0\u4ee3\u5206\u5e03\u5f0f\u8f6f\u4ef6QA\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5c55\u793a\u5176\u53d8\u9769\u6027\u4f46\u9700\u6218\u7565\u5b9e\u65bd\u3002", "motivation": "\u4f20\u7edfQA\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u8f6f\u4ef6\u7684\u590d\u6742\u6027\u3001\u89c4\u6a21\u548c\u5feb\u901f\u8fed\u4ee3\uff0cAI\u5de5\u5177\u7684\u6574\u5408\u53ef\u80fd\u63d0\u5347\u6548\u7387\u3002", "method": "\u7efc\u5408\u5206\u6790\u4e86AI\u5de5\u5177\u5bf9\u9a8c\u8bc1\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u591a\u79cd\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5c55\u793a\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684\u5931\u8d25\u7387\u4ec5\u4e3a8.3%\uff0c\u4f46\u53d1\u73b0\u8bed\u4e49\u8986\u76d6\u3001\u9ed1\u76d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7b49\u5b9e\u9645\u6311\u6218\u3002", "conclusion": "AI\u5bf9QA\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u5c40\u9650\u6027\u548c\u5f00\u53d1\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2506.16676", "pdf": "https://arxiv.org/pdf/2506.16676", "abs": "https://arxiv.org/abs/2506.16676", "authors": ["Mark F. Adams", "Jin Chen", "Benjamin Sturdevant"], "title": "Fast solvers for Tokamak fluid models with PETSC -- Part I", "categories": ["physics.plasm-ph", "cs.PF"], "comment": null, "summary": "This report develops the first step in adding multigrid solvers to scientific\nand engineering-relevant magnetohydrodynamics (MHD) models of Tokamaks. These\nmodels are characterized by a distinguished direction in the toroidal\ncoordinate that is partially aligned with the magnetic guide field, which\ndominates the plasma dynamics. All Tokamak models exploit this structure, for\nexample, NIMROD (https://nimrodteam.org/) uses $2D$, unstructured, high-order\nfinite elements in the poloidal plane with Fourier modes in the toroidal\ncoordinate, and the $3D$, extended MHD code M3D-C1\n(https://w3.pppl.gov/~nferraro/m3dc1.html) uses $2D$, unstructured $C^1$\nelements in the poloidal plane with cubic Hermite functions in the toroidal\ndirection. This structure suggests adding toroidal semi-coarsening multigrid to\nthe existing solver and thereby reducing reliance on direct solvers, which do\nnot scale optimally and are not well suited to modern hardware that demands\nextreme levels of parallelism. This report focuses on the velocity solve in\nM3D-C1, using the PETSC -- the Portable, Extensible Toolkit for Scientific\nComputation -- numerical library (https://petsc.org), and shows that with\nlittle new application code, one-dimensional multigrid is about $5x$ faster\nthan the existing one-level method on an MHD disruption, with runaway\nelectrons, test problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5728\u6258\u5361\u9a6c\u514b\u78c1\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u4e2d\u5f15\u5165\u591a\u91cd\u7f51\u683c\u6c42\u89e3\u5668\uff0c\u4ee5\u66ff\u4ee3\u76f4\u63a5\u6c42\u89e3\u5668\u3002", "motivation": "\u76f4\u63a5\u6c42\u89e3\u5668\u5728\u73b0\u4ee3\u786c\u4ef6\u4e0a\u5e76\u884c\u6027\u4e0d\u8db3\u4e14\u6548\u7387\u4e0d\u9ad8\uff0c\u4e3a\u6b64\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5728M3D-C1\u4ee3\u7801\u4e2d\uff0c\u5229\u7528PETSC\u5e93\u5b9e\u73b0\u4e00\u7ef4\u591a\u7ea7\u7f51\u683c\u6c42\u89e3\u5668\uff0c\u7279\u522b\u5173\u6ce8\u901f\u5ea6\u6c42\u89e3\u90e8\u5206\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u6bd4\u539f\u6709\u65b9\u6cd5\u5feb\u7ea65\u500d\u3002", "conclusion": "\u591a\u7ea7\u7f51\u683c\u6c42\u89e3\u5668\u80fd\u663e\u8457\u63d0\u5347\u6c42\u89e3\u6548\u7387\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u786c\u4ef6\u9700\u6c42\u3002"}}
{"id": "2506.17063", "pdf": "https://arxiv.org/pdf/2506.17063", "abs": "https://arxiv.org/abs/2506.17063", "authors": ["Samer Lahoud", "Kinda Khawam"], "title": "Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "The exponential growth of IoT devices presents critical challenges in\nbandwidth-constrained wireless networks, particularly regarding efficient data\ntransmission and privacy preservation. This paper presents a novel federated\nsemantic communication (SC) framework that enables collaborative training of\nbandwidth-efficient models for image reconstruction across heterogeneous IoT\ndevices. By leveraging SC principles to transmit only semantic features, our\napproach dramatically reduces communication overhead while preserving\nreconstruction quality. We address the fundamental challenge of client\nselection in federated learning environments where devices exhibit significant\ndisparities in dataset sizes and data distributions. Our framework implements\nthree distinct client selection strategies that explore different trade-offs\nbetween system performance and fairness in resource allocation. The system\nemploys an end-to-end SC architecture with semantic bottlenecks, coupled with a\nloss-based aggregation mechanism that naturally adapts to client heterogeneity.\nExperimental evaluation on image data demonstrates that while Utilitarian\nselection achieves the highest reconstruction quality, Proportional Fairness\nmaintains competitive performance while significantly reducing participation\ninequality and improving computational efficiency. These results establish that\nfederated SC can successfully balance reconstruction quality, resource\nefficiency, and fairness in heterogeneous IoT deployments, paving the way for\nsustainable and privacy-preserving edge intelligence applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u7269\u8054\u7f51\u8bbe\u5907\u95f4\u7684\u9ad8\u6548\u6570\u636e\u91cd\u5efa\uff0c\u901a\u8fc7\u8bed\u4e49\u7279\u5f81\u4f20\u8f93\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u63a2\u7d22\u4e86\u8bbe\u5907\u9009\u62e9\u7684\u516c\u5e73\u6027\u4e0e\u6027\u80fd\u5e73\u8861\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u6fc0\u589e\u5bfc\u81f4\u5e26\u5bbd\u53d7\u9650\u548c\u9690\u79c1\u4fdd\u62a4\u6311\u6218\uff0c\u9700\u89e3\u51b3\u5f02\u6784\u8bbe\u5907\u73af\u5883\u4e0b\u6570\u636e\u5206\u5e03\u4e0d\u5747\u548c\u8bbe\u5907\u9009\u62e9\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8054\u90a6\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7ed3\u5408\u4e09\u79cd\u8bbe\u5907\u9009\u62e9\u7b56\u7565\u548c\u5e26\u8bed\u4e49\u74f6\u9888\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u4ee5\u53ca\u57fa\u4e8e\u635f\u5931\u7684\u805a\u5408\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b9e\u7528\u4e3b\u4e49\u9009\u62e9\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u6700\u4f18\uff0c\u800c\u6bd4\u4f8b\u516c\u5e73\u6027\u5728\u51cf\u5c11\u53c2\u4e0e\u4e0d\u5e73\u7b49\u548c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8054\u90a6\u8bed\u4e49\u901a\u4fe1\u80fd\u5e73\u8861\u91cd\u5efa\u8d28\u91cf\u3001\u8d44\u6e90\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u4e3a\u53ef\u6301\u7eed\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u94fa\u8def\u3002"}}
{"id": "2506.16633", "pdf": "https://arxiv.org/pdf/2506.16633", "abs": "https://arxiv.org/abs/2506.16633", "authors": ["Fenghua Cheng", "Jinxiang Wang", "Sen Wang", "Zi Huang", "Xue Li"], "title": "GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "Multimodal reasoning is a process of understanding, integrating and inferring\ninformation across different data modalities. It has recently attracted surging\nacademic attention as a benchmark for Artificial Intelligence (AI). Although\nthere are various tasks for evaluating multimodal reasoning ability, they still\nhave limitations. Lack of reasoning on hierarchical visual clues at different\nlevels of granularity, e.g., local details and global context, is of little\ndiscussion, despite its frequent involvement in real scenarios. To bridge the\ngap, we introduce a novel and challenging task for multimodal reasoning, namely\nGeoGuess. Given a street view image, the task is to identify its location and\nprovide a detailed explanation. A system that succeeds in GeoGuess should be\nable to detect tiny visual clues, perceive the broader landscape, and associate\nwith vast geographic knowledge. Therefore, GeoGuess would require the ability\nto reason between hierarchical visual information and geographic knowledge. In\nthis work, we establish a benchmark for GeoGuess by introducing a specially\ncurated dataset GeoExplain which consists of\npanoramas-geocoordinates-explanation tuples. Additionally, we present a\nmultimodal and multilevel reasoning method, namely SightSense which can make\nprediction and generate comprehensive explanation based on hierarchy of visual\ninformation and external knowledge. Our analysis and experiments demonstrate\ntheir outstanding performance in GeoGuess.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGeoGuess\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\uff0c\u8981\u6c42\u6839\u636e\u8857\u666f\u56fe\u50cf\u5b9a\u4f4d\u5e76\u8be6\u7ec6\u89e3\u91ca\u4f4d\u7f6e\u3002\u4e3a\u89e3\u51b3\u73b0\u6709\u4efb\u52a1\u5728\u5c42\u6b21\u5316\u89c6\u89c9\u7ebf\u7d22\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5f15\u5165\u4e86\u6570\u636e\u96c6GeoExplain\u548c\u63a8\u7406\u65b9\u6cd5SightSense\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u5728\u5c42\u6b21\u5316\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\uff09\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800cGeoGuess\u4efb\u52a1\u901a\u8fc7\u7ed3\u5408\u591a\u5c42\u6b21\u89c6\u89c9\u4fe1\u606f\u4e0e\u5730\u7406\u77e5\u8bc6\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86GeoGuess\u4efb\u52a1\uff0c\u5e76\u521b\u5efa\u6570\u636e\u96c6GeoExplain\uff08\u5305\u542b\u8857\u666f-\u5750\u6807-\u89e3\u91ca\u4e09\u5143\u7ec4\uff09\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u591a\u7ea7\u63a8\u7406\u65b9\u6cd5SightSense\uff0c\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u5c42\u7ea7\u548c\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u9884\u6d4b\u4e0e\u89e3\u91ca\u3002", "result": "SightSense\u5728GeoGuess\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u591a\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "GeoGuess\u4efb\u52a1\u548cSightSense\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c24\u5176\u5728\u5c42\u6b21\u5316\u89c6\u89c9\u4fe1\u606f\u4e0e\u77e5\u8bc6\u5173\u8054\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.15860", "pdf": "https://arxiv.org/pdf/2506.15860", "abs": "https://arxiv.org/abs/2506.15860", "authors": ["Hasan Balci", "Augustin Luna"], "title": "User-Guided Force-Directed Graph Layout", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "Visual analysis of relational data is essential for many real-world analytics\ntasks, with layout quality being key to interpretability. However, existing\nlayout algorithms often require users to navigate complex parameters to express\ntheir intent. We present a user-guided force-directed layout approach that\nenables intuitive control through freehand sketching. Our method uses classical\nimage analysis techniques to extract structural information from sketches,\nwhich is then used to generate positional constraints that guide the layout\nprocess. We evaluate the approach on various real and synthetic graphs ranging\nfrom small to medium scale, demonstrating its ability to produce layouts\naligned with user expectations. An implementation of our method along with\ndocumentation and a demo page is freely available on GitHub at\nhttps://github.com/sciluna/uggly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u5bfc\u5411\u7684\u529b\u5bfc\u5411\u5e03\u5c40\u65b9\u6cd5\uff0c\u901a\u8fc7\u624b\u7ed8\u8349\u56fe\u76f4\u89c2\u63a7\u5236\u5e03\u5c40\uff0c\u63d0\u9ad8\u5173\u7cfb\u6570\u636e\u7684\u53ef\u89c6\u5316\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5e03\u5c40\u7b97\u6cd5\u9700\u8981\u590d\u6742\u53c2\u6570\u8bbe\u7f6e\uff0c\u7528\u6237\u610f\u56fe\u8868\u8fbe\u4e0d\u4fbf\uff0c\u56e0\u6b64\u7814\u7a76\u66f4\u76f4\u89c2\u7684\u63a7\u5236\u65b9\u5f0f\u3002", "method": "\u5229\u7528\u624b\u7ed8\u8349\u56fe\u63d0\u53d6\u7ed3\u6784\u4fe1\u606f\uff0c\u751f\u6210\u4f4d\u7f6e\u7ea6\u675f\u6307\u5bfc\u5e03\u5c40\u3002", "result": "\u5728\u591a\u79cd\u89c4\u6a21\u7684\u771f\u5b9e\u548c\u5408\u6210\u56fe\u4e0a\u9a8c\u8bc1\uff0c\u5e03\u5c40\u6548\u679c\u7b26\u5408\u7528\u6237\u9884\u671f\u3002", "conclusion": "\u65b9\u6cd5\u76f4\u89c2\u6709\u6548\uff0c\u5df2\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2506.16546", "pdf": "https://arxiv.org/pdf/2506.16546", "abs": "https://arxiv.org/abs/2506.16546", "authors": ["Liyang Yu", "Tianyi Wang", "Junfeng Jiao", "Fengwu Shan", "Hongqing Chu", "Bingzhao Gao"], "title": "BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures, 4 tables, accepted for IEEE Intelligent Vehicles\n  (IV) Symposium 2025", "summary": "In complex real-world traffic environments, autonomous vehicles (AVs) need to\ninteract with other traffic participants while making real-time and\nsafety-critical decisions accordingly. The unpredictability of human behaviors\nposes significant challenges, particularly in dynamic scenarios, such as\nmulti-lane highways and unsignalized T-intersections. To address this gap, we\ndesign a bi-level interaction decision-making algorithm (BIDA) that integrates\ninteractive Monte Carlo tree search (MCTS) with deep reinforcement learning\n(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs\nin dynamic key traffic scenarios. Specifically, we adopt three types of DRL\nalgorithms to construct a reliable value network and policy network, which\nguide the online deduction process of interactive MCTS by assisting in value\nupdate and node selection. Then, a dynamic trajectory planner and a trajectory\ntracking controller are designed and implemented in CARLA to ensure smooth\nexecution of planned maneuvers. Experimental evaluations demonstrate that our\nBIDA not only enhances interactive deduction and reduces computational costs,\nbut also outperforms other latest benchmarks, which exhibits superior safety,\nefficiency and interaction rationality under varying traffic conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4ea4\u4e92\u51b3\u7b56\u7b97\u6cd5\uff08BIDA\uff09\uff0c\u7ed3\u5408\u4ea4\u4e92\u5f0f\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u9700\u8981\u5b9e\u65f6\u3001\u5b89\u5168\u5730\u4e0e\u4eba\u7c7b\u4ea4\u901a\u53c2\u4e0e\u8005\u4ea4\u4e92\uff0c\u4f46\u4eba\u7c7b\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u79cdDRL\u7b97\u6cd5\u6784\u5efa\u53ef\u9760\u7684\u4ef7\u503c\u7f51\u7edc\u548c\u7b56\u7565\u7f51\u7edc\uff0c\u6307\u5bfc\u4ea4\u4e92\u5f0fMCTS\u7684\u5728\u7ebf\u63a8\u5bfc\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u8f68\u8ff9\u89c4\u5212\u5668\u548c\u8ddf\u8e2a\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBIDA\u4e0d\u4ec5\u63d0\u5347\u4e86\u4ea4\u4e92\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u8fd8\u5728\u5b89\u5168\u6027\u548c\u4ea4\u4e92\u5408\u7406\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u65b0\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "BIDA\u5728\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u4e92\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16015", "pdf": "https://arxiv.org/pdf/2506.16015", "abs": "https://arxiv.org/abs/2506.16015", "authors": ["Craig S. Wright"], "title": "Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LO", "math.LO", "68T27, 03B70, 68P20", "I.2.3; F.4.1; H.2.8"], "comment": "91 pages, 0 figures, includes mathematical appendix and formal\n  proofs. Designed as a foundational submission for a modular autonomous\n  epistemic reasoning system. Suitable for logic in computer science, AI\n  epistemology, and scientific informatics", "summary": "The exponential expansion of scientific literature has surpassed the\nepistemic processing capabilities of both human experts and current artificial\nintelligence systems. This paper introduces Bayesian Epistemology with Weighted\nAuthority (BEWA), a formally structured architecture that operationalises\nbelief as a dynamic, probabilistically coherent function over structured\nscientific claims. Each claim is contextualised, author-attributed, and\nevaluated through a system of replication scores, citation weighting, and\ntemporal decay. Belief updates are performed via evidence-conditioned Bayesian\ninference, contradiction processing, and epistemic decay mechanisms. The\narchitecture supports graph-based claim propagation, authorial credibility\nmodelling, cryptographic anchoring, and zero-knowledge audit verification. By\nformalising scientific reasoning into a computationally verifiable epistemic\nnetwork, BEWA advances the foundation for machine reasoning systems that\npromote truth utility, rational belief convergence, and audit-resilient\nintegrity across dynamic scientific domains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BEWA\uff08\u8d1d\u53f6\u65af\u8ba4\u8bc6\u8bba\u52a0\u6743\u6743\u5a01\uff09\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u6982\u7387\u4e00\u81f4\u6027\u548c\u52a8\u6001\u66f4\u65b0\u7684\u79d1\u5b66\u4e3b\u5f20\u5904\u7406\u79d1\u5b66\u6587\u732e\u7206\u70b8\u5f0f\u589e\u957f\u5e26\u6765\u7684\u8ba4\u77e5\u6311\u6218\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u8d85\u51fa\u4e86\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u8ba4\u77e5\u5904\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u7ed3\u6784\u5316\u548c\u8bc4\u4f30\u79d1\u5b66\u4e3b\u5f20\u3002", "method": "BEWA\u901a\u8fc7\u590d\u5236\u5206\u6570\u3001\u5f15\u7528\u52a0\u6743\u548c\u65f6\u95f4\u8870\u51cf\u5bf9\u79d1\u5b66\u4e3b\u5f20\u8fdb\u884c\u52a8\u6001\u8bc4\u4f30\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u3001\u77db\u76fe\u5904\u7406\u548c\u8ba4\u77e5\u8870\u51cf\u673a\u5236\u66f4\u65b0\u4fe1\u5ff5\u3002", "result": "BEWA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8ba1\u7b97\u9a8c\u8bc1\u7684\u8ba4\u8bc6\u8bba\u7f51\u7edc\uff0c\u652f\u6301\u79d1\u5b66\u63a8\u7406\u7684\u673a\u5668\u5316\uff0c\u4fc3\u8fdb\u4e86\u771f\u7406\u6548\u7528\u3001\u7406\u6027\u4fe1\u5ff5\u6536\u655b\u548c\u5ba1\u8ba1\u97e7\u6027\u3002", "conclusion": "BEWA\u4e3a\u5904\u7406\u52a8\u6001\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8ba1\u7b97\u57fa\u7840\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u548c\u79d1\u5b66\u5b8c\u6574\u6027\u3002"}}
{"id": "2506.16010", "pdf": "https://arxiv.org/pdf/2506.16010", "abs": "https://arxiv.org/abs/2506.16010", "authors": ["Xiangyang He", "Jiale Li", "Jiahao Chen", "Yang Yang", "Mingming Fan"], "title": "SimuPanel: A Novel Immersive Multi-Agent System to Simulate Interactive Expert Panel Discussion", "categories": ["cs.HC"], "comment": null, "summary": "Panel discussion allows the audience to learn different perspectives through\ninteractive discussions among experts moderated by a host and a Q&A session\nwith the audience. Despite its benefits, panel discussion in the real world is\ninaccessible to many who do not have the privilege to participate due to\ngeographical, financial, and time constraints. We present SimuPanel, which\nsimulates panel discussions among academic experts through LLM-based\nmulti-agent interaction. It enables users to define topics of interest for the\npanel, observe the expert discussion, engage in Q&A, and take notes. SimuPanel\nemploys a host-expert architecture where each panel member is simulated by an\nagent with specialized expertise, and the panel is visualized in an immersive\n3D environment to enhance engagement. Traditional dialogue generation struggles\nto capture the depth and interactivity of real-world panel discussions. To\naddress this limitation, we propose a novel multi-agent interaction framework\nthat simulates authentic panel dynamics by modeling reasoning strategies and\npersonas of experts grounded in multimedia sources. This framework enables\nagents to dynamically recall and contribute to the discussion based on past\nexperiences from diverse perspectives. Our technical evaluation and the user\nstudy with university students show that SimuPanel was able to simulate more\nin-depth discussions and engage participants to interact with and reflect on\nthe discussions. As a first step in this direction, we offer design\nimplications for future avenues to improve and harness the power of panel\ndiscussion for multimedia learning.", "AI": {"tldr": "SimuPanel \u662f\u4e00\u4e2a\u57fa\u4e8e LLM \u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u7cfb\u7edf\uff0c\u6a21\u62df\u5b66\u672f\u4e13\u5bb6\u7684\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u901a\u8fc7 3D \u73af\u5883\u589e\u5f3a\u53c2\u4e0e\u611f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ba8\u8bba\u7684\u5730\u7406\u3001\u65f6\u95f4\u548c\u7ecf\u6d4e\u9650\u5236\u3002\u901a\u8fc7\u6280\u672f\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5176\u6a21\u62df\u6df1\u5ea6\u8ba8\u8bba\u548c\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5c0f\u7ec4\u8ba8\u8bba\u53d7\u9650\u4e8e\u5730\u7406\u3001\u7ecf\u6d4e\u548c\u65f6\u95f4\u7b49\u56e0\u7d20\uff0c\u65e0\u6cd5\u666e\u53ca\u3002SimuPanel \u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6a21\u62df\u4e13\u5bb6\u8ba8\u8bba\uff0c\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u7684\u5b66\u4e60\u673a\u4f1a\u3002", "method": "\u91c7\u7528\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6846\u67b6\uff0c\u7ed3\u5408\u4e3b\u673a-\u4e13\u5bb6\u67b6\u6784\u548c 3D \u53ef\u89c6\u5316\u73af\u5883\uff0c\u6a21\u62df\u771f\u5b9e\u8ba8\u8bba\u52a8\u6001\uff0c\u652f\u6301\u7528\u6237\u4e92\u52a8\u4e0e\u7b14\u8bb0\u8bb0\u5f55\u3002", "result": "\u6280\u672f\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cSimuPanel \u80fd\u591f\u6a21\u62df\u66f4\u6df1\u5ea6\u7684\u8ba8\u8bba\u5e76\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u53cd\u601d\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5229\u7528\u5c0f\u7ec4\u8ba8\u8bba\u7684\u591a\u5a92\u4f53\u5b66\u4e60\u6f5c\u529b\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2506.16800", "pdf": "https://arxiv.org/pdf/2506.16800", "abs": "https://arxiv.org/abs/2506.16800", "authors": ["Hiroto Tagata", "Takashi Sato", "Hiromitsu Awano"], "title": "Lookup Table-based Multiplication-free All-digital DNN Accelerator Featuring Self-Synchronous Pipeline Accumulation", "categories": ["cs.AR"], "comment": null, "summary": "Deep neural networks (DNNs) have been widely applied in our society, yet\nreducing power consumption due to large-scale matrix computations remains a\ncritical challenge. MADDNESS is a known approach to improving energy efficiency\nby substituting matrix multiplication with table lookup operations. Previous\nresearch has employed large analog computing circuits to convert inputs into\nLUT addresses, which presents challenges to area efficiency and computational\naccuracy. This paper proposes a novel MADDNESS-based all-digital accelerator\nfeaturing a self-synchronous pipeline accumulator, resulting in a compact,\nenergy-efficient, and PVT-invariant computation. Post-layout simulation using a\ncommercial 22nm process showed that 2.5 times higher energy efficiency (174\nTOPS/W) and 5 times higher area efficiency (2.01 TOPS/mm2) can be achieved\ncompared to the conventional accelerator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMADDNESS\u7684\u5168\u6570\u5b57\u52a0\u901f\u5668\uff0c\u91c7\u7528\u81ea\u540c\u6b65\u6d41\u6c34\u7ebf\u7d2f\u52a0\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u91cf\u6548\u7387\u548c\u9762\u79ef\u6548\u7387\u3002", "motivation": "\u51cf\u5c11\u5927\u89c4\u6a21\u77e9\u9635\u8ba1\u7b97\u5e26\u6765\u7684\u9ad8\u80fd\u8017\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u7684MADDNESS\u65b9\u6cd5\u5b58\u5728\u9762\u79ef\u6548\u7387\u548c\u8ba1\u7b97\u7cbe\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8eMADDNESS\u7684\u5168\u6570\u5b57\u52a0\u901f\u5668\uff0c\u5229\u7528\u81ea\u540c\u6b65\u6d41\u6c34\u7ebf\u7d2f\u52a0\u5668\u6280\u672f\u3002", "result": "\u540e\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u80fd\u91cf\u6548\u7387\u63d0\u5347\u4e862.5\u500d\uff08174 TOPS/W\uff09\uff0c\u9762\u79ef\u6548\u7387\u63d0\u5347\u4e865\u500d\uff082.01 TOPS/mm\u00b2\uff09\u3002", "conclusion": "\u8be5\u5168\u6570\u5b57\u52a0\u901f\u5668\u5728\u80fd\u91cf\u6548\u7387\u548c\u9762\u79ef\u6548\u7387\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.16007", "pdf": "https://arxiv.org/pdf/2506.16007", "abs": "https://arxiv.org/abs/2506.16007", "authors": ["Peizhi Wu", "Rong Kang", "Tieying Zhang", "Jianjun Chen", "Ryan Marcus", "Zachary G. Ives"], "title": "Data-Agnostic Cardinality Learning from Imperfect Workloads", "categories": ["cs.DB", "cs.LG"], "comment": "14 pages. Technical Report (Extended Version)", "summary": "Cardinality estimation (CardEst) is a critical aspect of query optimization.\nTraditionally, it leverages statistics built directly over the data. However,\norganizational policies (e.g., regulatory compliance) may restrict global data\naccess. Fortunately, query-driven cardinality estimation can learn CardEst\nmodels using query workloads. However, existing query-driven models often\nrequire access to data or summaries for best performance, and they assume\nperfect training workloads with complete and balanced join templates (or join\ngraphs). Such assumptions rarely hold in real-world scenarios, in which join\ntemplates are incomplete and imbalanced. We present GRASP, a data-agnostic\ncardinality learning system designed to work under these real-world\nconstraints. GRASP's compositional design generalizes to unseen join templates\nand is robust to join template imbalance. It also introduces a new per-table\nCardEst model that handles value distribution shifts for range predicates, and\na novel learned count sketch model that captures join correlations across base\nrelations. Across three database instances, we demonstrate that GRASP\nconsistently outperforms existing query-driven models on imperfect workloads,\nboth in terms of estimation accuracy and query latency. Remarkably, GRASP\nachieves performance comparable to, or even surpassing, traditional approaches\nbuilt over the underlying data on the complex CEB-IMDb-full benchmark --\ndespite operating without any data access and using only 10% of all possible\njoin templates.", "AI": {"tldr": "GRASP\u662f\u4e00\u79cd\u6570\u636e\u65e0\u5173\u7684\u57fa\u6570\u4f30\u8ba1\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u4e2d\u4e0d\u5b8c\u6574\u548c\u4e0d\u5e73\u8861\u7684\u67e5\u8be2\u8d1f\u8f7d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u67e5\u8be2\u9a71\u52a8\u6a21\u578b\uff0c\u751a\u81f3\u63a5\u8fd1\u4f20\u7edf\u57fa\u4e8e\u6570\u636e\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u6570\u4f30\u8ba1\u4f9d\u8d56\u6570\u636e\u8bbf\u95ee\uff0c\u4f46\u5728\u5408\u89c4\u8981\u6c42\u4e0b\u53d7\u9650\uff1b\u73b0\u6709\u67e5\u8be2\u9a71\u52a8\u6a21\u578b\u9700\u8981\u6570\u636e\u6216\u5b8c\u7f8e\u8bad\u7ec3\u8d1f\u8f7d\uff0c\u800c\u771f\u5b9e\u573a\u666f\u5f80\u5f80\u4e0d\u6ee1\u8db3\u3002", "method": "GRASP\u91c7\u7528\u7ec4\u5408\u8bbe\u8ba1\uff0c\u5305\u62ec\u65b0\u6bcf\u8868\u57fa\u6570\u6a21\u578b\u5904\u7406\u533a\u95f4\u8c13\u8bcd\u5206\u5e03\u53d8\u5316\uff0c\u53ca\u5b66\u4e60\u578b\u8ba1\u6570\u8349\u56fe\u6a21\u578b\u6355\u83b7\u8de8\u8868\u8fde\u63a5\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u4e2d\uff0cGRASP\u5728\u4e0d\u5b8c\u7f8e\u8d1f\u8f7d\u4e0b\u4f30\u8ba1\u7cbe\u5ea6\u548c\u67e5\u8be2\u5ef6\u8fdf\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5728CEB-IMDb-full\u57fa\u51c6\u4e0a\u5ab2\u7f8e\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "conclusion": "GRASP\u5728\u65e0\u6570\u636e\u8bbf\u95ee\u4e14\u4ec5\u752810%\u8fde\u63a5\u6a21\u677f\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u73b0\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u7ea6\u675f\u573a\u666f\u3002"}}
{"id": "2506.16488", "pdf": "https://arxiv.org/pdf/2506.16488", "abs": "https://arxiv.org/abs/2506.16488", "authors": ["Xiaojun Dong", "Andy Li", "Yan Gu", "Yihan Sun"], "title": "Parallel Point-to-Point Shortest Paths and Batch Queries", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "We propose Orionet, efficient parallel implementations of Point-to-Point\nShortest Paths (PPSP) queries using bidirectional search (BiDS) and other\nheuristics, with an additional focus on batch PPSP queries. We present a\nframework for parallel PPSP built on existing single-source shortest paths\n(SSSP) frameworks by incorporating pruning conditions. As a result, we develop\nefficient parallel PPSP algorithms based on early termination, bidirectional\nsearch, A$^*$ search, and bidirectional A$^*$ all with simple and efficient\nimplementations.\n  We extend our idea to batch PPSP queries, which are widely used in real-world\nscenarios. We first design a simple and flexible abstraction to represent the\nbatch so PPSP can leverage the shared information of the batch. Orionet\nformalizes the batch as a query graph represented by edges between queried\nsources and targets. In this way, we directly extended our PPSP framework to\nbatched queries in a simple and efficient way.\n  We evaluate Orionet on both single and batch PPSP queries using various graph\ntypes and distance percentiles of queried pairs, and compare it against two\nbaselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$\nusing unidirectional search. On 14 graphs we tested, on average, our\nbidirectional search is 2.9$\\times$ faster than GraphIt, and 6.8$\\times$ faster\nthan MBQ. Our bidirectional A$^*$ is 4.4$\\times$ and 6.2$\\times$ faster than\nthe A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also\nprovide in-depth experimental evaluation, and show that Orionet provides strong\nperformance compared to the plain solutions.", "AI": {"tldr": "Orionet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u70b9\u5bf9\u70b9\u6700\u77ed\u8def\u5f84(PPSP)\u67e5\u8be2\u5b9e\u73b0,\u7ed3\u5408\u53cc\u5411\u641c\u7d22(BiDS)\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5,\u7279\u522b\u5173\u6ce8\u6279\u91cfPPSP\u67e5\u8be2\u3002\u5176\u57fa\u4e8e\u73b0\u6709\u5355\u6e90\u6700\u77ed\u8def\u5f84(SSSP)\u6846\u67b6,\u901a\u8fc7\u5f15\u5165\u526a\u679d\u6761\u4ef6,\u5b9e\u73b0\u4e86\u5177\u6709\u7b80\u5355\u9ad8\u6548\u5b9e\u73b0\u7684\u5e76\u884cPPSP\u7b97\u6cd5\u3002\u572814\u4e2a\u6d4b\u8bd5\u56fe\u4e0a,\u53cc\u5411\u641c\u7d22\u5e73\u5747\u6bd4GraphIt\u5feb2.9\u500d,\u6bd4MBQ\u5feb6.8\u500d;\u53cc\u5411A*\u641c\u7d22\u6bd4GraphIt\u548cMBQ\u5206\u522b\u5feb4.4\u500d\u548c6.2\u500d\u3002", "motivation": "\u89e3\u51b3\u70b9\u5bf9\u70b9\u6700\u77ed\u8def\u5f84(PPSP)\u67e5\u8be2\u7684\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u95ee\u9898,\u7279\u522b\u662f\u5728\u6279\u91cf\u67e5\u8be2\u573a\u666f\u4e2d,\u4ee5\u63d0\u9ad8\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u53cc\u5411\u641c\u7d22(BiDS)\u3001A*\u641c\u7d22\u53ca\u5176\u53cc\u5411\u53d8\u4f53,\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5e76\u884cPPSP\u6846\u67b6,\u5e76\u6269\u5c55\u5230\u6279\u91cf\u67e5\u8be2,\u5229\u7528\u6279\u91cf\u67e5\u8be2\u7684\u5171\u4eab\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u56fe\u7c7b\u578b\u548c\u67e5\u8be2\u5bf9\u4e0a,Orionet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf(GraphIt\u548cMBQ),\u53cc\u5411\u641c\u7d22\u548c\u53cc\u5411A*\u641c\u7d22\u5206\u522b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Orionet\u901a\u8fc7\u7b80\u5355\u9ad8\u6548\u7684\u5b9e\u73b0\u548c\u6279\u91cf\u67e5\u8be2\u4f18\u5316,\u663e\u8457\u63d0\u5347\u4e86PPSP\u67e5\u8be2\u7684\u8ba1\u7b97\u6548\u7387,\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.16639", "pdf": "https://arxiv.org/pdf/2506.16639", "abs": "https://arxiv.org/abs/2506.16639", "authors": ["Boqi Chen", "Aren A. Babikian", "Shuzhao Feng", "D\u00e1niel Varr\u00f3", "Gunter Mussbacher"], "title": "LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation", "categories": ["cs.SE"], "comment": "Accepted at the 33rd IEEE International Requirements Engineering 2025\n  conference", "summary": "Requirements over strings, commonly represented using natural language (NL),\nare particularly relevant for software systems due to their heavy reliance on\nstring data manipulation. While individual requirements can usually be analyzed\nmanually, verifying properties (e.g., satisfiability) over sets of NL\nrequirements is particularly challenging. Formal approaches (e.g., SMT solvers)\nmay efficiently verify such properties, but are known to have theoretical\nlimitations. Additionally, the translation of NL requirements into formal\nconstraints typically requires significant manual effort. Recently, large\nlanguage models (LLMs) have emerged as an alternative approach for formal\nreasoning tasks, but their effectiveness in verifying requirements over strings\nis less studied. In this paper, we introduce a hybrid approach that verifies\nthe satisfiability of NL requirements over strings by using LLMs (1) to derive\na satisfiability outcome (and a consistent string, if possible), and (2) to\ngenerate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used\nto validate the correctness of (1). In our experiments, we assess the\nperformance of four LLMs. Results show that LLMs effectively translate natural\nlanguage into checkers, even achieving perfect testing accuracy for\nPython-based checkers. These checkers substantially help LLMs in generating a\nconsistent string and accurately identifying unsatisfiable requirements,\nleading to more than doubled generation success rate and F1-score in certain\ncases compared to baselines without generated checkers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u9a8c\u8bc1\u81ea\u7136\u8bed\u8a00\u5b57\u7b26\u4e32\u9700\u6c42\u7684\u6ee1\u8db3\u6027\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u68c0\u67e5\u5668\u9a8c\u8bc1\u7ed3\u679c\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728\u751f\u6210\u68c0\u67e5\u5668\u548c\u63d0\u9ad8\u751f\u6210\u4e00\u81f4\u6027\u5b57\u7b26\u4e32\u7684\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9a8c\u8bc1\u81ea\u7136\u8bed\u8a00\u5b57\u7b26\u4e32\u9700\u6c42\u7684\u6ee1\u8db3\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982SMT\u6c42\u89e3\u5668\u5b58\u5728\u7406\u8bba\u9650\u5236\u4e14\u9700\u8981\u5927\u91cf\u624b\u52a8\u7ffb\u8bd1\uff0c\u800cLLMs\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u7ed3\u5408LLMs\u8fdb\u884c\u4e24\u9879\u4efb\u52a1\uff1a1) \u751f\u6210\u6ee1\u8db3\u6027\u7ed3\u679c\u53ca\u53ef\u80fd\u7684\u4e00\u81f4\u6027\u5b57\u7b26\u4e32\uff1b2) \u751f\u6210\u58f0\u660e\u5f0f(SMT)\u548c\u547d\u4ee4\u5f0f(Python)\u68c0\u67e5\u5668\u4ee5\u9a8c\u8bc1\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u56db\u79cdLLMs\u3002", "result": "LLMs\u5728\u751f\u6210\u68c0\u67e5\u5668\u548c\u63d0\u9ad8\u4e00\u81f4\u6027\u5b57\u7b26\u4e32\u751f\u6210\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751f\u6210\u6210\u529f\u7387\u548cF1\u5206\u6570\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u4e00\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u9a8c\u8bc1\u4e86LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5b57\u7b26\u4e32\u9700\u6c42\u6ee1\u8db3\u6027\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u751f\u6210\u68c0\u67e5\u5668\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u9700\u6c42\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17084", "pdf": "https://arxiv.org/pdf/2506.17084", "abs": "https://arxiv.org/abs/2506.17084", "authors": ["Vladislav Esaulov", "Jieyang Chen", "Norbert Podhorszki", "Fred Suter", "Scott Klasky", "Anu G Bourgeois", "Lipeng Wan"], "title": "JANUS: Resilient and Adaptive Data Transmission for Enabling Timely and Efficient Cross-Facility Scientific Workflows", "categories": ["cs.DC", "cs.NI", "cs.PF"], "comment": null, "summary": "In modern science, the growing complexity of large-scale projects has\nincreased reliance on cross-facility workflows, where institutions share\nresources and expertise to accelerate discovery. These workflows often involve\ntransferring massive data over wide-area networks. While high-speed networks\nlike ESnet and data transfer services like Globus have improved data mobility,\nchallenges remain. Large data volumes can strain bandwidth, TCP suffers from\nretransmissions due to packet loss, and traditional fault-tolerance methods\nlike erasure coding introduce significant overhead.\n  This paper presents JANUS, a resilient and adaptive data transmission\napproach for cross-facility scientific workflows. JANUS uses UDP, integrates\nerasure coding for fault tolerance, and applies error-bounded lossy compression\nto reduce overhead. This design enables users to balance transmission time and\naccuracy based on specific needs. JANUS also adapts coding parameters to\nreal-time network conditions and uses optimization models to determine ideal\nconfigurations. Experiments show that JANUS significantly improves data\ntransfer efficiency while preserving fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJANUS\u7684\u5f39\u6027\u81ea\u9002\u5e94\u6570\u636e\u4f20\u8f93\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u8bbe\u65bd\u79d1\u5b66\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7UDP\u3001\u7ea0\u5220\u7801\u548c\u8bef\u5dee\u6709\u635f\u538b\u7f29\u7b49\u6280\u672f\u63d0\u5347\u6570\u636e\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u8de8\u8bbe\u65bd\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u4f20\u8f93\u9762\u4e34\u5e26\u5bbd\u538b\u529b\u3001TCP\u91cd\u4f20\u548c\u4f20\u7edf\u5bb9\u9519\u65b9\u6cd5\u7684\u9ad8\u5f00\u9500\u95ee\u9898\u3002", "method": "JANUS\u7ed3\u5408UDP\u3001\u7ea0\u5220\u7801\u548c\u8bef\u5dee\u6709\u635f\u538b\u7f29\uff0c\u52a8\u6001\u8c03\u6574\u7f16\u7801\u53c2\u6570\u4ee5\u9002\u5e94\u5b9e\u65f6\u7f51\u7edc\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u786e\u5b9a\u6700\u4f73\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJANUS\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u4f20\u8f93\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u4fdd\u771f\u5ea6\u3002", "conclusion": "JANUS\u4e3a\u8de8\u8bbe\u65bd\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5f39\u6027\u7684\u6570\u636e\u4f20\u8f93\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15910", "pdf": "https://arxiv.org/pdf/2506.15910", "abs": "https://arxiv.org/abs/2506.15910", "authors": ["Zakria Qadir", "Muhammad Bilal", "Guoqiang Liu", "Xiaolong Xu"], "title": "Autonomous Trajectory Optimization for UAVs in Disaster Zone Using Henry Gas Optimization Scheme", "categories": ["eess.SY", "cs.DC", "cs.NI", "cs.SY", "C.2; I.6"], "comment": "12 pages, 9 figuers", "summary": "The unmanned aerial vehicles (UAVs) in a disaster-prone environment plays\nimportant role in assisting the rescue services and providing the internet\nconnectivity with the outside world. However, in such a complex environment the\nselection of optimum trajectory of UAVs is of utmost importance. UAV trajectory\noptimization deals with finding the shortest path in the minimal possible time.\nIn this paper, a cluster optimization scheme (COS) is proposed using the Henry\ngas optimization (HGO) metaheuristic algorithm to identify the shortest path\nhaving minimal transportation cost and algorithm complexity. The mathematical\nmodel is designed for COS using the HGO algorithm and compared with the\nstate-of-the-art metaheuristic algorithms such as particle swarm optimization\n(PSO), grey wolf optimization (GWO), cuckoo search algorithm (CSA) and\nbarnacles mating optimizer (BMO). In order to prove the robustness of the\nproposed model, four different scenarios are evaluated that includes ambient\nenvironment, constrict environment, tangled environment, and complex\nenvironment. In all the aforementioned scenarios, the HGO algorithm outperforms\nthe existing algorithms. Particularly, in the ambient environment, the HGO\nalgorithm achieves a 39.3% reduction in transportation cost and a 16.8%\nreduction in computational time as compared to the PSO algorithm. Hence, the\nHGO algorithm can be used for autonomous trajectory optimization of UAVs in\nsmart cities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea8\u5229\u6c14\u4f53\u4f18\u5316\uff08HGO\uff09\u7684\u96c6\u7fa4\u4f18\u5316\u65b9\u6848\uff08COS\uff09\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u8fd0\u8f93\u6210\u672c\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u5728\u707e\u5bb3\u591a\u53d1\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u6551\u63f4\u670d\u52a1\u548c\u63d0\u4f9b\u4e92\u8054\u7f51\u8fde\u63a5\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u4f18\u5316\u8f68\u8ff9\u9009\u62e9\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528HGO\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u8bbe\u8ba1\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u4e0ePSO\u3001GWO\u3001CSA\u548cBMO\u7b49\u73b0\u6709\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u591a\u79cd\u73af\u5883\u6d4b\u8bd5\u4e2d\uff0cHGO\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u5c24\u5176\u5728\u5e38\u6001\u73af\u5883\u4e2d\uff0c\u8fd0\u8f93\u6210\u672c\u964d\u4f4e39.3%\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1116.8%\u3002", "conclusion": "HGO\u7b97\u6cd5\u9002\u7528\u4e8e\u667a\u80fd\u57ce\u5e02\u4e2d\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u8f68\u8ff9\u4f18\u5316\u3002"}}
{"id": "2506.16745", "pdf": "https://arxiv.org/pdf/2506.16745", "abs": "https://arxiv.org/abs/2506.16745", "authors": ["Qi-Ying Sun", "Wan-Lei Zhao", "Yi-Bo Miao", "Chong-Wah Ngo"], "title": "Class Agnostic Instance-level Descriptor for Visual Instance Search", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Despite the great success of the deep features in content-based image\nretrieval, the visual instance search remains challenging due to the lack of\neffective instance level feature representation. Supervised or weakly\nsupervised object detection methods are not among the options due to their poor\nperformance on the unknown object categories. In this paper, based on the\nfeature set output from self-supervised ViT, the instance level region\ndiscovery is modeled as detecting the compact feature subsets in a hierarchical\nfashion. The hierarchical decomposition results in a hierarchy of feature\nsubsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the\nvarious instance regions in an image of different semantic scales. The\nhierarchical decomposition well addresses the problem of object embedding and\nocclusions, which are widely observed in the real scenarios. The features\nderived from the nodes on the hierarchy make up a comprehensive representation\nfor the latent instances in the image. Our instance-level descriptor remains\neffective on both the known and unknown object categories. Empirical studies on\nthree instance search benchmarks show that it outperforms state-of-the-art\nmethods considerably.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763ViT\u7684\u5206\u5c42\u7279\u5f81\u5b50\u96c6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u50cf\u68c0\u7d22\u4e2d\u5b9e\u4f8b\u7ea7\u522b\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7279\u5f81\u5728\u56fe\u50cf\u68c0\u7d22\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b9e\u4f8b\u7ea7\u522b\u7279\u5f81\u8868\u793a\u4e0d\u8db3\uff0c\u4e14\u76d1\u7763\u6216\u5f31\u76d1\u7763\u65b9\u6cd5\u5bf9\u672a\u77e5\u7c7b\u522b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u81ea\u76d1\u7763ViT\u8f93\u51fa\u7684\u7279\u5f81\u96c6\uff0c\u901a\u8fc7\u5206\u5c42\u65b9\u5f0f\u68c0\u6d4b\u7d27\u51d1\u7279\u5f81\u5b50\u96c6\uff0c\u6784\u5efa\u591a\u5c42\u6b21\u5b9e\u4f8b\u533a\u57df\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5b9e\u4f8b\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u5bf9\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u5747\u6709\u6548\u3002", "conclusion": "\u5206\u5c42\u7279\u5f81\u5b50\u96c6\u65b9\u6cd5\u89e3\u51b3\u4e86\u5bf9\u8c61\u5d4c\u5165\u548c\u906e\u6321\u95ee\u9898\uff0c\u4e3a\u5b9e\u4f8b\u641c\u7d22\u63d0\u4f9b\u4e86\u5168\u9762\u8868\u793a\u3002"}}
{"id": "2506.16627", "pdf": "https://arxiv.org/pdf/2506.16627", "abs": "https://arxiv.org/abs/2506.16627", "authors": ["Haotian Yin", "Aleksander Plocharski", "Michal Jan Wlodarczyk", "Mikolaj Kida", "Przemyslaw Musialski"], "title": "FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models", "categories": ["cs.GR", "cs.CV", "cs.LG", "65D18, 68U05, 68T07, 53A07", "I.3.5; I.3.7; I.2.6"], "comment": "12 page, 10 figures, preprint", "summary": "Neural signed-distance fields (SDFs) have become a versatile backbone for\ngeometric learning, yet enforcing developable, CAD-style behavior still hinges\non Gaussian curvature penalties that require full Hessian evaluation and\nsecond-order automatic differentiation, both of which are costly in memory and\nruntime. We present a curvature proxy that regularizes only the mixed\nsecond-order term (Weingarten term), allowing the two principal curvatures to\nadapt freely to data while suppressing unwanted warp. Two complementary\ninstantiations realize this idea: (i) a finite-difference proxy that replaces\neach Hessian entry with four forward SDF evaluations and a single first-order\ngradient, and (ii) an autodiff proxy that computes the same mixed derivative\nvia one Hessian-vector product, sidestepping explicit full Hessian assembly and\nremaining faster in practice. Both variants converge to the exact mixed second\nderivative, thus preserving the intended geometric bias without incurring full\nsecond-order graphs. On the ABC benchmarks, the proxies match or exceed the\nreconstruction fidelity of Hessian-based baselines while reducing GPU memory\nuse and wall-clock time by a factor of two. Because the method is drop-in and\nframework-agnostic, it opens a practical path toward scalable, curvature-aware\nSDF learning for engineering-grade shape reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u9ad8\u65af\u66f2\u7387\u60e9\u7f5a\u7684\u66f2\u7387\u4ee3\u7406\u65b9\u6cd5\uff0c\u4ec5\u6b63\u5219\u5316\u6df7\u5408\u4e8c\u9636\u9879\uff08Weingarten\u9879\uff09\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u51e0\u4f55\u504f\u89c1\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u5728\u51e0\u4f55\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684Hessian\u8ba1\u7b97\u548c\u4e8c\u9636\u81ea\u52a8\u5fae\u5206\uff0c\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u95f4\u6210\u672c\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u66f2\u7387\u4ee3\u7406\uff1a(1) \u6709\u9650\u5dee\u5206\u4ee3\u7406\uff0c\u7528\u56db\u4e2aSDF\u8bc4\u4f30\u548c\u4e00\u4e2a\u4e00\u9636\u68af\u5ea6\u66ff\u6362Hessian\u6761\u76ee\uff1b(2) \u81ea\u52a8\u5fae\u5206\u4ee3\u7406\uff0c\u901a\u8fc7Hessian\u5411\u91cf\u79ef\u8ba1\u7b97\u6df7\u5408\u5bfc\u6570\uff0c\u907f\u514d\u663e\u5f0fHessian\u7ec4\u88c5\u3002", "result": "\u5728ABC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee3\u7406\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8fc7\u57fa\u4e8eHessian\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u5c06GPU\u5185\u5b58\u4f7f\u7528\u548c\u8fd0\u884c\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u7a0b\u7ea7\u5f62\u72b6\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u66f2\u7387\u611f\u77e5\u7684SDF\u5b66\u4e60\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.16924", "pdf": "https://arxiv.org/pdf/2506.16924", "abs": "https://arxiv.org/abs/2506.16924", "authors": ["Tomoya Kashimata", "Yohei Hamakawa", "Masaya Yamasaki", "Kosuke Tatsumura"], "title": "Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines", "categories": ["cs.AI", "cs.ET", "I.2.8"], "comment": "18 pages, 6figures", "summary": "Many real-time systems require the optimization of discrete variables.\nBlack-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms\nperform optimization by repeatedly taking actions and observing the\ncorresponding instant rewards without any prior knowledge. Recently, a BBO\nmethod using an Ising machine has been proposed to find the best action that is\nrepresented by a combination of discrete values and maximizes the instant\nreward in static environments. In contrast, dynamic environments, where\nreal-time systems operate, necessitate MAB algorithms that maximize the average\nreward over multiple trials. However, due to the enormous number of actions\nresulting from the combinatorial nature of discrete optimization, conventional\nMAB algorithms cannot effectively optimize dynamic, discrete environments.\nHere, we show a heuristic MAB method for dynamic, discrete environments by\nextending the BBO method, in which an Ising machine effectively explores the\nactions while considering interactions between variables and changes in dynamic\nenvironments. We demonstrate the dynamic adaptability of the proposed method in\na wireless communication system with moving users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u591a\u81c2\u8d4c\u535a\u673a\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u79bb\u6563\u73af\u5883\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528Ising\u673a\u5668\u9ad8\u6548\u63a2\u7d22\u52a8\u4f5c\u5e76\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u5b9e\u65f6\u7cfb\u7edf\u9700\u8981\u4f18\u5316\u79bb\u6563\u53d8\u91cf\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u591a\u81c2\u8d4c\u535a\u673a\u7b97\u6cd5\u7531\u4e8e\u7ec4\u5408\u79bb\u6563\u4f18\u5316\u7684\u590d\u6742\u6027\uff0c\u96be\u4ee5\u6709\u6548\u4f18\u5316\u3002", "method": "\u6269\u5c55\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528Ising\u673a\u5668\u63a2\u7d22\u52a8\u4f5c\u5e76\u8003\u8651\u53d8\u91cf\u95f4\u7684\u4ea4\u4e92\u53ca\u52a8\u6001\u73af\u5883\u7684\u53d8\u5316\u3002", "result": "\u5728\u79fb\u52a8\u7528\u6237\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u79bb\u6563\u73af\u5883\u4e2d\u7684\u4f18\u5316\u6311\u6218\u3002"}}
{"id": "2506.16044", "pdf": "https://arxiv.org/pdf/2506.16044", "abs": "https://arxiv.org/abs/2506.16044", "authors": ["MH Farhadi", "Ali Rabiee", "Sima Ghafoori", "Anna Cetera", "Wei Xu", "Reza Abiri"], "title": "Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "With recent advancements in AI and computational tools, intelligent paradigms\nhave emerged to enhance fields like shared autonomy and human-machine teaming\nin healthcare. Advanced AI algorithms (e.g., reinforcement learning) can\nautonomously make decisions to achieve planning and motion goals. However, in\nhealthcare, where human intent is crucial, fully independent machine decisions\nmay not be ideal. This chapter presents a comprehensive review of\nhuman-centered shared autonomy AI frameworks, focusing on upper limb\nbiosignal-based machine interfaces and associated motor control systems,\nincluding computer cursors, robotic arms, and planar platforms. We examine\nmotor planning, learning (rehabilitation), and control, covering conceptual\nfoundations of human-machine teaming in reach-and-grasp tasks and analyzing\nboth theoretical and practical implementations. Each section explores how human\nand machine inputs can be blended for shared autonomy in healthcare\napplications. Topics include human factors, biosignal processing for intent\ndetection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,\nassistive robotics, and Large Language Models (LLMs) as the next frontier. We\npropose adaptive shared autonomy AI as a high-performance paradigm for\ncollaborative human-AI systems, identify key implementation challenges, and\noutline future directions, particularly regarding AI reasoning agents. This\nanalysis aims to bridge neuroscientific insights with robotics to create more\nintuitive, effective, and ethical human-machine teaming frameworks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u7597\u9886\u57df\u4e2d\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5171\u4eab\u81ea\u4e3bAI\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0a\u80a2\u751f\u7269\u4fe1\u53f7\u673a\u5668\u63a5\u53e3\u53ca\u5176\u63a7\u5236\u7cfb\u7edf\u7684\u7406\u8bba\u3001\u5b9e\u8df5\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5728\u533b\u7597\u9886\u57df\uff0c\u5b8c\u5168\u72ec\u7acb\u7684\u673a\u5668\u51b3\u7b56\u53ef\u80fd\u4e0d\u7406\u60f3\uff0c\u56e0\u4e3a\u4eba\u7c7b\u610f\u56fe\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u7ed3\u5408AI\u4e0e\u4eba\u7c7b\u8f93\u5165\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5171\u4eab\u81ea\u4e3b\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u4e0a\u80a2\u751f\u7269\u4fe1\u53f7\u63a5\u53e3\u53ca\u63a7\u5236\u7cfb\u7edf\uff08\u5982\u8ba1\u7b97\u673a\u5149\u6807\u3001\u673a\u68b0\u81c2\u7b49\uff09\uff0c\u63a2\u8ba8\u4eba\u673a\u534f\u540c\u5728\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u3001\u5b66\u4e60\u4e0e\u63a7\u5236\u3002", "result": "\u63d0\u51fa\u81ea\u9002\u5e94\u5171\u4eab\u81ea\u4e3bAI\u4f5c\u4e3a\u9ad8\u6027\u80fd\u4eba\u673a\u534f\u4f5c\u8303\u5f0f\u7684\u6f5c\u529b\uff0c\u5e76\u5206\u6790\u4e86\u5b9e\u73b0\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u4e0e\u673a\u5668\u4eba\u6280\u672f\uff0c\u6784\u5efa\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u4e14\u7b26\u5408\u4f26\u7406\u7684\u4eba\u673a\u534f\u4f5c\u6846\u67b6\u662f\u672a\u6765\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2506.16903", "pdf": "https://arxiv.org/pdf/2506.16903", "abs": "https://arxiv.org/abs/2506.16903", "authors": ["Arnaud Verdant", "William Guicquero", "J\u00e9r\u00f4me Chossat"], "title": "RCNet: $\u0394\u03a3$ IADCs as Recurrent AutoEncoders", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "This paper proposes a deep learning model (RCNet) for Delta-Sigma\n($\\Delta\\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both\nmodulators and filters. This analogy is applied to Incremental ADCs (IADC).\nHigh-end optimizers combined with full-custom losses are used to define\nadditional hardware design constraints: quantized weights, signal saturation,\ntemporal noise injection, devices area. Focusing on DC conversion, our early\nresults demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)\ncan be optimized under a certain hardware mapping complexity. The proposed\nRCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus\narea constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).\nInterestingly, it appears that the best RCNet architectures do not necessarily\nrely on high-order modulators, leveraging additional topology exploration\ndegrees of freedom.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RCNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u0394\u03a3 ADC\uff0c\u7ed3\u5408RNN\u548c\u786c\u4ef6\u7ea6\u675f\u4f18\u5316\uff0c\u5c55\u793a\u4e86\u5728\u7279\u5b9a\u786c\u4ef6\u590d\u6742\u5ea6\u4e0bSNR\u4e0e\u9762\u79ef\u7684\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u0394\u03a3 ADC\u7684\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5982\u4f55\u5728\u786c\u4ef6\u7ea6\u675f\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528RNN\u63cf\u8ff0\u8c03\u5236\u5668\u548c\u6ee4\u6ce2\u5668\uff0c\u7ed3\u5408\u9ad8\u7aef\u4f18\u5316\u5668\u548c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u8003\u8651\u91cf\u5316\u6743\u91cd\u3001\u4fe1\u53f7\u9971\u548c\u7b49\u786c\u4ef6\u9650\u5236\u3002", "result": "\u5728DC\u8f6c\u6362\u4e2d\uff0cRCNet\u6210\u529f\u5728SNR\uff08>13bit\uff09\u548c\u9762\u79ef\u7ea6\u675f\uff08<14pF\uff09\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u4e14\u4f18\u5316\u67b6\u6784\u4e0d\u4e00\u5b9a\u4f9d\u8d56\u9ad8\u9636\u8c03\u5236\u5668\u3002", "conclusion": "RCNet\u4e3aADC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u7531\u5ea6\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u786c\u4ef6\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.16379", "pdf": "https://arxiv.org/pdf/2506.16379", "abs": "https://arxiv.org/abs/2506.16379", "authors": ["Yan Zhou", "Chunwei Liu", "Bhuvan Urgaonkar", "Zhengle Wang", "Magnus Mueller", "Chao Zhang", "Songyue Zhang", "Pascal Pfeil", "Dominik Horn", "Zhengchun Liu", "Davide Pagano", "Tim Kraska", "Samuel Madden", "Ju Fan"], "title": "PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking", "categories": ["cs.DB"], "comment": null, "summary": "Cloud service providers commonly use standard benchmarks like TPC-H and\nTPC-DS to evaluate and optimize cloud data analytics systems. However, these\nbenchmarks rely on fixed query patterns and fail to capture the real execution\nstatistics of production cloud workloads. Although some cloud database vendors\nhave recently released real workload traces, these traces alone do not qualify\nas benchmarks, as they typically lack essential components like the original\nSQL queries and their underlying databases. To overcome this limitation, this\npaper introduces a new problem of workload synthesis with real statistics,\nwhich aims to generate synthetic workloads that closely approximate real\nexecution statistics, including key performance metrics and operator\ndistributions, in real cloud workloads. To address this problem, we propose\nPBench, a novel workload synthesizer that constructs synthetic workloads by\njudiciously selecting and combining workload components (i.e., queries and\ndatabases) from existing benchmarks. This paper studies the key challenges in\nPBench. First, we address the challenge of balancing performance metrics and\noperator distributions by introducing a multi-objective optimization-based\ncomponent selection method. Second, to capture the temporal dynamics of real\nworkloads, we design a timestamp assignment method that progressively refines\nworkload timestamps. Third, to handle the disparity between the original\nworkload and the candidate workload, we propose a component augmentation\napproach that leverages large language models (LLMs) to generate additional\nworkload components while maintaining statistical fidelity. We evaluate PBench\non real cloud workload traces, demonstrating that it reduces approximation\nerror by up to 6x compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPBench\uff0c\u4e00\u79cd\u901a\u8fc7\u7ec4\u5408\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7ec4\u4ef6\u751f\u6210\u8fd1\u4f3c\u771f\u5b9e\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u6267\u884c\u7edf\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u4e91\u670d\u52a1\u63d0\u4f9b\u5546\u5e38\u7528\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4f46\u8fd9\u4e9b\u6d4b\u8bd5\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u751f\u4ea7\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u6cd5\u3002", "method": "PBench\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u9009\u62e9\u7ec4\u4ef6\u3001\u6e10\u8fdb\u5f0f\u65f6\u95f4\u6233\u5206\u914d\u548c\u57fa\u4e8eLLM\u7684\u7ec4\u4ef6\u589e\u5f3a\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u6307\u6807\u548c\u64cd\u4f5c\u7b26\u5206\u5e03\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPBench\u5c06\u8fd1\u4f3c\u8bef\u5dee\u964d\u4f4e\u81f3\u73b0\u6709\u65b9\u6cd5\u76841/6\u3002", "conclusion": "PBench\u6709\u6548\u751f\u6210\u4e86\u7edf\u8ba1\u8fd1\u4f3c\u771f\u5b9e\u7684\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u586b\u8865\u4e86\u6807\u51c6\u57fa\u51c6\u4e0e\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.16611", "pdf": "https://arxiv.org/pdf/2506.16611", "abs": "https://arxiv.org/abs/2506.16611", "authors": ["Khalid Hassan", "Amirreza Sokhankhosh", "Sara Rouhani"], "title": "Enabling Blockchain Interoperability Through Network Discovery Services", "categories": ["cs.DC", "cs.NI"], "comment": "Published in the IEEE DApps conference", "summary": "Web3 technologies have experienced unprecedented growth in the last decade,\nachieving widespread adoption. As various blockchain networks continue to\nevolve, we are on the cusp of a paradigm shift in which they could provide\nservices traditionally offered by the Internet, but in a decentralized manner,\nmarking the emergence of the Internet of Blockchains. While significant\nprogress has been achieved in enabling interoperability between blockchain\nnetworks, existing solutions often assume that networks are already mutually\naware. This reveals a critical gap: the initial discovery of blockchain\nnetworks remains largely unaddressed. This paper proposes a decentralized\narchitecture for blockchain network discovery that operates independently of\nany centralized authority. We also introduce a mechanism for discovering assets\nand services within a blockchain from external networks. Given the\ndecentralized nature of the proposed discovery architecture, we design an\nincentive mechanism to encourage nodes to actively participate in maintaining\nthe discovery network. The proposed architecture implemented and evaluated,\nusing the Substrate framework, demonstrates its resilience and scalability,\neffectively handling up to 130,000 concurrent requests under the tested network\nconfigurations, with a median response time of 5.5 milliseconds, demonstrating\nthe ability to scale its processing capacity further by increasing its network\nsize.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u533a\u5757\u94fe\u7f51\u7edc\u53d1\u73b0\u67b6\u6784\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6848\u672a\u89e3\u51b3\u7684\u521d\u59cb\u7f51\u7edc\u53d1\u73b0\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6fc0\u52b1\u673a\u5236\u9f13\u52b1\u8282\u70b9\u53c2\u4e0e\u7f51\u7edc\u7ef4\u62a4\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5177\u6709\u9ad8\u6269\u5c55\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740Web3\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u533a\u5757\u94fe\u7f51\u7edc\u7684\u4e92\u64cd\u4f5c\u6027\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u521d\u59cb\u7f51\u7edc\u53d1\u73b0\u95ee\u9898\u4ecd\u672a\u88ab\u89e3\u51b3\uff0c\u963b\u788d\u4e86\u53bb\u4e2d\u5fc3\u5316\u670d\u52a1\u7684\u5168\u9762\u5b9e\u73b0\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u533a\u5757\u94fe\u7f51\u7edc\u53d1\u73b0\u67b6\u6784\uff0c\u5305\u542b\u8d44\u4ea7\u548c\u670d\u52a1\u53d1\u73b0\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6fc0\u52b1\u673a\u5236\u4fc3\u8fdb\u8282\u70b9\u53c2\u4e0e\u3002\u91c7\u7528Substrate\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5b9e\u9a8c\u7f51\u7edc\u4e2d\uff0c\u8be5\u67b6\u6784\u80fd\u5904\u740613\u4e07\u5e76\u53d1\u8bf7\u6c42\uff0c\u4e2d\u4f4d\u54cd\u5e94\u65f6\u95f4\u4e3a5.5\u6beb\u79d2\uff0c\u4e14\u80fd\u901a\u8fc7\u6269\u5c55\u7f51\u7edc\u8fdb\u4e00\u6b65\u63d0\u5347\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u53d1\u73b0\u67b6\u6784\u5177\u6709\u9ad8\u6269\u5c55\u6027\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4e3a\u533a\u5757\u94fe\u7f51\u7edc\u7684\u521d\u59cb\u53d1\u73b0\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16650", "pdf": "https://arxiv.org/pdf/2506.16650", "abs": "https://arxiv.org/abs/2506.16650", "authors": ["Anvith Pabba", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "SemAgent: A Semantics Aware Program Repair Agent", "categories": ["cs.SE", "cs.AI", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities in downstream\nsoftware engineering tasks such as Automated Program Repair (APR). In\nparticular, there has been a lot of research on repository-level\nissue-resolution benchmarks such as SWE-Bench. Although there has been\nsignificant progress on this topic, we notice that in the process of solving\nsuch issues, existing agentic systems tend to hyper-localize on immediately\nsuspicious lines of code and fix them in isolation, without a deeper\nunderstanding of the issue semantics, code semantics, or execution semantics.\nConsequently, many existing systems generate patches that overfit to the user\nissue, even when a more general fix is preferable. To address this limitation,\nwe introduce SemAgent, a novel workflow-based procedure that leverages issue,\ncode, and execution semantics to generate patches that are complete -\nidentifying and fixing all lines relevant to the issue. We achieve this through\na novel pipeline that (a) leverages execution semantics to retrieve relevant\ncontext, (b) comprehends issue-semantics via generalized abstraction, (c)\nisolates code-semantics within the context of this abstraction, and (d)\nleverages this understanding in a two-stage architecture: a repair stage that\nproposes fine-grained fixes, followed by a reviewer stage that filters relevant\nfixes based on the inferred issue-semantics. Our evaluations show that our\nmethodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark\nbeating all other workflow-based approaches, and an absolute improvement of\n7.66% compared to our baseline, which lacks such deep semantic understanding.\nWe note that our approach performs particularly well on issues requiring\nmulti-line reasoning (and editing) and edge-case handling, suggesting that\nincorporating issue and code semantics into APR pipelines can lead to robust\nand semantically consistent repairs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSemAgent\uff0c\u901a\u8fc7\u7ed3\u5408\u95ee\u9898\u3001\u4ee3\u7801\u548c\u6267\u884c\u8bed\u4e49\uff0c\u6539\u8fdb\u73b0\u6709\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728SWEBench-Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709APR\u65b9\u6cd5\u8fc7\u4e8e\u805a\u7126\u5c40\u90e8\u4ee3\u7801\u4fee\u590d\uff0c\u7f3a\u4e4f\u5bf9\u95ee\u9898\u3001\u4ee3\u7801\u548c\u6267\u884c\u8bed\u4e49\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u4fee\u590d\u8865\u4e01\u8fc7\u62df\u5408\u3002", "method": "SemAgent\u91c7\u7528\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6267\u884c\u8bed\u4e49\u83b7\u53d6\u4e0a\u4e0b\u6587\uff0c\u62bd\u8c61\u7406\u89e3\u95ee\u9898\u8bed\u4e49\uff0c\u9694\u79bb\u4ee3\u7801\u8bed\u4e49\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\uff08\u4fee\u590d\u548c\u5ba1\u9605\uff09\u751f\u6210\u66f4\u5168\u9762\u7684\u4fee\u590d\u8865\u4e01\u3002", "result": "\u5728SWEBench-Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemAgent\u7684\u89e3\u51b3\u7387\u8fbe\u523044.66%\uff0c\u4f18\u4e8e\u5176\u4ed6\u5de5\u4f5c\u6d41\u65b9\u6cd5\uff0c\u4e14\u6bd4\u57fa\u51c6\u65b9\u6cd5\u63d0\u9ad8\u4e867.66%\u3002", "conclusion": "\u7ed3\u5408\u95ee\u9898\u4e0e\u4ee3\u7801\u8bed\u4e49\u7684APR\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u9c81\u68d2\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u4fee\u590d\u8865\u4e01\uff0c\u5c24\u5176\u5728\u591a\u884c\u63a8\u7406\u548c\u8fb9\u7f18\u60c5\u51b5\u5904\u7406\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.16545", "pdf": "https://arxiv.org/pdf/2506.16545", "abs": "https://arxiv.org/abs/2506.16545", "authors": ["Marco Stadler", "Michael Vierhauser", "Michael Riegler", "Daniel Waghubinger", "Johannes Sametinger"], "title": "SAFER-D: A Self-Adaptive Security Framework for Distributed Computing Architectures", "categories": ["cs.CR", "cs.NI"], "comment": "Preprint accepted for publication at 19th European Conference on\n  Software Architecture (ECSA)", "summary": "The rise of the Internet of Things and Cyber-Physical Systems has introduced\nnew challenges on ensuring secure and robust communication. The growing number\nof connected devices increases network complexity, leading to higher latency\nand traffic. Distributed computing architectures (DCAs) have gained prominence\nto address these issues. This shift has significantly expanded the attack\nsurface, requiring additional security measures to protect all components --\nfrom sensors and actuators to edge nodes and central servers. Recent incidents\nhighlight the difficulty of this task: Cyberattacks, like distributed denial of\nservice attacks, continue to pose severe threats and cause substantial damage.\nImplementing a holistic defense mechanism remains an open challenge,\nparticularly against attacks that demand both enhanced resilience and rapid\nresponse. Addressing this gap requires innovative solutions to enhance the\nsecurity of DCAs. In this work, we present our holistic self-adaptive security\nframework which combines different adaptation strategies to create\ncomprehensive and efficient defense mechanisms. We describe how to incorporate\nthe framework into a real-world use case scenario and further evaluate its\napplicability and efficiency. Our evaluation yields promising results,\nindicating great potential to further extend the research on our framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u6574\u4f53\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u7684\u5b89\u5168\u6027\uff0c\u5e94\u5bf9\u7269\u8054\u7f51\u548c\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u7f51\u7edc\u653b\u51fb\u3002", "motivation": "\u7269\u8054\u7f51\u548c\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u7684\u5174\u8d77\u589e\u52a0\u4e86\u7f51\u7edc\u590d\u6742\u6027\uff0c\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u96be\u4ee5\u5e94\u5bf9\u65b0\u578b\u653b\u51fb\uff0c\u4e9f\u9700\u521b\u65b0\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u79cd\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6574\u4f53\u5b89\u5168\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u6846\u67b6\u5177\u6709\u826f\u597d\u7684\u9002\u7528\u6027\u548c\u6548\u7387\uff0c\u5c55\u73b0\u4e86\u8fdb\u4e00\u6b65\u6269\u5c55\u7814\u7a76\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5b89\u5168\u6846\u67b6\u4e3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u5176\u529f\u80fd\u3002"}}
{"id": "2506.16784", "pdf": "https://arxiv.org/pdf/2506.16784", "abs": "https://arxiv.org/abs/2506.16784", "authors": ["Xiaoyu Shi", "Rahul Kumar Jain", "Yinhao Li", "Ruibo Hou", "Jingliang Cheng", "Jie Bai", "Guohua Zhao", "Lanfen Lin", "Rui Xu", "Yen-wei Chen"], "title": "TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Deep learning has demonstrated remarkable success in medical image\nsegmentation and computer-aided diagnosis. In particular, numerous advanced\nmethods have achieved state-of-the-art performance in brain tumor segmentation\nfrom MRI scans. While recent studies in other medical imaging domains have\nrevealed that integrating textual reports with visual data can enhance\nsegmentation accuracy, the field of brain tumor analysis lacks a comprehensive\ndataset that combines radiological images with corresponding textual\nannotations. This limitation has hindered the exploration of multimodal\napproaches that leverage both imaging and textual data.\n  To bridge this critical gap, we introduce the TextBraTS dataset, the first\npublicly available volume-level multimodal dataset that contains paired MRI\nvolumes and rich textual annotations, derived from the widely adopted BraTS2020\nbenchmark. Building upon this novel dataset, we propose a novel baseline\nframework and sequential cross-attention method for text-guided volumetric\nmedical image segmentation. Through extensive experiments with various\ntext-image fusion strategies and templated text formulations, our approach\ndemonstrates significant improvements in brain tumor segmentation accuracy,\noffering valuable insights into effective multimodal integration techniques.\n  Our dataset, implementation code, and pre-trained models are publicly\navailable at https://github.com/Jupitern52/TextBraTS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6TextBraTS\uff0c\u7ed3\u5408MRI\u548c\u6587\u672c\u6ce8\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5f15\u5bfc\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8111\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8111\u80bf\u7624\u5206\u6790\u9886\u57df\u7f3a\u4e4f\u7ed3\u5408\u5f71\u50cf\u548c\u6587\u672c\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u8005\u63a8\u51fa\u4e86TextBraTS\u6570\u636e\u96c6\u3002", "method": "\u57fa\u4e8eTextBraTS\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u7ebf\u6846\u67b6\u548c\u5e8f\u5217\u4ea4\u53c9\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u6587\u672c-\u56fe\u50cf\u878d\u5408\u7b56\u7565\u548c\u6a21\u677f\u5316\u6587\u672c\u7684\u5b9e\u9a8c\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8111\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u4ee3\u7801\uff0c\u8fd8\u4e3a\u591a\u6a21\u6001\u96c6\u6210\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.16827", "pdf": "https://arxiv.org/pdf/2506.16827", "abs": "https://arxiv.org/abs/2506.16827", "authors": ["Grzegorz Gruszczynski", "Michal Jan Wlodarczyk", "Jakub J Meixner", "Przemyslaw Musialski"], "title": "Beyond Blur: A Fluid Perspective on Generative Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG", "I.2.6; I.4.10; I.4.8"], "comment": "11 pages, 8 figures, pre-print, supplementary pseudocode in appendix", "summary": "We propose a novel PDE-driven corruption process for generative image\nsynthesis based on advection-diffusion processes which generalizes existing\nPDE-based approaches. Our forward pass formulates image corruption via a\nphysically motivated PDE that couples directional advection with isotropic\ndiffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,\nFourier). We implement this PDE numerically through a GPU-accelerated custom\nLattice Boltzmann solver for fast evaluation. To induce realistic turbulence,\nwe generate stochastic velocity fields that introduce coherent motion and\ncapture multi-scale mixing. In the generative process, a neural network learns\nto reverse the advection-diffusion operator thus constituting a novel\ngenerative model. We discuss how previous methods emerge as specific cases of\nour operator, demonstrating that our framework generalizes prior PDE-based\ncorruption techniques. We illustrate how advection improves the diversity and\nquality of the generated images while keeping the overall color palette\nunaffected. This work bridges fluid dynamics, dimensionless PDE theory, and\ndeep generative modeling, offering a fresh perspective on physically informed\nimage corruption processes for diffusion-based synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u542f\u53d1\u7684PDE\u9a71\u52a8\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u4f53\u52a8\u529b\u5b66\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u7ed3\u5408\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u65e0\u91cf\u7eb2PDE\u7406\u8bba\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "method": "\u4f7f\u7528Lattice Boltzmann\u6c42\u89e3\u5668\u5b9e\u73b0PDE\u6570\u503c\u8ba1\u7b97\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u53cd\u8f6cPDE\u8fc7\u7a0b\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u66f4\u9ad8\u4e14\u989c\u8272\u4e0d\u53d7\u5f71\u54cd\uff0c\u6cdb\u5316\u4e86\u73b0\u6709PDE\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u57fa\u4e8ePDE\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u7ed3\u5408\u4e86\u7269\u7406\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u3002"}}
{"id": "2506.16938", "pdf": "https://arxiv.org/pdf/2506.16938", "abs": "https://arxiv.org/abs/2506.16938", "authors": ["Sebastian Nagies", "Emiliano Tolotti", "Davide Pastorello", "Enrico Blanzieri"], "title": "Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test", "categories": ["quant-ph", "cs.ET", "cs.LG"], "comment": "15 pages, 7 figures", "summary": "Parameterized quantum circuits represent promising architectures for machine\nlearning applications, yet many lack clear connections to classical models,\npotentially limiting their ability to translate the wide success of classical\nneural networks to the quantum realm. We examine a specific type of quantum\nneural network (QNN) built exclusively from SWAP test circuits, and discuss its\nmathematical equivalence to a classical two-layer feedforward network with\nquadratic activation functions under amplitude encoding. Our analysis across\nclassical real-world and synthetic datasets reveals that while this\narchitecture can successfully learn many practical tasks, it exhibits\nfundamental expressivity limitations due to violating the universal\napproximation theorem, particularly failing on harder problems like the parity\ncheck function. To address this limitation, we introduce a circuit modification\nusing generalized SWAP test circuits that effectively implements classical\nneural networks with product layers. This enhancement enables successful\nlearning of parity check functions in arbitrary dimensions which we\nanalytically argue to be impossible for the original architecture beyond two\ndimensions regardless of network size. Our results establish a framework for\nenhancing QNN expressivity through classical task analysis and demonstrate that\nour SWAP test-based architecture offers broad representational capacity,\nsuggesting potential promise also for quantum learning tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eSWAP\u6d4b\u8bd5\u7535\u8def\u7684\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u5728\u6570\u5b66\u4e0a\u7b49\u4ef7\u4e8e\u5177\u6709\u4e8c\u6b21\u6fc0\u6d3b\u51fd\u6570\u7684\u7ecf\u5178\u4e24\u5c42\u524d\u9988\u7f51\u7edc\uff0c\u5206\u6790\u4e86\u5176\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u4e0e\u7ecf\u5178\u6a21\u578b\u7684\u8054\u7cfb\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u673a\u5668\u5b66\u4e60\u548c\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76SWAP\u6d4b\u8bd5\u7535\u8def\u7684QNN\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u5e7f\u4e49SWAP\u6d4b\u8bd5\u7535\u8def\u6539\u8fdb\u67b6\u6784\u3002", "result": "\u539f\u59cb\u67b6\u6784\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4f46\u6539\u8fdb\u540e\u7684\u67b6\u6784\u80fd\u6210\u529f\u5b66\u4e60\u9ad8\u7ef4\u5947\u5076\u6821\u9a8c\u51fd\u6570\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u7ecf\u5178\u4efb\u52a1\u589e\u5f3aQNN\u8868\u8fbe\u80fd\u529b\uff0c\u6539\u8fdb\u7684SWAP\u6d4b\u8bd5\u67b6\u6784\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.16294", "pdf": "https://arxiv.org/pdf/2506.16294", "abs": "https://arxiv.org/abs/2506.16294", "authors": ["Linde Vanbesien", "Bart Bogaerts", "Marc Denecker"], "title": "Approximation Fixpoint Theory with Refined Approximation Spaces", "categories": ["cs.AI", "cs.LO"], "comment": "Submitted to KR 2024", "summary": "Approximation Fixpoint Theory (AFT) is a powerful theory covering various\nsemantics of non-monotonic reasoning formalisms in knowledge representation\nsuch as Logic Programming and Answer Set Programming. Many semantics of such\nnon-monotonic formalisms can be characterized as suitable fixpoints of a\nnon-monotonic operator on a suitable lattice. Instead of working on the\noriginal lattice, AFT operates on intervals in such lattice to approximate or\nconstruct the fixpoints of interest. While AFT has been applied successfully\nacross a broad range of non-monotonic reasoning formalisms, it is confronted by\nits limitations in other, relatively simple, examples. In this paper, we\novercome those limitations by extending consistent AFT to deal with\napproximations that are more refined than intervals. Therefore, we introduce a\nmore general notion of approximation spaces, showcase the improved\nexpressiveness and investigate relations between different approximation\nspaces.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\uff08AFT\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u66f4\u7cbe\u7ec6\u7684\u8fd1\u4f3c\u7a7a\u95f4\u6765\u514b\u670d\u539f\u6709\u5c40\u9650\u6027\u3002", "motivation": "\u867d\u7136AFT\u5728\u975e\u5355\u8c03\u63a8\u7406\u5f62\u5f0f\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u67d0\u4e9b\u7b80\u5355\u4f8b\u5b50\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u66f4\u4e00\u822c\u7684\u8fd1\u4f3c\u7a7a\u95f4\u6982\u5ff5\uff0c\u5c55\u793a\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u8fd1\u4f3c\u7a7a\u95f4\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u65b0\u65b9\u6cd5\u63d0\u5347\u4e86AFT\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u5176\u9002\u7528\u6027\u3002", "conclusion": "\u6269\u5c55\u540e\u7684AFT\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u8fd1\u4f3c\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.16107", "pdf": "https://arxiv.org/pdf/2506.16107", "abs": "https://arxiv.org/abs/2506.16107", "authors": ["Mariann Kornelia Smith", "Jacqueline Meijer-Irons", "Andrew Millar"], "title": "From 600 Tools to 1 Console: A UX-Driven Transformation", "categories": ["cs.HC"], "comment": null, "summary": "In 2021 the Technical Infrastructure (TI) User Experience (UX) team sent a\nsurvey to 10,000 Google Developers (Googlers) and uncovered that Google's\ninternal infrastructure tools were fragmented and inefficient, hindering\ndevelopers' productivity. Using user centered research and design methodologies\nthe team first created a story map and service blueprint to visualize the\nrelationship between internal applications, then formulated a strategic vision\nto consolidate tools, streamline workflows, and measure the impact of their\nwork. We secured executive buy-in and delivered incremental improvements.", "AI": {"tldr": "\u8c37\u6b4c\u5185\u90e8\u57fa\u7840\u8bbe\u65bd\u5de5\u5177\u6548\u7387\u4f4e\u4e0b\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u8bbe\u8ba1\u65b9\u6cd5\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\u5e76\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\u3002", "motivation": "\u89e3\u51b3\u8c37\u6b4c\u5185\u90e8\u57fa\u7840\u8bbe\u65bd\u5de5\u5177\u5206\u6563\u548c\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u3002", "method": "\u91c7\u7528\u7528\u6237\u4e2d\u5fc3\u5316\u7684\u7814\u7a76\u548c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u521b\u5efa\u6545\u4e8b\u5730\u56fe\u548c\u670d\u52a1\u84dd\u56fe\uff0c\u5236\u5b9a\u6218\u7565\u613f\u666f\u5e76\u5206\u6b65\u5b9e\u65bd\u6539\u8fdb\u3002", "result": "\u6210\u529f\u7b80\u5316\u5de5\u5177\u3001\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u83b7\u5f97\u4e86\u7ba1\u7406\u5c42\u652f\u6301\u3002", "conclusion": "\u7528\u6237\u4e2d\u5fc3\u5316\u7684\u8bbe\u8ba1\u548c\u5206\u6b65\u6539\u8fdb\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u5185\u90e8\u5de5\u5177\u7684\u6548\u7387\u548c\u5f00\u53d1\u8005\u4f53\u9a8c\u3002"}}
{"id": "2506.16616", "pdf": "https://arxiv.org/pdf/2506.16616", "abs": "https://arxiv.org/abs/2506.16616", "authors": ["Soroush Omidvartehrani", "Davood Rafiei"], "title": "LDI: Localized Data Imputation", "categories": ["cs.DB"], "comment": null, "summary": "Missing values are a common challenge in real-world tabular data and can\nsignificantly impair downstream analysis. While Large Language Models (LLMs)\nhave recently shown promise in data imputation, existing methods often rely on\nbroad, unfiltered prompts that compromise accuracy, scalability, and\nexplainability. We introduce LDI (Localized Data Imputation), a novel framework\nthat improves both the accuracy and transparency of LLM-based imputation by\nselecting a compact, contextually relevant subset of attributes and tuples for\neach missing value. This localized prompting reduces noise, enables\ntraceability by revealing which data influenced each prediction, and is\neffective across both hosted LLMs and lightweight local models. Our extensive\nexperiments on four real-world datasets show that LDI outperforms\nstate-of-the-art methods, achieving up to 8% higher accuracy when using hosted\nLLMs. The gains are more substantial with lightweight local models, reaching\nnearly 17% and 97% accuracy on some datasets when using 3 and 10 examples,\nrespectively. In addition to higher accuracy, LDI offers improved\ninterpretability and robustness to data inconsistencies, making it well-suited\nfor high-stakes and privacy-sensitive applications.", "AI": {"tldr": "LDI\u6846\u67b6\u901a\u8fc7\u672c\u5730\u5316\u63d0\u793a\u63d0\u5347LLM\u5728\u6570\u636e\u586b\u5145\u4e2d\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u6570\u636e\u586b\u5145\u65b9\u6cd5\u56e0\u5bbd\u6cdb\u63d0\u793a\u5bfc\u81f4\u7684\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "method": "LDI\u9009\u62e9\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5c5e\u6027\u548c\u5143\u7ec4\u5b50\u96c6\uff0c\u8fdb\u884c\u672c\u5730\u5316\u63d0\u793a\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "LDI\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe8%\uff08\u6258\u7ba1LLM\uff09\u548c97%\uff08\u8f7b\u91cf\u672c\u5730\u6a21\u578b\uff09\u3002", "conclusion": "LDI\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u548c\u9690\u79c1\u654f\u611f\u573a\u666f\u3002"}}
{"id": "2506.16653", "pdf": "https://arxiv.org/pdf/2506.16653", "abs": "https://arxiv.org/abs/2506.16653", "authors": ["Vladislav Belozerov", "Peter J Barclay", "Askhan Sami"], "title": "LLMs in Coding and their Impact on the Commercial Software Engineering Landscape", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Large-language-model coding tools are now mainstream in software engineering.\nBut as these same tools move human effort up the development stack, they\npresent fresh dangers: 10% of real prompts leak private data, 42% of generated\nsnippets hide security flaws, and the models can even ``agree'' with wrong\nideas, a trait called sycophancy. We argue that firms must tag and review every\nAI-generated line of code, keep prompts and outputs inside private or\non-premises deployments, obey emerging safety regulations, and add tests that\ncatch sycophantic answers -- so they can gain speed without losing security and\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5de5\u5177\u7684\u666e\u53ca\u53ca\u5176\u5e26\u6765\u7684\u9690\u79c1\u6cc4\u9732\u3001\u5b89\u5168\u6f0f\u6d1e\u548c\u6a21\u578b\u8fce\u5408\u9519\u8bef\u89c2\u70b9\uff08\u963f\u8c00\u5949\u627f\uff09\u7b49\u65b0\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4f01\u4e1a\u5e94\u91c7\u53d6\u7684\u63aa\u65bd\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5de5\u5177\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5e26\u6765\u7684\u9690\u79c1\u6cc4\u9732\u3001\u5b89\u5168\u6f0f\u6d1e\u548c\u6a21\u578b\u963f\u8c00\u5949\u627f\u7b49\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5efa\u8bae\u4f01\u4e1a\u5bf9AI\u751f\u6210\u7684\u4ee3\u7801\u8fdb\u884c\u6807\u8bb0\u548c\u5ba1\u67e5\uff0c\u5c06\u63d0\u793a\u548c\u8f93\u51fa\u9650\u5236\u5728\u79c1\u6709\u6216\u672c\u5730\u90e8\u7f72\u4e2d\uff0c\u9075\u5faa\u65b0\u5174\u7684\u5b89\u5168\u6cd5\u89c4\uff0c\u5e76\u589e\u52a0\u6d4b\u8bd5\u4ee5\u68c0\u6d4b\u963f\u8c00\u5949\u627f\u7684\u56de\u7b54\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c10%\u7684\u771f\u5b9e\u63d0\u793a\u4f1a\u6cc4\u9732\u9690\u79c1\u6570\u636e\uff0c42%\u7684\u751f\u6210\u4ee3\u7801\u7247\u6bb5\u9690\u85cf\u5b89\u5168\u6f0f\u6d1e\uff0c\u6a21\u578b\u8fd8\u4f1a\u8868\u73b0\u51fa\u963f\u8c00\u5949\u627f\u7684\u884c\u4e3a\u3002", "conclusion": "\u4f01\u4e1a\u9700\u901a\u8fc7\u4e25\u683c\u5ba1\u67e5\u548c\u6d4b\u8bd5\uff0c\u786e\u4fdd\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u7684\u5feb\u901f\u5f00\u53d1\u4e2d\u4e0d\u727a\u7272\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.17016", "pdf": "https://arxiv.org/pdf/2506.17016", "abs": "https://arxiv.org/abs/2506.17016", "authors": ["Giulia Bertazzini", "Chiara Albisani", "Daniele Baracchi", "Dasara Shullani", "Roberto Verdecchia"], "title": "The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation", "categories": ["cs.LG", "cs.MM"], "comment": null, "summary": "With the growing adoption of AI image generation, in conjunction with the\never-increasing environmental resources demanded by AI, we are urged to answer\na fundamental question: What is the environmental impact hidden behind each\nimage we generate? In this research, we present a comprehensive empirical\nexperiment designed to assess the energy consumption of AI image generation.\nOur experiment compares 17 state-of-the-art image generation models by\nconsidering multiple factors that could affect their energy consumption, such\nas model quantization, image resolution, and prompt length. Additionally, we\nconsider established image quality metrics to study potential trade-offs\nbetween energy consumption and generated image quality. Results show that image\ngeneration models vary drastically in terms of the energy they consume, with up\nto a 46x difference. Image resolution affects energy consumption\ninconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.\nU-Net-based models tend to consume less than Transformer-based one. Model\nquantization instead results to deteriorate the energy efficiency of most\nmodels, while prompt length and content have no statistically significant\nimpact. Improving image quality does not always come at the cost of a higher\nenergy consumption, with some of the models producing the highest quality\nimages also being among the most energy efficient ones.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5b9e\u9a8c\u6bd4\u8f83\u4e8617\u79cdAI\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u80fd\u6e90\u6d88\u8017\uff0c\u53d1\u73b0\u6a21\u578b\u95f4\u80fd\u6e90\u5dee\u5f02\u9ad8\u8fbe46\u500d\uff0c\u5206\u8fa8\u7387\u5f71\u54cd\u4e0d\u4e00\uff0cU-Net\u6a21\u578b\u66f4\u8282\u80fd\uff0c\u91cf\u5316\u53cd\u800c\u964d\u4f4e\u6548\u7387\uff0c\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u672a\u5fc5\u589e\u52a0\u80fd\u8017\u3002", "motivation": "\u968f\u7740AI\u56fe\u50cf\u751f\u6210\u7684\u666e\u53ca\u53ca\u5176\u5bf9\u8d44\u6e90\u7684\u9700\u6c42\u589e\u957f\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u751f\u6210\u6bcf\u5f20\u56fe\u50cf\u80cc\u540e\u7684\u73af\u5883\u5f71\u54cd\u3002", "method": "\u5b9e\u9a8c\u6bd4\u8f83\u4e8617\u79cd\u5148\u8fdb\u6a21\u578b\uff0c\u91cf\u5316\u4e86\u6a21\u578b\u3001\u5206\u8fa8\u7387\u3001\u63d0\u793a\u957f\u5ea6\u7b49\u56e0\u7d20\u5bf9\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u5206\u6790\u3002", "result": "\u6a21\u578b\u80fd\u8017\u5dee\u5f02\u663e\u8457\uff0cU-Net\u6bd4Transformer\u8282\u80fd\uff0c\u91cf\u5316\u964d\u4f4e\u6548\u7387\uff0c\u5206\u8fa8\u7387\u5f71\u54cd\u4e0d\u4e00\u81f4\uff0c\u56fe\u50cf\u8d28\u91cf\u4e0e\u80fd\u8017\u65e0\u76f4\u63a5\u5173\u8054\u3002", "conclusion": "\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u672a\u5fc5\u589e\u52a0\u80fd\u8017\uff0c\u90e8\u5206\u9ad8\u6548\u6a21\u578b\u540c\u65f6\u5177\u6709\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2506.17206", "pdf": "https://arxiv.org/pdf/2506.17206", "abs": "https://arxiv.org/abs/2506.17206", "authors": ["Yukun Huang", "Yanning Zhou", "Jianan Wang", "Kaiyi Huang", "Xihui Liu"], "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project page: https://yukun-huang.github.io/DreamCube/", "summary": "3D panorama synthesis is a promising yet challenging task that demands\nhigh-quality and diverse visual appearance and geometry of the generated\nomnidirectional content. Existing methods leverage rich image priors from\npre-trained 2D foundation models to circumvent the scarcity of 3D panoramic\ndata, but the incompatibility between 3D panoramas and 2D single views limits\ntheir effectiveness. In this work, we demonstrate that by applying multi-plane\nsynchronization to the operators from 2D foundation models, their capabilities\ncan be seamlessly extended to the omnidirectional domain. Based on this design,\nwe further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D\npanorama generation, which maximizes the reuse of 2D foundation model priors to\nachieve diverse appearances and accurate geometry while maintaining multi-view\nconsistency. Extensive experiments demonstrate the effectiveness of our\napproach in panoramic image generation, panoramic depth estimation, and 3D\nscene generation.", "AI": {"tldr": "\u901a\u8fc7\u591a\u5e73\u9762\u540c\u6b65\u6280\u672f\u6269\u5c552D\u57fa\u7840\u6a21\u578b\u80fd\u529b\uff0c\u63d0\u51faDreamCube\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u5168\u666f\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e03D\u5168\u666f\u4e0e2D\u5355\u89c6\u56fe\u4e0d\u517c\u5bb9\u800c\u6548\u679c\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5229\u75282D\u57fa\u7840\u6a21\u578b\u7684\u4e30\u5bcc\u5148\u9a8c\u63d0\u53473D\u5168\u666f\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u591a\u5e73\u9762\u540c\u6b65\u6280\u672f\uff0c\u5c062D\u57fa\u7840\u6a21\u578b\u7684\u7b97\u5b50\u6269\u5c55\u5230\u5168\u666f\u9886\u57df\uff1b\u63d0\u51faDreamCube\uff0c\u4e00\u79cd\u57fa\u4e8eRGB-D\u6269\u6563\u6a21\u578b\u7684\u591a\u5e73\u97623D\u5168\u666f\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u3001\u5168\u666f\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u573a\u666f\u751f\u6210\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "DreamCube\u80fd\u9ad8\u6548\u590d\u75282D\u5148\u9a8c\uff0c\u5b9e\u73b0\u591a\u6837\u5916\u89c2\u548c\u7cbe\u786e\u51e0\u4f55\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.17068", "pdf": "https://arxiv.org/pdf/2506.17068", "abs": "https://arxiv.org/abs/2506.17068", "authors": ["Runkai Zhang", "Hua Yu", "John Q. Gan", "Haixian Wang"], "title": "Cross-Modal Epileptic Signal Harmonization: Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer", "categories": ["q-bio.NC", "cs.ET", "eess.SP"], "comment": null, "summary": "Scalp electroencephalography (EEG) and intracranial EEG (iEEG) are vital for\nepilepsy diagnosis and treatment. Their unified analysis offers the potential\nto harness the complementary strengths of each modality but is challenging due\nto variations in recording montages, amplitude and signal-to-noise ratio (SNR),\nand frequency components. To address the aforementioned challenges, this paper\nintroduces EpiNT, a novel Transformer-based pre-trained model for unified EEG\nand iEEG analysis. EpiNT employs channel-independent modeling with masked\nautoencoders (MAE) and vector quantization (VQ), along with a frequency domain\nmapping quantizer to capture crucial frequency features. Pre-trained on over\n2,700 hours of multi-modal clinical neurophysiological data from 1,199\npatients, EpiNT outperformed both randomly initialized models and other\npre-trained methods on six downstream classification tasks, demonstrating\nrobust representation learning capabilities. This work presents a promising\napproach for unified epilepsy neurophysiology analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpiNT\u7684Transformer\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u5206\u6790\u5934\u76ae\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u9885\u5185\u8111\u7535\u56fe\uff08iEEG\uff09\uff0c\u4ee5\u89e3\u51b3\u4e24\u79cd\u6a21\u6001\u5728\u8bb0\u5f55\u65b9\u5f0f\u3001\u4fe1\u53f7\u8d28\u91cf\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "EEG\u548ciEEG\u5728\u766b\u75eb\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u7edf\u4e00\u5206\u6790\u8fd9\u4e24\u79cd\u6a21\u6001\u5b58\u5728\u6311\u6218\uff0c\u5982\u8bb0\u5f55\u65b9\u5f0f\u3001\u4fe1\u53f7\u5e45\u503c\u548c\u4fe1\u566a\u6bd4\u7684\u5dee\u5f02\u3002", "method": "EpiNT\u91c7\u7528\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u548c\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u7684\u901a\u9053\u72ec\u7acb\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u9891\u57df\u6620\u5c04\u91cf\u5316\u5668\u4ee5\u6355\u6349\u5173\u952e\u9891\u7387\u7279\u5f81\u3002", "result": "\u57281,199\u540d\u60a3\u8005\u76842,700\u591a\u5c0f\u65f6\u591a\u6a21\u6001\u4e34\u5e8a\u795e\u7ecf\u751f\u7406\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cEpiNT\u5728\u516d\u9879\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u548c\u5176\u4ed6\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "EpiNT\u4e3a\u766b\u75eb\u795e\u7ecf\u751f\u7406\u5b66\u7684\u7edf\u4e00\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.16397", "pdf": "https://arxiv.org/pdf/2506.16397", "abs": "https://arxiv.org/abs/2506.16397", "authors": ["Amik Raj Behera", "Nutan Limaye", "Varun Ramanathan", "Srikanth Srinivasan"], "title": "New Bounds for the Ideal Proof System in Positive Characteristic", "categories": ["cs.CC", "cs.LO", "math.LO", "F.4.1"], "comment": "57 pages, To appear in the 52nd EATCS International Colloquium on\n  Automata, Languages, and Programming (ICALP) 2025", "summary": "In this work, we prove upper and lower bounds over fields of positive\ncharacteristics for several fragments of the Ideal Proof System (IPS), an\nalgebraic proof system introduced by Grochow and Pitassi (J. ACM 2018). Our\nresults extend the works of Forbes, Shpilka, Tzameret, and Wigderson (Theory of\nComputing 2021) and also of Govindasamy, Hakoniemi, and Tzameret (FOCS 2022).\nThese works primarily focused on proof systems over fields of characteristic\n$0$, and we are able to extend these results to positive characteristic.\n  The question of proving general IPS lower bounds over positive characteristic\nis motivated by the important question of proving $AC^{0}[p]$-Frege lower\nbounds. This connection was observed by Grochow and Pitassi (J. ACM 2018).\nAdditional motivation comes from recent developments in algebraic complexity\ntheory due to Forbes (CCC 2024) who showed how to extend previous lower bounds\nover characteristic $0$ to positive characteristic.\n  In our work, we adapt the functional lower bound method of Forbes et al.\n(Theory of Computing 2021) to prove exponential-size lower bounds for various\nsubsystems of IPS. Additionally, we derive upper bounds for the instances\npresented above. We show that they have efficient constant-depth IPS\nrefutations. We also show that constant-depth IPS can efficiently refute a\ngeneral class of instances, namely all symmetric instances, thereby further\nuncovering the strength of these algebraic proofs in positive characteristic.\n  Notably, our lower bounds hold for fields of arbitrary characteristic but\nrequire the field size to be $n^{\\omega(1)}$. In a concurrent work, Elbaz,\nGovindasamy, Lu, and Tzameret have shown lower bounds against restricted\nclasses of IPS over finite fields of any size by considering different hard\ninstances.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u6b63\u7279\u5f81\u57df\u4e0a\u4ee3\u6570\u8bc1\u660e\u7cfb\u7edf\uff08IPS\uff09\u591a\u4e2a\u5b50\u7cfb\u7edf\u7684\u4e0a\u4e0b\u754c\uff0c\u6269\u5c55\u4e86\u4e4b\u524d\u5173\u4e8e\u7279\u5f810\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u8bc1\u660e$AC^{0}[p]$-Frege\u4e0b\u754c\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u4ee3\u6570\u590d\u6742\u6027\u7406\u8bba\u4e2d\u7684\u65b0\u8fdb\u5c55\u3002", "method": "\u91c7\u7528Forbes\u7b49\u4eba\u7684\u51fd\u6570\u4e0b\u754c\u65b9\u6cd5\uff0c\u8bc1\u660eIPS\u5b50\u7cfb\u7edf\u7684\u6307\u6570\u89c4\u6a21\u4e0b\u754c\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u4e0a\u754c\u7ed3\u679c\u3002", "result": "\u8bc1\u660e\u4e86\u6b63\u7279\u5f81\u57df\u4e0aIPS\u5b50\u7cfb\u7edf\u7684\u6307\u6570\u89c4\u6a21\u4e0b\u754c\uff0c\u4ee5\u53ca\u9ad8\u6548\u5e38\u6570\u6df1\u5ea6IPS\u53cd\u9a73\u80fd\u529b\u3002", "conclusion": "\u5de5\u4f5c\u63ed\u793a\u4e86\u6b63\u7279\u5f81\u57df\u4e0a\u4ee3\u6570\u8bc1\u660e\u7cfb\u7edf\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.16168", "pdf": "https://arxiv.org/pdf/2506.16168", "abs": "https://arxiv.org/abs/2506.16168", "authors": ["Thomas Barbera", "Jacopo Burger", "Alessandro D'Amelio", "Simone Zini", "Simone Bianco", "Raffaella Lanzarotti", "Paolo Napoletano", "Giuseppe Boccignone", "Jose Luis Contreras-Vidal"], "title": "On using AI for EEG-based BCI applications: problems, current challenges and future trends", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Imagine unlocking the power of the mind to communicate, create, and even\ninteract with the world around us. Recent breakthroughs in Artificial\nIntelligence (AI), especially in how machines \"see\" and \"understand\" language,\nare now fueling exciting progress in decoding brain signals from scalp\nelectroencephalography (EEG). Prima facie, this opens the door to revolutionary\nbrain-computer interfaces (BCIs) designed for real life, moving beyond\ntraditional uses to envision Brain-to-Speech, Brain-to-Image, and even a\nBrain-to-Internet of Things (BCIoT).\n  However, the journey is not as straightforward as it was for Computer Vision\n(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based\nBCIs, particularly in building powerful foundational models, presents unique\nand intricate hurdles that could affect their reliability.\n  Here, we unfold a guided exploration of this dynamic and rapidly evolving\nresearch area. Rather than barely outlining a map of current endeavors and\nresults, the goal is to provide a principled navigation of this hot and\ncutting-edge research landscape. We consider the basic paradigms that emerge\nfrom a causal perspective and the attendant challenges presented to AI-based\nmodels. Looking ahead, we then discuss promising research avenues that could\novercome today's technological, methodological, and ethical limitations. Our\naim is to lay out a clear roadmap for creating truly practical and effective\nEEG-based BCI solutions that can thrive in everyday environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u89e3\u7801\u8111\u7535\u56fe\uff08EEG\uff09\u4fe1\u53f7\u4ee5\u5f00\u53d1\u5b9e\u7528\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5229\u7528AI\u6280\u672f\u63a8\u52a8EEG-based BCI\u7684\u53d1\u5c55\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u8111\u8bed\u97f3\u3001\u8111\u56fe\u50cf\u7b49\uff09\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u5206\u6790\u57fa\u672c\u8303\u5f0f\uff0c\u63a2\u8ba8AI\u6a21\u578b\u5728EEG-based BCI\u4e2d\u7684\u72ec\u7279\u6311\u6218\u3002", "result": "\u8bc6\u522b\u4e86\u6280\u672f\u3001\u65b9\u6cd5\u548c\u4f26\u7406\u4e0a\u7684\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4efd\u6e05\u6670\u7684\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u5f00\u53d1\u5b9e\u7528\u4e14\u9ad8\u6548\u7684EEG-based BCI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16923", "pdf": "https://arxiv.org/pdf/2506.16923", "abs": "https://arxiv.org/abs/2506.16923", "authors": ["Omer Abramovich", "Daniel Deutch", "Nave Frost", "Ahmet Kara", "Dan Olteanu"], "title": "Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms", "categories": ["cs.DB"], "comment": null, "summary": "In this paper, we introduce a novel approach to computing the contribution of\ninput tuples to the result of the query, quantified by the Banzhaf and Shapley\nvalues. In contrast to prior algorithmic work that focuses on\nSelect-Project-Join-Union queries, ours is the first practical approach for\nqueries with aggregates. It relies on two novel optimizations that are\nessential for its practicality and significantly improve the runtime\nperformance already for queries without aggregates. The first optimization\nexploits the observation that many input tuples have the same contribution to\nthe query result, so it is enough to compute the contribution of one of them.\nThe second optimization uses the gradient of the query lineage to compute the\ncontributions of all tuples with the same complexity as for one of them.\nExperiments with a million instances over 3 databases show that our approach\nachieves up to 3 orders of magnitude runtime improvements over the\nstate-of-the-art for queries without aggregates, and that it is practical for\naggregate queries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u8ba1\u7b97\u8f93\u5165\u5143\u7ec4\u5bf9\u67e5\u8be2\u7ed3\u679c\u7684\u8d21\u732e\uff0c\u9996\u6b21\u9002\u7528\u4e8e\u805a\u5408\u67e5\u8be2\uff0c\u5e76\u4f18\u5316\u4e86\u6027\u80fd\uff0c\u5b9e\u73b0\u4e863\u4e2a\u6570\u91cf\u7ea7\u7684\u8fd0\u884c\u65f6\u95f4\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5de5\u4f5c\u65e0\u6cd5\u5904\u7406\u805a\u5408\u67e5\u8be2\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u67e5\u8be2\u8ba1\u7b97\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u65b0\u9896\u4f18\u5316\uff1a1. \u8ba1\u7b97\u76f8\u540c\u8d21\u732e\u7684\u5143\u7ec4\u4e4b\u4e00\uff1b2. \u5229\u7528\u67e5\u8be2\u8c31\u7cfb\u7684\u68af\u5ea6\u5feb\u901f\u8ba1\u7b97\u6240\u6709\u5143\u7ec4\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u805a\u5408\u67e5\u8be2\u6027\u80fd\u63d0\u53473\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u9996\u6b21\u5b9e\u7528\u4e8e\u805a\u5408\u67e5\u8be2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u805a\u5408\u67e5\u8be2\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16831", "pdf": "https://arxiv.org/pdf/2506.16831", "abs": "https://arxiv.org/abs/2506.16831", "authors": ["Filippo Scaramuzza", "Damian A. Tamburri", "Willem-Jan van den Heuvel"], "title": "Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap", "categories": ["cs.SE"], "comment": "To be published in https://link.springer.com/book/9789819672370", "summary": "This vision paper presents initial research on assessing the robustness and\nreliability of AI-enabled systems, and key factors in ensuring their safety and\neffectiveness in practical applications, including a focus on accountability.\nBy exploring evolving definitions of these concepts and reviewing current\nliterature, the study highlights major challenges and approaches in the field.\nA case study is used to illustrate real-world applications, emphasizing the\nneed for innovative testing solutions. The incorporation of accountability is\ncrucial for building trust and ensuring responsible AI development. The paper\noutlines potential future research directions and identifies existing gaps,\npositioning robustness, reliability, and accountability as vital areas for the\ndevelopment of trustworthy AI systems of the future.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u7cfb\u7edf\u7a33\u5065\u6027\u3001\u53ef\u9760\u6027\u548c\u95ee\u8d23\u5236\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u521b\u65b0\u6d4b\u8bd5\u65b9\u6848\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5065\u6027\u3001\u53ef\u9760\u6027\u548c\u95ee\u8d23\u5236\uff0c\u4ee5\u63a8\u52a8\u53ef\u4fe1AI\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790AI\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u3001\u53ef\u9760\u6027\u53ca\u95ee\u8d23\u5236\u7684\u5b9a\u4e49\u548c\u6311\u6218\u3002", "result": "\u53d1\u73b0\u521b\u65b0\u6d4b\u8bd5\u65b9\u6848\u548c\u95ee\u8d23\u5236\u662f\u786e\u4fddAI\u7cfb\u7edf\u53ef\u4fe1\u7684\u5173\u952e\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u4e0d\u8db3\u3002", "conclusion": "\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76AI\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u3001\u53ef\u9760\u6027\u548c\u95ee\u8d23\u5236\uff0c\u8fd9\u662f\u6784\u5efa\u53ef\u4fe1AI\u7cfb\u7edf\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2506.17025", "pdf": "https://arxiv.org/pdf/2506.17025", "abs": "https://arxiv.org/abs/2506.17025", "authors": ["Zhiyuan Lyu", "Qiguang Chen", "Gary P. T. Choi", "Lok Ming Lui"], "title": "Volumetric Parameterization for 3-Dimensional Simply-Connected Manifolds", "categories": ["cs.CG", "cs.GR", "math.DG"], "comment": null, "summary": "With advances in technology, there has been growing interest in developing\neffective mapping methods for 3-dimensional objects in recent years. Volumetric\nparameterization for 3D solid manifolds plays an important role in processing\n3D data. However, the conventional approaches cannot control the bijectivity\nand local geometric distortions of the result mappings due to the complex\nstructure of the solid manifolds. Moreover, prior methods mainly focus on one\nproperty instead of balancing different properties during the mapping process.\nIn this paper, we propose several novel methods for computing volumetric\nparameterizations for 3D simply-connected manifolds. Analogous to surface\nparameterization, our framework incorporates several models designed to\npreserve geometric structure, achieve density equalization, and optimally\nbalance geometric and density distortions. With these methods, various 3D\nmanifold parameterizations with different desired properties can be achieved.\nThese methods are tested on different examples and manifold remeshing\napplications, demonstrating their effectiveness and accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f53\u79ef\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u7b80\u5355\u8fde\u901a\u6d41\u5f62\uff0c\u65e8\u5728\u5e73\u8861\u51e0\u4f55\u7ed3\u6784\u548c\u5bc6\u5ea6\u5931\u771f\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63a7\u52363D\u6d41\u5f62\u6620\u5c04\u7684\u53cc\u5c04\u6027\u548c\u5c40\u90e8\u51e0\u4f55\u5931\u771f\uff0c\u4e14\u672a\u5e73\u8861\u4e0d\u540c\u5c5e\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u9896\u7684\u4f53\u79ef\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u7ed3\u6784\u4fdd\u6301\u3001\u5bc6\u5ea6\u5747\u8861\u548c\u5931\u771f\u5e73\u8861\u7684\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u4e0d\u540c\u7406\u60f3\u5c5e\u6027\u76843D\u6d41\u5f62\u53c2\u6570\u5316\uff0c\u5e76\u5728\u6d41\u5f62\u91cd\u7f51\u683c\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a3D\u6d41\u5f62\u53c2\u6570\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17112", "pdf": "https://arxiv.org/pdf/2506.17112", "abs": "https://arxiv.org/abs/2506.17112", "authors": ["Lukas Brand", "Fardad Vakilipoor", "S\u00f6ren Botsch", "Timo Jakumeit", "Sebastian Lotter", "Robert Schober", "Maximilian Sch\u00e4fer"], "title": "Closed-Loop Molecular Communication with Local and Global Degradation: Modeling and ISI Analysis", "categories": ["eess.SY", "cs.ET", "cs.SY"], "comment": null, "summary": "This paper presents a novel physics-based model for signal propagation in\nclosed-loop molecular communication (MC) systems, which are particularly\nrelevant for many envisioned biomedical applications, such as health monitoring\nor drug delivery within the closed-loop human cardiovascular system (CVS).\nCompared to open-loop systems, which are mostly considered in MC, closed-loop\nsystems exhibit different characteristic effects influencing signaling molecule\n(SM) propagation. One key phenomenon are the periodic SM arrivals at the\nreceiver (RX), leading to various types of inter-symbol interference (ISI)\ninherent to closed-loop system. To capture these characteristic effects, we\npropose an analytical model for the SM propagation inside closed-loop systems.\nThe model accounts for arbitrary spatio-temporal SM release patterns at the\ntransmitter (TX), and incorporates several environmental effects such as fluid\nflow, SM diffusion, and SM degradation. Moreover, to capture a wide range of\npractically relevant degradation and clearance mechanisms, the model includes\nboth local removal (e.g., due to SM absorption into organs) and global removal\n(e.g., due to chemical degradation) of SMs. The accuracy of the proposed model\nis validated with three-dimensional (3-D) particle-based simulations (PBSs).\nMoreover, we utilize the proposed model to develop a rigorous characterization\nof the various types of ISI encountered in closed-loop MC systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u7269\u7406\u7684\u95ed\u73af\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u4fe1\u53f7\u4f20\u64ad\u6a21\u578b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u751f\u7269\u533b\u5b66\u5e94\u7528\uff0c\u5982\u5fc3\u8840\u7ba1\u7cfb\u7edf\u4e2d\u7684\u5065\u5eb7\u76d1\u6d4b\u6216\u836f\u7269\u8f93\u9001\u3002", "motivation": "\u95ed\u73af\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u4e0e\u5f00\u73af\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5177\u6709\u4e0d\u540c\u7684\u4fe1\u53f7\u4f20\u64ad\u7279\u6027\uff0c\u5c24\u5176\u662f\u5468\u671f\u6027\u4fe1\u53f7\u5206\u5b50\u5230\u8fbe\u5f15\u8d77\u7684\u7b26\u53f7\u95f4\u5e72\u6270\uff08ISI\uff09\u3002\u4e3a\u4e86\u51c6\u786e\u5efa\u6a21\u8fd9\u4e9b\u7279\u6027\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u4e2a\u8003\u8651\u591a\u79cd\u73af\u5883\u56e0\u7d20\u7684\u4f20\u64ad\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6a21\u578b\uff0c\u8003\u8651\u4e86\u4fe1\u53f7\u5206\u5b50\u7684\u65f6\u7a7a\u91ca\u653e\u6a21\u5f0f\u3001\u6d41\u4f53\u6d41\u52a8\u3001\u6269\u6563\u548c\u964d\u89e3\u7b49\u73af\u5883\u6548\u5e94\uff0c\u5e76\u533a\u5206\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7684\u4fe1\u53f7\u5206\u5b50\u6e05\u9664\u673a\u5236\u3002\u6a21\u578b\u7684\u51c6\u786e\u6027\u901a\u8fc73D\u7c92\u5b50\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u95ed\u73af\u7cfb\u7edf\u4e2d\u7684\u4fe1\u53f7\u4f20\u64ad\u7279\u6027\uff0c\u5e76\u7528\u4e8e\u4e25\u683c\u8868\u5f81\u4e0d\u540c\u7c7b\u578b\u7684\u7b26\u53f7\u95f4\u5e72\u6270\u73b0\u8c61\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u95ed\u73af\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.16491", "pdf": "https://arxiv.org/pdf/2506.16491", "abs": "https://arxiv.org/abs/2506.16491", "authors": ["Henry Towsner"], "title": "Proofs that Modify Proofs, 1/2", "categories": ["math.LO", "cs.LO"], "comment": null, "summary": "This paper is a prelude and elaboration on Proofs that Modify Proofs. Here we\npresent an ordinal analysis of a fragment of the $\\mu$-calculus around the\nstrength of parameter-free $\\Pi^1_2$-comprehension using the same approach as\nthat paper, interpreting functions on proofs as proofs in an expanded system.\nWe build up the ordinal analysis in several stages, beginning by illustrating\nthe method systems at the strength of paremeter-free $\\Pi^1_1$-comprehension\nand full $\\Pi^1_1$-comprehension.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u300a\u8bc1\u660e\u4fee\u6539\u8bc1\u660e\u300b\u7684\u5e8f\u6570\u5206\u6790\uff0c\u901a\u8fc7\u6269\u5c55\u7cfb\u7edf\u89e3\u91ca\u8bc1\u660e\u51fd\u6570\uff0c\u9010\u6b65\u5206\u6790\u65e0\u53c2\u6570\u03a0\u2081\u2082-\u7406\u89e3\u7684\u7247\u6bb5\u3002", "motivation": "\u63a2\u8ba8\u8bc1\u660e\u4fee\u6539\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u5728\u65e0\u53c2\u6570\u03a0\u2081\u2082-\u7406\u89e3\u7247\u6bb5\u4e2d\u7684\u5e8f\u6570\u5f3a\u5ea6\u3002", "method": "\u91c7\u7528\u4e0e\u300a\u8bc1\u660e\u4fee\u6539\u8bc1\u660e\u300b\u76f8\u540c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u7cfb\u7edf\u89e3\u91ca\u8bc1\u660e\u51fd\u6570\uff0c\u5206\u9636\u6bb5\u6784\u5efa\u5e8f\u6570\u5206\u6790\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u65e0\u53c2\u6570\u03a0\u2081\u2081-\u7406\u89e3\u5230\u5b8c\u6574\u03a0\u2081\u2081-\u7406\u89e3\u7684\u5e8f\u6570\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u5206\u6790\u65e0\u53c2\u6570\u03a0\u2081\u2082-\u7406\u89e3\u7247\u6bb5\u7684\u5e8f\u6570\u5f3a\u5ea6\u3002"}}
{"id": "2506.16199", "pdf": "https://arxiv.org/pdf/2506.16199", "abs": "https://arxiv.org/abs/2506.16199", "authors": ["Mohammad Naiseh", "Huseyin Dogan", "Stephen Giff", "Nan Jiang"], "title": "Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)", "categories": ["cs.HC"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) plays a critical role in fostering\nuser trust and understanding in AI-driven systems. However, the design of\neffective XAI interfaces presents significant challenges, particularly for UX\nprofessionals who may lack technical expertise in AI or machine learning.\nExisting explanation methods, such as SHAP, LIME, and counterfactual\nexplanations, often rely on complex technical language and assumptions that are\ndifficult for non-expert users to interpret. To address these gaps, we propose\na UX Research (UXR) Playbook for XAI - a practical framework aimed at\nsupporting UX professionals in designing accessible, transparent, and\ntrustworthy AI experiences. Our playbook offers actionable guidance to help\nbridge the gap between technical explainability methods and user centred\ndesign, empowering designers to create AI interactions that foster better\nunderstanding, trust, and responsible AI adoption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4efd\u9488\u5bf9\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684UX\u7814\u7a76\u624b\u518c\uff0c\u65e8\u5728\u5e2e\u52a9UX\u4e13\u4e1a\u4eba\u5458\u8bbe\u8ba1\u66f4\u900f\u660e\u548c\u53ef\u4fe1\u7684AI\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524dXAI\u7684\u89e3\u91ca\u65b9\u6cd5\uff08\u5982SHAP\u3001LIME\uff09\u5bf9\u975e\u6280\u672f\u7528\u6237\u96be\u4ee5\u7406\u89e3\uff0cUX\u4e13\u4e1a\u4eba\u5458\u7f3a\u4e4f\u6280\u672f\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e86UXR Playbook\u6846\u67b6\uff0c\u8fde\u63a5\u6280\u672f\u89e3\u91ca\u65b9\u6cd5\u4e0e\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u3002", "result": "\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u6539\u5584\u7528\u6237\u5bf9AI\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "conclusion": "\u901a\u8fc7\u8be5\u624b\u518c\uff0c\u53ef\u4fc3\u8fdbAI\u7684\u8d1f\u8d23\u4efb\u91c7\u7528\u548c\u7528\u6237\u4f53\u9a8c\u4f18\u5316\u3002"}}
{"id": "2506.15888", "pdf": "https://arxiv.org/pdf/2506.15888", "abs": "https://arxiv.org/abs/2506.15888", "authors": ["Md Sakibur Sajal", "Hunter Guthrie", "Marc Dandin"], "title": "Bias Variation Compensation in Perimeter-Gated SPAD TRNGs", "categories": ["physics.ins-det", "cs.AR", "cs.CR", "cs.CV"], "comment": "5 pages, 8 figures, 1 software, accepted at MWSCAS 2025 conference", "summary": "Random number generators that utilize arrays of entropy source elements\nsuffer from bias variation (BV). Despite the availability of efficient\ndebiasing algorithms, optimized implementations of hardware friendly options\ndepend on the bit bias in the raw bit streams and cannot accommodate a wide BV.\nIn this work, we present a 64 x 64 array of perimeter gated single photon\navalanche diodes (pgSPADs), fabricated in a 0.35 {\\mu}m standard CMOS\ntechnology, as a source of entropy to generate random binary strings with a BV\ncompensation technique. By applying proper gate voltages based on the devices'\nnative dark count rates, we demonstrate less than 1% BV for a raw-bit\ngeneration rate of 2 kHz/pixel at room temperature. The raw bits were debiased\nusing the classical iterative Von Neumann's algorithm and the debiased bits\nwere found to pass all of the 16 tests from NIST's Statistical Test Suite.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u752864x64\u9635\u5217\u7684pgSPADs\u4f5c\u4e3a\u71b5\u6e90\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7BV\u8865\u507f\u6280\u672f\u751f\u6210\u968f\u673a\u4e8c\u8fdb\u5236\u5b57\u7b26\u4e32\uff0c\u6210\u529f\u5c06\u504f\u5dee\u964d\u81f31%\u4ee5\u4e0b\uff0c\u5e76\u901a\u8fc7\u4e86NIST\u7684\u7edf\u8ba1\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u968f\u673a\u6570\u751f\u6210\u5668\u4e2d\u71b5\u6e90\u9635\u5217\u7684\u504f\u5dee\u53d8\u5316\u95ee\u9898\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u80fd\u591f\u5728\u5bbd\u8303\u56f4BV\u4e0b\u6709\u6548\u5de5\u4f5c\u7684\u786c\u4ef6\u53cb\u597d\u65b9\u6848\u3002", "method": "\u91c7\u75280.35\u5fae\u7c73\u6807\u51c6CMOS\u6280\u672f\u5236\u902064x64\u9635\u5217\u7684pgSPADs\u4f5c\u4e3a\u71b5\u6e90\uff0c\u901a\u8fc7\u8c03\u8282\u6805\u6781\u7535\u538b\u8865\u507fBV\uff0c\u5e76\u5e94\u7528Von Neumann\u7b97\u6cd5\u53bb\u9664\u504f\u5dee\u3002", "result": "\u5728\u5ba4\u6e29\u4e0b\uff0c\u6bcf\u50cf\u7d202kHz\u7684\u539f\u59cb\u6bd4\u7279\u751f\u6210\u7387\u4e0b\uff0cBV\u4f4e\u4e8e1%\uff0c\u4e14\u53bb\u504f\u540e\u7684\u6bd4\u7279\u901a\u8fc7\u4e86NIST\u7684\u6240\u670916\u9879\u7edf\u8ba1\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u4f4e\u504f\u5dee\u7684\u9ad8\u6548\u968f\u673a\u6570\u751f\u6210\uff0c\u4e3a\u786c\u4ef6\u71b5\u6e90\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16976", "pdf": "https://arxiv.org/pdf/2506.16976", "abs": "https://arxiv.org/abs/2506.16976", "authors": ["Arthur Bernhardt", "Sajjad Tamimi", "Florian Stock", "Andreas Koch", "Ilia Petrov"], "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along", "categories": ["cs.DB"], "comment": null, "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u7684\u9884\u53d6\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u8fd1\u6570\u636e\u5904\u7406\u73af\u5883\u4e2d\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u4e0eI/O\u4ea4\u7ec7\u6700\u5927\u5316\u8ba1\u7b97\u5229\u7528\u7387\u3002", "motivation": "\u5185\u5b58\u5ef6\u8fdf\u548c\u5e26\u5bbd\u662f\u9650\u5236\u7cfb\u7edf\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u8f6f\u4ef6\u9884\u53d6\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u5c24\u5176\u5728\u65b0\u578bCPU\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "method": "\u7814\u7a76\u4e86\u8f6f\u4ef6\u9a71\u52a8\u7684\u3001\u540e\u6469\u5c14\u65f6\u4ee3\u7684\u7cfb\u7edf\uff0c\u5c06\u64cd\u4f5c\u5378\u8f7d\u5230\u667a\u80fd\u5185\u5b58\u4e2d\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u4e0eI/O\u7684\u4ea4\u7ec7\u4f18\u5316\u8ba1\u7b97\u5229\u7528\u7387\u3002", "result": "\u8f6f\u4ef6\u9884\u53d6\u5728\u8fd1\u6570\u636e\u5904\u7406\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u6f5c\u529b\u3002", "conclusion": "\u8f6f\u4ef6\u9884\u53d6\u6280\u672f\u5728\u672a\u6765\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\uff0c\u5c24\u5176\u662f\u5728\u8fd1\u6570\u636e\u5904\u7406\u65b9\u9762\u3002"}}
{"id": "2506.16876", "pdf": "https://arxiv.org/pdf/2506.16876", "abs": "https://arxiv.org/abs/2506.16876", "authors": ["Halit Eris", "Stefan Wagner"], "title": "Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems", "categories": ["cs.SE"], "comment": "Preprint to be published at SE4ADS", "summary": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with\nmultimodal sensory inputs, making rigorous validation and verification (V&V)\nessential for safety and reliability. These models pose challenges in\ndiagnosing failures, tracing anomalies, and maintaining transparency, with\ncurrent manual testing methods being inefficient and labor-intensive. This\nvision paper presents a methodology that integrates explainability,\ntransparency, and interpretability into V&V processes. We propose refining V&V\nrequirements through literature reviews and stakeholder input, generating\nexplainable test scenarios via large language models (LLMs), and enabling\nreal-time validation in simulation environments. Our framework includes test\noracle, explanation generation, and a test chatbot, with empirical studies\nplanned to evaluate improvements in diagnostic efficiency and transparency. Our\ngoal is to streamline V&V, reduce resources, and build user trust in autonomous\ntechnologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u53ef\u89e3\u91ca\u6027\u3001\u900f\u660e\u5ea6\u548c\u53ef\u7406\u89e3\u6027\u878d\u5165\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9a8c\u8bc1\u4e0e\u9a8c\u8bc1\uff08V&V\uff09\u6d41\u7a0b\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6548\u7387\u3001\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u5e76\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u590d\u6742\u51b3\u7b56\u6a21\u578b\u548c\u591a\u6a21\u6001\u8f93\u5165\u4f7f\u5176\u9a8c\u8bc1\u4e0e\u9a8c\u8bc1\u8fc7\u7a0b\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u624b\u52a8\u6d4b\u8bd5\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u8d39\u65f6\u8d39\u529b\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u5229\u76ca\u76f8\u5173\u8005\u53cd\u9988\u5b8c\u5584V&V\u9700\u6c42\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6d4b\u8bd5\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u73af\u5883\u5b9e\u73b0\u5b9e\u65f6\u9a8c\u8bc1\u3002\u6846\u67b6\u5305\u62ec\u6d4b\u8bd5\u9884\u8a00\u3001\u89e3\u91ca\u751f\u6210\u548c\u6d4b\u8bd5\u804a\u5929\u673a\u5668\u4eba\u3002", "result": "\u8ba1\u5212\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30\u6846\u67b6\u5728\u8bca\u65ad\u6548\u7387\u548c\u900f\u660e\u5ea6\u65b9\u9762\u7684\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u7b80\u5316V&V\u6d41\u7a0b\uff0c\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u5347\u7528\u6237\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u4fe1\u4efb\u3002"}}
{"id": "2506.17032", "pdf": "https://arxiv.org/pdf/2506.17032", "abs": "https://arxiv.org/abs/2506.17032", "authors": ["Abdulhaq Adetunji Salako", "Christian Tominski"], "title": "Toward Understanding Similarity of Visualization Techniques", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "The literature describes many visualization techniques for different types of\ndata, tasks, and application contexts, and new techniques are proposed on a\nregular basis. Visualization surveys try to capture the immense space of\ntechniques and structure it with meaningful categorizations. Yet, it remains\ndifficult to understand the similarity of visualization techniques in general.\nWe approach this open research question from two angles. First, we follow a\nmodel-driven approach that is based on defining the signature of visualization\ntechniques and interpreting the similarity of signatures as the similarity of\ntheir associated techniques. Second, following an expert-driven approach, we\nasked visualization experts in a small online study for their ad-hoc intuitive\nassessment of the similarity of pairs visualization techniques. From both\napproaches, we gain insight into the similarity of a set of 13 basic and\nadvanced visualizations for different types of data. While our results are so\nfar preliminary and academic, they are first steps toward better understanding\nthe similarity of visualization techniques.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u578b\u9a71\u52a8\u548c\u4e13\u5bb6\u9a71\u52a8\u4e24\u79cd\u65b9\u6cd5\u63a2\u8ba8\u4e86\u53ef\u89c6\u5316\u6280\u672f\u7684\u76f8\u4f3c\u6027\uff0c\u521d\u6b65\u5206\u6790\u4e8613\u79cd\u57fa\u672c\u548c\u9ad8\u7ea7\u53ef\u89c6\u5316\u6280\u672f\u7684\u76f8\u4f3c\u5ea6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u56e0\u4e3a\u73b0\u6709\u53ef\u89c6\u5316\u6280\u672f\u6570\u91cf\u5e9e\u5927\u4e14\u5206\u7c7b\u590d\u6742\uff0c\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u662f\u57fa\u4e8e\u6a21\u578b\u9a71\u52a8\u7684\u7b7e\u540d\u5b9a\u4e49\uff0c\u4e8c\u662f\u901a\u8fc7\u4e13\u5bb6\u5728\u7ebf\u8c03\u67e5\u8bc4\u4f30\u76f4\u89c2\u76f8\u4f3c\u6027\u3002", "result": "\u7814\u7a76\u521d\u6b65\u5f97\u51fa\u4e8613\u79cd\u53ef\u89c6\u5316\u6280\u672f\u7684\u76f8\u4f3c\u6027\uff0c\u7ed3\u679c\u5177\u6709\u5b66\u672f\u4ef7\u503c\u3002", "conclusion": "\u5c3d\u7ba1\u7ed3\u679c\u662f\u521d\u6b65\u7684\uff0c\u4f46\u5b83\u4eec\u4e3a\u7406\u89e3\u53ef\u89c6\u5316\u6280\u672f\u7684\u76f8\u4f3c\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u4e00\u6b65\u3002"}}
{"id": "2506.17135", "pdf": "https://arxiv.org/pdf/2506.17135", "abs": "https://arxiv.org/abs/2506.17135", "authors": ["Omid Faizy", "Norbert Wehn", "Paul Lukowicz", "Maximilian Kiefer-Emmanouilidis"], "title": "No Scratch Quantum Computing by Reducing Qubit Overhead for Efficient Arithmetics", "categories": ["quant-ph", "cs.ET", "cs.LO"], "comment": null, "summary": "Quantum arithmetic computation requires a substantial number of scratch\nqubits to stay reversible. These operations necessitate qubit and gate\nresources equivalent to those needed for the larger of the input or output\nregisters due to state encoding. Quantum Hamiltonian Computing (QHC) introduces\na novel approach by encoding input for logic operations within a single\nrotating quantum gate. This innovation reduces the required qubit register $ N\n$ to the size of the output states $ O $, where $ N = \\log_2 O $. Leveraging\nQHC principles, we present reversible half-adder and full-adder circuits that\ncompress the standard Toffoli + CNOT layout [Vedral et al., PRA, 54, 11,\n(1996)] from three-qubit and four-qubit formats for the Quantum half-adder\ncircuit and five sequential Fredkin gates using five qubits [Moutinho et al.,\nPRX Energy 2, 033002 (2023)] for full-adder circuit; into a two-qubit, 4$\\times\n$4 Hilbert space. This scheme, presented here, is optimized for classical logic\nevaluated on quantum hardware, which due to unitary evolution can bypass\nclassical CMOS energy limitations to certain degree. Although we avoid\nsuperposition of input and output states in this manuscript, this remains\nfeasible in principle. We see the best application for QHC in finding the\nminimal qubit and gate resources needed to evaluate any truth table, advancing\nFPGA capabilities using integrated quantum circuits or photonics.", "AI": {"tldr": "\u91cf\u5b50\u54c8\u5bc6\u987f\u8ba1\u7b97\uff08QHC\uff09\u901a\u8fc7\u5355\u65cb\u8f6c\u91cf\u5b50\u95e8\u7f16\u7801\u8f93\u5165\uff0c\u51cf\u5c11\u91cf\u5b50\u7b97\u903b\u8f91\u8fd0\u7b97\u6240\u9700\u7684\u91cf\u5b50\u6bd4\u7279\u8d44\u6e90\uff0c\u63d0\u51fa\u4f18\u5316\u7684\u534a\u52a0\u5668\u548c\u5168\u52a0\u5668\u7535\u8def\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u7b97\u672f\u8fd0\u7b97\u9700\u8981\u5927\u91cf\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u4ee5\u4fdd\u6301\u53ef\u9006\u6027\uff0c\u800cQHC\u65e8\u5728\u51cf\u5c11\u8d44\u6e90\u548c\u63d0\u9ad8\u6548\u7387\uff0c\u7a81\u7834\u7ecf\u5178CMOS\u80fd\u91cf\u9650\u5236\u3002", "method": "\u5229\u7528QHC\u539f\u7406\u8bbe\u8ba1\u534a\u52a0\u5668\u548c\u5168\u52a0\u5668\u7535\u8def\uff0c\u5c06\u4f20\u7edf\u591a\u91cf\u5b50\u6bd4\u7279\u7ed3\u6784\u538b\u7f29\u81f3\u4e24\u91cf\u5b50\u6bd4\u7279\u76844\u00d74\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u3002", "result": "QHC\u663e\u8457\u51cf\u5c11\u6240\u9700\u91cf\u5b50\u6bd4\u7279\u6570\u548c\u95e8\u8d44\u6e90\uff0c\u9002\u7528\u4e8e\u7ecf\u5178\u903b\u8f91\u5728\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u4f18\u5316\u5b9e\u73b0\u3002", "conclusion": "QHC\u4e3a\u91cf\u5b50\u7535\u8def\u548c\u5149\u5b50\u5b66\u63d0\u4f9b\u4e86\u8d44\u6e90\u4f18\u5316\u7684\u65b0\u9014\u5f84\uff0c\u5c24\u5176\u9002\u5408FPGA\u5e94\u7528\u3002"}}
{"id": "2506.16956", "pdf": "https://arxiv.org/pdf/2506.16956", "abs": "https://arxiv.org/abs/2506.16956", "authors": ["Noel Arteche", "Albert Atserias", "Susanna F. de Rezende", "Erfan Khaniki"], "title": "The Proof Analysis Problem", "categories": ["cs.CC", "cs.LO", "math.LO"], "comment": null, "summary": "Atserias and M\\\"uller (JACM, 2020) proved that for every unsatisfiable CNF\nformula $\\varphi$, the formula $\\operatorname{Ref}(\\varphi)$, stating\n\"$\\varphi$ has small Resolution refutations\", does not have subexponential-size\nResolution refutations. Conversely, when $\\varphi$ is satisfiable, Pudl\\'ak\n(TCS, 2003) showed how to construct a polynomial-size Resolution refutation of\n$\\operatorname{Ref}(\\varphi)$ given a satisfying assignment of $\\varphi$. A\nquestion that remained open is: do all short Resolution refutations of\n$\\operatorname{Ref}(\\varphi)$ explicitly leak a satisfying assignment of\n$\\varphi$?\n  We answer this question affirmatively by giving a polynomial-time algorithm\nthat extracts a satisfying assignment for $\\varphi$ given any short Resolution\nrefutation of $\\operatorname{Ref}(\\varphi)$. The algorithm follows from a new\nfeasibly constructive proof of the Atserias-M\\\"uller lower bound, formalizable\nin Cook's theory $\\mathsf{PV_1}$ of bounded arithmetic.\n  Motivated by this, we introduce a computational problem concerning Resolution\nlower bounds: the Proof Analysis Problem (PAP). For a proof system $Q$, the\nProof Analysis Problem for $Q$ asks, given a CNF formula $\\varphi$ and a\n$Q$-proof of a Resolution lower bound for $\\varphi$, encoded as $\\neg\n\\operatorname{Ref}(\\varphi)$, whether $\\varphi$ is satisfiable. In contrast to\nPAP for Resolution, we prove that PAP for Extended Frege (EF) is NP-complete.\n  Our results yield new insights into proof complexity: (i) every proof system\nsimulating EF is (weakly) automatable if and only if it is (weakly) automatable\non formulas stating Resolution lower bounds; (ii) we provide Ref formulas\nexponentially hard for bounded-depth Frege systems; and (iii) for every strong\nenough theory of arithmetic $T$ we construct unsatisfiable CNF formulas\nexponentially hard for Resolution but for which $T$ cannot prove even a\nquadratic lower bound.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Resolution\u8bc1\u660e\u7cfb\u7edf\u4e2d\uff0c\u77ed\u8bc1\u660e\u662f\u5426\u6cc4\u9732\u4e86\u6ee1\u8db3\u8d4b\u503c\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u548c\u6982\u5ff5\u3002", "motivation": "\u63a2\u8ba8Resolution\u77ed\u8bc1\u660e\u662f\u5426\u9690\u542b\u6ee1\u8db3\u8d4b\u503c\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u7b97\u6cd5\u63d0\u53d6\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8fdb\u4e00\u6b65\u7814\u7a76\u8bc1\u660e\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u4ece\u77edResolution\u8bc1\u660e\u4e2d\u63d0\u53d6\u6ee1\u8db3\u8d4b\u503c\uff0c\u5e76\u5f15\u5165Proof Analysis Problem (PAP)\u4ee5\u7814\u7a76\u8bc1\u660e\u7cfb\u7edf\u3002", "result": "\u786e\u8ba4\u77edResolution\u8bc1\u660e\u786e\u5b9e\u6cc4\u9732\u6ee1\u8db3\u8d4b\u503c\uff0c\u5e76\u8bc1\u660eEF\u7684PAP\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc1\u660e\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u8bc1\u660e\u7cfb\u7edf\u81ea\u52a8\u5316\u548c\u4e0b\u754c\u95ee\u9898\u7684\u8054\u7cfb\uff0c\u5e76\u6784\u9020\u4e86\u65b0\u7684\u786c\u5b9e\u4f8b\u3002"}}
{"id": "2506.16312", "pdf": "https://arxiv.org/pdf/2506.16312", "abs": "https://arxiv.org/abs/2506.16312", "authors": ["Angxuan Chen"], "title": "When learning analytics dashboard is explainable: An exploratory study on the effect of GenAI-supported learning analytics dashboard", "categories": ["cs.HC"], "comment": null, "summary": "This study investigated the impact of a theory-driven, explainable Learning\nAnalytics Dashboard (LAD) on university students' human-AI collaborative\nacademic abstract writing task. Grounded in Self-Regulated Learning (SRL)\ntheory and incorporating Explainable AI (XAI) principles, our LAD featured a\nthree-layered design (Visual, Explainable, Interactive). In an experimental\nstudy, participants were randomly assigned to either an experimental group\n(using the full explainable LAD) or a control group (using a visual-only LAD)\nto collaboratively write an academic abstract with a Generative AI. While\nquantitative analysis revealed no significant difference in the quality of\nco-authored abstracts between the two groups, a significant and noteworthy\ndifference emerged in conceptual understanding: students in the explainable LAD\ngroup demonstrated a superior grasp of abstract writing principles, as\nevidenced by their higher scores on a knowledge test (p= .026). These findings\nhighlight that while basic AI-generated feedback may suffice for immediate task\ncompletion, the provision of explainable feedback is crucial for fostering\ndeeper learning, enhancing conceptual understanding, and developing\ntransferable skills fundamental to self-regulated learning in academic writing\ncontexts.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u7406\u8bba\u548c\u53ef\u89e3\u91ca\u5b66\u4e60\u5206\u6790\u4eea\u8868\u76d8\uff08LAD\uff09\u5bf9\u5927\u5b66\u751f\u4e0eAI\u534f\u4f5c\u5199\u4f5c\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u4e24\u7ec4\u5199\u4f5c\u8d28\u91cf\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u53ef\u89e3\u91caLAD\u7ec4\u5728\u6982\u5ff5\u7406\u89e3\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\u7406\u8bba\u548c\u53ef\u89e3\u91caAI\uff08XAI\uff09\u539f\u5219\uff0c\u63d0\u5347\u5b66\u751f\u5728AI\u534f\u4f5c\u5199\u4f5c\u4e2d\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4f7f\u7528\u5b8c\u6574\u53ef\u89e3\u91caLAD\u7684\u5b9e\u9a8c\u7ec4\u548c\u4ec5\u89c6\u89c9LAD\u7684\u5bf9\u7167\u7ec4\u5728\u534f\u4f5c\u5199\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53ef\u89e3\u91caLAD\u7ec4\u5728\u6982\u5ff5\u7406\u89e3\u6d4b\u8bd5\u4e2d\u5f97\u5206\u663e\u8457\u66f4\u9ad8\uff08p=0.026\uff09\uff0c\u4f46\u5199\u4f5c\u8d28\u91cf\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u63d0\u4f9b\u53ef\u89e3\u91ca\u53cd\u9988\u5bf9\u6df1\u5316\u5b66\u4e60\u6982\u5ff5\u548c\u57f9\u517b\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.15918", "pdf": "https://arxiv.org/pdf/2506.15918", "abs": "https://arxiv.org/abs/2506.15918", "authors": ["Minbok Wi", "Seungmin Baek", "Seonyong Park", "Mattan Erez", "Jung Ho Ahn"], "title": "Sudoku: Decomposing DRAM Address Mapping into Component Functions", "categories": ["cs.CR", "cs.AR"], "comment": "6 pages, 6 figures, 2 tables, DRAMSec 2025", "summary": "Decomposing DRAM address mappings into component-level functions is critical\nfor understanding memory behavior and enabling precise RowHammer attacks, yet\nexisting reverse-engineering methods fall short. We introduce novel\ntiming-based techniques leveraging DRAM refresh intervals and consecutive\naccess latencies to infer component-specific functions. Based on this, we\npresent Sudoku, the first software-based tool to automatically decompose full\nDRAM address mappings into channel, rank, bank group, and bank functions while\nidentifying row and column bits. We validate Sudoku's effectiveness,\nsuccessfully decomposing mappings on recent Intel and AMD processors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u7684\u6280\u672fSudoku\uff0c\u80fd\u591f\u81ea\u52a8\u5206\u89e3DRAM\u5730\u5740\u6620\u5c04\u5230\u5404\u7ec4\u4ef6\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u9006\u5411\u5de5\u7a0b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5206\u89e3DRAM\u5730\u5740\u6620\u5c04\uff0c\u5f71\u54cd\u5185\u5b58\u884c\u4e3a\u7406\u89e3\u548cRowHammer\u653b\u51fb\u7684\u7cbe\u51c6\u6027\u3002", "method": "\u5229\u7528DRAM\u5237\u65b0\u95f4\u9694\u548c\u8fde\u7eed\u8bbf\u95ee\u5ef6\u8fdf\u7684\u65f6\u5e8f\u6280\u672f\uff0c\u5f00\u53d1\u8f6f\u4ef6\u5de5\u5177Sudoku\u3002", "result": "\u6210\u529f\u5728Intel\u548cAMD\u5904\u7406\u5668\u4e0a\u5206\u89e3\u51fa\u901a\u9053\u3001rank\u3001\u7ec4\u548cbank\u7b49\u529f\u80fd\u7684\u5730\u5740\u6620\u5c04\u3002", "conclusion": "Sudoku\u89e3\u51b3\u4e86DRAM\u5730\u5740\u6620\u5c04\u5206\u89e3\u7684\u96be\u9898\uff0c\u63d0\u5347\u4e86\u5185\u5b58\u884c\u4e3a\u7684\u7406\u89e3\u548c\u653b\u51fb\u7cbe\u786e\u6027\u3002"}}
{"id": "2506.15923", "pdf": "https://arxiv.org/pdf/2506.15923", "abs": "https://arxiv.org/abs/2506.15923", "authors": ["Liangyan Li", "Yangyi Liu", "Yimo Ning", "Stefano Rini", "Jun Chen"], "title": "PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for leveraging\ndiverse datasets from multiple sources while preserving data privacy by\navoiding centralized storage. However, many existing approaches fail to account\nfor the intricate gradient correlations between remote clients, a limitation\nthat becomes especially problematic in data heterogeneity scenarios. In this\nwork, we propose a novel FL framework utilizing Power-Norm Cosine Similarity\n(PNCS) to improve client selection for model aggregation. By capturing\nhigher-order gradient moments, PNCS addresses non-IID data challenges,\nenhancing convergence speed and accuracy. Additionally, we introduce a simple\nalgorithm ensuring diverse client selection through a selection history queue.\nExperiments with a VGG16 model across varied data partitions demonstrate\nconsistent improvements over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528PNCS\u6539\u8fdb\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u9009\u62e9\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u9ad8\u9636\u68af\u5ea6\u77e9\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5ba2\u6237\u7aef\u95f4\u7684\u68af\u5ea6\u76f8\u5173\u6027\uff0c\u5c24\u5176\u5728\u6570\u636e\u5f02\u8d28\u6027\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPNCS\u65b9\u6cd5\u7528\u4e8e\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u901a\u8fc7\u5386\u53f2\u961f\u5217\u4fdd\u8bc1\u591a\u6837\u6027\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u5212\u5206\u4e0b\uff0cVGG16\u6a21\u578b\u7684\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PNCS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.16878", "pdf": "https://arxiv.org/pdf/2506.16878", "abs": "https://arxiv.org/abs/2506.16878", "authors": ["Man Zhang", "Yuechen Li", "Tao Yue", "Kai-Yuan Cai"], "title": "Quantum Optimization for Software Engineering: A Survey", "categories": ["cs.SE"], "comment": null, "summary": "Quantum computing, particularly in the area of quantum optimization, is\nsteadily progressing toward practical applications, supported by an expanding\nrange of hardware platforms and simulators. While Software Engineering (SE)\noptimization has a strong foundation, which is exemplified by the active\nSearch-Based Software Engineering (SBSE) community and numerous classical\noptimization methods, the growing complexity of modern software systems and\ntheir engineering processes demands innovative solutions. This Systematic\nLiterature Review (SLR) focuses specifically on studying the literature that\napplies quantum or quantum-inspired algorithms to solve classical SE\noptimization problems. We examine 77 primary studies selected from an initial\npool of 2083 publications obtained through systematic searches of six digital\ndatabases using carefully crafted search strings. Our findings reveal\nconcentrated research efforts in areas such as SE operations and software\ntesting, while exposing significant gaps across other SE activities.\nAdditionally, the SLR uncovers relevant works published outside traditional SE\nvenues, underscoring the necessity of this comprehensive review. Overall, our\nstudy provides a broad overview of the research landscape, empowering the SBSE\ncommunity to leverage quantum advancements in addressing next-generation SE\nchallenges.", "AI": {"tldr": "\u672c\u6587\u732e\u56de\u987e\u7814\u7a76\u91cf\u5b50\u7b97\u6cd5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4f18\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e8677\u7bc7\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u7814\u7a76\u96c6\u4e2d\u5728\u67d0\u4e9b\u9886\u57df\u800c\u5176\u4ed6\u9886\u57df\u5b58\u5728\u7a7a\u767d\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u548c\u5de5\u7a0b\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u9700\u8981\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u91cf\u5b50\u8ba1\u7b97\u4e3a\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u6587\u732e\u56de\u987e\uff08SLR\uff09\uff0c\u4ece2083\u7bc7\u6587\u732e\u4e2d\u7b5b\u900977\u7bc7\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7814\u7a76\u96c6\u4e2d\u5728\u8f6f\u4ef6\u5de5\u7a0b\u64cd\u4f5c\u548c\u6d4b\u8bd5\u9886\u57df\uff0c\u5176\u4ed6\u6d3b\u52a8\u5b58\u5728\u663e\u8457\u7a7a\u767d\u3002", "conclusion": "\u7efc\u8ff0\u4e3a\u641c\u7d22\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\uff08SBSE\uff09\u793e\u533a\u63d0\u4f9b\u4e86\u91cf\u5b50\u8fdb\u5c55\u7684\u5168\u9762\u89c6\u91ce\uff0c\u52a9\u529b\u4e0b\u4e00\u4ee3\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\u3002"}}
{"id": "2506.17104", "pdf": "https://arxiv.org/pdf/2506.17104", "abs": "https://arxiv.org/abs/2506.17104", "authors": ["Chuxue Cao", "Mengze Li", "Juntao Dai", "Jinluan Yang", "Zijian Zhao", "Shengyu Zhang", "Weijie Shi", "Chengzhong Liu", "Sirui Han", "Yike Guo"], "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": null, "summary": "Large language models (LLMs) have shown promising first-order logic (FOL)\nreasoning capabilities with applications in various areas. However, their\neffectiveness in complex mathematical reasoning involving multi-step FOL\ndeductions is still under-researched. While LLMs perform competitively on\nestablished mathematical reasoning benchmarks, they struggle with multi-step\nFOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on\nour proposed theorem proving dataset. This issue arises from the limited\nexploration of diverse proof strategies and the potential for early reasoning\nmistakes to undermine entire proofs. To address these issues, we propose DREAM,\na self-adaptive solution that enhances the Diversity and REAsonability of LLMs'\ngeneration strategies. DREAM incorporates an Axiom-Driven Strategy\nDiversification mechanism to promote varied strategic outcomes and a\nSub-Proposition Error Feedback to help LLMs reflect on and correct their\nproofs. Our contributions include pioneering advancements in LLMs' mathematical\nreasoning through FOL theorem proving, introducing a novel inference stage\nsolution that improves performance by 0.6% to 6.4%, and providing a curated\ndataset of 447 mathematical theorems in Lean 4 format for evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DREAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728FOL\u63a8\u7406\u4e2d\u7684\u591a\u6837\u6027\u548c\u5408\u7406\u6027\uff0c\u6539\u8fdb\u4e86LLM\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLM\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u591a\u6b65FOL\u63a8\u7406\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u63d0\u5347\u5176\u591a\u6837\u6027\u548c\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u4e86DREAM\u65b9\u6cd5\uff0c\u5305\u62ecAxiom-Driven Strategy Diversification\u548cSub-Proposition Error Feedback\u673a\u5236\u3002", "result": "DREAM\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u6b65FOL\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6539\u8fdb\u5e45\u5ea6\u4e3a0.6%\u81f36.4%\u3002", "conclusion": "DREAM\u901a\u8fc7\u591a\u6837\u5316\u7b56\u7565\u548c\u9519\u8bef\u53cd\u9988\u673a\u5236\uff0c\u4e3aLLM\u5728\u591a\u6b65FOL\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16345", "pdf": "https://arxiv.org/pdf/2506.16345", "abs": "https://arxiv.org/abs/2506.16345", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Bruna Capeleti", "Rafael Ferreira Mello", "Andr\u00e9 Freire", "Luciana Zaina"], "title": "Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation", "categories": ["cs.HC", "H.5.2"], "comment": "Paper accepted at the 20th IFIP TC13 International Conference on\n  Human-Computer Interaction (INTERACT) 2025", "summary": "Heuristic evaluation is a widely used method in Human-Computer Interaction\n(HCI) to inspect interfaces and identify issues based on heuristics. Recently,\nLarge Language Models (LLMs), such as GPT-4o, have been applied in HCI to\nassist in persona creation, the ideation process, and the analysis of\nsemi-structured interviews. However, considering the need to understand\nheuristics and the high degree of abstraction required to evaluate them, LLMs\nmay have difficulty conducting heuristic evaluation. However, prior research\nhas not investigated GPT-4o's performance in heuristic evaluation compared to\nHCI experts in web-based systems. In this context, this study aims to compare\nthe results of a heuristic evaluation performed by GPT-4o and human experts. To\nthis end, we selected a set of screenshots from a web system and asked GPT-4o\nto perform a heuristic evaluation based on Nielsen's Heuristics from a\nliterature-grounded prompt. Our results indicate that only 21.2% of the issues\nidentified by human experts were also identified by GPT-4o, despite it found 27\nnew issues. We also found that GPT-4o performed better for heuristics related\nto aesthetic and minimalist design and match between system and real world,\nwhereas it has difficulty identifying issues in heuristics related to\nflexibility, control, and user efficiency. Additionally, we noticed that GPT-4o\ngenerated several false positives due to hallucinations and attempts to predict\nissues. Finally, we highlight five takeaways for the conscious use of GPT-4o in\nheuristic evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86GPT-4o\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5728\u542f\u53d1\u5f0f\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u4ec5\u8bc6\u522b\u51fa21.2%\u7684\u4eba\u7c7b\u4e13\u5bb6\u53d1\u73b0\u7684\u95ee\u9898\uff0c\u4f46\u4e5f\u53d1\u73b0\u4e8627\u4e2a\u65b0\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8GPT-4o\u5728\u542f\u53d1\u5f0f\u8bc4\u4f30\u4e2d\u7684\u80fd\u529b\u53ca\u5176\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u6587\u732e\u63d0\u793a\uff0c\u8ba9GPT-4o\u548c\u4eba\u7c7b\u4e13\u5bb6\u5206\u522b\u5bf9\u7f51\u9875\u7cfb\u7edf\u622a\u56fe\u8fdb\u884c\u542f\u53d1\u5f0f\u8bc4\u4f30\u3002", "result": "GPT-4o\u5728\u67d0\u4e9b\u542f\u53d1\u5f0f\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u7075\u6d3b\u6027\u3001\u63a7\u5236\u548c\u7528\u6237\u6548\u7387\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5b58\u5728\u8bef\u62a5\u95ee\u9898\u3002", "conclusion": "GPT-4o\u5728\u542f\u53d1\u5f0f\u8bc4\u4f30\u4e2d\u53ef\u8f85\u52a9\u4f7f\u7528\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u5c40\u9650\u6027\u5e76\u8c28\u614e\u4f7f\u7528\u3002"}}
{"id": "2506.16051", "pdf": "https://arxiv.org/pdf/2506.16051", "abs": "https://arxiv.org/abs/2506.16051", "authors": ["Zhiwei Li", "Carl Kesselman", "Tran Huy Nguyen", "Benjamin Yixing Xu", "Kyle Bolo", "Kimberley Yu"], "title": "From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience", "categories": ["cs.LG", "cs.DB", "cs.DL", "cs.HC"], "comment": null, "summary": "Reproducibility remains a central challenge in machine learning (ML),\nespecially in collaborative eScience projects where teams iterate over data,\nfeatures, and models. Current ML workflows are often dynamic yet fragmented,\nrelying on informal data sharing, ad hoc scripts, and loosely connected tools.\nThis fragmentation impedes transparency, reproducibility, and the adaptability\nof experiments over time. This paper introduces a data-centric framework for\nlifecycle-aware reproducibility, centered around six structured artifacts:\nDataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These\nartifacts formalize the relationships between data, code, and decisions,\nenabling ML experiments to be versioned, interpretable, and traceable over\ntime. The approach is demonstrated through a clinical ML use case of glaucoma\ndetection, illustrating how the system supports iterative exploration, improves\nreproducibility, and preserves the provenance of collaborative decisions across\nthe ML lifecycle.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53ef\u91cd\u73b0\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u79cd\u7ed3\u6784\u5316\u5de5\u4ef6\u652f\u6301\u5b9e\u9a8c\u7684\u7248\u672c\u63a7\u5236\u548c\u6eaf\u6e90\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u7f3a\u4e4f\u900f\u660e\u5ea6\u4e14\u788e\u7247\u5316\uff0c\u5bfc\u81f4\u53ef\u91cd\u73b0\u6027\u548c\u534f\u4f5c\u51b3\u7b56\u7684\u9002\u5e94\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4ee5\u516d\u79cd\u7ed3\u6784\u5316\u5de5\u4ef6\uff08\u6570\u636e\u96c6\u3001\u7279\u5f81\u3001\u5de5\u4f5c\u6d41\u3001\u6267\u884c\u3001\u8d44\u4ea7\u548c\u53d7\u63a7\u8bcd\u6c47\uff09\u4e3a\u6838\u5fc3\u7684\u6570\u636e\u4e2d\u5fc3\u6846\u67b6\u3002", "result": "\u6846\u67b6\u5728\u9752\u5149\u773c\u68c0\u6d4b\u7684\u4e34\u5e8a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u652f\u6301\u8fed\u4ee3\u63a2\u7d22\u3001\u63d0\u5347\u53ef\u91cd\u73b0\u6027\u548c\u4fdd\u7559\u6eaf\u6e90\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u5b66\u4e60\u7684\u751f\u547d\u5468\u671f\u7ba1\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u534f\u4f5c\u9879\u76ee\u7684\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u73b0\u6027\u3002"}}
{"id": "2506.16731", "pdf": "https://arxiv.org/pdf/2506.16731", "abs": "https://arxiv.org/abs/2506.16731", "authors": ["Jinlong Pang", "Jiaheng Wei", "Yifan Hua", "Chen Qian", "Yang Liu"], "title": "Incentivizing High-quality Participation From Federated Learning Agents", "categories": ["cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Federated learning (FL) provides a promising paradigm for facilitating\ncollaboration between multiple clients that jointly learn a global model\nwithout directly sharing their local data. However, existing research suffers\nfrom two caveats: 1) From the perspective of agents, voluntary and unselfish\nparticipation is often assumed. But self-interested agents may opt out of the\nsystem or provide low-quality contributions without proper incentives; 2) From\nthe mechanism designer's perspective, the aggregated models can be\nunsatisfactory as the existing game-theoretical federated learning approach for\ndata collection ignores the potential heterogeneous effort caused by\ncontributed data. To alleviate above challenges, we propose an incentive-aware\nframework for agent participation that considers data heterogeneity to\naccelerate the convergence process. Specifically, we first introduce the notion\nof Wasserstein distance to explicitly illustrate the heterogeneous effort and\nreformulate the existing upper bound of convergence. To induce truthful\nreporting from agents, we analyze and measure the generalization error gap of\nany two agents by leveraging the peer prediction mechanism to develop score\nfunctions. We further present a two-stage Stackelberg game model that\nformalizes the process and examines the existence of equilibrium. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of our\nproposed mechanism.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6fc0\u52b1\u673a\u5236\u6846\u67b6\uff0c\u8003\u8651\u6570\u636e\u5f02\u8d28\u6027\u4ee5\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u5047\u8bbe\u4ee3\u7406\u81ea\u613f\u65e0\u79c1\u53c2\u4e0e\uff0c\u4f46\u5b9e\u9645\u4e2d\u4ee3\u7406\u53ef\u80fd\u56e0\u81ea\u79c1\u800c\u9000\u51fa\u6216\u63d0\u4f9b\u4f4e\u8d28\u91cf\u8d21\u732e\uff0c\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u6570\u636e\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u52aa\u529b\u5dee\u5f02\u3002", "method": "\u5f15\u5165Wasserstein\u8ddd\u79bb\u91cf\u5316\u6570\u636e\u5f02\u8d28\u6027\uff0c\u5229\u7528\u540c\u4f34\u9884\u6d4b\u673a\u5236\u8bbe\u8ba1\u8bc4\u5206\u51fd\u6570\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6fc0\u52b1\u611f\u77e5\u6846\u67b6\u89e3\u51b3\u4e86\u4ee3\u7406\u53c2\u4e0e\u548c\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2506.16997", "pdf": "https://arxiv.org/pdf/2506.16997", "abs": "https://arxiv.org/abs/2506.16997", "authors": ["Hannah Deters", "Laura Reinhardt", "Jakob Droste", "Martin Obaidi", "Kurt Schneider"], "title": "Identifying Explanation Needs: Towards a Catalog of User-based Indicators", "categories": ["cs.SE"], "comment": "This paper has been accepted at the research track of the 33rd IEEE\n  International Requirements Engineering Conference (RE 2025)", "summary": "In today's digitalized world, where software systems are becoming\nincreasingly ubiquitous and complex, the quality aspect of explainability is\ngaining relevance. A major challenge in achieving adequate explanations is the\nelicitation of individual explanation needs, as it may be subject to severe\nhypothetical or confirmation biases. To address these challenges, we aim to\nestablish user-based indicators concerning user behavior or system events that\ncan be captured at runtime to determine when a need for explanations arises. In\nthis work, we conducted explorative research in form of an online study to\ncollect self-reported indicators that could indicate a need for explanation. We\ncompiled a catalog containing 17 relevant indicators concerning user behavior,\n8 indicators concerning system events and 14 indicators concerning emotional\nstates or physical reactions. We also analyze the relationships between these\nindicators and different types of need for explanation. The established\nindicators can be used in the elicitation process through prototypes, as well\nas after publication to gather requirements from already deployed applications\nusing telemetry and usage data. Moreover, these indicators can be used to\ntrigger explanations at appropriate moments during the runtime.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u6570\u5b57\u5316\u4e16\u754c\u4e2d\u8f6f\u4ef6\u7cfb\u7edf\u89e3\u91ca\u6027\u9700\u6c42\uff0c\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\u6536\u96c6\u5e76\u5206\u7c7b\u4e8639\u4e2a\u6f5c\u5728\u6307\u6807\uff0c\u7528\u4e8e\u8fd0\u884c\u65f6\u89e6\u53d1\u89e3\u91ca\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u5316\uff0c\u89e3\u91ca\u6027\u9700\u6c42\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u4e2a\u4f53\u9700\u6c42\u63d0\u53d6\u6613\u53d7\u504f\u89c1\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\u6536\u96c6\u7528\u6237\u81ea\u6211\u62a5\u544a\u6307\u6807\uff0c\u5206\u7c7b\u4e3a\u884c\u4e3a\u3001\u7cfb\u7edf\u4e8b\u4ef6\u548c\u60c5\u611f/\u8eab\u4f53\u53cd\u5e94\u4e09\u7c7b\u3002", "result": "\u786e\u5b9a\u4e8617\u79cd\u7528\u6237\u884c\u4e3a\u30018\u79cd\u7cfb\u7edf\u4e8b\u4ef6\u548c14\u79cd\u60c5\u611f/\u8eab\u4f53\u53cd\u5e94\u6307\u6807\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4e0e\u89e3\u91ca\u9700\u6c42\u7684\u5173\u7cfb\u3002", "conclusion": "\u8fd9\u4e9b\u6307\u6807\u53ef\u7528\u4e8e\u539f\u578b\u8bbe\u8ba1\u4e2d\u63d0\u53d6\u9700\u6c42\uff0c\u6216\u8fd0\u884c\u65f6\u89e6\u53d1\u89e3\u91ca\uff0c\u63d0\u5347\u7cfb\u7edf\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.16468", "pdf": "https://arxiv.org/pdf/2506.16468", "abs": "https://arxiv.org/abs/2506.16468", "authors": ["Vlad Cnejevici", "Matthias Ponfick", "Raul C. S\u00eempetru", "Alessandro Del Vecchio"], "title": "Closed-Loop Control of Electrical Stimulation through Spared Motor Unit Ensembles Restores Foot Movements after Spinal Cord Injury", "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "26 pages, 7 figures", "summary": "Restoring movement of a paralyzed foot is a key challenge in helping\nindividuals with neurological conditions such as spinal cord injury (SCI) to\nimprove their quality of life. Neuroprostheses based on functional electrical\nstimulation (FES) can restore the physiological range of motion by stimulating\nthe affected muscles using surface electrodes. We have previously shown that,\ndespite chronic motor-complete SCI, it is possible to capture paralyzed hand\nmovements in individuals with tetraplegia using spared and modulated motor unit\n(MU) activity decoded with non-invasive electromyography (EMG) sensors. This\nstudy investigated whether a wearable high-density surface EMG system could\ncapture and control paralyzed foot kinematics in closed-loop control with an\nFES system. We found that all our participants with SCI (2 with chronic SCI and\n3 with acute SCI) retained distinct spared EMG activity for at least three\nankle movements, which allowed them to reliably control a digital cursor using\ntheir spared tibialis anterior and triceps surae MU activity. Movement\nseparability was further reconfirmed by extracting task-modulated MU activity\nduring foot flexion/extension (3-7 modulated MUs/participant). Three\nparticipants were further able to modulate and maintain their foot\nflexion/extension EMG levels with an accuracy of >70%. Lastly, we show that\nreal-time control of a FES system using EMG from the affected limb can restore\nfoot movements in a highly intuitive way, significantly improving the lost or\npathological foot range of motion. Our system provides an intuitive approach\nfor closed-loop control of FES that has the potential to assist individuals\nwith SCI in regaining lost motor functions.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u5bc6\u5ea6\u8868\u9762EMG\u7684\u795e\u7ecf\u5047\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u901a\u8fc7\u95ed\u73afFES\u63a7\u5236\u6062\u590d\u762b\u75ea\u8db3\u90e8\u8fd0\u52a8\u3002", "motivation": "\u5e2e\u52a9\u810a\u9ad3\u635f\u4f24\uff08SCI\uff09\u60a3\u8005\u6062\u590d\u8db3\u90e8\u8fd0\u52a8\u529f\u80fd\uff0c\u63d0\u5347\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u53ef\u7a7f\u6234\u9ad8\u5bc6\u5ea6\u8868\u9762EMG\u7cfb\u7edf\u89e3\u7801\u6b8b\u4f59\u8fd0\u52a8\u5355\u5143\uff08MU\uff09\u6d3b\u52a8\uff0c\u7ed3\u5408FES\u5b9e\u73b0\u95ed\u73af\u63a7\u5236\u3002", "result": "SCI\u60a3\u8005\u4fdd\u7559\u4e86\u53ef\u533a\u5206\u7684\u8e1d\u90e8\u8fd0\u52a8EMG\u6d3b\u52a8\uff0c\u90e8\u5206\u53c2\u4e0e\u8005\u80fd\u7ef4\u6301>70%\u7684\u51c6\u786e\u7387\uff0c\u95ed\u73afFES\u6210\u529f\u6062\u590d\u4e86\u8db3\u90e8\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aSCI\u60a3\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u7684\u95ed\u73afFES\u63a7\u5236\u65b9\u6cd5\uff0c\u6709\u671b\u5e2e\u52a9\u6062\u590d\u8fd0\u52a8\u529f\u80fd\u3002"}}
{"id": "2506.16240", "pdf": "https://arxiv.org/pdf/2506.16240", "abs": "https://arxiv.org/abs/2506.16240", "authors": ["M. Bernaschi", "L. A. Fernandez", "I. Gonz\u00e1lez-Adalid Pemart\u00edn", "E. Marinari", "V. Martin-Mayor", "G. Parisi", "F. Ricci-Tersenghi", "J. J. Ruiz-Lorenzo", "D. Yllanes"], "title": "Microcanonical simulated annealing: Massively parallel Monte Carlo simulations with sporadic random-number generation", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "cs.AR", "physics.comp-ph"], "comment": "16 pages, 5 figures, 3 tables", "summary": "Numerical simulations of models and theories that describe complex\nexperimental systems $\\unicode{x2014}$in fields like high-energy and\ncondensed-matter physics$\\unicode{x2014}$ are becoming increasingly important.\nExamples include lattice gauge theories, which can describe, among others,\nquantum chromodynamics (the Standard Model description of strong interactions\nbetween elementary particles), and spin-glass systems. Beyond fundamental\nresearch, these computational methods also find practical applications, among\nmany others, in optimization, finance, and complex biological problems.\nHowever, Monte Carlo simulations, an important subcategory of these methods,\nare plagued by a major drawback: they are extremely greedy for (pseudo) random\nnumbers. The total fraction of computer time dedicated to random-number\ngeneration increases as the hardware grows more sophisticated, and can get\nprohibitive for special-purpose computing platforms. We propose here a\ngeneral-purpose microcanonical simulated annealing (mic.SA) formalism that\ndramatically reduces such a burden. The algorithm is fully adapted to a\nmassively parallel computation, as we show in the particularly demanding\nbenchmark of the three-dimensional Ising spin glass. We carry out very\nstringent numerical tests of the new algorithm by comparing our results,\nobtained on GPUs, with high-precision standard (i.e., random-number-greedy)\nsimulations performed on the Janus II custom-built supercomputer. In those\ncases where thermal equilibrium is reachable (i.e., in the paramagnetic phase),\nboth simulations reach compatible values. More significantly, barring\nshort-time corrections, a simple time rescaling suffices to map the mic.SA\noff-equilibrium dynamics onto the results obtained with standard simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540d\u4e3amic.SA\u7684\u5fae\u89c4\u8303\u6a21\u62df\u9000\u706b\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\u968f\u673a\u6570\u751f\u6210\u7684\u8d1f\u62c5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5728\u590d\u6742\u7cfb\u7edf\u6570\u503c\u6a21\u62df\u4e2d\u8017\u8d39\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u4e8e\u968f\u673a\u6570\u751f\u6210\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86mic.SA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u89c4\u8303\u6a21\u62df\u9000\u706b\u51cf\u5c11\u968f\u673a\u6570\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u5728\u4e09\u7ef4\u4f0a\u8f9b\u81ea\u65cb\u73bb\u7483\u6a21\u578b\u4e2d\u9a8c\u8bc1\uff0cmic.SA\u5728\u70ed\u5e73\u8861\u533a\u57df\u4e0e\u6807\u51c6\u6a21\u62df\u7ed3\u679c\u4e00\u81f4\uff0c\u4e14\u52a8\u529b\u5b66\u884c\u4e3a\u53ef\u901a\u8fc7\u7b80\u5355\u65f6\u95f4\u7f29\u653e\u5339\u914d\u3002", "conclusion": "mic.SA\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.16444", "pdf": "https://arxiv.org/pdf/2506.16444", "abs": "https://arxiv.org/abs/2506.16444", "authors": ["Kangqi Chen", "Andreas Kosmas Kakolyris", "Rakesh Nadig", "Manos Frouzakis", "Nika Mansouri Ghiasi", "Yu Liang", "Haiyu Mao", "Jisung Park", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing", "categories": ["cs.CL", "cs.AR", "cs.DB", "H.3.3; I.2.7"], "comment": "Extended version of our publication at the 52nd International\n  Symposium on Computer Architecture (ISCA-52), 2025", "summary": "Large Language Models (LLMs) face an inherent challenge: their knowledge is\nconfined to the data that they have been trained on. To overcome this issue,\nRetrieval-Augmented Generation (RAG) complements the static training-derived\nknowledge of LLMs with an external knowledge repository. RAG consists of three\nstages: indexing, retrieval, and generation. The retrieval stage of RAG becomes\na significant bottleneck in inference pipelines. In this stage, a user query is\nmapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)\nalgorithm searches for similar vectors in the database to identify relevant\nitems. Due to the large database sizes, ANNS incurs significant data movement\noverheads between the host and the storage system. To alleviate these\noverheads, prior works propose In-Storage Processing (ISP) techniques that\naccelerate ANNS by performing computations inside storage. However, existing\nworks that leverage ISP for ANNS (i) employ algorithms that are not tailored to\nISP systems, (ii) do not accelerate data retrieval operations for data selected\nby ANNS, and (iii) introduce significant hardware modifications, limiting\nperformance and hindering their adoption. We propose REIS, the first ISP system\ntailored for RAG that addresses these limitations with three key mechanisms.\nFirst, REIS employs a database layout that links database embedding vectors to\ntheir associated documents, enabling efficient retrieval. Second, it enables\nefficient ANNS by introducing an ISP-tailored data placement technique that\ndistributes embeddings across the planes of the storage system and employs a\nlightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that\nuses the existing computational resources inside the storage system. Compared\nto a server-grade system, REIS improves the performance (energy efficiency) of\nretrieval by an average of 13x (55x).", "AI": {"tldr": "REIS\u662f\u4e00\u79cd\u9488\u5bf9RAG\u6539\u8fdb\u7684ISP\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5e93\u5e03\u5c40\u3001\u6570\u636e\u653e\u7f6e\u6280\u672f\u548cANNS\u5f15\u64ce\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\u53d7\u9650\u4e8e\u5176\u8bad\u7ec3\u6570\u636e\uff0cRAG\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u5e93\u8865\u5145\u9759\u6001\u77e5\u8bc6\uff0c\u4f46\u68c0\u7d22\u9636\u6bb5\u6210\u4e3a\u74f6\u9888\u3002\u73b0\u6709ISP\u6280\u672f\u5b58\u5728\u7b97\u6cd5\u4e0d\u5339\u914d\u3001\u6570\u636e\u68c0\u7d22\u672a\u52a0\u901f\u548c\u786c\u4ef6\u4fee\u6539\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faREIS\u7cfb\u7edf\uff0c\u91c7\u7528\u4f18\u5316\u7684\u6570\u636e\u5e93\u5e03\u5c40\u3001ISP\u91cf\u8eab\u5b9a\u5236\u7684\u6570\u636e\u653e\u7f6e\u6280\u672f\u548c\u5229\u7528\u5b58\u50a8\u7cfb\u7edf\u73b0\u6709\u8d44\u6e90\u7684ANNS\u5f15\u64ce\u3002", "result": "\u76f8\u6bd4\u670d\u52a1\u5668\u7ea7\u7cfb\u7edf\uff0cREIS\u5728\u68c0\u7d22\u6027\u80fd\u4e0a\u5e73\u5747\u63d0\u534713\u500d\uff0c\u80fd\u6548\u63d0\u534755\u500d\u3002", "conclusion": "REIS\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u68c0\u7d22\u9636\u6bb5\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u672a\u6765ISP\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16875", "pdf": "https://arxiv.org/pdf/2506.16875", "abs": "https://arxiv.org/abs/2506.16875", "authors": ["Boris Martin", "Pierre Jolivet", "Christophe Geuzaine"], "title": "Comparison of substructured non-overlapping domain decomposition and overlapping additive Schwarz methods for large-scale Helmholtz problems with multiple sources", "categories": ["math.NA", "cs.DC", "cs.NA", "math.AP", "35J05, 65N55, 68W10, 35-04, 86-08", "J.2; G.1.3; G.1.8; G.4"], "comment": "21 pages, 10 figures, 5 tables. Preprint for a submission to SIAM\n  SISC", "summary": "Solving large-scale Helmholtz problems discretized with high-order finite\nelements is notoriously difficult, especially in 3D where direct factorization\nof the system matrix is very expensive and memory demanding, and robust\nconvergence of iterative methods is difficult to obtain. Domain decomposition\nmethods (DDM) constitute one of the most promising strategy so far, by\ncombining direct and iterative approaches: using direct solvers on overlapping\nor non-overlapping subdomains, as a preconditioner for a Krylov subspace method\non the original Helmholtz system or as an iterative solver on a substructured\nproblem involving field values or Lagrange multipliers on the interfaces\nbetween the subdomains. In this work we compare the computational performance\nof non-overlapping substructured DDM and Optimized Restricted Additive Schwarz\n(ORAS) preconditioners for solving large-scale Helmholtz problems with multiple\nsources, as is encountered, e.g., in frequency-domain Full Waveform Inversion.\nWe show on a realistic geophysical test-case that, when appropriately tuned,\nthe non-overlapping methods can reduce the convergence gap sufficiently to\nsignificantly outperform the overlapping methods.", "AI": {"tldr": "\u6bd4\u8f83\u975e\u91cd\u53e0\u5b50\u7ed3\u6784\u57df\u5206\u89e3\u65b9\u6cd5\u548c\u4f18\u5316\u7684\u9650\u5236\u52a0\u6027Schwarz\u9884\u6761\u4ef6\u5668\u5728\u89e3\u51b3\u5927\u89c4\u6a21Helmholtz\u95ee\u9898\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u793a\u9002\u5f53\u8c03\u6574\u540e\u975e\u91cd\u53e0\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u91cd\u53e0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21Helmholtz\u95ee\u9898\u5728\u9ad8\u9636\u6709\u9650\u5143\u79bb\u6563\u5316\u4e0b\u7684\u8ba1\u7b97\u56f0\u96be\uff0c\u5c24\u5176\u662f\u57283D\u4e2d\u76f4\u63a5\u5206\u89e3\u7cfb\u7edf\u77e9\u9635\u6210\u672c\u9ad8\u4e14\u5185\u5b58\u9700\u6c42\u5927\uff0c\u8fed\u4ee3\u65b9\u6cd5\u96be\u4ee5\u7a33\u5065\u6536\u655b\u3002", "method": "\u6bd4\u8f83\u975e\u91cd\u53e0\u5b50\u7ed3\u6784\u57df\u5206\u89e3\u65b9\u6cd5\uff08DDM\uff09\u548c\u4f18\u5316\u7684\u9650\u5236\u52a0\u6027Schwarz\u9884\u6761\u4ef6\u5668\uff08ORAS\uff09\u5728\u89e3\u51b3\u591a\u6e90Helmholtz\u95ee\u9898\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u5728\u73b0\u5b9e\u5730\u7403\u7269\u7406\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u9002\u5f53\u8c03\u6574\u540e\uff0c\u975e\u91cd\u53e0\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6536\u655b\u5dee\u8ddd\uff0c\u6027\u80fd\u4f18\u4e8e\u91cd\u53e0\u65b9\u6cd5\u3002", "conclusion": "\u975e\u91cd\u53e0\u5b50\u7ed3\u6784\u57df\u5206\u89e3\u65b9\u6cd5\u5728\u9002\u5f53\u8c03\u6574\u540e\uff0c\u80fd\u591f\u6210\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21Helmholtz\u95ee\u9898\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2506.17057", "pdf": "https://arxiv.org/pdf/2506.17057", "abs": "https://arxiv.org/abs/2506.17057", "authors": ["Fernando Pastor Ric\u00f3s", "Beatriz Mar\u00edn", "I. S. W. B. Prasetya", "Tanja E. J. Vos", "Joseph Davidson", "Karel Hovorka"], "title": "Behavior Driven Development for 3D Games", "categories": ["cs.SE"], "comment": null, "summary": "Computer 3D games are complex software environments that require novel\ntesting processes to ensure high-quality standards. The Intelligent\nVerification/Validation for Extended Reality Based Systems (iv4XR) framework\naddresses this need by enabling the implementation of autonomous agents to\nautomate game testing scenarios. This framework facilitates the automation of\nregression test cases for complex 3D games like Space Engineers. Nevertheless,\nthe technical expertise required to define test scripts using iv4XR can\nconstrain seamless collaboration between developers and testers. This paper\nreports how integrating a Behavior-driven Development (BDD) approach with the\niv4XR framework allows the industrial company behind Space Engineers to\nautomate regression testing. The success of this industrial collaboration has\ninspired the iv4XR team to integrate the BDD approach to improve the automation\nof play-testing for the experimental 3D game LabRecruits. Furthermore, the\niv4XR framework has been extended with tactical programming to enable the\nautomation of long-play test scenarios in Space Engineers. These results\nunderscore the versatility of the iv4XR framework in supporting diverse testing\napproaches while showcasing how BDD empowers users to create, manage, and\nexecute automated game tests using comprehensive and human-readable statements.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7\u5c06\u884c\u4e3a\u9a71\u52a8\u5f00\u53d1(BDD)\u4e0eiv4XR\u6846\u67b6\u7ed3\u5408\uff0c\u81ea\u52a8\u53163D\u6e38\u620f\u7684\u56de\u5f52\u6d4b\u8bd5\uff0c\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u7387\u4e0e\u534f\u4f5c\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfiv4XR\u6846\u67b6\u5728\u5b9a\u4e49\u6d4b\u8bd5\u811a\u672c\u65f6\u5bf9\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u4fc3\u8fdb\u5f00\u53d1\u8005\u4e0e\u6d4b\u8bd5\u8005\u4e4b\u95f4\u7684\u534f\u4f5c\u3002", "method": "\u5c06BDD\u65b9\u6cd5\u6574\u5408\u5230iv4XR\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u56de\u5f52\u6d4b\u8bd5\u548c\u6e38\u620f\u6d4b\u8bd5\u573a\u666f\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8eSpace Engineers\u548cLabRecruits\u6e38\u620f\u4e2d\uff0c\u63d0\u5347\u4e86\u6d4b\u8bd5\u81ea\u52a8\u5316\u6548\u7387\u548c\u53ef\u8bfb\u6027\u3002", "conclusion": "iv4XR\u6846\u67b6\u901a\u8fc7BDD\u96c6\u6210\u5c55\u73b0\u4e86\u5176\u591a\u529f\u80fd\u6027\uff0c\u540c\u65f6BDD\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u811a\u672c\u7684\u53ef\u7ba1\u7406\u6027\u548c\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2506.16473", "pdf": "https://arxiv.org/pdf/2506.16473", "abs": "https://arxiv.org/abs/2506.16473", "authors": ["Sophie Chiang", "Guy Laban", "Hatice Gunes"], "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As conversational agents increasingly engage in emotionally supportive\ndialogue, it is important to understand how closely their interactions resemble\nthose in traditional therapy settings. This study investigates whether the\nconcerns shared with a robot align with those shared in human-to-human (H2H)\ntherapy sessions, and whether robot responses semantically mirror those of\nhuman therapists. We analyzed two datasets: one of interactions between users\nand professional therapists (Hugging Face's NLP Mental Health Conversations),\nand another involving supportive conversations with a social robot (QTrobot\nfrom LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence\nembeddings and K-means clustering, we assessed cross-agent thematic alignment\nby applying a distance-based cluster-fitting method that evaluates whether\nresponses from one agent type map to clusters derived from the other, and\nvalidated it using Euclidean distances. Results showed that 90.88% of robot\nconversation disclosures could be mapped to clusters from the human therapy\ndataset, suggesting shared topical structure. For matched clusters, we compared\nthe subjects as well as therapist and robot responses using Transformer,\nWord2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'\ndisclosures in both datasets, as well as in the responses given to similar\nhuman disclosure themes across agent types (robot vs. human therapist). These\nfindings highlight both the parallels and boundaries of robot-led support\nconversations and their potential for augmenting mental health interventions.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u673a\u5668\u4eba\u60c5\u611f\u652f\u6301\u7684\u5bf9\u8bdd\u5185\u5bb9\u4e0e\u4eba\u7c7b\u6cbb\u7597\u5e08\u7684\u4f1a\u8bdd\u4e3b\u9898\u9ad8\u5ea6\u76f8\u4f3c\uff0c90.88%\u7684\u673a\u5668\u4eba\u5bf9\u8bdd\u53ef\u4ee5\u6620\u5c04\u5230\u4eba\u7c7b\u6cbb\u7597\u6570\u636e\u96c6\u7684\u805a\u7c7b\u4e2d\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e0e\u4eba\u7c7b\u6cbb\u7597\u5bf9\u8bdd\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u8bc4\u4f30\u673a\u5668\u4eba\u5728\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u53e5\u5b50\u5d4c\u5165\u548cK-means\u805a\u7c7b\uff0c\u6bd4\u8f83\u4eba\u4e0e\u673a\u5668\u4eba\u5bf9\u8bdd\u7684\u4e3b\u9898\u7ed3\u6784\u548c\u8bed\u4e49\u91cd\u53e0\u3002", "result": "\u673a\u5668\u4eba\u5bf9\u8bdd\u7684\u4e3b\u9898\u7ed3\u6784\u4e0e\u4eba\u7c7b\u6cbb\u7597\u5bf9\u8bdd\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u56de\u5e94\u8bed\u4e49\u76f8\u4f3c\u3002", "conclusion": "\u673a\u5668\u4eba\u5bf9\u8bdd\u5728\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6ce8\u610f\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2506.16654", "pdf": "https://arxiv.org/pdf/2506.16654", "abs": "https://arxiv.org/abs/2506.16654", "authors": ["Vijay Prakash Dwivedi", "Charilaos Kanatsoulis", "Shenyang Huang", "Jure Leskovec"], "title": "Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Graph machine learning has led to a significant increase in the capabilities\nof models that learn on arbitrary graph-structured data and has been applied to\nmolecules, social networks, recommendation systems, and transportation, among\nother domains. Data in multi-tabular relational databases can also be\nconstructed as 'relational entity graphs' for Relational Deep Learning (RDL) -\na new blueprint that enables end-to-end representation learning without\ntraditional feature engineering. Compared to arbitrary graph-structured data,\nrelational entity graphs have key properties: (i) their structure is defined by\nprimary-foreign key relationships between entities in different tables, (ii)\nthe structural connectivity is a function of the relational schema defining a\ndatabase, and (iii) the graph connectivity is temporal and heterogeneous in\nnature. In this paper, we provide a comprehensive review of RDL by first\nintroducing the representation of relational databases as relational entity\ngraphs, and then reviewing public benchmark datasets that have been used to\ndevelop and evaluate recent GNN-based RDL models. We discuss key challenges\nincluding large-scale multi-table integration and the complexities of modeling\ntemporal dynamics and heterogeneous data, while also surveying foundational\nneural network methods and recent architectural advances specialized for\nrelational entity graphs. Finally, we explore opportunities to unify these\ndistinct modeling challenges, highlighting how RDL converges multiple\nsub-fields in graph machine learning towards the design of foundation models\nthat can transform the processing of relational data.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\uff08RDL\uff09\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u591a\u8868\u5173\u7cfb\u6570\u636e\u5e93\u8868\u793a\u4e3a\u5173\u7cfb\u5b9e\u4f53\u56fe\uff0c\u5e76\u603b\u7ed3\u4e86GNN\u6a21\u578b\u5728RDL\u4e2d\u7684\u5e94\u7528\u4e0e\u6311\u6218\u3002", "motivation": "\u591a\u8868\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u7684\u6570\u636e\u53ef\u4ee5\u901a\u8fc7\u5173\u7cfb\u5b9e\u4f53\u56fe\u8fdb\u884c\u8868\u793a\uff0c\u4ece\u800c\u652f\u6301\u7aef\u5230\u7aef\u7684\u8868\u793a\u5b66\u4e60\uff0c\u65e0\u9700\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u3002", "method": "\u901a\u8fc7\u5c06\u5173\u7cfb\u6570\u636e\u5e93\u8868\u793a\u4e3a\u5173\u7cfb\u5b9e\u4f53\u56fe\uff0c\u5229\u7528GNN\u7b49\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u9488\u5bf9\u5927\u89c4\u6a21\u591a\u8868\u96c6\u6210\u3001\u65f6\u95f4\u52a8\u6001\u6027\u548c\u5f02\u6784\u6570\u636e\u7b49\u6311\u6218\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u603b\u7ed3\u4e86RDL\u9886\u57df\u7684\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u3001\u5173\u952e\u6311\u6218\u53ca\u6700\u65b0\u67b6\u6784\u8fdb\u5c55\uff0c\u4e3a\u5173\u7cfb\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u5efa\u6a21\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "RDL\u5c06\u56fe\u673a\u5668\u5b66\u4e60\u7684\u591a\u4e2a\u5b50\u9886\u57df\u7edf\u4e00\u8d77\u6765\uff0c\u4e3a\u8bbe\u8ba1\u80fd\u591f\u5904\u7406\u5173\u7cfb\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.17095", "pdf": "https://arxiv.org/pdf/2506.17095", "abs": "https://arxiv.org/abs/2506.17095", "authors": ["Ronnie de Souza Santos", "Matheus de Morais Leca", "Reydne Santos", "Cleyton Magalhaes"], "title": "Software Fairness Testing in Practice", "categories": ["cs.SE"], "comment": null, "summary": "Software testing ensures that a system functions correctly, meets specified\nrequirements, and maintains high quality. As artificial intelligence and\nmachine learning (ML) technologies become integral to software systems, testing\nhas evolved to address their unique complexities. A critical advancement in\nthis space is fairness testing, which identifies and mitigates biases in AI\napplications to promote ethical and equitable outcomes. Despite extensive\nacademic research on fairness testing, including test input generation, test\noracle identification, and component testing, practical adoption remains\nlimited. Industry practitioners often lack clear guidelines and effective tools\nto integrate fairness testing into real-world AI development. This study\ninvestigates how software professionals test AI-powered systems for fairness\nthrough interviews with 22 practitioners working on AI and ML projects. Our\nfindings highlight a significant gap between theoretical fairness concepts and\nindustry practice. While fairness definitions continue to evolve, they remain\ndifficult for practitioners to interpret and apply. The absence of\nindustry-aligned fairness testing tools further complicates adoption,\nnecessitating research into practical, accessible solutions. Key challenges\ninclude data quality and diversity, time constraints, defining effective\nmetrics, and ensuring model interoperability. These insights emphasize the need\nto bridge academic advancements with actionable strategies and tools, enabling\npractitioners to systematically address fairness in AI systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u548c\u6307\u5357\u4ee5\u5e2e\u52a9\u5de5\u4e1a\u754c\u91c7\u7eb3\u3002", "motivation": "\u968f\u7740AI\u548cML\u6280\u672f\u6210\u4e3a\u8f6f\u4ef6\u7cfb\u7edf\u7684\u6838\u5fc3\uff0c\u516c\u5e73\u6027\u6d4b\u8bd5\u6210\u4e3a\u786e\u4fdd\u5176\u4f26\u7406\u548c\u516c\u6b63\u6027\u7684\u5173\u952e\uff0c\u4f46\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u7f3a\u4e4f\u660e\u786e\u7684\u6307\u5bfc\u548c\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0822\u540d\u4ece\u4e8bAI\u548cML\u9879\u76ee\u7684\u4e13\u4e1a\u4eba\u58eb\uff0c\u7814\u7a76\u4ed6\u4eec\u5728\u5b9e\u9645\u5de5\u4f5c\u4e2d\u5982\u4f55\u6d4b\u8bd5AI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u516c\u5e73\u6027\u6982\u5ff5\u96be\u4ee5\u88ab\u5b9e\u8df5\u8005\u89e3\u8bfb\u548c\u5e94\u7528\uff0c\u4e14\u7f3a\u4e4f\u884c\u4e1a\u5bf9\u9f50\u7684\u5de5\u5177\uff0c\u5bfc\u81f4\u7406\u8bba\u6210\u679c\u96be\u4ee5\u843d\u5730\u3002", "conclusion": "\u9700\u8981\u5c06\u5b66\u672f\u8fdb\u5c55\u8f6c\u5316\u4e3a\u5b9e\u7528\u5de5\u5177\u548c\u7b56\u7565\uff0c\u4ee5\u7cfb\u7edf\u6027\u89e3\u51b3AI\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2506.16542", "pdf": "https://arxiv.org/pdf/2506.16542", "abs": "https://arxiv.org/abs/2506.16542", "authors": ["Nathalia Gomez", "S. Sue Batham", "Mathias Volonte", "Tiffany D. Do"], "title": "Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence", "categories": ["cs.HC"], "comment": "6 pages, To Appear in Companion Publication of the 2025 Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW Companion '25)", "summary": "Technical interviews are a critical yet stressful step in the hiring process\nfor computer science graduates, often hindered by limited access to practice\nopportunities. This formative qualitative study (n=20) explores whether a\nmultimodal AI system can realistically simulate technical interviews and\nsupport confidence-building among candidates. Participants engaged with an\nAI-driven mock interview tool featuring whiteboarding tasks and real-time\nfeedback. Many described the experience as realistic and helpful, noting\nincreased confidence and improved articulation of problem-solving decisions.\nHowever, challenges with conversational flow and timing were noted. These\nfindings demonstrate the potential of AI-driven technical interviews as\nscalable and realistic preparation tools, suggesting that future research could\nexplore variations in interviewer behavior and their potential effects on\ncandidate preparation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001AI\u7cfb\u7edf\u5982\u4f55\u6a21\u62df\u6280\u672f\u9762\u8bd5\uff0c\u5e2e\u52a9\u6c42\u804c\u8005\u5efa\u7acb\u4fe1\u5fc3\u3002\u53c2\u4e0e\u8005\u8ba4\u4e3a\u4f53\u9a8c\u771f\u5b9e\u4e14\u6709\u76ca\uff0c\u4f46\u4e5f\u5b58\u5728\u5bf9\u8bdd\u6d41\u7545\u6027\u548c\u65f6\u95f4\u63a7\u5236\u7684\u6311\u6218\u3002", "motivation": "\u6280\u672f\u9762\u8bd5\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u6bd5\u4e1a\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u8df5\u673a\u4f1a\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u662f\u5426\u80fd\u591f\u6709\u6548\u6a21\u62df\u9762\u8bd5\uff0c\u63d0\u5347\u6c42\u804c\u8005\u4fe1\u5fc3\u3002", "method": "20\u540d\u53c2\u4e0e\u8005\u4f7f\u7528\u4e00\u6b3eAI\u9a71\u52a8\u7684\u6a21\u62df\u9762\u8bd5\u5de5\u5177\uff0c\u5b8c\u6210\u767d\u677f\u9898\u76ee\u5e76\u83b7\u53d6\u5b9e\u65f6\u53cd\u9988\u3002", "result": "\u591a\u6570\u53c2\u4e0e\u8005\u8ba4\u4e3a\u4f53\u9a8c\u771f\u5b9e\u4e14\u63d0\u5347\u4e86\u4fe1\u5fc3\uff0c\u4f46\u5bf9\u8bdd\u6d41\u7545\u6027\u548c\u65f6\u95f4\u7ba1\u7406\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u6280\u672f\u9762\u8bd5\u5de5\u5177\u6709\u6f5c\u529b\u6210\u4e3a\u53ef\u6269\u5c55\u7684\u51c6\u5907\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u7814\u7a76\u9762\u8bd5\u5b98\u884c\u4e3a\u53d8\u5316\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.17120", "pdf": "https://arxiv.org/pdf/2506.17120", "abs": "https://arxiv.org/abs/2506.17120", "authors": ["Atish Kumar Dipongkor", "Ziyu Yao", "Kevin Moran"], "title": "Reassessing Code Authorship Attribution in the Era of Language Models", "categories": ["cs.SE"], "comment": "12 pages", "summary": "The study of Code Stylometry, and in particular Code Authorship Attribution\n(CAA), aims to analyze coding styles to identify the authors of code samples.\nCAA is crucial in cybersecurity and software forensics for addressing,\ndetecting plagiarism, and supporting criminal prosecutions. However, CAA is a\ncomplex and error prone task, due to the need for recognizing nuanced\nrelationships between coding patterns. This challenge is compounded in large\nsoftware systems with numerous authors due to the subtle variability of\npatterns that signify the coding style of one author among many. Given the\nchallenges related to this task, researchers have proposed and studied\nautomated approaches that rely upon classical Machine Learning and Deep\nLearning techniques. However, such techniques have historically relied upon\nhand-crafted features, and due to the often intricate interaction of different\nfeatures (e.g., formatting, etc.), have key limitations in properly\ncharacterizing authorship, and are sensitive to adversarial code perturbations.\nRecently, transformer-based Language Models (LMs) have shown remarkable\nefficacy across a range of software engineering tasks, and in the authorship\nattribution on natural language in the NLP domain. However, their effectiveness\nin CAA is not well understood. As such, we conduct the first extensive\nempirical study applying two larger state-of-the-art code LMs, and five smaller\ncode LMs to the task of CAA to 6 diverse datasets that encompass 12k code\nsnippets written by 463 developers. Furthermore, we perform an in-depth\nanalysis of our studied models' performance on CAA using established machine\nlearning interpretability techniques. The results of our analysis illustrate\nimportant findings that illuminate the behavior of LMs in understanding\nstylometric code patterns during the task of CAA, and point towards important\ndirections for future work.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u98ce\u683c\u6d4b\u5b9a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u4ee3\u7801\u4f5c\u8005\u5f52\u5c5e\uff08CAA\uff09\u4efb\u52a1\uff0c\u63a2\u8ba8\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8bc6\u522b\u4ee3\u7801\u4f5c\u8005\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u9996\u6b21\u6df1\u5165\u8bc4\u4f30\u4e86\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728CAA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4ee3\u7801\u4f5c\u8005\u5f52\u5c5e\u5728\u7f51\u7edc\u5b89\u5168\u548c\u8f6f\u4ef6\u53d6\u8bc1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u4e14\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u8fd9\u4e00\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e24\u79cd\u5927\u578b\u548c\u6700\u5148\u8fdb\u7684\u4ee3\u7801LMs\u4ee5\u53ca\u4e94\u79cd\u5c0f\u578b\u4ee3\u7801LMs\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u57286\u4e2a\u6570\u636e\u96c6\uff08\u5305\u542b12k\u4ee3\u7801\u7247\u6bb5\uff0c\u6765\u81ea463\u540d\u5f00\u53d1\u8005\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002", "result": "\u5206\u6790\u7ed3\u679c\u63ed\u793a\u4e86LMs\u5728\u7406\u89e3\u4ee3\u7801\u98ce\u683c\u6a21\u5f0f\u65f6\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002", "conclusion": "Transformer-based LMs\u5728CAA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002"}}
{"id": "2506.16571", "pdf": "https://arxiv.org/pdf/2506.16571", "abs": "https://arxiv.org/abs/2506.16571", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Capturing Visualization Design Rationale", "categories": ["cs.HC"], "comment": null, "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a2\u7a76\u53ef\u89c6\u5316\u8bbe\u8ba1\u80cc\u540e\u7684\u903b\u8f91\uff0c\u5229\u7528\u5b66\u751f\u7684\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u95ee\u9898-\u7b54\u6848-\u903b\u8f91\u4e09\u5143\u7ec4\u3002", "motivation": "\u4ee5\u5f80\u7684\u6570\u636e\u96c6\u591a\u5173\u6ce8\u53ef\u89c6\u5316\u89e3\u7801\uff0c\u800c\u5ffd\u7565\u4e86\u8bbe\u8ba1\u7684\u7f16\u7801\u903b\u8f91\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u5b66\u751f\u7684\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u548cLLMs\u751f\u6210\u95ee\u9898-\u7b54\u6848-\u903b\u8f91\u4e09\u5143\u7ec4\uff0c\u5e76\u9a8c\u8bc1\u548c\u6574\u7406\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6355\u6349\u5b66\u751f\u53ef\u89c6\u5316\u8bbe\u8ba1\u9009\u62e9\u53ca\u903b\u8f91\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u53ef\u89c6\u5316\u8bbe\u8ba1\u80cc\u540e\u7684\u903b\u8f91\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.17125", "pdf": "https://arxiv.org/pdf/2506.17125", "abs": "https://arxiv.org/abs/2506.17125", "authors": ["Xue Jiang", "Yihong Dong", "Zheng Fang", "Yingwei Ma", "Tangxinyu Wang", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Yongbin Li", "Ge Li"], "title": "Large Language Model Unlearning for Source Code", "categories": ["cs.SE"], "comment": null, "summary": "LLM4SE has demonstrated significant success, but LLMs' potential memorization\nof sensitive or outdated training data introduces critical risks to legal\ncompliance, software security, and code quality. LLM unlearning techniques,\nwhich can eliminate the influence of undesired data from LLMs in a\npost-training way, present a promising solution to address these concerns.\nWhile recent efforts in LLM unlearning show effectiveness in natural language,\ntheir applicability to source code remains underexplored. Our empirical study\nreveals that existing LLM unlearning approaches, when applied to source code,\ncause severe model utility degradation, rendering models practically unusable\nfor code generation. In this paper, we propose PROD, a novel unlearning\napproach that enables LLMs to forget undesired code content while effectively\npreserving their code generation capabilities. PROD suppresses the probability\nof forget data in LLMs' output distribution while promoting candidate\ndistributional components, enabling the model to jointly learn to forget\nspecific content and retain its general capabilities. To facilitate this study,\nwe establish a benchmark for code unlearning evaluation, which includes three\ncritical downstream tasks: copyrighted code unlearning, insecure code\nunlearning, and deprecated API unlearning. Our evaluation demonstrates that\nPROD achieves superior balance between forget quality and model utility\ncompared to existing unlearning approaches across three downstream tasks, while\nconsistently exhibiting improvements when applied to LLMs of varying series.\nPROD also exhibits superior robustness against adversarial attacks without\ngenerating or exposing the data to be forgotten. The results underscore that\nour approach not only extends the application boundary of unlearning techniques\nto source code, but also holds significant implications for advancing reliable\ncode generation.", "AI": {"tldr": "PROD\u662f\u4e00\u79cd\u65b0\u578b\u7684LLM\u9057\u5fd8\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u6e90\u4ee3\u7801\u95ee\u9898\uff0c\u80fd\u591f\u5728\u6d88\u9664\u4e0d\u5fc5\u8981\u6570\u636e\u7684\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "motivation": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u867d\u6210\u529f\uff0c\u4f46\u8bb0\u5fc6\u654f\u611f\u6216\u8fc7\u65f6\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u5e26\u6765\u5408\u89c4\u6027\u3001\u5b89\u5168\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u9057\u5fd8\u6280\u672f\u3002", "method": "PROD\u901a\u8fc7\u6291\u5236\u9057\u5fd8\u6570\u636e\u7684\u8f93\u51fa\u6982\u7387\u5e76\u63d0\u5347\u5019\u9009\u5206\u5e03\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u540c\u65f6\u9057\u5fd8\u7279\u5b9a\u5185\u5bb9\u548c\u4fdd\u7559\u901a\u7528\u80fd\u529b\u3002", "result": "PROD\u5728\u7248\u6743\u4ee3\u7801\u3001\u4e0d\u5b89\u5168\u4ee3\u7801\u548c\u8fc7\u65f6API\u4e09\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PROD\u5c06\u9057\u5fd8\u6280\u672f\u5e94\u7528\u4e8e\u6e90\u4ee3\u7801\u9886\u57df\uff0c\u4e3a\u63d0\u5347\u53ef\u9760\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2506.16677", "pdf": "https://arxiv.org/pdf/2506.16677", "abs": "https://arxiv.org/abs/2506.16677", "authors": ["Hao Guo", "Wei Fan", "Shaohui Liu", "Feng Jiang", "Chunzhi Yi"], "title": "PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Trust prediction is a key issue in human-robot collaboration, especially in\nconstruction scenarios where maintaining appropriate trust calibration is\ncritical for safety and efficiency. This paper introduces the\nPerformance-guided Physiological signal-based Trust Prediction (PPTP), a novel\nframework designed to improve trust assessment. We designed a human-robot\nconstruction scenario with three difficulty levels to induce different trust\nstates. Our approach integrates synchronized multimodal physiological signals\n(ECG, GSR, and EMG) with collaboration performance evaluation to predict human\ntrust levels. Individual physiological signals are processed using\ncollaboration performance information as guiding cues, leveraging the\nstandardized nature of collaboration performance to compensate for individual\nvariations in physiological responses. Extensive experiments demonstrate the\nefficacy of our cross-modality fusion method in significantly improving trust\nclassification performance. Our model achieves over 81% accuracy in three-level\ntrust classification, outperforming the best baseline method by 6.7%, and\nnotably reaches 74.3% accuracy in high-resolution seven-level classification,\nwhich is a first in trust prediction research. Ablation experiments further\nvalidate the superiority of physiological signal processing guided by\ncollaboration performance assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPPTP\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u548c\u534f\u4f5c\u6027\u80fd\u8bc4\u4f30\u6765\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u4efb\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u7684\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u5efa\u7b51\u9886\u57df\uff0c\u4fe1\u4efb\u9884\u6d4b\u5bf9\u5b89\u5168\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e2a\u4f53\u751f\u7406\u4fe1\u53f7\u5dee\u5f02\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4fe1\u4efb\u3002", "method": "PPTP\u6846\u67b6\u901a\u8fc7\u540c\u6b65\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\uff08ECG\u3001GSR\u548cEMG\uff09\u548c\u534f\u4f5c\u6027\u80fd\u8bc4\u4f30\uff0c\u5229\u7528\u534f\u4f5c\u6027\u80fd\u7684\u6807\u51c6\u5316\u7279\u6027\u8865\u507f\u4e2a\u4f53\u751f\u7406\u53cd\u5e94\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u9884\u6d4b\u4fe1\u4efb\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e09\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8681%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd56.7%\u3002\u5728\u4e03\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u8fbe\u523074.3%\uff0c\u662f\u4fe1\u4efb\u9884\u6d4b\u9886\u57df\u7684\u9996\u6b21\u7a81\u7834\u3002", "conclusion": "PPTP\u6846\u67b6\u901a\u8fc7\u534f\u4f5c\u6027\u80fd\u5f15\u5bfc\u7684\u751f\u7406\u4fe1\u53f7\u5904\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u4efb\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2506.17208", "pdf": "https://arxiv.org/pdf/2506.17208", "abs": "https://arxiv.org/abs/2506.17208", "authors": ["Matias Martinez", "Xavier Franch"], "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86SWE-Bench Lite\u548cVerified\u4e0a\u7684\u63d0\u4ea4\uff0c\u5206\u6790\u4e8667\u79cd\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4e13\u6709LLM\u7684\u4e3b\u5bfc\u5730\u4f4d\u4ee5\u53ca\u5f00\u53d1\u8005\u591a\u6837\u6027\u3002", "motivation": "\u7531\u4e8eSWE-Bench\u63d0\u4ea4\u7f3a\u4e4f\u8be6\u7ec6\u6587\u6863\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u6027\u7814\u7a76\u63ed\u793a\u5176\u67b6\u6784\u8bbe\u8ba1\u548c\u6765\u6e90\u3002", "method": "\u5206\u6790\u4e8668\u4e2aSWE-Bench Lite\u548c79\u4e2aVerified\u7684\u63d0\u4ea4\uff0c\u4ece\u63d0\u4ea4\u8005\u7c7b\u578b\u3001\u4ea7\u54c1\u53ef\u7528\u6027\u3001LLM\u4f7f\u7528\u548c\u7cfb\u7edf\u67b6\u6784\u7b49\u7ef4\u5ea6\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4e13\u6709LLM\uff08\u5982Claude 3.5/3.7\uff09\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u8bbe\u8ba1\u4e2d\u6709\u4ee3\u7406\u548c\u975e\u4ee3\u7406\u4e24\u79cd\u6a21\u5f0f\uff0c\u63d0\u4ea4\u8005\u4ece\u4e2a\u4eba\u5f00\u53d1\u8005\u5230\u5927\u516c\u53f8\u5747\u6709\u3002", "conclusion": "\u7814\u7a76\u4e3aAPR\u9886\u57df\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u5168\u9762\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u6280\u672f\u8d8b\u52bf\u548c\u5f00\u53d1\u8005\u751f\u6001\u3002"}}
{"id": "2506.16716", "pdf": "https://arxiv.org/pdf/2506.16716", "abs": "https://arxiv.org/abs/2506.16716", "authors": ["Qixin Wang", "Songtao Zhou", "Zeyu Jin", "Chenglin Guo", "Shikun Sun", "Xiaoyu Qin"], "title": "V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos", "categories": ["cs.HC"], "comment": "Accepted by IJCNN 2025", "summary": "Automatic video commentary systems are widely used on multimedia social media\nplatforms to extract factual information about video content. However, current\nsystems may overlook essential para-linguistic cues, including emotion and\nattitude, which are critical for fully conveying the meaning of visual content.\nThe absence of these cues can limit user understanding or, in some cases,\ndistort the video's original intent. Expressive speech effectively conveys\nthese cues and enhances the user's comprehension of videos. Building on these\ninsights, this paper explores the usage of vision-context-aware expressive\nspeech in enhancing users' understanding of videos in video commentary systems.\nFirstly, our formatting study indicates that semantic-only speech can lead to\nambiguity, and misaligned emotions between speech and visuals may distort\ncontent interpretation. To address this, we propose a method called\nvision-context-aware speech synthesis (V-CASS). It analyzes para-linguistic\ncues from visuals using a vision-language model and leverages a\nknowledge-infused language model to guide the expressive speech model in\ngenerating context-aligned speech. User studies show that V-CASS enhances\nemotional and attitudinal resonance, as well as user audio-visual understanding\nand engagement, with 74.68% of participants preferring the system. Finally, we\nexplore the potential of our method in helping blind and low-vision users\nnavigate web videos, improving universal accessibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bed\u97f3\u5408\u6210\u65b9\u6cd5\uff08V-CASS\uff09\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u9891\u4e2d\u7684\u526f\u8bed\u8a00\u7ebf\u7d22\u751f\u6210\u60c5\u611f\u548c\u6001\u5ea6\u4e00\u81f4\u7684\u8bed\u97f3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u89c6\u542c\u7406\u89e3\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u89e3\u8bf4\u7cfb\u7edf\u5e38\u5ffd\u89c6\u60c5\u611f\u548c\u6001\u5ea6\u7b49\u526f\u8bed\u8a00\u7ebf\u7d22\uff0c\u5bfc\u81f4\u7528\u6237\u7406\u89e3\u53d7\u9650\u6216\u5185\u5bb9\u539f\u610f\u88ab\u66f2\u89e3\u3002", "method": "\u63d0\u51faV-CASS\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5206\u6790\u526f\u8bed\u8a00\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u8bed\u97f3\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u8bed\u97f3\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cV-CASS\u663e\u8457\u63d0\u9ad8\u4e86\u60c5\u611f\u5171\u9e23\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c74.68%\u7684\u53c2\u4e0e\u8005\u66f4\u504f\u597d\u8be5\u7cfb\u7edf\u3002", "conclusion": "V-CASS\u4e0d\u4ec5\u589e\u5f3a\u4e86\u7528\u6237\u7684\u89c6\u542c\u4f53\u9a8c\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u5e2e\u52a9\u89c6\u969c\u7528\u6237\u8bbf\u95ee\u7f51\u7edc\u89c6\u9891\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u63d0\u5347\u4e86\u666e\u9002\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2506.15790", "pdf": "https://arxiv.org/pdf/2506.15790", "abs": "https://arxiv.org/abs/2506.15790", "authors": ["Chenyang Peng", "Haijun Wang", "Yin Wu", "Hao Wu", "Ming Fan", "Yitao Zhao", "Ting Liu"], "title": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis", "categories": ["cs.CR", "cs.SE", "D.2"], "comment": "4 pages, 1 figure. Submitted to the 16th Asia-Pacific Symposium on\n  Internetware (Internetware 2025)", "summary": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aETrace\u7684\u65b0\u578b\u4e8b\u4ef6\u9a71\u52a8\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\uff0c\u65e0\u9700\u6e90\u4ee3\u7801\u5373\u53ef\u901a\u8fc7LLM\u8ffd\u8e2a\u5206\u6790\u6f5c\u5728\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u533a\u5757\u94fe\u6280\u672f\u5e94\u7528\u5e7f\u6cdb\uff0c\u667a\u80fd\u5408\u7ea6\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6e90\u4ee3\u7801\u5206\u6790\uff0c\u4f46\u5e76\u975e\u6240\u6709\u5408\u7ea6\u90fd\u63d0\u4f9b\u6e90\u4ee3\u7801\u3002", "method": "ETrace\u901a\u8fc7\u4ece\u4ea4\u6613\u65e5\u5fd7\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u81ea\u9002\u5e94\u8bed\u4e49\u89e3\u91ca\u5668\uff0c\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u91cd\u5efa\u4e8b\u4ef6\u5206\u6790\uff0c\u5e76\u91c7\u7528\u6a21\u5f0f\u5339\u914d\u5efa\u7acb\u4ea4\u6613\u884c\u4e3a\u4e0e\u5df2\u77e5\u653b\u51fb\u884c\u4e3a\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86ETrace\u7684\u6709\u6548\u6027\u3002", "conclusion": "ETrace\u4e3a\u65e0\u9700\u6e90\u4ee3\u7801\u7684\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16851", "pdf": "https://arxiv.org/pdf/2506.16851", "abs": "https://arxiv.org/abs/2506.16851", "authors": ["Ankolika De", "Kelley Cotter", "Shaheen Kanthawala", "Haley McAtee", "Amy Ritchart", "Gahana Kadur"], "title": "\"Whoever needs to see it, will see it\": Motivations and Labor of Creating Algorithmic Conspirituality Content on TikTok", "categories": ["cs.HC", "H.5.0"], "comment": "27 pages, Proc. ACM Hum.-Comput. Interact. 8", "summary": "Recent studies show that users often interpret social media algorithms as\nmystical or spiritual because of their unpredictability. This invites new\nquestions about how such perceptions affect the content that creators create\nand the communities they form online. In this study, 14 creators of algorithmic\nconspirituality content on TikTok were interviewed to explore their\ninterpretations and creation processes influenced by the platform's For You\nPage algorithm. We illustrate how creators' beliefs interact with TikTok's\nalgorithmic mediation to reinforce and shape their spiritual or relational\nthemes. Furthermore, we show how algorithmic conspirituality content impacts\nviewers, highlighting its role in generating significant emotional and\naffective labor for creators, stemming from complex relational dynamics\ninherent in this content creation. We discuss implications for design to\nsupport creators aimed at recognizing the unexpected spiritual and religious\nexperiences algorithms prompt, as well as supporting creators in effectively\nmanaging these challenges.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u793e\u4ea4\u5a92\u4f53\u7b97\u6cd5\u7684\u795e\u79d8\u5316\u89e3\u8bfb\u5982\u4f55\u5f71\u54cd\u5185\u5bb9\u521b\u4f5c\u8005\u53ca\u5176\u793e\u533a\uff0c\u57fa\u4e8e\u5bf914\u4f4dTikTok\u521b\u4f5c\u8005\u7684\u8bbf\u8c08\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u4e0e\u7075\u6027\u4e3b\u9898\u7684\u4e92\u52a8\u53ca\u5176\u5bf9\u89c2\u4f17\u548c\u521b\u4f5c\u8005\u7684\u60c5\u611f\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u5e38\u5c06\u793e\u4ea4\u5a92\u4f53\u7b97\u6cd5\u89c6\u4e3a\u795e\u79d8\u6216\u4e0d\u53ef\u9884\u6d4b\u7684\u5b58\u5728\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u7b97\u6cd5\u89e3\u8bfb\u5982\u4f55\u5851\u9020\u5185\u5bb9\u521b\u4f5c\u548c\u5728\u7ebf\u793e\u533a\u7684\u7591\u95ee\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0814\u4f4dTikTok\u7684\u201c\u7b97\u6cd5\u7075\u6027\u201d\u5185\u5bb9\u521b\u4f5c\u8005\uff0c\u7814\u7a76\u5176\u89e3\u8bfb\u548c\u521b\u4f5c\u8fc7\u7a0b\u5982\u4f55\u53d7\u5230\u5e73\u53f0\u7b97\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u521b\u4f5c\u8005\u7684\u4fe1\u4ef0\u4e0e\u7b97\u6cd5\u4e92\u52a8\uff0c\u5f3a\u5316\u4e86\u7075\u6027\u6216\u5173\u7cfb\u4e3b\u9898\uff0c\u5e76\u63ed\u793a\u4e86\u6b64\u7c7b\u5185\u5bb9\u5bf9\u89c2\u4f17\u53ca\u521b\u4f5c\u8005\u60c5\u611f\u52b3\u52a8\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u8bbe\u8ba1\u652f\u6301\uff0c\u4ee5\u8bc6\u522b\u7b97\u6cd5\u5f15\u53d1\u7684\u7075\u6027\u4f53\u9a8c\uff0c\u5e76\u5e2e\u52a9\u521b\u4f5c\u8005\u6709\u6548\u5e94\u5bf9\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2506.15955", "pdf": "https://arxiv.org/pdf/2506.15955", "abs": "https://arxiv.org/abs/2506.15955", "authors": ["Tong Hu", "Songzan Wang"], "title": "From Generation to Adaptation: Comparing AI-Assisted Strategies in High School Programming Education", "categories": ["cs.CY", "cs.SE"], "comment": null, "summary": "This exploratory case study investigated two contrasting pedagogical\napproaches for LCA-assisted programming with five novice high school students\npreparing for a WeChat Mini Program competition. In Phase 1, students used LCAs\nto generate code from abstract specifications (From-Scratch approach),\nachieving only 20% MVP completion. In Phase 2, students adapted existing\nMinimal Functional Units (MFUs), small, functional code examples, using LCAs,\nachieving 100% MVP completion. Analysis revealed that the MFU-based approach\nsucceeded by aligning with LCA strengths in pattern modification rather than de\nnovo generation, while providing cognitive scaffolds that enabled students to\nnavigate complex development tasks. The study introduces a dual-scaffolding\nmodel combining technical support (MFUs) with pedagogical guidance (structured\nprompting strategies), demonstrating that effective LCA integration depends\nless on AI capabilities than on instructional design. These findings offer\npractical guidance for educators seeking to transform AI tools from sources of\nfrustration into productive learning partners in programming education.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e24\u79cdLCA\u8f85\u52a9\u7f16\u7a0b\u6559\u5b66\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8eMFU\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7LCA\u8f85\u52a9\u7f16\u7a0b\u6559\u80b2\uff0c\u5e2e\u52a9\u5b66\u751f\u5728WeChat\u5c0f\u7a0b\u5e8f\u7ade\u8d5b\u4e2d\u6210\u529f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b9e\u9a8c\uff08From-Scratch\u4e0eMFU\u65b9\u6cd5\uff09\uff0c\u5206\u6790\u5b66\u751f\u8868\u73b0\u3002", "result": "MFU\u65b9\u6cd5\u5b8c\u6210\u7387100%\uff0cFrom-Scratch\u4ec520%\uff0cMFU\u66f4\u7b26\u5408LCA\u4f18\u52bf\u3002", "conclusion": "LCA\u7684\u6709\u6548\u6574\u5408\u4f9d\u8d56\u4e8e\u6559\u5b66\u8bbe\u8ba1\uff0c\u63d0\u51fa\u53cc\u811a\u624b\u67b6\u6a21\u578b\uff0c\u4e3a\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.16874", "pdf": "https://arxiv.org/pdf/2506.16874", "abs": "https://arxiv.org/abs/2506.16874", "authors": ["Zhiqing Wang", "Haoxiang Fan", "Shiwei Wu", "Qiaoyi Chen", "Yongqi Liang", "Zhenhui Peng"], "title": "Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools", "categories": ["cs.HC"], "comment": null, "summary": "The integration of Generative Artificial Intelligence (GenAI) in K-6\nproject-based art courses presents both opportunities and challenges for\nenhancing creativity, engagement, and group collaboration. This study\nintroduces a four-phase field study, involving in total two experienced K-6 art\nteachers and 132 students in eight offline course sessions, to investigate the\nusage and impact of GenAI. Specifically, based on findings in Phases 1 and 2,\nwe developed AskArt, an interactive interface that combines DALL-E and GPT and\nis tailored to support elementary school students in their art projects, and\ndeployed it in Phases 3 and 4. Our findings revealed the benefits of GenAI in\nproviding background information, inspirations, and personalized guidance.\nHowever, challenges in query formulation for generating expected content were\nalso observed. Moreover, students employed varied collaboration strategies, and\nteachers noted increased engagement alongside concerns regarding misuse and\ninterface suitability. This study offers insights into the effective\nintegration of GenAI in elementary education, presents AskArt as a practical\ntool, and provides recommendations for educators and researchers to enhance\nproject-based learning with GenAI technologies.", "AI": {"tldr": "\u7814\u7a76\u4e86GenAI\u5728K-6\u827a\u672f\u8bfe\u7a0b\u4e2d\u7684\u6574\u5408\u6548\u679c\uff0c\u5f00\u53d1\u4e86AskArt\u5de5\u5177\uff0c\u5c55\u793a\u4e86GenAI\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u63a2\u7d22GenAI\u5982\u4f55\u63d0\u5347\u5c0f\u5b66\u751f\u7684\u521b\u9020\u529b\u3001\u53c2\u4e0e\u5ea6\u548c\u56e2\u961f\u5408\u4f5c\u80fd\u529b\u3002", "method": "\u56db\u9636\u6bb5\u5b9e\u5730\u7814\u7a76\uff0c\u6d89\u53ca2\u540d\u6559\u5e08\u548c132\u540d\u5b66\u751f\uff0c\u5f00\u53d1\u5e76\u90e8\u7f72AskArt\u4ea4\u4e92\u754c\u9762\u3002", "result": "GenAI\u80fd\u63d0\u4f9b\u80cc\u666f\u4fe1\u606f\u3001\u7075\u611f\u548c\u4e2a\u6027\u5316\u6307\u5bfc\uff0c\u4f46\u5b58\u5728\u67e5\u8be2\u751f\u6210\u5185\u5bb9\u7684\u6311\u6218\u548c\u5b66\u751f\u5408\u4f5c\u7b56\u7565\u591a\u6837\u7684\u95ee\u9898\u3002", "conclusion": "GenAI\u5728\u57fa\u7840\u6559\u80b2\u4e2d\u5177\u6709\u6f5c\u529b\uff0cAskArt\u662f\u5b9e\u7528\u5de5\u5177\uff0c\u4f46\u9700\u6ce8\u610f\u4f7f\u7528\u4e2d\u7684\u6311\u6218\u548c\u6559\u80b2\u8005\u7684\u5f15\u5bfc\u3002"}}
{"id": "2506.16492", "pdf": "https://arxiv.org/pdf/2506.16492", "abs": "https://arxiv.org/abs/2506.16492", "authors": ["Renato Cordeiro Ferreira", "Thatiane de Oliveira Rosa", "Alfredo Goldman", "Eduardo Guerra"], "title": "Teaching Complex Systems based on Microservices", "categories": ["cs.CY", "cs.SE", "D.2.11; D.2.3; D.2.1"], "comment": "4 pages, 3 figures (2 diagrams, 2 tables), reviewed and presented at\n  AMP2020", "summary": "Developing complex systems using microservices is a current challenge. In\nthis paper, we present our experience with teaching this subject to more than\n80 students at the University of S\\~ao Paulo (USP), fostering team work and\nsimulating the industry's environment. We show it is possible to teach such\nadvanced concepts for senior undergraduate students of Computer Science and\nrelated fields.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u4eab\u4e86\u5728\u5723\u4fdd\u7f57\u5927\u5b66\u6559\u638880\u591a\u540d\u5b66\u751f\u5fae\u670d\u52a1\u5f00\u53d1\u7684\u7ecf\u9a8c\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5411\u9ad8\u5e74\u7ea7\u672c\u79d1\u751f\u6559\u6388\u590d\u6742\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u5f00\u53d1\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u590d\u6742\u6027\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6559\u5b66\u5b9e\u8df5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56e2\u961f\u5408\u4f5c\u548c\u6a21\u62df\u5de5\u4e1a\u73af\u5883\u7684\u65b9\u5f0f\uff0c\u6559\u6388\u9ad8\u5e74\u7ea7\u672c\u79d1\u751f\u5fae\u670d\u52a1\u5f00\u53d1\u3002", "result": "\u5b9e\u8df5\u8bc1\u660e\uff0c\u9ad8\u5e74\u7ea7\u672c\u79d1\u751f\u80fd\u591f\u638c\u63e1\u5fae\u670d\u52a1\u5f00\u53d1\u7684\u590d\u6742\u6982\u5ff5\u3002", "conclusion": "\u901a\u8fc7\u5408\u9002\u7684\u6559\u5b66\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5411\u9ad8\u5e74\u7ea7\u672c\u79d1\u751f\u4f20\u6388\u5fae\u670d\u52a1\u5f00\u53d1\u7684\u5148\u8fdb\u77e5\u8bc6\u548c\u6280\u672f\u3002"}}
{"id": "2506.17011", "pdf": "https://arxiv.org/pdf/2506.17011", "abs": "https://arxiv.org/abs/2506.17011", "authors": ["Bruno Campos"], "title": "Juicy or Dry? A Comparative Study of User Engagement and Information Retention in Interactive Infographics", "categories": ["cs.HC"], "comment": null, "summary": "This study compares the impact of \"juiciness\" on user engagement and\nshort-term information retention in interactive infographics. Juicy designs\ngenerally showed a slight advantage in overall user engagement scores compared\nto dry designs. Specifically, the juicy version of the Burcalories infographic\nhad the highest engagement score. However, the differences in engagement were\noften small. Regarding information retention, the results were mixed. The juicy\nversions of The Daily Routines of Famous Creative People and The Main Chakras\ninfographics showed marginally better average recall and more participants with\nhigher recall. Conversely, the dry version of Burcalories led to more correct\nanswers in multiple-choice questions. The study suggests that while juicy\ndesign elements can enhance user engagement and, in some cases, short-term\ninformation retention, their effectiveness depends on careful implementation.\nExcessive juiciness could be overwhelming or distracting, while\nwell-implemented juicy elements contributed to a more entertaining experience.\nThe findings emphasize the importance of balancing engaging feedback with\nclarity and usability.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u201c\u591a\u6c41\u201d\u8bbe\u8ba1\u5bf9\u7528\u6237\u4e92\u52a8\u548c\u77ed\u671f\u4fe1\u606f\u4fdd\u7559\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u591a\u6c41\u8bbe\u8ba1\u5728\u7528\u6237\u4e92\u52a8\u4e0a\u7565\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u4fe1\u606f\u4fdd\u7559\u4e0a\u7ed3\u679c\u4e0d\u4e00\uff0c\u5f3a\u8c03\u9700\u5e73\u8861\u5438\u5f15\u529b\u548c\u6e05\u6670\u5ea6\u3002", "motivation": "\u63a2\u8ba8\u591a\u6c41\u8bbe\u8ba1\u5143\u7d20\u5bf9\u7528\u6237\u4e92\u52a8\u548c\u4fe1\u606f\u4fdd\u7559\u7684\u5177\u4f53\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u591a\u6c41\u8bbe\u8ba1\u548c\u5e72\u71e5\u8bbe\u8ba1\u5728\u591a\u4e2a\u4fe1\u606f\u56fe\u4e2d\u7684\u7528\u6237\u4e92\u52a8\u548c\u4fe1\u606f\u4fdd\u7559\u8868\u73b0\u3002", "result": "\u591a\u6c41\u8bbe\u8ba1\u5728\u7528\u6237\u4e92\u52a8\u4e0a\u8868\u73b0\u7a0d\u597d\uff0c\u4fe1\u606f\u4fdd\u7559\u7ed3\u679c\u4e0d\u4e00\uff1b\u90e8\u5206\u591a\u6c41\u8bbe\u8ba1\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4e5f\u6709\u5e72\u71e5\u8bbe\u8ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u66f4\u4f18\u3002", "conclusion": "\u591a\u6c41\u8bbe\u8ba1\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u9700\u8c28\u614e\u5b9e\u65bd\u4ee5\u907f\u514d\u8fc7\u5ea6\u5e72\u6270\uff0c\u5e73\u8861\u5438\u5f15\u529b\u548c\u5b9e\u7528\u6027\u662f\u5173\u952e\u3002"}}
{"id": "2506.16652", "pdf": "https://arxiv.org/pdf/2506.16652", "abs": "https://arxiv.org/abs/2506.16652", "authors": ["Guang Yin", "Yitong Li", "Yixuan Wang", "Dale McConachie", "Paarth Shah", "Kunimatsu Hashimoto", "Huan Zhang", "Katherine Liu", "Yunzhu Li"], "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SE"], "comment": "Accepted to Robotics: Science and Systems (RSS) 2025. The first three\n  authors contributed equally. Project Page:\n  https://robopil.github.io/code-diffuser/", "summary": "Natural language instructions for robotic manipulation tasks often exhibit\nambiguity and vagueness. For instance, the instruction \"Hang a mug on the mug\ntree\" may involve multiple valid actions if there are several mugs and branches\nto choose from. Existing language-conditioned policies typically rely on\nend-to-end models that jointly handle high-level semantic understanding and\nlow-level action generation, which can result in suboptimal performance due to\ntheir lack of modularity and interpretability. To address these challenges, we\nintroduce a novel robotic manipulation framework that can accomplish tasks\nspecified by potentially ambiguous natural language. This framework employs a\nVision-Language Model (VLM) to interpret abstract concepts in natural language\ninstructions and generates task-specific code - an interpretable and executable\nintermediate representation. The generated code interfaces with the perception\nmodule to produce 3D attention maps that highlight task-relevant regions by\nintegrating spatial and semantic information, effectively resolving ambiguities\nin instructions. Through extensive experiments, we identify key limitations of\ncurrent imitation learning methods, such as poor adaptation to language and\nenvironmental variations. We show that our approach excels across challenging\nmanipulation tasks involving language ambiguity, contact-rich manipulation, and\nmulti-object interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b0\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u548c\u53ef\u6267\u884c\u7684\u4e2d\u95f4\u4ee3\u7801\u6765\u89e3\u51b3\u6307\u4ee4\u6b67\u4e49\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6761\u4ef6\u7b56\u7565\u901a\u5e38\u4f9d\u8d56\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u6a21\u7cca\u6027\u548c\u6b67\u4e49\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u4ee3\u7801\uff0c\u5e76\u4e0e\u611f\u77e5\u6a21\u5757\u7ed3\u5408\u751f\u62103D\u6ce8\u610f\u529b\u56fe\uff0c\u89e3\u51b3\u6307\u4ee4\u6b67\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u7cca\u6027\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u548c\u591a\u5bf9\u8c61\u4ea4\u4e92\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.17116", "pdf": "https://arxiv.org/pdf/2506.17116", "abs": "https://arxiv.org/abs/2506.17116", "authors": ["Samuel Rhys Cox", "Helena B\u00f8jer Djern\u00e6s", "Niels van Berkel"], "title": "Reflecting Human Values in XAI: Emotional and Reflective Benefits in Creativity Support Tools", "categories": ["cs.HC"], "comment": "Workshop paper presented at XAIxArts'25 - the third international\n  workshop on eXplainable AI for the Arts, held in conjunction with the ACM\n  Creativity and Cognition conference 2025, June 23rd, 2025. 3 pages", "summary": "In this workshop paper, we discuss the potential for measures of user-centric\nbenefits (such as emotional well-being) that could be explored when evaluating\nexplainable AI (XAI) systems within the arts. As a background to this, we draw\nfrom our recent review of creativity support tool (CST) evaluations, that found\na paucity of studies evaluating CSTs for user-centric measures that benefit the\nuser themselves. Specifically, we discuss measures of: (1) developing intrinsic\nabilities, (2) emotional well-being, (3) self-reflection, and (4)\nself-perception. By discussing these user-centric measures within the context\nof XAI and the arts, we wish to provoke discussion regarding the potential of\nsuch measures.", "AI": {"tldr": "\u63a2\u8ba8\u5728\u827a\u672f\u9886\u57df\u8bc4\u4f30\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7cfb\u7edf\u65f6\uff0c\u5982\u4f55\u6d4b\u91cf\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u76ca\u5904\uff08\u5982\u60c5\u611f\u5e78\u798f\u611f\uff09\u3002", "motivation": "\u5f53\u524d\u5bf9\u521b\u9020\u529b\u652f\u6301\u5de5\u5177\uff08CST\uff09\u7684\u8bc4\u4f30\u7f3a\u4e4f\u5173\u6ce8\u7528\u6237\u81ea\u8eab\u7684\u76ca\u5904\uff0c\u5c24\u5176\u662f\u5728\u827a\u672f\u9886\u57df\u3002", "method": "\u57fa\u4e8e\u5bf9CST\u8bc4\u4f30\u7684\u8fd1\u671f\u7efc\u8ff0\uff0c\u63d0\u51fa\u56db\u79cd\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u6d4b\u91cf\u7ef4\u5ea6\uff1a\u5185\u5728\u80fd\u529b\u53d1\u5c55\u3001\u60c5\u611f\u5e78\u798f\u611f\u3001\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u8ba4\u77e5\u3002", "result": "\u5e0c\u671b\u901a\u8fc7\u8fd9\u4e9b\u7ef4\u5ea6\u7684\u8ba8\u8bba\uff0c\u6fc0\u53d1\u5173\u4e8eXAI\u5728\u827a\u672f\u4e2d\u6f5c\u5728\u5f71\u54cd\u7684\u63a2\u8ba8\u3002", "conclusion": "\u5f3a\u8c03\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u5728XAI\u548c\u827a\u672f\u9886\u57df\u7684\u91cd\u8981\u6027\uff0c\u547c\u5401\u672a\u6765\u7814\u7a76\u5173\u6ce8\u6b64\u7c7b\u6d4b\u91cf\u3002"}}
{"id": "2506.17196", "pdf": "https://arxiv.org/pdf/2506.17196", "abs": "https://arxiv.org/abs/2506.17196", "authors": ["Shambhavi Bhushan", "Danielle R Thomas", "Conrad Borchers", "Isha Raghuvanshi", "Ralph Abboud", "Erin Gatz", "Shivang Gupta", "Kenneth Koedinger"], "title": "Detecting LLM-Generated Short Answers and Effects on Learner Performance", "categories": ["cs.HC"], "comment": "Accepted for publication at the 19th European Conference on\n  Technology Enhanced Learning (ECTEL 2025). This is the author's accepted\n  manuscript", "summary": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03GPT-4o\u68c0\u6d4bLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5de5\u5177GPTZero\uff0c\u5e76\u63a2\u8ba8\u4e86LLM\u6ee5\u7528\u5bf9\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u6559\u80b2\u4e2d\u7684\u6ee5\u7528\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u7684\u53ef\u9760\u6027\u53c2\u5dee\u4e0d\u9f50\uff0c\u4e14\u7f3a\u4e4f\u5bf9LLM\u6ee5\u7528\u5f71\u54cd\u5b66\u4e60\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5b9a\u4e49LLM\u751f\u6210\u7684\u6587\u672c\u4e3a\u672a\u7ecf\u6539\u5199\u6216\u4f18\u5316\u7684\u5185\u5bb9\uff0c\u5229\u7528\u5fae\u8c03\u7684GPT-4o\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u5206\u6790\u5b66\u4e60\u6548\u679c\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u51c6\u786e\u7387\u8fbe80%\uff0cF1\u5206\u65700.78\uff0c\u4f18\u4e8eGPTZero\uff1b\u5b66\u4e60\u8005\u6ee5\u7528LLM\u4f1a\u5bfc\u81f4\u7ed5\u8fc7\u5b66\u4e60\u8fc7\u7a0b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdbLLM\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u4f7f\u7528\u7edf\u8ba1\u6307\u6807\u8f85\u52a9\u5224\u65ad\uff0c\u4ee5\u652f\u6301\u5f00\u653e\u79d1\u5b66\u3002"}}
{"id": "2506.15794", "pdf": "https://arxiv.org/pdf/2506.15794", "abs": "https://arxiv.org/abs/2506.15794", "authors": ["Taylor Lynn Curtis", "Maximilian Puelma Touzel", "William Garneau", "Manon Gruaz", "Mike Pinder", "Li Wei Wang", "Sukanya Krishna", "Luda Cohen", "Jean-Fran\u00e7ois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "title": "Veracity: An Open-Source AI Fact-Checking System", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of misinformation poses a significant threat to society,\nexacerbated by the capabilities of generative AI. This demo paper introduces\nVeracity, an open-source AI system designed to empower individuals to combat\nmisinformation through transparent and accessible fact-checking. Veracity\nleverages the synergy between Large Language Models (LLMs) and web retrieval\nagents to analyze user-submitted claims and provide grounded veracity\nassessments with intuitive explanations. Key features include multilingual\nsupport, numerical scoring of claim veracity, and an interactive interface\ninspired by familiar messaging applications. This paper will showcase\nVeracity's ability to not only detect misinformation but also explain its\nreasoning, fostering media literacy and promoting a more informed society.", "AI": {"tldr": "Veracity\u662f\u4e00\u4e2a\u5f00\u6e90AI\u7cfb\u7edf\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7f51\u7edc\u68c0\u7d22\uff0c\u63d0\u4f9b\u591a\u8bed\u8a00\u652f\u6301\u3001\u8bc4\u5206\u548c\u4ea4\u4e92\u5f0f\u754c\u9762\uff0c\u5e2e\u52a9\u7528\u6237\u5bf9\u6297\u865a\u5047\u4fe1\u606f\u3002", "motivation": "\u865a\u5047\u4fe1\u606f\u7684\u6cdb\u6ee5\u5bf9\u793e\u4f1a\u6784\u6210\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\u52a0\u5267\u4e86\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7f51\u7edc\u68c0\u7d22\u4ee3\u7406\u5206\u6790\u7528\u6237\u63d0\u4ea4\u7684\u58f0\u660e\uff0c\u63d0\u4f9b\u6709\u4f9d\u636e\u7684\u771f\u5b9e\u6027\u8bc4\u4f30\u548c\u76f4\u89c2\u89e3\u91ca\u3002", "result": "\u7cfb\u7edf\u5177\u5907\u591a\u8bed\u8a00\u652f\u6301\u3001\u8bc4\u5206\u529f\u80fd\u548c\u4ea4\u4e92\u5f0f\u754c\u9762\uff0c\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\uff0c\u8fd8\u80fd\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "Veracity\u901a\u8fc7\u900f\u660e\u7684\u4e8b\u5b9e\u6838\u67e5\u63d0\u5347\u5a92\u4f53\u7d20\u517b\uff0c\u52a9\u529b\u6784\u5efa\u66f4\u77e5\u60c5\u7684\u793e\u4f1a\u3002"}}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928", "abs": "https://arxiv.org/abs/2506.15928", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Under review for KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5173\u952e\u4efb\u52a1\u8c08\u5224\u573a\u666f\u4e2d\u667a\u80fd\u4ee3\u7406AI\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u4eba\u683c\u7279\u8d28\u548cAI\u4ee3\u7406\u7279\u6027\u5bf9\u8c08\u5224\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u4eba\u7c7b\u64cd\u4f5c\u8005\u548c\u5229\u76ca\u76f8\u5173\u8005\u7684AI\u4ee3\u7406\uff0c\u6ee1\u8db3\u8de8\u56e2\u961f\u534f\u8c03\u548c\u519b\u6c11\u4e92\u52a8\u7b49\u9ad8\u4ef7\u503c\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528Sotopia\u6a21\u62df\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u7cfb\u7edf\u8bc4\u4f30\u4eba\u683c\u7279\u8d28\u548cAI\u4ee3\u7406\u7279\u6027\u5bf9\u8c08\u5224\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c1\u91c7\u7528\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5206\u6790\u4eba\u683c\u7279\u8d28\u5bf9\u4ef7\u683c\u8c08\u5224\u7684\u5f71\u54cd\uff1b\u5b9e\u9a8c2\u8bc4\u4f30\u4eba\u7c7b\u4e0eAI\u5728\u804c\u4f4d\u8c08\u5224\u4e2d\u7684\u4e92\u52a8\u3002", "result": "\u53d1\u73b0'\u5b9c\u4eba\u6027'\u548c'\u5916\u5411\u6027'\u663e\u8457\u5f71\u54cd\u8c08\u5224\u53ef\u4fe1\u5ea6\u3001\u76ee\u6807\u8fbe\u6210\u548c\u77e5\u8bc6\u83b7\u53d6\u7ed3\u679c\uff1bAI\u4ee3\u7406\u7684\u900f\u660e\u5ea6\u3001\u80fd\u529b\u548c\u9002\u5e94\u6027\u5bf9\u5176\u53ef\u4fe1\u5ea6\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bc4\u4f30AI\u4ee3\u7406\u5728\u591a\u6837\u5316\u4eba\u683c\u548c\u56e2\u961f\u52a8\u6001\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u590d\u6742\u4efb\u52a1\u4e2dAI\u7cfb\u7edf\u7684\u793e\u4f1a\u52a8\u6001\u8bc4\u4f30\u3002"}}
{"id": "2506.16202", "pdf": "https://arxiv.org/pdf/2506.16202", "abs": "https://arxiv.org/abs/2506.16202", "authors": ["Chuyao Wang", "Patrick Sturgis", "Daniel de Kadt"], "title": "AI labeling reduces the perceived accuracy of online content but has limited broader effects", "categories": ["cs.CY", "cs.HC", "stat.AP", "62P25, 91C99", "J.4; H.1.2"], "comment": "30 pages, 5 figures, 10 tables", "summary": "Explicit labeling of online content produced by artificial intelligence (AI)\nis a widely mooted policy for ensuring transparency and promoting public\nconfidence. Yet little is known about the scope of AI labeling effects on\npublic assessments of labeled content. We contribute new evidence on this\nquestion from a survey experiment using a high-quality nationally\nrepresentative probability sample (n = 3,861). First, we demonstrate that\nexplicit AI labeling of a news article about a proposed public policy reduces\nits perceived accuracy. Second, we test whether there are spillover effects in\nterms of policy interest, policy support, and general concerns about online\nmisinformation. We find that AI labeling reduces interest in the policy, but\nneither influences support for the policy nor triggers general concerns about\nonline misinformation. We further find that increasing the salience of AI use\nreduces the negative impact of AI labeling on perceived accuracy, while\none-sided versus two-sided framing of the policy has no moderating effect.\nOverall, our findings suggest that the effects of algorithm aversion induced by\nAI labeling of online content are limited in scope.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u663e\u5f0f\u6807\u6ce8AI\u751f\u6210\u7684\u5185\u5bb9\u4f1a\u964d\u4f4e\u516c\u4f17\u5bf9\u5176\u51c6\u786e\u6027\u7684\u611f\u77e5\uff0c\u4f46\u5bf9\u653f\u7b56\u652f\u6301\u6216\u865a\u5047\u4fe1\u606f\u7684\u6574\u4f53\u62c5\u5fe7\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u63a2\u8ba8AI\u6807\u6ce8\u5bf9\u516c\u4f17\u8bc4\u4f30\u5185\u5bb9\u7684\u5f71\u54cd\uff0c\u4ee5\u4fc3\u8fdb\u900f\u660e\u5ea6\u548c\u516c\u4f17\u4fe1\u5fc3\u3002", "method": "\u901a\u8fc7\u5168\u56fd\u4ee3\u8868\u6027\u8c03\u67e5\u5b9e\u9a8c\uff08n=3,861\uff09\u6d4b\u8bd5\u6807\u6ce8\u5bf9\u653f\u7b56\u5174\u8da3\u3001\u652f\u6301\u548c\u865a\u5047\u4fe1\u606f\u62c5\u5fe7\u7684\u5f71\u54cd\u3002", "result": "AI\u6807\u6ce8\u964d\u4f4e\u653f\u7b56\u5174\u8da3\u4f46\u5bf9\u653f\u7b56\u652f\u6301\u65e0\u5f71\u54cd\uff1b\u63d0\u9ad8AI\u4f7f\u7528\u7684\u663e\u8457\u6027\u53ef\u51cf\u8f7b\u51c6\u786e\u6027\u611f\u77e5\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "AI\u6807\u6ce8\u5f15\u53d1\u7684\u7b97\u6cd5\u538c\u6076\u6548\u5e94\u5f71\u54cd\u8303\u56f4\u6709\u9650\u3002"}}
{"id": "2506.16310", "pdf": "https://arxiv.org/pdf/2506.16310", "abs": "https://arxiv.org/abs/2506.16310", "authors": ["Pranav Pawar", "Akshansh Dwivedi", "Jenish Boricha", "Himanshu Gohil", "Aditya Dubey"], "title": "Optimizing Multilingual Text-To-Speech with Accents & Emotions", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "comment": "12 pages, 8 figures", "summary": "State-of-the-art text-to-speech (TTS) systems realize high naturalness in\nmonolingual environments, synthesizing speech with correct multilingual accents\n(especially for Indic languages) and context-relevant emotions still poses\ndifficulty owing to cultural nuance discrepancies in current frameworks. This\npaper introduces a new TTS architecture integrating accent along with\npreserving transliteration with multi-scale emotion modelling, in particularly\ntuned for Hindi and Indian English accent. Our approach extends the Parler-TTS\nmodel by integrating A language-specific phoneme alignment hybrid\nencoder-decoder architecture, and culture-sensitive emotion embedding layers\ntrained on native speaker corpora, as well as incorporating a dynamic accent\ncode switching with residual vector quantization. Quantitative tests\ndemonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction\nfrom 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native\nlisteners, surpassing METTS and VECL-TTS baselines. The novelty of the system\nis that it can mix code in real time - generating statements such as \"Namaste,\nlet's talk about <Hindi phrase>\" with uninterrupted accent shifts while\npreserving emotional consistency. Subjective evaluation with 200 users reported\na mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than\nexisting multilingual systems (p<0.01). This research makes cross-lingual\nsynthesis more feasible by showcasing scalable accent-emotion disentanglement,\nwith direct application in South Asian EdTech and accessibility software.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TTS\u67b6\u6784\uff0c\u7ed3\u5408\u53e3\u97f3\u4fdd\u6301\u548c\u591a\u5c3a\u5ea6\u60c5\u611f\u5efa\u6a21\uff0c\u7279\u522b\u9488\u5bf9\u5370\u5730\u8bed\u548c\u5370\u5ea6\u82f1\u8bed\u53e3\u97f3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53e3\u97f3\u51c6\u786e\u6027\u548c\u60c5\u611f\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5f53\u524dTTS\u7cfb\u7edf\u5728\u591a\u8bed\u8a00\u73af\u5883\uff08\u5c24\u5176\u662f\u5370\u5ea6\u8bed\u8a00\uff09\u4e2d\u96be\u4ee5\u6b63\u786e\u5408\u6210\u5e26\u6709\u591a\u8bed\u8a00\u53e3\u97f3\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u60c5\u611f\u7684\u8bed\u97f3\uff0c\u8fd9\u662f\u7531\u4e8e\u6587\u5316\u5dee\u5f02\u5bfc\u81f4\u7684\u3002", "method": "\u6269\u5c55Parler-TTS\u6a21\u578b\uff0c\u5f15\u5165\u8bed\u8a00\u7279\u5b9a\u97f3\u7d20\u5bf9\u9f50\u6df7\u5408\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3001\u6587\u5316\u654f\u611f\u7684\u60c5\u611f\u5d4c\u5165\u5c42\uff0c\u4ee5\u53ca\u52a8\u6001\u53e3\u97f3\u4ee3\u7801\u5207\u6362\u4e0e\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u3002", "result": "\u53e3\u97f3\u51c6\u786e\u6027\u63d0\u534723.7%\uff08WER\u4ece15.4%\u964d\u81f311.8%\uff09\uff0c\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\u8fbe85.3%\uff0c\u7528\u6237\u8bc4\u4ef7\uff08MOS\uff09\u4e3a4.2/5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u53e3\u97f3-\u60c5\u611f\u89e3\u8026\u6280\u672f\uff0c\u4e3a\u8de8\u8bed\u8a00\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5357\u4e9a\u6559\u80b2\u79d1\u6280\u548c\u8f85\u52a9\u8f6f\u4ef6\u3002"}}
{"id": "2506.16617", "pdf": "https://arxiv.org/pdf/2506.16617", "abs": "https://arxiv.org/abs/2506.16617", "authors": ["Soobin Chae", "Suhwan Lee", "Hanna Hauptmann", "Hajo A. Reijers", "Xixi Lu"], "title": "The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring", "categories": ["cs.AI", "cs.HC"], "comment": "Accepted at CAiSE'25", "summary": "Predictive Process Monitoring (PPM) often uses deep learning models to\npredict the future behavior of ongoing processes, such as predicting process\noutcomes. While these models achieve high accuracy, their lack of\ninterpretability undermines user trust and adoption. Explainable AI (XAI) aims\nto address this challenge by providing the reasoning behind the predictions.\nHowever, current evaluations of XAI in PPM focus primarily on functional\nmetrics (such as fidelity), overlooking user-centered aspects such as their\neffect on task performance and decision-making. This study investigates the\neffects of explanation styles (feature importance, rule-based, and\ncounterfactual) and perceived AI accuracy (low or high) on decision-making in\nPPM. We conducted a decision-making experiment, where users were presented with\nthe AI predictions, perceived accuracy levels, and explanations of different\nstyles. Users' decisions were measured both before and after receiving\nexplanations, allowing the assessment of objective metrics (Task Performance\nand Agreement) and subjective metrics (Decision Confidence). Our findings show\nthat perceived accuracy and explanation style have a significant effect.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u548cAI\u611f\u77e5\u51c6\u786e\u6027\u5bf9PPM\u4e2d\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e24\u8005\u5bf9\u4efb\u52a1\u8868\u73b0\u548c\u51b3\u7b56\u4fe1\u5fc3\u6709\u663e\u8457\u4f5c\u7528\u3002", "motivation": "\u5f53\u524dPPM\u4e2d\u7684XAI\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u4e2d\u5fc3\u7684\u51b3\u7b56\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u89e3\u91ca\u98ce\u683c\u548c\u611f\u77e5\u51c6\u786e\u6027\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u51b3\u7b56\u5b9e\u9a8c\uff0c\u7528\u6237\u5728\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u548c\u611f\u77e5\u51c6\u786e\u6027\u4e0b\u8fdb\u884c\u51b3\u7b56\uff0c\u6d4b\u91cf\u4efb\u52a1\u8868\u73b0\u3001\u4e00\u81f4\u6027\u548c\u51b3\u7b56\u4fe1\u5fc3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u611f\u77e5\u51c6\u786e\u6027\u548c\u89e3\u91ca\u98ce\u683c\u5bf9\u51b3\u7b56\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7528\u6237\u4e2d\u5fc3\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4e3aPPM\u4e2d\u89e3\u91ca\u5de5\u5177\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2506.16622", "pdf": "https://arxiv.org/pdf/2506.16622", "abs": "https://arxiv.org/abs/2506.16622", "authors": ["Jiaxin Pei", "Dustin Wright", "Isabelle Augenstin", "David Jurgens"], "title": "Modeling Public Perceptions of Science in Media", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u516c\u4f17\u5bf9\u79d1\u5b66\u65b0\u95fb\u7684\u591a\u7ef4\u5ea6\u611f\u77e5\uff0c\u5e76\u5f00\u53d1\u4e86NLP\u6a21\u578b\u9884\u6d4b\u611f\u77e5\u5206\u6570\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u79d1\u5b66\u65b0\u95fb\u7684\u516c\u4f17\u611f\u77e5\u4e0e\u6700\u7ec8\u53c2\u4e0e\u5ea6\u76f4\u63a5\u76f8\u5173\u3002", "motivation": "\u79d1\u5b66\u4f20\u64ad\u4e2d\uff0c\u516c\u4f17\u5bf9\u79d1\u5b66\u4fe1\u606f\u7684\u611f\u77e5\u548c\u4e92\u52a8\u96be\u4ee5\u9884\u6d4b\uff0c\u4f46\u8fd9\u5bf9\u5efa\u7acb\u4fe1\u4efb\u548c\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u8ba1\u7b97\u6846\u67b6\u6784\u5efa\u5927\u89c4\u6a21\u79d1\u5b66\u65b0\u95fb\u611f\u77e5\u6570\u636e\u96c6\uff0810,489\u4e2a\u6ce8\u91ca\uff09\uff0c\u5e76\u5f00\u53d1NLP\u6a21\u578b\u9884\u6d4b\u611f\u77e5\u5206\u6570\u3002\u901a\u8fc7Reddit\u5b9e\u9a8c\u5206\u6790\u611f\u77e5\u4e0e\u53c2\u4e0e\u5ea6\u7684\u5173\u7cfb\u3002", "result": "\u79d1\u5b66\u65b0\u95fb\u6d88\u8d39\u9891\u7387\u662f\u611f\u77e5\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff1b\u516c\u4f17\u611f\u77e5\u5206\u6570\u9ad8\u7684\u5e16\u5b50\u5728Reddit\u4e0a\u83b7\u5f97\u66f4\u591a\u8bc4\u8bba\u548c\u70b9\u8d5e\u3002", "conclusion": "\u5f3a\u8c03\u611f\u77e5\u5efa\u6a21\u5728\u79d1\u5b66\u4f20\u64ad\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u9884\u6d4b\u516c\u4f17\u5174\u8da3\u548c\u53c2\u4e0e\u63d0\u4f9b\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.16697", "pdf": "https://arxiv.org/pdf/2506.16697", "abs": "https://arxiv.org/abs/2506.16697", "authors": ["Zhicheng Lin"], "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are rapidly being adopted across psychology,\nserving as research tools, experimental subjects, human simulators, and\ncomputational models of cognition. However, the application of human\nmeasurement tools to these systems can produce contradictory results, raising\nconcerns that many findings are measurement phantoms--statistical artifacts\nrather than genuine psychological phenomena. In this Perspective, we argue that\nbuilding a robust science of AI psychology requires integrating two of our\nfield's foundational pillars: the principles of reliable measurement and the\nstandards for sound causal inference. We present a dual-validity framework to\nguide this integration, which clarifies how the evidence needed to support a\nclaim scales with its scientific ambition. Using an LLM to classify text may\nrequire only basic accuracy checks, whereas claiming it can simulate anxiety\ndemands a far more rigorous validation process. Current practice systematically\nfails to meet these requirements, often treating statistical pattern matching\nas evidence of psychological phenomena. The same model output--endorsing \"I am\nanxious\"--requires different validation strategies depending on whether\nresearchers claim to measure, characterize, simulate, or model psychological\nconstructs. Moving forward requires developing computational analogues of\npsychological constructs and establishing clear, scalable standards of evidence\nrather than the uncritical application of human measurement tools.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5728\u5fc3\u7406\u5b66\u4e2d\u5e7f\u6cdb\u5e94\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u80fd\u56e0\u4f7f\u7528\u4eba\u7c7b\u6d4b\u91cf\u5de5\u5177\u800c\u4ea7\u751f\u865a\u5047\u7ed3\u679c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6574\u5408\u53ef\u9760\u6d4b\u91cf\u539f\u5219\u548c\u56e0\u679c\u63a8\u65ad\u6807\u51c6\u7684\u6846\u67b6\uff0c\u4ee5\u5efa\u7acb\u7a33\u5065\u7684AI\u5fc3\u7406\u5b66\u79d1\u5b66\u3002", "motivation": "\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\uff0cLLM\u4f5c\u4e3a\u5de5\u5177\u6216\u6a21\u578b\u7684\u5e94\u7528\u53ef\u80fd\u5bfc\u81f4\u6d4b\u91cf\u5047\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e25\u8c28\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u6548\u5ea6\u6846\u67b6\uff0c\u6839\u636e\u79d1\u5b66\u91ce\u5fc3\u8c03\u6574\u9a8c\u8bc1\u6807\u51c6\uff0c\u533a\u5206\u4e0d\u540c\u7814\u7a76\u76ee\u6807\u6240\u9700\u7684\u8bc1\u636e\u5f3a\u5ea6\u3002", "result": "\u5f53\u524d\u5b9e\u8df5\u5e38\u5c06\u7edf\u8ba1\u6a21\u5f0f\u5339\u914d\u8bef\u8ba4\u4e3a\u5fc3\u7406\u73b0\u8c61\u8bc1\u636e\uff0c\u9700\u5f00\u53d1\u8ba1\u7b97\u7c7b\u6bd4\u548c\u660e\u786e\u7684\u8bc1\u636e\u6807\u51c6\u3002", "conclusion": "\u5efa\u7acb\u7a33\u5065\u7684AI\u5fc3\u7406\u5b66\u79d1\u5b66\u9700\u6574\u5408\u6d4b\u91cf\u4e0e\u56e0\u679c\u63a8\u65ad\u6807\u51c6\uff0c\u907f\u514d\u6ee5\u7528\u4eba\u7c7b\u6d4b\u91cf\u5de5\u5177\u3002"}}
{"id": "2506.16702", "pdf": "https://arxiv.org/pdf/2506.16702", "abs": "https://arxiv.org/abs/2506.16702", "authors": ["Zhicheng Lin"], "title": "Large Language Models as Psychological Simulators: A Methodological Guide", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) offer emerging opportunities for psychological\nand behavioral research, but methodological guidance is lacking. This article\nprovides a framework for using LLMs as psychological simulators across two\nprimary applications: simulating roles and personas to explore diverse\ncontexts, and serving as computational models to investigate cognitive\nprocesses. For simulation, we present methods for developing psychologically\ngrounded personas that move beyond demographic categories, with strategies for\nvalidation against human data and use cases ranging from studying inaccessible\npopulations to prototyping research instruments. For cognitive modeling, we\nsynthesize emerging approaches for probing internal representations,\nmethodological advances in causal interventions, and strategies for relating\nmodel behavior to human cognition. We address overarching challenges including\nprompt sensitivity, temporal limitations from training data cutoffs, and\nethical considerations that extend beyond traditional human subjects review.\nThroughout, we emphasize the need for transparency about model capabilities and\nconstraints. Together, this framework integrates emerging empirical evidence\nabout LLM performance--including systematic biases, cultural limitations, and\nprompt brittleness--to help researchers wrangle these challenges and leverage\nthe unique capabilities of LLMs in psychological research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u5fc3\u7406\u5b66\u6a21\u62df\u5668\uff0c\u5206\u4e3a\u89d2\u8272\u6a21\u62df\u548c\u8ba4\u77e5\u5efa\u6a21\u4e24\u90e8\u5206\uff0c\u5e76\u8ba8\u8bba\u4e86\u65b9\u6cd5\u9a8c\u8bc1\u3001\u4f26\u7406\u95ee\u9898\u53ca\u900f\u660e\u6027\u9700\u6c42\u3002", "motivation": "\u4e3a\u5fc3\u7406\u5b66\u548c\u884c\u4e3a\u5b66\u7814\u7a76\u63d0\u4f9b\u65b9\u6cd5\u8bba\u6307\u5bfc\uff0c\u5229\u7528LLMs\u6a21\u62df\u591a\u6837\u60c5\u5883\u548c\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u5fc3\u7406\u5b66\u57fa\u7840\u89d2\u8272\u6a21\u62df\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u9a8c\u8bc1\uff1b\u63a2\u7d22\u8ba4\u77e5\u5efa\u6a21\u4e2d\u7684\u5185\u90e8\u8868\u5f81\u5206\u6790\u548c\u56e0\u679c\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u6574\u5408\u4e86LLMs\u7684\u6027\u80fd\u8bc1\u636e\uff0c\u5305\u62ec\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u6587\u5316\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u5b9e\u7528\u7b56\u7565\u3002", "conclusion": "\u5f3a\u8c03\u900f\u660e\u6027\u548c\u4f26\u7406\u8003\u91cf\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5229\u7528LLMs\u7684\u72ec\u7279\u80fd\u529b\u5e94\u5bf9\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u7684\u6311\u6218\u3002"}}
