<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 24]
- [cs.GR](#cs.GR) [Total: 9]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 研究评估了GPT-4o mini在ML项目中生成文件级日志的能力，发现其虽有潜力但仍存在过度日志记录等问题。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在文件级日志生成中的潜力，尤其是机器学习应用中，以提升系统可靠性。

Method: 收集171个ML项目，移除原始日志后，用LLM生成日志并评估位置、级别和内容质量。

Result: LLM在63.9%情况下与人类日志位置一致，但过度日志率高达82.66%。手动分析揭示了文件级日志的挑战。

Conclusion: LLM有望生成完整文件日志，但需解决过度日志记录和项目规范对齐等限制。

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [2] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 论文提出了一种自动化方法，通过提取视频中最匹配Bug描述的关键帧，显著减少人工审查时间。


<details>
  <summary>Details</summary>
Motivation: 现代游戏工作室快速发布新版本和补丁，生成了大量包含游戏视频的Bug报告，手动审查费时费力。

Method: 使用FFmpeg提取关键帧，结合视觉-语言模型（GPT-4o）选择最匹配Bug描述的帧。

Result: 在真实游戏中测试，F1得分为0.79，准确率为0.89，对某些Bug类别表现优异（如Lighting & Shadow）。

Conclusion: 该方法显著减少了人工审查工作量，提升了Bug分类效率，对游戏QA团队具有实际价值。

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [3] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 探讨生成式AI对开源软件社区的潜在影响，提出应对策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展对开源软件社区的协作模式带来挑战，缺乏明确框架可能导致社区混乱。

Method: 采用基于McLuhan四元组的社会技术框架，进行情景驱动的概念性分析。

Result: 识别软件开发、文档、社区参与和治理四个领域的风险和机遇。

Conclusion: 建议开源社区领袖和研究者主动应对技术变革，而非被动适应。

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [4] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 论文研究了注意力机制在神经网络中的独特故障，提出了七种新的故障分类，并给出了诊断启发式方法。


<details>
  <summary>Details</summary>
Motivation: 由于现有深度学习故障分类未能充分捕捉注意力机制的独特故障，导致缺乏实用的诊断指导，论文旨在填补这一空白。

Method: 通过对来自96个项目的555个真实故障进行系统性分析，开发了七种注意力特有的故障分类，并分析了其根因和表现。

Result: 研究表明，超过一半的注意力机制故障源于其独特架构，并提出了四种诊断启发式方法，解释了33.0%的故障。

Conclusion: 论文为注意力模型提供了首个系统性诊断指导，填补了现有研究的不足。

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [5] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLMs）与面向对象编程（OOP）的结合，旨在填补当前研究的空白，提出LLMs如何提升OOP学习和代码编写效率，并探讨评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在OOP领域的应用研究不足，程序员、初学者和有经验者的需求未得到充分满足，亟需探索LLMs在编程中的潜在价值。

Method: 通过分析OOP任务中关键工作流程，识别LLMs的潜在应用场景，并提出增强逻辑推理和代码编写的具体方法。

Result: 确定了LLMs在OOP学习与代码编写中的关键应用点，并提出了增强编程体验的解决方案。

Conclusion: LLMs与OOP的结合具有广阔前景，能为不同水平的程序员提供实质性帮助，但需进一步研究评估方法。

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [6] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: 论文提出了一种半自动化的方法来预测和管理软件变更之间的依赖关系，解决了大型系统中依赖识别延迟的问题，并取得了较好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂度的增加，准确识别和管理变更之间的依赖关系变得至关重要，尤其是在多团队协作的大型系统中，依赖关系可能导致构建失败或功能部署不全。

Method: 通过研究OpenStack中的依赖关系，提出了两个机器学习模型：一个预测变更间依赖的可能性，另一个确定具体的依赖对。

Result: 模型表现出色，AUC分数分别为79.33%和91.89%，Brier分数为0.11和0.014。但模型的Top-k精确度仍有提升空间。

Conclusion: 研究表明依赖关系识别存在延迟，提出的半自动化方法能有效帮助开发者提前识别依赖关系，提升开发效率。

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [7] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: LadyBug是一款GitHub机器人，通过结合UI交互信息和文本检索自动定位Android应用中的bug。


<details>
  <summary>Details</summary>
Motivation: 解决Android应用bug定位问题，提升开发者的效率。

Method: 结合bug描述的文本信息和UI交互轨迹，生成可能包含bug的文件列表。

Result: 实证表明LadyBug优于纯文本检索方法，UI信息显著提高定位准确性。

Conclusion: LadyBug是开源工具，能有效帮助开发者定位Android应用中的bug。

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [8] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 提出统一框架，结合推理过程质量改进强化学习代码生成，避免奖励滥用，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习生成代码仅依赖测试结果，忽视中间推理质量，易导致奖励滥用。

Method: 开发LCB-RB基准评估推理，采用OD-based奖励模型训练，并提出P-GRPO方法基于任务成功条件奖励推理过程。

Result: 7B参数模型在代码生成任务中超越基线4.5%，与GPT-4-Turbo相当，并展示数学任务泛化能力。

Conclusion: 推理质量评估和新RL方法能显著提升代码生成性能，避免奖励滥用，模型和代码公开。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [9] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: 提出了一种结合大语言模型和确定性技术的混合方法，通过自然语言输入来创建和修改JSON Schema，并集成到开源工具MetaConfigurator中，降低了非专家进行结构化数据建模的难度。


<details>
  <summary>Details</summary>
Motivation: 许多领域缺乏标准化模型，且创建模型对非专家来说是一个重大障碍，因此需要一种更易用的方法来简化数据建模和集成。

Method: 结合大语言模型和确定性技术，提供自然语言输入的JSON Schema创建和修改功能，并在MetaConfigurator工具中集成这些能力。

Result: 通过化学领域的应用示例验证了方法的可行性，显著降低了非专家进行结构化数据建模和数据集成的门槛。

Conclusion: 结合自然语言交互和确定性保障的混合方法，为非专家提供了更友好的结构化数据建模和集成工具。

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [10] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: 提出了STEPWISE-CODEX-Bench（SX-Bench）新基准，专注于复杂多函数理解和细粒度执行推理，弥补现有基准在评估高级代码智能模型上的不足。其通过多子函数协作任务和计算步骤预测，显著区分模型能力，揭示复杂推理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 主流基准如HumanEval和MBPP主要评估功能正确性，而CRUXEVAL限于单函数低复杂度场景。现有基准分数接近饱和，区分度不足，需新方法评估复杂多函数动态推理能力。

Method: 设计SX-Bench基准，包含多子函数协作任务（如链式调用、嵌套循环），以计算步骤为最小执行单元，要求模型预测推理任务总步骤数。结合程序合成、符号执行和LLM辅助验证的自动化生成流程。

Result: 在20多个主流模型（含14个推理增强模型）上评估，SX-Bench区分度高：OpenAI-O3在Hard-Reasoning任务中仅78.37%准确率，远低于此前基准饱和分数，揭示复杂推理瓶颈。

Conclusion: SX-Bench将代码评估从单函数验证推进到多函数动态推理，为深入评估高级代码智能模型提供关键工具。

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [11] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph框架通过类型化有向图和专用小语言模型（SLMs）驱动，能够自动优化源代码、文档等，修复83%安全漏洞，并显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 应对遗留系统现代化中的挑战（如隐式契约、性能保持和集成演进），提出一种可控制的持续自适应系统路径。

Method: 使用类型化有向图表示构件，通过SLMs驱动的突变算子和多目标适应性选择。

Result: 修复83%安全漏洞，实现93%COBOL到Java功能等价，降低40%延迟和90%计算成本。

Conclusion: EvoGraph为软件持续自适应（Software 3.0）提供了可行方案，同时保持可控性。

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [12] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: 论文提出了一种概念模型和方法论，利用物联网技术提升业务流程的可持续性，超越了传统的环境维度，并通过旅游和医疗案例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 物联网技术在业务流程管理中潜力巨大，但现有研究多关注环境可持续性，缺乏多维度系统分析。本文旨在填补这一空白。

Method: 提出概念模型和方法论，将物联网与业务流程管理结合，系统分析并实施可持续性增强的业务流程。

Result: 模型和方法论成功应用于旅游和医疗案例，展示了物联网在提升多维度可持续性中的作用。

Conclusion: 通过物联网技术，可以实现业务流程的全面可持续性改进，为未来研究和实践提供了新方向。

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 提出了保证异构微服务更新一致性的算法，利用服务行为的语义属性（如交换性），避免了混合模式下的不一致。


<details>
  <summary>Details</summary>
Motivation: 在线服务的微服务架构需要实时更新，但混合模式（新旧版本同时运行）可能导致不一致问题，现有方法效率低下或风险高。

Method: 基于服务行为的语义属性（如交换性），提出保证一致性的算法和理论框架。

Result: 证明语义感知是避免不一致的必要条件，并提出新算法确保更新原子性。

Conclusion: 通过语义感知和理论框架，解决了混合模式更新的不一致问题。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [14] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: 通过细粒度域分解优化GPU上的稀疏三角求解，提升并行性，减少内存访问不规整性，实现了显著的性能加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性系统的预条件子应用中，稀疏三角求解的不规整内存访问和数据依赖成为性能瓶颈，亟需针对GPU架构的优化方法。

Method: 提出细粒度域分解策略，生成无重叠子域，每个子域由线程块处理，其向量存储在共享内存中，减少全局内存访问和同步开销。

Result: 在AMD GPU上，三角求解速度提升10.7倍，ILU0预条件BiCGSTAB求解器速度提升3.2倍。

Conclusion: 细粒度域分解方法显著提升了GPU上稀疏三角求解和预条件迭代求解器的性能，为相关应用提供高效解决方案。

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


### [15] [Back to Bits: Extending Shannon's communication performance framework to computing](https://arxiv.org/abs/2508.05621)
*Max Hawkins,Richard Vuduc*

Main category: cs.PF

TL;DR: 提出了基于信息论的新型计算性能单元，替代传统浮点运算指标，以适应多样化计算系统。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统日益多样化，传统性能指标无法准确反映其复杂性。

Method: 将计算视为信息通过通道的转换，基于输入输出的互信息定义性能。

Result: 提供了一种衡量计算中信息编码、处理和保留的方法。

Conclusion: 该框架为性能评估提供了与实现无关的理论基础。

Abstract: This work proposes a novel computing performance unit grounded in information
theory. Modern computing systems are increasingly diverse, supporting
low-precision formats, hardware specialization, and emerging paradigms such as
analog, quantum, and reversible logic. Traditional metrics like floating-point
operations (flops) no longer accurately capture this complexity. We frame
computing as the transformation of information through a channel and define
performance in terms of the mutual information between a system's inputs and
outputs. This approach measures not just the quantity of data processed, but
the amount of meaningful information encoded, manipulated, and retained through
computation. Our framework provides a principled, implementation-agnostic
foundation for evaluating performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks](https://arxiv.org/abs/2508.05130)
*Ali Raza,Muhammad Farhan Khan,Zeeshan Alam,Muhammad Saad,Ilyas Saleem,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: cs.NI

TL;DR: 论文提出一种结合可重构智能表面（RIS）、太赫兹通信和非正交多址接入（NOMA）的联合框架，以提升智能工业通信的频谱效率、覆盖范围和可靠性。


<details>
  <summary>Details</summary>
Motivation: 满足未来6G网络中工业自动化和实时通信对高效、可靠通信的需求。

Method: 研究两种功率分配策略：优化近远节点功率分配和优先网络需求分配。

Result: 相比固定功率分配方案，所提方案在30dBm下实现23%的总速率增益。

Conclusion: 仿真验证了RIS辅助的NOMA MIMO框架在太赫兹工业通信中的有效性和鲁棒性。

Abstract: This paper presents a joint framework that integrates reconfigurable
intelligent surfaces (RISs) with Terahertz (THz) communications and
non-orthogonal multiple access (NOMA) to enhance smart industrial
communications. The proposed system leverages the advantages of RIS and THz
bands to improve spectral efficiency, coverage, and reliability key
requirements for industrial automation and real-time communications in future
6G networks and beyond. Within this framework, two power allocation strategies
are investigated: the first optimally distributes power between near and far
industrial nodes, and the second prioritizes network demands to enhance system
performance further. A performance evaluation is conducted to compare the sum
rate and outage probability against a fixed power allocation scheme. Our scheme
achieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results
validate the theoretical analysis, demonstrating the effectiveness and
robustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial
communications.

</details>


### [17] [Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models](https://arxiv.org/abs/2508.05249)
*José Ruela,Ivan Cojocaru,André Coelho,Rui Campos,Manuel Ricardo*

Main category: cs.NI

TL;DR: 论文提出了一种5G移动单元(MC)的概念及架构设计，用于在基础设施有限或无线条件差的区域提供5G连接，并评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 为在固定5G基础设施有限或无线条件不佳的区域提供可靠的5G连接，研究MC的可行性和性能。

Method: 设计了两种MC架构（覆盖层模型和IAB模型），并使用OpenAirInterface测试台进行性能验证。

Result: 验证了MC概念的可行性，并表明MC的位置对网络性能有显著影响。

Conclusion: MC可为临时覆盖扩展和容量增强提供有效解决方案，适用于港口、工业场景和公共安全等环境。

Abstract: This paper presents the concept, architectural design, and performance
evaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to
User Equipment (UE) in areas with limited fixed 5G infrastructures or subject
to adverse radio conditions. We consider two main approaches to MC design: an
overlay model, where the MC obtains backhaul connectivity from a 5G overlay
network, and an Integrated Access and Backhaul (IAB)-based model, discussing
their protocol stacks and architectural implications. In order to validate the
MC's performance, we employ an emulation-based testbed using the
OpenAirInterface (OAI) implementation, considering different MC positions. The
results validate the MC concept and demonstrate that MC positioning
significantly influences network performance. This paper has the potential to
aid network operators and service providers in selecting and deploying MC
architectures for temporary coverage extension and capacity reinforcement in
different environments, including seaports, industrial scenarios, and public
safety.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
*Renmiao Chen,Shiyao Cui,Xuancheng Huang,Chengwei Pan,Victor Shea-Jay Huang,QingLin Zhang,Xuan Ouyang,Zhexin Zhang,Hongning Wang,Minlie Huang*

Main category: cs.MM

TL;DR: JPS是一种通过视觉干扰和文本引导实现多模态大语言模型（MLLMs）越狱的方法，旨在提高攻击成功率（ASR）和恶意意图满足率（MIFR）。该方法结合目标导向的对抗性图像扰动和优化的文本提示，通过多智能体系统实现协同优化，实验证明其在多个MLLMs和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前研究过分关注攻击成功率（ASR），而忽略了生成的响应是否真正满足攻击者的恶意意图，导致低质量输出。JPS旨在填补这一空白，通过协同优化视觉和文本组件，提高攻击的有效性和意图满足率。

Method: JPS采用目标导向的对抗性图像扰动绕过安全过滤器，并利用多智能体系统优化的“引导提示”指导模型响应。视觉和文本组件通过迭代协同优化提升性能。

Result: 实验表明，JPS在ASR和MIFR上均达到最新水平，在多个MLLMs和基准测试中表现出色。

Conclusion: JPS通过视觉干扰和文本引导的协同优化，实现了高效的多模态大语言模型越狱，同时提出了MIFR作为评估攻击结果质量的新指标。

Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a
significant research focus. Current research predominantly focuses on
maximizing attack success rate (ASR), often overlooking whether the generated
responses actually fulfill the attacker's malicious intent. This oversight
frequently leads to low-quality outputs that bypass safety filters but lack
substantial harmful content. To address this gap, we propose JPS,
\underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation
and textual \underline{S}teering, which achieves jailbreaks via corporation of
visual image and textually steering prompt. Specifically, JPS utilizes
target-guided adversarial image perturbations for effective safety bypass,
complemented by "steering prompt" optimized via a multi-agent system to
specifically guide LLM responses fulfilling the attackers' intent. These visual
and textual components undergo iterative co-optimization for enhanced
performance. To evaluate the quality of attack outcomes, we propose the
Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a
Reasoning-LLM-based evaluator. Our experiments show JPS sets a new
state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with
analyses confirming its efficacy. Codes are available at
\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.
\color{warningcolor}{Warning: This paper contains potentially sensitive
contents.}

</details>


### [19] [Embedding Alignment in Code Generation for Audio](https://arxiv.org/abs/2508.05473)
*Sam Kouteili,Hiren Madhu,George Typaldos,Mark Santolucito*

Main category: cs.MM

TL;DR: LLM代码生成可助力创意编程（如实时编码），但需多样化代码候选项以匹配音乐意图。研究探索代码与音频嵌入空间的关系，发现非线性关系，并提出预测模型以实现嵌入对齐。


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成代码在创意编程（如实时编码）中的应用潜力，解决生成代码多样性不足及与音频输出关系不明确的问题。

Method: 分析代码与音频嵌入空间的拓扑关系，构建预测模型以学习代码与音频嵌入的对齐映射。

Result: 发现代码与音频嵌入间不存在简单线性关系，但通过学习可实现嵌入对齐映射。

Conclusion: 通过预测模型建立代码与音频嵌入的映射关系，为生成更具音乐多样性的代码提供支持。

Abstract: LLM-powered code generation has the potential to revolutionize creative
coding endeavors, such as live-coding, by enabling users to focus on structural
motifs over syntactic details. In such domains, when prompting an LLM, users
may benefit from considering multiple varied code candidates to better realize
their musical intentions. Code generation models, however, struggle to present
unique and diverse code candidates, with no direct insight into the code's
audio output. To better establish a relationship between code candidates and
produced audio, we investigate the topology of the mapping between code and
audio embedding spaces. We find that code and audio embeddings do not exhibit a
simple linear relationship, but supplement this with a constructed predictive
model that shows an embedding alignment map could be learned. Supplementing the
aim for musically diverse output, we present a model that given code predicts
output audio embedding, constructing a code-audio embedding alignment map.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [20] [AI Should Be More Human, Not More Complex](https://arxiv.org/abs/2508.04713)
*Carlo Esposito*

Main category: cs.HC

TL;DR: 用户更喜欢简洁且注明来源的AI回答，而非冗长复杂的解释。研究表明，当前AI倾向于“人工复杂化”反而降低用户信任和满意度。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在搜索应用中生成冗长复杂回答的现象及其对用户满意度的影响。

Method: 通过对1万名参与者比较五大AI搜索系统的回答，分析用户偏好。

Result: 用户更倾向于简洁、注明来源的回答，复杂回答导致信任降低和认知负担增加。

Conclusion: AI沟通应模仿人类对话：简洁、透明、承认局限，而非追求复杂化以提升用户信任和参与度。

Abstract: Large Language Models (LLMs) in search applications increasingly prioritize
verbose, lexically complex responses that paradoxically reduce user
satisfaction and engagement. Through a comprehensive study of 10.000 (est.)
participants comparing responses from five major AI-powered search systems, we
demonstrate that users overwhelmingly prefer concise, source-attributed
responses over elaborate explanations. Our analysis reveals that current AI
development trends toward "artificial sophistication" create an uncanny valley
effect where systems sound knowledgeable but lack genuine critical thinking,
leading to reduced trust and increased cognitive load. We present evidence that
optimal AI communication mirrors effective human discourse: direct, properly
sourced, and honest about limitations. Our findings challenge the prevailing
assumption that more complex AI responses indicate better performance, instead
suggesting that human-like brevity and transparency are key to user engagement
and system reliability.

</details>


### [21] [Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts](https://arxiv.org/abs/2508.04787)
*Vishnu Menon,Andy Cherney,Elizabeth B. Cloude,Li Zhang,Tiffany D. Do*

Main category: cs.HC

TL;DR: 研究发现，在AI生成的互动播客中嵌入LLM引导的反思提示对学习效果无明显影响，但降低了用户感知吸引力，需进一步研究反思互动设计。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM引导的反思提示在互动播客中对学习效果和用户体验的影响。

Method: 36名本科生参与实验，对比有反思提示和无提示的互动播客版本。

Result: 学习效果无显著差异，但反思提示降低了用户感知吸引力。

Conclusion: 反思提示设计需优化，未来研究应关注其互动性设计。

Abstract: This study examined whether embedding LLM-guided reflection prompts in an
interactive AI-generated podcast improved learning and user experience compared
to a version without prompts. Thirty-six undergraduates participated, and while
learning outcomes were similar across conditions, reflection prompts reduced
perceived attractiveness, highlighting a call for more research on reflective
interactivity design.

</details>


### [22] [At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp](https://arxiv.org/abs/2508.04821)
*Yang Liu,Thorbjørn Mikkelsen,Zehai Liu,Gengchen Tian,Diako Mardanbegi,Qiushi Zhou,Hans Gellersen,Ken Pfeuffer*

Main category: cs.HC

TL;DR: SightWarp是一种利用眼手协调技术，将远处物体无缝召唤到用户指尖的交互技术，通过近空间代理实现直接手势操作，提高远处物体的操作效率。


<details>
  <summary>Details</summary>
Motivation: 3D用户界面中，直接手势操作在物体超出范围时效率低下，间接技术如凝视和捏合缺乏直接操作的即时性和本体反馈。SightWarp旨在填补这一空白。

Method: 通过眼手协调技术，当用户注视远处物体并将视线转移至手部或移动手部进入视野时，系统创建该物体的缩放近空间代理，从而支持直接手势操作。

Result: 用户研究显示，SightWarp能显著提高3D物体停靠任务的性能，优于凝视和捏合技术。其应用包括6自由度操作、概览与细节导航及微型世界交互。

Conclusion: SightWarp为近远空间物体交互提供了更灵活和高效的方法，扩展了3D界面的交互能力。

Abstract: In 3D user interfaces, reaching out to grab and manipulate something works
great until it is out of reach. Indirect techniques like gaze and pinch offer
an alternative for distant interaction, but do not provide the same immediacy
or proprioceptive feedback as direct gestures. To support direct gestures for
faraway objects, we introduce SightWarp, an interaction technique that exploits
eye-hand coordination to seamlessly summon object proxies to the user's
fingertips. The idea is that after looking at a distant object, users either
shift their gaze to the hand or move their hand into view-triggering the
creation of a scaled near-space proxy of the object and its surrounding
context. The proxy remains active until the eye-hand pattern is released. The
key benefit is that users always have an option to immediately operate on the
distant object through a natural, direct hand gesture. Through a user study of
a 3D object docking task, we show that users can easily employ SightWarp, and
that subsequent direct manipulation improves performance over gaze and pinch.
Application examples illustrate its utility for 6DOF manipulation,
overview-and-detail navigation, and world-in-miniature interaction. Our work
contributes to expressive and flexible object interactions across near and far
spaces.

</details>


### [23] [Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction](https://arxiv.org/abs/2508.04842)
*Amit Kumar Das,Mohammad Tarun,Klaus Mueller*

Main category: cs.HC

TL;DR: 论文通过引入Charts-of-Thought提示技术，评估大型语言模型（LLMs）的可视化素养，结果表明该方法显著提高了模型在可视化任务中的表现，甚至超越了人类基准。


<details>
  <summary>Details</summary>
Motivation: 评估现代多模态LLMs的可视化素养，并探索通过结构化提示技术是否能够提升其表现。

Method: 使用Charts-of-Thought方法，引导LLMs进行系统性数据提取、验证和分析后再回答问题，对比标准提示的效果。

Result: Claude-3.7-sonnet使用该方法得分50.17，远超人类基准28.82；其他模型的表现也有显著提升。

Conclusion: 结构化提示技术可显著提升LLMs的可视化素养，为复杂视觉任务提供了新的解决方案，并可能增强无障碍访问。

Abstract: This paper evaluates the visualization literacy of modern Large Language
Models (LLMs) and introduces a novel prompting technique called
Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,
GPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment
Test (VLAT) using standard prompts and our structured approach. The
Charts-of-Thought method guides LLMs through a systematic data extraction,
verification, and analysis process before answering visualization questions.
Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method,
far exceeding the human baseline of 28.82. This approach improved performance
across all models, with score increases of 21.8% for GPT-4.5, 9.4% for
Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The
performance gains were consistent across original and modified VLAT charts,
with Claude correctly answering 100% of questions for several chart types that
previously challenged LLMs. Our study reveals that modern multimodal LLMs can
surpass human performance on visualization literacy tasks when given the proper
analytical framework. These findings establish a new benchmark for LLM
visualization literacy and demonstrate the importance of structured prompting
strategies for complex visual interpretation tasks. Beyond improving LLM
visualization literacy, Charts-of-Thought could also enhance the accessibility
of visualizations, potentially benefiting individuals with visual impairments
or lower visualization literacy.

</details>


### [24] [An Implementation of a Visual Stepper in the GRASP Programming System](https://arxiv.org/abs/2508.04859)
*Panicz Maciej Godek*

Main category: cs.HC

TL;DR: 本文介绍了在GRASP编程系统中如何实现视觉评估器扩展，并围绕其设计及扩展机制架构提供了教程。


<details>
  <summary>Details</summary>
Motivation: 直接目的是展示GRASP系统中视觉评估器扩展的实现，间接目的是提供关于GRASP设计的教程，尤其是其扩展机制架构。

Method: 通过描述GRASP及其扩展机制的当前设计和实现细节，尽管这些细节可能在未来版本中发生变化。

Result: 尽管实现细节可能变化，但解决构建类似GRASP系统所需的问题集不变，这些问题对Scheme社区可能具有价值。

Conclusion: GRASP及其扩展机制的当前设计为社区提供了有价值的参考，尤其是构建类似系统时需解决的问题。

Abstract: The direct purpose of this paper - as its title suggests - is to present how
the visual evaluator extension is implemented in the GRASP programming system.
The indirect purpose is to provide a tutorial around the design of GRASP, and
in particular - around the architecture of its extension mechanism. Neither
GRASP nor its extension mechanisms are, at the moment of writing this paper,
final or complete, and we are certain that some details of the solutions
described in here will change even before the first release. What will not
change, though, is the set of problems that need to be solved in order to build
a system with capabilities similar to those of GRASP. We believe that these
problems might be of interest to the Scheme community.

</details>


### [25] [Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model](https://arxiv.org/abs/2508.04902)
*Luis Morales-Navarro,Michelle Gan,Evelyn Yu,Lauren Vogelstein,Yasmin B. Kafai,Danaé Metaxa*

Main category: cs.HC

TL;DR: 研究探讨高中生如何通过算法审计识别和理解日常AI/ML工具的偏见，强调青少年AI素养培养的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML技术普及，青少年需具备技术知识和社会影响的意识。

Method: 为期两周的参与式设计工作坊，14名14-15岁青少年审计TikTok Effect House的生成AI模型。

Result: 青少年参与者展现出创意和独立探索能力，发现专业审计中不常见的年龄偏见。

Conclusion: 算法审计可作为培养AI素养的有效活动，赋能青少年批判性思考AI系统，并为算法危害研究提供新视角。

Abstract: This study investigates how high school-aged youth engage in algorithm
auditing to identify and understand biases in artificial intelligence and
machine learning (AI/ML) tools they encounter daily. With AI/ML technologies
being increasingly integrated into young people's lives, there is an urgent
need to equip teenagers with AI literacies that build both technical knowledge
and awareness of social impacts. Algorithm audits (also called AI audits) have
traditionally been employed by experts to assess potential harmful biases, but
recent research suggests that non-expert users can also participate
productively in auditing. We conducted a two-week participatory design workshop
with 14 teenagers (ages 14-15), where they audited the generative AI model
behind TikTok's Effect House, a tool for creating interactive TikTok filters.
We present a case study describing how teenagers approached the audit, from
deciding what to audit to analyzing data using diverse strategies and
communicating their results. Our findings show that participants were engaged
and creative throughout the activities, independently raising and exploring new
considerations, such as age-related biases, that are uncommon in professional
audits. We drew on our expertise in algorithm auditing to triangulate their
findings as a way to examine if the workshop supported participants to reach
coherent conclusions in their audit. Although the resulting number of changes
in race, gender, and age representation uncovered by the teens were slightly
different from ours, we reached similar conclusions. This study highlights the
potential for auditing to inspire learning activities to foster AI literacies,
empower teenagers to critically examine AI systems, and contribute fresh
perspectives to the study of algorithmic harms.

</details>


### [26] [Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept](https://arxiv.org/abs/2508.04904)
*Yuqi Hu,Qiwen Xiong,Zhenzhen Qin,Brandon Watanabe,Yujing Wang,Mirjana Prpa,Ilmi Yoon*

Main category: cs.HC

TL;DR: 该论文提出了一种基于AI的3D模拟游戏，用于培训医疗专业人员进行根因分析（RCA），解决传统培训资源消耗高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RCA培训资源需求高，导致培训不足和实施不一致。

Method: 开发了一个AI驱动的3D模拟游戏，通过虚拟角色互动和沉浸式模拟进行RCA培训，并利用大型语言模型（LLMs）提供反馈。

Result: 原型系统成功模拟了ICU中的RCA调查，支持自然交互和反馈功能。

Conclusion: 论文计划通过实验进一步评估系统的有效性。

Abstract: Root Cause Analysis (RCA) is a critical tool for investigating adverse events
in healthcare and improving patient safety. However, existing RCA training
programs are often limited by high resource demands, leading to insufficient
training and inconsistent implementation. To address this challenge, we present
an AI-powered 3D simulation game that helps healthcare professionals develop
RCA skills through interactive, immersive simulations. This approach offers a
cost-effective, scalable, and accessible alternative to traditional training.
The prototype simulates an RCA investigation following a death in the ICU,
where learners interview five virtual avatars representing ICU team members to
investigate the incident and complete a written report. The system enables
natural, life-like interactions with avatars via large language models (LLMs),
emotional text-to-speech, and AI-powered animations. An additional LLM
component provides formative and summative feedback to support continual
improvement. We conclude by outlining plans to empirically evaluate the
system's efficacy.

</details>


### [27] [Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities](https://arxiv.org/abs/2508.04920)
*Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: 论文研究了分析师在叙事驱动探索数据时的关键障碍，并提出了改进设计的机会。


<details>
  <summary>Details</summary>
Motivation: 分析师在数据探索中常面临上下文丢失、推理路径难以追踪等问题，需要更好的支持工具。

Method: 通过48名参与者的形成性研究，识别了叙事驱动分析中的主要障碍。

Result: 研究发现包括上下文维护困难、推理路径追踪不便等设计问题。

Conclusion: 研究为叙事驱动分析工具的设计提供了改进方向。

Abstract: Analysts increasingly explore data through evolving, narrative-driven
inquiries, moving beyond static dashboards and predefined metrics as their
questions deepen and shift. As these explorations progress, insights often
become dispersed across views, making it challenging to maintain context or
clarify how conclusions arise. Through a formative study with 48 participants,
we identify key barriers that hinder narrative-driven exploration, including
difficulty maintaining context across views, tracing reasoning paths, and
externalizing evolving interpretations. Our findings surface design
opportunities to support narrative-driven analysis better.

</details>


### [28] [Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge](https://arxiv.org/abs/2508.04995)
*Matthew Kelly*

Main category: cs.HC

TL;DR: 论文提出情境化认知基础设施（SEI）框架，用于分析在大型语言模型影响下知识的权威性如何通过人机混合系统建立，强调协调而非分类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT）绕过了传统的知识验证方式，暴露了当代知识基础设施的脆弱性，需要新的框架来理解知识的权威性如何形成。

Method: 引入SEI框架，结合基础设施研究、平台理论和认识论，分析知识在人机系统中的可信度如何通过制度、计算和时间安排协调。

Result: SEI提供了一个前瞻性和适应性的认知管理模型，为AI治理、知识生产和信息系统伦理设计提供了非代表性的新视角。

Conclusion: SEI框架为后连贯条件下的知识权威性分析提供了有力工具，对学术交流和信息系统设计有重要意义。

Abstract: Large Language Models (LLMs) such as ChatGPT have rendered visible the
fragility of contemporary knowledge infrastructures by simulating coherence
while bypassing traditional modes of citation, authority, and validation. This
paper introduces the Situated Epistemic Infrastructures (SEI) framework as a
diagnostic tool for analyzing how knowledge becomes authoritative across hybrid
human-machine systems under post-coherence conditions. Rather than relying on
stable scholarly domains or bounded communities of practice, SEI traces how
credibility is mediated across institutional, computational, and temporal
arrangements. Integrating insights from infrastructure studies, platform
theory, and epistemology, the framework foregrounds coordination over
classification, emphasizing the need for anticipatory and adaptive models of
epistemic stewardship. The paper contributes to debates on AI governance,
knowledge production, and the ethical design of information systems by offering
a robust alternative to representationalist models of scholarly communication.

</details>


### [29] [Human-AI Schema Discovery and Application for Creative Problem Solving](https://arxiv.org/abs/2508.05045)
*Sitong Wang*

Main category: cs.HC

TL;DR: 论文提出了一种人机协作的框架，用于发现和应用结构模式（schema），以支持创造性问题的解决。


<details>
  <summary>Details</summary>
Motivation: 人类在创作过程中依赖结构模式（schema）来组织和探索想法，但这些模式在复杂或陌生领域中难以发现和应用。

Method: 研究设计了支持用户从示例中抽象出模式，并将其应用于人机协同创作工作流的系统。

Result: 框架使得隐式知识更易于获取和应用，推动了更透明和协作的人机系统。

Conclusion: 该研究为人机协作中的模式引导交互提供了新的见解，增强了系统的透明性和协作性。

Abstract: Humans often rely on underlying structural patterns-schemas-to create,
whether by writing stories, designing software, or composing music. Schemas
help organize ideas and guide exploration, but they are often difficult to
discover and apply, especially in complex or unfamiliar domains. My Ph.D.
research develops a framework for human-AI schema discovery and application to
support creative problem solving. I design systems that support users in
sensemaking over examples to abstract schemas, and in operationalizing schemas
into human-AI co-creative workflows for application. This research offers
insights into how schema-guided interaction can make implicit knowledge more
accessible and actionable, advancing more transparent and collaborative
human-AI systems.

</details>


### [30] [Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments](https://arxiv.org/abs/2508.05056)
*Vaanee Tripathi,Aalok Thakkar*

Main category: cs.HC

TL;DR: 本文提出了一个全面的框架，用于重新设计计算机科学入门课程，以公平地支持视觉障碍学生，避免依赖专业基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机科学教育工具和技术对视觉障碍学生支持不足，且实施复杂。

Method: 框架包含五个关键组成部分：无障碍学习资源、课堂学习工具包、结构化支持系统、在线工具库和心理社会支持。

Result: 该框架基于通用设计原则，通过专家咨询验证，为未来实证评估和推广奠定了基础。

Conclusion: 将无障碍性融入课程设计而非单独调整，能更全面地支持视觉障碍学生的教育需求。

Abstract: Computer science education has evolved extensively; however, systemic
barriers still prevent students with visual impairments from fully
participating. While existing research has developed specialized programming
tools and assistive technologies, these solutions remain fragmented and often
require complex technical infrastructure, which limits their classroom
implementation. Current approaches treat accessibility as individual
accommodations rather than integral curriculum design, creating gaps in
holistic educational support. This paper presents a comprehensive framework for
redesigning introductory computer science curricula to provide equitable
learning experiences for students with visual impairments without requiring
specialized technical infrastructure. The framework outlines five key
components that together contribute a systematic approach to curriculum
accessibility: accessible learning resources with pre-distributed materials and
tactile diagrams, in-class learning kits with hands-on demonstrations,
structured support systems with dedicated teaching assistance, an online tool
repository, and psychosocial support for classroom participation. Unlike
existing tool-focused solutions, this framework addresses both technical and
pedagogical dimensions of inclusive education while emphasizing practical
implementation in standard university settings. The design is grounded in
universal design principles and validated through expert consultation with
accessibility specialists and disability services professionals, establishing
foundations for future empirical evaluation of learning outcomes and student
engagement while serving as a template for broader institutional adoption.

</details>


### [31] [A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments](https://arxiv.org/abs/2508.05088)
*Sam Johnson-Lacoss,Santiago V. Lombeyda,S. George Djorgovski*

Main category: cs.HC

TL;DR: 论文探讨了混合现实（MR）技术如何通过沉浸式3D表示改进科研和临床工作中的数据分析和可视化，并提出了一种适合桌面物理空间的设计框架。


<details>
  <summary>Details</summary>
Motivation: 随着MR技术的普及，科研人员和临床医生需要从传统的2D界面转向更直观的3D交互方式，以更好地理解三维现象和患者病理。

Method: 论文分析了MR环境中的交互区域和模式，并将其转化为一个设计空间，以支持用户在桌面环境中进行精确的物体中心数据分析。

Result: 提出的设计空间优化了MR技术在桌面环境中的应用，提升了用户在3D数据分析和理解方面的效率。

Conclusion: MR技术为科研和临床领域提供了更高效的3D交互方式，未来的工作可以在设计空间的基础上进一步优化用户体验。

Abstract: Mixed reality (MR) environments are bound to become ubiquitous as MR
technology becomes lighter, higher resolution, more affordable, and overall
becomes a seamless extension of our current work and living spaces. For
research scientists and clinicians focused on understanding 3D phenomena or
patient pathologies within the context of the larger human anatomy, that means
a necessary evolution of their workstations currently only utilizing 2D
interfaces for everyday communication, logistics and data analysis. MR
technologies bring forth immersive 3D representations coexisting in our natural
spaces, while allowing for richer interconnected information displays, where 3D
representations greatly aid in the detailed understanding of physical
structures, spatial relationships, and 3D contextualization of 2D measurements,
projections, abstractions, and other data details. We present a breakdown of
the different interaction zones and modalities into a design space that best
accommodates the creation of applications for users engaged through MR
technologies in precise object-centric data analysis within the ergonomic
confines of their desktop physical spaces.

</details>


### [32] [SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures](https://arxiv.org/abs/2508.05098)
*Anand Kumar,Antony Albert Raj Irudayaraj,Ishita Chandra,Adwait Sharma,Aditya Shekhar Nittala*

Main category: cs.HC

TL;DR: 该论文提出了一种数据驱动的方法（SparseEMG），用于优化肌电信号（EMG）手势识别中的电极选择和分类器组合，以减少电极数量同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有工具包通常忽略电极选择对分类准确性的影响，因此需要一种系统性的方法来优化电极选择和分类器组合。

Method: 论文评估了28种组合（4种电极选择方案和7种分类器），通过多数据集分析，确定了能够最小化电极数量且不影响准确性的最优组合（Permutation Importance和Random Forest）。

Result: 结果显示，该方法可将电极数量减少53.5%。在此基础上开发的SparseEMG工具能够生成稀疏电极布局，并在多种硬件设置中得到验证。

Conclusion: SparseEMG工具生成的电极布局具有跨用户的可转移性，且性能变化小，为实际应用提供了实用支持。

Abstract: Gesture recognition with electromyography (EMG) is a complex problem
influenced by gesture sets, electrode count and placement, and machine learning
parameters (e.g., features, classifiers). Most existing toolkits focus on
streamlining model development but overlook the impact of electrode selection
on classification accuracy. In this work, we present the first data-driven
analysis of how electrode selection and classifier choice affect both accuracy
and sparsity. Through a systematic evaluation of 28 combinations (4 selection
schemes, 7 classifiers), across six datasets, we identify an approach that
minimizes electrode count without compromising accuracy. The results show that
Permutation Importance (selection scheme) with Random Forest (classifier)
reduces the number of electrodes by 53.5\%. Based on these findings, we
introduce SparseEMG, a design tool that generates sparse electrode layouts
based on user-selected gesture sets, electrode constraints, and ML parameters
while also predicting classification performance. SparseEMG supports 50+ unique
gestures and is validated in three real-world applications using different
hardware setups. Results from our multi-dataset evaluation show that the
layouts generated from the SparseEMG design tool are transferable across users
with only minimal variation in gesture recognition performance.

</details>


### [33] [Metacognition and self-regulated learning in manipulative robotic problem-solving task](https://arxiv.org/abs/2508.05112)
*Margarida Romero,George Kalmpourtzis*

Main category: cs.HC

TL;DR: 本章探讨了在创造性问题解决（CPS）中元认知的作用，特别是通过元推理监控学习者的推理和CPS活动，分析了探索与利用的调控方式。


<details>
  <summary>Details</summary>
Motivation: 研究元认知在解决未明确定义问题中的应用，以教育机器人案例展示元推理如何调控探索与利用知识的过程。

Method: 采用案例研究方法，通过参与者探索机器人模块的行为，分析元认知在CPS中的调控作用。

Result: 研究表明，元推理促进了问题空间的明确化，并帮助学习者逐步解决复杂问题。

Conclusion: 元认知通过调控探索与利用知识的过程，在创造性问题解决中发挥关键作用，尤其在未明确定义的问题中。

Abstract: Metacognition is an important aspect in creative problem solving (CPS) and
through this chapter we analyse the meta-reasoning aspects applied in the
different processes of monitoring the progress of learners' reasoning and CPS
activities. Meta-reasoning monitors the way that problem-solving processes
advance and regulate time and efforts towards a solution. In the context of an
ill-defined problem, exploration is required to develop a better-defined
problem space and advance towards the solution space. The way learners engage
in exploration and exploitations is regulated by the meta-reasoning within the
CPS activity. The objective of this chapter is to examine and identify the CPS
process with educational robots through a metacognitive and interactionist
approach. This chapter presents a case study, where, to solve a problem, a
participant had to explore a set of robot cubes to develop the technological
knowledge associated with each single component of the system, but also
conceptualize a system-level behaviour of the cubes when they are assembled.
The chapter presents the emergence of knowledge through the metacognitive
regulation of the process of exploration and exploitation of prior knowledge
and emergent knowledge until finding a solution

</details>


### [34] [AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study](https://arxiv.org/abs/2508.05156)
*Nikolaos Avouris*

Main category: cs.HC

TL;DR: 本文研究了AI语言导师在语言学习中的应用，通过混合方法评估了多种先进的AI导师工具，重点关注其对话功能和基于聊天记录的质量评估。


<details>
  <summary>Details</summary>
Motivation: 近年来，自然语言理解和实时处理技术取得了显著进展，AI语言导师的需求增加，尤其是提升语言技能（如口语、交际能力和理解能力）的需求。

Method: 采用混合方法的实证研究，评估不同类型的先进AI语言导师工具，包括用户体验评价和基于聊天记录的对话功能质量评估。

Result: 研究为评估此类系统的质量提供了标准，并为未来工具的设计提供了参考，包括数据隐私和学习者信息安全处理的考量。

Conclusion: 研究有助于建立AI语言导师系统的质量评估框架，并指导未来工具的设计和改进。

Abstract: This paper focuses on AI tutors in foreign language learning, a field of
application of AI tutors with great development, especially during the last
years, when great advances in natural language understanding and processing in
real time, have been achieved. These tutors attempt to address needs for
improving language skills (speaking, or communicative competence,
understanding). In this paper, a mixed-methos empirical study on the use of
different kinds of state-of-the-art AI tutors for language learning is
reported. This study involves a user experience evaluation of typical such
tools, with special focus in their conversation functionality and an evaluation
of their quality, based on chat transcripts. This study can help establish
criteria for assessing the quality of such systems and inform the design of
future tools, including concerns about data privacy and secure handling of
learner information.

</details>


### [35] [CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition](https://arxiv.org/abs/2508.05228)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 论文提出了一种新的通道级EEG特征选择方法（CWEFS），用于多维情感识别，解决现有方法忽略EEG特征结构对情感标签相关性的影响和通道重要性假设单一的问题。


<details>
  <summary>Details</summary>
Motivation: 高维多通道EEG特征包含大量冗余信息，影响情感识别效率和实时性。现有特征选择方法忽略EEG特征结构对情感标签相关性的影响，且假设通道重要性均一，限制了模型精度。

Method: CWEFS结合共享潜在结构模型和潜在语义分析，构建跨通道共识潜在空间，并引入自适应通道权重学习，自动确定不同通道的重要性。

Result: 在三个流行EEG数据集上验证，CWEFS选择的特征子集在六项评估指标中表现最优。

Conclusion: CWEFS有效提升了多维情感识别的性能，解决了现有特征选择方法的局限性。

Abstract: Due to the intracranial volume conduction effects, high-dimensional
multi-channel electroencephalography (EEG) features often contain substantial
redundant and irrelevant information. This issue not only hinders the
extraction of discriminative emotional representations but also compromises the
real-time performance. Feature selection has been established as an effective
approach to address the challenges while enhancing the transparency and
interpretability of emotion recognition models. However, existing EEG feature
selection research overlooks the influence of latent EEG feature structures on
emotional label correlations and assumes uniform importance across various
channels, directly limiting the precise construction of EEG feature selection
models for multi-dimensional affective computing. To address these limitations,
a novel channel-wise EEG feature selection (CWEFS) method is proposed for
multi-dimensional emotion recognition. Specifically, inspired by brain volume
conduction effects, CWEFS integrates EEG emotional feature selection into a
shared latent structure model designed to construct a consensus latent space
across diverse EEG channels. To preserve the local geometric structure, this
consensus space is further integrated with the latent semantic analysis of
multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive
channel-weight learning to automatically determine the significance of
different EEG channels in the emotional feature selection task. The
effectiveness of CWEFS was validated using three popular EEG datasets with
multi-dimensional emotional labels. Comprehensive experimental results,
compared against nineteen feature selection methods, demonstrate that the EEG
feature subsets chosen by CWEFS achieve optimal emotion recognition performance
across six evaluation metrics.

</details>


### [36] [ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging](https://arxiv.org/abs/2508.05229)
*Tianze Yu,Junming Zhang,Wenjia Dong,Xueyuan Xu,Li Zhuo*

Main category: cs.HC

TL;DR: 该论文提出了一种新的不完全多维度特征选择算法ADSEL，用于基于EEG的情绪识别，通过自适应双自我表达学习和最小二乘回归，解决了标签数据不完整和样本相关性被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 由于EEG特征的高维性和样本量有限，导致分类器过拟合和计算复杂度高。此外，现有的特征选择方法假设标签完整，忽视了标签不完整和样本与维度间的相关性。

Method: 结合自适应双自我表达学习（ADSEL）和最小二乘回归，建立样本级和维度级自我表达学习的双向路径，实现标签重建和特征选择。

Result: ADSEL提高了标签恢复的准确性，并有效识别出用于多维度情绪识别的最佳EEG特征子集。

Conclusion: ADSEL方法在处理不完全多维度标签和提升情绪识别性能方面表现出色，为解决类似问题提供了新思路。

Abstract: EEG based multi-dimension emotion recognition has attracted substantial
research interest in human computer interfaces. However, the high
dimensionality of EEG features, coupled with limited sample sizes, frequently
leads to classifier overfitting and high computational complexity. Feature
selection constitutes a critical strategy for mitigating these challenges. Most
existing EEG feature selection methods assume complete multi-dimensional
emotion labels. In practice, open acquisition environment, and the inherent
subjectivity of emotion perception often result in incomplete label data, which
can compromise model generalization. Additionally, existing feature selection
methods for handling incomplete multi-dimensional labels primarily focus on
correlations among various dimensions during label recovery, neglecting the
correlation between samples in the label space and their interaction with
various dimensions. To address these issues, we propose a novel incomplete
multi-dimensional feature selection algorithm for EEG-based emotion
recognition. The proposed method integrates an adaptive dual self-expression
learning (ADSEL) with least squares regression. ADSEL establishes a
bidirectional pathway between sample-level and dimension-level self-expression
learning processes within the label space. It could facilitate the
cross-sharing of learned information between these processes, enabling the
simultaneous exploitation of effective information across both samples and
dimensions for label reconstruction. Consequently, ADSEL could enhances label
recovery accuracy and effectively identifies the optimal EEG feature subset for
multi-dimensional emotion recognition.

</details>


### [37] [FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing](https://arxiv.org/abs/2508.05231)
*Wenjia Dong,Xueyuan Xu,Tianze Yu,Junming Zhang,Li Zhuo*

Main category: cs.HC

TL;DR: 文章提出了一种名为FDC-Net的新型框架，通过深度融合去噪和情感识别任务，实现了端到端的噪声鲁棒情感识别。


<details>
  <summary>Details</summary>
Motivation: 当前EEG情感识别方法通常将去噪和情感识别视为独立任务，导致误差累积且无法利用任务间的协同作用。论文旨在解决这一问题。

Method: 提出了FDC-Net框架，通过双向梯度传播、联合优化策略和门控注意力机制，动态耦合去噪和情感识别任务。

Result: 在DEAP和DREAMER数据集上，FDC-Net在去噪任务中分别达到96.30%和90.31%的相关性系数，在情感识别任务中分别达到82.3+7.1%和88.1+0.8%的准确率。

Conclusion: FDC-Net通过任务间的动态协作，显著提升了噪声环境下EEG情感识别的性能。

Abstract: Electroencephalogram (EEG)-based emotion recognition holds significant value
in affective computing and brain-computer interfaces. However, in practical
applications, EEG recordings are susceptible to the effects of various
physiological artifacts. Current approaches typically treat denoising and
emotion recognition as independent tasks using cascaded architectures, which
not only leads to error accumulation, but also fails to exploit potential
synergies between these tasks. Moreover, conventional EEG-based emotion
recognition models often rely on the idealized assumption of "perfectly
denoised data", lacking a systematic design for noise robustness. To address
these challenges, a novel framework that deeply couples denoising and emotion
recognition tasks is proposed for end-to-end noise-robust emotion recognition,
termed as Feedback-Driven Collaborative Network for Denoising-Classification
Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic
collaborative mechanism between artifact removal and emotion recognition
through: (1) bidirectional gradient propagation with joint optimization
strategies; (2) a gated attention mechanism integrated with frequency-adaptive
Transformer using learnable band-position encoding. Two most popular EEG-based
emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels
were employed to compare the artifact removal and emotion recognition
performance between ASLSL and nine state-of-the-art methods. In terms of the
denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of
96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the
emotion recognition task under physiological artifact interference, FDC-Net
achieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on
DREAMER.

</details>


### [38] [Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models](https://arxiv.org/abs/2508.05238)
*Wei Xiang,Muchen Li,Jie Yan,Manling Zheng,Hanfei Zhu,Mengyun Jiang,Lingyun Sun*

Main category: cs.HC

TL;DR: 研究使用大型语言模型（LLM）通过人性化建议帮助L3自动驾驶系统的驾驶员保持注意力，减少认知负担，有效协调次要任务与接管行为。


<details>
  <summary>Details</summary>
Motivation: L3自动驾驶允许驾驶员进行次要任务，但在紧急情况下需要快速反应，增加了认知负担。

Method: 利用LLM根据路况触发，通过视觉和听觉途径提供人性化建议，引导驾驶员行为。

Result: 工具能有效维持驾驶员注意力，减少认知负担，并协调次要任务与接管行为。

Conclusion: LLM在支持多任务自动驾驶中具有潜力。

Abstract: Level 3 automated driving systems allows drivers to engage in secondary tasks
while diminishing their perception of risk. In the event of an emergency
necessitating driver intervention, the system will alert the driver with a
limited window for reaction and imposing a substantial cognitive burden. To
address this challenge, this study employs a Large Language Model (LLM) to
assist drivers in maintaining an appropriate attention on road conditions
through a "humanized" persuasive advice. Our tool leverages the road conditions
encountered by Level 3 systems as triggers, proactively steering driver
behavior via both visual and auditory routes. Empirical study indicates that
our tool is effective in sustaining driver attention with reduced cognitive
load and coordinating secondary tasks with takeover behavior. Our work provides
insights into the potential of using LLMs to support drivers during multi-task
automated driving.

</details>


### [39] [A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness](https://arxiv.org/abs/2508.05281)
*Ahmed Abdal Shafi Rasel,Ahmed Mustafa Amlan,Tasmim Shajahan Mim,Tanvir Hasan*

Main category: cs.HC

TL;DR: 研究探讨孟加拉用户对算法决策公平性的认知，通过混合方法分析文化、社会及情境因素的影响，强调文化敏感设计对公平AI的重要性。


<details>
  <summary>Details</summary>
Motivation: 考察非西方背景下用户对AI公平性、透明度和问责的理解，补充全球伦理AI讨论。

Method: 结合定量调查与定性访谈，分析文化、社会及情境因素。

Result: 发现用户对人类监督、解释机制和可争议性的复杂态度，需文化敏感设计。

Conclusion: 研究为非西方视角的算法公平性讨论提供新见解，推动全球伦理AI发展。

Abstract: This study explores perceptions of fairness in algorithmic decision-making
among users in Bangladesh through a comprehensive mixed-methods approach. By
integrating quantitative survey data with qualitative interview insights, we
examine how cultural, social, and contextual factors influence users'
understanding of fairness, transparency, and accountability in AI systems. Our
findings reveal nuanced attitudes toward human oversight, explanation
mechanisms, and contestability, highlighting the importance of culturally aware
design principles for equitable and trustworthy algorithmic systems. These
insights contribute to ongoing discussions on algorithmic fairness by
foregrounding perspectives from a non-Western context, thus broadening the
global dialogue on ethical AI deployment.

</details>


### [40] [Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs](https://arxiv.org/abs/2508.05325)
*Jonathan C. Roberts,Hanan Alnjar,Aron E. Owen,Panagiotis D. Ritsos*

Main category: cs.HC

TL;DR: Critical Design Strategy (CDS) 是一个结构化方法，通过反思和批判性思维帮助设计师评估和改进可视化设计。


<details>
  <summary>Details</summary>
Motivation: 可视化设计师和工具开发者，尤其是初学者，需要一个系统的方法来批判性地评估和改进设计。

Method: CDS包括三个阶段：1) 初步印象（标题和形容词选择）；2) 深入批判（6个关键视角的30个启发式问题）；3) 综合反思和下一步行动。

Result: CDS在本科和研究生课程中得到应用，并通过多年的实践不断优化，包括措辞改进和应用开发。

Conclusion: CDS是一个实用且可扩展的工具，适合用于教学和设计实践。

Abstract: We present the Critical Design Strategy (CDS) - a structured method designed
to facilitate the examination of visualisation designs through reflection and
critical thought. The CDS helps designers think critically and make informed
improvements using heuristic evaluation. When developing a visual tool or
pioneering a novel visualisation approach, identifying areas for enhancement
can be challenging. Critical thinking is particularly crucial for visualisation
designers and tool developers, especially those new to the field, such as
studying visualisation in higher education. The CDS consists of three stages
across six perspectives: Stage 1 captures the essence of the idea by assigning
an indicative title and selecting five adjectives (from twenty options) to form
initial impressions of the design. Stage 2 involves an in-depth critique using
30 heuristic questions spanning six key perspectives - user, environment,
interface, components, design, and visual marks. Stage 3 focuses on
synthesising insights, reflecting on design decisions, and determining the next
steps forward. We introduce the CDS and explore its use across three
visualisation modules in both undergraduate and postgraduate courses. Our
longstanding experience with the CDS has allowed us to refine and develop it
over time: from its initial creation through workshops in 2017/18 to
improvements in wording and the development of two applications by 2020,
followed by the expansion of support notes and refinement of heuristics through
2023; while using it in our teaching each year. This sustained use allows us to
reflect on its practical application and offer guidance on how others can
incorporate it into their own work.

</details>


### [41] [Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform](https://arxiv.org/abs/2508.05332)
*Masanori Ibara,Yuichi Hiroi,Takushi Kamegai,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 本文探讨了如何通过工业元宇宙平台Cluster实现3D数据在工业和建筑领域的民主化应用，支持多设备接入和多用户协作决策。


<details>
  <summary>Details</summary>
Motivation: 传统3D设计数据（如BIM和CAD）仅限专家使用，阻碍了普通用户的参与。本文旨在通过工业元宇宙打破这一限制。

Method: 分析工业和建筑领域的主要数据格式特性与限制，整理元宇宙中的集成工作流程，并通过多领域应用案例展示协作决策支持。

Result: 工业元宇宙平台Cluster支持多设备访问和多用户同时参与，实现了传统专家依赖系统难以达到的民主化环境。

Conclusion: 元宇宙与数字孪生技术的结合为工业和建筑领域的协作决策提供了新的可能性，推动了3D数据的民主化使用。

Abstract: Traditionally, specialized 3D design data, such as BIM and CAD, have been
accessible only to a select group of experts, creating significant barriers
that prevent general users from participating in decision-making processes.
This paper provides a systematic overview of practical insights for utilizing
3D data in industrial and architectural domains by presenting implementation
cases of the industrial metaverse on Cluster, a commercial cross-device
metaverse platform. This paper analyzes the characteristics and constraints of
major data formats in the industrial and architectural fields and organizes
integration workflows for the metaverse. Through application cases utilizing 3D
data across multiple domains, we present practical examples of collaborative
decision-making support enabled by the fusion of metaverse and digital twin
technologies. Specifically, we demonstrate that multi-device access and
simultaneous multi-user participation capabilities foster democratic
environments in the industrial metaverse, which are challenging to achieve with
conventional, expert-dependent systems.

</details>


### [42] [Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study](https://arxiv.org/abs/2508.05497)
*Federico Scarì,Olger Siebinga,Arkady Zgonnikov*

Main category: cs.HC

TL;DR: 论文提出了一种结构化评估框架，结合人机交互指标，全面评估自动车辆（AV）与人类驾驶车辆（HDV）的交互效果，弥补了现有研究忽视人类中心维度的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AV控制算法的评估主要侧重于技术性能，忽视了对人类驾驶体验的影响，需要一种更全面的评估方法。

Method: 提出了一个包含四个关键领域（交互效果、交互感知、交互努力、交互能力）的评估框架，并通过模拟实验验证其有效性。

Result: 实验表明，涵盖四个领域的指标可以揭示AV与HDV交互中的关键差异，强调了全面评估的必要性。

Conclusion: 该框架为研究者和政策制定者提供了系统的方法，促进开发功能强大且对人类友好的AV。

Abstract: As automated vehicles (AVs) increasingly integrate into mixed-traffic
environments, evaluating their interaction with human-driven vehicles (HDVs)
becomes critical. In most research focused on developing new AV control
algorithms (controllers), the performance of these algorithms is assessed
solely based on performance metrics such as collision avoidance or lane-keeping
efficiency, while largely overlooking the human-centred dimensions of
interaction with HDVs. This paper proposes a structured evaluation framework
that addresses this gap by incorporating metrics grounded in the human-robot
interaction literature. The framework spans four key domains: a) interaction
effect, b) interaction perception, c) interaction effort, and d) interaction
ability. These domains capture both the performance of the AV and its impact on
human drivers around it. To demonstrate the utility of the framework, we apply
it to a case study evaluating how a state-of-the-art AV controller interacts
with human drivers in a merging scenario in a driving simulator. Measuring
HDV-HDV interactions as a baseline, this study included one representative
metric per domain: a) perceived safety, b) subjective ratings, specifically how
participants perceived the other vehicle's driving behaviour (e.g.,
aggressiveness or predictability) , c) driver workload, and d) merging success.
The results showed that incorporating metrics covering all four domains in the
evaluation of AV controllers can illuminate critical differences in driver
experience when interacting with AVs. This highlights the need for a more
comprehensive evaluation approach. Our framework offers researchers,
developers, and policymakers a systematic method for assessing AV behaviour
beyond technical performance, fostering the development of AVs that are not
only functionally capable but also understandable, acceptable, and safe from a
human perspective.

</details>


### [43] [Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2508.05572)
*Yifan Wang,Hongfeng Ai,Ruiqi Li,Maowei Jiang,Ruiyuan Kang,Jiahua Dong,Cheng Jiang,Chenzhong Li*

Main category: cs.HC

TL;DR: 论文提出AE-GAN和LMCF框架，解决医学时间序列诊断中数据标注成本高和对比学习泛化性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决医学数据标注成本高导致模型过拟合，以及现有对比学习方法复杂且泛化性差的问题。

Method: 结合外部数据和AE-GAN提取先验知识；提出LMCF框架，通过多视角对比学习自适应捕获疾病特征。

Result: 在三个数据集上优于七个基线方法，适用于心肌梗塞、阿尔茨海默症等疾病诊断。

Conclusion: AE-GAN和LMCF显著提升医学诊断效果，代码已开源。

Abstract: In medical time series disease diagnosis, two key challenges are identified.
First, the high annotation cost of medical data leads to overfitting in models
trained on label-limited, single-center datasets. To address this, we propose
incorporating external data from related tasks and leveraging AE-GAN to extract
prior knowledge, providing valuable references for downstream tasks. Second,
many existing studies employ contrastive learning to derive more generalized
medical sequence representations for diagnostic tasks, usually relying on
manually designed diverse positive and negative sample pairs. However, these
approaches are complex, lack generalizability, and fail to adaptively capture
disease-specific features across different conditions. To overcome this, we
introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that
integrates a multi-head attention mechanism and adaptively learns
representations from different views through inter-view and intra-view
contrastive learning strategies. Additionally, the pre-trained AE-GAN is used
to reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process. Experiments on three
target datasets demonstrate that our method consistently outperforms other
seven baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease. We release the source code at xxxxx.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [44] [Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off](https://arxiv.org/abs/2508.04825)
*Seungyong Lee,Jeong-gi Kwak*

Main category: cs.GR

TL;DR: Voost是一个统一的扩散变换框架，通过联合学习虚拟试穿和试脱任务，提升服装与身体的对应关系推理能力，无需额外网络或损失函数。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试穿在姿势和外观变化下难以准确建模服装与身体的对应关系，Voost旨在通过联合学习任务解决这一问题。

Method: Voost采用单扩散变换器联合学习试穿和试脱任务，支持灵活的生成条件和服装类别，并引入注意力温度缩放和自校正采样技术。

Result: Voost在多个基准测试中取得最佳性能，包括对齐精度、视觉保真度和泛化能力，优于现有基线。

Conclusion: Voost通过统一框架和推理技术显著提升虚拟试穿和试脱的性能，为服装与身体关系建模提供新方向。

Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a
target garment, but accurately modeling garment-body correspondence remains a
persistent challenge, especially under pose and appearance variation. In this
paper, we propose Voost - a unified and scalable framework that jointly learns
virtual try-on and try-off with a single diffusion transformer. By modeling
both tasks jointly, Voost enables each garment-person pair to supervise both
directions and supports flexible conditioning over generation direction and
garment category, enhancing garment-body relational reasoning without
task-specific networks, auxiliary losses, or additional labels. In addition, we
introduce two inference-time techniques: attention temperature scaling for
robustness to resolution or mask variation, and self-corrective sampling that
leverages bidirectional consistency between tasks. Extensive experiments
demonstrate that Voost achieves state-of-the-art results on both try-on and
try-off benchmarks, consistently outperforming strong baselines in alignment
accuracy, visual fidelity, and generalization.

</details>


### [45] [Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting](https://arxiv.org/abs/2508.04965)
*Zijian Wang,Beizhen Zhao,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种新型的感知-采样-压缩框架，用于改进3D高斯泼溅技术，通过场景感知补偿算法和金字塔采样表示，显著提升了内存效率和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 传统的3D高斯泼溅技术在大规模场景管理和高效存储方面存在挑战，尤其是在复杂环境或有限计算资源下。

Method: 引入了场景感知补偿算法和金字塔采样表示，并结合广义高斯混合模型压缩算法来实现高效存储。

Result: 实验表明，该方法显著提高了内存效率和视觉质量，同时保持了实时渲染速度。

Conclusion: 提出的框架有效解决了3D高斯泼溅技术在资源管理和存储方面的局限性，为实时和高质量渲染提供了可行的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
capabilities in real-time and photorealistic novel view synthesis. However,
traditional 3DGS representations often struggle with large-scale scene
management and efficient storage, particularly when dealing with complex
environments or limited computational resources. To address these limitations,
we introduce a novel perceive-sample-compress framework for 3D Gaussian
Splatting. Specifically, we propose a scene perception compensation algorithm
that intelligently refines Gaussian parameters at each level. This algorithm
intelligently prioritizes visual importance for higher fidelity rendering in
critical areas, while optimizing resource usage and improving overall visible
quality. Furthermore, we propose a pyramid sampling representation to manage
Gaussian primitives across hierarchical levels. Finally, to facilitate
efficient storage of proposed hierarchical pyramid representations, we develop
a Generalized Gaussian Mixed model compression algorithm to achieve significant
compression ratios without sacrificing visual fidelity. The extensive
experiments demonstrate that our method significantly improves memory
efficiency and high visual quality while maintaining real-time rendering speed.

</details>


### [46] [Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction](https://arxiv.org/abs/2508.04966)
*Yifan Zhou,Beizhen Zhao,Pengcheng Wu,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种新的动态3D高斯泼溅（3DGS）框架，通过混合显式-隐式函数解决现有方法在动态场景中的过平滑或特征冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态3DGS方法因低秩分解或高维网格采样导致过平滑或特征冲突，主要是频谱冲突问题未能解决。

Method: 采用频谱感知拉普拉斯编码架构、增强高斯动态属性和自适应高斯分裂策略。

Result: 在复杂动态场景重建中表现出色，达到更高重建保真度。

Conclusion: 新方法有效解决了动态3DGS的频谱冲突问题，提升了重建质量。

Abstract: While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its
extension to dynamic scenes introduces significant challenges. Existing dynamic
3DGS methods suffer from either over-smoothing due to low-rank decomposition or
feature collision from high-dimensional grid sampling. This is because of the
inherent spectral conflicts between preserving motion details and maintaining
deformation consistency at different frequency. To address these challenges, we
propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.
Our approach contains three key innovations: a spectral-aware Laplacian
encoding architecture which merges Hash encoding and Laplacian-based module for
flexible frequency motion control, an enhanced Gaussian dynamics attribute that
compensates for photometric distortions caused by geometric deformation, and an
adaptive Gaussian split strategy guided by KDTree-based primitive control to
efficiently query and optimize dynamic areas. Through extensive experiments,
our method demonstrates state-of-the-art performance in reconstructing complex
dynamic scenes, achieving better reconstruction fidelity.

</details>


### [47] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 该综述探讨了基于Gaussian Splatting的语言引导3D场景理解的研究现状，包括理论、集成策略和应用案例，同时指出了计算瓶颈和数据稀缺等挑战。


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting在实时3D场景表示中表现出色，但与语言模型的结合缺乏全面概述，本研究填补了这一空白。

Method: 通过结构化综述分析，总结了语言嵌入与Gaussian Splatting的集成方法、理论基础和应用实例。

Result: 揭示了当前研究的局限性，如计算效率、泛化能力和语义标注数据的不足。

Conclusion: 提出了未来研究方向，以推动语言引导的3D场景理解技术发展。

Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.

</details>


### [48] [RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer](https://arxiv.org/abs/2508.05115)
*Fangyu Du,Taiqing Li,Ziwei Zhang,Qian Qiao,Tan Yu,Dingcheng Zhen,Xu Jia,Yang Yang,Shunshun Yin,Siyuan Liu*

Main category: cs.GR

TL;DR: RAP提出了一种实时音频驱动的肖像动画框架，通过混合注意力机制和静态-动态训练-推断范式，实现了高质量、实时的肖像动画生成。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动肖像动画方法虽然能生成高质量结果，但计算复杂度高且难以实时部署，因此在实时应用中受限。

Method: RAP采用混合注意力机制实现精细音频控制，并引入静态-动态训练-推断范式，避免显式运动监督。

Result: RAP在精确音频驱动控制、减少时间漂移和保持视觉保真度方面表现出色，并在实验中达到SOTA性能。

Conclusion: RAP是首个在实时约束下生成高质量肖像动画的统一框架，为实时应用提供了可行解决方案。

Abstract: Audio-driven portrait animation aims to synthesize realistic and natural
talking head videos from an input audio signal and a single reference image.
While existing methods achieve high-quality results by leveraging
high-dimensional intermediate representations and explicitly modeling motion
dynamics, their computational complexity renders them unsuitable for real-time
deployment. Real-time inference imposes stringent latency and memory
constraints, often necessitating the use of highly compressed latent
representations. However, operating in such compact spaces hinders the
preservation of fine-grained spatiotemporal details, thereby complicating
audio-visual synchronization RAP (Real-time Audio-driven Portrait animation), a
unified framework for generating high-quality talking portraits under real-time
constraints. Specifically, RAP introduces a hybrid attention mechanism for
fine-grained audio control, and a static-dynamic training-inference paradigm
that avoids explicit motion supervision. Through these techniques, RAP achieves
precise audio-driven control, mitigates long-term temporal drift, and maintains
high visual fidelity. Extensive experiments demonstrate that RAP achieves
state-of-the-art performance while operating under real-time constraints.

</details>


### [49] [Refining Gaussian Splatting: A Volumetric Densification Approach](https://arxiv.org/abs/2508.05187)
*Mohamed Abdul Gafoor,Marius Preda,Titus Zaharia*

Main category: cs.GR

TL;DR: 提出了一种基于惯性体积的新型密度控制方法，用于改进3D高斯泼溅中的点基元管理，并在重建质量上超越了原始3DGS方法。


<details>
  <summary>Details</summary>
Motivation: 原始3DGS的点基元管理策略存在不足，影响了新视角合成的质量，需要更有效的密度控制方法。

Method: 利用高斯函数的惯性体积指导点基元的细化，同时研究了SfM和DIM方法对点云初始化的影响。

Result: 实验表明，该方法在Mip-NeRF 360数据集上显著提升了重建质量，适用于多样场景。

Conclusion: 提出的密度控制方法有效解决了3DGS的点管理问题，为高质量新视角合成提供了可行方案。

Abstract: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)
often depends on effective point primitive management. The underlying Adaptive
Density Control (ADC) process addresses this issue by automating densification
and pruning. Yet, the vanilla 3DGS densification strategy shows key
shortcomings. To address this issue, in this paper we introduce a novel density
control method, which exploits the volumes of inertia associated to each
Gaussian function to guide the refinement process. Furthermore, we study the
effect of both traditional Structure from Motion (SfM) and Deep Image Matching
(DIM) methods for point cloud initialization. Extensive experimental
evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses
3DGS in reconstruction quality, delivering encouraging performance across
diverse scenes.

</details>


### [50] [GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs](https://arxiv.org/abs/2508.05524)
*Sefat Rahman,Tushar M. Athawale,Paul Rosen*

Main category: cs.GR

TL;DR: 本文提出了一种名为GASP的算法，用于生成符合三个关键属性的Reeb图可视化，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Reeb图绘制算法忽视了边界约束、紧凑性和梯度对齐这三个关键属性，导致可视化结果不能准确反映数据的拓扑结构。

Method: 提出GASP算法，生成符合边界约束、紧凑性和梯度对齐的可视化Reeb图。

Result: 通过与TTK中的几何重心算法对比，GASP在定性和定量评估中表现出更好的效果。

Conclusion: GASP算法能更准确地表示Reeb图，提升可视化质量。

Abstract: Reeb graphs are an important tool for abstracting and representing the
topological structure of a function defined on a manifold. We have identified
three properties for faithfully representing Reeb graphs in a visualization.
Namely, they should be constrained to the boundary, compact, and aligned with
the function gradient. Existing algorithms for drawing Reeb graphs are agnostic
to or violate these properties. In this paper, we introduce an algorithm to
generate Reeb graph visualizations, called \textit{GASP}, that is cognizant of
these properties, thereby producing visualizations that are more representative
of the underlying data. To demonstrate the improvements, the resulting Reeb
graphs are evaluated both qualitatively and quantitatively against the
geometric barycenter algorithm, using its implementation available in the
Topology ToolKit (TTK), a widely adopted tool for calculating and visualizing
Reeb graphs.

</details>


### [51] [Point cloud segmentation for 3D Clothed Human Layering](https://arxiv.org/abs/2508.05531)
*Davide Garavaso,Federico Masi,Pietro Musoni,Umberto Castellani*

Main category: cs.GR

TL;DR: 提出了一种新的3D点云分割范式，用于多层衣物的人体建模，解决了现有方法在重叠层分割上的不足。


<details>
  <summary>Details</summary>
Motivation: 在3D人体建模中，准确分割多层衣物和人体部分是关键，但现有方法无法有效处理重叠层。

Method: 提出'clothed human layering'范式，允许3D点同时关联多层；并通过神经网络和合成数据集验证。

Result: 实验表明该范式在合成和真实扫描数据集上均优于传统分割方法。

Conclusion: 新方法为多层衣物的人体建模提供了更准确的分割方案，推动了3D服装建模的发展。

Abstract: 3D Cloth modeling and simulation is essential for avatars creation in several
fields, such as fashion, entertainment, and animation. Achieving high-quality
results is challenging due to the large variability of clothed body especially
in the generation of realistic wrinkles. 3D scan acquisitions provide more
accuracy in the representation of real-world objects but lack semantic
information that can be inferred with a reliable semantic reconstruction
pipeline. To this aim, shape segmentation plays a crucial role in identifying
the semantic shape parts. However, current 3D shape segmentation methods are
designed for scene understanding and interpretation and only few work is
devoted to modeling. In the context of clothed body modeling the segmentation
is a preliminary step for fully semantic shape parts reconstruction namely the
underlying body and the involved garments. These parts represent several layers
with strong overlap in contrast with standard segmentation methods that provide
disjoint sets. In this work we propose a new 3D point cloud segmentation
paradigm where each 3D point can be simultaneously associated to different
layers. In this fashion we can estimate the underlying body parts and the
unseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer
above. We name this segmentation paradigm clothed human layering. We create a
new synthetic dataset that simulates very realistic 3D scans with the ground
truth of the involved clothing layers. We propose and evaluate different neural
network settings to deal with 3D clothing layering. We considered both coarse
and fine grained per-layer garment identification. Our experiments demonstrates
the benefit in introducing proper strategies for the segmentation on the
garment domain on both the synthetic and real-world scan datasets.

</details>


### [52] [Physically Controllable Relighting of Photographs](https://arxiv.org/abs/2508.05626)
*Chris Careaga,Yağız Aksoy*

Main category: cs.GR

TL;DR: 提出一种自监督的图像重光照方法，结合物理渲染和神经渲染实现可控、真实的光照编辑。


<details>
  <summary>Details</summary>
Motivation: 结合传统渲染的物理精确性和神经渲染的逼真外观，实现对真实场景的光照控制。

Method: 通过单目估计几何和本征组件推断彩色网格表示，使用路径追踪引擎渲染新光照场景，并通过神经渲染器生成最终结果。

Result: 实现了对真实场景光照的高质量控制和编辑。

Conclusion: 该方法将3D图形工具中的光照控制能力引入真实场景重光照中。

Abstract: We present a self-supervised approach to in-the-wild image relighting that
enables fully controllable, physically based illumination editing. We achieve
this by combining the physical accuracy of traditional rendering with the
photorealistic appearance made possible by neural rendering. Our pipeline works
by inferring a colored mesh representation of a given scene using monocular
estimates of geometry and intrinsic components. This representation allows
users to define their desired illumination configuration in 3D. The scene under
the new lighting can then be rendered using a path-tracing engine. We send this
approximate rendering of the scene through a feed-forward neural renderer to
predict the final photorealistic relighting result. We develop a differentiable
rendering process to reconstruct in-the-wild scene illumination, enabling
self-supervised training of our neural renderer on raw image collections. Our
method represents a significant step in bringing the explicit physical control
over lights available in typical 3D computer graphics tools, such as Blender,
to in-the-wild relighting.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [53] [Injection Locking and Coupling Dynamics in Superconducting Nanowire based Cryogenic Oscillators](https://arxiv.org/abs/2508.04878)
*Md Mazharul Islam,Md Shafayat Hossain,Kathleen E Hamilton,Ahmedullah Aziz*

Main category: cs.ET

TL;DR: 研究了超导纳米线低温振荡器的注入锁定和互耦合动力学，揭示了频率同步和信号协调的关键机制。


<details>
  <summary>Details</summary>
Motivation: 超低温振荡器在超导电子和量子计算中至关重要，为提供稳定、低噪声信号和最小能量损耗。

Method: 通过数值研究注入锁定和两个ScNW振荡器之间的互耦合动力学，分析关键设计参数和耦合强度的影响。

Result: 发现锁定范围受分流电阻、纳米线电感和耦合强度控制，且耦合强度可精确控制振荡器间的相位差。

Conclusion: 研究结果有助于构建ScNW振荡神经网络、同步低温逻辑块和片上低温谐振器阵列。

Abstract: Oscillators designed to function at cryogenic temperatures play a critical
role in superconducting electronics and quantum computing by providing stable,
low noise signals with minimal energy loss.Here we present a comprehensive
numerical study of injection locking and mutual coupling dynamics in
superconducting nanowire based cryogenic oscillators.Using the design space of
standalone ScNW based oscillator, we investigate two critical mechanisms that
govern frequency synchronization and signal coordination in cryogenic computing
architectures.First, an injection locking induced by an external AC signal with
a frequency near the oscillators natural frequency, and second, the mutual
coupling dynamics between two ScNW oscillators under varying coupling
strengths.We identify key design parameters such as shunt resistance, nanowire
inductance, and coupling strength that govern the locking range.Additionally,
we examine how the amplitude of the injected signal affects the amplitude of
the locked oscillation, offering valuable insights for power aware oscillator
synchronization.Furthermore, we analyze mutual synchronization between coupled
ScNW oscillators using capacitive and resistive coupling elements.Our results
reveal that the phase difference between oscillators can be precisely
controlled by tuning the coupling strength, enabling programmable phase encoded
information processing.These findings could enable building ScNW based
oscillatory neural networks, synchronized cryogenic logic blocks, and on chip
cryogenic resonator arrays.

</details>


### [54] [Wave Computing based on Dynamical Networks: Applications in Optimization Problems](https://arxiv.org/abs/2508.05014)
*Yunwen Liu,Jiang Xiao*

Main category: cs.ET

TL;DR: 提出了一种基于波传播的计算框架，利用节点和边的波操作能力，在多维空间中实现并行计算，特别适合解决NP难问题。


<details>
  <summary>Details</summary>
Motivation: 探索一种新型计算范式，通过波传播和硬件并行性，高效解决传统计算中难以处理的NP难问题。

Method: 开发了基于波传播的计算框架，节点和边具有波操作能力（如频率混频、时间延迟），并将并行性扩展到多维空间（空间、时间、频率）。

Result: 通过SPICE模拟验证了该架构在解决数分割问题、0/1背包问题和旅行商问题等NP难问题中的潜力。

Conclusion: 该计算框架为高效解决NP难问题提供了一种新颖的并行计算途径。

Abstract: We develop a computing framework that leverages wave propagation within an
interconnected network, where nodes and edges possess wave manipulation
capabilities, such as frequency mixing or time delay. This computing paradigm
can not only achieve intrinsic parallelism like existing works by the
exploration of an exponential number of possibilities simultaneously with very
small number of hardware units, but also extend this unique characteristic to a
multidimensional space including spatial, temporal and frequency domains,
making it particularly effective for addressing NP-hard problems. The proposed
architecture has been validated through SPICE simulations, demonstrating its
potential capability in solving several NP-hard problems, such as the Number
Partitioning Problem, the 0/1 Knapsack Problem, and the Traveling Salesman
Problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [55] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: 该论文提出了一种名为OPTIMUMP2P的新型八卦算法，基于随机线性网络编码（RLNC），旨在提升libp2p库的性能和可靠性，尤其在存在恶意行为者的网络中。


<details>
  <summary>Details</summary>
Motivation: 现有的libp2p库中的Gossipsub协议在性能和可靠性方面存在不足，尤其是在恶意环境下。通过引入RLNC技术，可以加速信息传播并确保可靠性。

Method: 提出OPTIMUMP2P算法，利用RLNC技术优化信息传播。在模拟和真实环境中进行广泛评估。

Result: 实验表明，OPTIMUMP2P在性能和可靠性上优于Gossipsub协议，显著提升了区块链中的块传播时间。

Conclusion: OPTIMUMP2P是一种高效且可靠的八卦算法，适用于分布式系统，尤其是在区块链领域。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [56] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 论文研究两种具有不同通信能力的机器人如何协作捕捉移动目标，并分析了竞争比。


<details>
  <summary>Details</summary>
Motivation: 探讨不对称通信能力对线性搜索竞争比的影响。

Method: 设计新的线性搜索算法，考虑目标起始距离、速度及运动方向等因素。

Result: 分析了不同场景下的竞争比。

Conclusion: 研究表明通信不对称性对线性搜索效率有重要影响。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [57] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是为构建数据共享平台而开发的开源数据平台，目前已管理超过28 PB数据和6400万FAIR数据对象，支持自动生成数据门户和API。


<details>
  <summary>Details</summary>
Motivation: 旨在通过云平台支持研究社区高效管理、分析和共享数据。

Method: 通过定义数据模型自动生成数据门户（用于搜索和提交数据）和FAIR API，并基于标准软件服务构建以确保互操作性。

Result: 成功构建了多个数据共享平台，累计管理大量数据，并支持FAIR数据标准。

Conclusion: Gen3是一个可扩展且互操作的开源数据平台，能有效支持数据共享需求。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [58] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 本文提出了一种基于图匹配的调度策略Tesserae，用于优化深度学习集群的资源利用，显著提升了作业完成时间和集群效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练已成为数据中心的主要工作负载，而现有的调度策略要么性能不佳，要么扩展性差，亟需改进。

Method: 作者将调度约束转化为图匹配问题，设计了新型的调度策略以最小化作业迁移开销并优化资源打包，并将其集成到Tesserae调度器中。

Result: 实验表明，Tesserae将平均作业完成时间提高了1.62倍，集群总完成时间提高了1.15倍。

Conclusion: 通过图匹配方法设计的调度策略显著提升了深度学习集群的效率和扩展性。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [59] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus是一种生产级分布式加速器原生查询引擎，通过优化数据移动、内存利用和计算，在TPC-H基准测试中表现优于Databricks Photon。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集在线分析处理中的高成本问题，利用GPU等加速器提高吞吐量和降低成本。

Method: 设计Theseus引擎，采用异步控制机制和固定大小的页锁定主机内存分配，优化网络通信、数据加载、内存管理和GPU计算任务。

Result: 在TPC-H基准测试中，Theseus在成本相同的情况下性能提升高达4倍，并能在100TB规模下处理全部查询。

Conclusion: Theseus通过硬件资源紧密耦合和内存优化，显著提升了分布式查询引擎的性能和效率。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [60] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 该论文开发了一种基于自适应网格细化（AMR）的高阶求解器，使用Regent语言实现，解决了动态数据结构、网格有效性及任务融合等挑战，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 高阶求解器在可压缩流模拟中至关重要，而AMR可以减少计算成本。本文旨在利用Regent语言开发高效的AMR求解器，解决其实现中的技术挑战。

Method: 使用Regent语言实现AMR求解器，包括动态数据结构、网格有效性强制和任务融合优化，并通过GPU内核自动生成提升性能。

Result: 任务融合实现18倍加速，GPU内核生成带来9.7倍加速，并通过两个欧拉方程控制的可压缩流问题验证了方法有效性。

Conclusion: 本研究成功开发了高效的AMR求解器，显著提升了计算性能，为解决复杂流体问题提供了实用工具。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [61] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 为解决分布式模型训练中GPU集群异构性问题，提出了一种异构感知的LLM训练模拟器，可预测训练时间并支持自定义配置。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练模拟器假设基础设施同质，但实际中设备异构性不可避免，需开发能应对异构性的模拟器。

Method: 设计了异构感知的分布式LLM模拟器，包含非均匀工作负载分区等组件，支持自定义设备组和并行映射配置。

Result: 初步模拟结果显示异构性对模型计算和通信时间有显著影响。

Conclusion: 异构感知模拟器填补了现有技术与实际需求之间的差距，为分布式训练提供了更准确的预测工具。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [62] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL是一个用于大规模生物数据集的并行文件下载工具，通过动态调整并发流数量，显著提高下载速度，比现有工具快达4倍。


<details>
  <summary>Details</summary>
Motivation: 现有的下载工具使用固定的并发设置，无法适应动态网络条件，导致带宽利用低效和下载时间长。

Method: FastBioDL将下载过程建模为在线优化问题，使用效用函数和梯度下降动态调整并发流数量，以最大化吞吐量并减少资源开销。

Result: 在公共基因组数据集上的评估显示，FastBioDL比现有工具快达4倍，高速网络中快达2.1倍。

Conclusion: FastBioDL通过智能优化HTTP或FTP下载，为研究人员提供了一种高效、易用的大规模基因组数据获取方案。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [63] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT是一种基于深度强化学习的模块化数据传输架构，通过优化并发操作显著提升数据传输效率。


<details>
  <summary>Details</summary>
Motivation: 传统文件传输工具因配置固定或优化方法单一导致资源利用不足和不稳定，需更高效的解决方案。

Method: 采用深度强化学习（PPO）代理优化并发操作，结合轻量级网络-系统模拟器进行离线训练。模块化设计解耦I/O和网络任务。

Result: 在生产级测试中，AutoMDT实现8倍收敛速度和68%传输时间减少。

Conclusion: AutoMDT通过模块化和强化学习显著提升数据传输性能，优于现有技术。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [AgenticData: An Agentic Data Analytics System for Heterogeneous Data](https://arxiv.org/abs/2508.05002)
*Ji Sun,Guoliang Li,Peiyao Zhou,Yihui Ma,Jingzhe Xu,Yuan Li*

Main category: cs.DB

TL;DR: AgenticData是一种创新的数据代理分析系统，通过自然语言提问自主分析多领域数据，减少专家依赖。


<details>
  <summary>Details</summary>
Motivation: 现有非结构化数据分析系统依赖专家编写代码和管理复杂流程，成本高且耗时，需改进。

Method: 采用反馈驱动规划技术和多代理协作策略，包括数据剖析代理、语义交叉验证代理和智能记忆代理，并结合语义优化模型。

Result: 实验表明，AgenticData在多项测试中表现优异，准确性显著超过现有方法。

Conclusion: AgenticData通过自主代理和语义优化，有效提升数据分析效率与准确性。

Abstract: Existing unstructured data analytics systems rely on experts to write code
and manage complex analysis workflows, making them both expensive and
time-consuming. To address these challenges, we introduce AgenticData, an
innovative agentic data analytics system that allows users to simply pose
natural language (NL) questions while autonomously analyzing data sources
across multiple domains, including both unstructured and structured data.
First, AgenticData employs a feedback-driven planning technique that
automatically converts an NL query into a semantic plan composed of relational
and semantic operators. We propose a multi-agent collaboration strategy by
utilizing a data profiling agent for discovering relevant data, a semantic
cross-validation agent for iterative optimization based on feedback, and a
smart memory agent for maintaining short-term context and long-term knowledge.
Second, we propose a semantic optimization model to refine and execute semantic
plans effectively. Our system, AgenticData, has been tested using three
benchmarks. Experimental results showed that AgenticData achieved superior
accuracy on both easy and difficult tasks, significantly outperforming
state-of-the-art methods.

</details>


### [65] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: SPEAR是一种语言和运行时系统，旨在解决现代LLM流程中提示管理的问题，通过结构化、自适应的方式提升提示的可重用性和运行时控制。


<details>
  <summary>Details</summary>
Motivation: 现代LLM流程中，提示管理仍局限于脆弱的字符串形式，缺乏与数据流的结合，限制了重用和优化。

Method: 提出SPEAR框架，支持运行时提示动态调整和结构化管理，定义了提示代数以支持多种优化模式。

Result: 初步实验验证了动态调整和优化模式的有效性，相比静态提示和代理重试，性能有所提升。

Conclusion: SPEAR为提示管理提供了系统化解决方案，提升了灵活性和效率。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


### [66] [Data-Aware Socratic Query Refinement in Database Systems](https://arxiv.org/abs/2508.05061)
*Ruiyuan Zhang,Chrysanthi Kosyfaki,Xiaofang Zhou*

Main category: cs.DB

TL;DR: DASG是一个基于对话的查询增强框架，通过交互式澄清解决自然语言查询的歧义，优化查询过程。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中的歧义问题，提升查询的准确性和效率。

Method: 通过语言模糊性、模式基础置信度和预测成本量化歧义，结合语义相关性和信息增益选择最佳澄清问题。

Result: 在三个数据集中验证，DASG提高了查询精度并保持效率。

Conclusion: DASG建立了一种协作式分析范式，系统主动参与查询优化而非被动翻译用户请求。

Abstract: In this paper, we propose Data-Aware Socratic Guidance (DASG), a
dialogue-based query enhancement framework that embeds \linebreak interactive
clarification as a first-class operator within database systems to resolve
ambiguity in natural language queries. DASG treats dialogue as an optimization
decision, asking clarifying questions only when the expected execution cost
reduction exceeds the interaction overhead. The system quantifies ambiguity
through linguistic fuzziness, schema grounding confidence, and projected costs
across relational and vector backends. Our algorithm selects the optimal
clarifications by combining semantic relevance, catalog-based information gain,
and potential cost reduction. We evaluate our proposed framework on three
datasets. The results show that DASG demonstrates improved query precision
while maintaining efficiency, establishing a cooperative analytics paradigm
where systems actively participate in query formulation rather than passively
translating user requests.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [67] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 论文分析了LLM在RTL代码生成中的错误原因，提出针对性纠正技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: LLM在RTL代码生成中的低成功率源于多种错误，需深入分析以改进。

Method: 通过错误分类，结合知识库、规则检查和外部工具，采用迭代调试循环。

Result: 新框架在VerilogEval上的准确率达91.0%，比基线高32.7%。

Conclusion: 针对性纠错技术显著提升了LLM在RTL代码生成中的表现。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [68] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: 该论文提出了一种名为relOBI的改进方法，通过在关键握手信号中结合三重模块冗余（TMR）和其他信号的错误校正码（ECC）保护，提高了片上互连的可靠性。实验结果将故障注入的脆弱性从34.85%降至0%，面积开销为2.6倍，时间影响为1.4倍，且面积开销比文献中报告的细粒度三重复制和表决方法低1.8倍。


<details>
  <summary>Details</summary>
Motivation: 在现代片上系统中，互连是处理器核心与内存及外设交互的关键部件。在辐射强烈的环境中，互连中的任何软错误都可能导致整个系统失效。因此，需要一种方法来提高互连的可靠性。

Method: 论文提出relOBI，扩展了Open Bus Interface（OBI），结合了关键握手信号的三重模块冗余（TMR）和其他信号的ECC保护，以实现全面的可靠性。通过设计和测试一个完全可靠的交叉开关，验证了方法的有效性。

Result: 实验结果显示，relOBI将故障注入的脆弱性从34.85%降至0%。尽管面积增加了2.6倍且时间影响为1.4倍，但面积开销比现有方法低1.8倍。

Conclusion: relOBI提供了一种有效的方法来提高片上互连的可靠性，尤其是在恶劣环境中。尽管带来了一定的面积和时间开销，但其性能优于现有方法。

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [69] [Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions](https://arxiv.org/abs/2508.05377)
*Hongyu Zhou,Yinan Zhang,Aixin Sun,Zhiqi Shen*

Main category: cs.IR

TL;DR: 本文提出了一种结构化评估框架，系统分析了多模态推荐系统的性能，发现其在稀疏交互和召回阶段效果显著，并探讨了不同模态和集成策略的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统整合多种数据类型以提升性能，但其实际效果尚不明确，作者旨在通过系统评估框架厘清其优劣势。

Method: 提出四维评估框架（比较效率、推荐任务、推荐阶段、多模态集成），对多种可复现的模型进行基准测试。

Result: 多模态数据在稀疏场景和召回阶段表现突出；文本模态适合电商，视觉模态适合短视频；集成学习优于融合学习，模型大小与效果不一定正相关。

Conclusion: 研究为构建高效多模态推荐系统提供了实践指导，强调了模态选择、集成策略和模型设计的必要性。

Abstract: Multimodal recommendation systems are increasingly popular for their
potential to improve performance by integrating diverse data types. However,
the actual benefits of this integration remain unclear, raising questions about
when and how it truly enhances recommendations. In this paper, we propose a
structured evaluation framework to systematically assess multimodal
recommendations across four dimensions: Comparative Efficiency, Recommendation
Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a
set of reproducible multimodal models against strong traditional baselines and
evaluate their performance on different platforms. Our findings show that
multimodal data is particularly beneficial in sparse interaction scenarios and
during the recall stage of recommendation pipelines. We also observe that the
importance of each modality is task-specific, where text features are more
useful in e-commerce and visual features are more effective in short-video
recommendations. Additionally, we explore different integration strategies and
model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based
Learning, and that larger models do not necessarily deliver better results. To
deepen our understanding, we include case studies and review findings from
other recommendation domains. Our work provides practical insights for building
efficient and effective multimodal recommendation systems, emphasizing the need
for thoughtful modality selection, integration strategies, and model design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [70] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: 论文提出了Agnostics，一种语言无关的后训练流程，通过统一验证器测试多语言代码，显著提升了低资源语言代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在低资源语言代码生成上的困难，消除针对每种语言的额外开发成本。

Method: 采用I/O格式单元测试、配置编译运行环境，结合RLVR技术在统一执行环境训练。

Result: 在5种低资源语言上，Agnostics使Qwen-3 4B模型性能媲美更大规模模型，并创下新SOTA。

Conclusion: Agnostics通过简化后训练流程，显著提升了LLMs在低资源语言上的表现，并提供了易用的工具链。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [71] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen是一种新型的LVLM增强迭代框架，通过闭环反馈机制提升文本到图像（T2I）生成模型的性能，特别是在细粒度控制方面。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在复杂指令处理、细粒度内容控制和深度语义一致性方面存在挑战。

Method: LumiGen包括IPPA模块进行主动提示增强和IVFR模块作为“视觉评论家”迭代优化生成图像。

Result: 在LongBench-T2I基准测试中，LumiGen以3.08的平均分优于现有基线，尤其在文本渲染和姿态表达方面表现突出。

Conclusion: LVLM集成显著提升了T2I模型的可控性和图像生成质量。

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [72] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 论文提出了一种层次化联邦领域泛化（HFedDG）方法HFedATM，通过滤波器最优传输对齐和收缩感知正则化均值聚合，有效解决了领域偏移问题，并在多数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统层次化联邦学习（HFL）存在领域偏移问题，影响模型性能。现有联邦领域泛化（FedDG）方法在HFL中的应用尚未充分探索。

Method: 提出HFedATM方法：通过滤波器最优传输对齐不同站点的模型，并使用收缩感知正则化均值聚合合并模型。

Result: 实验表明HFedATM显著提升了现有FedDG基线的性能，同时保持了计算和通信效率。理论分析表明其具有更紧的泛化误差界。

Conclusion: HFedATM成功解决了HFL中的领域偏移问题，提升了模型性能与训练稳定性。

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [73] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL是一个新的纵向联邦学习框架，解决了数据样本不对齐和无法本地独立推理的问题，通过XCom和DS-Align模块实现了特征补全和决策子空间对齐。


<details>
  <summary>Details</summary>
Motivation: 解决VFL中数据样本必须对齐和无法本地独立推理的限制。

Method: 提出X-VFL框架，包含XCom（特征补全）和DS-Align（决策子空间对齐）模块。

Result: 在CIFAR-10和MIMIC-III数据集上分别实现15%和43%的准确率提升。

Conclusion: X-VFL在缺失特征和本地推理场景下表现优异，验证了其实际有效性。

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [74] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: 研究探讨了AR在CPR中的使用如何影响情境意识（SA），开发了一款AR应用并通过眼动追踪分析SA，提出了预测SA的深度学习模型FixGraphPool。


<details>
  <summary>Details</summary>
Motivation: AR系统在提升任务性能的同时可能导致认知隧道效应，影响SA。研究旨在评估AR引导CPR中的SA问题。

Method: 开发Magic Leap 2上的AR应用提供CPR反馈，通过用户研究模拟意外事件，结合眼动追踪和问卷调查分析SA。

Result: 高SA与更大的眼动幅度和速度相关，FixGraphPool模型在预测SA时表现优越（准确率83.0%）。

Conclusion: 眼动追踪能有效建模AR中的SA，为设计安全AR系统提供依据。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [75] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: ASkDAgger框架通过利用新手行动计划中的信息，减少了互动模仿学习中的人类教学工作量，并通过SAG、FIER和PIER三种方法优化了查询频率与失败率之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 减少互动模仿学习中的人类教学工作量，利用新手行动计划中的信息以提高学习效率。

Method: 提出了ASkDAgger框架，包括S-Aware Gating (SAG)、Foresight Interactive Experience Replay (FIER)和Prioritized Interactive Experience Replay (PIER)三种方法。

Result: ASkDAgger在仿真和现实环境中的语言条件操作任务中验证了其有效性。

Conclusion: ASkDAgger通过优化查询频率与失败率，减少了标注需求，提高了泛化能力和适应速度。

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [Minimal Model Reasoning in Description Logics: Don't Try This at Home!](https://arxiv.org/abs/2508.05350)
*Federica Di Stefano,Quentin Manière,Magdalena Ortiz,Mantas Šimkus*

Main category: cs.AI

TL;DR: 该论文研究了描述逻辑中的极小模型问题，发现即使是简单的逻辑如$​​\mathcal{EL}$，其概念可满足性在极小模型下也是不可判定的。通过引入非循环性条件，部分恢复了可判定性，并探讨了DL-Lite家族的复杂性。


<details>
  <summary>Details</summary>
Motivation: 描述逻辑中的极小模型问题尚未深入研究，尤其是在所有谓词必须极小化的纯极小模型情况下。了解这一问题对于知识表示技术的发展至关重要。

Method: 论文分析了流行描述逻辑中的极小模型问题，探讨了不可判定性，并引入了非循环性条件来恢复可判定性。还研究了DL-Lite家族的复杂性。

Result: 发现$​​\mathcal{EL}$的概念可满足性在极小模型下不可判定。通过非循环性条件，将复杂度降低到双指数时间以下。DL-Lite$_{\text{horn}}$的扩展被证明为ExpSpace-难。

Conclusion: 极小模型问题在描述逻辑中具有高度复杂性，即使简单逻辑也面临不可判定性。非循环性条件为恢复可判定性提供了一种途径，但复杂性问题仍需进一步研究。

Abstract: Reasoning with minimal models has always been at the core of many knowledge
representation techniques, but we still have only a limited understanding of
this problem in Description Logics (DLs). Minimization of some selected
predicates, letting the remaining predicates vary or be fixed, as proposed in
circumscription, has been explored and exhibits high complexity. The case of
`pure' minimal models, where the extension of all predicates must be minimal,
has remained largely uncharted. We address this problem in popular DLs and
obtain surprisingly negative results: concept satisfiability in minimal models
is undecidable already for $\mathcal{EL}$. This undecidability also extends to
a very restricted fragment of tuple-generating dependencies. To regain
decidability, we impose acyclicity conditions on the TBox that bring the
worst-case complexity below double exponential time and allow us to establish a
connection with the recently studied pointwise circumscription; we also derive
results in data complexity. We conclude with a brief excursion to the DL-Lite
family, where a positive result was known for DL-Lite$_{\text{core}}$, but our
investigation establishes ExpSpace-hardness already for its extension
DL-Lite$_{\text{horn}}$.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: HOW-Seg是一种人机交互框架，用于开开放世界的点云语义分割，通过稀疏人类标注和层次化原型消歧机制，显著提升了分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界点云语义分割（OW-Seg）方法依赖资源密集的增量学习或密集标注支持数据，实用性受限。

Method: 构建查询数据的类原型，利用稀疏人类标注指导分割，引入层次化原型消歧机制和密集条件随机场（CRF）优化标签分配。

Result: 在稀疏标注下（如每新类一点击），HOW-Seg达到或超越5-shot广义少样本分割（GFS-Seg）方法；使用高级骨干网络和更密标注时，在S3DIS和ScanNetv2上分别达到85.27%和66.37% mIoU。

Conclusion: HOW-Seg通过人机交互动态优化分割结果，显著提升开放世界点云语义分割性能。

Abstract: Open-world point cloud semantic segmentation (OW-Seg) aims to predict point
labels of both base and novel classes in real-world scenarios. However,
existing methods rely on resource-intensive offline incremental learning or
densely annotated support data, limiting their practicality. To address these
limitations, we propose HOW-Seg, the first human-in-the-loop framework for
OW-Seg. Specifically, we construct class prototypes, the fundamental
segmentation units, directly on the query data, avoiding the prototype bias
caused by intra-class distribution shifts between the support and query data.
By leveraging sparse human annotations as guidance, HOW-Seg enables
prototype-based segmentation for both base and novel classes. Considering the
lack of granularity of initial prototypes, we introduce a hierarchical
prototype disambiguation mechanism to refine ambiguous prototypes, which
correspond to annotations of different classes. To further enrich contextual
awareness, we employ a dense conditional random field (CRF) upon the refined
prototypes to optimize their label assignments. Through iterative human
feedback, HOW-Seg dynamically improves its predictions, achieving high-quality
segmentation for both base and novel classes. Experiments demonstrate that with
sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or
surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)
method under the 5-shot setting. When using advanced backbones (e.g.,
Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),
HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,
significantly outperforming alternatives.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 研究了在Microsoft Hololens 2设备上为现有手语（SL）虚拟形象添加调整功能的影响，发现用户偏好可调整设置但对用户体验（UX）和可理解性无显著提升，需改进SL元素缺失问题。


<details>
  <summary>Details</summary>
Motivation: 探究手语虚拟形象调整功能对用户可理解性、体验和接受度的影响，以优化SL技术。

Method: 通过专家用户的交互分析，比较可调整与不可调整虚拟形象的表现。

Result: 可调整功能未显著提升UX或可理解性，用户偏好情感吸引力多于功能实用。

Conclusion: 手语虚拟形象需默认具备高可理解性，建议改进动画质量与交互设计。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [79] [Probabilistic Alternating Simulations for Policy Synthesis in Uncertain Stochastic Dynamical Systems](https://arxiv.org/abs/2508.05062)
*Thom Badings,Alessandro Abate*

Main category: eess.SY

TL;DR: 扩展概率模拟关系以处理同时包含随机和非确定性扰动的系统，提出了一种新的交替模拟关系，并应用于4D Dubins车的策略合成。


<details>
  <summary>Details</summary>
Motivation: 现有概率模拟关系无法处理同时具有随机和非确定性扰动的系统，限制了策略合成的能力。

Method: 提出一种基于交替模拟的扩展关系，支持对随机扰动进行概率分析，对非确定性扰动进行鲁棒分析。

Result: 新关系通过实验验证了其在4D Dubins车策略合成中的适用性。

Conclusion: 扩展的关系为复杂扰动系统的策略合成提供了更通用的理论框架。

Abstract: A classical approach to formal policy synthesis in stochastic dynamical
systems is to construct a finite-state abstraction, often represented as a
Markov decision process (MDP). The correctness of these approaches hinges on a
behavioural relation between the dynamical system and its abstraction, such as
a probabilistic simulation relation. However, probabilistic simulation
relations do not suffice when the system dynamics are, next to being
stochastic, also subject to nondeterministic (i.e., set-valued) disturbances.
In this work, we extend probabilistic simulation relations to systems with both
stochastic and nondeterministic disturbances. Our relation, which is inspired
by a notion of alternating simulation, generalises existing relations used for
verification and policy synthesis used in several works. Intuitively, our
relation allows reasoning probabilistically over stochastic uncertainty, while
reasoning robustly (i.e., adversarially) over nondeterministic disturbances. We
experimentally demonstrate the applicability of our relations for policy
synthesis in a 4D-state Dubins vehicle.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [80] [Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications](https://arxiv.org/abs/2508.04889)
*Theia Henderson,David R. Karger,David D. Clark*

Main category: cs.SI

TL;DR: Graffiti是一个用于构建多样化社交应用的系统，支持个性化设计和互操作性，避免数据孤岛和背景崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有社交应用设计僵化且孤立，Graffiti旨在解决构建新应用的挑战，同时实现互操作性。

Method: 通过总具体化和频道的概念，Graffiti实现设计和规则的兼容与隔离，并提供客户端API和Vue.js插件。

Result: 成功构建类似Twitter、Messenger和Wikipedia的应用，并验证了其互操作性和扩展性。

Conclusion: Graffiti为社交应用设计提供了灵活性和互操作性，支持多样化生态系统的发展。

Abstract: Most social applications, from Twitter to Wikipedia, have rigid
one-size-fits-all designs, but building new social applications is both
technically challenging and results in applications that are siloed away from
existing communities. We present Graffiti, a system that can be used to build a
wide variety of personalized social applications with relative ease that also
interoperate with each other. People can freely move between a plurality of
designs -- each with its own aesthetic, feature set, and moderation -- all
without losing their friends or data.
  Our concept of total reification makes it possible for seemingly
contradictory designs, including conflicting moderation rules, to interoperate.
Conversely, our concept of channels prevents interoperation from occurring by
accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we
show admits at least two decentralized implementations. Above the API, we built
a Vue.js plugin, which we use to develop applications similar to Twitter,
Messenger, and Wikipedia using only client-side code. Our case studies explore
how these and other novel applications interoperate, as well as the broader
ecosystem that Graffiti enables.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [81] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 提出了一种名为MICoBot的人机协作系统，通过混合主动对话范式处理任务分配问题，显著提升了任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 为了适应多样化的人类伙伴及其随时间变化的行为和意愿，需开发一种灵活的通信和协作机制。

Method: 系统采用三级决策：元规划器制定策略，规划器优化任务分配，执行器决定具体行动或语言。

Result: 在仿真和现实环境中验证了方法的有效性，任务成功率和用户体验显著优于纯LLM基线和其他分配模型。

Conclusion: MICoBot通过多层次决策实现了高效的人机协作，适用于多样化的用户和任务场景。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [82] [Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications](https://arxiv.org/abs/2508.05248)
*Pradeep Kumar Shukla,Tanujit Chakraborty,Mustafa Sari,Joel Sarout,Partha Pratim Mandal*

Main category: physics.geo-ph

TL;DR: 研究分析了不同围压条件下盐岩时间依赖性变形（蠕变）的预测方法，重点评估了深度神经网络模型的性能。


<details>
  <summary>Details</summary>
Motivation: 蠕变变形评估对地下核废料、氢能等设施的设计和运营至关重要。盐岩因其低孔隙率、低渗透性和高蠕变能力成为研究对象。

Method: 使用多级三轴蠕变数据，通过季节性分解、格兰杰因果检验及统计测试验证数据特性，随后比较多种深度神经网络模型的预测性能。

Result: N-BEATS和TCN模型在各应力水平下表现最优，准确率比传统模型提高15-20%。

Conclusion: 深度神经网络能有效捕捉复杂时间依赖关系，适用于盐岩蠕变预测。

Abstract: This study provides an in-depth analysis of time series forecasting methods
to predict the time-dependent deformation trend (also known as creep) of salt
rock under varying confining pressure conditions. Creep deformation assessment
is essential for designing and operating underground storage facilities for
nuclear waste, hydrogen energy, or radioactive materials. Salt rocks, known for
their mechanical properties like low porosity, low permeability, high
ductility, and exceptional creep and self-healing capacities, were examined
using multi-stage triaxial (MSTL) creep data. After resampling, axial strain
datasets were recorded at 5--10 second intervals under confining pressure
levels ranging from 5 to 35 MPa over 5.8--21 days. Initial analyses, including
Seasonal-Trend Decomposition (STL) and Granger causality tests, revealed
minimal seasonality and causality between axial strain and temperature data.
Further statistical tests, such as the Augmented Dickey-Fuller (ADF) test,
confirmed the stationarity of the data with p-values less than 0.05, and
wavelet coherence plot (WCP) analysis indicated repeating trends. A suite of
deep neural network (DNN) models (Neural Basis Expansion Analysis for Time
Series (N-BEATS), Temporal Convolutional Networks (TCN), Recurrent Neural
Networks (RNN), and Transformers (TF)) was utilized and compared against
statistical baseline models. Predictive performance was evaluated using Root
Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage
Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE). Results
demonstrated that N-BEATS and TCN models outperformed others across various
stress levels, respectively. DNN models, particularly N-BEATS and TCN, showed a
15--20\% improvement in accuracy over traditional analytical models,
effectively capturing complex temporal dependencies and patterns.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [83] [QFOR: A Fidelity-aware Orchestrator for Quantum Computing Environments using Deep Reinforcement Learning](https://arxiv.org/abs/2508.04974)
*Hoa T. Nguyen,Muhammad Usman,Rajkumar Buyya*

Main category: quant-ph

TL;DR: 量子云计算面临资源分配的挑战，QFOR通过深度强化学习实现了异构量子节点的任务调度优化。


<details>
  <summary>Details</summary>
Motivation: 量子硬件的异构性和噪声导致任务分配和调度效率低下，现有启发式方法难以适应动态条件或平衡执行保真度和时间。

Method: 提出QFOR框架，将量子任务调度建模为马尔可夫决策过程，使用近端策略优化算法学习自适应调度策略，结合IBM量子处理器的校准数据估计噪声性能。

Result: QFOR在保真度性能上比基线方法提升29.5-84%，同时保持量子执行时间可比性。

Conclusion: QFOR为量子云计算的资源高效利用提供了可配置且自适应的解决方案。

Abstract: Quantum cloud computing enables remote access to quantum processors, yet the
heterogeneity and noise of available quantum hardware create significant
challenges for efficient resource orchestration. These issues complicate the
optimization of quantum task allocation and scheduling, as existing heuristic
methods fall short in adapting to dynamic conditions or effectively balancing
execution fidelity and time. Here, we propose QFOR, a Quantum Fidelity-aware
Orchestration of tasks across heterogeneous quantum nodes in cloud-based
environments using Deep Reinforcement learning. We model the quantum task
orchestration as a Markov Decision Process and employ the Proximal Policy
Optimization algorithm to learn adaptive scheduling policies, using IBM quantum
processor calibration data for noise-aware performance estimation. Our
configurable framework balances overall quantum task execution fidelity and
time, enabling adaptation to different operational priorities. Extensive
evaluation demonstrates that QFOR is adaptive and achieves significant
performance with 29.5-84% improvements in relative fidelity performance over
heuristic baselines. Furthermore, it maintains comparable quantum execution
times, contributing to cost-efficient use of quantum computation resources.

</details>


### [84] [A Design for an Early Quantum Network](https://arxiv.org/abs/2508.04967)
*Yuan Li,Chen Zhang,Hao Zhang,Tao Huang,Yunjie Liu*

Main category: quant-ph

TL;DR: 提出了一个适用于早期量子网络的设计方案，兼容现有三种量子中继技术，旨在最大化满足量子应用的多样化需求，并通过仿真验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 量子信息技术的快速发展使得量子网络支持多样化应用的需求增加，需要满足高保真度和快速完成时间等关键指标。

Method: 提出了一个兼容现有三种量子中继技术的设计，描述了所需的标识符和实施量子请求的具体过程。通过离散事件建模进行仿真，考虑早期网络中的噪声和不完美参数。

Result: 分析了参数对生成纠缠态保真度和请求完成时间的影响，并探讨了中央控制器在路径选择之外的额外决策。

Conclusion: 设计方案能够有效满足量子网络的多样化需求，即使在资源有限和性能次优的条件下，并通过仿真验证了其可行性。

Abstract: With the rapid advancement of quantum information technology, quantum
networks have become essential for supporting diverse applications, which often
have stringent demands for key metrics such as fidelity and request completion
time. In this work, we propose a design for early-stage quantum networks that
is compatible with the three existing quantum repeater technologies. The design
aims to maximize the ability of the network to accommodate the diverse needs of
quantum applications, even under conditions of limited quantum resources and
suboptimal network performance. We have also described the required identifiers
in the quantum network and the specific process for implementing quantum
requests. To assess the feasibility of our design, we conduct simulations based
on discrete-event modeling of quantum networks. The simulations consider
various types of noise and imperfect parameters that might exist in early-stage
networks. We analyze the impact of these parameters on the fidelity of the
generated entangled states and the request completion time. Furthermore, we
investigated additional decisions that the central controller can make beyond
path selection, such as the choice of cutoff time and the allocation of network
resources to requests.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [85] [Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants](https://arxiv.org/abs/2508.05286)
*Katsiaryna Dzialets,Aleksandra Makeeva,Ilya Vlasov,Anna Potriasaeva,Aleksei Rostovskii,Yaroslav Golubev,Anastasiia Birillo*

Main category: cs.CY

TL;DR: 该研究通过大规模调查（18,032名学习者）更新了计算机科学教育领域的多方面数据，并开放数据集以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 因应新趋势（如AI）和多样化的学习需求，需更新全面的计算机科学教育研究。

Method: 对来自173个国家的学习者进行涵盖广泛主题（如AI使用、学习动机）的调查。

Result: 提供了开放数据集，并展示了三个研究方向：学习挑战、新兴学习形式和IDE内学习。

Conclusion: 该数据集旨在推动计算机教育研究的进一步发展。

Abstract: Computer science education is a dynamic field with many aspects that
influence the learner's path. While these aspects are usually studied in depth
separately, it is also important to carry out broader large-scale studies that
touch on many topics, because they allow us to put different results into each
other's perspective. Past large-scale surveys have provided valuable insights,
however, the emergence of new trends (e.g., AI), new learning formats (e.g.,
in-IDE learning), and the increasing learner diversity highlight the need for
an updated comprehensive study. To address this, we conducted a survey with
18,032 learners from 173 countries, ensuring diverse representation and
exploring a wide range of topics - formal education, learning formats, AI
usage, challenges, motivation, and more. This paper introduces the results of
this survey as an open dataset, describes our methodology and the survey
questions, and highlights, as a motivating example, three possible research
directions within this data: challenges in learning, emerging formats, and
insights into the in-IDE format. The dataset aims to support further research
and foster advancements in computer education.

</details>
